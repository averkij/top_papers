{
    "paper_title": "LLaVE: Large Language and Vision Embedding Models with Hardness-Weighted Contrastive Learning",
    "authors": [
        "Zhibin Lan",
        "Liqiang Niu",
        "Fandong Meng",
        "Jie Zhou",
        "Jinsong Su"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Universal multimodal embedding models play a critical role in tasks such as interleaved image-text retrieval, multimodal RAG, and multimodal clustering. However, our empirical results indicate that existing LMM-based embedding models trained with the standard InfoNCE loss exhibit a high degree of overlap in similarity distribution between positive and negative pairs, making it challenging to distinguish hard negative pairs effectively. To deal with this issue, we propose a simple yet effective framework that dynamically improves the embedding model's representation learning for negative pairs based on their discriminative difficulty. Within this framework, we train a series of models, named LLaVE, and evaluate them on the MMEB benchmark, which covers 4 meta-tasks and 36 datasets. Experimental results show that LLaVE establishes stronger baselines that achieve state-of-the-art (SOTA) performance while demonstrating strong scalability and efficiency. Specifically, LLaVE-2B surpasses the previous SOTA 7B models, while LLaVE-7B achieves a further performance improvement of 6.2 points. Although LLaVE is trained on image-text data, it can generalize to text-video retrieval tasks in a zero-shot manner and achieve strong performance, demonstrating its remarkable potential for transfer to other embedding tasks."
        },
        {
            "title": "Start",
            "content": "LLaVE: Large Language and Vision Embedding Models with Hardness-Weighted Contrastive Learning Zhibin Lan1*, Liqiang Niu2, Fandong Meng2, Jie Zhou2, 1School of Informatics, Xiamen University, China, 2Pattern Recognition Center, WeChat AI, Tencent Inc, China, 3Shanghai Artificial Intelligence Laboratory, China Jinsong Su1,3 5 2 0 2 M 4 ] . [ 1 2 1 8 4 0 . 3 0 5 2 : r lanzhibin@stu.xmu.edu.cn, jssu@xmu.edu.cn {poetniu, fandongmeng, withtomzhou}@tencent.com"
        },
        {
            "title": "LLaVE",
            "content": "LLaVE-0.5B LLaVE-2B LLaVE-7B"
        },
        {
            "title": "Abstract",
            "content": "Universal multimodal embedding models play critical role in tasks such as interleaved imagetext retrieval, multimodal RAG, and multimodal clustering. However, our empirical results indicate that existing LMM-based embedding models trained with the standard InfoNCE loss exhibit high degree of overlap in similarity distribution between positive and negative pairs, making it challenging to distinguish hard negative pairs effectively. To deal with this issue, we propose simple yet effective framework that dynamically improves the embedding models representation learning for negative pairs based on their discriminative difficulty. Within this framework, we train series of models, named LLaVE, and evaluate them on the MMEB benchmark, which covers 4 meta-tasks and 36 datasets. Experimental results show that LLaVE establishes stronger baselines that achieve state-of-the-art (SOTA) performance while demonstrating strong scalability and efficiency. Specifically, LLaVE-2B surpasses the previous SOTA 7B models, while LLaVE-7B achieves further performance improvement of 6.2 points. Although LLaVE is trained on image-text data, it can generalize to text-video retrieval tasks in zero-shot manner and achieve strong performance, demonstrating its remarkable potential for transfer to other embedding tasks."
        },
        {
            "title": "Introduction",
            "content": "Multimodal embedding models aim to encode inputs from any modality into vector representations, which then facilitate various multimodal tasks, such as image-text retrieval (Wu et al., 2021; Zhang et al., 2024a), automatic evaluation (Hessel et al., 2021), and retrieval-augmented generation (RAG) (Zhao et al., 2023). Although advanced pretrained vision-language models such as CLIP (Radford * Work was done when Zhibin Lan was interning at Pattern Recognition Center, WeChat AI, Tencent Inc, China. Corresponding author. et al., 2021), ALIGN (Jia et al., 2021), and SigLIP (Zhai et al., 2023) can provide unified representations for text and images, they face difficulties when dealing with more complex tasks. Particularly, they adopt dual-encoder architecture that encodes images and text separately, leading to poor performance in tasks such as interleaved image-text retrieval (Wu et al., 2021). In recent years, the rapid development and exceptional performance of large multimodal models (LMMs) have prompted researchers to focus increasingly on LMM-based multimodal embedding models. Compared to traditional pretrained visionlanguage models, LMMs not only demonstrate superior multimodal semantic understanding capabilities but also naturally support interleaved text and image inputs. This advantage makes them more flexible and efficient in handling multimodal embedding tasks. As representative work, Jiang et al. (2024b) construct the Massive Multimodal Embedding Benchmark (MMEB), which encompasses 4 meta-tasks and 36 datasets, and train multimodal embedding models based on LMMs. Experiment results demonstrate that by providing suitable task instructions and employing contrastive learning for training, LMM can significantly outperform existing multimodal embedding models and generalize effectively across diverse tasks. However, as shown in Figure 1(a), our preliminary study finds that when training an LMM as multimodal embedding model using the standard InfoNCE loss (van den Oord et al., 2018), the query-target similarity distribution of positive and negative pairs exhibits significant overlap. This indicates that the model struggles to learn discriminative multimodal representations for positive and hard negative pairs. Building on the above observation and insights from preference learning (Rafailov et al., 2023; Song et al., 2024), we propose simple yet effective framework to encourage the model to focus more on hard negative pairs, forcing it to learn more discriminative multimodal representations. Under our framework, we consider the embedding model as policy model and introduce reward model to assign an adaptive weight to each negative pair, where harder pairs are assigned with larger weights. This ensures that harder negative pairs play more significant role in model training. In addition, by decoupling the reward model from the policy model, our framework can not only use different models for hardness estimation but also leverage manually annotated hardness to enhance the representation learning for specific samples. Inspired by SigLIP (Zhai et al., 2023), we introduce cross-device negative sample gathering strategy, which significantly alleviates the issue of limited negative samples in LMMs caused by excessive memory usage. As shown in Figure 1, we observe that our framework increases the query-target similarity gap between positive and negative pairs, indicating its effectiveness in helping the model learn more discriminative multimodal representations. To evaluate the effectiveness of our framework, we train series of multimodal embedding models, referred to as LLaVE (Large Language and Vision Embedding Models), within the proposed framework. These models are based on advanced open-source LMMs of varying scales, including LLaVA-OV-0.5B (Li et al., 2024), Aquila-VL-2B (Gu et al., 2024), and LLaVA-OV-7B (Li et al., 2024). Experimental results on MMEB demonstrate that LLaVE-0.5B achieves comparable results to that of the previous VLM2Vec (phi-3.5V-4B). When scaled up to LLaVE-2B, the model requires only about 17 hours of training on single machine equipped with 8 A100 GPUs (40GB) to surpass the state-of-the-art (SOTA) model MMRet7B, which is pretrained on 27 million image-text pairs. Furthermore, when expanded to LLaVE7B, its performance is even more impressive, surpassing the previous SOTA model by 6.2 points. Meanwhile, when being scaled to different sizes, LLaVE still outperforms the models trained with InfoNCE loss based on the same LMM, demonstrating the effectiveness of our framework. These results fully validate that our framework is both easily scalable and resource-efficient. In addition, despite being trained exclusively on image-text data, LLaVE generalizes effectively to text-video retrieval tasks, showcasing its strong potential for transferring to other embedding tasks. Figure 1: Similarity distributions of learned embeddings on the SUN397 (Xiao et al., 2010) and RefCOCO (Kazemzadeh et al., 2014) dataset. We present the querytarget cosine similarity histograms of positive, hard negative, and easy negative pairs for the model trained with the standard InfoNCE loss and LLaVE."
        },
        {
            "title": "2 Preliminary Study",
            "content": "In this section, we first briefly formulate the multimodal embedding task and review the standard InfoNCE loss. Then, we analyze the similarity distributions of positive and negative pairs in LMMbased embedding models trained with the standard InfoNCE loss."
        },
        {
            "title": "2.1 Contrastive Learning for LMM-based\nMultimodal Embedding Models",
            "content": "Following VLM2Vec (Jiang et al., 2024b), we address the challenge of universal retrieval with LMMs. Specifically, given query-target pair, it can be represented as (q, t+), where both and t+ could be an image, text, or interleaved image-text input. Note that will be equipped with corresponding task instructions for different tasks. The objective of this task is to ensure that the similarity between and t+ is greater than the similarities between and other negative candidates {t}. The aim of contrastive learning is to learn discriminative multimodal representations by pulling closer the representations of queries and targets in positive pairs while pushing apart the representations of queries and targets in negative pairs (Hadsell et al., 2006). Given an LMM, we input the query and the target separately into the model and obtain their representations by extracting the vector representations of the last token in the final layer. Formally, for mini-batch of training data {(q1, t1), ..., (qN , tN )}, the standard InfoNCE loss Type Classification VQA Retrieval Visual Grounding SUN397 ImageNet-R DocVQA TextVQA CIRR Wiki-SS-NQ MSCOCO RefCOCO InfoNCE Positive Hard Negative Easy Negative 0.71 0.65(-0.06) 0.36(-0.35) 0.68 0.59(-0.09) 0.38(-0.30) 0.70 0.62(-0.08) 0.30(-0.40) 0.69 0.62(-0.07) 0.32(-0.37) 0.78 0.76(-0.02) 0.41(-0.37) 0.57 0.55(-0.02) 0.29(-0.28) 0.82 0.76(-0.06) 0.39(-0.43) 0.73 0.64(-0.09) 0.27(-0.46) Precision@1 66.2 86.4 81.2 76.3 LLaVE 39.7 56.6 76.1 83.8 Positive Hard Negative Easy Negative 0.66 0.51(-0.15) 0.03(-0.63) 0.58 0.39(-0.19) 0.07(-0.51) 0.61 0.42(-0.19) -0.04(-0.65) 0.59 0.45(-0.14) 0.01(-0.58) 0.63 0.56(-0.07) 0.01(-0.62) 0.45 0.38(-0.07) 0.03(-0.42) 0.62 0.52(-0.10) -0.02(-0.64) 0.93 0.64(-0.29) 0.05(-0.88) Precision@1 75.5 89.1 88.5 78. 50.0 64.4 80.0 85.5 Table 1: The average cosine similarity between queries and targets is reported for positive, hard negative, and easy negative pairs across eight datasets. The numbers in parentheses represent the similarity difference between negative and positive pairs, where lower values indicate smaller overlap between the two similarity distributions. It can be observed that our method effectively increases the similarity gap between negative and positive pairs, resulting in higher precision. is defined as ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 log (cid:124) esi,i/τ esi,i/τ + (cid:80)N (cid:123)(cid:122) Li j=i esi,j /τ (cid:125) , (1) where si,j = cosine(LMM(qi), LMM(ti)), LMM() denotes the use of LMM for obtaining the representation, and τ represents the temperature hyper-parameter. In this work, τ is always set to 0.02 following the setting of VLM2Vec (Jiang et al., 2024b)."
        },
        {
            "title": "2.2 Analysis",
            "content": "We use Aquila-VL-2B (Gu et al., 2024) as the base model, which builds upon the LLaVA-OneVision architecture, and perform contrastive learning on the MMEB dataset following the VLM2Vec setup. Then, we define the five pairs with the highest query-target similarities (excluding the positive pairs) as hard negative pairs, and the five pairs with the lowest similarities as easy ones. Subsequently, we use the cosine function to calculate the average query-target similarity for the two groups, respectively. As shown in the left part of Figure 1, we visualize the similarity distributions of the trained model on SUN397 and RefCOCO. It can be observed that the distributions of positive and hard negative pairs exhibit significant overlap, while easy negative pairs also demonstrate relatively high similarities. To further explore, we randomly select one in-distribution dataset and one out-of-distribution dataset from the four meta-tasks in MMEB to evaluate the models average query-to-target similarity on positive, hard negative, and easy negative pairs, respectively. As shown in Table 1, the similarity difference between positive and negative pairs in models trained with InfoNCE loss is relatively small (no more than 0.09), especially on CIRR and Wiki-SS-NQ, where the difference is only 0.02. Besides, the model exhibits the lowest precision on these two datasets. Empirically, we observe that the smaller the similarity difference between positive and negative pairs, the lower the final precision tends to be. This observation validates the necessity of enhancing learning on hard negative pairs during training, motivating us to explore simple and effective approach to strengthen the models learning of negative pairs with varying difficulty levels."
        },
        {
            "title": "3 Our Framework",
            "content": "In this section, we first illustrate the inherent consistency between preference learning and contrastive learning. We then propose simple yet effective framework, which mainly involves hardnessweighted contrastive learning (See Section 3.1) and cross-device negative sample gathering (See Section 3.2)."
        },
        {
            "title": "3.1 Hardness-Weighted Contrastive Learning",
            "content": "Preference learning and contrastive learning share fundamental goal: modeling relationships between pairs based on relative preference or similarity of the target within the pairs. Generally, preference learning involves reward model and policy model. The reward model scores the outputs of the policy model, while the policy model where wij represents the weight of learning difficulty. As shown in Figure 2, to estimate the learning difficulty of pairs, we introduce reward model rθ and set wij = erθ(qi,tj ). Accordingly, the policy model adjusts its learning of different negative pairs based on the feedback from the reward model. In this work, to achieve higher training efficiency and simpler implementation, we directly set the reward model rθ to be the same as the policy model rπ, but without backpropagating gradients, i.e. rθ(qi, tj) = α sg(sij), where α is hyperparameter and sg() denotes the stop-gradient operation. Note that rθ can also adopt model structures other than the policy model. Finally, Li is defined as Li = log erπ(qi,ti) + (cid:80)N erπ(qi,ti) j=i e(rπ(qi,tj )+rθ(qi,tj )) (5) We further analyze the gradients with respect to the rπ(qi, tj) (j = i), which are formulated as . Li rπ(qi, tj) = erθ(qi,tj ) erπ(qi,tj ) Zi , (6) Zi = erπ(qi,ti) + (cid:88) j=i e(rπ(qi,tj )+rθ(qi,tj )). (7) From Equation 6, we can observe that the gradients of the negative pairs are proportional to the product rθ(qi, tj), which implies that the greater the learning difficulty of negative pair, the more significant its role in the gradient update."
        },
        {
            "title": "3.2 Cross-Device Negative Sample Gathering",
            "content": "The number of negative pairs in contrastive learning has an important effect on model training. However, LMM-based embedding models face the challenge of high memory consumption, making it difficult to use large batch size directly. To alleviate this issue, inspired by OpenCLIP (Cherti et al., 2023) and SigLIP (Zhai et al., 2023), we adopt cross-device negative sample gathering strategy, which increases the number of negative pairs by factor of the device number K. As illustrated in Figure 3, we expand the number of negative pairs on each device by gathering samples from other devices. Consequently, Li is reformulated as follows: Figure 2: Overview of hardness-weighted contrastive learning. Please note that the policy and reward models are identical in our work. The dashed line indicates directly copying the parameters of the policy model to the reward model. updates its parameters using the feedback from the reward model to produce higher-reward outputs. As typical representative of preference learnthe Bradley-Terry (BT) model (Bradley ing, and Terry, 1952) captures pairwise relationships through probabilistic comparisons. To directly optimize the embedding model (i.e. the policy model), we follow Song et al. (2024) to consider the embedding model as both the reward model and policy model. Formally, given query q1 and two targets t1 and t2, the BT model defines the training objective of preferring t1 over t2 as L1 = log erπ(q1,t1) erπ(q1,t1) + erπ(q1,t2) , (2) where, rπ() denotes the function of reward/policy model. Naturally, we can extend the Bradley-Terry (Bradley and Terry, 1952) model to one-to-N contrast setting (Song et al., 2024), which is essentially consistent with the InfoNCE loss. As result, Equation 2 is derived as Li = log erπ(qi,ti) erπ(qi,ti) + (cid:80)N j=i erπ(qi,tj ) , (3) where rπ(qi, tj) = si,j/τ , si,j and τ have been defined in Section 2.1. Based on observations from the preliminary study, we propose hardnessweighted contrastive learning that assigns weight according to the learning difficulty of the negative pair. Higher weights indicate greater difficulty and incur heavier penalties, encouraging the model to learn more from challenging negative pairs. To this end, the training objective is revised as Li = log erπ(qi,ti) erπ(qi,ti) + (cid:80)N j=i wij erπ(qi,tj ) Li = log , (4) erπ(qi,ti) + (cid:80)N erπ(qi,ti) j=i e(rπ(qi,tj )+rθ(qi,tj )) (8) . nique (Li et al., 2024) to support high-resolution images, setting the maximum image resolution to 672 672. The learning rate is set to 1e-5 for LLaVE-0.5B and LLaVE-2B, and 5e-6 for LLaVE7B. For efficient training, we freeze the vision encoder and train the model for one epoch using the DeepSpeed ZeRO-3 strategy. Regarding training costs, LLaVE-0.5B and LLaVE-2B are trained on 8 NVIDIA A100 GPUs (40GB) for 12 and 17 hours, respectively, while LLaVE-7B is trained on 16 Ascend 910B GPUs (64GB) for 33 hours. More details can be found in Appendix A.2. Baselines. Following VLM2Vec, we compare our model with CLIP (Radford et al., 2021), OpenCLIP (Cherti et al., 2023), BLIP2 (Li et al., 2023a), SigLIP (Zhai et al., 2023), UniIR (Wei et al., 2024), E5-V (Jiang et al., 2024a), and Magiclens (Zhang et al., 2024b). Additionally, we include two powerful models: VLM2Vec (Jiang et al., 2024b) and MMRet-MLLM (Zhou et al., 2024). Among them, MMRet-MLLM enhances downstream task performance through pretraining on self-built retrieval dataset consisting of 26 million pairs. To ensure fairer comparison, we also compare the VLM2Vec trained using the same base LMM, including VLM2Vec (LLaVA-OV-0.5B), VLM2Vec (Aquila-VL-2B), and VLM2Vec (LLaVA-OV-7B)."
        },
        {
            "title": "4.2 Main Results",
            "content": "Table 2 presents the performance comparison of our proposed LLaVE series (LLaVE-0.5B, LLaVE2B, LLaVE-7B) against existing baseline models. Among the baseline models, our trained VLM2Vec (LLaVA-OV-7B) achieves the highest overall average score of 65.8, surpassing the current state-ofthe-art model, MMRet, which achieves the secondbest score of 64.1. This indicates that more powerful foundational LMM can lead to better performance in the embedding models. Notably, MMRet excels in retrieval tasks with score of 69.9, which is attributed to its pretraining on its self-constructed 26M image-text retrieval dataset. VLM2Vec (LLaVA-NeXT-7B-HR) exhibits superior performance in grounding tasks, likely due to its higher input image resolution, which achieves 22.1-point improvement over VLM2Vec (LLaVANeXT-7B-LR). Although previous models have achieved strong performance, our LLaVE series demonstrates consistent improvements over the best baseline across all metrics. LLaVA-NeXT-7B achieves state-ofFigure 3: An example of cross-device negative sample gathering (N =4 and K=3). The plus signs represent positive pairs, and the minus signs represent negative pairs. Each device calculates the similarity between its own queries and the targets on all other devices, which is then used for loss computation. With this strategy, we can effectively increase the number of negative pairs without significantly increasing memory consumption."
        },
        {
            "title": "4.1 Setup",
            "content": "Datasets and Metrics. In this study, we follow VLM2Vec (Jiang et al., 2024b) to train our model on 20 in-distribution datasets from MMEB. These datasets encompass four meta-tasks: classification, VQA, multimodal retrieval, and visual grounding, with total of 662K training pairs. The model is then evaluated on both 20 in-distribution and 16 out-of-distribution test sets from MMEB. We report Precision@1 on each dataset, which measures the proportion of top-ranked candidates that are positive samples. Implementation Details. Our trained model, LLaVE, includes three scales: 0.5B, 2B, and 7B, based on LLaVA-OV-0.5B (Li et al., 2024), AquilaVL-2B (Gu et al., 2024), and LLaVA-OV-7B (Li et al., 2024), respectively. To facilitate community use, the training code for LLaVE is built on the widely-used Transformers (Wolf et al., 2020) and DeepSpeed (Rasley et al., 2020) packages. We use batch size of 256, set the weighting hyperparameter α to 9 1, and impose total length limit of 4096. Furthermore, we employ the Higher Anyres tech1We empirically analyze the impact of α on model performance in Appendix A.1. Model Classification Per Meta-Task Score Retrieval VQA Grounding IND OOD Overall Average Score # Datasets 10 10 12 20 16 36 CLIP (Radford et al., 2021) BLIP2 (Li et al., 2023a) SigLIP (Zhai et al., 2023) OpenCLIP (Cherti et al., 2023) UniIR (BLIPF ) (Wei et al., 2024) UniIR (CLIPSF ) (Wei et al., 2024) Magiclens (Zhang et al., 2024b) Baslines 42.8 27.0 40.3 47.8 42.1 44.3 38. 9.1 4.2 8.4 10.9 15.0 16.2 8.3 LMM-based Baselines E5-V (Jiang et al., 2024a) VLM2Vec (Phi-3.5-V-4B) (Jiang et al., 2024b) VLM2Vec (LLaVA-NeXT-7B-LR) (Jiang et al., 2024b) VLM2Vec (LLaVA-NeXT-7B-HR) (Jiang et al., 2024b) MMRet (LLaVA-NeXT-7B) (Zhou et al., 2024) 21.8 54.8 54.7 61.2 56.0 4.9 54.9 50.3 49.9 57.4 Our trained LMM-based Baselines VLM2Vec (LLaVA-OV-0.5B) VLM2Vec (Aquila-VL-2B) VLM2Vec (LLaVA-OV-7B) LLaVE-0.5B LLaVE-2B LLaVE-7B - Best baseline 54.6 61.1 63.5 Ours 57.4 62.1 65.7 +2.2 44.7 57.3 61. 50.3 60.2 65.4 +4.3 53.0 33.9 31.6 52.3 60.1 61.8 35.4 11.5 62.3 56.2 67.4 69.9 56.8 62.1 64.5 59.8 65.2 70.9 +1.0 51.8 47.0 59.5 53.3 62.2 65.3 26. 19.0 79.5 64.0 86.1 83.6 76.5 85.5 87.3 82.9 84.9 91.9 +4.6 37.1 25.3 32.3 39.3 44.7 47.1 31.0 14.9 66.5 61.0 67.5 68.0 59.8 67.2 69. 64.7 69.4 75.0 +5.3 38.7 25.1 38.0 40.2 40.4 41.7 23.7 11.5 52.0 47.5 57.1 59.1 49.1 58.1 61.0 52.0 59.8 64.4 +3.4 37.8 25.2 34.8 39.7 42.8 44.7 27. 13.3 60.1 55.0 62.9 64.1 55.0 63.1 65.8 59.1 65.2 70.3 +4.5 Table 2: Results on the MMEB benchmark. IND represents the in-distribution dataset, and OOD represents the out-of-distribution dataset. In UniIR, the FF and SF subscripts under CLIP or BLIP represent feature-level fusion and score-level fusion, respectively. LLaVA-NeXT-7B-LR indicates the use of low-resolution (336336) image inputs, while LLaVA-NeXT-7B-HR refers to the use of high-resolution (13441344) image inputs. The reported scores are the average Precision@1 over the corresponding datasets. The best results are marked in bold, and the second-best results are underlined. Part of the baseline results are sourced from (Jiang et al., 2024b) and (Zhou et al., 2024). Detailed results and qualitative evaluations can be found in Appendix A.3 and Appendix A.4. the-art overall score of 70.3, outperforming the previous SOTA model, MMRet, by 6.2 points and surpassing VLM2Vec (LLaVA-OV-7B) by 4.5 points. In grounding, LLaVE-7B attains an impressive score of 91.9, +4.6 point improvement over VLM2Vec. LLaVE-7B also leads in VQA (65.4) and classification (65.7), with improvements of +4.3 and +2.2 points, respectively. In addition, we observe that the performance of LLaVE models scales consistently with model size, indicating that our framework has excellent scalability. It is worth mentioning that the performance of LLaVE-0.5B is already comparable to VLM2Vec (Phi-3.5-V-4B), while LLaVE-2B achieves an overall score of 65.2, surpassing the previously pretrained MMRet-7B that utilizes an additional 27M image-text pairs."
        },
        {
            "title": "4.3 Ablation Study",
            "content": "Table 3 provides an ablation study analyzing the impact of various design choices on the performance of LLaVE-2B across IND, OOD, and overall metrics. The baseline model (ID 1) uses the standard InfoNCE with up to 50K samples per training dataset for balanced fine-tuning. Freezing the image encoder helps generalize to out-of-distribution datasets. Freezing the image encoder (1 2) can significantly improve performance on unseen datasets at the cost of slight decrease in in-distribution performance. This is likely because the original vision encoder possesses stronger generalization capabilities, and instruction fine-tuning on smaller datasets may affect this ability. However, freezing the projector (1 3) harms the models performance because transforming the LMM into an embedding model requires re-adaptation. When the data is sufficient, balanced distribution of various data is more important than simply having more data. Table 3 (2 4) demonstrates that with limited data, the models generalization ability is constrained. When the data samID Model 0 1 2 3 4 5 6 7 8 Previous SOTA (MMRet) Aquila-VL-2B + InfoNCE 1 + Freeze image encoder 2 + Freeze projector 2 + Less training data (Each training dataset samples up to 20K data) 2 + More training data (Each training dataset samples up to 100K data) 2 + Cross-device negative sample gathering 6 + Focal-InfoNCE loss (Hou and Li, 2023) 6 + Hardness-weighted contrastive learning (LLaVE-2B) IND 68. 60.6 60.5 (-0.1) 60.3 (-0.2) 60.4 (-0.1) 61.2 (+0.7) 68.6 (+8.1) 67.9 (-0.7) 69.4 (+0.8) OOD 59. 56.4 58.3 (+2.1) 57.0 (-1.3) 56.4 (-1.9) 57.4 (-0.9) 58.4 (+0.1) 59.5 (+1.1) 59.8 (+1.4) Overall 64. 58.7 59.5 (+0.8) 58.8 (-0.7) 58.6 (-0.9) 59.5 (+0.0) 64.0 (+4.5) 64.2 (+0.2) 65.1 (+1.1) Table 3: Ablation results on MMEB. The model with ID 1 is configured to use the standard InfoNCE for full fine-tuning, with each training dataset sampling up to 50K examples, so as to ensure balance across different datasets. The numbers in parentheses indicate the impact of the changes on performance compared to the model corresponding to the previously selected ID. pling limit is increased to 100K (2 5), certain training datasets expand further, while others remain unchanged due to limited availability. The results show that although this approach improves indistribution performance, it negatively impacts generalization, highlighting the importance of maintaining balanced distribution across different meta-tasks. The number of negatives is crucial for training LMM-based embedding models. By analyzing the performance of the model (ID 6), we observe that the introduction of the cross-device negative sample gathering strategy leads to substantial gains in IND (+8.1) with negligible impact of OOD (+0.1), resulting in notable overall improvement of +4.5, which highlights the importance of diverse negative samples. Hardness-weighted contrastive learning can further enhance the performance of powerful models on both in-distribution and outof-distribution datasets. Although the model (ID 6) already achieves near-SOTA performance, hardness-weighted contrastive learning further enhances its effectiveness, particularly with 1.4point improvement on out-of-distribution datasets, demonstrating the validity of our approach. Besides, we also compare the Focal-InfoNCE loss (ID 7), which also weights positive and negative pairs. Although it slightly improves performance on the out-of-distribution dataset, it reduces performance on the in-distribution dataset. As shown in Figure 1 and Table 1, our framework significantly increases the similarity gap between positive and negative pairs, thereby improving the models discriminative capability. Model MSR-VTT MSVD R@1 R@5 R@10 R@1 R@5 R@ Zero-shot (finetuned with text-video data) 40.0 InternVideo 42.4 ViCLIP UMT-L 42.6 InternVideo2-6B 55.9 65.3 - 64.4 78.3 74.1 - 73.1 85.1 43.4 49.1 49.9 59.3 69.9 - 77.7 84. Zero-shot (finetuned only with text-image data) VLM2Vec LamRA LLaVE-7B 43.5 44.7 46.8 69.3 68.6 71.1 78.9 78.6 80.0 49.5 52.4 52. 77.5 79.8 80.1 79.1 - 85.3 89.6 85.7 87.0 87.0 Table 4: Results of zero-shot text-to-video retrieval. The gray font indicates that the model is trained using contrastive learning on tens of millions of text-video data."
        },
        {
            "title": "4.4 Zero-shot Video Retrieval",
            "content": "We also evaluate LLaVE on the widely used textvideo retrieval datasets: MSR-VTT (Xu et al., 2016) and MSVD (Chen and Dolan, 2011), to explore its generalization capability. There are two types of comparative models considered. The first type includes models trained on tens of millions of video-text data, such as InternVideo (Wang et al., 2022), ViCLIP (Wang et al., 2024a), UMT-L (Li et al., 2023b), and InternVideo2-6B (Wang et al., 2024b). The second type is completely zero-shot, trained only on text-image data using contrastive learning and directly evaluated on text-video data. Particularly, we focus on the two strongest models, VLM2Vec (LLaVA-OV-7B) and LamRA (Liu et al., 2024), which is based on Qwen2-VL-7B and consists of two 7B models: LamRA-Ret and LamRA-Rank. LamRA-Ret retrieves the top-K candidates, while LamRA-Rank further re-ranks these retrieved candidates. To enable video embedding, we set the maximum number of sampled frames to 32, expand the total input length of the model to 8192, and reduce the visual features of the video by 4 times through bilinear interpolation. It is observed that, compared to LamRA, LLaVE-7B requires only single model and shows consistent improvements across all metrics. Notably, on MSR-VTT, the R@1, R@5, and R@10 scores increase by 2.1, 2.5, and 1.4, respectively. Moreover, although LLaVE7B does not utilize text-video data for contrastive training, its performance still surpasses most video retrieval models except for InternVideo2-6B, which are trained on tens of millions of video-text pairs. These results demonstrate that LLaVE-7B has strong potential for transferring to other embedding tasks."
        },
        {
            "title": "5 Related Work",
            "content": "Multimodal Embeddings. As significant research direction, multimodal embeddings aim to integrate the information from multiple modalities (e.g., vision and language) into shared representation space, which enables seamless understanding across modalities. Early research primarily focuses on leveraging dual-stream architectures to separately encode texts and images. For instance, CLIP (Radford et al., 2021), ALIGN (Jia et al., 2021), BLIP (Li et al., 2022), and SigLIP (Zhai et al., 2023) all adopt dual-encoder frameworks, learning universal representations from large-scale weakly supervised image-text pairs through contrastive learning. To learn the more universal multimodal representations, UniIR (Wei et al., 2024) proposes two fusion mechanisms to combine the visual and textual representations generated by the dual-encoder model. Although these models have achieved impressive results, they still face challenges in handling tasks such as interleaved imagetext retrieval (Wu et al., 2021) and instructionfollowing multimodal retrieval. LMM-based Multimodal Embeddings. To address the aforementioned issue, E5-V (Jiang et al., 2024a) and VLM2Vec (Jiang et al., 2024b) transform LMM into multimodal embedding models through contrastive learning, fully leveraging LMMs powerful multimodal understanding capability and its inherent advantage in handling interleaved text-image input. Recently, few concurrent studies further explored the application of LMMs in multimodal embeddings. For example, LamRA (Liu et al., 2024) adopts the retrieval model to select the top-K candidates, which are then scored by the reranking models. Finally, the scores from the retrieval and reranking models are combined using weighted sum to produce the final score for retrieval. MMRet (Zhou et al., 2024) creates large-scale multimodal instruction retrieval dataset called MegaPairs. By pretraining on this dataset, MMRet achieves SOTA results on MMEB. Contrastive Learning. Negative samples play crucial role in contrastive learning, Chen et al. (2020) show that incorporating more negative samples can enhance the performance of contrastive learning. Awasthi et al. (2022) further explore the impact of the number of negative samples from both theoretical and empirical perspectives. (2020) demonstrate that Moreover, Cai et al. hard negative samples are both necessary and sufficient to learn more discriminative representation, and Robinson et al. (2021) propose hard negative sampling strategy where the user can control the hardness. The study most similar to ours is Focal-InfoNCE (Hou and Li, 2023), which weighs both positive and negative pairs based on their query-target similarities. Specifically, it uses fixed threshold to determine the hardness of negative pairs, increasing the weight if the similarity exceeds the threshold and decreasing it otherwise. Unlike this work, our hardness-weighted contrastive learning introduces reward model to dynamically estimate the hardness of negative pairs and applies weighting only to the negative pairs based on the estimated hardness. Notably, the reward model can be decoupled from the policy model."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we conduct preliminary study to find that LMM-based embedding models trained with the standard InfoNCE loss face significant challenges in handling hard negative pairs. To address this issue, we propose simple yet effective framework that includes the hardness-weighted contrastive learning and the cross-device negative sample gathering strategy to enhance the models learning of negative pairs with varying difficulty levels. This framework significantly improves the models capacity to distinguish between positive and negative pairs. Experimental results and indepth analyses validate the effectiveness of the proposed framework. In the future, we plan to collect and construct universal multimodal embedding benchmark for video-text retrieval, aiming to investigate the more universal multimodal embedding models. We will open-source all models and code, hoping to inspire further research in this field."
        },
        {
            "title": "Limitations",
            "content": "LLaVE is trained only on embedding datasets that contain arbitrary combinations of text and image modalities. Although it can generalize to embedding tasks that include the video modality in zeroshot manner, there is still significant room for improvement. Constructing multimodal embedding benchmark that incorporates the video modality will be crucial direction for training more generalizable embedding models."
        },
        {
            "title": "References",
            "content": "Pranjal Awasthi, Nishanth Dikkala, and Pritish Kamath. 2022. Do more negative samples necessarily hurt in contrastive learning? In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 11011116. PMLR. Ralph Allan Bradley and Milton Terry. 1952. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324 345. Tiffany Tianhui Cai, Jonathan Frankle, David J. Schwab, and Ari S. Morcos. 2020. Are all negatives created equal in contrastive instance discrimination? CoRR, abs/2010.06682. David L. Chen and William B. Dolan. 2011. Collecting highly parallel data for paraphrase evaluation. In The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference, 19-24 June, 2011, Portland, Oregon, USA, pages 190200. The Association for Computer Linguistics. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. 2020. simple framework for contrastive learning of visual representations. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 15971607. PMLR. Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. 2023. Reproducible scaling laws for contrastive language-image learning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pages 28182829. IEEE. Shuhao Gu, Jialing Zhang, Siyuan Zhou, Kevin Yu, Zhaohu Xing, Liangdong Wang, Zhou Cao, Jintao Jia, Zhuoyi Zhang, Yixuan Wang, Zhenchong Hu, Bo-Wen Zhang, Jijie Li, Dong Liang, Yingli Zhao, Yulong Ao, Yaoqi Liu, Fangxiang Feng, and Guang Liu. 2024. Infinity-mm: Scaling multimodal performance with large-scale and high-quality instruction data. CoRR, abs/2410.18558. Raia Hadsell, Sumit Chopra, and Yann LeCun. 2006. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2006), 17-22 June 2006, New York, NY, USA, pages 17351742. IEEE Computer Society. Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. 2021. Clipscore: referencefree evaluation metric for image captioning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 711 November, 2021, pages 75147528. Association for Computational Linguistics. Pengyue Hou and Xingyu Li. 2023. Improving contrastive learning of sentence embeddings with focal infonce. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pages 47574762. Association for Computational Linguistics. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. 2021. Scaling up visual and vision-language representation learning with noisy text supervision. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 49044916. PMLR. Ting Jiang, Minghui Song, Zihan Zhang, Haizhen Huang, Weiwei Deng, Feng Sun, Qi Zhang, Deqing Wang, and Fuzhen Zhuang. 2024a. E5-V: universal embeddings with multimodal large language models. CoRR, abs/2407.12580. Ziyan Jiang, Rui Meng, Xinyi Yang, Semih Yavuz, Yingbo Zhou, and Wenhu Chen. 2024b. Vlm2vec: Training vision-language models for massive multimodal embedding tasks. CoRR, abs/2410.05160. Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara L. Berg. 2014. Referitgame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, meeting of SIGDAT, Special Interest Group of the ACL, pages 787798. ACL. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. 2024. Llava-onevision: Easy visual task transfer. CoRR, abs/2408.03326. Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. 2023a. BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 1973019742. PMLR. Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi. 2022. BLIP: bootstrapping language-image pretraining for unified vision-language understanding and generation. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 1288812900. PMLR. Kunchang Li, Yali Wang, Yizhuo Li, Yi Wang, Yinan He, Limin Wang, and Yu Qiao. 2023b. Unmasked teacher: Towards training-efficient video foundation models. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023, pages 1989119903. IEEE. Yikun Liu, Pingan Chen, Jiayin Cai, Xiaolong Jiang, Yao Hu, Jiangchao Yao, Yanfeng Wang, and Weidi Lamra: Large multimodal model Xie. 2024. Preprint, as your advanced retrieval assistant. arXiv:2412.01720. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning transferable visual models from natural language supervision. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 87488763. PMLR. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly reward model. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In KDD 20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020, pages 35053506. ACM. Joshua David Robinson, Ching-Yao Chuang, Suvrit Sra, and Stefanie Jegelka. 2021. Contrastive learning with hard negative samples. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, and Houfeng Wang. 2024. Preference ranking optimization for human alignment. In Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada, pages 18990 18998. AAAI Press. Aäron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding. CoRR, abs/1807.03748. Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, Ping Luo, Ziwei Liu, Yali Wang, Limin Wang, and Yu Qiao. 2024a. Internvid: large-scale video-text dataset for multimodal understanding and generation. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Zun Wang, Yansong Shi, Tianxiang Jiang, Songze Li, Jilan Xu, Hongjie Zhang, Yifei Huang, Yu Qiao, Yali Wang, and Limin Wang. 2024b. Internvideo2: Scaling foundation models for multimodal video understanding. In Computer Vision - ECCV 2024 - 18th European Conference, Milan, Italy, September 29October 4, 2024, Proceedings, Part LXXXV, volume 15143 of Lecture Notes in Computer Science, pages 396416. Springer. Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, Sen Xing, Guo Chen, Junting Pan, Jiashuo Yu, Yali Wang, Limin Wang, and Yu Qiao. 2022. Internvideo: General video foundation models via generative and discriminative learning. CoRR, abs/2212.03191. Cong Wei, Yang Chen, Haonan Chen, Hexiang Hu, Ge Zhang, Jie Fu, Alan Ritter, and Wenhu Chen. 2024. Uniir: Training and benchmarking universal multimodal information retrievers. In Computer Vision - ECCV 2024 - 18th European Conference, Milan, Italy, September 29-October 4, 2024, Proceedings, Part LXXXVII, volume 15145 of Lecture Notes in Computer Science, pages 387404. Springer. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, EMNLP 2020 - Demos, Online, November 16-20, 2020, pages 3845. Association for Computational Linguistics. Hui Wu, Yupeng Gao, Xiaoxiao Guo, Ziad Al-Halah, Steven Rennie, Kristen Grauman, and Rogério Feris. 2021. Fashion IQ: new dataset towards retrieving images by natural language feedback. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pages 1130711317. Computer Vision Foundation / IEEE. Jianxiong Xiao, James Hays, Krista A. Ehinger, Aude Oliva, and Antonio Torralba. 2010. SUN database: Large-scale scene recognition from abbey to zoo. In The Twenty-Third IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2010, San Francisco, CA, USA, 13-18 June 2010, pages 3485 3492. IEEE Computer Society. Jun Xu, Tao Mei, Ting Yao, and Yong Rui. 2016. MSRVTT: large video description dataset for bridging video and language. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 52885296. IEEE Computer Society. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. 2023. Sigmoid loss for language In IEEE/CVF International image pre-training. Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023, pages 1194111952. IEEE. Kai Zhang, Yi Luan, Hexiang Hu, Kenton Lee, Siyuan Qiao, Wenhu Chen, Yu Su, and Ming-Wei Chang. 2024a. Magiclens: Self-supervised image retrieval with open-ended instructions. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. Kai Zhang, Yi Luan, Hexiang Hu, Kenton Lee, Siyuan Qiao, Wenhu Chen, Yu Su, and Ming-Wei Chang. 2024b. Magiclens: Self-supervised image retrieval with open-ended instructions. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. Ruochen Zhao, Hailin Chen, Weishi Wang, Fangkai Jiao, Do Xuan Long, Chengwei Qin, Bosheng Ding, Xiaobao Guo, Minzhi Li, Xingxuan Li, and Shafiq Joty. 2023. Retrieving multimodal information for augmented generation: survey. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pages 4736 4756. Association for Computational Linguistics. Junjie Zhou, Zheng Liu, Ze Liu, Shitao Xiao, Yueze Wang, Bo Zhao, Chen Jason Zhang, Defu Lian, and Yongping Xiong. 2024. Megapairs: Massive data synthesis for universal multimodal retrieval. Preprint, arXiv:2412.14475. Figure 4: Influence of the α on model performance, measured on IND and OOD datasets, respectively. Hyperparameter LLaVE-0.5B LLaVE-2B LLaVE-7B #Data Batch size lr lr schedule lr warmup ratio Weight decay Epoch Optimizer DeepSpeed stage GPU Training cost (#Hours) 662K 256 1e-5 8 A100 12 662K 256 1e-5 cosine decay 0.03 0 1 AdamW 3 8 A100 17 662K 256 5e-6 16 910B Table 5: Training details of LLaVE."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Hyperparameter Analysis We experimentally explore the influence of the hyperparameter α on model performance. Figure 4 shows that increasing α positively influences both IND and OOD performance, stopping at certain point. This indicates that appropriately weighting hard negative samples helps the model learn effectively. Moreover, it can be observed that the models performance is robust to the α parameter, consistently outperforming the results obtained without hardness-weighted contrastive learning (i.e., when α=0). A.2 Training Details We present the training details of LLaVE in Table 5. Due to resource constraints, LLaVE-7B is trained on 910B GPUs, and the training time will significantly decrease when conducted on A100 GPUs. In addition, using more GPUs to adopt larger batch size or higher resolution will improves the models performance. A.3 Detailed results on MMEB We present the detailed results of each model on various datasets of MMEB in Table 6. A.4 Qualitative Evaluation In Figure 5, we present the qualitative evaluation results of LLaVE and the strongest baseline model, VLM2Vec (LLaVA-OV-7B). As shown in the upper part of the figure, LLaVE successfully identifies and retrieves the modified target images that meet the specified requirement (dogs walking through snow\") in the Top-2 retrieval, while VLM2Vec only retrieves images similar to the original image. Similarly, in the lower part of the figure, LLaVE fulfills the requirement of white in color and is shorter\" in the Top-3 retrieval. These examples demonstrate that our framework effectively facilitates the model to capture complex intents in challenging samples and enhances the discriminability of hard samples. CLIP OpenCLIP SigLIP BLIP2 MagicLens E5-V UniIR VLM2Vec MMRet LLaVE Classification (10 tasks) ImageNet-1K N24News HatefulMemes VOC2007 SUN397 Place365 ImageNet-A ImageNet-R ObjectNet Country-211 All Classification VQA (10 tasks) OK-VQA A-OKVQA DocVQA InfographicsVQA ChartQA Visual7W ScienceQA VizWiz GQA TextVQA All VQA Retrieval (12 tasks) VisDial CIRR VisualNews_t2i VisualNews_i2t MSCOCO_t2i MSCOCO_i2t NIGHTS WebQA FashionIQ Wiki-SS-NQ OVEN EDIS All Retrieval Visual Grounding (4 tasks) MSCOCO RefCOCO RefCOCO-matching Visual7W-pointing All Visual Grounding Final Score (36 tasks) All All IND All OOD 55.8 34.7 51.1 50.7 43.4 28.5 25.5 75.6 43.4 19.2 42.8 7.5 3.8 4.0 4.6 1.4 4.0 9.4 8.2 41.3 7.0 9.1 30.7 12.6 78.9 79.6 59.5 57.7 60.4 67.5 11.4 55.0 41.1 81.0 53.0 33.8 56.9 61.3 55.1 51. 37.8 37.1 38.7 63.5 38.6 51.7 52.4 68.8 37.8 14.2 83.0 51.4 16.8 47.8 11.5 3.3 5.3 4.6 1.5 2.6 10.2 6.6 52.5 10.9 10.9 25.4 15.4 74.0 78.0 63.6 62.1 66.1 62.1 13.8 44.6 45.0 77.5 52.3 34.5 54.2 68.3 56.3 53.3 39.7 39.3 40. 45.4 13.9 47.2 64.3 39.6 20.0 42.6 75.0 40.3 14.2 40.3 2.4 1.5 4.2 2.7 3.0 1.2 7.9 2.3 57.5 1.0 8.4 21.5 15.1 51.0 52.4 58.3 55.0 62.9 58.1 20.1 55.1 56.0 23.6 31.6 46.4 70.8 50.8 70.1 59.5 34.8 32.3 38.0 10.3 36.0 49.6 52.1 34.5 21.5 3.2 39.7 20.6 2.5 27. 8.7 3.2 2.6 2.0 0.5 1.3 6.8 4.0 9.7 3.3 4.2 18.0 9.8 48.1 13.5 53.7 20.3 56.5 55.4 9.3 28.7 39.5 54.4 33.9 28.9 47.4 59.5 52.0 47.0 25.2 25.3 25.1 48.0 33.7 49.0 51.6 57.0 31.5 8.0 70.9 31.6 6.2 38.8 12.7 2.9 3.0 5.9 0.9 2.5 5.2 1.7 43.5 4.6 8. 24.8 39.1 50.7 21.1 54.1 40.0 58.1 43.0 11.2 18.7 1.6 62.6 35.4 22.1 22.8 35.6 23.4 26.0 27.8 31.0 23.7 9.6 23.4 49.7 49.9 33.1 8.6 2.0 30.8 7.5 3.1 21.8 8.9 5.9 1.7 2.3 2.4 5.8 3.6 2.6 7.8 8.2 4.9 9.2 6.1 13.5 8.1 20.7 14.0 4.2 17.7 2.8 8.6 5.9 26.8 11. 10.8 11.9 38.9 14.3 19.0 13.3 14.9 11.5 58.3 42.5 56.4 66.2 63.2 36.5 9.8 66.2 32.2 11.3 44.3 25.4 8.8 6.2 4.6 1.6 14.5 12.8 24.3 48.8 15.1 16.2 42.2 51.3 74.3 76.8 68.5 72.1 66.2 89.6 40.2 12.2 69.4 79.2 61.8 46.6 67.8 62.9 71.3 65. 44.7 47.1 41.7 67.8 76.3 65.8 88.9 74.4 43.0 51.4 86.3 59.5 21.4 63.5 67.3 63.6 86.6 51.9 54.9 48.7 46.6 48.3 66.8 76.0 61.1 73.8 50.3 69.7 72.3 74.5 71.7 66.5 87.5 20.6 53.7 67.0 66.6 64.5 80.6 92.3 85.3 91.0 87.3 65.8 61.0 69. 58.8 71.3 53.7 85.0 70.0 43.0 36.1 71.6 55.8 14.7 56.0 73.3 56.7 78.5 39.3 41.7 49.5 45.2 51.7 59.0 79.0 57.4 83.0 61.4 74.2 78.1 78.6 72.4 68.3 90.2 54.9 24.9 87.5 65.6 69.9 76.8 89.8 90.6 77.0 83.6 64.1 59.1 68.0 77.1 82.1 74.3 90.3 79.1 45.1 51.6 90.9 46.2 20.1 65. 71.1 70.8 90.3 53.5 62.2 55.8 54.4 48.5 68.4 79.4 65.4 83.0 54.5 76.6 81.2 78.9 74.7 67.0 90.4 23.3 63.9 68.0 89.1 70.9 87.0 95.4 92.8 92.5 91.9 70.3 64.4 75.0 Table 6: The detailed results of the baselines and LLaVE on MMEB. The out-of-distribution datasets are highlighted with yellow background in the table. We only include the best version of each series of models in the table, such as LLaVE-7B and VLM2Vec (LLaVA-OV-7B). Figure 5: Qualitative evaluation comparing LLaVE and VLM2Vec. Retrievals consistent with the ground truth are highlighted with red borders. From left to right, the images represent the top-1 to top-3 retrieval results."
        }
    ],
    "affiliations": [
        "Pattern Recognition Center, WeChat AI, Tencent Inc, China",
        "School of Informatics, Xiamen University, China",
        "Shanghai Artificial Intelligence Laboratory, China"
    ]
}