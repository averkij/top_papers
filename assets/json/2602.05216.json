{
    "paper_title": "Semantic Search over 9 Million Mathematical Theorems",
    "authors": [
        "Luke Alexander",
        "Eric Leonen",
        "Sophie Szeto",
        "Artemii Remizov",
        "Ignacio Tejeda",
        "Giovanni Inchiostro",
        "Vasily Ilin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Searching for mathematical results remains difficult: most existing tools retrieve entire papers, while mathematicians and theorem-proving agents often seek a specific theorem, lemma, or proposition that answers a query. While semantic search has seen rapid progress, its behavior on large, highly technical corpora such as research-level mathematical theorems remains poorly understood. In this work, we introduce and study semantic theorem retrieval at scale over a unified corpus of $9.2$ million theorem statements extracted from arXiv and seven other sources, representing the largest publicly available corpus of human-authored, research-level theorems. We represent each theorem with a short natural-language description as a retrieval representation and systematically analyze how representation context, language model choice, embedding model, and prompting strategy affect retrieval quality. On a curated evaluation set of theorem-search queries written by professional mathematicians, our approach substantially improves both theorem-level and paper-level retrieval compared to existing baselines, demonstrating that semantic theorem search is feasible and effective at web scale. The theorem search tool is available at \\href{https://huggingface.co/spaces/uw-math-ai/theorem-search}{this link}, and the dataset is available at \\href{https://huggingface.co/datasets/uw-math-ai/TheoremSearch}{this link}."
        },
        {
            "title": "Start",
            "content": "Semantic Search over 9 Million Mathematical Theorems Luke Alexander * 1 2 Eric Leonen * 1 3 Sophie Szeto * 1 2 4 Artemii Remizov 1 5 Ignacio Tejeda 1 2 Giovanni Inchiostro 1 2 Vasily Ilin 1 2 6 2 0 2 5 ] I . [ 1 6 1 2 5 0 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Searching for mathematical results remains difficult: most existing tools retrieve entire papers, while mathematicians and theorem-proving agents often seek specific theorem, lemma, or proposition that answers query. While semantic search has seen rapid progress, its behavior on large, highly technical corpora such as researchlevel mathematical theorems remains poorly understood. In this work, we introduce and study semantic theorem retrieval at scale over unified corpus of 9.2 million theorem statements extracted from arXiv and seven other sources, representing the largest publicly available corpus of human-authored, research-level theorems. We represent each theorem with short naturallanguage description as retrieval representation and systematically analyze how representation context, language model choice, embedding model, and prompting strategy affect retrieval quality. On curated evaluation set of theoremsearch queries written by professional mathematicians, our approach substantially improves both theorem-level and paper-level retrieval compared to existing baselines, demonstrating that semantic theorem search is feasible and effective at web scale. The theorem search tool is available at this link, and the dataset is available at this link. ing for both human mathematicians and automated proof systems (Polu & Sutskever, 2020; Yang et al., 2023; Wu et al., 2022). researcher proving new result must first determine whether the statement already exists in the literature, and similarly, an AI agent generating formal proofs benefits from retrieving relevant lemmas to guide its search. Yet most existing tools Google Scholar, arXiv, and even modern LLMs with web access operate at the level of entire documents, forcing users to manually scan papers when they seek specific statement. This gap is increasingly significant. arXiv hosts over 2.4 million papers, including more than 690,000 in mathematics (Ginsparg, 1994). study of over 14,000 withdrawn arXiv preprints found that 2.5% were retracted because the authors results already appeared in prior literature (Rao et al., 2024). For example, Popescu-Pampu (2007), Zhang (2012), and Shahryari (2020) were withdrawn after discovering their main results had been previously established. AI systems face the same problem: the Erdos Problems Project documented cases where AI tools solved open problems that had been established decades earlier (Erdos & Newman, 1977; Wirsing & Schwarz, 1961; Klarner, 1966) underscoring the need for theorem-level search. In this work, we construct corpus of over 9 million theorem statements from arXiv, the Stacks Project, ProofWiki, and five other sources, and study semantic retrieval at scale. We represent each theorem using natural-language slogan generated by an LLM, then embed slogans and queries into shared semantic space. Our main contributions are: 1. Introduction Mathematical knowledge is organized around discrete results: theorems, lemmas, propositions, and corollaries. These statements serve as the fundamental units of reason- *Equal contribution 1Math AI Lab, University of Washington, Seattle, United States 2Department of Mathematics, University of Washington, Seattle, United States 3Department of Applied and Computational Mathematical Sciences, University of Washington, Seattle, United States 4Paul G. Allen School of Computer Science and Engineering, University of Washington, Seattle, United States 5Lake Washington High School, Kirkland, United States. Correspondence to: Vasily Ilin <vilin@uw.edu>. Preprint. February 6, 2026. 1. large-scale theorem corpus. We release over 9 million theorem statements with rich metadata, the largest collection of informal mathematical theorems to date. 2. systematic study of representation choices. We analyze how the context, LLM choice, embedding model, and prompting strategy affect retrieval. Notably, we find that embedding theorems via natural-language slogans significantly outperforms embedding their raw LATEX formulations. 3. State-of-the-art retrieval. On 111 queries from professional mathematicians, we achieve 45.0% Hit@20 Theorem Search at the theorem level, outperforming ChatGPT 5.2 with search (19.8%) and Gemini 3 Pro (27.0%). For paperlevel retrieval, we achieve 56.8% Hit@20 compared to 37.8% for Google Search. Our results demonstrate that semantic theorem search is feasible at web scale. public demo is available at huggingface.co/spaces/uw-math-ai/theorem-search. The dataset is available at huggingface.co/datasets/uw-mathai/theorem-search-dataset. datasheet for our dataset is provided in Appendix A. 2. Related Work Mathematical Information Retrieval. Early MathIR work focused on formula-level retrieval, with the NTCIR Math Tasks (Aizawa et al., 2014; Zanibbi et al., 2016) establishing benchmarks for formula search over arXiv and Wikipedia. The ARQMath shared tasks (Mansouri et al., 2022) extended this to mathematical question answering over Math Stack Exchange. recent survey (Dadure et al., 2024) notes that while formula retrieval has progressed, semantic understanding of mathematical statements remains an open challenge the gap our work addresses. Dense Retrieval and LLM-Augmented Search. Dense Passage Retrieval (Karpukhin et al., 2020) showed that dualencoder architectures can outperform sparse methods like BM25. Sentence-BERT (Reimers & Gurevych, 2019) enabled efficient semantic similarity via Siamese networks, while E5 (Wang et al., 2022), Qwen3-Embedding (Zhang et al., 2025), and Gemma Embedding (Vera et al., 2025) have pushed embedding quality further. ColBERT (Khattab & Zaharia, 2020) introduced late interaction for fine-grained token matching. Retrieval-augmented generation (RAG) (Lewis et al., 2020) combines retrieval with language model generation, enabling systems to ground responses in retrieved documents. These advances underpin our approach, though mathematical text poses unique challenges due to symbolic notation. Search for Formal Mathematics. LeanSearch (Gao et al., 2024) provides semantic search over Mathlib4s 230,000+ theorems by generating natural-language descriptions and using dense retrieval. ReProver (Yang et al., 2023) uses retrieval-augmented generation to select premises during proof search, and Numina-Lean-Agent (Liu et al., 2026) integrates multiple search tools, including LeanDex, to retrieve lemmas across libraries. LeanFinder (Lu et al., 2025) focuses on user intent, while LeanExplore (Asher, 2025) combines embeddings with BM25+ and PageRank. Jiang et al. (2023) demonstrate that language models can translate between formal and natural language mathematics. However, formal libraries cover only fraction of mathematical knowledge; our work extends informalization-based retrieval to millions of LATEX theorem statements. Scientific Literature Search. General academic search engines (Google Scholar, Semantic Scholar) and arXiv provide paper-level retrieval but cannot target individual theorems. The zbMATH database (Steinfeldt & Mihaljevic, 2024) offers curated paper-level indexing. Large language models with web access, such as GPT-4 (Achiam et al., 2023) and Gemini (Google DeepMind, 2025), can answer mathematical questions and sometimes locate relevant papers, but as we show in our experiments, they often provide incorrect theorem references or fail to locate specific statements. Our work treats theorem statements as first-class retrieval objects, enabling users to find specific results rather than papers that might contain them. 3. Data Collection Figure 1 gives an overview of the pipeline used to parse theorem statements from source documents, generate naturallanguage representations, and embed them for retrieval. Table 1. Distribution of theorem types in our dataset. Theorem Type Count Lemma Theorem Proposition Corollary 3,280,463 2,864,668 2,067,371 1,034,259 3.1. Theorem Parsing from arXiv We source 99.5% (9.2 million) of our theorems from papers in the arXiv database tagged with math, stat, cs, physics, eess, econ, q-fin, or q-bio. full breakdown of the statement types and arXiv categories across the dataset are provided in Table 1, Figure 2, and Table 7 in Appendix C. Our objective in parsing theorems is to extract theorem names and bodies. theorem name has three components: 1. The theorem type (e.g., Theorem, Proposition, Lemma, or Corollary), 2. An optional reference number, and 3. An optional note. For example, theorem name might look like Theorem 3.9 (Shokurov reduction). We define the body as the LATEX content of the theorem, in which basic author-defined macros are expanded. For instance, if an author defines as macro for mathbb{R} in the LATEX preamble, we replace all occurrences of accordingly. 2 Theorem Search Figure 1. Overview of the creation of our database and search engine. Items with * are variable and we tested different methods during our experiments (Sec. 4). The variety of ways authors define theorem environments makes parsing theorems difficult. Thus, we parse theorems for each paper using three strategies: 1. Node Search with plasTeX: We use the Python library plasTeX (Arnold, 2009) to convert LATEX sources into structured node tree. Every command and environment corresponds to node. We traverse this tree to find environments corresponding to theorem-like structures. Then, we extract the theorem type, reference number, note, and body from the node metadata. This method successfully parses 422 thousand papers, yielding approximately 6.9 million theorems. However, plasTeX occasionally truncates theorem bodies when papers source relies on LATEX packages unknown to the parser. To remove such malformed extractions, we apply simple heuristics, such as filtering out theorems shorter than 8 characters or ending in and or let. 2. TeX Logging: As fallback to plasTeX, we generate and inject custom LATEX package that logs theorem data. When papers source is compiled, the package records the theorem type, reference number, note, and body for all theorem environments. We parse 137 thousand papers this way, yielding approximately 1.8 million theorems. 3. Regex-based Parsing: As fallback to TeX logging, we use regular expressions to identify theorem delimiter tokens such as begin{...} and end{...} or proclaim and endproclaim. We then parse the content between delimiters to extract the theorems 3 Figure 2. Sunburst plot of the distribution of arXiv tags across theorems in our dataset. Theorem Search note and body. Reference numbers are not captured unless they are explicitly stated. We parse 30,000 papers this way, yielding approximately 542,000 theorems. 3.2. Theorem Parsing from Other Sources The remaining 0.5% (38,974) of our theorems come from variety of other sources: ProofWiki (ProofWiki Contributors) (23,871), the Stacks Project (The Stacks Project Authors) (12,693), the Open Logic Project (The Open Logic Project) (745), the CRing Project (The CRing Project Contributors) (546), Stacks and Moduli (Alper, 2026) (506), the HoTT Book (The Univalent Foundations Program, 2013) (382), and An Infinitely Large Napkin (Chen) (231). These sources were chosen for their verifiability through opensource contributions and structured, accessible LATEX files. We favored graduate-level texts, as their theorems would be useful to mathematician and are often cited by researchers. The Stacks Project, Open Logic Project, CRing Project, HoTT Book, and Infinitely Large Napkin are all hosted as open-source repositories on GitHub. Theorems from these sources were exclusively delimited in the source by begin{...} and end{...} tags. This uniform structure allowed us to build single regex-based parser, similar to our regex-based parser for arXiv, that could extract theorems from all five sources. Our parser first normalizes shorthand environment names, then extracts the theorem name, body, label, and type from each environment. When theorem body begins with bracketed note, we append it to the theorem name in parentheses for easier identification. Labels defined with label{...} are captured to preserve crossreferencing information from the original text. Since these repositories use GitHubs file structure, we generate source URLs pointing to the original .tex files, enabling users to verify theorems against their source material. In addition, custom theorem counter was used to reliably generate reference numbers. ProofWiki required different approach, as it is wikibased encyclopedia built on MediaWiki rather than LATEX. ProofWiki stores content in wikitext format with mathematical expressions enclosed in <math>...</math> tags. We developed separate parser that interfaces with the MediaWiki API to programmatically retrieve pages from relevant categories. For each page, we extract the theorem statement by identifying section headers and capturing content up to the proof section. The wikitext is then cleaned by removing MediaWiki-specific markup such as <onlyinclude> tags, template calls, and wiki links, while converting <math> blocks to standard delimiters. Since ProofWiki assigns unique URL to each theorem, we preserve these links to allow direct verification. 4 3.3. Theorem Representation Theorems in our corpus are exclusively represented in LATEX, and often lack concise natural-language summaries. To obtain searchable textual representations and improve retrieval performance, we generate short natural language description, or slogan, for each theorem using large language model (LLM). This converts the retrieval task from symmetric search over formal notation to an asymmetric task where informal queries retrieve formalized content (Wang et al., 2022). Given the parsed theorem body, we prompt the DeepSeek V3 model (Liu et al., 2024) to produce concise, declarative English description of the theorems main result. Prompts instruct the LLM to avoid symbolic notation, proof details, and references to surrounding document structure in the generated slogans. The resulting slogans are stored as the primary textual representation for theorem retrieval. To study the effect of additional mathematical context on slogan quality and retrieval performance, we evaluate three slogan-generation strategies: 1. Body Only: The prompt includes only the parsed theorem body. 2. Body + Abstract: The prompt includes the theorem body together with the paper abstract. 3. Body + Introduction: The prompt includes the theorem body together with the paper introduction. Each strategy uses similar prompt template as shown in 11. Their major differences are in the contextual fields provided to the model. Model temperature is fixed at 0.2, and the maximum output tokens is fixed at 1024. Slogans are generated independently for each variant and treated as separate representations of the same underlying theorem during evaluation. The total cost of building the corpus was approximately $6,000 USD: $4,000 for LLM API calls to generate slogans across all 9.2 million theorems, and $2,000 for compute and S3 storage. 3.4. Result Retrieval We embed theorem slogans and user queries using the Qwen3-Embedding-8B model (Zhang et al., 2025), which maps natural language inputs to fixed-dimensional vectors. All theorem slogans are embedded offline following their generation by the LLM and stored in PostgreSQL database (Stonebraker & Rowe, 1986) with the pgvector extension (pgvector Contributors, 2024). The database employs Hierarchical Navigable Small World (HNSW) index (Malkov & Yashunin, 2020) combined with binary quantization, enabling fast approximate nearest-neighbor search. User queries are embedded at inference time using the same method. Within this shared embedding space, we retrieve the top-k theorems ranked by their Hamming distance, then re-ranked by cosine similarity. query. Theorem Search 3.5. Validation Set Enlisting the help of three research mathematicians, we curated set of 111 distinct math queries across 14 arXiv tags, mainly Algebraic Geometry, Analysis, and PDEs. See Table 6 in the Appendix for detailed breakdown. These queries search for distinct theorems, lemmas, corollaries, and propositions written by small number of authors whose work our subject matter experts are well-acquainted with. Crucially, queries were written blind: each mathematician composed natural-language descriptions of theorems they knew from memory, without access to the corpus or its slogans, to prevent any leakage of retrieval representations into the evaluation queries. We then asked them to select larger dataset of papers in which they were confident that the results of the queries they wrote appeared at most once; this larger dataset consisted of 7,356 papers. We used this larger dataset to guide our decisions on which embedders to use, which LLM to use for the slogans, how to prompt the embedder, and how much context to give to the LLM when generating the slogans (body of the result only, body+abstract, body+first section). Every querytheorem pair was subsequently verified in two-stage quality check: an LLM confirmed that the target theorem exists in the corpus and semantically matches the query, and second mathematician independently reviewed each pair to ensure correctness. We remark that while small, this size is typical of mathematical information retrieval datasets, such as the query set of LeanSearch for Mathlib4 (Gao et al., 2024) and ARQMath3 (Mansouri et al., 2022), which contain 50 and 78 queries, respectively. 4. Experiments In this section, we report the evaluation performance of existing retrieval methods against our search engine. We outline the experiment setup, compare the performance against existing retrieval tools, and perform an ablation study on both the context window and the LLM used for slogan generation. 4.1. Metrics To measure the performance of our search engine against existing literature review tools, we employ three commonly used information retrieval metrics, following standard practice in mathematical retrieval benchmarks (Mansouri et al., 2022; Dadure et al., 2024). Firstly, Precision@k calculates the proportion of relevant documents in the first results of Precision@k = 1 Q (cid:88) i=1 1 (cid:88) j=1 I(i, j) where is the set of queries and I(i, j) is an indicator function that equals one if and only if the j-th result of the i-th query is an exact match, and is zero otherwise. Similar to Precision, we calculate Hit@k, also known as the hit rate: Hit@k = 1 Q (cid:88) i=1 I(i, di) where di,k denotes the top-k results of the i-th query. This indicator function is 1 if and only if at least one of the top-k results is an exact match, and is zero otherwise. Because each query targets at most one exact match, Precision@k and Hit@k target complementary behaviors: early accuracy and robustness to ranking noise. Another metric we use is Mean Reciprocal Rank (MRR@k) (Craswell, 2009), which takes the reciprocal of the rank at which an exact match is discovered: MRR@k = 1 Q (cid:88) i=1 1 ranki where ranki is the position of the first relevant result. 4.2. Setup We chose three embedders to use in our experiments: Googles Gemma 0.3B (Vera et al., 2025), and both Qwen3 0.6B and 8B (Zhang et al., 2025). Our baseline methods consist of the following: Filtered Google Search (site:arxiv.org + Query), arXiv advanced search, ChatGPT 5.2 w/ Search, and Gemini 3 Pro (Google DeepMind, 2025); the prompts used for the LLM baselines are provided in Table 12 in the Appendix. Both Google and arXiv are unable to return specific math statements, so their performance is graded on their ability to locate the correct paper. Conversely, LLM-based systems like ChatGPT and Gemini occasionally return correct results with incorrect reference numbers; we grade these as misses on theorem-level retrieval and treat them as paper matches for paper-level retrieval. We emphasize that these baselines are not designed for theorem-level retrieval, but we evaluate them to reflect the tools currently used by LLMs and research mathematicians in the absence of specialized theorem-level search systems. 4.3. Main Results Our results are presented in Table 2. We find that the embedder Qwen3 8B outperforms existing literature review methods across all metrics. In particular, Qwen3 8B achieves 5 Table 2. Results on the validation set, comparing embedder performance against existing literature search tools. Values are reported as theorem-level / paper-level. Google Search and arXiv search cannot return specific theorem statements, so only paper-level results are reported. Our corpus was restricted to arXiv papers for evaluation. Theorem Search MODEL P@ HIT@10 HIT@20 MRR@20 GOOGLE SEARCH ARXIV SEARCH CHAT-GPT 5.2 GEMINI 3 PRO GEMMA 0.3B (LATEX) GEMMA 0.3B QWEN3 0.6B QWEN3 8B QWEN3 8B W/ RERANKER BASELINE 0.162 0.009 0.117 0.171 0.378 0.018 0.180 0.252 OUR METHODS 0.378 0.027 0.198 0.270 0.237 0.011 0.139 0.196 0.027 / 0.054 0.081 / 0.108 0.081 / 0.153 0.171 / 0.243 0.189 / 0. 0.090 / 0.117 0.189 / 0.225 0.234 / 0.342 0.387 / 0.505 0.432 / 0.505 0.090 / 0.135 0.252 / 0.306 0.270 / 0.351 0.450 / 0.568 0.450 / 0.568 0.041 / 0.070 0.118 / 0.154 0.132 / 0.215 0.243 / 0.328 0.270 / 0.328 PAPER-LEVEL ONLY: THESE TOOLS CANNOT RETURN INDIVIDUAL THEOREMS. substantially higher Hit@10 and Hit@20, indicating that the correct theorem is frequently retrieved within the top candidate set even when it is not ranked first. This behavior is desirable in large-scale theorem retrieval, where downstream reranking or human inspection can refine results once relevant candidates have been surfaced. We further apply cross-encoder reranker (Qwen3-Reranker-0.6B (Zhang et al., 2025)) to rescore the top-100 candidates retrieved by Qwen3 8B; this improves theorem-level P@1 from 17.1% to 18.9% and MRR@20 from 24.3% to 27.0%, confirming that late interaction over full queryslogan pairs captures fine-grained semantic distinctions that the bi-encoder alone misses. For search methods at the paper-level, Qwen3 8B continues to outperform the baseline models, achieving higher Precision and MRR. The arXiv search is limited in the fields it can search over and does not search within the paper body, so it was only able to locate two papers across the whole query set in the top-20 results. One notable effect we observed concerns how Chat-GPT and Gemini rank retrieved results. Although both models used web-search-based retrieval, they would often organize their outputs primarily at the paper level, returning multiple theorems from the same paper in consecutive positions before moving to another paper. This ranking pattern reduces result diversity, which lowers Hit@k despite sometimes achieving high early precision. On average, Gemini returns only 10.98 distinct papers per query, compared to 16.89 for Qwen3 8B, indicating that our retrieval pipeline surfaces broader cross-section of the literature. Our approach is particularly advantageous for retrieving auxiliary lemmas and technical results that appear deep within paper, far from the title and abstract. Google Search and LLM-based tools rely heavily on paper-level metadata and thus struggle to surface results whose content is not reflected in the abstract, whereas our system indexes every theorem independently via its slogan, making it equally as capable of retrieving minor lemma in later section as papers headline theorem. 4.4. Ablation Studies In this subsection, we perform ablation studies on the context windows used for slogan generation, the LLMs used to generate slogans, and the document preparation process. For each ablation, we limit the corpus to arXiv papers labeled with the algebraic geometry tag (math.AG) and authored by individuals whose work contains an exact match to one of our queries. This filtering reduces the corpus to 7,356 statements written by 8 primary authors. 4.4.1. ABLATION OF CONTEXT WINDOWS Table 3 presents the effects of context on slogan generation. Increasing or decreasing the amount of context provided to LLMs when generating math slogans considerably affects our search engines performance, while holding the embedder constant. We find that supplying the LLM with additional paper context improves the retrieval performance of its generated slogans. Performance falters when the LLM is only given the abstract, but improves significantly with the inclusion of the first section of the paper, defined by section{}. We attribute this improvement to the LLM making better sense of the statement with more context, especially since theorems alone tend to be at most 34 sentences long and may reference earlier sections of the paper. Furthermore, the introduction of most math papers typically outlines the results and the existing research upon which they build. As result, context-rich slogans better capture the semantic intent of the theorem, yielding more reliable 6 Theorem Search and robust retrieval across queries. Results for an expanded ablation covering additional embedders are reported in Table 8 in Appendix D. Table 3. Comparing context window size in retrieval performance. Embedded with prompt using Qwen3 8B (Zhang et al., 2025). Slogans generated with DeepSeek V3 (Liu et al., 2024). MODEL P@1 HIT@10 HIT@20 MRR@20 CONTEXT WINDOWS BODY ONLY W/ ABSTRACT W/ FIRST SECTION 0.342 0.316 0.368 0.658 0.645 0. 0.737 0.737 0.763 0.451 0.429 0.496 4.4.2. ABLATION OF SLOGAN GENERATOR The LLM used to generate the slogans can also significantly affect our engines search performance, as some LLMs are better trained on mathematical texts than others and thus generate better informal versions of math statements. We find that leading proprietary models, such as Claudes Opus 4.5 and Gemini 3 Pro (Google DeepMind, 2025), outperform open-source models such as DeepSeek-V3 (Liu et al., 2024), as summarized in Table 4. broader comparison across additional slogan generators and embedding models is provided in Table 9 in Appendix D. Table 4. Comparing LLM slogans in retrieval performance. Embedded with prompt using Qwen3 8B (Zhang et al., 2025). Body + Abstract MODEL P@1 HIT@10 HIT@20 MRR@20 LLMS DEEPSEEK V3.1 DEEPSEEK R1 GEMINI 3 PRO CLAUDE OPUS 4.5 0.316 0.276 0.368 0.395 0.645 0.671 0.750 0. 0.737 0.697 0.816 0.842 0.429 0.388 0.507 0.536 Table 5. Comparing prompting performance. Slogans generated by DeepSeek V3.1 (Liu et al., 2024) on Body+Abstract context window. MODEL P@1 HIT@10 HIT@20 MRR@ UNPROMPTED GEMMA 0.3B QWEN3 0.6B QWEN3 8B 0.303 0.224 0.250 0.539 0.487 0.553 PROMPTED GEMMA 0.3B QWEN3 0.6B QWEN3 8B 0.197 0.237 0.316 0.434 0.566 0.645 0.566 0.526 0.618 0.487 0.658 0.737 0.376 0.297 0.332 0.265 0.346 0. 4.4.3. ABLATION OF TASK INSTRUCTION We examine the impact of the task instructions used during embeddings on retrieval performance in Table 5. We find that by providing math retrieval instruction, both Qwen embedders achieve higher performance than without any instruction at all. However, this effect is reversed with Gemma, with empty instruction performing better, though worse than Qwen 8B with instruction. We use modification of the prompts made by (Gao et al., 2024), the details of which are listed in Table 10. 4.5. Embedding Space Analysis Following (Gao et al., 2024; Jiang et al., 2023), we convert LATEX theorem statements into natural-language slogans before embedding, since embedders struggle with symbolheavy notation (Bleckmann & Tschisgale, 2025; Peng et al., 2021) and mathematicians typically query in informal language. To verify that sloganization induces semantically meaningful structure, we use PCA and UMAP as diagnostic tools on random sample of 10,000 theorems (1,000 from each of the 10 most common arXiv categories). Figure 3 shows PCA projections for three categories: Algebraic Geometry, Probability Theory, and Statistics Theory. Conceptually distant fields (Algebraic Geometry vs. Probability) are well separated, while closely related fields (Probability vs. Statistics) overlap substantially consistent with mathematical intuition. UMAP projections over all ten categories in Figure 4 reveal that Qwen3 8B produces tighter, better-separated clusters than Gemma 0.3B, consistent with the retrieval gap in Table 2. 5. Search Tool The search tool is currently accessible via HuggingFace Spaces (Link). Users can perform semantic searches using natural language (e.g., rational variety is simply connected). The sidebar interface allows results to be filtered by relevant metadata (e.g., author sets, arXiv category tags, Figure 3. PCA visualizations of theorems in our dataset using slogan embeddings. Left: Gemma embedding. Right: Qwen3 8B embedding. 7 Theorem Search Figure 4. UMAP visualizations of 10,000 theorem slogan embeddings across the ten most common arXiv categories. Gemma 0.3B (left) and Qwen3 8B (right). Qwen3 8B produces tighter, better-separated clusters. Figure 5. The interface of the Theorem Search Tool. The main window displays the top results: paper name, paper authors, and theorem statement, and link to the paper. In the left pane users can filter by paper metadata: type of result (theorem/proposition/lemma/corollary), author(s), arXiv tag, specific paper, year, and publication status. The thumbs up/down allow for user feedback, used to further improve the search results. 8 Theorem Search is separably closed, the set of k-points of An is dense in An. The first result was correct: Tag 056U in the Stacks Project. 7. Conclusion Mathematical knowledge is organized around discrete results theorems, lemmas, propositions, and corollaries yet existing search tools operate at the document level, forcing researchers and AI agents alike to manually locate specific statements within papers. In this work, we addressed this gap by constructing corpus of over 9.2 million theorem statements extracted from arXiv and seven other sources, constituting the largest unified collection of humanauthored, research-level theorems to date. We showed that representing theorems via natural-language slogans generated by an LLM, rather than embedding raw LATEX directly, yields substantially better retrieval performance. Our systematic ablations revealed that context matters: slogans generated with access to the papers introduction outperform those generated from the theorem body alone, and the choice of both the slogan-generating LLM and the embedding model significantly affects downstream retrieval quality. On curated evaluation set of 111 queries written by professional mathematicians, our best configuration achieves 45.0% Hit@20 at the theorem level and 56.8% Hit@20 at the paper level, outperforming ChatGPT 5.2 with search, Gemini 3 Pro, and Google Search. By treating theorems as first-class retrieval objects and enriching them with natural-language descriptions, we enable new forms of access to mathematical knowledge whether for retrieval-augmented generation by LLMs, for premise selection in formal proof search, or for literature review by expert mathematicians. We have deployed public search interface described in Appendix 5. Qualitative user feedback from research mathematicians, reported in Section 6 and Appendix B, further illustrates the practical utility of our system."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank Michele Pernice and Dori Bejleri for contributing several entries to the validation set. We thank the UW eScience School and Sophie Szeto for the AWS credits. We thank Nebius and Token Factory for the LLM tokens. And we thank the UW Applied Mathematics department for GPU access. and journal publication status). To optimize retrieval time, the search uses two-stage retrieval architecture. At inference time, the natural language query is encoded using the Qwen3-8B embedding model and binary quantized into 4096 bits. An initial pool of clamp(max(200, 12 k), 200, 800) candidates is returned using the HNSW index and Hamming distance, where is user-specified number of results to return. The candidate pool is then filtered by metadata and reranked using cosine similarity on the original 4096-dimensional embeddings. When weighting by citations is enabled, the candidates are instead re-ranked using the following composite scoring function: score = cosine similarity + λ log(max(citations, 1)) where λ is the user-specified citation-weight parameter. The interface retrieves the top results, each of which displays the theorem slogan, the theorem body rendered in LaTeX, any relevant paper metadata, and link to the paper. Finally, lightweight feedback system allows users to rate each results relevance to their query. See Figure 5 for the interface. 6. User Feedback In this section, we report users feedback on our theorem search engine, with an additional one reported in Appendix B. 6.1. Feedback One We will first include high-level description of the first feedback. The user wanted to check for specific reference, which we will call Theorem 1, which the user knew was true. They thought it follows from another result, which we call result X, bit stronger than Theorem 1. It did, after small lemma; in other words, (result + small lemma) implies what the user wanted. This shifted the users attention from finding reference for Theorem 1 to finding reference for the small lemma. The user searched for the small lemma in our tool, and the first result was reference for generalization of the small lemma. more detailed description now follows. We advise the non-mathematician reader to skip the next paragraph. The user wanted to check that smooth variety over separably closed field has k-point. The user thought it follows from the etale local structure of smooth morphisms, which captured the correct idea but reduced the problem to showing that k-points are dense in An . In essence, the etale local structure theorem translated the original problem of checking that smooth variety over separably closed field has k-point, to checking that for separably closed, the set of k-points of An is dense. The user entered the query if Theorem Search"
        },
        {
            "title": "References",
            "content": "Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Aizawa, A., Kohlhase, M., Ounis, I., and Schubotz, M. NTCIR-11 Math-2 Task Overview. In Proceedings of the 11th NTCIR Conference, pp. 8898, Tokyo, Japan, 2014. Alper, J. Stacks and moduli. https://sites.math. washington.edu//jarod/moduli.pdf, 2026. Arnold, T. Getting started with plasTeX. TUGboat, 30(2):180185, 2009. URL https://www.tug.org/ TUGboat/tb30-2/tb95arnold.pdf. Asher, J. Leanexplore: search engine for lean 4 declarations. arXiv preprint arXiv:2506.11085, 2025. Bahturin, Y., Mikhalev, A. V., Petrogradsky, V. M., and Zaicev, M. V. Infinite dimensional Lie superalgebras, volume 7. Walter de Gruyter, 2011. Bleckmann, T. and Tschisgale, P. Evaluating nlp embedding models for handling science-specific symbolic expressions in student texts, 2025. URL https://arxiv. org/abs/2505.17950. Chen, E. An infinitely large napkin. https://venhance. github.io/napkin/. Accessed 2026-01-25. Craswell, N. Mean Reciprocal Rank, pp. 17031703. doi: 10.1007/ Springer US, Boston, MA, 2009. 978-0-387-39940-9 488. Dadure, P., Pakray, P., and Bandyopadhyay, S. Mathematical information retrieval: review. ACM Computing Surveys, 57(3):134, 2024. doi: 10.1145/3699953. Erdos, P. and Newman, D. J. Bases for sets of integers. Journal of Number Theory, 9(4):420425, 1977. Gao, G., Ju, H., Jiang, J., Qin, Z., and Dong, B. semantic search engine for mathlib4. In Findings of the Association for Computational Linguistics: EMNLP 2024. Association for Computational Linguistics, 2024. URL https://arxiv.org/abs/2403.13310. Gebru, T., Morgenstern, J., Vecchione, B., Vaughan, J. W., Wallach, H., Daume III, H., and Crawford, K. Datasheets for datasets. Communications of the ACM, 64(12):8692, December 2021. doi: 10.1145/3458723. Ginsparg, P. First steps towards electronic research communication. Computers in Physics, 8(4):390396, 1994. doi: 10.1063/1.4823313. Google DeepMind. Gemini 3: Introducing the latest gemini AI model from Google. https://blog.google/ products/gemini/gemini-3/, November 2025. Accessed 2026-01-25. Hu, X., Shan, Z., Zhao, X., Sun, Z., Liu, Z., Li, D., Ye, S., Wei, X., Chen, Q., Hu, B., Wang, H., Yu, J., and Zhang, M. Kalm-embedding: Superior training data brings stronger embedding model, 2025. URL https: //arxiv.org/abs/2501.01028. Jiang, A. Q., Li, W., and Jamnik, M. Multilingual mathematical autoformalization. 2023. URL https://arxiv. org/abs/2311.03755. Karpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., and Yih, W.-t. Dense passage retrieval In Proceedings for open-domain question answering. of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 67696781. Association for Computational Linguistics, 2020. doi: 10.18653/v1/2020.emnlp-main.550. Khattab, O. and Zaharia, M. ColBERT: Efficient and effective passage search via contextualized late interaction over BERT. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 3948. ACM, 2020. doi: 10.1145/3397271.3401075. Klarner, D. A. Representations of as sum of distinct elements from special sequences. The Fibonacci Quarterly, 4(4):289306, 1966. Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Kuttler, H., Lewis, M., Yih, W.-t., Rocktaschel, T., Riedel, S., and Kiela, D. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Advances in Neural Information Processing Systems, volume 33, pp. 94599474. Curran Associates, Inc., 2020. Liu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. Liu, J., Zhou, Z., Zhu, Z., Dos Santos, M., He, W., Liu, J., Wang, R., Xie, Y., Zhao, J., Wang, Q., Zhi, L., Li, J., and Li, W. Numina-lean-agent: An open and general agentic reasoning system for formal mathematics. 2026. URL https://arxiv.org/abs/2601.14027. Lu, J., Emond, K., Yang, K., Chaudhuri, S., Sun, W., and Chen, W. Lean finder: Semantic search for mathlib that understands user intents. 2025. URL https://arxiv. org/abs/2510.15940. 10 Theorem Search Malkov, Y. A. and Yashunin, D. A. Efficient and robust approximate nearest neighbor search using hierarchical IEEE Transactions on navigable small world graphs. Pattern Analysis and Machine Intelligence, 42(4):824 836, 2020. doi: 10.1109/TPAMI.2018.2889473. Mansouri, B., Novotny, V., Agarwal, A., Oard, D. W., and Zanibbi, R. Overview of ARQMath-3 (2022): Third CLEF lab on answer retrieval for questions on math. In Experimental IR Meets Multilinguality, Multimodality, and Interaction, volume 13390 of Lecture Notes in Computer Science, pp. 286310. Springer, 2022. doi: 10.1007/978-3-031-13643-6 20. Peng, S., Yuan, K., Gao, L., and Tang, Z. Mathbert: pretrained model for mathematical formula understanding, 2021. URL https://arxiv.org/abs/2105.00377. pgvector Contributors. pgvector: Vector similarity search for Postgres. https://github.com/pgvector/ pgvector, 2024. Accessed 2026-01-25. Polu, S. and Sutskever, I. Generative language modelarXiv preprint ing for automated theorem proving. arXiv:2009.03393, 2020. Popescu-Pampu, P. Stein or Milnor fillability and cohomology, 2007. Withdrawn; merged into arXiv:0712.3484 after discovering theorem had already been proved by Durfee and Hain. ProofWiki Contributors. ProofWiki. https://proofwiki. org. Accessed 2026-01-25. Rao, D., Young, J., Dietterich, T., and Callison-Burch, C. Withdrarxiv: large-scale dataset for retraction study. arXiv preprint arXiv:2412.03775, 2024. Reimers, N. and Gurevych, I. Sentence-BERT: Sentence embeddings using siamese BERT-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pp. 39823992. Association for Computational Linguistics, 2019. doi: 10.18653/v1/D19-1410. Shahryari, M. note on surjunctive groups, 2020. Withthe main result had already been proved by drawn; Arzhantseva et al. Steinfeldt, C. and Mihaljevic, H. Evaluation and domain adaptation of similarity models for short mathematical In Intelligent Computer Mathematics: 17th Intexts. ternational Conference, CICM 2024, Lecture Notes in Computer Science, pp. 241260, Montreal, QC, Canada, 2024. Springer. doi: 10.1007/978-3-031-66997-2 14. Stonebraker, M. and Rowe, L. A. The design of POSTGRES. In Proceedings of the 1986 ACM SIGMOD International Conference on Management of Data, pp. 340355, 1986. doi: 10.1145/16894.16888. The CRing Project Contributors. The CRing project. https: //math.mit.edu/hrm/the_cring_project.pdf. Accessed 2026-01-25. The Open Logic Project. Open logic. https:// openlogicproject.org. Accessed 2026-01-25. The Stacks Project Authors. The stacks project. https: //stacks.math.columbia.edu. Accessed 2026-0125. The Univalent Foundations Program. Homotopy Type Theory: Univalent Foundations of Mathematics. Institute for Advanced Study, 2013. URL https:// homotopytypetheory.org/book/. Usefi, H. Isomorphism invariants of restricted enveloping algebras, 2009. URL https://arxiv.org/abs/0804. 2281. Vera, H. S., Dua, S., Zhang, B., Salz, D., Mullins, R., Panyam, S. R., Smoot, S., Naim, I., Zou, J., Chen, F., et al. Embeddinggemma: Powerful and lightweight text representations. arXiv preprint arXiv:2509.20354, 2025. Wang, L., Yang, N., Huang, X., Jiao, B., Yang, L., Jiang, D., Majumder, R., and Wei, F. Text embeddings by weaklyarXiv preprint supervised contrastive pre-training. arXiv:2212.03533, 2022. URL https://arxiv.org/ abs/2212.03533. Wang, L., Yang, N., Huang, X., Yang, L., Majumder, R., and Wei, F. Multilingual e5 text embeddings: technical report. arXiv preprint arXiv:2402.05672, 2024. Wirsing, E. and Schwarz, W. Approximation mit algebraischen zahlen beschrankten grades. 1961. Wu, Y., Jiang, A. Q., Li, W., Rabe, M., Staats, C., Jamnik, M., and Szegedy, C. Autoformalization with large language models. In Advances in Neural Information Processing Systems, volume 35, 2022. Yang, K., Swope, A. M., Gu, A., Chalamala, R., Song, P., Yu, S., Godil, S., Prenger, R., and Anandkumar, A. LeanDojo: Theorem proving with retrieval-augmented language models. In Advances in Neural Information Processing Systems, volume 36, 2023. Zanibbi, R., Aizawa, A., Kohlhase, M., Ounis, I., Topic, G., and Davila, K. NTCIR-12 MathIR Task Overview. In Proceedings of the 12th NTCIR Conference, pp. 299308, Tokyo, Japan, 2016. 11 Theorem Search Zhang, S. On non-congruent numbers with 3 (mod 8) prime factors, 2012. Withdrawn; the result is corollary of well-known result by Monsky. Zhang, Y., Li, M., Long, D., Zhang, X., Lin, H., Yang, B., Xie, P., Yang, A., Liu, D., Lin, J., Huang, F., and Zhou, J. Qwen3 embedding: Advancing text embedding and reranking through foundation models. 2025. URL https://arxiv.org/abs/2506.05176. Zhao, X., Hu, X., Shan, Z., Huang, S., Zhou, Y., Zhang, X., Sun, Z., Liu, Z., Li, D., Wei, X., Pan, Y., Xiang, Y., Zhang, M., Wang, H., Yu, J., Hu, B., and Zhang, M. Kalm-embedding-v2: Superior training techniques and data inspire versatile embedding model, 2025. URL https://arxiv.org/abs/2506.20923. 12 A. Datasheet This section is based on Datasheets for Datasets by (Gebru et al., 2021). Theorem Search A.1. Motivation 1. For what purpose was the dataset created? We created this dataset to create the largest unified corpus of mathematical theorems. We use this dataset ourselves to create theorem search engine. 2. Who created the dataset? This papers co-authors collected theorems. All theorems were written by the authors of the theorems papers and are cited in our database. 3. Who funded the creation of the dataset? Self-funded. A.2. Composition 1. What do the instances that comprise the dataset represent? Each instance of the dataset represents mathematical statement derived from paper, textbook, or repository online. 2. How many instances are there in total? As of writing, there are 9.2 million mathematical statements in the dataset. 3. Does the dataset contain all possible instances or is it sample (not necessarily random) of instances from larger set? The dataset contains non-random sample of instances from available math corpora. 4. What data does each instance consist of? Each instance contains the name of the theorem as displayed in the source document (e.g. Theorem 1.1), the raw TeX body, the author-written label inside the body, and the paper it was sourced from (e.g. arXivs convention of 1234.56789v1). In addition, we stored metadata from each source document. This data includes the title, the list of authors, the link to the document, the abstract, the journal the document was published in (if it was published), the primary and secondary categories, the number of citations (if available), and tag of which source it came from (e.g. arXiv, Stacks Project). 5. Is there label or target associated with each instance? Each theorem uses its name derived from the source document, and lists the document it came from, creating distinct method of identification. 6. Is any information missing from individual instances? No. 7. Are relationships between individual instances made explicit? Each instance is numbered according to the conventions of the source document. They also contain metadata linking them to specific paper or document. 8. Are there recommended data splits? (e.g., training, development/validation, testing) Not applicable. 9. Are there any errors, sources of noise, or redundancies in the dataset? 13 Some statement bodies, titles, and labels may be fragmented, as our parsing methods are not applicable to every TeX document in our sources. Theorem Search 10. Is the dataset self-contained, or does it link to or otherwise rely on external resources? The data is derived from sources online. 11. Does the dataset contain data that might be considered confidential? No, all data is publicly available online. 12. Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety? No. A.3. Collection Process 1. How was the data associated with each instance acquired? LaTeX parser was used to parse all theorem names and bodies from sources. 2. What mechanisms or procedures were used to collect the data? Our LaTeX theorem parser used the Python library plasTeX to parse LaTeX documents, with custom TeX logger and pure-regex parser as fallbacks. 3. If the dataset is sample from larger set, what was the sampling strategy? Our dataset is not sample. To the best of our knowledge, our dataset includes most mathematical theorems in arXiv. For the other sources, such as ProofWiki and the Stacks Project, we have included all theorems. 4. Who was involved in the data collection process, and how were they compensated? This papers co-authors were all involved in designing and implementing the pipeline to collect theorems. The mathematicians who created the validation set are volunteers. 5. Over what timeframe was the data collected? Data was collected over five months, from September 2025 to January 2026. 6. Were any ethical review processes conducted? Not applicable. A.4. Preprocessing/cleaning/labeling 1. Was any preprocessing/cleaning/labeling of the data done? We preprocessed theorem bodies by removing unnecessary whitespace and expanding simple author-defined macros. We filtered out truncated or corrupted theorem bodies by employing several heuristics. 2. Was the raw data saved in addition to the preprocessed/cleaned/labeled data? No. 3. Is the software that was used to preprocess/clean/label the data available? All code used to preprocess, clean, and label our data is available on our GitHub repo (Link). 14 A.5. Uses 1. Has the dataset been used for any tasks already? Theorem Search The dataset has been used to create semantic search engine, available on HuggingFace Spaces (Link). 2. Is there repository that links to any or all papers or systems that use the dataset? Not applicable. 3. What (other) tasks could the dataset be used for? Our dataset of theorems is useful for mathematicians and for AI agents for automated theorem proving. 4. Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses? If authors of our theorem sources were to significantly change the content of the papers/textbooks/projects, our theorems would be outdated. 5. Are there tasks for which the dataset should not be used? No. A.6. Distribution 1. Will the dataset be distributed to third parties outside of the entity on behalf of which the dataset was created? 2. How will the dataset be distributed? The dataset is publicly available on Hugging Face at huggingface.co/datasets/uw-math-ai/theorem-search-dataset. We release only theorems from arXiv papers that use permissive licenses (CC BY and CC0), as well as theorems from the seven other sources (ProofWiki, Stacks Project, Open Logic Project, CRing Project, Stacks and Moduli, HoTT Book, and An Infinitely Large Napkin). Theorems from arXiv papers with non-permissive licenses are excluded from the public release. 3. When will the dataset be distributed? The dataset is already publicly available. 4. Will the dataset be distributed under copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)? The released dataset inherits the licenses of the source materials: CC BY or CC0 for arXiv papers, and the respective licenses of the other sources. 5. Have any third parties imposed IP-based or other restrictions on the data associated with the instances? No. 6. Do any export controls or other regulatory restrictions apply to the dataset or individual instances? No. A.7. Maintenance 1. Who will be supporting/hosting/maintaining the dataset? The co-authors of this paper plan on continuing to build, host, and maintain this dataset. 2. How can the owner/curator/manager of the dataset be contacted? The co-authors of this paper own this dataset and can be contacted through Vasily Ilin at vilin@uw.edu. 3. Will the dataset be updated? 15 Theorem Search We plan on continuing to improve and grow the dataset in the future. For example, we plan on adding theorems from nLab and other open-source textbooks and projects. 4. If the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (e.g., were the individuals in question told that their data would be retained for fixed period of time and then deleted)? Not applicable. 5. Will older versions of the dataset continue to be supported/hosted/maintained? Older versions of the dataset will be available for download indefinitely on Hugging Face. However, all support and maintenance efforts will be focused on the latest version of the dataset. 6. If others want to extend/augment/build on/contribute to the dataset, is there mechanism for them to do so? Others can suggest sources to add to the dataset by contacting Vasily Ilin at vilin@uw.edu. B. User Feedback In this section, we present second users feedback on our theorem search engine. B.1. Feedback Two We now describe second use case. As before, we first provide high-level summary. The user searched for result known to experts but lacked specific citable reference; we call this result Theorem X. Many existing references point to similar results, but not exactly to Theorem X. The user queried our tool and found result, which we call Theorem Y, whose citation list directly referenced Theorem X. By using our tool, the user located the desired theorem with fewer queries than traditional search methods, which had previously led only to indirect references. more detailed description follows. We advise the non-mathematician reader to skip the next two paragraphs. The user needed structure theorem for abelian p-Lie algebras, as defined by N. Jacobson. The user conjectured that these algebras are classified analogously to finite abelian groups, provided the base field is algebraically closed. On MathOverflow, an answer directed the user to book that states the desired result. The book explained that the result follows from the structure theory for modules over PID, without providing precise proof, and attributes the technique to Jacobson. The book Infinite-Dimensional Lie Superalgebras (Bahturin et al., 2011) contains the result the user was looking for, labeled the 3.2 nil-radical Theorem, attributed to Seligman, 1967. However, the user skimmed through the hundred or so pages of Seligmans reference several times without finding where the desired result is proved. When the user queried our tool with over an algebraically closed field, any abelian p-Lie algebra splits as torus and sum of nilpotent cyclic Lie algebras, the first result was the desired statement: Theorem 2.7 in (Usefi, 2009). C. Validation Set arXiv Categories Table 6 shows the distribution of arXiv categories across the 65 unique papers in our validation set. Out of 32 math tags on arXiv, the validation set covers 12. 16 Theorem Search Table 6. Distribution of arXiv categories in the validation set. Each cell reports counts as primary / all, where primary counts only papers whose first-listed arXiv category matches the row, and all also includes cross-listed papers. arXiv Tag Subject Area Theorems Papers Algebraic Geometry Classical Analysis and ODEs Analysis of PDEs Functional Analysis math.AG math.CA math.AP math.FA math.MG Metric Geometry math.DG math.NT math.CT math.CO math.RT nlin.SI math.CV math.SG hep-th Differential Geometry Number Theory Category Theory Combinatorics Representation Theory Exactly Solvable and Int. Sys. Complex Variables Symplectic Geometry High Energy Physics Theory 70 / 75 26 / 36 17 / 23 / 12 7 / 9 / 9 2 / 7 / 4 1 / 3 / 3 2 / 2 / 1 / 1 / 1 47 / 50 9 / 12 4 / 7 / 4 2 / 3 / 2 1 / 4 / 1 1 / 2 / 1 1 / 1 / 1 / 1 / 1 17 Theorem Search Table 7. Distribution of arXiv categories in the whole dataset. Each cell reports counts as primary / all, where primary counts only papers whose first-listed arXiv category matches the row, and all also includes cross-listed papers. arXiv Tag Subject Area Theorems Papers math.CO math.AG math.AP math.PR math.NT math.DG math.RT math.DS math.FA math-ph math.GR math.OC math.GT math.RA math.AT cs.LG math.QA math.CA math.NA math.LO stat.ML math.CV math.OA math.ST math.AC math.MG math.CT math.SG math.SP math.KT cs.DM cs.CC cs.DS stat.ME cs.IT hep-th math.GN cs.AI cs.LO cs.CG quant-ph eess.SY gr-qc cs.CR cs.GT nlin.SI math.GM stat.CO econ.TH Combinatorics Algebraic Geometry Analysis of PDEs Probability Number Theory Differential Geometry Representation Theory Dynamical Systems Functional Analysis Mathematical Physics Group Theory Optimization and Control Geometric Topology Rings and Algebras Algebraic Topology Machine Learning Quantum Algebra Classical Analysis and ODEs Numerical Analysis Logic Machine Learning Complex Variables Operator Algebras Statistics Theory Commutative Algebra Metric Geometry Category Theory Symplectic Geometry Spectral Theory K-Theory and Homology Discrete Mathematics Computational Complexity Data Structures and Algorithms Methodology Information Theory High Energy Physics Theory General Topology Artificial Intelligence Logic in Computer Science Computational Geometry Quantum Physics Systems and Control General Relativity and Quantum Cosmology Cryptography and Security Computer Science and Game Theory Exactly Solvable and Integrable Systems General Mathematics Computation Theoretical Economics 18 727514 / 1107979 761230 / 1097654 791867 / 1018652 606057 / 868961 550466 / 752812 499694 / 721447 353605 / 618985 374124 / 574125 333709 / 544882 127036 / 533783 303021 / 488750 298096 / 470086 277653 / 441500 202501 / 352448 200267 / 348076 150507 / 323831 171100 / 322466 194189 / 309145 209943 / 285427 206528 / 285351 95327 / 281203 148649 / 278361 170685 / 267922 153124 / 260628 158613 / 253612 100465 / 207104 89204 / 202660 110807 / 191221 73394 / 162664 57905 / 145433 35079 / 134746 65534 / 127305 53579 / 121709 74267 / 117914 42758 / 109029 17512 / 108924 60256 / 102259 24644 / 82598 30251 / 68125 33920 / 59488 25484 / 58911 16997 / 52647 17771 / 43279 16687 / 40367 17909 / 36310 8253 / 32897 27194 / 29265 9804 / 28120 19057 / 26467 46929 / 68561 36350 / 51497 51176 / 64735 38793 / 55971 32235 / 42070 28901 / 40763 15219 / 26364 21561 / 33646 20193 / 31825 8604 / 32239 14269 / 22702 28546 / 43871 15348 / 22650 10783 / 17650 8886 / 15601 16346 / 33522 8258 / 15419 14237 / 21275 23450 / 30763 9324 / 12952 10123 / 28137 10203 / 16899 8033 / 12677 12979 / 21634 8762 / 13462 6022 / 11402 4024 / 8610 4864 / 8954 4666 / 9798 2441 / 5961 2246 / 8829 4016 / 7808 3269 / 7629 10991 / 15438 3422 / 8646 1753 / 6809 3223 / 5212 3072 / 10378 1772 / 3945 2904 / 4755 1947 / 4332 2549 / 7239 1125 / 2721 2204 / 4419 1504 / 3096 744 / 2414 2224 / 2380 1515 / 3830 1582 / 2167 Continued on next page Theorem Search Table 7 (continued) arXiv Tag Subject Area Statistical Mechanics cond-mat.stat-mech Mathematical Finance q-fin.MF Econometrics econ.EM Applications stat.AP Formal Languages and Automata Theory cs.FL Signal Processing eess.SP Populations and Evolution q-bio.PE Distributed, Parallel, and Cluster Computing cs.DC Symbolic Computation cs.SC Social and Information Networks cs.SI Computer Vision cs.CV Multiagent Systems cs.MA Fluid Dynamics physics.flu-dyn History and Overview math.HO Computational Finance q-fin.CP Chaotic Dynamics nlin.CD Neural and Evolutionary Computing cs.NE Risk Management q-fin.RM Pricing of Securities q-fin.PR Robotics cs.RO Portfolio Management q-fin.PM Disordered Systems and Neural Networks cond-mat.dis-nn Computational Physics physics.comp-ph General Economics econ.GN Pattern Formation and Solitons nlin.PS Physics and Society physics.soc-ph Quantitative Methods q-bio.QM Computation and Language cs.CL Networking and Internet Architecture cs.NI Molecular Networks q-bio.MN Computational Engineering, Finance, and Science cs.CE Strongly Correlated Electrons cond-mat.str-el Databases cs.DB Data Analysis, Statistics and Probability physics.data-an Programming Languages cs.PL Classical Physics physics.class-ph Statistical Finance q-fin.ST Adaptation and Self-Organizing Systems nlin.AO Neurons and Cognition q-bio.NC cs.PF Performance cond-mat.mes-hall Mesoscale and Nanoscale Physics cs.CY cond-mat.mtrl-sci q-fin.TR stat.OT cs.IR physics.optics nlin.CG physics.plasm-ph q-fin.GN Computers and Society Materials Science Trading and Market Microstructure Other Statistics Information Retrieval Optics Cellular Automata and Lattice Gases Plasma Physics General Finance Theorems Papers 2321 / 24482 14999 / 24160 14189 / 22632 5524 / 20294 6655 / 18854 10708 / 18805 7196 / 17583 3689 / 16269 5435 / 14312 3772 / 13795 3247 / 12628 1522 / 12284 1836 / 11450 5067 / 10829 3495 / 8114 1715 / 7802 2010 / 7620 2840 / 7611 3634 / 7537 1972 / 7091 3474 / 6878 565 / 6180 1202 / 6038 3402 / 5953 1042 / 5864 1052 / 5639 1417 / 5506 1381 / 5300 1565 / 5000 1339 / 4549 1425 / 4295 580 / 4123 1521 / 4043 583 / 3747 1120 / 3515 1082 / 3288 1317 / 3221 558 / 3204 768 / 3054 973 / 3019 231 / 2963 405 / 2949 164 / 2924 1200 / 2919 804 / 2794 564 / 2488 299 / 2033 229 / 1860 265 / 1731 391 / 1730 287 / 1834 1273 / 1960 1479 / 2234 1298 / 3778 439 / 1192 2449 / 3485 740 / 1743 371 / 1669 395 / 1113 516 / 1550 703 / 2169 208 / 1386 393 / 1335 427 / 815 465 / 928 214 / 834 235 / 884 239 / 677 357 / 710 432 / 1192 296 / 626 67 / 504 313 / 1149 446 / 695 121 / 518 211 / 819 263 / 775 277 / 767 222 / 664 134 / 408 280 / 731 55 / 275 144 / 415 111 / 541 83 / 281 178 / 459 197 / 421 79 / 377 102 / 378 107 / 320 15 / 209 69 / 415 29 / 271 107 / 269 111 / 339 108 / 387 40 / 211 22 / 136 42 / 187 44 / 156 Continued on next page 19 Table 7 (continued) arXiv Tag Subject Area Theorem Search"
        },
        {
            "title": "Soft Condensed Matter",
            "content": "cond-mat.soft cond-mat.quant-gas Quantum Gases cs.MS physics.chem-ph eess.IV physics.bio-ph cs.GR physics.geo-ph physics.ao-ph physics.gen-ph hep-ph cs.ET hep-lat q-bio.GN cs.HC q-bio.BM physics.med-ph astro-ph.IM q-bio.CB astro-ph.CO q-bio.TO cond-mat.other cond-mat.supr-con physics.app-ph cs.SE cond-mat physics.hist-ph cs.SD astro-ph.EP physics.atom-ph cs.AR eess.AS astro-ph q-bio.SC cs.OH nucl-th cs.MM astro-ph.SR cs.DL physics.acc-ph physics.space-ph hep-ex q-bio.OT cs.OS physics.pop-ph physics.ins-det Mathematical Software Chemical Physics Image and Video Processing Biological Physics Graphics Geophysics Atmospheric and Oceanic Physics General Physics High Energy Physics Phenomenology Emerging Technologies High Energy Physics Lattice Genomics Human-Computer Interaction Biomolecules Medical Physics Instrumentation and Methods for Astrophysics Cell Behavior Cosmology and Nongalactic Astrophysics Tissues and Organs Other Condensed Matter Superconductivity Applied Physics Software Engineering Condensed Matter History and Philosophy of Physics Sound Earth and Planetary Astrophysics Atomic Physics Hardware Architecture Audio and Speech Processing Astrophysics Subcellular Processes Other Computer Science Nuclear Theory Multimedia Solar and Stellar Astrophysics Digital Libraries Accelerator Physics Space Physics High Energy Physics Experiment Other Quantitative Biology Operating Systems Popular Physics Instrumentation and Detectors Theorems 224 / 1594 49 / 1502 174 / 1487 188 / 1353 349 / 1350 96 / 1333 254 / 1289 79 / 1258 113 / 1166 1010 / 1088 31 / 957 194 / 876 42 / 831 179 / 811 128 / 760 176 / 701 117 / 619 64 / 586 99 / 555 82 / 554 64 / 482 19 / 441 22 / 434 20 / 431 147 / 406 83 / 383 116 / 380 95 / 373 121 / 366 4 / 316 37 / 314 7 / 299 38 / 272 61 / 257 22 / 246 2 / 232 18 / 190 33 / 150 29 / 104 12 / 104 6 / 82 1 / 39 4 / 36 16 / 25 3 / 17 1 / 15 Papers 39 / 168 6 / 94 55 / 281 32 / 164 74 / 282 25 / 172 44 / 189 22 / 139 28 / 166 188 / 206 6 / 93 27 / 102 9 / 72 27 / 125 20 / 121 24 / 92 24 / 94 15 / 83 17 / 83 4 / 54 12 / 73 2 / 39 3 / 26 6 / 64 30 / 80 9 / 42 23 / 54 28 / 74 23 / 55 1 / 24 6 / 36 3 / 66 4 / 30 10 / 33 3 / 22 2 / 24 7 / 46 7 / 26 11 / 21 3 / 16 2 / 12 1 / 10 1 / 8 5 / 7 2 / 5 1 / D. More Experiments We ran our experiments with larger set of embedders, including Multilingual-E5-Large-Instruct (Wang et al., 2024), zbMath-Bert (Steinfeldt & Mihaljevic, 2024), and KaLM-Embedding-V2.5 (Zhao et al., 2025; Hu et al., 2025). 20 Theorem Search Table 8. Extended results comparing context window size in retrieval performance. Embedded without task instructions."
        },
        {
            "title": "MODEL",
            "content": "P@1 HIT@10 HIT@20 MRR@"
        },
        {
            "title": "BODY ONLY",
            "content": "ZBMATH-BERT 0.3B GEMMA 0.3B QWEN3 0.6B QWEN3 8B KALM-EMBEDDING-V2.5 MULTILINGUAL-E5-LARGE-INSTRUCT 0.053 0.184 0.224 0.224 0.131 0.171 W/ ABSTRACT ZBMATH-BERT 0.3B GEMMA 0.3B QWEN3 0.6B QWEN3 8B KALM-EMBEDDING-V2.5 MULTILINGUAL-E5-LARGE-INSTRUCT 0.092 0.303 0.224 0.250 0.197 0.197 W/ FIRST SECTION ZBMATH-BERT 0.3B GEMMA 0.3B QWEN3 0.6B QWEN3 8B KALM-EMBEDDING-V2.5 MULTILINGUAL-E5-LARGE-INSTRUCT 0.092 0.237 0.263 0.211 0.184 0.224 0.158 0.474 0.461 0.553 0.355 0.395 0.184 0.539 0.487 0.553 0.474 0.421 0.184 0.500 0.526 0.605 0.447 0.474 0.237 0.539 0.513 0.632 0.434 0. 0.237 0.566 0.526 0.618 0.513 0.553 0.237 0.566 0.645 0.645 0.474 0.539 0.086 0.290 0.299 0.313 0.203 0.237 0.113 0.376 0.297 0.332 0.272 0.283 0.122 0.321 0.352 0.343 0.272 0.304 Theorem Search Table 9. Extended results comparing LLM slogans in retrieval performance. Embedded without task instructions. Body+Abstract."
        },
        {
            "title": "MODEL",
            "content": "P@1 HIT@10 HIT@20 MRR@20 DEEPSEEK V3.1 ZBMATH-BERT 0.3B GEMMA 0.3B QWEN3 0.6B QWEN3 8B KALM-EMBEDDING-V2.5 MULTILINGUAL-E5-LARGE-INSTRUCT 0.092 0.303 0.224 0.250 0.197 0.197 DEEPSEEK ZBMATH-BERT 0.3B GEMMA 0.3B QWEN3 0.6B QWEN3 8B KALM-EMBEDDING-V2.5 MULTILINGUAL-E5-LARGE-INSTRUCT 0.000 0.158 0.132 0.118 0.066 0.066 GEMINI 3 PRO ZBMATH-BERT 0.3B GEMMA 0.3B QWEN3 0.6B QWEN3 8B KALM-EMBEDDING-V2.5 MULTILINGUAL-E5-LARGE-INSTRUCT 0.053 0.250 0.211 0.237 0.197 0.197 CLAUDE OPUS 4. ZBMATH-BERT 0.3B GEMMA 0.3B QWEN3 0.6B QWEN3 8B KALM-EMBEDDING-V2.5 MULTILINGUAL-E5-LARGE-INSTRUCT 0.053 0.303 0.224 0.263 0.197 0.224 0.184 0.539 0.487 0.553 0.474 0.421 0.079 0.421 0.316 0.355 0.289 0.250 0.171 0.553 0.487 0.566 0.434 0.513 0.184 0.553 0.539 0.632 0.421 0. 0.237 0.566 0.526 0.618 0.513 0.553 0.105 0.474 0.408 0.461 0.461 0.355 0.237 0.632 0.539 0.684 0.553 0.579 0.211 0.579 0.605 0.737 0.526 0.592 0.113 0.376 0.297 0.332 0.272 0.283 0.014 0.228 0.199 0.195 0.140 0. 0.092 0.348 0.290 0.348 0.265 0.297 0.087 0.387 0.325 0.394 0.271 0.322 22 E. Prompts Theorem Search In this section we list the prompts used during slogan generation and retrieval, shown in Tables 10, 11, and 12. Table 10. Task instructions for embedders SIDE THEOREM USER QUERY Context Body Only Body+Abstract Body+Introduction Represent the given math statement for retrieving PROMPT Instruct: related statement by natural language query.nQuery: Instruct: mathematically equivalent to the query.nQuery: Given math search query, retrieve theorems Table 11. Slogan prompts Prompt Do Output only Summaries are Summaries are plain ASCII sentences with Summaries are plain ASCII sentences with no Unicode. Describe the result without referencing it as this theorem or You generate summaries of math theorems based on theorem body. accurate and at most four sentences. no Unicode. similar. Avoid LaTeX and mathematical symbols; use words instead. the final summary sentences, with no reasoning, explanations, or commentary. not restate the prompt, input fields, or instructions. Do not include proof steps, motivation, or background discussion. You generate summaries of math theorems based on theorem body. consider paper summary in your summaries. four sentences. the result without referencing it as this theorem or similar. and mathematical symbols; use words instead. sentences, with no reasoning, explanations, or commentary. prompt, input fields, or instructions. or background discussion. You generate summaries of math theorems based on theorem body. consider paper summary and the first section of the paper in your summaries. Summaries are accurate and at most four sentences. sentences with no Unicode. theorem or similar. Output only the final summary sentences, with no reasoning, explanations, or commentary. include proof steps, motivation, or background discussion. Do not restate the prompt, input fields, or instructions. Output only the final summary Summaries are accurate and at most Avoid LaTeX and mathematical symbols; use words instead. Describe the result without referencing it as this Do not include proof steps, motivation, Summaries are plain ASCII Do not restate the Avoid LaTeX Describe You also You also Do not 23 Theorem Search Table 12. Retrieval prompts for baselines. PROMPT { role: content: ( system, CONTEXT GPT-5.2 You are an assistant that MUST answer immediately.n Do NOT ask the user questions or request permission.n Use web search as needed, including opening the latest arXiv PDF(s) to verify statement numbering.n If exact numbering cannot be verified for some item, still include it but mark number as UNVERIFIED.n Return ONLY the final listno preamble. ), }, { role: content: ( user, Return list of the top 20 most relevant math statements to the query below.n Constraints:n - Statements must be from arXiv papers.n - For each item include: (3) statement type+number exactly as in that arXiv version (e.g. (1) arXiv id, (2) version used (e.g. v3), Theorem 1.2 / Lemma 3.4), (4) section name/number, (5) statement title/short descriptor, (6) 1{2 sentence relevance note.n - Use the most recent arXiv version available.n Query:n f{row.query} ) GEMINI 3 PRO } SYSTEM INSTRUCTION = ( You are an assistant that MUST answer immediately.n Do NOT ask the user questions or request permission.n Use Google Search grounding as needed, including opening the latest arXiv PDF(s) to verify statement numbering.n If exact numbering cannot be verified for some item, still include it but mark number as UNVERIFIED.n Return ONLY the final listno preamble. ) user prompt = ( Return list of the top 20 most relevant math statements to the query below.n Constraints:n - Statements must be from arXiv papers.n - For each item include: (3) statement type+number exactly as in that arXiv version (e.g. (1) arXiv id, (2) version used (e.g. v3), Theorem 1.2 / Lemma 3.4), (4) section name/number, (5) statement title/short descriptor, (6) 1{2 sentence relevance note.n - Use the most recent arXiv version available.n Query:n f{row.query} )"
        }
    ],
    "affiliations": [
        "Department of Applied and Computational Mathematical Sciences, University of Washington, Seattle, United States",
        "Department of Mathematics, University of Washington, Seattle, United States",
        "Lake Washington High School, Kirkland, United States",
        "Math AI Lab, University of Washington, Seattle, United States",
        "Paul G. Allen School of Computer Science and Engineering, University of Washington, Seattle, United States"
    ]
}