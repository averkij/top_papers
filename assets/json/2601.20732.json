{
    "paper_title": "Continual GUI Agents",
    "authors": [
        "Ziwei Liu",
        "Borui Kang",
        "Hangjie Yuan",
        "Zixiang Zhao",
        "Wei Li",
        "Yifan Zhu",
        "Tao Feng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As digital environments (data distribution) are in flux, with new GUI data arriving over time-introducing new domains or resolutions-agents trained on static environments deteriorate in performance. In this work, we introduce Continual GUI Agents, a new task that requires GUI agents to perform continual learning under shifted domains and resolutions. We find existing methods fail to maintain stable grounding as GUI distributions shift over time, due to the diversity of UI interaction points and regions in fluxing scenarios. To address this, we introduce GUI-Anchoring in Flux (GUI-AiF), a new reinforcement fine-tuning framework that stabilizes continual learning through two novel rewards: Anchoring Point Reward in Flux (APR-iF) and Anchoring Region Reward in Flux (ARR-iF). These rewards guide the agents to align with shifting interaction points and regions, mitigating the tendency of existing reward strategies to over-adapt to static grounding cues (e.g., fixed coordinates or element scales). Extensive experiments show GUI-AiF surpasses state-of-the-art baselines. Our work establishes the first continual learning framework for GUI agents, revealing the untapped potential of reinforcement fine-tuning for continual GUI Agents."
        },
        {
            "title": "Start",
            "content": "Ziwei Liu 1 Borui Kang 1 Hangjie Yuan 2 Zixiang Zhao 3 Wei Li 1 Yifan Zhu 4 Tao Feng 1 6 2 0 2 9 2 ] . [ 2 2 3 7 0 2 . 1 0 6 2 : r Abstract As digital environments (data distribution) are in flux, with new GUI data arriving over time introducing new domains or resolutionsagents trained on static environments deteriorate in performance. In this work, we introduce Continual GUI Agents, new task that requires GUI agents to perform continual learning under shifted domains and resolutions. We find existing methods fail to maintain stable grounding as GUI distributions shift over time, due to the diversity of UI interaction points and regions in fluxing scenarios. To address this, we introduce GUI-Anchoring in Flux (GUI-AiF), new reinforcement finetuning framework that stabilizes continual learning through two novel rewards: Anchoring Point Reward in Flux (APR-iF) and Anchoring Region Reward in Flux (ARR-iF). These rewards guide the agents to align with shifting interaction points and regions, mitigating the tendency of existing reward strategies to over-adapt to static grounding cues (e.g., fixed coordinates or element scales). Extensive experiments show GUI-AiF surpasses state-of-the-art baselines. Our work establishes the first continual learning framework for GUI agents, revealing the untapped potential of reinforcement fine-tuning for continual GUI Agents. 1. Introduction Autonomous GUI agents empower users to perform actions in various digital applications through natural language instructions, such as clicking icons or locating items within an interface (Hong et al., 2024; Guan et al., 2025). The core underlying this process is grounding (Feng et al., 2022), which refers to mapping textual instructions to precise pixel coordiWork was done during an internship at Tsinghua University. 1Department of Computer Science and Technology, Tsinghua University, China 2College of Computer Science, Zhejiang University, China 3Photogrammetry and Remote Sensing Lab, ETH Zurich, Switzerland 4College of Computer Science, Beijing University of Posts and Telecommunications, China. Correspondence to: Tao Feng <fengtao.hi@gmail.com>. Preprint. January 30, 2026. 1 Figure 1. Workflow. Continual GUI Agents operate under two evolving scenarios: domain-in-flux (e.g., from Mobile OS to Web OS) and resolution-in-flux (e.g., scaling from 1080p to 4K). nates on the interactive interface (Zhao et al., 2025; Ye et al., 2025; Du et al., 2025). Currently, GUI grounding is typically achieved via Supervised Fine-Tuning (SFT) (Cheng et al., 2024; Gou et al., 2025; Lin et al., 2025; Wu et al., 2025) or Reinforcement Fine-Tuning (RFT) (Liu et al., 2025b; Gao et al., 2025; Yuan et al., 2025; Tang et al., 2025a) on fixed datasets that encompass variety of User Interface (UI) applications and interaction types (Cheng et al., 2024; Lin et al., 2025; Kapoor et al., 2024; Wu et al., 2024). Current GUI agents are only trained from fixed UI datasets. However, digital environments in the real-world are in flux as new GUI data arrives over time. Consider the following examples: GUI agents may need to adapt to new Operating System (OS) as updated versions are released, involving unfamiliar UI elements and layouts. Alternatively, GUI agents switch between OS platforms, e.g., from Mobile OS (Cheng et al., 2024) to Web OS (Lin et al., 2025). Even within the same OS, with device upgrades or customized OS versions for different devices, GUI agents are required to adapt not only to updated UIs but also to changes in screen resolution (e.g., 1080p to 4k (Li et al., 2025a)). To study this, we first propose the two scenarios in Figure 1: i) continual learning across different UI domains (from mobile, desktop to web applications), ii) continual learning under changing resolutions (from normal to high resolution). The specific settings are illustrated in Sec. 4.1. Subsequently, our in-depth analysis begins with the inherent dynamics of GUI environments. For instance, Mobile OS tends to include more textual elements, while Web OS primarily relies on icons, leading to significant differences in interaction points. Moreover, when the interface resolution varies from 1080p to 4K, significant proportional changes occur such as element scales (Li et al., 2025a). Under these dynamic conditions, the grounding capability of GUI agents"
        },
        {
            "title": "Continual GUI Agents",
            "content": "becomes difficult to maintain, as reflected by their inability to consistently anchor interaction locations and element regions in flux. Recent advances in GUI grounding often follow either the SFT or RFT framework. Among these, SFT tends to memorize and fit the specific data distribution of the current task, making it inherently unsuitable for handling dynamic and diverse GUIs (Guo et al., 2025; Liu et al., 2025a; Shenfeld et al., 2025). In contrast, RFT leverages on-policy rewards to find solutions, and minimizes the KL divergence from the reference model. This creates an implicit gradient update to softly re-aligning parameters rather than disrupting them, naturally facilitating continual learning (Bian et al., 2024; Feng et al., 2025; Lu et al., 2025a; Liu et al., 2026) for GUI agents on new tasks (Lai et al., 2025; Jin et al., 2025; Zhang et al., 2025b). Despite these advantages, the parameter-centric optimization of RFT primarily prevents drift from the reference model. When faced with domain or resolution shifts, UI interaction regions suffer from variations in both location and scaling (Shi et al., 2025; Lei et al., 2025), thus motivating method to empower agents to continually learn in shifted UI environments. In this paper, we introduce GUI-AiF, Continual GUI Agents framework with the ability to perform Anchoring in Flux, which makes agents adapt to the flux of UI interactions. Specifically, the framework is designed to optimize policy within the RFT paradigm. Beyond the RFT advantages obtained from grounding policy, GUI-AiF incorporates novel reward mechanism, to encourage agents to anchor diverse interaction points and element regions, thereby mitigating grounding bias from static GUI tasks and enhancing adaptability for continual GUI migration. GUI-AiF introduces two rewards: Anchoring Point Reward ) and Anchoring Region Reward in in Flux (APR-iF Flux (ARR-iF ). APR-iF enhances the agents ability to generalize interaction points by rewarding exploration of diverse locations, avoiding over-adapting to coordinates from static task. ARR-iF promotes robustness to scale variations by encouraging diversity in predicted element regions, helping the agents adapt to different sizes across GUI tasks. In summary, the main contributions are following: We introduce Continual GUI Agents, and establish two novel scenarios (shifting domains and resolutions) for GUI agents to perform continual learning. We propose GUI-AiF to optimize the grounding policy ) and reward within the RFT paradigm via APR-iF ( ARR-iF ( ) modules, mitigating policy over-adaption on single task and enhancing agents continual ability in shifted UI tasks. Our GUI-AiF achieves state-of-the-art performance on the ScreenSpot-V1, V2, and Pro benchmarks, outperforming existing baselines. 2. Related Works 2.1. GUI Agents GUI agents understand graphical user interfaces through natural language instructions, enabling automated execution of digital human-computer interaction tasks (Hong et al., 2024; Cheng et al., 2024; Lin et al., 2025; Yuan et al., 2025; Gao et al., 2025; Tang et al., 2025a; Wang et al., 2024; Gou et al., 2025; Qin et al., 2025; Zhang et al., 2025a; Liu et al., 2025b). Early GUI approaches rely on structured representations like HTML code (Wen et al., 2024), and designed workflow modules such as planners (Wang et al., 2024) on closed-source large language models (Achiam et al., 2023; Li et al., 2025b). By fine-tuning models on GUI corpora, agents are aligned with human-like processing through vision-language interaction, leading to significantly improved grounding ability across various GUI contexts (Gou et al., 2025; Tang et al., 2025b; Wu et al., 2024; Tang et al., 2025a). As one of the fine-tuning paradigms, SFT is the pioneering method adopted to build the GUI grounding (Cheng et al., 2024; Gou et al., 2025; Qin et al., 2025; Lin et al., 2025; Wu et al., 2025), while the dependency on extensive labeled datasets curtails its adaptability, leading to generalization challenges when faced with unseen scenarios. RFT leverages policy rewards to guide GUI grounding behavior (Tang et al., 2025a; Yuan et al., 2025; Gao et al., 2025; Lu et al., 2025b; Luo et al., 2025; Liu et al., 2025b), hence significantly enhancing performance. However, current GUI agents overlook the flux as new data appears, such as varying OS platforms and interface resolutions, hence motivating our GUI-AiF for Continual GUI Agents. 2.2. Continual Post-Training Post-training shares similar foundations with Continual GUI Agents. SFT paradigm often struggles as it leverages crossentropy objective forces the model to match the specific label distribution of the current task, leading to its convergence being arbitrarily far from previously learned capability (Shenfeld et al., 2025; Guo et al., 2025; Liu et al., 2025a). On the other hand, RFT offers potential advantages as its on-policy updates inherently favor solutions with minimal KL divergence from the base model, which is strongly correlated with reduced forgetting (Shenfeld et al., 2025; Jin et al., 2025; Liu et al., 2025c; Zhang et al., 2025b). Additionally, RFTs reward mechanism is able to implicitly scale parameter updates, hence alleviating the forgetting of prior knowledge (Jin et al., 2025). The efficacy of rule-based reward reinforcement learning, notably demonstrated by Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which has been successfully applied across various domains (Shen et al., 2023; Li et al., 2025c), makes its use within the RFT framework. However, standard RFT optimization, including GRPO advantage estimation and KL"
        },
        {
            "title": "Continual GUI Agents",
            "content": "Figure 2. Overview. The Above shows the Continual GUI Agents pipeline. GUI-AiF is proposed to advance the RFT paradigm in this setting by shaping grounding rewards: APR-iF ( ) refines element regions. Together, GUI-AiF enables agents to better adapt to varying interaction locations and element scales. ) encourages diverse interaction points, while ARR-iF ( divergence, is designed towards maximizing performance on the current task distribution. It lacks adaptability when encountering sequences with drastic shifts in visual layout, interaction domain, and resolutions inherent to GUI tasks. 3. Method We introduce GUI-AiF for the proposed Continual GUI Agents task, centering the grounding optimization by integrating rewards of predicted interaction locations and element scales into RFT policy. As illustrate in Figure 2, GUIAiF comprises two key innovations: APR-iF ( ) rewards ) encourages exploration of interaction points; ARR-iF ( diversity in predicted element regions, to enhance the adaptability of agents against the flux of GUI environments. 3.1. Problem Formation The fundamental task of GUI grounding is to align users natural language instruction with its corresponding interactive element on graphical interface at the pixel level. Given interface and an instruction i, the model predict 2, yp bounding box by fixed prompt bp = [xp 2] that localizes the region required by i, where (x1, y1) and (x2, y2) denote the top-left and bottom-right corners respectively. We also assume that the continual GUI tasks as TD or TR and tasks arrive sequentially, where TD = {td1, td2, ..., tdn} represents tasks from distinct domains and TR = {tr1, tr2, ..., trn} denotes tasks with shifting interface resolutions. Learning across such sequences requires the agents to continually adapt to the GUI task flux, preventing over-adaptation to any single task tk. 1, xp 1, yp In standard RFT policy, the model generates sequence of tokens representing the predicted bounding box bp, encompassing both the interaction location (center point) and element scale (predicted region). While the reward poli } to the ground truth {bgt icy method computes advantages solely by comparing predictions {bp }, this focuses on optimizing the current task, tending to over-adapt on coordinates and scales at specific GUI layouts. Our GUI-AiF modifies this objective by integrating two rewards designed to promote grounding exploration, achieved by encouraging both varying predicted center points and element regions. 3.2. GUI-AiF Rewards 1, cp 1, bp 2, ..., cp 2, ..., bp Anchoring Point Reward in Flux. To enhance the agents capacity to generalize interaction locations, avoiding overadapting in particular coordinates of learned single task, this APR-iF encourages the exploration of diverse interaction points of predicted bounding boxes bp. Given set of predicted {bp } generated by the agents for an instruction, APR-iF first computes their corresponding center points {cp is cal2,i, yp culated from its bounding box {bp 2,i] as cp defines candidate anchor point proposed by the agents based on the instruction understanding. To quantify the diversity among these proposed points, APR-iF computes their spatial varij=1 cp ance Rv. It first calculates the centroid cp = 1 , which represents the average interaction point, then Rp is formulated as the average squared Euclidean distance between each individual point cp and average prediction cp: } where each center point cp 1,i, xp } = [xp . Physically, each cp 1,i+yp yp 2 1,i+xp 2 1,i, yp = (cid:80)N (cid:16) xp (cid:17) 2,i 2,i , Rp ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 cp2 . cp (1) Intuitively, higher variance indicates that center points are more spread out across the interface, rather than being clustered around single coordinate. It helps the agents adapt to different interaction locations in GUI environments. Anchoring Region Reward in Flux. Beyond diversifying"
        },
        {
            "title": "Continual GUI Agents",
            "content": "interaction points, continual GUI grounding should also adapt to varying element scales, reflected in the predicted element regions. The ARR-iF addresses this by rewarding spatial separation among the predicted bounding boxes. ARR-iF first models each predicted bounding box bp as Gaussian distribution Ni(µi, Σi), which transforms the predicted region into continuous probability distribution, allowing us to formally measure their separation. The mean µi of each distribution is its center cp , and the covariance matrix Σi is diagonal matrix whose variances are proportional to the bounding boxs width and height, which encodes the predicted scale and shape of the element region. To quantify the separation between any two predicted regions, ARR-iF computes the Bhattacharyya distance DB between them. For two probability distributions Ni and Nj, this distance is derived from their Bhattacharyya coefficient and has closed-form solution which can be formulated as: DB(Ni, Nj) = 1 8 + (µi µj)T Σ1 avg(µi µj) (cid:32) 1 2 ln det(Σavg) (cid:112)det(Σi) det(Σj) (cid:33) , (2) where Σavg = Σi+Σj is the average covariance. Finally, the total regions separation Rr is defined as the average DB over pairs from all predictions: 2 Rr ="
        },
        {
            "title": "2\nN (N − 1)",
            "content": "N 1 (cid:88) (cid:88) i=1 j=i+1 DB(Ni, Nj). (3) This reward Rs incentivizes the agents to generate spatially distinct prediction regions, hence handling the diverse element scales encountered across fluxional GUI tasks. Integration Rewards. We formulate APR-iF and ARRiF rewards to RAiF as weighted sum, to consider the generalization of interaction locations and element scales: RAiF = α Rp + γ Rr, (4) where α and γ are hyperparameters for each reward, which control the intensity of exploration over diverse points and regions. By integrating RAiF into the RFT policy, GUI-AiF strikes trade-off between optimizing for the current task and promoting adaptability for future unseen tasks. 3.3. Reinforcement Fine-tuning with GUI-AiF The RFT employs policy method to compute score ri for each prediction by comparing it against the ground truth, and obtains set of reward scores {r1, r2, ..., rN }. We leverage the GRPO method to calculate the relative policy advantage (Shao et al., 2024). Assuming Ai is the i-th prediction, GRPO normalizes its score against the mean and standard deviation of the entire group of scores: Ai = ri ean({r1, r2, ..., rN }) Std({r1, r2, ..., rN }) . (5) Table 1. Illustration of typical and continual GUI agents. Definition Description Scenario Typical GUI Agents Continual GUI Agents Joint training on aggregated datasets Static UI environment Continual training on sequential datasets 1) Mobile Desktop Web 2) Normal High Resolution To control the parameters drift from the reference model, KL divergence DKL is further incorporated into the objective function. This term regularizes the policy updates by measuring the divergence between the current policy πθ, and reference policy πref , which is typically the model from before the optimization step. Functionally, RAiF serves as reward signal designed to encourage agents to explore interaction points and element regions that are in shifted location and scaling. The standard GRPO advantage At computes correctness compared with ground-truth, driving the policy to maximize performance on the current task. However, such exploitation risks overfit to current UI layouts. To mitigate this, RAiF is introduced as counterbalance to the exploitation tendency, explicitly reshaping the optimization landscape to favor policies that are not only accurate but also generalized robust, thereby preserving the agents continual learning ability for future environmental flux. Therefore, we combine Ai with the RAiF as comprehensive advantage at given timestep t, and form (θ) as the objective optimization to guide the GUI agents updates. The (θ) can be formulated: (θ) = Eτ πθ [rt(θ)(At + RAiF ) β DKL[πref (st)πθ(st)]] , (6) where rt(θ) = πθ(atst) πref (atst) is the probability ratio of the current and reference policies, β is hyperparameter that controls the strength of the KL divergence term. 4. Experiments 4.1. Continual GUI Agents Implementation Unlike typical GUI agent training pipelines that assume static environments, we propose two dynamic GUI scenarios for continual learning (domain and resolution sequences), as shown in Table 1. For domain sequence, we start to train GUI agents with Widget Captioning (Cheng et al., 2024) for mobile applications, and then separately train on the desktop and web data from ShowUI-web (Lin et al., 2025). For resolution sequence, we first train on ShowUI-web (Lin et al., 2025) (containing 1080p resolution desktop/web data), then we sequentially train on OmniACT (Kapoor et al., 2024) which includes desktop/web data resolution from 1080p up to 4K. Table 2. Performance (%) in continual GUI domain on ScreenSpot-V1 and ScreenSpot-V2. The M., D. and W. represents the Mobile, Desktop and Web platforms. The upper bound comes from the optimal results of typical GUI Agent method and serves as reference."
        },
        {
            "title": "Continual GUI Agents",
            "content": "Method Proprietary Model GPT-4o (OpenAI, 2024) Open-source Model Qwen2.5VL-3B (Bai et al., 2025) SFT-based GUI Method SeeClick (Cheng et al., 2024) GUI-Actor (Wu et al., 2025) RFT-based GUI Method SE-GUI (Yuan et al., 2025; Liu et al., 2025b) GUI-AiF GUI-G2 (Tang et al., 2025a) GUI-AiF SSv1 Accuracy (%) SSv2 Accuracy (%) Mobile Desktop Web Text Icon Text Icon Text Icon Avg. Mobile Desktop Web Text Icon Text Icon Text Icon Avg. - - - - - - 18.8 - - - - - - 20.1 90.2 72.9 78. 57.1 80.9 64.1 72.1 88.5 74. 78.9 58.6 76.9 64.5 73.6 Upper Bound M. M.D. M.D.W. Upper Bound M. M.D. M.D.W. Upper Bound M. M.D. M.D.W. M. M.D. M.D.W. Upper Bound M. M.D. M.D.W. M. M.D. M.D.W. 78.0 52.7 69.1 73. - 74.4 91.2 87.6 - 91.5 91.8 92.0 91.4 92.7 91.9 96.7 91.9 93.8 95.2 92.9 94.1 96.1 52.0 59.8 63.4 65. - 76.4 71.2 70.7 - 75.9 76.6 78.2 77.3 78.2 79.5 90.8 77.2 76.4 76.4 78.7 77.3 76.9 72.2 52.1 79.8 78. - 58.8 85.1 87.1 - 72.9 78.1 83.1 83.5 86.6 87.1 95.9 78.8 88.2 92.8 81.9 93.2 95.7 30.0 42.9 48.5 57. - 61.4 70.0 67.1 - 59.3 65.0 65.7 67.1 68.4 70.7 88.6 61.4 61.4 63.6 68.6 69.3 68.3 55.7 20.4 67.2 69. - 32.2 74.4 72.6 - 73.9 76.3 77.4 76.9 76.2 78.2 90.9 72.6 75.2 79.6 81.6 81.9 84.8 32.5 25.7 44.3 53. - 63.1 69.4 65.1 - 55.1 58.3 60.2 63.5 62.2 65.8 86.9 51.2 53.9 54.9 65.1 66.2 68.2 53.4 42.3 62.1 66. - 61.0 76.9 75.0 88.2 71.4 74.4 76.1 76.6 77.4 78.9 92.0 72.2 74.8 77.1 78.1 80.3 81.7 78.4 51.7 67.2 71. 96.9 76.7 95.4 91.1 - 90.4 92.2 93.2 91.3 94.1 94.8 - 92.5 94.2 94.8 93.3 95.9 96.6 50.7 58.8 63.3 63. 89.6 77.5 73.3 74.9 - 81.0 81.9 82.5 81.1 81.4 82.0 - 81.5 78.7 79.1 82.1 79.6 83.6 70.1 51.5 80.2 79. 97.4 61.1 85.0 87.8 - 84.1 87.6 90.2 88.1 88.7 90.2 - 85.2 87.2 89.3 88.5 89.7 90.2 29.3 45.0 53.6 57. 86.4 61.9 73.8 72.2 - 60.1 67.4 69.3 70.7 72.1 75.0 - 66.4 66.4 68.6 73.6 77.1 77.4 55.2 15.4 64.7 70. 95.7 35.0 75.4 80.0 - 71.8 73.7 75.1 76.1 76.7 76.8 - 73.5 77.8 81.2 80.3 81.2 82.9 32.5 27.6 49.5 49. 84.7 67.1 68.8 61.2 - 64.5 63.5 63.1 67.9 66.1 68.2 - 64.8 65.7 68.6 68.0 68.3 70.5 55.1 41.7 63.4 65. 92.5 63.2 78.6 77.9 90.3 75.3 77.7 78.9 79.2 79.9 81.2 93.3 77.3 78.3 80.3 80.9 81.9 83.5 We evaluate on three benchmarks: ScreenSpot-V1 (Cheng et al., 2024) (SSv1), ScreenSpot-V2 (Wu et al., 2024) (SSv2) and ScreenSpot-Pro (Li et al., 2025a) (SSPro). The former two cover mobile, desktop, and web tasks for continual domain evaluation, and the latter one contains 6 highresolution software interfaces including CAD, Development Programming (Dev), Creative Software (Creative), Scientific and Analytic (Scientific), Office Software (Office) and Operating System Commons (OS) to evaluate continual resolution performance. 4.2. Baselines and Training Details We consider the SeeClick (Cheng et al., 2024) and GUIActor (Wu et al., 2025) as SFT-based GUI agent methods. Among RFT-based SOTA methods approaches: InfiGUIR1 (Liu et al., 2025b), SE-GUI (Yuan et al., 2025) and GUIG2 (Tang et al., 2025a) employing distinct reward policies. InfiGUI-R1 only computes IoU rewards between predicted and ground truth bounding boxes, SE-GUI solely uses distance rewards between predicted and ground truth center points. GUI-G2 models bounding boxes with Gaussian distributions to calculate dense rewards considering both center point and region coverage. Since GUI-AiF focuses on the reward of interaction points and element regions, we combine the IoU rewards of InfiGUI-R1 with distance rewards of SE-GUI into an integrated baseline named by SE-GUI. We leverage Qwen2.5VL-3B for fine-tuning (Bai et al., 2025) in all experiments. Besides, we refer to the best results from baselines as upper bound. Since they leverage more abundant training datasets, upper bounds usually obtain better performance. SeeClick (Cheng et al., 2024) use older Qwen2-VL model hence getting worse upper bound. For training, we implement 4 NVIDIA A100-80G GPUs for one epoch with the following hyperparameters: learning rate 1e-6, global batch size 8 and 4 generated predictions for each instruction. We set KL divergence β to 0.04, employ Flash Attention 2 and bfloat16 precision with gradient checkpointing. The weights α and γ are set to 15 and 0.5. 4.3. Main Results Continual GUI Domain. We first evaluate the continual domain performance of GUI-AiF on SSv1 and SSv2 shown in Table 2. We first find the proprietary model struggles with GUI tasks. In contrast, while open-source models present some capability in this area, their performance indicates space for further improvement. Secondly, SFT methods 5 Table 3. Performance (%) in continual GUI resolution on ScreenSpot-Pro. The N. and H. represents the Normal and High resolution. The upper bound comes from the optimal results of traditional GUI Agent method and serves as reference."
        },
        {
            "title": "Continual GUI Agents",
            "content": "Method Proprietary Model GPT-4o (OpenAI, 2024) Claude Computer Use (Anthropic, 2024) Open-source Model Qwen2.5VL-3B (Bai et al., 2025) SFT-based GUI Methods SeeClick (Cheng et al., 2024) RFT-based GUI Methods GUI-G2 (Tang et al., 2025a) GUI-AiF SSPro Accuracy (%) CAD Dev Creative Scientific Office OS Avg. Text Icon Text Icon Text Icon Text Icon Text Icon Text Icon 2.0 14.5 0.0 3.7 1.3 22.0 0.0 3.9 1.0 25. 0.0 3.4 2.1 33.9 0.0 15.8 1.1 30.1 0.0 16.3 0.0 11. 0.0 4.5 0.8 17.1 9.1 7.3 22.1 1. 26.8 2.1 38.2 7.3 33.9 15. 10.3 1.1 16.1 Upper Bound N. N.H. Upper Bound N. N.H. N. N.H. 2.5 23.4 26.4 55.8 24.3 24.5 27.4 20.3 0.0 3.1 6.3 12.5 1.7 6.3 4.7 15. 0.6 16.2 25.3 68.8 17.5 24.4 24.7 29.2 0.0 0.7 0 17.2 2.1 4.1 1.4 2. 1.0 12.1 20.8 57.1 20.2 23.7 17.7 23.2 0.0 2.1 5.6 15.4 6.3 5.6 8.2 3. 3.5 18.1 28.1 77.1 22.2 27.3 29.2 33.3 0.0 11.8 14.6 24.5 12.8 12.6 14.6 14. 1.1 29.4 35.6 74.0 36.2 35.1 35.3 38.4 0.0 7.5 11.3 32.7 11.3 9.4 20.8 18. 2.8 5.6 17.7 57.9 17.7 21.2 17.8 22.4 0.0 3.4 8.9 21.3 4.5 5.6 10.1 6. 1.1 11.1 16.7 47.5 14.7 16.7 17.7 19.0 (a) ScreenSpot-V1 (b) ScreenSpot-V2 (c) ScreenSpot-Pro Figure 3. Hyperparameter sensitivity analysis for α and γ. Performance peaks at (α, γ) = (1,1) on three benchmarks. and RFT methods both can be adopted into Continual GUI Agents, while the performance of SFT methods is generally poor, verifying its over-adaptation tendency in single GUI task. While the RFT baselines show some adaptation, GUIAiF further boosts continual performance based on these methods, demonstrating that GUI-AiF enhances the GUI agents adaptability facing domain shifts, such as migrating from mobile OS to Web OS. Finally, the performance of single-domain generally increases after training on each task, indicating effective continual learning of GUI-AiF which accumulates knowledge from learned tasks. Continual GUI Resolution. We then evaluate the continual resolution performance of GUI-AiF in Table 3. Since the Gaussian modeling-based RFT provides better performance, we select the GUI-G2 (Tang et al., 2025a) as the representative RFT baseline in this and following evaluations. Firstly, while proprietary Claude (Anthropic, 2024) and open-source Qwen2.5VL-3B (Bai et al., 2025) show comparable general performance, they were not evaluated on the shifted resolution tasks. Crucially, when facing the normal to high resolution shift, both the SFT and the RFT baselines achieve the exact same final average accuracy and are worse than GUI-AiF. This indicates that neither above paradigms can inherently handle resolution variation. The finding implies shift in resolution is fundamental visual distortion of element scales and locations. By promoting generalization in these two aspects, our GUI-AiF enhances the GUI agents performance on this challenge. GUI-AiF Rewards Ablation. We conduct the ablation study of APR-iF and ARR-iF on ScreenSpot-V1 and ScreenSpot-V2, as shown in Table 4. Firstly, the full GUIAiF rewards achieve the best performance, significantly outperforming single APR-iF or ARR-iF. This indicates that rewarding perception ability of generalizing interaction locations and element scales is both core to Continual GUI Agents. We can observe the performance gain of full GUIAiF rewards is decreased when either reward is removed. Besides, we find the final Avg. accuracy of APR-iF outperforms ARR-iF under SSv1 and SSv2, which demonstrates the APR-iF reward has more significant positive impact on performance. This impact nature of the variation in GUI environment: task shifts such as migrating from Mobile to Web or scaling to higher resolution, are fundamental"
        },
        {
            "title": "Continual GUI Agents",
            "content": "Table 4. Ablation study of APR-iF ( Mobile, Desktop and Web platforms. The second and third lines refers to using APR-iF and ARR-iF, respectively. ) on ScreenSpot-V1 and ScreenSpot-V2. The M., D. and W. represents the ) and ARR-iF ( SSv1 Accuracy (%) SSv2 Accuracy (%)"
        },
        {
            "title": "Method",
            "content": "GUI-AiF APR-iF ( ) ARR-iF ( )"
        },
        {
            "title": "Icon",
            "content": "M. M.D. M.D.W. M. M.D. M.D.W. M. M.D. M.D.W. 92.9 94.1 96.1 91.6 93.4 93.4 89.7 92.7 92. 78.7 77.3 76.9 77.8 74.7 75.9 74.2 75.5 76.9 81.9 93.2 95.7 81.9 90.2 93.2 81.5 89.2 90. 68.6 69.3 68.3 67.1 66.4 65.0 65.0 62.9 63.6 81.6 81.9 84.8 80.1 76.5 78.7 76.5 73.9 75. 65.1 66.2 68.2 63.1 51.5 54.9 61.7 49.0 50.5 Avg. 78.1 80.3 81.7 76.9 75.5 76. 75.5 75.4 76."
        },
        {
            "title": "Icon",
            "content": "93.3 95.9 96.6 93.1 95.2 94.8 91.0 94.1 94.1 82.1 79.6 83.6 82.9 79.1 80.6 77.8 78.2 79. 88.5 89.7 90.2 86.0 88.8 90.5 85.1 88.1 90.7 73.6 77.1 77.4 71.4 68.6 70.0 70.0 67.9 67. 80.3 81.2 82.9 78.6 77.4 79.4 77.8 75.2 78.2 68.0 68.3 70.5 67.5 56.7 66.5 64.5 54.2 56. Avg. 80.9 81.9 83.5 80.0 77.6 80.3 77.7 76.3 77.8 form of layout transformation that causes element positions to change drastically. The fact that APR-iF which generalizes points, is better equipped to handle this variation. This experiment suggests that learning to adapt to new interaction locations is more critical factor than adapting to element scales for Continual GUI Agents. 4.4. Discussion Hyperparameter Sensitivity. GUI-AiF leverages the weights α and γ to control the intensity of exploration over diverse points and regions. We explore the effect of these two hyperparameters. Specifically, we scale two parameters to 2 times and 1/2, and present final average accuracy of (Tang et al., 2025a) GUI-AiF on SSv1, SSv2, and SSPro in Figure 3. Firstly, we find that α which is related to APR-iF has greater impact on performance in the continual GUI domain scenario. We consider that domain flux is more reflected in its impact on GUI interaction points. In contrast, varying γ expresses more obvious impact under continual GUI resolution, potentially caused by higher resolution leads to the diversity of GUI element scales. Forward Transfer Evaluation. We further find that GUIAiF enhances forward transfer capability of GUI agents, enabling them to leverage knowledge learned from prior tasks to generalize to subsequent tasks. As the results shown in Figure 4, after training on Mobile platforms, the performance achieved on the subsequent Desktop and Web platforms by GUI-AiF surpasses the corresponding baseline under SSv2. Similar improvement can be observed under SSPro: the results trained on normal resolution task with GUI-AiF generally outperform corresponding baseline in high resolution task. These results show that encouraging GUI agents to explore diverse interaction regions prevents them from over-adapting to specific layouts, leading to stronger generalization when facing interface shifts. This observation provides favorable support for GUI-AiF. (a) Trained on mobile platforms. (b) Trained on normal resolution. Figure 4. Forward transfer evaluation of GUI-AiF. KL Divergency. As the KL term is beneficial for providing continual capacity via measuring the gap from reference model, we remove KL term in the ablation study and conduct evaluations on three benchmarks shown in Table 5. As hypothesized, removing degrades continual learning performance of GUI agents compared to Table 2, the final average and specific task accuracy generally appear decline. This result demonstrates that solely pursuing diverse solutions is insufficient, as the unconstrained exploration can collapse the policy and overwrite prior knowledge. The KL term provides stable foundation, ensuring GUI-AiF is grounded and hence helping agents adapt to dynamic GUI tasks. Reward Comparison Analysis. We analyze the reward values within the initial 200 training steps of mobile platforms and normal resolution tasks. These rewards include APR-iF, ARR-iF, the RFT policys rewards for predicted bounding box point and region coverage. From Figures 5a and 5c, we can observe that APR-iF fluctuates more significantly than ARR-iF within each task. We consider that exploration of interaction points plays more critical role in the Continual GUI Agents, finding that aligns with Table 4. Besides, we count the rewards value (APR-iF + ARR-iF) of GUIAiF and rewards value (point + coverage) of RFT in each training step, and illustrate their changing trend by scatter plot. As shown in Figures 5b and 5d, this demonstrate an inverse relationship between the two reward types: rewards designed to promote the generalization of GUI agents and 7 Table 5. Ablation study w/o KL divergency under continual GUI domain and resolution. Bold represents the best result in each task."
        },
        {
            "title": "Continual GUI Agents",
            "content": "SSv1 Accuracy (%) SSv2 Accuracy (%)"
        },
        {
            "title": "Icon",
            "content": "M. M.D. M.D.W. 83.5 91.9 93.0 70.3 75.5 74.7 79.4 86.1 92.3 66.4 64.3 68.6 71.3 72.6 77. 58.7 48.5 56.3 Avg. 71.6 73.2 77."
        },
        {
            "title": "Icon",
            "content": "83.8 92.6 92.8 83.9 76.3 77.7 82.5 88.1 93.3 70.0 67.9 73.6 67.5 74.9 78.2 65.5 55.2 61. SSPro Accuracy (%)"
        },
        {
            "title": "Icon",
            "content": "N. N.H. 22.3 30.5 1.6 3.1 23.4 34.4 2.1 0.7 18.2 24. 3.5 5."
        },
        {
            "title": "Office",
            "content": "OS"
        },
        {
            "title": "Icon",
            "content": "22.2 30.2 11.8 13.6 28.8 34.1 11.3 9.4 15.9 28.0 4.5 6. Avg. 73.9 75.8 79.6 Avg. 13.8 18. (a) Mobile platforms. (b) Reward trend. (a) Original Web interface. (b) SFT learns all domains. (c) Normal resolution. (d) Reward trend. Figure 5. Reward value analysis within two scenarios. (a)/(c) show reward statistics, while (b)/(d) show their trend distributions. those focused on optimizing performance on static current task, which appear to be conflicting objectives. Visualization Comparison. We provide the interaction region heatmaps shown in Figure 6. Figure 6a displays the Web interface with the instruction Follow this player. First, the heatmap of SFT baseline learning all domain tasks (Figure 6b) fails to locate the correct interaction region, indicating the methods poor ability to adapt to the variation of shifting GUI interface. We then compare the heatmaps from GUI-AiF trained only on the Mobile domain (Figure 6c) versus GUI-AiF that has continually learned all domains (Figure 6d). We observe that while the former correctly identifies the interaction region, its bounding box remains unrelated to the content and lacks fine-grained precision, the latter accurately locates the region based on the instruction. This evolution demonstrates GUI-AiFs capability to refine its grounding through continual learning in shifting tasks. Limitations. Due to the computing resource constraints, we mainly explore 3B model scale, three specific training datasets, and three evaluation benchmarks; this scope inherently restricts the absolute performance of the GUI 8 (c) GUI-AiF trains on Mobile. (d) GUI-AiF learns all domains. Figure 6. Visualizations of interaction region heatmaps on web platforms. Instruction used: Follow this player. agents. We believe that expanding the model size and using more abundant data would enable the agents to further bridge the gap with the grounding boundary. Additionally, our exploration is in two types of GUI shifts (domain and resolution). However, the flux of real-world digital applications is far more diverse, such as layout shifts (app updates), appearance shifts (dark mode), and localization (language changes). Our future work continues to explore the Continual GUI Agents under broader and more complex scenarios. 5. Conclusion In this work, we formalize Continual GUI Agents, new task setting that models GUI interaction under domain and resolution shifts, and we highlight the limitations of static grounding reward optimization in dynamic GUI environments. Then, we propose GUI-AiF, equipped with APR-iF and ARR-iF, which allow agents to continuously anchor interaction points and regions despite evolving GUIs, thereby mitigating over-adaptation to static grounding cues. Extensive experiments show that GUI-AiF outperforms state-ofthe-art baselines, establishing the first continual learning framework for GUI agents."
        },
        {
            "title": "References",
            "content": "Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Guo, H., Zeng, F., Zhu, F., Wang, J., Wang, X., Zhou, J., Zhao, H., Liu, W., Ma, S., Zhang, X.-Y., et al. Continual Learning for Generative AI: From LLMs to MLLMs and Beyond. arXiv preprint arXiv:2506.13045, 2025. Anthropic."
        },
        {
            "title": "Claude",
            "content": "Computer https://www.anthropic.com/news/ developing-computer-use, 2024. accessed 30 October 2025. Use. Online; Hong, W., Wang, W., Lv, Q., Xu, J., Yu, W., Ji, J., Wang, Y., Wang, Z., Dong, Y., Ding, M., et al. Cogagent: visual language model for gui agents. In Proceedings of the Computer Vision and Pattern Recognition, pp. 14281 14290, 2024. Bai, S., Chen, K., Liu, X., Wang, J., Ge, W., Song, S., Dang, K., Wang, P., Wang, S., Tang, J., et al. Qwen2.5VL Technical Report. arXiv preprint arXiv:2502.13923, 2025. Bian, A., Li, W., Yuan, H., Wang, M., Zhao, Z., Lu, A., Ji, P., Feng, T., et al. Make Continual Learning Stronger via C-flat. Advances in Neural Information Processing Systems, 2024. Cheng, K., Sun, Q., Chu, Y., Xu, F., YanTao, L., Zhang, J., and Wu, Z. SeeClick: Harnessing GUI Grounding In Proceedings of for Advanced Visual GUI Agents. the 62nd Association for Computational Linguistics, pp. 93139332, 2024. Du, Y., Yan, Y., Tang, F., Lu, Z., Zong, C., Lu, W., Jiang, S., and Shen, Y. Test-time Reinforcement Learning for GUI Grounding via Region Consistency. arXiv preprint arXiv:2508.05615, 2025. Feng, T., Wang, M., and Yuan, H. Overcoming Catastrophic Forgetting in Incremental Object Detection via Elastic Response Distillation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022. Feng, T., Li, W., Zhu, D., Yuan, H., Zheng, W., Zhang, D., and Tang, J. Zeroflow: Overcoming Catastrophic Forgetting is Easier than You Think. ICML, 2025. Gao, L., Zhang, L., and Xu, M. UIShift: Enhancing VLMbased GUI Agents through Self-supervised Reinforcement Learning. arXiv preprint arXiv:2505.12493, 2025. Gou, B., Wang, R., Zheng, B., Xie, Y., Chang, C., Shu, Y., Sun, H., and Su, Y. Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents. In The Thirteenth International Conference on Learning Representations, 2025. Guan, Z., Li, J. C. L., Hou, Z., Zhang, P., Xu, D., Zhao, Y., Wu, M., Chen, J., Nguyen, T.-T., Xian, P., et al. KG-RAG: Enhancing GUI Agent Decision-Making via Knowledge Graph-Driven Retrieval-Augmented Generation. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pp. 53965405, 2025. Jin, H., Luan, S., Lyu, S., Rabusseau, G., Rabbany, R., Precup, D., and Hamdaqa, M. RL Fine-tuning Heals OOD Forgetting in SFT. arXiv preprint arXiv:2509.12235, 2025. Kapoor, R., Butala, Y. P., Russak, M., Koh, J. Y., Kamble, K., AlShikh, W., and Salakhutdinov, R. Omniact: Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web. In European Conference on Computer Vision, pp. 161178, 2024. Lai, S., Zhao, H., Feng, R., Ma, C., Liu, W., Zhao, H., Lin, X., Yi, D., Xie, M., Zhang, Q., et al. Reinforcement Fine-tuning Naturally Mitigates Forgetting in Continual Post-training. arXiv preprint arXiv:2507.05386, 2025. Lei, B., Xu, N., Payani, A., Hong, M., Liao, C., Cao, Y., and Ding, C. GUI-Spotlight: Adaptive Iterative Focus Refinement for Enhanced GUI Visual Grounding. arXiv preprint arXiv:2510.04039, 2025. Li, K., Meng, Z., Lin, H., Luo, Z., Tian, Y., Ma, J., Huang, Z., and Chua, T.-S. Screenspot-Pro: GUI Grounding for Professional High-Resolution Computer Use. arXiv preprint arXiv:2504.07981, 2025a. Li, W., Shao, Y., Yang, J., Lu, Y., Zhong, L., Wang, Y., and Duan, M. How Auxiliary Reasoning Unleashes GUI Grounding in VLMs. arXiv preprint arXiv:2509.11548, 2025b. Li, X., Yan, Z., Meng, D., Dong, L., Zeng, X., He, Y., Wang, Y., Qiao, Y., Wang, Y., and Wang, L. Videochat-R1: Enhancing Spatio-Temporal Perception via Reinforcement Fine-tuning. arXiv preprint arXiv:2504.06958, 2025c. Lin, K. Q., Li, L., Gao, D., Yang, Z., Wu, S., Bai, Z., Lei, S. W., Wang, L., and Shou, M. Z. ShowUI: One VisionLanguage-Action Model for GUI Visual Agent. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1949819508, 2025. Liu, W., Zhu, F., Guo, H., Wei, L., and Liu, C.-L. LLaVA-c: Continual Improved Visual Instruction Tuning. arXiv preprint arXiv:2506.08666, 2025a."
        },
        {
            "title": "Continual GUI Agents",
            "content": "Liu, Y., Li, P., Xie, C., Hu, X., Han, X., Zhang, S., Yang, H., and Wu, F. InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners. arXiv preprint arXiv:2504.14239, 2025b. Liu, Z., Sun, Z., Zang, Y., Dong, X., Cao, Y., Duan, H., Lin, D., and Wang, J. Visual-RFT: Visual Reinforcement Fine-tuning. arXiv preprint arXiv:2503.01785, 2025c. Liu, Z., Kang, B., Li, W., Yuan, H., Yang, Y., Li, W., Luo, J., Zhu, Y., and Feng, T. Branch, or Layer? Zeroth-Order Optimization for Continual Learning of Vision-Language Models. AAAI, 2026. Lu, A., Yuan, H., Feng, T., and Sun, Y. Rethinking the Stability-Plasticity Trade-off in Continual Learning from an Architectural Perspective. arXiv preprint arXiv:2506.03951, 2025a. Lu, Z., Chai, Y., Guo, Y., Yin, X., Liu, L., Wang, H., Xiong, G., and Li, H. UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement Learning. CoRR, 2025b. Luo, R., Wang, L., He, W., Chen, L., Li, J., and Xia, X. GUI-R1: Generalist R1-Style Vision-Language Action Model for GUI Agents. arXiv preprint arXiv:2504.10458, 2025. OpenAI. ChatGPT-4o. https://chatgpt.com, 2024. Online; accessed 30 October 2025. Qin, Y., Ye, Y., Fang, J., Wang, H., Liang, S., Tian, S., Zhang, J., Li, J., Li, Y., Huang, S., et al. UI-TARS: Pioneering Automated GUI Interaction with Native Agents. arXiv preprint arXiv:2501.12326, 2025. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Shen, Y., Song, K., Tan, X., Li, D., Lu, W., and Zhuang, Y. Hugginggpt: Solving AI Tasks with ChatGPT and its Friends in Hugging Face. Advances in Neural Information Processing Systems, 36:3815438180, 2023. Shenfeld, I., Pari, J., and Agrawal, P. RLs Razor: Why Online Reinforcement Learning Forgets Less. arXiv preprint arXiv:2509.04259, 2025. Shi, C., Yu, Z., Gao, Z., Feng, R., Liu, E., Wu, Y., Jia, Y., Xiang, L., He, Z., and Li, Q. GUI Knowledge Bench: Revealing the Knowledge Gap Behind VLM Failures in GUI Tasks. arXiv preprint arXiv:2510.26098, 2025. Tang, F., Gu, Z., Lu, Z., Liu, X., Shen, S., Meng, C., Wang, W., Zhang, W., Shen, Y., Lu, W., et al. GUI-G2: Gaussian Reward Modeling for GUI Grounding. arXiv preprint arXiv:2507.15846, 2025a. 10 Tang, F., Shen, Y., Zhang, H., Chen, S., Hou, G., Zhang, W., Zhang, W., Song, K., Lu, W., and Zhuang, Y. Think twice, Click Once: Enhancing GUI Grounding via Fast and Slow Systems. arXiv preprint arXiv:2503.06470, 2025b. Wang, J., Xu, H., Ye, J., Yan, M., Shen, W., Zhang, J., Huang, F., and Sang, J. Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception. In ICLR 2024 Workshop on Large Language Model (LLM) Agents, 2024. Wen, H., Li, Y., Liu, G., Zhao, S., Yu, T., Li, T. J.-J., Jiang, S., Liu, Y., Zhang, Y., and Liu, Y. Autodroid: LLMpowered Task Automation in Android. In Proceedings of the 30th Annual International Conference on Mobile Computing and Networking, pp. 543557, 2024. Wu, Q., Cheng, K., Yang, R., Zhang, C., Yang, J., Jiang, H., Mu, J., Peng, B., Qiao, B., Tan, R., et al. GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents. arXiv preprint arXiv:2506.03143, 2025. Wu, Z., Wu, Z., Xu, F., Wang, Y., Sun, Q., Jia, C., Cheng, K., Ding, Z., Chen, L., Liang, P. P., et al. Os-atlas: foundation action model for generalist gui agents. arXiv preprint arXiv:2410.23218, 2024. Ye, X., Li, Y., Dai, W., Liu, M., Chen, Z., Han, Z., Min, H., Ren, J., Zhang, X., Yang, W., et al. GUI-ARP: Enhancing Grounding with Adaptive Region Perception for GUI Agents. arXiv preprint arXiv:2509.15532, 2025. Yuan, X., Zhang, J., Li, K., Cai, Z., Yao, L., Chen, J., Wang, E., Hou, Q., Chen, J., Jiang, P.-T., et al. Enhancing Visual Grounding for GUI Agents via Self-Evolutionary Reinforcement Learning. arXiv preprint arXiv:2505.12370, 2025. Zhang, C., Li, L., He, S., Zhang, X., Rajmohan, S., et al. UFO: UI-Focused Agent for Windows OS Interaction. In Proceedings of the 2025 Conference of the Association for Computational Linguistics, pp. 597622, 2025a. Zhang, Z., Dong, Q., Zhang, Q., Zhao, J., Zhou, E., Xi, Z., Jin, S., Fan, X., Zhou, Y., Wu, M., et al. Why Reinforcement Fine-Tuning Enables MLLMs Preserve Prior Knowledge Better: Data Perspective. arXiv preprint arXiv:2506.23508, 2025b. Zhao, Y., Chen, W.-N., Inan, H. A., Kessler, S., Wang, L., Wutschitz, L., Yang, F., Zhang, C., Minervini, P., Rajmohan, S., et al. Learning GUI Grounding with Spatial Reasoning from Visual Feedback. arXiv preprint arXiv:2509.21552, 2025."
        },
        {
            "title": "Continual GUI Agents",
            "content": "Supplementary Material: Continual GUI Agents"
        },
        {
            "title": "Abstract",
            "content": "We provide four additional results in supplementrary material: (I) Training all the GUI datasets we leverage at once, investigating the upper boundary of GUI-AiF. (II) Exploring the reversing sequence of continual domain tasks. (III) Bias finding between Text and Icon Interaction. (IV) Providing more GUI visualization heatmaps. A. Upper Boundary of GUI-AiF Since the boundaries provided in main paper are choosen from the best results from references, which uses larger model size and more extensive datasets. We analyze our GUI-AiF boundary based on continual domain tasks datasets mentioned in main paper, and conduct evaluations in SSv1 and SSv2. The results is shown in Table 6 (represented by GUI-AiF). It shows the upper boundary is comparable to the performance achieved by completing all domain tasks. we consider it is attributed to chaos existing in SSv1 and SSv2 benchmarks, such as incorrect interface labelings, and it also verify the effectiveness of GUI-AiF in continual GUI agents. B. Reversed Continual Domain Tasks We then train domain tasks with the sequence of Web, Desktop and Mobile shown in Table 6 (represented by GUI-AiF). We can find the performance of continual learning tasks still aligns with the tendency in the main paper. C. Bias between Text and Icon Interaction. Beyond evaluations, our results generally demonstrate GUI agents bias towards text related interaction. From the Figure 7, it is evident that performance on text-based tasks is consistently higher than icon-based tasks, regardless of the domain or resolution. We consider this is due to fundamental bias in the agents visual branch, which defaults to its strong OCR capability for text comprehension, whereas it struggles with icons which are semantically ambiguous and lack this direct textual grounding. D. Visualizations of Continual GUI Agents We additionally provide some interaction region heatmaps of continual domains and resolutions, they demonstrate the changes in GUI agents perception of interaction interfaces based on instructions during the continual learning process. Table 6. Performance (%) of upper boundary (GUI-AiF) and reversed tasks (GUI-AiF) on ScreenSpot-V1 and ScreenSpot-V2. The M., D. and W. represents the Mobile, Desktop and Web platforms. Gray are results in main paper for reference. Bold represents the best result."
        },
        {
            "title": "Method",
            "content": "SSv1 Accuracy (%) SSv2 Accuracy (%)"
        },
        {
            "title": "Icon",
            "content": "Avg."
        },
        {
            "title": "Icon",
            "content": "Avg. GUI-AiF Upper Boundary 92.3 80.1 94.7 68. 83.3 67.4 81.1 97.1 83.2 92. 79.3 84.9 66.2 83.8 GUI-AiF GUI-AiF M. M.D. M.D.W. W. W.D. W.D.M. 92.9 94.1 96.1 94.9 95.6 89.7 78.7 77.3 76.9 74.7 69.4 72. 81.9 93.2 95.7 82.0 87.6 92.6 68.6 69.3 68.3 67.9 67.1 69.6 81.6 81.9 84.8 81.3 83.7 81. 65.1 66.2 68.2 64.1 64.8 69.5 78.1 80.3 81.7 77.5 78.1 79.2 93.3 95.9 96.6 95.2 95.9 89. 82.1 79.6 83.6 77.3 72.5 74.9 88.5 89.7 90.2 82.5 88.7 92.7 73.6 77.1 77.4 72.7 76.7 72. 80.3 81.2 82.9 81.2 77.8 86.8 68.0 68.3 70.5 64.0 69.6 72.2 80.9 81.9 83.5 78.8 80.2 81."
        },
        {
            "title": "Continual GUI Agents",
            "content": "(a) Bias on Domain. (b) Bias on Resolution. Figure 7. GUI performance bias between text and icon. (a) Original Vivado interface. (b) GUI-AiF trains on normal resolution. (c) GUI-AiF learns all resolutions. Figure 8. Visualizations of interaction region heatmaps on high resolution Vivado interface. Instruction used: Add sources in Vivado. (a) Original Desktop interface. (b) GUI-AiF trains on Mobile. (c) GUI-AiF trains on Desktop. (d) GUI-AiF learns all domains. Figure 9. Visualizations of interaction region heatmaps on desktop platforms. Instruction used: Find the Switch game console."
        },
        {
            "title": "Continual GUI Agents",
            "content": "(a) Original Web interface. (b) GUI-AiF trains on Mobile. (c) GUI-AiF trains on Desktop. (d) GUI-AiF learns all domains. Figure 10. Visualizations of interaction region heatmaps on web platforms. Instruction used: Search new game. (a) Original Mobile interface. (b) GUI-AiF trains on Mobile. (c) GUI-AiF trains on Desktop. (d) GUI-AiF learns all domains. Figure 11. Visualizations of interaction region heatmaps on mobile platforms. Instruction used: Play song."
        },
        {
            "title": "Continual GUI Agents",
            "content": "(a) Original Photoshop interface. (b) GUI-AiF trains on normal resolution. (c) GUI-AiF learns all resolutions. Figure 12. Visualizations of interaction region heatmaps on high resolution Photoshop interface. Instruction used: Select the color picker."
        }
    ],
    "affiliations": [
        "College of Computer Science, Zhejiang University, China",
        "Department of Computer Science and Technology, Tsinghua University, China",
        "Photogrammetry and Remote Sensing Lab, ETH Zurich"
    ]
}