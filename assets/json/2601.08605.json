{
    "paper_title": "ExpSeek: Self-Triggered Experience Seeking for Web Agents",
    "authors": [
        "Wenyuan Zhang",
        "Xinghua Zhang",
        "Haiyang Yu",
        "Shuaiyi Nie",
        "Bingli Wu",
        "Juwei Yue",
        "Tingwen Liu",
        "Yongbin Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Experience intervention in web agents emerges as a promising technical paradigm, enhancing agent interaction capabilities by providing valuable insights from accumulated experiences. However, existing methods predominantly inject experience passively as global context before task execution, struggling to adapt to dynamically changing contextual observations during agent-environment interaction. We propose ExpSeek, which shifts experience toward step-level proactive seeking: (1) estimating step-level entropy thresholds to determine intervention timing using the model's intrinsic signals; (2) designing step-level tailor-designed experience content. Experiments on Qwen3-8B and 32B models across four challenging web agent benchmarks demonstrate that ExpSeek achieves absolute improvements of 9.3% and 7.5%, respectively. Our experiments validate the feasibility and advantages of entropy as a self-triggering signal, reveal that even a 4B small-scale experience model can significantly boost the performance of larger agent models."
        },
        {
            "title": "Start",
            "content": "ExpSeek: Self-Triggered Experience Seeking for Web Agents Wenyuan Zhang1,2, Xinghua Zhang3, Haiyang Yu3, Shuaiyi Nie1,2, Bingli Wu3, Juwei Yue1,2, Tingwen Liu1,2, Yongbin Li3* 1Institute of Information Engineering, Chinese Academy of Sciences 2School of Cyber Security, University of Chinese Academy of Sciences 3Tongyi Lab , Alibaba Group {zhangwenyuan, liutingwen}@iie.ac.cn, {zhangxinghua.zxh, shuide.lyb}@alibaba-inc.com 6 2 0 2 3 1 ] . [ 1 5 0 6 8 0 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Experience intervention in web agents emerges as promising technical paradigm, enhancing agent interaction capabilities by providing valuable insights from accumulated experiences. However, existing methods predominantly inject experience passively as global context before task execution, struggling to adapt to dynamically changing contextual observations during agent-environment interaction. We propose ExpSeek, which shifts experience toward step-level proactive seeking: (1) estimating step-level entropy thresholds to determine intervention timing using the models intrinsic signals; (2) designing step-level tailor-designed experience content. Experiments on Qwen3-8B and 32B models across four challenging web agent benchmarks demonstrate that ExpSeek achieves absolute improvements of 9.3% and 7.5%, respectively. Our experiments validate the feasibility and advantages of entropy as self-triggering signal, reveal that even 4B small-scale experience model can significantly boost the performance of larger agent models."
        },
        {
            "title": "Introduction",
            "content": "Advances in large language models (LLMs) are gradually unlocking greater potential for agents (Team et al., 2025; Qu et al., 2025; Zhang et al., 2025b). Recently, web agents powered by search engines have gained considerable attention for their capability to retrieve relevant information from the web and address complex user queries (Ning et al., 2025; Song et al., 2025). Accordingly, the agent needs to possess the ability to conduct multi-turn interactions with the web to obtain evidence (Wei et al., 2025), and leverage it to provide answers to the users queries (Lu et al., 2024). However, the open web is noisy and partially observable with sparse useful evidence, posing the challenges to the agents reliability (Lee *corresponding authors. 1 Figure 1: Comparison of experience intervention frameworks. Panel shows the traditional global passive injection of experience, while we extend the framework to Panel B, where the agent proactively seeks guidance at each step based on its own signals. et al., 2025). Agents powered by LLMs, particularly small-scale, cost-effective models, often exhibit inefficient exploration in multi-turn interactions with the environment or respond prematurely, resulting in unreliable answers (Gao et al., 2025a). As demonstrated by previous studies (Zhao et al., 2024; Zheng et al., 2024), agents, like humans (Silver and Sutton, 2025), can learn efficiently from experience, drawing on information accumulated from historical interaction trajectories. Existing methods mainly follow two lines: offline refinement, which post-processes trajectories into reusable patterns and retrieves them at inference time (Kim et al., 2024; Gao et al., 2025b); and online self-evolution, which accumulates experience through iterative interaction and feedback (Wang et al., 2025d; Liu et al., 2025b; Zhang et al., 2025a). Despite their effectiveness, the experience is often passively injected as global context before the task execution, as shown in Figure 1 (A). However, during the agents interaction with the environment, the contextual observations continuously change. If the agent relies solely on the initial context without proactively acquiring and integrating fresh experience, its decision-making may become suboptimal or even misaligned with the current situation. Compared with passively injecting experience, why not empower the agent to proactively seek experience during its interaction with the environment for more precise guidance? This paper proposes ExpSeek, self-triggered experience-seeking framework that clarifies when to seek experiences and which ones to seek, as briefly depicted in Figure 1 (B): (1) The timing for seeking experience: Frequent or sparse experience seeking makes it difficult to achieve ideal results in terms of effectiveness and efficiency. An optimal timing for seeking experience is when the agent becomes confused and genuinely in need of guidance during interaction. To this end, we estimate promising threshold interval for each step based on entropy value through logistic regression and bootstrap resampling, and use the estimated interval to control the timing of experience seeking. (2) The concrete content of experience: After determining the timing for seeking experience, the next step is to decide its concrete content. Firstly, we formalize an experience base from pairs of successful and failed trajectories. Each triplet contains the erroneous behavior, mistake analysis, and corrective cues, and is grouped by topic. Subsequently, the experience model reads the history context of the current step, retrieves the related experience triplets from the experience base, and generates guidance tailored to the ongoing interaction. We evaluate ExpSeek on four challenging web agent benchmarks using agents powered by Qwen3-8B and Qwen3-32B. ExpSeek respectively achieves 9.3% and 7.5% absolute improvements over the 8B and 32B base models, and outperforms passive experience injection baselines by 6.7% and 6.0%, validating the effectiveness of proactive experience seeking during interaction with environment. Extensive analyses reveal some key insights: (1) The entropy of model itself is an effective trigger signal and can indicate whether it should receive experience guidance, controlling the timing for seeking experience. (2) Even with small-scale 4B model, the proactive experience seeking paradigm of ExpSeek can still yield significant performance gains for slightly larger 32B model. (3) The self-triggered experience seeking increases entropy of the model during the intermediate steps of interaction with the environment, while decreasing entropy at the final answer step, confirming the enhanced exploration and ultimately enabling more effective convergence toward the correct answer. In summary, our contributions are as follows: We propose ExpSeek, self-triggered experience seeking framework, inspiring proactive paradigm for seeking experience distinct from passive experience injection. We explore and confirm that the entropy of model itself can serve as an intrinsic signal of the timing for proactively seeking experience. Additionally, we build the experience base with experience triplets, and an experience model is designed to dynamically generate experience guidance during agentenvironment interactions, based on experience triplets and historical context. Extensive experiments and analyses show the significant advantages of ExpSeek with average improvements of 9.3% and 7.5% on 8B/32B models, an increase of up to 14.6%."
        },
        {
            "title": "2.1 Experience Intervenes in Agents",
            "content": "Experience (Silver and Sutton, 2025) serves as long-term memory to prevent repeated mistakes and accumulate insights, distinct from short-term contextual memory (Hu et al., 2025). Recent work can be categorized into two lines: (1) Offline Experience refines experience from offline training sets. Early methods directly use raw trajectories (Zheng et al., 2024), while later work structures experience (Zhao et al., 2024; Kim et al., 2024; Fang et al., 2025; Kirtania et al., 2025) and induces patterns from successful and failed trajectories (Cai et al., 2025a) to extract transferable (Gao et al., 2025b; Tang et al., 2025) and reusable reasoning units. (2) Self-Evolution accumulates domain-specific experience online through gradient-free training (Cai et al., 2025b) or by shifting the models output distribution (Luo et al., 2025), with real-time updates (Wang et al., 2025d; Liu et al., 2025b; Yang et al., 2025; Cao et al., 2025; Zhang et al., 2025a; Ouyang et al., 2025; Cui et al., 2025). However, such passive experience injection is difficult to align with step decisions."
        },
        {
            "title": "2.2 Entropy in LLM Reasoning",
            "content": "Entropy serves as fundamental uncertainty metric widely used for static reasoning evaluation, includ2 ing answer selection (Ren et al., 2023; Raj et al., 2025), confidence calibration (Chen and Mueller, 2024), and error detection (Farquhar et al., 2024; Liu et al., 2025a). As reasoning scales to multistep paradigms where responses are decomposed into atomic steps (Guo et al., 2025), the role of entropy extends to training. Particularly in reinforcement learning, entropy not only reflects sampling diversity to facilitate exploration (Wang et al., 2025c; Zheng et al., 2025), but also acts as finegrained signal for step-level credit assignment in long-horizon reasoning (Wang et al., 2025b,a), and further extends to incentivize exploration across multi-turn interactions (Dong et al., 2025). This demonstrates the potential of entropy as selftrigger signal for web agents."
        },
        {
            "title": "3 Preliminaries",
            "content": "Agent Framework. Following Wu et al. (2025b), we construct web agents based on the ReAct framework (Yao et al., 2023), modeling problem-solving as an interleaved sequence of reasoning and acting. trajectory τ is defined as: τ = (q, R1, O1, . . . , Rt, Ot, . . . , RT ), (1) where is the query, Rt is the agents response at step t, and Ot is the environment observation. We distinguish two types of steps: Process Step (Sp = (Rt, Ot), < ): The response Rt = zt, at contains reasoning thoughts zt (enclosed in <think> tags) and an action at (in <tool_call> tags), followed by the tools response Ot. Answer Step (Sa = RT ): The terminal step produces RT = zT , y, where the final answer is wrapped in <answer> tags. Step Entropy. To quantify the agents confidence in each step of interaction, we compute the step entropy as the average token entropy across each response Rt. Specifically, the entropy of the i-th token xi with preceding context hi is defined as H(xi) = (cid:80) vV (v hi) log (v hi), where is the vocabulary and ( hi) is the models predicted distribution. Step entropy is computed: Ht = 1 Rt (cid:88) xRt H(x), (2) where Rt denotes the number of tokens in Rt. Experience Intervention. We formally define experience intervention as class of methods comprising two phases: 3 Construction Phase: Given training corpus Dtrain = {(qi, τi, yi)}N i=1 with queries qi, trajectories τi, and ground truth yi, experience acquisition is formalized as = F(Dtrain), where is function that extracts an experience base from Dtrain 1. Utilization Phase: At step with context ht = (q, . . . , Rt, Ot), the agent obtains applicable experience et = G(E, ht) through mapping function G, which serves as additional input for reasoning. Traditional methods represent special case where experience is concatenated at the beginning of reasoning, with = G(E, q). The function typically returns the entire base or retrieves relevant cases, providing static experience. In contrast, our method invokes G(E, ht) at any step to allow the agent to seek appropriate guidance."
        },
        {
            "title": "4 Methodology",
            "content": "This section formally introduces ExpSeek, elaborating on three key components: experience base construction ( 4.1), self-triggering mechanism ( 4.2), and step-level guidance ( 4.3)."
        },
        {
            "title": "4.1 Experience Base Construction",
            "content": "The core of guiding experience lies in recreating the problem behavior and simulating guidance. Guiding Experience Schema. We design experience triplets containing: (1) Behavior: objectively describes the state and action at the current step; (2) Mistake: identifies errors by contrasting with correct trajectories; (3) Guidance: provides directional guidance based on error analysis, without directly offering answers or specific clues. Construction Process. As shown in Figure 2 (A), construction involves three steps. First, for each query in Dtrain, we sample trajectories with the agent model and pair successful and failed ones. Second, for each trajectory pair (τ +, τ ), we use tool model to analyze the failed trajectory τ = {S1, S2, . . . , ST } by contrasting with the successful one τ +. The tool model assigns each step St binary correctness label yt {0, 1} and outputs triplets for incorrect steps (where yt = 0). Finally, we prompt the tool model to induce topics for triplets using an iterative batch processing approach: when processing each new batch, the model takes all previously generated triplets with their assigned topics as input, then either assigns 1Self-evolution methods have the same Dtrain and Dtest, but typically do not provide ground truth y. Figure 2: The overall architecture of ExpSeek, including experience base construction and actively seeking experience guidance during inference. The step entropy threshold calculation process is not depicted here. existing topics, modifies them, or creates new topics for the current batch. This yields guiding experience base organized into topics, with separate collections Ep and Ea for process and answer steps respectively."
        },
        {
            "title": "4.2.1 Entropy Analysis for Step Correctness",
            "content": "Determining when to trigger intervention is challenging. Using reward models to analyze each step incurs prohibitive costs. Inspired by prior research on entropy-based error detection and confidence calibration (Chen and Mueller, 2024; Liu et al., 2025a), we hypothesize that the step entropy of web agents has the potential to reflect the agents inherent state. We focus on (1) whether entropy can distinguish correct from incorrect steps in web agent reasoning, and (2) whether this distinguishability differs between process and answer steps. We analyze training trajectories from 4.1. Based on correctness labels yt assigned during trajectory pairing, we construct two step collections: + = {St τ +} {St τ yt = 1}, = {St τ yt = 0}, (3) where + aggregates all steps from successful trajectories and correctly-labeled steps from incorrect trajectories, while comprises only the incorrect steps from failed trajectories. We further partition these collections by step type into process steps (S + ) and answer steps (S + ), then analyze their entropy distributions Ht. , , Figure 3 shows that + consistently exhibits lower entropy than for both step types. The Kolmogorov-Smirnov test (Berger and Zhou, 2014) 4 confirms that the distributions of correct and incorrect steps are statistically separable for both types (process steps: KS=0.1998, p<0.001; answer steps: KS=0.3809, p<0.001). However, separability differs substantially in practice: process steps show considerable overlap as agents naturally explore multiple paths, producing high entropy even when correct (AUC=0.6223, indicating weak discrimination (Bradley, 1997)), while answer steps demonstrate much clearer separation (AUC=0.7187, indicating acceptable discrimination). This suggests that entropy serves as noisy but valid signal for process steps, and becomes more reliable for answer steps, motivating us to estimate thresholds for triggering intervention."
        },
        {
            "title": "4.2.2 Threshold-Based Triggering",
            "content": "Threshold Estimation via Bootstrap. To quantify when intervention should occur, we estimate threshold intervals that capture uncertainty in distinguishing correct from incorrect steps. We formulate this as binary classification: given step entropy Ht, predict correctness yt {0, 1}. We fit separate logistic regression models for , ), ) and answer steps (S + process steps (S + where each model learns: , (yt = 0 Ht) = 1 1 + e(w Ht+b) , (4) modeling the probability of incorrectness, where higher entropy corresponds to higher error probability. The decision boundary at = 0.5 yields threshold θ = b/w. Since single fitted model on limited data may be unstable, we employ bootstrap resampling: we sample with replacement from the respective step Model Qwen3-8B Qwen3-32B θp low 0.314 0.877 θp high 0.413 1.384 θa low 0.225 0.714 θa high 0.257 0.820 Table 1: Threshold intervals obtained through bootstrap resampling for process and answer steps. We trigger experience guidance ( 4.3) with probability pintervene, where low-entropy steps receive no intervention, high-entropy steps always receive guidance (low confidence), and intermediate cases are handled probabilistically to balance intervention frequency with agent autonomy."
        },
        {
            "title": "4.3 Guided Intervention at Inference",
            "content": "At inference, we implement G(E, ht) through experience model Me that dynamically generates contextualized interventions. The process is illustrated in Figure 2 (B). For each step St, we first compute Ht and sample from pintervene ( 4.2). If triggered and the previous step was not intervened, Me selects the three most relevant topics from Ep (for process steps) or Ea (for answer steps) based on context ht, then adapts experiences under selected topics to generate guidance et for the current situation. The generated guidance is injected differently by step type. For process steps Sp , et is appended to Ot. For answer steps Sa , we extend it to {RT , OT } with et as OT , enabling the agent to continue at step + 1 (either refining the answer or invoking tools for further reasoning). To prevent over-intervention, we disable intervention at step + 1 after any intervention at step t, allowing the agent to incorporate guidance before receiving further intervention."
        },
        {
            "title": "5.1 Setup",
            "content": "Benchmarks and Metrics. We evaluate on four challenging real-world web agent reasoning benchmarks: GAIA (Mialon et al., 2023), WebWalkerQA (Wu et al., 2025b), xbenchDeepSearch (Chen et al., 2025), and SEALHARD (Pham et al., 2025). We construct our training set by sampling 25% from each difficulty level (easy, medium, hard) in WebWalkerQA with random seed 0, yielding 170 examples, with the remainder used as the test set. We employ the LLMas-a-Judge approach for evaluation and report average accuracy across five independent runs. Figure 3: Entropy distributions of process and answer steps on Dtrain for Qwen3-8B, with fitted logistic regression curves. Green zone indicates no intervention during inference, red indicates intervention, and yellow indicates probabilistic intervention. collections (preserving original sizes) times, fit logistic regression on each sample to obtain {θ(i)}N i=1, and compute the 95% confidence interval as our threshold range: θlower = Q0.025({θ(i)}), θupper = Q0.975({θ(i)}), (5) lower, θp where Qp denotes the p-th quantile. This yields separate threshold intervals [θp upper] for process steps and [θa lower, θa upper] for answer steps. More implementation details are provided in Appendix A.1. Probabilistic Intervention at Inference. During inference, for each step St, we compute its entropy Ht and determine the intervention probability based on the corresponding threshold interval (using [θp lower, θp upper] for process steps or [θa upper] for answer steps): lower, θa pintervene = 0 Htθlower θupperθlower 1 Ht < θlower θlower Ht θupper Ht > θupper (6) 5 Method WebWalkerQA Easy Medium Hard Avg. GAIA SEAL xbench Avg. No Experience 42.670.0 Training-Free GRPO 40.662.0 44.501.8 REASONINGBANK 51.008.3 ExpSeek (Ours) 44.171.5 - only process step 49.506.8 - only answer step + 46.670.0 No Experience Training-Free GRPO 46.920.3 47.500.8 REASONINGBANK 54.337.7 ExpSeek (Ours) 49.172.5 - only process step 50.834.2 - only answer step + 40.190.0 42.862.7 41.521.3 49.529.3 47.917.7 48.488.3 47.240.0 46.850.4 47.620.4 53.055.8 47.240.0 48.571.3 Qwen3-8B 32.560.0 38.335.8 36.333.8 44.2211.7 38.896.3 41.228.7 38.470.0 40.622.2 40.782.3 48.259.8 43.665.2 46.407.9 Qwen3-32B 41.110.0 41.670.6 41.670.6 45.894.8 45.564.5 42.781.7 45.010.0 45.150.1 45.600.6 51.096.1 47.322.3 47.392.4 29.130.0 29.320.2 32.042.9 36.897.8 33.013.9 30.291. 36.500.0 36.890.4 33.013.5 43.887.4 40.584.1 37.110.6 23.230.0 25.592.4 26.383.2 30.166.9 27.173.9 30.086.9 27.800.0 28.190.4 29.842.0 32.765.0 31.503.7 32.524.7 25.600.0 26.000.4 28.002.4 37.2011.6 28.402.8 34.809.2 27.400.0 28.200.8 36.338.9 42.0014.6 30.803.4 35.408.0 32.230.0 33.791.6 34.802.6 41.509.3 36.594.4 39.066. 37.790.0 38.120.3 39.331.5 45.327.5 40.813.0 41.203.4 Table 2: Main results of ExpSeek on four benchmarks using two backbone agents. We report mean accuracy (%) over five independent runs and absolute improvements over vanilla ReAct without experience. We also provide full ablation results for guiding only process or answer steps. Tool Environment. Agents are equipped with two fundamental tools: (1) Search, which queries search engine to return relevant URLs with snippets; and (2) Visit, which accesses specific URL to retrieve its content. Configuration. We set the maximum number of ReAct steps to 30, treating episodes exceeding this limit as failures. We use Qwen3-8B and Qwen3-32B as agents with sampling temperature of 1.0 and top-p of 0.95. For tool model and Me, we employ Qwen3-235B-A22B-Instruct-2507 in the main experiments. During experience construction, trajectories are sampled five times, and bootstrap sampling uses = 1000. The constructed experience repositories contain: for 8B, Ep = 196 (17 topics) and Ea = 190 (11 topics); for 32B, Ep = 276 (18 topics) and Ea = 143 (23 topics). The threshold intervals derived from bootstrap resampling are shown in Table 1. Baseline. We select two typical experience-based methods as baselines. Training-Free GRPO (Cai et al., 2025a) uses semantic advantages to iteratively acquire and update high-quality experiences from offline trajectories, globally leveraging the experience repository at test time. REASONINGBANK (Ouyang et al., 2025) is self-evolving experience acquisition scheme that accumulates experiences from online tasks and retrieves them from continuously updated experience repository in the system prompt during subsequent reasoning. We implement an enhanced version REASONINGBANK+ using 235B instead of weaker reasoning agent to generate experiences. All experimental and setup details are provided in Appendix C, D."
        },
        {
            "title": "5.2 Main Results",
            "content": "Table 2 reports the main experimental results. The key findings are as follows: (1) ExpSeek demonstrates significantly superior performance. ExpSeek achieves average absolute improvements of 9.3% and 7.5% over vanilla ReAct on Qwen3-8B and 32B respectively, substantially outperforming baselines across all benchmarks and highlighting the advantages of step-level experience guidance. (2) Global intervention methods show limited effectiveness. Both baseline types struggle with challenging open web reasoning tasks, showing improvements under 3% or even performance degradation. This indicates that carefully designed global experience injection struggles to adapt to noisy web environments while adding reasoning burden to smaller agents. (3) Strong cross-task generalization capability. Despite being derived entirely from WebWalkerQA training set, ExpSeek maintains robust performance across three out-of-distribution benchmarks. We also report pass@3 performance, demonstrating ExpSeeks competitive sampling diversity. Additionally, we provide extensive case studies to intuitively illustrate the effectiveness of our method. Supplementary materials and further discussions can be found in Appendix B."
        },
        {
            "title": "5.3 Ablation Study",
            "content": "To validate the individual effectiveness of guiding process and answer steps, we report complete ablation results in Table 2. Guiding only process or answer steps fails to surpass full intervention, 6 Figure 4: Entropy distributions of process and answer steps for Qwen3-8B before and after applying ExpSeek across all benchmarks. Results for Qwen3-32B are provided in Figure 9. Method Acc. Rej. Step Time No Experience Rule + Me Claude-4 + Me Entropy + Me Entropy + emb No Experience Rule + Me Claude-4 + Me Entropy + Me Entropy + emb Qwen3-8B GAIA 29.13 38.81 39.47 36.89 30.92 - 0.0 7.4 67.8 74.2 Qwen3-8B xbench 25.60 36.40 36.60 37.20 27.80 - 0.0 4.8 42.2 44. 4.47 9.52 8.55 5.75 5.54 3.31 8.52 7.40 5.73 4.69 66.94s 329.71s 370.82s 127.57s 110.61s 51.06s 300.09s 315.39s 143.81s 88.17s Table 3: Performance and efficiency with different triggers and retrieval-based guidance in GAIA and xbench. Rej: the proportion of trigger checks that result in nonintervention; Step & Time: average per question. showing significant performance drops on both 8B (-2.44%, -4.91%) and 32B (-4.12%, -4.51%). Notably, guiding only answer steps achieves performance closer to the full method, which validates our observation in 4.2 that answer steps exhibit stronger distributional distinguishability."
        },
        {
            "title": "6 Experimental Analyses",
            "content": "In this section, we conduct an in-depth analysis of ExpSeek across four dimensions: internal mechanisms, design rationale, scalability, and efficiency."
        },
        {
            "title": "6.1 How Does ExpSeek Work Internally?",
            "content": "To reveal how ExpSeek works internally, we visualize the entropy distribution shifts of agent outputs before and after experience guidance (Figure 4). The results show that guidance increases entropy in process steps, enabling the agent to escape local decisions and explore broader reasoning spaces; conversely, the entropy distribution of answer steps forms left-skewed peak, indicating the Figure 5: Scaling Law of experience model Me. Method GAIA xbench Qwen3-8B E-8B / 32B 36.89 / 35.60 Qwen3-32B E-32B / 8B 43.88 / 40. 37.20 / 36.00 42.00 / 37.20 Table 4: Experience repository swapping: E-8B/32B denote dedicated repositories built for each agent. agent converges to correct answers with higher confidence after sufficient exploration. This divergethen-converge behavior balances exploration and exploitation in complex reasoning."
        },
        {
            "title": "6.2 Why Self-Trigger and Generation?",
            "content": "To validate entropy-based self-triggering, we compare two variants: Reward Model-based2 (RM) and rule-based (continuous intervention from step one), both with one-step post-trigger silence. Table 3 shows similar accuracy but divergent efficiency. Rule-based triggers result in 1.7 step and 2.6 time overhead on GAIA (1.5 and 2.1 on xbench). RM reduces step overhead (1.3-1.5) but increases time (2.2-2.9), suggesting overintervention. In contrast, self-trigger balances efficiency and performance by adapting intervention 2We employ claude-sonnet-4-20250514 to judge intervention necessity at each step based on the full history. 7 Figure 6: Cross-comparison results of performance and efficiency after adjusting intervention intensity. edge it contains still holds transfer value. Furthermore, Figure 7 shows we reduce experiences per topic. Even with just one experience per topic, Me still identifies key intervention points and maintains high accuracy, demonstrating that the experience model can understand and generalize to current reasoning even with only few highquality seed experiences, highlighting the necessity of experience topics. While removing the repository entirely degrades performance, the remaining substantial accuracy indicates that the Mes inherent world experience alone is beneficial."
        },
        {
            "title": "6.4 What Is the Efficiency Trade-off?",
            "content": "Figure 6 illustrates the relationship between accuracy and reasoning steps under varying intervention frequencies and thresholds. We expand the intervention interval from 1 to {0, 1, 2} and shift the threshold by 0.05 three times, yielding 21 configurations. Results show that as the trigger threshold decreases, reasoning steps increase rapidly while accuracy rises then plateaus with diminishing returns. With 2 interventions, accuracy reaches 43.01%; beyond 6 interventions, performance barely improves, indicating that while increasing intervention intensity does not degrade performance, it also fails to yield higher gains. The results also demonstrate that in practice, the web agent produces stable performance even when thresholds fluctuate at small scale."
        },
        {
            "title": "7 Conclusion",
            "content": "We propose ExpSeek, framework enabling web agents to actively seek step-level guidance based on step entropy. Even with small open-source agents, ExpSeek demonstrates significant performance improvements and excellent properties on complex real-world web reasoning tasks, showing great potential for future development. Figure 7: Correlation between repository size and performance on Qwen3-8B. intensity to problem difficulty. On the more challenging xbench, its trigger rate increases 25.6% compared to GAIA while maintaining similar accuracy, confirming its ability to precisely identify intervention timing based on internal state. We also explore retrieval-based guidance, similar experience (via selecting the most text-embedding-v43) instead of generation. Table 3 shows that retrieval increases time without improving accuracy, confirming the necessity of generative guidance."
        },
        {
            "title": "6.3 Does ExpSeek Scale and Transfer?\nWe experiment with smaller 4B and 30B models4\nas Me to explore the scaling of intervention mod-\nels. As shown in Figure 5, the three model sizes\nexhibit scaling law on both GAIA and xbench, all\nachieving substantial performance gains. Notably,\nthe 4B guidance model improves the 32B agent\nby 5.2% and 9.7% points respectively, validating\nthe feasibility of weak-to-strong guidance given\nreasonable experience. Additionally, we swap the\nexperience pools between 8B and 32B Agents. Re-\nsults in Table 4 show that while experience has\nmodel dependency, the abstract guidance knowl-",
            "content": "3https://www.alibabacloud.com/help/en/modelstudio/embedding 4Qwen3-4B-Instruct and Qwen3-30B-A3B-Instruct"
        },
        {
            "title": "Limitations",
            "content": "Although ExpSeek achieves significant performance advantages, it still has the following limitations, which we consider as focuses for future research: (1) While ExpSeek validates the feasibility of step entropy as self-trigger, the current threshold estimation relies on the training set and the tool models assessment of step quality. More accurate threshold calculation strategies need to be investigated. (2) It remains unexplored whether ExpSeek has the potential to extend to other nonweb domains and integrate more tools. (3) Since ExpSeek can also significantly improve pass@k performance, it has not yet been studied whether it can serve as an enhancement technique for Agentic Reinforcement Learning rollout to improve training convergence speed and sampling quality."
        },
        {
            "title": "Ethical Considerations",
            "content": "Our method is intended for academic research only and does not support applications involving risks, religion, racial discrimination, or ethical violations."
        },
        {
            "title": "References",
            "content": "Vance Berger and YanYan Zhou. 2014. Kolmogorov smirnov test: Overview. Wiley statsref: Statistics reference online. Andrew Bradley. 1997. The use of the area under the roc curve in the evaluation of machine learning algorithms. Pattern recognition, 30(7):11451159. Yuzheng Cai, Siqi Cai, Yuchen Shi, Zihan Xu, Lichao Chen, Yulei Qin, Xiaoyu Tan, Gang Li, Zongyi Li, Haojia Lin, Yong Mao, Ke Li, and Xing Sun. 2025a. Training-free group relative policy optimization. Preprint, arXiv:2510.08191. Zhicheng Cai, Xinyuan Guo, Yu Pei, Jiangtao Feng, Jinsong Su, Jiangjie Chen, Ya-Qin Zhang, Wei-Ying Ma, Mingxuan Wang, and Hao Zhou. 2025b. Flex: Continuous agent evolution via forward learning from experience. Preprint, arXiv:2511.06449. Zouying Cao, Jiaji Deng, Li Yu, Weikang Zhou, Zhaoyang Liu, Bolin Ding, and Hai Zhao. 2025. Remember me, refine me: dynamic procedural memory framework for experience-driven agent evolution. Preprint, arXiv:2512.10696. Jiuhai Chen and Jonas Mueller. 2024. Quantifying uncertainty in answers from any language model and enhancing their trustworthiness. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 51865200, Bangkok, Thailand. Association for Computational Linguistics. Kaiyuan Chen, Yixin Ren, Yang Liu, Xiaobo Hu, Haotong Tian, Tianbao Xie, Fangfu Liu, Haoye Zhang, Hongzhang Liu, Yuan Gong, Chen Sun, Han Hou, Hui Yang, James Pan, Jianan Lou, Jiayi Mao, Jizheng Liu, Jinpeng Li, Kangyi Liu, and 14 others. 2025. xbench: Tracking agents productivity scaling with profession-aligned real-world evaluations. Preprint, arXiv:2506.13651. Sijia Cui, Aiyao He, Shuai Xu, Hongming Zhang, Yanna Wang, Qingyang Zhang, Yajing Wang, and Bo Xu. 2025. Self-guided function calling in large language models via stepwise experience recall. In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 1084210854, Suzhou, China. Association for Computational Linguistics. Guanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia Chen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, Guorui Zhou, Yutao Zhu, Ji-Rong Wen, and Zhicheng Dou. 2025. Agentic reinforced policy optimization. Preprint, arXiv:2507.19849. Runnan Fang, Yuan Liang, Xiaobin Wang, Jialong Wu, Shuofei Qiao, Pengjun Xie, Fei Huang, Huajun Chen, and Ningyu Zhang. 2025. Memp: Exploring agent procedural memory. Preprint, arXiv:2508.06433. Sebastian Farquhar, Jannik Kossen, Lorenz Kuhn, and Yarin Gal. 2024. Detecting hallucinations in large language models using semantic entropy. Nature, 630(8017):625630. Jiaxuan Gao, Wei Fu, Minyang Xie, Shusheng Xu, Chuyi He, Zhiyu Mei, Banghua Zhu, and Yi Wu. 2025a. Beyond ten turns: Unlocking long-horizon agentic search with large-scale asynchronous RL. In First Workshop on Multi-Turn Interactions in Large Language Models. Jinglong Gao, Xiao Ding, Lingxiao Zou, Bibo Cai, Bing Qin, and Ting Liu. 2025b. ExpeTrans: LLMs are experiential transfer learners. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1057710616, Vienna, Austria. Association for Computational Linguistics. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, and 1 others. 2025. Deepseekr1 incentivizes reasoning in llms through reinforcement learning. Nature, 645(8081):633638. Yuyang Hu, Shichun Liu, Yanwei Yue, Guibin Zhang, Boyang Liu, Fangyi Zhu, Jiahang Lin, Honglin Guo, Shihan Dou, Zhiheng Xi, Senjie Jin, Jiejun Tan, Yanbin Yin, Jiongnan Liu, Zeyu Zhang, Zhongxiang Sun, Yutao Zhu, Hao Sun, Boci Peng, and 28 others. 2025. Memory in the age of ai agents. Preprint, arXiv:2512.13564. 9 Minsoo Kim, Victor Bursztyn, Eunyee Koh, Shunan Guo, and Seung-won Hwang. 2024. RaDA: Retrieval-augmented web agent planning with LLMs. In Findings of the Association for Computational Linguistics: ACL 2024, pages 1351113525, Bangkok, Thailand. Association for Computational Linguistics. Shashank Kirtania, Param Biyani, Priyanshu Gupta, Yasharth Bajpai, Roshni Iyer, Sumit Gulwani, and Gustavo Soares. 2025. Improving language agents through brew. Preprint, arXiv:2511.20297. Dongjun Lee, Juyong Lee, Kyuyoung Kim, Jihoon Tack, Jinwoo Shin, Yee Whye Teh, and Kimin Lee. 2025. Learning to contextualize web pages for enhanced decision making by llm agents. In The Thirteenth International Conference on Learning Representations. Xiaoou Liu, Tiejin Chen, Longchao Da, Chacha Chen, Zhen Lin, and Hua Wei. 2025a. Uncertainty quantification and confidence calibration in large language models: survey. In Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V. 2, pages 61076117. Yitao Liu, Chenglei Si, Karthik Narasimhan, and Shunyu Yao. 2025b. Contextual experience replay for self-improvement of language agents. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1417914198, Vienna, Austria. Association for Computational Linguistics. Xing Han Lu, Zdenˇek Kasner, and Siva Reddy. 2024. Weblinx: Real-world website navigation with multiturn dialogue. In International Conference on Machine Learning, pages 3300733056. PMLR. Haohao Luo, Jiayi Kuang, Wei Liu, Ying Shen, Jian Luan, and Yang Deng. 2025. Browsing like human: multimodal web agent with experiential fastand-slow thinking. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 14232 14251, Vienna, Austria. Association for Computational Linguistics. Grégoire Mialon, Clémentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. 2023. Gaia: benchmark for general ai assistants. In The Twelfth International Conference on Learning Representations. Liangbo Ning, Ziran Liang, Zhuohang Jiang, Haohao Qu, Yujuan Ding, Wenqi Fan, Xiao-yong Wei, Shanru Lin, Hui Liu, Philip S. Yu, and Qing Li. 2025. survey of webagents: Towards next-generation ai agents for web automation with large foundation models. In Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2, KDD 25, page 61406150, New York, NY, USA. Association for Computing Machinery. Siru Ouyang, Jun Yan, I-Hung Hsu, Yanfei Chen, Ke Jiang, Zifeng Wang, Rujun Han, Long T. Le, Samira Daruki, Xiangru Tang, Vishy Tirumalashetty, George Lee, Mahsan Rofouei, Hangfei Lin, Jiawei Han, Chen-Yu Lee, and Tomas Pfister. 2025. Reasoningbank: Scaling agent self-evolving with reasoning memory. Preprint, arXiv:2509.25140. Thinh Pham, Nguyen Nguyen, Pratibha Zunjare, and Tu Vu. Weiyuan Chen, Yu-Min Tseng, 2025. Sealqa: Raising the bar for reasoning in search-augmented language models. Preprint, arXiv:2506.01062. Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, and Ji-rong Wen. 2025. Tool learning with large language models: survey. Frontiers of Computer Science, 19(8). Harsh Raj, Vipul Gupta, Domenic Rosati, and Subhabrata Majumdar. 2025. Improving consistency in large language models through chain of guidance. Transactions on Machine Learning Research. Jie Ren, Yao Zhao, Tu Vu, Peter J. Liu, and Balaji Lakshminarayanan. 2023. Self-evaluation improves selective generation in large language models. In Proceedings on \"I Cant Believe Its Not Better: Failure Modes in the Age of Foundation Models\" at NeurIPS 2023 Workshops, volume 239 of Proceedings of Machine Learning Research, pages 4964. PMLR. David Silver and Richard Sutton. 2025. Welcome to the era of experience. Google AI, 1. Yueqi Song, Frank F. Xu, Shuyan Zhou, and Graham Neubig. 2025. Beyond browsing: API-based web agents. In Findings of the Association for Computational Linguistics: ACL 2025, pages 1106611085, Vienna, Austria. Association for Computational Linguistics. Xiangru Tang, Tianrui Qin, Tianhao Peng, Ziyang Zhou, Daniel Shao, Tingting Du, Xinming Wei, He Zhu, Ge Zhang, Jiaheng Liu, Xingyao Wang, Sirui Hong, Chenglin Wu, and Wangchunshu Zhou. 2025. AGENT KB: hierarchical memory framework for cross-domain agentic problem solving. In ICML 2025 Workshop on Collaborative and Federated Agentic Workflows. Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, and 1 others. 2025. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534. Huaijie Wang, Shibo Hao, Hanze Dong, Shenao Zhang, Yilin Bao, Ziran Yang, and Yi Wu. 2025a. Offline reinforcement learning for LLM multi-step reasoning. In Findings of the Association for Computational Linguistics: ACL 2025, pages 88818893, Vienna, Austria. Association for Computational Linguistics. Jiawei Wang, Jiacai Liu, Yuqian Fu, Yingru Li, Xintao Wang, Yuan Lin, Yu Yue, Lin Zhang, Yang Wang, and Ke Wang. 2025b. Harnessing uncertainty: Entropy-modulated policy gradients for long-horizon llm agents. Preprint, arXiv:2509.09265. 10 Wen. 2025b. survey on the memory mechanism of large language model-based agents. ACM Transactions on Information Systems, 43(6):147. Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. 2024. Expel: Llm agents are experiential learners. Proceedings of the AAAI Conference on Artificial Intelligence, 38(17):1963219642. Longtao Zheng, Rundong Wang, Xinrun Wang, and Bo An. 2024. Synapse: Trajectory-as-exemplar prompting with memory for computer control. In The Twelfth International Conference on Learning Representations. Tianyu Zheng, Tianshun Xing, Qingshui Gu, Taoran Liang, Xingwei Qu, Xin Zhou, Yizhi Li, Zhoufutu Wen, Chenghua Lin, Wenhao Huang, Qian Liu, Ge Zhang, and Zejun Ma. 2025. First return, entropyeliciting explore. Preprint, arXiv:2507.07017. Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xiong-Hui Chen, Jianxin Yang, Zhenru Zhang, Yuqiong Liu, An Yang, Andrew Zhao, Yang Yue, Shiji Song, Bowen Yu, Gao Huang, and Junyang Lin. 2025c. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for LLM reasoning. In The Thirty-ninth Annual Conference on Neural Information Processing Systems. Zora Zhiruo Wang, Jiayuan Mao, Daniel Fried, and Graham Neubig. 2025d. Agent workflow memory. In Forty-second International Conference on Machine Learning. Zhepei Wei, Wenlin Yao, Yao Liu, Weizhi Zhang, Qin Lu, Liang Qiu, Changlong Yu, Puyang Xu, Chao Zhang, Bing Yin, Hyokun Yun, and Lihong Li. 2025. WebAgent-r1: Training web agents via end-to-end multi-turn reinforcement learning. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 79097928, Suzhou, China. Association for Computational Linguistics. Jialong Wu, Baixuan Li, Runnan Fang, Wenbiao Yin, Liwen Zhang, Zhenglin Wang, Zhengwei Tao, DingChu Zhang, Zekun Xi, Xiangru Tang, Yong Jiang, Pengjun Xie, Fei Huang, and Jingren Zhou. 2025a. Webdancer: Towards autonomous information seeking agency. In The Thirty-ninth Annual Conference on Neural Information Processing Systems. Jialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang, Zekun Xi, Runnan Fang, Linhai Zhang, Yulan He, Deyu Zhou, Pengjun Xie, and Fei Huang. 2025b. WebWalker: Benchmarking LLMs in web traversal. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1029010305, Vienna, Austria. Association for Computational Linguistics. Cheng Yang, Xuemeng Yang, Licheng Wen, Daocheng Fu, Jianbiao Mei, Rong Wu, Pinlong Cai, Yufan Shen, Nianchen Deng, Botian Shi, Yu Qiao, and Haifeng Li. 2025. Learning on the job: An experience-driven self-evolving agent for long-horizon tasks. Preprint, arXiv:2510.08002. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations. Ding-Chu Zhang, Yida Zhao, Jialong Wu, Liwen Zhang, Baixuan Li, Wenbiao Yin, Yong Jiang, Yu-Feng Li, Kewei Tu, Pengjun Xie, and Fei Huang. 2025a. EvolveSearch: An iterative self-evolving search agent. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 1313413147, Suzhou, China. Association for Computational Linguistics. Zeyu Zhang, Quanyu Dai, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Jieming Zhu, Zhenhua Dong, and Ji-Rong"
        },
        {
            "title": "A Details of Method",
            "content": "A.1 Details of Threshold Estimation We implement the threshold estimation algorithm using the LogisticRegression class from scikit-learn with default hyperparameters, which is well-suited for our single-feature binary classification task (correct vs. incorrect steps based on entropy Ht). Bootstrap Resampling. Sampling with replacement (Lines 4-5 of Algorithm 1) generates bootstrap datasets where some original samples appear multiple times while others are omitted. Across = 1000 iterations, this produces distribution of decision boundaries Θ = {θ(1), . . . , θ(N )}, enabling uncertainty quantification via confidence intervals. Decision Boundary Interpretation. Each logistic regression model learns parameters (w, b) such that the decision boundary θ = b/w represents the entropy value where (correct H) = 0.5. Geometrically, this corresponds to the inflection point of the sigmoid function σ(w + b) projected onto the entropy axisthe point of maximum model uncertainty, making it natural intervention threshold. Confidence Interval Construction. The 95% confidence interval [θlower, θupper] is derived by extracting the 2.5th and 97.5th percentiles of the bootstrap distribution: θlower = Q0.025(Θ), θupper = Q0.975(Θ) (7) where Qp() denotes the p-th quantile function. Computational Efficiency. On an Intel Xeon CPU, estimating thresholds for one step type (process or answer) completes within seconds. Since we estimate thresholds independently for both types, the total offline computation time is negligible compared to online inference costs. A.2 Additional Advantages of the Algorithm As illustrated in Figure 8, the threshold estimation algorithm adapts effectively to cases with reduced distributional differences. In the 32B process steps, the predominance of the yellow region indicates that under higher uncertainty, the algorithm adaptively randomizes trigger decisions to avoid excessive intervention. Algorithm 1 Threshold Estimation Input: Entropy sets H+ (correct steps), (incorrect steps), Bootstrap iterations Output: Threshold interval [θlower, θupper] 1: Initialize Θ 2: for = 1 to do 3: 4: // Bootstrap resampling Sample H+ H+ with replacement Sample H with replacement // Construct training data 6: 7: Di {( H, 0) H+ 5: by drawing + elements from by drawing elements from } {( H, 1) H } // Train logistic regression Train logistic regression: (cid:80) arg minw,b where σ(z) = 1/(1 + ez) // Extract threshold at decision boundary θ(i) bi/wi L(y, σ(w + b)) ( H,y)Di (wi, bi) 8: 9: 10: 11: 12: 13: Θ Θ {θ(i)} 14: end for 15: Sort Θ in ascending order 16: θlower Q0.025(Θ) 17: θupper Q0.975(Θ) 18: return [θlower, θupper]"
        },
        {
            "title": "B Details of Experiment Results",
            "content": "B.1 Pass@k Results As shown in Figure 5, ExpSeek demonstrates stronger performance in sampling diversity. Compared to vanilla, the absolute improvements in pass@3 are 12.9% and 8.8%, exceeding the improvement margins in average accuracy. In contrast, the two baselines do not exhibit better diversity, and even fall below vanilla on multiple datasets. This highlights the significant advantage of ExpSeek, with potential even as rollout augmentation strategy in agentic RL training. B.2 Why prior methods underperform? Based on the experimental results, we reflect on why prior experience-based intervention methods are ineffective. We attribute the reasons to two aspects. (1) Experience acquisition is disconnected from actual reasoning. Most experience repository construction methods meticulously process training trajectories, perform multiple rounds of careful denoising, and cautiously distill core experience items Method WebWalkerQA Easy Medium Hard Avg. GAIA SEAL xbench Avg. No Experience Training-Free GRPO + REASONINGBANK ExpSeek (Ours) 57.500.0 56.670.8 61.173.7 69.1711. 60.000.0 62.542.5 58.191.8 72.4512.5 Qwen3-8B 46.110.0 52.676.6 50.003.9 63.3317.2 54.540.0 57.292.8 56.451.9 68.3213.8 Qwen3-32B 47.570.0 48.541.0 48.701.1 54.376. 39.760.0 42.132.4 44.094.3 50.0010.2 43.000.0 45.002.0 46.803.8 62.0019.0 48.990.0 51.262.3 51.492.5 61.8912.9 No Experience Training-Free GRPO + REASONINGBANK ExpSeek (Ours) 62.500.0 63.951.5 68.335.8 72.5010.0 61.430.0 59.571.9 64.332.9 72.8611. 58.330.0 57.450.9 56.441.9 63.335.0 60.750.0 60.320.4 63.032.3 69.568.8 54.370.0 56.622.3 46.607.8 63.118.7 47.640.0 48.350.7 48.821.2 51.974.3 53.000.0 53.600.6 59.006.0 66.0013.0 56.210.0 56.590.4 57.251.0 64.968. Table 5: Extended main results reporting Pass@3 accuracy for baselines and ExpSeek. We also provide absolute performance differences from experience-free ReAct. with deep observation of successes and failures. However, the construction process itself is challenging, and when such heavily processed experiences are given to reasoning agents, the agents may not even understand some abstract expressions. This differs from how humans use experiencewhen humans recall experience, the amount of information retrieved instantly is enormous, including scenes, behavioral details, and even emotions, far exceeding simplified experience items. (2) Experience is difficult to utilize. During multi-turn agent interactions, even when accurate experiences are provided in system prompts, it is difficult to require models to precisely locate few short effective experience items across ultra-long contexts. Moreover, models must also leverage experience to correct their original reasoning tendencies. ExpSeek essentially simulates user through an experience model, providing effective guidance that does not require deep model understanding to be leveraged. Since each guidance is generated in real-time based on context, the experience is not disconnected from actual reasoning. B.3 Case Studys As shown in Table 7, 8, 9and 10. The contrasting outcomes reveal critical limitation in agents reasoning when relying on incomplete information, and demonstrate how strategic guidance can redirect agents toward reliable evidence. Why the unguided trajectory failed: The agents error stemmed from reliance on search snippets that listed \"Spider-Man: No Way Home\" alongside \"The Super Mario Bros. Movie\" and \"Jurassic World: Dominion\" in billion-dollar film discussions, but omitted distributor information. Figure 8: Entropy distributions of process and answer steps on Dtrain for Qwen3-32B, with fitted logistic regression curves. Without verification mechanisms, the agent incorrectly inferred all three were Universal releases, when \"Spider-Man: No Way Home\" was actually distributed by Sony Pictures. This represents classic entity attribute confusionthe agent failed to verify the critical \"distributor\" attribute before counting. Why guidance succeeded: The guidance implemented two-pronged strategy. First, it redirected the agent toward authoritative box office 13 Figure 9: Entropy distributions of process and answer steps for Qwen3-32B before and after applying ExpSeek across all benchmarks. sites (Screen Rant, Rotten Tomatoes, Box Office Mojo) that systematically label films with distributors. Second, it reinforced the tasks dual requirementcounting films satisfying both revenue threshold and distributor constraintprompting verification-based approach. The Screen Rant article provided explicit evidence that only two Universal-distributed films (via Illumination and Amblin partnerships) appeared on the billion-dollar list, enabling correct filtering and counting. The guidances success lies in methodological redirection rather than direct answer provision. By steering the agent toward sources with richer structural metadata and emphasizing attribute verification, it enabled the agent to overcome reasoning failure through improved evidence quality rather than external correction. B.4 Guiding Experience In this section, we demonstrate the details of the guiding experience. Table 6 presents experience demos from both stages. The process step experience focuses on strategic decisions during reasoning, such as how the student selects information sources for verification after searching. Its mistake identifies methodological issues affecting subsequent reasoning paths (e.g., relying on snippets rather than accessing authoritative sources), while guidance provides directional suggestions for process optimization (e.g., prioritizing official sources). In contrast, the final step experience concentrates on answer formulation, such as extracting the final answer from search results. Its mistake targets detail-oriented errors directly impacting output quality (e.g., failing to faithfully reproduce complete proper nouns), while guidance emphasizes result accuracy, prompting the model to focus on answer details and encouraging trajectory extension for thorough verification when necessary. This triplet design reproduces step-level error patterns from training trajectories and provides targeted guidance, enabling the experience model to more accurately guide both process and final steps during test inference."
        },
        {
            "title": "C Details of Setting",
            "content": "As Qwen3-8B and Qwen3-32B are hybrid reasoning models, we deploy them in non-thinking mode and adopt the <thought></thought> tag rather than <think></think> to prevent conflicts with their original output structure. For tool implementation, we use Bright Data5 to provide stable web API services for the search tool, Jina6 as the web access service for the visit tool, and Qwen3-235B-A22B-Instruct-2507 as the summarization model within the visit tool. All other experimental settings and prompts remain consistent with prior work (Wu et al., 2025a). The four benchmarks employed in our evaluation are widely recognized in web agent research and are permitted for use in academic studies under their distribution terms."
        },
        {
            "title": "D Details of Prompts",
            "content": "In this section, we present all key prompts. Table 11 shows the system prompt for ReAct, which we have slightly adjusted based on guidance experience. Tables 12 and 13 are prompts for generating the experience base, where Table 12 implements identifying step mistakes and generating triplets, and Table 13 iteratively generates topics based on all 5https://www.bright.cn 6https://jina.ai 14 Experience base demos. Process Step demo { \"Behavior\": \"The student recognized the need to confirm which African countries participated in the 7th China International Import Expo (CIIE) under the theme of 60th Anniversary of Diplomatic Relations through external information sources, and therefore initiated multi-angle keyword searches.\", \"Mistake\": \"Although the search keywords were reasonably designed, the student did not prioritize visiting sources most likely to contain authoritative information (such as the official CIIE website) for in-depth access. Instead, they relied only on search result snippets for preliminary judgment, laying hidden risks for subsequent misjudgment.\", \"Guidance\": \"When reviewing search results, you should prioritize identifying and planning to visit authoritative or official sources (such as ciie.org). Especially when multiple results cite the same webpage, it should be set as the primary verification target to avoid prematurely synthesizing incomplete information to draw conclusions.\" } Final Step demo { \"Behavior\": \"The student reviewed the search results, found that multiple results pointed to website design company called Niegao (niegoweb.com), and inferred from this that the website was designed by this company.\", \"Mistake\": \"The student overlooked the complete official company name that clearly appeared in the search resultsNiegao Brand Network or Niegao Brand Designand instead simplified it on their own to Niegao Website Design Company, resulting in an answer inconsistent with the standard answer. Although semantically similar, the student failed to faithfully reproduce the proper noun from the original text.\", \"Guidance\": \"Before drawing conclusions, please carefully check the complete institutional names that appear in the search results, especially the official names of collaborating parties in case study pages. Prioritize using the full brand name directly mentioned in the original text rather than summarizing it based on your own understanding.\" } Table 6: Experience base demos for Qwen3-8B. triplets. Tables 14 and 15 are prompts for utilizing the experience model during inference. Table 14 is for the experience model to determine which topics to utilize, and Table 15 generates appropriate proactive guidance based on all experiences under the topics and the current history context. Table 17 is the prompt for using LLM as an intervention trigger, where outputting yes triggers guidance for that step. Table 18 is for utilizing retrieved experience base experiences during inference. Table 16 is for generating guidance directly using the world experience of the experience model without relying on the experience base. Case study (Incorrect): Qwen3-8B interaction without guiding experience Question: How many Universal Studios films have grossed over $1 billion worldwide since 2020? Agent Action: <think> To determine how many Universal Studios films have grossed over $1 billion worldwide since 2020, need to search for list of Universal Studios films and their box office earnings. This will involve first identifying which films are associated with Universal Studios and then checking their gross earnings. Ill begin by searching for list of Universal Studios films and their box office performance since 2020. </think> <tool_call> Search: Universal Studios films gross over $1 billion since 2020 Search: List of Universal Studios movies with box office earnings over $1 billion since 2020 </tool_call> Env Tool Response: Google search for Universal Studios films gross over $1 billion since 2020 found 5 results: ...... 2. [Universal - The First Half of the 2020s In Review : r/boxoffice] Universal - The First Half of the 2020s In Review The Super Mario Bros. Movie - $1.36B Jurassic World Dominion - $1.001B Oppenheimer- $975.6M. ...... 5. [6 Movies That Have Made $1 Billion At The Box Office Since Covid] 6 Jurassic World: Dominion (2022) 5 The Super Mario Bros. Movie (2023) 4 Spider-Man: No Way Home (2021) 3 Avatar: The Way of Water (2022) 2 ... ...... Agent Action: <think> Based on the search results, need to determine which Universal Studios films have grossed over $1 billion worldwide since 2020. The key information comes from the Reddit post and the The Numbers website. According to the Reddit post, \"The Super Mario Bros. Movie\" has grossed $1.36B, and \"Jurassic World Dominion\" has grossed $1.001B. From the The Numbers website, there is list of the top Universal movies by worldwide box office, and the most recent ones include \"The Super Mario Bros. Movie\" and \"Jurassic World Dominion.\" Additionally, it appears that other films such as \"Avatar: The Way of Water\" might also qualify based on the context provided in the search results. To confirm this, will visit the The Numbers website directly to get the exact data for Universal Studios films and their box office earnings since 2020. </think> <tool_call> Visit: https://www.the-numbers.com/box-office-records/worldwide/all-movies/theatrical-distributors/universal Visit Goal: Find the list of Universal Studios films with box office earnings over $1 billion since 2020. </tool_call> Env Tool Response: The requested webpage did not load fully and contains no usable data. As result, there is insufficient information to identify Universal Studios films with box office earnings exceeding $1 billion since 2020. ...... Agent Action: <think> The attempt to access the The Numbers website directly did not yield usable data. This may be due to technical issues or incomplete page loading. will need to find an alternative source of information. Based on the previous search results, the Reddit post mentioned \"The Super Mario Bros. Movie\" and \"Jurassic World Dominion\" as having grossed over $1 billion. However, should verify this with another reliable source to confirm the accuracy of these figures. </think> <tool_call> Search: Universal Studios films over $1 billion since 2020 Search: List of Universal Studios movies with box office earnings over $1 billion since 2020 </tool_call> Table 7: Case study: Qwen3-8B interaction without guiding experience 16 Case study (Incorrect): Qwen3-8B interaction without guiding experience Env Tool Response: Google search for Universal Studios films over $1 billion since 2020 found 5 results: ...... 3. [8 Movies That Broke $1 Billion at the Box Office Since Covid]Avatar: The Way of Waters box office comes in at over $2.3 billion... ...... 5. [6 Movies That Have Made $1 Billion At The Box Office Since Covid]6 Jurassic World: Dominion (2022) 5 The Super Mario Bros. Movie (2023) 4 Spider-Man: No Way Home (2021) 3 Avatar: The Way of Water (2022) 2 ... ...... Agent Action: <think> From the search results, can see that \"The Super Mario Bros. Movie\" (2023) and \"Jurassic World: Dominion\" (2022) are mentioned as having grossed over $1 billion worldwide. Additionally, \"Spider-Man: No Way Home\" (2021) is noted as one of the first films to reach the $1 billion mark since the start of the pandemic. These three films are distributed by Universal Studios, and they all have box office earnings exceeding $1 billion globally since 2020. Therefore, the answer to the question is that three Universal Studios films have grossed over $1 billion worldwide since 2020. </think> <answer> Three Universal Studios films have grossed over $1 billion worldwide since 2020: \"The Super Mario Bros. Movie,\" \"Jurassic World: Dominion,\" and \"Spider-Man: No Way Home.\" </answer> Table 8: Case study: Qwen3-8B interaction without guiding experience (Table continued) 17 Case study (Correct): Qwen3-8B interaction with guiding experience Question: How many Universal Studios films have grossed over $1 billion worldwide since 2020? Agent Action: <think> To determine how many Universal Studios films have grossed over $1 billion worldwide since 2020, need to search for list of Universal Studios films and their box office earnings. This will involve first identifying which films are associated with Universal Studios and then checking their gross earnings. Ill begin by searching for list of Universal Studios films and their box office performance since 2020. </think> <tool_call> Search: Universal Studios films gross over $1 billion since 2020 Search: List of Universal Studios movies with box office earnings over $1 billion since 2020 </tool_call> Env Tool Response: Google search for Universal Studios films gross over $1 billion since 2020 found 5 results: ...... 2. [Universal - The First Half of the 2020s In Review : r/boxoffice] Universal - The First Half of the 2020s In Review The Super Mario Bros. Movie - $1.36B Jurassic World Dominion - $1.001B Oppenheimer- $975.6M. ...... 5. [6 Movies That Have Made $1 Billion At The Box Office Since Covid] 6 Jurassic World: Dominion (2022) 5 The Super Mario Bros. Movie (2023) 4 Spider-Man: No Way Home (2021) 3 Avatar: The Way of Water (2022) 2 ... ...... Agent Action: <think> Based on the search results, need to determine which Universal Studios films have grossed over $1 billion worldwide since 2020. The key information comes from the Reddit post and the The Numbers website. According to the Reddit post, \"The Super Mario Bros. Movie\" has grossed $1.36B, and \"Jurassic World Dominion\" has grossed $1.001B. From the The Numbers website, there is list of the top Universal movies by worldwide box office, and the most recent ones include \"The Super Mario Bros. Movie\" and \"Jurassic World Dominion.\" Additionally, it appears that other films such as \"Avatar: The Way of Water\" might also qualify based on the context provided in the search results. To confirm this, will visit the The Numbers website directly to get the exact data for Universal Studios films and their box office earnings since 2020. </think> <tool_call> Visit: https://www.the-numbers.com/box-office-records/worldwide/all-movies/theatrical-distributors/universal Visit goal: Find the list of Universal Studios films with box office earnings over $1 billion since 2020 </tool_call> Env Tool Response: The requested webpage did not load fully and contains no usable data. As result, there is insufficient information to identify Universal Studios films with box office earnings exceeding $1 billion since 2020. ...... Agent Action: <think> The attempt to access the The Numbers website directly did not yield usable data. This may be due to technical issues or incomplete page loading. will need to find an alternative source of information. Based on the previous search results, the Reddit post mentioned \"The Super Mario Bros. Movie\" and \"Jurassic World Dominion\" as having grossed over $1 billion. However, should verify this with another reliable source to confirm the accuracy of these figures. </think> <tool_call> Search: Universal Studios films over $1 billion since 2020 Search: List of Universal Studios movies with box office earnings over $1 billion since 2020 </tool_call> Table 9: Case study: Qwen3-8B interaction with guiding experience 18 Case study (Correct): Qwen3-8B interaction with guiding experience Env Tool Response (with Guidance): Google search for Universal Studios films over $1 billion since 2020 found 5 results: ...... 3. [8 Movies That Broke $1 Billion at the Box Office Since Covid] Avatar: The Way of Waters box office comes in at over $2.3 billion... ...... 5. [6 Movies That Have Made $1 Billion At The Box Office Since Covid] 6 Jurassic World: Dominion (2022) 5 The Super Mario Bros. Movie (2023) 4 Spider-Man: No Way Home (2021) 3 Avatar: The Way of Water (2022) 2 ... ...... <guidance> Review the core of the question: you need to count films released by Universal Studios with worldwide box office over $1 billion since 2020. This means the answer is not listing specific film, but complete count based on reliable sources. You have found some films that may meet the criteria, but different sources show inconsistencies. Instead of repeating previous searches, carefully examine those webpages in the existing search results that specifically compile \"billion-dollar movie\" lists, such as links from Screen Rant, Rotten Tomatoes, or Box Office Mojo. These pages often label each film with its distributor and specific box office figures, helping you verify one by one whether it belongs to Universal Studios and meets the time and amount conditions. Special reminder: Even if page doesnt directly state \"there are films in total,\" you can still obtain the answer through manual countingthe key is finding reliable list that completely catalogs such films and marks their distributors. Prioritize visiting such summary articles or professional box office ranking pages. </guidance> Agent Action: <think> Based on the guidance, should focus on reliable sources that list billion-dollar movies and identify which ones are distributed by Universal Studios. The Screen Rant article mentioned \"Jurassic World: Dominion\" and \"The Super Mario Bros. Movie\" as having grossed over $1 billion, but need to confirm this with direct access to the full article or more comprehensive list. To do this, will visit the Screen Rant page to verify the information. </think> <tool_call> Visit: https://screenrant.com/billion-dollar-movies-box-office-2020 Visit goal: Identify the number of Universal Studios films with box office earnings over $1 billion since 2020 </tool_call> Env Tool Response: Evidence in page: [\"5 The Super Mario Bros. Movie (2023) - The movies success has led to Nintendo teasing range of films based on more of its iconic video game characters.\", \"Illumination earned much higher net profit than other billion-dollar movies\", \"6 Jurassic World: Dominion (2022) - However, after scathing reviews... the movie was always going to flounder. The movie did face unlikely box office competition, too. Top Gun: Maverick was still playing when Jurassic World: Dominion was released...\", \"Jurassic World: Dominion made almost exactly $1 billion worldwide.\"] Summary: Since 2020, two Universal Studios films have earned over $1 billion at the global box office: *Jurassic World: Dominion* (2022) and *The Super Mario Bros. Movie* (2023). Both were produced by Universal-owned or partnered studiosIllumination for *The Super Mario Bros. Movie* and Amblin Entertainment in partnership with Universal for *Jurassic World: Dominion*. These are the only two Universal-released films on the list of six post-2020 billion-dollar earners. Agent Action: <think> Based on the information from the Screen Rant article, there are exactly two Universal Studios films that have grossed over $1 billion worldwide since 2020: \"Jurassic World: Dominion\" (2022) and \"The Super Mario Bros. Movie\" (2023). These are the only two films from Universal Studios listed among the billion-dollar earners since 2020. </think> <answer> Since 2020, two Universal Studios films have grossed over $1 billion worldwide: \"Jurassic World: Dominion\" (2022) and \"The Super Mario Bros. Movie\" (2023). Therefore, the answer is 2. </answer> Table 10: Case study: Qwen3-8B interaction with guiding experience (Table continued) ReAct system prompt. You are problem-solving expert, skilled at analyzing complex problems, forming solutions, and gradually resolving them step-by-step. Your approach is flexible and can be adjusted based on the progress of problem-solving. Any problem you encounter will certainly be solvable; therefore, no matter how complex, you must not give up until you find the answer you consider correct. You must output two parts based on the current input, both enclosed by specific angle bracket position markers: **The first part** is enclosed by <thought></thought> and contains your self-thinking process, including analysis of the problem, determining whether tool usage is required, which tool to call, and what values to pass to the tool. **The second part** is divided into two situations: (1) If you wish to call tool, then after </thought> output <tool_call></tool_call> containing standard tool call content, with the format introduced below. (2) If you believe the current problem can already be solved, then after </thought> output <answer></answer> containing the final answer. **Available tools and their function overview** 1. search: Input keyword you think needs to be searched, returns multiple website links and corresponding summaries. 2. visit: Input website link and the problem you hope to solve by visiting that site, returns the problems answer and snippet of the original content from the site corresponding to the answer. **Detailed tool call format** {search_format} {visit_format} **Model workflow description** 1. Model input consists of the historical loop of interactions, including the models generated thoughts and tool calls, and the users returned tool results. The history may have multiple rounds, all designed to solve given problem. 2. The model must output two parts: thinking process, and tool call or answer, each enclosed by specific angle bracket position markers: - If thinking, follow the format: <thought> here is the thinking process </thought> - If calling tool, follow the format: <tool_call> tool call here </tool_call> - If determining the final answer, follow the format: <answer> final answer </answer> 3. The user will respond with tool call results or occasionally provide guidance. If there is guidance, you should carefully consider whether the users ideas are reasonable and try to follow them: - If responding to tool call, you will see the format: <tool_response> here is the tools return value </tool_response> - If the user provides careful guidance, you will see the format: <user_guidance> here is the users guidance content </user_guidance> 4. Every problem must have an answer; during multi-round resolution, do not forget your past planning and process results, and do not forget the details embedded in the problem. 5. The search tools return value is only website links and snippet summaries; they are hardly reliable references and can only serve as search direction. To obtain accurate information, you must call the visit tool to visit site. 6. If the interaction includes resolution plan, follow the plan. Do not blindly ignore key constraints in the plan to avoid potential cascading errors. 7. In tool_call generation, the format must follow the above definitions and be valid JSON. An incorrect format will cause tool calls to fail. *8*. **IMPORTANT**: If the user provides guidance after the answer, prioritize regenerating <tool_call></tool_call> to continue searching for missing clues, or provide only when you are absolutely certain of the answer. **Multi-round interaction full example** ˋˋˋ {example} ˋˋˋ **Notes** 1. **You must generate the position markers** in accordance with the requirements stated above (<thought></thought>; <tool_call></tool_call> or <answer></answer>). 2. In particular, do not forget to generate the closing tags: </thought>, </tool_call>, OR </answer> 3. You must not generate extra angle bracket position markers. Table 11: ReAct system prompt. Prompt for generating experience triplets. # Questions for Students to Solve {question} # Standard Answer for the Question {answer} # This is complete trajectory that ultimately got the correct answer as your reference: ˋˋˋ{true_traj}ˋˋˋ # This is complete trajectory that ultimately got the wrong answer: ˋˋˋ{false_traj}ˋˋˋ # Pattern of All Trajectories: Question, R1, O1, R2, O2, R3, O3, R4... # Instructions and Your Task 1. Define STEP as R_i+O_i, but the last STEP only has R_N 2. Each is students response, attempting to call tools to further solve the problem, but the second trajectory with wrong answer always has some issues 3. Your core task is to answer this question for each STEP: ˋˋˋIn order to avoid the final error, if guidance is provided after this STEP ends, what should be done to make the agent perform better?ˋˋˋ 4. Of course, complete guidance is triplet <students current state, reason why this STEP leads to the final error, what to say before the next STEP to improve the current state> ˋˋˋExplanation of the triplet: - Students current state: relatively general description, introducing what the student saw and what they did. The description does not involve error attribution, focuses on objectively stating the students behavior, and does not evaluate whether the behavior is good/bad or right/wrong - Reason why this STEP leads to the final error: Unlike the current state, this part explicitly points out what mistake the student made in this STEP - What to say before the next STEP to improve the current state: Based on the errors mentioned above, provide specific guidance that will help the student perform better in the next STEP if they follow it. Of course, do not directly tell the student the answer! ˋˋˋ 5. The guidance opinion in the triplet generated for STEP_i will be concatenated after O_i, which means the student can see it before generating R_i+1 6. Not every STEP necessarily needs guidance, you can skip after analysis, but since the trajectory is wrong, **there must be at least one STEP that has issues and can be summarized into triplet** 7. Finally, briefly summarize what three good pieces of advice could be given before working on this problem 8. **!!Must Note!!** The total number of rounds you analyze in the trajectory is **{step_num}**, you must generate the corresponding number of STEPs before you can continue to generate TOTAL! # Output Format (strictly follow the markdown format give you) ˋˋˋ # STEP 1: ## Analysis - Write analysis content here ## Triplet (If there is no error, directly write \"- None\", do not generate triplet when there is no error) - Students current state: Write current state here - Reason why this STEP leads to the final error: Write reason here - What to say before the next STEP to improve the current state: Write guidance here # STEP 2: ... ˋˋˋ Table 12: Prompt for generating experience triplets. 21 Prompt for iteratively generating topics. teacher is analyzing each step taken by students when solving complex problems. will give you several behavior + mistake items that the teacher has summarized for students at certain steps, defined as: ˋˋˋ behavior: relatively general description introducing what the student saw and then did. The description does not involve error attribution, focuses on objectively stating the students behavior, and does not evaluate whether the behavior is good/bad or right/wrong mistake: The student is satisfied with preliminary information (such as Deputy Party Secretary) and fails to realize the need to cross-reference multiple search results to extract complete and accurate answer, particularly overlooking that the specific name xx xx xx has already appeared in the first entry of the third set of search results. ˋˋˋ # Overall Overview Your goal is to give **each behavior + mistake** **scenario-narrative label**. labels description should be concise enough to clearly express the characteristics of the behavior and be reusable. For each given new behavior + mistake, you can choose one of the following three actions: 1. Reuse: Do not change any current labels, and select an existing label for the new behavior (recognizing the existing classification) 2. Create: Do not change existing labels, create new label for the new behavior (existing classification is incomplete) 3. Modify: Modify certain current labels, and assign that label to the new behavior (existing classification is inaccurate) # Detailed Requirements 1. Each label must be concise and clear, but needs to have certain semantic information that allows people to understand the characteristics of the current behavior + mistake without explanation. It should be at least dozen or dozens of words (e.g., in the pattern of xxx: xxx xxx xx). 2. There cannot be too many labels; each label should have distinguishability in scenario content. 3. One label can correspond to multiple behaviors, so you must ensure their textual content is consistent. 4. Use the given id as the unique identifier for behaviors. When outputting, you need to output the ids and labels of all existing behaviors and new behaviors. 5. Try to keep the number of different labels balanced. # List of Behaviors Already Given Labels {exp_list} # List of New Behaviors {new_exp_list} # Output Format: ˋˋˋ {output_format} ˋˋˋ Table 13: Prompt for iteratively generating topics. Prompt for experience model topic selection stage. # Overall Instructions You are teacher who is very good at guiding students to answer difficult questions Now, the student has made some attempts on the given problem, but there may be some non-absolute but potential issues, or even errors, in the **current step**. will give you the students multiple rounds of attempts from the beginning to the current state, where assistant represents the students response, user represents the environments return value, and may also include your previous guidance. In addition, topic list full of your guidance experience is provided for you to choose from. # Students Multiple Rounds of Interaction ˋˋˋ{history}ˋˋˋ # Candidate Topic List ˋˋˋ{topic_list}ˋˋˋ # Your Task 1. You need to combine the students current state and select **3** topics from the several potential error topics give you. After you carefully state the reasons for selection, just output the idx of the selected topics. 2. The student may not have actually made mistake, but your subsequent guidance can prevent problems before they occur. # Output Format Strictly follow the markdown format below for output ˋˋˋ # Analysis of the Current Step Write your analysis here # Selected Topic idx (separated by spaces) idx1 idx2 idx3 ˋˋˋ Output: Table 14: Prompt for experience model topic selection stage. 23 Prompt for experience model guidance generation stage. # Overall Instructions You are teacher who is very good at guiding students to answer difficult questions. Now, the student has made some attempts on the given problem, but there may be some non-absolute but potential issues, or even errors, in the **current step**. will give you the students multiple rounds of attempts from the beginning to the current state, where assistant represents the students response, user represents the environments return value, and may also include your previous guidance. In addition, you are given your previous rich teaching experience, which consists of guidance you provided for different problem steps. Your goal is to provide the most appropriate guidance for the current step. # Students Multiple Rounds of Interaction ˋˋˋ {history} ˋˋˋ # Previous Rich Guidance Experience These guidances are based on the step problems you discovered in students, so each content is triplet, including the students behavior, the error, and your previous guidance ˋˋˋ {topic_kb} ˋˋˋ # Your Task and Requirements 1. The question must have an answer. If the student thinks there is insufficient evidence, it must be because they havent found the evidence. After careful analysis, provide your guidance for the students current step, with the goal of helping the student actually answer the question correctly. 2. Since this step may not necessarily be wrong, please carefully choose your wording to prevent your guidance from introducing bias. 3. Your analysis must include brief review of the **problem** that the student needs to solve, emphasizing the content of the problem to the student to prevent answering off-topic. 4. The guidance you provide will be given to the student together with the tool call results after this step ends. - If the student generates tool call in this step, the guidance will be given to the student together with the tool return value - If the student generates an answer in this step, the guidance will be given directly to the student, and the student will choose whether to answer again or continue to generate tool calls based on your guidance 5. It is forbidden to find answers on behalf of the student, and it is forbidden to hint at what the answer is under any circumstances. **You are teacher, not someone helping students cheat**. 6. It is forbidden to provide **direct clues** to students. Your purpose is only to **guide**. 7. Guidance should be clear and easy to understand. If necessary, you can encourage students to continue calling tools or switch tools. 8. The guidance you provide should try to imitate the previous guidance patterns, dont improvise freely. # Output Format Strictly follow the markdown format below for output ˋˋˋ # Analysis combining students current behavior and previous experience to provide appropriate guidance for the present moment Write your detailed analysis here # Guidance Content Write your guidance content here ˋˋˋ Output: Table 15: Prompt for experience model guidance generation stage. 24 Prompt for generating guidance experience directly without referencing the experience base. # Overall Instructions You are teacher who is very good at guiding students to answer difficult questions. Now, the student has made some attempts on the given problem, but there may be some non-absolute but potential issues, or even errors, in the **current step**. will give you the students multiple rounds of attempts from the beginning to the current state, where assistant represents the students response, user represents the environments return value, and may also include your previous guidance. Your goal is to provide the most appropriate guidance for the current step. # Students Multiple Rounds of Interaction ˋˋˋ {history} ˋˋˋ # Your Task and Requirements 1. The question must have an answer. If the student thinks there is insufficient evidence, it must be because they havent found the evidence. After careful analysis, provide your guidance for the students current step, with the goal of helping the student actually answer the question correctly. 2. Since this step may not necessarily be wrong, please carefully choose your wording to prevent your guidance from introducing bias. 3. Your analysis must include brief review of the **problem** that the student needs to solve, emphasizing the content of the problem to the student to prevent answering off-topic. 4. The guidance you provide will be given to the student together with the tool call results after this step ends - If the student generates tool call in this step, the guidance will be given to the student together with the tool return value - If the student generates an answer in this step, the guidance will be given directly to the student, and the student will choose whether to answer again or continue to generate tool calls based on your guidance 5. It is forbidden to find answers on behalf of the student, and it is forbidden to hint at what the answer is under any circumstances. **You are teacher, not someone helping students cheat**. 6. It is forbidden to provide **direct clues** to students. Your purpose is only to **guide**. 7. Guidance should be clear and easy to understand. If necessary, you can encourage students to continue calling tools or switch tools. # Output Format Strictly follow the markdown format below for output ˋˋˋ # Analysis combining students current behavior to provide appropriate guidance for the present moment Write your detailed analysis here # Guidance Content Write your guidance content here ˋˋˋ Output: Table 16: Prompt for generating guidance experience directly without referencing the experience base. Prompt for model-based guidance triggering decision. # Overall Instructions You are teacher who is very good at guiding students to answer difficult questions. Now, the student has made some attempts on the given question, but there may be some non-absolute yet potential issues or even errors in the current step. will provide you with the students multi-turn attempts from the beginning to the current point, where assistant represents the students responses and \"user\" represents environmental feedback, which may also include the teachers prior guidance. Your purpose is to determine whether guidance is needed at the current step. # Students Multi-turn Interactions ˋˋˋ {history} ˋˋˋ # Your Task and Requirements Analyze whether the students behavior in the current step has potential issues, and provide signal indicating whether guidance is needed. # Output Format Strictly follow the markdown format below for output ˋˋˋ # Your Analysis Your analysis content here # Whether to Provide Guidance yes/no ˋˋˋ Output: Table 17: Prompt for model-based guidance triggering decision. Prompt for retrieving and directly using the experience base. Your current situation resembles mistake made by previous student. That students behavior was: {behavior} Their mistake was: {mistake} My guidance to them was: {guidance} Learn from this error, examine your own behavior, and proceed with better decision-making. Table 18: Prompt for retrieving and directly using the experience base. Prompt for evaluation. You are an evaluation assistant. Please determine if the predicted answer is equivalent to the labeled answer. As long as the Labeled Answer is contained within the Predicted Answer, it is considered correct, even if additional, more detailed explanations are included. You should focus on whether the Predicted Answer truly answers the question correctly. # Question: {question} # Labeled Answer: {correct_answer} # Predicted Answer: {response} Respond **only with** \"Correct\" or \"Incorrect\", no other tokens. Table 19: Prompt for evaluation."
        }
    ],
    "affiliations": [
        "Institute of Information Engineering, Chinese Academy of Sciences",
        "School of Cyber Security, University of Chinese Academy of Sciences",
        "Tongyi Lab, Alibaba Group"
    ]
}