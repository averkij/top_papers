{
    "paper_title": "Adaptive Ability Decomposing for Unlocking Large Reasoning Model Effective Reinforcement Learning",
    "authors": [
        "Zhipeng Chen",
        "Xiaobo Qin",
        "Wayne Xin Zhao",
        "Youbin Wu",
        "Ji-Rong Wen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning with verifiable rewards (RLVR) has shown great potential to enhance the reasoning ability of large language models (LLMs). However, due to the limited amount of information provided during the RLVR process, the model can only engage in largely blind exploration, which often results in failure on challenging problems. To provide additional information for the RLVR process without relying on a teacher model, we propose A$^2$D, an Adaptive Ability Decomposing method for enhancing the effectiveness of RLVR. Specifically, we first train a decomposer via RLVR without distillation, enabling it to decompose complex questions into a set of simpler sub-questions. Next, we use this decomposer to annotate sub-questions for each question in the training dataset, and then train the reasoner under RLVR with sub-question guidance. To better understand A$^2$D, we first compare its performance with competitive baselines, showing its effectiveness. Next, we observe that our method functions as a plug-and-play module that can be applied to different RLVR algorithms. Furthermore, we conduct an analysis of the decomposer, revealing how the RLVR process affects its performance and behavior, and which type of guidance is better suited for enhancing the reasoner's exploration and exploitation abilities."
        },
        {
            "title": "Start",
            "content": "Zhipeng Chen1,2,, Xiaobo Qin2, Wayne Xin Zhao1,, Youbin Wu2,, Ji-Rong Wen1 1Renmin University of China, 2ByteDance Seed Work done at ByteDance Seed, Corresponding authors"
        },
        {
            "title": "Abstract",
            "content": "Reinforcement learning with verifiable rewards (RLVR) has shown great potential to enhance the reasoning ability of large language models (LLMs). However, due to the limited amount of information provided during the RLVR process, the model can only engage in largely blind exploration, which often results in failure on challenging problems. To provide additional information for the RLVR process without relying on teacher model, we propose A2D, an Adaptive Ability Decomposing method for enhancing the effectiveness of RLVR. Specifically, we first train decomposer via RLVR without distillation, enabling it to decompose complex questions into set of simpler sub-questions. Next, we use this decomposer to annotate sub-questions for each question in the training dataset, and then train the reasoner under RLVR with sub-question guidance. To better understand A2D, we first compare its performance with competitive baselines, showing its effectiveness. Next, we observe that our method functions as plug-and-play module that can be applied to different RLVR algorithms. Furthermore, we conduct an analysis of the decomposer, revealing how the RLVR process affects its performance and behavior, and which type of guidance is better suited for enhancing the reasoners exploration and exploitation abilities. Date: February 3, 2026 Correspondence: Zhipeng Chen at zhipeng_chen@ruc.edu.cn, Wayne Xin Zhao at batmanfly@gmail.com, Youbin Wu at youbinwu@bytedance.com 6 2 0 2 1 3 ] . [ 1 9 5 7 0 0 . 2 0 6 2 : r Figure 1 Average performance of various models on mathematical tasks. By incorporating sub-question guidance into the prompts, LLMs can perform exploration and reasoning more effectively, thereby achieving higher Pass@k scores."
        },
        {
            "title": "Introduction",
            "content": "Recently, reinforcement learning with verifiable rewards (RLVR) has become widely used approach to enhance the reasoning ability of large language models (LLMs), e.g., OpenAI o3 [42], DeepSeek-R1 [18], and Seed1.5-VL [24]. Through the RLVR training procedure, the model can possess the ability to perform complex reasoning actions [15, 28], e.g., self-reflection, self-verification, and self-correction, which can also be called the large reasoning model (LRM). During the RLVR training process, the LRMs first explore and reason over given question, and then verifier provides rewards for the generated solutions. The LRMs learn from these experiences and gradually acquire the ability to perform complex reasoning behaviors [14, 58]. Although RLVR has achieved substantial success in effectively improving the reasoning capabilities of LRMs, series of studies [41, 65, 71, 74] have pointed out that RLVR struggles to raise the upper bound of model performance, thereby hindering continued capability growth in later training stages. To alleviate this issue, prior work [16, 17, 22, 61] has focused on designing reward functions and advantage functions to encourage LRMs to explore and solve more challenging problems, thereby enhancing the models exploration capability. Despite encouraging the model to explore, supervision provided only at the outcome level, which is the popular RLVR training settings [18, 50], offers limited guidance for effective exploration [38], leaving the model in state of largely blind exploration [54, 64, 78]. In this case, the models exploration often leads to failure in solving the given problems, which will be punished by the RLVR process, causing the model to adopt conservative reasoning strategies and reduce further exploration. In the RLVR procedure, which is low-information setting [11, 36], the loss of the models exploration capability implies that further training is unlikely to yield performance improvements [20, 63, 74], limiting the scaling process. Existing studies [38] have discussed the amount of information provided to the model by different training methods, noting that RLVR can only supply less information than the distillation process. Following this theory, to increase the amount of information available during RLVR training, one can introduce distilled data or manually annotated data. We consider whether decomposing the original problem into subproblems and providing them as additional information to the model can further raise its performance upper bound, since we observe that some prior work suggests that planning then reasoning can achieve better performance [21, 43]. To verify this hypothesis, we conduct the empirical study and present the results in Fig. 1. The model achieves higher Pass@k scores with subquestion-based prompts, indicating that the model already possesses the requisite knowledge to solve the task and can effectively integrate this knowledge to reach solution when guided appropriately. This phenomenon suggests that the model lacks compositional generalization, i.e., the ability to combine simple knowledge components to solve complex problems [5, 37], demonstrating the effectiveness of incorporating additional information into RLVR and sub-question guidance can serve as one of the effective forms of additional information. To provide additional information to the RLVR procedure, prior work (e.g., LUFFY [67] and Scaf-GRPO [77] has guided the model to perform more effective exploration during RLVR training by introducing data generated by powerful model (e.g., DeepSeek-R1) as prompts, including knowledge point, planning, and solution. These methods rely on stronger models to annotate data, which on one hand incurs higher costs, and on the other hand, they remain within the paradigm of imitation learning, making it difficult for the student model to surpass the upper bound of the teacher model [52, 60]. According to the above discussion, we consider whether it is possible to start from single model and, by employing different tasks and RLVR training methods, derive hint model and reasoning model. Based on given problem, the hint model can supply additional information to assist the reasoning model in performing more effective exploration. In this setting, the decomposer and reasoner trained from the same model can mutually reinforce each other, thereby enabling model self-evolution and surpassing its reasoning performance limits. Building on this idea, in this paper, we propose an Adaptive Ability Decomposing method for enhancing the effectiveness of RLVR, named as A2D. Concretely, we first utilize RLVR to train decomposer that can serve as the hint model and decompose the original questions into simple sub-questions, providing additional information for training the reasoning model, since decomposing problem can help the model solve the complex original tasks [46, 79]. Next, differing from earlier studies that simply insert prompt-generated answers into the training dataset [67, 77], we utilized in-context distillation loss (IDL) to guide the model to learn from the experiences obtained through sub-question-guided exploration. This makes our algorithm plug-and-play component, capable of adapting to various RLVR algorithms. In summary, the major contributions of our work can be summarized as follows: To provide more information without relying on external models, we propose A2D, training decomposer and reasoner starting from the same model. The decomposer is used to decompose questions, and the decomposed sub-questions serve as additional information to assist the reasoner during RLVR. We evaluate and compare our A2D with competitive baselines on eight mathematical tasks to validate the effectiveness. With the assistance of sub-question guidance from the decomposer, the reasoner is able to explore more effectively during various RLVR algorithms and achieve better performance. We analyze the decomposer and its training process, revealing how different methods affect its performance and behavior. We also find that specific and fine-grained hints help improve the reasoners Pass@1 score, while abstract and coarse-grained hints can enhance the reasoners Pass@k score."
        },
        {
            "title": "2 Approach",
            "content": "In this section, we first review the popular RLVR process (Sec. 2.1). Next, we introduce our approach A2D, including decomposer training (Sec. 2.2) and RLVR with guidance (Sec. 2.3). Concretely, in A2D, we first utilize reinforcement learning to train decomposer ğœ‹ ğœƒD , which can decompose the complex question into several sub-questions that LLMs can easily solve. Next, we leverage it to generate the sub-questions of each question from the training dataset as the corresponding guidance. After that, the guidance can be employed to improve the effectiveness of the RLVR process to train reasoner ğœ‹ ğœƒR , which can solve the downstream tasks better. We present the overview of A2D in Fig. 2."
        },
        {
            "title": "2.1 Preliminary: Reinforcement Learning with Verifiable Reward",
            "content": "ğ‘–=1 Reinforcement learning with verifiable reward (RLVR) is popular algorithm to further enhance the reasoning abilities of LLMs [18, 30, 58], e.g., GRPO [50], RLOO [1], and REINFORCE++ [27]. In the RLVR training dataset = {(ğ‘¥ğ‘–, ğ‘¦ğ‘–)}ğ‘› , the ğ‘–-th training instance contains question description ğ‘¥ğ‘– and ground truth answer ğ‘¦ğ‘–. Give the question ğ‘¥, policy with the parameter ğœƒ (denoted as ğœ‹ ğœƒ ) is required to generate solution with final answer Ë†ğ‘¦. Once the predicted answer Ë†ğ‘¦ is obtained, verifier will be utilized to verify its correctness and provide the reward ğ‘Ÿ based on the ground truth answer ğ‘¦. When the predicted answer is correct, the verifier provides positive reward ğ‘…pos, while it provides negative reward ğ‘…neg for other situations. Based on question ğ‘¥, generated solution and answer Ë†ğ‘¦, and reward ğ‘Ÿ from the verifier, following the process of GRPO, the objective function of the RLVR can be formulated as follows, (ğœƒ) = ( ğ‘¥,ğ‘¦)D,{Ë†ğ‘¦ }ğº ğ‘–=1 ğœ‹ğœƒ ( ğ‘¥) ğº ğ‘–=1 1 Ë†ğ‘¦ğ‘– Ë†ğ‘¦ğ‘– ğ‘¡=1 (cid:16) min 1 ğº ğ‘Ÿğ‘–,ğ‘¡ Ë†ğ´ğ‘–,ğ‘¡ , clip (cid:0)ğ‘Ÿğ‘–,ğ‘¡ , 1 ğœ€, 1 + ğœ€(cid:1) Ë†ğ´ğ‘–,ğ‘¡ ğ›½ğ·KL , (cid:17) (1) where ğ‘Ÿğ‘¡ and Ë†ğ´ğ‘¡ refer to the importance sampling coefficient and estimated advantage values, and ğº denotes the number of generated responses based on question ğ‘¥. In GRPO, the average value and variance of the rewards in the same group are leveraged to estimate the advantage value as the following equation, Ë†ğ´ğ‘–,1 = . . . , Ë†ğ´ğ‘–, ğ‘¦ğ‘– = Ë†ğ´ğ‘– = ğ‘…ğ‘– mean(ğ‘…1, . . . , ğ‘…ğº) std(ğ‘…1, . . . , ğ‘…ğº) , (2) where mean(ğ‘…1, . . . , ğ‘…ğº) and std(ğ‘…1, . . . , ğ‘…ğº) denote the average value and variance of the rewards in the corresponding group, respectively. Moreover, each token in response Ë†ğ‘¦ğ‘– will receive the same advantage Ë†ğ´ğ‘–."
        },
        {
            "title": "2.2 Training Decomposer via RLVR",
            "content": "Given the large consumption of data annotating for the SFT process of the decomposer, we consider utilizing RLVR to guide the LLMs to learn to decompose the complex question through self-exploration. In the following, we first introduce the construction of the training dataset of the decomposer ğœ‹ ğœƒD , and then present the details of the RLVR process. 3 Figure 2 The comparison between vanilla RLVR and our approach A2D. In A2D, we first train decomposer ğœ‹ ğœƒD through RLVR, guided by the format reward ğ‘…F and quality reward ğ‘…Q. Next, we use this decomposer to annotate sub-questions for each question in the training dataset, and design the in-context distillation loss (IDL), i.e., Eq. 5, to integrate the sub-question guidance into the RLVR process for reasoner ğœ‹ ğœƒR . To better demonstrate the pipeline of our A2D, we present the pseudo code in Algorithm 1. Training Dataset Construction. We collect the question and its corresponding answer to construct an instance of the training dataset. Formally, we construct training dataset = {ğ‘¥ğ‘–, ğ‘¦ğ‘–}ğ‘› , where ğ‘¥ğ‘–, ğ‘¦ğ‘–, and ğ‘› denote the question, the corresponding ground truth answer, and the number of training data. As discussed in previous work [45, 70], decomposing the original question is essentially planning process, during which the model possesses greater capacity to explore potential solution strategies. Under this assumption, we think that the trained decomposer can provide useful guidance for the subsequent training of the reasoner, even though no additional information is introduced throughout the entire training process. ğ‘–=1 RLVR Training Procedure. To effectively train the Decomposer, we design multi-dimensional reward function, including both quality reward ğ‘…Q and format reward ğ‘…F. Concretely, to evaluate the quality of the sub-questions generated from decomposer, we conduct proxy reasoner to generate the solution for ğ‘›proxy times based on the original question and the decomposed sub-questions. The quelity reward is set to 1 if and only if at least one of the ğ‘›proxy attempts is correct (i.e., ğ‘…Q = 1). Otherwise, the quality reward is set to 0 (i.e., ğ‘…Q = 0). Moreover, for the format reward, we examine whether the responses from the decomposer satisfy the required format, i.e., whether it uses <subquestion> to indicate the beginning of the decomposed sub-questions and whether the corresponding content is non-empty (The details can be found in Appendix B). The format reward will be set to 1 when and only when the response passes the format examination (i.e., ğ‘…F = 1), while it will be set to 0 in other situations (i.e., ğ‘…F = 0). After obtaining the quality reward and format reward, we compute the final reward for response from the decomposer by multiplying ğ‘…Q and ğ‘…F, i.e., ğ‘… = ğ‘…Q ğ‘…F. With this reward design, we encourage the model to produce responses in the correct format and to assist the proxy reasoner in generating correct answers, thereby reducing the likelihood of reward hacking. In practice, we utilize GRPO [50] to train the decomposer based on the above reward function."
        },
        {
            "title": "2.3 Improving RLVR Effectiveness with Sub-question Guidance\nIn this part, we present the training details of the reasoner ğœ‹ ğœƒR\n. First, to provide richer guidance for RLVR\ntraining, we use the trained decomposer to annotate the training data, supplying sub-question hints for each\noriginal question, before the beginning of the reasoner RLVR process. We then introduce an In-Context",
            "content": "4 Distillation NLL Loss to guide the models exploration using the sub-question hints. After that, we provide the methods to integrate this loss function into the RLVR algorithms. Annotating Training Data with Sub-question Hint. In Sec. 2.2, we introduce the approach to train decomposer only with the dataset containing questions and corresponding answers = {ğ‘¥ğ‘–, ğ‘¦ğ‘–}ğ‘› . This dataset will also be utilized in the RLVR process of the reasoner. To provide additional guidance for the training procedure of the reasoner, we use the decomposer to decomposer each question in the training dataset and annotate it with list of its potential sub-questions. Formally, given the question ğ‘¥ğ‘–, the decomposer will decompose this question into several sub-questions Sğ‘– = {ğ‘ ğ‘–,1, . . . , ğ‘ ğ‘–, ğ‘ }, and construct them into triples ğ‘¥ğ‘–, ğ‘¦ğ‘–, Sğ‘–. After annotating all questions in the dataset D, we construct the training dataset with sub-question hint using the above triples, i.e., DSQ = {ğ‘¥ğ‘–, ğ‘¦ğ‘–, Sğ‘–}ğ‘› . These sub-questions can be viewed as an idea of solving the corresponding question. After the reasoner sequentially addresses these sub-questions, it is likely to arrive at the solution to the original problem. ğ‘–=1 ğ‘–=1 In-context Distillation Loss. In this part, we introduce in-context distillation loss (IDL) to enhance the models capabilities after RLVR training with the guidance of the sub-question hint. IDL can be regarded as plug-and-play component that is compatible with various RLVR algorithms. Specifically, given the question ğ‘¥ğ‘– and its decomposed sub-questions Sğ‘–, the reasoner generates multiple solutions Ë†Y = {Ë†ğ‘¦ }. With the guidance of sub-questions Sğ‘–, the generated solutions Ë†Y are more likely to contain the correct answer. After generation, we verify the correctness of each response in Ë†Y, obtaining the corresponding rewards }. Based on the rewards of the solutions, we can select the positive solutions to fine-tune {ğ‘… the parameters of the reasoner ğœ‹ ğœƒR . Since no sub-question hints are available during real testing, we train the model to generate solutions directly from the question, aiming to internalize the correct problem-solving strategies into the inner knowledge of the reasoner. The loss function can be formulated as follows, , . . . , Ë†ğ‘¦ , . . . , ğ‘… ğ‘–,ğ‘›rollout ğ‘–,ğ‘›rollout ğ‘–,1 ğ‘–, LIDL (ğœƒR) = 1 ğ‘pos ğ‘›rollout ğ‘—=1 ğœ‹ ğœƒR (Ë†ğ‘¦ ğ‘–, ğ‘— ğ‘¥ğ‘–) I[ğ‘… ğ‘–, ğ‘— = 1], ğ‘pos = ğ‘›rollout ğ‘—=1 I[ğ‘… ğ‘–, ğ‘— = 1], (3) ğ‘–, ğ‘— = 1] is an indicator function, which returns 1 when and only when ğ‘… where I[ğ‘… ğ‘–, ğ‘— = 1 and returns 0 for other situations. By optimizing the reasoners parameters ğœƒR to minimize the loss function LIDL, the reasoner ğœ‹ ğœƒR can learn more from these hints, enabling it to surpass its prior performance boundary. Integrating IDL into RLVR Procedure. Following the process of vanilla RLVR training, for given problem, we first have the reasoner generate multiple solutions Ë†Y = {Ë†ğ‘¦ğ‘–,1, . . . , Ë†ğ‘¦ğ‘–,ğ‘›rollout }, and then compute the reward for each solution as well as the average reward ğ‘…. For integrating the IDL function into the RLVR training process, the purpose is to provide auxiliary guidance for questions on which the reasoner is unable to obtain correct answers through the rollout process. Following this motivation, we preset threshold ğ‘˜1 (0 < ğ‘˜1 < 1) such that when the average reward falls below this threshold (i.e., ğ‘˜1 > ğ‘…), the IDL is applied. Otherwise, the IDL is not activated. Moreover, since the reasoner ğœ‹ ğœƒR is likely to follow the guidance to solve the question, the generated responses might show high similarity to each other. Thus, to prevent overfitting, we collect the positive response generated under the sub-question guidance Ë†Y, and retain at most ğ‘˜2 ğ‘›rollout (0 < ğ‘˜2 < 1) positive responses. Formally, the set of the selected positive responses can be denoted as Ë†Y Sel = {Ë†ğ‘¦ğ‘–,1,Sel, . . . , Ë†ğ‘¦ğ‘–,ğ‘,Sel}, ğ‘ = min(ğ‘pos, ğ‘˜2 ğ‘›rollout). (4) Furthermore, we observe that several generated solutions in Ë†Y are not rigorous, as they might fail to consider Sel the scenarios not covered in the hints, thereby performing imprecise reasoning. In this situation, directly fine-tuning the parameters of the reasoner on these imprecise data will hurt its performance. Therefore, we appended different prompts to the original problem and used these prompts to segment the models capabilities, as mentioned in previous work [25, 26, 51]. The following formula presents the adjusted IDL, IDL (ğœƒR) = 1 Ë†Y Sel Ë†Y Sel ğ‘—=1 ğœ‹ ğœƒR (Ë†ğ‘¦ ğ‘–, ğ‘—,Selğ‘¥ğ‘–, ğ‘). (5) Based on the above discussion, we can rewrite the function of IDL and integrate it into the training process of RLVR, which can be formulated as follows, where (ğœƒR) is the objective function of RLVR (Eq. 1), and ğ›¼ is hyperparameter to balance the effect of the above two objective functions. LA2D (ğœƒR) = (ğœƒR) + ğ›¼L IDL (ğœƒR) I[ğ‘˜1 > ğ‘…], (6)"
        },
        {
            "title": "3 Experiments",
            "content": "In this section, we conduct the experiment to evaluate the effectiveness of our approach and further analyze its features. First, we introduce the details of the experimental settings (Sec. 3.1). Next, we present the results of the evaluation process (Sec. 3.2). Finally, we present the detailed analysis of the feature of our approach (Sec. 3.3)."
        },
        {
            "title": "3.1 Experimental Settings",
            "content": "In this part, we introduce the datasets that are utilized in the training and evaluation process, the baseline approaches, and the implementation details of our experiments. Datasets. We utilize the dataset proposed in previous work [15] as the training dataset for the decomposer and reasoner. To perform the evaluation, we adopt eight mathematical benchmarks as the downstream tasks, including AIME24 [2], AIME25 [3], AMC23 [4], BeyondAIME [9], MATH500 [35], Minerva [40], OlymMATH-Easy [56], and OlymMATH-Hard [56]. Baselines. To assess the effectiveness of our approach, we conduct several popular approaches as the baselines in our evaluation, including SFT-based and RLVR-based methods. For SFT-based methods, we utilize seed-1.6 (no_thinking mode) [10] to generate the solution and leverage the generated instances to perform SFT on the backbone models (i.e., SFT w/ CoT). Besides, given the sub-questions generated by the decomposer mentioned in our approach (Sec. 2.2), we combine these generated sub-questions and the solutions mentioned above to conduct the SFT process (i.e., SFT w/ CoT & SQ). For RLVR-based methods, we compare the performance of our approach with the previous work, i.e., LUFFY [67] and Scaf-GRPO [77]. Since these two approaches require an external teacher model to provide guidance, to conduct fair comparison, we utilize our well-trained decomposer mentioned in Sec. 2.2 to provide the guidance. Implementation Details. We adopt the Qwen2.5-7B-Instruct as the backbone model of the decomposer and employ GRPO [50] as the backbone RLVR algorithm for the whole training process. To enhance the effectiveness, we follow the previous work [72] to set the clip ratio ğœ€low = 0.2 and ğœ€high = 0.28. Moreover, we set the max_prompt_length and max_response_length as 2048 and 6144, and set the train_batch_size and mini_ppo_batch_size as 128 and 32, respectively. For each question, we perform 32 times rollout process to conduct the experience for RLVR training, and utilize 1.0 and 1.0 as the temperature and top_p. For the evaluation procedure, we adopt the temperature as 1.0 and top_p as 1.0 for the decoding process. Besides, to reduce the influence of random values, we repeat the experiment 8 times and report the average accuracy."
        },
        {
            "title": "3.2 Main Results",
            "content": "For more comprehensive evaluation, we use four backbone models, train them with our method and baseline approaches, and present the evaluation results in Table 1. First, from the experimental results, we observe that our method A2D consistently yields substantial improvements across different backbone models and achieves superior performance on mathematical reasoning tasks. Compared with both the SFT-based (i.e., SFT w/ CoT and SFT w/ CoT & SQ) and RLVR-based (i.e., GRPO, LUFFY, and Scaf-GRPO) baseline approaches, the models trained with our method also surpass these baselines and achieve better performance. The experimental results demonstrate that our method is effective across models of different families, scales, and capability levels. 6 Table 1 Accuracy of different training approaches on mathematical tasks. BeAIME, OMATH-E, and OMATH-H denote BeyondAIME, OlymMATH-Easy, and OlymMATH-Hard, respectively. Avg. refers to the average accuracy of all tasks. The best is in bold and the second best is underlined. AIME24 AIME25 AMC23 BeAIME MATH500 Minerva OMATH-E OMATH-H Avg. Baseline + GRPO + GRPO w/ ğ‘›rollout = 64 + SFT w/ CoT + SFT w/ CoT & SQ + LUFFY + Scaf-GRPO + A2D (Ours) Baseline + GRPO + GRPO w/ ğ‘›rollout = 64 + SFT w/ CoT + SFT w/ CoT & SQ + LUFFY + Scaf-GRPO + A2D (Ours) Baseline + GRPO + A2D (Ours) Baseline + GRPO + A2D (Ours) 9.6 15.0 15.0 7.1 5.8 10.4 14.6 20.0 1.7 11.3 10.8 2.1 3.8 1.7 11.3 13.8 10.8 10.4 11. 2.5 7.1 8.3 6.3 8.3 6.7 2.9 1.3 8.8 9.6 17.1 0.0 0.8 1.7 1.7 0.4 0.0 0.8 2.1 6.7 8.8 11.3 0.0 0.0 1.3 Qwen2.5-7B-Instruct 3.4 47.5 5.3 52.8 5.4 51.9 2.8 41.3 2.5 40.0 3.9 47.2 5.5 54.7 53.4 6. 70.8 73.3 73.9 63.6 62.7 69.2 74.5 75.6 LLaMA3.2-3B-Instruct 0.1 0.6 1.1 0.5 0.8 0.6 0.8 2.0 10.9 29.4 30.0 22.5 21.9 9.7 30.6 30.9 26.8 52.5 52.5 41.5 41.6 26.4 51.2 52.6 Qwen2.5-Math-7B-Instruct 78.5 6.4 79.7 8.3 80.3 9.0 56.3 58.1 58. LLaMA3.1-8B-Instruct 0.1 0.8 1.4 13.8 28.8 30.6 33.3 50.3 50.6 22.9 24.8 25.3 24.4 23.3 22.4 25.1 28.9 6.6 13.4 14.6 9.6 9.9 6.6 15.3 15.3 23.4 24.6 25. 10.9 19.6 19.9 4.8 4.6 4.6 2.9 4.3 4.4 5.6 8.4 1.0 2.5 2.8 2.1 1.5 1.6 2.6 3.1 5.9 8.7 8.4 0.4 1.6 1.6 2.1 2.9 3.0 1.4 1.4 2.8 2.8 1. 0.4 1.3 1.3 0.6 1.0 1.1 0.9 1.6 2.0 2.0 3.0 0.6 1.0 1.6 20.9 23.4 23.2 18.3 17.7 21.1 24.1 26.5 5.9 14.0 14.4 10.1 10.1 6.0 14.2 15.2 23.8 25.1 26. 7.7 13.7 14.4 Second, our method A2D demonstrates the robustness across various models. During the training of the Decomposer, we use Qwen2.5-7B-Instruct as the proxy reasoner. Once the Decomposer is trained, its decomposed sub-questions can assist not only the RLVR training of Qwen2.5-7B-Instruct but also that of Qwen2.5-Math-7B-Instruct and other models from the LLaMA family, yielding performance improvements for these models. This indicates that the trained Decomposer can guide the training of various models without requiring specific training for different reasoners, demonstrating that our method A2D is both efficient and effective. Third, comparing the two SFT-based training methods, we find that adding decomposer-generated subquestions to the training data does not improve model performance. Instead, it even leads to performance degradation in certain scenarios, e.g., AIME25 and AMC23. The underlying reasons are twofold. First, learning to decompose the original question may not directly aid in solving it, as these are two dissimilar abilities. Second, although the decomposer-generated sub-questions can stimulate exploration that occasionally leads to the correct answer, they do not necessarily represent complete reasoning process and may overlook certain cases. For these reasons, directly learning the decomposer-generated sub-questions does not contribute to improving the models reasoning ability. Our method effectively avoids this issue by leveraging the decomposed sub-questions through the IDL function, thereby providing richer guidance for the RLVR process. Finally, directly training the model using teacher-generated solutions that do not contain the thinking process (i.e., SFT w/ CoT and LUFFY) is unlikely to yield improvements and may even impair the models reasoning ability. possible explanation for this phenomenon is that these backbone models have already learned substantial amount of instructions during post-training. Further training on such data may lead to overfitting, 7 Table 2 Results of the ablation study. We first present the ablation study about the training process of the decomposer, and then provide the analysis of the effectiveness of each module in our approach. The best is in bold. AIME24 AIME25 MATH500 Minerva Avg. Pass@k Performance of the Decomposer A2D (Ours) w/o Format Reward w/ Pass@k Reward w/o Format Reward w/ Pass@1 Reward w/o RLVR 20.8 15.8 16.3 17.1 18.3 16.7 14.9 13. Pass@1 Performance of the Reasoner A2D (Ours) w/o Removing Guidance w/o Selection w/o Selection w/o Diversity Prompt w/o Sub-question Guidance 20.0 15.0 16.3 13.3 15.0 17.1 9.6 11.7 10.0 8.3 82.9 83.2 81.5 78. 75.6 74.5 73.7 69.6 73.3 30.7 30.4 28.5 28.1 28.9 25.0 26.6 22.6 24.8 38.2 36.5 35.3 34.2 35.4 31.0 32.1 28.9 30.4 thereby degrading their performance on the test set. Compared to SFT with CoT, LUFFY introduces data distilled from the teacher model only when the model struggles to answer question, reinforcing knowledge that the model has not yet mastered and thereby partially alleviating the overfitting caused by learning from teacher data. However, this approach requires the introduction of an external model and relies on the teacher model possessing strong capabilities to produce correct and complete reasoning processes."
        },
        {
            "title": "3.3 Detailed Analysis",
            "content": "To further understand the feature of our approach, in this part, we present the detailed analysis, including the ablation study (Sec. 3.3.1), the effectiveness of integrating A2D into different RLVR algorithms (Sec. 3.3.2), effects of different utilization for the generated sub-questions (Sec. 3.3.3), the statistical analysis of the subquestions from the decomposer (Sec. 3.3.4), and the case study of the generated sub-questions (Appendix 3.3.5)."
        },
        {
            "title": "3.3.1 Ablation Study",
            "content": "To assess the effectiveness of each module in the training process of A2D, we conduct the ablation study and present the results in Table 2. Ablation Study about Training Process of Decomposer. To evaluate the capability of the decomposer, we concatenate the sub-questions that it generates with the original question as combined prompt, and then ask the LLM (i.e., Qwen2.5-7B-Instruct) to produce an answer. We report the Pass@k score in the top part of Table 2. Based on the evaluation results, the Format Reward and Pass@k Reward both contribute to the enhancement of the capacities of decomposer. Format Reward teachs decomposer to generate formal list of subproblems, and Pass@k Reward helps it to present the sub-problems that are more likely to guide the reasoner to arrive at the correct answer. By training with the above two rewards, the decomposer can better assist the subsequent RLVR training process of the reasoner, thereby improving the training effect. Ablation Study about RLVR with Sub-question Guidance. In the bottom part of the Table 2, we present the results of removing Selection and Deversirty Prompt mechanism of our approach. We can observe the decrease of the models performance without these two modules. For problems that the model is already capable of solving through its own exploratory reasoning, it is unnecessary to introduce additional external guidance, so as to avoid potential conflicts between such guidance and the models inherent problem-solving trajectory. For problems that require external guidance, training with diverse prompts can mitigate the influence of external knowledge on the model, thereby improving its overall performance. 8 Table 3 Accuracy of Qwen2.5-7B-Instruct trained through different approaches on mathematical tasks. BeAIME, OMATH-E, and OMATH-H denote BeyondAIME, OlymMATH-Easy, and OlymMATH-Hard, respectively. Avg. refers to the average accuracy of all tasks. The best is in bold. AIME24 AIME25 AMC23 BeAIME MATH500 Minerva OMATH-E OMATH-H Avg. Baseline + GRPO + A2D (Ours) + RLOO + A2D (Ours) + REINFORCE++ + A2D (Ours) 9.6 15.0 20. 12.9 17.9 14.2 14.6 6.3 8.3 17.1 7.1 12.5 8.3 10. 47.5 52.8 53.4 52.5 55.0 50.6 52.8 3.4 5.3 6. 4.6 5.8 3.6 5.6 70.8 73.3 75.6 74.9 75.2 73.6 72. 22.9 24.8 28.9 25.2 26.8 26.1 26.7 4.8 4.6 8. 5.9 6.4 3.6 4.9 2.1 2.9 1.8 2.8 2.3 2.1 2. 20.9 23.4 26.5 23.2 25.2 22.8 23.8 (a) Pass@1 Performance without Training. (b) AIME24 Performance of AIME24 during GRPO. Figure 3 Performance comparison of Qwen2.5-7B-Instruct of whether sub-questions are included in the prompt."
        },
        {
            "title": "3.3.2 Adaptation to Other RLVR Algorithms",
            "content": "Our method introduces the IDL function to guide the model in leveraging hints during RLVR training to further enhance its capabilities. Theoretically, our approach is orthogonal to existing RLVR methods and can be combined with other RLVR algorithms to improve their training effectiveness. To further validate our approach experimentally, we collected commonly used RLVR training algorithms (e.g., GRPO [50], RLOO [1], and REINFORCE++ [27]) and combined A2D with these methods. The results are presented in Table 3. From the experimental results, we observe that traditional RLVR algorithms can enhance the models reasoning ability. Building on these algorithms, our A2D can further improve the models capabilities and achieve better performance on downstream tasks. For more challenging tasks (e.g., AIME25 and BeyondAIME), vanilla RLVR struggles to yield improvements, as the models limited capabilities make it difficult to discover correct solutions through self-exploration and thus produce accurate answers. In such cases, the decomposers breakdown of the original problem can provide the reasoner with novel problem-solving strategies, extending its capability boundaries and enabling it to tackle these more complex tasks during the RLVR training process. Through our analysis, we find that A2D can be adapted to various RLVR algorithms, effectively enhancing the models capabilities."
        },
        {
            "title": "3.3.3 Effect of Concatenating Decomposed Sub-questions into Prompt",
            "content": "As mentioned in Sec. 2.2, we can train decomposer through RLVR, enabling it to break down the original question into simpler sub-questions that help the reasoner perform more effective reasoning. In fact, the trained decomposer can not only be used to decompose questions in the training set, but also to add sub-question hints to test-time queries, thereby assisting the model in solving downstream tasks. To evaluate the effectiveness of 9 Table 4 The statistical information of the generated sub-questions from A2D. AIME24 AIME25 MATH500 Minerva Average value Standard deviation Maximum value Minimum value Number of Sub-questions 2.07 0.25 3.00 2.00 2.1 0.4 4.00 2.00 2.02 0.15 3.00 1.00 Average value Standard deviation Maximum value Minimum value Number of Tokens of Generated Sub-questions 50.08 15.99 130.00 26.00 58.33 22.19 154.00 36. 60.37 23.93 115.00 31.00 Whether contain boxed{} Whether contain Answer: Whether contain Answer is Contained Content 1.99 0.28 5.00 1. 53.01 17.33 167.00 16.00 different methods, we conduct experiments and present the results in Fig. 3b. In Fig. 3a, we observe that adding sub-questions generated by the decomposer into the prompt (i.e., Prompt w/ SQ) leads to improvements in both the Pass@1 score and the Pass@k score. This phenomenon indicates that decomposing the original problem can guide large models to explore and reason more effectively, resulting in improved performance. This also serves as one of the key motivations behind our method. Moreover, we added sub-question hints to both the training and test sets and trained the reasoner using GRPO on this augmented data i.e., GRPO + Prompt w/ SQ. In Fig. 3b, we compare its performance with our approach (i.e., GRPO + A2D). We observe that in the early stages of training (first 100 steps), GRPO + Prompt w/ SQ achieves better performance. This is because the sub-question hints can directly provide the reasoner with solution strategies, leading to improved reasoning performance. However, as training progresses, the model becomes overly reliant on the sub-question hints provided by the decomposer for reasoning, which diminishes its exploratory capability and prevents further improvement in reasoning performance. In contrast, during the training of A2D, the model uses hints only when the question is sufficiently difficult, while relying on self-exploration in all other cases. As result, A2D does not lead to rapid decline in the models exploratory ability and enables the model to achieve better performance on downstream tasks through RLVR."
        },
        {
            "title": "3.3.4 Statistical Analysis of the Generated Sub-questions from A2D",
            "content": "To further understand the feature of the decomposer, we collect the generated sub-questions from decomposer for four dataset, i.e., AIME24, AIME25, MATH500, and Minerva, and present the statistical information about the generated sub-questions in Table 4. First, by examining the number of sub-questions produced by the decomposer and the number of generated tokens, we observe that both remain at relatively low levels. Specifically, on average, question can be decomposed into two sub-questions, and describing these sub-questions requires only about 60 tokens. This phenomenon indicates that the sub-questions produced by the decomposer operate at relatively high level of abstraction and can be regarded as coarse-grained reasoning process rather than step-by-step solution. Moreover, we find that the responses generated by the decomposer do not contain keywords related to the final answer, indicating that the decomposer provides problem-solving guidance without revealing the solution itself, thereby not constraining the reasoners reasoning and exploratory process. Such decomposer can provide the reasoner with guidance and inspiration while retaining certain degree of exploratory capability, which is precisely what we aim for. The decomposer does not engage in overly detailed reasoning that would excessively reinforce exploitative behavior and consequently diminish its exploratory capacity."
        },
        {
            "title": "3.3.5 Case Study of the Generated Sub-questions from Different Models",
            "content": "Figure 4 An example of the generated sub-question from models trained through different methods. In Fig. 4, we present the case study of the generated sub-questions of different models, i.e., our method (i.e., A2D) and the approach that removes the Format Reward while using the Pass@1 Reward to replace the Pass@k Reward (i.e., A2D w/o Format Reward w/ Pss@1 Reward). The example problem involves sets and requires knowledge of intersections and unions. The model trained with our method decomposes this problem into two sub-questions: first, it identifies that the problem involves sets, guiding the reasoner to select relevant knowledge for solving the problem; then, it guides the reasoner to apply this knowledge to the specific question. The Decomposer performs relatively coarse-grained decomposition of the problem, which can inspire the reasoner to explore while preventing over-reliance on the hints. This phenomenon reflects the effect of using the Pass@k Reward during training. In contrast, the model trained by removing the Format Reward and using the Pass@1 Reward instead of the Pass@k Reward produces more concrete content, with the sub-questions degenerating into step-by-step reasoning solutions. This phenomenon indicates that high-level, coarse-grained hints can guide the models exploration, thereby improving its Pass@k performance. In contrast, fine-grained hints better enhance the models exploitative capability, leading to higher Pass@1 performance. The analysis of the models generated content validates the rationale of our training method."
        },
        {
            "title": "4 Related Work",
            "content": "4.1 Improving Exploration Ability of LLMs via Value-based Guidance RLVR is popular approach to achieve test-time scaling, which utilizes reward scores to supervise LRMs [18, 30, 58]. During the RLVR process, the reward design [7, 55], advantage shaping [16, 17, 19], and sampling mechanism [62, 81] are leveraged to unlock the potential of LRMs, balancing their exploration and exploitation that guides them to find the global optimal. In addition to enabling the model to perform exploration via one-step generation, previous works [8, 12, 31, 70] have employed reward models to provide supervision signals for reasoning steps, thereby facilitating more robust reasoning. Besides, existing work [75] also adopts outcome-level reward models to guide the reasoning processes of the models. Given that models can effectively solve simple problems but struggle with multi-hop reasoning tasks [44, 66, 77], this work focuses on how to guide the model to transfer its ability to solve subproblems toward solving more complex problems, thereby enhancing the effectiveness of RLVR. In principle, our approach is orthogonal to previous work and can be effectively combined with those methods."
        },
        {
            "title": "4.2 Test-time Scaling via Natural Language Guidance",
            "content": "Beyond the score-based supervision, natural language guidance is also leveraged to enhance the reasoning performance of LRMs. Supervised finetuning (SFT) is widely used on LRMs to elicit their inner knowledge and improve their exploration ability [18, 59]. The training instances that contains the complex actions for the SFT process are usually distilled from the powerful teacher models [23, 29], e.g., GPT-5 or Gemini-2.5-pro. Moreover, previous studies have shown that responses from the teacher model [67, 77] and self-reflection [32, 49] can be used to elicit the reasoning ability of LRMs, in the RLVR process. Besides, parallel reasoning enhances the performance of LRMs by enabling the model to explore multiple reasoning paths and obtain the final answer based on the previous generation [39, 47, 53], which is an alternative way of leveraging natural language guidance. In this work, we investigate the use of the tips (i.e., the sub-problems derived from the decomposition of the original problem) during training to aid the model in accurately solving these problems and to activate its internal knowledge."
        },
        {
            "title": "4.3 Compositional Generalization Ability of LLMs",
            "content": "Compositional generalization ability refers to the ability to combine known simple knowledge to solve more complex problems [5, 37]. However, the compositional generalization ability of LLMs is limited [6, 48, 68]. Specifically, the model has acquired some basic knowledge, but it still cannot combine these related pieces of knowledge to solve more complex tasks, e.g., puzzle tasks [13, 16] and question answering tasks [34, 69]. To improve such an ability of LLMs, previous studies usually adopt several simple problems to synthesize the hard multi-hop problems and utilize these problems to perform training process [57, 73], or utilize multi-turns reasoning solve the complex problems step-by-step [33, 80]. The compositional generalization ability is vital for the self-evolution of LLMs [48, 68], such as RLVR, where LLMs should explore the potential solutions to the given question based on their knowledge and learn from these experiences [76, 77]. Inspired by this, in this work, we focus on teaching LLMs to combine the knowledge it has learned to answer complex questions in RLVR, which can increase the efficiency of the models exploration, thereby improving the training effectiveness."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we proposed A2D, adaptive ability decomposing for enhancing the effectiveness of the RLVR process. Concretely, we first trained decomposer to decompose the complex questions into several subquestions, and then utilized these generated sub-questions to assist the RLVR procedure of the reasoner. With the additional information, the reasoner can explore more effectively and further enhance its reasoning ability during the RLVR process. To better understand the features of the decomposer, we conducted detailed analysis of its training approach and the behaviours after training. Through our experiments, we find that abstract and coarse-grained hints, e.g., sub-question guidance, are beneficial for enhancing the models exploration ability, whereas concrete and fine-grained prompts are more effective for improving the models exploitation ability. Our work investigated how to guide the model toward more effective exploration. 12 In future work, the compatibility between the decomposer and the reasoner is promising direction for future research. As the reasoners capability improves, the style and granularity of the hints provided by the decomposer should also evolve accordingly. Furthermore, whether our algorithm can be extended to online settings is also an important question for future exploration. While AI assistants help humans solve problems, they must also continuously evolve and improve based on real feedback provided by users."
        },
        {
            "title": "References",
            "content": "[1] Arash Ahmadian, Chris Cremer, Matthias GallÃ©, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet ÃœstÃ¼n, and Sara Hooker. Back to basics: Revisiting reinforce-style optimization for learning from human feedback in llms. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 1224812267. Association for Computational Linguistics, 2024. [2] AIME2024. Aime2024, 2024. URL https://huggingface.co/datasets/HuggingFaceH4/aime_2024. [3] AIME2025. Aime2025, 2025. URL https://huggingface.co/datasets/opencompass/AIME2025. [4] AMC2023. Amc2023, 2023. URL https://huggingface.co/datasets/math-ai/amc23. [5] Shengnan An, Zeqi Lin, Qiang Fu, Bei Chen, Nanning Zheng, Jian-Guang Lou, and Dongmei Zhang. How do in-context examples affect compositional generalization? In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 1102711052. Association for Computational Linguistics, 2023. [6] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 3948. IEEE Computer Society, 2016. doi: 10.1109/CVPR.2016.12. URL https://doi.org/10.1109/CVPR. 2016.12. [7] Charles Arnal, GaÃ«tan Narozniak, Vivien Cabannes, Yunhao Tang, Julia Kempe, and RÃ©mi Munos. Asymmetric REINFORCE for off-policy reinforcement learning: Balancing positive and negative rewards. CoRR, abs/2506.20520, 2025. doi: 10.48550/ARXIV.2506.20520. URL https://doi.org/10.48550/arXiv.2506.20520. [8] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoefler. Graph of thoughts: Solving elaborate problems with large language models. In Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada, pages 1768217690. AAAI Press, 2024. [9] ByteDance-Seed. Beyondaime: Advancing math reasoning evaluation beyond high school olympiads, 2025. URL https://huggingface.co/datasets/ByteDance-Seed/BeyondAIME. [10] ByteDance-Seed. Seed1.6 tech introduction, 2025. URL https://seed.bytedance.com/en/seed1_6. [11] Roger Creus Castanyer, Johan S. Obando-Ceron, Lu Li, Pierre-Luc Bacon, Glen Berseth, Aaron C. Courville, and Pablo Samuel Castro. Stable gradients for stable learning at scale in deep reinforcement learning. CoRR, abs/2506.15544, 2025. doi: 10.48550/ARXIV.2506.15544. URL https://doi.org/10.48550/arXiv.2506.15544. [12] Guoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan. Alphamath almost zero: Process supervision without process. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. [13] Jiangjie Chen, Qianyu He, Siyu Yuan, Aili Chen, Zhicheng Cai, Weinan Dai, Hongli Yu, Qiying Yu, Xuefeng Li, Jiaze Chen, Hao Zhou, and Mingxuan Wang. Enigmata: Scaling logical reasoning in large language models with synthetic verifiable puzzles. CoRR, abs/2505.19914, 2025. [14] Jiaze Chen, Tiantian Fan, Xin Liu, Lingjun Liu, Zhiqi Lin, Mingxuan Wang, Chengyi Wang, Xiangpeng Wei, Wenyuan Xu, Yufeng Yuan, Yu Yue, Lin Yan, Qiying Yu, Xiaochen Zuo, Chi Zhang, Ruofei Zhu, Zhecheng An, Zhihao Bai, Yu Bao, Xingyan Bin, Jiangjie Chen, Feng Chen, Hongmin Chen, Riwei Chen, Liangqiang Chen, Zixin Chen, Jinsong Chen, Siyan Chen, Kaiyuan Chen, Zhi Chen, Jin Chen, Jiecao Chen, Jinxin Chi, Weinan Dai, Ning Dai, Jiahui Dai, Shihan Dou, Yantao Du, Zhengyin Du, Jianhui Duan, Chen Dun, Ting-Han Fan, Jiazhan Feng, Junda Feng, Ziyuan Feng, Yuwei Fu, Wenqi Fu, Hanjie Fu, Hao Ge, Hongyi Guo, Mingji Han, Li Han, Wenhao Hao, Xintong Hao, Qianyu He, Jerry He, Feng He, Wen Heng, Zehua Hong, Qi Hou, Liang Hu, Shengding Hu, Nan Hu, Kai Hua, Qi Huang, Ziyue Huang, Hongzhi Huang, Zihao Huang, Ting Huang, Wenhao Huang, Wei Jia, Bin Jia, Xiaoying Jia, Yuhua Jiang, Haobin Jiang, Ziheng Jiang, Kaihua Jiang, Chengquan Jiang, Jianpeng Jiao, Xiaoran Jin, Xing Jin, Xunhao Lai, Zheng Li, Xiang Li, Liyi Li, Hongkai Li, Zheng Li, Shengxian Wan, Ya Wang, Yunshui Li, Chenggang Li, Niuniu Li, Siyu Li, Xi Li, Xiao Li, Aoyan Li, Yuntao Li, Nianning Liang, 14 and Xinnian Liang. Seed1.5-thinking: Advancing superb reasoning models with reinforcement learning. CoRR, abs/2504.13914, 2025. [15] Zhipeng Chen, Yingqian Min, Beichen Zhang, Jie Chen, Jinhao Jiang, Daixuan Cheng, Wayne Xin Zhao, Zheng Liu, Xu Miao, Yang Lu, Lei Fang, Zhongyuan Wang, and Ji-Rong Wen. An empirical study on eliciting and improving r1-like reasoning models. CoRR, abs/2503.04548, 2025. [16] Zhipeng Chen, Xiaobo Qin, Youbin Wu, Yue Ling, Qinghao Ye, Wayne Xin Zhao, and Guang Shi. Pass@k training for adaptively balancing exploration and exploitation of large reasoning models. CoRR, abs/2508.10751, 2025. [17] Daixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, and Furu Wei. Reasoning with exploration: An entropy perspective. CoRR, abs/2506.14758, 2025. [18] DeepSeek-AI. Deepseek-r1: abs/2501.12948, 2025. Incentivizing reasoning capability in llms via reinforcement learning. CoRR, [19] Jia Deng, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, and Ji-Rong Wen. Decomposing the entropy-performance exchange: The missing keys to unlocking effective reinforcement learning. CoRR, abs/2508.02260, 2025. [20] Yihong Dong, Xue Jiang, Yongding Tao, Huanyu Liu, Kechi Zhang, Lili Mou, Rongyu Cao, Yingwei Ma, Jue Chen, Binhua Li, Zhi Jin, Fei Huang, Yongbin Li, and Ge Li. RL-PLUS: countering capability boundary collapse of llms in reinforcement learning with hybrid-policy optimization. CoRR, abs/2508.00222, 2025. [21] Lutfi Eren Erdogan, Nicholas Lee, Sehoon Kim, Suhong Moon, Hiroki Furuta, Gopala Anumanchipalli, Kurt Keutzer, and Amir Gholami. Plan-and-act: Improving planning of agents for long-horizon tasks. In Forty-second International Conference on Machine Learning, ICML 2025, Vancouver, BC, Canada, July 13-19, 2025. OpenReview.net, 2025. [22] Jingchu Gai, Guanning Zeng, Huaqing Zhang, and Aditi Raghunathan. Differential smoothing mitigates sharpening and improves llm reasoning. arXiv preprint arXiv:2511.19942, 2025. [23] Etash Kumar Guha, Ryan Marten, Sedrick Keh, Negin Raoof, Georgios Smyrnis, Hritik Bansal, Marianna Nezhurina, Jean Mercat, Trung Vu, Zayne Sprague, Ashima Suvarna, Benjamin Feuer, Liangyu Chen, Zaid Khan, Eric Frankel, Sachin Grover, Caroline Choi, Niklas Muennighoff, Shiye Su, Wanjia Zhao, John Yang, Shreyas Pimpalgaonkar, Kartik Sharma, Charlie Cheng-Jie Ji, Yichuan Deng, Sarah M. Pratt, Vivek Ramanujan, Jon Saad-Falcon, Jeffrey Li, Achal Dave, Alon Albalak, Kushal Arora, Blake Wulfe, Chinmay Hegde, Greg Durrett, Sewoong Oh, Mohit Bansal, Saadia Gabriel, Aditya Grover, Kai-Wei Chang, Vaishaal Shankar, Aaron Gokaslan, Mike A. Merrill, Tatsunori Hashimoto, Yejin Choi, Jenia Jitsev, Reinhard Heckel, Maheswaran Sathiamoorthy, Alexandros G. Dimakis, and Ludwig Schmidt. Openthoughts: Data recipes for reasoning models. CoRR, abs/2506.04178, 2025. [24] Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, Jingji Chen, Jingjia Huang, Kang Lei, Liping Yuan, Lishu Luo, Pengfei Liu, Qinghao Ye, Rui Qian, Shen Yan, Shixiong Zhao, Shuai Peng, Shuangye Li, Sihang Yuan, Sijin Wu, Tianheng Cheng, Weiwei Liu, Wenqian Wang, Xianhan Zeng, Xiao Liu, Xiaobo Qin, Xiaohan Ding, Xiaojun Xiao, Xiaoying Zhang, Xuanwei Zhang, Xuehan Xiong, Yanghua Peng, Yangrui Chen, Yanwei Li, Yanxu Hu, Yi Lin, Yiyuan Hu, Yiyuan Zhang, Youbin Wu, Yu Li, Yudong Liu, Yue Ling, Yujia Qin, Zanbo Wang, Zhiwu He, Aoxue Zhang, Bairen Yi, Bencheng Liao, Can Huang, Can Zhang, Chaorui Deng, Chaoyi Deng, Cheng Lin, Cheng Yuan, Chenggang Li, Chenhui Gou, Chenwei Lou, Chengzhi Wei, Chundian Liu, Chunyuan Li, Deyao Zhu, Donghong Zhong, Feng Li, Feng Zhang, Gang Wu, Guodong Li, Guohong Xiao, Haibin Lin, Haihua Yang, Haoming Wang, Heng Ji, Hongxiang Hao, Hui Shen, Huixia Li, Jiahao Li, Jialong Wu, Jianhua Zhu, Jianpeng Jiao, Jiashi Feng, Jiaze Chen, Jianhui Duan, Jihao Liu, Jin Zeng, Jingqun Tang, Jingyu Sun, Joya Chen, Jun Long, Junda Feng, Junfeng Zhan, Junjie Fang, Junting Lu, Kai Hua, Kai Liu, Kai Shen, Kaiyuan Zhang, and Ke Shen. Seed1.5-vl technical report. CoRR, abs/2505.07062, 2025. [25] Jia He, Mukund Rungta, David Koleczek, Arshdeep Sekhon, Franklin X. Wang, and Sadid Hasan. Does prompt formatting have any impact on LLM performance? CoRR, abs/2411.10541, 2024. [26] Qianyu He, Siyu Yuan, Xuefeng Li, Mingxuan Wang, and Jiangjie Chen. Thinkdial: An open recipe for controlling reasoning effort in large language models. CoRR, abs/2508.18773, 2025. [27] Jian Hu, Jason Klein Liu, Haotian Xu, and Wei Shen. Reinforce++: Stabilizing critic-free policy optimization with global normalization. CoRR, abs/2501.03262, 2025. 15 [28] Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model. CoRR, abs/2503.24290, 2025. [29] Hugging Face. Open r1: fully open reproduction of deepseek-r1, January 2025. URL https://github.com/ huggingface/open-r1. [30] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko, Alex Tachard Passos, Alexander Neitz, Alexander Prokofiev, Alexander Wei, Allison Tam, Ally Bennett, Ananya Kumar, Andre Saraiva, Andrea Vallone, Andrew Duberstein, Andrew Kondrich, Andrey Mishchenko, Andy Applebaum, Angela Jiang, Ashvin Nair, Barret Zoph, Behrooz Ghorbani, Ben Rossen, Benjamin Sokolowsky, Boaz Barak, Bob McGrew, Borys Minaiev, Botao Hao, Bowen Baker, Brandon Houghton, Brandon McKinzie, Brydon Eastman, Camillo Lugaresi, Cary Bassin, Cary Hudson, Chak Ming Li, Charles de Bourcy, Chelsea Voss, Chen Shen, Chong Zhang, Chris Koch, Chris Orsinger, Christopher Hesse, Claudia Fischer, Clive Chan, Dan Roberts, Daniel Kappler, Daniel Levy, Daniel Selsam, David Dohan, David Farhi, David Mely, David Robinson, Dimitris Tsipras, Doug Li, Dragos Oprica, Eben Freeman, Eddie Zhang, Edmund Wong, Elizabeth Proehl, Enoch Cheung, Eric Mitchell, Eric Wallace, Erik Ritter, Evan Mays, Fan Wang, Felipe Petroski Such, Filippo Raso, Florencia Leoni, Foivos Tsimpourlas, Francis Song, Fred von Lohmann, Freddie Sulit, Geoff Salmon, Giambattista Parascandolo, Gildas Chabot, Grace Zhao, Greg Brockman, Guillaume Leclerc, Hadi Salman, Haiming Bao, Hao Sheng, Hart Andrin, Hessam Bagherinezhad, Hongyu Ren, Hunter Lightman, Hyung Won Chung, Ian Kivlichan, Ian OConnell, Ian Osband, Ignasi Clavera Gilaberte, and Ilge Akkaya. Openai o1 system card. CoRR, abs/2412.16720, 2024. [31] Jinhao Jiang, Zhipeng Chen, Yingqian Min, Jie Chen, Xiaoxue Cheng, Jiapeng Wang, Yiru Tang, Haoxiang Sun, Jia Deng, Wayne Xin Zhao, Zheng Liu, Dong Yan, Jian Xie, Zhongyuan Wang, and Ji-Rong Wen. Technical report: Enhancing LLM reasoning with reward-guided tree search. CoRR, abs/2411.11694, 2024. [32] Yuhua Jiang, Yuwen Xiong, Yufeng Yuan, Chao Xin, Wenyuan Xu, Yu Yue, Qianchuan Zhao, and Lin Yan. PAG: multi-turn reinforced LLM self-correction with policy as generative verifier. CoRR, abs/2506.10406, 2025. [33] Zhouyu Jiang, Mengshu Sun, Lei Liang, and Zhiqiang Zhang. Retrieve, summarize, plan: Advancing multi-hop question answering with an iterative approach. In Companion Proceedings of the ACM on Web Conference 2025, WWW 2025, Sydney, NSW, Australia, 28 April 2025 - 2 May 2025, pages 16771686. ACM, 2025. [34] Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. CoRR, abs/2503.09516, 2025. [35] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. [36] Jiarui Liu, Kaustubh Dhole, Yingheng Wang, Haoyang Wen, Sarah Zhang, Haitao Mao, Gaotang Li, Neeraj Varshney, Jingguo Liu, and Xiaoman Pan. Stabilizing reinforcement learning for honesty alignment in language models on deductive reasoning. arXiv preprint arXiv:2511.09222, 2025. [37] Qian Liu, Shengnan An, Jian-Guang Lou, Bei Chen, Zeqi Lin, Yan Gao, Bin Zhou, Nanning Zheng, and Dongmei Zhang. Compositional generalization by learning analytical expressions. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. [38] Kevin Lu and Thinking Machines Lab. On-policy distillation. Thinking Machines Lab: Connectionism, 2025. URL https://thinkingmachines.ai/blog/on-policy-distillation. [39] Tongxu Luo, Wenyu Du, Jiaxi Bi, Stephen Chung, Zhengyang Tang, Hao Yang, Min Zhang, and Benyou Wang. Learning from peers in reasoning models. CoRR, abs/2505.07787, 2025. doi: 10.48550/ARXIV.2505.07787. URL https://doi.org/10.48550/arXiv.2505.07787. [40] Minerva. Minerva. URL https://huggingface.co/datasets/math-ai/minervamath. [41] Phuc Minh Nguyen, Chinh D. La, Duy M. H. Nguyen, Nitesh V. Chawla, Binh T. Nguyen, and Khoa D. Doan. The reasoning boundary paradox: How reinforcement learning constrains language models. CoRR, abs/2510.02230, 2025. [42] OpenAI. Introducing openai and o4-mini, 2025. URL https://openai.com/index/ introducing-o3-and-o4-mini/. [43] Mihir Parmar, Palash Goyal, Xin Liu, Yiwen Song, Mingyang Ling, Chitta Baral, Hamid Palangi, and Tomas Pfister. PLAN-TUNING: post-training language models to learn step-by-step planning for complex problem solving. CoRR, abs/2507.07495, 2025. [44] Yixing Peng, Quan Wang, Licheng Zhang, Yi Liu, and Zhendong Mao. Chain-of-question: progressive question decomposition approach for complex knowledge base question answering. In Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 47634776. Association for Computational Linguistics, 2024. [45] Archiki Prasad, Alexander Koller, Mareike Hartmann, Peter Clark, Ashish Sabharwal, Mohit Bansal, and Tushar Khot. Adapt: As-needed decomposition and planning with language models. In Findings of the Association for Computational Linguistics: NAACL 2024, Mexico City, Mexico, June 16-21, 2024, pages 42264252. Association for Computational Linguistics, 2024. [46] Kaleem Ullah Qasim, Jiashu Zhang, Tariq Alsahfi, and Ateeq Ur Rehman Butt. Recursive decomposition of logical thoughts: Framework for superior reasoning and knowledge propagation in large language models. J. Artif. Intell. Res., 83, 2025. [47] Jianing Qi, Xi Ye, Hao Tang, Zhigang Zhu, and Eunsol Choi. Learning to reason across parallel samples for LLM reasoning. CoRR, abs/2506.09014, 2025. doi: 10.48550/ARXIV.2506.09014. URL https://doi.org/10.48550/ arXiv.2506.09014. [48] Yusuke Sakai, Hidetaka Kamigaito, and Taro Watanabe. Revisiting compositional generalization capability of large language models considering instruction following ability. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar, editors, Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025, Vienna, Austria, July 27 - August 1, 2025, pages 3121931238. Association for Computational Linguistics, 2025. URL https://aclanthology.org/2025.acl-long.1508/. [49] Kusha Sareen, Morgane M. Moss, Alessandro Sordoni, Rishabh Agarwal, and Arian Hosseini. Putting the value back in RL: better test-time scaling by unifying LLM reasoners with verifiers. CoRR, abs/2505.04842, 2025. [50] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. CoRR, abs/2402.03300, 2024. [51] Jiho Shin, Clark Tang, Tahmineh Mohati, Maleknaz Nayebi, Song Wang, and Hadi Hemmati. Prompt engineering or fine-tuning: An empirical assessment of llms for code. In 22nd IEEE/ACM International Conference on Mining Software Repositories, MSR@ICSE 2025, Ottawa, ON, Canada, April 28-29, 2025, pages 490502. IEEE, 2025. doi: 10.1109/MSR66628.2025.00082. URL https://doi.org/10.1109/MSR66628.2025.00082. [52] Makoto Shing, Kou Misaki, Han Bao, Sho Yokoi, and Takuya Akiba. TAID: temporally adaptive interpolated distillation for efficient knowledge transfer in language models. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. [53] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling LLM test-time compute optimally can be more effective than scaling model parameters. CoRR, abs/2408.03314, 2024. doi: 10.48550/ARXIV.2408.03314. URL https://doi.org/10.48550/arXiv.2408.03314. [54] Yuda Song, Julia Kempe, and RÃ©mi Munos. Outcome-based exploration for LLM reasoning. CoRR, abs/2509.06941, 2025. [55] Yi Su, Dian Yu, Linfeng Song, Juntao Li, Haitao Mi, Zhaopeng Tu, Min Zhang, and Dong Yu. Crossing the reward bridge: Expanding RL with verifiable rewards across diverse domains. CoRR, abs/2503.23829, 2025. [56] Haoxiang Sun, Yingqian Min, Zhipeng Chen, Wayne Xin Zhao, Zheng Liu, Zhongyuan Wang, Lei Fang, and Ji-Rong Wen. Challenging the boundaries of reasoning: An olympiad-level math benchmark for large language models. CoRR, abs/2503.21380, 2025. [57] Yiyou Sun, Shawn Hu, Georgia Zhou, Ken Zheng, Hannaneh Hajishirzi, Nouha Dziri, and Dawn Song. OMEGA: can llms reason outside the box in math? evaluating exploratory, compositional, and transformative generalization. CoRR, abs/2506.18880, 2025. [58] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y. Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, and Zonghan Yang. Kimi k1.5: Scaling reinforcement learning with llms. CoRR, abs/2501.12599, 2025. [59] Llama-Nemotron Team. Llama-nemotron: Efficient reasoning models. CoRR, abs/2505.00949, 2025. [60] Khoa Trinh, Gaurav Menghani, and Erik Vee. GUIDE: guided initialization and distillation of embeddings. CoRR, abs/2510.06502, 2025. [61] Christian Walder and Deep Karkhanis. Pass@k policy optimization: Solving harder reinforcement learning problems. CoRR, abs/2505.15201, 2025. [62] Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, Yuqiong Liu, An Yang, Andrew Zhao, Yang Yue, Shiji Song, Bowen Yu, Gao Huang, and Junyang Lin. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for LLM reasoning. CoRR, abs/2506.01939, 2025. [63] Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Lucas Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, Weizhu Chen, Shuohang Wang, Simon Shaolei Du, and Yelong Shen. Reinforcement learning for reasoning in large language models with one training example. CoRR, abs/2504.20571, 2025. [64] Zhen Wang, Zhifeng Gao, and Guolin Ke. Masked-and-reordered self-supervision for reinforcement learning from verifiable rewards. arXiv preprint arXiv:2511.17473, 2025. [65] Fang Wu, Weihao Xuan, Ximing Lu, ZaÃ¯d Harchaoui, and Yejin Choi. The invisible leash: Why RLVR may not escape its origin. CoRR, abs/2507.14843, 2025. doi: 10.48550/ARXIV.2507.14843. URL https: //doi.org/10.48550/arXiv.2507.14843. [66] Shangzi Xue, Zhenya Huang, Jiayu Liu, Xin Lin, Yuting Ning, Binbin Jin, Xin Li, and Qi Liu. Decompose, analyze and rethink: Solving intricate problems with human-like reasoning cycle. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. [67] Jianhao Yan, Yafu Li, Zican Hu, Zhi Wang, Ganqu Cui, Xiaoye Qu, Yu Cheng, and Yue Zhang. Learning to reason under off-policy guidance. CoRR, abs/2504.14945, 2025. [68] Haoran Yang, Hongyuan Lu, Wai Lam, and Deng Cai. Exploring compositional generalization of large language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Student Research Workshop, NAACL 2024, Mexico City, Mexico, June 18, 2024, pages 1624. Association for Computational Linguistics, 2024. [69] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 23692380. Association for Computational Linguistics, 2018. [70] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. [71] Xinhao Yao, Lu Yu, Xiaolin Hu, Fengwei Teng, Qing Cui, Jun Zhou, and Yong Liu. The debate on RLVR reasoning capability boundary: Shrinkage, expansion, or both? two-stage dynamic view. CoRR, abs/2510.04028, 2025. [72] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Weinan Dai, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and 18 Mingxuan Wang. DAPO: an open-source LLM reinforcement learning system at scale. CoRR, abs/2503.14476, 2025. [73] Lifan Yuan, Weize Chen, Yuchen Zhang, Ganqu Cui, Hanbin Wang, Ziming You, Ning Ding, Zhiyuan Liu, Maosong Sun, and Hao Peng. From f(x) and g(x) to f(g(x)): Llms learn new skills in RL by composing old ones. CoRR, abs/2509.25123, 2025. [74] Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Yang Yue, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? CoRR, abs/2504.13837, 2025. [75] Di Zhang, Jianbo Wu, Jingdi Lei, Tong Che, Jiatong Li, Tong Xie, Xiaoshui Huang, Shufei Zhang, Marco Pavone, Yuqiang Li, Wanli Ouyang, and Dongzhan Zhou. Llama-berry: Pairwise optimization for olympiad-level mathematical reasoning via o1-like monte carlo tree search. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2025 - Volume 1: Long Papers, Albuquerque, New Mexico, USA, April 29 - May 4, 2025, pages 73157337. Association for Computational Linguistics, 2025. [76] Kaiyi Zhang, Ang Lv, Jinpeng Li, Yongbo Wang, Feng Wang, Haoyuan Hu, and Rui Yan. Stephint: Multi-level stepwise hints enhance reinforcement learning to reason. CoRR, abs/2507.02841, 2025. [77] Xichen Zhang, Sitong Wu, Yinghao Zhu, Haoru Tan, Shaozuo Yu, Ziyi He, and Jiaya Jia. Scaf-grpo: Scaffolded group relative policy optimization for enhancing LLM reasoning. CoRR, abs/2510.19807, 2025. [78] Zijing Zhang, Ziyang Chen, Mingxiao Li, Zhaopeng Tu, and Xiaolong Li. RLVMR: reinforcement learning with verifiable meta-reasoning rewards for robust long-horizon agents. CoRR, abs/2507.22844, 2025. [79] Tianle Zhou, Jiakai Xu, Guanhong Liu, Jiaxiang Liu, Haonan Wang, and Eugene Wu. An approach for systematic decomposition of complex llm tasks. CoRR, abs/2510.07772, 2025. [80] Rongzhi Zhu, Xiangyu Liu, Zequn Sun, Yiwei Wang, and Wei Hu. Mitigating lost-in-retrieval problems in retrieval augmented multi-hop question answering. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025, Vienna, Austria, July 27 - August 1, 2025, pages 2236222375. Association for Computational Linguistics, 2025. [81] Xinyu Zhu, Mengzhou Xia, Zhepei Wei, Wei-Lin Chen, Danqi Chen, and Yu Meng. The surprising effectiveness of negative reinforcement in LLM reasoning. CoRR, abs/2506.01347, 2025."
        },
        {
            "title": "A Prompt Templates",
            "content": "In this part, we introduce the details of the prompt templates that are utilized in out experiments. Below is the prompt used to train the decomposer model, where the placeholder {QUESTION} is replaced with the corresponding question ğ‘¥ğ‘–. Prompt for Decomposer {QUESTION} Please reason step-by-step and decompose this question into several sub-questions. You only provide the sub-questions and do not provide the any details reasoning steps and solution. Use the format <subquestion> and <subquestion> to identify each sub-question. Below is the prompt used to guide the reasoner to perform reasoning, which is employed in the objective function (ğœƒ ğ‘…) in Eq. 1. The placeholder {QUESTION} is replaced with the corresponding question ğ‘¥ğ‘–. Vanilla Prompt for Reasoner {QUESTION} Please reason step by step, and put your final answer within boxed{}. Below is the prompt used by the reasoner for inference under sub-question guidance. The placeholder {QUESTION} is replaced with the corresponding original question ğ‘¥ğ‘–, and {LIST OF SUB-QUESTIONS} is replaced with the decomposed sub-questions Sğ‘–. Prompt with Sub-question Guidance for Reasoner {QUESTION} Now, give you some tips and you can refer to the provided tips to solve the problem. Tips: {LIST OF SUB-QUESTION} Please reason step by step, and put your final answer within boxed{}. Below is the prompt used during IDN Loss training, and the placeholder QUESTION is replaced with the corresponding question. IDN Loss Prompt for Reasoner {QUESTION} You can try to reason step by step through different approaches, and try to explore the correct the solution. You should put your final answer within boxed{}."
        },
        {
            "title": "B Details of the Format Reward for Decomposer Training",
            "content": "We enforce series of constraints to verify whether the model responses follow the required format. The models generated response is required to begin with the <subquestion> tag. Each sub-question is not allowed to contain more than one <subquestion> tag. The models generated response must contain more than 10 characters. 20 Pseudo Code of A2D In Algorithm 1, we present the pseudocode the our A2D to better demonstrate its training pipeline. Algorithm 1: The Pseudo Code for A2D. Input Output : powerful reasoner ğœ‹ ğœƒR . : backbone model ğœ‹ ğœƒ and the training dataset = {(ğ‘¥ğ‘–, ğ‘¦ğ‘–)}ğ‘› ğ‘–=1. 1 # (1) Training Decomposer ğœ‹ ğœƒD via RLVR. 2 Initialize the decomposer ğœ‹ ğœƒD and proxy reasoner using the backbone model ğœ‹ ğœƒ . 3 for each training step do 4 Prompt the decomposer ğœ‹ ğœƒD to decompose the questions in the training dataset. Compute the quality reward of the decomposed question ğ‘…Q by the proxy reasoner. Compute the format reward of the decomposed question ğ‘…F by rules in Appendix B. Utilize Eq. 1 to optimize the decomposer ğœ‹ ğœƒD based on the reward ğ‘… = ğ‘…Q ğ‘…F. 5 6 7 8 Obtain decomposer ğœ‹ ğœƒD , which can decompose the complex question into several simpler sub-questions. 9 # (2) Decompose the Complex Questions. 10 for each instance ğ‘¥ğ‘–, ğ‘¦ğ‘– in training dataset do 11 Use decomposer ğœ‹ ğœƒD to decompose the question ğ‘¥ğ‘–, obtaining list of sub-questions Sğ‘–. Add the triple ğ‘¥ğ‘–, ğ‘¦ğ‘–, Sğ‘– into the training dataset of the reasoner DSQ. 13 Obtain the training dataset of the reasoner DSQ = {ğ‘¥ğ‘–, ğ‘¦ğ‘–, Sğ‘–}ğ‘› ğ‘–=1. 14 # (3) Training Reasoner ğœ‹ ğœƒR through RLVR with Sub-question Guidance. 15 Initialize the reasoner ğœ‹ ğœƒR using the backbone model ğœ‹ ğœƒ . 16 for each instance ğ‘¥ğ‘–, ğ‘¦ğ‘–, in training dataset SSQ do 17 Prompt the reasoner ğœ‹ ğœƒR to generate the solutions Ë†Y, based on the question ğ‘¥ğ‘–. Compute the loss function of RLVR (ğœƒR) using Eq. 1, based on Ë†Y. Prompt the reasoner ğœ‹ ğœƒR to generate the solutions Ë†Y, based on the question ğ‘¥ğ‘– and the sub-questions Sğ‘–. Process the generated solutions Ë†Y through Eq. 4. Compute the in-context distillation loss Utilize Eq. 6 to optimize the reasoner ğœ‹ ğœƒR based on (ğœƒR) and (ğœƒR) using Eq. 5, based on Ë†Y. (ğœƒR). IDL IDL 12 18 19 21 22 23 Obtain the powerful reasoner ğœ‹ ğœƒR ."
        }
    ],
    "affiliations": [
        "ByteDance Seed",
        "Renmin University of China"
    ]
}