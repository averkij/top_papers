{
    "paper_title": "Rethinking Optimal Verification Granularity for Compute-Efficient Test-Time Scaling",
    "authors": [
        "Hao Mark Chen",
        "Guanxi Lu",
        "Yasuyuki Okoshi",
        "Zhiwen Mo",
        "Masato Motomura",
        "Hongxiang Fan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Test-time scaling (TTS) has proven effective in enhancing the reasoning capabilities of large language models (LLMs). Verification plays a key role in TTS, simultaneously influencing (1) reasoning performance and (2) compute efficiency, due to the quality and computational cost of verification. In this work, we challenge the conventional paradigms of verification, and make the first attempt toward systematically investigating the impact of verification granularity-that is, how frequently the verifier is invoked during generation, beyond verifying only the final output or individual generation steps. To this end, we introduce Variable Granularity Search (VG-Search), a unified algorithm that generalizes beam search and Best-of-N sampling via a tunable granularity parameter g. Extensive experiments with VG-Search under varying compute budgets, generator-verifier configurations, and task attributes reveal that dynamically selecting g can improve the compute efficiency and scaling behavior. Building on these findings, we propose adaptive VG-Search strategies that achieve accuracy gains of up to 3.1\\% over Beam Search and 3.6\\% over Best-of-N, while reducing FLOPs by over 52\\%. We will open-source the code to support future research."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 0 3 7 1 1 . 5 0 5 2 : r Rethinking Optimal Verification Granularity for Compute-Efficient Test-Time Scaling Hao (Mark) Chen1 Guanxi Lu1 Yasuyuki Okoshi2 Zhiwen Mo1 Masato Motomura2 Hongxiang Fan1 1Imperial College London, UK 2Institute of Science Tokyo, Japan {hao.chen20, guanxi.lu22, zhiwen.mo25, hongxiang.fan}@imperial.ac.uk"
        },
        {
            "title": "Abstract",
            "content": "Test-time scaling (TTS) has proven effective in enhancing the reasoning capabilities of large language models (LLMs). Verification plays key role in TTS, simultaneously influencing (1) reasoning performance and (2) compute efficiency, due to the quality and computational cost of verification. In this work, we challenge the conventional paradigms of verification, and make the first attempt toward systematically investigating the impact of verification granularitythat is, how frequently the verifier is invoked during generation, beyond verifying only the final output or individual generation steps. To this end, we introduce Variable Granularity Search (VG-Search), unified algorithm that generalizes beam search and Best-of-N sampling via tunable granularity parameter g. Extensive experiments with VG-Search under varying compute budgets, generator-verifier configurations, and task attributes reveal that dynamically selecting can improve the compute efficiency and scaling behavior. Building on these findings, we propose adaptive VG-Search strategies that achieve accuracy gains of up to 3.1% over Beam Search and 3.6% over Best-of-N, while reducing FLOPs by over 52%. We will open-source the code to support future research."
        },
        {
            "title": "Introduction",
            "content": "The past few years have witnessed the rapid advancement of large language models (LLMs), driven by scaling of model size and training data [6, 17, 28]. However, further training-time scaling is increasingly constrained by prohibitive computational costs and the limited availability of high-quality human-generated data [34]. Test-time scaling (TTS) [33, 5, 32] offers promising alternative by enhancing performance through additional compute at inference time. TTS techniques generally fall into two categories: internal scaling [33, 25], which focuses on optimizing single generation trajectory, and sampling-based scaling [5, 32, 35], which improves performance by exploring multiple candidate generations. These two approaches are orthogonal and can be combined to achieve higher performance [22]. Sampling-based methods are typically training-free and easy to integrate with internal scaling approaches in plug-and-play manner. Verification, commonly implemented as learned reward model or scoring function, plays key role in both TTS paradigms. State-of-the-art (SOTA) sampling-based methods, such as Diverse Verifier Tree Search (DVTS) [4] and verifier-guided Beam Search [32], leverage separate verifier LLM to guide the generation of generator LLM, improving sampling efficiency and accuracy. In these methods, generation step is typically defined as chunk of text delimited by special tokens (e.g., newline characters), which serves as the atomic unit of verification. However, this verification granularity choice is heuristic [32, 22] and remains static [36], with no guarantee of optimality. As shown in Figure 1a, our profiling results show that verifier scores often remain stable across multiple Preprint. Under review. (b) Verification Cost (a) Score Stability (c) Inference Latency (d) Optimized Performance Figure 1: Motivations for Optimizing Verification Granularity g. (a) Small PRM (Llama3.18B-PRM) score deltas across generation steps indicate redundant verification. (b) Verification incurs notable latency overheads. (c) Increasing verification granularity lowers the latency of both Proposer and Verifier. (d) Optimizing improves the accuracy-compute tradeoff over fixed-granularity baselines (Qwen2.5-Math-7B as generator, Skywork-o1-1.5B as verifier). generation steps (e.g., over 50% of 2-step score differences are below 1% of score range), suggesting redundancy in the current verification granularity. This inefficiency causes verification to account for an increasingly large proportion of the overall inference latency, as illustrated in Figure 1b. These insights and observations motivate us to explore the following two research questions (RQs): RQ1. Is the conventional verification granularity optimal for the accuracycompute scaling? RQ2. If not, how can we optimize it to achieve better accuracycompute frontier? To explore RQ1, we introduce Variable Granularity Search (VG-Search), unified algorithm that generalizes verifier-guided Beam Search and Best-of-N methods. VG-Search employs granularity parameter that controls the verification granularity (Section 2). Our hardware performance profiling (Figure 1c) shows that has significant impact on the latency contributions of both the generator LLM and the verifier. To further investigate the accuracycompute trade-off under varying g, we conduct extensive experiments with VG-Search across different values of g, models, compute budget, and datasets (Section 4). Building on top of the insights from RQ1, we propose an adaptive VG-Search approach which dynamically adjusts the verification granularity based on the compute budget and task attributes (Section 5) to further address RQ2. As shown in Figure 1d, our results demonstrate better performance-compute frontiers than previous sampling-based TTS methods. Overall, our main contributions are: We propose Variable Granularity Search (VG-Search), enabling systematic analysis of how verification granularity affects performance and compute scaling. Through extensive experiments, we show that the conventional static verification granularity is sub-optimal, limiting the performance scaling of verifier-guided search methods. We develop adaptive VG-Search strategies that dynamically tune granularity, achieving up to 3.1% higher accuracy than Beam Search, 3.6% over Best-of-N, with over 52% compute reduction."
        },
        {
            "title": "2 Exploring Verification Granularity for Reasoning",
            "content": "2.1 Verification Granularity in Verifier-Guided Search In verifier-guided search, generator model proposes candidate steps and verifier model evaluates them to guide the search process. The task of generating correct solution can be framed as collaboration between two components: Generator (Proposer) G: probabilistic model, typically an LLM, responsible for generating partial or complete candidate sequence st:t+g, where denotes the number of the generation steps. The generation process is governed by the distribution (st:t+g G). 2 Figure 2: Variable-Granularity Search (VG-Search). VG-Search unifies verifier-guided Beam Search and Best-of-N via the granularity parameter g. At each step, B1 B2 candidates are verified and B1 = 2 candidates are selected. B2 = 2 continuations are generated per selected path. Only the B1 selected candidates proceed to the Extend Step, reducing proposer FLOPs by pruning early. Verifier V: scoring function V(st:t+g) that assigns scalar score to each partial or complete candidate sequence st:t+g. The verifier is invoked after every generation steps. This score is intended to correlate with the expected quality or correctness of the sequence prefix. The goal of the search process is to generate correct or high-quality solution 1:T of length via interaction between the generator and verifier. key hyperparameter in this process is the verification granularity g, which controls how frequently the verifier can intervene and guide the generator. To illustrate the impact of g, we link the verifier-guided search to the Infinite Monkey Theorem (IMT) [38], where random generator (monkey) tries to reproduce target sequence (e.g., Shakespeares work) under perfect verifier invoked every characters. When verification is infrequent (large g), the expected number of attempts grows exponentially as E[Ag], making success computationally infeasible. In contrast, with frequent verification, early error pruning reduces the expected cost E[Ag], at the expense of higher verification overhead. This toy setting underscores fundamental trade-off between verification cost and exploration efficiency. Optimizing is thus critical for effective collaboration between generator and verifier under compute constraints. The most common approach to defining relies on heuristics and remains static. However, this convention might be neither consistent nor optimal: the model distribution varies widelysome generators are more verbose, others more structuredand the number of tokens required to conceptualize coherent \"thinking step\" can differ significantly across tasks. This motivates systematic study of how performance scales with different granularity g. 2.2 Variable-Granularity Search Beam Search [5] and Best-of-N sampling [32] are two prevalent methods for scaling large language model (LLM) performance at test-time by leveraging additional computation. These methods represent two opposing extremes within the spectrum of verifier-guided search, primarily differing in the frequency with which the verifier checks the outputs from the proposer G. In Beam Search, verification is highly fine-grained: actively guides candidate selection at each generation step, enabling the early pruning of less promising paths. Conversely, Best-of-N sampling employs coarsegrained approach, where the verification is performed on an entire candidate solution. While this allows for broader exploration, it lacks the intermediate guidance provided by frequent verification. To bridge this gap and enable continuous exploration of verification granularity, we introduce Variable-Granularity Search (VG-Search). VG-Search unifies Beam Search and Best-of-N sampling through the concept of verification granularity (g). By varying g, VG-Search allows for continuous control over the trade-off between fine-grained guidance and broad exploration. Key hyperparameters of VG-Search include: Beam Width (B1): The number of top-scoring candidate sequences (beams) retained after verification and selection. Branch-Out Factor (B2): The number of alternative continuations generated by from each of the B1 retained beams before the next verification phase. Verification Granularity (g): The number of generation steps evaluated per verificationi.e., the interval between verifier (V) calls. As illustrated in Figure 2, single cycle of VG-Search advances the search from candidate length to + g. The full algorithm proceeds as follows: 1. Start: Initialize with B1 B2 candidates from the initial prompt. 2. Verify & Select: Evaluate B1 B2 candidates using V, and retain the top B1 beams. 3. Extend: For each of the B1 selected beams, produce 1 generation steps using G. 4. Branch: For each extended beam, produce B2 single-generation-step continuations using G. 5. Repeat: Go back to Step 2 and iterate until termination criteria are met. Omitting Step 3 (Extend) or setting = 1 reduces VG-Search to standard Beam Search. At the other extreme, when = (the number of generation steps in full solution), VG-Search becomes equivalent to Best-of-N sampling. One alternative design is to produce 1 generation steps in Branch, mimicking Beam Search but with variable verification granularity. However, prior work [32] shows that rolling out multiple future tokens, as in LookAhead Search, does not yield the best performance. We therefore adopt simpler, more compute-efficient design: Branch generates just one step per candidate, and early pruning via the verifier occurs before Extend. As result, only the top B1 candidates continue in Extend, instead of the full B1 B2 set used in Beam Search or Best-of-Nleading to significantly lower proposer FLOPs. Larger further amplifies savings for both verifier and proposer. We validate this design in Section 4.1. case study of VG-Search is illustrated in Appendix A.5. 2.3 Compute Cost Model for Generator and Verifier To analyze and optimize g, we define compute cost model in FLOPs with the following parameters: (1) L: average solution length in generation steps, (2) Pg, Pv: parameter counts of the generator and verifier, (3) Fg: FLOPs per generation step, (4) Fv: FLOPs per verifier call. The number of VG-Search cycles is approximately Ncycles = L/g. Following prior work [31], we approximate Fg 2T Pg, where is the average number of tokens per generation step. Similarly, Fv 2αPv, where α is verifier-specific variable. The total compute for generation, CG, includes extending the B1 primary beams and generating B2 branches across all cycles: CG = B1 1 (cid:124) (cid:123)(cid:122) (cid:125) Extending + B2 (cid:124)(cid:123)(cid:122)(cid:125) Branching Ncycles Fg = 2 B1 (g 1 + B2) Pg (1) The total compute for verification, CV , accounts for scoring B1 B2 candidates across all cycles: CV = B1 B2 Ncycles Fv = 2 α B1 B2 Pv (2) The total compute cost Ctotal becomes: Ctotal = CG + CV = 2 B1 [T (g 1 + B2) Pg + α B2 Pv] (3) We adopt the proposed cost model to study the scaling behavior of VG-Search (Section 4), which provides reliable proxy for measured inference latency as demonstrated in Figure 1c."
        },
        {
            "title": "3 Experiment Setup",
            "content": "To investigate the optimality of the conventional verification granularity mentioned in RQ1, we conduct experiments on mathematical reasoning benchmarks. Task and Datasets. We evaluate on the MATH-500 [21] and AIME [2] datasets. MATH-500 samples 500 problems from the MATH [16] benchmark, with difficulty levels labelled. AIME comprises 90 advanced high school mathematics problems from the past three years (AIME24, AIME23, AIME22). Additionally, we sample 250 problems from the MATH [16] benchmark as validation set for Section 5, referred to as MATH-250. It consists of 50 problems per difficulty level. Policy and Reward Models. For Proposer, we use both general-purpose model (Llama-3.2-3BInstruct [15]) and models with internal scaling ability, fine-tuned to generate Chain-of-Thought (CoT) for math tasks, Qwen2.5-Math-7B and Qwen2.5-Math-1.5B [39]. All models are prompted to generate step-by-step solutions. For the Llama models, we use the same prompt template as the official evaluation, which explicitly states the response template. For the Qwen models, it is suggested to use simple prompt template. Following the community convention and prior work [22, 32, 4], we use nn to delimit each generation step, which serves as the smallest unit of verification granularity. For Verifier, we employ discriminative Process Reward Models (PRMs), including Skywork-o1-1.5B1 and Skywork-o1-7B2 [27], as verifiers. Following [4], we use Last (final) scores for selection. Details of FLOPs calculation are in Appendix A.2."
        },
        {
            "title": "4 Understanding the Limits of the Verification Granularity Convention",
            "content": "4.1 Test-Time Scaling Law with Verification Granularity In this section, we aim to answer RQ1, investigating how the optimal verification granularity varies with both the compute budget and the capabilities of the Proposer-Verifier pair. To explore different operating points on the accuracy-compute trade-off curve (Figure 3), we vary and adjust the number of generations to modulate the total compute budget. We analyze three representative generator (G)verifier (V) pairs, specifically chosen to probe the interplay between model capability and compute allocation: 1. Strong (Qwen2.5-Math-7B) with Small (Skywork-o1-1.5B), 2. Weak (Llama-3.2-3B-Instruct, not math-finetuned) with Small (Skywork-o1-1.5B), 3. Medium (Qwen2.5-Math-1.5B) with Large (Skywork-o1-7B). Our findings in Figure 3 indicate that the optimal verification granularity depends on both the generators capability and the available compute budget: Strong Generators Prefer Sparse Verification, while weak ones need frequent checks. For the strong Qwen2.5-Math-7B generator paired with small verifier, while standard Beam Search (g = 1) is effective at lower compute costs, sparser verification (g {2, 3, 4}) achieves superior accuracy at medium to high compute budgets, as shown in the first column of Figure 3. Notably, = 3 reaches the highest peak accuracy, outperforming = 1 by approximately 4% on MATH-500. This suggests that strong generator can reliably produce longer correct partial solutions, making frequent verification less critical and allowing compute to be reallocated (e.g., to wider beams via B1) for better overall performance. As shown in the second column of Figure 3, moderately strong Qwen2.5-Math-1.5B, when paired with large verifier, also benefits from sparser verification, with = 2 consistently outperforming = 1 from mid-to-high compute and achieving the best peak accuracy. However, excessively sparse verification (g = 4) becomes detrimental, indicating an optimal balance point. In contrast, as shown in the last column of Figure 3, the non-specialized Llama-3B generator has peak performance with frequent verification (g = 1) on MATH-500 and MATH-250, implying that weaker or less task-aligned generators require continuous guidance. Interestingly, on the more challenging AIME dataset, all Proposer-Verifier pairs show tendency for sparser verification (g > 1, sometimes even = 4) to outperform = 1 at medium and high compute budgets. This might be because AIME problems benefit more from the broader exploration encouraged by sparser verification (which 1Skywork-o1-Open-PRM-Qwen-2.5-1.5B 2Skywork-o1-Open-PRM-Qwen-2.5-7B 5 Figure 3: Accuracy vs. compute (FLOPs) across different verification granularities g. makes the search resemble Best-of-N), or the PRMs step-wise utility is less consistently high on AIME, reducing the benefit of frequent checks. Optimal Granularity Varies with Compute Budget. Across most models and datasets, sparser verification (g > 1) tends to become more competitive as the total compute budget increases. At very low compute budgets, the aggressive pruning of standard Beam Search (g = 1) often provides more robust performance baseline. This aligns with the intuition that investing in more extended generation phases between verifications is more viable when more overall compute is available to support wider exploration or more candidate paths. Optimal Granularity Saves Computation Significantly. key advantage of using sparser verification (g > 1) is the potential for substantial FLOPs savings while maintainingor even improvingperformance. For example, in the Strong G, Small setting on MATH-500, setting = 3 achieves approximately 88% accuracy at 213 FLOPs, whereas = 1 requires 215 FLOPs to reach slightly lower accuracy of around 87.5%. Similarly, on AIME, = 4 attains comparable peak accuracy to = 1, but with noticeably lower FLOPs cost across several configurations. This efficiency gain arises from fewer verifier invocations and reduced branching operations overall, as detailed in our cost model (Section 2.3). Moreover, the reduction in FLOPs effectively translates into decreased latency, as shown in Figure 1c. Rethinking the Optimal \"Thinking Step\". The observation that increasing (sparser verification) can improve accuracy while reducing compute budget is significant. This challenges the conventional verification granularity (g = 1), where verification typically occurs at arbitrary delimiter boundaries (e.g., newlines), which may not always align with meaningful reasoning steps. First, as shown in Figure 1a, consecutive PRM score differences are often negligible, indicating that the current definition of thinking step is overly fine-grained. Second, larger can be interpreted as defining more substantial and semantically coherent \"thinking step\". Delaying verification and branching until the end of these extended segments may avoid injecting noise by evaluating incomplete reasoning fragments, thus enabling more effective search. Thus, in response to RQ1, we conclude that the current convention for verification granularity is sub-optimal, motivating more flexible definition of verification granularitybeyond fixed token-level delimitersand highlighting the need for more sophisticated approach to optimizing (Section 5). 6 4.2 Trade-offs: Granularity, Verifier Parameter, and Branching This section further explores how interacts with other key hyper-parameters: the verifiers model parameter and the branch-out factor B2 to optimize compute allocation under fixed total compute budgets. All experiments in this section utilize Qwen2.5-Math-7B as the generator. Granularity vs. Verifier Parameter. We investigate whether it is more beneficial to use strong verifier sparsely (larger g, larger verifier model) or weaker verifier frequently (smaller g, smaller verifier model). Figure 4a compares two configurations: (1) = 1 (frequent verification) with Small Verifier (Skywork-o1-1.5B), and (2) = 2 (sparser verification) with Large Verifier (Skyworko1-7B). As shown in Figure 4a, at lower compute budgets, sparse verification with larger verifier yields low accuracy. However, as the compute budget increases, employing larger verifier more sparsely achieves higher peak accuracy. This suggests clear preference for leveraging stronger, more discerning verifiers less frequently when sufficient computational resources are available. (a) = 1, Small VS = 2, Large (d) Acc. diff. Majority Voting (b) Matched FLOPs with diff. and B2 (c) Largest effective values (e) Acc. diff. Weighted Voting Figure 4: (a) shows = 2 with strong verifier outperforms = 1 with weak verifier at high compute budget. (b) demonstrates limited performance gain from increasing branch factor B2. (c) plots the largest effective across difficulty levels for different generators. (d),(e) show the accuracy gain of sparser verification with stronger verifier across difficulty levels and number of generations. Granularity vs. Branch-Out Factor Next, we explore the trade-off between the verification granularity and the branch-out factor B2, at matched total compute FLOPs. Increasing means fewer verification calls and longer generation segments, while increasing B2 means exploring more alternatives at each verification point. The verifier used in this experiment is Skywork-o1-1.5B. Figure 4b shows that configurations with small granularity (g = 1, B2 = 2) with low branching tend to outperform other configurations at low or mid-range compute budgets. However, all configurations converge at very high compute. Comparing the above findings, clear pattern emerges: investing the saved compute in the verifier parameter appears to be more effective scaling strategy than simply increasing the number of branches. 4.3 Optimal Verification Granularity vs. Task Difficulty and Number of Samples To assess the practical limits of sparse verification, we define the largest effective as the maximum verification granularity that retains at least 95% of the accuracy achieved at g=1. We also analyze the accuracy gain of g=2 over g=1 across varying sample budgets and task difficulty levels. Figure 4d, 4e show that the optimal is strongly influenced by both difficulty and sampling budget. 7 Increased Task Difficulty Demands Denser Verification. Across all models and compute budgets, we observe consistent trend: as task difficulty increases, the largest effective decreases. For the hardest problems (Levels 45), g=1 or g=2 is often required to maintain performance at lower compute budget. Likewise, the advantage of g=2 over g=1 diminishes with difficulty, likely because harder tasks involve more reasoning errors, necessitating more frequent verifier intervention to stay on track. Higher Number of Samples Tolerates Sparser Verification. In general, larger sample budgets allow for sparser verification, especially on easy to moderately difficult tasks. Higher budgets also amplify the performance gain of g=2 over g=1, as the generator can explore wider search space, increasing the likelihood of finding correct path despite less frequent verification."
        },
        {
            "title": "5 Towards Adaptive Redefinition of the Verification Granularity",
            "content": "Our results (Figures 3, 4) show that the optimal in VG-Search depends on task difficulty, compute budget, and model capability. This motivates more systematic approach to defining the search step, addressing RQ2. We formulate an optimization dual for selecting verification granularity: minimizing compute for target accuracy vs. maximizing accuracy under compute budget. For each, we propose corresponding adaptive granularity strategy: Strategy 1: Compute Minimization with Performance Parity (CM-g) CM-g finds the largest that maintains accuracy within tolerance ϵ of = 1, reducing compute. Given generator, difficulty d, and number of generations n: 1. Compute baseline accuracy Acc(g=1, d, n). 2. Increase while Acc(g, d, n) Acc(g=1, d, n) ϵ; return the largest satisfying the accuracy constraint. Strategy 2: Accuracy Maximization with Budget Constraint (AM-g) AM-g selects that maximizes accuracy under fixed compute budget (as in number of generations): = arg maxg{1,...,gmax} Acc(g, d, n). Metric Adaptive VG-Search Oracle Val CM-g AM-g CM-g AM-g 64 128 Acc. FLOPs Acc. FLOPs Acc. FLOPs 89.2 5393 90.5 15899 90.7 90.1 5844 90.6 16200 90.8 22775 88.6 5543 89.9 11086 90.4 90.1 6145 90.1 12290 90.4 28189 Beam Search 87.0 12010 89.3 88.8 48042 DVTS Best-of-N 89.2 12010 89.7 24021 89.5 48042 87.6 87.4 23904 87.2 47808 Table 1: Performance of adaptive granularity strategies (CM-g, AM-g) vs. baselines on MATH-500, using Qwen2.5-Math-7B as the proposer and Skywork-o1-1.5B as the verifier. Best accuracy and Lowest FLOPs are bold underlined. is the number of generations per question. We compare against Beam Search, DVTS and Best-of-N baselines on the MATH-500 test set. \"val\" indicates was tuned on validation set MATH-250, while strategies under \"oracle\" use oracle selected on the test set. We set ϵ = 0 for CM-g. Table 1 shows that both AM-g and CM-g improve performance and efficiency. AM-g consistently achieves higher accuracy, with gains up to 3.1% over Beam Search and 3.6% over Best-of-N, while CM-g provides substantial FLOPs savings, reducing compute by over 50% at = 64 and = 256 while maintaining or improving accuracy. For instance, CM-g (val) at = 128 achieves 89.9% accuracy using just 11086 FLOPsonly 46% of the baseline budget. Although oracle-tuned variants perform slightly better, validation-tuned versions (CM-g (val), AM-g (val)) still outperform fixed-g methods, demonstrating strong generalization and practicality. Under our cost model, CG dominates, so most FLOPs savings come from pruning candidate paths during the Extend Step (Section 2.2). In summary, adapting verification granularity to task difficulty and compute budget offers simple yet effective way, enabling more efficient and performant LLM reasoning."
        },
        {
            "title": "6 Related Work",
            "content": "Test-Time Scaling (TTS). Recent work on scaling test-time compute can be broadly categorized into two approaches [42, 19]: i) Internal scaling and ii) sampling-based TTS. Internal scaling methods enable models to scale their test-time compute by adapting their parameters through Supervised Fine-tuning (SFT) [41, 24] or Reinforcement Learning (RL) [33, 23, 40]. However, without explicit compute budget control [1, 3], these approaches often result in unnecessarily long CoT, even for simple queries, leading to inefficient inference costs. Sampling-based TTS, by contrast, dynamically allocates test-time computation during deployment, without requiring additional tuning to modify the models parameters offline. Sampling-based TTS [10, 32, 29, 9], orthogonal to internal scaling approaches, allows for more fine-grained control over compute usage and can further improve algorithmic performance. As noted by [32, 22], multi-step lookahead approaches, such as Monte Carlo Tree Search (MCTS) [14], adaptive thought rollback [8], and hierarchical multi-step verification [36], incur substantial sampling overhead during inference, we do not consider such methods in this work. Verification-Guided Search. Verifying the quality of generated outputs to guide sampling-based TTS has been shown to significantly improve the efficiency of test-time computation [22, 32, 4]. Based on the granularity of verification, existing approaches can be broadly classified into two categories: (i) outcome verification and (ii) process verification. Outcome verification [10] evaluates the entire response after the generation of complete sample. This approach is widely used in sampling-based TTS, such as Best-of-N strategy [9], where verifier selects the most promising answer from set of full samples. While simple and effective, outcome verification provides no intermediate feedback, which can lead to wasted computation on low-quality completions. Process verification [21, 43], often implemented as process reward model (PRM), evaluates intermediate reasoning steps during generation. These models assign scores to partial outputs, allowing the inference process to be guided. PRMs have been shown to improve sample efficiency and enable early pruning of unpromising token paths, leading to more compute-efficient search. Despite their promise, both outcome and process verification methods treat the verifier as fixed component and typically apply it at pre-defined granularity. In contrast, our work introduces an adaptive approach that dynamically optimizes verification granularity to maximize performance. Due to the verification inefficiency of Generative PRMs [43] under matched compute budgets [31], we leave their investigation to future work. Compute Optimal Scaling. Previous research [32] indicates that the optimal choice between Bestof-N and Beam Search often depends on factors such as the available compute budget and the problem characteristics. Yet, restricting selection to these two discrete options limits flexibility and expressiveness in identifying an optimal compute scaling strategy. By introducing variable granularity (g), VG-Search spans continuous spectrum of search behaviors between these extremes, offering potentially greater expressiveness for the optimal TTS strategy. Prior work has investigated how test-time strategies, compute budgets, and problem difficulty interact, showing that optimal allocation of inference-time compute can be more effective than merely scaling model parameters [32]. Scaling the number of verification attempts per candidate has been shown to improve verification accuracy and overall task performance [30]. Recent work [31] also explores compute-optimal strategies for deciding when to generate new solutions versus verifying existing ones, taking into account the number of verification and output samples. [20] demonstrates that employing multiple diverse verifiers can enhance performance, as different verifiers are likely to detect different types of errors. [7] proposes consistency-based model switching method that leverages multiple generator models to efficiently scale test-time compute, showing that diverse generator pool can more effectively explore the solution space. [8] introduces adaptive thought rollback, but does not consider verification granularity. Despite the progress, optimally adapting the verifiers invocation frequency during test-time scaling remains key unanswered question."
        },
        {
            "title": "7 Conclusion",
            "content": "This paper investigates the critical role of the verification granularity in verifier-guided test-time scaling, introducing VG-Search to facilitate exploration. Our experiments show that the optimal verification granularity is highly dynamicdepending on generator capability, task difficulty, and compute budget. Building on these insights, we challenge the current convention and propose adaptive strategies for selecting to optimize efficiency and performance. This work motivates future research that explores more advanced approaches in optimizing the verification granularity g."
        },
        {
            "title": "References",
            "content": "[1] Pranjal Aggarwal and Sean Welleck. L1: Controlling how long reasoning model thinks with reinforcement learning. arXiv preprint arXiv:2503.04697, 2025. [2] AI-MO. Aime 2024, 2024. URL https://huggingface.co/datasets/AI-MO/ aimo-validation-aime. [3] Daman Arora and Andrea Zanette. Training language models to reason efficiently. arXiv preprint arXiv:2502.04463, 2025. [4] Edward Beeching, Lewis Tunstall, and Sasha Rush. Scaling test-time compute with open models, 2024. URL https://huggingface.co/spaces/HuggingFaceH4/ blogpost-scaling-test-time-compute. [5] Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc Le, Christopher Ré, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. [6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [7] Jianhao Chen, Zishuo Xun, Bocheng Zhou, Han Qi, Qiaosheng Zhang, Yang Chen, Wei Hu, Yuzhong Qu, Wanli Ouyang, and Shuyue Hu. Do we truly need so many samples? multi-llm repeated sampling efficiently scale test-time compute. arXiv preprint arXiv:2504.00762, 2025. [8] Sijia Chen and Baochun Li. Toward adaptive reasoning in large language models with thought rollback. arXiv preprint arXiv:2412.19707, 2024. [9] Yinlam Chow, Guy Tennenholtz, Izzeddin Gur, Vincent Zhuang, Bo Dai, Sridhar Thiagarajan, Craig Boutilier, Rishabh Agarwal, Aviral Kumar, and Aleksandra Faust. Inference-aware fine-tuning for best-of-n sampling in large language models. arXiv preprint arXiv:2412.15287, 2024. [10] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [11] NVIDIA Corporation. Nvidia a100 tensor core gpu architecture. Technical report, NVIDIA Corporation, 2020. URL https://images.nvidia.com/aem-dam/en-zz/Solutions/ data-center/nvidia-ampere-architecture-whitepaper.pdf. [12] NVIDIA Corporation. Nvidia h100 tensor core gpu architecture. Technical report, NVIDIA Corporation, 2023. URL https://resources.nvidia.com/en-us-tensor-core. [13] NVIDIA Corporation. Nvidia tools extension library (nvtx). https://github.com/NVIDIA/ NVTX, 2025. Version 3.2.0. [14] Xidong Feng, Ziyu Wan, Muning Wen, Stephen Marcus McAleer, Ying Wen, Weinan Zhang, and Jun Wang. Alphazero-like tree-search can guide large language model decoding and training. arXiv preprint arXiv:2309.17179, 2023. [15] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [16] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. [17] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. 10 [18] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pages 611626, 2023. [19] Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, et al. From system 1 to system 2: survey of reasoning large language models. arXiv preprint arXiv:2502.17419, 2025. [20] Shalev Lifshitz, Sheila McIlraith, and Yilun Du. Multi-agent verification: Scaling test-time compute with multiple verifiers. arXiv preprint arXiv:2502.20379, 2025. [21] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. [22] Runze Liu, Junqi Gao, Jian Zhao, Kaiyan Zhang, Xiu Li, Biqing Qi, Wanli Ouyang, and Bowen Zhou. Can 1b llm surpass 405b llm? rethinking compute-optimal test-time scaling. arXiv preprint arXiv:2502.06703, 2025. [23] Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Tianjun Zhang, Li Erran Li, et al. Deepscaler: Surpassing o1-preview with 1.5 model by scaling rl. Notion Blog, 2025. [24] Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, and Hang Li. Reft: Reasoning with reinforced fine-tuning. arXiv preprint arXiv:2401.08967, 3, 2024. [25] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. [26] NVIDIA Corporation. Nvidia nsight systems, 2025. URL https://developer.nvidia. com/nsight-systems. Accessed: 2025-04-15. [27] Skywork o1 Team. Skywork-o1 open series, 2024. URL https://huggingface.co/ Skywork. [28] OpenAI. Gpt-4 technical report. https://arxiv.org/abs/2303.08774, 2023. arXiv:2303.08774 [cs.CL]. [29] Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, and Aviral Kumar. Rewarding progress: Scaling automated process verifiers for llm reasoning. arXiv preprint arXiv:2410.08146, 2024. [30] Wenlei Shi and Xing Jin. Heimdall: test-time scaling on the generative verification. arXiv preprint arXiv:2504.10337, 2025. [31] Nishad Singhi, Hritik Bansal, Arian Hosseini, Aditya Grover, Kai-Wei Chang, Marcus Rohrbach, and Anna Rohrbach. When to solve, when to verify: Compute-optimal problem solving and generative verification for llm reasoning. arXiv preprint arXiv:2504.01005, 2025. [32] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. [33] DeepSeek Team et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. https://arxiv.org/abs/2501.12948, 2025. arXiv:2501.12948 [cs.CL]. [34] Pablo Villalobos, Anson Ho, Jaime Sevilla, Tamay Besiroglu, Lennart Heim, and Marius Hobbhahn. Will we run out of data? limits of llm scaling based on human-generated data. arXiv preprint arXiv:2211.04325, 2022. [35] Jun Wang, Meng Fang, Ziyu Wan, Muning Wen, Jiachen Zhu, Anjie Liu, Ziqin Gong, Yan Song, Lei Chen, Lionel Ni, et al. Openr: An open source framework for advanced reasoning with large language models. arXiv preprint arXiv:2410.09671, 2024. [36] Teng Wang, Zhangyi Jiang, Zhenqi He, Wenhan Yang, Yanan Zheng, Zeyu Li, Zifan He, Shenyang Tong, and Hailei Gong. Towards hierarchical multi-step reward models for enhanced reasoning in large language models. arXiv preprint arXiv:2503.13551, 2025. [37] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. [38] Wikipedia. Infinite monkey theorem. URL http://en.wikipedia.org/wiki/Infinite_ monkey_theorem. [39] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [40] Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. Demystifying long chain-of-thought reasoning in llms, 2025. URL https://arxiv.org/abs/2502.03373. [41] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:1547615488, 2022. [42] Qiyuan Zhang, Fuyuan Lyu, Zexu Sun, Lei Wang, Weixu Zhang, Zhihan Guo, Yufei Wang, Irwin King, Xue Liu, and Chen Ma. What, how, where, and how well? survey on test-time scaling in large language models. arXiv preprint arXiv:2503.24235, 2025. [43] Jian Zhao, Runze Liu, Kaiyan Zhang, Zhimu Zhou, Junqi Gao, Dong Li, Jiafei Lyu, Zhouyi Qian, Biqing Qi, Xiu Li, et al. Genprm: Scaling test-time compute of process reward models via generative reasoning. arXiv preprint arXiv:2504.00891, 2025."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Optimal Compute Allocation between Proposer and Verifier Optimizing the allocation of computational resources between proposers and verifiers is crucial for maximizing overall system performance. While previous work [31] under fixed compute budget has primarily focused on varying the ratio of samples generated by the proposer to those evaluated by the verifier, the impact of asymmetric model capabilitiesspecifically, how distributing model parameters differently between the proposer and verifier affects performancehas remained largely unexplored. This section addresses this gap by investigating the optimal allocation of model parameters between these components, while adhering to constant total compute budget, to determine how performance scales with their relative complexities. We evaluate two configurations: Small Proposer, Large Verifier (S-P, L-V) and Large Proposer, Small Verifier (L-P, S-V), using DVTS and Beam Search. Performance is measured by accuracy against the number of samples (n) on tasks of varying difficulty (Level 1, Level 5) and averaged (Figure 5). (a) DVTS (b) Beam Search Figure 5: Comparison of Proposer-Verifier configurations under (a) DVTS and (b) Beam Search strategies. Accuracy (%) is plotted against the number of samples (n) for Level 1, Level 5, and Average Accuracy. Blue line: Small Proposer, Large Verifier (S-P, L-V). Orange line: Large Proposer, Small Verifier (L-P, S-V). Key observations include: Verifier Quality Dictates Scaling: With both DVTS and beam search (Figure 5), L-P, S-V starts strong on easier tasks but scales poorly with more samples (n), especially on harder tasks, suggesting that more samples without effective verification is counterproductive.. S-P, L-V shows more robust scaling, indicating strong verifier better utilizes increased compute for broader exploration. weak verifier bottlenecks the scaling. 13 Strategy Determines Optimal Configuration: The best Proposer-Verifier pairing is strategydependent. For DVTS, S-P, L-V generally achieves higher peak performance or scales better for complex tasks. For Beam Search (Figure 5b), L-P, S-V consistently and significantly outperforms. DVTSs diverse exploration of the solution space benefits more from strong verifier, making simpler proposer adequate. Task Difficulty Modulates Preference: On easier tasks (Level 1), S-P, L-V can be competitive, probably because the proposers performance is not bottleneck for easier tasks. However, for harder tasks (Level 5), strong proposer becomes more critical for beam search. These results highlight that optimal compute allocation between proposer and verifier is not fixed but depends on the search strategy, task complexity, and available budget. The observation motivates more advanced compute allocation between verifiers and proposers. A.2 FLOPs Calculation using the Cost Model Following prior work [31], for discriminative PRMs, we assume that CV = 2Pv, since the verifier outputs single score per evaluation. Under this assumption, Equation 3 simplifies to: Ctotal = 2 B1 [T (g 1 + B2) Pg + B2 Pv] , (4) where is average solution length in generation steps, is the average number of tokens per generation step. To simplify compute tracking across experiments, we define normalized compute proxy: C(g, B1, B2) = λ B1 (g 1 + B2) + B1 B2 , (5) where λ = Pg Pv to the actual total compute budget and is used throughout all experiments in the paper. captures the relative cost of generation versus verification. This proxy is proportional A.3 Experiment Details for VG-Search Experiment Setup. To investigate the optimal for given difficulty level and computational budget, we conduct experiments assessing both the oracle and the validation-tuned accuracies for the CM-g and AM-g strategies. We use Qwen2.5-Math-7B [39] as proposer, and Skywork-o1-1.5B [27] as verifier. The number of generations is set to {4, 16, 64, 128, 256}, with Branch-Out Factor B2 = 4, temperature 0.8, and Top-p 1.0. For each generation step, the maximum token number is set to 2, 048, employing Last scoring and majority voting. The number of iterations corresponds to g: for {1, 2, 3, 4}, we set {12, 6, 4, 3} to ensure equal compute budget. The prompt template from Qwen [39] is adopted. DVTS, Best-of-N (BoN), and Beam Search are used as baselines, configured with the same parameters as VG-Search. Prompt Template for Qwen: {question} Please reason step by step, and put your final answer within boxed{} For ideal accuracy, we search for optimal in test dataset for every combination of number of generations and difficulty level, and compute two accuracies for each number of generation: unweighted and weighted, the former corresponds to the dataset having equal number of questions in different difficulty levels, and the latter corresponds to actual difficulty distribution in the dataset. For the searched accuracy, the only difference is that we search in the validation dataset, and obtain the accuracy in the test dataset for searched g. Baselines and Voting Methods. Our codebase is based on HuggingFaces search-and-learn [4]. We adopt three mainstream TTS strategies: Best-of-N [5], Beam Search [32] and DVTS [4]. For all strategies we explore number of samples of 4, 16, 64, 128 and 256. For Beam Search and DVTS we use fixed Branch-Out Factor B2 = 4 following [32]. For all strategies, we use majority voting [37]. Temperature is set to 0.8 for all problems following HuggingFaces setup. 14 Hardware and Frameworks. We conduct our experiments on NVIDIA H100 [12] and A100 [11] GPUs, using CUDA 12.8 on Ubuntu 22.04. The vLLM library (v0.6.3) [18] is employed for model execution. The experiments for VG-Search on MATH-500 take from 1 to 5 hours, depending on the candidate number. To obtain accurate system-level runtime measurements, we use Nsight Systems [26], and employ the NVIDIA NVTX [13] extension to categorize execution time across different models. For the latency measurement in Figure 1c, we use Qwen2.5-Math-1.5B as generator, while using RLHFlow/Llama3.1-8B-PRM-Deepseek-Data as the verifier. A.4 Limitations While our proposed adaptive strategies to optimize the verification granularity based on overall problem characteristics like difficulty and compute budget, key limitation is that the chosen remains fixed throughout the generation process for single problem instance. This approach does not account for potential variations in reasoning complexity or generator confidence within different stages of solving single problem. more sophisticated approach, potentially offering further efficiency gains, would involve dynamically adjusting the granularity on step-by-step basis during inference, adapting verification frequency based on real-time signals. Developing such fine-grained, intra-problem adaptive granularity remains an important direction for future research. 15 A.5 Case Study of VG-Search Figure 6: case study of VG-Search. B1 = 4, B2 = 2, ="
        }
    ],
    "affiliations": [
        "Imperial College London, UK",
        "Institute of Science Tokyo, Japan"
    ]
}