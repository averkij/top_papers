{
    "paper_title": "MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning",
    "authors": [
        "Yuanchen Ju",
        "Yongyuan Liang",
        "Yen-Jen Wang",
        "Nandiraju Gireesh",
        "Yuanliang Ju",
        "Seungjae Lee",
        "Qiao Gu",
        "Elvis Hsieh",
        "Furong Huang",
        "Koushil Sreenath"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Mobile manipulators in households must both navigate and manipulate. This requires a compact, semantically rich scene representation that captures where objects are, how they function, and which parts are actionable. Scene graphs are a natural choice, yet prior work often separates spatial and functional relations, treats scenes as static snapshots without object states or temporal updates, and overlooks information most relevant for accomplishing the current task. To address these limitations, we introduce MomaGraph, a unified scene representation for embodied agents that integrates spatial-functional relationships and part-level interactive elements. However, advancing such a representation requires both suitable data and rigorous evaluation, which have been largely missing. We thus contribute MomaGraph-Scenes, the first large-scale dataset of richly annotated, task-driven scene graphs in household environments, along with MomaGraph-Bench, a systematic evaluation suite spanning six reasoning capabilities from high-level planning to fine-grained scene understanding. Built upon this foundation, we further develop MomaGraph-R1, a 7B vision-language model trained with reinforcement learning on MomaGraph-Scenes. MomaGraph-R1 predicts task-oriented scene graphs and serves as a zero-shot task planner under a Graph-then-Plan framework. Extensive experiments demonstrate that our model achieves state-of-the-art results among open-source models, reaching 71.6% accuracy on the benchmark (+11.4% over the best baseline), while generalizing across public benchmarks and transferring effectively to real-robot experiments."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 9 0 9 6 1 . 2 1 5 2 : r Preprint. MOMAGRAPH : STATE-AWARE UNIFIED SCENE GRAPHS WITH VISIONLANGUAGE MODEL FOR EMBODIED TASK PLANNING Yuanchen Ju1, Yongyuan Liang2, Yen-Jen Wang1, Nandiraju Gireesh, Yuanliang Ju3 Seungjae Lee2, Qiao Gu3, Elvis Hsieh1, Furong Huang2, Koushil Sreenath1 1University of California, Berkeley 3University of Toronto Project website: https://HybridRobotics.github.io/MomaGraph/ 2University of Maryland, College Park Figure 1: Overview of the MomaGraph. Given task instruction, MomaGraph constructs taskspecific scene graph that highlights relevant objects and parts along with their spatial-functional relationships, enabling the robot to perform spatial understanding and task planning."
        },
        {
            "title": "ABSTRACT",
            "content": "Mobile manipulators in households must both navigate and manipulate. This requires compact, semantically rich scene representation that captures where objects are, how they function, and which parts are actionable. Scene graphs are natural choice, yet prior work often separates spatial and functional relations, treats scenes as static snapshots without object states or temporal updates, and overlooks information most relevant for accomplishing the current task. To address these limitations, we introduce MomaGraph, unified scene representation for embodied agents that integrates spatial-functional relationships and part-level interactive elements. However, advancing such representation requires both suitable data and rigorous evaluation, which have been largely missing. We thus contribute MomaGraph-Scenes, the first large-scale dataset of richly annotated, task-driven scene graphs in household environments, along with MomaGraph-Bench, systematic evaluation suite spanning six reasoning capabilities from high-level planning to finegrained scene understanding. Built upon this foundation, we further develop MomaGraph-R1, 7B visionlanguage model trained with reinforcement learning on MomaGraph-Scenes. MomaGraph-R1 predicts task-oriented scene graphs and serves as zero-shot task planner under Graph-then-Plan framework. Extensive experiments demonstrate that our model achieves state-of-the-art results among open-source models, reaching 71.6% accuracy on the benchmark (+11.4% over the best baseline), while generalizing across public benchmarks and transferring effectively to real-robot experiments. Equal Contribution, Equal Advising 1 Preprint."
        },
        {
            "title": "INTRODUCTION",
            "content": "When mobile manipulators (Qiu et al., 2024; Honerkamp et al., 2024a; Wu et al., 2023; Zhang et al., 2024a) enter household environments, they face the fundamental challenge of understanding how the environment works, which objects are interactive, and how they can be used. In other words, such robots must not only be capable of navigating through the home, but also manipulate objects within it. While navigation requires modeling the overall spatial layout, manipulation demands capturing more fine-grained object affordances (Ju et al., 2024; Zhu et al., 2025). This naturally raises central question: What is the most effective, compact, and semantically rich representation of an indoor scene? An intuitive answer is the scene graph, which organizes objects and their relationships in scene through graph structure (Armeni et al., 2019; Koch et al., 2024a;b) and has shown great potential in various downstream robotic applications (Rana et al., 2023; Werby et al., 2024; Ekpo et al., 2024). However, existing scene graphs suffer from notable limitations. (1) Their edges typically encode only single type of relationship, either spatial (Jatavallabhula et al., 2023; Gu et al., 2024a; Loo et al., 2025) or functional (Zhang et al., 2025; Dong et al., 2021)(e.g., remote controlling TV, knob adjusting parameters). Relying solely on spatial relationships captures geometric layout but overlooks operability, while relying solely on functional relationships ignores spatial constraints, leading to incomplete and less actionable structures. (2) Most existing methods (Wu et al., 2021; Takmaz et al., 2025; Zhang et al., 2021) are limited to static scenes and struggle to adapt to dynamic environments where object positions change or object states change. (3) They lack task relevance, as they fail to emphasize information directly tied to task execution, thereby reducing efficiency and effectiveness. In contrast, cognitive science research (Uithol et al., 2021; Kondyli et al., 2020; Castanheira et al., 2025) shows that human perception in new environments is both dynamic and task-oriented. Humans do not process all information equally; instead, they flexibly adjust their attention according to the current task. This process is similar to browsing map on an iPad: people first take broad view to roughly locate the area of interest, and then zoom in to focus on the specific details needed for the task. Motivated by these insights, we emphasize that an ideal scene graph should integrate both spatial and functional relationships, include fine-grained object parts as nodes, making the representation compact, adaptive to dynamic changes, and highly aligned with task instructions, thus providing more concrete guidance for embodied perception and task planning. To achieve this goal, we present MomaGraph, novel scene representation specifically designed for embodied agents. It is the first to unify spatial and functional relationships while introducing partlevel interactive nodes, providing more fine-grained, compact, and task-relevant structured representation than existing approaches. To support this representation, we build MomaGraph-Scenes, the first dataset that jointly models spatial and functional relationships with part-level annotations, encompassing multi-view observations, executed actions, and their interactive object parts, and taskaligned scene graph annotations. trained with the DAPO (Yu et al., 2025) Building on this foundation, we propose MomaGraph-R1, 7B visionlanguage model (VLM) learning algorithm on MomaGraph-Scenes. We design graph-alignment reward function to guide the model toward constructing accurate, task-oriented scene graphs. MomaGraph-R1 not only predicts scene graphs but also serves as zero-shot task planner within Graph-then-Plan framework: the model first generates structured scene graph as an intermediate representation and then performs task planning based on this graph, significantly improving reasoning effectiveness and interpretability. reinforcement Despite progress in task-graph planning (Agia et al., 2022), the community still lacks unified benchmark to systematically evaluate whether and how task-oriented scene graphs improve planning performance. To address this gap, we introduce MomaGraph-Bench, comprehensive evaluation suite that systematically assesses six key reasoning capabilities, spanning from high-level task planning to fine-grained scene understanding. In summary, our work makes the following key contributions: We propose MomaGraph, the first scene graph representation that jointly models spatial and functional relationships while incorporating part-level interactive nodes, providing compact, dynamic, and task-aligned knowledge structure for embodied intelligence. 2 Preprint. We construct MomaGraph-Scenes, the first large-scale dataset of richly annotated, task-driven scene graphs in household environments, and build MomaGraph-Bench, unified evaluation suite that systematically measures the impact of scene graph representations on task planning across six core reasoning capabilities. We develop MomaGraph-R1, 7B vision-language model that leverages reinforcement learning to optimize spatialfunctional reasoning, enabling zero-shot planning in Graph-then-Plan paradigm. MomaGraph-R1 surpasses all open-source baseline models, delivering substantial gains across public benchmarks and translating these improvements into strong generalization and effectiveness in real-world robotic experiments."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Scene Graphs for 3D Indoor Scene Understanding. Scene graphs have emerged as structured and hierarchical representation in autonomous driving (Zhang et al., 2024b; Greve et al., 2024), robot manipulation (Jiang et al., 2024; Wang et al., 2025; Engelbracht et al., 2024; Jiang et al., 2025), and spatial intelligence (Yin et al., 2025; Zemskova & Yudin; Liang et al., 2025a;b) community. They function not only as means of scene representation but also as critical bridge between spatial understanding (Cao et al., 2024; Yang et al., 2024; Gu et al., 2024b)and action planning. We focus on the household scenes. However, existing works often focus on single type of scene graphs. For example, ConceptGraphs (Gu et al., 2024a) primarily model spatial layouts, representing object instances and their geometric relations in an open-vocabulary manner. While spatial graphs (Honerkamp et al., 2024b; Yan et al., 2025) provide useful geometric and semantic grounding, they overlook how objects can functionally interact with one another. Conversely, functional graphs (Li et al., 2021; Dong et al., 2021; Zhang et al., 2025) highlight object affordances and control relations but do not capture the overall spatial structure. Relying solely on either spatial or functional graphs leads to incomplete and less actionable representations. This motivates us to build MomaGraph, which unifies spatial and functional relationships, incorporates part-level nodes, and explicitly models state changes, providing more comprehensive foundation for embodied task planning. Zero-shot Embodied Task Planning with VLMs. VLMs (OpenAI, 2023; Team et al., 2025; Ahn et al., 2022) have gained significant attention in robotic task planning (Niu et al., 2024; Yue et al., 2024; Lu et al., 2023; Liang et al., 2024; Guo et al., 2024) due to their powerful capabilities in processing multimodal inputs, such as images and language instructions. However, when directly used as task planners, VLMs (Huang et al., 2023; 2024; Ahn et al., 2022; Zheng et al., 2025a; Yang et al., 2025) often suffer from sensitivity to visual noise and shallow semantic grounding; more fundamentally, their lack of structured objectrelationship representations necessitates extracting or constructing more effective representations from the same visual inputs to support accurate and reliable high-level planning. Prior approaches such as SayPlan (Ahn et al., 2022) assume access to reliable 3D scene graph, which is often unrealistic in practice. To overcome this gap, we propose the Graph-then-Plan strategy, which first generates task-specific scene graphs as an intermediate structured representation before high-level planning. By explicitly modeling objects and their relations, this approach significantly improves the accuracy and robustness of task planning. Unlike prior graph-then-plan methods (Dai et al., 2024; Ekpo et al., 2024) that either assume reliable scene graphs or treat graph construction and planning as separate modules, our approach enables single VLM to jointly generate structured, task-oriented scene graphs and perform high-level planning."
        },
        {
            "title": "3 PRELIMINARY FINDINGS AND MOTIVATION EXPERIMENTS",
            "content": "To ground our analysis, before the full evaluations we perform two motivating experiments on the MomaGraph-Bench. These comparisons are designed to validate our motivation and design principles, and to reveal why our proposed model is essential for embodied task planning. In this section, we aim to answer the following questions. 3.1 ARE VLMS RELIABLE FOR DIRECT PLANNING WITHOUT SCENE GRAPHS? To examine whether direct planning from visual inputs is reliable even for strong closed-source VLMs, we design controlled evaluations on real-world household tasks such as Open the window 3 Preprint. Figure 2: Direct planning often fails even for strong closed-source models like GPT-5, producing wrong actions or missing key steps, while our Graph-then-Plan approach with structured scene graphs enables accurate and complete task sequences aligned with ground truth. and Obtain clean boiled water. In these scenarios, models must reason over functional relationships, spatial constraints, and multi-step dependencies (e.g., plug-in before activation, filtration before boiling). As shown in Fig. 2, despite their scale, closed-source VLMs like GPT-5 produce incorrect or incomplete plans, missing prerequisite steps, or misidentifying interaction types. In contrast, our Graph-then-Plan approach, which first generates task-specific scene graph and then performs planning, consistently produces correct and complete action sequences aligned with ground-truth logic. This demonstrates that incorporating structured scene representations significantly enhances planning accuracy and robustness beyond what direct planning can achieve. Preliminary Findings 1 In contrast to directly relying on vision-language models for task planning from raw scene images, our Graph-then-Plan strategywhich incorporates task-oriented scene graph generation as an intermediate structured representation prior to high-level planning, substantially improves both the accuracy and robustness of task planning. 3.2 ARE SINGLE-RELATIONSHIP GRAPHS ADEQUATE FOR EMBODIED AGENTS? To ensure fair comparison, we retrain our model using the same graph structure as in MomaGraph, but constrain the edge types to encode only single kind of relationeither spatial or functional. This setup allows us to isolate the effect of relation types while keeping the graph topology consistent, thereby directly examining whether single-relation representations are sufficient for task planning. To ensure this finding generalizes beyond one specific architecture, we evaluate this comparison across different base models using the same dataset and experimental configurations. As demonstrated in Table 1, both MomaGraph-R1(trained from Qwen-2.5-VL-7B) and LLaVA-Onevision consistently show superior performance with unified spatial-functional scene graphs compared to single-relationship variants, supporting our hypothesis that integrated representations are essential for effective embodied task planning. Detailed training methodology is described in the Sec. 4.2. Table 1: Comparison between MomaGraph-R1and LLaVA variants across task tiers. Models T1 T2 T3 T4 Overall Models T2 T3 T4 Overall MomaGraph-R1 (Spatial-only) 69.1 67.0 58.4 45.4 MomaGraph-R1 (Functional-only) 71.4 65.8 63.6 59.0 76.4 71.9 70.1 68.1 MomaGraph-R1 (Unified) 59.9 64.9 71.6 LLaVA-Onevision (Spatial-only) 63.4 56.7 59.7 36.3 LLaVA-Onevision (Functional-only) 65.1 61.7 55.8 45.4 68.6 62.9 67.5 56.5 LLaVA-Onevision (Unified) 54.0 57.0 66.0 4 Preprint. Preliminary Findings 2 Graph representations that rely solely on spatial relationships or solely on functional relationships are insufficient. For embodied agents, unified representation that jointly models both spatial and functional relationships provides more complete and effective foundation for perception and action."
        },
        {
            "title": "4.1 MOMAGRAPH DEFINITION\nGiven a single indoor room, the agent receives as input a set of multi-view images {Ii}n\ni=1 and a nat-\nural language instruction T . The objective is to construct an instruction-conditioned, task-oriented\nscene graph GT = (NT , E T\nf ). Here, NT denotes the set of nodes representing objects relevant\nto task T . E T\nf captures their functional\nrelationships. This task-oriented scene graph provides a minimal yet sufficient structured represen-\ntation that grounds the instruction T in the observed scene and facilitates downstream embodied\ntask planning. Both E T\nf are modeled as directed edges, pointing from the triggering object\nto the affected object.",
            "content": "s encodes the spatial relationships among these nodes, and and , 4.2 VLMS LEARN SCENE GRAPH REPRESENTATIONS WITH REINFORCEMENT LEARNING Existing open-source VLMs have demonstrated limited capability in generating accurate taskoriented scene graphs GT from multi-view observations {Ii}n i=1 and natural language instructions . VLMs do not form structured spatial-functional representations or reason effectively about taskrelevant object relationships needed for embodied tasks. To go further, we want to know: Can reinforcement learning teach VLMs to build more precise and task-relevant scene graph representations with MomaGraph? Reinforcement learning offers more principled approach by encouraging the model to explore, reason, and iteratively refine its representations through outcome-driven feedback. Rather than replicating memorized patterns, RL enables models to discover effective strategies for constructing task-relevant scene graphs through structured thinking and reasoning. We apply the DAPO (Yu et al., 2025). The key innovation lies in our carefully designed graph-based reward function R(Gpred denote the predicted and ground truth task-oriented scene graphs, respectively, which evaluates how well predicted graphs embody these principles through three key components. ), where Gpred , Ggt and Ggt Action type prediction. Given the task instruction , we ensure correct prediction of the required action type through Raction = I[apred = agt], where apred and agt denote the predicted and ground truth action types, respectively. Spatial-functional integration on edges. We jointly evaluate both spatial relationships functional relationships truth edge sets: and gt represent the predicted and ground within each edge, where pred and Redges = 1 gt (cid:88) ej gt max eiE pred Sedge(ei, ej) (1) where Sedge(ei, ej) measures semantic similarity between edges ei and ej based on their spatial and functional relationship labels. Node completeness. We compute intersection-over-union similarity for task-relevant objects in NT , where pred denote the predicted and ground truth sets of task-relevant nodes: Rnodes = N gt pred gt pred and gt . The final reward function integrates these task-oriented design principles with format validation and length control, where Rformat ensures valid JSON structure and Rlength penalizes overly verbose outputs: R(Gpred , Ggt ) = wa (Raction + Redges + Rnodes) + wf Rformat + wl Rlength (2) 5 Preprint. where wa, wf , and wl are hyperparameters controlling the relative importance of each component. ) and functional relationships (E This reward design directly implements our core insight: scene graphs must simultaneously capture spatial layout (E ) while remaining tightly coupled to task requirements (T ). With RL training on MomaGraph-Scenes, we develop MomaGraph-R1, 7B vision-language model built on Qwen2.5-VL-7B-Instruct (Qwen, 2025), which learns to generate compact, task-relevant representations that provide concrete guidance for embodied planning. We demonstrate that RL significantly enhances both the effectiveness and generalizability of opensource VLMs for scene graph generation in the following section. This aligns with broader findings that combining structured scene representations with reasoning consistently improves VLM scene understanding. Critically, MomaGraph-R1 achieves robust performance across diverse environments and task configurations, enabling practical deployment in unseen embodied scenarios."
        },
        {
            "title": "4.3 STATE-AWARE DYNAMIC SCENE GRAPH UPDATE",
            "content": "In realistic environments, multiple objects of the same category may coexist, and their task-related correspondences are often initially uncertain. Take Figure 3 as an example, kitchen stove may have several knobs, but only one controls the burner required for the current cooking task. Simply relying on visual appearance is insufficient to determine the correct functional relationship. In this work, we do not focus on the agents interaction policy; instead, our emphasis lies on how to capture and incorporate observed state changes in the environment into the scene graph to resolve such ambiguities. Formally, at time step t, the task-oriented scene graph is represented as: = (cid:0)N (t) G(t) , ,(t) , ,(t) (3) (cid:1), where (t) denotes the set of task-relevant candidate objects, ,(t) encodes their spatial layout, and ,(t) captures hypothesized functional relationships, which may initially include one-to-many mappings. After the agent executes an action at and observes the new environment state st+1, the scene graph is refined as: G(t+1) = (cid:16) G(t) , at, st+1 (cid:17) , (4) Figure 3: MomaGraph captures state changes in the environment and dynamically updates the task-specific scene graph accordingly, enabling the graph to evolve as interactions occur and reflecting updated spatialfunctional relationships. where the update function U() removes inconsistent hypotheses and strengthens confirmed correspondences based on the observed state transition. As illustrated in Fig. 3, if rotating specific knob ignites the burner while others have no effect, the functional edge [control] between that knob and the burner is established, while edges from other knobs are pruned. This process enables the scene graph to evolve from ambiguous, one-to-many hypotheses into compact, state-aware dynamic representation with unique and reliable object-to-object correspondences."
        },
        {
            "title": "5 DATASET AND BENCHMARK",
            "content": "5.1 MOMAGRAPH-SCENES DATASET Existing scene graph datasets for 3D indoor environments are often constrained to single relationship: some focus exclusively on spatial layouts of objects (Armeni et al., 2019; Koch et al., 2024b), while others emphasize functional interactions (Dong et al., 2021; Zhang et al., 2025). However, these scene graph representations that are restricted to single relationship type are insufficient for embodied agents, as task execution in household environments requires reasoning about both where objects are and how they can be used. To address these limitations, we introduce MomaGraph-Scenes, the first dataset designed to provide more comprehensive and task-relevant scene representation. MomaGraph-Scenes jointly encodes spatial relationships and functional relationships, covering 9 spatial relationship types and 6 functional relationship 6 Preprint. types, explicitly representing interactive elements such as handles and buttons. Our dataset consists of approximately 1,050 task-oriented subgraphs and 6278 multi-view RGB images, collected from combination of manually collected real-world data, re-annotated existing datasets (Zhang et al., 2025; Delitzas et al., 2024), and simulated environments built with AI2-THOR (Kolve et al., 2017). These samples span more than 350 diverse household scenes and encompass 93 distinct task instructions. Compared with prior datasets, our annotations are significantly more detailed, and capturing interaction semantics at both the object and part levels. This broad coverage ensures rich variability in scene layouts, object configurations, and interaction types, supporting robust learning and evaluation of embodied reasoning."
        },
        {
            "title": "5.1.1 DATASET ANNOTATION",
            "content": "Multi-View Observation. The multi-view images provided for each graph are not constrained to always contain every relevant object within each single view. We also do not impose restrictions on the number of viewpoints or their exact configurations. This flexible setup better reflects realistic perception conditions, where embodied agents must reason across partial and diverse observations to build consistent scene graph representations. Task Instruction. It is worth noting that the task instructions in our dataset do not explicitly mention all the objects required to accomplish the task. Instead, they are expressed in simple and natural forms (e.g., Fill the bathtub), where the relevant objects such as the bathtub, faucet, and button must be inferred by the model. This design encourages the model to learn how to ground natural instructions into the appropriate set of objects and relationships, rather than relying on object names being explicitly stated. Nodes. NT primarily consists of the objects necessary to accomplish the instruction. When the task execution requires interacting with specific parts, the graph may additionally include part-level interactive elements (e.g., handles, knobs, or buttons). For example, for the instruction Open the fridge, NT includes both the fridge and its handle; for the instruction Turn on the light, NT consists of the switch and the ceiling light. Edges. Edges in the task-oriented scene graph capture both functional and spatial relationships between nodes. Functional Relationships. We define functional relationship as the ability of one object to change the state of another object. In indoor environments, common tasks can be broadly categorized as Parameter Adjustment, Device Control, Open/Close the Cabinet or Door, Water Flow Control, Power Supply, and Assembly. Accordingly, we identify six major types: [OPEN OR CLOSE], [ADJUST], [CONTROL], [ACTIVATE], [POWER BY], [PAIR WITH]. Notably, [PAIR WITH] does not alter the internal state of objects but instead modifies their spatial configuration, which is essential for assembly tasks (Qi et al., 2025). Since such tasks are critical for robotic interaction and task planning, we explicitly include [PAIR WITH] as functional relationship. Through this definition, our dataset extends beyond physical and electronic interactions to encompass fine-grained reasoning about assembly and pairing, enhancing its utility for downstream action execution and planning. Spatial Relationships. Capture geometric dependencies between objects and parts. The dataset [LEFT OF], [RIGHT OF], [IN FRONT OF], [BEHIND], [HIGHER THAN], primarily annotates: Directional: [LOWER THAN]. Distance-based: [CLOSE], [FAR], [TOUCHING]. These annotations provide the rich context necessary for reasoning about layout, reachability, and interaction feasibility. 5.2 MOMAGRAPH BENCHMARK AND EVALUATION We introduce MomaGraph-Bench, the first benchmark that jointly evaluates fine-grained scene understanding and task planning abilities across diverse levels of difficulty. Our design principle for MomaGraph-Bench is to evaluate whether advances in scene understanding provide tangible improvements in downstream task planning and reasoning. Our evaluation framework examines six essential reasoning capabilities in four tiers of difficulty levels: (1) Action Sequence Reasoning, (2) 7 Preprint. Figure 4: Examples of evaluation Multi-Choices VQA tasks in the MomaGraph-Bench. We showcase example questions covering six core reasoning capabilities. Beyond these core capabilities, we further design tasks on Dynamic Verification and Long-horizon Task Decomposition to evaluate temporal reasoning and multi-steps planning. Spatial Reasoning, (3) Object Affordance Reasoning, (4) Precondition & Effect Reasoning, (5) Goal Decomposition, and (6) Visual Correspondence (with concrete examples shown in Fig. 4). Action Sequence Reasoning: examines whether models understand the order and dependency of actions and can plan efficient sequences. Spatial Reasoning: focuses on reasoning over spatial relations such as left of or in front of, judging reachability, and selecting the most suitable object among candidates. Object Affordance Reasoning: evaluates whether models can infer the functionality of objects (e.g., knobs can be turned, cabinets can be opened), match objects to task requirements, and reason about indirect tool use. Precondition & Effect Reasoning: assesses whether models understand the preconditions and effects of actions, such as door needing to be closed before it can be opened, and can predict possible side effects. Goal Decomposition: measures the ability to break down complex tasks into sub-goals, prioritize them, and determine parallel versus sequential execution strategies. Visual Correspondence (extended capability): tests whether models can maintain object consistency across multiple views and integrate information under viewpoint changes. MomaGraph-Bench is formulated as multi-choice VQA task which comprises 294 diverse indoor scenes with 1,446 multi-view images, featuring 352 task-oriented scene graphs spanning 1,315 instances that range from simple step object manipulation(Tier 1) to complex multi-step replanning (Tier 4) scenarios (detailed breakdown in Appendix A.4). MomaGraph-Bench offers the most comprehensive assessment for embodied agents capacity to generalize across tasks and scenarios. To ensure that the evaluation truly reflects generalization rather than memorization, all scenarios are drawn from entirely unseen environments."
        },
        {
            "title": "6 EXPERIMENTS",
            "content": "6.1 BENCHMARK EVALUATION FOR EMBODIED TASK PLANNING We compare the performance of our MomaGraph-R1 with other models across all task tiers in MomaGraph-Bench to rigorously assess embodied planning, including state-of-the-art closed 8 Preprint. Table 2: Performance comparison on the MomaGraph-Bench. We report accuracy (%) across four tiers (T1T4) and the overall score, with and without graph-based reasoning. Type Models Params Tier 1 Tier 2 Tier 3 Tier 4 Overall MomaGraph Benchmark w/o Graph w/ Graph w/o Graph w/ Graph w/o Graph w/ Graph w/o Graph w/ Graph w/o Graph w/ Graph o c S O u Claude-4.5-Sonnet GPT-5 Gemini-2.5-Pro InstructBLIP-7B LLaVA-V1.5-7B DeepSeek-VL2 InternVL2.5-8B LLaVA-Onevision-7B Qwen2.5-VL-7B-Instruct MomaGraph-R1(Ours) - - - 7B 7B 4.5B 8B 7B 7B 7B 77.3 77.3 76.6 43.1 51.0 54.2 53.6 60.0 62.1 70.2 83.7 79.8 79. 44.1 53.4 56.9 51.0 63.8 66.3 76.4 67.0 63.4 65.8 42.6 46.3 51.2 51.2 52.4 58.5 65.8 70.3 68.2 69.5 41.4 48.7 53.6 53.0 56.0 58.5 71.9 69.7 70.8 67. 38.6 40.2 61.8 55.8 58.4 51.9 63.6 72.3 75.0 72.7 36.3 36.3 61.3 59.7 59.2 57.1 70.1 65.2 54.5 60.8 31.8 38.9 40.9 33.3 43.4 56.5 60.8 69.5 63.6 65. 36.3 40.9 45.4 40.9 43.4 59.0 68.1 69.8 66.5 67.6 39.0 44.1 52.0 48.4 53.5 57.2 65.1 73.9 71.6 71.6 39.5 44.8 54.3 51.1 55.6 60.2 71.6 Table 3: Performance comparison on the BLINK and MomaGraph-Bench. By enforcing multiview consistency, our method significantly improves correspondence reasoning across all opensource models. Model GPT-5 LLaVA-Onevision Qwen2.5-VL-7B-Instruct DeepSeek-VL2 MomaGraph-R BLINK MomaGraph-Bench BLINK MomaGraph-Bench BLINK MomaGraph-Bench BLINK MomaGraph-Bench BLINK MomaGraph-Bench Results 66.1 81.2 59.7 70. 58.7 72.7 57.4 68.4 63.5 77. source models (Claude-4.5-Sonnet, GPT-5, Gemini-2.5-Pro) and leading open source models (InstructBLIP, LLaVA-V1.5, DeepSeek-VL2, InternVL2.5, LLaVA-OneVision, Qwen2.5). We further examine whether Graph-then-Plan brings performance gains by evaluating each model under two controlled settings: (i) Direct Plan (w/o Graph): the model is directly evaluated on task planning in MomaGraph-Bench using multi-view observations and instructions; (ii) Graph-then-Plan (w/ Graph): the model first generates task-oriented scene graph GT , capturing nodes, spatial and functional edges, and action types, and then performs task planning conditioned on the graph. 6.1.1 RESULT ANALYSIS. The results in Table 2 yield several key insights: (1) Effectiveness of Graph-then-Plan. Across all models, the w/ Graph setting consistently outperforms the w/o Graph baseline, demonstrating that explicitly structuring task-oriented scene graphs provides tangible benefit for downstream planning. This validates our central hypothesis that disentangling scene representation from action generation improves reasoning reliability. (2) Competitiveness of MomaGraph-R1. Our MomaGraph-R1 achieves performance on par with closed-source giants like Claude-4.5-Sonnet and GPT-5, while clearly surpassing all leading opensource VLMs. Notably, MomaGraph-R1 delivers +11.4% relative improvement over its base model (Qwen2.5-VL-7B) under w/ Graph, highlighting the effectiveness of reinforcement learning with graph-based rewards. (3) Scalability with Task Complexity. As task complexity increases from Tier 1 to Tier 4, the performance of most open-source baselines drops sharply, reflecting their limited ability to generalize to multi-step reasoning. In contrast, MomaGraph-R1 exhibits much smaller degradation, preserving strong performance in Tier 3 and Tier 4. This indicates superior scalability to long-horizon planning scenarios, crucial capability for embodied agents. (4) General Trend Across Communities. Closed-source models still maintain the highest absolute performance, benefiting from larger-scale pretraining and proprietary data. However, the consistent gap reduction achieved by MomaGraph-R1 shows that reinforcement learning with graphstructured intermediate representations can substantially narrow the divide, offering practical path toward competitive open-source systems. 6.2 BENCHMARK EVALUATION FOR VISUAL CORRESPONDENCE As the model learns scene representations from multi-view observations, it exhibits an emergent ability of cross-view consistency , which can reason about the same point across different viewpoints. 9 Preprint. Figure 5: Real Robot experiments on the RobotEra Q5 with D455, demonstrating four household tasks that require spatial, functional, and part-level interactive elements reasoning for task execution. This capability is most evident in visual correspondence tasks. As shown in Table 3, we compare model performance on visual correspondence tasks from public benchmark BLINK Fu et al. (2024) and our MomaGraph-Bench. Scene graph representations enhance performance universally by reducing VLM hallucinations in visual perception. By prompting models to first generate structured scene graphs (w/ Graph) and then answer questions in single-turn interactions, we force them to explicitly reason about spatial and functional relationships between objects before answering. We primarily evaluate perception on multi-view reasoning and visual correspondence tasks from BLINK, as well as multi-view correspondence in MomaGraph-Bench. Our MomaGraph-R1 achieves state-of-the-art performance among open-source VLMs, leading by 3.8% on BLINK and 4.8% on our correspondence benchmark compared to the best competing open-source models. These results confirm that MomaGraph-R1 enables more nuanced and detailed perception of complex indoor scenes, effectively mitigating hallucinations and enabling more reliable scene perception. 6.3 REAL ROBOT DEMONSTRATIONS Setup. To validate the effectiveness of our model in real-world settings, we deploy on the RobotEra Q5, bimanual humanoid platform with mobile base. An Intel RealSense D455 camera is mounted to enhance RGB-D perception. Importantly, all evaluation scenes are unseen, ensuring that performance reflects true generalization. Tasks. We design four representative tasks (Figure 5), consisting of two local interactions (e.g., opening cabinet, opening microwave) and two remote interactions (e.g., turning on the TV, turning off light). Deployment. Prior to execution, the robot performs active perception by adjusting its head pose to acquire multi-view observations. MomaGraph-R1 processes these observations together with the task instruction to generate task-specific subgraph, which explicitly encodes the relevant objects and their spatialfunctional relationships, see more deployment details in B.3. Following the Graph-then-Plan paradigm, MomaGraph-R1 then functions as task planner, producing structured action sequence. These specifications are subsequently instantiated as low-level trajectories through library of parameterized primitive skills. We note that the primitive skills are task-specific and derived from teleoperation data for each scenario; the primary contribution of this work lies in the high-level planning and scene graph generating enabled by MomaGraph-R1. Summary. Our real-world evaluations show that MomaGraph-R1 delivers robust scene understanding and task planning even in unseen scenarios, while remaining directly compatible with standard mobile humanoid systems. This combination underscores the strength of our model and its practicality for real-world deployment. 10 Preprint."
        },
        {
            "title": "6.4 QUANTITATIVE REAL-ROBOT EVALUATION",
            "content": "To provide rigorous quantitative validation of our systems robustness, we conduct comprehensive evaluation on complex multi-step long-horizon task. This evaluation includes success rates and failure analysis across different stages to validate overall system performance under realistic, sequential conditions (see Figure 6). Figure 6: Quantitative real-robot evaluation. (a) Environment setup of the real-robot experiment. (b) Failure analysis illustrating success/failure rates across different reasoning stages. Task Setup. We evaluate the following natural language instruction that requires sequential reasoning and manipulation: need better lighting. Turn on the light closest to the remote so can find it and turn on the monitor to watch. To assess system robustness, we conducted 10 experimental trials, changing the camera viewpoint in each trial. This task requires spatial reasoning (finding the switch and the remote), functional understanding (linking switches, lights, remote, monitor), and state-dependent planning (lighting affects perception). Additionally, theres object uncertainty (multiple similar lamps or switches), complex spatial relations between objects, and sequential manipulation under partial observability. Results. As shown in Figure 6, our system achieves an 80% success rate in graph generation, 87.5% success rate in planning (conditioned on correct graphs), and an overall task success rate of 70% over 10 trials. The main failure modes were: (1)spatial relation errors or missing nodes during graph generation; and (2) action sequencing errors in the planning phase, suggesting that the system sometimes plans the right actions but in suboptimal order. These results demonstrate that MomaGraph remains robust across multiple reasoning and execution stages, achieving 70% overall success rate on complex multi-step task. This validates the systems reliability under realistic long-horizon conditions where errors can compound across stages."
        },
        {
            "title": "7 CONCLUSION\nThis work addresses to the fundamental limitations of existing scene graphs for embodied agents:\nreliance on a single type of relationship, inability to adapt to dynamic environments, and lack of\ntask relevance. To overcome these issues, we introduce MomaGraph, a novel scene representation\nthat unifies spatial and functional scene graphs with interactive elements. To learn this represen-\ntation, we construct a large-scale dataset MomaGraph-Scenes and propose MomaGraph-R1,\na 7B VLM trained with reinforcement learning, which predicts task-oriented scene graphs and\nserves as a zero-shot task planner under a Graph-then-Plan framework. Furthermore, we design\nthe MomaGraph-Bench, a comprehensive benchmark that rigorously evaluates both fine-grained\nreasoning and high-level planning. Through extensive experiments, we demonstrate that our ap-\nproach achieves state-of-the-art performance among open source models, remains competitive with\nclosed source systems, and transfers effectively to public benchmarks and real robot experiments.\nWe hope that MomaGraph will serve as a foundation for advancing scene representations, fostering\nstronger connections between the spatial VLM and robotics communities, and ultimately enabling\nmore intelligent and adaptive embodied agents.",
            "content": "11 Preprint."
        },
        {
            "title": "8 ACKNOWLEDGEMENTS",
            "content": "We would like to express our heartfelt thanks to Chenyangguang Zhang, Prof. Florian Shkurti, and Prof. Tom Silver for their insightful suggestions and constructive feedback. We also thank Guowei Zhang, Yuman Gao, Bike Zhang, Gechen Qu, Lihan Zha, Yuanhang Zhang, and Yu Qi for their valuable assistance in the collection of benchmark data. We thank Robot Era for providing their Q5 Mobile Manipulator for our experiments. Liang, Lee and Huang are supported by DARPA Transfer from Imprecise and Abstract Models to Autonomous Technologies (TIAMAT) 80321, DARPA HR001124S0029-AIQ-FP-019, DODAFOSR-Air Force Office of Scientific Research under award number FA9550-23-1-0048, National Science Foundation NSF-IIS-2147276 FAI, National Science Foundation NAIRR240045, National Science Foundation TRAILS Institute (2229885). Private support was provided by Peraton and Open Philanthropy. The work by Ju, Wang, and Sreenath was supported by The Robotics and AI Institute."
        },
        {
            "title": "REFERENCES",
            "content": "Christopher Agia, Krishna Murthy Jatavallabhula, Mohamed Khodeir, Ondrej Miksik, Vibhav Vineet, Mustafa Mukadam, Liam Paull, and Florian Shkurti. Taskography: Evaluating robot task planning over large 3d scene graphs. In Conference on Robot Learning, pp. 4658. PMLR, 2022. Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. Do as can, not as say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022. Iro Armeni, Zhi-Yang He, JunYoung Gwak, Amir Zamir, Martin Fischer, Jitendra Malik, and Silvio Savarese. 3d scene graph: structure for unified semantics, 3d space, and camera. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 56645673, 2019. Yang Cao, Yuanliang Jv, and Dan Xu. 3dgs-det: Empower 3d gaussian splatting with boundary guidance and box-focused sampling for 3d object detection. arXiv preprint arXiv:2410.01647, 2024. Jason da Silva Castanheira, Nicholas Shea, and Stephen Fleming. How attention simplifies mental representations for planning. arXiv preprint arXiv:2506.09520, 2025. Zhirui Dai, Arash Asgharivaskasi, Thai Duong, Shusen Lin, Maria-Elizabeth Tzes, George Pappas, and Nikolay Atanasov. Optimal scene graph planning with large language model guidance. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pp. 1406214069. IEEE, 2024. Alexandros Delitzas, Ayca Takmaz, Federico Tombari, Robert Sumner, Marc Pollefeys, and Francis Engelmann. SceneFun3D: Fine-Grained Functionality and Affordance Understanding in 3D Scenes. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. Ang Dong, Li Feng, Dengcheng Yang, Shuang Wu, Jinshuai Zhao, Jing Wang, and Rongling Wu. Fungraph: statistical protocol to reconstruct omnigenic multilayer interactome networks for complex traits. Star Protocols, 2(4):100985, 2021. Daniel Ekpo, Mara Levy, Saksham Suri, Chuong Huynh, and Abhinav Shrivastava. Verigraph: Scene graphs for execution verifiable robot planning. arXiv preprint arXiv:2411.10446, 2024. Tim Engelbracht, Rene Zurbrugg, Marc Pollefeys, Hermann Blum, and Zuria Bauer. Spotlight: Robotic scene understanding through interaction and affordance detection. arXiv preprint arXiv:2409.11870, 2024. Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. In European Conference on Computer Vision, pp. 148166. Springer, 2024. 12 Preprint. Elias Greve, Martin Buchner, Niclas Vodisch, Wolfram Burgard, and Abhinav Valada. Collaborative In 2024 IEEE International Conference on dynamic 3d scene graphs for automated driving. Robotics and Automation (ICRA), pp. 1111811124. IEEE, 2024. Qiao Gu, Ali Kuwajerwala, Sacha Morin, Krishna Murthy Jatavallabhula, Bipasha Sen, Aditya Agarwal, Corban Rivera, William Paul, Kirsty Ellis, Rama Chellappa, et al. Conceptgraphs: Open-vocabulary 3d scene graphs for perception and planning. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pp. 50215028. IEEE, 2024a. Qiao Gu, Zhaoyang Lv, Duncan Frost, Simon Green, Julian Straub, and Chris Sweeney. Egolifter: Open-world 3d segmentation for egocentric perception. In European Conference on Computer Vision, pp. 382400, 2024b. Yanjiang Guo, Yen-Jen Wang, Lihan Zha, and Jianyu Chen. Doremi: Grounding language model by detecting and recovering from plan-execution misalignment. In 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 1212412131. IEEE, 2024. Daniel Honerkamp, Martin Buchner, Fabien Despinoy, Tim Welschehold, and Abhinav Valada. Language-grounded dynamic scene graphs for interactive object search with mobile manipulation. IEEE Robotics and Automation Letters, 2024a. Daniel Honerkamp, Martin Buchner, Fabien Despinoy, Tim Welschehold, and Abhinav Valada. Language-grounded dynamic scene graphs for interactive object search with mobile manipulation. IEEE Robotics and Automation Letters, 2024b. Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, and Li Fei-Fei. Voxposer: Composable 3d value maps for robotic manipulation with language models. arXiv preprint arXiv:2307.05973, 2023. Wenlong Huang, Chen Wang, Yunzhu Li, Ruohan Zhang, and Li Fei-Fei. Rekep: Spatiotemporal reasoning of relational keypoint constraints for robotic manipulation. arXiv preprint arXiv:2409.01652, 2024. Krishna Murthy Jatavallabhula, Alihusein Kuwajerwala, Qiao Gu, Mohd. Omama, Ganesh Iyer, Soroush Saryazdi, Tao Chen, Alaa Maalouf, Shuang Li, Nikhil Varma Keetha, Ayush Tewari, Joshua B. Tenenbaum, Celso Miguel de Melo, K. Madhava Krishna, Liam Paull, Florian Shkurti, and Antonio Torralba. Conceptfusion: Open-set multimodal 3d mapping. In Robotics: Science and Systems, 2023. Guangqi Jiang, Yifei Sun, Tao Huang, Huanyu Li, Yongyuan Liang, and Huazhe Xu. Robots pretrain robots: Manipulation-centric robotic representation from large-scale robot datasets. 2025. Hanxiao Jiang, Binghao Huang, Ruihai Wu, Zhuoran Li, Shubham Garg, Hooshang Nayyeri, Shenlong Wang, and Yunzhu Li. Roboexp: Action-conditioned scene graph via interactive exploration for robotic manipulation. arXiv preprint arXiv:2402.15487, 2024. Yuanchen Ju, Kaizhe Hu, Guowei Zhang, Gu Zhang, Mingrun Jiang, and Huazhe Xu. Robo-abc: Affordance generalization beyond categories via semantic correspondence for robot manipulation. In European Conference on Computer Vision (ECCV), 2024. Sebastian Koch, Pedro Hermosilla, Narunas Vaskevicius, Mirco Colosi, and Timo Ropinski. Lang3dsg: Language-based contrastive pre-training for 3d scene graph prediction. In 2024 International Conference on 3D Vision (3DV), pp. 10371047. IEEE, 2024a. Sebastian Koch, Narunas Vaskevicius, Mirco Colosi, Pedro Hermosilla, and Timo Ropinski. Open3dsg: Open-vocabulary 3d scene graphs from point clouds with queryable objects and openset relationships. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1418314193, 2024b. Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Matt Deitke, Kiana Ehsani, Daniel Gordon, Yuke Zhu, et al. Ai2-thor: An interactive 3d environment for visual ai. arXiv preprint arXiv:1712.05474, 2017. 13 Preprint. Vasiliki Kondyli, Mehul Bhatt, and Jakob Suchan. Towards human-centred cognitive model of visuospatial complexity in everyday driving. arXiv preprint arXiv:2006.00059, 2020. Qi Li, Kaichun Mo, Yanchao Yang, Hang Zhao, and Leonidas Guibas. Ifr-explore: Learning interobject functional relationships in 3d indoor scenes. arXiv preprint arXiv:2112.05298, 2021. Yongyuan Liang, Tingqiang Xu, Kaizhe Hu, Guangqi Jiang, Furong Huang, and Huazhe Xu. Makean-agent: generalizable policy network generator with behavior-prompted diffusion. 2024. Yongyuan Liang, Wei Chow, Feng Li, Ziqiao Ma, Xiyao Wang, Jiageng Mao, Jiuhai Chen, Jiatao Gu, Yue Wang, and Furong Huang. Rover: Benchmarking reciprocal cross-modal reasoning for omnimodal generation. arXiv preprint arXiv:2511.01163, 2025a. Yongyuan Liang, Xiyao Wang, Yuanchen Ju, Jianwei Yang, and Furong Huang. Lemon: unified and scalable 3d multimodal model for universal spatial understanding. arXiv preprint arXiv:2512.12822, 2025b. Joel Loo, Zhanxin Wu, and David Hsu. Open scene graphs for open-world object-goal navigation. arXiv preprint arXiv:2508.04678, 2025. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, KaiWei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. Dantong Niu, Yuvan Sharma, Giscard Biamby, Jerome Quenum, Yutong Bai, Baifeng Shi, Trevor Darrell, and Roei Herzig. Llarva: Vision-action instruction tuning enhances robot learning. arXiv preprint arXiv:2406.11815, 2024. OpenAI. Gpt-4 technical report. Technical report, OpenAI, 2023. URL https://api. semanticscholar.org/CorpusID:257532815. Yu Qi, Yuanchen Ju, Tianming Wei, Chi Chu, Lawson LS Wong, and Huazhe Xu. Two by two: Learning multi-task pairwise objects assembly for generalizable robot manipulation. CVPR 2025, 2025. Ri-Zhao Qiu, Yafei Hu, Yuchen Song, Ge Yang, Yang Fu, Jianglong Ye, Jiteng Mu, Ruihan Yang, Nikolay Atanasov, Sebastian Scherer, et al. Learning generalizable feature fields for mobile manipulation. arXiv preprint arXiv:2403.07563, 2024. Qwen. Qwen2.5-vl, January 2025. URL https://qwenlm.github.io/blog/qwen2. 5-vl/. Krishan Rana, Jesse Haviland, Sourav Garg, Jad Abou-Chakra, Ian Reid, and Niko Suenderhauf. Sayplan: Grounding large language models using 3d scene graphs for scalable robot task planning. arXiv preprint arXiv:2307.06135, 2023. Ayca Takmaz, Alexandros Delitzas, Robert Sumner, Francis Engelmann, Johanna Wald, and Federico Tombari. Search3d: Hierarchical open-vocabulary 3d segmentation. IEEE Robotics and Automation Letters, 2025. Gemini Robotics Team, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Travis Armstrong, Ashwin Balakrishna, Robert Baruch, Maria Bauza, Michiel Blokzijl, et al. Gemini robotics: Bringing ai into the physical world. arXiv preprint arXiv:2503.20020, 2025. Sebo Uithol, Katherine Bryant, Ivan Toni, and Rogier Mars. The anticipatory and task-driven nature of visual perception. Cerebral Cortex, 31(12):53545362, 2021. Yixuan Wang, Leonor Fermoselle, Tarik Kelestemur, Jiuguang Wang, and Yunzhu Li. Curiousbot: Interactive mobile exploration via actionable 3d relational object graph. arXiv preprint arXiv:2501.13338, 2025. Abdelrhman Werby, Chenguang Huang, Martin Buchner, Abhinav Valada, and Wolfram Burgard. Hierarchical open-vocabulary 3d scene graphs for language-grounded robot navigation. In First Workshop on Vision-Language Models for Navigation and Manipulation at ICRA 2024, 2024. 14 Preprint. Jimmy Wu, Rika Antonova, Adam Kan, Marion Lepert, Andy Zeng, Shuran Song, Jeannette Bohg, Szymon Rusinkiewicz, and Thomas Funkhouser. Tidybot: Personalized robot assistance with large language models. Autonomous Robots, 47(8):10871102, 2023. Shun-Cheng Wu, Johanna Wald, Keisuke Tateno, Nassir Navab, and Federico Tombari. Scenegraphfusion: Incremental 3d scene graph prediction from rgb-d sequences. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 75157525, 2021. Zhijie Yan, Shufei Li, Zuoxu Wang, Lixiu Wu, Han Wang, Jun Zhu, Lijiang Chen, and Jihong Liu. Dynamic open-vocabulary 3d scene graphs for long-term language-guided mobile manipulation. IEEE Robotics and Automation Letters, 2025. Jianwei Yang, Reuben Tan, Qianhui Wu, Ruijie Zheng, Baolin Peng, Yongyuan Liang, Yu Gu, Mu Cai, Seonghyeon Ye, Joel Jang, et al. Magma: foundation model for multimodal ai agents. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1420314214, 2025. Timing Yang, Yuanliang Ju, and Li Yi. Imov3d: Learning open vocabulary point clouds 3d object detection from only 2d images. Advances in Neural Information Processing Systems, 37:141261 141291, 2024. Baiqiao Yin, Qineng Wang, Pingyue Zhang, Jianshu Zhang, Kangrui Wang, Zihan Wang, Jieyu Zhang, Keshigeyan Chandrasegaran, Han Liu, Ranjay Krishna, et al. Spatial mental modeling from limited views. arXiv preprint arXiv:2506.21458, 2025. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95569567, 2024. Tatiana Zemskova and Dmitry Yudin. 3dgraphllm: Combining semantic graphs and large language models for 3d referred object grounding. Chenyangguang Zhang, Alexandros Delitzas, Fangjinhua Wang, Ruida Zhang, Xiangyang Ji, Marc Pollefeys, and Francis Engelmann. Open-vocabulary functional 3d scene graphs for real-world indoor spaces. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1940119413, 2025. Shoulong Zhang, Aimin Hao, Hong Qin, et al. Knowledge-inspired 3d scene graph prediction in point cloud. Advances in Neural Information Processing Systems, 34:1862018632, 2021. Yuanhang Zhang, Tianhai Liang, Zhenyang Chen, Yanjie Ze, and Huazhe Xu. Catch it! learning to catch in flight with mobile dexterous hands. arXiv preprint arXiv:2409.10319, 2024a. Yunpeng Zhang, Deheng Qian, Ding Li, Yifeng Pan, Yong Chen, Zhenbao Liang, Zhiyao Zhang, Shurui Zhang, Hongxu Li, Maolei Fu, et al. Graphad: Interaction scene graph for end-to-end autonomous driving. arXiv preprint arXiv:2403.19098, 2024b. Ruijie Zheng, Yongyuan Liang, Shuaiyi Huang, Jianfeng Gao, Hal Daume III, Andrey Kolobov, Furong Huang, and Jianwei Yang. Tracevla: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies. 2025a. Yaowei Zheng, Junting Lu, Shenzhi Wang, Zhangchi Feng, Dongdong Kuang, and Yuwen Xiong. Easyr1: An efficient, scalable, multi-modality rl training framework. https://github.com/ hiyouga/EasyR1, 2025b. Junzhe Zhu, Yuanchen Ju, Junyi Zhang, Muhan Wang, Zhecheng Yuan, Kaizhe Hu, and Huazhe Xu. Densematcher: Learning 3d semantic correspondence for category-level manipulation from single demo. International Conference on Learning Representations (ICLR) Spotlight, 2025. Preprint."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 MOMAGRAPH-SCENES DATASET A.1.1 REAL-WORLD DATASET SOURCE AND COLLECTION. Our dataset is built through synergistic integration of newly curated data and existing public resources. We manually collected substantial portion of the data in real-world household environments, capturing diverse interaction scenarios under natural conditions. To further enrich the dataset, we incorporated samples from two public benchmarks, OpenFunGraph (Zhang et al., 2025) and SceneFun3D (Delitzas et al., 2024), both of which contain videos depicting humanobject interactions in indoor contexts. From these videos, we carefully curated representative keyframes to derive multi-view RGB observations, ensuring comprehensive coverage of interaction dynamics and spatial variability. A.1.2 SIMULATION DATA COLLECTION To complement the real-world data, we additionally generated samples within the AI2-THOR simulation environment Kolve et al. (2017). We strategically positioned the embodied agent at diverse, reachable viewpoints and captured multi-view observations from varying perspectives, as illustrated in Fig. 7. Throughout this process, we applied manual post-filtering to exclude non-interactable elements, thereby ensuring that the curated dataset remains focused on actionable objects and emphasizes functional relevance critical for downstream embodied reasoning tasks. Figure 7: Simulated indoor environments in our benchmark. Each row shows three scenes (Floor 15, Floor 224 and Floor 301) with top-down view of the layout, reachable locations for the robot, and multiview observations from different viewpoints. A.1.3 DATASET ANNOTATION AND FORMAT. Annotation and Format. Each task-oriented subgraph in MomaGraph-Scenes is stored in structured JSON format and linked to its corresponding scene. Annotations include subgraph identifier, the associated scene identifier, the action type, the functional category, the natural language task instruction, set of nodes, and set of edges. Nodes correspond to the objects or part-level interactive elements required to accomplish the task, while edges capture both functional relationships (e.g., control, open or close) and spatial relationships (e.g., close, in front of, lower than). This example corresponds to the instruction Turn on the television, where the relevant nodes are the remote control and the TV, connected by control functional edge and spatial relations lower than, in front of, and close. In addition, each subgraph is grounded in multi-view observations. For every scene, we provide synchronized RGB images captured from multiple viewpoints. This multi-view grounding allows the annotated subgraphs to be consistently aligned with visual evidence, supporting both instructionconditioned graph prediction from perception and multi-view reasoning tasks. 16 Preprint. 1 { 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 } \"subgraph_id\": \"da21b9f9-f4fa-4a85-961b-2e2c2e182d3e\", \"scene_id\": \"466828\", \"action_type\": \"press\", \"function_type\": \"device_control\", \"task_instruction\": \"Turn on the television.\", \"nodes\": [ {\"label\": \"remote control\", \"id\": \"f15474de-7b35-4a5e-ac8a-dc02f93960b3\"}, {\"label\": \"tv\", \"id\": \"91486017-94ce-4788-aabd-0d07262c9bed\"} ], \"edges\": [ { \"relation_id\": \"ef3e72fe-ae9f-42e4-9b5a-505b5cb1844a\", \"functional_relationship\": \"control\", \"object1\": {\"label\": \"remote control\", \"id\": \"f15474de-7b35-4a5e-ac8a-dc02f93960b3\"}, \"object2\": {\"label\": \"tv\", \"id\": \"91486017-94ce-4788-aabd-0d07262c9bed\"}, \"spatial_relations\": [\"lower_than\", \"in_front_of\", \"close\"], \"is_touching\": false } ] Figure 8: Example JSON annotation for the task Turn on the television. A.1.4 MULTI-ASPECT STATISTICS OF THE TRAINING DATASET Our dataset consists of approximately 1,050 subgraphs and 6278 multi-view RGB images, collected across more than 350 diverse household scenes and encompassing 93 distinct task instructions. This broad coverage ensures rich variability in scene layouts, object configurations, and interaction types. To provide comprehensive overview of our training data, we present multi-aspect statistics covering scene context, action diversity, functional relationships, and object distributions. As shown in Fig. 9, the dataset spans four common household room types and captures the correspondence between action types and functional categories, reflecting the diversity and richness of real-world manipulation scenarios. Fig. 10 illustrates the distribution of action types across different room contexts, while Fig. 11 summarizes the prevalence of various functional relationships and Fig. 12 summarizes the frequency of object occurrences. Together, these statistics highlight the diversity and task relevance of our dataset, ensuring broad coverage of spatialfunctional interactions essential for embodied planning and reasoning. (a) Room-type distribution. (b) Actionfunction correspondence. Figure 9: Dataset statistics: (a) Distribution across four room types; (b)Heatmap showing the correspondence between action types and functional types. 17 Preprint. Figure 10: Task distribution across four room types: kitchen, living room, bedroom, and bathroom. Figure 11: Distribution of functional relationships across all tasks in the dataset. A.2 TRAINING DETAILS We train our model using 8 80GB A100 GPUs for approximately 13 hours based on the EasyR1 (Zheng et al., 2025b) training framework. The complete training configuration for DAPO algorithm is presented in Table 4. A.3 TRAINING CURVE Figure 13 and 14 shows the training curves during DAPO optimization. The training and validation curves closely align across all metrics, indicating good generalization without significant overfitting. The overall reward converges to 0.93, while accuracy reward stabilizes at 0.9. The format reward quickly reaches 1.0 within the first 25 steps, showing the model rapidly learns to produce valid JSON-structured outputs. A.4 MOMAGRAPH BENCHMARK A.4.1 BENCHMARK DESIGN To rigorously evaluate spatialfunctional reasoning and task planning capabilities, we design comprehensive multi-choice VQA benchmark based on the scenes and tasks in our dataset. Rather than manually crafting all questions, we leverage large visionlanguage models (VLMs) to generate them in scalable and diverse manner. Specifically, we provide the model with structured prompts describing the scene images, state-aware scene graph, and task instructions, and instruct it to produce questionanswer pairs that probe different reasoning skills, such as spatial relation understanding, affordance inference, precondition reasoning, and goal decomposition. To ensure the reliability and 18 Preprint. Figure 12: Statistics of object occurrences, highlighting the most frequent objects in tasks. Figure 13: Training reward curves during MomaGraph-R1 training. correctness of the benchmark, all generated questions and answers undergo several rounds of manual verification, during which ambiguous or low-quality samples are refined or removed. Moreover, since the benchmark is formulated as multi-choice VQA task with clearly defined correct answers, it does not require complex evaluation metrics. Model performance can be directly measured by simple accuracy i.e., the proportion of correctly answered questions which provides an intuitive and reliable indicator of spatialfunctional reasoning and planning capabilities. This simplicity enables straightforward comparison across models while ensuring that the evaluation remains rigorous and meaningful. Data Source and Task Scope. We leverage long video sequences from SceneFun3D (Delitzas et al., 2024) that capture human-recorded layouts of entire indoor environments, from which key frames are extracted and manually annotated with task-specific graphs. To enhance diversity and coverage, we additionally collect data from real indoor scenes. Our benchmark spans four representative indoor room categories: bathroom, kitchen, living room, and bedroom. The task scope is organized into four levels of difficulty: T1 Single-step actions: e.g., turning on light, pulling drawer, opening door. T2 Two complementary steps: e.g., filling bathtub by first pressing the drain button and then turning on the faucet. T3 Multi-step or preconditioned tasks: e.g., making coffee (pick up cup add water start the coffee machine). T4 Dynamic verification tasks: e.g., when the target object is missing, the system must perform graph-based replanning and identify alternative interactive objects. 19 Preprint. Table 4: DAPO Training Configuration Parameter Value Model Configuration Base Model Mixed Precision Training Setup Total Epochs Training Steps Actor Global Batch Size Critic Global Batch Size Micro Batch Size (Actor) Micro Batch Size (Critic) Optimization Learning Rate Optimizer Weight Decay Beta1, Beta2 Gradient Clipping DAPO Algorithm KL Coefficient KL Penalty Disable KL Clip Ratio Low Clip Ratio High Clip Ratio Dual Qwen2.5-VL-7B-Instruct bfloat16 25 175 128 256 1 4 1e-6 AdamW 0.01 0.9, 0.999 1.0 0.01 low var kl True 0.2 0.28 3.0 Reward Function Format Weight Max Response Length Overlong Penalty Factor Generation Config Temperature Top-p Rollout Samples 0.2 2048 0.5 1.0 1.0 5 Figure 14: Validation reward curves during MomaGraph-R1 training."
        },
        {
            "title": "B ADDITIONAL ABLATION STUDIES",
            "content": "B.1 COMPARISON WITH SFT AND ICL BASELINES To validate our choice of RL-based training over alternative learning paradigms, we compare our method against two additional baselines: 20 Preprint. SFT baseline: We fine-tune Qwen2.5-VL-7B on MomaGraph-Scenes using supervised learning only (without RL), with the same graph-alignment objectives as our full method. ICL baseline: We evaluate the base model with 3-5 in-context graph examples provided in the prompt (same setting as Qwen2.5-VL-7B-Instruct (w/ Graph) in Table 2 and 3 of the main paper). Method BLINK MomaGraph-Bench (Overall) SFT baseline ICL baseline RL w/ Graph (Ours) 60.4 58.7 63.5 63.9 60.2 71.6 Table 5: Comparison of our RL-based training with SFT and ICL baselines. Our method achieves substantially better performance on both benchmarks. As shown in Table 5, our RL training method achieves clearly superior performance compared to both the SFT baseline (+3.1 on BLINK, +7.7 on MomaGraph-Bench) and the ICL baseline (+4.8 on BLINK, +11.4 on MomaGraph-Bench). This demonstrates that the RL formulation is crucial for learning high-quality scene graph generation that effectively improves downstream planning performance. B.2 REWARD WEIGHT SENSITIVITY STUDY We follow the original DAPO implementation in the EasyR1 framework for default settings of wf and wl in Eq. 2 of the main paper. We conduct sensitivity study by varying (wa, wf , wl) around the default configuration: Setting ID wa wf wl BLINK MomaGraph-Bench (Overall) 61.3 63.1 63.7 63.5 Default 68.2 70.9 71.2 71.6 0.5 0.5 0.7 0. 0.5 0.3 0.2 0.2 0.5 0.7 0.8 0.8 Table 6: Sensitivity analysis of reward weights (wa, wf , wl) in our DAPO training. The models performance remains stable across different weight configurations. As shown in Table 6, the models performance remains stable across these weight configurations, with variations of less than 2.4% on BLINK and 3.4% on MomaGraph-Bench. This indicates low sensitivity to reward-weight choices and demonstrates the robustness of our training approach. B.3 DETAILED REAL-WORLD DEMONSTRATIONS. To provide closer look into the behavior of our system, this section presents fine-grained realworld examples. We illustrate how the model processes raw images captured in realistic household environments, transforms them into task-oriented scene graphs, and generates corresponding planner outputs. These case studies highlight the systems ability to capture subtle details, encode them into structured graphs, and reason over them to produce actionable plans. To validate the effectiveness of our approach in real-world settings, we deploy the system on mobile manipulator to perform variety of everyday tasks, as shown in Fig. 15. These tasks span multiple functional categories, such as turning off light, opening microwave, turning on TV, and opening cabinet. In each case, the robot leverages the predicted spatialfunctional scene graph to plan and execute sequence of actions without task-specific fine-tuning. The successful completion of these tasks demonstrates the systems ability to generalize from structured graph representations to real-world interaction scenarios, highlighting its potential for practical household assistance. 21 Preprint. Figure 15: Real-world robot execution of household tasks. Figure 16: Real-world example of MomaGraph-R1 performing the task Open the Cabinet. From multiview images, the system generates scene graph capturing spatialfunctional relations and outputs the corresponding action plan. Preprint. Figure 17: Real-world example of MomaGraph-R1 performing the task Turn off the light. From multiview images, the system generates scene graph capturing spatialfunctional relations and outputs the corresponding action plan. 23 Preprint. Figure 18: Real-world example of MomaGraph-R1 performing the task Open the microwave. From multiview images, the system generates scene graph capturing spatialfunctional relations and outputs the corresponding action plan. Preprint. Figure 19: Real-world example of MomaGraph-R1 performing the task Turn on the TV. From multiview images, the system generates scene graph capturing spatialfunctional relations and outputs the corresponding action plan."
        }
    ],
    "affiliations": [
        "University of California, Berkeley",
        "University of Maryland, College Park",
        "University of Toronto"
    ]
}