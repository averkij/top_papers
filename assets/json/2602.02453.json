{
    "paper_title": "Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling",
    "authors": [
        "Andong Chen",
        "Wenxin Zhu",
        "Qiuyu Ding",
        "Yuchen Song",
        "Muyun Yang",
        "Tiejun Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Chain-of-Thought reasoning has driven large language models to extend from thinking with text to thinking with images and videos. However, different modalities still have clear limitations: static images struggle to represent temporal structure, while videos introduce substantial redundancy and computational cost. In this work, we propose Thinking with Comics, a visual reasoning paradigm that uses comics as a high information-density medium positioned between images and videos. Comics preserve temporal structure, embedded text, and narrative coherence while requiring significantly lower reasoning cost. We systematically study two reasoning paths based on comics and evaluate them on a range of reasoning tasks and long-context understanding tasks. Experimental results show that Thinking with Comics outperforms Thinking with Images on multi-step temporal and causal reasoning tasks, while remaining substantially more efficient than Thinking with Video. Further analysis indicates that different comic narrative structures and styles consistently affect performance across tasks, suggesting that comics serve as an effective intermediate visual representation for improving multimodal reasoning."
        },
        {
            "title": "Start",
            "content": "Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling Andong Chen 1 Wenxin Zhu 1 Qiuyu Ding 1 Yuchen Song 1 Muyun Yang 1 Tiejun Zhao 1 Website: https://thinking-with-comics.github.io/ Repository: https://github.com/andongBlue/Think-with-Comics."
        },
        {
            "title": "Abstract",
            "content": "Chain-of-Thought reasoning has driven large language models to extend from thinking with text to thinking with images and videos. However, different modalities still have clear limitations: static images struggle to represent temporal structure, while videos introduce substantial redundancy and computational cost. In this work, we propose Thinking with Comics, visual reasoning paradigm that uses comics as high information-density medium positioned between images and videos. Comics preserve temporal structure, embedded text, and narrative coherence while requiring significantly lower reasoning cost. We systematically study two reasoning paths based on comics and evaluate them on range of reasoning tasks and long-context understanding tasks. Experimental results show that Thinking with Comics outperforms Thinking with Images on multi-step temporal and causal reasoning tasks, while remaining substantially more efficient than Thinking with Video. Further analysis indicates that different comic narrative structures and styles consistently affect performance across tasks, suggesting that comics serve as an effective intermediate visual representation for improving multimodal reasoning. 6 2 0 2 2 ] . [ 1 3 5 4 2 0 . 2 0 6 2 : r 1. Introduction Large language models (LLMs) have significantly improved their reasoning ability on complex tasks by adopting explicit Chain-of-Thought (CoT) (Wei et al., 2022; Kojima et al., 2022; Besta et al., 2024; Yao et al., 2023), making stepby-step textual reasoning (Think with text) common 1Harbin Institute of Technology. Correspondence to: Andong Chen <ands691119@gmail.com>, Tiejun Zhao <tjzhao@hit.edu.cn>. Preprint. February 2026. Copyright 2026 by the author(s). paradigm. With the development of multimodal large language models (MLLM), this idea of explicit reasoning has extended from pure text to the visual domain. Under the Thinking with Images (TWI) paradigm (Hurst et al., 2024; Ope; Zhang et al., 2023; Wang et al., 2025; Chen et al., 2025), models not only use images as input signals but also generate intermediate visual representations during reasoning to supplement critical visual information (Li et al., 2025; Hu et al., 2024), thereby improving the reasoning performance of visionlanguage models (VLMs). Building on this, Thinking with Video further introduces temporal structure by generating short video sequences, enabling more complex forms of dynamic reasoning (Tong et al., 2025). Despite the extension of reasoning paradigms from text to images and videos, each modality still exhibits clear limitations. Static images struggle to represent temporal structure and dynamic processes, while the absence of explicit textual cues complicates cross-modal alignment. Videos provide temporal information but introduce substantial redundancy and significantly higher computational overhead, which limits their practical efficiency for reasoning. To address these limitations, we turn to more natural reasoning medium from daily life-comics-and introduce the Thinking with Comics (TwC) paradigm. Comics are distinctive narrative form. Compared with static images, they retain most key properties of video, including temporal logic, embedded text, and dynamic reasoning (Augereau et al., 2017). Yet compared with video, each panel is more information-dense and requires far lower reasoning cost. Recent generative models such as Gemini-3 Pro Image (Google DeepMind, 2025) can convert long text into coherent sequential panels while embedding text naturally within images. This allows comics to combine the highdensity reasoning benefits of images with the dynamic logic of video. Thus, Thinking with Comics has strong potential to expand visual reasoning into new research direction. To comprehensively explore this field, we adopted two paths of Thinking with Comics, namely End-to-End Visualized Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling Figure 1. The selected reasoning tasks and (Long) Context Understanding tasks, along with the Thinking with Comics solution based on Gemini-3 Pro Image. The reasoning tasks primarily involve mathematical and logical reasoning, while the (Long) Context Understanding tasks require the model to comprehend cultural contexts, documents, and other extended information. The model provides the reasoning process and correct answers within the generated comic panels. Reasoning and Comic as Conditioning Context for VLM. Then we evaluate our method on mainstream generalpurpose benchmarks across two task types, as shown in Figure 1: (1) reasoning tasks and (2) (long) context understanding tasks. In the evaluation, we test the two paths and compare them with leading MLLMs as well as models that following the paradigms of Thinking with Text, Thinking with Images, and Thinking with Video. The results show that comics, as form of structured visual storytelling, consistently yield systematic performance gains across different tasks. Further analysis reveals that: (1) different tasks benefit from different role-playing narrative structures in comicsfor example, detective-style narratives are better suited for logical reasoning tasks, while culture-centric narratives are more effective for cultural understanding; (2) Thinking with Comics exhibits scaling behavior similar to Chainof-Thought, where more difficult tasks require larger number of comic panels to support reasoning; (3) comic panels exhibit clear temporal and logical dependencies, and disrupting or permuting their order leads to noticeable performance degradation; (4) embedded textual elements in comics, such as dialogue and narration, work jointly with visual cues to reduce semantic ambiguity in purely visual reasoning; and (5) compared to Thinking with Video, Thinking with Comics achieves substantially lower inference cost while preserving essential temporal structure. These findings indicate that visual expression still offers substantial room for exploration, and that comics provide new reasoning medium positioned between static images and videos. We hope this work will inspire further exploration of Thinking with paradigms and help establish comics as an important component of unified visual reasoning framework. 2. Related Works Reasoning Paradigm Transfer: CoT enhances the interpretability of reasoning in LLMs by incorporating explicit intermediate reasoning steps, and significantly improves their reasoning capabilities (Wei et al., 2022; Kojima et al., 2022; Wang et al., 2022; Huang & Chang, 2023). Inspired by this paradigm, some works have further introduced it into MLLMs, developing the Thinking with Images paradigm (Hurst et al., 2024; Zhang et al., 2023; Zheng et al., 2023; Mitra et al., 2024; Gao et al., 2024), where MLLMs process original images or generate new ones and perform reasoning within an interleaved flow of textual and visual information. For both aforementioned paradigms, models typically employ large-scale reinforcement learning (Shao et al., 2024; Guo et al., 2025; Liu et al., 2025) or some training-free inferencetime scaling methods (Kojima et al., 2022; Xu et al., 2025; Dhuliawala et al., 2024) to enhance their CoT reasoning Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling abilities. Recently, addressing issues in the Thinking with Images paradigm, such as the lack of temporal information in single image and the relative independence between textual and visual modalities, Tong et al., 2025 proposed the Thinking with Video paradigm. This approach leverages video generation models like Sora 2 to integrate visual and textual reasoning within unified temporal framework, where the video generation process itself constitutes the reasoning process. Vision Generation Model: The development of visual generation models has been profoundly influenced by diffusion models, which have become the mainstream methods for image and video generation (Ho et al., 2020). key milestone in this field is Stable Diffusion (Rombach et al., 2022), latent diffusion model used for efficiently generating high-resolution images. Building on these foundational architectures, the latest advances in image generation focus on enhancing text-to-image consistency, controllability, and fidelity. For example, DALLE 3 (Betker et al., 2023) integrates advanced captioning and multimodal training to generate highly detailed and contextually accurate images from text prompts, addressing limitations in compositionality observed in earlier models. Similarly, Googles Nano Banana and its enhanced version Nano Banana Pro employ advanced image generation and editing techniques to achieve studiolevel precise control and prompt accuracy, supporting natural language-described photo editing and high-quality image creation (Google DeepMind, 2025). Extending these principles to video generation, models such as OpenAIs Sora and its successor Sora 2 utilize spatiotemporal diffusion to generate coherent video sequences from text, incorporating world simulation capabilities to achieve realistic motion and long-range consistency (OpenAI, 2025). Meanwhile, Google DeepMinds Veo 3 advances audiovisual generation by natively integrating sound effects and dialogue with high-fidelity video frames, utilizing 3D latent diffusion to enhance temporal coherence and multimodal expressiveness (Google, 2025). These models collectively represent trajectory toward more versatile and integrated visual generative systems, paving the way for applications in creative industries and beyond. 3. Method In this section, we introduce Thinking with Comics, novel structured visual storytelling reasoning paradigm that explicitly externalizes intermediate reasoning processes into sequence of comic panels with temporal and causal structures. These panels serve either as the reasoning carrier itself or as conditioning context for downstream inference, enabling more interpretable and structurally grounded reasoning in multimodal models. From the implementation perspective, Thinking with Comics can be instantiated through two paths. As shown in Figure 2, the first path treats comic generation as the reasoning process itself, where generative model performs end-to-end visualized reasoning from the input question to the final answer. The second path instead regards the generated comic as an explicit intermediate reasoning representation, which is then combined with the original question and processed by MLLM for joint reasoning. In the following, we describe these two paths in detail. 3.1. Path I: End-to-End Visualized Reasoning The first path uses an image generation model to produce comic based on the input question, visually depicting the reasoning process, and extracts the final answer from the last panel of the comic. Formally, let denote the input question, and let θ be the parameters of the image generation model. The model generates sequence of comic panels = c1, c2, . . . , cT , where each panel ct depicts an intermediate reasoning step. The generation process is expressed as: = Gθ(q). (1) During generation, the model progressively unfolds the reasoning process, with each panel corresponding to reasoning state. In this path, reasoning and generation are tightly coupled. We assume that the model implicitly learns latent state transition process: ht = (ht 1, q), ct = g(ht), (2) where ht denotes the latent reasoning state at step t, and g() maps the latent state to visual comic panel. The final answer ˆa is obtained by extracting information from the last panel: ˆa = R(cT ), (3) where R() denotes an answer extraction process that identifies relevant textual or symbolic information from the final panel. This path provides an end-to-end reasoning framework with relatively low computational cost, while offering interpretable intermediate representations. The sequential and causally coherent nature of comic panels enables the reasoning trajectory to be directly visualized. However, since all reasoning is performed implicitly within the generation model, the overall reasoning capability is constrained by the model itself. 3.2. Path II: Comic as Conditioning Context for VLM The second path treats comics as an explicit intermediate reasoning medium and incorporates MLLMs for 3 Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling Figure 2. Overview of the two paths of Thinking with Comics paradigm. Path 1 directly utilizes an image generation model to create comic, where the process of generating the comic constitutes the reasoning process for the problem, and the answer is obtained by extracting the final panel of the comic. Path 2 takes the generated comic along with the original problem as context and inputs them into VLM, which then performs reasoning and outputs the answer. downstream inference. This design is related to imageassisted reasoning approaches in the Thinking with Images paradigm, while providing more structured and temporally consistent representation through multi-panel comics. In this path, comic is first generated though image generation model: = Gθ(q), (4) and the original question together with the comic are then provided as input to MLLMs: ˆa = Fϕ(q, C), (5) where Fϕ denotes MLLMs parameterized by ϕ. To formalize the influence of comics in the reasoning process, we treat the comic as an explicit intermediate variable z: = C, ˆa = arg max p(a q, z). (6) Compared to textual intermediate variables used in traditional CoT reasoning, the comic representation jointly encodes spatial structure, object relationships, and temporal evolution. This richer representation provides the MLLMs with structured and multimodal reasoning context. 4. Experiments 4.1. Evaluation Datasets We evaluate the proposed Thinking with Comics on diverse set of benchmarks covering both explicit reasoning and multimodal understanding capabilities. The evaluation datasets are grouped into two task categories: reasoning tasks and (long) context understanding tasks. The reasoning tasks include MATH500 (Lightman et al., 2023), GSM8K (Cobbe et al., 2021), and MathVista (Lu et al., 2023), which primarily require multi-step logical or mathematical inference. MATH500 and GSM8K focus on symbolic and numerical reasoning in purely textual settings, while MathVista extends these challenges to visually grounded mathematical problems that demand joint visual perception and logical reasoning. tasks context include understanding (Long) DocVQA (Mathew et al., 2021), eBDtheque (Guerin et al., 2013), and CulturalBench (Chiu et al., 2024). DocVQA primarily evaluates ability to aggregate and understand document-level inputs; eBDtheque, designed for comic translation, focuses on document-level multilingual understanding and visualtext alignment across multiple Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling Table 1. Main results on reasoning and context understanding benchmarks. M-Vista and Cultu. denote MathVista and CulturalBench, respectively. G-t-R is Generate-then-Reason. For CulturalBench, and represent the Easy and Hard subsets. The symbol indicates that the model does not support the specific task. * denotes results from Tong et al., 2025; indicates evaluation on 50 sampled instances, following Tong et al., 2025. Category Model / Method Notes Reasoning Benchmarks (Acc %) Context Understanding (Acc %) MATH-500 GSM8K M-Vista DocVQA Cultu. (E / H) MLLM Reasoning LLM GPT-5.2 Gemini-3-Pro Claude-Sonnet 4.5 DeepSeek-R1 Qwen3-235B-A22B Think with Image TWI-1-Generated Photo DREAMLLM direct direct direct CoT CoT G-t-R G-t-R 99.0 100.0 99.0 90.4 92.4 70.2 12.6 Think with Video Sora 2 V-o-T 67.0* Think with Comic TwC (Ours) - Only Image direct TwC (Ours) - Img & Txt G-t-R 90.0 92. 100.0 99.0 100.0 96.1 94.3 69.4 18.4 75.7* 100.0 95.4 67.5 71.5 72. 63.6 35.9 67.6 75.0 85.8 72.8 94.5 92.6 67.5 65.5 50.5 92.8 99.4 88.3 / 84.4 90.4 / 90.0 87.2 / 76.5 87.2 / 85.1 83.1 / 82.5 69.7 / 71.4 52.3 / 42. 60.0 / 70.0 70.0 / 80.5 88.3 / 82.2 panels; and CulturalBench is text-only benchmark with two subsets (Easy / Hard) for evaluating contextualized cultural understanding. these benchmarks emphasize sensitivity to long documents, narrative structure, and cultural context, rather than explicit logical reasoning. Overall, 4.2. Models and Experimental Setup. In the experiments, we evaluate implementation paths of the Thinking with Comics paradigm introduced in Section 3. For path (End-to-End Visualized Reasoning), we directly employ Gemini-3 Pro Image (Google DeepMind, 2025) 1 to generate comics conditioned on the input question. The generated comic serves as the complete reasoning trajectory, and the final answer is extracted from the last panel. For path II (Comic as Conditioning Context for MLLM), we first use Gemini-3 Pro Image to generate comic, which is then provided together with the original question as input to MLLM for joint reasoning. For convenience, we choose Gemini-3 Pro (Google DeepMind, 2025) for further reasoning. Unless otherwise specified, all models are evaluated in zero-shot setting. Prompt templates are designed to ensure fair comparison across different reasoning paradigms while avoiding task-specific tuning. Baselines. We compare against four groups of strong (i) textbaselines, including several frontier models: only MLLMs, including GPT-5.2 (Singh et al., 2025), Gemini 3 Pro (Google DeepMind, 2025), and Claude Sonnet 4.5 (Anthropic, 2025), which perform reasoning 1https://deepmind.google/models/gemini-image/pro/ without explicit intermediate reasoning process 2; (ii) Reasoning-oriented LLMs, including DeepSeek-R1 (Guo et al., 2025), Qwen3-235B-A22B (Yang et al., 2025), which are specifically designed to enhance multi-step reasoning through structured or implicit Chain-of-Thought mechanisms 3; (iii) models following the Thinking with Images paradigm, including prompt-based approaches such as G-IMG (Cheng et al., 2025)(the prompt provided in the Appendix D.1), as well as training-based methods like DREAMLLM (Dong et al., 2023). These models assist reasoning by incorporating image generation or imageconditioned inputs during inference, with DREAMLLM relying on end-to-end training with 7B-scale model.; and (iv) models following the Thinking with Video paradigm, represented by Sora 2, where temporal video generation implicitly encodes the reasoning process. Metrics. For most benchmarks, we adopt accuracy as the evaluation metric, including MATH500, GSM8K, MathVista, DocVQA and CulturalBench, which directly measures the correctness of the final predicted answers. The details of answer extraction for the two TwC pathways are provided in Appendix C. 4.3. Main Results Table 1 summarizes our systematic evaluation of TwC across reasoning benchmarks (MATH-500, GSM8K, MathVista) and context understanding benchmarks (DocVQA and 2The versions of the three MLLMs are respectively: gpt5.2-2025-12-11, gemini-3-pro-preview, and claude-sonnet-4-520250929. 3The version of two reasoning LLMs are respectively: deepseek-r1-0528 and qwen3-235b-a22b-thinking5 Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling CulturalBench). The results show that TwC performs strongly on multimodal reasoning tasks, achieving 85.8% accuracy on MathVista and significantly outperforming Thinking with Video. On pure text-based mathematical reasoning benchmarks, TwC remains competitive with strong proprietary models. For context understanding tasks, TwC reaches 99.4% accuracy on DocVQA and achieves leading performance on CulturalBench, particularly on the hard subset. Overall, these results demonstrate that introducing comic-style reasoning processes not only enhances both textual and visual reasoning, but also generalizes effectively to diverse context understanding tasks, validating the soundness and generalization capability of the TwC paradigm. 5. Analysis Experiment 5.1. Role-playing Narrative Alignment We investigate how specific Role-playing narrative frameworkssuch as documentary-style, detective-style, and slice-of-life comic picturesserve as Role-playing Narratives to induce specific reasoning paths in path of TwC. We compare three comic-mediated styles (documentary, detective, and slice-of-life) on the MathVista and GSM8K benchmarks, and observe performance variance when the model handles complex spatial and logical deduction tasks. The prompts and examples for each style are provided in Appendix D.2 and F.1. Table 2. Narrative Style Ablation on MathVista and GSM8K. Detective style acts as the most effective visual prompt for tasks. 5.2. Scaling the Panels This experiment explores the scaling law of reasoning capability by varying the number of generated panels (N {1, 2, 4, 6, 8}) in the path of TwC. Note that = 1 represents degeneration into the traditional Think with Image (TWI) mode. We record the accuracy and token consumption when solving complex MATH500 problems to quantify the information compression efficiency of comics. ) % ( a c 100 90 80 70 50 40 1,300 1,250 1,200 1, 1,100 ) k ( C t R Token Cost Accuracy 1 4 6 8 Number of Panels (N ) Figure 3. The performance-cost curve across different panel counts [4, 6]. On the MATH500 . Accuracy enters plateau at dataset, token cost ranges between 1100 and 1300. As illustrated in the performance-cost curve in Figure 3, reasoning accuracy enters visible plateau at 46 panels, while marginal gains from increasing panels diminish rapidly. The experimental results demonstrate that comics capture dynamic logic with minimal redundancy through high-level abstraction of continuous temporality. We conclude that 46 panels represent the Pareto optimal state between information density and computational overhead. Style (Visual Prompt) M-Vista GSM8K Avg. 5.3. Panel Distribution Across Task Difficulties Documentary (Base) Slice-of-Life Detective Style 60.0 80.0 85.0* 68.0 86.3 100.0* +19.1 +28.5 Experimental results, as shown in Table 2, reveal that the detective-style significantly outperforms the standard documentary-style comic in logical reasoning tasks. Averaged across the two benchmarks, accuracy increases from (60.0 + 68.0)/2 = 64.0 to (85.0 + 100.0)/2 = 92.5, yielding 28.5-point absolute gain. This corresponds to relative improvement of 28.5 / 64.0 = 44.5% over the documentary baseline. This suggests that role-playing narrative style is not merely visual decoration but potent Visual System Prompt. The results confirm that specific role-playing narrative structures established via comic panels can effectively activate the potential of MLLM for causal reasoning, leading to more focused inference path. Appendix analyzes the advantages of comic narratives over realistic-style images, comparing full comics with interleaved realistic image sequences in reasoning coherence and information organization. the number of generated This experiment counts panels across different difficulty levels to reveal the adaptive mechanism of TwC. We analyzed thousands of samples from GSM8K (basic logic), MathVista (visual reasoning), DocVQA (long-document understanding), and CulturalBench-hard (cultural understanding). The model decides the number of panels based on the complexity of the problem. This tests if the model can allocate visual resources dynamically according to task difficulty. Results are visualized in Figure 4. GSM8K exhibits bimodal distribution: while substantial portion of easier samples (33.28%) are efficiently solved with single In panel, the majority (62.82%) still utilize 4 panels. contrast, MathVista demonstrates higher hard reasoning task; although also peaking at 4 panels, its distribution significantly extends towards higher panel counts, with notable 30.41% of samples requiring 6 panels. These shifts confirm that TwC allocates minimal resources (1 panel) for simple queries while dynamically extending reasoning for more complex tasks like MathVista. 6 Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling ) % ( e r 100 60 40 20 0 GSM8K DocVQA CultureBench-hard MathVista 2 3 4 5 6 8 9 10 12 Number of Panels Figure 4. Frequency distribution of generated panels across tasks with varying difficulty levels. The shift to the right indicates the models adaptive allocation of reasoning steps for complex tasks. For Random Intermediate Deletion, we randomly remove subset of panels while preserving the relative order of the remaining ones. The deletion ratio ρ [0, 1] is defined as: ρ = , (8) where denotes the set of deleted panels. Experimental data in Figure 5 show that under Shuffle and Deletion conditions, the models accuracy exhibits decline from 75.0% to 71.5%. These results verify that the model depends on the temporal logic across panels, rather than treating them as isolated images. Notably, missing temporal sequence information harms the reasoning process more than disordered inputs. 5.4. The Role of Temporal Sequence in Reasoning 5.5. Ablation on Textual Anchoring the model captures temporal To examine whether relationships across panels rather than relying on singleimage features, we conduct controlled logic test on path II of TwC by systematically perturbing the temporal structure of comic panel sequences. We design two controlled groups: Complete Shuffle and Random Intermediate Deletion observing model performance in MATH500 stepby-step solutions and comic translation tasks. Formally, given an ordered panel sequence = c1, c2, . . . , cT , we define the shuffle intensity σ [0, 1] as the proportion of panels whose temporal positions are permuted: σ = 1 T (cid:88) i=1 I[π(i) = i], (7) where π denotes random permutation of panel indices. Here, σ = 0 corresponds to the original generated comic, while σ = 1 denotes Complete Shuffle. This experiment quantifies the contribution of embedded textual elementssuch as speech bubbles, narration, and onomatopoeiain eliminating visual ambiguity and In Path II, we enhancing semantic comprehension. perform an ablation study on CulturalBench and MathVista, comparing pure visual panels with comics containing complete bubbles and symbols. We focus on the speed at which textual signals complement visual cognition in highly coupled scenarios. The prompts for each style are provided in Appendix D.2. 88.3 70.2 82.2 73.9 85.8 72. ) % ( r A 100 60 40 TwC (Shuffle) TwC (Deletion) CulBen-Easy CulBen-Hard MathVista Pure Visual Textual Anchoring Figure 6. Ablation results on textual anchoring. Embedded text (bubbles, narration) provides precise semantic cues. As shown in Figure 6, comics with embedded text consistently outperform pure visual panels across all evaluated tasks. Textual anchoring yields an accuracy gain of 18.1 points on CulturalBench-Easy, 8.3 points on CulturalBench-Hard, and 13.2 points on MathVista. These results confirm that speech bubbles serve Semantic Anchoring role in comic contexts, eliminating image polysemy through precise linguistic instructions. This textual and visual modality integration significantly reduces the complexity of searching for correct solutions within the cross-modal space. 0 0. 0.4 0.6 0.8 1 Perturbation Intensity (Shuffle σ / Deletion ρ) Figure 5. Effect of temporal perturbations on comic-based Accuracy under Complete Shuffle (blue) and reasoning. Intermediate Deletion (orange) decreases as perturbation intensity increases, with deletion causing larger drop than shuffling. 7 76 74 72 ) % ( g o R Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling 5.6. Cross-Model Generalization 5.7. Efficiency Analysis of TwC and Think with Video This experiment evaluates the cross-model generalization of path II in the TwC paradigm across diverse MLLMs We use the same TwC generated architectures. large-scale comic as unified input and conduct evaluations on Claude 3.7 Sonnet, Qwen-VL-72B, GPT5.2, Gemini 3 Pro, and GPT-4o 4. The evaluation covers four capability categories and five benchmarks: logical reasoning (MATH-500, GSM8K), visual reasoning (MathVista), cultural understanding (CulturalBench), and long document understanding (DocVQA). By comparing model performance under an identical comic path, we assess TwCs potential as model-agnostic visual reasoning plugin in terms of transferability and stability. Claude 3.7 Qwen 72B GPT-4o GPT-5.2 Gemini 3 Pro DocVQA CulturalBench-E MathVista MATH-500 CulturalBench-H gsm8k 70 75 80 85 90 Accuracy (%) 95 100 Figure 7. Architectural Robustness Analysis. The tight clustering of colored markers along the horizontal tracks (especially in DocVQA, CulturalBench, and MathVista) visually demonstrates the high stability of the TwC paradigm across diverse MLLMs architectures. Notable outliers indicate model-specific strengths (e.g., Gemini on gsm8k) rather than method failure. Results are summarized in Figure 7. Results across different tasks show that TwC path II leads to largely consistent performance trends across models. On the DocVQA benchmark, all models maintain accuracy above 99.4%, indicating that emphasizing key visual regions in comics, together with accompanying textual prompts, provides reliable auxiliary information. Notably, Gemini 3 Pro achieves relatively stronger performance on several tasks, reaching 95.3% accuracy on GSM8K. Overall, comic panels function as reusable intermediate representation that delivers stable performance gains across tasks and model configurations, demonstrating certain degree of crossmodel generalization. 4The versions of these models are respectively: claude-3-7sonnet-20250219, qwen2.5-vl-72b-instruct, gpt-5.2-2025-12-11, gemini-3-pro-preview, and gpt-4o-2024-05-13. To formalize the economic feasibility, we define the different visual signal generation cost function C(). For video generation (Think with Video), the cost is time-dependent: Cvideo(t) = α t, where α denotes the unit price per second. For our comic-based approach (TwC), the cost is imagedependent: Ccomic = β, where β represents the fixed cost of single composite image. ) ( ) ( o i e G 1 0.5 0 0 Video: C(t) = 0.10 TwC: = 0.134 $1.00 86.6% Cost Reduction Break-even 1.34s 6 Reasoning Task Duration (seconds) 4 8 $0.134 10 12 Figure 8. Comparing the image generation cost models. While video generation cost (Cvideo) scales linearly with task duration due to temporal redundancy, TwC maintains low, constant cost (Ccomic) regardless of the events temporal length. The shaded area represents the economic advantage of our approach. Adopting standard industrial pricing (α = $0.10/s 5, β = $0.134/img 6), 10-second dynamic reasoning task under the Thinking with Video (Tong et al., 2025) setting (consistent with prior work) costs $1.00 via video generation, compared to only $0.134 with TwC. This corresponds to cost compression ratio of Ccomic 13.4%, Cvideo i.e., an 86.6% reduction in media generation cost for typical reasoning instance. Notably, the two cost functions intersect at break-even point of 1.34 s, beyond which video-based reasoning becomes strictly more expensive. These results demonstrate that TwC achieves reduction in computational overhead without compromising reasoning accuracy. We theoretically analyze in Appendix B.3 why comics are more budget-efficient than videos. 6. Conclusion We introduce Thinking with Comics, multimodal reasoning paradigm that uses multi-panel comics as an efficient intermediate representation for temporal and multistep reasoning. TwC improves reasoning performance while avoiding video-generation overhead, with analyses highlighting the roles of narrative structure and embedded text, pointing to future directions in controllability, faithfulness, and evaluation. 5https://openai.com/api/pricing/ 6https://ai.google.dev/gemini-api/docs/pricing Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling"
        },
        {
            "title": "Impact Statement",
            "content": "This paper proposes Thinking with Comics, an efficient multimodal reasoning paradigm that uses comics as an intermediate representation between images and videos. By reducing redundancy and computational cost while preserving temporal and narrative structure, the approach improves the efficiency and practicality of multimodal reasoning systems for long-context and temporal reasoning tasks. We do not foresee immediate harmful applications; nevertheless, future work should consider the influence of narrative style and cultural conventions in comics to ensure robust and fair deployment across diverse settings."
        },
        {
            "title": "References",
            "content": "Openai o3 and o4-mini system card. URL https: //api.semanticscholar.org/CorpusID: 278283461. Anthropic. Introducing claude sonnet 4.5. https://www. anthropic.com/news/claude-sonnet-4-5, 2025. Augereau, O., Iwata, M., and Kise, K. An overview of In 2017 14th comics research in computer science. IAPR International Conference on Document Analysis and Recognition (ICDAR), volume 3, pp. 5459. IEEE, 2017. Besta, M., Blach, N., Kubicek, A., Gerstenberger, R., Podstawski, M., Gianinazzi, L., Gajda, J., Lehmann, T., Niewiadomski, H., Nyczyk, P., et al. Graph of thoughts: Solving elaborate problems with large language models. In Proceedings of the AAAI conference on artificial intelligence, volume 38, pp. 1768217690, 2024. Betker, J., Goh, G., Jing, L., Brooks, T., Wang, J., Li, L., Ouyang, L., Zhuang, J., Lee, J., Guo, Y., et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. Chen, A., Song, Y., Chen, K., Bai, X., Yang, M., Nie, L., Liu, J., Zhao, T., and Zhang, M. Make imagination clearer! stable diffusion-based visual imagination for multimodal machine translation. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2656726583, 2025. Cheng, Z., Chen, Q., Xu, X., Wang, J., Wang, W., Fei, H., Wang, Y., Wang, A. J., Chen, Z., Che, W., et al. Visual thoughts: unified perspective of understanding multimodal chain-of-thought. arXiv preprint arXiv:2505.15510, 2025. Chiu, Y. Y., Jiang, L., Lin, B. Y., Park, C. Y., Li, S. S., Ravi, S., Bhatia, M., Antoniak, M., Tsvetkov, Y., Shwartz, V., et al. Culturalbench: robust, diverse, and challenging cultural benchmark by human-ai culturalteaming. arXiv preprint arXiv:2410.02677, 2024. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Dhuliawala, S., Komeili, M., Xu, J., Raileanu, R., Li, X., Celikyilmaz, A., and Weston, J. Chain-of-verification reduces hallucination in large language models. In Findings of the association for computational linguistics: ACL 2024, pp. 35633578, 2024. Dong, R., Han, C., Peng, Y., Qi, Z., Ge, Z., Yang, J., Zhao, L., Sun, J., Zhou, H., Wei, H., et al. Dreamllm: Synergistic multimodal comprehension and creation. arXiv preprint arXiv:2309.11499, 2023. Gao, T., Chen, P., Zhang, M., Fu, C., Shen, Y., Zhang, Y., Zhang, S., Zheng, X., Sun, X., Cao, L., et al. Cantor: Inspiring multimodal chain-of-thought of mllm. In Proceedings of the 32nd ACM International Conference on Multimedia, pp. 90969105, 2024. Google. Gemini ai video generator powered by veo 3.1. https://gemini.google/overview/ video-generation/, 2025. Google DeepMind. Gemini 3 pro. https://deepmind. google/models/gemini/pro/, 2025. Guerin, C., Rigaud, C., Mercier, A., Ammar-Boudjelal, F., Bertet, K., Bouju, A., Burie, J.-C., Louis, G., Ogier, J.- M., and Revel, A. ebdtheque: representative database of comics. In 2013 12th International Conference on Document Analysis and Recognition, pp. 11451149. IEEE, 2013. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseekr1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Hu, Y., Shi, W., Fu, X., Roth, D., Ostendorf, M., Zettlemoyer, L., Smith, N. A., and Krishna, R. Visual sketchpad: Sketching as visual chain of thought for Advances in Neural multimodal language models. Information Processing Systems, 37:139348139379, 2024. 9 Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling Huang, J. and Chang, K. C.-C. Towards reasoning in large language models: survey. In Findings of the association for computational linguistics: ACL 2023, pp. 10491065, 2023. Hurst, A., Lerer, A., Goucher, A. P., Perelman, A., Ramesh, A., Clark, A., Ostrow, A., Welihinda, A., Hayes, A., Radford, A., et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199 22213, 2022. Pushing the limits of mathematical reasoning in open arXiv preprint arXiv:2402.03300, language models. 2024. Singh, A., Fry, A., Perelman, A., Tart, A., Ganesh, A., El-Kishky, A., McLaughlin, A., Low, A., Ostrow, A., Ananthram, A., et al. Openai gpt-5 system card. arXiv preprint arXiv:2601.03267, 2025. Tong, J., Mou, Y., Li, H., Li, M., Yang, Y., Zhang, M., Chen, Q., Liang, T., Hu, X., Zheng, Y., et al. Thinking with video: Video generation as promising multimodal reasoning paradigm. arXiv preprint arXiv:2511.04570, 2025. Li, C., Wu, W., Zhang, H., Xia, Y., Mao, S., Dong, L., Vulic, I., and Wei, F. Imagine while reasoning in space: arXiv preprint Multimodal visualization-of-thought. arXiv:2501.07542, 2025. Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., and Zhou, D. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and In The Twelfth Cobbe, K. Lets verify step by step. International Conference on Learning Representations, 2023. Liu, Z., Sun, Z., Zang, Y., Dong, X., Cao, Y., Duan, H., Lin, D., and Wang, J. Visual-rft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785, 2025. Lu, P., Bansal, H., Xia, T., Liu, J., Li, C., Hajishirzi, H., Cheng, H., Chang, K.-W., Galley, M., and Gao, J. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. Mathew, M., Karatzas, D., and Jawahar, C. Docvqa: In Proceedings dataset for vqa on document images. of the IEEE/CVF winter conference on applications of computer vision, pp. 22002209, 2021. Mitra, C., Huang, B., Darrell, T., and Herzig, R. Compositional chain-of-thought prompting for large multimodal models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1442014431, 2024. OpenAI. Sora 2 is here. https://openai.com/ index/sora-2/, 2025. Wang, Y., Wu, S., Zhang, Y., Yan, S., Liu, Z., Luo, J., and Fei, H. Multimodal chain-of-thought reasoning: comprehensive survey. arXiv preprint arXiv:2503.12605, 2025. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Xu, G., Jin, P., Wu, Z., Li, H., Song, Y., Sun, L., and Yuan, L. Llava-cot: Let vision language models reason stepby-step. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 20872098, 2025. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T., Cao, Y., and Narasimhan, K. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822, 2023. Zhang, Z., Zhang, A., Li, M., Zhao, H., Karypis, G., and Smola, A. Multimodal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923, 2023. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent In Proceedings of the IEEE/CVF diffusion models. conference on computer vision and pattern recognition, pp. 1068410695, 2022. Zheng, G., Yang, B., Tang, J., Zhou, H.-Y., and Yang, S. Ddcot: Duty-distinct chain-of-thought prompting for multimodal reasoning in language models. Advances in Neural Information Processing Systems, 36:51685191, 2023. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath: 10 Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling A. Empirical Analysis: Why Comics Are Privileged Visual Reasoning Medium Building on the theoretical analysis in Appendix B, this section empirically evaluates the advantages of comics as visual reasoning medium. Specifically, we investigate (i) the structural stability of comic-based multi-panel generation, and (ii) the benefits of treating comics as global structure compared to incremental visual reasoning. A.1. Prompt-Induced Structural Stability in Multi-Panel Visual Generation This experiment examines whether comics, compared to non-comic visual styles, more naturally and stably support multi-panel generation. This setting is motivated by our theoretical analysis in Appendix B.4. We design two controlled prompt settings. In the Comic condition, the model is instructed to draw four-panel comic to solve the problem. In the Non-Comic condition, the model is instructed to draw four-step visual storyboard in realistic style, with the number of panels explicitly constrained to match the comic setting. Except for the presence of the word comic, all other prompt components and decoding parameters are kept identical. For evaluation, we sample 20 instances each from MATH-500 and MathVista. The generated images are answered by Gemini-3 Pro. We evaluate (i) the success rate of generating the required number of panels, and (ii) answer accuracy. Table 3. Comparison of structural stability and reasoning accuracy between Comic and Non-Comic prompts on MATH-500 and MathVista. Metric Dataset Comic Non-Comic Improvement Layout Success Rate (%) (Panel Consistency) MATH-500 MathVista Reasoning Accuracy (%) MATH-500 MathVista 95.0 90.0 75.0 70.0 70.0 65.0 60.0 55. +25.0 +25.0 +15.0 +15.0 As shown in Table 3, comic prompts consistently induce structurally complete multi-panel layouts across tasks, whereas Non-Comic instructions more frequently suffer from layout collapse or unintended merging of multiple steps, failing to reliably satisfy the step-wise generation constraint. The inherent panel-based structure of comics provides strong structural prior, aligning multi-step visual reasoning with chain-of-thought in the visual domain, and thereby improving the stability and overall performance of multimodal reasoning. These observations provide empirical support for our domain-shift analysis, suggesting that the comic format offers natural and robust scaffold for multi-panel generation that is difficult to reproduce with ad-hoc non-comic visual styles. A.2. Structural Coherence in Global vs. Incremental Visual Reasoning This experiment compares Global Comic generation and Incremental image chaining for multi-step visual reasoning. This comparison is motivated by our theoretical analysis in Appendix B.2. The former generates complete multi-panel comic in single pass, while the latter produces panels sequentially conditioned on previous outputs, with an identical number of panels in both settings. We evaluate on 20 samples each from MATH-500 and MathVista using human judgments on cross-panel logical continuity, state transitions, and textual quality (Appendix E.1). Table 4. Human evaluation results comparing Global and Incremental generation. We evaluate Accuracy (ACC) and three structural metrics (1-5 scale): Logic (reasoning flow), State (consistency between panels), and Quality (visual-textual fidelity). Global generation shows significant superiority in both objective performance and structural coherence. Benchmark Method ACC (%) Logic State Quality MATH-500 MathVista Average Incremental Global (Ours) Incremental Global (Ours) Incremental Global (Ours) 80.0 95.0 50.0 85.0 65.0 90. 11 4.17 4.86 3.50 4.47 3.83 4.67 3.72 4.67 3.50 4. 3.61 4.56 3.58 4.61 3.40 4.58 3.49 4.59 Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling As shown in Table 4 and bad case (Figure 9), results show that global generation yields significantly stronger cross-panel coherence, with more stable entity representations and smoother reasoning progression, whereas incremental generation suffers from error accumulation. This suggests that treating comics as holistic structured representation is crucial for preserving multi-step reasoning quality. These findings empirically support our claim that global structural planning, rather than stepwise local generation, is essential for maintaining coherent multi-step reasoning trajectories in the visual domain. Figure 9. Qualitative comparison between (a) Global Comic (Ours) and (b) Incremental Non-Comic generation for mathematical reasoning task (finding divisors of 196). Global generation maintains consistent character (FactoBot) and smooth logical flow, whereas the incremental baseline exhibits static scenes and lacks narrative coherence. B. Theoretical Justification of Comics as High-Quality and Efficient Visual Reasoning Medium B.1. Representation and Utility Let denote question, the ground-truth answer, and an intermediate representation generated by visual generator Gθ. Under Path II, the final prediction is ˆa = Fϕ(q, z), consistent with Eq. (46) in the main paper. We characterize an intermediate representation by two orthogonal criteria: (i) generation fidelity (how well Gθ can Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling produce z), and (ii) task sufficiency (how informative is for predicting a). Information-efficiency. We define the information-efficiency of for task solving as η(z) I(a; q) C(z) , (9) where I(; ) is conditional mutual information and C(z) is the media generation cost. Our main paper already instantiates C() for video and comics (constant per image vs. linear per second), providing an empirical cost rationale. B.2. Comics Outperform Single Images Due to Temporal Structure and Textual Anchoring single image is typically snapshot of an underlying latent trajectory s1:T (temporal/causal process). If the answer depends on multi-step temporal or causal relations in s1:T , then any snapshot = h(st) may discard relevant states. Formally, whenever is not conditionally independent of the latent trajectory given snapshot, i.e., we have strict information gap: I(a; s1:T q, x) > 0, I(a; s1:T q) = I(a; s1:T , q) I(a q) > I(a; x, q) I(a q) = I(a; q) (10) (11) Comics represent structured summary zcomic = (c1:K, τ ) consisting of panels c1:K (selected intermediate states) and embedded text τ (bubbles/narration). By the chain rule, I(a; zcomic q) = I(a; c1:K q) + I(a; τ q, c1:K), (12) where the second term is non-negative and captures the additional semantic anchoring channel. Therefore, comics can strictly dominate pure-visual sequences whenever textual anchoring carries answer-relevant cues: I(a; zcomic q) I(a; c1:K q), and if I(a; τ q, c1:K) > 0 then the inequality is strict. (13) This matches our ablation that adding bubbles/narration improves robustness and accuracy. B.3. Comics Are More Efficient Than Videos Under Budget Let video be = (x1, . . . , xT ) with frames. By the chain rule, I(a; q) = (cid:88) t=1 I(a; xt q, x<t). (14) In realistic videos, consecutive frames are highly correlated, hence I(a; xt q, x<t) quickly diminishes as grows (temporal redundancy). Thus, I(a; q) grows sublinearly with while video cost grows linearly with (or duration). Consequently, the efficiency η(v) = I(a; q)/C(v) decreases with longer videos once redundancy dominates. Comics can be seen as selecting key states (panels) from the latent trajectory to maximize task-relevant information: c1:K arg max , } I(a; xS q) . =K (15) 1,...,T { When the set function (S) = I(a; xS q) is approximately submodular (a standard diminishing-returns property for information measures), greedy selection achieves (1 1/e) approximation to the optimal subset. Hence, with far fewer visual tokens, comics retain most of the answer-relevant information while avoiding redundant frames, leading to higher η(zcomic) than η(v) at the same budget. This aligns with our observed panel-scaling curve where accuracy saturates around [4, 6]. 13 Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling B.4. Why Comics Generate Better than Synthetic Sequential Images: Domain-Shift Bound We now justify the claim that comics (a real, widely observed visual genre) are generated with higher fidelity than ad-hoc synthetic sequential images with logical relations that do not correspond to well-established visual manifold. Let Ptrain be the (unknown) effective training distribution of the image generator. Let Pcomic be the target distribution of real comics, and Psyn the distribution of synthetic sequential images. Consider perceptual fidelity loss L(x) (e.g., measuring artifacts, inconsistency, or poor alignment with prompts). standard domain adaptation bound (Ben-David type) implies that for any hypothesis class induced by the generator, Ex Ptarget[L(x)] Ex Ptrain [L(x)] + Div(Ptrain, Ptarget) + λ, (16) where Div(, ) is distribution divergence (e.g., HH-divergence or an IPM), and λ is the irreducible joint error term. If comics are real, frequent genre, then Pcomic is closer to Ptrain than an ad-hoc synthetic style: Therefore, the expected fidelity loss is lower for comics: Div(Ptrain, Pcomic) < Div(Ptrain, Psyn). Ex Pcomic[L(x)] < Ex Psyn[L(x)], (17) (18) i.e., the generator produces higher-quality outputs in the comic domain than in less natural, distribution-shifted synthetic domain. Comics simultaneously (i) reduce task uncertainty via structured temporal panels and textual anchoring, (ii) avoid the redundancy and high cost of video, and (iii) achieve higher generation fidelity due to smaller domain shift. Together, these provide principled justification for Thinking with Comics as high-density intermediate reasoning representation. C. Answer Extraction Protocol for Thinking with Comics C.1. Path I: End-to-End Comic Reasoning. In Path I, the model generates multi-panel comic as the complete reasoning resut, where the final answer is visually embedded in the comic, typically appearing in the last panel as explicit text or highlighted result. We perform answer extraction using GPT-5.2 as an external answer reader (The detail of prompt is in Appendix D.2). The extractor is provided with the generated comic panels together with the original question, and is instructed to identify the final answer depicted in the comic. The extracted answer is matched against the ground-truth label to compute ACC. Human Verification for Path I. To validate the reliability of model-based answer extraction, we randomly sample 20% of the evaluation instances for manual inspection. For each sampled instance, human annotator independently reads the answer from the comic and compares it with the answer extracted by GPT-5.2. We observe 100% agreement between automated extraction and human judgment, indicating that GPT-5.2 serves as stable proxy for answer reading in comic-based reasoning. The detail of human verification is in Appendix E.2. C.2. Path II: Comics-as-Context Reasoning. In Path II, comics are used solely as intermediate contextual representations, while the final answer is explicitly generated by MLLMs in textual form. Answer extraction in this setting is performed by directly parsing the final model output. The predicted answer is matched against the ground-truth label using standard normalization and exact-match rules to compute ACC. D. Prompt Examples D.1. Prompt in the Main Experiment Prompt for Gemini-3 Pro Image in MATH500 & GSM8K Please help me draw comic to solve this math problem: {question} Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling Prompt for Gemini-3 Pro Image in MathVista Please draw suitable comic strip to help solve this math/visual reasoning problem based on the provided image. Context: This is visual question answering problem at not applicable level, involving natural image. Required skills: numeric commonsense, arithmetic reasoning. Question: {question} The answer should be integer value. Goal: Create comic that illustrates the problem-solving process step-by-step, showing the complete reasoning from understanding the question to finding the answer. Prompt for Gemini-3 Pro Image in CulturalBench-Easy Please draw comic strip to help solve this multiple-choice cultural knowledge question about {country}. Question: {question} Options:{options} Your comic should: 1. Visually depict the cultural scenario described in the question. 2. Show the correct cultural practice/behavior/knowledge of {country}. 3. Through the comic story, clearly demonstrate which option (A, B, C, or D) is the correct answer. 4. The comic should lead the viewer to understand and identify the correct choice. Goal: Help the viewer select the correct answer from the four options by illustrating the authentic cultural context of {country}. Prompt for Gemini-3 Pro Image in CulturalBench-Hard Please draw comic strip to help determine if the following cultural statement about {country} is TRUE or FALSE. Question: {question} Statement to evaluate: {statement to judge} Your comic should: 1. Visually depict the authentic cultural practice/behavior in {country}. 2. Show whether this statement accurately represents the real cultural norm. 3. Through the comic story, clearly demonstrate if this statement is TRUE or FALSE. 4. The comic should help the viewer judge the truthfulness of this cultural claim. Goal: Help the viewer determine TRUE or FALSE by illustrating the actual cultural reality of {country}. Prompt for Gemini-3 Pro Image in DocVQA Please draw suitable comic strip to help answer this document question based on the provided document image. Task Type: Document Visual Question Answering {question types} Question: {question} Goal: Create comic that: 1. Shows the key information extraction process from the document. 2. Highlights the relevant parts of the document that contain the answer. 3. Illustrates the reasoning steps to find the correct answer. 4. Makes the final answer clear through visual storytelling. The comic should help explain how to locate and extract the answer from the document. 15 Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling Prompt for TWI method: G-Image Instruction: You are an expert in writing prompts for text-to-image generation. Based on the following image and textual query, write precise and detailed prompt to generate image highly relevant to the query. This image will serve as an auxiliary tool to help resolve the task accurately. Consider composition, style, and detail to ensure practicality. Reasoning Protocol: Based on the question and the additional synthesized image, lets think step by step, but avoid adding visual descriptions during the reasoning process! Output Format: End your thinking process with the most appropriate answer in the format ANSWER: (x) followed by the choice. ### Question: ### Choices: ### Prompt Generated: <Extra Image Input> Your Response: D.2. Prompt in the Analysis Experiment Prompt for Gemini-3 Pro Image in Role-playing Narrative Alignment Experiment Please help me draw Slice-of-Life style comic to solve this math problem: {question} Please help me draw Documentary realistic style picture to solve this math problem: {question} Prompt for Gemini-3 Pro Image in Ablation on Textual Anchoring Experiment: CulturalBench-Easy Please draw comic strip to help solve this multiple-choice cultural knowledge question about {country}. Question: {question} Options:{options} Your comic should: 1. Visually depict the cultural scenario described in the question. 2. Show the correct cultural practice/behavior/knowledge of {country}. 3. Through the comic story, clearly demonstrate which option (A, B, C, or D) is the correct answer. 4. The comic should lead the viewer to understand and identify the correct choice. 5. Relies solely on visual storytelling; DO NOT contain any text, speech bubbles, narration boxes, or onomatopoeia. Goal: Help the viewer select the correct answer from the four options by illustrating the authentic cultural context of {country}. Prompt for Gemini-3 Pro Image in Ablation on Textual Anchoring Experiment: CulturalBench-Hard Please draw comic strip to help determine if the following cultural statement about {country} is TRUE or FALSE. Question: {question} Statement to evaluate: {statement to judge} Your comic should: 1. Visually depict the authentic cultural practice/behavior in {country}. 2. Show whether this statement accurately represents the real cultural norm. 3. Through the comic story, clearly demonstrate if this statement is TRUE or FALSE. 4. The comic should help the viewer judge the truthfulness of this cultural claim. 5. Relies solely on visual storytelling; DO NOT contain any text, speech bubbles, narration boxes, or onomatopoeia. Goal: Help the viewer determine TRUE or FALSE by illustrating the actual cultural reality of {country}. 16 Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling Prompt for Gemini-3 Pro in Structural Coherence Experiment: Global Visual Reasoning Please create complete {num panels}-panel comic strip that illustrates the step-by-step solution process for this math problem. Problem: {question} Requirements: 1. Create exactly {num panels} panels arranged in coherent sequence. 2. Panel 1: Introduce the problem and key information. 3. Panel 2-{num panels-1}: Show the logical reasoning steps progressively. 4. Panel {num panels}: Present the final solution and answer. 5. Maintain consistent characters/elements across all panels. 6. Include clear mathematical notation and explanations in each panel. 7. Ensure smooth visual transitions between panels. 8. Each panel should build logically on the previous one. Important: Generate ALL {num panels} panels as single cohesive comic image with clear panel divisions. Prompt for Gemini-3 Pro in Structural Coherence Experiment: Incremental Visual Reasoning Case 1: First Panel (panel num = 1) Create realistic photo-style image (Step 1 of {total panels}) for solving this math problem. Problem: {Question} Style: Realistic photo, NOT cartoon or comic style. This is the FIRST step image. It should: 1. Introduce the problem scenario with realistic visual elements. 2. Set up real-world objects or scenes that represent the mathematical concepts. 3. Clearly present the mathematical question in realistic context. 4. Use photorealistic rendering, natural lighting, and realistic textures. Generate ONLY Step 1 as single realistic photo-style image. Case 2: Intermediate Panels (1 < panel num < total panels) Create realistic photo-style image (Step {panel num} of {total panels}) for solving this math problem. Problem: {Question} Style: Realistic photo, NOT cartoon or comic style. This is Step {panel num} of {total panels}. It should: 1. Continue logically from the previous step. 2. Show the next step in the reasoning or calculation process with realistic visuals. 3. Maintain consistent visual elements from previous image. 4. Prepare for the next step in the solution. 5. Use photorealistic rendering, natural lighting, and realistic textures. Based on the previous image shown, continue the problem-solving process. Generate ONLY Step {panel num} as single realistic photo-style image. Case 3: Final Panel (panel num = total panels) Create realistic photo-style image (Step {panel num} of {total panels}, the FINAL step) for solving this math problem. Problem: {Question} Style: Realistic photo, NOT cartoon or comic style. This is the LAST step (Step panel num of total panels). It should: 1. Continue from the previous steps reasoning. 2. Show the final calculation or conclusion with realistic visual elements. 3. Present the final answer clearly in realistic context. 4. Provide satisfying conclusion to the problem-solving journey. Based on the previous image shown, maintain visual consistency and complete the solution. Generate ONLY Step {panel num} as single realistic photo-style image. 17 Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling Prompt for GPT-5.2 Answer Extraction in Path You are an answer extraction model. Context: You are given multi-panel comic that visually depicts complete reasoning process, together with the original question. Task: Read the comic and identify the final answer shown in the last panel. Rules: Only output the final answer. Do not explain the reasoning. If the answer is numeric, output the normalized numeric form. If the answer is short phrase or option, output it verbatim. Question: {q} E. Human Evaluation Protocol E.1. Evaluation for Global and Incremental Visual We employed three expert annotators to conduct human evaluations for all experiments described in Section A.2. All annotators hold masters degree or higher and have prior experience with visionlanguage evaluation tasks. Before annotation, we provided detailed training session covering task definitions, scoring rubrics, and representative examples. The annotators then completed pre-annotation phase, during which we aligned interpretations of the evaluation criteria (Accuracy, Logic, State, and Quality) and resolved ambiguities in the scoring guidelines. Each sample was independently rated by all three annotators. We used the averaged score across annotators as the final reported value. Inter-annotator agreement was monitored throughout the process, and inconsistencies were discussed and resolved according to the established rubric. E.2. Evaluation for external answer reader To verify the reliability of model-based answer extraction in Path I, we conduct manual cross-validation study involving three independent human annotators. shared subset comprising 20% of the evaluation instances is randomly sampled across benchmarks. For each sampled instance, all three annotators are provided with the original question and the generated multi-panel comic, and independently identify the final answer depicted in the panel. The annotations are then compared across annotators to ensure consistency, and any discrepancies are resolved through discussion to reach consensus. The consensus human answer is finally compared against the answer extracted by GPT-5.2 under identical normalization rules. We observe complete agreement between the consensus human judgments and the automated extraction, supporting the reliability of GPT-5.2 as an answer reader in comic-based reasoning. F. Examples of TwC This section provides qualitative examples of Thinking with Comics (TwC). It begins with comparison of different comic styles, followed by illustrative examples from Reasoning Tasks and (Long) Context Understanding Tasks. In total, five benchmarks are included to demonstrate the use of TwC under different task formulations and contextual requirements. F.1. Comparison of Different Comic Styles We provide qualitative examples of different comic-style visualizations for problem solving. The Documentary style mainly relies on realistic images to directly present the problem context and relevant information. The Role-playing style introduces explicit characters or professional roles, through which the reasoning process is narrated and unfolded in role-driven manner. In contrast, the Slice-of-life style embeds the reasoning process within everyday scenarios, illustrating problem solving through familiar daily-life activities. Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling Figure F1-1: Examples of different comic-style visualizations for problem solving. 19 Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling F.2. Reasoning Tasks Figure F2-1: MATH 20 Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling Figure F2-2: MathVista 21 Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling Figure F2-3: GSM8K 22 Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling F.3. (Long) Context Understanding Tasks Figure F3-1: CulturalBench 23 Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling Figure F3-2: DocVQA"
        }
    ],
    "affiliations": [
        "Harbin Institute of Technology"
    ]
}