{
    "paper_title": "PISA: Piecewise Sparse Attention Is Wiser for Efficient Diffusion Transformers",
    "authors": [
        "Haopeng Li",
        "Shitong Shao",
        "Wenliang Zhong",
        "Zikai Zhou",
        "Lichen Bai",
        "Hui Xiong",
        "Zeke Xie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion Transformers are fundamental for video and image generation, but their efficiency is bottlenecked by the quadratic complexity of attention. While block sparse attention accelerates computation by attending only critical key-value blocks, it suffers from degradation at high sparsity by discarding context. In this work, we discover that attention scores of non-critical blocks exhibit distributional stability, allowing them to be approximated accurately and efficiently rather than discarded, which is essentially important for sparse attention design. Motivated by this key insight, we propose PISA, a training-free Piecewise Sparse Attention that covers the full attention span with sub-quadratic complexity. Unlike the conventional keep-or-drop paradigm that directly drop the non-critical block information, PISA introduces a novel exact-or-approximate strategy: it maintains exact computation for critical blocks while efficiently approximating the remainder through block-wise Taylor expansion. This design allows PISA to serve as a faithful proxy to full attention, effectively bridging the gap between speed and quality. Experimental results demonstrate that PISA achieves 1.91 times and 2.57 times speedups on Wan2.1-14B and Hunyuan-Video, respectively, while consistently maintaining the highest quality among sparse attention methods. Notably, even for image generation on FLUX, PISA achieves a 1.2 times acceleration without compromising visual quality. Code is available at: https://github.com/xie-lab-ml/piecewise-sparse-attention."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 1 ] . [ 1 7 7 0 1 0 . 2 0 6 2 : r PISA: Piecewise Sparse Attention Is Wiser for Efficient Diffusion Transformers Haopeng Li 1 Shitong Shao 1 Wenliang Zhong 1 Zikai Zhou 1 Lichen Bai 1 Hui Xiong 1 Zeke Xie 1 calico cat, with carrot and pack of tissues placed in front of it, is staring at the camera with confused look and resting one of its front paws on the carrot, in realistic painting style. (a) Wan2.1-14B Dense Attention Latency: 1468 Dense Attention Ours (r=85%) SpargeAttn (r=80%) Dense Attention Ours (r=85%) SpargeAttn (r=80%) Dense Attention Ours (r=85%) SpargeAttn (r=80%) (b) Wan2.1-14B PISA (Ours) Latency: 687 (2.14 Speedup) PSNR: 24.69 LPIPS: 0.129 LPIPS: 0.373 Latency: 6.87 (c) Text-to-Image generation results on FLUX.1-dev, where denotes the sparsity ratio. Latency: 7.47 Latency: 8.32 PSNR: 22.60 PSNR: 13.32 Figure 1. PISA accelerates diverse generation tasks. Top (a, b): Wan2.1-14B video generation. PISA achieves 2.14 speedup over Dense Attention with no appreciable quality loss. Bottom (c): FLUX.1-dev text-to-image generation. PISA at higher sparsity ratio r=85% preserves better quality and structure than SpargeAttn (Zhang et al., 2025b)."
        },
        {
            "title": "Abstract",
            "content": "Diffusion Transformers are fundamental for video and image generation, but their efficiency is bottlenecked by the quadratic complexity of attention. While block sparse attention accelerates computation by attending only critical key-value blocks, it suffers from degradation at high sparsity by discarding context. In this work, we discover that attention scores of non-critical blocks exhibit distributional stability, allowing them to be approximated accurately and efficiently rather than discarded, which is essentially important for sparse attention design. Motivated by this key insight, we propose PISA, training-free Piecewise 1The Hong Kong University of Science and Technology (Guangzhou). Correspondence to: Zeke Xie <zekexie@hkustgz.edu.cn>. Preprint. February 3, 2026. 1 Sparse Attention that covers the full attention span with sub-quadratic complexity. Unlike the conventional keep-or-drop paradigm that directly drop the non-critical block information, PISA introduces novel exact-or-approximate strategy: it maintains exact computation for critical blocks while efficiently approximating the remainder through block-wise Taylor expansion. This design allows PISA to serve as faithful proxy to full attention, effectively bridging the gap between speed and quality. Experimental results demonstrate that PISA achieves 1.91 and 2.57 speedups on Wan2.1-14B and Hunyuan-Video, respectively, while consistently maintaining the highest quality among sparse attention methods. Notably, even for image generation on FLUX, PISA achieves 1.2 acceleration without compromising visual quality. Code is available. PISA: Piecewise Sparse Attention Is Wiser for Efficient Diffusion Transformers 1. Introduction Sparse Attention Full Attention PISA (Ours) Diffusion Transformers (DiTs) (Peebles & Xie, 2023) have demonstrated impressive performance and scalability in generating high-fidelity images and videos, leading to their widespread adoption across diverse visual generation tasks (Arnab et al., 2021; Hong et al., 2022; Wan et al., 2025). However, as the demand for higher resolutions and longer video durations grows, the sequence length of input tokens increases dramatically. Consequently, the quadratic complexity of the self-attention mechanism (Vaswani et al., 2017) becomes significant bottleneck, resulting in prohibitively low inference efficiency for large-scale DiTs. To address the computational bottleneck, especially in highresolution image and video generation, recent research has leveraged the inherent sparsity in DiTs to enable sparse attention. Early works (Zhang et al., 2025d; Xi et al., 2025; Yang et al., 2025) capitalized on the spatiotemporal redundancy of video diffusion transformers to introduce static, training-free sparse attention patterns. To improve adaptability, other methods (Zhang et al., 2025b; Xu et al., 2025; Xia et al., 2025) propose computing sparse patterns dynamically at runtime. Moving beyond training-free approaches, methods such as VSA (Zhang et al., 2025c) and Radial Attention (Li et al., 2025) have explored trainable sparse attention; works like SANA (Xie et al., 2024; Chen et al., 2025) and Linfusion (Liu et al., 2024) have adopted linear attention for efficient generation, while methods such as SLA (Zhang et al., 2025a) have made preliminary attempts to combine sparse and linear attention. However, existing methods still face inherent limitations: (1) Hard truncation: Sparse attention directly discards keyvalue pairs, leading to performance drops at high sparsity and inefficiency on shorter sequences (e.g., 4K tokens). (2) Incompatibility with pre-trained weights: Linear and hybrid attention fundamentally alter the attention distribution of pre-trained models, precluding the direct reuse of weights and necessitating expensive retraining. These limitations underscore the need for unified mechanism that enhances efficiency without sacrificing quality or requiring retraining. To this end, we propose PISA, novel training-free sparse attention that accelerates DiTs while maintaining high accuracy through piecewise computation. Unlike standard sparse attention, which computes only critical blocks and discards the rest, PISA treats attention as piecewise process: (1) Exact computation for sparse key-value blocks to preserve critical information; (2) Approximation for the remaining blocks using block-wise Taylor expansion to cover the massive amount of non-critical information. Specifically, we propose hybrid-order approximation strategy that uses block-wise zero-order expansion and global first-order approximation to efficiently improve accuracy. This enables PISA to significantly enhance approximation fidelity relActive Blocks: 20% Active Blocks: 100% Active Blocks: 100% FLOPs: 20.0% L1 Error: 10.34% FLOPs: 100% L1 Error: 0% FLOPs: 20.4% L1 Error: 1.36% Figure 2. Visualization of attention patterns on Wan2.1-1.3B. PISA achieves 100% effective block coverage similar to full attention. This near-lossless approximation with only negligible computational overhead relative to standard sparse attention. ative to full attention, incurring only negligible computational overhead compared to standard sparse attention, as illustrated in Fig. 2. These dual computational pathways are fused into the online softmax process via custom kernel, allowing PISA to achieve state-of-the-art trade-off efficiency speed and accuracy without any training. Extensive experiments demonstrate the superiority of PISA. It accelerates Wan2.1-14B (Wan et al., 2025) and HunyuanVideo (Kong et al., 2024) by 1.91 and 2.57, respectively, while preserving state-of-the-art quality. Even for image generation tasks with lower inherent sparsity, PISA outperforms existing methods in both efficiency and quality. Our contributions are summarized as follows: 1. We propose novel piecewise sparse attention that enables full attention span with sub-quadratic complexity. Through unified exact-or-approximate execution, it resolves the critical dilemma between accuracy and efficiency. 2. We develop hybrid-order approximation scheme that boosts accuracy with negligible cost. Additionally, we derive covariance-aware routing strategy from error analysis, which effectively minimizes approximation divergence. 3. Experiments demonstrate that PISA achieves SOTA quality and efficiency across diverse tasks, setting new sparse attention paradigm for efficient visual generation. 2. Related Work Block Sparse Attention. To address the quadratic complexity of standard attention, sparse attention (Zhang et al., 2025b;c; Xi et al., 2025; Yang et al., 2025; Li et al., 2025; Wu et al., 2025) limits computation to subset of critical key-value blocks via static priors or dynamic routing. However, current methods simply discard the unselected blocks, which inevitably exacerbates output error and degrades performance at high sparsity ratio. In contrast, PISA guarantees strictly lower error bound than standard sparse attention by approximating the unselected blocks instead of dropping them, enabling superior performance. 2 PISA: Piecewise Sparse Attention Is Wiser for Efficient Diffusion Transformers Linear Attention. Linear Bidirectional attention (Katharopoulos et al., 2020) achieves linear complexity by replacing the exponential kernel with feature mappings. In the vision domain, existing bidirectional methods primarily focus on modifying these feature mappings (Liu et al., 2024; Meng et al., 2025; Han et al., 2023), yet their core formulation remains consistent with the canonical framework. Notably, Taylor-based approaches (Arora et al., 2024) typically expand around zero for the entire sequence. Although kernel tricks (Gelada et al., 2025) can yield high-order approximations, they incur severe dimension explosion and fail to preserve the pre-trained attention distribution. Consequently, these methods require computationally expensive retraining. Native Hybrid Attention. Recognizing the limitations of pure sparse or linear approaches, recent works have explored hybrid architectures. Methods like SLA (Zhang et al., 2025a) and NHA (Du et al., 2025) attempt to combine sparse (or sliding window) attention with linear attention. However, these methods rely on an additive strategy that directly sums the outputs of different branches. This formulation disrupts the intrinsic normalization of attention weights. Consequently, they suffer from distribution shifts that prevent the inheritance of pre-trained weights, necessitating expensive fine-tuning. In contrast, PISA applies block-wise Taylor expansion to both the normalization numerator and denominator. By natively mixing exact and approximate terms under unified softmax framework instead of simply adding outputs, our method preserves the intrinsic distribution and enables superior training-free performance. 3. Methodology 3.1. Preliminaries Given an input sequence RLd, where is the length and is the feature dimension, the query, key, and value Q, K, RLd are derived from via learnable linear projections. The output RLd of attention is: Figure 3. Visualization of pre-softmax attention scores (QK ) in Wan2.1-1.3B. The block-wise scores exhibit symmetric bellshaped distribution. uncritical blocks (Left) cluster in negative regions where the 1st-order Taylor expansion is highly accurate, whereas important blocks (Right) diverge. This property remains robust under Safe-Exp shift for numerical stability (Bottom). However, directly discarding blocks inevitably causes the output to deviate sharply from the original attention distribution. This limitation motivated us to design fast and accurate sparse attention mechanism capable of accelerating inference in training-free manner. Key Insights. To identify superior alternative to the keep-or-drop strategy, we analyzed the statistical properties of pre-trained models, yielding two key insights: (1) Pre-softmax scores of uncritical blocks exhibit symmetric distribution centered around zero or negative values, rendering them highly amenable to approximation via meancentered Taylor expansion, as illustrated in Fig. 3. (2) Normalization Consistency. Existing hybrid methods rely on an additive strategy (Osparse+Olinear) that violates the intrinsic weighted sum rule. To ensure training-free compatibility, the approximation must be integrated internally into the softmax numerator and denominator. = Softmax (cid:19) (cid:18) QK V . (1) 3.2. Piecewise Sparse Attention FlashAttention (Dao et al., 2022) introduces online softmax algorithm that avoids materializing attention scores in high bandwidth memory (HBM), significantly reducing memory access overhead. However, it still has quadratic complexity. Based on these insights, we propose Piecewise Sparse Attention (PISA), which unifies exact sparse computation with block-wise approximation directly within the online softmax. The following subsection outline our framework. To mitigate this, sparse attention restrict computation to subset of critical key-value blocks, which is formulated as: (cid:18) QK O = Softmax + . (2) (cid:19) Here, {0, }LL denotes mask, where entries of indicate the corresponding key-value pairs are ignored. 3 Formulation. We partition the query, key, value and output matrices (Q, K, , O) into blocks of size B. To streamline notation, we express the query and output vectors using global index. Let qt := QiB+m R1d (where = iB+m) denote the query vector at global index t, which is the m-th row vector belongs to the i-th block. For the key and value sides, we preserve the block-internal structure: PISA: Piecewise Sparse Attention Is Wiser for Efficient Diffusion Transformers let kj,n and vj,n denote the n-th row vectors of the j-th key and value blocks, respectively (where 1 B). For an arbitrary query qt, we partition the key-value block indices into sparse selected set Si (computed exactly) and long-tail unselected set Ui. Instead of discarding the unselected blocks as in standard sparse attention, we approximate the contribution from each block Ui via First-Order Taylor expansion centered at the block cenn=1 kj,n. The troid αt,j := exp(qt output vector ot R1d is derived by normalizing the weighted value aggregation (omit scale factor for briefly): ), where kj := 1 (cid:80)B ot :="
        },
        {
            "title": "Nt\nDt",
            "content": ", where (cid:88) (cid:88) Dt := exp(qtk j,n) + (cid:88) exp(qt ) , (3) n=1 jSi (cid:124) (cid:123)(cid:122) Exact Sparse Term (cid:125) jUi (cid:124) (cid:123)(cid:122) Block-wise Approx (cid:125) (cid:88) (cid:88) Nt := exp(qtk j,n)vj,n jSi (cid:124) n=1 (cid:123)(cid:122) Exact Sparse Term (cid:125) (cid:88) + exp(qt ) jUi (cid:124) (cid:123)(cid:122) Block-wise Zeroth-order Approx (cid:32) (cid:88) n= vj,n (4) (5) (cid:33) (cid:125) (cid:88) + jUi (cid:124) (cid:32) exp(qt ) qt (cid:88) (cid:33) (kj,n kj)vj,n . (6) n=1 (cid:123)(cid:122) Block-wise First-order Approx (cid:125) Note that the first-order term in the denominator Dt cancels out because (cid:80)B n=1(kj,n kj) = 0. Since queries within the same block share identical block masking patterns, they can naturally generalize to the full block via matrix operations. Practical Challenge. The term (4) corresponds to standard block sparse attention, which can be computed efficiently. Similarly, term (5) can be efficiently computed via matrix multiplication (GEMM) by grouping pre-computed kj and (cid:80)B n=1 vj,n into sub-blocks. However, for term (6), although it theoretically exhibits linear complexity, its practical implementation is severely bottlenecked by memory access. Computing this first-order term necessitates handling distinct matrices (cid:80)B n=1(kj,n kj)vj,n Rdd for each block Ui, weighted by block-specific scalar. Whether these matrices are pre-computed and loaded from HBM or computed on-the-fly in SRAM, the process results in memory-bound operation with low arithmetic intensity, rendering the theoretical speedup unattainable in practice. 3.3. Hybrid Approximation To resolve this conflict between theoretical complexity and hardware efficiency, we propose two solutions. Global First-Order Correction. We propose hybridorder approximation in which the first-order term is formulated globally across all unselected blocks, rather than block-wise. This allows all blocks to share unified scale factor βt, thereby eliminating the need for weighted summation. Let kt denote the global centroid of keys across all blocks in Ui. The term (6) can be rewritten as: (cid:88) (cid:88) βtqt (kj,n kt)vj,n. (7) jUi n=1 Determining the expansion coefficient βt presents critical challenge. While standard first-order Taylor expansion at the global centroid kt would suggest βt = exp(qt ), Jensens Inequality indicates that this approach severely underestimates the slope of the global first-order term, rendering the correction ineffective. To address this, we define βt using the mean of exp(qt ) effectively employing an average slope to estimate the magnitude of the first-order correction for all unselected blocks. Let Hj := (cid:80)B n=1(kj,n kt)vj,n Rdd. The term (6) can be rewritten as: 1 Ui (cid:88) jUi exp(qt ) qt (cid:88) jUi Hj . (8) Furthermore, by associating the normalization factor 1/Ui with the summation of Hj, we essentially compute the mean of Hj over the unselected blocks. Observing that the set of unselected blocks Ui typically constitutes the vast majority of the total blocks, we approximate the query-dependent mean over Ui using query-independent global statistic = 1 j=1 Hj, where is number of blocks. This global statistic can be precomputed via single pass. Consequently, we arrive at computationally efficient formulation: (cid:80)N (cid:88) (cid:88) Nt := exp(qtk j,n)vj,n (9) jSi (cid:124) (cid:88) + n=1 (cid:123)(cid:122) Exact Sparse Term (cid:32) (cid:88) vj,n αt,j jUi (cid:124) n=1 (cid:123)(cid:122) Block-wise 0th-order (cid:125) (cid:33) (cid:125) + qt (cid:88) αt,j . (10) jUi (cid:123)(cid:122) Global 1st-order (cid:125) (cid:124) Error Analysis. As proved in Theorem 3.1, the error induced by this replacement is bounded by the product of the tail probability mass and the variance of block matrices. Since αt,j is small in the unselected set, the approximation error remains controlled. 4 PISA: Piecewise Sparse Attention Is Wiser for Efficient Diffusion Transformers Figure 4. The algorithm pipeline of PISA. Prepare Phase: We pre-compute block-wise mean of the queries and keys (Q, K), block-wise sum of value ( ˆV ) and the global in single pass. block-wise Top-K selection identifies critical blocks. Fused Attention Kernel: The kernel dynamically switches execution paths: selected blocks (e.g., indices 2, 3) undergo exact computation (Phase 1), while unselected blocks (e.g., indices 1, 4) are approximated using block-wise zeroth-order expansion (Phase 2). In Phase 3, the is applied to inject global first-order approximation. This design allows loading the global correction term once, avoiding memory-bound streaming. Theorem 3.1 (Error Analysis of Global First-Order Approximation). Following our previous notation. Assume there exists constant Cq > 0, such that the query norm is bounded, i.e., qt2 Cq. Let ot = Nt be the attention output computed using Dt the exact block-wise first-order approximation and let ot be the output computed after replacing (cid:80) αt,jHj by ((cid:80) (cid:80)B αt,j) H. Define τt := (cid:80) j,n). Let := maxjUi Hj H2. If we denote the tail fraction ρt := τt/Dt (0, 1), then jUi n=1 exp(qtk jUi jUi ot ot2 Cq ρt . Proofs and further discussions are provided in Appendix C. Covariance-Aware Block Selection. Practically, to guarantee small approximation error it suffices to ensure either the tail fraction ρt is small, or the per-block heterogeneity is small. Based on the Theorem 3.1, we propose Covariance-Aware Top-k Strategy that incorporates the norm of the block covariance matrix as prior for importance routing. Let Mj := Hj H2, the selection score for block with respect to query qt is defined as: Scoret,j = Softmax (cid:32) qt d (cid:33) + log (Mj + ϵ) , (11) where ϵ > 0 is small constant for numerical stability. The derivation of Eq. (11) is further elaborated on in the Appendix D. This strategy ensures that blocks with either high semantic relevance are preserved in the exact set Si, while the smoother blocks are delegated to the Taylor expansion. 3.4. Hardware-aware Kernel Implementation We design fused kernel which efficiently interleaves exact computation with approximate scanning in online softmax. Phase 1 Omit the safe softmax 0, mi . Load Kj, Vj into SRAM. On-chip: Sij = QiK On-chip: Pij = exp(Sij) On-chip: ℓi ℓi + rowsum(Pij) On-chip: Oi Oi + PijVj Algorithm 1 PISA Forward Pass Input: Q, K, RLd, block size and C, sparsity 1: Compute block means Q, K, block sum ˆV and global H. 2: Compute block indices Si via Eq. (11) for each query block i. 3: Partition K, ˆV into groups { K[g], V[g]} of size C. 4: for 1 to = L/B do Init Oi 0, ℓi 0, ℓtail 5: Load Qi into SRAM. 6: for Si do 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: end for Output: = {Oi}M Load K[g], V[g] into SRAM. On-chip: Sig = Qi [g] On-chip: Sig[{j Si}] . On-chip: Pig = exp(Sig) On-chip: ℓi ℓi + rowsum(Pig) On-chip: ℓtail ℓtail Oi Oi + Pig V[g] end for On-chip: Ri = diag(ℓtail On-chip: Oi diag(ℓ1 end for for 1 to N/C do )(QiH) L1 )(Oi + Ri) + rowsum(Pig). Omit the safe softmax procedure for brevity i=1 RLd. Column masking Phase 2 Phase 3 We illustrate the pipeline in Fig. 4 and present the pseudocode in Algorithm 1. In Prepare Phase, we pre-computes block centroids and the global statistic while selecting critical blocks Si. In Phase 1, we load the selected blocks Si into SRAM to perform exact attention. Then in Phase 2, we scan aggregated block vectors in groups using coalesced memory access. dynamic mask excludes selected blocks, enabling high-throughput for the tail mass. Finally, we inject the global approximation in Phase 3. This operation incurs negligible cost yet effectively recovers firstorder gradient information. 5 PISA: Piecewise Sparse Attention Is Wiser for Efficient Diffusion Transformers (a) Latency Analysis (b) Speedup Analysis Figure 5. Kernel efficiency profile. (a) Latency comparison across sequence lengths at 12.5% density (87.5% sparsity) under two mainstream configurations with notation B-H-D (batch size, num heads, head dim). (b) Relative speedup against FlashAttn-2/3 across varying densities and sequence lengths under the B2-H16-D128 configuration. The dashed line indicates the baseline performance. 4. Experiments 4.1. Experimental Setup Models. We evaluate PISA on both video and image generation tasks. For video generation, we employ Wan2.1 (1.3B/14B) (Wan et al., 2025) to produce videos at 480p and 720p resolutions, respectively. For image generation, we utilize SD 3.5 (Esser et al., 2024) and FLUX.1 (Labs et al., 2025) to generate images at resolution of 10241024. Benchmarks. We employ VBench (Huang et al., 2024) to evaluate both video quality and temporal consistency. For image generation, we utilize FID (Heusel et al., 2017) alongside suite of human preference metrics, including ImageReward (IR) (Xu et al., 2023), HPSv2 (Wu et al., 2023), and MPS (Zhang et al., 2024b). Furthermore, we compute SSIM, PSNR and LPIPS (Zhang et al., 2018) to quantify the similarity between our method and full attention. Regarding efficiency, we note that theoretical FLOPs often fail to reflect real-world speeds. Therefore, to ensure fair comparison of efficiency, we focus exclusively on end-to-end latency and the speedup ratio relative to full attention. Implementation Details. We define sparsity as the proportion of blocks computed using approximation, analogous to the ratio of skipped blocks in standard sparse attention. Our kernel implementation uses block size of 6464. We only employ the Covariance-Aware Block Selection to boost performance for image generation. Following prior works (Li et al., 2025; Yang et al., 2025), we adopt warmup strategy where early layers and inference steps retain dense. Detailed configurations are provided in the Appendix E. 4.2. Kernel Efficiency Evaluation To evaluate the efficiency of our method, we benchmark against state-of-the-art implementations. Specifically, we adopt FlashAttn-2/3 (FA) (Dao, 2024; Shah et al., 2024) as the standard for exact full attention, and SpargeAttn (Zhang et al., 2025b) as the baseline for block sparse attention. All efficiency results were profiled on NVIDIA H800 GPUs. 6 Figure 6. Latency breakdown of PISA. We profile the cumulative runtime of the four kernel phases: (1) block reduction, (2) block selection, (3) exact attention, (4) approximate attention (comprises block-wise zeroth-order and global first-order approximation). Efficiency vs. Sequence Length. As illustrated in Fig. 5a, PISA at density of 12.5% (sparsity 87.5%) consistently outperforms FA3 and SpargeAttn. Notably, even at shorter sequence (e.g., 4K), our method maintains speed advantage over FA3, whereas SpargeAttn exhibits performance degradation in this regime, becoming slower than FA3. Efficiency vs. Density. Fig. 5b illustrates the speedup of PISA relative to FA2 and FA3 across varying densities and sequence lengths. Notably, even when the density exceeds 70% (less than 30% sparsity), our method consistently outperforms FA2 across all sequence lengths. Regarding FA3, our method surpasses it on longer sequences (>8K) when the density is below 50%, whereas for shorter sequences (4K), it outperforms FA3 provided the density is below 70%. Runtime Breakdown. We profile the runtime of distinct phases within our kernel. As illustrated in Fig. 6, the approximation phase incurs minimal overhead, confirming that our method improves accuracy without sacrificing efficiency. 4.3. Visual Generation Evaluation Video Generation. We evaluate our PISA against recent sparse attention works (e.g., SpargeAttn (Zhang et al., 2025b), SVG2 (Yang et al., 2025)) on the Wan2.1 (Wan et al., 2025) and Hunyuan-Video (Kong et al., 2024). PISA: Piecewise Sparse Attention Is Wiser for Efficient Diffusion Transformers Dense Attention Dense Attention PISA (Ours) PISA (Ours) Figure 7. Video generation samples of different attention mechanisms. Left: Hunyuan-Video 13B. Right: Wan2.1-14B. SVG2 SpargeAttn Table 1. Comparison of different sparse attention on video generation with warmup strategy. : The sparsity is derived from the actual statistical data of all samples, using the official-provided configuration for reproducing the 87.13% sparsity reported in SVG2 paper. Model Method Sparsity Wan2.1-1.3B Text-to-Video 480P Wan2.1-14B Text-to-Video 720P Hunyuan-13B Text-to-Video 720P Dense Sparge SVG2 Ours Dense Sparge SVG2 Ours Dense Sparge SVG2 Ours 0.00% 87.5% 84.4% 87.5% 0.00% 87.5% 80.6% 87.5% 0.00% 87.5% 81.4% 87.5% S.C. 94.96 93.76 93.79 94.58 95.98 95.69 95.39 95.80 95.60 95.38 94.88 95. Vbench(%) I.Q. 66.09 64.89 64.51 65.94 67.71 67.11 66.97 67.88 67. 67.37 63.10 68.16 A.Q. 60.79 58.66 59.09 60.03 63.08 62.72 61.92 63. 61.55 61.35 58.58 61.85 SSIM Similarity PSNR LPIPS Efficiency Latency Speedup 0.761 0.809 0.800 0.766 0.796 0.787 0.837 0.848 0. 20.75 22.85 22.62 21.47 22.92 22.69 24.85 26.40 26. 0.138 0.104 0.111 98 51 62 48 1564 0.144 0.121 0.124 844 882 818 1651 0.107 0.109 0.106 658 649 641 1.00 1.92 1.58 2.04 1.00 1.85 1.77 1.91 1.00 2.51 2.54 2.57 Table 2. Comparison of different sparse attention on video generation without warmup strategy. : Same clarification as in Table 1. Method Sparsity VBench(%) Similarity I.Q. A.Q. PSNR LPIPS Speedup Wan2.1-1.3B (Text-to-Video 480P) 87.5% Sparge SVG2 75.8% 87.5% Ours 60.10 63.22 66. Wan2.1-14B (Text-to-Video 720P) 87.5% Sparge SVG2 74.7% 87.5% Ours 60.03 58.85 69.95 55.26 57.25 60.32 51.45 55.72 64.47 Hunyuan-13B (Text-to-Video 720P) 87.5% Sparge SVG2 81.0% 87.6% Ours 65.10 63.96 67. 60.34 60.34 61.65 10.92 13.42 14.16 9.48 10.67 12.04 14.91 16.20 18.73 0.397 0.292 0.267 0.520 0.489 0. 0.283 0.275 0.264 2.72 2.33 3.06 2.59 2.37 2.75 3.65 3.60 3.82 As demonstrated in Table 1, our method achieves state-ofthe-art performance on the VBench benchmark, significantly outperforming existing approaches while delivering substantial speedup advantages. Remarkably, it even surpasses full attention on certain generation quality metrics. In contrast, although SVG2 achieves high similarity scores, it suffers from severe degradation in overall video quality and consistency, exhibiting noticeable blurring and flickering between frames. Similarly, SpargeAttn struggles with the loss of fine-grained details, whereas PISA exhibits visual quality consistent with full attention, as shown in Fig. 7. Furthermore, unlike competing methods that deteriorate precipitously without warmup strategy, PISA maintains high-fidelity generation, as detailed in Table 2. This underscores the critical advantage of our efficient piecewise computation covering the full attention span. Image Generation. We compare our method against SparseAttn on Stable Diffusion 3.5 (SD3.5) (Esser et al., 2024) and FLUX.1 (Labs et al., 2025), with results shown in Table 3. At similar or higher sparsity levels, our method significantly outperforms SparseAttn in FID and other human preference benchmarks, while achieving greater speedups. In terms of similarity, our method consistently surpasses SparseAttn across different models, demonstrating that our 7 PISA: Piecewise Sparse Attention Is Wiser for Efficient Diffusion Transformers Table 3. Comparison of different sparse attention on text-to-image generation with warmup strategy. Model Method Sparsity SD 3.5-M SD 3.5-L Turbo FLUX.1 schnell FLUX.1 dev Dense-Attn SpargeAttn Ours Dense-Attn SpargeAttn Ours Dense-Attn SpargeAttn Ours Dense-Attn SpargeAttn Ours 0% 70% 70% 0% 80% 85% 0% 80% 85% 0% 80% 85% FID 15.48 19.52 14.78 17.70 22.27 17.56 15.21 15.74 15. 16.35 19.20 15.91 IR 0.912 0.465 0.883 0.877 0.235 0.782 0.911 0.728 0.887 0.965 0.906 1. MPS HPSv2 SSIM PSNR LPIPS Latency 32.32% 49.71% 26.03% 43.35% 38.69% 45.73% 48.13% 51.98% 30.13 26.30 30.40 30.75 27.15 29. 30.22 28.81 29.90 31.31 31.31 31.69 0.627 0.790 0.558 0.677 0.563 0.625 0.539 0. 14.60 19.55 13.88 16.64 13.62 15.17 12.88 17.07 0.269 0.158 0.281 0. 0.311 0.247 0.296 0.241 3.03 2.69 2.15 0.81 0.71 0.65 0.82 0.75 0.71 8.32 7.47 6.87 Figure 8. Relative error with respect to dense attention. Left: varying density (32K tokens). Right: varying length (20% density). Table 4. Ablation of hybrid approximation. The suffixes -0th, -1st, and -hyd denote block-wise 0th/1st-order, and hybrid approximations, respectively. : With covariance-aware selection. Model Method SSIM PSNR LPIPS Speedup FLUX.1-dev Wan2.1-14B PISA-0th PISA-1st PISA-hyd PISA-hyd PISA-0th PISA-hyd 0.643 0.679 0.677 0.682 0.772 0.787 16.10 17.04 17.01 17.09 21.68 22.69 0.274 0.246 0.248 0.241 0.136 0. 1.21 0.96 1.24 1.22 1.93 1.91 piecewise attention mechanism preserves critical structural and semantic information. Notably, on FLUX.1-dev, our method even outperforms full attention on certain metrics. Visual results are presented in Fig. 1 and Appendix F. 4.4. Ablation Study Quantitative Validation. We examine the normalized L1 error ratio of attention on Wan2.1-13B. As shown in Fig. 8, the zeroth-order variant of PISA (Sparse + Block-0th Approx) consistently achieves lower errors than standard sparse attention, while the hybrid-order variant (Sparse + Hybrid Approx) yields further improvements. This validates the numerical precision of our method, confirming its ability to minimize output error. Beyond output error of attention, we evaluate generation n S ) O r ( P portrait of human growing colorful flowers from her hair. Hyperrealistic oil painting. Intricate details. An old car is sitting by the street in front of the sign 2026, in the style of futuristic, scifi elements, dark themes. Figure 9. Qualitative ablation of the approximation strategy. similarity against the exact block-wise first-order baseline. As shown in Table 4, our hybrid approximation significantly improves similarity over the zeroth-order method and approaches the exact baseline with superior efficiency. These results quantitatively demonstrate an optimal trade-off between generation fidelity and computational cost. Qualitative Validation. We visualized the effects on the FLUX.1-dev ( 85% sparsity) in Fig. 9. While the zerothorder approximation maintains semantics and structural integrity, it struggles with local details. In contrast, the hybrid approximation significantly recovers these fine-grained details. This provides intuitive visual evidence that our hybrid strategy effectively preserves human-perceptible details. 5. Conclusion In this work, we transcend the conventional keep-or-drop paradigm of sparse attention. We propose novel Piecewise Sparse Attention that, through unified exact-orapproximate execution, enables full attention span with sub-quadratic complexity, achieving an optimal trade-off between efficiency and accuracy. PISA: Piecewise Sparse Attention Is Wiser for Efficient Diffusion Transformers"
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Attention Mechanism. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Ainslie, J., Lee-Thorp, J., De Jong, M., Zemlyanskiy, Y., Lebron, F., and Sanghai, S. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023. Arnab, A., Dehghani, M., Heigold, G., Sun, C., Luˇcic, M., and Schmid, C. Vivit: video vision transformer. In ICCV, 2021. Han, D., Pan, X., Han, Y., Song, S., and Huang, G. Flatten transformer: Vision transformer using focused linear attention. In ICCV, 2023. Han, D., Wang, Z., Xia, Z., Han, Y., Pu, Y., Ge, C., Song, J., Song, S., Zheng, B., and Huang, G. Demystify mamba in vision: linear attention perspective. In NeurIPS, 2024. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. Gans trained by two time-scale update rule converge to local nash equilibrium. In NeurIPS, 2017. Hong, W., Ding, M., Zheng, W., Liu, X., and Tang, J. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. Arora, S., Eyuboglu, S., Zhang, M., Timalsina, A., Alberti, S., Zinsley, D., Zou, J., Rudra, A., and Re, C. Simple linear attention language models balance the recallthroughput tradeoff. arXiv preprint arXiv:2402.18668, 2024. Huang, Z., He, Y., Yu, J., Zhang, F., Si, C., Jiang, Y., Zhang, Y., Wu, T., Jin, Q., Chanpaisit, N., Wang, Y., Chen, X., Wang, L., Lin, D., Qiao, Y., and Liu, Z. VBench: Comprehensive benchmark suite for video generative models. In CVPR, 2024. Chen, J., Zhao, Y., Yu, J., Chu, R., Chen, J., Yang, S., Wang, X., Pan, Y., Zhou, D., Ling, H., et al. Sana-video: Efficient video generation with block linear diffusion transformer. arXiv preprint arXiv:2509.24695, 2025. Dao, T. FlashAttention-2: Faster attention with better parallelism and work partitioning. In ICLR, 2024. Dao, T., Fu, D. Y., Ermon, S., Rudra, A., and Re, C. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In NeurIPS, 2022. Dass, J., Wu, S., Shi, H., Li, C., Ye, Z., Wang, Z., and Lin, Y. Vitality: Unifying low-rank and sparse approximation for vision transformer acceleration with linear taylor attention. In HPCA, 2023. Du, J., Hu, J., Zhang, T., Sun, W., and Cheng, Y. Native hybrid attention for efficient sequence modeling. arXiv preprint arXiv:2510.07019, 2025. Esser, P., Kulal, S., Blattmann, A., Entezari, R., Muller, J., Saini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. Fan, Q., Huang, H., and He, R. Breaking the low-rank dilemma of linear attention. In CVPR, 2025. Gelada, C., Buckman, J., Zhang, S., and Bach, T. Scaling context requires rethinking attention. arXiv preprint arXiv:2507.04239, 2025. Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are rnns: Fast autoregressive transformers with linear attention. In ICML, 2020. Kong, W., Tian, Q., Zhang, Z., Min, R., Dai, Z., Zhou, J., Xiong, J., Li, X., Wu, B., Zhang, J., et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. Labs, B. F., Batifol, S., Blattmann, A., Boesel, F., Consul, S., Diagne, C., Dockhorn, T., English, J., English, Z., Esser, P., Kulal, S., Lacey, K., Levi, Y., Li, C., Lorenz, D., Muller, J., Podell, D., Rombach, R., Saini, H., Sauer, A., and Smith, L. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv preprint arXiv:2506.15742, 2025. Li, X., Li, M., Cai, T., Xi, H., Yang, S., Lin, Y., Zhang, L., Yang, S., Hu, J., Peng, K., et al. Radial attention: O(n log n) sparse attention with energy decay for long video generation. arXiv preprint arXiv:2506.19852, 2025. Lieber, O., Lenz, B., Bata, H., Cohen, G., Osin, J., Dalmedigos, I., Safahi, E., Meirom, S., Belinkov, Y., ShalevShwartz, S., et al. Jamba: hybrid transformer-mamba language model. arXiv preprint arXiv:2403.19887, 2024. Liu, A., Zhang, Z., Li, Z., Bai, X., Han, Y., Tang, J., Xing, Y., Wu, J., Yang, M., Chen, W., et al. Fpsattention: Trainingaware fp8 and sparsity co-design for fast video diffusion. arXiv preprint arXiv:2506.04648, 2025. 9 PISA: Piecewise Sparse Attention Is Wiser for Efficient Diffusion Transformers Liu, S., Yu, W., Tan, Z., and Wang, X. Linfusion: 1 gpu, 1 minute, 16k image. arXiv preprint arXiv:2409.02097, 2024. Lu, E., Jiang, Z., Liu, J., Du, Y., Jiang, T., Hong, C., Liu, S., He, W., Yuan, E., Wang, Y., et al. Moba: Mixture of block attention for long-context llms. arXiv preprint arXiv:2502.13189, 2025. Meng, W., Luo, Y., Li, X., Jiang, D., and Zhang, Z. Polaformer: Polarity-aware linear attention for vision transformers. In ICLR, 2025. Peebles, W. and Xie, S. Scalable diffusion models with transformers. In ICCV, 2023. Qin, Z., Han, X., Sun, W., Li, D., Kong, L., Barnes, N., and Zhong, Y. The devil in linear transformer. arXiv preprint arXiv:2210.10340, 2022. Shah, J., Bikshandi, G., Zhang, Y., Thakkar, V., Ramani, P., and Dao, T. Flashattention-3: Fast and accurate attention with asynchrony and low-precision. In NeurIPS, 2024. Shmilovich, D., Wu, T., Dahan, A., and Domb, Y. Liteattention: temporal sparse attention for diffusion transformers. arXiv preprint arXiv:2511.11062, 2025. Sun, W., Tu, R.-C., Ding, Y., Jin, Z., Liao, J., Liu, S., and Tao, D. Vorta: Efficient video diffusion via routing sparse attention. arXiv preprint arXiv:2505.18809, 2025. Sun, Y., Dong, L., Huang, S., Ma, S., Xia, Y., Xue, J., Wang, J., and Wei, F. Retentive network: successor to transformer for large language models. arXiv preprint arXiv:2307.08621, 2023. Tan, X., Chen, Y., Jiang, Y., Chen, X., Yan, K., Duan, N., Zhu, Y., Jiang, D., and Xu, H. Dsv: Exploiting dynamic sparsity to accelerate large-scale video dit training. arXiv preprint arXiv:2502.07590, 2025. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. In NeurIPS, 2017. Wan, T., Wang, A., Ai, B., Wen, B., Mao, C., Xie, C.-W., Chen, D., Yu, F., Zhao, H., Yang, J., Zeng, J., Wang, J., Zhang, J., Zhou, J., Wang, J., Chen, J., Zhu, K., Zhao, K., Yan, K., Huang, L., Feng, M., Zhang, N., Li, P., Wu, P., Chu, R., Feng, R., Zhang, S., Sun, S., Fang, T., Wang, T., Gui, T., Weng, T., Shen, T., Lin, W., Wang, W., Wang, W., Zhou, W., Wang, W., Shen, W., Yu, W., Shi, X., Huang, X., Xu, X., Kou, Y., Lv, Y., Li, Y., Liu, Y., Wang, Y., Zhang, Y., Huang, Y., Li, Y., Wu, Y., Liu, Y., Pan, Y., Zheng, Y., Hong, Y., Shi, Y., Feng, Y., Jiang, Z., Han, Z., Wu, Z.-F., and Liu, Z. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Wu, J., Hou, L., Yang, H., Tao, X., Tian, Y., Wan, P., Zhang, D., and Tong, Y. Vmoba: Mixture-of-block attention for video diffusion models. arXiv preprint arXiv:2506.23858, 2025. Wu, X., Hao, Y., Sun, K., Chen, Y., Zhu, F., Zhao, R., and Li, H. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. Xi, H., Yang, S., Zhao, Y., Xu, C., Li, M., Li, X., Lin, Y., Cai, H., Zhang, J., Li, D., et al. Sparse video-gen: Accelerating video diffusion transformers with spatialtemporal sparsity. In ICML, 2025. Xia, Y., Ling, S., Fu, F., Wang, Y., Li, H., Xiao, X., and Cui, B. Training-free and adaptive sparse attention for efficient long video generation. arXiv preprint arXiv:2502.21079, 2025. Xie, E., Chen, J., Chen, J., Cai, H., Tang, H., Lin, Y., Zhang, Z., Li, M., Zhu, L., Lu, Y., et al. Sana: Efficient highresolution image synthesis with linear diffusion transformers. arXiv preprint arXiv:2410.10629, 2024. Xu, J., Liu, X., Wu, Y., Tong, Y., Li, Q., Ding, M., Tang, J., and Dong, Y. Imagereward: learning and evaluating human preferences for text-to-image generation. In NeurIPS, 2023. Xu, R., Xiao, G., Huang, H., Guo, J., and Han, S. Xattention: Block sparse attention with antidiagonal scoring. In ICML, 2025. Yang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y. Gated linear attention transformers with hardware-efficient training. In ICML, 2024a. Yang, S., Wang, B., Zhang, Y., Shen, Y., and Kim, Y. Parallelizing linear transformers with the delta rule over sequence length. In NeurIPS, 2024b. Yang, S., Xi, H., Zhao, Y., Li, M., Zhang, J., Cai, H., Lin, Y., Li, X., Xu, C., Peng, K., et al. Sparse videogen2: Accelerate video generation with sparse attention via semanticaware permutation. arXiv preprint arXiv:2505.18875, 2025. Yuan, J., Gao, H., Dai, D., Luo, J., Zhao, L., Zhang, Z., Xie, Z., Wei, Y., Wang, L., Xiao, Z., et al. Native sparse attention: Hardware-aligned and natively trainable sparse attention. In ACL, 2025. Zhang, J., Wang, H., Jiang, K., Yang, S., Zheng, K., Xi, H., Wang, Z., Zhu, H., Zhao, M., Stoica, I., et al. Sla: Beyond sparsity in diffusion transformers via fine-tunable sparse-linear attention. arXiv preprint arXiv:2509.24006, 2025a. 10 PISA: Piecewise Sparse Attention Is Wiser for Efficient Diffusion Transformers Zhang, J., Xiang, C., Huang, H., Wei, J., Xi, H., Zhu, J., and Chen, J. Spargeattn: Accurate sparse attention accelerating any model inference. In ICML, 2025b. Zhang, M., Bhatia, K., Kumbong, H., and Re, C. The hedgehog & the porcupine: Expressive linear attentions with softmax mimicry. arXiv preprint arXiv:2402.04347, 2024a. Zhang, P., Chen, Y., Huang, H., Lin, W., Liu, Z., Stoica, I., Xing, E., and Zhang, H. Vsa: Faster video diffusion with trainable sparse attention. arXiv preprint arXiv:2505.13389, 2025c. Zhang, P., Chen, Y., Su, R., Ding, H., Stoica, I., Liu, Z., and Zhang, H. Fast video generation with sliding tile attention. arXiv preprint arXiv:2502.04507, 2025d. Zhang, R., Isola, P., Efros, A. A., Shechtman, E., and Wang, O. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. Zhang, S., Wang, B., Wu, J., Li, Y., Gao, T., Zhang, D., and Wang, Z. Learning multi-dimensional human preference for text-to-image generation. In CVPR, 2024b. Zhang, T., Bi, S., Hong, Y., Zhang, K., Luan, F., Yang, S., Sunkavalli, K., Freeman, W. T., and Tan, H. Test-time training done right. arXiv preprint arXiv:2505.23884, 2025e. Zhang, Y., Yang, S., Zhu, R., Zhang, Y., Cui, L., Wang, Y., Wang, B., Shi, F., Wang, B., Bi, W., Zhou, P., and Fu, G. Gated slot attention for efficient linear-time sequence modeling. In NeurIPS, 2024c. Zhang, Y., Xing, J., Xia, B., Liu, S., Peng, B., Tao, X., Wan, P., Lo, E., and Jia, J. Training-free efficient video generation via dynamic token carving. arXiv preprint arXiv:2505.16864, 2025f. 11 PISA: Piecewise Sparse Attention Is Wiser for Efficient Diffusion Transformers A. Expanded Related Work Sparse Attention for DiTs. Sparse attention reduces computational complexity by selectively computing only critical key-value pairs. In the field of video generation, static sparse attention (Xi et al., 2025; Zhang et al., 2025d) relies on attention masks derived from intrinsic spatio-temporal sparsity priors, whereas dynamic sparse attention (Yang et al., 2025; Xia et al., 2025; Zhang et al., 2025b; Tan et al., 2025; Liu et al., 2025; Sun et al., 2025; Zhang et al., 2025f) determines which key-values to prune during runtime. To maximize hardware utilization, sparse attention typically operates at block-level granularity where block of queries attends to shared set of key-value blocks. Although some methods in Large Language Models (LLMs) have achieved finer granularity by allowing token-wise queries to attend to block-wise key-values (Lu et al., 2025; Yuan et al., 2025), they utilize the inherent properties of causal attention. For instance, NSA (Yuan et al., 2025) leverages Grouped Query Attention (GQA) (Ainslie et al., 2023) to construct GEMM via the attention head dimension. Consequently, these techniques are difficult to implement in bidirectional attention. Bidirectional Linear Attention. Linear attention reduces computational complexity to linear scale by decomposing exp(qk) into ϕ(q)ϕ(k) and leveraging the associative property of matrix multiplication to compute the kv product first. Initially, to mimic the normalization scheme of softmax attention, the kernel function ϕ() was typically restricted to element-wise non-negative activation functions (e.g., ReLU or ELU() + 1) to ensure normalization validity. However, Recent works (Qin et al., 2022; Sun et al., 2023) demonstrated that the normalization denominator is unnecessary and can be replaced by post-normalization of the output. Consequently, the non-negativity constraint on the kernel function is no longer required, allowing for functions like SiLU, practice now widely adopted in recent LLMs with causal linear attention (Yang et al., 2024a; Zhang et al., 2024c; Yang et al., 2024b). Despite these advancements, existing variants of bidirectional linear attention continue to adhere to non-negative kernel functions to preserve the form of softmax normalization (Han et al., 2023; Liu et al., 2024; Xie et al., 2024; Meng et al., 2025; Han et al., 2024; Fan et al., 2025). Linear Attention with Taylor Expansion. In the field of LLMs, several studies (Arora et al., 2024; Gelada et al., 2025; Zhang et al., 2024a) utilize Taylor expansion to linearize softmax attention. Recent approaches, such as Based (Arora et al., 2024) and power attention (Zhang et al., 2024a), have adopted second-order Taylor expansion using the feature map ϕp(q)ϕp(k) = (qk)p (note that this is an exact equivalence) to efficiently compute second-order terms. However, ϕp significantly increases the head dimension of and k. For instance, with base head dimension of 64, Based requires an expansion from 64 4096. Although power attention propose symmetric powers map to mitigate this dimensional growth (e.g., reducing the expansion to 64 2080), the computational cost remains high due to its O(N d2) complexity. This overhead is considered tolerable in causal attention because inference is performed via token-wise decoding. Furthermore, since these methods expand the function around 0 for the entire sequence, they necessitate training from scratch. Hybrid Attention. Recent works (Lieber et al., 2024; Zhang et al., 2025e) have explored hybridizing softmax and linear attention either across or within layers. Common strategies involve computing the full sequence with both attention types independently and performing weighted sum of their outputs, or allocating specific attention heads to each mechanism followed by concatenation and linear projection. Recently, efforts have shifted toward hybridization at finer granularity. For instance, in LLMs, NHA (Du et al., 2025) combines linear attention with sliding window attention, where tokens lying outside the window are processed via the linear branch. Similarly, in the vision domain, SLA (Zhang et al., 2025a) integrates sparse and linear attention by dynamically selecting subset of key-value pairs for linear computation. However, these methods typically aggregate the two branches via direct summation. This approach inevitably destroys the inherent normalized property of softmax attention, thereby precluding the training-free integration of pre-trained weights. B. Compare with Other Methods In contrast to recent studies, our approach distinguishes itself from existing paradigms in three key aspects. First, unlike pure sparse attention (Xu et al., 2025; Xi et al., 2025; Yang et al., 2025; Zhang et al., 2025b; Shmilovich et al., 2025) which adheres to keep-or-drop paradigm that discards the majority of key-value pairs, our method adopts an exact-or-approximate paradigm, blending sparse exact computation with efficient approximation. Second, in contrast to hybrid attention mechanisms (Du et al., 2025; Zhang et al., 2025a) that typically perform naive summation of softmax and linear attention outputs, we propose fine-grained and mathematically rigorous mixing strategy. By employing block-wise Taylor expansion to approximate both the numerator and denominator, we unify the hybridization of softmax and linear attention within the canonical normalization framework of attention. 12 PISA: Piecewise Sparse Attention Is Wiser for Efficient Diffusion Transformers Third, compared to other Taylor-based methods (Arora et al., 2024; Gelada et al., 2025; Dass et al., 2023) that perform global expansion over the entire sequence, resulting in prohibitive approximation errors that prevent the reuse of pre-trained weights, our method performs expansion around the local mean of each block. This significantly reduces approximation error, enabling the direct, training-free inheritance of pre-trained weights using only first-order expansion. To the best of our knowledge, we are the first to natively integrate block-wise Taylor expansion and sparse attention into unified framework. C. Error Analysis of the Hybrid-Order Approximation Notation. Let the input length be L, block size B, number of blocks = L/B. For fixed query row qt R1d (with = iB + m), denote the block-wise key/value rows by {kj,n}n=1,...,B j=1,...,N and {vj,n}. Define the block centroid and the block-wise first-order matrix The global first-order statistic is kj :="
        },
        {
            "title": "1\nB",
            "content": "B (cid:88) n=1 kj,n, Hj := (cid:88) n= (kj,n kj)vj,n Rdd. := 1 (cid:88) j=1 Hj. For brevity define the block-centroid exponentials (including the scaling d) αt,j := exp (cid:16) qt j (cid:17) . Let Si be the set of selected (exact) blocks for query-block i, and Ui its complement (the unselected / tail blocks). Define the full attention denominator (cid:88) (cid:88) Dt := exp (cid:16) qtk j,n (cid:17) , and the tail (unselected) mass j=1 n= τt := (cid:88) (cid:88) jUi n=1 exp (cid:16) qtk j,n (cid:17) . Finally denote the operator (spectral) norm by 2 and the Euclidean norm for vectors also by 2. The core approximation step of our Hybrid-Order Approximation replaces the exact first-order contribution Define the residual matrix (cid:88) jUi αt,j Hj by (cid:16) (cid:88) (cid:17) H. αt,j jUi Rt := (cid:88) jUi αt,j(Hj H). Lemma C.1 (Residual operator norm bound). Let Then := max jUi Hj H2. Rt2 (cid:88) jUi αt,j. 13 PISA: Piecewise Sparse Attention Is Wiser for Efficient Diffusion Transformers Proof. By triangle inequality and submultiplicativity of the operator norm, Rt2 = (cid:13) (cid:13) (cid:13) (cid:88) jUi αt,j(Hj H) (cid:13) (cid:13) (cid:13)2 (cid:88) jUi αt,jHj H2 (cid:88) jUi αt,j, which proves the lemma. We restate Theorem 3.1 for completeness as follows: Theorem C.2 (Error Analysis of Global First-Order Approximation). Assume there exists constant Cq > 0, such that the query norm is bounded, i.e., qt2 Cq. Let ot be the (vector) attention output computed using the exact first-order term and let ot be the output computed after replacing (cid:80) Dt). Then jUi αt,j) (while keeping the same denominator jUi , ot = Nt Dt αt,jHj by ((cid:80) ot ot2 qt2 Rt2 Dt"
        },
        {
            "title": "Cq M\nDt",
            "content": "(cid:88) jUi αt,j. Moreover, by Jensens inequality for the convex exponential function (applied block-wise), and consequently αt,j τt , (cid:88) jUi ot ot2 Cq τt Dt . Equivalently, if we denote the tail fraction ρt := τt/Dt [0, 1], then ot ot2 Cq ρt . (12) (13) Proof. Write the exact numerator as Nt = (S) denotes the block-wise zeroth-order term (the grouped value sums multiplied by centroid exponentials), and is the selected (exact) contribution, (0) , where (S) + (0) + (1) t (1) = qt (cid:88) jUi αt,jHj is the exact first-order contribution from the unselected blocks. The approximated numerator replaces (1) by (1) = qt (cid:16) (cid:88) (cid:17) H. αt,j jUi Therefore the numerator error induced by this replacement is (cid:16) (cid:88) Nt := (1) (1) = qt jUi (cid:124) αt,j (cid:88) jUi αt,jHj (cid:123)(cid:122) Rt (cid:17) (cid:125) = qtRt. Since both outputs share the same denominator Dt by assumption, we have ot ot = Nt Dt = qtRt Dt . Taking Euclidean norm and using submultiplicativity of operator norm, ot ot2 qt2 Rt2 Dt . PISA: Piecewise Sparse Attention Is Wiser for Efficient Diffusion Transformers Applying Lemma C.1 yields the first inequality in (12). To obtain (13), note that for any block j, αt,j = exp (cid:16) qt j (cid:17) = exp (cid:16) 1 (cid:88) n=1 qtk j,n (cid:17)"
        },
        {
            "title": "1\nB",
            "content": "B (cid:88) n=1 exp (cid:16) qtk j,n (cid:17) , where the inequality follows from Jensens inequality since exp() is convex. Summing over Ui gives (13). Combining yields the stated bounds and completes the proof. Remarks. 1. The assumption that the query norm is bounded is mild and justifiable, as modern models predominantly employ QK-Norm (Query-Key Normalization) in their attention layers. 2. The bound in Theorem C.2 cleanly separates (i) structural heterogeneity term = maxj Hj H2, which measures how similar each blocks first-order contribution is to the global statistic, and (ii) tail mass factor ρt = τt/Dt, which measures how much attention weight remains in the unselected (approximated) blocks. The block size appears in the denominator because single block-centroid exponential αt,j is at most the average of the per-row exponentials (Jensen), hence (cid:80) αt,j τt/B. 3. Theorem C.2 bounds only the error caused by replacing the exact per-block matrices by their global mean. Additional error terms originate from the the truncation of the Taylor series. According to the Lagrange remainder form of Taylors theorem, this error is bounded by the second-order moment of the block deviations. Since PISA operates on unselected blocks where the attention distribution is sparse and flat (it is precisely this insight that motivates our work), this quadratic residual is negligible compared to the first-order term. Therefore, Theorem C.2 effectively captures the dominant error bound of our method. D. Covariance-Aware Block Selection Recalling the theoretical error bound in Theorem C.2 that error exp(qk) H2, our goal is to identify blocks with both large attention scores and high approximation errors. We simplify this objective by rectifying the attention score using Mj := Hj H. To eliminate the influence of the absolute magnitude of Mj, we define the block-wise routing factor as ) Mj/ , where is the mean of MjU . To integrate this into the softmax operation, we take its logarithm: exp(qt Scoret,j = Softmax (cid:32) qt d + log(Mj) log( ) . (cid:33) (14) Since is constant term, it cancels out during the Softmax normalization, thereby yielding Eq. (11). E. Implementation Details for Reproducibility We detail the specific configuration of the warmup strategy in Table 5 and Table 6. Beyond this, to ensure strictly fair comparison, all other parameters regarding sparse attention adhere to the official implementations. When integrating different attention methods into video and image generation models, we utilize the official recommended configurations for all remaining hyperparameters (e.g., sampling steps, classifier-free guidance (CFG) scale), without further enumeration. Table 5. Configuration of Video Generation Table 6. Configuration of Image Generation Ref. Config Table Table 2 Dense Layer Dense Step Dense Layer Dense Step Wan2.1 1.3B Wan2.1 14B Hunyuan 13B 1 15 0 0 1 10 0 0 1 10 0 Ref. Config Table 3 Table 4 Dense Layer Dense Step Dense Layer Dense Step SD 3.5 medium SD 3.5 turbo FLUX.1 schnell FLUX.1 dev 4 0 - - 4 0 - - 4 0 - - 4 0 4 F. More Generation Samples We provide additional image and video generation samples in Fig. 10 and Fig. 11. 15 PISA: Piecewise Sparse Attention Is Wiser for Efficient Diffusion Transformers Dense Attention PISA (Ours) SpargeAttn Dense Attention PISA (Ours) SpargeAttn Photorealistic high detail grizzly caught in forest fire with huge flames at night, ground perspective, shot on Nikon z6ii with 20mm at f1.4 aperture. Open bible on fire. Hyper realistic photo about two cute young kids that are lost in forest. boy and girl. There must be river. 4k resolution. In 3D and with depth perception. The decayed Berlin. Reichstag with destroyed dome. Everything is overgrown with plants and giant fungi. Year 2222. Rain. Dim atmosphere. Photo style, city buildings by the sea, business style, city overlooking, overall blue tone, gorgeous light and lighting, complex and gorgeous texture, calm feeling. Vertical screen. Drone photo of New York, New York center of long shot, soft color, night time, 16k. Troupial in the ray traced morning light. 4K, hyper realistic style, fantastical, otherworldly, brave. Cinematic photo of 7 year old red hair girl on top of crocodile on jungle lake. Narnia style. Wes Anderson color palette. Sunrise natural light. Close up of man and woman passionately kissing in the rain. Male friends hanging out in Seattle in 1994, smoking cigarettes, wearing grunge outfits, 90s, Style of Petra Collins, 35mm film, Canon EF 50mm, ProMist filter, low contrast, full body pose, wide angle, natural lighting, pastel coloring. comic book depicting an awesome black dragon attacking castle ,epic dynamic battle, in the style of dark red and light orange, Mike Deodato, Egyptian iconography, close up, dark blue and red, whiplash lines, afrofuturism. Medieval young woman raising hand volunteering as tribute, zoomed in, dark setting, by Syd Mead, Donato Giancola, Paul Lehr, Frazetta, frank miller, cinematic lighting, raking light. Figure 10. Text-to-Image generation samples on FLUX.1-dev. The SpargeAttn under the 80% sparsity while PISA under the 85% sparsity. PISA: Piecewise Sparse Attention Is Wiser for Efficient Diffusion Transformers An astronaut is riding horse in the space in photorealistic style. happy fuzzy panda playing guitar nearby campfire, snow mountain in the background. space shuttle launching into orbit, with flames and smoke billowing out from the engines. cute raccoon playing guitar in boat on the ocean. FULL PISA FULL PISA FULL PISA FULL PISA FULL PISA teddy bear is playing drum kit in NYC Times Square. Figure 11. Text-to-Video generation samples on Wan2.1-14B. The PISA under the 87.5% sparsity with 10 steps and 1 layer warmup."
        }
    ],
    "affiliations": [
        "The Hong Kong University of Science and Technology (Guangzhou)"
    ]
}