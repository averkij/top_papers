{
    "paper_title": "Revisiting Residual Connections: Orthogonal Updates for Stable and Efficient Deep Networks",
    "authors": [
        "Giyeong Oh",
        "Woohyun Cho",
        "Siyeol Kim",
        "Suhwan Choi",
        "Younjae Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Residual connections are pivotal for deep neural networks, enabling greater depth by mitigating vanishing gradients. However, in standard residual updates, the module's output is directly added to the input stream. This can lead to updates that predominantly reinforce or modulate the existing stream direction, potentially underutilizing the module's capacity for learning entirely novel features. In this work, we introduce Orthogonal Residual Update: we decompose the module's output relative to the input stream and add only the component orthogonal to this stream. This design aims to guide modules to contribute primarily new representational directions, fostering richer feature learning while promoting more efficient training. We demonstrate that our orthogonal update strategy improves generalization accuracy and training stability across diverse architectures (ResNetV2, Vision Transformers) and datasets (CIFARs, TinyImageNet, ImageNet-1k), achieving, for instance, a +4.3\\%p top-1 accuracy gain for ViT-B on ImageNet-1k."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 1 8 8 1 1 . 5 0 5 2 : r Revisiting Residual Connections: Orthogonal Updates for Stable and Efficient Deep Networks Giyeong Oh Woohyun Cho Siyeol Kim Suhwan Choi Younjae Yu Yonsei University {hard2251,k106419,cykim0528,yjy}@yonsei.ac.kr Maum.AI claude@maum.ai"
        },
        {
            "title": "Abstract",
            "content": "Residual connections are pivotal for deep neural networks, enabling greater depth by mitigating vanishing gradients. However, in standard residual updates, the modules output is directly added to the input stream. This can lead to updates that predominantly reinforce or modulate the existing stream direction, potentially underutilizing the modules capacity for learning entirely novel features. In this work, we introduce Orthogonal Residual Update: we decompose the modules output relative to the input stream and add only the component orthogonal to this stream. This design aims to guide modules to contribute primarily new representational directions, fostering richer feature learning while promoting more efficient training. We demonstrate that our orthogonal update strategy improves generalization accuracy and training stability across diverse architectures (ResNetV2, Vision Transformers) and datasets (CIFARs, TinyImageNet, ImageNet-1k), achieving, for instance, +4.3%p top-1 accuracy gain for ViT-B on ImageNet-1k."
        },
        {
            "title": "Introduction",
            "content": "Residual connections [1] have been cornerstone in deep learning, fundamentally enabling the training of substantially deeper neural networks by mitigating vanishing gradients. The original ResNet architecture updated an internal state xn via xn+1 = σact(xn + (xn)), where the nonlinear activation σact was applied after the summation, meaning xn did not propagate purely linearly. Subsequent work, notably ResNetV2 [2], introduced full identity mappings of the form xn+1 = xn + (σpre(xn)). This design, where σpre (e.g., normalization and activation) precedes the residual function and no transformation follows the addition, allows the unmodified xn to serve as linear residual stream [3] that passes representation directly across layers. This principle of linear residual stream is now prevalent feature in many modern high-capacity architectures, including contemporary Transformers [4, 5] and large language models [612] that predominantly employ pre-layer normalization [2, 13]. In such architectures, complex modules (e.g., attention layers or MLP blocks) operate on (or normalized version of this, σ(xn)) linear residual stream xn, and their output (σ(xn)) is additively combined with xn. Conceptually, this module output (σ(xn)) can be decomposed with respect to the input stream xn into two components: f, parallel to xn, and f, orthogonal to xn. The parallel component primarily acts to re-scale the existing representation through xn. This observation prompts consideration of how efficiently these high-capacity modules are utilized: if notable portion of their learned transformation merely modulates the magnitude of the existing stream, is their full representational power being optimally directed towards learning new, complex features? Corresponding author Preprint. Under review. Figure 1: Intuition behind our orthogonal residual update. Left: The standard residual update adds the full output of module (xn) to the input stream xn. Right: Our proposed update first decomposes the module output (xn) into component parallel to xn (f) and component orthogonal to xn (f). We then discard and add only the orthogonal component to the stream. It is plausible that such scaling operations might not always necessitate the full complexity of these high-capacity modules. The orthogonal component f, in contrast, inherently introduces new directional components into the representation space, distinct from the current stream xn. We hypothesize that by explicitly isolating and utilizing only the orthogonal component for updating the residual stream, modules are able to focus on contributing novel aspects to the representation. Motivated by this hypothesis, we propose and systematically investigate the Orthogonal Residual Update mechanism. As conceptually illustrated in Fig. 1, instead of linear additive update xn + (σ(xn)), our approach employs xn + f(xn), where f(xn) is the component of the modules output (σ(xn)) explicitly made orthogonal to the input stream xn (detailed in Section 3). We evaluate this approach on ResNetV2 [2] and the Vision Transformer (ViT) [5] across standard image classification benchmarks, including the CIFAR datasets [14], TinyImageNet [15], and ImageNet1k [16]. Our empirical results demonstrate that this orthogonal update strategy consistently improves generalization performance and training dynamics, as shown in Fig. 2. Our main contributions are: We re-examine additive updates in modern deep networks with linear residual streams. We highlight that the component of modules output parallel to the input stream (f) not only re-scales existing representations but, as our analysis suggests, can also become anti-aligned with the stream, potentially hindering efficient information propagation. We propose principled modification, the Orthogonal Residual Update. This mechanism isolates and utilizes only the component of modules output that is orthogonal to the input stream (f), specifically designed to encourage the injection of novel representational directions and mitigate the potential interference from the parallel component. We empirically demonstrate that our orthogonal update strategy consistently enhances generalization performance, training stability, and overall learning efficiency. These benefits are observed across diverse architectures (including Vision Transformers and ResNetV2) and standard image classification datasets."
        },
        {
            "title": "2 Related Works",
            "content": "Modifying the Stream Itself Beyond the standard identity skip connection [2], some research has explored modifying the skip connection path itself, Ixn, aiming to control signal propagation further or embed different inductive biases. Some studies proposed replacing the identity matrix with alternative fixed linear transformations or Γ, such as norm-preserving orthogonal matrices or structure-inducing entangled mappings [17, 18]. These approaches fundamentally alter the nature of the skip path (i.e., becomes or Γ), and investigations focus on how such changes to the streams direct transformation properties (e.g., eigenvalues, sparsity) impact learning. The efficacy of these direct stream modifications often varies with architectural and task specifics. This line of inquiry is distinct from our work, as we preserve the standard identity skip path and instead focus on refining the additive update term (σ(xn)) that is subsequently combined with this unaltered stream. Orthogonality in Deep Learning Orthogonality is recurring principle in deep learning, valued for promoting stable signal propagation and beneficial representation properties. Orthogonal weight initialization [19, 20], for instance, can accelerate convergence by enabling dynamical isometry 2 (a) Training loss vs. training iterations. (b) Validation accuracy vs. wall-clock runtime. Figure 2: Orthogonal update accelerates convergence and enhances generalization efficiency compared to the standard identity update baseline (ViT-B on ImageNet-1k results shown; EMA smoothed). (a) Faster Convergence: Orthogonal update (blue) achieves significantly lower training loss in fewer iterations. (b) Improved Time-to-Accuracy: Orthogonal update (blue) attains higher top-1 validation accuracy consistently outperforms identity update (red) after an initial phase ( 5.6 hours). early in training. Methods also exist to enforce orthogonality during training, either via explicit regularization terms [21, 22] or optimization constrained to orthogonal manifolds (e.g., Stiefel manifold) [23, 24], aiming for gradient stability and potentially encouraging feature decorrelation. More recently, Orthogonal Finetuning (OFT) adapts pre-trained models by learning orthogonal weight transformations to preserve learned relationships [25]."
        },
        {
            "title": "3 Orthogonal Residual Stream Updates",
            "content": "3.1 Preliminary Conceptually, for any non-zero vector xn Rd, module output (σ(xn)) can be uniquely expressed as the sum of two distinct components: one component that is directly related to the current stream xn, and another component that is independent of xn. We can write this abstractly as: (σ(xn)) = + f, (1) where is the component parallel to xn, and is the component orthogonal to xn. Since must lie along the direction of xn, it necessarily takes the form = αxn. This projection step, analogous to the fundamental operation in the Gram-Schmidt orthogonalization process, yields α as follows: = αxn, where α = xn, (σ(xn)) xn, xn and = (σ(xn)) f. (2) Here , denotes the dot product. Consequently, the orthogonal component f, capturing the part of (σ(xn)) linearly independent from xn, is obtained by subtracting this parallel component. By construction, this component satisfies xn, = 0. Interpreting these components, signifies the portion of the modules output that merely scales the direction already present in the stream xn. Conversely, indicates the novel representation contributed by the current module relative to xn. 3.2 Orthogonal-only Update Rule Building upon the decomposition introduced in Sec. 3.1, we now propose our core update mechanism. Instead of adding the full module output (σ(xn)) to the stream as in standard residual updates, we advocate for using only the component f(xn) derived from (σ(xn)), as illustrated in Fig. 1. This component is explicitly defined as: f(xn) = (σ(xn)) snxn, where sn = xn, (σ(xn)) xn2 + ϵ . (3) Note that we now explicitly include the small constant ϵ > 0 in the denominator for numerical stability during computation, detail omitted in the ideal decomposition in Sec. 3.1. Throughout our experiments, we use stability constant ϵ = 106. Please refer Sec. 4.6 for selection details. 3 Our proposed orthogonal-only update rule is thus simply: xn+1 = xn + f(xn). (4) This formulation aims to primarily inject new representational aspects (orthogonal to xn) from the module into the stream at each step. While the component of (σ(xn)) parallel to xn is explicitly subtracted in forming f(xn), its influence on the learning dynamics and the stream itself is not entirely eliminated, as we discuss in the following subsections. 3.3 Preservation of the Identity Gradient Path Eq. 4 is its compatibility with the fundamental mechanism of residual networks [1]. The derivative of the next stream xn+1 with respect to the current stream xn is: xn+1 xn = (1 sn)I xn(xnsn)T + = + (cid:20) (σ(xn)) xn (snxn) xn (cid:21) (σ(xn)) xn = + f(xn) xn . (5) (6) The crucial identity path I, which allows gradients to flow unimpeded across layers, is thus fundamentally preserved. Algorithm 1 Feature-wise Orthogonal Update Input: Stream x, module output , feature index idx_f, stability ϵ > 0. 1: sd sum(x , dim=idx_f,keepdim=True) 2: sn sum(x x, dim=idx_f,keepdim=True) 3: α sd/(sn + ϵ) 4: α 5: return + Algorithm 2 Global Orthogonal Update Input: Stream x, module output , stability ϵ > 0. 1: xvec x.flatten(start_dim=1) 2: fvec .flatten(start_dim=1) 3: sd sum(xvec fvec, dim=-1,keepdim=True) 4: sn sum(xvec xvec, dim=-1,keepdim=True) Shape (N, 1) 5: α sd/(sn + ϵ) 6: α EnsureBroadcastable(α, x) 7: α 8: return + 3.4 Implementation and Computational Overhead The specific computation of sn and the subsequent projection can be adapted based on tensor dimensionality, leading to two main strategies: Feature-wise Orthogonal Update (Alg. 1) and Global Orthogonal Update (Alg. 2). The Feature-wise approach applies orthogonalization independently along chosen feature dimension (e.g., per token along the hidden dimension in Transformers, or per spatial location across channels in CNNs). The Global approach flattens all non-batch dimensions to form single vector per sample for projection, offering holistic update. PyTorch implementations are in Appendix F. Tab. 1 provides an approximate breakdown of Floating Point Table 1: Approximate FLOPs comparison for Transformer block components. Here, = nseq (sequence length) and = dmodel (model hidden dimension). The MLP (FFN) assumes an intermediate hidden dimension expansion factor of 4. Our feature-wise orthogonal connection introduce only O(sd) FLOPs, which substantially smaller than modules complexity. Module Connection Total FLOPs Attention Linear Orthogonal 8sd2 + 4s2d + sd + 6sd + 2s 8sd2 + 4s2d + sd MLP (FFN) Linear Orthogonal 16sd2 + sd + 6sd + 2s 16sd2 + sd Operations (FLOPs) for typical Transformer blocks, with either linear connection or feature-wise orthogonal connection. The primary computational load in such blocks stems from the main module, such as self-attention ( 8sd2 + 4s2d) or the MLP/Feed-Forward Network (FFN) layers ( 16sd2, assuming an intermediate dimension of 4d). In contrast, standard identity connection and orthogonal connection add minimal FLOPs (6sd for calculating orthogonal part, 2s for division). This overhead, being of order O(sd), is substantially smaller than the main modules complexity. Consequently, the 4 (a) Stream Norm for MLP Blocks. (b) Cosine Similarity for MLP Blocks. (c) Stream Norm for Attention Block. (d) Cosine Similarity for Attention Blocks. Figure 3: Evolution of internal dynamics for MLP (top row, a-b) and Attention (bottom row, c-d) blocks in ViT-S during training on Tiny ImageNet. Each subfigure (a-d) contains metrics for blocks 0 through 5. Blue lines denote the Orthogonal Residual Update; red lines denote the linear residual update. (a, c) Squared L2 norm of the input stream xn. Our method maintains more stable stream norm. (b, d) Cosine similarity between xn and entire module output (σ(xn)). The dashed black line at = 0 indicates ideal orthogonality of (σ(xn)) to xn. Note the characteristic Transition Point as the point around which the metric trends for linear and orthogonal updates begin to diverge. observed benefits in convergence and generalization are not achieved at prohibitive computational cost. 3.5 Observed Internal Dynamics of Orthogonal Updates To empirically investigate the internal dynamics of our orthogonal update rule (Eq. 4), we trained ViT-S models on TinyImageNet (5-run avg.), tracking key metrics including the stream L2 norm (xn2) and cosine similarity (cos(xn, (σ(xn)))). These are presented in Fig. 3.A detailed analysis of module output component norms, f2 and f2, is in Appendix D. We observe that linear residual updates and our orthogonal updates exhibit diverging metric trajectories from an early, layer-dependent training stage, around juncture we term the Transition Point. The Transition Point Around this Transition Point, behaviors diverge significantly. For linear updates (red lines), xn2 (Fig. 3a, 3c) typically peaks and then decreases, while cos(xn, (σ(xn))) (Fig. 3b, 3d) consistently falls, often becoming negative. Conversely, with orthogonal updates (blue lines), xn2 stabilizes after an initial inflection, and cos(xn, (σ(xn))) first dips then trends positive. Stream Norm Behavior and Update Characteristics. The notable norm stability (xn const post-Transition Point) under orthogonal updates suggests xn evolves primarily on hypersphere. Unlike linear updates where module directly scales xn, our orthogonal update xn + f(xn) tends to rotate xn once its norm stabilizes, particularly when f(xn) xn. While f(xn) might initially adjust the norm, its subsequent rotational effect preserves magnitude while guiding xn towards new feature directions, contributing to stability. Cosine Similarity Dynamics. The post-Transition Point increase in cos(xn, (σ(xn))) for orthogonal updates, implying learns to align with xn, may seem counterintuitive but is explainable. While the update mechanism uses f(xn), the module itself is not explicitly regularized to produce outputs orthogonal to xn. Instead, is trained based on its full output (σ(xn)). Its effective Jacobian 5 Table 2: Mean std. of top1 accuracy (%) from 5 independent runs. Results are averaged over the 5 best validation epochs from each run. Performance of standard Linear updates is compared against our orthogonal updates: Orthogonal-F (Feature-wise) and Orthogonal-G (Global). Due to computational constraints, ImageNet-1k experiments focused on ViTs. denotes that the metric is averaged over 3 runs (instead of 5). The larger standard deviation reflects the smaller sample size. Architecture Connection Dataset (top-1 acc. % mean std.) CIFAR-10 CIFAR-100 TinyImageNet ViT-S ViT-B ResNetV2-18 ResNetV2-34 ResNetV2-50 ResNetV2Linear 89.820.34 Orthogonal-F 90.610.21 71.920.24 73.860.31 Linear 87.280.41 Orthogonal-F 88.736.06 68.250.88 75.070.43 Linear 95.060.15 Orthogonal-F 95.260.12 Orthogonal-G 95.250.11 Linear 95.490.09 Orthogonal-F 95.750.13 Orthogonal-G 95.530. 94.750.09 Linear Orthogonal-F 94.710.11 Orthogonal-G 94.750.10 94.860.05 Identity Orthogonal-F 94.800.13 Orthogonal-G 94.750.13 77.670.28 77.870.27 77.530.19 78.920.31 78.970.04 78.710.24 77.900.24 77.430.10 77.560.34 77.720.33 78.500.26 78.370. 51.300.40 52.570.71 55.290.71 57.870.37 62.040.29 62.650.14 62.320.22 64.610.24 65.460.30 65.380.35 63.740.18 64.220.28 64.400.36 63.770.52 65.780.22 65.870. ImageNet-1k 67.422.12 70.170.25 71.090.32 75.450.12 with respect to xn (see Appendix B.1 for derivation details) can be expressed as: (σ(xn)) xn = f(xn) xn + (snxn) xn . (7) Thus, is also influenced by gradients related to the parallel component snxn. If the learned parallel part of the modules output, f(σ(xn)), strengthens relative to its orthogonal part f(σ(xn)) (further discussed in Appendix B.1), the overall cos(xn, (σ(xn))) can increase. The small ϵ in sn also permits minor parallel influences on the update and learning."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setting Datasets. We evaluate our orthogonal update mechanism against standard identity connections [2] by training models from scratch on several image classification benchmarks: CIFAR-10, CIFAR100 [14] (both 32 32, 10/100 classes, 50k train/10k val. images), Tiny ImageNet [15] (64 64, 200 classes, 100k train/10k val.), and ImageNet-1k [16] (224 224, 1000 classes, 1.28M train/50k val.). Standard data augmentation strategies appropriate for each dataset and architecture were employed (full details in Appendix A.1). Architectures. Our experiments utilize two primary architecture families: ResNetV2 [2] (employing standard configurations like -18, -34, -50, -101) and Vision Transformers (ViT) [5] (ViT-S: 384 hidden dim, 6 layers, 6 heads; ViT-B: 768 hidden dim, 12 layers, 12 heads). For ViTs, input images are processed into patch embeddings with [CLS] token and 1D positional embeddings, and the [CLS] tokens output representation is used for classification. Due to computational constraints, our ImageNet-1k evaluations focused on ViT models. All models use final linear classification layer. Training Setup. Training protocols were established based on common practices for each architecture family. ResNetV2 models were trained for 200 epochs using SGD [26] (momentum 0.9, weight decay 5 104, initial LR 0.1 decayed at epochs 80, 120, batch size 128, gradient clipping at norm 1.0). ViTs were trained for 300 epochs using AdamW [27] (β1 = 0.9, β2 = 0.999, weight decay 1 104; batch sizes 1024 for smaller datasets, 4096 for ImageNet-1k). ViT [5] learning rates (1e-3 6 or 5e-4 based on dataset) followed cosine decay schedule [28] with 10-epoch linear warmup, and included label smoothing [29] (ε = 0.1). No gradient clipping was applied to ViTs. Comprehensive dataset-specific settings (e.g., patch sizes for ViT, precise learning rates, all augmentation details) and training times are provided in Appendix A.2. Table 3: Mean std. of top-1 (Acc@1) and top-5 (Acc@5) accuracy (%) from 5 independent runs for ViT-S, evaluating adaptability to connection type changes. Models are trained for 300 epochs (Start Arch.) then another 300 epochs (End Arch.) on the same dataset, with connections (Linear or Orthogonal O) potentially switched. Compared are LL, LO, OL, and OO on CIFAR-10, CIFAR-100, and Tiny ImageNet. Results are averaged over the final epochs of the End Arch. phase. Dataset Start Arch. End Arch. Acc@1 (%) Acc@5 (%) CIFAR-10 CIFAR-100 TinyImageNet Linear Orthogonal Linear Linear Orthogonal Linear Orthogonal Orthogonal Linear Orthogonal Linear Linear Orthogonal Linear Orthogonal Orthogonal Linear Orthogonal Linear Linear Orthogonal Linear Orthogonal Orthogonal 92.780.06 92.880.14 93.890.12 94.100.12 74.220.13 74.020.24 75.630.17 75.380.35 53.240.13 52.140.18 54.580.10 53.880.29 99.740.03 99.720.03 99.750.04 99.730.04 92.260.13 91.960.17 92.910.17 92.200.13 75.250.21 74.200.20 76.450.24 75.340. 4.2 Image Classification We evaluated our orthogonal update mechanism by training various ResNetV2 and Vision Transformer (ViT) architectures from scratch on standard image classification datasets using consistent augmentation strategies per architecture family (see Appendix for full settings). Due to computational constraints, our ImageNet-1k experiments focused on ViT models. All performance metrics are presented in Tab. 2. Orthogonal updates demonstrated particularly strong benefits in ViT models. For instance, ViT-B with our method achieved notable +4.3%p increase in top-1 accuracy on ImageNet-1k over the baseline. In contrast, while generally positive, the performance gains from orthogonal updates on ResNetV2 architectures appeared more modest relative to those in ViTs. We posit that this difference in efficacy may, in part, be related to the \"dimensionality-to-depth\" characteristics of these architecture families. To explore this, we consider ratio γ quantifying the average representational width per sequential processing block. For ViT, γViT = dmodel/Lblocks, where dmodel is the hidden dimension and Lblocks is the number of transformer blocks. For ResNet, we define an analogous γResNet = Davg/Btotal, where Davg = ((cid:80) kstages BkCk)/Btotal is the average channel dimensionality block operates on (Bk, Ck are blocks and channels per stage k, Btotal = (cid:80) Bk). For models of comparable size, ViT-S (22.0M params) has γViT-S = 384/6 64, whereas ResNetV2-34 (21.8M params) yields γResNetV2-34 ((cid:80) BkCk)/(B2 total) = (3776)/(162) 14.75. (Detailed calculations for γ across variants are in Appendix E). This markedly lower γ for ResNetV2-34 suggests its blocks operate within more \"compact\" representational space. We hypothesize that this architectural compactness may result in feature stream with inherently less directional redundancy for standard updates to exploit. The precise nature of this interplay warrants further dedicated investigation. 4.3 Ablation on Adapting Connection Types To evaluate the adaptability of representations when altering residual connection types, we conducted experiments with ViT-S on CIFAR-10, CIFAR-100, and Tiny ImageNet. Models were first trained for an initial 300 epochs using either standard linear residual update (L) or our Orthogonal Feature-wise update (O) connections (termed Start Arch.). Subsequently, after re-initializing the optimizer and learning rate scheduler, training continued on the same dataset for an additional 300 7 epochs (End Arch.), where the connection type was either maintained or switched. This allowed comparison of four configurations: LL (Identity throughout), LO (Identity then Orthogonal), OL (Orthogonal then Identity), and OO (Orthogonal throughout), with results in Tab. 3. The results in Tab. 3 highlight several key findings. Consistently across all datasets, employing Orthogonal connections throughout both training phases (OO) significantly outperformed using only Identity connections (LL), yielded remarkably strong results. This OL approach not only substantially surpassed the LL baseline but also often matched or even exceeded the OO performance. Conversely, transitioning from Identity to Orthogonal connections (LO) did not offer consistent benefits over the LL baseline. These findings suggest that orthogonal updates are particularly effective for fostering robust and generalizable representations during the initial training phase. The strong performance of the OL configuration indicates that such well-structured representations can be efficiently adapted and refined by simpler identity connections in subsequent phase. Encouraged by these results, we also conducted experiments, where the optimizer and learning rate scheduler were maintained without re-initialization, and training simply continued after switching the architecture at the 150-epoch mark. For precise results, please refer Appendix C.3. 4.4 Orthogonal Connection Probability To investigate the impact of stochastic orthogonal connections, we conducted an ablation study on ViT-S using TinyImageNet (N=3 runs). In this setup, each residual connection employs an orthogonal update with probability π, otherwise defaulting to the standard identity connection. We varied π from 0.0 (all identity) to 1.0 (all orthogonal). The effect of varying π is visualized in Fig. 4. clear and statistically positive trend is evident for both Top-1 and Top-5 accuracy (statistical p-value < 0.05, based on Pearson correlation shown in Fig. 4). This strongly suggests that consistent application of orthogonal updates (i.e., higher π) is beneficial for ViT-S performance on this task. Figure 4: Effect of orthogonal update probability on ViT-S performance on TinyImageNet (N=3). Plots show Top-1 (left) and Top-5 (right) accuracy (%) versus π. Error bars represent 1 standard deviation. The Pearson correlation coefficient (R) and its p-value between π and accuracy are displayed in each subplot, indicating positive correlation. 4.5 Orthogonal Layer Pattern Ablation This ablation study investigates the impact of the placement and number of orthogonal connections within ViT-S model on the TinyImageNet dataset. We compare various patterns of applying the orthogonal update to specific layer indices (or groups thereof) against baseline model (None) with no orthogonal updates and model where all layers employ orthogonal updates (All (0-5)). Tab. 4 shows that applying orthogonal connections to greater number of layers tends to yield improved performance. 4.6 Stability Constant ϵ We examine the sensitivity of our method to the stability constant ϵ, introduced in Eq. 3, which prevents division by zero during the orthogonal projection. We varied ϵ across several orders of magnitude, ϵ {108, 107, 106, 105, 104, 103}, using ViT-S on TinyImageNet over five runs. 8 Table 4: Ablation study on ViT-S layer patterns for orthogonal updates on TinyImageNet (N=3 runs). Layer indices indicate where orthogonal connections were applied. \"None\" is the identity update. \"All\" applies them to all 6 layers. Metrics are Top-1 and Top-5 accuracy (%) with mean std. Metric (%) Applied Orthogonal Connection Layer Indices None 0,1 2,3 4,5 0,1,2,3 0,1,4, 2,3,4,5 All (0-5) Top-1 Acc. Top-5 Acc. 51.880.42 51.450.16 51.320.63 52.200.27 51.550.30 52.440.38 52.140.15 52.980.45 75.200.44 75.010.23 74.790.29 75.620.41 75.050.25 75.190.09 75.170.14 75.930.58 Fig. 5 visualizes the best validation Top-1 accuracy and validation loss achieved for each ϵ value. We observed that the ϵ = 106 setting yielded results with the lowest standard deviation across the runs indicating greater stability and reproducibility. Prioritizing this stability, we adopted ϵ = 106 for the default constant throughout this paper, noting that its average performance remains competitive. Figure 5: Effect of stability constant ϵ on ViT-S best validation performance. The x-axis represents ϵ on logarithmic scale. Error bars indicate 1 standard deviation across runs."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we revisited additive residual connections, highlighting that standard updates can yield module outputs that not only cause representational redundancy but may also become antialigned with the input stream, potentially hindering efficient information propagation. To address this, we introduced the Orthogonal Residual Update, straightforward mechanism that isolates and utilizes only the component of modules output orthogonal to the stream, thereby encouraging the injection of novel information. Our extensive experiments on image classification benchmarks, across diverse architectures (including Vision Transformers and ResNetV2), demonstrated that this strategy consistently improves generalization performance, training stability, and overall learning efficiency. The Orthogonal Residual Update thus offers promising, computationally efficient means to enhance deep neural networks, inviting further research into its broader applications and theoretical foundations. Limitations While this work demonstrates clear benefits of our Orthogonal Residual Update, we acknowledge certain boundaries to the current study, primarily shaped by computational resource constraints. These constraints necessarily focused our empirical validation on models up to the scale of Vision Transformer (ViT-B) and datasets such as ImageNet-1k. This also restricted more extensive evaluations on, for example, deeper ResNetV2 variants on ImageNet-1k or exhaustive hyperparameter optimization across all presented configurations. Consequently, scaling our method to significantly larger models (e.g., contemporary large language models) or web-scale datasets remains an important open avenue. Furthermore, our methods application has centered on image classification. Its systematic exploration and potential adaptations to other domainssuch as generative modeling (e.g., with Diffusion Models) or broader sequence processing tasks in large language modelsrepresent important areas for future validation. Future Works This research opens several exciting avenues for future inquiry. One key direction is deeper analysis of how orthogonal updates interact with different network modules (e.g., attention vs. MLP layers) to understand if tailored strategies could yield further gains. Building on our method, which relates to the initial step of Gram-Schmidt process, exploring higher-order or more comprehensive orthogonalization techniques presents another promising path. We also hypothesize that architectural propertiesparticularly the ratio of hidden dimensionality to layer count (γ), which 9 appeared to correlate with the varying efficacy of our method observed in ViTs versus ResNetV2s in this studysignificantly influence the Orthogonal Residual Updates effectiveness. Investigating this hypothesis further, perhaps via wider range of standard and non-standard architectures, could offer valuable insights."
        },
        {
            "title": "Acknowledgement",
            "content": "This work was partly supported by an IITP grant funded by the Korean Government (MSIT) (No. RS-2020-II201361, Artificial Intelligence Graduate School Program (Yonsei University))"
        },
        {
            "title": "References",
            "content": "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. 1, 4, 24 [2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pages 630645. Springer, 2016. 1, 2, 6, 17 [3] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. mathematical framework for transformer circuits. Transformer Circuits Thread, 2021. https://transformer-circuits.pub/2021/framework/index.html. 1 [4] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 1 [5] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 1, 2, 6, 24 [6] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 1, [7] Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. [8] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [9] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b, 2023. URL https://arxiv.org/abs/2310.06825. [10] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. [11] Sungjun Han, Juyoung Suk, Suyeong An, Hyungguk Kim, Kyuseok Kim, Wonsuk Yang, Seungtaek Choi, and Jamin Shin. Trillion 7b technical report. arXiv preprint arXiv:2504.15431, 2025. [12] LG Research, Soyoung An, Kyunghoon Bae, Eunbi Choi, Kibong Choi, Stanley Jungkyu Choi, Seokhee Hong, Junwon Hwang, Hyojin Jeon, Gerrard Jeongwon Jo, et al. Exaone 3.5: Series of large language models for real-world use cases. arXiv preprint arXiv:2412.04862, 2024. 1 [13] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture. In International conference on machine learning, pages 1052410533. PMLR, 2020. 1 [14] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 2, 6 [15] Yann Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015. 2, [16] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211252, 2015. doi: 10.1007/s11263-015-0816-y. 2, 6 [17] Jingdong Wang, Yajie Xing, Kexin Zhang, and Cha Zhang. Orthogonal and idempotent transformations for learning deep neural networks. arXiv preprint arXiv:1707.05974, 2017. 2 [18] Mathias Lechner, Ramin Hasani, Zahra Babaiee, Radu Grosu, Daniela Rus, Thomas Henzinger, and Sepp Hochreiter. Entangled residual mappings. arXiv preprint arXiv:2206.01261, 2022. 2 [19] Saxe, McClelland, and Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. In Proceedings of the International Conference on Learning Represenatations 2014. International Conference on Learning Represenatations 2014, 2014. 2, 17 [20] Jeffrey Pennington, Samuel Schoenholz, and Surya Ganguli. Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice. Advances in neural information processing systems, 30, 2017. 2 [21] Lei Huang, Li Liu, Fan Zhu, Diwen Wan, Zehuan Yuan, Bo Li, and Ling Shao. Controllable orthogonalization in training dnns. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 64296438, 2020. [22] Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Parseval networks: Improving robustness to adversarial examples. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 854863. PMLR, 0611 Aug 2017. URL https://proceedings.mlr.press/v70/cisse17a.html. 3 [23] P-A Absil, Robert Mahony, and Rodolphe Sepulchre. Optimization algorithms on matrix manifolds. In Optimization Algorithms on Matrix Manifolds. Princeton University Press, 2009. 3 [24] Jun Li, Fuxin Li, and Sinisa Todorovic. Efficient riemannian optimization on the stiefel manifold via the cayley transform. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=HJxV-ANKDH. 3 [25] Zeju Qiu, Weiyang Liu, Haiwen Feng, Yuxuan Xue, Yao Feng, Zhen Liu, Dan Zhang, Adrian Weller, and Bernhard Schölkopf. Controlling text-to-image diffusion by orthogonal finetuning. Advances in Neural Information Processing Systems, 36:7932079362, 2023. 3 [26] Sebastian Ruder. An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747, 2016. [27] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum? id=Bkg6RiCqY7. 6 11 [28] Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. In International Conference on Learning Representations, 2017. URL https://openreview. net/forum?id=Skq89Scxx. 7 [29] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 28182826, 2016. 7 [30] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Yee Whye Teh and Mike Titterington, editors, Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, volume 9 of Proceedings of Machine Learning Research, pages 249256, Chia Laguna Resort, Sardinia, Italy, 1315 May 2010. PMLR. URL https://proceedings.mlr.press/v9/glorot10a.html. 17 [31] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In 2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015, pages 10261034. IEEE Computer Society, 2015. doi: 10.1109/ICCV.2015.123. URL https: //doi.org/10.1109/ICCV.2015.123. [32] Maciej Skorski, Alessandro Temperoni, and Martin Theobald. Revisiting weight initialization of deep neural networks. In Vineeth N. Balasubramanian and Ivor Tsang, editors, Proceedings of The 13th Asian Conference on Machine Learning, volume 157 of Proceedings of Machine Learning Research, pages 11921207. PMLR, 1719 Nov 2021. URL https://proceedings. mlr.press/v157/skorski21a.html. 17 [33] Soufiane Hayou, Arnaud Doucet, and Judith Rousseau. On the impact of the activation function on deep neural networks training. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 26722680. PMLR, 2019. URL http://proceedings.mlr.press/v97/hayou19a. html. [34] Pieter-Jan Hoedt and Günter Klambauer. Principled weight initialisation for input-convex neural networks. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/ 2023/hash/9062b7d6e522dadf4f7d85d49b60d81e-Abstract-Conference.html. 17 [35] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey Hinton. Layer normalization. Advances in Neural Information Processing Systems, 2016. URL https://openreview.net/forum?id= BJLa_ZC9. 19 [36] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [37] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. 24 [38] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023."
        },
        {
            "title": "A Hyperparameters and Hardware",
            "content": "This section details the training details and data augmentation settings used for the Vision Transformer (ViT-S) and ResNetV2 experiments presented in the main paper. Specific configurations for each model across the different datasets are provided in Tab. 5 (ViT-S) and Tab. 6 (ResNetV2). A.1 Architecture Specific Hyperparameters Table 5: Full training hyper-parameters of ViT-S and data augmentations (mean/std. results in Tab. 2 are based on five independent runs with these settings). Hyperparameter CIFAR-10 CIFAR-100 Tiny-ImageNet ImageNet-1k Epochs Batch size Base LR Optimizer LR scheduler Resolution (px) Random crop Patch size Color jitter Horizontal flip MixUp α / prob CutMix α Label smoothing Random erase RandAug (N, M) Normalization 300 1024 1103 300 1024 1103 300 4096 5104 AdamW, (β1, β2) = (0.9, 0.999), weight-decay 1 104 Cosine, 10-epoch linear warm-up 300 1024 510 32 pad 4 44 32 pad 4 4 64 scale (0.8, 1.0) ratio (0.75, 1.33) 88 224 scale (0.08, 1.0) ratio (0.75, 1.33) 1616 brightness/contrast/saturation = 0.4, hue = 0.1 = 0.5 α = 0.8, prob = 1.0 α = 1.0 ε = 0.1 = 0.25, scale (0.02, 0.33), ratio (0.3, 3.3) (9, 5) (9, 5) (9, 5) (9, 9) dataset-specific mean/std Table 6: Full training hyper-parameters and data augmentations for ResNetV2 models (e.g., ResNetV2-18, -34, -50, -101) on CIFAR and Tiny-ImageNet datasets. Mean/std. results in Tab. 2 are based on five independent runs with these settings. Hyperparameter CIFAR-10 CIFAR-100 Tiny-ImageNet Epochs Batch size Base LR Optimizer Weight decay LR scheduler Warm-up epochs Resolution (px) Random crop Horizontal flip Color jitter MixUp α / prob CutMix α Label smoothing Random erase RandAug (N, M) Gradient Clipping Normalization 200 128 1 101 SGD, momentum 0.9 5 104 MultiStep, decay by 0.2 at epochs 80, 120 0 (Not used) 32 32 Pad 4px, random 3232 crop 64 Random 6464 crop, scale (0.08-1.0), ratio (0.75-1.33) = 0.5 (Not used) (Not used) (Not used) (Not used) (Not used) (Not used) (Not used) Dataset-specific mean/std A.2 Computational Resources and Training Times The experiments were conducted using variety of NVIDIA GPUs. The following provides an overview of the typical hardware configurations and approximate training times for key models and 13 datasets. Actual training times could vary based on the specific GPU models available and system load during experimentation. ImageNet-1k Training (Vision Transformers): ViT-B: Trained on system with 8x NVIDIA L40S GPUs for approximately 42 hours. ViT-S: Trained on system with 8x NVIDIA L40S GPUs for approximately 15 hours. CIFAR-10/100 and Tiny ImageNet Training (Vision Transformers): ViT-B: * CIFAR-10/100: Typically trained on 2x NVIDIA H100 GPUs. Approximate training time was 1 hours per run. * Tiny ImageNet: Typically trained on 2x NVIDIA H100 GPUs. Approximate training time was roughly double that of the CIFAR experiments, around 2 hours per run. ViT-S: * CIFAR-10/100: Typically trained on 8x NVIDIA RTX 3090 GPUs for approximately 1 hour per run. * Tiny ImageNet: Typically trained on 8x NVIDIA RTX 3090 GPUs for approximately 2 hours per run. CIFAR-10/100 and Tiny ImageNet Training (ResNetV2 Models): All ResNetV2 models (ResNetV2-18, -34, -50, -101) were trained on systems equipped with 4x NVIDIA RTX 3090 GPUs. As representative example, full training run for ResNetV2-34 on CIFAR-10 took approximately 18 hours. Training times for other ResNetV2 model variants on Tiny ImageNet, and for all ResNetV2 variants on the CIFAR-10/100 datasets, scaled accordingly with model depth and dataset size. All training times are approximate and reflect the end-to-end duration for the specified number of epochs."
        },
        {
            "title": "B Formal Derivation and Theoretical Guarantees",
            "content": "B.1 Analysis of the Derivative Term (snxn) xn (snxn) xn = + snI (cid:124)(cid:123)(cid:122)(cid:125) Isotropic scaling; affects all components, incl. parallel to xn xnf (cid:124) (cid:123)(cid:122) (cid:125) Output along xn; magnitude from v; propagates and effects + xn(J xn) (cid:124) (cid:125) (cid:123)(cid:122) Output along xn; via Jacobian Jf interaction 2xn, b2 xnx (cid:125) (cid:124) (cid:123)(cid:122) Rank-1 update along xn; modulates xn-parallel strength , (8) 2 + ϵ. where = xn2 Deconstructing these terms reveals how information related to the xn-parallel component of (σ(xn)) (denoted f) and its xn-orthogonal component (f true , to distinguish from the update f(xn)) can propagate through the gradients: The first term, snI, isotropically scales all components of an input perturbation, including any component parallel to xn. Since sn = xn, /b, its magnitude is directly influenced by the alignment of with xn. The second term, xnf , always maps an input perturbation to an output in the direction of xn. , this term becomes xn(f+f true + ). The first part explicitly carries the influence via rank-1 update in the xn contributes If we decompose = + true 1 xn(f true direction. The second part shows how even the true orthogonal component true to gradient term that effectively leaks into the xn direction. = xn,f /xn2 xnx ) xn(J xn) The third term, , similarly produces outputs only along xn, with its magnitude depending on complex interactions involving the modules Jacobian Jf . This term can mix influences from both parallel and orthogonal components of as captured by Jf . The fourth term, 2xn,f , is rank-1 update that directly adjusts components in the xn direction, scaled by the alignment xn, . It often acts as form of negative feedback or dampening for the xn-parallel contributions. xnx b2 xn = (σ(xn)) xn (snxn) Thus, the derivative f(xn) does not imply that the xn-parallel aspects of xn (σ(xn)) are ignored during backpropagation. Instead, their influence is intricately incorporated into the learning dynamics. The presence of ϵ in also means that sn (and consequently f(xn) itself, as shown in Eq. (9)) does not perfectly nullify the parallel component, allowing for controlled modulation of information along the xn stream, which can be beneficial for norm stability (further discussed in Sec. B.2). xn, f(xn) = xn, (σ(xn) ϵ xn2 + ϵ (9) 15 B.2 Proof of the Non-Vanishing of the Parallel Component Suppose that ϵ 0 but, not exactly zero. (snxn) xn = (snI + xn, xn 2xn, b2 x2 2) (snxn) + xn, xn, xn = Jf xn, xn snxn = 2s2 nxn + snxn 2s nxn (10) (11) (12) The expression is zero if 1. Condition 1: (i.e., (σ(xn)) xn) sn = 0 Jf xn, xn = 0 (Jacobian action is orthogonal) xn = 0 (zero input) In our setting, this equality never arises. Empirically, the inner-product projection of onto each basis vector xn does not coincide with the exact scalar coefficient sn required by the model, and hence the condition is never satisfied. 2. Condition 2: This orthogonality condition holds precisely when the Jacobian Jf of the mapping is orthogonal to the basis direction xn. In practice, whenever perturbations along xn do not influence the differential behaviour of , the gradient and xn are orthogonal, and the condition is met. 3. Condition 3: Requiring sn to be rational number is untenable for real-valued data. Since observed coefficients are typically irrational in continuous domains, imposing rationality on sn is not realistic."
        },
        {
            "title": "C Ablation Studies",
            "content": "C.1 Learning Curves of ResNetV2 and Ours-G Differing the Initialization Method We evaluated the robustness of our Orthogonal Residual Update to various initialization methods. Specifically, we implemented three initialization approaches: Xavier [30], Kaiming He [31], and Orthogonal [19] initialization for the network weights. For all experiments, the biases of the convolution layers and batch normalization layers were initialized to zero, while the batch normalization weights were set to one. Previous research has established that initialization methods significantly impact model performance and convergence properties [3234]. Therefore, we conducted comparative analysis of the variation of the top-5 validation accuracy (reported in Fig. 6) and final accuracy values (reported in Tab. 7) between the original identity connection and our proposed approach. Our experimental configuration followed the hyperparameter settings from ResNetV2 [2], with training conducted on the CIFAR-100 dataset for 60,000 steps. Table 7: Mean std oftop-1 (Acc@1) and top-5 (Acc@5) accuracy from 5 independent runs.We used ResNetV2-50-G for OURS. Initialization Connection Acc@1(%) Acc@5(%) Orthogonal[19] Xavier[30] Kaiming He[31] ResNetV2 Ours-G ResNetV2 Ours-G ResNetV2 Ours-G 77.40 0.33 77.22 0. 77.43 0.37 77.09 0.31 77.78 0.16 77.54 0.20 93.00 0.20 93.28 0.15 93.07 0.26 93.07 0.09 92.82 0.20 93.37 0.12 Table 8: Hyperparameter settings of the experiment varying the initialization method. We used identical hyperparameter setting across all experiments. Hyperparameter Values Epochs Batch size Base LR Optimizer Weight decay LR scheduler Warm-up epochs Resolution (px) Random crop Horizontal flip Color jitter MixUp α / prob CutMix α Label smoothing Random erase RandAug (N, M) Gradient Clipping Normalization 200 128 1 101 SGD, momentum 0.9 5 104 MultiStep, decay by 0.1 at epochs 80, 120 0 (Not used) 32 Pad 4px, random 3232 crop = 0.5 (Not used) (Not used) (Not used) (Not used) (Not used) (Not used) 1.0 Dataset-specific mean/std 17 (a) Kaiming - Top 5 Validation Accuracy (b) Xavier - Top 5 Validation Accuracy (c) Orthogonal - Top 5 Validation Accuracy Figure 6: Mean value of validation accuracy over training steps from 5 independent runs for different initialization methods: (a) Kaiming Top-5, (b) Xavier Top-5, (c) Orthogonal Top-5. The shaded regions show the intervals one standard deviation above and below the mean. C. Norm Exploding in ResNetV2 with Orthogonal Updates We observed notable phenomenon in ResNetV2 architectures during later training stages: the L2 norm of the output of the final convolutional layer module, (xn), tended to increase dramatically, particularly with linear residual updates. Interestingly, this explosion in (xn)2 was not always accompanied by similarly large input stream norm xn2. While this behavior was noted across various ResNetV2 configurations, we focus here on ResNetV2-34 trained on Tiny ImageNet, using the same experimental settings as detailed in Tab. 6. 18 Fig. 7 illustrates this trend, comparing the evolution of the module output norm (f (xn)2), its component parallel to the input stream (f(xn)2), and its component orthogonal to the input stream (f(xn)2) for both linear and orthogonal residual updates in the final block. For linear updates, significant increase, especially in (xn)2 and often its parallel component f(xn)2, is evident at later steps. (a) Orthogonal component norm (f2). (b) Parallel component norm (f2). Figure 7: Component norms of the final convolutional modules output in ResNetV2-34 on Tiny ImageNet without final LayerNorm. Both plots compare Linear, Orthogonal-F (feature-wise), and Orthogonal-G (global) updates. (X-axis: Training Steps, Y-axis: L2 Norm Squared). The explosion or significant growth of component norms is visible, particularly for the parallel component under certain updates. This phenomenon is suspected to be related to the accumulation of the parallel component f(xn) over layers, as discussed in Sec. 3.5, which can influence the overall module output (xn) when the module itself is not explicitly regularized for orthogonality. To mitigate this, we implemented simple solution: adding LayerNorm [35] (LN) layer immediately before the final classifier head, applied to the output of the last residual stream (i.e., after the final global average pooling in ResNetV2). This LayerNorm effectively normalizes the activations passed to the classifier. As shown in Fig. 8, the introduction of this final LayerNorm significantly stabilized the norms of (xn) and its components for the final convolutional module, especially for the linear residual updates, preventing the previously observed explosion. (a) Orthogonal component norm (f2). (b) Parallel component norm (f2). Figure 8: Component norms of the final convolutional modules output in ResNetV2-34 on Tiny ImageNet with final LayerNorm applied before the classifier head. (X-axis: Training Steps, Y-axis: L2 Norm Squared). The norm explosion is mitigated, and component norms remain more stable throughout training. With this LayerNorm fix applied to ResNetV2-34 on Tiny ImageNet, the model with linear residual updates achieved top-1 accuracy of 65.88%0.22%, while the model with our Orthogonal Residual Updates (Feature-wise) achieved 65.83%0.38%. Tab. 9 summarizes these indicative results. While the norm explosion was primarily an issue for linear updates, applying LayerNorm is common practice and ensures fair comparison. 19 Table 9: Indicative top-1 accuracy (%) of ResNetV2-34 on TinyImageNet with and without the final LayerNorm (LN) before the classifier. Values are mean std. from 5 runs. denote metrics from Tab. 2 Configuration Connection Top-1 Acc. (%) W/O final LN W/ final LN Linear Orthogonal-F Orthogonal-G Linear Orthogonal-F Orthogonal-G 64.610.24 65.460.30 65.380.35 65.880.22 65.830.38 65.660.15 This investigation suggests that while our orthogonal updates inherently promote more stable norm dynamics within the residual blocks (as discussed in Sec. 3.5), careful consideration of normalization at the networks extremity, particularly before the classifier, can be beneficial for all types of residual connections in deep CNNs to prevent potential instabilities arising from the final layers. C.3 Adapting Connection Types without Optimizer Re-initialization Table 10: Performance (Acc@1 and Acc@5, mean std over 5 runs) of ViT-S under continuous training conditions. The LL and OO rows show results from 300 epochs of training with consistent Linear or Orthogonal connection type, respectively (no mid-training switch). The LO and OL rows show results where models were trained for 150 epochs with the starting architecture, then the connection type was switched, and training continued for another 150 epochs on the same dataset, all without re-initializing the optimizer or learning rate scheduler. Dataset Start Arch. End Arch. Acc@1 (%) Acc@5 (%) CIFAR-10 CIFARTiny ImageNet Linear Linear Orthogonal Orthogonal Orthogonal Linear Orthogonal Linear Linear Linear Orthogonal Orthogonal Orthogonal Linear Orthogonal Linear Linear Linear Orthogonal Orthogonal Orthogonal Linear Orthogonal Linear 89.820.34 90.610.21 91.000.14 93.180.15 71.920.24 73.860.31 71.640.56 74.140.35 51.300.40 52.570.71 50.780.42 53.330.62 99.650.03 99.690.03 99.660.02 99.720.03 92.110.18 92.230.26 91.960.24 92.690. 75.190.66 75.330.57 73.910.32 76.060.46 Building upon the findings presented in Sec. 4.3, where re-initializing the optimizer and scheduler between training phases was standard practice, we further investigated the adaptability of connection types under more continuous training regime. Specifically, we explored scenarios where the optimizer and learning rate scheduler were not re-initialized when the connection type was switched. For these experiments, models were trained for an initial 150 epochs with designated connection type. Subsequently, for the LO and OL configurations, the connection type was altered, and training continued seamlessly for an additional 150 epochs on the same dataset. The LL and OO configurations serve as baselines in this experiment. They represent standard, uninterrupted training for the full 300 epochs using either consistently Linear or consistently Orthogonal connections, respectively, without any mid-training architectural change. As such, the performance metrics reported for LL and OO in Tab. 10 are identical to those presented for the \"Linear\" and \"Orthogonal-F\" (ViT-S) entries in Tab. 2 for the corresponding datasets. This setup allows us to assess the impact of architectural transitions against these established baselines. The learning trajectories, depicting Validation Accuracy@1 (Acc@1) against training steps for these experiments on CIFAR-10, CIFAR-100, and Tiny ImageNet, are visualized in Fig. 9. These plots illustrate the adaptation process throughout both training phases, highlighting the models performance evolution across the architectural switch. The final performance for ViT-S on CIFAR-10, CIFAR-100, and Tiny 20 (a) CIFAR-10, Acc@1 (b) CIFAR-100, Acc@1 (c) Tiny ImageNet, Acc@ Figure 9: Validation Accuracy@1 (Acc@1) curves versus training steps for ViT-S when switching residual connection types mid-training (at 150 epochs) without optimizer or learning rate scheduler re-initialization. The x-axis represents Training Steps and the y-axis represents Validation Acc@1. ImageNet, comparing transitions from Identity to Orthogonal connections and Orthogonal to Identity connections, are detailed in Tab. 10."
        },
        {
            "title": "D Additional Details on Internal Norm Dynamics",
            "content": "This section provides more detailed examination of the internal dynamics within Vision Transformer (ViT-S, trained on Tiny ImageNet) blocks, complementing the analysis in Sec. 3.5 of the main paper. We focus here on the L2 norms squared of the parallel component (f(σ(xn))2) and the orthogonal component (f(σ(xn))2) of the module output (σ(xn)). Fig. 10 presents these evolving norms for both MLP and Attention blocks across different layers, comparing linear residual updates against our orthogonal residual updates. (a) MLP Blocks: Parallel component norm. (b) MLP Blocks: Orthogonal component norm. (c) Attention Blocks: Parallel component norm. (d) Attention Blocks: Orthogonal component norm. Figure 10: Evolution of squared L2 norms for parallel (f) and orthogonal (f) components of the module output (σ(xn)) across different layers (Blocks 0-5) of MLP and Attention modules in ViT-S on TinyImageNet. Blue lines denote Orthogonal Residual Update, and red lines denote Linear Residual Updates. These plots illustrate the differing contributions of module output components under each update scheme. General Observations on Component Magnitudes. primary observation from Fig. 10 is the substantial difference in the magnitudes and characteristic dynamics of f(σ(xn))2 and f(σ(xn))2 when comparing linear and orthogonal update schemes. These differences are generally consistent across both MLP and Attention blocks. For our orthogonal updates (blue lines), the orthogonal component norm, f2, is typically maintained at significant level throughout training across most layers (e.g., MLP Blocks 0, 1, 2, 4, 5 and most Attention Blocks). This directly aligns with our design objective of encouraging modules to generate outputs with strong, novel directional components. In the case of MLP Block 3, while f2 for orthogonal updates (blue line, Fig. 10b) may be smaller than that of linear updates (red line) during the initial training phase, it still represents substantial component and is not negligible compared to its own parallel component f2 (blue line, Fig. 10a). Subsequently, it surpasses the linear updates f2. In contrast, for linear updates (red lines), the orthogonal component norm f2 (Fig. 10b and 10d) generally shows little tendency to increase or be sustained; it typically diminishes significantly over training, often approaching zero, particularly in deeper layers. This indicates that the module output (σ(xn)) becomes predominantly parallel (or anti-parallel) to the input stream xn. slight variation is observed in MLP Block 0 under linear updates, where f2 (red line, Fig. 10b) is maintained at higher level for longer initial period before sharply decreasing (around 16k-24k steps). However, when compared to the f2 dynamics under our orthogonal updates (blue line) in the same block, 22 which remains high or even increases, the linear updates f2 still exhibits an overall diminishing pattern relative to what our method achieves. Dynamics of the Parallel Component (f(σ(xn))2). Under linear updates, f2 (Fig. 10a and 10c, red lines) can be substantial. Notably, for MLP Block 0 under linear updates, f2 is significantly larger than that of the corresponding Attention Block 0. This large parallel component in MLP Block 0 (linear) does not exhibit sharp peak and decline like some other MLP layers; instead, after an initial rise, it maintains relatively high norm or shows more gradual decay. This sustained, large parallel component, when combined with the cosine similarity dynamics observed in Fig. 3 of the main paper (where MLP Block 0 for linear updates shows complex cosine trajectory), might contribute to less efficient utilization or even misalignment of the modules capacity in early network stages. For orthogonal updates (blue lines), as discussed in Sec. 3.5, the accumulation of f2 in (σ(xn)) (the full module output, not the update term f(xn)) is evident, especially in later layers (e.g., MLP Blocks 3-5, Attention Blocks 3-5). This accumulation is key factor explaining the rise in cos(xn, (σ(xn))) for orthogonal updates, as the module learns based on its entire output. The Transition Point and Cosine Similarity Drivers for Orthogonal Updates. The Transition Point, identified from the dynamics of xn2 and cos(xn, (σ(xn))) in the main paper, often corresponds to shifts in the f(σ(xn))2 and f(σ(xn))2 trends detailed in Fig. 10. The subsequent increase in cos(xn, (σ(xn))) for orthogonal updates, viewed from the perspective of these -module components, can arise from varying interplay between f2 and f2: In some layers, particularly earlier ones (e.g., MLP Blocks 0-1, Attention Blocks 0-1), both f2 and f2 for orthogonal updates (blue lines) may increase or be maintained after an initial phase. significant rise in an xn-aligned f2, potentially becoming more dominant or growing rapidly relative to f2, can drive the overall cosine similarity cos(xn, (σ(xn))) upwards. Visual inspection of Fig. 10 shows that in these specific early blocks, f2 often exhibits more substantial increase or is of larger magnitude than f2 for orthogonal updates, but both contribute to the module output (σ(xn)). In other layers, or later in the training process (e.g., MLP Blocks 3, 4, and 5 for orthogonal updates), continued accumulation and significant growth of f2 can occur, sometimes becoming much larger than f2 (which itself might stabilize, slightly decrease from peak, or grow less rapidly). This increasing dominance of the xn-aligned parallel component f(σ(xn)) within the overall module output (σ(xn)) is primary driver for the observed increase in cos(xn, (σ(xn))). These component-level norm dynamics  (Fig. 10)  thus provide direct evidence for how the full module output (σ(xn)) adapts under the orthogonal update scheme, elaborating on the mechanisms behind the cosine similarity changes discussed in Sec. 3.5. Implications for Layer-wise Application of Orthogonality. The observed layer-dependent behaviors of f2 and f2 are intriguing. If considering ablation studies where orthogonal updates are applied only to specific layers Sec. 4.5, these detailed component dynamics could offer insights. For instance, if the primary benefits of orthogonal updates (such as establishing diverse representations via strong component) manifest strongly in early layers and are then propagated, it might suggest that targeted application of orthogonality could be viable strategy. However, Tab. 2 apply orthogonal updates throughout, and the sustained f2 across many layers suggests ongoing benefits rather than applying specific layer to be orthogonal, as shown in Tab. 4. 23 γ: Architectural Ratio of Width to Depth In the main paper (specifically in Sec. 4.2 when discussing Tab. 2), we introduced the ratio γ as heuristic to quantify the relationship between networks effective representational width and its depth. This ratio was used to hypothesize potential reasons for the varying efficacy of our Orthogonal Residual Update across different architectures, notably between Vision Transformers (ViTs) and ResNetV2s. higher γ is conjectured to imply relatively larger representational capacity per sequential processing stage, which might offer more room for orthogonal components to provide novel information without excessive redundancy. This appendix provides the calculation details and broader comparison of γ values. Definition of γ. For Vision Transformers and other Transformer-based architectures (e.g., GPT2 [36], LLaMA [6], T5 [37], DiT [38]), γ is calculated as the ratio of the models hidden dimensionality (dmodel) to the number of transformer layers or blocks (Lblocks or Nlayers in one stack for encoder-decoder models): γTransformer = dmodel Lblocks For ResNet architectures, γ is defined based on the average output channel dimensionality of its residual blocks (Davg) relative to the total number of residual blocks (Btotal), which represents the effective depth of the residual stream: γResNet = Davg Btotal (cid:80) kstages BkCout Btotal , Bk is the number of residual blocks in stage k, out where Davg = is the output channel dimension of the blocks in stage (this is the dimension of the feature map to which the residkstages BkCout ual (xn) is added), and Btotal = (cid:80) which was used for calculations and matches the example in the main paper. Bk. This formula is equivalent to γResNet = total B2 (cid:80) , Comparative γ Values. Tab. 11 lists the architectural parameters and calculated γ values for models discussed in this paper and several external reference models. Table 11: Comparison of γ values across various model architectures. dmodel denotes hidden dimension; denotes number of layers/blocks. For ResNets, Bk is blocks per stage, out is output channels per stage, Btotal is total residual blocks, and Davg = ((cid:80) BkC out )/Btotal. The γ for ResNets is Davg/Btotal. Model dmodel or Davg or Btotal Family Notes γ Models from this papers experiments ResNetV2 ResNet-18 ResNet-34 ResNet-50 ResNet-101 240.00 (Davg) 236.00 (Davg) 944.00 (Davg) 985.21 (Davg) ViT ViT-S (ours) ViT-B (ours) 384 (dmodel) 768 (dmodel) 8 16 16 33 6 12 External Reference Models ResNet ResNet-1001 37.33 (Davg) 168 ViT Language Models ViT-S (std.) ViT-L ViT-H GPT-2 XL LLaMA-7B T5-XL Diffusion DiT-XL/2 384 (dmodel) 1024 (dmodel) 1280 (dmodel) 1600 (dmodel) 4096 (dmodel) 2048 (dmodel) 1152 (dmodel) 12 24 32 48 32 24 28 30.00 14.75 59.00 29.85 64.00 64. 0.22 32.00 42.67 40.00 33.33 128.00 85.33 41.14 Basic blocks Basic blocks Bottleneck blocks Bottleneck blocks Used in paper Used in paper [1] [5] 1.5B params [36] [6] Encoder/Decoder [37] [38] 24 Discussion. The γ values presented in Tab. 11 span wide range, reflecting diverse architectural philosophies. As noted in the main paper, the ViT models used in our experiments (ViT-S γ = 64, ViT-B γ = 64) have substantially higher γ values compared to ResNetV2-34 (γ = 14.75) and are comparable to or higher than ResNetV2-18 (γ = 30.00) and ResNetV2-101 (γ 29.85). Interestingly, ResNetV2-50 (γ = 59.00) also exhibits high γ value, approaching that of our ViT models. This is primarily due to its bottleneck architecture: our γ definition for ResNets calculates Davg based on the expanded output channel dimensions of the blocks (i.e., the dimensionality of the stream xn where the residual (xn) is added) and Btotal as the count of these residual blocks. Since bottleneck blocks in ResNetV2-50 feature significantly wider output dimensions (resulting in Davg = 944) than the basic blocks in models like ResNetV2-34 (Davg = 236) for the same number of residual blocks (Btotal = 16), its calculated γ is consequently higher. It is important to acknowledge nuance in this γ definition when applied to bottleneck ResNets versus Transformers or basic-block ResNets. While our γ calculation consistently uses the dimensionality of the stream where the residual sum occurs, single bottleneck block internally performs multiple distinct convolutional operations with varying channel dimensions (e.g., 1x1 channel reduction, 3x3 convolution in the narrower space, and 1x1 channel expansion). If one were to define an \"effective γ\" that accounts for this internal sequential processing or averages across these varying internal channel widths (rather than primarily considering the wide output where the addition happens), the γ value for bottleneck architectures like ResNetV2-50 could be interpreted as being substantially lower. Thus, while our current γ definition highlights the width of the feature space available for the residual addition, it may not fully capture the operational \"compactness\" imposed by the narrower processing stages within each bottleneck block, especially when comparing against Transformer layers that typically maintain more uniform processing width throughout. Despite this definitional consideration, the consistently calculated high γ for ResNetV2-50 indicates large dimensional space for its residual updates. The precise interplay between this γ metric, specific architectural inductive biases (e.g., local receptive fields in CNNs vs. global attention in Transformers), and the observed efficacy of the Orthogonal Residual Update warrants further nuanced investigation. The diverse γ values seen in other reference modelssuch as the extremely low γ for the deep and narrow ResNet-1001 (CIFAR config) versus the very high γ values for large language models like LLaMA-7B and T5-XLfurther emphasize that γ is one of several factors influencing how different residual update mechanisms might perform across various architectural paradigms."
        },
        {
            "title": "F PyTorch Implementation",
            "content": "This section provides example PyTorch implementations for the channel-wise and global orthogonalization functions used to compute the orthogonal component f(xn) from module output (xn) and an input stream xn. These functions encapsulate the core logic of Eq. 3 from the main paper. def _orthogonal_channel(x: torch.Tensor, f_x: torch.Tensor, dim: int, eps: : residual stream tensor torch.Tensor) -> torch.Tensor: \"\"\" Orthogonal residual connection (channel-wise). f_x : module output tensor (e.g., from Attention, MLP, or Conv if channel-wise) dim : dimension along which to compute orthogonality (e.g., channel dimension) eps : small epsilon tensor for numerical stability \"\"\" # Ensure eps is on the same device as if its tensor eps = eps.to(x.device) dot_product = (x * f_x).sum(dim, keepdim=True) norm_x_squared = (x * x).sum(dim, keepdim=True).float() + eps # Ensure scale is cast back to original dtype if was float16/bfloat16 scale_factor = (dot_product / norm_x_squared).to(dtype=x.dtype) projection_onto_x = scale_factor * f_orthogonal = f_x - projection_onto_x return f_orthogonal Listing 1: PyTorch function for channel-wise orthogonalization. def _orthogonal_global(x: torch.Tensor, f_x: torch.Tensor, dim: int, eps: : residual stream tensor torch.Tensor) -> torch.Tensor: \"\"\" Orthogonal residual connection (global). f_x : module output tensor (e.g., from convolutional block) dim : starting dimension for flattening (all subsequent dims will be flattened) eps : small epsilon tensor for numerical stability \"\"\" original_shape = x.shape # Convert negative dim to positive for consistent unsqueezing later positive_dim_idx = dim if dim >= 0 else len(original_shape) + dim eps = eps.to(x.device) x_flattened = x.flatten(start_dim=positive_dim_idx) # [B, C, H, W] -> [B, C*H*W] if dim=1 (for NCHW) f_x_flattened = f_x.flatten(start_dim=positive_dim_idx) # or [B, D] if already 2D from start_dim # Sum over the flattened dimensions (which is now dim=1 if start_dim was 1, or the last dim) # For clarity, explicitly use dim=-1 for sum over the last (flattened) dimension dot_product = (x_flattened * f_x_flattened).sum(dim=-1, keepdim=True) norm_x_squared = (x_flattened * x_flattened).sum(dim=-1, keepdim=True).float() + eps scale_factor = (dot_product / norm_x_squared).to(dtype=x.dtype) # Reshape scale_factor to allow broadcasting with original shape # It needs to have trailing dimensions of size 1 to match xs rank post-flattening start_dim num_dims_to_unsqueeze = len(original_shape) - (positive_dim_idx + 1) # +1 because dot_product keeps one dim for _ in range(num_dims_to_unsqueeze): scale_factor = scale_factor.unsqueeze(-1) projection_onto_x = scale_factor * # Broadcasting happens here f_orthogonal = f_x - projection_onto_x return f_orthogonal Listing 2: PyTorch function for global orthogonalization. The dim argument in _orthogonal_channel typically refers to the channel dimension (e.g., dim=1 for NCHW tensors) where orthogonality is computed independently for each spatial location or token. For _orthogonal_global, dim specifies the starting dimension from which the tensor is flattened before computing single global projection scale per batch element; for instance, for an NCHW tensor, dim=1 would flatten C, H, and dimensions together. The choice between these depends on the layer type and desired granularity of orthogonalization as discussed in Sec. 3.4 of the main paper."
        }
    ],
    "affiliations": [
        "Maum.AI",
        "Yonsei University"
    ]
}