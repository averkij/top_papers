{
    "paper_title": "StructEval: Benchmarking LLMs' Capabilities to Generate Structural Outputs",
    "authors": [
        "Jialin Yang",
        "Dongfu Jiang",
        "Lipeng He",
        "Sherman Siu",
        "Yuxuan Zhang",
        "Disen Liao",
        "Zhuofeng Li",
        "Huaye Zeng",
        "Yiming Jia",
        "Haozhe Wang",
        "Benjamin Schneider",
        "Chi Ruan",
        "Wentao Ma",
        "Zhiheng Lyu",
        "Yifei Wang",
        "Yi Lu",
        "Quy Duc Do",
        "Ziyan Jiang",
        "Ping Nie",
        "Wenhu Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As Large Language Models (LLMs) become integral to software development workflows, their ability to generate structured outputs has become critically important. We introduce StructEval, a comprehensive benchmark for evaluating LLMs' capabilities in producing both non-renderable (JSON, YAML, CSV) and renderable (HTML, React, SVG) structured formats. Unlike prior benchmarks, StructEval systematically evaluates structural fidelity across diverse formats through two paradigms: 1) generation tasks, producing structured output from natural language prompts, and 2) conversion tasks, translating between structured formats. Our benchmark encompasses 18 formats and 44 types of task, with novel metrics for format adherence and structural correctness. Results reveal significant performance gaps, even state-of-the-art models like o1-mini achieve only 75.58 average score, with open-source alternatives lagging approximately 10 points behind. We find generation tasks more challenging than conversion tasks, and producing correct visual content more difficult than generating text-only structures."
        },
        {
            "title": "Start",
            "content": "StructEval: Benchmarking LLMs Capabilities to Generate Structural Outputs Lipeng He1 Jialin Yang 1 Dongfu Jiang 1 Sherman Siu1 Yuxuan Zhang2 5 2 0 2 6 2 ] . [ 1 9 3 1 0 2 . 5 0 5 2 : r Disen Liao1 Zhuofeng Li4 Huaye Zeng1 Yiming Jia1 Haozhe Wang3 Benjamin Schneider1 Chi Ruan5 Wentao Ma1 Zhiheng Lyu1 Yifei Wang1 Yi Lu2 Quy Duc Do Ziyan Jiang1 Ping Nie5 Wenhu Chen 6 1University of Waterloo, 2University of Toronto, 3HKUST, 4Shanghai University, 5Independent Contributor, 6Vector Institute Equal Contribution {j586yang, dongfu.jiang, wenhuchen}@uwaterloo.ca https://tiger-ai-lab.github.io/StructEval/"
        },
        {
            "title": "Abstract",
            "content": "As Large Language Models (LLMs) become integral to software development workflows, their ability to generate structured outputs has become critically important. We introduce StructEval, comprehensive benchmark for evaluating LLMs capabilities in producing both non-renderable (JSON, YAML, CSV) and renderable (HTML, React, SVG) structured formats. Unlike prior benchmarks, StructEval systematically evaluates structural fidelity across diverse formats through two paradigms: (1) generation tasks, producing structured output from natural language prompts, and (2) conversion tasks, translating between structured formats. Our benchmark encompasses 18 formats and 44 types of task, with novel metrics for format adherence and structural correctness. Results reveal significant performance gapseven state-of-the-art models like o1-mini achieve only 75.58 average score, with open-source alternatives lagging approximately 10 points behind. We find generation tasks more challenging than conversion tasks, and producing correct visual content more difficult than generating text-only structures."
        },
        {
            "title": "Introduction",
            "content": "In recent years, there has been significant surge in the capabilities of large language models (LLMs) in generating human-like text and performing wide range of natural language processing tasks. State-of-the-art models like GPT-4o (Hurst et al., 2024), OpenAI o1/o3 (Contributors et al., 2024), and Googles Gemini (Team et al., 2023) have achieved superior performance in knowledge QA (Hendrycks et al., 2020; Wang et al., 2024), instruction-following (Chiang et al., 2024; Zhou et al., 2023), and code generation (Zhuo et al., 2024; Jain et al., 2024). 1 Figure 1: STRUCTEVAL evaluates the LLMs capability to generate structured outputs, including text-only tasks like JSON, TOML, etc, and visual rendering tasks like HTML, React, Latex, etc. Despite recent advances, many real-world applications require not only fluency in the content of the output but also precise control over its structure. This includes tasks where the expected output must follow specific formats such as JSON, XML, LaTeX, HTML, or code in frameworks like React or Vue. Additionally, in these tasks, in these tasks, we also want the code to render page that correctly places elements according to the requirements. These types of structured output are essential in domains like software development, data pipelines, user interface generation, and scientific publishing, where incorrect formatting can lead to disrupted pipelines or non-functional outputs. However, most existing benchmarks focus on the semantic quality (Wang et al., 2024) or reasoning ability of LLMs (Hendrycks et al., 2021; He Figure 2: The overall designed annotation pipeline of STRUCTEVAL dataset et al., 2024), with limited emphasis on their ability to produce format-conforming structured outputs. Some recently proposed benchmarks aim to evaluate the quality of structured outputs tend to target specific modalities, such as code generation (Zhuo et al., 2024) or text-only structures (Gu et al., 2024; Tang et al., 2023), rather than offering comprehensive evaluations across diverse structured formats. As existing benchmarks gradually become more saturated, it is still unknown how the current stateof-the-art models perform in structured generation tasks. We argue that effectively evaluating the models performance on such tasks is inherently challenging due to the following issues: (1) Data Collection Challenges: Gathering diverse structured tasks and corresponding examples requires domain expertise across multiple formats, with high-quality annotations demanding significant effort and specialized knowledge. (2) Evaluation Metric Complexity: Designing reasonable metrics in unified form for both textonly structures (JSON, YAML) and visual outputs (HTML, SVG) is difficult, as they require different assessment approaches for structural correctness and visual fidelity. (3) Technical Implementation Barriers: Building framework that supports execution and evaluation across numerous rendering environments requires complex integration of multiple language interpreters and visualization tools. To address these challenges, we introduce STRUCTEVAL, comprehensive benchmark that systematically evaluates LLMs abilities to produce highly structured output. Our benchmark encompasses 21 distinct formats and 44 task types organized into two complementary subsets: StructEvalT, which assesses the generation of text-only structures such as JSON and TOML, and StructEval-V, which evaluates the quality of visually rendered outputs from code such as HTML and SVG. Both subsets include generation tasks (converting natural language to structured outputs) and conversion tasks (transforming between two structured formats). To ensure robust evaluation across these diverse formats, we have developed novel assessment framework that integrates syntactic validity checking, keyword matching, and visual question answering, providing holistic measure of both structural correctness and output fidelity. Our comprehensive evaluation reveals significant performance gaps across models and tasks. Even state-of-the-art commercial models like o1mini achieve only an average score of 75.58, while the best open-source model, such as Llama-3-8BInstruct, lags 10 points behind, underscoring the performance gap between commercial and opensource LLMs. We observe that generation tasks generally pose greater challenges than conversion tasks, and producing code capable of rendering correct visual content proves more difficult than generating text-only structured outputs. Task difficulty varies considerably across formats: while some tasks are effectively solved by all LLMs with scores exceeding 0.95 (such as TextMarkdown and TextHTML), others remain particularly challenging with all models scoring below 0.5 (including TextMermaid and MatplotlibTikZ). Through this systematic analysis, we aim to drive progress in structured output generation capabilities that are increasingly crucial for the real-world applications of language models."
        },
        {
            "title": "2 StructEval Dataset",
            "content": "In this section, we first present an overview of our STRUCTEVAL dataset and statistical analysis in subsection 2.1. Next, we elaborate on how we de2 Subset SE-T-gen SE-T-conv SE-V-gen SE-V-conv StructEval # Total Tasks # Total Examples # Avg Keywords # Avg VQA pairs 5 14 13 12 44 250 700 650 2035 7.9 17.5 11.1 22.2 14.7 - - 7.9 9.0 8.5 Table 1: The overall statistics of the STRUCTEVAL dataset. Here \"SE\" denotes StructEval. \"T\" and \"V\" represents the StructEval-T and StructEval-V subsets respectively. \"gen\" and \"conv\" represent the \"generation\" and \"conversion\" task types respectively. StructEval-T Question, KeyWords Please output JSON code. Task: Summarize metadata about fictional scientific article. Feature Requirements: 1. Top-level field \"title\" is string containing the article title. 2. Field \"authors\" is list of exactly two items. 3. Each element of \"authors\" contains \"name\" (string) and \"affiliation\" (string). 4. Field \"publication.year\" is an integer. 5. Field \"keywords\" is list of strings. sign the whole pipeline for annotation and quality review in subsection 2.2. We will introduce how we design the evaluation metrics for each task in our dataset in section 3."
        },
        {
            "title": "2.1 Overview",
            "content": "As shown in Table 1, our STRUCTEVAL dataset comprises total of 2,035 examples, covering 44 unique structure generation tasks across 18 structured output formats. The dataset is organized into two main subsets: StructEval-T and StructEval-V. StructEval-T is designed to evaluate an LLMs ability to generate structured outputs directly from natural language prompts without rendering. Supported formats include JSON, XML, YAML, Markdown, CSV, TOML, among others. These are highly useful formats in many downstream applications. StructEval-V assesses an LLMs ability to generate executable code for visual rendering that fulfills specified visual requirement. This subset includes formats such as HTML, React, Matplotlib, Canvas, LaTeX, SVG, Mermaid, and more. These are widely adopted formats for various applications. Each example in the dataset is categorized as either generation or conversion. In generation tasks, the model is required to produce structured output based on natural language description with detailed specifications. In conversion tasks, the model must translate structured content from one format to another (e.g., JSON to YAML, HTML to React). Formally, each example is represented as triplet (q, K, Qv), where denotes the structure generation question, = {k1, . . . , kK} is set of keywords expected to appear in the output, and Keywords: title authors[0].name authors[1].affiliation publication.year keywords[2] Figure 3: Example question and key words of the StructEval-T generation task StructEval-V Question, Keywords Matching, VQA Pairs Please output HTML code. Task: Design webpage that presents users travel itinerary. Feature Requirements: Include centered <h1> header with the text \"Trip Summary\". Use <table> to list destinations; include 3 rows and 2 columns. Apply class \"highlight\" to the second row. Add <button> labeled \"Export PDF\" at the bottom of the page. Keywords: Trip Summary highlight <h1> Export PDF VQA Pairs: Q: What text is displayed in the <h1> header? A: Trip Summary Q: How many rows are in the table? A: 3 Q: What class is applied to the second table row? A: highlight Q: What text is on the button at the bottom? A: Export PDF Figure 4: Example question, keywords, and VQA pairs for STRUCTEVAL-V generation task"
        },
        {
            "title": "Description",
            "content": "Literal key access Nested lists with index Wildcard in lists Backtick quoting planet.name planet.moons[0].name planet.moons.*.name data.key.with.dots Checks if key name exists as child of object planet. Verifies first item in moons list has name field. Confirms that name exists for any moon in the list. Treats entire quoted token as single key, useful for special characters. CSV header check csv::discovery.location Ensures CSV output has column named XML attribute fallback @id discovery.location. Looks for id attribute, using @ to indicate XML format. Table 2: Supported rule types in our path-based evaluation. 1, av Qv, av 1), . . . , (qv Qv = {(qv Qv)} is set of visual question-answer (VQA) pairs used for evaluating examples in the StructEval-V subset. In contrast, for StructEval-T, Qv is empty and not used during evaluation. To ensure comprehensive evaluation, each example in the dataset contains on average 14.7 keywords and 8.5 VQA pairs, as detailed in Table 1. The dataset encompasses wide spectrum of structured output formats, ranging from widelyused data serialization types like JSON and YAML to visually-renderable formats such as SVG, Mermaid, and TikZ. This diverse format coverage enables more holistic evaluation of LLMs capabilities in both structured data modeling and visual code generation. Notably, the inclusion of niche yet expressive formatssuch as Typst for typesetting, Mermaid for diagram specification, and TikZ for LaTeX-based graphicsbroadens the evaluative scope beyond conventional tasks. These formats collectively span domains including web front-end development, data exchange, scientific visualization, and technical documentation. The distribution of tasks across these formats is shown in Table 6, highlighting the balanced composition of generation and conversion tasks across both textual and visual modalities."
        },
        {
            "title": "2.2 Annotation Pipeline",
            "content": "To construct high-quality and diverse benchmark, we design multi-stage annotation pipeline consisting of three key components: 1) task curation, 2) LLM-based synthesis, and 3) expert review. This pipeline ensures both the scalability and accuracy of the STRUCTEVAL dataset. Task Prompt We begin by identifying broad spectrum of structure generation and conversion tasks that span both text-based and executable visual formats. These tasks are selected to reflect practical use cases and diverse real-world scenarios, covering 18 target formats and 44 distinct task 4 types (also shown in Table 6. Each task specification includes format constraints, input-output expectations, and, where applicable, conversion rules. Please refer to subsection A.4 for sample task prompt. Query/Metric Generation Given the high cost of fully manual annotation, we leverage large language model to synthesize an initial pool of candidate examples. Each example consists of task query and set of associated evaluation metrics, including keywords for text outputs and visual question-answer (VQA) pairs for visual outputs. This step allows us to rapidly generate large and varied collection of plausible instances that serve as drafts for human refinement. Expert Review To ensure quality and correctness, we employ two-pass human review process. Annotators first validate and refine the generated task queries and associated metrics. They are allowed to freely modify, add, or remove any part of the synthesized content to ensure task clarity, completeness, and evaluability. In the second pass, separate reviewer verifies the consistency and correctness of each example. All annotation is conducted using LabelStudio (Tkachenko et al., 2020-2025), an open-source collaborative annotation tool designed for structured data. The final dataset contains 2035 curated examples, carefully reviewed to support robust evaluation across both StructEval-T and StructEval-V settings."
        },
        {
            "title": "3 StructEval Evaluation",
            "content": "Before the evaluation, we feed the LLM with the questions in the datasets with the corresponding prompt template defined in Table 3. We require the LLM to output the desired structured outputs between \"<BEGIN_CODE>\" and \"<END_CODE>\" so we can correctly parse the structured outputs for evaluation. For the StructEval-V, parsed outputs will be additionally sent to our rendering engines to acquire the rendered visual outputs (see examples in subsection A.3). We then evaluate model outputs using an automatic evaluation pipeline that captures both structural correctness and semantic fidelity. Specifically, we have designed core metrics depending on the task format: 1) Syntax Score, 2) Keyword Matching Score, and 3) Visual Question Answering (VQA) Score. {StructEval Question} IMPORTANT: Only output the required output format. You must start the format/code with <BEGIN_CODE> and end the format/code with <END_CODE>. No other text output (explanation, comments, etc.) are allowed. Do not use markdown code fences. Table 3: Prompt template used for LLM inference before the evaluation Syntax Score. The Syntax Score verifies the structural correctness of the generated output. For text-based formats such as JSON, YAML, and CSV, this involves parsing the output using formatspecific Python parser. For executable visual formats like HTML, LaTeX, or SVG, the code is rendered using headless renderer to determine whether it executes successfully. score of 1 is assigned if the output is syntactically valid or successfully rendered; otherwise, the score is 0. See the subsection A.3 for some correctly rendered images, code produced by the tested LLMs. Keyword Matching Score This metric evaluates whether the generated output contains the required structural elements. Given the reference set of expected keywords = {k1, . . . , kK} for given task, we assess their presence using exact matching or regular expression rules. For the tasks of StructEval-T such as JSON or XML, keyword matching is performed over field names and values using dot-path references to account for nested hierarchies. The score is computed as the proportion of expected keywords correctly matched in the models output. Our evaluation supports variety of path formats as shown in Table 2. The way dot-path rules are created differs depending on the task type. For generation tasks, each task prompt includes feature requirements stated in natural language. These requirements define target keys and their relationships to one another (e.g., nesting depth, list membership). Annotators translate each requirement into concrete dot-path rule using the syn-"
        },
        {
            "title": "VQA Prompt Template",
            "content": "You are given an image and list of question-answer pairs. For each pair, verify if the image content supports the expected answer based on the corresponding question. Base your judgment solely on the visual content of the provided image, and the question. Do not use any external information or common-sense reasoning beyond what is visible. Respond with JSON object mapping each question number to true or false (e.g., {\"1\": true, \"2\": false}). If the image is unclear or does not contain enough information to answer, use null for that question. Here are the question-answer pairs: {qa_list} Figure 5: Prompt template used for VQA evaluation. We use GPT-4.1-mini in the benchmark evaluation. tax rules shown in Table 2. For conversion tasks, the input is itself structured format (e.g., YAML or XML). We use an LLM to parse the structural schema of the inputidentifying key names, nesting levels, and list structuresand convert them into target dot-path rules that the generated output must preserve. This approach ensures that models are not only producing syntactically valid outputs, but also preserving the expected structural relationships. For the tasks of StructEval-V such as HTML, and Matplotlib, we simply detect whether the annotated keyword is in the structured outputs and give scores accordingly. VQA Score This score is used exclusively for tasks in the StructEval-V subset, where the output is expected to be visually rendered. After rendering the output, GPT-4.1-mini (Hurst et al., 2024), vision-language model (VLM), is employed to answer set of visual questions Qv = {(qv Qv)}. The VLM will be given both the questions and answers and required to decide whether the VQA pair matches this rendered image. The VQA score is computed as the proportion of correctly answered questions. 1), . . . , (qv Qv, av 1, av Final task scores are calculated as weighted combinations of these metrics, with weights adjusted based on whether the task is renderable. Let ss, sk, sv [0, 1] denotes the syntax, keyword matching, and VQA score respectively. The for 5 StructEval-T task, the final score is computed as:"
        },
        {
            "title": "Value",
            "content": "s = 0.2 ss + 0.8 sk (1) For StructEval-V, the final score in computed as: = 0.2 ss + 0.1 sk + 0.7 sv (2) This evaluation framework provides unified, finegrained view of model performance across both structured data generation and visual code synthesis tasks, supporting deeper insights into LLM capabilities across modalities."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "Evaluation Models. We evaluate range of large language open-source and commercial models (LLMs) using our benchmark. For open-source models, we use Meta-Llama-3-8BInstruct (Grattafiori et al., 2024), Phi-3-mini128k-instruct (Abdin et al., 2024a), Phi-4-miniinstruct (Abdin et al., 2024b), Qwen2.5-7BInstruct (Yang et al., 2024), and Qwen3-4B (Yang et al., 2025). For commercial models, we use Gemini-1.5-pro and Gemini-2.0-flash (Team et al., 2023), GPT-4.1-mini and GPT-4o (Hurst et al., 2024), GPT-4o-mini, and o1-mini (Contributors et al., 2024). All tasks are evaluated in zero-shot setting using consistent prompts and parameters. Inference Setup. All model generations are performed using LLM-Engine (Jiang, 2024), unified inference framework that supports both opensource backends (e.g., VLLM, SGLang, Together), and commercial APIs (e.g., OpenAI, Claude, Gemini). For open-source models, we specifically utilize the vLLM engine for efficiency (Kwon et al., 2023). For close-source models, we simply call the APIs. As shown in Table 4, we use greedy decoding by default. All tasks are evaluated zeroshot using uniform task prompts defined in Table 3. When performing the VQA evaluation, we select GPT-4.1-mini as the VLM due to its superior multimodal abilities (OpenAI, 2025). We apply the VQA prompt template defined in Figure 5 and ask the VLM to decide whether each VQA pair matches the rendered visual image at once. Evaluation. Output generations are automatically scored using the evaluation pipeline described in section 3, including syntactic validity checking, keyword matching, and VQA accuracy. GPT-4.1mini (Hurst et al., 2024) is used as the visionlanguage model for all VQA-based evaluations. Max tokens Temperature num_proc time_out num_workers num_gpu_per_worker Cache usage Batch API Hardware Unlimited 0.0 (deterministic) 32 None 5 1 Disabled Disabled NVIDIA RTX A6000 GPU Table 4: Inference configuration"
        },
        {
            "title": "4.2 Main Results",
            "content": "Overall Performance Table 5 summarizes the performance of all evaluated models across the two main task groups: StructEval-T and StructEval-V, each further divided into generation and conversion subtasks. Overall, GPT-4o achieves the highest average score of 76.02% among all 12 models. The best-performing open-source model is Qwen3-4B, with score of 67.04%, trailing GPT-4o by approximately 10 percentage points. While GPT-4o excels particularly in the generation tasks within the StructEval-V category, Qwen3-4B demonstrates consistently strong performance across all task types among open-source models. This likely reflects Qwen3-4Bs robust reasoning capabilities relative to other open-source alternatives. In contrast, the lowest-performing model is phi-3-mini-128k-instruct, with an average score of only 40.79%. Although one might attribute this to its relatively small size of 3.8 billion parameters, model size alone does not fully explain the poor results. For example, phi-3-mini underperforms even compared to similarly sized models such as phi-4-mini-instruct. Notably, it achieves the lowest score in StructEval-T conversion tasks, category where models with strong reasoning abilitiessuch as o1-mini (81.82%) and Qwen3-4B (81.13%)tend to perform well. Error analysis reveals two key failure modes for phi-3-mini-128k-instruct. First, in the TOML-to-YAML conversion task, the model frequently produces malformed closing tags, outputting <END_CODE> instead of the correct <END_CODE>, which significantly penalizes its score. Second, in the CSV-to-JSON conversion task, the model fails to capture hierarchical relationships (e.g., parent-child) specified in the CSV headers, leading to structurally incorrect JSON outputs. These recurring structural errors in StructEval-T conversion tasks substantially contribute to the models overall low performance. 6 Models StructEval-T StructEval-V generation conversion generation conversion Average Open Source Llama-3.1-8B-Instruct (Grattafiori et al., 2024) Meta-Llama-3-8B-Instruct (Grattafiori et al., 2024) Phi-3-mini-128k-instruct (Abdin et al., 2024a) Phi-4-mini-instruct (Abdin et al., 2024b) Qwen2.5-7B-Instruct (Team, 2024) Qwen3-4B (Yang et al., 2025) 60.22 49.18 47.39 51.38 59.21 64.95 Gemini-1.5-pro (Team et al., 2023) Gemini-2.0-flash (Team et al., 2023) GPT-4.1-mini (OpenAI, 2025) GPT-4o (Hurst et al., 2024) GPT-4o-mini (Hurst et al., 2024) o1-mini (Contributors et al., 2024) (o1-mini - Qwen3-4B) Close Source 88.07 72.42 92.57 91.52 79.86 88.12 23. 71.26 53.65 29.78 72.39 62.18 81.13 74.24 72.20 75.63 73.95 75.57 81.82 0.70 54.44 46.61 44.77 51.62 53.28 57.00 58.11 53.62 64.30 65.39 60.77 61.98 4. 61.15 56.91 41.23 52.48 61.43 65.08 66.59 51.97 70.04 73.20 76.54 70.40 5.32 61.77 51.59 40.79 56.97 59.03 67.04 71.75 62.55 75.64 76.02 73.19 75.58 8. Table 5: Main evaluation results of STRUCTEVAL Figure 6: Average score over all models based on the most challenging subtasks Figure 7: Average score over all models based on the four task types Open-Source vs. Closed-Source Models When comparing open-source models and commercial models, we can see that by (closeavg - openavg) value, which is the difference between the average score of commercial source model and open model, that commercial models score is consistently higher than open-source models, this makes sense given the much larger parameters of commercial models by scaling law. We can see that commercial models exceed open-source models on average the most on generation tasks in StructEvalT setting, and the performance gap is smallest on generation tasks in StructEval-V setting. Generation vs. Conversion As shown in Figure 7, comparison between generation and conversion tasks in both StructEval-T and StructEvalV settings reveals that, in general, models perform better on conversion tasks than on generation tasks. An exception to this trend occurs in the StructEvalT setting, where commercial models tend to outperform on generation tasks, while open-source models show the opposite behaviorachieving higher scores on conversion tasks. Under temperature setting of 1, commercial models attain an average score of 75.78% on StructEval-T generation tasks. In contrast, open-source models average only 8.58% on the same tasks for the TOML format. This considerable disparity in TOML generation performance partly explains why commercial models perform better on StructEval-T generation tasks overall. However, the performance gap is not confined to TOMLcommercial models also lead in the other four generation formats within StructEval-T. In the StructEval-V setting, commercial models significantly outperform open-source counterparts on generation tasks involving complex visual formats such as Mermaid and TikZ. These tasks require advanced visual reasoning capabilities, which are more prevalent in multimodal commercial LLMs like GPT-4o and GPT-4o-mini. Subtasks Analysis Meanwhile, several tasks in both in generation and conversion types appear to be saturated, with most models achieving scores exceeding 90%. These include generation tasks for common formats such as JSON, HTML, CSV, 7 Markdown, and YAML, as well as conversion tasks like YAML-to-JSON, React-to-HTML, TOML-toJSON, and Markdown-to-HTML. Such results indicate that LLMs have already mastered many structurally straightforward format transformations. including generation tasks There remain several challenging tasks where all models struggle significantly (shown in like Figure 6), TextTOML, TextSVG, TextMermaid, and TextVega, as well as conversion tasks like YAMLXML, CSVYAML, MatplotlibTikZ, and MarkdownAngular(see scores in subsection A.2). Both closed-source and open-source models achieve low scores on these tasks, which typically require complex structural or visual reasoning. Notably, the performance gap between closed-source and open-source models is even wider on these challenging subtasks, suggesting that proprietary models may have advantages in handling more complex structural representations and transformation logic."
        },
        {
            "title": "5.1 Large Language Models",
            "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities and gained surging popularity in recent years, ever since the release of ChatGPT (OpenAI, 2023). Over the years, open-source models like Llama (Grattafiori et al., 2024), Phi (Abdin et al., 2024b,a), and Qwen (Yang et al., 2024, 2025) developed by companies like Meta, Microsoft, and Alibaba further facilitated widespread integration of AI into diverse workflows and everyday applications. Leveraging their large parameter sizes and extensive post-training, LLMs are capable of performing diverse array of Natural Language Processing (NLP) tasks (Wan et al., 2023). One of the key aspects of the generative capabilities of these models is their ability to generate structured data and transform data from one type to another while maintaining strict adherence to specified formats (Guo et al., 2024). In this paper, we design new and comprehensive benchmark that evaluates the capability of LLMs to understand, generate, and manipulate structured data across range of complex, real-world tasks."
        },
        {
            "title": "5.2 Evaluation of LLMs",
            "content": "Evaluating structured output has become focal point for understanding LLMs limitations (Ning et al., 2025). SoEval (Liu et al., 2024) offers fast, rule-based check for JSON and XML, but its flat schemas fail to reveal errors in deeper hierarchies. StrucText-Eval (Gu et al., 2024) shifts the task to reasoning over structure-rich text (JSON, YAML, LaTeX) rather than generating the structures themselves, while FOFO (Xia et al., 2024) extends to domains such as law and finance yet covers only few formats and still relies on human verification. Developer-focused suites like StackEval (Shah et al., 2024) for HTML, CSS, and plotting libraries, and CodeXGLUE (Lu et al., 2021) for multilingual code tasks remain limited to programming artifacts, and Struc-Bench (Tang et al., 2023) concentrates on tabular generation with bespoke metrics. Each benchmark highlights part of the challengebe it format adherence, domain coverage, or table fidelity. However, none simultaneously demands broad format coverage, automated grading, and robust transformation capabilities. StructEval addresses these gaps by spanning 18 code and non-code formats, unifying generation, completion, and conversion tasks, and scoring outputs with fully automated structural and visionbased metrics, offering comprehensive lens on how well LLMs respect and manipulate complex schemas."
        },
        {
            "title": "5.3 Structured Output Generation",
            "content": "The ability to generate structured outputs is central to many real-world applications of LLMs (Gu et al., 2024; Tang et al., 2023). These outputs are not only expected to be semantically coherent but must also adhere strictly to syntactic and structural constraintsviolations of which can lead to parsing failures, rendering errors, or broken downstream applications. Common tasks include generating JSON for API responses (Geng et al., 2025), YAML or TOML for configuration files (Peddireddy, 2024), HTML or React for UI components (Si et al., 2024), and LaTeX or Markdown for technical writing (Wen et al., 2024). Moreover, in data science, models are used to transform unstructured descriptions into structured formats like CSV or tables for integration into analysis pipelines (Li et al., 2023; Su et al., 2024). In publishing and education, tools that convert textual prompts into diagrams (e.g., using TikZ, SVG, or Mermaid) help automate visualization generation (Lee et al., 2025; Rodriguez et al., 2025; Ku et al., 2025). Despite its significance, structured output generation remains challenging due to the need for models to internalize both syntax rules and hierarchical schema 8 relationships across wide variety of formats. Our STRUCTEVAL first conducts comprehensive evaluation of existing LLMs on both renderable and non-renderable tasks, showing that they still struggle to correctly generate some data formats including TOML, SVG, and Mermaid."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we have comprehensively studied LLMs abilities to generate highly structured content. Having the ability to generate fully structured content is highly useful for many downstream tasks. Our paper is among the first few to provide an evaluation suite for that. Our results indicate that current models are still lagging on the renderable structured content, especially on less frequent format. We advocate that the future models should invest more time to optimize their abilities to generate highly structured output."
        },
        {
            "title": "Limitations",
            "content": "Non-interactive formats Our benchmark focuses on evaluating LLMs ability to generate static visual rendering formats such as HTML, React, Mermaid, etc. While this approach effectively assesses the models capacity to produce well-structured and visually coherent outputs, it is currently limited to single-page, non-interactive formats. The evaluation does not account for dynamic behaviors such as button interactions, page transitions, animations, or scroll events, which are essential to many real world user interfaces. Future work could extend the benchmark to include dynamic rendering tasks, enabling more comprehensive assessment of LLM capabilities in producing fully interactive and responsive user experiences. Expert Review While our dataset underwent two-pass expert review process to ensure correctness, diversity, and minimize potential biases, the initial content was still generated by large language models. Despite expert oversight, residual biases inherent in the model outputs may persist, particularly in subtle or context-dependent scenarios that are challenging to detect through manual review. Moreover, expert validation, while thorough, may not fully capture the wide range of cultural, social, or contextual sensitivities relevant to diverse user populations. Future work could incorporate broader multi-annotator audits or automated bias detection techniques to further enhance dataset reliability and inclusiveness."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Martin Cai, Qin Cai, Vishrav Chaudhary, Dong Chen, Dongdong Chen, and 110 others. 2024a. Phi-3 technical report: highly capable language model locally on your phone. Preprint, arXiv:2404.14219. Marah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell J. Hewett, Mojan Javaheripi, Piero Kauffmann, James R. Lee, Yin Tat Lee, Yuanzhi Li, Weishung Liu, Caio C. T. Mendes, Anh Nguyen, Eric Price, Gustavo de Rosa, Olli Saarikivi, and 8 others. 2024b. Phi-4 technical report. Preprint, arXiv:2412.08905. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph Gonzalez, and Ion Stoica. 2024. Chatbot arena: An open platform for evaluating llms by human preference. ArXiv, abs/2403.04132. Foundational Contributors, Ahmed El-Kishky, Daniel Selsam, Francis Song, Giambattista Parascandolo, Hongyu Ren, Hunter Lightman, Hyung Won, Ilge Akkaya, Ilya Sutskever, Jason Wei, Jonathan Gordon, Karl Cobbe, Kevin Yu, Lukasz Kondraciuk, Max Schwarzer, Mostafa Rohaninejad, Noam Brown, Shengjia Zhao, and 189 others. 2024. Openai o1 system card. ArXiv, abs/2412.16720. Saibo Geng, Hudson Cooper, Michał Moskal, Samuel Jenkins, Julian Berman, Nathan Ranchin, Robert West, Eric Horvitz, and Harsha Nori. 2025. Generating structured outputs from language models: Benchmark and studies. arXiv preprint arXiv:2501.10868. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, and 542 others. 2024. The llama 3 herd of models. Preprint, arXiv:2407.21783. Zhouhong Gu, Haoning Ye, Xingzhou Chen, Zeyang Zhou, Hongwei Feng, and Yanghua Xiao. 2024. Structext-eval: Evaluating large language models reasoning ability in structure-rich text. arXiv preprint arXiv:2406.10621. Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh Chawla, Olaf Wiest, and Xiangliang Zhang. 2024. Large language model based multi-agents: survey of progress and challenges. arXiv preprint arXiv:2402.01680. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan 9 Liu, and Maosong Sun. 2024. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. In Annual Meeting of the Association for Computational Linguistics. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Xiaodong Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. ArXiv, abs/2009.03300. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Xiaodong Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. ArXiv, abs/2103.03874. OpenAI Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander Mkadry, Alex Baker-Whitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alexander Kirillov, Alex Nichol, Alex Paino, and 397 others. 2024. Gpt-4o system card. ArXiv, abs/2410.21276. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. 2024. Livecodebench: Holistic and contamination free evaluation of large language models for code. ArXiv, abs/2403.07974. Dongfu Jiang. 2024. Llm-engines: unified and parallel inference engine for large language models. https://github.com/jdf-prog/LLM-Engines. Max W.F. Ku, Thomas Chong, Jonathan Leung, Krish Shah, Alvin Yu, and Wenhu Chen. 2025. Theoremexplainagent: Towards multimodal explanations for llm theorem understanding. ArXiv, abs/2502.19400. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. Jaewook Lee, Jeongah Lee, Wanyong Feng, and Andrew Lan. 2025. From text to visuals: Using llms to generate math diagrams with vector graphics. ArXiv, abs/2503.07429. Peng Li, Yeye He, Dror Yashar, Weiwei Cui, Song Ge, Haidong Zhang, Danielle Rifinski Fainman, Dongmei Zhang, and Surajit Chaudhuri. 2023. Tablegpt: Table-tuned gpt for diverse table tasks. ArXiv, abs/2310.09263. Jian Liu, Jian Wang, Wei Zhang, and Ming Li. 2024. Are llms good at structured outputs? benchmark for evaluating structured output generation. Information Processing & Management, 61(5):103809. Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin B. Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, and 3 others. 2021. Codexglue: machine learning benchmark dataset for code understanding and generation. arXiv preprint arXiv:2102.04664. Kun-Peng Ning, Shuo Yang, Yu-Yang Liu, Jia-Yu Yao, Zhen-Hui Liu, Yong-Hong Tian, Yibing Song, and Li Yuan. 2025. Pico: Peer review in llms based on the consistency optimization. Preprint, arXiv:2402.01830. OpenAI. 2023. Chat generative pre-trained transformer (chatgpt). https://www.openai.com/. OpenAI. 2025. Introducing gpt-4.1 in the api. Accessed: 2025-05-20. Abhiram Reddy Peddireddy. 2024. Effective workflow automation in github: Leveraging bash and yaml. Journal of Artificial Intelligence & Cloud Computing. Juan Rodriguez, Abhay Puri, Shubham Agarwal, Issam Laradji, Sai Rajeswar, David Vazquez, Christopher Pal, and Marco Pedersoli. 2025. Starvector: Generating scalable vector graphics code from images and text. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 29691 29693. Nidhish Shah, Zulkuf Genc, and Dogu Araci. 2024. Stackeval: Benchmarking llms in coding assistance. In Advances in Neural Information Processing Systems 37 (NeurIPS 2024), Datasets and Benchmarks Track. Chenglei Si, Yanzhe Zhang, Ryan Li, Zhengyuan Yang, Ruibo Liu, and Diyi Yang. 2024. Design2code: Benchmarking multimodal code generation for auarXiv preprint tomated front-end engineering. arXiv:2403.03163. Aofeng Su, Aowen Wang, Chaonan Ye, Chengcheng Zhou, Ga Zhang, Gang Chen, Guangcheng Zhu, Haobo Wang, Haokai Xu, Hao Chen, Haoze Li, Haoxuan Lan, Jiaming Tian, Jing Yuan, Junbo Zhao, Junlin Zhou, Kaizhe Shou, Liangyu Zha, Lin Long, and 14 others. 2024. Tablegpt2: large multimodal model with tabular data integration. ArXiv, abs/2411.02059. Xiangru Tang, Yiming Zong, Jason Phang, Yilun Zhao, Wangchunshu Zhou, Arman Cohan, and Mark Gerstein. 2023. Struc-bench: Are large language models really good at generating complex structured data? arXiv preprint arXiv:2309.08963. Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, and 1 others. 2023. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805. 2024. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. ArXiv, abs/2406.15877. Qwen Team. 2024. Qwen2.5: party of foundation models. Maxim Tkachenko, Mikhail Malyuk, Andrey 2020Data labeling softOpen source software available from Holmanyuk, 2025. ware. https://github.com/HumanSignal/label-studio. and Nikolai Liubimov. Label Studio: Zhen Wan, Fei Cheng, Zhuoyuan Mao, Qianying Liu, Haiyue Song, Jiwei Li, and Sadao Kurohashi. 2023. Gpt-re: In-context learning for relation extraction using large language models. Preprint, arXiv:2305.02105. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max W.F. Ku, Kai Wang, Alex Zhuang, Rongqi \"Richard\" Fan, Xiang Yue, and Wenhu Chen. 2024. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. ArXiv, abs/2406.01574. Haomin Wen, Zhenjie Wei, Yan Lin, Jiyuan Wang, Yuxuan Liang, and Huaiyu Wan. 2024. Overleafcopilot: Empowering academic writing in overleaf with large language models. ArXiv, abs/2403.09733. Congying Xia, Chen Xing, Jiangshu Du, Xinyi Yang, Yihao Feng, Ran Xu, Wenpeng Yin, and Caiming Xiong. 2024. FOFO: benchmark to evaluate LLMs format-following capability. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 680699, Bangkok, Thailand. Association for Computational Linguistics. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, and 41 others. 2025. Qwen3 technical report. Preprint, arXiv:2505.09388. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, and 43 others. 2024. Qwen2 technical report. Preprint, arXiv:2407.10671. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. 2023. Instruction-following evaluation for large language models. ArXiv, abs/2311.07911. Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, Simon Brunner, Chen Gong, Thong Hoang, Armel Randy Zebaze, Xiao ke Hong, Wen-Ding Li, Jean Kaddour, Minglian Xu, Zhihan Zhang, and 14 others."
        },
        {
            "title": "A Example Appendix",
            "content": "A.1 Task Distributions Subset Tasks # Examples StructEval-T StructEval-V StructEval-T StructEval-V Generation Text JSON Text CSV Text TOML Text XML Text YAML Text Angular Text Canvas Text HTML Text LaTeX Text Markdown Text Matplotlib Text Mermaid Text React Text SVG Text TikZ Text Typst Text Vega Text Vue Conversion CSV JSON JSON CSV XML JSON JSON XML YAML JSON JSON YAML XML CSV CSV XML XML YAML YAML XML YAML CSV TOML JSON CSV YAML TOML YAML Matplotlib TikZ Markdown HTML HTML React React HTML Vue HTML HTML Vue Markdown React HTML Angular Markdown Vue Vue React Markdown Angular React Angular 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 100 50 45 45 40 40 30 30 25 15 10 Table 6: Statistics of number examples for each task in all the 4 subsets of STRUCTEVAL. 12 A.2 Subtask Performance Model Llama-3.1-8B-Instruct Meta-Llama-3-8B-Instruct Phi-3-mini-128k-Instruct Phi-4-mini-Instruct Qwen-2.5-7B-Instruct Qwen-3-4B Gemini-1.5-pro Gemini-2.0-flash GPT-4.1-mini GPT-4o GPT-4o-mini o1-mini TJSO 78.82 69.08 68.84 51.50 84.40 90.96 94.06 48.88 99.26 99.36 97.88 92.56 TCSV 81.68 45.04 93.50 82.56 90.62 76.44 100.00 98.40 99.92 100.00 99.90 99.24 TTO 6.76 7.94 0.00 16.12 13.22 7.44 75.38 78.78 91.34 90.22 29.56 89.40 TX 59.38 45.30 37.68 40.20 61.30 71.16 73.32 44.60 77.06 70.32 75.10 71.12 TYA 74.44 78.54 36.92 66.54 46.52 78.74 97.58 91.44 95.26 97.68 96.84 88. Table 7: StructEval-T Generation Scores Avg. 60.22 49.18 47.39 51.38 59.21 64.95 88.07 72.42 92.57 91.52 79.86 88.12 Model Llama-3.1-8B-Instruct Meta-Llama-3-8B-Instruct Phi-3-mini-128k-Instruct Phi-4-mini-Instruct Qwen-2.5-7B-Instruct Qwen-3-4B Gemini-1.5-pro Gemini-2.0-flash GPT-4.1-mini GPT-4o GPT-4o-mini o1-mini Model Llama-3.1-8B-Instruct Meta-Llama-3-8B-Instruct Phi-3-mini-128k-Instruct Phi-4-mini-Instruct Qwen-2.5-7B-Instruct Qwen-3-4B Gemini-1.5-pro Gemini-2.0-flash GPT-4.1-mini GPT-4o GPT-4o-mini o1-mini TAng. 61.22 48.92 48.28 62.60 63.08 48.80 90.62 44.28 84.52 87.42 86.72 89.30 TH 95.96 72.52 92.10 97.24 92.92 98.80 99.30 99.26 99.30 99.22 99.02 99.44 TLATE 78.04 68.40 63.88 72.92 66.68 72.60 76.94 75.26 76.20 75.18 78.44 49.24 TM 87.34 72.06 64.16 88.90 81.02 92.80 94.00 92.06 91.80 93.02 94.36 92.08 TMPL 80.52 56.54 59.38 71.30 74.70 89.54 84.96 75.34 96.34 95.76 95.36 96.06 TReact 64.30 55.24 44.12 58.46 65.48 77.06 33.68 46.64 69.58 74.66 75.46 71.98 Table 8: StructEval-V Generation Scores (Part 1) TMermaid 9.02 6.04 11.12 9.30 6.16 13.62 15.94 9.66 43.46 36.00 30.50 27. TTypst 23.38 29.46 22.90 42.22 33.44 9.92 11.60 45.28 9.96 23.94 9.96 9.98 TVega 28.36 30.74 35.56 34.72 30.56 45.28 65.18 29.74 48.28 72.20 41.28 65.68 TVue 57.90 66.50 39.84 29.48 37.90 29.42 29.66 32.46 38.44 40.04 33.66 40. Table 9: StructEval-V Generation Scores (Part 2) TSV 44.18 40.16 35.78 39.72 47.30 53.44 54.72 56.72 58.74 56.78 53.98 58.12 TCanvas 30.56 31.28 32.50 28.90 44.52 54.28 29.36 29.16 49.60 33.54 30.50 33. TTikZ 46.92 28.04 32.44 35.28 48.88 55.38 69.44 61.24 69.74 62.32 60.76 71.86 Avg. 54.44 46.61 44.77 51.62 53.28 57.00 58.11 53.62 64.30 65.39 60.77 61.98 13 Model Llama-3.1-8B-Instruct Meta-Llama-3-8B-Instruct Phi-3-mini-128k-Instruct Phi-4-mini-Instruct Qwen-2.5-7B-Instruct Qwen-3-4B Gemini-1.5-pro Gemini-2.0-flash GPT-4.1-mini GPT-4o GPT-4o-mini o1-mini Model Llama-3.1-8B-Instruct Meta-Llama-3-8B-Instruct Phi-3-mini-128k-Instruct Phi-4-mini-Instruct Qwen-2.5-7B-Instruct Qwen-3-4B Gemini-1.5-pro Gemini-2.0-flash GPT-4.1-mini GPT-4o GPT-4o-mini o1-mini Model Llama-3.1-8B-Instruct Meta-Llama-3-8B-Instruct Phi-3-mini-128k-Instruct Phi-4-mini-Instruct Qwen-2.5-7B-Instruct Qwen-3-4B Gemini-1.5-pro Gemini-2.0-flash GPT-4.1-mini GPT-4o GPT-4o-mini o1-mini Model Llama-3.1-8B-Instruct Meta-Llama-3-8B-Instruct Phi-3-mini-128k-Instruct Phi-4-mini-Instruct Qwen-2.5-7B-Instruct Qwen-3-4B Gemini-1.5-pro Gemini-2.0-flash GPT-4.1-mini GPT-4o GPT-4o-mini o1-mini CJSO 34.14 31.40 24.88 45.42 31.36 55.28 48.14 25.72 55.52 38.56 58.52 58.46 JCSV 95.96 48.00 87.28 97.62 95.74 100.00 100.00 100.00 100.00 99.74 100.00 100. XJSO 68.62 69.24 8.00 89.56 33.14 92.84 40.14 32.60 38.68 66.46 73.26 82.70 JX 56.02 55.40 12.40 61.90 31.04 65.98 67.14 69.76 69.76 69.76 65.98 68.60 YJSO 94.00 90.00 23.20 100.00 50.00 100.00 98.00 100.00 100.00 100.00 98.00 100.00 Table 10: StructEval-T Conversion Scores (Part 1) CX 20.20 17.28 9.50 21.72 18.12 24.82 27.14 17.74 29.36 27.40 29.62 29.26 XYA 86.96 54.48 20.56 60.00 81.62 94.10 42.96 59.02 59.18 44.28 40.20 88.62 YX 39.90 38.12 22.42 48.28 24.16 48.68 47.56 46.36 48.36 48.76 48.76 48.36 YCSV 88.32 61.90 87.58 84.14 97.62 98.94 100.00 100.00 100.00 100.00 98.10 100. TomlJSO 86.90 63.38 8.80 86.02 78.22 96.92 99.76 99.26 100.00 100.00 100.00 100.00 CYA 49.54 36.50 19.10 66.22 70.86 65.08 71.40 63.18 60.82 43.20 50.00 72. Table 11: StructEval-T Conversion Scores (Part 2) RH 88.36 86.82 70.73 92.27 89.29 95.53 95.24 93.02 95.22 95.36 95.07 95.09 VH 84.65 85.23 73.85 81.82 79.53 89.65 91.27 88.67 90.12 90.55 91.58 89.65 DReact 43.23 33.73 30.80 28.50 34.70 54.23 34.83 32.37 52.87 74.20 80.40 58.37 LAng. 60.90 52.83 32.77 33.47 68.67 55.10 86.43 29.30 81.97 87.17 87.73 87.90 Table 12: StructEval-V Conversion Scores (Part 1) T DH 88.28 84.52 65.60 92.44 85.16 90.20 95.28 96.60 96.40 95.32 93.14 94.48 LReact 55.02 73.91 42.16 57.11 69.20 65.31 40.62 41.04 88.09 88.31 88.42 72.18 LVue H 72.93 75.28 34.65 41.05 80.02 83.05 86.65 67.77 46.28 62.55 79.75 77.77 VReact 75.73 62.73 33.00 55.87 50.87 68.13 64.00 68.00 86.47 78.93 81.20 65.60 DAng. 26.90 33.10 25.10 26.50 35.00 34.50 49.80 28.20 49.10 48.20 49.20 41.20 Table 13: StructEval-V Conversion Scores (Part 2) JYA 92.52 74.00 32.80 100.00 95.24 98.00 100.00 100.00 100.00 100.00 100.00 100.00 TomlYA 85.62 63.18 26.46 61.84 85.68 95.36 97.36 97.36 97.36 97.36 97.36 97.36 DVue 36.36 29.52 27.32 33.88 33.80 34.64 30.96 32.00 31.96 37.56 31.96 36.80 RAng. 85.20 57.00 41.60 71.20 84.60 85.00 85.20 29.20 85.20 80.60 97.60 85.20 XCSV 98.98 48.26 33.92 90.70 77.72 99.78 99.78 99.78 99.78 99.78 98.22 99.78 Avg. 71.26 53.65 29.78 72.39 62.18 81.13 74.24 72.20 75.63 73.95 75.57 81.82 MPLTikZ 16.26 8.29 17.15 15.70 26.32 25.64 38.82 17.46 36.80 39.69 42.47 40.60 Avg. 61.15 56.91 41.23 52.48 61.43 65.08 66.59 51.97 70.04 73.20 76.54 70.40 * - Text, CSV, JSON, XML, YAML, Ang. Angular, MD Markdown, MPL Matplotlib, React, Vue. 14 A.3 Examples of rendered image (a) Angular (b) Matplotlib (c) Mermaid (d) React (e) SVG (f) TikZ (g) Vega Figure 8: Example images rendered in STRUCTEVAL tasks. 15 A.4 Task Generation Prompt"
        },
        {
            "title": "Sample Prompt",
            "content": "You are prompt-design assistant building benchmark items for conversion tasks. Input Format: {input_type} Output Format: {output_type} Your task: Think silently through the checklist and then output single JSON object with: \"raw_output_metric\": dot-paths for the expected keys/attributes in the {output_type} structure \"query\": generated input format {input_type} code inside <code>...</code> tags. Assumed Mapping Rule (state it implicitly in the paths): No XML attributes unless absolutely necessary. If an attribute is required, map it to key prefixed with \"@\", and include that in dot-paths. CHECKLIST (INTERNAL DO NOT OUTPUT) 1. Pick super creative and random domain. 2. Generate {input_type} code with: At least two levels of nesting At least one list inside an object/element 3. Avoid XML attributes where possible; prefer child elements. 4. Wrap the code in <code>...</code> tags. 5. Dot-path rules: JSON / YAML / TOML: parent.child, list[0].child XML: element.child or element.@attr (only if used) CSV: csv::Header (not used here)"
        },
        {
            "title": "OUTPUT FORMAT",
            "content": "{ } \"raw_output_metric\": [\"<dot_path1>\", \"<dot_path2>\", ...], \"query\": \"<code>...</code>\" Figure 9: Example task generation prompt"
        }
    ],
    "affiliations": [
        "HKUST",
        "Independent Contributor",
        "Shanghai University",
        "University of Toronto",
        "University of Waterloo",
        "Vector Institute"
    ]
}