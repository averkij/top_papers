{
    "paper_title": "Option-aware Temporally Abstracted Value for Offline Goal-Conditioned Reinforcement Learning",
    "authors": [
        "Hongjoon Ahn",
        "Heewoong Choi",
        "Jisu Han",
        "Taesup Moon"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Offline goal-conditioned reinforcement learning (GCRL) offers a practical learning paradigm where goal-reaching policies are trained from abundant unlabeled (reward-free) datasets without additional environment interaction. However, offline GCRL still struggles with long-horizon tasks, even with recent advances that employ hierarchical policy structures, such as HIQL. By identifying the root cause of this challenge, we observe the following insights: First, performance bottlenecks mainly stem from the high-level policy's inability to generate appropriate subgoals. Second, when learning the high-level policy in the long-horizon regime, the sign of the advantage signal frequently becomes incorrect. Thus, we argue that improving the value function to produce a clear advantage signal for learning the high-level policy is essential. In this paper, we propose a simple yet effective solution: Option-aware Temporally Abstracted value learning, dubbed OTA, which incorporates temporal abstraction into the temporal-difference learning process. By modifying the value update to be option-aware, the proposed learning scheme contracts the effective horizon length, enabling better advantage estimates even in long-horizon regimes. We experimentally show that the high-level policy extracted using the OTA value function achieves strong performance on complex tasks from OGBench, a recently proposed offline GCRL benchmark, including maze navigation and visual robotic manipulation environments."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 1 7 3 7 2 1 . 5 0 5 2 : r Option-aware Temporally Abstracted Value for Offline Goal-Conditioned Reinforcement Learning Hongjoon Ahn1, Heewoong Choi1, Jisu Han1 , and Taesup Moon2 1 Department of Electrical and Computer Engineering (ECE), Seoul National University 2 Department of ECE / IPAI / ASRI / INMC, Seoul National University {hong0805, chw0501}@snu.ac.kr {jshcdi6658}@gmail.com {tsmoon}@snu.ac.kr"
        },
        {
            "title": "Abstract",
            "content": "Offline goal-conditioned reinforcement learning (GCRL) offers practical learning paradigm where goal-reaching policies are trained from abundant unlabeled (reward-free) datasets without additional environment interaction. However, offline GCRL still struggles with long-horizon tasks, even with recent advances that employ hierarchical policy structures, such as HIQL [34]. By identifying the root cause of this challenge, we observe the following insights: First, performance bottlenecks mainly stem from the high-level policys inability to generate appropriate subgoals. Second, when learning the high-level policy in the long-horizon regime, the sign of the advantage signal frequently becomes incorrect. Thus, we argue that improving the value function to produce clear advantage signal for learning the high-level policy is essential. In this paper, we propose simple yet effective solution: Option-aware Temporally Abstracted value learning, dubbed OTA, which incorporates temporal abstraction into the temporal-difference learning process. By modifying the value update to be option-aware, the proposed learning scheme contracts the effective horizon length, enabling better advantage estimates even in long-horizon regimes. We experimentally show that the high-level policy extracted using the OTA value function achieves strong performance on complex tasks from OGBench [33], recently proposed offline GCRL benchmark, including maze navigation and visual robotic manipulation environments."
        },
        {
            "title": "Introduction",
            "content": "Offline goal-conditioned reinforcement learning (GCRL) has emerged as practical framework for real-world applications by leveraging pre-collected datasets to train goal-reaching policies without requiring additional environment interaction [24, 33]. However, learning an accurate goal-conditioned value function in long-horizon settings remains major challenge: Naively training goal-conditioned value function often leads to noisy estimates and erroneous policies [36, 21, 34]. To mitigate the learning of an erroneous policy, Hierarchical Implicit Q-Learning (HIQL) [34], one of the state-of-theart methods, adopts simple hierarchical structure in which high-level policy predicts subgoals, and low-level policy learns to execute actions toward those subgoals. In such hierarchical structure, the separated policy extraction with same value function has different objectives for each policy. Though hierarchical policy is still extracted from the noisy value function, both policies receive more reliable learning signals than when training flat, non-hierarchical policy. However, despite reasonable performance gains of hierarchical methods in some long-horizon environments, recent Equal Contribution Work completed during an internship at M.IN.D Lab in SNU. Preprint. Under review. challenging benchmark [33] reveals that such hierarchical policy still cannot solve more complex tasks, such as long-horizon robotic locomotion or robotic manipulation. To understand the failure in complex tasks more deeply, we raise the following question: Lowlevel policy vs. high-level policy: Which is the Bottleneck of HIQL? To answer this question, we analyze the hierarchical policy in failure cases by generating oracle subgoals for the low-level policy. Interestingly, we observe that the low-level policy achieves subgoals with high accuracy, indicating that the failure stems from the inability of the high-level policy to generate appropriate subgoals. The main reason for the limited performance of the high-level policy is that the noisy value function still cannot provide useful learning signals for training the high-level policy in long-horizon scenarios. Based on the phenomenon that the high-level policy eventually failed to obtain useful learning signals from the value function, the main cause of noisy learning signals is the order inconsistency of the learned value function in the long-horizon setting. Our analysis reveals that when the distance between the state and the goal exceeds certain temporal horizon, critical issue arises with the advantage signal. In the long-horizon regime, it is frequently observed that the sign of the advantage signal is incorrect, causing erroneous regression weights for learning the high-level policy. Considering the issue with the value function, we argue that designing value function that can produce clear advantage signal for extracting the high-level policy is necessary. Motivated by the observation that the low-level policy performs remarkably well at reaching shorthorizon subgoals, we propose simple yet effective value function learning scheme for high-level policy extraction by reducing the horizon between the state and the goal. Leveraging the notion of option [46], also known as macro actions consisting of sequence of primitive actions, we redesign the value learning scheme to be option-aware. Specifically, by updating the value over sequence of primitive actions, the effective horizon between the state and the goal is significantly reduced compared to the primitive action-aware value learning scheme [22]. In our experiment, we show that our value learning scheme effectively mitigates the errors of the value function in the long-horizon regime. Furthermore, we evaluate our approach in various tasks, including maze and robotic visual manipulation environments from OGBench [33], and empirically demonstrate that using our value function to extract the high-level policy yields superior performance on long-horizon tasks compared to baselines. In summary, our contributions are threefold: Through analysis of the failure cases of hierarchical policies, we identify that the failures stem from the inability of the high-level policy to generate appropriate subgoals. Furthermore, we observe that the value function used for extracting the high-level policy has significant errors when the distance between the state and the goal is large. To tackle this problem, we propose Option-aware Temporally Abstracted (OTA) value learning, which produces reduced horizon compared to the conventional value learning objective [22]. In our experiments, we demonstrate that, despite long state-to-goal horizons, our value function yields significantly lower errors, and the hierarchical policy extracted using our value function successfully solves complex maze and robotic manipulation tasks."
        },
        {
            "title": "2 Related Work",
            "content": "GCRL. GCRL aims to train goal-conditioned policies to reach arbitrary goal states from given initial states, rather than optimizing for single, fixed task [43, 26]. Our work focuses specifically on offline GCRL [4, 27, 53, 34, 44, 33], in which goal-conditioned policies are learned entirely from pre-collected datasets without further environment interaction. Due to the sparse rewards in goalreaching tasks, offline GCRL has relied on hindsight data relabeling [1, 41, 56], and more recently, imitation learning and value-based methods have been explored to better leverage suboptimal datasets [6, 12, 53, 13]. In these works, the value function is typically learned through temporal-difference (TD) methods [22, 35], or through alternative techniques such as state-occupancy matching [27, 7], contrastive learning [28, 9, 25] and quasimetric learning [51]. However, whether the value functions can effectively generalize to long-horizon tasks remains an open question [33]. Hierarchical RL. Achieving long-horizon goals remains fundamental challenge in GCRL [38, 36, 21, 34]. To address this, hierarchical RL methods have adopted either graph-based planning in the 2 state space [8, 17, 55, 21, 54, 20] or waypoint-based subgoal generation [5, 23, 48, 29, 18, 14, 32, 31, 19, 3, 34, 52]. However, graph-based planning methods incur high computational overhead and architectural complexity. Waypoint-based approaches also face challenges in generating effective subgoals in long-horizon settings, due to inaccurate value estimates when the state is far from the goal. Option framework. To enhance the planning capabilities of an agent over long time horizons, one effective approach is to leverage temporal abstraction through the option framework, which involves learning sub-policies known as options [15, 46, 42, 36]. In this framework, options serve as temporally extended actions that enable planning across multiple time scales. After establishing the theoretical connection between the option framework and semi-Markov decision processes [46], research has progressed toward end-to-end option learning [40, 45, 2, 47] and automatic option discovery [39, 16]. Our method is closely related to HIQL [34], which trains high-level policy to generate subgoals. However, unlike HIQL, our approach leverages options defined in offline datasets to effectively reduce the planning horizon during value function training. As result, our high-level policy can generate subgoals over longer temporal horizons without relying on explicit option learning or option discovery."
        },
        {
            "title": "3 Preliminaries",
            "content": "Problem setting. Offline GCRL is defined over Markov Decision Process (MDP), consisting of (S, A, G, r, γ, p0, p) in which denotes the state space, the action space, the goal space, r(s, g) the goal-conditioned reward function for state and goal , γ the discount factor, p0() the initial state distribution, and p(s, a) the environment transition dynamics for state and action A. We also denote (s, g) as the goal-conditioned value function at state given goal g. We assume that the goal space is the same as the state space (i.e., = G). An offline dataset consists of trajectories τ = (s0, a0, s1, . . . , sT ), each sampled from an unknown behavior policy µ. The objective is to learn an optimal goal-conditioned policy π(as, g) that maximizes the expected cumulative return (π) = Eτ pπ(τ ),gp(g)[(cid:80)T t=0 γtr(st, g)], where pπ(τ ) = p0(s0)ΠT 1 t=0 p(st+1st, at)π(atst, g), and p(g) is goal distribution. Hierarchical Implicit Q-Learning (HIQL). In GCRL, accurately estimating the value function for distant goals is the main challenge in solving complex long-horizon tasks [17, 21, 34]. To address this issue, HIQL [34] proposed hierarchical policy structure that utilizes value function learned with IQL [22]. This hierarchical design enables the agent to produce effective actions even when value estimates for distant goals are noisy or unreliable. More specifically, HIQL trains goal-conditioned state-value function with the following loss: (cid:2)Lτ (cid:0)r(s, g) + γ (s, g) (s, g)(cid:1)(cid:3) , L(V ) = E(s,s)D, gp(g) (1) 2 2 (u) = τ 1(u < 0)u2, with τ > 0.5, and denotes where the expectile loss is defined as Lτ the target network. Following prior works [1, 8, 3, 51, 34, 52], we adopt the sparse reward r(s, g) = 1{s = g}. Under this reward, the optimal value (s, g) corresponds to the discounted temporal distance, i.e., discounted measure of the minimum number of environment steps required to reach the goal from state s. HIQL separates policy extraction into two levels: high-level policy πh(st+kst, g) generates k-step subgoal to guide progress toward the goal, while low-level policy πℓ(atst, st+k) produces primitive actions to reach the subgoal. Both policies are learned using advantage-weighted regression (AWR) [49, 37, 30] with the following objective: (πh) = E(st,st+k,g)D (πℓ) = E(st,at,st+1,st+k)D (cid:2)exp (cid:0)βh Ah(st, st+k, g)(cid:1) log πh(st+kst, g)(cid:3) , (cid:2)exp (cid:0)βℓ Aℓ(st, st+1, st+k)(cid:1) log πℓ(atst, st+k)(cid:3) , (2) (3) where βh and βl are inverse temperature parameters, Ah(st, st+k, g) = h(st+k, g) h(st, g) denotes the high-level policy advantage, and Aℓ(st, st+1, st+k) = ℓ(st+1, st+k) ℓ(st, st+k) denotes the low-level policy advantage. HIQL uses single goal-conditioned value function , which is shared between both πh and πℓ (i.e., = ℓ = ). However, despite this design, HIQL still struggles with long-horizon, complex tasks, as shown in the recent offline GCRL benchmark, OGBench [33]. 3 Figure 1: High-level policy is problematic. We evaluate HIQL by varying only the high-level policy while keeping the low-level policy fixed. The x-axis denotes different tasks under maze sizes and data types (See Section 6.1 for task details). Using learned high-level policy (HIQL, π = πℓ πh), performance drops, whereas using the oracle high-level policy (HIQLOS, π = πℓ πh oracle) achieves high success rates, indicating the high-level policy is the main bottleneck."
        },
        {
            "title": "4.1 Low-Level Policy vs. High-Level Policy: Which is the Bottleneck of HIQL?",
            "content": "In this subsection, we investigate the failure cases of HIQL in long-horizon scenarios by identifying whether the main performance bottleneck is in the low-level policy or the high-level policy. To examine this, we fix the low-level policy πℓ and replace the high-level policy πh with an oracle policy, πh oracle, which always provides optimal subgoals reachable within short horizon.3 We refer to this variant as HIQLOS, and pose the following hypothesis: if HIQLOS still fails in long-horizon tasks, then the low-level policy struggles to reach short-horizon subgoals. Conversely, if it achieves high success rate, the main problem lies in the high-level policy. Figure 1 shows the results of HIQL and HIQLOS on eight challenging maze navigation tasks from OGBench [33]. HIQL achieves less than 20% success rate on many tasks, indicating that HIQL significantly fails to solve the long-horizon tasks. In contrast, we note that HIQLOS achieves much higher success rate around 90%. These results indicate that, while the low-level policy generalizes well in short-horizon settings when provided with accurate subgoals, the inaccuracy of the high-level policy is the primary cause of HIQLs failure in long-horizon scenarios. We identify two potential issues in Equation (2) that may underlie the failure of high-level policy learning: 1) an inadequate policy extraction scheme (i.e., the regression component in Equation (2)), and 2) an inaccurately learned value function (i.e., the advantage term in Equation (2)). Since the same policy extraction scheme enables successful low-level policy learning, we do not consider it to be the primary cause of failure. This suggests that the inaccurate value function used in the high-level policy advantage term may be the key contributor to the failure. In particular, as the distance between st and increases, the value estimates become increasingly erroneous, leading to an imprecise evaluation of the high-level advantage. Although HIQL attempts to mitigate the noise in estimating the long-horizon value through its hierarchical structure, it is possible that the high-level advantage may still be significantly erroneous. In the following subsection, we carefully analyze how such errors in estimating adversely affect high-level policy learning. 4.2 Order Inconsistency of the Learned Value Function in the Long-Horizon Setting Before analyzing the learned in HIQL, we first define order consistency of the value function. Definition 4.1. (Order consistency) Let τ = (s0, s1, . . . , sT = g) denote the optimal trajectory induced by the optimal policy π( s, g), from the initial state s0 to the goal g, and let be learned value function. Given si, sj τ with > i, we say that satisfies order consistency with respect to (si, sj, g) if and only if (sj, g) > (si, g). Consider an optimal trajectory τ = {s0, s1, . . . , sT }, generated by an oracle policy. Along this trajectory, the optimal value function is increasing, such that (sj, g) > (si, g) for all > i. 3Specifically, πh oracle provides as subgoal the center of an adjacent maze cell that lies on the shortest path from the current state to the goal. 4 Figure 2: Value order inconsistency in long-horizon settings. (Left) We collect optimal trajectories ). (Middle) At each state along the trajectory, we compare from the initial state ( ) to the goal ( the high-level value from HIQL (V h) and the optimal (V ). (Right) To better illustrate value order consistency, we convert the values into temporal distances: HIQL (dh) and the optimal (d). Thus, value order consistency refers to the alignment between the order induced by and that induced by . We argue that achieving the order consistency between (st, g) and (st+k, g) is critical, as sign mismatches can invert the high-level advantage signal Ah and hinder the learning of an appropriate high-level policy. To check whether the learned of HIQL achieves the order consistency or not, we have collected optimal trajectories across four different long-horizon tasks with specified goals using near-optimal policies, as illustrated in Figure 2. The trajectory lengths varied from 250 to 2000 steps. For each state st in the trajectory, we then visualize the learned value h(st, g) alongside the optimal value function, computed as (st, g) = (cid:0)1 γd(st,g)(cid:1) / (1 γ), in which d(st, g) denotes the temporal distance between st and g. Since the value decays exponentially as the distance to the goal increases due to the discount factor γ, it becomes difficult to visually interpret the relative differences in value. Hence, we convert h(s, g) into estimated temporal distances using the following equation: dh(s, g) = log (cid:0)1 + (1 γ)V h(s, g)(cid:1) / log γ. In this form, the condition for value order consistency is equivalent to dh(si, g) > dh(sj, g), where > i. As shown in Figure 2, we note that closely matches when the state is near the goal (i.e., d(s, g) 0). This alignment explains the strong performance of the low-level policy presented in Figure 1. However, when the state-goal distance exceeds certain temporal horizon, the value order inconsistency frequently arises between h(st, g) and h(st+k, g) due to the non-monotonicity of the learned h.4 This is due to the well-known fact that the learning target for the value in Equation (1) becomes noisier as the horizon becomes longer, and shows why the use of becomes less effective in high-level policy learning for long-horizon setting. Motivated by the observation that aligns well with and achieves order consistency in shorthorizon settings, in the next section, we propose simple yet effective solution based on temporal abstraction [46]. This approach enables high-level value function learning to provide appropriate advantage signals, even when d(s, g) is large."
        },
        {
            "title": "5 Option-aware Temporally Abstracted (OTA) Value",
            "content": "In this section, we propose simple solution for learning h(s, g) by leveraging options [46] to reduce the horizon length. Option, also known as macro actions, is sequences of primitive actions that enable temporal abstraction. In our offline RL setting, an option starting from the state st corresponds to sequence of actions (at, at+1, . . . , at+n1), which are extracted from trajectories in the offline dataset. By using temporally extended actions in planning, we reduce the effective 4The hyperparameter in HIQL is chosen based on the characteristics of the environments and datasets. In Figure 2, = 25 for the AntMaze task and = 100 for the HumanoidMaze task. 5 (Left) OTA achieves temporal abstraction by Figure 3: Option-aware temporal abstraction. computing the reward and target value from the state reached after executing the option (i.e., sΩ). (Right) By leveraging temporal abstraction, OTA provides clear high-level advantage estimates, particularly in long-horizon tasks. horizon length, referring to the number of planning steps, to approximately d(st, g)/n. Therefore, to ensure that the high-level value is suitable for long-term planning, we modify the reward and target value in Equation (1) to be option-aware. More specifically, for given abstraction factor and goal g, we define an option Ωn,g = (I, µ, βn,g), where = is the initiation set, µ is the behavior policy used to collect the offline dataset D, and βn,g is timeout-based termination condition that ends the option after steps or upon reaching g. Let s(Ωn,g, st) denote the state resulting from executing Ωn,g at state st, which is either st+n or g. For brevity, we denote s(Ωn,g, s) as sΩ. Then, we reformulate the value learning objective in Equation (1) into an option-aware variant as follows: L(V OTA, n) = E(s,sΩ)D,gp(g)[Lτ 2 (r(sΩ, g) + γ OTA(sΩ, g) OTA(s, g))], (4) OTA as the Option-aware Temporally Abstracted where r(sΩ, g) = 1{sΩ = g}.5 We refer to (OTA) value function. We argue that the high-level value function OTA would effectively address the value order inconsistency due to the following reason: Using 1-step target for value learning tends to be more sensitive to noise, particularly in long-horizon tasks. In contrast, employing an option-aware target mitigates the effects of noise and empirically leads to more order-consistent value estimates. The overall framework for learning OTA is illustrated in Figure 3."
        },
        {
            "title": "6 Experiments",
            "content": "6.1 Experiment Setup Tasks. We use OGBench [33], recently proposed offline GCRL benchmark designed for realistic environments, long-horizon scenarios, and multi-goal evaluation. The Maze environment consists of long-horizon navigation tasks that evaluate whether the agent can reach specified goal position from given initial state. The Maze environments are categorized by agent type (PointMaze, AntMaze, and HumanoidMaze), maze size (medium, large, and giant), and the type of trajectories in the dataset (navigate, stitch, and explore). The Maze environments are well suited to evaluating performance in long-horizon settings. For example, the HumanoidMaze-giant environment has maximum episode length of 4000 steps. The Visual-cube and Visual-scene environments focus on visual robotic manipulation tasks. In Visual-cube, the task involves manipulating and stacking cube blocks to reach specified goal configuration. This environment is categorized by the number of cubes: single, double, and triple. In contrast, Visual-scene requires the agent to control everyday objects like windows, drawers, or two-button locks in specific sequence. Both visual environments use high-dimensional, pixel-based observations with 64 64 3 RGB images. The robotic manipulation environments have shorter episode lengths (250 to 1000 steps) compared to the Maze tasks. These robotic manipulation environments are strong benchmark for 5We highlight the differences from Equation (1) in Equation (4). 6 Figure 4: Evaluation on OGBench. We run 8 seeds for each dataset and use the performance reported in OGBench for the baselines. For maze tasks, we report the average success rate grouped by maze size. For visual robotic manipulation, we report the average success rate across the four tasks. evaluating the performance of an algorithm on high-dimensional visual inputs. detailed description of the environments is provided in Appendix B.1. Baselines. For brevity, we will refer to the policy that utilizes the high-level policy learned with the OTA value as OTA. We compare OTA against six representative offline GCRL methods included in OGBench. Goal-conditioned behavioral cloning (GCBC) [11] is simple behavior cloning method that directly imitates actions from the dataset conditioned on the goal. Goal-conditioned implicit V-learning (GCIVL) and goal-conditioned implicit Q-learning (GCIQL) [22, 34] estimate the goalconditioned optimal value function using IQL-based expectile regression, and extract policies using AWR [37] and behavior-regularized deep deterministic policy gradient (DDPG+BC) [10], respectively. Quasimetric RL (QRL) [51] learns value function that estimates the undiscounted temporal distance between state and goal via quasimetric learning and trains policy using DDPG+BC. Contrastive RL (CRL) [9] approximates the Q-function via contrastive learning between state-action pairs and future states from the same trajectory, and trains the policy using DDPG+BC. HIQL [34] extends GCIVL with hierarchical policy, as detailed in Section 3. 6.2 Evaluation on OGBench We evaluate success rates on 14 datasets, including {AntMaze, HumanoidMaze}-{medium, large, giant}-{navigate, stitch} and AntMaze-{medium, large}-explore. For both AntMaze and HumanoidMaze, we report the average success rate grouped by maze size. Additionally, for visual robotic manipulation, we evaluate the average performance across four tasks: Visual-Cube-{single, double, triple} and Visual-Scene. As shown in Figure 4, most non-hierarchical baselines (i.e., GCBC, GCIVL, GCIQL, QRL, CRL) consistently fail on long-horizon tasks. While HIQL, hierarchical policy, achieves up to 40% success on challenging tasks such as AntMaze-giant and HumanoidMaze-large, its performance drops significantly in the most difficult setting, HumanoidMaze-giant, highlighting its limitations in long-horizon settings. In contrast, we observe that OTA achieves significant performance improvement over all baselines. Notably, as the maze size increases (i.e., from medium to large to giant), the performance gap between OTA and other methods widens substantially. These results suggest that OTA performs effective temporal abstraction and enhances high-level policy performance, even as task horizons become longer. Full benchmark results, including the PointMaze tasks, are provided in Appendix D. 6.3 High-level Value Function Visualization In Figure 5, we compare the high-level value function learned with HIQL and OTA learned with OTA across six challenging tasks. Using the visualization method from Figure 2, we plot and OTA along optimal long-horizon trajectories τ , together with the corresponding temporal distances dh and dh OTA exhibits more monotonic increase than h, particularly when the distance between and is large. To quantify this improvement, we compute the order consistency ratio rc, which measures how reliably value estimates from (st, st+k, g) τ produce directionally correct signals for high-level advantage estimation. Specifically, rc(V ) = (cid:80)T t=0 1{V (st+k, g) > (st, g)}/(T + 1), where is fixed and st, st+k τ . Across all OTA. The figure clearly shows that 7 Figure 5: Value and temporal distance estimation. We visualize min-max normalized h, and dh OTA), across six different datasets. OTA, and the order consistency ratios rc(V h) and rc(V OTA, dh, Figure 6: Value estimation and order consistency. (ac) Estimation of the value function varying abstraction factor (d) Order consistency ratio rc(V OTA) across different values of n. OTA with tasks, we observe that rc(V OTA) > rc(V h), indicating that OTA yields more order-consistent value estimates.6 Therefore, we confirm that OTA improves high-level value estimation in long-horizon tasks, leading to better high-level policy learning. 6.4 Effect of Varying Abstraction Factor The training of the value function OTA depends on the abstraction factor n, which determines the degree of temporal abstraction. Figure 6(a-c) illustrates how the value function changes as is varied across 1, 2, 3, 5, 10, and 20 in Equation 4, while keeping the optimal trajectory and goal fixed for each dataset. As shown in Figure 6(b,c), for long-horizon trajectories (i.e., those exceeding length of 1500), the absolute scale of the value function increases with larger n. This trend arises since the option termination condition introduces reward of 1 every steps, which effectively compresses the value range as increases. Temporal abstraction not only changes the scale of the value function but also impacts the quality of the value estimation. Figure 6(a-c) shows that when = 1, the value function fails to learn as d(s, g) increases, which aligns with limitations commonly observed in standard HIQL. However, as increases, the value function becomes more suitable for long-horizon tasks. To further evaluate the effect of temporal abstraction, we examine the order consistency ratio rc, as shown in Figure 6(d), which generally increases with n. However, beyond certain point, larger causes drop in rc(V h), indicating that excessive temporal abstraction may lead to loss of information. 6.5 Effect of Scaling the Discount Factor γ In the original HIQL, the high-level value function is discounted by γ at every step. In contrast, the OTA value function OTA applies discounting every steps, leading to more increased optimal 6We set = 25 for AntMaze environment and = 100 for HumanoidMaze environment. 8 Table 1: Average success rate and order consistency ratio. Simply increasing the discount factor in HIQL is insufficient to achieve the performance improvements of OTA. Datasets Success rates HIQL (γ) HIQL (γ1/n) Order consistency ratios rc OTA HIQL (γ) HIQL (γ1/n) OTA AntMaze-large-explore AntMaze-giant-stitch HumanoidMaze-large-stitch HumanoidMaze-giant-stitch 4 5 2 2 12 4 28 3 3 3 0 0 22 3 2 1 75 16 37 6 57 3 79 3 0.75 0.01 0.91 0.01 0.76 0.01 0.71 0. 0.76 0.02 0.79 0.02 0.75 0.02 0.72 0.01 0.97 0.01 0.94 0.01 0.89 0.03 0.94 0.01 value function (st, g) = (1 γd(st,g)/n)/(1 γ). straightforward approach to mimic temporally abstracted optimal value function is to increase the discounting factor γ in Equation (1). Specifically, we modify the standard discount factor by using γ1/n instead of γ for training the high-level value function of HIQL. We compare standard HIQL, which uses the original discount factor γ (denoted as HIQL (γ)), with variant learned using modified discount factor γ1/n (denoted as HIQL (γ1/n)). We evaluate both the success rate and the order consistency ratio rc across four datasets. For OTA and HIQL (γ1/n), we use = 15 for AntMaze and = 20 for HumanoidMaze. To compute rc, we collect 5 trajectories per dataset and report the average consistency ratio (see Appendix B.2.3 for details of the collected trajectories). Table 1 shows that HIQL (γ1/n) fails to outperform standard HIQL in either success rate or rc in most cases, whereas OTA achieves significant gains in long-horizon tasks such as HumanoidMaze-giant-stitch. These results highlight that simply altering the discounting factor is insufficient and that temporal abstraction is crucial for effective value learning in long-horizon environments. 6.6 Scalability Comparison of TD-Based OTA and QRL Datasets Table 2: Success rates for different high-level values. Here, we demonstrate that OTA, which leverages TD-based IQL loss, scales effectively with increasing state and action dimensionality. As discussed in Section 4.2, conventional TD methods use discount factor, leading to exponential decay of the advantage signal in longhorizon settings. This decay weakens the advantage signal in long-horizon settings and can hinder high-level policy learning in the absence of OTA. AntMaze-giant-navigate 76 2 65 5 77 4 HumanoidMaze-giant-navigate 12 3 12 4 92 0 Visual-cube-double 6 2 59 3 65 2 Visual-scene 5 2 50 1 54 2 QRL HIQL OTA To address the limitation, we consider alternative value learning approaches that do not rely on discount factor. One such approach is QRL, which learns undiscounted temporal distances between states through quasimetric learning (see Appendix for more details). However, QRL relies on min-max optimization, which becomes computationally challenging in high-dimensional state spaces. As shown in Table 2, QRL achieves significantly lower success rates on complex tasks such as HumanoidMaze and Visual-scene. These results highlight the scalability and the practical advantages of our TD-based OTA, particularly in environments with high-dimensional state spaces."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we investigated the limitations of the hierarchical offline GCRL method HIQL, particularly in long-horizon tasks. Our analysis revealed that the main performance bottleneck lay in the high-level policy, which suffers from inaccurate value estimates when the state and goal are far apart. To address this challenge, we proposed OTA, method that incorporates temporal abstraction into IQL-based value learning by leveraging the concept of options. Our experiments on challenging long-horizon goal-reaching tasks demonstrated that high-level policies learned with OTA achieved significant performance gains in long-term planning. We believe that the simplicity and effectiveness of OTA present promising direction for long-horizon offline GCRL in real-world applications."
        },
        {
            "title": "References",
            "content": "[1] Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. In Advances in Neural Information Processing Systems (NeurIPS), 2017. [2] Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. In Association for the Advancement of Artificial Intelligence (AAAI), 2017. [3] Elliot Chane-Sane, Cordelia Schmid, and Ivan Laptev. Goal-conditioned reinforcement learning with imagined subgoals. In International Conference on Machine Learning (ICML), 2021. [4] Yevgen Chebotar, Karol Hausman, Yao Lu, Ted Xiao, Dmitry Kalashnikov, Jake Varley, Alex Irpan, Benjamin Eysenbach, Ryan Julian, Chelsea Finn, et al. Actionable models: Unsupervised In International Conference on Machine offline reinforcement learning of robotic skills. Learning (ICML), 2021. [5] Peter Dayan and Geoffrey Hinton. Feudal reinforcement learning. In Advances in Neural Information Processing Systems (NeurIPS), 1992. [6] Yiming Ding, Carlos Florensa, Pieter Abbeel, and Mariano Phielipp. Goal-conditioned imitation learning. In Advances in Neural Information Processing Systems (NeurIPS), 2019. [7] Ishan Durugkar, Mauricio Tec, Scott Niekum, and Peter Stone. Adversarial intrinsic motivation for reinforcement learning. Advances in Neural Information Processing Systems (NeurIPS), 34:86228636, 2021. [8] Ben Eysenbach, Russ Salakhutdinov, and Sergey Levine. Search on the replay buffer: Bridging planning and reinforcement learning. In Advances in Neural Information Processing Systems (NeurIPS), 2019. [9] Benjamin Eysenbach, Tianjun Zhang, Sergey Levine, and Russ Salakhutdinov. Contrastive In Advances in Neural Information learning as goal-conditioned reinforcement learning. Processing Systems (NeurIPS), 2022. [10] Scott Fujimoto and Shixiang Shane Gu. minimalist approach to offline reinforcement learning. Advances in Neural Information Processing Systems (NeurIPS), 2021. [11] Dibya Ghosh, Abhishek Gupta, Justin Fu, Ashwin Reddy, Coline Devin, Benjamin Eysenbach, and Sergey Levine. Learning to reach goals without reinforcement learning. ArXiv, abs/1912.06088, 2019. [12] Dibya Ghosh, Abhishek Gupta, Ashwin Reddy, Justin Fu, Coline Devin, Benjamin Eysenbach, and Sergey Levine. Learning to reach goals via iterated supervised learning. In International Conference on Learning Representations (ICLR), 2021. [13] Xudong Gong, Feng Dawei, Kele Xu, Bo Ding, and Huaimin Wang. Goal-conditioned on-policy reinforcement learning. Advances in Neural Information Processing Systems (NeurIPS), 2024. [14] Nico Gürtler, Dieter Büchler, and Georg Martius. Hierarchical reinforcement learning with timed subgoals. In Advances in Neural Information Processing Systems (NeurIPS), 2021. [15] Milos Hauskrecht, Nicolas Meuleau, Leslie Pack Kaelbling, Thomas Dean, and Craig Boutilier. Hierarchical solution of markov decision processes using macro-actions. In Conference on Uncertainty in Artificial Intelligence (UAI), 1998. [16] Po-Wei Huang, Pei-Chiun Peng, Hung Guei, and Ti-Rong Wu. Optionzero: Planning with learned options. In International Conference on Learning Representations (ICLR), 2025. [17] Zhiao Huang, Fangchen Liu, and Hao Su. Mapping state space using landmarks for universal goal reaching. In Advances in Neural Information Processing Systems (NeurIPS), 2019. [18] Yiding Jiang, Shixiang Shane Gu, Kevin Murphy, and Chelsea Finn. Language as an abstraction for hierarchical deep reinforcement learning. In Advances in Neural Information Processing Systems (NeurIPS), 2019. [19] Tom Jurgenson, Or Avner, Edward Groshev, and Aviv Tamar. Sub-goal trees framework for goal-based reinforcement learning. In International conference on machine learning (ICML), 2020. 10 [20] Junsu Kim, Younggyo Seo, Sungsoo Ahn, Kyunghwan Son, and Jinwoo Shin. Imitating graph-based planning with goal-conditioned policies. In International Conference on Learning Representations (ICLR), 2023. [21] Junsu Kim, Younggyo Seo, and Jinwoo Shin. Landmark-guided subgoal generation in hierarchical reinforcement learning. In Advances in Neural Information Processing Systems (NeurIPS), 2021. [22] Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit q-learning. In International Conference on Learning Representations (ICLR), 2022. [23] Tejas Kulkarni, Karthik Narasimhan, Ardavan Saeedi, and Josh Tenenbaum. Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. In Advances in Neural Information Processing Systems (NeurIPS), 2016. [24] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020. [25] Grace Liu, Michael Tang, and Benjamin Eysenbach. single goal is all you need: Skills and exploration emerge from contrastive rl without rewards, demonstrations, or subgoals. In International Conference on Learning Representations (ICLR), 2025. [26] Minghuan Liu, Menghui Zhu, and Weinan Zhang. Goal-conditioned reinforcement learning: Problems and solutions. In International Joint Conference on Artificial Intelligence (IJCAI), 2022. [27] Jason Yecheng Ma, Jason Yan, Dinesh Jayaraman, and Osbert Bastani. Offline goal-conditioned reinforcement learning via -advantage regression. In Advances in Neural Information Processing Systems (NeurIPS), 2022. [28] Yecheng Jason Ma, Shagun Sodhani, Dinesh Jayaraman, Osbert Bastani, Vikash Kumar, and Amy Zhang. Vip: Towards universal visual reward and representation via value-implicit pre-training. In International Conference on Learning Representations (ICLR), 2023. [29] Ofir Nachum, Shixiang Shane Gu, Honglak Lee, and Sergey Levine. Data-efficient hierarchical reinforcement learning. In Advances in Neural Information Processing Systems (NeurIPS), 2018. [30] Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine. Awac: Accelerating online reinforcement learning with offline datasets. arXiv preprint arXiv:2006.09359, 2020. [31] Suraj Nair and Chelsea Finn. Hierarchical foresight: Self-supervised learning of long-horizon tasks via visual subgoal generation. In International Conference on Learning Representations (ICLR), 2020. [32] Soroush Nasiriany, Vitchyr Pong, Steven Lin, and Sergey Levine. Planning with goalconditioned policies. In Advances in Neural Information Processing Systems (NeurIPS), 2019. [33] Seohong Park, Kevin Frans, Benjamin Eysenbach, and Sergey Levine. Ogbench: Benchmarking offline goal-conditioned rl. In International Conference on Learning Representations (ICLR), 2025. [34] Seohong Park, Dibya Ghosh, Benjamin Eysenbach, and Sergey Levine. Hiql: Offline goalconditioned rl with latent states as actions. In Advances in Neural Information Processing Systems (NeurIPS), 2023. [35] Seohong Park, Tobias Kreiman, and Sergey Levine. Foundation policies with hilbert representations. In International Conference on Machine Learning (ICML), 2024. [36] Shubham Pateria, Budhitama Subagdja, Ah-hwee Tan, and Chai Quek. Hierarchical reinforcement learning: comprehensive survey. ACM Computing Surveys (CSUR), 54(5):135, 2021. [37] Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019. [38] Doina Precup. Temporal abstraction in reinforcement learning. University of Massachusetts Amherst, 2000. [39] Rahul Ramesh, Manan Tomar, and Balaraman Ravindran. Successor options: An option discovery framework for reinforcement learning. In International Joint Conference on Artificial Intelligence (IJCAI), 2019. [40] Jette Randlov. Learning macro-actions in reinforcement learning. In Advances in Neural Information Processing Systems (NeurIPS), 1998. [41] Zhizhou Ren, Kefan Dong, Yuan Zhou, Qiang Liu, and Jian Peng. Exploration via hindsight goal generation. Advances in Neural Information Processing Systems (NeurIPS), 32, 2019. [42] Matthew Riemer, Ignacio Cases, Clemens Rosenbaum, Miao Liu, and Gerald Tesauro. On the role of weight sharing during deep option learning. In Association for the Advancement of Artificial Intelligence (AAAI), 2020. [43] Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approximators. In International Conference on Machine Learning (ICML), 2015. [44] Harshit Sikchi, Rohan Chitnis, Ahmed Touati, Alborz Geramifard, Amy Zhang, and Scott Niekum. Score models for offline goal-conditioned reinforcement learning. In International Conference on Learning Representations (ICLR), 2024. [45] Martin Stolle and Doina Precup. Learning options in reinforcement learning. In Symposium on Abstraction, Reformulation, and Approximation (SARA), 2002. [46] Richard Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: framework for temporal abstraction in reinforcement learning. Artificial intelligence, 112(12):181211, 1999. [47] Alexander Vezhnevets, Volodymyr Mnih, Simon Osindero, Alex Graves, Oriol Vinyals, John Agapiou, et al. Strategic attentive writer for learning macro-actions. In Advances in Neural Information Processing Systems (NeurIPS), 2016. [48] Alexander Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Heess, Max Jaderberg, David Silver, and Koray Kavukcuoglu. Feudal networks for hierarchical reinforcement learning. In International conference on machine learning (ICML), 2017. [49] Qing Wang, Jiechao Xiong, Lei Han, Han Liu, Tong Zhang, et al. Exponentially weighted imitation learning for batched historical data. In Advances in Neural Information Processing Systems (NeurIPS), 2018. [50] Tongzhou Wang and Phillip Isola. Improved representation of asymmetrical distances with interval quasimetric embeddings. In NeurIPS 2022 Workshop on Symmetry and Geometry in Neural Representations, 2022. [51] Tongzhou Wang, Antonio Torralba, Phillip Isola, and Amy Zhang. Optimal goal-reaching reinforcement learning via quasimetric learning. In International Conference on Machine Learning (ICML), 2023. [52] Chengjie Wu, Hao Hu, Yiqin Yang, Ning Zhang, and Chongjie Zhang. Planning, fast and slow: online reinforcement learning with action-free offline data via multiscale planners. In International Conference on Machine Learning (ICML), 2024. [53] Rui Yang, Yiming Lu, Wenzhe Li, Hao Sun, Meng Fang, Yali Du, Xiu Li, Lei Han, and Chongjie Zhang. Rethinking goal-conditioned supervised learning and its connection to offline rl. In International Conference on Learning Representations (ICLR), 2022. [54] Lunjun Zhang, Ge Yang, and Bradly Stadie. World model as graph: Learning latent landmarks for planning. In International Conference on Machine Learning (ICML), 2021. [55] Tianren Zhang, Shangqi Guo, Tian Tan, Xiaolin Hu, and Feng Chen. Generating adjacencyconstrained subgoals in hierarchical reinforcement learning. In Advances in Neural Information Processing Systems (NeurIPS), 2020. [56] Sirui Zheng, Chenjia Bai, Zhuoran Yang, and Zhaoran Wang. How does goal relabeling improve sample efficiency? In International Conference on Machine Learning (ICML), 2024. 12 Figure 7: Dataset examples. For Maze environment, the task differ by (a) environment type (b) and dataset type. (c) In Visual-cube, the robot must manipulate the cube to the location specified by the blurred cube, which denotes the goal position."
        },
        {
            "title": "A Limitations",
            "content": "Our method, OTA, has several following limitations. First, we introduce new hyperparameter, temporal abstraction factor n, to reduce the effective horizon of the value function. Due to the additional hyperparameter, we should carefully select both k, the number of steps to reach subgoal, and n. Second, though we carry out temporal abstraction on the value function, we still cannot obtain an order consistent value function for all state and goal pairs. Third, for the experiments on long-horizon tasks in which the trajectory length is more than 1000, we only use the maze dataset to evaluate our method."
        },
        {
            "title": "B Experimental Details",
            "content": "B.1 Environments, Tasks, and Datasets In this section, we provide detailed descriptions of each task, with dataset examples illustrated in Figure 7. For more detailed description of the environment, see OGBench [33]. Maze (Maze) is challenging long-horizon locomotion task, where the agent needs to reach the given goal position from the initial position. This environment is categorized into three different types of agent based on state and action dimension: 1) Pointmaze (PointMaze) , which controls 2 degrees of freedom (DoF) point mass, 2) Antmaze (AntMaze), which controls quadrupedal Ant with 8-DoF, and 3) Humanoidmaze (HumanoidMaze), which controls 21DoF Humanoid agent. Each maze environment is divided into medium, large, and giant based on maze size, from PointMaze-medium requiring horizon length (i.e., maximum episode steps) of 1000, to HumanoidMaze-giant requiring 4000. Each environment includes diverse datasetsnavigate, stitch, and explorecollected via different dataset features: navigate: This dataset consists of trajectories collected as an agent, guided by noisy expert policy, that attempted to reach randomly sampled goals. Table 3: Common hyperparameters for OTA. We refer to Appendix B.2.1 hyperparameter definition. Hyperparameter Value 3e-4 Learning rate Adam Optimizer 1024 (Maze), 256 (Visual env) Minibatch size 1000000 (Maze), 500000 (Visual env) Total gradient steps [512, 512, 512] MLP dimensions Activation function GELU Target network smoothing coefficient 0.005 Discount factor γ Image augmentation probability traj, pD Policy (pD traj, pD Value (pD 0.99 (default), 0.995 (Antmaze-giant,HumanoidMaze) 0.5 (random crop) (0,1,0) (default), (0,0.5, 0.5) (stitch), (0,0,1) (explore) (0.2, 0.5, 0.3) rand) ratio rand) ratio cur, pD cur, pD stitch: This dataset contains shorter trajectories compared to those collected in the navigate setting. They are designed to evaluate goal-stitching capabilities. explore: This includes higher levels of action noise, resulting in lower-quality data, but with increased state coverage. Visual-cube (Visual-cube) is challenging robotic visual manipulation task, where the agent must move and stack cube blocks to reach specified goal configuration. The task includes three variantssingle, double, and triplecorresponding to the number of cubes that must be manipulated. The agent receives pixel-based images of the current observation and goal, each of size 64 64 3, and outputs 5-DoF action vector. The task horizon ranges from 200 steps (single) to 1000 steps (triple). The agent is learned with noisy dataset, which is built from suboptimal dataset with action noise, leading to extremely low-quality data and longer effective horizons. Visual-scene (Visual-scene) is also robotic visual manipulation task, where the agent needs to manipulate everyday objects -a window, drawer, two button lockswhere pressing button toggles the lock status of the corresponding object (i.e., the drawer or the window). The agent receives pixel-based images of the current observation and goal, each of size 64 64 3, and outputs 5-DoF action vector. The task horizon range is 750, in that it involves unlocking object and manipulating the object. The agent is learned with noisy dataset, as mentioned above. B.2 Implementation Details B.2.1 Hyperparameters We implemented OTA on top of the official implementation of OGBench [33]7. We use goal-sampling distribution for value and policy learning, following OGBench. Data sampling scheme is based on HER [1], taking three different goal-sampling distributions, definition is as follows: pD pD pD cur(gs) is Dirac delta distribution centered at the current state (i.e., = s), traj(gs) is the probability distribution over goals g, where each goal is uniformly sampled from the future states within the same trajectory as the state s, rand(gs) is the probability distribution that goal is uniformly sampled from the entire dataset D. Task-specific hyperparameters are organized in Table 4, where hyperparameters are described in Equation (1) to Equation (4). 7https://github.com/seohongpark/ogbench 14 Task category Type"
        },
        {
            "title": "Size",
            "content": "OTA hyperparameters βh βℓ k"
        },
        {
            "title": "Environment",
            "content": "Maze PointMaze navigate medium 0.5 3.0 25 large 3.0 3.0 25 giant 3.0 3.0 20 stitch medium 1.0 3.0 20 large 1.0 3.0 20 giant 5.0 3.0 20 navigate medium 1.0 3.0 25 large 1.0 3.0 25 giant 0.5 3.0 AntMaze stitch medium 0.5 3.0 25 large 1.0 3.0 25 giant 3.0 3.0 30 HumanoidMaze explore medium 3.0 3.0 25 large 3.0 3.0 20 navigate medium 0.5 3.0 100 large 0.5 3.0 100 giant 0.5 3.0 stitch medium 3.0 3.0 100 large 1.0 3.0 100 giant 0.5 3.0 100 Robotic visual manipulation Visual-cube noisy single 1.0 3.0 20 double 3.0 3.0 20 triple 3.0 3.0 25 Visual-scene noisy 3.0 3.0 10 5 5 5 4 10 5 5 5 4 5 5 10 5 20 20 20 20 20 20 4 4 4 4 Table 4: Task specific hyperparameters for OTA. We refer to Appendix B.2.1 for each hyperparameter variable. Note that we individually tune the hyperparameters for each task. Task category"
        },
        {
            "title": "Size",
            "content": "Maximum episode length Maze PointMaze AntMaze HumanoidMaze medium large giant medium large giant medium large giant Robotic visual manipulation single double triple Visual-cube Visual-scene 1000 1000 1000 1000 1000 1000 2000 2000 4000 200 500 1000 750 Table 5: Maximum episode length of environments. B.2.2 Training and Evaluation detail In Maze environment, the model is trained for up to 1M gradient steps. We evaluate the model at 800K, 900K, and 1M steps. At each evaluation point, we measure the success rate using five fixed task goals provided by OGBench. Each goal is evaluated with 50 rollouts, resulting in 750 evaluation episodes per seed (i.e., 3 evaluation steps 5 goals 50 rollouts). We report the average success rate across these episodes and across 8 different random seeds. For Visual-cube and Visual-scene environments, the model is trained for 500K gradient steps. Evaluations are conducted at 300K, 400K, and 500K steps using the same protocol: five fixed goals and 50 rollouts per goal. The maximum episode length of each environment is shown in the Table 5. All results are averaged across 8 seeds. All experiments are conducted using NVIDIA RTX A5000 and A6000 GPUs. 16 Figure 8: Collected optimal trajectories for AntMaze environment. We collect the optimal ) trajectories from the initial state ( ) to the goal ( Figure 9: Collected optimal trajectories for HumanoidMaze environment. We collect the optimal ) trajectories from the initial state ( ) to the goal ( B.2.3 Collected Optimal trajectories To evaluate the order consistency of value for high-level advantage, we collect five optimal trajectories for each environment: AntMaze-{large, giant} and HumanoidMaze-{large, giant}. Each optimal trajectory is generated using the expert policy that was originally used during the offline dataset collection in OGBench. The collected optimal trajectories for AntMaze and HumanoidMaze are shown in Figures 8 and 9, respectively. Order consistency, as reported in Table 1, is evaluated based on the five trajectories illustrated in these figures and averaged over 8 random seeds. The optimal trajectories used for value visualizations in Figures 5 and 6 are as follows: Trajectory selection for Figure 5: Figure 5(a): path 5 Figure 5(b): path 5 Figure 5(c): path 2 Figure 5(d): path 5 Figure 5(e): path 5 Figure 5(f): path 1 17 Trajectory selection for Figure 6: Figure 6(a): path 5 Figure 6(b): path 2 Figure 6(c): path 5 Quasimetric Reinforcement Learning (QRL) QRL [51] is an goal-conditioned RL algorithm by utilizing the quasimetric structure for learning optimal value function . The quasimetrics are generalization of metrics in that they do require symmetry. The optimal value function in QRL is an undiscounted temporal distance, (s, g) = d(s, g), and the value function satisfies the triangular inequality, d(s, s) + d(s, g) d(s, g) for any s, S, and G. To obtain the optimal value function using the quasimetric structure, the value function should have two properties: First, the value function should should have locally consistent value, d(s, s) r. Second, the distance should be globally spread out, d(s, g) = total cost of path connecting to g. To achieve those properties, QRL optimizes the following objective to obtain the optimal value function: min θ max λ0 E(s,g)D[ϕ(dIQE θ (s, g))] + λ(cid:0)E(s,a,s,r)D[relu(dIQE θ (s, s) + r)2] ϵ2(cid:1), (5) where ϕ is monotonically increasing convex function, dIQE(, ) is Interval Quasimetric Embeddings (IQE) [50] for the quasimetric model. In the above objective, both the min and max operations should be applied simultaneously, which can induce unstable training. Using the value function, QRL learns policy through optimizing the DDPG + BC [10] like objective."
        },
        {
            "title": "D Additional Results",
            "content": "We show the full per-environment results in Table 6. In this table, OTA outperforms the baselines in most cases."
        },
        {
            "title": "Environment",
            "content": "Task category Type"
        },
        {
            "title": "Size",
            "content": "Hierarchical GCBC GCIVL GCIQL QRL CRL HIQL OTA Non-hierarchical Maze PointMaze stitch navigate medium large giant 9 6 29 6 1 2 63 6 45 5 0 0 medium 23 18 70 14 large 12 6 giant 0 0 7 5 0 0 AntMaze large giant navigate medium 29 4 24 2 0 0 medium 45 11 large 3 3 giant 0 0 stitch explore medium large 2 1 0 0 navigate medium large giant 8 2 1 0 0 0 medium 29 5 large 6 3 giant 0 0 HumanoidMaze stitch Robotic visual manipulation Visual-cube noisy 53 8 82 5 29 7 79 5 86 2 34 3 86 9 39 7 58 5 85 5 0 0 68 7 27 10 46 9 72 6 21 9 80 12 31 2 84 15 0 0 50 8 0 1 74 6 75 5 0 0 13 6 66 8 0 0 52 7 0 0 71 4 88 3 95 1 96 1 96 1 34 4 75 6 83 4 91 2 92 1 0 0 14 3 16 3 65 5 77 4 29 6 59 7 53 6 94 1 93 1 7 2 18 2 11 2 67 5 84 3 2 2 37 6 0 0 0 0 0 13 2 0 0 1 1 0 0 3 2 37 10 94 3 4 5 75 16 0 0 27 2 21 8 60 4 89 2 94 1 5 1 24 4 49 4 83 2 2 1 3 2 12 4 92 1 1 0 0 0 12 3 18 2 36 2 88 2 88 2 4 1 28 3 57 3 0 0 3 2 79 3 0 0 0 0 3 1 0 72 8 16 5 0 0 44 6 18 2 0 0 19 3 10 3 24 2 2 1 0 0 12 2 1 1 0 0 single 14 3 double 5 1 triple 16 75 3 17 4 18 1 48 3 10 5 39 30 99 0 99 0 6 3 59 3 65 2 6 2 22 2 9 4 16 1 23 2 26 2 12 1 Visual-scene noisy 13 2 23 2 12 2 0 15 2 50 1 54 2 Table 6: Performance comparison across various policy types and benchmarks. We shot average success rate on 8 random seeds. Bold values indicate the best performance in each row. Baseline performances are the official results provided by OGBench."
        }
    ],
    "affiliations": [
        "Department of ECE / IPAI / ASRI / INMC, Seoul National University",
        "Department of Electrical and Computer Engineering (ECE), Seoul National University"
    ]
}