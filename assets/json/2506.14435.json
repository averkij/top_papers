{
    "paper_title": "MoTE: Mixture of Ternary Experts for Memory-efficient Large Multimodal Models",
    "authors": [
        "Hongyu Wang",
        "Jiayu Xu",
        "Ruiping Wang",
        "Yan Feng",
        "Yitao Zhai",
        "Peng Pei",
        "Xunliang Cai",
        "Xilin Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large multimodal Mixture-of-Experts (MoEs) effectively scale the model size to boost performance while maintaining fixed active parameters. However, previous works primarily utilized full-precision experts during sparse up-cycling. Despite they show superior performance on end tasks, the large amount of experts introduces higher memory footprint, which poses significant challenges for the deployment on edge devices. In this work, we propose MoTE, a scalable and memory-efficient approach to train Mixture-of-Ternary-Experts models from dense checkpoint. Instead of training fewer high-precision experts, we propose to train more low-precision experts during up-cycling. Specifically, we use the pre-trained FFN as a shared expert and train ternary routed experts with parameters in {-1, 0, 1}. Extensive experiments show that our approach has promising scaling trend along model size. MoTE achieves comparable performance to full-precision baseline MoE-LLaVA while offering lower memory footprint. Furthermore, our approach is compatible with post-training quantization methods and the advantage further amplifies when memory-constraint goes lower. Given the same amount of expert memory footprint of 3.4GB and combined with post-training quantization, MoTE outperforms MoE-LLaVA by a gain of 4.3% average accuracy on end tasks, demonstrating its effectiveness and potential for memory-constrained devices."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 5 3 4 4 1 . 6 0 5 2 : r MoTE: Mixture of Ternary Experts for Memory-efficient Large Multimodal Models Hongyu Wang Jiayu Xu Ruiping Wang Yan Feng Yitao Zhai Peng Pei Xunliang Cai Xilin Chen Key Laboratory of AI Safety, Institute of Computing Technology, Chinese Academy of Sciences Meituan University of Chinese Academy of Sciences"
        },
        {
            "title": "Abstract",
            "content": "Large multimodal Mixture-of-Experts (MoEs) effectively scale the model size to boost performance while maintaining fixed active parameters. However, previous works primarily utilized full-precision experts during sparse up-cycling. Despite they show superior performance on end tasks, the large amount of experts introduces higher memory footprint, which poses significant challenges for the deployment on edge devices. In this work, we propose MoTE, scalable and memory-efficient approach to train Mixture-of-Ternary-Experts models from dense checkpoint. Instead of training fewer high-precision experts, we propose to train more low-precision experts during up-cycling. Specifically, we use the pre-trained FFN as shared expert and train ternary routed experts with parameters in {-1, 0, 1}. Extensive experiments show that our approach has promising scaling trend along model size. MoTE achieves comparable performance to full-precision baseline MoE-LLaVA while offering lower memory footprint. Furthermore, our approach is compatible with post-training quantization methods and the advantage further amplifies when memory-constraint goes lower. Given the same amount of expert memory footprint of 3.4GB and combined with post-training quantization, MoTE outperforms MoE-LLaVA by gain of 4.3% average accuracy on end tasks, demonstrating its effectiveness and potential for memory-constrained devices."
        },
        {
            "title": "Introduction",
            "content": "Large Multimodal Models (LMMs) [AAA+24, MGF+24, ZGG+24, WBT+24, CWT+24, BCL+25] have achieved remarkable performance across wide range of downstream tasks, including visual question answering and autonomous computer agents. However, as model size increases, the rising inference cost presents significant challenges for deploying LMMs efficiently. To address this, Mixture-of-Experts (MoE) [LLX+21, FZS22, DLF+24] introduces mechanism that maintains large pool of experts while activating only subset for each input, thereby improving computational efficiency. Although MoE models significantly reduce FLOPs, they generally have higher memory footprint, making deployment on edge devices challenging. For example, when training multimodal MoE up-cycled from Qwen2.5-3B, if all feed-forward network (FFN) layers are replaced with MoE layers containing 16 experts, the resulting models non-embedding memory footprint will increase from 5.2GB to 73.2GB. This limitation is particularly pronounced for consumer-grade GPUs, which often have constrained memory capacities. Model quantization is promising approach to reducing the memory footprint of LMMs while maintaining comparable performance. Most mainstream quantization methods [FAHA22, LTT+24, CCKDS24, TSHDS24] aim to compress the bit-width of pre-trained, full-precision model. Although these methods have low training cost, they suffer from significant performance degradation when the bit-width is reduced below 4-bit. Recent studies [MWM+24, KVM+24, ZZS+24] have demonstrated Preprint. promising scaling trends for ternary pre-training in Large Language Models (LLMs). At sufficiently large model sizes, ternary models can achieve accuracy comparable to full-precision models on downstream tasks while maintaining the same pre-training cost. Furthermore, they have much lower inference costs in terms of memory, latency, and energy consumption [WZS+24]. However, since these models have only been trained on billions of tokens, substantial performance gap remains between open-sourced ternary models and full-precision dense models. As result, directly training MoE models initialized from these under-trained models leads to weak performance on end tasks. In this work, we introduce MoTE, scalable and memory-efficient architecture designed to train Mixture-of-Ternary Experts model from pre-trained, full-precision dense checkpoint in multimodal tuning. Our approach addresses the inefficiency of multimodal MoE models in terms of memory footprint. Prior works [LTY+24, LJH+25] primarily replace the FFN layer in dense checkpoints with an MoE layer, initializing the experts using the pre-trained FFN. However, we observed that in ternary training, replacing the FFN layer leads to significant performance degradation, as weight ternarization disrupts the pre-trained FFN. To mitigate this, we retain the FFN from the dense checkpoint as shared expert activated for all inputs. During up-cycling, the layers inherited from the dense model remain frozen, while only the ternary MoE layers are trainable. We first conduct strict and controlled experiments to evaluate the proposed approach against fullprecision up-cycling MoE-LLaVA [LTY+24] across various model scales on wide range of image understanding tasks. Our results show that ternary up-cycling exhibits surprising effectiveness as model size scales. As the size of the up-cycled dense checkpoint increases, the performance gap between MoTE and MoE-LLaVA narrows, eventually reaching comparable performance at scales larger than 1.5 billion parameters. Additionally, MoTE is compatible with post-training quantization techniques [FAHA22]. Given the same expert memory footprint and combined with post-training quantization, MoTE outperforms full-precision MoE-LLaVA at both 1.5B and 3B model sizes. This advantage becomes even more pronounced as memory constraints tighten. Specifically, under an expert memory budget of 3.4GB, our approach achieves 4.3% improvement in average accuracy on downstream task. These results demonstrate that given the same amount of total memory footprint and active parameter counts, training with larger number of low-precision experts yields better performance than using fewer high-precision experts."
        },
        {
            "title": "2 Related Work",
            "content": "Mixture of Experts. LMMs demonstrate superior performance across various tasks as model size and training data scale increase. MoE models [LLX+21, FZS22, MSG+24, WCP+24] maintain large pool of experts but activate only subset for each token, enabling improved performance at the same FLOPs budget. [KPL+23] introduced sparse up-cycling to reduce the training costs of MoE models by initializing them from dense checkpoints. [LTY+24] explored the up-cycling of LMMs in the context of multimodal training, while [SLZ+24] proposed progressive knowledge transfer strategy to train small-scale multimodal MoEs from dense models. [LJH+25] presented scalable multimodal model that utilizes MoE with modality-specific encoders. While previous [LTY+24, LWZ+24, LJH+25] primarily focused on full-precision experts for up-cycling, our work investigates up-cycling with ternary experts to develop memory-efficient multimodal MoE models. Model Quantization. Quantization is promising approach to reducing the memory footprint of LMMs while maintaining competitive performance, which can be categorized into two types based on the stage at which it is applied: post-training [DLBZ22, FAHA22, LTT+24, CCKDS24, TCS+24, TSHDS24] and pre-training quantization [WMD+23, MWM+24, WGL+25, PWW+23]. Post-training quantization compresses high-precision pre-trained models after training. Due to its lower cost, it is widely adopted for mainstream large-scale models. GPTQ [FAHA22] and AWQ [LTT+24] reduce the bit-width to 4 bits while incurring minimal degradation. QuIP# [TCS+24] builds on QuIP [CCKDS24] by improving incoherence processing and applying vector quantization to incoherent weights. With additional fine-tuning, QuIP# achieves state-of-the-art performance in 2-bit models. However, when the bit-width is reduced below 4-bit, these methods all suffer from significant performance degradation compared to BF16 baselines. In contrast, pre-training quantization integrates quantization into the training process, requiring models to be trained from scratch, which results in better performance. Recent [MWM+24] showed that ternary LLMs match the performance of full-precision counterpart starting from 3B parameter counts. [FA24] quantized 2 Figure 1: The overview of MoTE. We retain the pre-trained full-precision FFN as shared expert and add top-1 activated MoE layer with ternary experts. All experts and attention layers are initialized from the dense checkpoint. 1.6 trillion parameter Switch Transformer to sub 1-bit precision. [LJCC24] proposed to quantize the experts with mixed precision recipe and introduced novel data-driven techniques for optimizing bit allocation."
        },
        {
            "title": "3 MoTE: Mixture-of-Ternary-Experts",
            "content": "In this section, we provide an overview of the proposed MoTE, including model architecture in Section 3.1, training recipe in Section 3.2 and objectives in Section 3.3. 3.1 Architecture We illustrate the architecture of MoTE in Figure 1. Previous studies [KPL+23, LTY+24] expanded dense model into an MoE model by directly replacing the FFN layer with an MoE layer, where each expert is initialized from the dense FFN to accelerate convergence. However, as shown in Table 6, we found that directly replacing the FFN with an MoE in ternary up-cycling leads to significant performance degradation. We hypothesize that this occurs because the FFN encodes substantial amount of factual knowledge acquired during pre-training [GSBL21, DDH+22], and weight ternarization severely disrupts pre-trained information. To mitigate this issue, we retain the FFN module from the dense model as shared expert, ensuring it is activated for every token. Specifically, the forward computation of the l-th layer of MoTE can be formulated as: xa = xl1 + MSA(LN(xl1)) + MoE(LN(xa xl = xa )) + FFN(LN(xa )) (1) (2) where MSA and LN stands for multi-head self-attention and layer normalization, respectively. As illustrated in Figure 1, we initialize the FFN, MSA and MoE layers from the dense model. We implement the MoE mechanism following the GShard [LLX+21], with each expert modeled as Gated Linear Unit (GLU) [Sha20]. An MoE layer which consists of ternary experts FFNT 1 ... 3 (3) (4) (5) (7) (8) (9) FFNT satisfies that: P(x)i = (cid:80)E ef (x)i j=1 ef (x)j (cid:88) i=1 MoE(x) = P(x)i FFNT (x) where (x) is the gating logits produced by the router. We leave the projection in router as BF16, since it only accounts for very small portion of total memory footprint. The forward computation of the i-th ternary expert FFNT (x) satisfies that: FFNT = Qw(W (x) = Qw(W down)Qa(h) up)Qa(x) σ[Qw(W (6) σ is SiLU function. We apply absmean quantizer and per-token absmax quantizer for weight and activation quantization in experts linear layers following BitNet [MWM+24]. Specifically, the quantization can be formulated as: gate)Qa(x)] Qw(W ) = α RoundClip( , 1, 1), RoundClip( , 128, 127) β 127 Qa(x) = α = 1 nm 1, β = α 127x β RoundClip(x, a, b) = max(a, min(b, round(x))) (10) The weight Rmn is quantized into ternary values, i.e., {1, 0, 1}. The activations are per-token quantized into 8-bit integers, i.e., [128, 127]. The output of ternary linear layer is Qw(W )Qa(x). During inference, we use the kernel from BitBlas [WMC+24] to save the memory footprint and accelerate the inference. Despite ternary values results in 1.58-bit, i.e., log 3/ log 2, BitBlas still stores and processes ternary weight in INT2 format since current GPUs are still based on binary system. 3.2 Training recipe Following MoE-LLaVA [LTY+24], the training of MoTE consists of three stages. In Stage I, we train two-layer MLP connector to align the visual encoder and LLM. As for Stage II, we fine-tune the LLM and connector using more complex vision-language instruction data. In Stage III, we expand the dense model from Stage II to an MoE model with ternary experts. The visual encoder is frozen through the training process. As presented in Figure 1, during up-cycling, only ternary MoE layers are trainable, and the shared expert and MSA layers are frozen. We adopt quantization-aware training for MoTE. The weights and activations are quantized into ternary and INT8 values on-the-fly. Since many operations in the quantization are no-differentiable, we deploy straight-through estimator [BLC13] for gradient approximation. The gradients are directly by-passing through non-differentiable functions, i.e., Q(X) . The gradients and optimizer states are retained as full-precision. Q(W ) and W = X = 3.3 Training objectives The training objective of MoTE Ltotal requires the minimization of both the loss of specific multimodal tasks LLM and an auxiliary load balancing loss Lbalance. Language modeling loss. The auto-regressive language modeling loss LLM is widely adopted in the training of LMMs. Specifically, let and denote sequences of visual tokens and textual tokens, respectively. can be divided as the instruction part Tins and the response part Tans. The language modeling loss is calculated as: LLM = (cid:88) log Pr(Y V, [:i1]) tokeniTans (11) where is the models output. We only calculate the loss on the response part. 4 Load balancing loss. To ease the expert load imbalance problem in MoE, we adopt an auxiliary loss following Switch Transformers [FZS22]. Given batch of training tokens X, the balancing loss can be formulated as: Lbalance = X (cid:88) (cid:88) i=1 xX ti P(x)i (12) where is the number of training tokens in X, P(x)i is the routing logits depicted in Equation 3, ti is the number of tokens routed to the i-th expert. Above all, the training objective of MoTE is: Ltotal = LLM + γ Lbalance (13) where γ is coefficient for load balancing."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Setup Method # Active/Total Params Table 1: The active/total parameter counts and expert memory of MoTE and MoE-LLaVA in various model sizes. settings. We Model select MoELLaVA [LTY+24] as the baseline. It adopts similar three-stage MoE training recipe and utilizes full-precision experts. Since MoE-LLaVA activates the top-2 experts, and our model includes shared expert, we use top-1 gating in MoTE to ensure fair comparison in terms of FLOPs. All MoE layers consist of four routed experts. We adopt SigLIP-L [ZMKB23] as the vision encoder and the instruct-version of [YYZ+24a] Qwen2.5-series model as the base LLM. The connector is two-layer MLP with GELU activation. Table 1 presents the active and total parameter counts in the training of MoTE and MoE-LLaVA across different model sizes. The expert memory footprint includes contributions from both shared and routed experts. 1.5B Model Up-cycling MoE-LLaVA MoTE 0.5B Model Up-cycling MoE-LLaVA MoTE 3B Model Up-cycling MoE-LLaVA MoTE 18.1GB (2.66) 6.8GB (1.00) 2.3GB (2.55) 0.9GB (1.00) 8.6GB (2.69) 3.2GB (1.00) 5.9B/10.8B 5.9B/13.2B Expert Memory 1.3B/1.8B 1.3B/2.1B 3.1B/5.4B 3.1B/6.6B Stage III Stage II Stage 3.4B 3.4B 2B 1B 1B 2B Implementation details. We adopt expert parallelism for efficient training of MoE models. The coefficient γ for load balancing loss is set as 0.01. The value is recommended by [FZS22] to ensure auxiliary loss not to overwhelm the primary language modeling objective. All experiments are conducted on 16 NVIDIA A100 cards with 40GB memory. Due to the limited computation resources, we do not perform dynamic resolution processing for the images, since it leads to extremely long training sequence. The length of the total sequence is set as 2048 tokens, and the visual input includes 729 tokens. More hyper-parameters can be found in Appendix B. Training data. We train MoTE and MoE-LLaVA on the same dataset to ensure fair comparison. The training dataset consists of total of 5 million samples. For the first stage, we use the pretraining data of LLaVA 1.5 [LLLL24]. For the second stage, we use the mixture of SViT [ZWH23], LVIS [WMW+23], LRV [LLL+23] and MIMIC-IT [LZC+23]. For the third stage, we use subset of MAmmoTH-VL [GZB+24], which includes 3.4 million instruction-response pairs, each associated with single image as the visual input. Evaluation. We report the zero-shot performance of these models on range of image unincluding MMMU [YNZ+24], Mathderstanding tasks using LMM-Eval toolkit [ZLZ+24], Vista [LBX+24] (MathV), MMBench [LDZ+24] (MMB), MMStar [CLD+24] (MMS), MMVet [YYL+23] (MMV), SeedBench-2-Plus [LGC+24] (Seed2+), SeedBench [LWW+23] (Seed), AI2D [KSK+16], ChartQA [MLT+22], InfoVQA [MBT+22] and DocVQA [MKJ21]. 5 Table 2: The results of MoTE and MoE-LLaVA on image understanding tasks in different model sizes. All models utilize the same base LLM, vision encoder and training dataset to ensure fair comparison. Method 0.5B Model Up-cycling MoE-LLaVA MoTE compare to MoE-LLaVA 1.5B Model Up-cycling MoE-LLaVA MoTE compare to MoE-LLaVA 3B Model Up-cycling MoE-LLaVA MoTE compare to MoE-LLaVA MMMU (val) MathV (testmini) MMB (en test) MMS (test) Seed2+ (test) AI2D (test) ChartQA (test) InfoVQA (val) DocVQA (val) Avg. 35.4 34.2 -1.2 41.2 42.6 +1.4 42.3 43.4 +1.1 35.4 35.2 -0.2 41.7 44.8 +3. 48.6 52.3 +3.7 57.3 57.6 +0.3 68.4 70.0 +1.6 75.4 74.5 -0.9 39.5 37.9 -1.6 45.0 46.4 +1. 45.5 48.2 +2.7 43.3 44.8 +1.5 52.9 54.8 +1.9 56.2 57.5 +1.3 57.4 55.2 -2.2 67.8 68.7 +0. 73.5 73.9 +0.4 56.0 54.9 -1.1 59.4 61.3 +1.9 65.0 67.6 +2.6 25.8 25.2 -0.6 31.8 32.5 +0. 35.1 36.7 +1.6 49.3 49.7 +0.4 55.1 57.4 +2.3 60.1 61.3 +1.2 44.4 43.8 -0.6 51.5 53.2 +1. 55.7 57.3 +1.6 Table 3: The results of MoTE and MoE-LLaVA given the same amount of expert memory in 1.5B and 3B model size. Both of them are combined with post-training quantization (PTQ). The expert memory footprint includes contributions from both shared and routed experts. Method Expert Memory MMMU (val) MMB (en test) Seed2+ (test) AI2D (test) DocVQA (val) Avg. 1.5B Model Up-cycling MoE-LLaVA + PTQ MoTE + PTQ MoE-LLaVA + PTQ MoTE + PTQ 3B Model Up-cycling MoE-LLaVA + PTQ MoTE + PTQ MoE-LLaVA + PTQ MoTE + PTQ 2.2GB 2.2GB 1.6GB 1.6GB 4.5GB 4.5GB 3.4GB 3.4GB 41.1 42.7 36.0 40.3 42.2 43.2 37.7 42.8 68.0 70.1 60.3 69.3 75.3 74.8 69.7 71. 53.1 54.4 49.8 55.2 55.4 57.0 52.2 56.9 67.3 68.2 62.6 67.8 72.3 73.3 67.5 73.0 55.0 57.4 50.0 57.1 59.4 60.9 56.8 60. 56.9 58.6 51.7 57.9 60.9 61.8 56.8 61.1 4.2 Main results We compared the performance of ternary up-cycling MoTE to MoE-LLaVA across different model sizes on various multimodal tasks. As shown in Table 2, MoTE underperformed full-precision up-cycling MoE-LLaVA when converting 0.5B dense model to an MoE model. However, the performance gap between MoTE and MoE-LLaVA narrows as the parameter counts of the dense model increases. Similar phenomenons are also reported by the low-bit pre-training of LLMs [WMD+23, MWM+24, KVM+24], which suggests promising trends of scaling model size for ternary MoEs. As the model size scales to 1.5B parameters, due to larger total parameter counts, MoTE surpasses MoE-LLaVA across various image understanding tasks, achieving an average accuracy improvement of 1.7% with the same FLOPs. This demonstrates the effectiveness of our proposed method. Moreover, since the expert weights in MoTE are trained to adapt to ternary values, despite it has larger total parameter counts, the ternary MoE layer can be losslessly compressed to low-bit after training, significantly reducing the memory footprint caused by the ensemble of experts. As shown in Table 1, at the 3B model size, MoTEs expert memory is only 6.8GB just 38% of MoE-LLaVAs 18.1GB. 4.3 Compatibility with post-training quantization Despite the MoE layers of our model contain ternary experts, there still leaves shared expert in full-precision in each layer. These shared experts can be quantized into low-bit using post-training quantization methods. 6 Table 4: The results of MoTE and the other methods in similar model size on general VQA and multimodal reasoning tasks. Model Dense Model MM1.5-1B [ZGG+24] MM1.5-3B [ZGG+24] MiniCPM-V2-3B [YYZ+24b] TinyLLaVA-3B [ZHW+24] Phi-3-Vision-4B [AAA+24] Qwen2-VL-2B [WBT+24] Sparse Model MoE-LLaVA [LTY+24] MolmoE-1B [DCL+24] LLaVA-MoD-2B [SLZ+24] MM1-3B-MoE [MGF+24] MM1-7B-MoE [MGF+24] MM1.5-1B-MoE [ZGG+24] MoTE-1.5B (ours) w/o initialize experts from FFN Training Tokens MMMU (val) MMB (en test) Seed (image) MMS (test) MMV (test) MathV (testmini) Avg. >200B >200B - 4B >0.8T >1.4T 4B 1.5B 10B >400B >400B >200B 21.6B 21.6B 35.8 37.1 38.2 39.9 40.4 41.1 33.9 34.9 - 38.6 40.9 41.2 40.4 41.8 - - 69.1 - 73.9 74.9 52.6 63.6 68.9 70.8 72.7 - 75.0 75.0 70.2 72.4 - - 71.8 72.1 64.8 68.7 - 69.4 70.9 71.4 72.5 71.3 - - 41.7 - 47.9 48.0 32.5 43.3 - - - - 50.2 48.1 37.4 41.0 - 34.8 45.4 49.5 32.3 38.5 - 42.2 45.2 39.8 52.6 48.6 37.2 44.4 38.7 - 44.5 43.0 25.6 34.0 - 32.6 40.9 42. 49.8 48.2 - - - - 54.0 54.8 40.3 47.2 - - - - 56.8 55.5 We apply GPTQ [FAHA22] and AWQ [LTT+24] at various bit-widths and report the best results given the same expert memory footprint. We use 512 samples with the length of 2048 tokens from Stage IIIs data as the calibration set. For MoE-LLaVA, all full-precision experts are quantized, resulting in expert memory footprints of 2.2GB and 4.5GB under INT4 quantization for the 1.5B and 3B models, respectively. To ensure fair comparison, we quantize the shared expert of MoTE to INT8 using RTN [DLBZ22]. Additionally, we extend the comparison to scenarios with lower memory constraints. For expert memory footprints of 1.6GB and 3.4GB in the 1.5B and 3B models, MoE-LLaVAs experts are quantized to 3-bit integers using GPTQ, while the shared experts of MoTE are quantized to INT4. Table 3 presents the results for MoTE and MoE-LLaVA, both combined with post-training quantization. Given the same expert memory footprint, MoTE achieves better performance than MoE-LLaVA. Under the same expert memory footprint, our method outperforms MoE-LLaVA across different model sizes. Notably, under stricter memory constraints, we observe significant performance drop for MoE-LLaVA combined with GPTQ at 3-bit precision. However, since the parameters of our MoE layer are ternary, we can achieve the same memory footprint by applying INT4 quantization only to the shared expert. This further amplifies the advantages of our approach. Specifically, given the same expert memory of 3.4GB, MoTE achieves gain of 4.3% average accuracy compared with MoE-LLaVA on the end tasks. These results demonstrate that our method can achieve lower memory footprint combined with post-training quantization, while maintaining competitive performance. 4.4 Scaling with more data To examine whether our method is friendly for scaling with data, we train 1.5B MoTE model with more data during ternary up-cycling. We adopt the same data recipe for Stage and Stage II as shown in Section 4.1. Then we use full set of MammoTH-VL [GZB+24] for Stage III, which contains 10 million samples, each associated with single image. Every dense layer is replaced with an MoTE layer with one full-precision shared expert and four routed ternary experts. The training steps is set as 40k. The other hyper-parameters are consistent with the setup presented in Section 4.1. Table 4 summarizes the zero-shot accuracy of MoTE and the baselines across various multimodal reasoning and general VQA tasks. For the baselines, we use their reported scores when available; otherwise, we evaluate the open-sourced models using the same prompts as ours to ensure fair comparison. As shown in Table 4, although MoTE-1.5B is only trained with 21.6B tokens, our model achieves an improvement of 2.0% average accuracy compared to Qwen2-VL-2B [WBT+24]. Furthermore, MoTE outperforms the larger dense model with fewer FLOPs. Specifically, MoTE outperforms MiniCPM-V-2.0-3B and Phi-3-Vision-4B by gain of 11.1% and 5.3% accuracy on the testmini set of MathVista. 7 Table 5: Ablations on the precision of routed experts in MoTE. Precision of Routed Expert MMMU (val) MMB (en test) AI2D (test) ChartQA (test) 1-bit 1.58-bit 40.3 42. 69.5 70.0 67.6 68.7 60.2 61.3 Seed2+ (test) 53.9 54.8 MMS (test) 43.1 46.4 Avg. 55.7 57.3 Table 6: Ablations on the precision of shared experts and the initialization methods of routed experts in MoTE. Precision of Shared Expert Initialize from FFN MMMU (val) MMB (en test) AI2D (test) ChartQA (test) Ternary BF16 BF16 34.6 40.1 42.6 49.4 69.9 70.0 62.7 67.1 68.7 56.4 59.9 61.3 Seed2+ (test) MMS (test) 46.2 53.2 54.8 39.8 44.5 46.4 Avg. 48.2 55.8 57.3 For sparse model, due to stronger base LLM and vision encoder, our model significantly outperforms MoE-LLaVA of similar total and active model size by gain of 16.5% average accuracy. Notably, MM1.5-1B-MoE is strong multimodal MoE baseline, which was trained from an 1B dense model with 64 experts replacing dense layers every two layers. MoTE outperforms it by gain of 0.6%, 1.1%, 12.8% and 6.9% on MMMU, SeedBench (image), MMVet and MathVista, respectively. These results proves the effectiveness of the proposed MoTE on multimodal reasoning and general VQA. 4.5 Ablation studies Precision of routed experts. We investigate the impact of expert precision on the performance of MoTE. Specifically, we compare ternary (i.e., 1.58-bit) up-cycling to 1-bit up-cycling with BWN [RORF16] as the weight quantizers. Both models are up-cycled from Qwen2.5-1.5B with SigLIP-L as the vision encoder to ensure fair comparison. As shown in Table 5, using binary experts results in performance degradation across most tasks. Similar findings have been reported in the quantization-aware training of BERT models [BZH+21], where transitioning from ternary to binary weights leads to substantially more complex and irregular loss landscape, making optimization notably more difficult. Above all, ternary up-cycling is memory-effective and high-performance solution for MoE models. Precision of shared experts. We ablate the effect of the precision of the shared expert reused from the FFN of pre-trained dense checkpoint. MoTE retains the precision of shared expert as BF16 and freezes the modules during up-cycling. We compare it to model with the ternary shared expert. All ternary experts are trainable. Table 6 presents the zero-shot performance of these models on MMMU, MMBench, AI2D, ChartQA, SeedBench-2-Plus and MMStar tasks. Weight ternarization of the shared experts has significant effect on overall performance. Specifically, the model with full-precison shared experts outperforms it with ternary shared experts by an improvement of 7.6% average accuracy on the end tasks. This demonstrates the importance of keeping the pre-trained FFN as high-precision shared expert during ternary up-cycling. Initialization of routed experts. We compare MoTE to randomly initialized routed experts in Stage III. Table 6 presents the results for 1.5B model, where initializing from the FFN yields 1.5% improvement in average accuracy on end tasks compared to random initialization. Moreover, we analyze the impact of data scaling using the data recipe described in Section 4.4. As demonstrated in Table 4, FFN-based initialization maintains its advantage with additional training data, achieving 1.3% higher average accuracy than random initialization. These findings suggest that leveraging pre-trained full-precision FFN for MoTEs initialization not only enhances performance but also accelerates the convergence of ternary experts. Additional results for the 0.5B and 3B models are provided in the Appendix C. Training recipe. We conduct ablation studies on the training strategy of ternary up-cycling in MoTE to assess the effectiveness of first training with full-precision experts before fine-tuning the 8 Table 7: Ablations on the training recipe of MoTE. Given the same training FLOPs, we do not observe performance improvement from initially training with full-precision experts then fine-tuning them into ternary precision. Ternary Training Full-Precision Training MMMU (val) MMB (en test) AI2D (test) ChartQA (test) 20% 60% 100% 80% 40% 0% 39.3 41.3 42.6 60.5 64.0 70.0 62.6 65.3 68.7 56.8 57.0 61. Seed2+ (test) MMS (test) 53.2 54.0 54.8 42.0 45.1 46.4 Avg. 52.4 54.4 57. (a) All tokens. (b) Text tokens. (c) Image tokens. Figure 2: Visualization of the routing distributions of all tokens, text tokens, image tokens across all experts on the en-test set of MMBench. model to ternary precision. All models are trained on 6.25B tokens and up-cycled from Qwen2.5-1.5B. We vary the proportion of training conducted in full-precision versus ternary precision. As shown in Table 7, we do not observe performance gain from initially training with full-precision experts. In fact, accuracy improves as the proportion of ternary training increases. Therefore, for both simplicity and improved performance, MoTE is trained directly in ternary precision without full-precision training phase during up-cycling."
        },
        {
            "title": "5 Analysis",
            "content": "We visualize the routing distribution of all tokens in MoTE-1.5B on the en-test split of the MMBench dataset. As shown in Figure 2a, expert utilization across all tokens is well-balanced. To further investigate modality-specific behavior, we present the routing distributions for text and image tokens separately in Figures 2b and 2c, respectively. Notably, text and image tokens exhibit distinct routing patterns. For example, expert #1 is frequently activated for image tokens in the first layer and the final five layers. Additional visualizations across various tasks are provided in Appendix D.1. We observe that routing distributions remain largely consistent across different tasks, suggesting that the experts in MoTE specialize based on modality rather than task-specific features. Moreover, we include per-expert routing distributions by modality in Appendix D.2. Interestingly, some experts exhibit clear modality preferences despite the absence of explicit modality conditioning during training. To better understand expert specialization, we further apply PCA [Pea01] to extract the top-10 routing pathways for text and image tokens. More visualizations are included in AppendixD.3. These findings enhance our understanding of MoTEs behavior and workflow from token-level perspective."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we introduce MoTE, scalable and memory-efficient approach to train multimodal Mixture-of-Ternary-Experts models from full-precision dense checkpoints. Extensive experiments show that our model matches the full-precision up-cycling MoE-LLaVA in zero-shot performance on end tasks, starting from model sizes exceeding 1.5B parameters. Furthermore, MoTE is compatible with post-training quantization methods, enabling further reductions in the memory footprint of MoE models. Given the same expert memory footprint of 3.4GB, MoTE surpasses MoE-LLaVA with an average accuracy gain of 4.3% on image understanding tasks, highlighting the effectiveness of our approach, particularly for memory-constrained edge devices."
        },
        {
            "title": "References",
            "content": "[AAA+24] Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. [BCL+25] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report, 2025. [BLC13] Yoshua Bengio, Nicholas Léonard, and Aaron C. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. CoRR, abs/1308.3432, 2013. [BZH+21] Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jin Jin, Xin Jiang, Qun Liu, Michael R. Lyu, and Irwin King. Binarybert: Pushing the limit of BERT quantization. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 43344348. Association for Computational Linguistics, 2021. [CCKDS24] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher De Sa. Quip: 2-bit quantization of large language models with guarantees. Advances in Neural Information Processing Systems, 36, 2024. [CLD+24] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024. [CWT+24] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, Ji Ma, Jiaqi Wang, Xiaoyi Dong, Hang Yan, Hewei Guo, Conghui He, Botian Shi, Zhenjiang Jin, Chao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, Bo Zhang, Pinlong Cai, Licheng Wen, Xiangchao Yan, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. CoRR, abs/2404.16821, 2024. [DCL+24] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024. [DDH+22] Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge neurons in pretrained transformers. In ACL 2022, pages 84938502, 2022. [DLBZ22] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm.int8(): 8-bit matrix multiplication for transformers at scale. CoRR, abs/2208.07339, 2022. [DLF+24] DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, 10 Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, and Wangding Zeng. Deepseek-v3 technical report. CoRR, abs/2412.19437, 2024. [FA24] Elias Frantar and Dan Alistarh. Qmoe: Sub-1-bit compression of trillion parameter models. In Proceedings of the Seventh Annual Conference on Machine Learning and Systems, MLSys 2024, Santa Clara, CA, USA, May 13-16, 2024. mlsys.org, 2024. [FAHA22] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. [FZS22] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. J. Mach. Learn. Res., 23:120:1120:39, 2022. [GSBL21] Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value memories. In EMNLP 2021, pages 54845495, 2021. [GZB+24] Jarvis Guo, Tuney Zheng, Yuelin Bai, Bo Li, Yubo Wang, King Zhu, Yizhi Li, Graham Neubig, Wenhu Chen, and Xiang Yue. Mammoth-vl: Eliciting multimodal reasoning with instruction tuning at scale. 2024. [KPL+23] Aran Komatsuzaki, Joan Puigcerver, James Lee-Thorp, Carlos Riquelme Ruiz, Basil Mustafa, Joshua Ainslie, Yi Tay, Mostafa Dehghani, and Neil Houlsby. Sparse upcycling: Training mixture-of-experts from dense checkpoints. In ICLR 2023. OpenReview.net, 2023. [KSK+16] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pages 235251. Springer, 2016. [KVM+24] Ayush Kaushal, Tejas Vaidhya, Arnab Kumar Mondal, Tejas Pandey, Aaryan Bhagat, and Irina Rish. Spectra: Surprising effectiveness of pretraining ternary language models at scale. arXiv preprint arXiv:2407.12327, 2024. [LBX+24] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In International Conference on Learning Representations (ICLR), 2024. [LDZ+24] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal In European conference on computer vision, pages model an all-around player? 216233. Springer, 2024. [LGC+24] Bohao Li, Yuying Ge, Yi Chen, Yixiao Ge, Ruimao Zhang, and Ying Shan. Seedbench-2-plus: Benchmarking multimodal large language models with text-rich visual comprehension. arXiv preprint arXiv:2404.16790, 2024. [LJCC24] Pingzhi Li, Xiaolong Jin, Yu Cheng, and Tianlong Chen. Examining post-training quantization for mixture-of-experts: benchmark. CoRR, abs/2406.08155, 2024. [LJH+25] Yunxin Li, Shenyuan Jiang, Baotian Hu, Longyue Wang, Wanqi Zhong, Wenhan Luo, Lin Ma, and Min Zhang. Uni-moe: Scaling unified multimodal llms with mixture of experts. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. [LLL+23] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Mitigating hallucination in large multi-modal models via robust instruction tuning. In The Twelfth International Conference on Learning Representations, 2023. [LLLL24] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. [LLX+21] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. In ICLR 2021,, 2021. [LTT+24] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware weight quantization for on-device llm compression and acceleration. Proceedings of Machine Learning and Systems, 6:87100, 2024. [LTY+24] Bin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin Zhu, Peng Jin, Junwu Zhang, Munan Ning, and Li Yuan. Moe-llava: Mixture of experts for large vision-language models. CoRR, abs/2401.15947, 2024. [LWW+23] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. CoRR, abs/2307.16125, 2023. [LWZ+24] Jiachen Li, Xinyao Wang, Sijie Zhu, Chia-Wen Kuo, Lu Xu, Fan Chen, Jitesh Jain, Humphrey Shi, and Longyin Wen. Cumo: Scaling multimodal LLM with co-upcycled mixture-of-experts. In Advances in Neural Information Processing Systems, 2024. [LZC+23] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, Chunyuan Li, and Ziwei Liu. MIMIC-IT: multi-modal in-context instruction tuning. CoRR, abs/2306.05425, 2023. [MBT+22] Minesh Mathew, Viraj Bagal, Rubèn Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 16971706, 2022. [MGF+24] Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Anton Belyi, Haotian Zhang, Karanjeet Singh, Doug Kang, Hongyu Hè, Max Schwarzer, Tom Gunter, Xiang Kong, Aonan Zhang, Jianyu Wang, Chong Wang, Nan Du, Tao Lei, Sam Wiseman, Mark Lee, Zirui Wang, Ruoming Pang, Peter Grasch, Alexander Toshev, and Yinfei Yang. MM1: methods, analysis and insights from multimodal LLM pre-training. In ECCV 2024, volume 15087, pages 304323, 2024. [MKJ21] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. [MLT+22] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. [MSG+24] Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi, Pete Walsh, Oyvind Tafjord, Nathan Lambert, et al. Olmoe: Open mixture-of-experts language models. arXiv preprint arXiv:2409.02060, 2024. [MWM+24] Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Li Dong, Ruiping Wang, Jilong Xue, and Furu Wei. The era of 1-bit llms: All large language models are in 1.58 bits. CoRR, abs/2402.17764, 2024. [Pea01] Karl Pearson. Liii. on lines and planes of closest fit to systems of points in space. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, 2(11):559572, 1901. [PWW+23] Houwen Peng, Kan Wu, Yixuan Wei, Guoshuai Zhao, Yuxiang Yang, Ze Liu, Yifan Xiong, Ziyue Yang, Bolin Ni, Jingcheng Hu, et al. Fp8-lm: Training fp8 large language models. arXiv preprint arXiv:2310.18313, 2023. 12 [RORF16] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet classification using binary convolutional neural networks. In Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV, volume 9908 of Lecture Notes in Computer Science, pages 525542. Springer, 2016. [Sha20] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. [SLZ+24] Fangxun Shu, Yue Liao, Le Zhuo, Chenning Xu, Lei Zhang, Guanghao Zhang, Haonan Shi, Long Chen, Tao Zhong, Wanggui He, et al. Llava-mod: Making llava tiny via moe knowledge distillation. arXiv preprint arXiv:2408.15881, 2024. [TCS+24] Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, and Christopher De Sa. Quip#: Even better LLM quantization with hadamard incoherence and lattice codebooks. In ICML, 2024. [TSHDS24] Albert Tseng, Qingyao Sun, David Hou, and Christopher De Sa. Qtip: Quantization with trellises and incoherence processing. arXiv preprint arXiv:2406.11235, 2024. [WBT+24] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [WCP+24] Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, et al. Deepseek-vl2: Mixture-ofexperts vision-language models for advanced multimodal understanding. arXiv preprint arXiv:2412.10302, 2024. [WGL+25] Ruizhe Wang, Yeyun Gong, Xiao Liu, Guoshuai Zhao, Ziyue Yang, Baining Guo, Zhengjun Zha, and Peng Cheng. Optimizing large language model training using fp4 quantization. arXiv preprint arXiv:2501.17116, 2025. [WMC+24] Lei Wang, Lingxiao Ma, Shijie Cao, Quanlu Zhang, Jilong Xue, Yining Shi, Ningxin Zheng, Ziming Miao, Fan Yang, Ting Cao, Yuqing Yang, and Mao Yang. Ladder: Enabling efficient low-precision deep learning computing through hardware-aware tensor transformation. In 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24), 2024. [WMD+23] Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan Yang, Ruiping Wang, Yi Wu, and Furu Wei. Bitnet: Scaling 1-bit transformers for large language models. arXiv preprint arXiv:2310.11453, 2023. [WMW+23] Junke Wang, Lingchen Meng, Zejia Weng, Bo He, Zuxuan Wu, and Yu-Gang Jiang. To see is to believe: Prompting gpt-4v for better visual instruction tuning. arXiv preprint arXiv:2311.07574, 2023. [WZS+24] Jinheng Wang, Hansong Zhou, Ting Song, Shaoguang Mao, Shuming Ma, Hongyu Wang, Yan Xia, and Furu Wei. 1-bit ai infra: Part 1.1, fast and lossless bitnet b1. 58 inference on cpus. arXiv preprint arXiv:2410.16144, 2024. [YNZ+24] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multidiscipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. [YYL+23] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. 13 [YYZ+24a] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [YYZ+24b] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, Qianyu Chen, Huarong Zhou, Zhensheng Zou, Haoye Zhang, Shengding Hu, Zhi Zheng, Jie Zhou, Jie Cai, Xu Han, Guoyang Zeng, Dahai Li, Zhiyuan Liu, and Maosong Sun. Minicpm-v: GPT-4V level MLLM on your phone. CoRR, abs/2408.01800, 2024. [ZGG+24] Haotian Zhang, Mingfei Gao, Zhe Gan, Philipp Dufter, Nina Wenzel, Forrest Huang, Dhruti Shah, Xianzhi Du, Bowen Zhang, Yanghao Li, Sam Dodge, Keen You, Zhen Yang, Aleksei Timofeev, Mingze Xu, Hong-You Chen, Jean-Philippe Fauconnier, Zhengfeng Lai, Haoxuan You, Zirui Wang, Afshin Dehghan, Peter Grasch, and Yinfei Yang. MM1.5: methods, analysis & insights from multimodal LLM fine-tuning. CoRR, abs/2409.20566, 2024. [ZHW+24] Baichuan Zhou, Ying Hu, Xi Weng, Junlong Jia, Jie Luo, Xien Liu, Ji Wu, and Lei Huang. Tinyllava: framework of small-scale large multimodal models. arXiv preprint arXiv:2402.14289, 2024. [ZLZ+24] Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, et al. Lmms-eval: Reality check on the evaluation of large multimodal models. arXiv preprint arXiv:2407.12772, 2024. [ZMKB23] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1197511986, 2023. [ZWH23] Bo Zhao, Boya Wu, and Tiejun Huang. SVIT: scaling up visual instruction tuning. CoRR, abs/2307.04087, 2023. [ZZS+24] Rui-Jie Zhu, Yu Zhang, Ethan Sifferman, Tyler Sheaves, Yiqiao Wang, Dustin Richmond, Peng Zhou, and Jason Eshraghian. Scalable matmul-free language modeling. arXiv preprint arXiv:2406.02528, 2024."
        },
        {
            "title": "A Limitations",
            "content": "In this work, we explore ternary MoE up-cycling for large multimodal models. Extensive experiments demonstrate that the proposed MoTE achieves both strong performance and significant memory savings compared to the widely adopted full-precision up-cycling baseline, MoE-LLaVA. However, this study does not provide theoretical explanation for why ternary up-cycling can match the performance of its full-precision counterpart. We leave deeper investigation into the training dynamics and theoretical underpinnings of ternary MoE models as future work. Hyper-parameters In this section, we present the detailed hyper-parameters used for the training of MoTE and fullprecision up-cycling baseline MoE-LLaVA. For Stage and Stage II, we adopt the same training recipe, data and hyper-parameters, for both MoTE and MoE-LLaVA. For Stage III, we use the learning rate and scheduler recommended by MoE-LLaVA for full-precision training. For MoTE, following BitNet, we use much large learning rate and two-stage weight decay for ternary experts which is critical for the optimization of extremely low-bit training. We utilize torch.compile to compile the PyTorch code in the quantization into optimized kernels, which significantly speed up the training of MoTE. As for the training of 1.5B models up-cycling in Stage III, MoTE costs 43.3 hours on 16 NVIDIA A100 cards (40GB), while MoE-LLaVA uses 41.8 hours. Above all, MoTE has similar training time compared to full-precision up-cycling MoE-LLaVA. 14 Table 8: Hyper-parameters for the training of MoTE and MoE-LLaVA with 0.5B model. a/b denotes the value of MoTE/MoE-LLaVA. 1 + 4 denotes that the model has one shared expert and four routed experts. Hyper-parameter Stage Stage II Stage III Learning rate Batch Size Weight decay Training steps Training sequence Vision sequence AdamW β AdamW ϵ # MoE layer # Experts # Top-k 1e-3 256 2500 1024 5e-5 128 8000 1024 1.5e-4/5e-5 256 0.1 0/ 12500 2048 729 (0.9, 0.999) 1e-8 - - - - - - 24 1+4 / 0+4 1+1 / 0+2 Table 9: Hyper-parameters for the training of MoTE and MoE-LLaVA with 1.5B and 3B model. a/b denotes the value of MoTE/MoE-LLaVA. 1 + 4 denotes that the model has one shared expert and four routed experts. Hyper-parameter Stage Stage II Stage III Learning rate Batch Size Weight decay Training steps Training sequence Vision sequence AdamW β AdamW ϵ # MoE layer # Experts # Top-k 1e-3 256 2500 1024 1e-4/2e-5 256 0.1 0/ 12500 2048 2e-5 128 8000 1024 729 (0.9, 0.999) 1e-8 - - - - - - 28 1+4 / 0+4 1+1 / 0+"
        },
        {
            "title": "C More Ablation Studies",
            "content": "We compare MoTE with the randomly initialized routed experts in Stage III. We evaluate the zero-shot performance of these models on range of image understanding tasks, including MMMU, MMBench, AI2D, ChartQA, SeedBench-2-Plus and MMStar dataset. Table 10 shows the results of both methods in 0.5B, 1.5B and 3B model size. Initializing from FFN outperforms random initialization by gain of 1.0%, 1.5% and 0.3% average accuracy on end tasks in 0.5B, 1.5B and 3B model size, respectively. The results demonstrate that using the pre-trained full-precision FFN for MoTEs initialization achieves better performance across various model size."
        },
        {
            "title": "D Visualization",
            "content": "We visualize the workflows of MoTE-1.5B at three distinct levels: expert, modality, and token. Specifically, we selected the AI2D, SeedBench-2-Plus, ChartQA, DocVQA, InfoVQA, MMStar, and MMBench datasets. Figures 3, 4, and 5 respectively illustrate the load distributions across different experts, the modality-aware routing distributions for each expert, and the top-10 activated pathways obtained via PCA. Our analysis indicates that, although the routing distributions of MoTE remain quite similar across tasks, they are predominantly influenced by the input modality. D.1 Routing distribution for tokens 15 Table 10: Ablations on the initialization methods of the routed experts for MoTE across different model sizes. Initialize from FFN MMMU MMBench AI2D ChartQA SeedBench2+ MMStar Avg. 0.5B Model Up-cycling 1.5B Model Up-cycling 3B Model Up-cycling 34.8 34.2 40.1 42.6 43.3 43. 50.5 57.6 69.9 70.0 75.5 74. 55.2 55.2 67.1 68.7 72.7 73.9 55.8 54.9 59.9 61.3 65.5 67. 43.0 44.8 53.2 54.8 57.1 57.5 39.1 37.9 44.5 46.4 48.8 48. 46.4 47.4 55.8 57.3 60.5 60.8 (a) All tokens (AI2D) (b) Text tokens (AI2D) (c) Image tokens (AI2D) (d) All tokens (SeedBench2+) (e) Text tokens (SeedBench2+) (f) Image tokens (SeedBench2+) (g) All tokens (ChartQA) (h) Text tokens (ChartQA) (i) Image tokens (ChartQA) 16 (j) All tokens (DocVQA) (k) Text tokens (DocVQA) (l) Image tokens (DocVQA) (m) All tokens (InfoVQA) (n) Text tokens (InfoVQA) (o) Image tokens (InfoVQA) (p) All tokens (MMStar) (q) Text tokens (MMStar) (r) Image tokens (MMStar) (s) All tokens (MMBench) (t) Text tokens (MMBench) (u) Image tokens (MMBench) Figure 3: Visualization of the routing distributions of all tokens, text tokens, image tokens across all experts on various tasks. 17 D.2 Routing distribution for each experts (a) Routing distribution on AI2D. (b) Routing distribution on SeedBench-2-Plus. (c) Routing distribution on ChartQA. (d) Routing distribution on DocVQA. 18 (e) Routing distribution on InfoVQA. (f) Routing distribution on MMStar. Figure 4: Visualization of the modality-aware routing distributions for each expert on various tasks. (g) Routing distribution on MMBench. 19 D.3 Activated Pathways (a) The top-10 pathways for text and image tokens on MMBench. (b) The top-10 pathways for text and image tokens on AI2D. (c) The top-10 pathways for text and image tokens on SeedBench-2-Plus. 20 (d) The top-10 pathways for text and image tokens on ChartQA. (e) The top-10 pathways for text and image tokens on DocVQA. (f) The top-10 pathways for text and image tokens on InfoVQA. 21 (g) The top-10 pathways for text and image tokens on MMStar. Figure 5: Visualization of the top-10 activated pathways for text and image modality on various tasks."
        }
    ],
    "affiliations": [
        "Key Laboratory of AI Safety, Institute of Computing Technology, Chinese Academy of Sciences",
        "Meituan University of Chinese Academy of Sciences"
    ]
}