{
    "paper_title": "VLM-3R: Vision-Language Models Augmented with Instruction-Aligned 3D Reconstruction",
    "authors": [
        "Zhiwen Fan",
        "Jian Zhang",
        "Renjie Li",
        "Junge Zhang",
        "Runjin Chen",
        "Hezhen Hu",
        "Kevin Wang",
        "Huaizhi Qu",
        "Dilin Wang",
        "Zhicheng Yan",
        "Hongyu Xu",
        "Justin Theiss",
        "Tianlong Chen",
        "Jiachen Li",
        "Zhengzhong Tu",
        "Zhangyang Wang",
        "Rakesh Ranjan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid advancement of Large Multimodal Models (LMMs) for 2D images and videos has motivated extending these models to understand 3D scenes, aiming for human-like visual-spatial intelligence. Nevertheless, achieving deep spatial understanding comparable to human capabilities poses significant challenges in model encoding and data acquisition. Existing methods frequently depend on external depth sensors for geometry capture or utilize off-the-shelf algorithms for pre-constructing 3D maps, thereby limiting their scalability, especially with prevalent monocular video inputs and for time-sensitive applications. In this work, we introduce VLM-3R, a unified framework for Vision-Language Models (VLMs) that incorporates 3D Reconstructive instruction tuning. VLM-3R processes monocular video frames by employing a geometry encoder to derive implicit 3D tokens that represent spatial understanding. Leveraging our Spatial-Visual-View Fusion and over 200K curated 3D reconstructive instruction tuning question-answer (QA) pairs, VLM-3R effectively aligns real-world spatial context with language instructions. This enables monocular 3D spatial assistance and embodied reasoning. To facilitate the evaluation of temporal reasoning, we introduce the Vision-Spatial-Temporal Intelligence benchmark, featuring over 138.6K QA pairs across five distinct tasks focused on evolving spatial relationships. Extensive experiments demonstrate that our model, VLM-3R, not only facilitates robust visual-spatial reasoning but also enables the understanding of temporal 3D context changes, excelling in both accuracy and scalability."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 9 7 2 0 2 . 5 0 5 2 : r VLM-3R: Vision-Language Models Augmented with Instruction-Aligned 3D Reconstruction Zhiwen Fan1, Jian Zhang2, Renjie Li3, Junge Zhang4, Runjin Chen1, Hezhen Hu1, Kevin Wang1, Huaizhi Qu5, Dilin Wang6, Zhicheng Yan6, Hongyu Xu6, Justin Theiss6, Tianlong Chen5, Jiachen Li4, Zhengzhong Tu3, Zhangyang Wang1, Rakesh Ranjan6 1UT Austin 2XMU 3TAMU 4UCR 5UNC 6Meta Project Website: https://vlm-3r.github.io Figure 1: VLM-3R: 3D Spatial-Temporal Reasoning from Monocular Video. Our VLM-3R framework (b) utilizes an end-to-end architecture with integrated encoders to process video directly, bypassing the reliance of prior methods (a) on explicit 3D data (e.g., depth sensors, pre-built maps). This enables deep understanding of spatial context, instance layout, and temporal dynamics, achieving leading performance on VSI-Bench and our novel VSTemporalI-Bench (results in c)."
        },
        {
            "title": "Abstract",
            "content": "The rapid advancement of Large Multimodal Models (LMMs) for 2D images and videos has motivated extending these models to understand 3D scenes, aiming for human-like visual-spatial intelligence. Nevertheless, achieving deep spatial understanding comparable to human capabilities poses significant challenges in model encoding and data acquisition. Existing methods frequently depend on external depth sensors for geometry capture or utilize off-the-shelf algorithms for pre-constructing 3D maps, thereby limiting their scalability, especially with prevalent monocular video inputs and for time-sensitive applications. In this work, we introduce VLM-3R, unified framework for Vision-Language Models (VLMs) that incorporates 3D Reconstructive instruction tuning. VLM-3R processes monocular video frames by employing geometry encoder to derive implicit 3D tokens that represent spatial understanding. Leveraging our SpatialVisualView Fusion and over 200K curated 3D reconstructive instruction tuning question-answer (QA) pairs, VLM-3R effectively aligns real-world spatial context with language instructions. This enables monocular 3D spatial assistance and embodied reasoning. To facilitate the evaluation of temporal reasoning, we introduce the Vision-Spatial-Temporal Intelligence benchmark, featuring over 138.6K QA pairs across five distinct tasks focused on evolving spatial relationships. Extensive experiments demonstrate that our model, VLM-3R, not only facilitates robust visual-spatial reasoning but also enables the understanding of temporal 3D context changes, excelling in both accuracy and scalability. Z. Fan is the Corresponding Author; Z. Fan and J. Zhang contributed equally. Preprint."
        },
        {
            "title": "Introduction",
            "content": "Humans develop visual-spatial intelligence by learning from the physical world, primarily through vision, which enables them to implicitly build environmental maps and recall experiences encompassing spatial layouts, object sizes, potential interactions, and temporal reasoning. Despite rapid advancements in large foundation models (Large Language Models, LLMs [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]; Large Multimodal Models, LMMs [13, 14, 15, 16, 17, 15, 18, 19, 20]) demonstrating significant capabilities in open-ended dialogue, single-image understanding, and practical tasks like web agents, existing LMMs that integrate language and vision still struggle with fundamental spatial tasks requiring geometric understanding, such as the simple task of distance estimation. Existing methods for 3D VLMs [21, 22] or 3D LLMs [23] frequently face significant limitations due to their design . Many approaches depend on specialized depth sensors [24, 25] for 3D data acquisition during both fine-tuning and inference. This hardware dependency critically constrains their scalability to environments equipped with specific sensors and largely prevents leveraging the vast amount of readily available monocular video data. On the other hand, there are approaches that employ off-the-shelf algorithms [23, 26], such as 3D Gaussian [27] or NeRF [28] with points initialized from Structure-from-Motion [29], to pre-construct explicit 3D mapstypically point cloudswhich are then aligned with, or fed as input to, language models. This strategy faces several deep-seated challenges that impede robust spatial understanding. The reliance on pre-constructed geometry means these multi-stage pipelines are inherently susceptible to failures of perceiving the metric space world as the traditional reconstruction algorithm ignores the real-world scale [29], which can irreversibly degrade the VLMs spatial comprehension. Moreover, the crucial step of aligning these prebuilt 3D geometric representations with language is significantly hampered by the current lack of powerful, pre-trained 3D point cloud encoders [30] analogous to CLIP in the 2D domain, which would ideally provide strong language-aligned 3D features. This absence necessitates substantial ad-hoc efforts in designing methods to encode point clouds of varying scales into effective latent tokens, often incurring significant inference time due to requirements such as offline 3D scanning for initial map creation (illustrated in Figure 1 (a)). The aforementioned limitations, along with the long-term goal of advancing LMMs towards humanlevel spatial intelligence [31] by leveraging prevalent and information-rich data formats such as monocular video, motivate our work. We aim to imbue VLMs with robust visual-spatial intelligence for reliable spatial reasoning directly from these accessible inputs. This raises the central question of how to construct truly end-to-end 3D VLM system capable of interpreting 3D geometry and spatial relationships from input image sequences, without necessitating intermediate reconstruction steps, while simultaneously integrating common sense knowledge from language models for intelligent 3D spatial assistance and embodied reasoning. In this work, we propose VLM-3R, framework that augments Vision-Language Models with instruction-aligned 3D reconstruction. To enable VLM-3R to perceive rich spatial information directly from monocular video, we leverage pre-trained end-toend 3D reconstruction models from the DUSt3R series [32, 33], specifically employing model CUT3R capable of generating metric-scale global 3D point clouds from uncalibrated video as our spatial encoder. To effectively integrate these diverse inputs, our proposed Spatial-Visual-View Fusion mechanism, illustrated in Figure 1(b), is designed to merge 3D geometric tokens, per-view camera tokens, and 2D appearance features, aligning them with language representations for joint spatio-linguistic understanding within the LMM. This approach enables our model to interpret spatial context directly from raw video and to disentangle cameraobject relative distance change. Complementing this, we develop scalable 3D video data curation pipeline to support the instruction tuning. To further examine spatial-temporal understanding derived from monocular video, we introduce Visual-Spatial-Temporal intelligence benchmark consisting of five tasks designed to evaluate the comprehension of spatial relationships as they evolve over time. VLM-3R not only excels in reasoning about static spatial contexts but also demonstrates robust ability to understand dynamic spatio-temporal changes within 3D environments from monocular video input, as evidenced by its performance across benchmarks in Figure 1(c). To summarize, our contributions are: We introduce VLM-3R, the first 3D vision-language framework to achieve robust spatial reasoning and instruction-guided 3D scene VQA directly from monocular RGB video. VLM-3R uniquely operates without requiring depth sensors or pre-built 3D maps, offering better scalability. To enable this, we introduce data curation pipeline and new model architecture for spatial reasoning. On the data front, we develop scalable pipeline to curate high-quality, 3D-reconstructive instructional data by repurposing existing 3D videos datasets at minimal cost. Concurrently, on the modeling front, we propose specialized geometric and camera view tokens derived from video as strong 3D prior, which are embedded and unified within the LMMs latent space via Spatial-visualView Alignment. This approach endows the model with inherent 3D perception capabilities and facilitates end-to-end fine-tuning. Furthermore, to comprehensively evaluate and advance the understanding of dynamic 3D environments, we enhance existing 3D benchmark methodologies by introducing novel tasks focused on Spatial-Temporal awareness. Across both established visual-spatial and our new visual-spatial-temporal evaluations, VLM-3R achieves state-of-the-art performance, surpassing open-source and proprietary models."
        },
        {
            "title": "2 Related Work",
            "content": "Large Multimodal Models. Large Multimodal models (LMMs) aim to unify vision, language, and other modalities within single model. Early efforts such as CLIP [34] and ALIGN [35] learn joint image-text representations via contrastive pretraining, enabling strong zero-shot capabilities. Subsequent models like Flamingo [16] and BLIP-2 [36] improved efficiency by decoupling vision and language modules, while enabling stronger cross-modal reasoning. Recent works have extended MMFMs to broader settings, such as embodied agents (PaLM-E [37]), grounding [38], and general-purpose task solving [39, 40]. Given LMMs strong modeling capability, several recent works [21, 41, 24, 23, 25] have adapted them with explicit 3D representations for spatial reasoning. LLaVA-3D [24] and ROSS3D [42] incorporates multi-view images and depths, enabling RGB-D based spatial QA. SpatialRGPT [22] enhances vision-language models spatial reasoning by integrating regional representations and monocular estimated depth information. Despite their strong performance, these models depend on explicit 3D inputseither depth sensor data or 3D map/depth reconstructionwhich limits their applicability to standard monocular RGB video. Spatial Reasoning towards human-level intelligence. Spatial reasoning is hallmark of human cognition, enabling individuals to understand and navigate complex environments [43, 44, 45]. There remains major challenge for machines to achieve human-level spatial reasoning, especially under the realistic constraint of monocular RGB video as the input. Recent benchmarks such as VSI-Bench [31] evaluate how well LMMs understand spatial relations from real-world videos. VSI-Bench encompasses eight tasks classified into three categories: configurational, measurement estimation, and spatiotemporal, which assess models holistic understanding of spatial configurations within given video that are typically intuitive for humans. Despite promising progress in 2D visual perception, state-of-the-art large-scale industrial foundation models like Gemini 1.5 [14] and GPT-4o [46], as well as open-source models including InternVL [47], ViLA [48], LongViLA [49], LongVA [50], and LLaVA-OneVision [51], show substantial performance gaps in spatial tasks such as localization, layout inference, and memory recall. The shortcomings of current video LMMs on spatial reasoning tasks stem from their absence of structured spatial representationslong emphasized by cognitive scienceand their lack of multi-view encoding analogous to human binocular vision. This gap underscores the critical role of spatial schemas and hierarchical scene encoding in human cognition [52, 53, 45]. In this paper, we propose an end-to-end approach that extracts 3D spatial information directly from monocular RGB video and integrates it into existing LMMs, enabling spatial reasoning without modular pipelines or additional depth sensors. 3D Reconstruction from Images. Classical 3D reconstruction methods [54, 29], such as Structurefrom-Motion (SfM) combined with Multi-view Stereo (MVS), involve distinct stages, such as feature extraction, point matching, and geometric verification, forming modular and often timeconsuming optimization pipelines. Learnable MVS approaches, many stemming from the MVSNet series [55, 56], generally assume known camera information and map multi-view images to depth maps. More recently, methods like DUSt3R [32] and MASt3R [33] alleviate the needs of known camera parameters by utilizing Transformers [57] to directly predict pixel-aligned 3D point maps. Subsequent works have extended DUSt3R to handle multi-view scenarios [58, 59, 60, 61], integrate semantic understanding [62], and perform dynamic reconstruction [63, 64]. 3 Figure 2: VSTemporalI-Bench Overview. (a) Statistical distribution of QA pairs by primary categories (inner ring of chart, detailed in legend) and their sub-categories (outer ring). (b) Example Question-Answer pairs illustrating different task types within scene from the benchmark."
        },
        {
            "title": "3 Scalable Visual-Spatial and Visual-Spatial-Temporal Datasets",
            "content": "3.1 Overview Earlier works [31] created small-scale spatial question-answering (QA) datasets via manual annotation or semi-automated tools, methods that are difficult to scale. Our objective is to construct scalable pipeline for curating training data to evaluate spatial QA capabilities. We first design an automated pipeline leveraging existing 3D datasets (containing video or multi-view images), aided by simulators. We then extend this to examine Large Multimodal Models (LMMs) comprehension of temporal changes by designing spatial-temporal QA pairs, which focus on reasoning about spatial configurations evolving over time (Figure 2). 3.2 Multimodal Spatial Instruction Data Generation While existing benchmarks like VSI-Bench [31] offer valuable evaluation sets (approximately 5,000 question-answer pairs from real-world videos), their scale is often insufficient for comprehensively developing robust spatial reasoning capabilities in Large Multimodal Models (LMMs). Furthermore, many current LMMs, including large proprietary systems, still exhibit difficulties with fundamental spatial tasks such as inter-object distance measurement, challenge often linked to inadequate training data coverage and limitations in their spatial perception encoding. These identified gaps underscore the necessity for the scalable, automated data generation approach previously outlined. Through this pipeline, we aim to produce over 200K general question-answer pairs focused on spatial reasoning from monocular video, and 4,225 embodied route planning data instances using simulators, thereby injecting more robust spatial intelligence into LMMs. General Spatial Question-Answer Generation We consolidate diverse data sources into unified meta-information structure. For open-source 3D datasets that provide 3D geometry, semantic, and instance meta-informationincluding ScanNet [65], ScanNet++ [66], and ARKitScenes [67]we derive QA data targeting seven of the eight core task types in VSI-Bench. The foundation for our QA generation is detailed spatio-temporal scene graph (see more details in the supplementary). In this graph, each frame acts as temporal node, while object instances are distinct nodes possessing attributes like global and local coordinates and detailed semantic information. Such scene graph enables us to ascertain the precise global and local status of each instance, including its association with the camera, at every time step. Leveraging this rich, temporal relationship, we then automatically generate question-answer pairs for seven key tasks: Object Counting, Relative Distance, Relative Direction, Appearance Order, Object Size, Absolute Distance, and Room Size. Route Planing Data Generation The remaining task, Route Planning, involves generating highlevel navigation descriptions within an environment. While the VSI-Bench benchmark [31] includes limited set of only 194 human-annotated planning instances, this quantity is insufficient for robust model training. Therefore, to develop 3D VLM adept at general spatial route planning, we employ the Habitat simulator [68] to generate accurate and plausible navigation routes at scale. 4 In this simulation task, an agent begins at specified starting location and receives human-generated navigation instruction that describes path to target goal location. The agent must follow this instruction by executing sequence of discrete actions (e.g., turning, moving forward) to navigate towards the goal, concluding with stop action upon arrival or task completion. Throughout this navigation process, we first sample thousands of navigable paths in the 3D scenes and then generate corresponding textual descriptions based on the agents position, orientation (including objects in view), and the specific path segments traversed. Object descriptions utilize accurate 3D ground-truth labels, selecting annotations that are relevant to the agents current viewpoint and location. Finally, these textual descriptions are used to formulate 4,225 suitable QA pairs conforming to the VSI-Bench route planning task format. 3.3 Visual-Spatial-Temporal Intelligence Benchmark Beyond assessing only global comprehension and static understanding of 3D environments, we introduce the Visual-Spatial-Temporal Intelligence Benchmark. This benchmark is designed to enable AI agents not only to answer global questions based on input video but also to perform reasoning about objects, cameras, and their relationships as they evolve over time. Its primary aim is to evaluate and enhance the spatial-temporal reasoning abilities of egocentric video-based Large Multimodal Models (LMMs). Figure 2 depicts the distribution of tasks alongside representative examples. Task Definition: Temporal Reasoning While Large Multimodal Models (LMMs) often effectively process static scene descriptions, evaluating their temporal reasoning from monocular video is crucial for advancing spatial-temporal understanding. The tasks within our VSTemporalI-Bench are specifically designed for this purpose, probing an LMMs ability to perceive and reason about relative camera/object motion, dynamic object-camera relationships, and evolving spatial configurations. As detailed in Figure 2(a), VSTemporalI-Bench comprises approximately 138,600 question-answer pairs. These are distributed across three primary categories: Camera Dynamics (49.6%), Camera-Object Interactions (38.4%), and Object Relative Position (12.0%). For example, tasks evaluating Camera Dynamics might ask questions like: \"Approximately how far (in meters) did the camera move between frame 1 and frame 16 of 32?\" We provide further details for each meta-class and category in the supplementary materials. All current tasks in VSTemporalI-Bench utilize static environments; scenarios incorporating independently moving objects are planned as future work. Temporal Question-Answer Generation The question-answer pairs for VSTemporalI-Bench originate from diverse 3D datasets (ScanNet [65], ScanNet++ [66], and ARKitScenes [67]), which feature videos captured by hand-held monocular cameras, often accompanied by ground-truth depth maps and instance annotations from varied settings. To generate these QA pairs, we construct detailed spatio-temporal scene graph as detailed in previous section. This graph explicitly models each object instance with its geometric attributes (e.g., position, size, rotation) and semantic labels, alongside precise camera viewpoint information (pose) at every relevant time step, thereby capturing the global and local status of instances and their evolving associations with the camera. By leveraging this rich scene graph, we can accurately compute various temporal changes (such as net camera displacement) that define our tasks. This detailed, structured understanding then enables the automated formulation of diverse question-answer pairs that interrogate camera dynamics, object states, and complex camera-object interactions over time, as exemplified in Figure 2(b). Metrics For Multiple-Choice Answer (MCA) tasks we use standard Accuracy (ACC) [69, 70, 71] via exact (or possible fuzzy) matching. For Numerical Answer (NA) tasks we utilize Mean Relative 1(cid:0)ˆy y/y < 1 θ(cid:1), which (cid:80) Accuracy (MRA) [31], defined as MRA = 1 10 captures prediction proximity across multiple tolerance levels. θ{0.5,0.55,...,0.95}"
        },
        {
            "title": "4 VLM-3R Architecture",
            "content": "Overview Figure 3 illustrates the architecture of VLM-3R. During both training and inference, the primary inputs are monocular video, represented as sequence of frames {It}N t=1 (where each frame It RHW 3), and accompanying language instructions. The model extracts visual, geometric, and camera pose tokens from this video via its end-to-end architecture; these tokens are subsequently aligned with language representations using instruction tuning, facilitated by our curated data and few learnable layers. 5 Figure 3: Network Architecture. Our method takes monocular video and language instruction as input. Visual Encoder coupled with Spatial Encoder extract frame-level appearance, camera view position, and globally aligned geometry. Visual-Geometry Fusion consists of 2D-3D attention and one layer projector were adopted to integrate the spatial context into latent tokens. During the inference stage, our approach is capable of performing reliable spatial and temporal reasoning thanks for the fusion of global geometry and frame-level view token. 4.1 End-to-end VLM Augmented by Multi-view Geometry We propose VLM-3R, vision-based LMM designed for 3D scene understanding and reasoning. Its architecture features pre-trained LMM [72], integrated with modules for deriving geometric encodings, camera view encodings, and visual features from the input video; these diverse inputs are subsequently fused effectively with language representations. As shown in Figure 3, VLM-3R does not rely on pre-built 3D maps or external depth sensors. This design directly addresses key limitations of existing approaches: the common inadequacy of Video LLMs in perceiving rich spatial context from monocular video, and the restrictive dependency of many specialized 3D-LLMs on prior 3D map or depth sensor inputs. 3D Reconstructive Tokenization. Many existing Video LLMs, such as LLaVA and its successors, primarily focus on aligning visual encodings from pre-trained image encoders (e.g., CLIP) with the text latent space. Consequently, they often fall short in capturing detailed scene geometry at scale. To imbue our Video LLM with robust 3D geometric understanding for our VLM-3R framework, we adopt the pre-trained CUT3R model [64] as core component for 3D reconstructive tokenization. CUT3R processes monocular video frame-by-frame: each incoming image It is first passed through an image encoder, fenc (e.g., Vision Transformer), to extract initial feature tokens Ft. Subsequently, these tokens Ft, along with learnable pose query token and the previous recurrent state st1, are processed by transformer-based decoder module, fdec. This two-stage operation yields the updated state st, context-aware image tokens Ft = fenc(It), (1) Following this, dedicated prediction heads utilize these enriched tokens (F t) to output 3D point maps Pmapt and the relative camera pose (ego-motion) Tt. Rather than directly fusing explicit point clouds which can be sparse, non-uniformly sized across frames, and challenging to encode we leverage the implicit latent representations. Specifically, the enriched feature tokens and the camera view tokens (derived from fdec) collectively serve as our rich 3D reconstructive tokens, compactly encoding the observed 3D geometry and camera perspective. To ensure consistency in the number of tokens generated by the visual and these reconstructive (spatial) encoders, input images are resized to standard size (e.g., 432 432 pixels). During subsequent model training, the weights of both the pre-trained visual encoder and the CUT3R-based spatial encoders (fenc, fdec) are frozen. Spatial-visual View Fusion. VLM-3R integrates 3D geometric information using specialized 3D reconstructive tokens derived from our 3D tokenization process (Sec. 4.1). These tokens comprise concatenated enriched 3D feature tokens and camera view tokens t, which together form unified 3D representation Z3D = Concat(F t). We then fuse this Z3D representation with the VLMs native visual tokens Hv (extracted from video frames by its pre-trained visual encoder, e.g., CLIP ViT) via cross-attention mechanism. In this cross-attention block, the VLMs visual tokens Hv serve as queries, attending to the unified 3D representation Z3D which provides both keys and values. This process yields 3D-aware visual tokens v: , and pose-related output tokens ], st = fdec([z, Ft], st1) t, as formulated below: and t, , [z = softmax (cid:18) (HvWQ)(Z3DWK)T dk (cid:19) (Z3DWV ) (2) 6 where WQ, WK, WV are learnable projection matrices and dk is the key dimension. These 3Dinfused visual tokens subsequently pass through two-layer projector, strategy also employed in models like LLaVA-Next-Video [72]. The resulting projected tokens, now fully contextualized with 3D information, replace the original Hv and are combined with language instruction tokens Hinstruct (e.g., as [H v; Hinstruct]) to form the input sequence for the LMMs transformer backbone. End-to-end supervised fine-tuning (SFT) on our curated reconstructive-instructional data then enables the LMM to jointly reason over visual appearance, 3D geometric priors, and camera perspective, significantly enhancing 3D spatial context understanding. We incorporate skip connection, described in detail in the appendix. Training Objective and Fine-tuning Strategy For training VLM-3R, we adopt the same learning objective as LLaVA-NeXT-Video. To achieve efficient adaptation, we employ Low-Rank Adaptation (LoRA) [73] for fine-tuning for VLM, and involves updating parameters within 3D fusion attention block and the projection layers."
        },
        {
            "title": "5 Experiments",
            "content": "Implementation Details 5.1 Baselines Extending prior Visual-Spatial intelligence work [31], which presented evaluations of 16 diverse video LMMs, we consider these as our baselines. These include proprietary models (Gemini- [14], GPT-4o [13]) and various open-source ones such as InternVL2 [74], ViLA [48], LongViLA [75], LongVA [76], LLaVA-OneVision [77], and LLaVA-NeXT-Video [72]. For fine-tuning the VLM, we employed Low-Rank Adaptation (LoRA [73]) configured with an update matrix rank of 128 and an adaptation scaling parameter of 256. The visual and spatial encoders are frozen with one attention block for alignment. Training was conducted for one epoch over 5 hours using 16 NVIDIA H200 GPUs. Datasets We evaluate methods on VSI-Bench [31], for Video-Language Models 3D understanding using visual cues in Table 1. It features configurational tasks (object count, relative distance, relative direction, route plan) assessing spatial configuration comprehension (appearance order), alongside measurement estimation tasks (object size, room size, absolute distance) valuable for embodied agents. For temporal dynamics comprehension, we also use VSTemporalI-Bench, assessing camera, cameraobject, and inter-object dynamics. Tasks include CameraObject Absolute Distance, CameraObject Relative Distance, ObjectObject Relative Position, Camera Displacement, and Camera Movement Direction. 5.2 Evaluations Evaluation on VSI-Bench VLM-3R ranks as the top-performing model among those with under 7B parameters and even outperforms certain 72B ones and proprietary models, demonstrating highly competitive performance thanks to its reconstructive instruction tuning. We observe that incorporating spatial encoding significantly boosts LMM capabilities, particularly leading to substantial improvements in distance, size, and direction estimation tasks. On the appearance order task, which is less reliant on detailed spatial geometry awareness, VLM-3Rs performance improved from 24.4 to 40.1, though this still lags behind that of larger-scale models (e.g., 72B). We also compare LMM performance against key baselines: Chance Level (frequency), representing the score achievable by always selecting the most frequent answer; Chance Level (Random), random selection strategy; and human-level performance. Unsurprisingly, human evaluators excel in planning and tasks requiring reasoning over long-horizon visual cues, such as appearance order. However, they perform less accurately than leading LMMs in precise spatial distance estimation. Evaluation on VSTI-Bench To evaluate LMMs capabilities in spatial reasoning over time, we report performance on VSTI-Bench, which assesses the understanding of temporally evolving dynamics. Table 2 details these tasks, which involve interpreting complex events in monocular videos (e.g., tracking static objects, moving cameras, and their interactions across space and time). On VSTI-Bench, VLM-3R effectively demonstrates its understanding of both spatial context and temporal movement from pre-trained geometry encoder, enabling it to answer questions and make inferences about video content. We finetune VLM-3R on the VSTI-Bench training set and evaluate all models on the shared evaluation split. u j. t. s. z j. z o Methods Rank Avg. Numerical Answer t. l. e r. Multiple-Choice Answer P o r. l. Baseline Chance Level (Random) Chance Level (Frequency) VSI-Bench Perf. (= Tiny Set) Human Level Gemini-1.5 Flash Gemini-1.5 Pro Gemini-2.0 Flash Proprietary Models (API) GPT-4o Gemini-1.5 Flash Gemini-1.5 Pro Open-sourced VLMs LLaVA-OneVision-0.5B InternVL2-2B LLaVA-NeXT-Video-7B InternVL2-8B LLaVA-OneVision-7B LongVA-7B VILA-1.5-8B LongVILA-8B InternVL2-40B VILA-1.5-40B LLaVA-NeXT-Video-72B LLaVA-OneVision-72B VLM-3R (7B) - - - - - - 3 2 1 11 12 5 6 7 9 10 13 4 8 2 3 1 - 34.0 79.2 45.7 48.8 45.4 34.0 42.1 45.4 28.0 27.4 35.6 34.6 32.4 29.2 28.9 21.6 36.0 31.2 40.9 40.2 60. - 62.1 94.3 50.8 49.6 52.4 46.2 49.8 56.2 46.1 21.8 48.5 23.1 47.7 38.0 17.4 29.1 34.9 22.4 48.9 43.5 70.2 - 32.0 47.0 33.6 28.8 30. 5.3 30.8 30.9 28.4 24.9 14.0 28.7 20.2 16.6 21.8 9.1 26.9 24.8 22.8 23.9 49.4 - 29.9 60.4 56.5 58.6 66.7 43.8 53.5 64.1 15.4 22.0 47.8 48.2 47.4 38.9 50.3 16.7 46.5 48.7 57.4 57.6 69. - 33.1 45.9 45.2 49.4 31.8 38.2 54.4 43.6 28.3 35.0 24.2 39.8 12.3 22.2 18.8 0.0 31.8 22.7 35.3 37.5 67.1 25.0 25.1 94.7 48.0 46.0 56. 37.0 37.7 51.3 28.9 33.8 43.5 36.7 42.5 33.1 32.1 29.6 42.1 40.5 42.4 42.5 65.4 36.1 47.9 95.8 39.8 48.1 46.3 41.3 41.0 46.3 36.9 44.2 42.4 30.7 35.2 43.3 34.8 30.7 32.2 25.7 36.7 39.9 80. 28.3 28.4 95.8 32.7 42.0 24.5 31.5 31.5 36.0 34.5 30.5 34.0 29.9 29.4 25.4 31.0 32.5 34.0 31.5 35.0 32.5 45.4 25.0 25.2 100.0 59.2 68.0 55. 28.5 37.8 34.6 5.8 7.1 30.6 39.6 24.4 15.7 24.8 25.5 39.6 32.9 48.6 44.6 40.1 Table 1: Evaluations on VSI-Bench. VLM-3R ranks first among open-sourced VLMs, showcasing the effectiveness of its reconstructive instruction tuning. This validates our models spatial encoding significantly improves 3D understanding and reasoning, particularly in distance, size, direction, and spatial planning tasks. For each task within the open-sourced VLMs group, Dark gray highlights the overall best-performing model; light gray denotes the second-best open-source model. Results on the VSI-Bench tiny set are presented following the setup in [31]. 5.3 Ablation Studies We conduct ablation studies to assess the effectiveness and contribution of our proposed multi-modal data, and the model components. All ablated model variants are trained using our 200K Multimodal Spatial Instruction Data (detailed in Sec. 3.2). Evaluations are performed on VSI-Bench following the identical experimental setup used for our full VLM-3R model, with results presented in Table 3. Effectiveness of Geometry Tokens Fusion To isolate the impact of our proposed geometric token fusion, we compare the full VLM-3R with VLM-3R w/o Geometry Token, where these specific 3D structural cues are omitted. As evidenced in Table 3, removing the geometry tokens leads to notable performance degradation. This drop is particularly pronounced in tasks heavily reliant on understanding scene structure and object properties, such as Relative Distance (65.35 (cid:55) 61.27) and Route Planning (45.36 (cid:55) 41.75). This highlight the role of explicit geometric token fusion in enabling the VLM to effectively perceive and reason about the spatial context of the 3D scene. Effectiveness of Camera Tokens Fusion We evaluate the contribution of camera tokens (VLM-3R w/o Camera Token). The results, detailed in Table 3, reveal notable performance decrease when camera tokens are omitted, particularly on tasks requiring the distinguishing of direction. For instance, performance on the Relative Direction task decreased from 80.52 to 78.86. This suggests that camera tokens, which encode camera pose and ego-motion information, are vital for the VLM to correctly interpret its egocentric viewpoint and to effectively disentangle the true spatial context from camera movement and perspective changes over time. Effectiveness of The Overall 3D Fusion The efficacy of our proposed 3D tokenization and crossmodal fusion layer (projector), which integrate visual, geometric, and camera token information for the LMM, is evident from Table 3. When comparing our full VLM-3R with the fine-tuned LLaVA-NeXT-Video ft (w/o C&G Tok.) baseline (which, despite being trained on our multimodal visual-spatial data, inherently lacks VLM-3Rs specialized 3D tokens and 3D fusion projector), 8 t. s. b - r. v. m. s. l. b - O t. l. b - e. p m. Methods Rank Avg. Numerical Answer Multiple-Choice Answer Baseline Chance Level (Random) Chance Level (Frequency) Human Performance Human Level Proprietary Models (API) GPT-4o Gemini-1.5 Flash Open-sourced VLMs LLaVA-OneVision-0.5B InternVL2-2B LLaVA-NeXT-Video-7B LLaVA-OneVision-7B LongVA-7B InternVL2-8B LongVILA-8B VILA-1.5-8B VILA-1.5-40B LLaVA-NeXT-Video-72B VLM-3R (7B) - - - 1 9 7 5 4 10 3 11 8 6 2 1 - 27.4 - 5.4 - 6.2 36.1 40.7 50.0 52. 36.1 32.4 77.0 51.4 46.8 95.1 97. 94.3 38.2 32.1 36.9 38.1 40.0 41.7 32.3 43.5 30.5 37.3 38.2 44.0 58.8 29.5 28.5 16.5 17.7 28.2 29.9 13.5 32.9 20.0 30.1 28.2 32.3 39.4 23.4 20. 32.4 27.8 1.8 19.3 5.1 13.5 11.6 27.3 15.7 10.5 39.6 37.3 24.4 46.1 43.0 49.8 47.5 43.7 48.0 35.4 42.2 28.8 48.1 60.6 58.1 52.6 50.5 54.9 64.7 62.1 57.9 68.0 52.3 50.4 65.4 78.3 86.5 42.5 33. 39.0 47.2 55.6 49.8 41.2 55.0 33.4 36.7 53.0 50.9 68.6 Table 2: Evaluations on VSTemporalI-Bench. VLM-3R demonstrates leading performance across all models on this benchmark, showcasing its strong capabilities in spatio-temporal reasoning. This highlights its effectiveness in understanding evolving camera dynamics, camera-object interactions, and inter-object relationships from monocular video. clear performance advantage for VLM-3R is observed (average score of 60.90 vs. 57.74). However, it is worth noting that for object size estimationa task typically requiring very high-quality geometryVLM-3R (69.15) performs comparably but slightly trails this specific baseline (70.82). Our hypothesis is that point maps reconstructed from monocular video, while generally effective, may not yet match the precision of sensor-derived depth or optimization-based 3D maps for assessing fine-grained details such as small object sizes. Improving this reconstruction quality presents an interesting avenue for future work. Methods LLaVA-NeXT-Video ft (w/o C&G Tok). VLM-3R w/o Cam. Tok. VLM-3R w/o Geo. Tok. VLM-3R (Full) Rank 4 3 2 1 Avg. 57.74 59.09 59.46 60. u j. 70.64 69.50 70.30 70.16 t. s. z j. Numerical Answer 43.67 48.66 49.27 49. 70.82 68.47 68.36 69.15 S R 63.72 65.21 66.01 67.12 t. l. e r. Multiple-Choice Answer P o r. l. 64.93 62.82 61.27 65.35 68.93 78.86 81.35 80.52 40.72 42.78 41.75 45.36 38.51 36.41 37.38 40.13 Table 3: Ablation Study of VLM-3R Components on VSI-Bench. This table illustrates the impact of key components in VLM-3R, specifically Geometry Tokens and Camera Tokens. The performance of the full VLM-3R model is compared against fine-tuned LLaVA-NeXT-Video baseline and VLM-3R variants with ablated components. Scores indicate percentage accuracy or an appropriate metric for each task. The VLM-3R (Full) model row is highlighted."
        },
        {
            "title": "6 Conclusion, Limitations, and Broader Impact",
            "content": "In this work, we introduce VLM-3R, novel framework that significantly enhances Vision-Language Models (VLMs) via reconstructive instruction tuning. By leveraging over 200K curated training data instances and our Spatial-Visual-View fusionwhich integrates geometric information with camera view contextVLM-3R empowers VLMs with robust 3D spatial understanding directly from monocular video. This approach notably eliminates the need for depth sensors or pre-computed 3D maps, offering scalable solution that also helps to disentangle object semantics from egocentric camera motion. Furthermore, to address critical gap in current evaluations, we propose new benchmark comprising approximately 138.6K question-answer pairs, specifically designed to assess the often overlooked temporal reasoning capabilities of these advanced models. However, the broader applicability and ultimate performance of such approaches are still intrinsically linked to, and constrained by, ongoing challenges in large-scale 4D data acquisition and the fidelity of underlying end-to-end 3D reconstruction techniques. 9 Broader Impact VLM-3R enhances 3D spatial reasoning from monocular video, promising benefits in areas like assistive robotics and augmented reality. However, its advanced environmental understanding risks misuse (e.g., in sophisticated surveillance), necessitating ethical deployment with robust safeguards to ensure responsible AI innovation."
        },
        {
            "title": "References",
            "content": "[1] Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad Usman, Naveed Akhtar, Nick Barnes, and Ajmal Mian. comprehensive overview of large language models. arXiv preprint arXiv:2307.06435, 2023. [2] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. TMLR, 2022. [3] Gašper Beguš, Maksymilian abkowski, and Ryan Rhodes. Large linguistic models: Analyzing theoretical linguistic abilities of llms. arXiv preprint arXiv:2305.00948, 2023. [4] Zhihao Zhang, Jun Zhao, Qi Zhang, Tao Gui, and Xuanjing Huang. Unveiling linguistic regions in large language models. In ACL, 2024. [5] Nora Kassner, Oyvind Tafjord, Ashish Sabharwal, Kyle Richardson, Hinrich Schütze, and Peter Clark. Language models with rationality. In EMNLP, 2023. [6] Alec Radford. Improving language understanding by generative pre-training. OpenAI Blog, 2018. [7] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. NeurIPS, 2020. [9] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [10] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [11] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. [12] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [13] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [14] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [15] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS, 2024. 10 [16] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. 2022. [17] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, 2023. [18] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. [19] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In CVPR, 2024. [20] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6):186345, 2024. [21] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In CVPR, pages 1445514465, 2024. [22] An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision language models. arXiv preprint arXiv:2406.01584, 2024. [23] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan. 3d-llm: Injecting the 3d world into large language models. In NeurIPS, 2023. [24] Chenming Zhu, Tai Wang, Wenwei Zhang, Jiangmiao Pang, and Xihui Liu. Llava-3d: simple yet effective pathway to empowering lmms with 3d-awareness. arXiv, 2024. [25] Duo Zheng, Shijia Huang, and Liwei Wang. Video-3d llm: Learning position-aware video representation for 3d scene understanding. arXiv, 2024. [26] Yining Hong, Zishuo Zheng, Peihao Chen, Yian Wang, Junyan Li, and Chuang Gan. Multiply: multisensory object-centric embodied large language model in 3d world. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2640626416, 2024. [27] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. [28] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. [29] Johannes Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 41044113, 2016. [30] Runsen Xu, Xiaolong Wang, Tai Wang, Yilun Chen, Jiangmiao Pang, and Dahua Lin. Pointllm: Empowering large language models to understand point clouds. In European Conference on Computer Vision, pages 131147. Springer, 2024. [31] Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. arXiv, 2024. [32] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2069720709, 2024. 11 [33] Vincent Leroy, Yohann Cabon, and Jérôme Revaud. Grounding image matching in 3d with mast3r. In European Conference on Computer Vision, pages 7191. Springer, 2024. [34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. [35] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In ICML, 2021. [36] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models. In ICML, pages 1973019742, 2023. [37] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, et al. Palm-e: An embodied multimodal language model. arxiv, 2023. [38] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv, 2023. [39] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. Unified-io: unified model for vision, language, and multi-modal tasks. arXiv, 2022. [40] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through simple sequence-to-sequence learning framework. In ICML, 2022. [41] Jiajun Deng, Tianyu He, Li Jiang, Tianyu Wang, Feras Dayoub, and Ian Reid. 3d-llava: Towards generalist 3d lmms with omni superpoint transformer. arXiv, 2025. [42] Haochen Wang, Yucheng Zhao, Tiancai Wang, Haoqiang Fan, Xiangyu Zhang, and Zhaoxiang Zhang. Ross3d: Reconstructive visual instruction tuning with 3d-awareness. arXiv preprint arXiv:2504.01901, 2025. [43] Howard Gardner. Frames of mind: The theory of multiple intelligences. 2011. [44] Timothy McNamara. How are the locations of objects in the environment represented in memory? In International conference on spatial cognition, pages 174191, 2002. [45] David Ed Waller and Lynn Ed Nadel. Handbook of spatial cognition. American Psychological Association, 2013. [46] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv, 2024. [47] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. Science China Information Sciences, 67(12):220101, 2024. [48] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. In CVPR, pages 2668926699, 2024. [49] Yukang Chen, Fuzhao Xue, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, et al. Longvila: Scaling long-context visual language models for long videos. arXiv, 2024. [50] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv, 2024. [51] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv, 2024. 12 [52] Mary Hegarty and Waller. Individual differences in spatial abilities. The Cambridge handbook of visuospatial thinking, pages 121169, 2005. [53] Chiara Meneghetti, Laura Miola, Tommaso Feraco, Veronica Muffato, and Tommaso Feraco Miola. Individual differences in navigation: an introductory overview. Prime archives in psychology, 2022. [54] Richard Hartley and Andrew Zisserman. Multiple view geometry in computer vision. Cambridge university press, 2003. [55] Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan. Mvsnet: Depth inference for unstructured multi-view stereo. In Proceedings of the European conference on computer vision (ECCV), pages 767783, 2018. [56] Xiaodong Gu, Zhiwen Fan, Siyu Zhu, Zuozhuo Dai, Feitong Tan, and Ping Tan. Cascade cost volume for high-resolution multi-view stereo and stereo matching. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 24952504, 2020. [57] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. [58] Zhenggang Tang, Yuchen Fan, Dilin Wang, Hongyu Xu, Rakesh Ranjan, Alexander Schwing, and Zhicheng Yan. Mv-dust3r+: Single-stage scene reconstruction from sparse views in 2 seconds. arXiv preprint arXiv:2412.06974, 2024. [59] Hengyi Wang and Lourdes Agapito. 3d reconstruction with spatial memory. arXiv preprint arXiv:2408.16061, 2024. [60] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. arXiv preprint arXiv:2503.11651, 2025. [61] Jianing Yang, Alexander Sax, Kevin Liang, Mikael Henaff, Hao Tang, Ang Cao, Joyce Chai, Franziska Meier, and Matt Feiszli. Fast3r: Towards 3d reconstruction of 1000+ images in one forward pass. arXiv preprint arXiv:2501.13928, 2025. [62] Zhiwen Fan, Jian Zhang, Wenyan Cong, Peihao Wang, Renjie Li, Kairun Wen, Shijie Zhou, Achuta Kadambi, Zhangyang Wang, Danfei Xu, et al. Large spatial model: End-to-end unposed images to semantic 3d. Advances in neural information processing systems, 37:4021240229, 2024. [63] Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, and Ming-Hsuan Yang. Monst3r: simple approach for estimating geometry in the presence of motion. arXiv preprint arXiv:2410.03825, 2024. [64] Qianqian Wang, Yifei Zhang, Aleksander Holynski, Alexei Efros, and Angjoo Kanazawa. Continuous 3d perception model with persistent state. arXiv preprint arXiv:2501.12387, 2025. [65] Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 58285839, 2017. [66] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner, and Angela Dai. Scannet++: highfidelity dataset of 3d indoor scenes. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1222, 2023. [67] Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry, Yuri Feigin, Peter Fu, Thomas Gebauer, Brandon Joffe, Daniel Kurz, Arik Schwartz, et al. Arkitscenes: diverse realworld dataset for 3d indoor scene understanding using mobile rgb-d data. arXiv preprint arXiv:2111.08897, 2021. 13 [68] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al. Habitat: platform for embodied ai research. In Proceedings of the IEEE/CVF international conference on computer vision, pages 93399347, 2019. [69] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. [70] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. [71] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal In Proceedings of the IEEE/CVF understanding and reasoning benchmark for expert agi. Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. [72] Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llava-next: strong zero-shot video understanding model, 2024. [73] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. [74] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. [75] Fuzhao Xue, Yukang Chen, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, et al. Longvila: Scaling long-context visual language models for long videos. arXiv preprint arXiv:2408.10188, 2024. [76] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. [77] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [78] Open-asset-importer-library (assimp). https://github.com/assimp/assimp, 2024."
        },
        {
            "title": "A Additional Implementation Details",
            "content": "A.1 VLM-3R Architecture Specifics Spatial Encoder Configuration For 3D scene reconstruction/understanding from monocular video, our VLM-3R utilizes feed-forward (end-to-end) dense 3D reconstruction model as its spatial encoder. This type of encoder directly maps sequences of input RGB images {Ii}N i=1, where each Ii R3HW , to 3D representations, including estimated camera intrinsics, extrinsics, and point maps that associate pixel information with 3D coordinates. To circumvent the extensive computational costs of using global point cloud and scaling difficulties across different scenes, we propose aligning the implicit encoder tokens instead. We employ the CUT3R model [64] to process sequences of input images. Given multiple images, such as frames {It} from monocular video stream, CUT3R directly outputs corresponding dense ) and relative camera poses ( ˆPt) for each view. 3D point maps (e.g., ˆX world and tokens are concatenated to form unified 3D representation, Z3D = Concat(F Spatial-Visual-View Fusion Design VLM-3R employs Spatial-Visual-View Fusion stage to integrate diverse multimodal inputs, as depicted in our overall architecture (e.g., Figure 3). This stage utilizes 3D reconstructive tokens from our spatial encoder [64]specifically, geometry tokens (encoding scene structure, dimension 729 768) and camera view tokens (capturing global camera motion, dimension 1 768). These 3D tokens are fused with 2D appearance-based visual tokens Hv (dimension 729 1152) derived from pre-trained visual encoder, such as CLIP ViT. Initially, the , t). This Z3D representation is then processed by one-layer cross-attention block where it interacts with the visual tokens Hv (e.g., Hv as queries attending to Z3D as keys and values). The output of this attention stage, lets call it Hattn, is then residually connected with the original visual tokens Hv to form an enriched representation subsequently passes through two-layer MLP projector, which transforms the feature dimensions (e.g., from 1152 to 3584, and then to 3584) for effective alignment with the Large Multimodal Model (LMM). Finally, these fused and projected 3D-aware visual features, which constitute the final visual input to the LMM, are concatenated with language instruction tokens for processing by the LMM backbone. = Hv + Hattn. This enriched A.2 Training Details The VLM-3R model was trained using the following configuration and hyperparameters. Base Model and Pretraining The training initialized from the LLaVA-Video-7B-Qwen2 checkpoint. The vision tower used was google/siglip-so400m-patch14-384. Hardware and Distributed Training Setup Training was conducted using distributed setup. The specific training run utilized 16 H200 GPUs and lasted for approximately 5 hours. For distributed training, NUM_GPUS_PER_NODE was set to 1, and the master address and port were dynamically determined using SLURM environment variables. The accelerate library with torchrun was used for launching distributed training, employing the DeepSpeed ZeRO stage 2 optimization (scripts/zero2.json). CPU affinity was set to 1 for the accelerate launcher. Key Hyperparameters LoRA Configuration: Enable LoRA: True (lora_enable True). LoRA rank: 128 (lora_r 128). LoRA alpha: 256 (lora_alpha 256). Training Epochs: The training was set for 5 epochs (num_train_epochs $NUM_TRAIN_EPOCHS), but concluded after the first epoch. Batch Size: Per device training batch size: 1 (per_device_train_batch_size 1). Gradient steps: $GRADIENT_ACCUMULATION_STEPS). accumulation 8 (gradient_accumulation_steps 15 Learning rate: 2 105 (learning_rate 2e-5). Optimizer and Scheduler: Weight decay: 0.0 (weight_decay 0.). Warmup ratio: 0.03 (warmup_ratio 0.03). LR scheduler type: cosine (lr_scheduler_type \"cosine\"). Precision: BF16 was enabled (bf16 True). TF32 was also enabled (tf32 True). Model maximum length: 32768 tokens (model_max_length 32768). Gradient Checkpointing: Enabled (gradient_checkpointing True). Spatial Module Configuration Spatial tower: CUT3R (spatial_tower \"cut3r\"). Spatial tower feature selection: all (spatial_tower_select_feature \"all\"). Spatial feature dimension: 768 (spatial_feature_dim \"768\"). Fusion block: cross_attention (fusion_block \"cross_attention\"). Tunable Components: Tune spatial tower: False (tune_spatial_tower False). Tune fusion block: True (tune_fusion_block True). Tune MM MLP adapter: True (tune_mm_mlp_adapter True)."
        },
        {
            "title": "B Dataset Curation and Benchmark Design",
            "content": "3D Reconstructive Instructional Tuning relies on large-scale Question-Answer (QA) pairs to fine-tune Large Multimodal Models (LMMs), enabling them to perform tasks related to spatial and temporal reasoning. We explain the construction of our 3D Reconstructive datasets, which are utilized for training models subsequently evaluated on VSI-Bench [31]. Furthermore, we detail the creation of the Visual-Spatial-Temporal Intelligence Benchmark (VSTI-Bench), new resource containing comprehensive training and evaluation pairs specifically for spatial-temporal tasks. B.1 Scalable 3D Reconstructive Instructional QA Creation Data Sources and Preprocessing Methodology Our training dataset for 3D Reconstructive Instructional QA is generated using methodology inspired by approaches such as those used in the VSI-Bench (Visual-Spatial Intelligence Benchmark). Data is sourced from established 3D scene datasets such as ScanNet [65], ScanNet++ [66], and ARKitScenes [67], which are unified and processed. The core of the data preparation involves detailed preprocessing pipeline to extract structured scene-level and frame-level metadata, similar to established practices. The input data requirements for each scene include: 1. Point Cloud (PCD): 3D point cloud (e.g., from .ply files) with per-point semantic labels, instance labels, coordinates, and color. 2. Video: An RGB video traversal of the scene. 3. Sampled Frame Data: collection of frames sampled from the video, including color images, depth maps, instance segmentation masks, and their corresponding 6DoF camera poses (4x4 transformation matrices) in the world coordinate system. 4. Camera Intrinsics: Parameters like focal length (fx, fy) and principal point (cx, cy). This raw data undergoes preprocessing pipeline that generates two primary types of structured metadata files in JSON format: scene_metadata.json: Contains scene-wide information, including overall scene dimensions, room center, counts of different object categories, and detailed 3D bounding boxes (center, size, orientation, instance ID) for every object instance within the scene. 16 frame_metadata.json: Contains frame-specific information for each scene, such as camera intrinsics, image dimensions, and for each sampled frame: its ID, paths to color/depth images, the camera pose, and 2D bounding boxes for visible object instances. These metadata files are crucial for the subsequent automated generation of diverse QA pairs for our training dataset. Scene Graph Construction Spatio-Temporal generated scene_metadata.json and frame_metadata.json files effectively serve as structured spatio-temporal scene graph. This representation includes precise 3D locations, sizes, and orientations of objects, their semantic and instance-level identities, and the dynamic viewpoint of the camera across multiple frames. Such detailed and organized metadata enables the querying of complex spatial relationships (e.g., object-object relations, object-camera distances) and spatiotemporal changes (e.g., camera movement, object appearance order). This structured understanding forms the foundation for the diverse QA tasks in our generated dataset. from Metadata The Spatial QA Generation Methodology The QA pairs for our training dataset are systematically generated using dedicated scripts for each task type, leveraging the aforementioned structured metadata. The generation logic for these tasks is the same as that used in VSI-Bench, and the data is intended for training models on various facets of visual-spatial intelligence. Configurational Tasks: These assess understanding of spatial layout and inter-object relationships. Object Count: Counting instances of specific object category in room (numerical answer; objects with single instances are excluded). Relative Distance: Determining which of four candidate objects is closest in 3D space to target object (multiple-choice answer). Relative Direction: Identifying the direction of query object relative to an observer at specific position and orientation (multiple-choice answer, e.g., left/right/back). Route Plan: Completing sequence of navigation actions (Go forward, Turn) to reach target object from starting object (multiple-choice answer). Measurement Estimation Tasks: These require quantitative estimation of spatial properties. Object Size: Estimating the length of the longest dimension of unique object instance in centimeters (numerical answer). Absolute Distance: Estimating the direct Euclidean distance between the closest points of two specified objects in meters (numerical answer). Room Size: Estimating the area of the room in square meters (numerical answer). Spatiotemporal Task: This tests the processing of spatial information over time. Appearance Order: Determining the first-appearance order of four object categories in the video sequence (multiple-choice answer). Route Plan Data Generation and Template Formatting To generate route planning data, we utilize the Habitat simulator [68] in conjunction with 3D scene data from sources including ScanNet [65], ScanNet++ [66], and ARKitScenes [67]. Mesh files from these datasets are converted from the .ply format to the .glb format using the assimp library [78]. These .glb files then serve as environments within Habitat, where its pathfinder function generates diverse navigation trajectories, as illustrated in Figure 4. The pathfinder computes navigable trajectory between two designated points within scene, outputting sequence of waypoints. These trajectories are then categorized: those containing turns (defined as change in direction greater than 30 degrees) are classified as with turns, while others are designated without turns. Trajectories with turns are further labeled as Turn Right or Turn Left, depending on the specific angle. For such trajectories, anchor points are defined at the start, the turn itself, and the end. The agent is conceptualized as starting at the initial point, facing the turn point (which serves as midpoint), and then proceeding to the endpoint. If turning angle exceeds 45 degrees, an alternative navigation mode is supported where the agent begins at this midpoint, faces the original endpoint, and navigates towards what was the original starting point. Trajectories 17 classified as without turns (i.e., without significant directional changes) are assigned Turn Back action. For these Turn Back paths, anchor points are defined at the start, midpoint, and end of the overall segment; the agent begins at the midpoint, oriented towards the starting point, and then moves to the ending point. For each anchor point along these generated trajectories, we identify the nearest object using available 3D bounding box annotations from the source datasets. This closest object then provides semantic description for that anchor point. Once these descriptive anchor points are established for given path, we formulate the final question-answer templates for our route planning tasks. Figure 4: Illustrative diverse trajectories generated by the Habitat simulator within single scene. These demonstrate fundamental navigation actions (e.g., Turn Right, Turn Left, Turn Back), which are foundational for generating route planning QA data via our specialized templates. We use SRC, TGT, and MID to represent the object description derived from the annotations. The templates contain two types: Templates for Route planning Template 1 \"You are robot beginning at the SRC facing the MID. You want to navigate to the TGT. You will perform the following actions (Note: for each [please fill in], choose either turn back, turn left, or turn right.): 1. Go forward until the MID. 2. [please fill in] 3. Go forward until the TGT. You have reached the final destination.\" Template 2 \"You are robot beginning at the MID facing the TGT. You want to navigate to the SRC. You will perform the following actions (Note: for each [please fill in], choose either turn back, turn left, or turn right.): 1. [please fill in] 2. Go forward until the SRC. You have reached the final destination.\" For the trajectories with \"Turn Right\" or \"Turn Left\", the corresponding description is Template 1 or Template 2. For \"Turn Back\", the description is Template 2. The descriptions and corresponding actions are paired as question-and-answer (QA) sets and then converted into multiple-choice format to serve as training data. 18 Task Type Object Relative Direction Object Absolute Distance Object Relative Distance Object Size Estimate Object Count Route Plan Room Size Total Count 86,441 50,757 42,025 12,917 9,357 4,225 2,057 207,779 Table 4: VSI-Bench Training Data Distribution by Task Type (Total QA pairs: 207,779). B.2 VSTemporalI-Bench: New Evaluation Benchmark (138.6K Set) Motivation and Design Principles Current multimodal models often struggle with spatio-temporal reasoning from monocular video. Even for static scene understanding, accurately interpreting camera motion, along with camera-object and object-object relative movements, remains challenging for LMMs. The VSTemporalI-Bench is thus motivated by the critical need to rigorously evaluate and drive progress in LMMs abilities to comprehend dynamic changes within 3D environments. Our core design principle is to create diverse question-answer pairs that probe the understanding of evolving spatial relationships, such as how camera displacement affects perceived object distances, or how the relative positions of objects change from the cameras perspective over sequence of frames. The benchmark therefore includes both frame-level tasks requiring precise spatial localization at specific moments and sequence-level tasks demanding an aggregation of information over time to determine motion, direction, or relational changes. This dataset aims to facilitate the examination and advancement of LMMs towards more reliable and human-like visual-spatial-temporal intelligence. Note that this benchmark currently focuses on static 3D scenes, where all depicted motion is from camera movement. Meta-classes, Task Categories, and Detailed Distribution The VSTemporalI-Bench (VSTIbench) is structured into two primary task categories based on the temporal scope of reasoning required: Frame-Level tasks and Sequence-Level tasks. These categories encompass five distinct types of questions designed to probe various aspects of visual-spatial and temporal understanding. The benchmark contains total of approximately 138.6K question-answer pairs. The generation of these QA pairs relies on processed metadata derived from raw 3D scene data (point clouds, videos, and sampled frames with depth, instance masks, and camera poses). Specifically, scene_metadata.json (containing 3D object bounding boxes, instance IDs, scene properties) and frame_metadata.json (containing per-frame camera poses as 4x4 matrices, camera intrinsics, and 2D object bounding boxes) serve as primary inputs to task-specific generation scripts. The task categories and their definitions are as follows: Frame-Level Tasks: These tasks generate questions based on information available within single frame or the scenes overall point cloud. 1. Camera-Object Absolute Distance QA: Generates questions asking for the approximate Euclidean distance (in meters, numerical answer) between the cameras position in specific frame and the closest point on the 3D bounding box of target unique object instance. Generation Logic: The 3D Euclidean distance is calculated between the cameras world coordinates (from the frames pose in frame_metadata.json) and the closest point on the 3D bounding box of the object (derived from scene_metadata.json). 2. Camera-Object Relative Distance QA: Generates multiple-choice questions asking which of several candidate object instances is closest to the camera in specific frame. Generation Logic: The 3D Euclidean distance is calculated between the cameras position (from the frames pose in frame_metadata.json) and the closest point on the 3D bounding box of each candidate object instance (derived from scene_metadata.json). The candidate object with the minimum distance to the camera is the correct answer. 3. Object-Object Relative Position QA: Generates multiple-choice questions asking whether specific target object instance is Near/Far, Left/Right, or Up/Down relative to another unique target object instance, from the cameras perspective in that frame. Generation Logic: The 3D bounding box vertices of both unique objects (from scene_metadata.json) are transformed into the cameras coordinate system for the given frame (using the camera pose from frame_metadata.json). For Near/Far, Zcoordinates are compared. For Left/Right, X-coordinates are compared. For Up/Down, Y-coordinates are compared. Only pairs where one object is entirely nearer/farther, left/right, or up/down than the other (by defined threshold) are used. Sequence-Level Tasks (Temporal Tasks): These tasks generate questions based on information aggregated across sequence of frames. (The generation logic for these tasks is detailed in the subsequent paragraph). 1. Camera Displacement QA: Generates questions asking for the approximate Euclidean distance (numerical answer) the camera traveled between two specified frames, calculated from the cameras world positions. 2. Camera Movement Direction QA: Generates multiple-choice questions about the primary direction of the cameras translation (e.g., Forward, Backward, Left, Right) during sequence, relative to its starting orientation. This is determined by analyzing the sequence of camera positions and transforming the overall displacement vector into the starting frames coordinate system. Data Splits (Train/Test) The VSTemporalI-Bench comprises total of 138,610 question-answer pairs, which are divided into training and testing splits as detailed below: VSTI Train (Total: 132,568 QA pairs) Task Group / Filename camera_displacement camera_movement_direction camera_obj_abs_dist camera_obj_rel_dist obj_obj_relative_pos Total Length 32,292 34,641 38,407 12,155 15,073 Table 5: VSTemporalI-Bench Training Set Distribution VSTI Test (Total: 6,042 QA pairs) Task Group / Filename camera_displacement camera_movement_direction camera_obj_abs_dist camera_obj_rel_dist obj_obj_relative_pos Total Length 839 913 905 1,740 1, Table 6: VSTemporalI-Bench Test Set Distribution"
        },
        {
            "title": "C Additional Experimental Analysis",
            "content": "Our error analysis, with quantitative results detailed in the ablation table in the main draft, provides insights into VLM-3Rs performance characteristics across different spatial reasoning tasks. For instance, in Absolute Distance estimation, the full VLM-3R (achieving 49.38) significantly outperforms the LLaVA-NeXT-Video ft (w/o C&G Tok.) baseline (43.67). This highlights the critical role of perceiving detailed spatial geometry, enabled by our models architecture, for accurate visual-based distance prediction at an absolute scale; ablating either camera or geometry tokens from VLM-3R also results in performance drops (to 48.66 and 49.27, respectively), further highlighting their individual contributions. 20 Conversely, for Object Counting, VLM-3R (70.16) performs comparably to both the baseline (70.64) and its own variant lacking geometry tokens (70.30). This suggests that the task, in its current VSI-Bench formulation, is less correlated with intricate 3D geometric understanding and may rely more substantially on 2D visual feature extraction and the models ability to track instances across frames."
        }
    ],
    "affiliations": [
        "Meta",
        "TAMU",
        "UCR",
        "UNC",
        "UT Austin",
        "XMU"
    ]
}