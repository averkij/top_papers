{
    "paper_title": "PerCoV2: Improved Ultra-Low Bit-Rate Perceptual Image Compression with Implicit Hierarchical Masked Image Modeling",
    "authors": [
        "Nikolai Körber",
        "Eduard Kromer",
        "Andreas Siebert",
        "Sascha Hauke",
        "Daniel Mueller-Gritschneder",
        "Björn Schuller"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce PerCoV2, a novel and open ultra-low bit-rate perceptual image compression system designed for bandwidth- and storage-constrained applications. Building upon prior work by Careil et al., PerCoV2 extends the original formulation to the Stable Diffusion 3 ecosystem and enhances entropy coding efficiency by explicitly modeling the discrete hyper-latent image distribution. To this end, we conduct a comprehensive comparison of recent autoregressive methods (VAR and MaskGIT) for entropy modeling and evaluate our approach on the large-scale MSCOCO-30k benchmark. Compared to previous work, PerCoV2 (i) achieves higher image fidelity at even lower bit-rates while maintaining competitive perceptual quality, (ii) features a hybrid generation mode for further bit-rate savings, and (iii) is built solely on public components. Code and trained models will be released at https://github.com/Nikolai10/PerCoV2."
        },
        {
            "title": "Start",
            "content": "PerCoV2: Improved Ultra-Low Bit-Rate Perceptual Image Compression with Implicit Hierarchical Masked Image Modeling Nikolai Korber1,2 Eduard Kromer2 Andreas Siebert2 Sascha Hauke2 Daniel Mueller-Gritschneder3 Bjorn Schuller1 1Technical University of Munich 2University of Applied Sciences Landshut 3TU Wien 5 2 0 M 2 1 ] . [ 1 8 6 3 9 0 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We introduce PerCoV2, novel and open ultra-low bitrate perceptual image compression system designed for bandwidthand storage-constrained applications. Building upon prior work by Careil et al., PerCoV2 extends the original formulation to the Stable Diffusion 3 ecosystem and enhances entropy coding efficiency by explicitly modeling the discrete hyper-latent image distribution. To this end, we conduct comprehensive comparison of recent autoregressive methods (VAR and MaskGIT) for entropy modeling and evaluate our approach on the largescale MSCOCO-30k benchmark. Compared to previous work, PerCoV2 (i) achieves higher image fidelity at even lower bit-rates while maintaining competitive perceptual quality, (ii) features hybrid generation mode for further bit-rate savings, and (iii) is built solely on public components. Code and trained models will be released at https: //github.com/Nikolai10/PerCoV2. 1. Introduction Perceptual compression, also known as generative compression [1, 44] or distribution-preserving compression [63], represents class of neural image compression techniques that integrate generative modelssuch as generative adversarial networks (GANs) [21] and diffusion models [28, 58]into their optimization objectives. Unlike traditional codecs like JPEG, which focus primarily on minimizing pixel-wise distortion, these methods further constrain reconstructions to align with the underlying data distribution [8]. By leveraging powerful generative priors, they can synthesize realistic details, such as textures, enabling superior perceptual quality at considerably lower bit-rates. These advantages make perceptual compression particularly compelling for storageand bandwidth-constrained applications. Recently, foundation models [9], large-scale machine learning models trained on broad data at scale, have shown Figure 1. Distortion-perception comparison on the Kodak dataset at 512 512 resolution (top left is best). We show different operating modes for PerCo and PerCoV2 by varying the number of sampling steps/ classifier-free-guidance; see Sec. 5.3. great potential in their adaption to wide variety of downstream tasks, including ultra-low bit-rate perceptual image compression [11, 37, 51]. Notably, PerCo [11], the current state-of-the-art, is the first method to explore bit-rates from 0.1 down to 0.003 bpp. This is achieved by extending the conditioning mechanism of pre-trained text-conditional latent diffusion model (LDM) with vector-quantized hyperlatent image features. As result, only short text description and compressed image representation are required during decoding. Despite its great potential and fascinating results, PerCo remains unavailable to the public, largely due to its reliance on proprietary LDM built upon the GLIDE [49] architecture. Although good community efforts have been made to bring PerCo to the public domain, e.g., PerCo (SD) [34], we find that its reconstructions typically deviate considerably from the original inputs. From this study, it becomes evident that, the design of the latent space and the LDM capacity play critical role for the overall perceptual compression performance. As for now, it remains unclear how the pro1 Original PICS [37] (ICML 2023 Workshop) MS-ILLM [48] (ICML 2023) DiffC (SD v1.5) [66] (ICLR 2025) PerCo (SD v2.1) [11, 34] (ICLR 2024) PerCoV2 (SD v3.0) Ours kodim10 0.02038 bpp (6.62) 0.00730 bpp (2.37) 0.00415 bpp (1.35) 0.00339 bpp (1.1) 0.00308 bpp kodim21 0.01788 bpp (4.97) 0.00781 bpp (2.17) 0.00421 bpp (1.17) 0.00388 bpp (1.08) 0.00360 bpp kodim13 0.01965 bpp (5.2) 0.00757 bpp (2.0) 0.00464 bpp (1.23) 0.00406 bpp (1.07) 0.00378 bpp kodim07 0.02248 bpp (6.4) 0.00793 bpp (2.26) 0.00491 bpp (1.4) 0.00372 bpp (1.06) 0.00351 bpp Figure 2. Visual comparison of PerCoV2 on the Kodak dataset at our lowest bit-rate configuration. Bit-rate increases relative to our method are indicated by (). For comparisons at higher bit-rates, see Fig. 5. Best viewed electronically. prietary LDM performs in comparison to existing off-theshelf models, given the current analysis of the consistencydiversity-realism fronts [3]. To address this gap and better quantify our observations, we propose PerCoV2, novel and open ultra-low bit-rate perceptual image compression system based on the Stable Diffusion 3 architecture [18]. PerCoV2 is optimized using the powerful flow matching objective [42], while also benefiting from Stable Diffusions enhanced auto-encoder design and increased LDM capacity (8B). PerCoV2 further introduces several architectural improvements. Most importantly, we show that incorporating dedicated entropy model within the learning objective can considerably improve entropy coding efficiency. Different from competing methods (e.g., [40]), we keep the VQ-based encoding design and model the discrete hyper-latent image distribution using an implicit hierarchical masked image model. For that, we conduct comprehensive comparison of recent autoregressive methods (VAR [62] and MaskGIT [12]) for entropy modeling and evaluate our approach on the large-scale MSCOCO-30k benchmark. While we acknowledge that prior works [17, 45] have explored MaskGIT-inspired entropy coding, we note that these approaches have not been made publicly available and no direct performance comparisons have been conducted (e.g., between the quincunx and QLDS masking schedules). In contrast, we offer thorough evaluation for the ultra-low bit-range and extend this line of research by novel entropy model, drawing inspiration from the recent success of visual autoregressive models [62]. Furthermore, we demonstrate its advantages in hybrid compression/generation mode, potentially enabling further bit-rate savings. Compared to previous work, PerCoV2 particularly excels at the ultra-low to extreme bit-rates (0.003 0.03 bpp), 2 achieving higher image fidelity at even lower bit-rates while maintaining competitive perceptual quality, see Figs. 2 and 5. At higher bit-rates, we find PerCoV2 to be less effective, confirming recent observations that better autoencoder reconstruction ability does not necessarily lead to improved overall generation performance [54]. In summary, our contributions are as follows: We introduce PerCoV2, novel and open ultra-low bitrate perceptual image compression system based on the Stable Diffusion 3 architecture [18]. PerCoV2 builds upon previous work by Careil et al. [11] and improves entropy coding efficiency by integrating dedicated entropy model into the learning objective. We conduct comprehensive comparison of recent autoregressive entropy modeling techniquesincluding VAR [62] and MaskGIT [12]and demonstrate the benefits of our approach for both compression and generation in the ultra-low bit-range. We empirically evaluate our method on the MSCOCO30k and Kodak datasets, showing that PerCoV2 delivers more faithful reconstructions while preserving high perceptual quality compared to strong baselines (PerCo [11, 34], MS-ILLM [48], DiffC [66], and DiffEIC [40]). 2. Related Work Generative/ Foundation Models. Generative models form the foundation of modern state-of-the-art text-to-image systems, such as GLIDE [49], GigaGAN [31], and Stable Diffusion [18, 56]. Early approaches primarily relied on single-step generation models like GANs [21]. However, recent advancements have shifted towards iterative refinement paradigms, notably diffusion models [28, 58], which benefit from improved training formulations and more efficient sampling strategies [59]. More recently, flow models [2, 42, 43] have gained popularity due to their simpler formulation and efficient training and sampling processes. These advantages make them the foundation of newer models like Stable Diffusion 3 [18] and FLUX1. Latent diffusion models [56] have also played crucial role in enabling scalable and efficient training by operating in compressed latent spaces. Concurrently, autoregressive approaches have demonstrated competitive performance with diffusion transformers and are emerging as strong alternative for text-toimage generation [22, 62]. Foundation models [9], trained on large-scale multimodal datasets, further enhance generalization and adaptability across generative tasks [18, 39, 52]. Perceptual Image Compression. The Rate-DistortionPerception (RDP) trade-off formalizes the observation that 1https://huggingface.co/black-forest-labs/FLUX.1-dev higher pixel-wise fidelity does not necessarily lead to better perceptual quality [8]. Early work in learned image compression showed that neural networks can outperform traditional codecs [5, 60]. Inspired by these results, follow-up work has focused on building more sophisticated entropy models [6, 23, 24, 47] and network architectures [24, 46, 75]. Other work combined these methods with generative models, including GANs [1, 35, 44, 48, 63] and diffusion models [20, 29, 71], demonstrating improved perceptual quality. Good performance has also been reported by VQ-VAE [64]-inspired approaches [17, 30, 45]. Recent work has explored foundation models as strong generative priors for neural image compression [11, 37, 38, 40, 51, 69], with differences in conditioning modalities and finetuning methods. These techniques include prompt inversion and compressed sketches [37, 68], text descriptions generated by commercial large language model (GPT4 Vision [50]), semantic label maps combined with compressed image features [38], CLIP-derived image embeddings and color palettes [4, 52], as well as textual inversion paired with variation of classifier guidance known as compression guidance [16, 19, 51]. distinct approach is taken by Relic et al. [55], which formulates quantization error removal as denoising problem, aiming to restore lost information in the transmitted image latent. With the exception of PerCo [11], all these methods incorporate some version of Stable Diffusion [56], such as ControlNet [73], DiffBIR [41], and Stable unCLIP [56], while keeping the official model weights unchanged. Finally, promising direction is compression with diffusion models and reverse channel coding [28, 61]. We compare against DiffC [66], which constitutes the first practical prototype for this line of work. 3. Background Neural Image Compression. Neural image compression uses deep learning techniques to learn compact image representations. This is typically achieved by an auto-encoderlike structure, consisting of an image encoder = E(x), decoder = D(y) that operates on the quantized latent representation y, and an entropy model (y). The learning objective is to minimize the rate-distortion trade-off [14], with λ > 0: LRD = ExpX [λr(y) + d(x, x)]. (1) In Eq. (1), d(x, x) captures the distance of the reconstruction to the original input image x, whereas the bitrate is estimated using the cross entropy r(y) = log (y). In practice, an entropy coding method based on the probability model is used to obtain the final bit representation, see [72] for more general overview. 3 Visual Autoregressive Models. Visual autoregressive models (VAR) [62] form novel hierarchical paradigma for autoregressive image modeling. The core idea is to represent images as multi-scale residual token maps (r1, r2, ..., rK), each at increasingly higher resolution hk wk. The autoregressive likelihood is expressed as: p(r1, r2, . . . , rK) = (cid:89) k=1 p(rk r1, r2, . . . , rk1). (2) At each step, the generation of rk is conditioned on its prefix r1, r2, . . . , rk1. Note that in this context, the prefix corresponds to an entire token map, which is generated in parallel. This is different from the traditional languageinspired raster-scan next-token prediction scheme and better mimics the human visual system. In practice, GPT-2-like transformer with block-wise causal attention is trained on top of pre-trained multi-scale VQ-VAE [64]. During inference, kv-caching is enabled to sequentially sample from the generative model. Flow Models. Flow models [2, 42, 43] are another popular choice for generative modeling. flow, ϕ : [0, 1] Rd Rd is time-dependent function that characterizes the transition from (simple) prior distribution p0 to target distribution via the ordinary differential equation (ODE): dt ϕt(x) = vt(ϕt(x)), ϕ0(x) = x; (3) where : [0, 1] Rd Rd is time-dependent vector field. Assuming we have access to target vector field ut and its corresponding probability density path : [0, 1] Rd R>0, the flow matching objective is given by: LFM(θ) = Et,pt(x) vt(x) ut(x)2 , which boils down to directly regressing the vector field ut via neural network vt(x, θ). During inference, an ODE solver can then be used to sample from the generative model. In practice, however, we do not have access to ut in closed form that generates pt. (4) Gaussian distribution concentrated around x1, while for all other pt the mean and standard deviation simply change linearly in time. The corresponding conditional vector field ut and conditional flow ϕt(x) are given by: ut(xx1) = x1 (1 σmin)x 1 (1 σmin)t , ϕt(x) = (cid:0)1 (1 σmin)t(cid:1)x + tx1. (7) (8) The final learning objective is obtained by reparameterizing pt(xx1) in terms of just x0: LCFM(θ) = Et,q(x1),p(x0) vt(ϕt(x0)) (x1 (1 σmin)x0)2 . (9) 4. Our Method 4.1. Overview We present an overview of our model in Fig. 3. PerCoV2 retains the core design principles of PerCo [11], with two notable differences: i) we replace the proprietary LDM with an open alternative based on the Stable Diffusion 3 [18] architecture (Sec. 4.2), and ii) we enhance entropy coding efficiency by explicitly modeling the discrete hyper-latent image distribution (Sec. 4.3), drawing inspiration from recent advancements in masked image modeling [12, 17, 45, 62]. 4.2. Open Perceptual Compression PerCoV2 consists of the following components: Stable Diffusion 3: An LDM encoder and decoder, text encoders (CLIP-G/14 [52], CLIP-L/14 [52], and T5 XXL [53]), and latent flow model [18, 43]. Feature extractors: an image captioning model (e.g., BLIP 2 [39] or Molmo [15]) and hyper-encoder. VAR/MIM: discrete entropy model. We denote the LDM encoder and hyper-encoder as and tractable yet equivalent learning objective is the conH, respectively. ditional flow matching objective [42]: LCFM(θ) = Et,q(x1),pt(xx1) vt(x) ut(xx1)2 , (5) which defines conditional probability path pt and conditional vector field ut per sample x1 q(x1). For practical applications, pt typically takes the form: pt(xx1) = (x µt(x1), σt(x1)2I), (6) with µt(x) = tx1 and σt(x) = 1 (1 σmin)t, such that for p0, we get standard Gaussian distribution; for p1, Encoding. Given an image of shape 3, PerCoV2 extracts side information to better adapt the flow model for compression. This side information is represented as = (zl, zg), where zl and zg correspond to local and global features, respectively. The local features zl are vector-quantized (VQ) hyperlatent representations, defined as zl = H(E(x)). The encoder maps the input image to latent representation of shape H/8 W/8 16, which is then processed by the hyper-encoder to yield zl with shape 320. The global features zg correspond to image captions generated by pre-trained large language model. 4 Figure 3. PerCoV2 model overview based on our lowest bit-rate configuration. Colors follow [18, Fig. 2]. Both zl and zg are then losslessly compressed using arithmetic coding and Lempel-Ziv coding. The bit rates are controlled by varying w, the VQ codebook size, and the number of tokens in the image caption. In PerCo [11], uniform entropy model is assumed for entropy coding. We discuss this design decision in Sec. 4.3. Decoding. At the decoder stage, the compressed representations (zl, zg) are decoded and subsequently fed into the conditional flow model. Following standard practices, zl is concatenated with the noised latents along the channel dimension (see Fig. 3). Similarly, zg is passed to Stable Diffusions pre-trained text encoders to compute textual embeddings, which are incorporated into the flow model using cross-attention layers, see [18, Fig. 2]. Finally, the processed latents are passed into the LDM decoder to produce the final image reconstruction. 4.3. Hierarchical Masked Image Modeling straightforward method for transmitting vector-quantized hyper-latent features is uniform coding. In this approach, the indices of the feature embeddings (zl) are transmitted, assuming they are uniformly and independently distributed for entropy coding. The bit-rate is then given by r(zl) = hw log2 HW , (10) where is the size of the VQ codebook. In practice, however, this assumption does not hold, leading to suboptimal bit-rates [17, 30]. We explore two types of masked image transformers for discrete entropy modeling (see Fig. 3): the masked image model (MIM) [12], originally designed for image generation and later adapted for compression [17, 45], and the visual autoregressive model (VAR) [62], which, to the best of our knowledge, remains unexplored for image compression. Both MIM and VAR are autoregressive methods that model the image formation process in either an implicit or explicit hierarchical manner. For MIM, the autoregressive unit corresponds to subset of single-scale token map, whereas VAR uses an explicit multi-scale image representation, where the autoregressive unit is an entire token map. In the case of VAR, the VQ-module in Fig. 3 is replaced by multi-scale quantizer (see Sec. 3 and [62, Algorithm 1] for further details). In both methods, the image is modeled as product of conditionals over token subsets/maps. The joint probability distribution is factorized as: p(q) = (cid:89) k=1 p(qk Ck), (11) where = (q1, q2, . . . , qK) denotes the sequence of token subsets/maps, and Ck represents the context used to predict qk. For MIM, Ck is the context derived from previously predicted token subsets, whereas for VAR, Ck corresponds to the previously generated token maps. For p(q1 C1), we use uniform prior for compression. During training, MIM learns to predict missing tokens from randomly masked inputs, effectively constructing supernet that encompasses all possible masked combinations. During inference, deterministic masking schedule for entropy coding is required, which must be pre-established between the sender and receiver. In this work, we review the checkerboard [23], quincunx [17], and QLDS [45] variants and compare their performance to the VAR formulation. visual overview of the methods considered is provided in the Appendix, Figs. 15 to 18. 4.4. Optimization To train PerCoV2, we use two-stage training protocol. In the first stage, PerCoV2 is optimized using the conditional flow matching objective Eq. (9), extended by = (zl, zg): LCFM+(Θ) = Et,q(x1),p(x0) vt(ϕt(x0), z) (x1 (1 σmin)x0)2 . (12) with Θ = (θ1, θ2), denoting the model parameters of the flow model and hyper-encoder, respectively. We keep all other components frozen during optimization. Note that in PerCoV2, Eq. (12) is formulated in the latent space of the auto-encoder, rather than in the pixel space. This allows for more compact and efficient representation of the data. Finally, we note that Stable Diffusion 3 employs timedependent loss-weighting scheme [18, Sec. 3.1], which we omit in our notation for the sake of simplicity. As common in the literature, we drop the text conditioning zg in 10% of the training iterations. During inference, we apply classifier-free-guidance [11, 27]: = vt(ϕt(x0), (zl, )) + λ(cid:0)vt(ϕt(x0), (zl, zg)) vt(ϕt(x0), (zl, ))(cid:1). (13) In the second stage, we then proceed to train MIM/VAR, based on the previously learned hyper-encoder representations. For both methods, we use the standard cross-entropy loss for optimization; the resulting MIM/VAR can then be employed for both compression and generation. 5. Experimental Results Implementational Details. PerCoV2 builds upon the open reimplementation of PerCo (SD) [34] and is developed within the diffusers framework [65]. For training, we consider the OpenImagesV6 [36] (9M) and SA-1B [32] (11M) datasets. To generate captions, we compare the concise descriptions produced by BLIP2 [39] with the more detailed outputs of Molmo-7B-D-0924 [15]. Model training is conducted on DGX H100 system in distributed, multiGPU configuration (8 H100) with mixed-precision computation. To enhance efficiency, all captions are precomputed and loaded into memory at runtime. Our MIM/VAR models are derived from the VAR-d16 configuration [62], ensuring fair comparison with models of the same capacity. For MIM, we replace the masked tokens with learnable token, following [12]. Evaluation Setup. We follow the evaluation protocol of PerCo [11], assessing performance on the Kodak [33] and MSCOCO-30k [10] datasets at resolution of 512 512. These datasets contain 24k and 30k images, respectively. We report FID [26] and KID [7] to quantify perception, PSNR, MS-SSIM [67], and LPIPS [74] to quantify distortion, CLIP-score [25] to measure global alignment between reconstructed images and ground-truth captions, and mean intersection over union (mIoU) to assess semantic preservation [57]. For the latter, we use the ViT-Adapter segmentation network [13]. All evaluations are performed on single H100 GPU. Baselines. We compare PerCoV2 (SD v3.0) to PICS [37], MS-ILLM [48], DiffC [66], PerCo [11, 34], and DiffEIC [40]. For PerCo, we use both the official variant, if possible, and the open community reimplementation by Korber et al. [34]. For DiffC, we choose the Stable Diffusion v1.5 backbone over v2.1, which better reflects the target distribution (512 512px). We further compare against VTM-20.0, the state-of-the-art non-learned image codec, BPG-0.9.8, and JPEG. 5.1. Main Results We summarize our results on the MSCOCO-30k benchmark in Fig. 4. We report both our stage-one model, PerCo (SD v3), and our joint stage-one and stage-two model, PerCoV2 (SD v3), to better isolate the effect of different text-toimage backbones. By default, we use 20 sampling steps and λ = 3.0, chosen to match the official PerCo [11] perception scores. As our entropy model, we use MIM/ QLDS [45] (α = 2.2, = {5, 12}). Additionally, we report the Stable Diffusion auto-encoder bounds, namely SD v2.1 autoencoder and SD v3 auto-encoder, corresponding to PerCo (SD) [34] and our model variants, respectively. Compared to the PerCo line of work [11, 34], our model variants considerably improve all metrics at ultralow to extreme bit-rates (0.003 0.03 bpp) while maintain6 Figure 4. Quantitative comparison of PerCoV2 on MSCOCO-30k. ing competitive perceptual quality. However, at higher bitrates, they become less effective, e.g., compared to PerCo Interestingly, this occurs despite our use of (SD) [34]. higher-capacity auto-encoder, as measured by reconstruction ability (SD v2.1 vs. SD v3 auto-encoder). This aligns with recent findings [54], suggesting that more compact latent space combined with high-capacity LDM might be advantageous. Both PerCo (SD) and PerCo (official) employ 4-channel auto-encoder (vs. 16 in our case), paired with 866M and 1.4B LDMs, respectively. Compared to DiffEIC [40], our model variants consistently achieve better perception scores while also outperforming in distortion-oriented metrics (except at higher bitrates). This trend is also reflected in the visual comparisons (see Fig. 5). Notably, this is achieved despite DiffEIC using more sampling steps than the PerCo line (50 vs. 20). MS-ILLM [48] exemplifies the RDP trade-off [48]. While it dominates across all distortion metrics (PSNR, MS-SSIM, and LPIPS), it tends to produce blurry and unpleasing results (see Fig. 2). This also highlights that good distortion scores alone do not align well with machine vision, as confirmed by the mIoU scores. DiffC is excluded in our large-scale benchmark due to slow runtimes [66, Table 1]. We provide additional quantitative results and visual comparisons in the appendix. 5.2. Hierarchical Masked Entropy Modeling We summarize the effect of our MIM/VAR models in Tab. 1. As can be observed, all configurations successfully improve upon the baseline (uniform coding). The best results are achieved by QLDS [45], closely followed by the quincunx masking schedule [17]. Regarding our VAR formulation, we found the residual multi-scale quantizer [62] to be highly unstable, often leading to NaN values after several hundred iterations. It is interesting to note that, while both the transformer architecture and quantizer have been publicly released, details about the auto-encoder training protocol remain unavailable to the research community2. We also explored non-residual multiscale quantizer variants; however, we found that these lead to weaker stage-one models (for the ultra-low bit range). To still prove the general technical feasibility of this approach, we have devised an implicit hierarchical VAR variant, which directly extracts the feature maps from singlescale token map (see appendix, Figs. 14 and 18). We note that this formulation can generally also be achieved from the MIM variants directly we leave the exploration of better explicit hierarchical representations to future work. 2See GitHub discussion online: https : / / github . com / FoundationVision/VAR/issues/125 7 Original PerCo (SD v2.1) [11, 34] (ICLR 2024, NeurIPS 2024 Workshop) DiffEIC [40] (TCSVT 2024) PerCoV2 (SD v3.0) Ours kodim05 0.03293 bpp (1.09) 0.03015 bpp (1.0) 0.03027 bpp Figure 5. Visual comparison of PerCoV2 on the Kodak dataset at an extreme bit-rate configuration. Bit-rate increases relative to our method are indicated by (). Best viewed electronically."
        },
        {
            "title": "Method",
            "content": "bpp Savings (%) 6. Conclusion and Future Work Ultra-Low Bit-Rate Setting"
        },
        {
            "title": "0.00363\nBaseline\nImplicit Hierarchical VAR 0.00348\n0.00342\nCheckerboard [23]\n0.00340\nQuincunx [17]\n0.00340\nQLDS (S = 5) [45]",
            "content": "4.13 5.79 6.34 6.34 Extreme-Low Bit-Rate Setting Baseline Checkerboard [23] Quincunx [17] QLDS (S = 5) [45] QLDS (S = 12) [45] 0.03293 0.02854 0.02697 0.02667 0.02616 13.33 18.10 19.01 20.56 Table 1. Implicit vs. Hierarchical Entropy Modeling Methods. 5.3. Distortion-Perception Trade-Off We visualize various sampling steps and classifierfree guidance [27] configurations (1, 5, 10, 15, 20, 50 1.0, 3.0, 5.0) in Fig. 1. Smaller λ values and fewer sampling steps generally lead to higher PSNR scores but reduced perceptual quality. On the other hand, increasing the number of sampling steps improves perceptual quality, but at the cost of lower pixel-wise fidelity. We further observe that PerCoV2 achieves more consistent reconstructions across test runs compared to PerCo (SD) [34]. PerCoV2 also provides wider distortion-perception plane, offering more degrees of freedom. 8 In this work, we introduced PerCoV2, novel and open ultra-low bit-rate perceptual image compression system designed for bandwidthand storage-constrained applications. PerCoV2 extends PerCo [11] to the Stable Diffusion 3 ecosystem and enhances entropy coding efficiency by explicitly modeling the discrete hyper-latent image distribution. To achieve this, we proposed novel entropy model inspired by the success of visual autoregressive models [62] and evaluated it against existing masked image modeling approaches for both compression and generation. Compared to previous work, PerCoV2 particularly excels at ultra-low to extreme bit-rates, outperforming strong baselines on the large-scale MSCOCO-30k benchmark. At higher bit-rates, we found PerCoV2 to be less effective, suggesting that more compact auto-encoder representations might be beneficial [11, 54]. Other interesting directions for future work include finding better hierarchical representations for VAR-based entropy modeling, extending PerCo to other generative modeling domains (e.g., [22]), and addressing the high computational cost via more efficient network architectures [70]. Limitations. In its current state, PerCoV2 can only handle medium-sized images (512 512). This is not fundamental limitation and can be addressed via advanced training strategies; see [18, C.2. Finetuning on High Resolutions]. Finally, as with all generative models, PerCoV2 retains certain artistic freedom and is therefore not suitable for highly sensitive data (e.g., medical data)."
        },
        {
            "title": "Acknowledgments",
            "content": "This work was partially supported by the German Federal Ministry of Education and Research through the funding program Forschung an Fachhochschulen (FKZ 13FH019KI2). We also thank Lambda Labs for providing GPU cloud credits through their Research Grant Program, which helped us finalize this work. Special thanks to Marl`ene Careil for her valuable insights and evaluation data, and to Jeremy Vonderfecht for providing visuals of DiffC."
        },
        {
            "title": "References",
            "content": "[1] Eirikur Agustsson, Michael Tschannen, Fabian Mentzer, Radu Timofte, and Luc Van Gool. Generative adversarial networks for extreme learned image compression. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019. 1, 3 [2] Michael Samuel Albergo and Eric Vanden-Eijnden. BuildIn The ing normalizing flows with stochastic interpolants. Eleventh International Conference on Learning Representations, 2023. 3, 4 [3] P. Astolfi, M. Careil, M. Hall, O. Manas, M. Muckley, J. Verbeek, A. R. Soriano, and M. Drozdzal. Consistencydiversity-realism pareto fronts of conditional image generative models. arXiv: 2406.10429, 2024. 2 [4] Tom Bachard, Tom Bordin, and Thomas Maugey. CoCliCo: Extremely low bitrate image compression based on CLIP semantic and tiny color map. In PCS 2024 - Picture Coding Symposium, 2024. 3 [5] Johannes Balle, Valero Laparra, and Eero P. Simoncelli. End-to-end optimized image compression. In International Conference on Learning Representations, 2017. 3 [6] Johannes Balle, David Minnen, Saurabh Singh, Sung Jin Hwang, and Nick Johnston. Variational image compresIn International Conference sion with scale hyperprior. on Learning Representations, 2018. [7] Mikołaj Binkowski, Dougal J. Sutherland, Michael Arbel, and Arthur Gretton. Demystifying MMD GANs. In International Conference on Learning Representations, 2018. 6 [8] Yochai Blau and Tomer Michaeli. Rethinking Lossy Compression: The Rate-Distortion-Perception Tradeoff. In Proceedings of the 36th International Conference on Machine Learning, 2019. 1, 3 [9] Rishi Bommasani et al. On the opportunities and risks of foundation models. arXiv: 2108.07258, 2021. 1, 3 [10] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. CocoIn Proceedings stuff: Thing and stuff classes in context. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 6 [11] Marlene Careil, Matthew J. Muckley, Jakob Verbeek, and Stephane Lathuili`ere. Towards image compression with perfect realism at ultra-low bitrates. In The Twelfth International Conference on Learning Representations, 2024. 1, 2, 3, 4, 5, 6, 8, 12, 15, 16 [12] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T. Freeman. Maskgit: Masked generative image transformer. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 2, 3, 4, 5, 6 [13] Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision transformer adapter for dense predictions. In The Eleventh International Conference on Learning Representations, 2023. 6 [14] Thomas Cover and Joy Thomas. Elements of information theory. John Wiley & Sons, 2012. 3 [15] Matt Deitke et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv: 2409.17146, 2024. 4, 6, 12 [16] Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat GANs on image synthesis. In Advances in Neural Information Processing Systems, 2021. [17] Alaaeldin El-Nouby, Matthew J. Muckley, Karen Ullrich, Ivan Laptev, Jakob Verbeek, and Herve Jegou. Image compression with product quantized masked image modeling. Transactions on Machine Learning Research, 2023. 2, 3, 4, 5, 6, 7, 8, 22 [18] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 2, 3, 4, 5, 6, 8 [19] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik, and Daniel Cohen-or. An image is worth one word: Personalizing text-to-image generation using textual inversion. In The Eleventh International Conference on Learning Representations, 2023. 3 [20] Noor Fathima Khanum Mohamed Ghouse, Jens Petersen, Auke J. Wiggers, Tianlin Xu, and Guillaume Sautiere. Neural image compression with diffusion-based decoder. arXiv: 2301.05489, 2023. 3 [21] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems, 2014. 1, 3 [22] Jian Han, Jinlai Liu, Yi Jiang, Bin Yan, Yuqi Zhang, Zehuan Yuan, Bingyue Peng, and Xiaobing Liu. Infinity: Scaling bitwise autoregressive modeling for high-resolution image synthesis. arXiv: 2412.04431, 2024. 3, [23] Dailan He, Yaoyan Zheng, Baocheng Sun, Yan Wang, and Hongwei Qin. Checkerboard context model for effiIn Proceedings of the cient learned image compression. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 3, 6, 8, 22 [24] Dailan He, Ziming Yang, Weikun Peng, Rui Ma, Hongwei Qin, and Yan Wang. Elic: Efficient learned image compression with unevenly grouped space-channel contextual adaptive coding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 3 [25] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. CLIPScore: reference-free evaluation In Proceedings of the 2021 metric for image captioning. Conference on Empirical Methods in Natural Language Processing, 2021. 6, 12 9 [26] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibIn Advances in Neural Information Processing Sysrium. tems, 2017. 6 [27] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021. 6, 8 [28] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems, 2020. 1, [29] E. Hoogeboom, E. Agustsson, F. Mentzer, L. Versari, G. Toderici, and L. Theis. High-fidelity image compression with score-based generative models. arXiv: 2305.18231, 2023. 3 [30] Zhaoyang Jia, Jiahao Li, Bin Li, Houqiang Li, and Yan Lu. Generative latent coding for ultra-low bitrate image comIn Proceedings of the IEEE/CVF Conference on pression. Computer Vision and Pattern Recognition (CVPR), 2024. 3, 5 [31] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park. Scaling up gans for text-to-image synthesis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 3 [32] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and In Proceedings of the Ross Girshick. Segment anything. IEEE/CVF International Conference on Computer Vision (ICCV), 2023. 6 [33] Eastman Kodak. Kodak lossless true color image suite (PhotoCD PCD0992). [34] Nikolai Korber, Eduard Kromer, Andreas Siebert, Sascha Hauke, Daniel Mueller-Gritschneder, and Bjorn Schuller. Perco (SD): Open perceptual compression. In Workshop on Machine Learning and Compression, NeurIPS 2024, 2024. 1, 2, 3, 6, 7, 8, 12, 15 [35] Nikolai Korber, Eduard Kromer, Andreas Siebert, Sascha Hauke, Daniel Mueller-Gritschneder, and Bjorn Schuller. Egic: Enhanced low-bit-rate generative image compression In Computer Vision guided by semantic segmentation. ECCV 2024, 2025. 3 [36] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, Tom Duerig, and Vittorio Ferrari. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. IJCV, 2020. 6 [37] Eric Lei, Yigit Berkay Uslu, Hamed Hassani, and Shirin Saeedi Bidokhti. Text+ sketch: Image compression at ultra low rates. In ICML 2023 Workshop on Neural Compression: From Information Theory to Applications, 2023. 1, 2, 3, 6 [38] Chunyi Li, Guo Lu, Donghui Feng, Haoning Wu, Zicheng Zhang, Xiaohong Liu, Guangtao Zhai, Weisi Lin, and Wenjun Zhang. Misc: Ultra-low bitrate image semantic compression driven by large multimodal model. arXiv: 2402.16749, 2024. 3 [39] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In Proceedings of the 40th International Conference on Machine Learning, 2023. 3, 4, 6, 12 [40] Zhiyuan Li, Yanhui Zhou, Hao Wei, Chenyang Ge, and Jingwen Jiang. Towards extreme image compression with latent feature guidance and diffusion prior. IEEE Transactions on Circuits and Systems for Video Technology, 2024. 2, 3, 6, 7, 8, 12, 15 [41] Xinqi Lin, Jingwen He, Ziyan Chen, Zhaoyang Lyu, Bo Dai, Fanghua Yu, Wanli Ouyang, Yu Qiao, and Chao Dong. Diffbir: Towards blind image restoration with generative diffusion prior. arXiv: 2308.15070, 2024. 3 [42] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023. 2, 3, 4 [43] Xingchao Liu, Chengyue Gong, and qiang liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations, 2023. 3, [44] Fabian Mentzer, George Toderici, Michael Tschannen, and Eirikur Agustsson. High-fidelity generative image compression. Advances in Neural Information Processing Systems, 2020. 1, 3 [45] Fabian Mentzer, Eirikur Agustson, and Michael Tschannen. M2t: Masking transformers twice for faster decoding. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023. 2, 3, 4, 5, 6, 7, 8, 22 [46] David Minnen and Nick Johnston. Advancing the ratedistortion-computation frontier for neural image compression. In 2023 IEEE International Conference on Image Processing (ICIP), 2023. 3 [47] David Minnen and Saurabh Singh. Channel-wise autoregressive entropy models for learned image compression. In 2020 IEEE International Conference on Image Processing (ICIP), 2020. 3 [48] Matthew J. Muckley, Alaaeldin El-Nouby, Karen Ullrich, Improving statistical fiHerve Jegou, and Jakob Verbeek. delity for neural image compression with implicit local likelihood models. In Proceedings of the 40th International Conference on Machine Learning, 2023. 2, 3, 6, 7 [49] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Ilya Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Sutskever, and Mark Chen. GLIDE: Towards photorealistic image generation and editing with text-guided diffusion models. In Proceedings of the 39th International Conference on Machine Learning, 2022. 1, 3 [50] OpenAI, Josh Achiam, et al. Gpt-4 technical report. arXiv: 2303.08774, 2024. 3 [51] Zhihong Pan, Xin Zhou, and Hao Tian. Extreme generative image compression by learning text embedding from diffusion models. arXiv: 2211.07793, 2022. 1, 3 [52] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, 10 [66] Jeremy Vonderfecht and Feng Liu. Lossy compression with pretrained diffusion models. In The Thirteenth International Conference on Learning Representations, 2025. 2, 3, 6, 7, 12, 17, 18 [67] Zhou Wang, Eero Simoncelli, and Alan Bovik. Multiscale structural similarity for image quality assessment. In The Thrity-Seventh Asilomar Conference on Signals, Systems & Computers, 2003, 2003. [68] Yuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. 3 [69] Yichong Xia, Yimin Zhou, Jinpeng Wang, Baoyi An, Haoqian Wang, Yaowei Wang, and Bin Chen. DiffPC: Diffusionbased high perceptual fidelity image compression with semantic refinement. In The Thirteenth International Conference on Learning Representations, 2025. 3 [70] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, and Song Han. SANA: Efficient high-resolution text-to-image synthesis with linear diffusion transformers. In The Thirteenth International Conference on Learning Representations, 2025. 8 [71] Ruihan Yang and Stephan Mandt. Lossy image compression In Advances in Neural with conditional diffusion models. Information Processing Systems, 2023. 3 [72] Yibo Yang, Stephan Mandt, and Lucas Theis. An introduction to neural data compression. Foundations and Trends in Computer Graphics and Vision, 2023. [73] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023. 3 [74] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 6 [75] Yinhao Zhu, Yang Yang, and Taco Cohen. TransformerIn International Conference on based transform coding. Learning Representations, 2022. 3 Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Proceedings of the 38th International Conference on Machine Learning, 2021. 3, 4 [53] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of Machine Learning Research, 2020. [54] Vivek Ramanujan, Kushal Tirumala, Armen Aghajanyan, Luke Zettlemoyer, and Ali Farhadi. When Worse is Better: Navigating the compression-generation tradeoff in visual tokenization. arXiv: 2412.16326, 2024. 3, 7, 8 [55] Lucas Relic, Roberto Azevedo, Markus Gross, and Christopher Schroers. Lossy image compression with foundation diffusion models. arXiv: 2303.08774, 2024. 3 [56] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 3 [57] Edgar Schonfeld, Vadim Sushko, Dan Zhang, Juergen Gall, Bernt Schiele, and Anna Khoreva. You only need adversarial supervision for semantic image synthesis. In International Conference on Learning Representations, 2021. 6 [58] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In Proceedings of the 32nd International Conference on Machine Learning, 2015. 1, 3 [59] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021. [60] Lucas Theis, Wenzhe Shi, Andrew Cunningham, and Ferenc Huszar. Lossy image compression with compressive autoencoders. In International Conference on Learning Representations, 2017. 3 [61] Lucas Theis, Tim Salimans, Matthew Douglas Hoffman, and Fabian Mentzer. Lossy compression with gaussian diffusion. arXiv: 2206.08889, 2023. 3 [62] Keyu Tian, Yi Jiang, Zehuan Yuan, BINGYUE PENG, and Liwei Wang. Visual autoregressive modeling: Scalable imIn The Thirtyage generation via next-scale prediction. eighth Annual Conference on Neural Information Processing Systems, 2024. 2, 3, 4, 5, 6, 7, 8, 12 [63] Michael Tschannen, Eirikur Agustsson, and Mario Lucic. Deep generative models for distribution-preserving lossy compression. In Advances in Neural Information Processing Systems, 2018. 1, 3 [64] Aaron van den Oord, Oriol Vinyals, and koray kavukcuoglu. Neural discrete representation learning. In Advances in Neural Information Processing Systems, 2017. 3, 4 [65] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj, Dhruv Nair, Sayak Paul, William Berman, Yiyi Xu, Steven Liu, and Thomas Wolf. Diffusers: State-of-the-art diffusion models, 2022. 11 v3.0) achieves the highest CLIP-scores [25] across all tested model variants. Additionally, we include PerCo (official) in this comparison but find that its values exceed our calculated upper bound, suggesting methodological differences. All other scores are recomputed using our evaluation framework. A.5. Additional Visual Results Visual Comparisons. We present additional visual comparisons with PerCo (SD) [34] and DiffEIC [40] in Fig. 8. Global Conditioning. In Fig. 9, we examine the effect of global conditioning zg and demonstrate that PerCoV2 exhibits similar internal characteristics to those of PerCo [11]. Comparison to DiffC. In Figs. 10 and 11, we provide additional visual comparisons to DiffC [66] at ultra-low and extreme bit rates. Comparison to Traditional Codecs. In Fig. 12, we compare PerCoV2 (SD v3.0) with traditional, widely-used codecs such as JPEG and VTM-20.0, the state-of-the-art non-learned image codec. Semantic Preservation. Finally, in Fig. 13, we evaluate the semantic preservation capabilities of PerCo (SD) and PerCoV2 across range of tested bit rates. Overall, we observe that our method consistently achieves more faithful reconstructions while preserving perceptual quality. A.6. Beyond Compression As discussed in Sec. 4, the resulting MIM and VAR models can be used for both compression and generation. We summarize visual results for generation in Figs. 15 to 18. A. Appendix A.1. Technical Note on VAR-based Learning In Fig. 3 and Fig. 14, we present simplified conceptual illustrations of VAR-based learning. Technically, upsampled representations of the tokens are fed into the visual autoregressive model, and not the tokens themselves, to match the output shapes of the corresponding predictions. more detailed technical overview is provided in [62, Figure 4]. For our implicit hierarchical VAR formulation, we apply the cross-entropy loss exclusively to the border predictions, as shown in Fig. 14 and Fig. 18. A.2. Computational Complexity In Tab. 2, we compare the computational complexity of PerCo (SD v2.1) [34], PerCo (SD v3.0), and PerCoV2 (SD v3.0). For PerCoV2 (SD v3.0), we report results using MIM (α = 2.2, = 12), our slowest configuration. We present the average encoding and decoding times on the Kodak dataset at 0.03 bpp, excluding the first three samples to mitigate device warm-up effects. All models are evaluated with full precision (float32); as such, we expect further speed-ups when using reduced precision. The performance optimization has not been the main focus of this work."
        },
        {
            "title": "Method",
            "content": "Encoding (s) Decoding (s) PerCo (SD v2.1) [34] PerCo (SD v3.0) PerCoV2 (SD v3.0) 0.04 0.04 0.31 1.05 3.08 3.35 Table 2. Computational Complexity. A.3. BLIP 2 vs. Molmo Captions By default, we use BLIP 2 [39] captions, limited to 32 tokens, in line with the original formulation [11]. Additionally, we explore the impact of more detailed image descriptions based on Molmo [15]. Specifically, we use the allenai/Molmo-7B-D-09243 variant, with token limit of 77, and employ the prompt Provide detailed image caption in one sentence. to evaluate its effect at the ultra-low bit-rate setting. Our findings indicate that longer captions improve perceptual scores, but at the cost of pixelwise fidelity. While we believe this approach holds promise, it requires further exploration and careful design to balance the trade-offs. A.4. Additional Quantitative Results We present additional quantitative results on the MSCOCO30k and Kodak datasets in Figs. 6 and 7, observing trends consistent with the main results. Notably, PerCoV2 (SD 3https://huggingface.co/allenai/Molmo-7B-D-0924 12 Figure 6. Quantitative comparison of PerCoV2 on MSCOCO-30k (CLIP-score). 13 Figure 7. Quantitative comparison of PerCoV2 on the Kodak dataset. 14 Original PerCo (SD v2.1) [11, 34] (ICLR 2024, NeurIPS 2024 Workshop) DiffEIC [40] (TCSVT 2024) PerCoV2 (SD v3.0) Ours kodim08 0.03302 bpp (1.2) 0.02820 bpp (1.02) 0.02756 bpp Figure 8. Visual comparison of PerCoV2 on the Kodak dataset at an extreme bit-rate configuration. Bit-rate increases relative to our method are indicated by (). Best viewed electronically. 15 Original no text Spatial bpp: 0.00171, Text bpp: 0.0 white fence with lighthouse behind it (BLIP 2) Spatial bpp: 0.00171, Text bpp: 0.0014 an old castle Spatial bpp: 0.00171, Text bpp: 0.0006 Figure 9. Visual illustration of the impact of the global conditioning zg on the Kodak dataset (kodim19) at our lowest bit-rate configuration. Samples are generated from the same initial Gaussian noise. Inspiration taken from [11, fig. 13]. 16 Original DiffC (SD v1.5) [66] (ICLR 2025) PerCoV2 (SD v3.0) (Ours) kodim14 0.00455 bpp (1.37) 0.00330 bpp kodim22 0.00427 bpp (1.32) 0.00323 bpp Figure 10. Additional comparison with DiffC at ultra-low bit-rate setting. Original DiffC (SD v1.5) [66] (ICLR 2025) PerCoV2 (SD v3.0) (Ours) kodim14 0.03024 bpp (1.06) 0.02853 bpp kodim22 0.02789 bpp (1.05) 0.02664 bpp Figure 11. Additional comparison with DiffC at extreme-low bit-rate setting. 18 Original (000000000827) JPEG 0.30771 bpp (10.55) VTM-20.0 0.04166 bpp (1.43) PerCoV2 (SD v3.0) 0.02917 bpp Figure 12. Visual comparison with traditional codecs (JPEG and VTM-20.0). 19 Original GT label PerCo (SD) PerCoV2 0.00363 bpp Predicted label 0.00342 bpp Predicted label 0.03293 bpp Predicted label 0.02777 bpp Predicted label 0.12668 bpp"
        },
        {
            "title": "Predicted label",
            "content": "0.10809 bpp"
        },
        {
            "title": "Predicted label",
            "content": "Figure 13. Visual comparison of the semantic preservation of PerCo (SD) and PerCoV2 across various bit-rates on the MSCOCO-30k dataset (000000442539). Global conditioning: herd of sheep standing in field next to fence. 20 Figure 14. Implicit Hierarchical VAR (Ours). 21 32/64 tokens 0.00302 bpp 64/64 tokens 0.00381 bpp Figure 15. Checkerboard masking schedule [23]. 4/64 tokens 0.00217 bpp 8/64 tokens 0.00229 bpp 16/64 tokens 0.00253 bpp 32/64 tokens 0.00305 bpp 64/64 tokens 0.00385 bpp Figure 16. Quincunx masking schedule [17]. 2/64 tokens 0.00211 bpp 9/64 tokens 0.00232 bpp 21/64 tokens 0.00272 bpp 40/64 tokens 0.00323 bpp 64/64 tokens 0.00381 bpp Figure 17. QLDS masking schedule [45]. 4/64 tokens 0.00217 bpp 16/64 tokens 0.00256 bpp 36/64 tokens 0.00314 bpp 64/64 tokens 0.00385 bpp Figure 18. Implicit VAR-based masking schedule (Ours)."
        }
    ],
    "affiliations": [
        "TU Wien",
        "Technical University of Munich",
        "University of Applied Sciences Landshut"
    ]
}