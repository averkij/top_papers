{
    "paper_title": "$ΔL$ Normalization: Rethink Loss Aggregation in RLVR",
    "authors": [
        "Zhiyuan He",
        "Xufang Luo",
        "Yike Zhang",
        "Yuqing Yang",
        "Lili Qiu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose $\\Delta L$ Normalization, a simple yet effective loss aggregation method tailored to the characteristic of dynamic generation lengths in Reinforcement Learning with Verifiable Rewards (RLVR). Recently, RLVR has demonstrated strong potential in improving the reasoning capabilities of large language models (LLMs), but a major challenge lies in the large variability of response lengths during training, which leads to high gradient variance and unstable optimization. Although previous methods such as GRPO, DAPO, and Dr. GRPO introduce different loss normalization terms to address this issue, they either produce biased estimates or still suffer from high gradient variance. By analyzing the effect of varying lengths on policy loss both theoretically and empirically, we reformulate the problem as finding a minimum-variance unbiased estimator. Our proposed $\\Delta L$ Normalization not only provides an unbiased estimate of the true policy loss but also minimizes gradient variance in theory. Extensive experiments show that it consistently achieves superior results across different model sizes, maximum lengths, and tasks. Our code will be made public at https://github.com/zerolllin/Delta-L-Normalization."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 8 5 5 7 0 . 9 0 5 2 : r Preprint Normalization: RETHINK LOSS AGGREGATION IN RLVR Zhiyuan He1, Xufang Luo1, Yike Zhang2, Yuqing Yang1, Lili Qiu1 1Microsoft Research, 2Tsinghua University zhiyuhe@microsoft.com, xufluo@microsoft.com"
        },
        {
            "title": "ABSTRACT",
            "content": "We propose Normalization, simple yet effective loss aggregation method tailored to the characteristic of dynamic generation lengths in Reinforcement Learning with Verifiable Rewards (RLVR). Recently, RLVR has demonstrated strong potential in improving the reasoning capabilities of large language models (LLMs), but major challenge lies in the large variability of response lengths during training, which leads to high gradient variance and unstable optimization. Although previous methods such as GRPO, DAPO, and Dr. GRPO introduce different loss normalization terms to address this issue, they either produce biased estimates or still suffer from high gradient variance. By analyzing the effect of varying lengths on policy loss both theoretically and empirically, we reformulate the problem as finding minimum-variance unbiased estimator. Our proposed Normalization not only provides an unbiased estimate of the true policy loss but also minimizes gradient variance in theory. Extensive experiments show that it consistently achieves superior results across different model sizes, maximum lengths, and tasks. Our code will be made public at https://github.com/zerolllin/Delta-L-Normalization. Figure 1: Left: In RLVR, trajectory lengths vary significantly, and long trajectories induce high gradient variance, causing unstable training. Right: Existing gradient aggregation methods across different lengths either lead to biased updates or suffer from high variance. In this paper, we propose new aggregation method, Normalization, that is both unbiased and variance-minimized. Figure 2: Training dynamics of Normalization compared with baselines across tasks (CountDown, Math), maximum lengths (3072, 8192), and model sizes (3B, 7B). Performance is measured by Avg@8 on CountDown and by weighted Avg@8 across four math datasets. Normalization consistently yields more stable training and consistently converges to higher accuracy. 1 Preprint"
        },
        {
            "title": "INTRODUCTION",
            "content": "Recently, significant progress has been made in improving the reasoning abilities of Large Language Models (LLMs) (Guo et al., 2025; Jaech et al., 2024; Team et al., 2025)) The key of these advances is Reinforcement Learning with Verifiable Rewards (RLVR), which applies rule-based rewards and uses reinforcement learning to update model weights accordingly (Shao et al., 2024; Liu et al., 2025; Yu et al., 2025). Unlike traditional reinforcement learning, RLVR introduces unique challenge: the response trajectories vary drastically in length, ranging from few dozen to several thousand tokens, and often growing longer as training progresses (Team et al., 2025; Yu et al., 2025). Such variability complicates the optimization process. Prior studies have shown that rapid fluctuations in response length can lead to model accuracy collapse (Dai et al., 2025), truncated overlong responses may trigger entropy explosion and degrade performance (Yu et al., 2025), and excessively long responses introduce high variance and destabilize training (Zheng et al., 2025). To address these issues, the common practice is applying length-dependent normalization in loss aggregation. Specifically, GRPO applies sample-level normalization, dividing each sample-level loss by its response length (Shao et al., 2024); while DAPO uses batch-level approach, normalizing the total loss by the sum of response lengths in the batch (Yu et al., 2025). Intuitively, these lengthdependent aggregation methods normalize longer responses or batches, which stabilizes training by preventing long responses from dominating. In addition, because such length-dependent factors deviate from standard reinforcement learning theory, Dr. GRPO, in contrast, avoids any lengthdependent factor and normalizes the gradient with fixed constant (Liu et al., 2025). While these aggregation strategies have shown empirical benefits, their theoretical properties remain largely underexplored. There is lack of analysis on how they influence the statistical properties of gradient estimation in RLVR, with gradient variance being particularly important because high variance leads to inefficient training and even model collapse (Williams, 1992; Sutton et al., 1998; Schulman et al., 2015). We illustrate this comparison in Figure 4. Estimators with higher gradient variance are more prone to deviating from the optimal optimization direction, which hinders effective training. In this work, we present the first systematic analysis of how different aggregation strategies affect gradient estimation in RLVR training. We begin by reformulating the aggregated gradient as linear combination of sample-level unnormalized gradients. Then we observe directly that the variance of each unnormalized gradient grows linearly with its response length. With this perspective in place, we carefully examine the biasvariance properties of existing methods and uncover two major issues: (1) The aggregation strategies in GRPO and DAPO introduce length-dependent bias in estimating the true policy gradient. As response lengths increase, their parameter updates shrink in gradient norm, slowing convergence. (2) The aggregation strategies in DAPO and Dr. GRPO lead to higher coefficient of variation (CV). Since CV measures normalized variance, higher value means greater relative noise for the same gradient norm, resulting in less stable training. Motivated by these observations, we propose Normalization, an unbiased loss aggregation technique with minimized variance. These two properties make it stable update algorithm and enable convergence to better model accuracy. In addition, Normalization is simple to implement and requires fewer than ten lines of code changes. We conduct extensive experiments on two models, Qwen2.5-3B and Qwen2.5-7B (Yang et al., 2024), across two tasks: CountDown and Math, and across different maximum response lengths. Results show that the proposed Normalization not only stabilizes the training but also achieves higher accuracy. We further compare our approach with alternative strategies for handling long responses, including Overlong filtering and Soft punishment from DAPO (Yu et al., 2025), and find that Normalization achieves significant performance gains over these methods."
        },
        {
            "title": "2 ANALYSIS OF LOSS AGGREGATION METHODS IN RLVR",
            "content": "2.1 REFORMULATE EXISTING METHODS FROM UNIFIED PERSPECTIVE Given parameterized policy πθ and trajectories τ = {(st, at, rt)}T environment, vanilla policy gradient updates πθ by the following gradient estimator: t=0 sampled from πθ and the 2 Preprint θJ(θ) = Eτ πθ (cid:34) (cid:88) t=0 Atθ log πθ(at st) (1) (cid:35) where J(θ) is the expected return and At is advantage function (Williams, 1992). Recently, reinforcement Learning with Verifiable Reward (RLVR) has achieved notable success in boosting reasoning capabilities of large language models (LLMs). representative method is Group Relative Policy Optimization (GRPO), which eliminates the need for critic model by instead estimating the baseline within each group (Shao et al., 2024). For given question q, GRPO first sample group of outputs o1, o2, ..., oG from the old policy πθold, and update current πθ by: θ 1 (cid:88) i=1 1 Li Li(cid:88) t=1 min (ri,t(θ) Ai, clip (ri,t(θ), 1 ϵ, 1 + ϵ) Ai) , where Li = oi denotes the length of the i-th response, and ri,t(θ) = πθ(oi,tq,oi,<t) πθold (oi,tq,oi,<t) is the token-level importance sampling ratio. The advantage Ai is computed via group-level normalization Ai = rimean({r1,r2,...,rG}) . Notably, GRPO first normalizes the per-sample gradient by response length Li, and then averages across the group, which has been observed to have an impact on the algorithm performance in prior works (Yu et al., 2025). To analyze the underlying effect, we define the unnormalized sample-level gradient gi as: std({r1,r2,...,rG}) gi = θ Li(cid:88) t=1 min (ri,t(θ) Ai, clip (ri,t(θ), 1 ϵ, 1 + ϵ) Ai) . If we omit clipping and importance sampling, the unnormalized gradient gi approximates the standard policy gradient estimator with advantage estimation: gi Li(cid:88) t=1 Aiθ log πθ(oi,t q, oi,<t), This aligns with Equation 1, thus the expectation of gi also aligns with the true policy gradient, which translates to: Eoiπθ [gi] θJ(θ). (cid:80)G Although E[gi] θJ(θ) holds, GRPO does not update the model using the maximum likelihood estimator 1 i=1 gi. Instead, it applies an additional length-based normalization, estimating the (cid:80)G 1 gradient as 1 gi. Beyond GRPO, alternative loss aggregation strategies have been proG Li posed by DAPO and Dr. GRPO, given by: i=1 gDAPO = 1 (cid:80)G i=1 Li (cid:88) i=1 gi ; gDr.GRPO = 1 GM (cid:88) i=1 gi DAPO normalizes the gradient by the total response length within batch (Yu et al., 2025), whereas Dr. GRPO applies normalization using constant followed by sample-level normalization (Liu et al., 2025). Since GRPO and DAPO both rely directly on response lengths, we refer to their aggregation methods as length-dependent. Dr. GRPO, however, is length-independent, as its normalization does not vary with response length during training. We acknowledge that the computation of each gi may differ across methods: for instance, DAPO introduces additional techniques such as dynamic sampling, varied clipping ratios, and overlong filtering or punishment; Dr. GRPO, in contrast, estimates the advantage using Ai = ri 3 Preprint mean(r1, . . . , rG) rather than the original form Ai = rimean(r1,...,rG) . These auxiliary modifications, however, are orthogonal to the loss aggregation methods. For simplicity, our analysis focuses on aggregation methods, and we will provide further empirical studies on these auxiliary modifications in the evaluation section. std(r1,...,rG) 2.2 GRADIENT VARIANCE GROWS PROPORTIONALLY WITH RESPONSE LENGTH Both GRPO and DAPO adopt length-dependent aggregation methods. We hypothesize that their designs are motivated by the statistical property that the variance of the unnormalized gradient gi grows approximately proportionally to the response length. Consider Var(gi) Var((cid:80)Li t=1 Aiθ log πθ(oi,t q, oi,<t)) 1. We approximate this as Var(gi) (cid:80)Li t=1 Var(Aiθ log πθ(oi,t q, oi,<t)), by ignoring covariance between individual token-level gradients. Assuming each token-level term contributes approximately constant variance , we thus have Var(gi) Li. This approximation indicates that samples with longer responses inherently exhibit higher gradient variance due to increased randomness, and the gradient variance grows proportionally with response length. Figure 3: Deviation gi E[gi]2 for random selected sample on the Q, K, projection in the last layer. E[gi] is estimated by the average of 128 rollouts. Figure 4: Comparison between low and high gradient variance. We also verify this property empirically using the Qwen2.5-7B model. We randomly select one math question from the Open Reasoner Zero dataset (Hu et al., 2025) and generate 128 responses with temperature set to 1. We calculate each responses unnormalized gradient gi, along with the mean gradient E[gi] = 1 i=1 gi, which serves as the overall expected gradient. We then compute the 128 squared deviation for each gradient sample gi E[gi]2, the expectation of which corresponds to the gradient variance. Figure 3 illustrates this deviation for the Q, K, projection matrices in the last layer, clearly confirming that gradient variance increases proportionally with response length. (cid:80)128 During RLVR training, the response length typically grows significantly over time. Consequently, using the standard estimator 1 i=1 gi without normalization would lead to increasingly high graG dient variance. To mitigate this, GRPO introduces sample-level normalization, while DAPO employs batch-level length normalization, both aiming to stabilize training. (cid:80)G 2.3 BIAS AND VARIANCE PROPERTIES OF EXISTING METHODS In this subsection, we provide detailed analysis on how different loss aggregation methods influence the bias and variance properties of the combined gradient g. Bias Properties. We analyze the bias of each method by calculating the expectation of these estimators and comparing them with the standard policy gradient θJ(θ). Because E[gi] θJ(θ), without any further assumption, we have the E[g] shown in Table 1. GRPO and DAPO introduce length-dependent coefficients determined by the response lengths Li, which results in bias issue. As Li grows during RLVR training, these methods yield larger updates in the early stage and progressively smaller gradients later. For instance, the expected gradient norm 1The variance here is strictly defined as Var(gi) = E(cid:2)gi E[gi]2(cid:3) , which measures the expected deviation in norm and corresponds to the total variance in statistics. For simplicity, we refer to it as variance throughout this paper without further specification. 4 Preprint per step of GRPO is E[gGRPO] = 1 decreases as reLi sponse lengths increase. Consequently, the gradient norm decreases over time, slowing convergence in the later training phase. In contrast, Dr. GRPO does not suffer from this issue, since E[gDr. GRPO] always remains constant scaling of θJ(θ), independent of response length. θJ(θ), where 1 1 Li i= i=1 (cid:80)G (cid:16) 1 (cid:80)G (cid:17) Method GRPO DAPO (cid:16) 1 (cid:16) E(g) (cid:17) 1 Li (cid:80)G i=1 θJ(θ) Var(g) G2 (cid:80)G i=1 1 Li (cid:18)(cid:113)(cid:80)G i=1 1 Li CV(g) (cid:19) θJ(θ) (cid:17) (cid:80)G i=1 Li θJ(θ) (cid:80)G i=1 Li (cid:80)G i=1 Li G2M 2 (cid:80)G i=1 Li (cid:80)G i=1 Li θJ(θ) θJ(θ) Dr. GRPO 1 θJ(θ) Ours (α = 1) 1 θJ(θ) 2 (cid:80)G i=1 1 Li (cid:18)(cid:113)(cid:80)G i=1 1 Li (cid:19) θJ(θ) Table 1: Comparison of E(g), Var(g), and CV(g). : biased, : unbiased, : high CV, : low CV. Variance Properties. Beyond the bias issue, more fundamental challenge is gradient variance. On-policy methods are well known for high gradient variance due to the fact that gradients are estimated from limited number of sampled trajectories at each update step, leading to significant estimation noise (Williams, 1992; Sutton et al., 1998; Schulman et al., 2015). In the context of RLVR, variability in response length amplify this problem, as longer responses tend to produce higher variance. To analyze how different normalization strategies affect gradient variance, we introduce two assumptions here. First, according to the analysis in Section 2.2, we assume that the variance of each sample-level gradient gi is proportional to its length Li. Formally we assume Var(gi) Li, where is the variance in token-level. std(r1,...,rG) Second, we assume that the gradients gi are independent across samples, as each is computed from an independently sampled trajectory oi. We acknowledge that the use of group-based baselines, such as Ai = ri mean(r1, . . . , rG) or Ai = rimean(r1,...,rG) , introduces dependence between these gradients. However, this effect diminishes when the group-based baseline closely approximates the true return expectation E[ri], which is only dependent on the current policy πθ and question q. Under the assumption that Var(gi) Li and all gi are independent, the variance results are summarized in Table 1. Since gGRPO, gDAPO, and gDr. GRPO have different expectations, variance alone is not comparable, as larger E(g) naturally leads to larger variance. To normalize this Var(g) effect, we use the Coefficient of Variation (CV), defined as CV(g) = E(g) , which quantifies the deviation per unit of gradient norm. We can prove CV(gGRPO) CV(gDAPO) = CV(gDr. GRPO) (details in Appendix A.1), implying that DAPO and Dr. GRPO induce higher relative variability than GRPO, and thus each unit of gradient norm update carries larger statistical deviation, leading to less stable optimization. This difference arises because GRPO re-weights each sample-level gradient by its response length to suppress variance, whereas DAPO and Dr. GRPO do not, resulting in higher CV."
        },
        {
            "title": "3 RETHINK LOSS NORMALIZATION IN RLVR",
            "content": "Existing aggregation methods introduce either bias or excessive variance. We therefore ask: Can loss aggregation method be both unbiased and minimum-variance? The answer is yes. We observe that this problem can be naturally reformulated within the framework of minimum variance unbiased estimation in statistics. Specifically, we treat the gradients obtained from responses of different lengths as independent observations of the same underlying variable (the ground-truth policy gradient), each with its own variance. Our objective is then to construct new unbiased estimator by 5 Preprint optimally combining these observations so that the resulting variance is minimized. Formally, we define the problem as follows. Problem Definition. Given set of independent sample-level gradient estimators {gi}G i=1 satisfying E[gi] = θJ(θ) and Var(gi) = Li, where Li > 0 denotes the length associated with sample and is constant scalar, the objective is to determine coefficients {xi}G i=1 in the linear combination ˆg = (cid:80)G i=1 xigi, such that E[ˆg] = θJ(θ)/M for given > 0, while minimizing the variance Var[ˆg]. Noting that the unbiasedness constraint E[ˆg] = θJ(θ)/M is equivalent to (cid:80)G and, by independence, the variance satisfies Var[ˆg] = (cid:80)G , the problem reduces to convex quadratic program with single linear constraint. Solving with the Lagrange multiplier method yields the unique minimizer: Var(gi) = (cid:80)G i=1 xi = 1 = 1, . . . , G, i=1 Lix2 i=1 x2 , = 1 L1 j=1 L1 (cid:80)G In practice, we find it is beneficial to introduce hyperparameter 0 α 1 to the normalization factor, which gives the following normalization weights: xi = 1 Lα j=1 Lα (cid:80)G , = 1, . . . , G, The parameter α provides tradeoff between variance reduction and utilization of long responses. While longer responses tend to introduce higher variance, sometimes they also carry richer learning signals. Choosing α < 1 allows these signals to contribute more effectively, at the cost of increased gradient variance. We name this method Normalization, as it is specially designed to match the dynamic length nature in RLVR. It has four key properties: Unbiasedness: For any α, Normalization is unbiased since (cid:80)G i=1 xi = 1 , ensuring E[ˆg] = θJ(θ)/M . This preserves theoretical consistency with vanilla reinforcement learning. Minimum Variance: Choosing α = 1 achieves the minimum possible variance under the unbiasedness constraint. Under the assumptions in Section 2.2, this is the unique solution when loss aggregation is linear, unbiased combination. Controlled Coefficient of Variation (CV): We can show (detailed proof in Appendix A.1): CV(gGRPO) = CV(ˆgα=1) CV(ˆg0<α<1) CV(gDAPO) = CV(gDr. GRPO). Thus, Normalization guarantees lower CV than DAPO and Dr. GRPO, while matching GRPO at α = 1. When α < 1, variance increases slightly, but long responses contribute more effectively. Transition to Dr. GRPO: Setting α = 0 recovers the aggregation method introduced in Dr. GRPO, making it special case of Normalization. These properties make Normalization highly valuable for RLVR training. The unbiasedness property ensures consistency with standard reinforcement learning theory, preventing unexpected slowdowns caused by biased gradient estimates. Variance reduction further stabilizes training and accelerates convergence. In practice, we find that, setting α = 1, which minimizes the variance, is universal good choice. α = 0.75 further increase the performance in Math, which might be due to the fact that the long response in Math task should be better utilized."
        },
        {
            "title": "4 EVALUATION",
            "content": "4.1 SETTINGS Basics. We evaluate the proposed Normalization on two models, Qwen2.5-3B and Qwen2.57B (Yang et al., 2024), across two tasks: CountDown and Math. For CountDown, we train on the TinyZero dataset (Pan et al., 2025) and test on held-out set of 1,000 samples. For Math, we train on Open Reasoner Zero dataset (Hu et al., 2025) and evaluate on MATH500 (Hendrycks et al., 2021), Minerva (Lewkowycz et al., 2022), AMC (Li et al., 2024), and AIME2024 (Li et al., 2024). The maximum response length is set to 3072, with an additional evaluation of the 3B model at length 8192. Detailed training settings are provided in Section A.2. 6 Preprint Method GRPO Norm DAPO Norm Dr. GRPO Norm Dr. GRPO Ours 3B Model 7B Model 3B Model; 8192 Avg@8 Pass@8 Avg@8 Pass@8 Avg@8 Pass@8 0.811 0.800 0.781 0.755 0.847 0.954 0.913 0.916 0.923 0.967 0.934 0.924 0.897 0.924 0.937 0.928 0.922 0.889 0.879 0.938 0.874 0.841 0.819 0.816 0. 0.827 0.804 0.790 0.809 0.821 Table 2: Detailed results on the CountDown task evaluated using Avg@8 and Pass@8. Baselines. Our primary goal is to compare different loss aggregation methods under identical settings. Baselines include the aggregation methods from GRPO, DAPO, and Dr. GRPO, which we denote as GRPO Norm, DAPO Norm, and Dr. GRPO Norm, respectively. For Normalization, we mainly set α = 1 for CountDown and α = 0.75 for Math. The impact of different α values is further analyzed in Section 4.4. To ensure fairness, we adopt the same advantage estimator, Ai = rimean({r1,r2,...,rG}) , for GRPO Norm, DAPO Norm, Dr. GRPO Norm, and Normalization. This differs from the original Dr. GRPO, which uses Ai = ri mean({r1, r2, . . . , rG}) (Liu et al., 2025). Therefore, we also include the original Dr. GRPO as an additional baseline. Furthermore, we acknowledge that DAPO incorporates additional techniques beyond loss aggregation to mitigate the impact of lengthy responses, such as overlong response filtering and soft punishment. It also introduces dynamic sampling to improve training performance. To compare our method and these techniques, we provide further experiments in Section 4.3. std({r1,r2,...,rG}) 4.2 MAIN RESULTS Training Dynamics. Figure 2 presents the training dynamics across all 6 settings, while Figure 5 highlights 2 representative settings for better readability. For the CountDown task, the score is measured by Avg@8 with temperature = 1 on the held-out test subset. For the Math task, we report weighted average (weighted by the question number) of Avg@8 scores across four test datasets. Across nearly all settings, the proposed Normalization outperforms all baselines, achieving both higher stability during training and superior test scores. The only exception is the CountDown 7B setting, where GRPO Norm slightly surpasses our method, though Normalization still shows clear advantages over the other three baselines. We find the proposed method exhibits highly monotonicity of performance improvement. To quantify this, we introduce monotonicity score, defined as the Pearson correlation between {0, 1, 2, . . . } and the test scores {score0, score50, score100, . . . } recorded every 50 steps. As shown in Table 5, our method achieves the highest average correlation, with values consistently above 0.94 across all settings. This demonstrates that Normalization promotes stable and steadily improving training dynamics. Figure 5: Selected training dynamics. Please refer to Figure 2 for training dynamics on all settings. Figure 6: Entropy on CountDown and 3B model. Legend is the same as Figure 5. Figure 7: Response length on Math and 7B model. Legend is the same as Figure 5. CountDown. We report the best Avg@8 and Pass@8 metrics throughout training of each method in Table 2 for the CountDown task. Overall, the proposed method achieves the strongest performance, with GRPO Norm being the second best. We find that LLMs often generate responses exceeding the maximum response length in the CountDown task, which tends to dominate the gradient and increase its variance. The low-CV property shared by GRPO Norm and our method helps Preprint alleviate this problem and stabilize training. However, GRPO introduces bias issue in gradient estimation, which may hinder convergence in the later stage of training. As shown in Figure 5, both our method and GRPO Norm converge quickly within the first 300 steps, but GRPO Norm becomes less effective as training progresses, while our method continues to converge at fast rate. We observe explicit entropy difference caused by different aggregation method. During training, Dr. GRPO Norm, Dr. GRPO, and DAPO Norm typically exhibit entropy spikes, which are often associated with sudden drops in performance, as shown in Figure 5 and Figure 6, potentially due to their high-CV property. In contrast, the proposed method consistently maintains an entropy between 0.1 and 0.2, which is beneficial for stable training. Math. For Math task, we present the performance of the models achieving the best weighted average Avg@8 score across the four test datasets in Table 4. Our method consistently outperforms all baselines, achieving the highest weighted average score as well as the highest average score. We observe that the proposed method can lead to sudden increases in response length during training, which are closely associated with performance improvements. As shown in Figure 5 and Figure 7, for the 7B model, the response length exhibits sharp increases around 300 and 400 steps, coinciding with noticeable boosts in test accuracy. However, unlike in the CountDown task, we do not observe significant differences in entropy. 4.3 COMBINATION WITH FULL DAPO Figure 8: Comparison between our methods and full DAPO on CountDown task and 3B model. We acknowledge DAPO introduces two additional techniques to mitigate the high variance issues of long responses: Overlong filtering and Soft punishment (Yu et al., 2025). To understand their effects, we experiment with the full DAPO method, including Dynamic sampling, Clip higher, Pertoken loss, and either Overlong filtering or Soft punishment. We conduct the following experiments: DAPO w/ OF: Full DAPO with Overlong filtering. DAPO w/ SP: Full DAPO with Soft punishment. The punishment interval is set to 2048. DAPO w/o OF, w/o SP: Dynamic sampling, clip higher, and per-token loss only. DAPO w/ Ours: Dynamic sampling and clip higher, with Normalization (α = 1). Figure 8 shows the training dynamics. DAPO w/ Ours clearly outperforms all baselines, achieving the best Avg@8 of 0.913. In Table 2, Normalization achieves the 0.847. The improvement from 0.847 to 0.913 comes from Dynamic sampling, which is the only difference. Neither overlong filtering nor soft punishment is effective in this setting. Overlong filtering produces the longest responses, likely because overlong responses receive no penalty once masked. Soft punishment yields the shortest responses, which limits both exploration and performance. These observations align with recent works (Du et al., 2025; Shang et al., 2025). In comparison, Normalization provides simpler and more unified approach to handle long responses. It also stabilizes entropy, consistent with the findings in subsection 4.2. 4.4 CHOICE OF HYPERPARAMETERS In this section, we investigate the hyperparameter 0 α 1 in Normalization. Since each gradient is scaled by Lα , larger α weakens the contribution of long responses in the aggregated Preprint Method GRPO Norm DAPO Norm Dr. GRPO Norm Dr. GRPO Ours; α=0.5 Ours; α=0.75 Ours; α=1.0 CD; 3B CD; 7B Math; 3B Math; 7B AVG 0.680 0.509 0.811 0.672 0.505 0.800 0.662 0.502 0.781 0.662 0.506 0.755 0.680 0.505 0.805 0.517 0.686 0.838 0.691 0.847 0.506 0.827 0.804 0.790 0.809 0.825 0.797 0.821 0.573 0.578 0.576 0.579 0.586 0.592 0.590 Table 3: Performance comparison across different hyperparameters (α = 0.5, 0.75, 1.0). Cells highlighted in light green denote results surpassing at least three out of the four baselines, while those in dark green denote results surpassing all baselines. gradient but results in smaller gradient variance. α = 1 achieves the minimum variance, while 0 < α < 1 increases variance but leverages long responses more effectively. Setting α = 0 is equivalent to the aggregation method proposed in Dr. GRPO. We evaluate α = 0.5, 0.75, 1.0 on four settings (Math, CountDown; 3B and 7B), with results shown in Table 3. For clarity, we mark parameter choices in light green if they outperform at least three baselines and in dark green if they surpass all baselines. Importantly, all three tested values (α = 0.5, 0.75, 1.0) outperform the baselines on four tasks average. On Math, α = 0.75 performs better than α = 1, likely because longer responses are more informative and should be better utilized. Overall, α = 1 usually gives good results."
        },
        {
            "title": "5 RELATED WORKS",
            "content": "Loss Aggregation Methods in RLVR RLVR methods differ in how they aggregate the policy loss. GRPO applies length-dependent normalizer to each sample (Shao et al., 2024); Dr. GRPO removes this term and normalize the gradient with constant (Liu et al., 2025); DAPO uses batchlevel length-dependent normalizer (Yu et al., 2025). We show that the length-dependent normalizers in GRPO and DAPO have bias in gradient estimating, while the constant normalizer in Dr. GRPO incurs high variance. To address both issues, we propose Normalization, an aggregation method that is unbiased and minimizes variance. Classical Gradient Variance Reduction Methods. Gradient variance is central challenge in classical policy-gradient methods. REINFORCE reduces variance by subtracting baseline from returns (Williams, 1992). Actorcritic methods extend this idea by learning value-function baseline (Sutton et al., 1998). GAE further stabilizes learning by interpolating between Monte Carlo and low-variance temporal-difference returns (Schulman et al., 2015). While these techniques primarily address variance from rewards, RLVR introduces an additional variance source: long and variable trajectory lengths in reasoning tasks. Our method provides loss-aggregation scheme that aligns with classical RL and explicitly minimizes gradient variance, yielding strong empirical results. Length-dependent Rewards in RLVR. Several works make the reward depend on response length to mitigate the overthinking problem. Kimi K1.5 adds additional rewards for short correct answers and penalties for long incorrect ones (Team et al., 2025). GRPO-LEAD avoids penalizing incorrect answers and applies an exponential, length-based reward only to correct ones (Zhang & Zuo, 2025). ShortRL introduces neutral-length zone that neither rewards nor penalizes moderately long responses to preserve diversity (Yuan et al., 2025). Our method leaves the reward unchanged and is therefore orthogonal to these designs."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper, we introduce Normalization, loss-aggregation method tailored to the high length variability in RLVR. It yields an unbiased policy-gradient estimator consistent with vanilla RL theory and minimizes gradient variance, enabling more stable training and convergence to stronger models. Empirically, Normalization consistently outperforms alternatives across model sizes, maximum response lengths, and tasks. 9 Preprint"
        },
        {
            "title": "REFERENCES",
            "content": "Muzhi Dai, Shixuan Liu, and Qingyi Si. Stable reinforcement learning for efficient reasoning. arXiv preprint arXiv:2505.18086, 2025. Dong Du, Shulin Liu, Tao Yang, Shaohua Chen, and Yang Li. Ulorl: An ultra-long output reinforcement learning approach for advancing large language models reasoning abilities. arXiv preprint arXiv:2507.19766, 2025. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model. arXiv preprint arXiv:2503.24290, 2025. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in neural information processing systems, 35:38433857, 2022. Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13(9):9, 2024. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, arXiv preprint and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv:2503.20783, 2025. Jiayi Pan, Junjie Zhang, Xingyao Wang, Lifan Yuan, Hao Peng, and Alane Suhr. Tinyzero. https://github.com/Jiayi-Pan/TinyZero, 2025. Accessed: 2025-01-24. John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. HigharXiv preprint dimensional continuous control using generalized advantage estimation. arXiv:1506.02438, 2015. Ning Shang, Yifei Liu, Yi Zhu, Li Lyna Zhang, Weijiang Xu, Xinyu Guan, Buze Zhang, Bingcheng Dong, Xudong Zhou, Bowen Zhang, et al. rstar2-agent: Agentic reasoning technical report. arXiv preprint arXiv:2508.20722, 2025. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Richard Sutton, Andrew Barto, et al. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. Ronald Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3):229256, 1992. 10 Preprint An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, arXiv preprint Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv:2412.15115, 2024. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Danlong Yuan, Tian Xie, Shaohan Huang, Zhuocheng Gong, Huishuai Zhang, Chong Luo, Furu Wei, and Dongyan Zhao. Efficient rl training for reasoning models via length-aware optimization. arXiv preprint arXiv:2505.12284, 2025. Jixiao Zhang and Chunsheng Zuo. Grpo-lead: difficulty-aware reinforcement learning approach for concise mathematical reasoning in language models. arXiv preprint arXiv:2504.09696, 2025. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 PROOF OF MAGNITUDE RELATION OF CV VALUES We have the following CV values for each method: θJ(θ) θJ(θ) θJ(θ) , , , CV(gGRPO) = 1 (cid:113)(cid:80)G i=1 1 Li (cid:113)(cid:80)G i=1 Li (cid:113)(cid:80)G i=1 Li (cid:113)(cid:80)G CV(gDAPO) = CV(gDr.GRPO) = CV(gOurs) = i=1 12α j=1 Lα (cid:80)G θJ(θ) In this subsection, we provide proof of their magnitude relation. Proof of CV(gGRPO) CV(gDAPO). It is equal to prove: 1 (cid:113)(cid:80)G i= 1 Li (cid:113)(cid:80)G i=1 Li , with Li > 0. This is equivalent to (cid:16) (cid:88) (cid:118) (cid:117) (cid:117) (cid:116) By the CauchySchwarz inequality applied to (cid:0) have: (cid:17)(cid:16) (cid:88) 1 Li Li i=1 i=1 (cid:17) G2 (cid:16) (cid:88) Li (cid:17)(cid:16) (cid:88) (cid:17) . 1 Li L1, . . . , LG (cid:1) and (cid:0)1/ L1, . . . , 1/ (cid:1), we LG i=1 i=1 (cid:32) (cid:88) i=1 (cid:112) Li 1 Li (cid:33)2 (cid:32) (cid:88) (cid:33) (cid:32) (cid:88) Li i=1 i=1 (cid:33) , 1 Li whose left-hand side equals G2. Hence the desired inequality holds. Equality occurs if and only if all Li are equal. 11 Preprint Proof of CV(gGRPO) CV(gOurs). It is equal to prove: 1 (cid:113)(cid:80)G i= 1 Li (cid:113)(cid:80)G i=1 12α j=1 Lα (cid:80)G , Li > 0. Since all terms are positive, this is equivalent to (cid:33)2 Lα (cid:32) (cid:88) i=1 (cid:32) (cid:88) i=1 12α (cid:33) (cid:32) (cid:88) i=1 (cid:33) . 1 Li Apply the CauchySchwarz inequality to (cid:0)L 12α 2 1 , . . . , 12α 2 (cid:1) and (cid:0)L 2 1 , . . . , 1 2 (cid:1): (cid:32) (cid:88) i=1 12α 2 L 2 (cid:33)2 (cid:32) (cid:88) i= 12α (cid:33) (cid:32) (cid:88) i=1 (cid:33) . 1 Li The left-hand side simplifies to (cid:0)(cid:80)G for all {Li} when α = 1, and for α = 1 only when all Li are equal. i=1 Lα (cid:1)2 , which yields the desired inequality. Equality holds Proof of CV(gOurs) CV(gDAPO) for α [0, 1]. α = 1, then CV(gOurs) = CV(gGRPO) CV(gDAPO). If α = 0, then CV(gOurs) = CV(gDAPO). If For α [0, 1], define (cid:16) (cid:88) Φ(t) := ln (cid:17) , i (cid:16) (α) := ln i=1 (cid:113)(cid:80)G i=1 12α i=1 Lα (cid:80)G (cid:17) = 1 2 Φ(1 2α) Φ(α). It is well known that the log-sum-exp function is convex; hence Φ is convex on R, and therefore Φ is nondecreasing on R. Differentiating gives (α) = Φ(1 2α) + Φ(α). Since α 1 2α for all α [0, 1] and Φ is nondecreasing, we obtain Φ(α) Φ(1 2α) = (α) 0. Thus is nonincreasing on [0, 1], so (α) (0): (cid:113)(cid:80)G i=1 12α i=1 Lα (cid:80)G (cid:113)(cid:80)G i=1 Li , which is exactly CV(gOurs) CV(gDAPO) for all α [0, 1]. Equality holds at α = 0; for α (0, 1], equality occurs if and only if all Li are equal. Overall, we have the following magnitude order: CV(gGRPO) = CV(gOurs; α=1) CV(gOurs; 0<α<1) CV(gDAPO) = CV(gDr. GRPO). A.2 TRAINING DETAILS We provide additional details of the reinforcement learning setup. All experiments are conducted with batch size of 1280, consisting of 128 prompts with 10 rollouts per prompt. The mini-batch size is 320 and the learning rate is 1 106. In both tasks, we use binary reward: 1 for success and 0 for failure. The maximum response length is set to 3072 for all models and tasks, and we further evaluate the 3B model with response length 8192 to test robustness. Following previous works, we do not use KL loss or entropy loss (Yu et al., 2025). We adopt the clip higher trick, setting the lower clip ratio to 0.2 and the upper clip ratio to 0.3 (Yu et al., 2025). For Dr. GRPO and our method, is set to the maximum response length, consistent with Liu et al. (2025). 12 Preprint Method Minerva AIME2024 MATH500 AMC W.Average Average 3B Model GRPO Norm DAPO Norm Dr. GRPO Norm Dr. GRPO Ours GRPO Norm DAPO Norm Dr. GRPO Norm Dr. GRPO Ours GRPO Norm DAPO Norm Dr. GRPO Norm Dr. GRPO Ours 0.228 0.231 0.224 0.231 0.228 0.233 0.227 0.221 0.226 0.239 0.709 0.703 0.697 0.699 0.720 0.088 0.058 0.121 0.088 0. 0.377 0.369 0.383 0.392 0.395 3B Model; 8192 Max Response Length 0.378 0.399 0.386 0.383 0.370 0.703 0.699 0.691 0.699 0.704 0.096 0.104 0.096 0.100 0.121 7B Model 0.275 0.290 0.299 0.295 0.302 0.117 0.158 0.163 0.142 0. 0.777 0.778 0.769 0.777 0.794 0.488 0.474 0.471 0.476 0.483 0.509 0.505 0.502 0.506 0.517 0.508 0.506 0.498 0.503 0.510 0.573 0.578 0.576 0.579 0.592 0.350 0.340 0.356 0.352 0. 0.353 0.357 0.348 0.352 0.359 0.414 0.425 0.426 0.422 0.436 Table 4: Detailed results on the Math task. Performance is evaluated using Avg@8 on each dataset. W.Average denotes the weighted average of the four Avg@8 scores by question counts, and Average is the direct mean. Across three settings, our method consistently yields the best W.Average and Average. Method GRPO Norm DAPO Norm Dr. GRPO Norm Dr. GRPO Ours CD; 3B CD; 3B; 8192 CD; 7B Math; 3B Math; 3B; 8192 Math; 7B 0.652 0.804 0.134 0.171 0.986 0.975 0.900 0.867 0.942 0. 0.726 0.902 0.736 0.581 0.992 0.759 0.854 0.904 0.954 0.974 0.818 0.748 0.937 0.755 0.967 0.743 0.882 0.736 0.970 0.948 Table 5: Monotonicity score, defined as the Pearson correlation between {0, 1, 2, . . . } and the test scores {score0, score50, score100, . . . } recorded every 50 steps. Our method exhibits the highest monotonicity score overall, indicating it promotes steady improvement between training steps. For Math, we train the 7B model for 1 epoch, the 3B model for 3 epochs, the 3B model with length 8192 for 2 epochs, and we evaluate the model on MATH500, Minerva, AMC, and AIME2024 every 50 steps. For CountDown, we train for 500 steps with the 7B model, and for 1000 steps with the 3B models under both length settings (3072 and 8192), and the model is evaluated on held-out test set of 1000 samples every 50 steps. For both Math and Countdown, we use Avg@8 accuracy with the sampling temperature fixed at 1 as the evaluation metric. The training dynamics shown in Figures 2, 5, and 8 are based on Avg@8 accuracy evaluated every 50 steps. For Countdown, this corresponds directly to Avg@8 accuracy on its test set, while for Math, it is computed as weighted average over four test datasets, where the weights are proportional to the number of questions in each dataset. The results in Tables 2 and 4 are reported for the best model under each setting. The best model is selected by the highest Avg@8 for Countdown or the highest weighted Avg@8 for Math, and this criterion is applied consistently across all methods. Table 5 further characterizes the training process using monotonicity score, defined as the Pearson correlation between the step indices {0, 1, 2, . . . } and the corresponding Avg@8 scores {score0, score50, score100, . . . } evaluated every 50 steps."
        }
    ],
    "affiliations": [
        "Microsoft Research",
        "Tsinghua University"
    ]
}