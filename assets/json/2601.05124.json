{
    "paper_title": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing",
    "authors": [
        "Runze He",
        "Yiji Cheng",
        "Tiankai Hang",
        "Zhimin Li",
        "Yu Xu",
        "Zijin Yin",
        "Shiyi Zhang",
        "Wenxun Dai",
        "Penghui Du",
        "Ao Ma",
        "Chunyu Wang",
        "Qinglin Lu",
        "Jizhong Han",
        "Jiao Dai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In-context image generation and editing (ICGE) enables users to specify visual concepts through interleaved image-text prompts, demanding precise understanding and faithful execution of user intent. Although recent unified multimodal models exhibit promising understanding capabilities, these strengths often fail to transfer effectively to image generation. We introduce Re-Align, a unified framework that bridges the gap between understanding and generation through structured reasoning-guided alignment. At its core lies the In-Context Chain-of-Thought (IC-CoT), a structured reasoning paradigm that decouples semantic guidance and reference association, providing clear textual target and mitigating confusion among reference images. Furthermore, Re-Align introduces an effective RL training scheme that leverages a surrogate reward to measure the alignment between structured reasoning text and the generated image, thereby improving the model's overall performance on ICGE tasks. Extensive experiments verify that Re-Align outperforms competitive methods of comparable model scale and resources on both in-context image generation and editing tasks."
        },
        {
            "title": "Start",
            "content": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing Runze He1,2,3, Yiji Cheng1, Tiankai Hang1, Zhimin Li1, Yu Xu1, Zijin Yin1, Shiyi Zhang1, Wenxun Dai1, Penghui Du3, Ao Ma3, Chunyu Wang1, Qinglin Lu1, Jizhong Han2,3, Jiao Dai2,3 1Hunyuan, Tencent 2IIE, CAS 3UCAS Project Page: https://hrz2000.github.io/realign 6 2 0 2 8 ] . [ 1 4 2 1 5 0 . 1 0 6 2 : r Figure 1. Our proposed Re-Align supports image synthesis conditioned on flexible image-text interleaved prompts, namely a) in-context image generation, also referred to as subject-driven image generation, and b) in-context image editing, also referred to as reference-based image editing. c) An inference example from Re-Align, including an aligned reasoningimage pair. The reasoning text is converted from XML to JSON for clearer visualization. Abstract ful execution of user intent. Although recent unified multimodal models exhibit promising understanding capabilities, these strengths often fail to transfer effectively to image generation. We introduce Re-Align, unified framework that bridges the gap between understanding and generation through structured reasoning-guided alignment. At In-context image generation and editing (ICGE) enables users to specify visual concepts through interleaved imagetext prompts, demanding precise understanding and faithProject lead. Corresponding author. its core lies the In-Context Chain-of-Thought (IC-CoT), structured reasoning paradigm that decouples semantic guidance and reference association, providing clear textual target and mitigating confusion among reference images. Furthermore, Re-Align introduces an effective RL training scheme that leverages surrogate reward to measure the alignment between structured reasoning text and the generated image, thereby improving the models overall performance on ICGE tasks. Extensive experiments verify that Re-Align outperforms competitive methods of comparable model scale and resources on both in-context image generation and editing tasks. 1. Introduction In recent years, the field of image synthesis [7, 17, 23, 30, 35, 44, 50, 51, 54, 59] has attracted widespread attention from the research community. Among them, diffusion models [23, 35, 50] have made significant progress due to their ability to generate diverse and high-quality samples. Given that pure text prompts often fail to accurately express visual concepts defined by reference images, image-conditioned visual generation [16, 29, 45, 52, 61] has also been extensively explored. Recently, with the ability to process interleaved imagetext inputs, in-context image generation and editing (ICGE) has become increasingly popular. Nevertheless, implementing ICGE is non-trivial, as it requires both precise understanding of the complex interleaved inputs and faithful execution of the users intent. Reasoning mechanisms that are effective for text-to-image and image editing, however, fail to function effectively in ICGE tasks. For example, the leading native multimodal model BAGEL [12] can accurately interpret instructions and produce plausible reasoning, yet the final generated image fails to align with this reasoning, as shown in Figure 2. This suggests that although the reasoning ability is strong, it has not yet helped downstream image generation, and there is misalignment between the two. Building on these insights, we propose Re-Align, unified framework designed for in-context image generation and editing with structured Reasoning-guided Alignment. Re-Align adopts structured reasoning mechanism, namely In-Context Chain-of-Thought (IC-CoT), which explicitly decomposes the reasoning process into semantic guidance and reference association and is uniformly applied to both image generation and editing. The former provides clear textual target for image generation, partly simplifying the image-text interleaved task into text-to-image generation; the latter analyzes the role of each reference image within the multi-image context to prevent reference confusion. To further enhance models performance on complex interleaved prompts, we employ Group Relative Policy Optimization (GRPO) with surrogate reward that measures Figure 2. Comparison of the reasoning paradigms of BAGEL and Re-Align. While BAGEL exhibits competent reasoning abilities, the resulting images fail to reflect its reasoning process in the complex image-text interleaved prompt. In contrast, Re-Align achieves strong reasoninggeneration alignment, facilitated by the structured IC-CoT. the correspondence between the CoT context and the resulting image. The reasoning-induced diversity strategy is proposed to improve the diversity of samples between groups, thereby stabilizing the training of GRPO. To support model training, we develop an automated data construction and filtering pipeline, yielding Re-Align-410K, high-quality ICGE dataset with IC-CoT annotations spanning multiple in-context image generation and editing tasks. Our contributions are summarized as follows: (1) We present Re-Align, which achieves state-of-the-art performance among methods with comparable resources and model scales on both in-context image generation and editing tasks. (2) We propose structured reasoning paradigm, IC-CoT, which provides clear target for visual generation through decoupled semantic guidance and reference asso- (3) We further introduce surrogate reward that ciation. measures the alignment between the reasoning context and the generated image, along with reasoning-induced diversity strategy, enabling effective policy optimization for improved model performance. 2. Related Works 2.1. In-Context Image Generation and Editing Instead of pursuing isolated, single-purpose conditional generation, such as image customization [16, 29, 33, 45] or image editing [4, 21, 25, 70, 73], in-context image generation and editing focuses on general-purpose generation tasks guided by flexible interleaved image-text prompts. Closed-source systems such as GPT-4o [40] and Nano Banana [19] have exhibited remarkable performance on such tasks. Meanwhile, open-source models [11, 12, 60, 62, 69] are steadily progressing toward this goal. BAGEL [12], as native multimodal foundation model, inherently supports simple in-context image generation and editing tasks. OmniGen2 [60], conditioned on the hidden states of an MLLM [2], demonstrates versatile image generation capabilities. Our concurrent work, DreamOmni2 [62], employs joint training framework for the generation/editing model and MLLM, sharing the same goal as ICGE. Despite their promising results, these methods remain inadequate when handling complex imagetext interleaved instructions. 2.2. Unified Understanding and Generation Recently, unified models [912, 22, 36, 41, 56, 58, 60, 65, 74] that integrate both understanding and generation capabilities have been extensively explored. Among them, Emu3 [56], Janus [58], and Janus-Pro [10] model understanding and generation solely through next-token prediction. Show-o [64] unifies autoregressive and discrete diffusion modeling, enabling adaptive handling of inputs and outputs across mixed modalities. Several approaches [9, 41, 60] employ frozen LLMs for understanding and an additional DiT [42] for image generation, thereby reducing training overhead and mitigating interference between the two capabilities. Transfusion [74] and BAGEL [12] employ autoregressive modeling for understanding and diffusion modeling for image generation within single transformer architecture, with BAGEL further introducing Mixture-ofTransformers structure to enhance performance. 2.3. Reinforcement Learning for Visual Generation Reinforcement learning (RL) has recently achieved remarkable progress in the development of large language models (LLMs) [5, 13, 20, 24, 47], which has in turn spurred growing interest in applying RL techniques to visual generation tasks [3, 14, 34, 48, 55, 66, 67]. Recent research has particularly explored the use of Group Relative Policy Optimization (GRPO) [47], owing to its ability to eliminate the need for separate value network, thereby improving memory efficiency compared with Proximal Policy Optimization (PPO) [46]. Building on this, FlowGRPO [34] and DanceGRPO [67] extend the GRPO to image and video generation. However, existing RL-based approaches predominantly focus on optimizing text-conditioned generation, and still lack effective reward design and comprehensive experimental validation for more complex in-context image generation and editing tasks. 3. Method 3.1. Overview As illustrated in Figure 3, Re-Align serves as unified framework designed for in-context image generation and editing, based on the architecture of the multimodal foundation model BAGEL [12]. Given image-text interleaved prompt , including serveral reference images, text instruction which couples visual concepts like Replace the hat in the first image with the cup in the second image, ReAlign generates structured reasoning text, i.e. In-Context Chain-of-Thought, denoted as = {r1, r2, ..., rM } with reasoning tokens, and resulting image sequentially. Specifically, we maximize the likelihood of reasoning tokens given prompt and all previously generated reasoning tokens by employing the standard language modeling objective: Lcot(θ) = (cid:88) log pθ(riP , r<i), (1) where indicates the conditional probability of the model, parameterized by weights θ. Let x0 p0 be sample from the real data distribution and x1 p1 noise sample from the Gaussian distribution. We adopt the Rectified Flow [35] to learn the image generation following BAGEL [12], with the objective: Limg(θ) = Et,x0p0,x1p (cid:2)v vθ(xt, t, , R)2(cid:3) , (2) where xt = (1t)x0+tx1 for [0, 1] denotes noisy data, vθ(xt, t, ) is the predicted velocity field, and = x1 x0 is the target velocity field. 3.2. In-Context Chain-of-Thought Previous works [12, 15, 57] have demonstrated the benefits of introducing the reasoning capability into visual generation. Nevertheless, these approaches are limited to textconditioned image generation and editing, while effective reasoning in more complex ICGE tasks remains unexplored. When faced with complex interleaved image-text prompts, the leading unified multimodal model BAGEL [12] fails to produce consistent reasoning and image outputs, indicating that its reasoning mechanism is not effectively utilized. Thereby, we aim to leverage the reasoning mechanism to bridge the gap between the models understanding and generation abilities. Specifically, we propose InContext Chain-of-Thought (IC-CoT), which is structured reasoning framework, including two complementary components: semantic guidance and reference association. The former provides an explicit caption to facilitate image generation under complex conditions, while the latter captures the associative relationships between each reference image and the target to prevent reference confusion. Semantic Guidance Interleaved image-text prompts often convey complex and implicit user intentions, making direct image generation challenging due to the intricate semantic interactions between visual and textual elements. IC-CoT explicitly predicts the caption of the resulting image as part of its reasoning process, starting with <out caption> 3 Figure 3. The two-stage training pipeline of Re-Align. First, we perform supervised fine-tuning on carefully curated training data to enable the model to generate images guided by IC-CoT reasoning. Next, we apply policy optimization to further enhance reasoninggeneration consistency, using an alignment score between the structured IC-CoT and the corresponding generated image. and ending with </out caption>. The predicted caption provides direct semantic guidance for subsequent image generation, while remaining compatible with both instructional and descriptive user inputs. By incorporating the predicted caption into the reasoning process, complex incontext image generation and editing tasks can be partially reduced to text-conditioned generation, thereby easing the learning process. Reference Association ICGE features user-provided reference images. However, the flexible nature of interleaved image-text inputs often leads users to omit explicit references to image indices or corresponding subjects, and instead use ambiguous expressions such as put them together, making it more difficult for the model to interpret the users intended output. To address this, ICCoT introduces reference associations in the reasoning process, starting with <relation i> and ending with </relation i> for the reference image i. Each reference association specifies the role of the corresponding reference image in generating the final output, and the number of associations matches the number of provided reference images. The structured IC-CoT plays key role in bridging the gap between the models understanding and generation capabilities. Compared to the prompt-expansion paradigm such as BAGEL [12], IC-CoT employs compact structured representation to provide clear semantic and reference cues for image generation, thereby reducing ambiguity and lowering both training and inference overhead. Moreover, the structured IC-CoT enables effective extraction of key elements, facilitating the subsequent alignment stage. 3.3. Reasoning-Generation Alignment Despite the remarkable progress of GRPO [47] in visual generation [34, 67], existing reward models are typically designed for text-conditioned generation. In contrast, ICGE tasks involve interleaved imagetext inputs, diverse generation and editing tasks, and multidimensional evaluation criteria, making the construction of dedicated reward model extremely costly and complex. Therefore, applying reinforcement learning to ICGE remains challenging. Surrogate Reward for ICGE Instead of designing taskspecific reward models, we introduce surrogate reward that measures the alignment between the reasoning context and the generated image, thus indirectly improving the models overall performance. Aligning unstructured reasoning texts with images is challenging, as it is difficult to extract representative semantic content that is useful for bridging the two modalities. Thanks to the ICCoTs structured format, we can readily extract the semantic guidance enclosed within <out caption> and </out caption>, i.e., the predicted caption c. The image-text similarity between and the generated image then serves as the reward signal s, which is computed as 4 Figure 4. The data construction pipeline of Re-Align-410K and its task distribution. a) reference images preparation, b) adaptive instruction generation, c) reasoning text generation, d) target image generation, e) data filtering, and f) the data distribution of Re-Align-410K. follows: s(x, c) = E(x)T (c) E(x) (c) , (3) Here, and are the image and text encoders of CLIP [43], respectively, and denotes the L2 norm. Reasoning-Induced Diversity Strategy In ICGE tasks, the explicit visual concepts provided in the input impose strong constraints on the generation process, thereby reducing sample diversity compared with text-conditioned generation. When the differences among generated samples become small, the reward variance also diminishes; after normalization, even minor fluctuations may be disproportionately amplified, ultimately hindering the models ability to learn effectively from the reward signal. Prior works [34, 67] attempt to enlarge sample diversity by increasing the SDE noise scale, but excessive noise often degrades image quality. In contrast, we generate distinct ICCoT reasoning chains for each sample within group, introducing diverse reasoning trajectories that naturally diversify the outputs. This strategy increases reward variance in controlled manner, providing more informative learning signals and thereby stabilizing the training process. 3.4. Dataset Construction As shown in Figure 4, to support model training, we introduce Re-Align-410K, high-quality collection covering the task types summarized in Table 1. The dataset is constructed via an automated data construction pipeline that integrates advanced MLLMs [1, 53] and state-of-the-art image generation models [39]. Reference Images Preparation Unlike conventional singleimage conditioned generation or editing tasks [4, 37, 49, 68, 7072], ICGE supports flexible interleaving of multiple image and text inputs. This setting demands dataset with diverse reference-image combinations. To accommodate this requirement, we construct source image pool covering characters, objects, and scenes, from which multiple references are sampled according to each task type. For subject-reference tasks, the sampled references are drawn from character and object categories, whereas scene-centric tasks additionally incorporate scene images. For attributereference editing, references are selected with greater flexibility to support broad spectrum of attribute-guided modifications. 1. In-Context Image Generation Subject-driven Generation: Generate referenced subject in novel context. Subject-Subject Compositional Generation: Combine multiple referenced subjects within new scene. Subject-Scene Compositional Generation: Place multiple referenced subjects into referenced scene under new context. 2. In-Context Image Editing Reference Subject Editing: Add referenced subject to an input image or replace an existing subject with the referenced one. Reference Attribute Editing: Transfer attributes from reference, such as texture, pose, style or other visual characteristics, to modify the appearance of the subject (Local) or target image (Global). Reference Scene Editing: Modify the scene of an image based on referenced one. Table 1. Overview of the tasks covered in Re-Align-410K. Adaptive Instruction Construction Next, we generate instructions tailored to each group of reference images. Since fixed manual rules cannot capture the diversity of visual content, we leverage the advanced Gemini 2.5 [53] for adaptive instruction generation. carefully designed system prompt guides the MLLM to produce executable instructions conditioned on the input images, while additionally encouraging attention to secondary visual details to increase the complexity and richness of the generated instructions. Reasoning Text Generation Unlike previous works [60, 62, 69], which focus solely on constructing inputoutput 5 pairs while neglecting the underlying reasoning process, we additionally prompt the MLLM to generate the structured IC-CoT introduced in Section 3.2. The reference images together with the corresponding instructions are provided to the MLLM, which produces the structured reasoning output under predefined system prompt. Notably, the target image is intentionally omitted during this stage, as introducing additional visual inputs may increase hallucination and impair the models ability to correctly interpret complex multi-image relationships. Target Image Generation As demonstrated in prior studies [8, 9, 69], data generated by the state-of-the-art image generator GPT-4o [39] effectively handles complex and long-tail visual generation scenarios. Accordingly, we feed the reference image group along with the corresponding generated instructions into GPT-4o to synthesize the target images. Unlike video frame extraction methods like OmniGen2 [60], which are largely confined to in-context image generation, our approach can effectively handle wider variety of editing tasks. Data Filtering To ensure high data quality, we adopt multidimensional filtering strategy. First, we leverage the structured IC-CoT format to compute imagetext similarity between the caption predicted by the reasoning process and the target image, with low similarity indicating misalignment between reasoning and generation. Second, we assess visual quality using image aesthetics [32] and human preference metrics [27, 66]. Third, we evaluate instruction following and semantic consistency capability using OmniContextScore [60]. Samples below any threshold are discarded, removing approximately 20% of the data and resulting in final dataset of 410K high-quality samples. 4. Experiments 4.1. Experimental Setup Baselines. We compare Re-Align with several recent representative methods widely recognized for in-context image generation and editing, including: (1) BAGEL [12], foundational model that natively supports multimodal understanding and generation; (2) OmniGen2 [60], versatile generative model providing unified solution for diverse tasks; (3) Echo-4o [69], fine-tuning BAGEL on high-quality synthetic image dataset; (4) Qwen-ImageEdit(2509) [59], version of Qwen-Image-Edit supporting multi-image input; and (5) DreamOmni2 [62], concurrent work focusing on multimodal instruction-based image editing and generation. Implementation Details. Proposed Re-Align builds upon BAGEL [12] and is compatible with other models [6, 10, 74] that provide unified understanding and generation capabilities. We employ mixed training strategy, both with and without IC-CoT, to provide flexibility during inference. The SFT stage is trained for 100,000 steps on 64 NVIDIA H20 GPUs with learning rate of 5 106, while the reasoninggeneration alignment stage is trained for 200 steps with group size of 32 and learning rate of 1 106, which is sufficient to ensure alignment convergence and avoid unnecessary reward hacking in subsequent training. By default, images are generated at 10241024 resolution using 50 denoising steps, following [12]. Benchmarks. We evaluate models ICGE capabilities on two mainstream benchmarks: OmniContext [60] and DreamOmni2Bench [62]. OmniContext provides comprehensive suite for evaluating in-context image generation across diverse scenarios. In contrast, DreamOmni2Bench offers large collection of generation and editing tasks, with one to five reference images as input, covering diverse editing settings ranging from local and global attributes to object-level manipulations. Evaluation Metrics. Similar to VIEScore [28] in image editing, OmniContext [60] uses the multimodal large language model GPT-4.1 [38] as an automatic evaluator for incontext visual generation. It includes three metrics: Prompt Following (PF), measuring whether the generated image fulfills the editing intent; Subject Consistency (SC), evaluating the consistency of visual concepts between the generated image and reference images; and an Overall Score, computed as the geometric mean of PF and SC. Since the official evaluation code for DreamOmni2Bench [62] is not yet available, we employ OmniContexts metric framework to evaluate model performance on this benchmark as well. 4.2. Qualitative Results As shown in Figure 5, we provide qualitative comparisons with recent baselines on the in-context image generation and editing tasks. For the in-context image generation task in the first two rows, most methods are able to produce roughly correct images. However, OmniGen2 [60] often incorporates irrelevant elements from the reference images, such as the instrument in the 1st row and the blue background in the 2nd row. BAGEL [12], Qwen-ImageEdit [59], and DreamOmni2 [62] exhibit weaker subject consistency, resulting in significant mismatch in the appearance of the humans in the 2nd row. For the in-context image editing task in the last three rows, most existing models fail to correctly interpret the editing intent. This is particularly evident in the material replacement shown in the 3rd row and the object addition in the 4th row, where the generated outputs often deviate substantially from the desired edits. Although DreamOmni2 has almost finished editing the 4th row, there are changes in hand gestures and inconsistencies in background light. Overall, Re-Align demonstrates distinct advantage in addressing complex incontext generation and editing challenges. 6 Figure 5. Qualitative comparisons of proposed Re-Align with BAGEL [12], OmniGen2 [60], Echo-4o [69], Qwen-Image-Edit(2509) [59] and DreamOmni2 [62] on the in-context image generation and editing tasks."
        },
        {
            "title": "Model",
            "content": "FLUX.1 Kontext [Max] [31] Gemini 2.0 Flash [18] Gemini 2.5 Flash Image [19] GPT-4o [39] Emu3.5 [11] OmniGen [63] InfiniteYou [26] UNO [61] BAGEL [12] OmniGen2 [60] Qwen-Image-Edit-2509 [59] DreamOmni2 [62] Re-Align (Ours)"
        },
        {
            "title": "SCENE",
            "content": "Average Character Object Character Object Char. + Obj. Character Object Char. + Obj. 8.48 5.06 8.62 8.90 8.72 7.21 6.05 6.60 5.48 8.05 8.35 7.36 8.25 8.68 5.17 8.91 9.01 9.46 5.71 - 6.83 7.03 7.58 9.13 7.43 8. - 2.91 7.88 9.07 8.65 5.65 - 2.54 5.17 7.11 7.65 6.10 8.25 - 2.16 8.92 8.95 9.09 5.44 - 6.51 6.64 7.13 8.85 6.73 8.07 - 3.80 7.39 8.54 8.78 4.68 - 4.39 6.24 7.45 7.90 6.66 8. - 3.02 7.29 8.90 8.78 3.59 - 2.06 4.07 6.38 5.16 5.20 8.21 - 3.89 7.05 8.44 8.89 4.32 - 4.33 5.71 6.71 7.75 5.34 8.25 - 2.92 6.68 8.60 8.15 5.12 - 4.37 5.47 7.04 6.73 5.64 7. - 3.62 7.84 8.80 8.82 4.34 - 4.71 5.73 7.18 7.69 6.31 8.21 Table 2. Quantitative comparison results on OmniContext [60]. Char. + Obj. indicates Character + Object. 4.3. Quantitative Results We present quantitative comparisons for in-context image generation on the OmniContext benchmark [60], as reported in Table 2, and for both in-context image editing and generation on DreamOmni2Bench [62], as reported in Table 3. Compared with models having comparable scale and computational resources, Re-Align achieves the highest overall average score  (Table 2)  . It ranks second only to Qwen-Image-Edit [59] in the SINGLE task and achieves the best overall performance in MULTIPLE and SCENE tasks, demonstrating the effectiveness of our approach for in-context image generation. This finding is consistent with the assessment in the generation section of Table 3. The editing section of DreamOmni2Bench [62] covers Add, Replace, Global, and Local edits, where Add and Replace focus on subject-referenced editing, and Global and Local on attribute-referenced editing. Echo-4o [69] performs well in the Add task but poorly in the more complex Global and 7 Figure 6. More examples for In-Context Image Generation and Editing. The last image in each group is the generated result, and the others are input reference images. 8 Model Add Replace Global Local Editing Generation PF SC Overall PF SC Overall PF SC Overall PF SC Overall PF SC Overall BAGEL [12] OmniGen2 [60] Echo-4o [69] Qwen-Image-Edit(2509) [59] DreamOmni2* [62] Re-Align (Ours) 4.09 7.64 8.36 6.09 6.73 9.27 5.18 7.55 8.73 7.91 7.91 9.27 4.58 7.52 8.51 6.51 6.87 9.27 1.15 5.37 3.85 2.48 6.78 8. 5.48 6.07 6.52 4.93 7.56 8.81 1.37 5.6 4.51 2.79 7.05 8.61 2.09 6.81 4.38 3.26 7.34 7.47 4.98 7.28 7.66 5.28 8.68 8.57 2.46 6.88 5.16 3.21 7.76 7.85 1.61 2.76 1.92 2.49 5.14 6. 3.58 5.28 6.24 4.98 8.18 8.54 1.63 2.99 2.41 2.73 5.44 6.35 5.72 5.15 6.68 5.98 7.01 7.74 5.77 5.74 7.2 5.78 6.71 7.67 5.25 4.99 6.59 5.45 6.56 7.24 Table 3. Quantitative comparison results on DreamOmni2Bench [62]. Prompt Following (PF), Subject Consistency (SC), and Overall scores are reported (higher is better). * denotes that DreamOmni2 employs different parameters for editing and generation tasks. Model Editing 2 1 2 4 Generation PF SC Overall PF SC Overall PF SC Overall PF SC Overall PF SC Overall BAGEL [12] Echo-4o [69] OmniGen2 [60] Qwen-Image-Edit(2509) [59] DreamOmni2* [62] Re-Align (Ours) 1.8 3.16 4.41 2.88 6.01 6.94 4.28 6.78 6.02 5.21 8.21 8.62 1.97 3.72 4.58 3.06 6.33 7.19 6.38 7.44 5.77 7.33 6.97 7.92 5.49 6.79 4.97 5.36 5.74 7.1 4.76 6.68 4.45 5.34 5.69 6. 4.92 4.8 4.64 5.28 7.52 8.04 5.52 6.84 6.64 6.4 8.2 8.04 5.04 5.1 5.34 5.65 7.65 7.93 5.28 6.59 5.17 5.72 7.0 7.72 6.03 7.52 6.0 6.1 6.62 7.93 5.52 6.9 5.42 5.76 6.76 7. 6.05 7.67 4.57 4.67 6.48 7.05 6.24 7.95 5.71 5.38 6.86 7.9 6.04 7.75 5.0 4.95 6.61 7.39 Table 4. Impact of reference image number on DreamOmni2Bench [62]. Local edits. DreamOmni2 [62], which employs separate parameters for generation and editing, exhibits balanced performance across editing types but remains inferior overall to Re-Align. In contrast, Re-Align consistently attains higher PF and SC scores across tasks, highlighting its strong advantage in in-context image editing. SFT RGA RID PF SC Overall CLIPout 6.92 7.51 7.46 7.61 5.47 6.46 6.54 6. 5.80 6.77 6.80 6.89 32.44 33.32 33.50 33.90 Table 5. Ablation studies on the training stages and strategies. SFT denotes supervised fine-tuning for image generation conditioned on IC-CoT, RGA represents reasoninggeneration alignment, and RID refers to the reasoning-induced diversity strategy. 4.4. Ablation Study We perform an ablation study to validate the effectiveness of the proposed IC-CoT. Specifically, we compare it with two variants: one that excludes the reasoning process (w/o CoT) and another that adopts unstructured reasoning following in [12] (BagelCoT). As illustrated in Figure 7, results from the GSB (Good/Same/Bad) evaluation clearly demonstrate that IC-CoT outperforms the two variants, with win rates 20% and 16.25% higher, respectively, confirming the superiority of IC-CoT design. Besides, we conduct ablation studies to evaluate the effectiveness of the training stages and strategies in ReAlign. As shown in Table 5, we report the OmniContextScore [60] along with an additional metric CLIPout assessing textimage consistency between the generated image and the ground-truth caption on subset of OmniContext. After supervised fine-tuning (SFT), the model learns image generation guided by the IC-CoT reasoning, achieving significant improvements across all metrics. Reasoninggeneration alignment (RGA) improves the CLIPout score but brings no significant gain in PF score, indicating that low sample diversity adversely affects RL training. When the reasoning-induced diversity (RID) strategy is subsequently applied, overall performance improves, highlighting the critical role of output diversity in alignment training. This is consistent with the results shown in Figure 8, where the well-aligned model produces images that better reflect the intended instructions. 4.5. More Results More Visualization Figure 6(a) provides additional incontext image generation and editing examples, demonstrating that the model produces accurate and highly consistent images when conditioned on one to four reference inputs. Furthermore, Figure 6(b) showcases in-context image editing capabilities, where the first, second, and third rows illustrate object addition, object replacement, and attribute modification with reference images, respectively. These results underscore the strong versatility and effectiveness of Re-Align across broad range of creative generation tasks. 9 Figure 7. GSB evaluation results from the ablation study on the reasoning mechanism. Figure 8. Ablation visualization of reasoninggeneration alignment with reasoning-induced diversity (RGA+RID). Impact of Reference Image Number As presented in Table 4, to evaluate the models performance with varying numbers of reference images, we conduct experiments on DreamOmni2Bench [62]. Two reference images are used for all editing tasks, while generation tasks employ one to four reference images.Re-Align consistently delivers strong PF and SC scores across all configurations, frequently ranking first or second across all metrics and thus demonstrating the most robust overall performance. In contrast, other models struggle to maintain comparable results. Failure Cases We also show several failure cases on the ICGE task as shown in Figure 9. First, in rare cases, the model fails to generate correct reasoning texts. For example, when processing the complex action semantics of come here, it results in subpar image outputs. Furthermore, in editing tasks without dedicated training (e.g., editing based on referenced text styles or object color schemes), the model demonstrates semantic comprehension but produces images with low reference consistency. Scaling up the model size and integrating more comprehensive training data may help alleviate these issues. 5. Conclusion and Limitation In this work, we propose unified framework for in-context image generation and editing that bridges understanding 10 Figure 9. Failure cases of Re-Align. In the first row, the modelgenerated reasoning text appears on an orange background, with incorrect parts marked in red. and generation via reasoning mechanism. We design an IC-CoT that provides explicit semantic guidance and reference association, providing clear targets for subsequent image generation. The reasoning alignment stage further enhances consistency between the reasoning content and the generated image via policy optimization. Despite these advances, our work still faces several challenges. First, our model size and data scale are limited compared to production-level work like GPT-4o [39], which may constrain the models performance in diverse scenarios. Second, the current IC-CoT operates purely at the textual level; extending it to visual Chain-of-Thought reasoning may be promising direction for future work."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 5 [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 3 [3] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023. 3 [4] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1839218402, 2023. 2, 5 sion models. Advances in Neural Information Processing Systems, 36:7985879885, 2023. 3 [5] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models In Advances in Neural Information are few-shot learners. Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. [6] Siyu Cao, Hangting Chen, Peng Chen, Yiji Cheng, Yutao Cui, Xinchi Deng, Ying Dong, Kipper Gong, Tianpeng Gu, Xiusen Gu, et al. Hunyuanimage 3.0 technical report. arXiv preprint arXiv:2509.23951, 2025. 6 [7] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1131511325, 2022. 2 [8] Junying Chen, Zhenyang Cai, Pengcheng Chen, Shunian Chen, Ke Ji, Xidong Wang, Yunjin Yang, and Benyou Sharegpt-4o-image: Aligning multimodal modWang. arXiv preprint els with gpt-4o-level image generation. arXiv:2506.18095, 2025. 6 [9] Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, et al. Blip3-o: family of fully open unified multimodal models-architecture, training and dataset. arXiv preprint arXiv:2505.09568, 2025. 3, 6 [10] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Januspro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. 3, 6 [11] Yufeng Cui, Honghao Chen, Haoge Deng, Xu Huang, Xinghang Li, Jirong Liu, Yang Liu, Zhuoyan Luo, Jinsheng Wang, Wenxuan Wang, et al. Emu3. 5: Native multimodal models are world learners. arXiv preprint arXiv:2510.26583, 2025. 3, [12] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. 2, 3, 4, 6, 7, 9 [13] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 3 [14] Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Dpok: Reinforcement learning for fine-tuning text-to-image diffu- [15] Rongyao Fang, Chengqi Duan, Kun Wang, Linjiang Huang, Hao Li, Shilin Yan, Hao Tian, Xingyu Zeng, Rui Zhao, Jifeng Dai, et al. Got: Unleashing reasoning capability of multimodal large language model for visual generation and editing. arXiv preprint arXiv:2503.10639, 2025. 3 [16] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion, 2022. 2 [17] Rinon Gal, Or Patashnik, Haggai Maron, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. Stylegan-nada: Clipguided domain adaptation of image generators. ACM Transactions on Graphics (TOG), 41(4):113, 2022. [18] Google. Gemini 2.0 flash. https://developers. googleblog . com / en / experiment - with - gemini-20-flash-native-image-generation, 2025. 7 [19] Google. Nano banana. Technical report, Google, 2025. 2, 7 [20] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 3 [21] Runze He, Kai Ma, Linjiang Huang, Shaofei Huang, Jialin Gao, Xiaoming Wei, Jiao Dai, Jizhong Han, and Si Liu. Freeedit: Mask-free reference-based image editing with multi-modal instruction, 2024. 2 [22] Runze He, Bo Cheng, Yuhang Ma, Qingxiang Jia, Shanyuan Liu, Ao Ma, Xiaoyu Wu, Liebucha Wu, Dawei Leng, and Yuhui Yin. Plangen: Towards unified layout planning and image generation in auto-regressive vision language models. arXiv preprint arXiv:2503.10127, 2025. 3 [23] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. [24] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. 3 [25] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation with conditional adversarial networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 59675976. IEEE Computer Society, 2017. 2 [26] Liming Jiang, Qing Yan, Yumin Jia, Zichuan Liu, Hao Kang, and Xin Lu. Infiniteyou: Flexible photo recrafting while preserving your identity. arXiv preprint arXiv:2503.16418, 2025. 7 [27] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. arXiv preprint arXiv:2305.01569, 2023. 6 [28] Max Ku, Dongfu Jiang, Cong Wei, Xiang Yue, and Wenhu Chen. Viescore: Towards explainable metrics for arXiv preprint conditional arXiv:2312.14867, 2023. 6 image synthesis evaluation. 11 [29] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. 2023. 2 [30] Black Forest Labs. https : / / blackforestlabs . ai / announcing - black - forest-labs, 2024. 2 Flux. [31] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, et al. Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv preprint arXiv:2506.15742, 2025. 7 [32] LAION. Laion-aesthetics v2. https://github.com/ christophschuhmann / improved - aesthetic - predictor, 2022. 6 [33] Dongxu Li, Junnan Li, and Steven CH Hoi. Blipdiffusion: Pre-trained subject representation for controllable text-to-image generation and editing. arXiv preprint arXiv:2305.14720, 2023. 2 [34] Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. Flow-grpo: Training flow matching models via online rl. arXiv preprint arXiv:2505.05470, 2025. 3, 4, 5 [35] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 2, 3 [36] Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Xingkai yu, Liang Zhao, Yisong Wang, Jiaying Liu, and Chong Ruan. Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation, 2024. 3 [37] Sicheng Mo, Fangzhou Mu, Kuan Heng Lin, Yanli Liu, Bochen Guan, Yin Li, and Bolei Zhou. Freecontrol: Training-free spatial control of any text-to-image diffusion model with any condition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 74657475, 2024. 5 [38] OpenAI. Gpt-4-1. https://openai.com/index/ gpt-4-1, 2025. 6 [39] OpenAI. Gpt-4o. https://openai.com/index/ introducing-4o-image-generation, 2025. 5, 6, 7, 10 [40] OpenAI. Gpt-image. Technical report, OpenAI, 2025. 2 [41] Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, et al. Transfer between modalities with metaqueries. arXiv preprint arXiv:2504.06256, 2025. [42] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. 3 [43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 5 12 [44] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 2 [45] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv preprint arXiv:2208.12242, 2022. 2 [46] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 3 [47] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 3, 4 [48] Xiangwei Shen, Zhimin Li, Zhantao Yang, Shiyi Zhang, Yingfang Zhang, Donghao Li, Chunyu Wang, Qinglin Lu, and Yansong Tang. Directly aligning the full diffusion trajectory with fine-grained human preference. arXiv preprint arXiv:2509.06942, 2025. [49] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks. 2023. 5 [50] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021. 2 [51] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. 2 [52] Jiale Tao, Yanbing Zhang, Qixun Wang, Yiji Cheng, Haofan Wang, Xu Bai, Zhengguang Zhou, Ruihuang Li, Linqing Wang, Chunyu Wang, et al. Instantcharacter: Personalize any characters with scalable diffusion transformer framework. arXiv preprint arXiv:2504.12395, 2025. 2 [53] Gemini Team. Gemini 2.5: Pushing the frontier with long context, and arXiv preprint advanced reasoning, multimodality, next generation agentic capabilities. arXiv:2507.06261, 2025. 5 [54] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. 2024. 2 [55] Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 82288238, 2024. 3 [56] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. 3 [57] Yi Wang, Mushui Liu, Wanggui He, Longxiang Zhang, Ziwei Huang, Guanghao Zhang, Fangxun Shu, Zhong Tao, Dong She, Zhelun Yu, et al. Mint: Multi-modal chain of thought in unified generative models for enhanced image generation. arXiv preprint arXiv:2503.01298, 2025. 3 4o synthetic images for improved image generation. arXiv preprint arXiv:2508.09987, 2025. 3, 5, 6, 7, 9 [70] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instructionIn Advances in Neural Information guided image editing. Processing Systems, 2023. 2, [71] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV, 2023. [72] Zechuan Zhang, Ji Xie, Yu Lu, Zongxin Yang, and Yi Yang. In-context edit: Enabling instructional image editing with incontext generation in large scale diffusion transformer. arXiv preprint arXiv:2504.20690, 2025. 5 [73] Haozhe Zhao, Xiaojian Shawn Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. Ultraedit: Instruction-based fine-grained image editing at scale. Advances in Neural Information Processing Systems, 37:30583093, 2024. 2 [74] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. 3, 6 [58] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. arXiv preprint arXiv:2410.13848, 2024. 3 [59] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, and Zenan Liu. Qwen-image technical report, 2025. 2, 6, 7, [60] Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, et al. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025. 3, 5, 6, 7, 9 [61] Shaojin Wu, Mengqi Huang, Wenxu Wu, Yufeng Cheng, Fei Ding, and Qian He. Less-to-more generalization: Unlocking more controllability by in-context generation. arXiv preprint arXiv:2504.02160, 2025. 2, 7 [62] Bin Xia, Bohao Peng, Yuechen Zhang, Junjia Huang, Jiyang Liu, Jingyao Li, Haoru Tan, Sitong Wu, Chengyao Wang, Yitong Wang, et al. Dreamomni2: Multimodal arXiv preprint instruction-based editing and generation. arXiv:2510.06679, 2025. 3, 5, 6, 7, 9, 10 [63] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generaIn Proceedings of the Computer Vision and Pattern tion. Recognition Conference, pages 1329413304, 2025. 7 [64] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. 3 [65] Jinheng Xie, Zhenheng Yang, and Mike Zheng Shou. ShowarXiv Improved native unified multimodal models. o2: preprint arXiv:2506.15564, 2025. [66] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for textto-image generation. In NeurIPS, 2023. 3, 6 [67] Zeyue Xue, Jie Wu, Yu Gao, Fangyuan Kong, Lingting Zhu, Mengzhao Chen, Zhiheng Liu, Wei Liu, Qiushan Guo, Weilin Huang, et al. Dancegrpo: Unleashing grpo on visual generation. arXiv preprint arXiv:2505.07818, 2025. 3, 4, 5 [68] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 5 [69] Junyan Ye, Dongzhi Jiang, Zihao Wang, Leqi Zhu, Zhenghao Hu, Zilong Huang, Jun He, Zhiyuan Yan, Jinghua Yu, Hongsheng Li, et al. Echo-4o: Harnessing the power of gpt-"
        }
    ],
    "affiliations": [
        "Hunyuan, Tencent",
        "IIE, CAS",
        "UCAS"
    ]
}