{
    "paper_title": "Switti: Designing Scale-Wise Transformers for Text-to-Image Synthesis",
    "authors": [
        "Anton Voronov",
        "Denis Kuznedelev",
        "Mikhail Khoroshikh",
        "Valentin Khrulkov",
        "Dmitry Baranchuk"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This work presents Switti, a scale-wise transformer for text-to-image generation. Starting from existing next-scale prediction AR models, we first explore them for T2I generation and propose architectural modifications to improve their convergence and overall performance. We then observe that self-attention maps of our pretrained scale-wise AR model exhibit weak dependence on preceding scales. Based on this insight, we propose a non-AR counterpart facilitating ${\\sim}11\\%$ faster sampling and lower memory usage while also achieving slightly better generation quality.Furthermore, we reveal that classifier-free guidance at high-resolution scales is often unnecessary and can even degrade performance. %may be not only unnecessary but potentially detrimental. By disabling guidance at these scales, we achieve an additional sampling acceleration of ${\\sim}20\\%$ and improve the generation of fine-grained details. Extensive human preference studies and automated evaluations show that Switti outperforms existing T2I AR models and competes with state-of-the-art T2I diffusion models while being up to $7{\\times}$ faster."
        },
        {
            "title": "Start",
            "content": "SWITTI: Designing Scale-Wise Transformers for Text-to-Image Synthesis Anton Voronov1,2,3 Denis Kuznedelev1,4 Mikhail Khoroshikh1 Valentin Khrulkov1 Dmitry Baranchuk 1Yandex Research 2HSE University 3MIPT 4Skoltech https://yandex-research.github.io/switti 4 2 0 2 ] . [ 1 9 1 8 1 0 . 2 1 4 2 : r Figure 1. SWITTI produces high quality and aesthetic 512512 image samples in around 0.13 seconds."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction This work presents SWITTI, scale-wise transformer for text-to-image generation. Starting from existing next-scale prediction AR models, we first explore them for T2I generation and propose architectural modifications to improve their convergence and overall performance. We then observe that self-attention maps of our pretrained scale-wise AR model exhibit weak dependence on preceding scales. Based on this insight, we propose non-AR counterpart facilitating 11% faster sampling and lower memory usage while also achieving slightly better generation quality. Furthermore, we reveal that classifier-free guidance at high-resolution scales is often unnecessary and can even degrade performance. By disabling guidance at these scales, we achieve an additional sampling acceleration of 20% and improve the generation of fine-grained details. Extensive human preference studies and automated evaluations show that SWITTI outperforms existing T2I AR models and competes with state-of-the-art T2I diffusion models while being up to 7 faster. Diffusion models (DMs) [26, 29, 30, 6466] are dominating paradigm in visual content generation and have achieved remarkable performance in text conditional image [4, 15, 39, 51], video [5, 52] and 3D modeling [17, 48]. Inspired by the unprecedented success of autoregressive (AR) models in natural language generation [13, 71, 73], numerous studies have focused on developing AR models specifically for visual content generation [14, 16, 38, 41, 68, 72, 87] to offer more practical solution to the generative trilemma [80]. Traditional visual AR generative models perform nexttoken prediction [3, 14, 38, 44, 44, 68, 70, 78]. These models flatten 2D image into 1D token sequence, and causal transformer then predicts each token sequentially, resembling the text generation pipeline [13, 53, 54, 73]. While this direction aims to unify vision and language modeling within single AR framework, it still does not reach stateof-the-art diffusion models in terms of both speed and visual generation quality. 1 This discrepancy raises an important question: why do traditional AR models struggle in vision domains, whereas diffusion models excel Tian et al. [72] and Chang et al. [6] argue that next-token prediction imposes an unsuitable inductive bias for visual content modeling. In contrast, diffusion models generate images in coarse-to-fine manner [2, 11], process that closely resembles human perception and drawing starting with global structure and gradually adding details. Dieleman [11] shows that diffusion models approximate spectral autoregression, progressively generating higher-frequency image components at each diffusion step. Recently, scale-wise AR modeling has emerged as natural and highly effective image generation solution via next-scale prediction paradigm [47, 69, 72, 90]. Unlike nexttoken prediction or masked image modeling [6, 7, 16, 40, 41], scale-wise models start with single pixel and progressively predict higher resolution versions of the image, while attending to the previously generated scales. Roughly speaking, the model adds finer details at each prediction step, exhibiting similar behavior to diffusion models. This makes scale-wise AR models highly promising direction in visual generative modeling. An important advantage of scale-wise models over DMs is that they perform first steps in lower resolutions, while diffusion models always operate in the fixed target resolution during the entire sampling process. Therefore, scale-wise models yield significantly faster sampling while having the potential to provide similar generation performance to DMs. This work advances the family of scale-wise image models by introducing novel transformer architecture for largescale text-to-image generation. We begin by implementing next-scale prediction AR models for text-to-image generation, drawing on recent developments [47, 72]. However, the basic architecture encountered instabilities during training, resulting in suboptimal convergence. To address these issues, we introduce several architectural modifications to the transformer backbone, resulting in strong next-scale prediction AR model for text-to-image synthesis. Then, we investigate whether the scale-wise AR models truly require attending to all previous scales. Our intuition suggests that an input image at the current resolution contains sufficient information about the preceding scales to accurately predict the next one. By examining the self-attention maps in the pretrained scale-wise AR model, we confirm this hypothesis and find that attention layers predominantly focus on the current input, indicating minimal dependence on preceding levels. Based on this insight, we propose removing the autoregressive component from scale-wise models, resulting in Scale-wise transformer for text-to-image synthesis (SWITTI). This approach achieves slightly better generative performance than its AR counterpart, while enabling faster inference and higher scaling potential. Additionally, we explore the influence of text conditioning across different resolution scales, observing that higher scales show minimal reliance on textual information. Leveraging this insight, we disable classifier-free guidance (CFG) [25] at the last scales, thereby reducing inference time by skipping extra forward passes required for CFG calculation. Interestingly, this not only accelerates sampling but also sometimes mitigates generation artifacts. To sum up, the paper presents the following contributions: We introduce SWITTI, text-to-image next-scale prediction transformer that employs architectural modifications improving training stability and convergence and excludes explicit autoregression for more efficient sampling and better scalability. As evidenced by human preference studies and automated evaluation, SWITTI outperforms previous publicly available visual AR models. Compared to stateof-the-art text-to-image diffusion models, SWITTI is up to 7 faster while maintaining competitive performance. We investigate self-attention maps of our pretrained scalewise AR model at different resolution scales and find that most of the attention density is concentrated at the current scales. As result, we propose the model without the AR component which is 11% more efficient for 512512 image generation due to cheaper attention operations. Also, SWITTI reduces memory consumption during inference, previously needed for storing key-value (KV) cache, and hence enables better scaling to higher resolution image generation. Interestingly, SWITTI also slightly surpasses its AR counterpart in generation quality under the same training setups. We find that SWITTI has weaker reliance on the text at high resolution scales. This observation allows us to disable classifier-free guidance at the last two steps, resulting in further 20% acceleration and better generation of finegrained details, as confirmed by human evaluation. 2. Related work 2.1. Text-to-image diffusion models Text-conditional diffusion models (DMs) [3, 4, 15, 51, 56, 59, 92] have become the de facto solution for text-to-image (T2I) generation. Despite their impressive performance, well-known limitation of DMs is slow sequential inference, which hampers real-time or large-scale generation tasks. Most publicly available state-of-the-art T2I diffusion models [4, 15, 39, 51, 58, 92] operate in the VAE [33] latent space, allowing for more efficient sampling of highresolution images. However, these models still require 2050 diffusion steps in the latent space. Diffusion distillation methods [32, 46, 49, 60, 61, 67, 84, 85] are the most promising direction for reducing the number of diffusion steps to just 24. Current state-of-the-art approaches, such as DMD2 [84] and ADD [60], demonstrate strong generation performance in 4 steps and may even surpass the teacher performance in terms of image quality thanks to additional adversarial training on real images. 2 2.2. Visual autoregressive modeling Autoregressive (AR) models is promising alternative paradigm for image generation that can be categorized into three main groups: next-token prediction [14, 38, 68, 87], next-scale prediction [47, 69, 72], and masked autoregressive models [16, 41]. Next-token prediction AR models are similar to GPTlike causal transformers [13, 53, 54, 73] and generate an image token by token using some scanning strategy, e.g., raster order (left to right, top to bottom). The tokens are typically obtained using VQ-VAE-based discrete image tokenizers [14, 38, 74, 86]. VQ-VAE maps an image to lowresolution 2D latent space and assigns each latent \"pixel\" to an element in the learned vocabulary. Masked autoregressive image modeling (MAR) [16, 41] extends masked image generative models [6, 7, 40] and predicts multiple masked tokens in random order at single step. Notably, MAR operates with continuous tokens, using diffusion loss for training and lightweight token-wise diffusion model for token sampling. Fluid [16] applies this approach to T2I generation and explores its scaling behavior. introduced by VAR [72], represents an image as sequence of scales of different resolutions. Unlike next-token prediction and masked AR modeling, the scale-wise transformer predicts all tokens at higher resolution in parallel, attending to previously generated lower-resolution scales. Next-scale prediction AR modeling, To represent an image with sequence of scales, VAR [72] uses hierarchical VQ-VAE that maps an image to pyramid of latent variables of different resolutions (scales), progressively constructed using residual quantization (RQ) [38]. In the following, we will refer to this VAE model as RQ-VAE. Each latent variable in RQ-VAE is associated with set of discrete tokens from shared vocabulary across all scales, similar to single-layer VQ-VAE. During sampling, scale-wise AR model θ iteratively predicts image tokens scale-by-scale, formulated as: pθ(s1, . . . , sN c) = (cid:89) i=1 pθ(sis1, . . . , si1, c), where si represents RQ-VAE tokens at the current scale, is the total number of scales, and is the conditioning information. The model is transformer [54] with blockwise causal attention mask, as shown in Figure 5 (Left). VAR [72] adopts transformer architecture from DiT [50]. Recent works have applied next-scale prediction models to T2I generation [47, 69, 90]. STAR [47] uses the pretrained RQ-VAE model from VAR and modifies its generator to effectively handle text conditioning. Although STAR has not been released as of the writing of this paper, we consider STAR as our baseline architecture, from which we gradually progress towards the proposed model, SWITTI. concurrent work, HART [69], proposes lightweight T2I scale-wise AR model with only 0.7B parameters. It mainly addresses the limitations of the discrete RQ-VAE in VAR by introducing an additional diffusion model to model continuous error residuals, resulting in hybrid model: scale-wise AR model combined with diffusion model for refining the reconstructed latents. In contrast, we focus solely on designing the scale-wise generative transformer using the pretrained RQ-VAE from VAR [72], slightly tuning it for 512512 resolution. Combining our scale-wise generative model design with HARTs hybrid tokenization could be promising direction for future work. MAR [41] can also be considered as hybrid model that combines both autoregressive and diffusion priors. Disco-diff [83] conditions diffusion model on discrete tokens produced with next-token prediction transformer. DART [19] introduces AR transformers as backbone for non-markovian diffusion models. Opposed to this line of works, SWITTI does not use any diffusion prior. 3. Method 3.1. Basic architecture As starting point, we design basic text-conditional architecture closely following VAR [72] and STAR [47]. Scalewise AR text-to-image generation pipeline comprises three main components: RQ-VAE [38] as an image tokenizer, pretrained text encoder [55], and scale-wise block-wise causal transformer [72]. Our model adopts the pretrained RQ-VAE from VAR [72], which represents an image with =10 scales. We slightly tune it on 512512 resolution, as discussed in Section 4.3. To ensure strong image-text alignment of the resulting model, we follow the literature in T2I diffusion modeling [15, 51] and employ two text encoders: CLIP ViT-L [55] and OpenCLIP ViT-bigG [27]. The text embeddings extracted from each model are concatenated along the channel axis. The basic transformer architecture is adopted from VAR [72], where we incorporate cross-attention [75] layers between self-attention layer and feed-forward network (FFN) in each transformer block. pooled embedding from OpenCLIP ViT-bigG is propagated to the transformer blocks via Adaptive Layer Normalization (AdaLN) [81]. We also incorporate conditioning on the cropping parameters following SDXL [51] to mitigate unintended object cropping in generated images. Specifically, we transform center crop coordinates ctop, clef into Fourier feature embeddings and concatenate them. Then, we map the obtained vector to the hidden size of OpenCLIP ViT-bigG via linear layer and add it to the pooled embedding. Layer-normalization (LN) layers [1] are applied to the inputs of attention and FFN blocks. RMSNorm [89] layers are used for query-key (QK) normalization. We also use normalized 2D rotary positional embeddings (RoPE) [22, 47], 3 Figure 3. Last transformer block activation norms over training. Figure 4. Evaluation metrics over training for various architectures. is to cast the model head to FP32 during training. We find this as critical technical detail that significantly reduces activation norms, resulting in much better convergence, as we show in Figure 4 (Orange). However, this trick does not fully address the problem since activation norms still keep growing and reach high values of 104 by the end of training. To further reduce the growth of activation norms during training, we employ sandwich-like normalizations [12, 92], to keep the activation norms in reasonable range. Specifically, we insert additional normalization layers right after each attention and feed-forward blocks and replace LN layers with RMSNorm for efficiency. As we show in Figure 3, this modification further mitigates the growth of activation norms during training, resulting in slightly better model performance, as evidenced in Figure 4 (Green). Finally, following standard practices in language transformers [13, 73], we replace the GELU [21] activation in the FFN blocks with SwiGLU [63] activation that allows the model to dynamically control the information flow via learnable gating mechanism. Although SwiGLU is prevalent choice in LLMs, we notice that it provides negligible effect on the final performance. Overall, we illustrate the transformer block of the described architecture in Figure 2 and denote the scale-wise AR model with the proposed architecture as SWITTI (AR). 3.3. Exploring self-attention maps Below, we analyze attention maps across different scales of the pretrained SWITTI (AR). Specifically, for each pair of scales si and sj where j, we calculate the average Figure 2. Transformer block in the SWITTI model. which allow faster model adaptation to higher resolutions. The FFN block uses GeLU activation function [21]. The visualization of the basic architecture is in Appendix A. 3.2. Training dynamics of the basic architecture Here, we analyze the training performance of the basic model with d=20 transformer blocks and introduce the modifications improving the model stability and convergence. We train the model in mixed-precision BF16/FP32 for 150K iterations on the 256256 image-text dataset described in Section 4.1. The detailed training and evaluation setups are in Appendix B. During the training, we track activation norms and standard metrics, such as FID [24], CLIPscore [23] and PickScore [34]. First, we observe stability issues during training, leading to eventual divergence in our large scale experiments or suboptimal performance. Our investigation reveals that the root of this issue lies in the rapid growth of transformer activation norms, as illustrated in Figure 3 (Blue). Activation norms of the last transformer block grow throughout training iterations, reaching extremely large values of 1016. Therefore, the first step towards stabilizing the training 4 Figure 7. Visualization of cross-attention maps at different scales for random text prompt. Figure 5. Visualization of the block-wise self-attention masks in VAR (Left) and SWITTI (Right). Figure 8. Textual prompt switching during image generation. pattern for randomly selected prompt. The attention scores are primarily concentrated on the first and last tokens at most scales, while the highest scales tend to focus more on the beginning of the prompt. This pattern suggests that the model relies less on the prompt at higher scales. Such behavior is consistent across different prompts. Prompt switching. Then, we investigate the impact of textconditioning at various scales by switching the text prompt to new one starting from certain scale. The visualization of prompt switching is presented in Figure 8, with additional examples provided in Appendix C. Indeed, the prompt has minimal influence on the image semantics at the last two scales. Interestingly, switching prompt at the middle scales results in simple image blending approach. Practical implications. Classifier-free guidance (CFG) [25] is an important sampling technique for high-quality textconditional generation that requires an extra model forward pass at each step. Specifically for the scale-wise models, calculations at the last scales take up most of the computing time of the entire sampling process. To save costly model forward passes in high resolution, we propose disabling CFG at the last scales, expecting little effect on generation performance, as also was recently noted in diffusion models [36]. 4. Model Training 4.1. Pretraining Data. We collect the dataset of 100M image-text pairs that are prefiltered with particular emphasis on relevance from the base set of 6B pairs from the web. We consider only images of sufficiently high aesthetic quality, based on the AADB [35] and TAD66k [20] aesthetic filters. We additionally consider sufficiently high-resolution images with at least Figure 6. Visualization of self-attention maps at different scales for SWITTI (AR). The model attends mostly to the current scale. (cid:80) msi,nsj attention score from si to sj using the formula Asi,sj = 1 Amn. Here, Amn represents the original si self-attention map, and si indicates the number of tokens at scale si. The visualization, averaged over batch of textual prompts, is in Figure 6. It can be seen that self-attention among image tokens primarily focuses on the current scale and is significantly weaker for preceding scales. Based on the attention behavior, we propose to update the attention mask so that self-attention layers only attend to the tokens at the current scale, see Figure 5 (Right). This implies that the transformer is no longer block-wise causal, enabling more efficient sampling due to cheaper attention operations and eliminating the need for key-value (KV) cache. Interestingly, this modification also slightly improves the performance in terms of CLIP score and PickScore, as shown in Figure 4. Overall, we refer to the scale-wise model with the proposed non-causal transformer architecture as SWITTI. 3.4. The role of text conditioning Finally, we examine the effect of text conditioning at different model scales. Specifically, we analyze cross-attention maps and the model behavior when the textual prompt is switched during sampling. Cross-attention. We plot cross-attention map between image and text tokens, averaged across transformer blocks and image tokens at different scales. Figure 7 shows typical 5 512px on each side. The dataset contains central crops of the images with an aspect ratio in [0.75, 1.33]. The images are recaptioned using the LLaVA-v1.4-13B, LLaVA-v1.634B [45], and ShareGPT4V [9] models. The best caption is selected according to OpenCLIP ViT-G/14 [62]. Technical details. Following the transformer scaling setup in VAR [72], we set the number of transformer layers d=30 for our main model, resulting in 2.5B trainable parameters. During the first stage of pretraining, we train the model on 256256 resolution using batch size of 2, 560 for 400K iterations that takes 25K NVIDIA A100 GPU hours. We start with learning rate of 1e4 with linear decay to 1e5. We use FSDP with hybrid strategy for effective multi-host training and mixed precision BF16/FP32. To additionally reduce memory usage and speed up the training steps, we use precomputed textual embeddings from the text encoders. Next, we train on 512512 resolution for 200K iterations using batch size of 768 and learning rate of 1e5, linearly decaying to 5e7. This stage takes another 12K NVIDIA A100 GPU hours. 4.2. Supervised fine-tuning After the pretraining phase, we further fine-tune the model using 40, 000 text-image pairs, inspired by practices in large-scale T2I diffusion models [31, 39, 43]. The pairs are manually selected by assessors instructed to capture exceptionally aesthetic, high quality images with highly relevant and detailed textual descriptions. The model is fine-tuned for 40K iterations with batch size of 80 and learning rate 5e7 on image central crops of resolution 512512. We show the effect of supervised fine-tuning in Appendix D. In addition, we slightly perturb the RQ-VAE latents prior the quantization step with Gaussian noise (σ=0.01) as an augmentation to mitigate overfitting. We observed that the perturbed latents after the quantization and dequantization steps produce almost identical images while resulting in 70% different tokens in sequence. 4.3. RQ-VAE tuning In this work, we use the released RQ-VAE from VAR [72] that was trained on 256256 images and fine-tune its decoder to adapt it for 512512 resolution. The encoder and codebook are kept frozen to preserve the same latent space, allowing to fine-tune the autoencoder independently from the generator. Following the standard practice in superresolution literature [37], we use combination of L1 reconstruction, LPIPS perceptual [91] and adversarial [37] losses, resulting in the following fine-tuning objective: trained for 100K steps with batch size of 256 and constant learning rate of 1e5. To compare the reconstruction quality of the original RQVAE and the one with tuned decoder, we compute classic full-reference metrics PSNR, SSIM [79], LPIPS [91] and no-reference CLIP-IQA [76] metric on held-out dataset of 300 images. Results presented in Table 1 demonstrate that the fine-tuned RQ-VAE outperforms an original model with respect to all metrics. Additionally, we provide several representative comparisons in Appendix E. Model PSNR SSIM LPIPS CLIP-IQA Original Fine-tuned 21.603 22.267 0.634 0.653 0.200 0.188 0.727 0.772 Table 1. Comparison of fine-tuned and an original RQ-VAE. We believe that more pronounced gains can be achieved via more thorough investigation of RQ-VAE and training the entire model. We leave this direction for future work. 5. Experiments 5.1. Automated metrics evaluation Evaluation setting. To comprehensively evaluate the performance of our models, we use combination of established automated metrics and human preference study. For automated metrics, we report CLIPScore [23], ImageReward [82], PickScore [34], FID [24] and GenEval [18]. For all evaluated models, we generate images in their native resolution and resize them to 512512. More details about the evaluation pipeline are in Appendix F. We calculate metrics on two validation datasets frequently used for text-to-image evaluation: MS-COCO [42] and MJHQ [39]. For both datasets, we generate one image for each of 30, 000 validation prompts."
        },
        {
            "title": "We compare our final model with several competitive",
            "content": "text-to-image baselines from various architecture types: Diffusion models: Stable Diffusion XL [51], Stable Diffusion 3 [15], Lumina-Next [92]. Diffusion distillation: SDXL-Turbo [60], DMD2 [84] Autoregressive models: Emu3 [78], Lumina-mGPT [44], LlamaGen-XL [68], HART [69]. Results. Table 2 presents the results of quantitative comparison. SWITTI achieves comparable performance to the baselines, ranking top-3 for 8 out of 9 automated metrics while exhibiting higher efficiency than most competitors. We do not provide automated metrics for Emu3 and LuminamGPT due to their exceptionally long sampling times, that makes generating 60,000 images infeasible in reasonable time. = LL1 + LLPIPS + Ladv (1) 5.2. Human evaluation We adopt UNetSN discriminator from Wang et al. [77] for adversarial loss. The generator and discriminator are While automated metrics are widely adopted for evaluating text-to-image models, we argue that they do not fully reflect 6 Model Latency, s/image Parameters count, Pickscore CLIP IR FID Pickscore CLIP IR FID GenEval COCO 30K eval prompts MJHQ 30K eval prompts SDXL-Turbo [60] DMD2 [84] 0.251 0.251 SDXL [51] SD3 [15] Lumina-Next [92] LlamaGen [68] HART [69] SWITTI (AR) (ours) SWITTI (ours) 0.867 0.934 5.812 3.821 0.063 0. 0.127 2.6 2.6 2.6 2.0 2.0 0.8 0.7 2.5 2.5 Distilled Diffusion Models 0.229 0.231 0.355 0.356 0.83 0.87 17.6 14.3 0.216 0.219 0.365 0. 0.84 0.87 15.7 7.2 Diffusion Models 0.226 0.227 0.224 0.360 0.354 0.329 0.77 1.01 0. 14.4 19.5 18.4 Autoregressive Models 0.208 0.223 0.227 0.227 0.274 0.341 0.354 -0.25 44.8 20.9 0.75 18.1 0. 0.356 0.95 17.6 0.217 0.215 0.216 0.194 0.216 0.217 0. 0.384 0.363 0.353 0.78 0.91 0.75 7.6 13.1 5.9 0.288 0.366 0.378 -0.45 26.9 5.8 0.84 9.5 0.88 0. 0.91 9.5 0.55 0.58 0.55 0.65 0.47 0.32 0.55 0.61 0. Table 2. Quantitative comparison of SWITTI to other competing open-source models. The best model is highlighted in red, the second-best in blue, and the third-best in yellow according to the respective automated metric. Figure 9. Human study comparing SWITTI with competing AR, diffusion-based models. Error bars correspond to 95% confidence interval. all aspects of image generation quality. Therefore, we conduct rigorous human preference study and consider it as our primary evaluation metric. This study compares models across several key aspects: the presence of defects, textual alignment, image complexity and aesthetics quality. Our evaluators are specially trained experts who receive detailed and fine-grained instructions for each evaluation aspect, ensuring consistency and reliability. More details about the human evaluation setup are provided in Appendix G. For human evaluation, we generate two images for each of 128 captions from Parti prompts collection [88], set specifically curated for human preference study [15, 60, 84]. Sampling setup. For side-by-side comparison, we generate images with classifier-free guidance set to 4 and deactivate it at the last two scales, as described in Section 3.4. We follow the original VAR inference implementation and apply Gumbel softmax sampling [28] with decreasing temperature, starting from the third scale. At the first two scales, we use nucleus sampling with top-k=400 and top-p=0.95. Results. We provide the results of side-by-side comparison of SWITTI against the baselines for various image quality aspects in Figure 9. As follows from the human evaluation, SWITTI outperforms all AR baselines in most aspects, only lagging behind the much heavier models, Lumina-mGPT and Emu3, in terms of image complexity. As for diffusion models, DMD2 slightly outperforms SWITTI in terms of defects presence, which can be attributed to its adversarial training, whereas SD3 is better at text alignment, likely due to an additional text encoder. In other comparisons, SWITTI is on par with the diffusion models and their distilled ver7 Figure 10. Qualitative comparison of SWITTI against the baselines. sions, with respect to statistical error. We provide qualitative comparisons in Figures 10 and 14. this resolution to ensure fair comparison. All models are evaluated on single NVIDIA A100 80GB GPU. 5.3. Inference performance evaluation Next, we analyze the efficiency of SWITTIs sampling and compare it to the baselines. We consider two settings: measurement of single generator step without considering time for text encoding and VAE decoding; and inference time of full text-to-image pipeline. All models are evaluated in half-precision with batch size of 8. KV-cache is enabled for all AR models. Since SWITTIs generation resolution is currently limited to 512512, we evaluate all models in As follows from Table 3, SWITTI is by far the most efficient among other image generation models of similar size, being 7 faster than SDXL. Notably, SWITTI takes only 0.06 seconds more than HART to generate batch of images while being more than three times larger. This efficiency stems from the fact that we do not employ an additional diffusion model during de-tokenization in RQ-VAE, from the transitioning to non-causal architecture, and from disabling classifier-free guidance at the latest scales. 8 Model Generator size, steps 1 step, ms/image Full, s/image Distilled Diffusion Models SDXL-Turbo DMD2 2.6 2.6 4 4 SDXL SD Diffusion Models 2.6 2.0 25 28 Autoregressive Models Lumina-mGPT LlamaGen HART SWITTI (AR) 7.0 0.8 0.7 2. SWITTI *Time averaged over 10 steps. 2.5 1024 1024 10 10 10 12.4 12.4 12.4 16. 4.7* 11.2* 9.5* 0.251 0.251 0.867 0.934 224.2 3.821 0.063 0.139 0. Table 3. Comparison of models 512512 image generation time. All models are run in half-precision with batch size of 8. 5.4. Ablation study Finally, we evaluate the effect of our principal architectural choices on the image generation quality and inference time. The human evaluation results in Table 4 demonstrate that the non-AR version of SWITTI is not only more sample efficient than its AR alternative, but also results in slightly better visual quality with respect to all aspects of human evaluation. Moreover, disabling classifier-free guidance at the last two scales noticeably reduces defect presence in SWITTIs synthesized images without affecting other aspects of evaluation, as illustrated in Figure 11. Nevertheless, it should be noted that enabling CFG at the last scales can still be beneficial. For example, in scenarios where prompts contain text to be rendered in small font size, guidance can improve the spelling. Setup 1 Setup 2 Relevance Aesthetics Complexity Defects SWITTI SWITTI AR 0.550.06 0.560.06 0.560.06 0.550.06 SWITTI SWITTI + late CFG 0.500.06 0.510.06 0.500.06 0.580.06 Figure 11. Illustrative examples when disabled CFG at the last scales (Bottom) mitigates the artifacts in fine-grained details (Top)."
        },
        {
            "title": "Model",
            "content": "Generator step, ms/image"
        },
        {
            "title": "Disable\nlate CFG",
            "content": "Full, ms/image SWITTI (AR)"
        },
        {
            "title": "SWITTI",
            "content": "112 95 179 139 161 127 Table 5. Latency comparison of architecture and sampling modifications proposed in SWITTI. hierarchical discrete VAE performance compared to the recent continuous [4, 8, 51, 57] or discrete single-level [57, 68] counterparts. Typical failure cases are distorted middle/long-shot faces, text rendering and checker-board artifacts on high-frequency textures such as distant foliage or rocky surfaces. We hope that future advances in hierarchical image tokenizers, either discrete or continuous, may significantly improve the performance of scale-wise generative models without using an additional diffusion prior [41, 69]. Adapting to higher resolutions. This work uses publicly available RQ-VAE that is not designed for 10241024 resolutions or higher. In future work, we expect to see effective RQ-VAE models for higher-resolution sampling. Table 4. Human evaluation of SWITTI design choices. Scores are the mean of Setup 1 wins plus half-ties, with subscripts denoting half the 95% confidence interval. 7. Conclusion In terms of sampling efficiency, as we show in Table 5, disabling CFG on the last two scales reduces the latency by nearly 20%, whereas transitioning to non-autoregressive architecture further reduces latency by an additional 11%. 6. Limitations and future directions Hierarchical tokenizers. We believe that one of the major limitations of the existing scale-wise models is the inferior This work introduces SWITTI, novel scale-wise generative transformer for text-to-image generation. In contrast to previous next-scale prediction approaches, SWITTI incorporates the revised architecture design for improved convergence, eliminates explicit autoregressive prior for efficient inference and makes use of more effective sampling with guidance. Trained on large-scale curated text-image dataset, SWITTI outperforms prior text-conditional autoregressive models and competes with state-of-the-art T2I diffusion models, such as SDXL and SD3, achieving up to 7 faster sampling."
        },
        {
            "title": "References",
            "content": "[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016. 3 [2] Dmitry Baranchuk, Andrey Voynov, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko. Label-efficient semantic segmentation with diffusion models. In International Conference on Learning Representations, 2022. 2 [3] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Improving image generation with Lee, Yufei Guo, et al. better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. 1, 2 [4] Black Forest Labs. Flux.1. https://huggingface. co/black-forest-labs/FLUX.1-dev, 2024. 1, 2, 9 [5] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. 1 [6] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T. Freeman. Maskgit: Masked generative image transformer. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 2, [7] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William T. Freeman, Michael Rubinstein, Yuanzhen Li, and Dilip Krishnan. Muse: Text-to-image generation via masked generative transformers, 2023. 2, 3 [8] Junyu Chen, Han Cai, Junsong Chen, Enze Xie, Shang Yang, Haotian Tang, Muyang Li, Yao Lu, and Song Han. Deep compression autoencoder for efficient high-resolution diffusion models, 2024. 9 [9] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023. 6 [10] Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention mask transformer for universal image segmentation. 2022. 1 [11] Sander Dieleman. Diffusion is spectral autoregression, 2024. [12] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, and Jie Tang. Cogview: Mastering text-to-image generation via transformers. In Advances in Neural Information Processing Systems, 2021. 4 [13] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 1, 3, 4 [14] Patrick Esser, Robin Rombach, and Björn Ommer. Taming transformers for high-resolution image synthesis, 2020. 1, 3 [15] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for highresolution image synthesis. CoRR, abs/2403.03206, 2024. 1, 2, 3, 6, 7 [16] Lijie Fan, Tianhong Li, Siyang Qin, Yuanzhen Li, Chen Sun, Michael Rubinstein, Deqing Sun, Kaiming He, and Yonglong Tian. Fluid: Scaling autoregressive text-to-image generative models with continuous tokens, 2024. 1, 2, 3 [17] Ruiqi Gao*, Aleksander Holynski*, Philipp Henzler, Arthur Brussee, Ricardo Martin-Brualla, Pratul P. Srinivasan, Jonathan T. Barron, and Ben Poole*. Cat3d: Create anything in 3d with multi-view diffusion models. Advances in Neural Information Processing Systems, 2024. [18] Dhruba Ghosh, Hanna Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-toimage alignment, 2023. 6 [19] Jiatao Gu, Yuyang Wang, Yizhe Zhang, Qihang Zhang, Dinghuai Zhang, Navdeep Jaitly, Josh Susskind, and Shuangfei Zhai. Dart: Denoising autoregressive transformer for scalable text-to-image generation, 2024. 3 [20] Shuai He, Yongchang Zhang, Rui Xie, Dongxiang Jiang, and Anlong Ming. Rethinking image aesthetics assessment: Models, datasets and benchmarks. In IJCAI, pages 942948, 2022. 5 [21] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus), 2023. 4 [22] Byeongho Heo, Song Park, Dongyoon Han, and Sangdoo Yun. Rotary position embedding for vision transformer. In European Conference on Computer Vision (ECCV), 2024. 3 [23] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning, 2022. 4, 6, [24] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 4, 6 [25] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 2, 5 [26] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 1 [27] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip, 2021. If you use this software, please cite it as below. 3 [28] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax, 2017. 7 [29] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in Neural Information Processing Systems, 35:2656526577, 2022. [30] Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine. Analyzing and improving the training dynamics of diffusion models. In Proc. CVPR, 2024. 1 [31] Sergey Kastryulin, Artem Konev, Alexander Shishenya, Eugene Lyapustin, Artem Khurshudov, Alexander Tselousov, 10 Nikita Vinokurov, Denis Kuznedelev, Alexander Markovich, Grigoriy Livshits, Alexey Kirillov, Anastasiia Tabisheva, Liubov Chubarova, Marina Kaminskaia, Alexander Ustyuzhanin, Artemii Shvetsov, Daniil Shlenskii, Valerii Startsev, Dmitrii Kornilov, Mikhail Romanov, Artem Babenko, Sergei Ovcharenko, and Valentin Khrulkov. Yaart: Yet another art rendering technology, 2024. 6 [32] Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Yuhta Takida, Naoki Murata, Toshimitsu Uesaka, Yuki Mitsufuji, and Stefano Ermon. Pagoda: Progressive growing of onestep generator from low-resolution diffusion teacher, 2024. 2 [33] Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014. [34] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. 2023. 4, 6 [35] Shu Kong, Xiaohui Shen, Zhe Lin, Radomir Mech, and Charless Fowlkes. Photo aesthetics ranking network with attributes and content adaptation. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part 14, pages 662679. Springer, 2016. 5 [36] Tuomas Kynkäänniemi, Miika Aittala, Tero Karras, Samuli Laine, Timo Aila, and Jaakko Lehtinen. Applying guidance in limited interval improves sample and distribution quality in diffusion models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 5 [37] Christian Ledig, Lucas Theis, Ferenc Huszár, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photorealistic single image super-resolution using generative adversarial network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 46814690, 2017. 6 [38] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization, 2022. 1, 3 [39] Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground v2.5: Three insights towards enhancing aesthetic quality in text-to-image generation, 2024. 1, 2, 6 [40] Tianhong Li, Huiwen Chang, Shlok Kumar Mishra, Han Zhang, Dina Katabi, and Dilip Krishnan. Mage: Masked generative encoder to unify representation learning and image synthesis, 2023. 2, [41] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization, 2024. 1, 2, 3, 9 [42] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollár. Microsoft coco: Common objects in context, 2015. 6, 1 [43] Bingchen Liu, Ehsan Akhgari, Alexander Visheratin, Aleks Kamko, Linmiao Xu, Shivam Shrirao, Chase Lambert, Joao Souza, Suhail Doshi, and Daiqing Li. Playground v3: Improving text-to-image alignment with deep-fusion large language models, 2024. 6 [44] Dongyang Liu, Shitian Zhao, Le Zhuo, Weifeng Lin, Yu Qiao, Hongsheng Li, and Peng Gao. Lumina-mgpt: Illuminate flexible photorealistic text-to-image generation with multimodal generative pretraining, 2024. 1, 6 [45] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. 6 [46] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Latent consistency models: Synthesizing highZhao. resolution images with few-step inference, 2023. 2 [47] Xiaoxiao Ma, Mohan Zhou, Tao Liang, Yalong Bai, Tiejun Zhao, Huaian Chen, and Yi Jin. Star: Scale-wise text-toimage generation via auto-regressive representations, 2024. 2, 3 [48] Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, Natalia Neverova, Andrea Vedaldi, Oran Gafni, and Filippos Kokkinos. Im-3d: Iterative multiview diffusion and reconstruction for high-quality 3d generation. arXiv preprint arXiv:2402.08682, 2024. 1 [49] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1429714306, 2023. [50] William Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748, 2022. 3 [51] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. In The Twelfth International Conference on Learning Representations, 2024. 1, 2, 3, 6, 9 [52] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary, Dingkang Wang, Geet Sethi, Guan Pang, Haoyu Ma, Ishan Misra, Ji Hou, Jialiang Wang, Kiran Jagadeesh, Kunpeng Li, Luxin Zhang, Mannat Singh, Mary Williamson, Matt Le, Matthew Yu, Mitesh Kumar Singh, Peizhao Zhang, Peter Vajda, Quentin Duval, Rohit Girdhar, Roshan Sumbaly, Sai Saketh Rambhatla, Sam Tsai, Samaneh Azadi, Samyak Datta, Sanyuan Chen, Sean Bell, Sharadh Ramaswamy, Shelly Sheynin, Siddharth Bhattacharya, Simran Motwani, Tao Xu, Tianhe Li, Tingbo Hou, Wei-Ning Hsu, Xi Yin, Xiaoliang Dai, Yaniv Taigman, Yaqiao Luo, Yen-Cheng Liu, Yi-Chiao Wu, Yue Zhao, Yuval Kirstain, Zecheng He, Zijian He, Albert Pumarola, Ali Thabet, Artsiom Sanakoyeu, Arun Mallya, Baishan Guo, Boris Araya, Breena Kerr, Carleigh Wood, Ce Liu, Cen Peng, Dimitry Vengertsev, Edgar Schonfeld, Elliot Blanchard, Felix Juefei-Xu, Fraylie Nord, Jeff Liang, John Hoffman, Jonas Kohler, Kaolin Fire, Karthik Sivakumar, Lawrence Chen, Licheng Yu, Luya Gao, Markos Georgopoulos, Rashel Moritz, Sara K. Sampson, Shikai Li, Simone Parmeggiani, Steve Fine, Tara Fowler, Vladan Petrovic, and Yuming Du. Movie gen: cast of media foundation models, 2024. 1 [53] Alec Radford and Karthik Narasimhan. Improving language understanding by generative pre-training. 2018. 1, 3 11 [54] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. 1, 3 [55] Alec Radford, Jong Wook Kim, Chris Hallacy, A. Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021. [56] Recraft AI. Recraft - ai. https://www.recraft.ai/ about, Accessed 2024. 2 [57] Fitsum Reda, Jinwei Gu, Xian Liu, Songwei Ge, TingChun Wang, Haoxiang Wang, and Ming-Yu Liu. Cosmos tokenizer: suite of image and video neural tokenizers. http://https://research.nvidia.com/labs/ dir/cosmos-tokenizer/, 2024. 9 [58] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models, 2021. 2 [59] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Raphael Gontijo-Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. In Advances in Neural Information Processing Systems, 2022. 2 [60] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation, 2023. 2, 6, 7 [61] Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattmann, Patrick Esser, and Robin Rombach. Fast highresolution image synthesis with latent adversarial diffusion distillation, 2024. 2 [62] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5b: An open large-scale dataset for training next generation image-text models. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. 6, 1 [63] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. 4 [64] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. [65] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. [66] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 1 [67] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. arXiv preprint arXiv:2303.01469, 2023. 2 [68] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. 1, 3, 6, 9 [69] Haotian Tang, Yecheng Wu, Shang Yang, Enze Xie, Junsong Chen, Junyu Chen, Zhuoyang Zhang, Han Cai, Yao Lu, and Song Han. Hart: Efficient visual generation with hybrid autoregressive transformer. arXiv preprint, 2024. 2, 3, 6, 9 [70] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models, 2024. 1 [71] Gemini Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, 2024. 1 [72] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. 2024. 1, 2, 3, 6 [73] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023. 1, 3, 4 [74] Aaron van den Oord, Oriol Vinyals, and koray kavukcuoglu. Neural discrete representation learning. In Advances in Neural Information Processing Systems. Curran Associates, Inc., 2017. 3 [75] Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. [76] Jianyi Wang, Kelvin CK Chan, and Chen Change Loy. Exploring clip for assessing the look and feel of images. In AAAI, 2023. 6 [77] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan. Real-esrgan: Training real-world blind super-resolution with pure synthetic data, 2021. 6 [78] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. 1, 6 [79] Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4): 600612, 2004. 6 [80] Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling the generative learning trilemma with denoising diffusion GANs. In International Conference on Learning Representations, 2022. 1 [81] Jingjing Xu, Xu Sun, Zhiyuan Zhang, Guangxiang Zhao, and Junyang Lin. Understanding and improving layer normalization, 2019. [82] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. 6 [83] Yilun Xu, Gabriele Corso, Tommi Jaakkola, Arash Vahdat, and Karsten Kreis. Disco-diff: Enhancing continuous diffusion models with discrete latents. In International Conference on Machine Learning, 2024. 3 [84] Tianwei Yin, Michaël Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and William Freeman. Improved distribution matching distillation for fast image synthesis. In NeurIPS, 2024. 2, 6, 7 12 [85] Tianwei Yin, Michaël Gharbi, Richard Zhang, Eli Shechtman, Frédo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In CVPR, 2024. 2 [86] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved VQGAN. In International Conference on Learning Representations, 2022. [87] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich text-toimage generation, 2022. 1, 3 [88] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich text-toimage generation, 2022. 7 [89] Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019. 3 [90] Qian Zhang, Xiangzi Dai, Ninghua Yang, Xiang An, Ziyong Feng, and Xingyu Ren. Var-clip: Text-to-image generator with visual auto-regressive modeling, 2024. 2, 3 [91] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. 6 [92] Le Zhuo, Ruoyi Du, Xiao Han, Yangguang Li, Dongyang Liu, Rongjie Huang, Wenze Liu, et al. Lumina-next: Making lumina-t2x stronger and faster with next-dit. arXiv preprint arXiv:2406.18583, 2024. 2, 4, 13 SWITTI: Designing Scale-Wise Transformers for Text-to-Image Synthesis Supplementary Material Setup 1 Setup 2 Relevance Aesthetics Complexity Defects SFT no-SFT 0.610.06 0.660.06 0.720.06 0.720.06 Table 6. Human evaluation of SWITTI against its no-SFT version. C. Additional prompt switching visualizations In Figures 16 and 17, we provide additional examples of the prompt switching analysis, discussed in Section 3.4. D. Effect of supervised fine-tuning To evaluate to which extent supervised fine-tuning affects SWITTI, we conduct human evaluation study. As illustrated in Table 6, SFT largely improves the generation quality in terms of all aspects. E. Visual comparison between original and finetuned RQ-VAE To illustrate the difference between the original RQ-VAE checkpoint and fine-tuned version, we depict several representative examples in Figure 13. One can observe that the fine-tuned VAE decoder is less prone to reconstruction artifacts and color shifts and produces more contrast images. F. Evaluation details We compute the CLIP Score [23], using features from pretrained CLIP-ViT-H-14-laion2B-s32B-b79K encoder [62], to assess image-text alignment. FID is measured on 30K generated images reduced to 512512 resolution using bicubic interpolation. Then, the resolution is further reduced to 256256 using Lanczos interpolation following the practices in FID calculation on COCO2014. Real data statistics are collected for all images in the validation sets: 40K images in COCO2014 and 30, 000 images in MJHQ. For GenEval, we generate 4 images for each of 533 evaluation prompts, followed by an original evaluation protocol using Mask2Former [10] with Swin-S backbone as object detector. G. Human evaluation setup The evaluation is performed using Side-by-Side (SbS) comparisons, i.e., the assessors are asked to make decision between two images given textual prompt. For each evaluated pair, three responses are collected and the final prediction is determined by majority voting. The human evaluation is performed by professional assessors. They are officially hired, paid competitive salaries, and informed about potential risks. The assessors have received Figure 12. Transformer block of the basic architecture. A. Basic architecture Figure 12 illustrates the design of our basic transformer architecture described in Section 3.1. Note that the normalization layers are applied only to the inputs of the attention and FFN blocks. B. Training details in analysis For the experiments in Section 3.2, we train more lightweight models, with d=20 transformer blocks, resulting in approximately 0.7B parameters. All models are trained in mixed precision BF16/FP32 for 150, 000 iterations using Adam (β1=0.9, β2=0.95) with learning rate of 1e4, linearly decaying to 1e5. Batch size is 768. Image resolution is 256256. In the normalized RoPE, we use θ=10, 000 and max size 128. For these experiments, we disable the conditioning on the cropping parameters. For evaluation, we use 30, 000 prompts from the COCO2014 validation set [42]. 1 detailed and fine-grained instructions for each evaluation aspect and passed training and testing before accessing the main tasks. In our human preference study, we compare the models in terms of four aspects: relevance to textual prompt, presence of defects, image aesthetics, and complexity. Figures 18 to 21 present the interface for each of these criteria. Note that the selected answers on the images are random. H. Visual comparison against T2I models We provide qualitative comparison of SWITTI against the baselines considered in this work in Figure 14 and Figure 10. I. Effect of disabling CFG at different scales In Figure 15, we provide some examples of disabling CFG at various level ranges. One can observe that presence of CFG at first scales improves image quality and relevance. At the same time, It can be turned off at the last scales without noticeable quality degradation or loss of details. J. List of prompts used in our figures Figure 1 prompts 1) Cute winter dragon baby, kawaii, Pixar, ultra detailed, glacial background, extremely realistic. 2) Cat as wizard 3) An ancient ruined archway on the moon, fantasy, ruins of an alien civilization, concept art, blue sky, reflection in water pool, large white planet rising behind it 4) lizard that looks very much like man, with developed muscles, leather armor with metal elements, in the hands of large trident decorated with ancient runes, against the background of small lake, everything is well drawn in the style of fantasy 5) The Mandalorian by masamune shirow, fighting stance, in the snow, cinematic lighting, intricate detail, character design 6) Phoenix woman brown skin asian eyes silver scales, full body, high detail 7) Portrait of an alien family from the 1970s, futuristic clothes, absurd alien helmet, straight line, surreal, strange, absurd, photorealistic, Hasselblad, Kodak, portra 800, 35mm lens, 2.8, photo studio. 8) 32 bit pixelated future Hiphop producer in glowing power street ware, noriyoshi ohrai, in the style of minecraft tomer hanuka. Figure 11 prompts 1) beautiful, pleasant, cute, charming young man in futuristic suit with elements of white and green, powerful eco-friendly motif, against the background of beautiful landscape design with an abundance of green vegetation and water, harmony of color, shape and semantic content, light positive emotional coloring, random angle, high detail, photorealism, highly artistic image. 2 2) squirrell driving toy car. 3) Baby Yoda Walking in Manhattan. Figure 15 prompts 1) young badger delicately sniffing yellow rose, richly textured oil painting. 2) 3D,game background,beach,shell, conch,starfish,the sea is far away, cloudy day. Figure 13. Visual comparison between original RQ-VAE (left) and with fine-tuned decoder (right). Figure 14. Qualitative comparison of SWITTI against the baselines. 3 Figure 15. Impact of CFG at different resolution levels. There are 10 levels: higher index indicates higher resolution. The guidance scale is 6. 4 Figure 16. Impact of the prompt switching during image generation. Figure 17. Impact of the prompt switching during image generation. Figure 18. Human evaluation interface for aesthetics. Figure 20. Human evaluation interface for relevance. Figure 21. Human evaluation interface for image complexity. Figure 19. Human evaluation interface for defects."
        }
    ],
    "affiliations": [
        "HSE University",
        "MIPT",
        "Skoltech",
        "Yandex Research"
    ]
}