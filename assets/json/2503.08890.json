{
    "paper_title": "PlainQAFact: Automatic Factuality Evaluation Metric for Biomedical Plain Language Summaries Generation",
    "authors": [
        "Zhiwen You",
        "Yue Guo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Hallucinated outputs from language models pose risks in the medical domain, especially for lay audiences making health-related decisions. Existing factuality evaluation methods, such as entailment- and question-answering-based (QA), struggle with plain language summary (PLS) generation due to elaborative explanation phenomenon, which introduces external content (e.g., definitions, background, examples) absent from the source document to enhance comprehension. To address this, we introduce PlainQAFact, a framework trained on a fine-grained, human-annotated dataset PlainFact, to evaluate the factuality of both source-simplified and elaboratively explained sentences. PlainQAFact first classifies factuality type and then assesses factuality using a retrieval-augmented QA-based scoring method. Our approach is lightweight and computationally efficient. Empirical results show that existing factuality metrics fail to effectively evaluate factuality in PLS, especially for elaborative explanations, whereas PlainQAFact achieves state-of-the-art performance. We further analyze its effectiveness across external knowledge sources, answer extraction strategies, overlap measures, and document granularity levels, refining its overall factuality assessment."
        },
        {
            "title": "Start",
            "content": "PLAINQAFACT: Automatic Factuality Evaluation Metric for Biomedical Plain Language Summaries Generation Zhiwen You Yue Guo University of Illinois Urbana-Champaign {zhiweny2, yueg}@illinois.edu 5 2 0 2 1 1 ] . [ 1 0 9 8 8 0 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Hallucinated outputs from language models pose risks in the medical domain, especially for lay audiences making health-related decisions. Existing factuality evaluation methods, such as entailmentand question-answering-based (QA), struggle with plain language summary (PLS) generation due to elaborative explanation phenomenon, which introduces external content (e.g., definitions, background, examples) absent from the source document to enhance comprehension. To address this, we introduce PLAINQAFACT, framework trained on fine-grained, human-annotated dataset PLAINFACT, to evaluate the factuality of both source-simplified and elaboratively explained sentences. PLAINQAFACT first classifies factuality type and then assesses factuality using retrieval-augmented QA-based scoring method. Our approach is lightweight and computationally efficient. Empirical results show that existing factuality metrics fail to effectively evaluate factuality in PLS, especially for elaborative explanations, whereas PLAINQAFACT achieves state-of-the-art performance. We further analyze its effectiveness across external knowledge sources, answer extraction strategies, overlap measures, and document granularity levels, refining its overall factuality assessment1."
        },
        {
            "title": "Introduction",
            "content": "Communicating biomedical scientific knowledge in plain language is essential for improving health information accessibility (Stoll et al., 2022; Kuehne and Olden, 2015). While large language models (LLMs) have made significant progress in generating plain language summaries (PLSs) (Ondov et al., 2022; Guo et al., 2023; Goldsack et al., 2023; You et al., 2024b), the issue of hallucination remains unresolved (Cao et al., 2022; Guo et al., 2024b). 1PLAINQAFACT and PLAINFACT are available at: https: //github.com/zhiwenyou103/PlainQAFact Factuality evaluation for PLS is particularly challenging due to the presence of elaborative explanations, where external content such as definitions, background information, and examples is added to enhance comprehension but is absent from the original scientific abstract (source) (Guo et al., 2021; Srikanth and Li, 2021; Joseph et al., 2024). This creates two key issues. First, commonly used factuality metrics can effectively verify content supported by the source but fail to assess factual consistency of added information (Guo et al., 2024a). Second, existing datasets for evaluating PLS factuality are limited in both data and annotation quality. FactPICO (Joseph et al., 2024), for instance, focuses on LLM-generated summaries and annotates only newly added information, overlooking potential hallucinations from source simplifications. This limitation prevents comprehensive evaluation of factual consistency across the entire PLS. To address theses challenges, we hypothesize that improving factuality evaluation in elaborative explanations enhances overall PLS factuality assessment. To test this, we introduce the first granular factuality dataset for PLS, with expert annotations for factuality type (elaborative explanation vs. source simplification), functional role (e.g., definition, background, example), and sentencealignment between human-authored PLS and corresponding scientific abstracts. These fine-grained annotations establish benchmark for advancing PLS generation and factuality evaluation methods. Building on this, we propose dual-stage QAbased factuality evaluation framework PLAINQAFACT, that independently scores each PLS sentence (see Figure 1). First, pre-trained classifier predicts whether PLS sentence is source simplification or an elaborative explanation. For elaborative explanations, dense retrieval via MedCPT (Jin et al., 2023) retrieves relevant external information, for verification beyond the source abstract. The abstract and retrieved content are then concatenated Figure 1: Illustration of our PLAINQAFACT framework. The inputs are PLSs and scientific abstracts. Our evaluation framework consists of pre-trained classifier for factuality type categorization, retrieval of external domain knowledge for elaborative explanation sentences, and local LLM-based question-answering system. as references for QA-based factuality evaluation. For source simplifications, the abstract alone serves as the reference for evaluation. Finally, sentencelevel evaluation scores are averaged to produce final factuality score for the PLS. Experiments on PLAINFACT and the CELLS dataset (Guo et al., 2024b) demonstrate that PLAINQAFACT significantly enhances PLS factuality assessment, especially for elaborative explanations. By enabling more accurate factuality assessment in PLS, our approach enhances robustness and supports foundation for enhancing open-ended PLS generation. Our main contributions are as follows: PLAINFACT: the first expert-annotated benchmark for granular factuality evaluation in PLS, covering factuality type, functional role, and sentence alignment (3). PLAINQAFACT: factuality evaluation framework integrates sentence type classification with retrieval-augmented QA, leveraging expertdomain resources (4). PLAINQAFACT outperforms existing factuality metrics in PLS evaluation, particularly in handling elaborative explanations (5, 6)."
        },
        {
            "title": "2 Related Work",
            "content": "Limitations of Existing Factuality Evaluation The primary approach for evaluating PLS generation combines automated metrics with human evaluation (Jain et al., 2021; Ondov et al., 2022). While human assessment provides thorough analysis (Hardy et al., 2019), its high cost and time demands make it impractical for large-scale datasets. Evaluating factuality in biomedical PLS is particularly challenging, as it requires domain expertise. Entailment- (Lee et al., 2022; Laban et al., 2022), similarity- (Wan and Bansal, 2022; Ye et al., 2024), model- (Zha et al., 2023) and QA-based (Deutsch et al., 2021; Fabbri et al., 2022b) metrics are commonly used for factuality assessment but rely heavily on high-quality reference summaries, which are often unavailable or difficult to obtain for PLS. Recent advancements in prompt-based evaluation show promise (Luo et al., 2023); however, their sensitivity to factuality perturbations in PLS remains limited (Guo et al., 2024a). To address these limitations, we propose reference-free solution for factuality evaluation of PLS that effectively assesses factual consistency with external information retrieval to augment the reference summary. Retrieval-Augmented Generation Retrievalaugmented methods enhance text generation by extracting relevant information from external sources to supplement input queries (Lewis et al., 2020). These methods have been shown to be effective in open-domain QA (Ren et al., 2023; Mao et al., 2020), knowledge-based QA (Kim, 2025), and multi-step reasoning (Gao et al., 2023; Tang and Yang, 2024). In the context of PLS, retrieval from structured knowledge bases (KBs) has been shown to improve factual accuracy compared to language models alone (Guo et al., 2024b). However, retrieval-augmented approaches have not been extensively explored for factuality evaluation in PLS, despite their potential for addressing elaborative explanations. In this work, we investigate retrieval-augmented QA to enhance PLS factuality assessment while also examining its limitations."
        },
        {
            "title": "3 PLAINFACT Benchmark",
            "content": "To develop high-quality PLS factuality benchmark, we select subset of the largest humanauthored PLS dataset (3.1) and hire domain experts to provide fine-grained annotations (3.2)."
        },
        {
            "title": "3.1 Human-Authored PLS Dataset",
            "content": "Rather than relying on LLM-generated PLSs, we construct our benchmark using human-authored summaries. CELLS (Guo et al., 2024b), the largest parallel corpus of scientific abstracts and their corresponding PLSs, is written by the original authors and sourced from 12 biomedical journals. We primarily select data from the Cochrane Database of Systematic Reviews (CDSR)2 within CELLS, as CDSR contains systematic reviews that support evidence-based medical decision-making across healthcare domains (Murad et al., 2016). Since systematic reviews represent the highest level of scientific evidence, this selection enhances the factual rigor of our dataset. To ensure readability, we filter the 200 most readable PLSs based on average scores from three standard readability metrics: Flesch-Kincaid Grade Level (FKGL) (Kincaid, 1975), Dale-Chall Readability Score (DCRS) (Dale and Chall, 1948), and Coleman-Liau Index (CLI) (Coleman, 1975)."
        },
        {
            "title": "3.2 Expert Annotation",
            "content": "Since each PLS-abstract pair is authored by the same individual, we assume all information to be 2http:///www.cochranelibrary.com Explanation Simplification # of Sentences Average Length (token) Vocabulary Size Has Reference # of Background # of Definition # of Method/Result # of Example # of Other Chall Coleman-Liau 1,213 29 4,230 417 533 82 512 10 76 11.3 13.5 1,527 28 4,046 1,527 329 44 1,107 3 44 11.6 13.9 Table 1: Overview of the PLAINFACT benchmark. Medical experts annotated 200 pairs of PLSs and their corresponding scientific abstracts from the CELLS dataset, categorizing each PLS sentence as either source simplification or an elaborative explanation related to the abstract. factual. The annotation aims to capture how PLS sentences relate to their scientific abstracts. Annotators analyzed each PLS sentence across three dimensions: (1) Factuality type: identify whether sentence is source simplification (derived from the abstract) or an elaborative explanation (introducing new content); (2) Functional role: categorize the sentence as background, definition, example, method/result, or other; and (3) Sentence alignment: map each PLS sentence to its corresponding sentence(s) in the scientific abstract. Details of annotation guidelines are in App. A. Annotations were conducted by four independent annotators, each with at least bachelors degree in biomedical sciences and prior fact-checking experience. Annotators were recruited via Upwork and compensated from $15 to $20 per hour. Each PLS-abstract pair was annotated by two independent annotators, with disagreements resolved by third. Inter-rater agreement, measured by Cohens Kappa for factuality type, functional role, and sentence alignment are 0.43, 0.60, and 0.55 respectively, indicated moderate agreement for all tasks (Artstein and Poesio, 2008). Table 1 summarizes the dataset characteristics. Notably, 44% of PLS sentences contain elaborative explanations, highlighting their role in enhancing PLS readability. This underscores the need for factuality evaluation methods that account for such phenomena. Moreover, elaborative explanations include more background, definitions, and examples than source simplifications. For annotation examples, see App. B."
        },
        {
            "title": "4 PLAINQAFACT Framework",
            "content": "PLAINQAFACT conducts fine-grained factuality evaluation for PLS by assessing each sentence individually and retrieving tailored external knowledge within retrieval-augmented QA framework. Figure 1 provides an overview of its three key components: sentence-level classification (4.1), domain knowledge retrieval (4.2), and dual-stage QA-based factuality evaluation (4.3)."
        },
        {
            "title": "4.1 Learned Factuality Type Classifier",
            "content": "As elaborative explanations are prevalent in PLS generation and existing metrics struggle to capture added information (Guo et al., 2024a), we fine-tune classifier to categorize factuality types. Using factuality type annotations (source simplification vs. elaborative explanation) in PLAINFACT, we finetune classifier with PubMedBERT-base model3. The dataset is split 8:1:1 for training, validation, and testing. The classifier is fine-tuned with early stopping, stopping at the third epoch based on validation performance. Additionally, we compare the learned classifier with zero-shot GPT-4o (OpenAI, 2024) classifier (details in App. C) to explore extending our factuality evaluation framework to domains and tasks lacking human-annotated data."
        },
        {
            "title": "4.2 Domain Knowledge Retrieval",
            "content": "Retrieval-augmented methods have proven effective for explanation generation (Guo et al., 2024b; Xu et al., 2024), making them natural fit for evaluating elaborative explanations. Since our dataset is in the medical domain, we adopt MedCPT (Jin et al., 2023), retriever pretrained on large-scale PubMed search logs to generate biomedical text embeddings. For external resources, we incorporate StatPearls (Jin et al., 2021) for clinical decision support and medical textbooks (Xiong et al., 2024) for domain-specific knowledge."
        },
        {
            "title": "4.3 QA Evaluation Components",
            "content": "QA-based metrics have outperformed other factuality metrics (Guo et al., 2024a) in PLS and align more closely with human annotations (Fabbri et al., 2022a; Deutsch et al., 2021). Therefore, we adopt QA-based approach in our framework to verify factuality, incorporating answer extraction, question generation, question filtering, question answering, and answer overlap calculation. 3https://huggingface.co/microsoft/ BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext Gold Answer Extraction The first step in QAbased factuality evaluation is to extract answer entities (keyphrases) from the PLS as gold answer, then verify factuality by comparing them with answers generated by QA model for the same questions. If the generated answer is correct or relevant, the summary is considered factual. To compare answer extraction strategies, we use noun phrase chunks (NP Chunks) as the baseline, and evaluate LLM-based extractors, including Llama 3.1 8B Instruct (Llama 3.1)(Meta, 2024), GPT-4o (OpenAI, 2024), and PromptRank (Kong et al., 2023). Prompts for Llama and GPT-4o are in App. C. PromptRank is keyphrase generation method based on the T5 model. The number of extracted entities is set to 50 and the T5-large is used for better performance. Question Generation (QG) Given an input document (PLS), the QG model generates questions based on the extracted answers and their context. Following previous studies (Deutsch et al., 2021; Fabbri et al., 2022b)4, we fine-tune BART-large model on standard QG datasets, including SQuAD (Rajpurkar et al., 2016) and QA2D (Demszky et al., 2018), for use as the QG model in our framework. Question Filtering (QF) To prevent unanswerable questions from impacting the final factuality score, we follow QAFactEval (Fabbri et al., 2022a) and remove the unanswerable questions using pre-trained Electra-large model (Clark et al.). During QF, the QA model receives only the PLS and its corresponding questions. In the subsequent QA stage, answers are extracted from the source for answer overlap evaluation, while QF only determines whether the questions can be answered by the PLS. Question Answering The QA model extracts answers to filtered questions from the source document. To prevent hallucinated output, we use an extractive QA model, pre-trained Electra-large, which was the best performing QA model in QAFactEval (Fabbri et al., 2022a). Answer Overlap Evaluation (AOE) We evaluate the alignment between the gold and generated answers from the QA model using the LERC score (Chen et al., 2020) and BERTScore (Zhang et al.). The final step in PLAINQAFACT is weighting the 4These studies use allennlp, an open-source NLP research library built on PyTorch, which is no longer actively maintained. scores for source simplification and elaborative explanation sentences. Specifically, the simplification score is computed using only abstracts as the source in QA, while the explanation score incorporates both abstracts and retrieved knowledge. PLAINQAFACT = sAvg. ns + eAvg. ne ns + ne , (1) where ns is the number of simplification sentences, and ne is the number labeled as explanation."
        },
        {
            "title": "5 Experiments",
            "content": "Since the classifier is trained on the PLAINFACT training set and its test set is limited to 20 scientificPLS pairs, we conduct experiments on both PLAINFACT and the CELLS test set, excluding PLAINFACT to prevent data leakage."
        },
        {
            "title": "5.1 Experiment Settings",
            "content": "Detailed model selections for each module are in 4. Prompts for GPT-4o (OpenAI, 2024) and Llama 3.1 8B Instruct (Meta, 2024) are in App. C. For the QG model, we fine-tune the BART-large with batch size of 16, learning rate of 3e-5, and two training epochs. In MedCPT retrieval, we set (the number of retrieved snippets) to 3, ensuring the retrieved information remains within short context window for the subsequent QA process (details in App. D)."
        },
        {
            "title": "5.2 Existing Factuality Metrics",
            "content": "To the best of our knowledge, no prior work has efficiently evaluated factuality metrics for detecting elaborative explanations in PLS. Moreover, most factuality metrics are designed for general-domain applications, largely due to limited quality and annotation of existing PLS datasets. To address this gap, we incorporate the following metrics in our experiment, informed by prior work (Joseph et al., 2024): (1) Dependency-Arc Entailment (DAE) (Goyal and Durrett, 2021) is an entailment-based method that evaluates summary factuality by breaking it into smaller entailment tasks at the arc level. The model independently determines whether each arcs relationship is supported by the input. (2) AlignScore (Zha et al., 2023) is model-based factuality metric using RoBERTa (Liu, 2019). It extracts claims from the summary and calculates the alignment scores with all the context chunks from the input document. The final score is an average of all highest alignment probabilities. (3) SummaC (Laban et al., 2022) is an NLI-based inconsistency detection method designed for summarization tasks. It segments documents into sentences and aggregates scores between sentence pairs using trained NLI model. (4) QAFactEval (Fabbri et al., 2022a) is QA-based factuality evaluation metric that assesses summary consistency by generating questions from the source document and comparing the model-generated answers with ex- (5) QuestEval (Scialom et al., pected answers. 2021a) is reference-less evaluation metric employs T5 model to generate questions from the source document and verifies whether the summary can correctly answer them."
        },
        {
            "title": "6 Results and Analysis",
            "content": "We first analyze the impact of each component incorporated into PLAINQAFACT and conduct additional ablation studies to assess retrieval effectiveness on PLAINFACT (6.1). Next, we evaluate PLAINQAFACT on two datasets, comparing it with five widely used factuality metrics (6.2). Finally, we perform an error analysis to investigate the failure cases of PLAINQAFACT (6.5)."
        },
        {
            "title": "Classifier",
            "content": "Learned classifier5 GPT-4o No (retrieve for all) Answer Extraction Llama3.1 8B Instruct"
        },
        {
            "title": "Granular Level",
            "content": "GPT-4o PromptRank NP Chunks Abs + TB + SP TB + SP Abs (no retrieval)"
        },
        {
            "title": "Sentence\nSummary",
            "content": "56.0 (0.02) - 56.1 (0.02) 58.4* (0.03) - 56.8 (0.05) 54.1* 53.9* - 48.5* (0.02) 50.6* (0.03) - 28.9* (0.03) - 55.8 (0.06) Table 2: Ablation study on PLAINFACT, evaluating the impact of individual components in PLAINQAFACT. The first row represents our proposed combination. Ablations are conducted by replacing one component at time while keeping all others fixed to match the best overall model. Abs: abstract; TB: Textbooks; SP: StatPearls. Using the first row as the baseline, * indicates statistically significant difference based on paired t-test (p < 0.05). We run each setting for five times and report the standard deviation (std.). Note that only the settings using LLMs may produce fluctuated scores."
        },
        {
            "title": "Source Reference Benchmark",
            "content": "Table 2 is the sentence-level ablation study of PLAINQAFACT where each component of our evaluation metric is modified individually. See App. for the summary-level evaluation. Additional analysis of retrieval resources is presented in Table 3. Learned Classifier Evaluating all PLS sentences without classification and retrieving for all improves factuality assessment but significantly increase evaluation time. Retrieval for every sentence extends processing time from 2 hours and 40 minutes with our learned classifier to 6 hours and 30 minutes without it for 2,740 evaluation examples. This highlights the efficiency gains of incorporating classifier in PLS factuality evaluation. Notably, the GPT-4o classifier achieves results comparable to our learned classifier trained on expert-annotated data, demonstrating the effectiveness of LLMs as alternative classifiers. This extends the applicability of PLAINQAFACT to factuality evaluation in other domains, as LLMs can serve as classifiers without requiring costly human annotations. Llama for Gold Answer Extraction The Llamabased answer extraction method outperforms NP Chunks by 2.1% by providing context-aware, flexible boundaries and robust answer selection (Yu et al., 2024), making it more effective for factuality evaluation. Although GPT-4o achieves slightly better performance, the improvement is marginal, making Llama more cost-effective alternative. Combined Domain Resources for Retrieval The best performance is achieved when abstracts, medical Textbooks, and StatPearls are combined as the source document. In contrast, excluding the source abstract for QA-based factuality evaluation achieves the worst performance, which is expected given that abstracts contain most of the key information. We further examine the impact of retrieval resources on different information types. As shown in Table 3, for elaborative explanation sentences, using abstracts alone leads to significantly lower scores compared to source simplification sentences, indicating that abstracts alone are insufficient for verifying external elaborative explanation content. Incorporating StatPearls alongside abstracts substantially improves performance for 5We acknowledge that applying the learned classifier to its training data may introduce data leakage. To further validate PLAINQAFACT, we evaluate its performance on the CELLS dataset, as shown in Figure 2. Simplification Abs Explanation Full PLS Abs TB SP TB + SP Abs + TB Abs + SP Abs + TB + SP Abs TB SP TB + SP Abs + TB Abs + SP Abs + TB + SP 63.3 (0.05) 30.5* (0.03) 17.0* (0.02) 18.4* (0.03) 25.2* (0.02) 39.8* (0.08) 40.1* (0.03) 44.6 (0.06) 50.6* (0.03) 45.4* (0.02) 45.9* (0.03) 48.5* (0.02) 54.1* (0.02) 54.3* (0.01) 56.0 (0.02) Table 3: Ablation study results on the PLAINFACT, evaluating how different retrieval sources affect various information types using PLAINQAFACT. The learned classifier categorizes sentences as source simplification (1,685) or elaborativeexplanation (1,055). Abs: abstracts; TB: Textbooks; SP: StatPearls. Using the Abs+TB+SP as the baseline, * indicates statistically significant difference based on paired t-test (p < 0.05). We run each setting for five times and report the standard deviation (std.) in the brackets. both elaborative explanation and full PLS, underscoring the importance of integrating high-quality domain-specific resources for factuality verification. BERTScore for Answer Overlap Although LERC performs well in QAFactEval, BERTScore achieves significantly better results for PLS factuality evaluation, improving performance by 27.1% on PLAINFACT. This disparity arises from their fundamental differences: LERC is learned metric trained on human factual consistency scores, whereas BERTScore relies on embedding-based similarity. Given that PLAINQAFACT evaluates factuality by comparing generated answers with extracted gold answers from the source, BERTScores ability to measure fine-grained semantic similarity makes it more effective choice. Sentence-level Evaluation Sentence-level evaluation slightly outperforms summary-level evaluation, primarily due to differences in input context for the QG model. In sentence-level evaluation, the QG model processes one PLS sentence at time, enabling more targeted and precise factual verification. In contrast, summary-level evaluation takes the entire PLS as input, introducing additional contextual dependencies that can lead to factual inconsistencies. This further confirms the effectiveness of finer granularity in PLS evaluation. robust capability for context-aware factuality assessment in plain language summarization tasks. However, its performance at the sentence level is inconsistent, while it slightly outperforms PLAINQAFACT on the CELLS dataset, it decreases by 16.5% on the PLAINFACT dataset (see App. for the prompt used with Llama 3.1). Figure 2: Scaled factuality accuracy on PLAINFACT and the CELLS test set. PLAINQAFACT significantly outperforms all other metrics on both datasets, except for AlignScore on PLAINFACT (p < 0.05, paired t-test). We compute the std. by running the experiment five times for both our method and Llama 3.1."
        },
        {
            "title": "Performance in PLS Factuality Evaluation",
            "content": "Figure 2 compares existing factuality metrics with PLAINQAFACT on the PLAINFACT and CELLS datasets. To ensure fair comparison, we scale factuality accuracy to range of 0 to 1, as QAFactEval scores range from 1 to 5. To further assess generalizability, we evaluate on 200 PLS-abstract pairs randomly sampled from the CELLS test set, sourced from journals outside PLAINFACT. Results indicate that AlignScores effectiveness depends on the proportion of elaborative versus simplification sentences. When fewer external elaborative sentences are present, its performance approaches that of PLAINQAFACT. However, as elaborative content increases, PLAINQAFACT consistently outperforms AlignScore, demonstrating its ability to better handle elaborative information absent from the source abstract. Compared to QA-based metrics such as QAFactEval and QuestEval, PLAINQAFACT benefits from incorporating retrieval module, which enables more comprehensive factuality assessment for elaborative explanations. DAE consistently performs the worst in PLAINFACT settings, highlighting the limitations of dependency-based factuality evaluation for PLS. Additionally, we assess the performance of Llama 3.1 as judge for factuality evaluation. In summary-level evaluations, Llama 3.1 demonstrates superior performance across both the PLAINFACT and CELLS datasets, showing its Figure 3: Scaled factuality accuracy on humanannotated elaborative explanation sentences from PLAINFACT. PLAINQAFACT significantly outperforms all other metrics on both evaluation levels (p < 0.05, paired t-test). We compute the std. by running the experiment five times for both our method and Llama 3.1."
        },
        {
            "title": "6.3 Explanation-Only Evaluation",
            "content": "We evaluate PLAINQAFACT on human-annotated explanation-only sentences from PLAINFACT (see Figure 3), conducting both sentenceand summarylevel evaluations. For summary-level setting, we aggregate all explanation sentences from each PLS to form new summary, resulting in 196 summaries in total (4 PLS are labeled as all simplification sentences.). Results show PLAINQAFACT outperforms all other factuality metrics in assessing explanation information at both evaluation levels, surpassing the second-best metric by 12.2% at the sentence-level and 10.5% at the summary-level. These findings highlight PLAINQAFACT is effective in evaluating the factuality of elaborative explanations, particularly when external knowledge is required for verification beyond the scientific abstracts. While Llama 3.1 demonstrates superior performance at the summary-level evaluation, PLAINQAFACT emerges as the best performer in sentence-level setting. Compared to the results in Figure 2, AlignScore performs similarly to PLAINQAFACT overall on PLAINFACT but struggles with external sentences, where its performance drops significantly. Thus, PLAINQAFACT proves to be the most suitable method for the factuality evaluan a fi a i Plain Language Sentence: This review looked at how well the methods worked to prevent pregnancy[extracted answer], if they caused bleeding problems, if women used them as prescribed, and how safe they were. (Lopez et al., 2013) Scientific Abstract: To compare the contraceptive effectiveness, cycle control, compliance (adherence), and safety of the contraceptive patch or the vaginal ring versus combination oral contraceptives (COCs)... (Lopez et al., 2013) Generated Question: This review looked at how well the methods worked to prevent what? Retrieved Knowledge: Appropriate treatment for the underlying etiology should start as soon as possible, and the patients and family members should receive appropriately targeted education... QA Answer: cycle control Plain Language Sentence: The patch is small, thin, adhesive[extracted answer] square that is applied to the skin. (Lopez a l i R et al., 2013) Scientific Abstract: Users of the norelgestromin-containing patch reported more breast discomfort, dysmenorrhea, nausea, and vomiting. In the levonorgestrel-containing patch trial, patch users reported less vomiting, headaches, and fatigue...(Lopez et al., 2013) Generated Question: The patch is small, thin, what kind of square applied to the skin? Retrieved Knowledge: Nonstick dressing Petrolatum-infused gauze strip or other material to form bolster over the graft site. This may be sutured or taped securely in place to provide some pressure and to keep graft immobilized. QA Answer: Nonstick dressing Petrolatum-infused gauze strip s e r n n Plain Language Sentence: Government[extracted answer] officials, business people and health professionals implementing such measures should work together with researchers to find out more about their effects in the short and long term. (von Philipsborn et al., 2019) Scientific Abstract: To assess the effects of environmental interventions (excluding taxation) on the consumption of sugar-sweetened beverages and sugar-sweetened milk, diet-related anthropometric measures and health outcomes, and on any reported unintended consequences or adverse outcomes...(von Philipsborn et al., 2019) Generated Question: What officials, business people and health professionals implementing such measures should work together with researchers to find out more about their effects in the short and long term? Retrieved Knowledge: Implementation should be accompanied by high-quality evaluations using appropriate study designs, with particular focus on the long-term effects of approaches suitable for large-scale implementation. QA Answer: long-term effects Table 4: Error analysis of PLAINQAFACT with model intermediate outputs. Each plain language sentence is extracted from the PLS, the QG model generates questions based on the extracted answer and PLS sentence, and \"Retrieved Knowledge\" refers to the context used by the QA model for answer extraction. tion of PLS. Figure 4: Evaluation time comparison of PLAINQAFACT. We compare both sentenceand summarylevel evaluation time over four settings: without information retrieval, PLAINQAFACT, and using GPT-4o as the classifier and answer extractor. We report the percentage reduction in time for summary-level evaluation given the sentence-level running time."
        },
        {
            "title": "6.4 Evaluation Time",
            "content": "Figure 4 shows the evaluation times for PLAINQAFACT across both sentence-level and summarylevel settings using PLAINFACT. The percentage scores indicate the reduction in evaluation time when transitioning from sentence-level to summary-level analysis. Our findings reveal that summary-level evaluation consistently reduces evaluation time across all cases. Moreover, employing PLAINQAFACT without MedRAG-based retrieval results in 33.3% reduction in evaluation time for sentence-level analysis. Although integrating GPT-4o as both the classifier and answer extractor slightly improves the factuality evaluation performance (shown in Table 2), it also increases the evaluation time, particularly when GPT-4o is used as the classifier."
        },
        {
            "title": "6.5 Error Analysis",
            "content": "To analyze cases where PLAINQAFACT fails on the PLAINFACT benchmark, we categorize three types of error in Table 4). For each example, we report the original PLS sentences, the corresponding scientific abstract with retrieved knowledge (from medical Textbooks and StatPearls), modelgenerated questions based on the extracted answers, and the QA models responses. In the first case, the extracted answer is pregnancy, but the QA model returns cycle control from the abstract due to the absence of this term in the retrieved content. Since this sentence is labeled as explanation which requires additional knowledge for verification, the error suggests potential misclassification between simplification and explanation. The second example shows how noisy retrieved information can affect evaluation accuracy, as the QA models answer is irrelevant to the extracted answer. In the final case, an unanswerable question generated by the QG model (e.g., regarding Government) reveals limitations in QA-based evaluation, as the question remains unanswered even with retrieval. Additionally, we acknowledge that some PLS summaries or sentences may not generate any questions, resulting in empty question sets. Addressing these challenges requires further improvements in classification, retrieval, and question generation."
        },
        {
            "title": "7 Discussion and Conclusion",
            "content": "Automated PLS generation has great potential for improving health information accessibility, but factuality evaluation remains challenging due to elaborative explanations. To address this, we introduce PLAINQAFACT, the first PLS factuality evaluation framework. PLAINQAFACT classifies sentence types, retrieves domain-specific knowledge, and utilizes QA-based approach to assess the factual consistency of both source-simplified and elaborative sentences. By leveraging open-source models, PLAINQAFACT ensures both accessibility and computational efficiency. To support granular factuality evaluation, we construct PLAINFACT, the first expert-annotated dataset for PLS, covering factuality type, functional role, and sentence alignment. Using PLAINFACT, our experiments demonstrate that PLAINQAFACT outperforms existing factuality metrics, particularly in handling external elaborative content. However, applying sentence-level evaluation to large datasets can be computationally expensive. For extensive data, we recommend using summary-level evaluation as more efficient alternative while maintaining comparable performance. Although our study focuses on biomedical PLS generation, the retrieval framework is adaptable to other domains by substituting domain-specific knowledge bases. Our findings highlight the effectiveness of retrieval-augmented QA for factuality evaluation, suggesting its broader applicability beyond PLS tasks. Future research should explore its integration into diverse summarization applications, particularly in LLM-generated text, where source coverage is often incomplete. By introducing fine-grained and explainable evaluation framework, our work establishes foundation for advancing QA-based factuality metrics. We hope PLAINQAFACT will contribute to the development of more reliable and interpretable factuality assessment methods in NLP."
        },
        {
            "title": "Limitations",
            "content": "We acknowledge that using an LLM as an answer extractor may yield non-deterministic outputs, even when setting the temperature to 0.01 for the Llama 3.1 8B Instruct model. For example, in our retrieval effectiveness evaluation  (Table 3)  , we run PLAINQAFACT four times to examine the impact of LLM-based answer extraction on the final factuality prediction. The resulting scores were 50.60, 50.64, 50.57, and 50.63, with standard deviation of 0.027. We consider this variability to be minimal, demonstrating that our factuality evaluation metric shows robust performance in the plain language summarization evaluation task. Our studys major limitations are as follows: 1) Although we provide substitute solutions for classifying input summaries or sentences, our pre-trained classifier is limited to the biomedical domain and specifically designed for plain language summarization tasks, fine-tuned through PLAINFACT. We acknowledge that the dataset used for model finetuning is insufficient, resulting in only moderate classification accuracy. Given the scarcity of high-quality human-annotated data for classifying source simplification and elaborative explanation information, future efforts may focus on developing domainand task-specific classifiers (You et al., 2024a) for factuality evaluation. 2) In the knowledge retrieval phase, we currently incorporate only Textbooks and StatPearls as external resources. This limited selection may not be sufficient for evaluating the factuality of plain language summaries. Further investigation into additional relevant knowledge bases, such as Wikipedia and PubMed, could enhance performance in explanation-needed factuality evaluation tasks. 3) For retrieval-augmented QA, we directly concatenate the scientific abstract with the retrieved information to form the reference for QA. However, we do not apply knowledge filtering to remove irrelevant information from external resources, which may introduce noise into the subsequent QA process for assessing answerlevel factual correctness."
        },
        {
            "title": "Acknowledgment",
            "content": "This work used Delta GPU at NCSA through allocation [CIS240504] from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by U.S. National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296."
        },
        {
            "title": "References",
            "content": "Ron Artstein and Massimo Poesio. 2008. Inter-coder agreement for computational linguistics. Computational linguistics, 34(4):555596. Meng Cao, Yue Dong, and Jackie Chi Kit Cheung. 2022. Hallucinated but factual! inspecting the factuality of hallucinations in abstractive summarization. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 33403354. Anthony Chen, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2020. MOCHA: dataset for training and evaluating generative reading comprehension metrics. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 65216532, Online. Association for Computational Linguistics. Kevin Clark, Minh-Thang Luong, Quoc Le, and Christopher Manning. Electra: Pre-training text encoders as discriminators rather than generators. In International Conference on Learning Representations. Meri Coleman. 1975. computer readability formula designed for machine scoring. Journal of Applied Psychology, 60(2):283. Edgar Dale and Jeanne Chall. 1948. formula for predicting readability: Instructions. Educational research bulletin, pages 3754. Dorottya Demszky, Kelvin Guu, and Percy Liang. 2018. Transforming question answering datasets into natural language inference datasets. arXiv preprint arXiv:1809.02922. Daniel Deutsch, Tania Bedrax-Weiss, and Dan Roth. 2021. Towards question-answering as an automatic metric for evaluating the content quality of summary. Transactions of the Association for Computational Linguistics, 9:774789. Alexander Fabbri, Chien-Sheng Wu, Wenhao Liu, and Caiming Xiong. 2022a. QAFactEval: Improved QAbased factual consistency evaluation for summarization. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 25872601, Seattle, United States. Association for Computational Linguistics. Alexander Richard Fabbri, Chien-Sheng Wu, Wenhao Liu, and Caiming Xiong. 2022b. Qafacteval: Improved qa-based factual consistency evaluation for summarization. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 25872601. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, and Haofen Wang. 2023. Retrievalaugmented generation for large language models: survey. ArXiv, abs/2312.10997. Tomas Goldsack, Zhihao Zhang, Chenghua Lin, and Carolina Scarton. 2023. Domain-driven and In Addiscourse-guided scientific summarisation. vances in Information Retrieval: 45th European Conference on Information Retrieval, ECIR 2023, Dublin, Ireland, April 26, 2023, Proceedings, Part I, pages 361376. Springer. Tanya Goyal and Greg Durrett. 2021. Annotating and modeling fine-grained factuality in summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 14491462. Yue Guo, Tal August, Gondy Leroy, Trevor Cohen, and Lucy Lu Wang. 2024a. APPLS: Evaluating evaluation metrics for plain language summarization. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 91949211, Miami, Florida, USA. Association for Computational Linguistics. Yue Guo, Joseph Chee Chang, Maria Antoniak, Erin Bransom, Trevor Cohen, Lucy Lu Wang, and Tal August. 2023. Personalized jargon identification for enhanced interdisciplinary communication. Proceedings of the conference. Association for Computational Linguistics. North American Chapter. Meeting, 2024:45354550. Yue Guo, Wei Qiu, Gondy Leroy, Sheng Wang, and Trevor Cohen. 2024b. Retrieval augmentation of large language models for lay language generation. Journal of Biomedical Informatics, 149:104580. Yue Guo, Wei Qiu, Yizhong Wang, and Trevor Cohen. 2021. Automated lay language summarization of biomedical scientific reviews. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 160168. Hardy Hardy, Shashi Narayan, and Andreas Vlachos. 2019. Highres: Highlight-based reference-less evaluation of summarization. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 33813392. Deepali Jain, Malaya Dutta Borah, and Anupam Biswas. 2021. Summarization of legal documents: Where are we now and the way forward. Computer Science Review, 40:100388. Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. 2021. What disease does this patient have? large-scale open domain question answering dataset from medical exams. Applied Sciences, 11(14):6421. Laureen Lopez, David Grimes, Maria Gallo, Laurie Stockton, and Kenneth Schulz. 2013. Skin patch and vaginal ring versus combined oral contraceptives for contraception. Cochrane Database of Systematic Reviews, (4). Qiao Jin, Won Kim, Qingyu Chen, Donald Comeau, Lana Yeganova, John Wilbur, and Zhiyong Lu. 2023. Medcpt: Contrastive pre-trained transformers with large-scale pubmed search logs for zero-shot biomedical information retrieval. Bioinformatics, 39(11):btad651. Sebastian Joseph, Lily Chen, Jan Trienes, Hannah Göke, Monika Coers, Wei Xu, Byron Wallace, and Junyi Jessy Li. 2024. FactPICO: Factuality evaluation for plain language summarization of medical evidence. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 84378464, Bangkok, Thailand. Association for Computational Linguistics. Seonok Kim. 2025. Medbiolm: Optimizing medical and biological qa with fine-tuned large language models and retrieval-augmented generation. JP Kincaid. 1975. Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel. Chief of Naval Technical Training. Aobo Kong, Shiwan Zhao, Hao Chen, Qicheng Li, Yong Qin, Ruiqi Sun, and Xiaoyan Bai. 2023. Promptrank: Unsupervised keyphrase extraction using prompt. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 97889801. Lauren Kuehne and Julian Olden. 2015. Lay summaries needed to enhance science communication. Proceedings of the National Academy of Sciences, 112(12):35853586. Philippe Laban, Tobias Schnabel, Paul Bennett, and Marti Hearst. 2022. Summac: Re-visiting nlibased models for inconsistency detection in summarization. Transactions of the Association for Computational Linguistics, 10:163177. Nayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Pascale Fung, Mohammad Shoeybi, and Bryan Catanzaro. 2022. Factuality enhanced language models for open-ended text generation. Advances in Neural Information Processing Systems, 35:3458634599. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:94599474. Yinhan Liu. 2019. Roberta: robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 364. Zheheng Luo, Qianqian Xie, and Sophia Ananiadou. 2023. Chatgpt as factual inconsistency evaluator for abstractive text summarization. arXiv preprint arXiv:2303.15621. Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, and Weizhu Chen. 2020. Generation-augmented retrieval for opendomain question answering. In Annual Meeting of the Association for Computational Linguistics. Meta. 2024. Llama 3.1: Open foundation and instruction-tuned models. Hassan Murad, Noor Asi, Mouaz Alsawas, and Fares Alahdab. 2016. New evidence pyramid. BMJ Evidence-Based Medicine, 21(4):125127. Brian Ondov, Kush Attal, and Dina Demner-Fushman. 2022. survey of automated methods for biomedical text simplification. Journal of the American Medical Informatics Association, 29(11):19761988. OpenAI. 2024. Gpt-4o. https://platform.openai. com/docs/models#gpt-4o. Accessed: 2025-01-20. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 23832392, Austin, Texas. Association for Computational Linguistics. Ruiyang Ren, Yuhao Wang, Yingqi Qu, Wayne Xin Zhao, J. Liu, Hao Tian, Huaqin Wu, Ji rong Wen, and Haifeng Wang. 2023. Investigating the factual knowledge boundary of large language models with retrieval augmentation. In International Conference on Computational Linguistics. Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano, Alex Wang, and Patrick Gallinari. 2021a. Questeval: Summarization asks for fact-based evaluation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 65946604. Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano, Alex Wang, and Patrick Gallinari. 2021b. QuestEval: Summarization asks for fact-based evaluation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 65946604, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Neha Srikanth and Junyi Jessy Li. 2021. Elaborative simplification: Content addition and explanation generation in text simplification. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 51235137. Yuheng Zha, Yichi Yang, Ruichen Li, and Zhiting Hu. 2023. Alignscore: Evaluating factual consistency with unified alignment function. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1132811348. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations. Marlene Stoll, Martin Kerwer, Klaus Lieb, and Anita Chasiotis. 2022. Plain language summaries: systematic review of theory, guidelines and empirical research. Plos one, 17(6):e0268789. Yixuan Tang and Yi Yang. 2024. Multihop-rag: Benchmarking retrieval-augmented generation for multihop queries. ArXiv, abs/2401.15391. Peter von Philipsborn, Jan Stratil, Jacob Burns, Laura Busert, Lisa Pfadenhauer, Stephanie Polus, Christina Holzapfel, Hans Hauner, and Eva Rehfuess. 2019. Environmental interventions to reduce the consumption of sugar-sweetened beverages and their effects on health. Cochrane Database of Systematic Reviews, (6). David Wan and Mohit Bansal. 2022. Evaluating and improving factuality in multimodal abstractive summarization. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 96329648. Guangzhi Xiong, Qiao Jin, Zhiyong Lu, and Aidong Zhang. 2024. Benchmarking retrieval-augmented generation for medicine. In Findings of the Association for Computational Linguistics: ACL 2024, pages 62336251, Bangkok, Thailand. Association for Computational Linguistics. Zhentao Xu, Mark Jerome Cruz, Matthew Guevara, Tie Wang, Manasi Deshpande, Xiaofeng Wang, and Zheng Li. 2024. Retrieval-augmented generation with knowledge graphs for customer service question answering. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 29052909. Yuxuan Ye, Edwin Simpson, and Raul Santos Rodriguez. 2024. Using similarity to evaluate factual consistency in summaries. arXiv preprint arXiv:2409.15090. Zhiwen You, Kanyao Han, Haotian Zhu, Bertram Ludaescher, and Jana Diesner. 2024a. SciPrompt: Knowledge-augmented prompting for fine-grained categorization of scientific topics. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 60876104, Miami, Florida, USA. Association for Computational Linguistics. Zhiwen You, Shruthan Radhakrishna, Shufan Ming, and Halil Kilicoglu. 2024b. UIUC_BioNLP at BioLaySumm: An extract-then-summarize approach augmented with Wikipedia knowledge for biomedical lay summarization. In Proceedings of the 23rd Workshop on Biomedical Natural Language Processing, pages 132143, Bangkok, Thailand. Association for Computational Linguistics. Qingchen Yu, Zifan Zheng, Shichao Song, Zhiyu Li, Feiyu Xiong, Bo Tang, and Ding Chen. 2024. xfinder: Robust and pinpoint answer extraction for large language models. arXiv preprint arXiv:2405.11874."
        },
        {
            "title": "A Dataset Annotation Protocol",
            "content": "We developed comprehensive annotation procedure for freelancers on Upwork to conduct factchecking annotations. Our annotation procedure involves two stages, starting with thorough training using detailed examples to ensure consistent understanding of the task between annotators. Annotators receive spreadsheet where each row contains pair of data: sentence extracted from the PLS and its corresponding scientific abstract. For every sentence-abstract pair, the annotators are required to label three features: External, Category, and Relation, with the appropriate labels. Step 1: Sentence Annotation: Compared to the scientific abstract, analyze each sentence of the plain language summary across three dimensions: external information, category, and relation. 1. External: Determine whether the sentence includes information does not present in the scientific abstract. 1) Yes: The sentence contains external information that is not explicitly mentioned, paraphrased, or implied in the scientific abstract. 2) No: The sentence contains information that is explicitly stated or closely paraphrased from the scientific abstract. 2. Category: Classify the sentence of the plain language summary into one of the following categories (you can only choose one category per annotation) 1) Definition: Provides fundamental explanation of term. 2) Background: Information that helps understand the term within the context of the abstract, such as relevance, significance, or motivation. 3) Example: Specific instances that illustrate the use of the term in the scientific abstract. 4) Method/result: Details about the methodology or results described in the scientific abstract. 5) Other: For sentences that do not fit into the categories above, please indicate the category 3. Relation: Identify the sentence(s) in the scientific abstract that the sentence of the plain language summary is related to. Use indices of sentences from the scientific abstract to link the sentence of the plain language summary. You can select one or more sentences from the scientific abstract. List the indices like s1_1,s2_3,s3_6. If no relation is found, mark it as external. In Step 2, we provide explanations of the existing sentence-level indexes for plain language sentences and scientific abstracts. Step 2: Using the Annotation Spreadsheet: You will work within structured spreadsheet containing the segmented sentences from both the summaries and the scientific abstracts. 1. Target_Summary_ID: unique identifier for each plain language summary. Sentences from the same plain language summary share the same ID. 2. Target_Sentence_Index: An identifier for each sentence within summary, forming as tx_y, where tx is the same as its Target_Summary_ID, and represents the index of each sentence in the plain language summary, starting from 1. e.g., t0_1 refers to the first sentence of the first summary) 3. Target_Sentence: The plain language sentence you are annotating. 4. Original_Abstract: The abstract corresponding to each summary, with each sentence indexed for easy reference. Each annotator will annotate 40 PLS-abstract pairs to ensure each sentence of the PLS has two sets of annotations from different people. For each row of the spreadsheet, they need to annotate three columns: External, Category, and Relation. <TO BE ANNOTATED> External: Mark Yes or No to indicate if the sentence in the Target_Sentence column contains external information. <TO BE ANNOTATED> Category: Choose the most fitting category of the sentence from the list (Definition, Background, Example, Method/Result, Other). <TO BE ANNOTATED> Relation: List the relevant sentence indices from the abstract in the Original_Abstract column that relate to the plain language sentence (e.g., s10_1,s10_5). For example, filling in s10_1,s10_5,s10_9 if you think these three sentences from the abstract are relevant to sentence t1_3. Use commas to separate multiple indices. To ensure annotators fully understand the context and task requirements, we provide comprehensive annotation training and screening test prior to the annotation process. We select candidates from freelancers with medical education background, and only those who pass the screening test are finalized as annotators. 1 m 2 m e a E"
        },
        {
            "title": "Annotation",
            "content": "Plain Language Sentence Gout caused by crystal formation in the joints due to high uric acid levels in the blood. Need External Information? yes Category Relation Background external Corresponding Abstract None Plain Language Sentence Reducing blood pressure with drugs has been strategy used in patients suffering from an acute event in the heart or in the brain, such as heart attack or stroke. Need External Information? yes Category Relation Corresponding Abstract Background s10_1,s10_2 <s10_1>Acute challenge. <s10_2>Blood pressure lowering drugs are commonly used and recommended in the early phase of these settings. cardiovascular therapeutic represent events Plain Language Sentence We looked at whether choice of antibiotic made difference in the number of people who experienced failed treatment, and we determined the proportions who had resolution of fever at 48 hours. Need External Information? no"
        },
        {
            "title": "Corresponding Abstract",
            "content": "Method/Result s15_16,s15_17,s15_20 <s15_16>For treatment failure, the difference between doxycycline and tetracycline is uncertain (very low-certainty evidence). <s15_17>Doxycycline compared to tetracycline may make little or no difference in resolution of fever within 48 hours (risk ratio (RR) 1.14, 95% confidence interval (CI) 0.90 to 1.44, 55 participants; one trial; low-certainty evidence) and in time to defervescence (116 participants; one trial; lowcertainty evidence). <s15_20>For most outcomes, including treatment failure, resolution of fever within 48 hours, time to defervescence, and serious adverse events, we are uncertain whether study results show difference between doxycycline and macrolides (very low-certainty evidence). Table 5: Examples of our curated dataset. Need External Information feature represents whether plain language sentence is simplification or an explanation. label of yes indicates that the sentence is an explanation and requires additional elaborative information beyond the source abstract to verify its factuality. Conversely, label of no shows that the sentence is simplification that can be validated using only the source abstract."
        },
        {
            "title": "B Dataset Examples",
            "content": "In Table 5, we presents three representative examples from PLAINFACT. Each example is annotated with five features: plain language sentence, an indicator of whether the sentence is simplification or explanation, its category, its relation, and the corresponding abstract. All plain language sentences are factual, as they were written by the authors from the Cochrane database. Need External Information? feature specifies whether sentence can be validated solely by the abstract. Yes label indicates that the sentence includes information not explicitly mentioned in the abstract, and vice versa. The Relation feature identifies the sentence(s) in the scientific abstract most relevant to the PLS; if no corresponding content exists in the abstract, the relation is marked as external. We randomly sample three examples from the dataset to to illustrate the datasets structure. Additionally, indexes have been created for both the plain language sentences and the abstract sentences to facilitate annotation."
        },
        {
            "title": "C LLM Prompts",
            "content": "We utilize two types of prompts to guide LLMs through two stages of factuality evaluation: classification and answer extraction. In accordance with the benchmark annotation protocol (Appendix A), we employ GPT-4o as classifier to determine whether given sentence or summary requires elaborative explanations for factual verification. For both stages, we set the max_tokens parameter to 512 and configure the temperature to 0 for GPT-4o and 0.01 for the Llama 3.1 8B Instruc model. Developer Annotate whether sentence or summary includes information not present in the original abstract. The sentence or summary contains external information that is not explicitly mentioned, paraphrased, or implied in the original abstract will be labeled as 'Yes'."
        },
        {
            "title": "PLAINQAFACT",
            "content": "Classifier Learned classifier GPT-4o No (retrieve for all) Answer Extraction Llama3.1 8B Instruct 55.8 (0.12) - 59.1 (0.05) 60.1* (0.18) - 54.8 (0.15) 48.4* 50.5* - 47.9* (0.11) 55.2 (0.07) - 32.9* (0.18) GPT-4o PromptRank NP Chunks Abs + TB + SP TB + SP Abs (no retrieval)"
        },
        {
            "title": "BERTScore\nLERC",
            "content": "The sentence or summary contains information that is explicitly stated or closely paraphrased from the original abstract will be labeled as 'No'. Retrieval Source User Sentence or summary: <input> Original abstract: <abstract> Answer Overlap Example 1: Prompt of GPT-4o as the Classifier. For the AE stage, we explore both GPT-4o and Llama 3.1 8B Instruct as backbone models to assess the factuality evaluation performance of PLAINQAFACT. Following the task description outlined in QAFactEval (Fabbri et al., 2022a), we instruct both LLMs to extract potential answer entities from the input PLS. Developer QA-based metrics compare information units between the summary and source, so it is thus necessary to first extract such units, or answers, from the given summary. Please extract answers or information units from plain language summary. User Extract comma-separated list of the most important keywords from the following text: <input> Example 2: Prompt of GPT-4o as the Answer Extractor. System QA-based metrics compare information units between the summary and source, so it is thus necessary to first extract such units, or answers, from the given summary. Please extract answers or information units from plain language summary. User Extract comma-separated list of the most important keywords from the following text: <input> Example 3: Prompt of Llama 3.1 8B Instruct as the Answer Extractor. Additionally, we report the performance of using Llama 3.1 8B Instruct as judge to evaluate the factuality of given PLS based on its source scientific abstract. System Rate the factuality of the given plain language sentence or summary compared with the scientific abstract. Output numeric score from 0 to 100, with 100 meaning the sentence is completely factually consistent with the abstract and 0 meaning the sentence is completely non-factual with the abstract. User Sentence or summary: <input> Original abstract: <abstract> Factuality score (only output numeric score): <score> Example 4: Prompt of Llama 3.1 8B Instruct as factuality judge. Table 6: Ablation study of PLAINQAFACT on PLAINFACT in the summary level, analyzing the impact of individual components in PLAINQAFACT. Abs: abstract; TB: Textbooks; SP: StatPearls. Using the first row as the baseline, * indicates statistically significant difference based on paired t-test (p < 0.05). We run each setting for five times and report the standard deviation (std.). Note that only the settings using LLMs may produce fluctuated scores."
        },
        {
            "title": "D Detailed Experiment Settings",
            "content": "All experiments we conduct are under one NVIDIA A100 GPU with 40 GB GPU memory. We employ the Natural Language Toolkit (NLTK)6 for PLS tokenization and sentence segmentation, utilizing the punkt package. For classifier fine-tuning, we set the random seed to 42 to split the dataset into training, validation, and test sets. We tune the PubMedBERT-base model for three epochs with early stopping under the validation set. The same seed (42) is also used to sample 200 PLS-abstract pairs from the CELLS test set for the comparison in Figure 2. The QA model used in our PLAINQAFACT is downloaded from QAFactEval (Fabbri et al., 2022a); however, we set its maximum input length to 512 (from 364) tokens to incorporate more source context. Similarly, to ensure that the answers extracted during the AE stage are valid (i.e., within the 512-token limit to maintain consistency with the subsequent QA model), we configure the LLMs input lengths to 512 tokens. Summary-Level Ablation Study of"
        },
        {
            "title": "PLAINQAFACT",
            "content": "Similar to the sentence-level ablation studies in Table 2, we conduct experiments to evaluate the 6https://www.nltk.org/ impact of various components in PLAINQAFACT at the summary level using PLSs as inputs. Although the overall evaluation score using GPT-4o as the classifier is 3.3% higher than that of PLAINQAFACT, the paired t-test (p<0.05) does not provide sufficient statistical evidence to conclude that GPT-4o significantly outperforms our learned classifier. This finding indicates that GPT-4os performance is comparable with our learned classifier, consistent with the sentence-level results. Notably, in the summary-level evaluation, using only scientific abstracts as references for QA-based evaluation yields performance that is competitive with using retrieved external knowledge. We assume that summary-level retrieval may not consistently produce highly relevant knowledge snippets and might even introduce noise that undermines factuality evaluation accuracy. Additionally, omitting the classification stage and retrieving information for all summaries produces the best performance among all settings. Although this approach increases the runtime to approximately 30 minutes compared to using the learned classifier for 200 summaries, the performance improvement is significant. Therefore, we recommend either employing more targeted knowledge bases for summary-level retrieval, compressing abstracts to include more relevant information, or retrieving for all input summaries when conducting summary-level factuality evaluation with PLAINQAFACT."
        },
        {
            "title": "F Explanation Sentence Removal",
            "content": "To further assess the performance of existing factuality metrics on elaborative explanation sentences in PLSs, we conduct experiments in which these explanation sentences were iteratively removed from the summaries. Figure 5 illustrates the percentage change in scores relative to the baselines as more explanation sentences are removed. Notably, PLAINQAFACT can even predict 100% factual ratings for the given PLS when more explanation information is removed, whereas other metrics struggle to do so. Although our framework shows moderate performance in Figure 5, with gradual increase in score as explanation sentences are removedwe believe that our retrieval-augmented, QA-based factuality evaluation framework more effectively assesses the factuality of plain language sentences that require elaborative explanations for verification. Figure 5: Evaluation performance across metrics for explanation sentences under PLAINFACT. We select only those summaries containing at least five explanation sentences, resulting in total of 116 summaries. Note that this experiment is conducted at the summary level. higher change percentage indicates metric is more sensitive to the explanation information, implying that such metrics may not effectively verify the factuality of PLS. Ideally, we expect the score change percentage to increase slowly. Among the metrics evaluated, only AlignScore, SummaC, and our method show this expected pattern."
        },
        {
            "title": "G Pilot Study on FactPICO",
            "content": "In PLS generation tasks, we employ the recently introduced FactPICO dataset (Joseph et al., 2024). This dataset comprises human-labeled plain language summaries of Randomized Controlled Trials (RCTs) that address several critical elements: Populations, Interventions, Comparators, Outcomes (PICO), and additional information. All summaries are generated by various LLMs based on scientific literature and include extensive explanations not present in the original abstracts. We hypothesize that existing factuality evaluation metrics for text summarization may struggle to accurately assess the factuality of this added information. To validate this assumption, we conduct pilot studies using four factuality evaluation metrics from FactPICO alongside one NLI-based metric: DAE (Goyal and Durrett, 2021), AlignScore (Zha et al., 2023), SummaC (Laban et al., 2022), QAFactEval (Fabbri et al., 2022a), and QuestEval (Scialom et al., 2021b). G.1 Dataset Pre-processing FactPICO provides span-level annotations for LLM-generated summaries, assessing whether the additional information is present and determining its factuality, labeled as either yes or no. We first remove all special identification tags from the and abstracts, such as ABSTRACT.BACKGROUND, ABSTRACT.RESULTS, ABSTRACT.CONCLUSIONS. We then deduplicate the generated summaries, while retaining duplicates within abstracts, as each abstract has three summaries generated by different LLMs, and each abstract may have varying numbers of generated summaries, from none to multiple. This pilot study focuses on two key research questions: evaluating the effectiveness of existing factuality metrics in assessing added information and non-factual added information. G.2 Experiments and Results We conduct experiments using the processed FactPICO dataset to address both research questions. It is important to note that there are some outliers in the PLSs which contain extraneous content and exceed the input length limitations of the DAE and QAFactEval metrics. Therefore, we are unable to include these two metrics in this pilot study. span through exact matching, continuing this process until no added information remains. This procedure allows us to determine how effectively existing metrics handle external information that is not present in the original abstracts. As shown in Figure 6, we report performance changes relative to baseline scores. For example, AlignScore increases by 3.33 (on scale of 100) when six units of added information are removed, leading to change of approximately 4.9%. These results indicate that added information has impacts on model-based factuality evaluation metrics such as AlignScore and SummaC, with both scores improving as more added information is removed. In contrast, QuestEvals score decreases with the removal of added information. Figure 6: Score change percentage from baselines over three metrics on the FactPICO dataset in removing factual added information (88 pairs). We expect each model get higher performance when more external information is removed. The evaluation dataset contains 88 valid summary-abstract pairs. RQ1: Do existing metrics perform well when external information is added to PLS compared with no added information? In this study, we evaluate the ability of existing metrics to detect added information in PLSs. We focus on PLSs where all added information is annotated as factual, resulting in deduplicated dataset of 88 PLSabstract pairs. To assess metric sensitivity to the added information, we iteratively remove sentences from each PLS that contain added spans. For example, if span such as of medicine called haloperidol is labeled as factual (yes), we remove the entire sentence in the original PLS containing that Figure 7: Score change percentage from baselines over three metrics on the FactPICO dataset in removing nonfactual added information (60 pairs). We expect each metrics score increases when more non-factual information is removed. RQ2: Can existing metrics distinguish between non-factual and factual added information? In this research question, our goal is to evaluate the sensitivity of factuality evaluation metrics in detecting non-factual information within PLSs. The FactPICO dataset labels added information as either yes (factual) or no (non-factual). For this scenario, we sample only those PLSs that contain both yes and no labels for added spans. As with RQ1, we iteratively remove sentences containing non-factual added spans from each PLS until no non-factual sentences remain. Overall, as illustrated in Figure 7, most evaluation metrics show improved performance as more non-factual information is removed. Nevertheless, our findings suggest that existing factuality evaluation metrics have limited capacity to accurately distinguish between factual and non-factual added information in text summarization tasks."
        }
    ],
    "affiliations": [
        "University of Illinois Urbana-Champaign"
    ]
}