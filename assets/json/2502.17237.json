{
    "paper_title": "MegaLoc: One Retrieval to Place Them All",
    "authors": [
        "Gabriele Berton",
        "Carlo Masone"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Retrieving images from the same location as a given query is an important component of multiple computer vision tasks, like Visual Place Recognition, Landmark Retrieval, Visual Localization, 3D reconstruction, and SLAM. However, existing solutions are built to specifically work for one of these tasks, and are known to fail when the requirements slightly change or when they meet out-of-distribution data. In this paper we combine a variety of existing methods, training techniques, and datasets to train a retrieval model, called MegaLoc, that is performant on multiple tasks. We find that MegaLoc (1) achieves state of the art on a large number of Visual Place Recognition datasets, (2) impressive results on common Landmark Retrieval datasets, and (3) sets a new state of the art for Visual Localization on the LaMAR datasets, where we only changed the retrieval method to the existing localization pipeline. The code for MegaLoc is available at https://github.com/gmberton/MegaLoc"
        },
        {
            "title": "Start",
            "content": "MegaLoc: One Retrieval to Place Them All Gabriele Berton Polytechnic of Turin bertongabri@gmail.com"
        },
        {
            "title": "Carlo Masone\nPolytechnic of Turin",
            "content": "5 2 0 2 4 2 ] . [ 1 7 3 2 7 1 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Retrieving images from the same location as given query is an important component of multiple computer vision tasks, like Visual Place Recognition, Landmark Retrieval, Visual Localization, 3D reconstruction, and SLAM. However, existing solutions are built to specifically work for one of these tasks, and are known to fail when the requirements slightly change or when they meet out-ofIn this paper we combine variety of distribution data. existing methods, training techniques, and datasets to train retrieval model, called MegaLoc, that is performant on multiple tasks. We find that MegaLoc (1) achieves state of the art on large number of Visual Place Recognition datasets, (2) impressive results on common Landmark Retrieval datasets, and (3) sets new state of the art for Visual Localization on the LaMAR datasets, where we only changed the retrieval method to the existing localization pipeline. The code for MegaLoc is available at https: //github.com/gmberton/MegaLoc 1. Introduction This paper tackles the task of retrieving images from large database that represent the same place as given query image. But what does it mean for two images to be from the same place? Depending on who you ask, youll get different answers: 1. Landmark Retrieval (LR) folks will tell you that two photos are from the same place if they depict the same landmark, regardless of how close to each other the two photos were taken [40]; 2. Visual Place Recognition (VPR) people set camera pose distance of 25 meters to define if two images are positives (i.e. from the same place) [4]; 3. Visual Localization (VL) / 3D Vision researchers will tell you that two images need to have their pose as close as possible to be considered the same place. Even though image retrieval is core component in all three tasks, their different definitions and requirement has inevitably led to the development of ad-hoc image retrieval Figure 1. Qualitative examples of predictions by MegaLoc. Each pair of images represents query and its top-1 prediction from the SF-XL dataset, searched across the 2.8M database spanning 150 km2 across San Francisco. Predictions in green are correct, red are wrong. solutions for each of them. As these three tasks continued to diverge, over the years papers have avoided showing results of their methods on more than one of these tasks: VPR papers dont show results on LR, and LR papers dont show results on VPR. In the meantime, 3D vision pipelines like COLMAP [30], Hierarchical Localization [28] and GLOMAP [22] keep using outdated retrieval methods, like RootSIFT with bag-of-words [3, 10, 32] and NetVLAD [4]. In this paper we aim to put an end to this, by training single model that achieves SOTA (or almost) on all of these tasks, showcasing robustness across diverse domains. To train this model we do not propose any technical novelty, but we use all the lessons learned from all these three task, putting together combination of good samplers, datasets, and general training techniques. Why does it matter?, you may ask. Imagine you are doing 3D reconstruction, where image retrieval is fundamental component, on collection of diverse scenes (e.g. to 1 create datasets like MegaDepth [18], MegaScenes [37], or for the evergreen Image Matching Challenge [6]). In some cases there would be small scenes (e.g. reconstruction of fountain), requiring retrieval model that is able to retrieve nearby images (few meters away), which is something VPR models excel at, but LR models underperform (see [8] Tab. 14). In other cases however, the scene might be large (e.g. big landmark like church), with images hundreds of meters away: while LR models are designed for this, VPR models achieve poor results in this situations (see Sec. 3). Given these considerations, we note how neither VPR nor LR provide models for the diverse cases of 3D reconstructions, creating gap in literature that is filled by MegaLoc. As another example where model like MegaLoc is necessary, one can think of Visual Place Recognition (which is also the first step for Visual Localization), where models are evaluated by using 25 meters threshold (and queries in popular datasets always have at least one positive within 25 meters). However, in the real world the nearest image to given query might be 100 meters away, and while ideally we would still want to retrieve it, VPR model is unlikely to work in such case, as it has been trained to ignore anything further away from the camera. In this paper we demonstrate that, by leveraging diverse set of data sources and best practices from LR, VPR and VL, we obtain single image retrieval model that works well across all these tasks. Our model is called MegaLoc and it is released at https://github.com/ gmberton/MegaLoc 2. Method The core idea of this paper is to fuse data from multiple datasets, and train single model. We use five datasets containing both outdoor and indoor images and catering to different image localization tasks: GSV-Cities [1], Mapillary Street-Level Sequences (MSLS) [39], MegaScenes [37], ScanNet [13] and San Francisco eXtra Large (SF-XL) [7]. At each training iteration, we extract six-sub batches of data, one for each dataset (except SF-XL, from which two sub-batches are sampled) and use multi-similarity loss [38] computed over each sub-batch. Each sub-batch is made of 128 images, containing 4 images (called quadruplets) from 32 different places/classes. Given that these datasets have diverse format, they require different sampling techniques. In the following paragraphs we explain how data is sampled from each dataset. method assures that each class contains images that represent given place from diverse perspectives, while ensuring that no visual overlap exists between two different places. EigenPlaces provides two sub-batches, one made of frontalfacing images (i.e. with the camera facing straight along the street) and one of lateral-facing images. Google Street View Cities (GSV-Cities) is dataset of 530k images split into 62k places/classes from 40 cities, where each class contains at least 4 images with same orientation and is at least 100 meters from any other class. Given that GSV-Cities is already split into non-overlapping classes, it is not strictly necessary to apply particular sampling technique. We therefore directly feed the GSV-Cities dataset to the multi-similarity loss, as in the original GSVCities paper [1]. Mapillary Street-Level Sequences (MSLS) is dataset of 1.6M images split in contiguous sequences, across 30 different cities over 9 years. To ideally sample data from the MSLS dataset, we use the mining technique described in the CliqueMining paper [33]. This method ensures that the places selected for each batch depict visually similar (but geographically different) places (i.e. hard negatives), so that the loss can be as high as possible and effectively teach the model to disambiguate between similar-looking places. MegaScenes is collection of 100k 3D structure-frommotion reconstructions, composed of 2M images from Wikimedia Commons. Simply using each reconstruction as class, and sampling random images from such class, could lead to images that do not have any visual overlap, e.g. two images could show opposites facades of building, therefore having no visual overlap while belonging to the same 3D reconstruction. Therefore we make sure that when we sample set of four images from given reconstruction, each of these four images should have visual overlap with each other (we define visual overlap as having at least 1% of 3D points in common in the 3D reconstruction). ScanNet is dataset of 2.5M views from 1500 scans from 707 indoor places. To train on ScanNet we use each scene as class, and select quadruplets so that each pair of images within quadruplet has visual overlap (i.e. less than 10 meters and 30 apart); simultaneously we ensure that no two images from different quadruplets has visual overlap. San Francisco eXtra Large (SF-XL) is dataset of 41M images with GPS and orientation from 12 different years, densely covering the entire city of San Francisco across time. To select ideal quadruplets for training, we use the sampling technique presented in EigenPlaces [9]. This 3. Experiments 3.1. Implementation details During training, images are resized to 224224, while for inference we resize them to 322322, following [16]. We 2 Method NetVLAD [4] AP-GeM [27] CosPlace [7] MixVPR [2] EigenPlaces [9] AnyLoc [17] Salad [16] CricaVPR [20] CliqueMining [33] MegaLoc (Ours) Desc. Dim. 4096 2048 2048 4096 2048 49152 8448 10752 8448 8448 Baidu [34] R10 R1 Eynsham [8, 12] R1 MSLS val [39] R1 R10 Pitts250k [4, 14] R1 R10 Pitts30k [4, 14] R1 SF-XL v1 [7] R10 R1 SF-XL v2 [7] R10 R1 SF-XL night [5] R1 R10 SF-XL occlusion [5] R1 Tokyo 24/7 [36] R1 R10 69.0 59.8 52.0 71.9 69.1 75.6 72.7 65.6 72.9 87.7 95.0 90.8 80.4 94.7 91.9 95.2 93.6 93.2 92.7 98.0 77.7 68.3 90.0 89.6 90.7 85.0 91.6 88.0 91.9 92.6 90.5 84.0 94.9 94.4 95.4 94.1 95.9 94.3 96.2 96. 54.5 56.0 85.0 83.2 85.9 58.7 88.2 76.7 91.6 91.0 70.4 72.9 92.6 91.9 93.1 74.5 95.0 87.2 95.9 95.8 85.9 80.0 92.3 94.3 94.1 89.4 95.0 92.6 95.3 96.4 95.0 93.5 98.4 98.9 98.7 98.0 99.2 98.3 99.2 99.3 85.0 80.7 90.9 91.6 92.5 86.3 92.3 90.0 92.6 94.1 94.4 94.1 96.7 96.4 97.6 96.7 97.4 96.7 97.8 98. 40.1 37.9 76.6 72.5 84.0 - 88.7 62.6 85.5 95.3 57.7 54.1 85.5 80.9 90.7 - 94.4 78.9 92.6 98.0 76.9 66.4 88.8 88.6 90.8 - 94.6 86.3 94.5 94.8 91.1 84.6 96.8 95.0 96.7 - 98.2 96.0 98.3 98.5 6.7 7.5 23.6 19.5 23.6 - 46.1 25.8 46.1 52.8 14.2 16.7 32.8 30.5 34.5 - 62.4 40.6 60.9 73. 9.2 5.3 30.3 30.3 32.9 - 50.0 27.6 44.7 51.3 22.4 14.5 44.7 38.2 52.6 - 68.4 47.4 64.5 75.0 69.8 57.5 87.3 87.0 93.0 87.6 94.6 82.9 96.8 96.5 82.9 77.5 95.6 94.0 97.5 97.5 98.1 93.7 97.8 99.4 Table 1. Recall@1 and Recall@10 on multiple VPR datasets. Best overall results on each dataset are in bold, second best results underlined. Results marked with - did not fit in 480GB of RAM (2.8M features of 49k dimensions require 560GB for float32-based kNN)."
        },
        {
            "title": "Method",
            "content": "CAB (Phone) HGE (Phone) LIN (Phone) (1, 0.1) (5, 1.0) (1, 0.1) (5, 1.0) (1, 0.1) (5, 1.0) CAB (HoloLens) (5, 1.0) (1, 0.1) HGE (HoloLens) (5, 1.0) (1, 0.1) LIN (HoloLens) (5, 1.0) (1, 0.1) NetVLAD AP-GeM Fusion (NetVLAD+AP-GeM) CosPlace MixVPR EigenPlaces AnyLoc Salad CricaVPR CliqueMining MegaLoc (Ours) 43.4 39.4 41.4 29.0 40.9 32.3 48.0 44.2 40.4 44.2 47.0 54.0 52.0 53.8 37.4 50.8 44.7 59.8 55.6 52.0 55.6 60.4 54.8 58.0 56.3 54.4 59.2 56.3 58.8 65.3 63.7 66.0 67.2 80.0 81.3 82.4 81.3 83.8 81.3 83.0 92.2 89.3 91.4 92.9 74.4 69.1 76.0 63.3 77.5 70.2 77.2 81.7 80.7 80.5 83. 87.8 82.0 89.4 75.7 89.8 82.6 92.4 94.0 93.1 93.1 94.9 63.1 62.9 63.2 56.4 65.2 63.9 69.7 71.5 73.9 74.2 77.4 81.4 82.5 83.1 77.8 84.7 81.8 88.5 90.7 90.7 90.9 93.4 57.9 65.6 63.1 55.6 63.3 60.2 70.1 75.3 72.5 77.3 72.9 71.6 76.6 75.1 69.8 74.7 72.5 81.0 85.2 81.6 86.3 83.5 76.1 80.7 78.5 80.6 83.6 84.8 81.4 91.3 89.1 92.0 92. 83.0 91.1 87.0 91.4 92.2 93.1 90.4 99.4 98.4 98.8 99.0 Table 2. Results on LaMARs datasets, computed on each of the three locations, for both types of queries (HoloLens and Phone), which include both indoor and outdoor. For each location we report the recall at (1, 10cm) and (5, 1m), following the LaMAR paper [29]. use RandAugment [11] for data augmentation, as in [1], and AdamW [19] as optimizer. Training is performed for 40k iterations. The loss is simply computed as = L1 + L2 + L3 + L4 + L5 + L6, where each Ln is the multi-similarity loss computed on one of the sub-batches. The architecture consists of DINO-v2-base backbone [21] followed by SALAD [16] aggregation layer, which has shown state-of-the-art performances over multiple VPR datasets [16, 33]. The SALAD layer is computed with 64 clusters, 256 channels per cluster, global token of 256 and an MLP dimension of 512. The SALAD layer is followed by linear projection (from dimension of 16640 to 8448) and an L2 normalization. Memory-efficient GPU training is achieved using PyTorch [23], by ensuring that the computational graph for each loss stays in memory as little as possible. In practice (in the code), instead of adding the computational graph for each loss into single giant graph, we compute each loss and perform the backward() operation independently: calling backward() in PyTorch not only computes the gradient (which is added to any existing gradient), but also frees the computational graph (hence freeing memory). The step() (and zero grad()) method is then called only once (after six backward() calls. This simple technique reduces the VRAM requirement of training MegaLoc from (roughly) 300GB to 60GB. 3.2. Results We perform experiments on three different types of tasks: Visual Place Recognition, where the task is to retrieve images that are within 25 meters from the query (Sec. 3.2.1); Visual Localization, where retrieval is part of bigger pipeline that aims at finding the precise pose of the query given set of posed images (Sec. 3.2.2); Landmark Retrieval, i.e. retrieving images that depict the same landmark as the query (Sec. 3.2.3). 3.2.1. Visual Place Recognition We run experiments on comprehensive set of Visual Place Recognition datasets. These datasets contain large variety of domains, including: outdoor, indoor, street-view, hand-held camera, car-mounted camera, night, occlusions, long-term changes, grayscale. Results are shown in Tab. 1. While other high-performing VPR models (like SALAD and CliqueMining) achieve very good results (i.e. comparable to MegaLoc) on most datasets, MegaLoc vastly outperforms every other model on Baidu, which is an indoor-only dataset. 3 Figure 2. Failure cases, grouped in 4 categories. Each one of the 4 column represent category of failure cases: for each category we show 5 examples, made of 3 images, namely the query and its top-2 predictions with MegaLoc, which can be in red or green depending if the prediction is correct (i.e. within 25 meters). The 4 categories that we identified are (1) very difficult cases, which are unlikely to be solved any time soon; (2) difficult cases, which can probably be solved by slightly better models than the current ones or simple postprocessing; (3) incorrect GPS labels, which, surprisingly, exist also in Mapillary and Google StreetView data; (4) predictions just out of the 25m threshold, which despite being considered negatives in VPR, are actually useful predictions for real-world applications."
        },
        {
            "title": "Method",
            "content": "NetVLAD AP-GeM CosPlace MixVPR EigenPlaces AnyLoc Salad CricaVPR CliqueMining MegaLoc (Ours) R-Oxford 16.1 37.6 23.4 28.4 22.9 45.5 42.3 39.2 41.0 79.0 4.7 19.3 10.3 10.8 11.8 18.9 21.4 15.3 22.1 62.1 24.1 49.6 32.1 38.2 29.4 64.2 55.2 57.0 52.2 91.0 R-Paris 46.3 69.5 45.0 48.3 47.3 68.5 66.2 68.9 60.5 89.6 22.0 45.5 22.3 25.0 23.6 48.8 44.8 48.9 41.2 77. 61.2 82.5 57.6 61.9 60.9 82.8 76.6 80.0 71.8 95.3 Table 3. Results on Landmark Retrieval datasets, respectively Revisited Paris 6k [25, 26] and Revisited Oxford 5k [24, 26]. 3.2.2. Visual Localization Image retrieval is core tool to solve 3D vision tasks, in pipelines like visual localization (e.g. Hierarchical Localization [28] and InLoc [35]) and 3D reconstructions (e.g. COLMAP [30, 31] and GLOMAP [22]). To understand if our method can help this use case, we compute results on the three datasets of Lamar [29], which comprise various challenges, including plenty of visual aliasing from both indoor and outdoor imagery. To do this, we relied on the official LaMAR codebase1 by simply replacing the retrieval method. Results are reported in Tab. 2. 3.2.3. Landmark Retrieval For the task of Landmark Retrieval we compute results on the most used datasets in literature, namely (the revisited versions of [26]) Oxford5k [24] and Paris6k [25]. To do this we relied on the official codebase for the datasets2, by simply swapping the retrieval method. Results, reported in Tab. 3, show large gap between MegaLoc and previous VPR models on this task, which can be simply explained by the fact that previous models were only optimized for the standard VPR metric of retrieving images within 25 meters from the query. 3.2.4. Failure Cases We identified series of 4 main categories of failure cases that prevent the results from reaching 100% recalls, and we present them in Fig. 2. We note however that, from practical perspective, the only real failure cases are depicted in the second category/column of Fig. 2: furthermore, in most similar cases SOTA models (i.e. not only MegaLoc, but also other recent ones) can actually retrieve precise predictions, meaning that these failure cases can be likely solvable by some simple post-processing techniques (e.g. reranking with image matchers, or majority voting). Finally, another failure case that we noted, is when database images 1https://github.com/microsoft/lamar-benchmark 2https://github.com/filipradenovic/revisitop this is very comdo not cover properly the search area: mon in the Mapillary (MSLS) dataset, where database images only show one direction (e.g. photos along road taken from north to south), while the queries are photos facing the other direction. We note however, that in the real world this can be easily solved by collecting database images in multiple directions, which is also common in most test datasets, like Eynsham, Pitts30k, Tokyo 24/7 and SF-XL. 4. Conclusion and limitations So, is image retrieval for localization solved? Well, almost. While some datasets still show some room for improvement, we note that this is often due to either arguably unsolvable failure cases, wrong labels, and very few cases that can be solved by better models. We emphasize however that this has been the case for some time, as previous DINO-v2based models, like SALAD and CliqueMining, show very high results on classic VPR datasets. What is still missing from literature is models like MegaLoc that achieve good results in variety of diverse tasks and domains. Should you always use MegaLoc? Well, almost, except for at least 3 use-cases. MegaLoc has shown great results on variety of related tasks, and, unlike other VPR models, achieves good results on landmark retrieval, which make it great option also for retrieval for 3D reconstruction tasks, besides standard VPR and visual localization tasks. However, experiments show that MegaLoc is outperformed by CliqueMining in MSLS, which is dataset made of (almost entirely) forward facing images (i.e. photos where the camera is facing the same direction of the street, instead of facing sideways towards the side of the street). Another use case where MegaLoc is likely to be suboptimal is in very unusual natural environments, like forests or caves, where instead AnyLoc has been shown to work well [17]. third and final use case where other models might be preferred to MegaLoc is for embedded systems, where one might opt for more lightweight models, like the ResNet-18 [15] versions of CosPlace [7], which has 11M parameters instead of MegaLocs 228M."
        },
        {
            "title": "References",
            "content": "[1] Amar Ali-bey, Brahim Chaib-draa, and Philippe Gigu`ere. Gsv-cities: Toward appropriate supervised visual place recognition. Neurocomputing, 513:194203, 2022. 2, 3 [2] Amar Ali-bey, Brahim Chaib-draa, and Philippe Gigu`ere. Mixvpr: Feature mixing for visual place recognition. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 29983007, 2023. 3 [3] R. Arandjelovic and Andrew Zisserman. Three things everyone should know to improve object retrieval. pages 2911 2918, 2012. 1 [4] Relja Arandjelovic, Petr Gronat, Akihiko Torii, Tomas Pajdla, and Josef Sivic. NetVLAD: CNN architecture for IEEE Transactions weakly supervised place recognition. on Pattern Analysis and Machine Intelligence, 40(6):1437 1451, 2018. 1, 3 [5] Giovanni Barbarani, Mohamad Mostafa, Hajali Bayramov, Gabriele Trivigno, Gabriele Berton, Carlo Masone, and Barbara Caputo. Are local features all you need for crossdomain visual place recognition? In CVPRW, pages 6155 6165, 2023. 3 [6] Fabio Bellavia, Jiri Matas, Dmytro Mishkin, Luca Morelli, Fabio Remondino, Weiwei Sun, Amy Tabb, Eduard Trulls, Kwang Moo Yi, Sohier Dane, and Ashley Chow. Image matching challenge 2024 - hexathlon. https:// kaggle.com/competitions/imagematchingchallenge-2024, 2024. Kaggle. [7] Gabriele Berton, Carlo Masone, and Barbara Caputo. Rethinking visual geo-localization for large-scale applications. In IEEE Conference on Computer Vision and Pattern Recognition, pages 48684878, 2022. 2, 3, 5 [8] Gabriele Berton, Riccardo Mereu, Gabriele Trivigno, Carlo Masone, Gabriela Csurka, Torsten Sattler, and Barbara Caputo. Deep visual geo-localization benchmark, 2023. 2, 3 [9] Gabriele Berton, Gabriele Trivigno, Barbara Caputo, and Carlo Masone. Eigenplaces: Training viewpoint robust models for visual place recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1108011090, 2023. 2, 3 [10] Gabriela Csurka, Christopher Dance, Lixin Fan, Jutta Willamowski, and Cedric Bray. Visual categorization with In European Conference on Computer bags of keypoints. Vision, 2004. 1 [11] Ekin Dogus Cubuk, Barret Zoph, Jon Shlens, and Quoc Le. Randaugment: Practical automated data augmentation with reduced search space. In Advances in Neural Information Processing Systems, pages 1861318624. Curran Associates, Inc., 2020. 3 [12] M. Cummins and P. Newman. Highly scalable appearanceonly slam - FAB-MAP 2.0. In Robotics: Science and Systems, 2009. 3 [13] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proc. Computer Vision and Pattern Recognition (CVPR), IEEE, 2017. [14] Petr Gronat, Guillaume Obozinski, Josef Sivic, and Toma Pajdla. Learning and calibrating per-location classifiers for visual place recognition. In 2013 IEEE Conference on Computer Vision and Pattern Recognition, pages 907914, 2013. 3 [15] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning In IEEE Conference on Computer for image recognition. Vision and Pattern Recognition, pages 770778, 2016. 5 [16] Sergio Izquierdo and Javier Civera. Optimal transport aggregation for visual place recognition. In IEEE Conference on Computer Vision and Pattern Recognition, 2024. 2, 3 [17] Nikhil Keetha, Avneesh Mishra, Jay Karhade, Krishna Murthy Jatavallabhula, Sebastian Scherer, Madhava Krishna, and Sourav Garg. Anyloc: Towards universal visual place recognition. arXiv, 2023. 3, 5 Ondrej Miksik, and Marc Pollefeys. LaMAR: Benchmarking Localization and Mapping for Augmented Reality. In ECCV, 2022. 3, 4 [30] Johannes Lutz Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In CVPR, 2016. 1, 4 [31] Johannes Lutz Schonberger, Enliang Zheng, Marc Pollefeys, and Jan-Michael Frahm. Pixelwise view selection for unstructured multi-view stereo. In ECCV, 2016. 4 [32] Johannes L. Schonberger, True Price, Torsten Sattler, JanMichael Frahm, and Marc Pollefeys. vote-and-verify strategy for fast spatial verification in image retrieval. In Computer Vision ACCV 2016, pages 321337, Cham, 2017. Springer International Publishing. 1 [33] Javier Civera Sergio Izquierdo. Close, but not there: Boosting geographic distance sensitivity in visual place recognition. In European Conference on Computer Vision (ECCV), 2024. 2, 3 [34] Xun Sun, Yuanfan Xie, Peiwen Luo, and Liang Wang. dataset for benchmarking image-based localization. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 56415649, 2017. 3 [35] Hajime Taira, Masatoshi Okutomi, Torsten Sattler, Mircea Cimpoi, Marc Pollefeys, Josef Sivic, Tomas Pajdla, and AkInLoc: Indoor visual localization with dense ihiko Torii. matching and view synthesis. In IEEE Conference on Computer Vision and Pattern Recognition, 2018. [36] A. Torii, R. Arandjelovic, J. Sivic, M. Okutomi, and T. Pajdla. 24/7 place recognition by view synthesis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(2): 257271, 2018. 3 [37] Joseph Tung, Gene Chou, Ruojin Cai, Guandao Yang, Kai Zhang, Gordon Wetzstein, Bharath Hariharan, and Noah Snavely. Megascenes: Scene-level view synthesis at scale. In ECCV, 2024. 2 [38] Xun Wang, Xintong Han, Weilin Huang, Dengke Dong, and Matthew Scott. Multi-similarity loss with general pair In Proceedings of the weighting for deep metric learning. IEEE Conference on Computer Vision and Pattern Recognition, pages 50225030, 2019. 2 [39] Frederik Warburg, Søren Hauberg, Manuel Lopezand Javier Antequera, Pau Gargallo, Yubin Kuang, Civera. Mapillary street-level sequences: dataset for lifelong place recognition. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 26232632, 2020. 2, 3 [40] Tobias Weyand, A. Araujo, Bingyi Cao, and Jack Sim. Google landmarks dataset v2 large-scale benchmark for instance-level recognition and retrieval. In IEEE Conference on Computer Vision and Pattern Recognition, pages 2572 2581, 2020. [18] Zhengqi Li and Noah Snavely. Megadepth: Learning singleIn Proceedview depth prediction from internet photos. ings of the IEEE conference on computer vision and pattern recognition, pages 20412050, 2018. 2 [19] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. 3 [20] Feng Lu, Xiangyuan Lan, Lijun Zhang, Dongmei Jiang, Yaowei Wang, and Chun Yuan. Cricavpr: Cross-image correlation-aware representation learning for visual place In Proceedings of the IEEE/CVF Conference recognition. on Computer Vision and Pattern Recognition (CVPR), 2024. 3 [21] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, ShangWen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2023. 3 [22] Linfei Pan, Daniel Barath, Marc Pollefeys, and Johannes Lutz Schonberger. Global Structure-from-Motion In European Conference on Computer Vision Revisited. (ECCV), 2024. 1, 4 [23] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32, pages 80248035. Curran Associates, Inc., 2019. [24] James Philbin, Ondrej Chum, Michael Isard, Josef Sivic, and Andrew Zisserman. Object retrieval with large vocabularies and fast spatial matching. In IEEE Conference on Computer Vision and Pattern Recognition. IEEE Computer Society, 2007. 4 [25] James Philbin, Ondrej Chum, Michael Isard, Josef Sivic, and Andrew Zisserman. Lost in quantization: Improving particular object retrieval in large scale image databases. In IEEE Conference on Computer Vision and Pattern Recognition, 2008. 4 [26] F. Radenovic, A. Iscen, G. Tolias, Y. Avrithis, and O. Chum. Revisiting oxford and paris: Large-scale image retrieval benchmarking. In CVPR, 2018. 4 [27] Jerˆome Revaud, Jon Almazan, R. S. Rezende, and Cesar Roberto de Souza. Learning with average precision: Training image retrieval with listwise loss. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 51065115, 2019. [28] Paul-Edouard Sarlin, Cesar Cadena, Roland Siegwart, and Marcin Dymczyk. From coarse to fine: Robust hierarchical localization at large scale. In CVPR, 2019. 1, 4 [29] Paul-Edouard Sarlin, Mihai Dusmanu, Johannes L. Schonberger, Pablo Speciale, Lukas Gruber, Viktor Larsson,"
        }
    ],
    "affiliations": [
        "Polytechnic of Turin"
    ]
}