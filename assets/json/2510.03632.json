{
    "paper_title": "MITS: Enhanced Tree Search Reasoning for LLMs via Pointwise Mutual Information",
    "authors": [
        "Jiaxi Li",
        "Yucheng Shi",
        "Jin Lu",
        "Ninghao Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Tree search has become as a representative framework for test-time reasoning with large language models (LLMs), exemplified by methods such as Tree-of-Thought and Monte Carlo Tree Search that explore multiple reasoning paths. However, it remains difficult to provide instant and reliable quantitative assessments of intermediate reasoning step quality, and extensive path exploration is computationally costly. To address this, we propose Mutual Information Tree Search (MITS), a novel framework that guides reasoning with information-theoretic principles. MITS introduces an effective scoring function based on pointwise mutual information (PMI), which enables step-wise evaluation of reasoning paths and search tree expansion via beam search without expensive look-ahead simulations, achieving superior reasoning performances while maintaining computational efficiency. The framework is complemented by an entropy-based dynamic sampling strategy that adaptively allocates computational resources to uncertain reasoning steps where exploration is most beneficial. For final prediction, MITS employs a weighted voting scheme that combines PMI scores with prediction consensus. Through comprehensive experiments on diverse reasoning benchmarks, MITS consistently surpasses baseline methods, establishing a principled and efficient framework for LLM reasoning."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 2 3 6 3 0 . 0 1 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "MITS: ENHANCED TREE SEARCH REASONING FOR LLMS VIA POINTWISE MUTUAL INFORMATION Jiaxi Li1 Yucheng Shi1 Jin Lu1 Ninghao Liu2 1University of Georgia 2The Hong Kong Polytechnic University"
        },
        {
            "title": "ABSTRACT",
            "content": "Tree search has become as representative framework for test-time reasoning with large language models (LLMs), exemplified by methods such as Tree-of-Thought and Monte Carlo Tree Search that explore multiple reasoning paths. However, it remains difficult to provide instant and reliable quantitative assessments of intermediate reasoning step quality, and extensive path exploration is computationally costly. To address this, we propose Mutual Information Tree Search (MITS), novel framework that guides reasoning with information-theoretic principles. MITS introduces an effective scoring function based on pointwise mutual information (PMI), which enables step-wise evaluation of reasoning paths and search tree expansion via beam search without expensive look-ahead simulations, achieving superior reasoning performances while maintaining computational efficiency. The framework is complemented by an entropy-based dynamic sampling strategy that adaptively allocates computational resources to uncertain reasoning steps where exploration is most beneficial. For final prediction, MITS employs weighted voting scheme that combines PMI scores with prediction consensus. Through comprehensive experiments on diverse reasoning benchmarks, MITS consistently surpasses baseline methods, establishing principled and efficient framework for LLM reasoning."
        },
        {
            "title": "INTRODUCTION",
            "content": "Complex multi-step reasoning remains fundamental challenge for Large Language Models (LLMs), particularly in tasks that require logical deduction, mathematical computation, or systematic problem-solving (Yang et al., 2025a; Zhu et al., 2024; Yi et al., 2024). While Chain-of-Thought (CoT) prompting (Wei et al., 2022; Kojima et al., 2022) has emerged as powerful technique to enhance reasoning by decomposing problems into intermediate steps, it typically generates single reasoning path, which may lead to incorrect solutions due to error accumulation or the selection of suboptimal reasoning strategies. This limitation becomes particularly pronounced in complex reasoning tasks where multiple valid approaches exist, but only specific paths lead to correct answers. Recent work on inference-time scaling law (Snell et al., 2024) reveals promising direction: generating multiple reasoning paths can substantially improve accuracy. The key insight is that solution coverage, i.e., the probability of finding at least one correct answer, scales predictably with the number of attempts. This observation has been proven valuable in domains with verifiable answers, such as code generation and theorem proving (Wei et al., 2025). However, exhaustively enumerating reasoning paths is infeasible, as the search space grows exponentially with problem complexity. This leads to the central research problem: How can we efficiently search through the vast space of possible reasoning paths to identify those most likely to yield correct solutions? To tackle this problem, existing approaches face several limitations. Methods based on exhaustive rollouts or Monte Carlo Tree Search (MCTS; Hao et al., 2023; Qi et al., 2025; Ding et al., 2024) require extensive forward simulation, which is computationally prohibitive at scale. Self-evaluation methods struggle to provide quantitative assessments of reasoning quality, often relying on pairwise comparisons or binary judgments that fail to capture nuanced differences between paths (Xie et al., 2023; Gu et al., 2024). Most critically, existing scoring mechanisms usually favor plausible but generic reasoning paths that could apply to many problems, rather than those specifically Equal Contribution Corresponding Author"
        },
        {
            "title": "Preprint",
            "content": "tailored to the given question (Sharma et al., 2024). These limitations highlight the need for principled framework that can (i) evaluate reasoning quality efficiently without costly simulations, and (ii) distinguish between generic reasoning and question-specific reasoning. In this work, we propose Mutual Information Tree Search (MITS), principled framework that addresses these challenges through information-theoretic guidance. Our key insight is that effective reasoning paths should exhibit high mutual information with the question, which means they should contain information that is both relevant and uniquely tied to solving the given problem. We leverage Pointwise Mutual Information (PMI) as scoring mechanism that quantifies how much reasoning paths plausibility increases because of the specific question, effectively filtering out generic or spurious reasoning patterns. Moreover, by computing scores dynamically as the reasoning process unfolds, MITS evaluates reasoning quality efficiently without expensive look-ahead simulations. Overall, our framework introduces three technical innovations. First, we propose principled scoring approach for intermediate reasoning steps using Pointwise Mutual Information (PMI), which reduces computational burden and establishes reliable criteria for trajectory selection. Second, we propose an entropy-based dynamic sampling strategy that adaptively allocates computational resources according to the uncertainty at each reasoning step, concentrating exploration on uncertain decision points where diverse reasoning approaches yield the greatest potential benefit. Third, we introduce weighted voting scheme that combines PMI scores with prediction consensus, reducing the risk of selecting high-scoring but spurious reasoning paths by utilizing agreement across multiple reasoning trajectories. Through extensive experiments across diverse reasoning benchmarks, we show that MITS achieves substantial improvements over several strong baseline methods. The remainder of this paper is organized as follows: Section 2 reviews background on tree search methods for LLM reasoning and current challenges, Section 3 presents our MITS framework in detail, Section 4 evaluates our approach on multiple reasoning benchmarks, Section 5 summarizes related work, and Section 6 gives conclusion, discusses broader implications and future directions."
        },
        {
            "title": "2 PRELIMINARY",
            "content": "Tree-Search Based Reasoning. To address reasoning tasks with large language models (LLMs), the problem is often framed as multi-step generation process with chain-of-thought (CoT) prompting (Wei et al., 2022; Kojima et al., 2022), where the model decomposes and solves the problem step by step. However, CoT is restricted to single reasoning trajectory, which limits exploration of the solution space. To overcome the limitation, more and more works adopt tree-based frameworks (Yao et al., 2023; Hao et al., 2023; Qi et al., 2025) that enable systematic exploration of multiple reasoning paths. Specifically, given reasoning problem q, we construct search tree by producing and sampling the nodes step by step, where the root node corresponds to the question and each child node is an intermediate reasoning step s. path from the root node to node sn(n 1) constitutes candidate solution path = s1 s2 sn, where the operator denotes the sequential concatenation. Within the search tree , set of solution paths = {S1, S2, . . . , Sc}(c 1) is extracted, from which we aim to find the most plausible path as the final answer. Challenges in Tree Search. Recently, within the tree-search paradigm (Yao et al., 2023) for LLM reasoning, Monte-Carlo Tree Search (MCTS) emerges as one of the most powerful and widely-used algorithms (Hao et al., 2023; Qi et al., 2025; Ding et al., 2024). However, evaluating the quality of intermediate reasoning steps in MCTS typically relies on repeated look-ahead rollouts to simulate future trajectories and back-propagate for value estimation, incurring substantial computational cost. In addition, prior work highlights the difficulty of defining reliable metric for selecting the correct trajectory (Qi et al., 2025). To this end, we ask whether we can design principled scoring function that can assess the quality of reasoning traces without simulated rollouts, thereby addressing both the computational efficiency and trajectory selection challenges simultaneously."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "This section introduces the proposed Mutual Information Tree Search (MITS) method, which constructs search tree and evaluates reasoning path quality through an effective mutual information based criterion, without relying on looking-ahead simulations. As illustrated in Figure 1, there are three major components as follows."
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Overview for the MITS approach. The search tree is incrementally constructed by generating reasoning steps conditioned on prior context. Dynamic sampling controls the number of candidates (i.e., nodes) generated at each step. Beam search is applied to prune less promising paths. Weighted average voting is used to aggregate candidates and select the final output. 1. Pointwise Mutual Information (PMI) Scoring. Given question and its reasoning path, our objective is to evaluate the paths quality. We employ Pointwise Mutual Information (PMI) as principled and effective scoring metric for intermediate reasoning steps (section 3.1), which then serves as the criterion for guiding search tree expansion. 2. Search Tree Construction. Using PMI as guidance, we build the search tree with the beam search technique as demonstrated in section 3.2. Furthermore, we develop dynamic sampling strategy, which allocates computation resources dynamically according to the uncertainty of the current step. 3. Weighted Average Voting for Output Selection. The search tree yields set of candidate reasoning chains, from which we make the final prediction decision as in section 3.3. Unlike standard majority voting (Wang et al., 2023), which treats all predictions equally and overlooks their relative strengths, we propose weighted average voting scheme that assigns weights to predictions according to their PMI scores, leading to more reliable answer selection. 3.1 POINTWISE MUTUAL INFORMATION (PMI) SCORING An effective scoring function for reasoning tasks should not only favor solutions that appear plausible, but also quantify how much specific information solution contributes to answering the question (Shi et al., 2024; Cui et al., 2025; Agarwal et al., 2025). In particular, the function should: (i) assign higher scores to solutions that are tightly coupled with the question, and (ii) penalize generic or spurious solutions that could apply broadly to unrelated queries. Mutual Information (MI) naturally satisfies these requirements, as it measures the reduction in uncertainty about solutions once questions are observed. Formally, the mutual information between questions and solutions is: I(S; q) = Ep(S,q) (cid:20) log p(S, q) p(S) p(q) (cid:21) , (1) where p(S, q) is the joint probability of and S; p(q) and p(S) are the marginal probability of the question and reasoning path, respectively. While MI captures the average dependence between questions and reasoning paths, the quantity inside the expectation, log p(S,q) p(S) p(q) , evaluates the contribution of particular question-reasoning pair. This term is known as the pointwise mutual information (PMI). Since p(S,q) p(q) = p(S q), the PMI score simplifies to PMI(q; S) = log p(S q) p(S) . (2) Here, PMI(q; S) satisfies the aforementioned requirements for an effective scoring function, because (i) the probability p(S q) measures the degree of match for the reasoning process given the question q, and (ii) the denominator p(S) penalizes generic paths that remain likely even without observing q. Probability Estimation. To compute the PMI score, we need to estimate the probabilities p(S q) and p(S) in Equation 2. Suppose reasoning path is currently expanded up to the n-th step"
        },
        {
            "title": "Preprint",
            "content": "(n > 1), comprising sequence of reasoning steps (s1, s2, . . . , sn). The conditional probability p(S q) is decomposed as p(S q) = (cid:81)n i=1 p(si q, s1, . . . , si1) and the marginal probability is p(S) = (cid:81)n i=1 p(si s1, . . . , si1). To estimate the conditional probabilities p(si q, s1, . . . , si1) and p(si s1, . . . , si1), we leverage the next-token prediction abilities of auto-regressive LLMs. Specifically, both probabilities are computed by multiplying token distributions within step si, with p(si q, s1, . . . , si1) conditioned on the full context including question q, and p(si s1, . . . , si1) conditioned only on previous reasoning steps. When = 1, p(S q) = p(s1 q), and p(S) = p(s1). In implementation, to approximate p(s1), we prepend the special Begin-of-Sequence (<bos>) token to the language model. PMI Scoring with Incremental Update. We are now able to calculate the PMI score given complete question and reasoning chain. However, to extend the reasoning process step-by-step in search tree, it is preferable to use an incremental update formula rather than re-computing PMI from the beginning at each step. Let Sn = (s1, s2, . . . , sn) denote the reasoning path at step n, and PMIn be the PMI score up to the n-th step. We can have PMIn := PMI(q; Sn) = (cid:88) i=1 (cid:2) log p(si q, s1, . . . , si1) log p(si s1, . . . , si1)(cid:3). (3) The detailed derivation can be found in Appendix A.1. We transform it into an additive form using log probabilities for ease of computation. Then, the incremental update for extending the reasoning path from step to step + 1 is given by: PMIn+1 = PMIn + (cid:2) log p(sn+1 q, s1, . . . , sn) log p(sn+1 s1, . . . , sn)(cid:3), (4) with the initial condition PMI0 = 0, i.e., the PMI score is zero before any reasoning step is generated. In this way, each update captures the incremental information gain contributed by the new step sn+1 toward answering the question q, beyond what has already been accounted for by the preceding steps. This not only enables efficient computation during reasoning, but also provides an interpretable view of how individual steps contribute to problem-solving. 3.2 SEARCH TREE CONSTRUCTION In this subsection, we introduce how to build the reasoning search tree using the PMI criterion. We use generator model to produce each step and an evaluator model to compute PMI scores. We first introduce how to generate each reasoning step, then the construction of the reasoning tree , which couples two mechanisms: (1) dynamic sampling that decides how many next-step candidates to propose; (2) PMI-guided beam search that evaluates and prunes candidates to retain only the most promising paths for further expansion. In short, dynamic sampling produces candidates, and beam search selects which of them advance to the next level of the tree. Reasoning Step Generation. To the tree, fundamenconstruct tal operation is to generate intermediate reasoning steps. At step + 1, the generator model produces multiple candidate steps conditioned on the question and the partial reasoning path (s1, . . . , sn). As shown in Figure 2, we format generation with CoT-style instruction (e.g., \"Lets think step by step\") and delimit each step using special marker (e.g., \"Step\"), so that produces one step at time before returning control for evaluation by evaluator model E. The first set of candidates is conditioned solely on q, while later candidates are conditioned on both and the preceding reasoning context. Figure 2: Prompt examples for step-by-step reasoning generation, showing the iterative generation from initial step to subsequent steps, similar to (Lai et al., 2024)."
        },
        {
            "title": "Preprint",
            "content": "Dynamic Sampling via Entropy Quantification. Through reasoning step generation, we can produce multiple candidate steps conditioned on the previous one. However, how many candidate steps to sample requires careful consideration to optimize test-time compute (Singhi et al., 2025; Snell et al., 2024). In particular, more computational resources should be allocated to reasoning branches with higher uncertainty, since they offer greater potential for generating and exploring diverse paths. To this end, we use entropy to quantify uncertainty and sample next-step candidates dynamically. For reasoning step si, its entropy is computed over the token distribution as: Hi = (cid:80) vV pi(v) log pi(v), (5) where is the vocabulary and pi(v) denotes the probability of the token at step i. To adjust sampling adaptively, we divide the entropy into three intervals: high, moderate, and low. However, it is hard to use fixed thresholds to divide them, because entropy values can fluctuate given different tasks or different questions due to varying difficulty. To address this, we maintain history of entropies for all generated steps H1:m = {H1, H2, ..., Hm}, where is the total number of nodes in the current search tree, and use empirical quantiles to compute adaptive thresholds. Specifically, we set Hlow = percentile(H1:m, 33%) and Hhigh = percentile(H1:m, 67%), which automatically partition entropy values into three regions: low (Hi < Hlow), moderate (Hlow Hi Hhigh), and high (Hi > Hhigh). Unlike fixed cutoffs, these quantile-based thresholds adapt to the varying difficulty across tasks and questions. Finally, the number of samples for step + 1 is adjusted based on the entropy region. The final sampling number is set to Nbase + Ni, where Nbase is the default sample size and Ni is determined by the entropy level. The complete formulation for computing Ni is given in Appendix A.2. Beam Search with PMI. Once candidate steps are generated, the evaluator model computes their incremental PMI contributions, and updates the cumulative score to PMIn+1 according to Equation 4. Since generating and evaluating all possible paths quickly becomes computationally expensive, we employ beam search pruning to ensure efficiency. Candidates are ranked by their cumulative PMI scores, and only the top-B paths are retained for the next step, where denotes the beam width. This pruning keeps the tree computationally tractable while focusing exploration on the most informative reasoning directions. By tightly coupling candidate generation with PMI-based pruning, the method balances diversity and quality throughout the tree search. 3.3 WEIGHTED AVERAGE VOTING FOR OUTPUT SELECTION After building the search tree , we need to select the final output from the collected reasoning chains = {S1, S2, . . . , Sc}, where is the total number of candidate chains. straightforward approach is to choose the chain with the highest PMI score. However, this can be brittle: the top-scoring chain may arise from spurious correlations between the query and particular reasoning steps (Zhao et al., 2024; Shao et al., 2025; Xie et al., 2025), or from overfitting to specific linguistic patterns (Wu et al., 2025; Li et al., 2025), leading to overconfident yet incorrect predictions. To mitigate this risk, we propose Weighted Average Voting, simple but effective post-hoc operation by incorporating prediction frequency as weight on PMI scores, as consensus among multiple reasoning paths often indicates more reliable conclusions. Formally, given the reasoning chains, we first sort them according to PMI scores from high to low, and then select the top chains: Stop-K = {S1, S2, . . . , SK}. For each chain Stop-K, we extract its final prediction Pred(S). The frequency of each unique prediction is then computed as: Freq(p) = {S Stop-K : Pred(S) = p}. After that, we can compute the frequency-weighted PMI score (PMI) for each reasoning chain: PMI(q; S) = PMI(q; S) Freq(Pred(S)) . (6) We re-rank the top chains according to PMI(q; S) and select the chain with the highest one as our final prediction: = arg maxSStop-K PMI(q; S). This reweighting mechanism provides regularization effect and achieves balance between confidence and consensus."
        },
        {
            "title": "Preprint",
            "content": "Table 1: Accuracies of MITS compared with several baselines on different datasets. Columns are grouped by LLM backbones. Best results are in bold, and second best results are underlined. Method QWEN2.5-3B QWEN2.5-7B PHI-3.5-MINI PHI-4-MINI CoT CoT-SC ToT RAP rStar MITS-F MITS CoT CoT-SC ToT RAP rStar MITS-F MITS CoT CoT-SC ToT RAP rStar MITS-F MITS 47.34 56.12 49.86 60.56 65.32 67.84 68.45 69.11 81.16 85.45 83.96 86.74 87.45 90.68 66.67 74.72 75.56 73.24 79.34 79.68 78. StrategyQA 65.47 68.74 66.54 68.14 70.41 74.73 75.76 ARC-Challenge 71.50 81.02 85.86 83.45 87.24 93.45 92.55 CommonsenseQA 66.50 76.52 79.32 82.23 85.73 86.25 84.80 54.47 56.71 54.12 57.36 62.57 65.75 66.47 67.52 77.95 77.86 83.56 86.84 90.54 90. 55.86 67.93 66.49 73.23 77.65 79.56 78.14 51.25 55.47 57.41 59.45 64.89 70.45 68.19 72.25 82.48 83.56 85.48 87.74 91.56 90. 65.71 69.82 72.56 75.84 79.41 81.49 80."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 SETUP Backbone models and datasets. MITS is general algorithm and applicable to various LLMs and different reasoning tasks. We conduct main experiments on the LLMs QWEN2.5-3BINSTRUCT, QWEN2.5-7B-INSTRUCT (Team, 2024), PHI-3.5-MINI-INSTRUCT and PHI-4-MINIthe INSTRUCT (Abdin et al., 2024). All used models are instruction-tuned, and we omit -Instruct suffix for brevity in the following sections and tables. We perform experiments on three datasets, StrategyQA (Geva et al., 2021) of questions that require implicit multi-hop strategies to answer, ARC-Challenge (Clark et al., 2018), which represents scientific knowledge-intensive reasoning task, and CommonsenseQA (Talmor et al., 2019) of commonsense questions that require prior world knowledge to answer. Implementation details. When calculating the PMI score (3), we observe that single-step PMI value tends to be positive, i.e., p(sn+1 q, s1, . . . , sn) > p(sn+1 s1, . . . , sn), which induces length bias whereby longer reasoning chains tend to yield larger PMI scores. To mitigate this effect, we normalize PMI through multiplying PMIn by factor 1 . In the stage of search tree construction, the maximum depth of the tree is set to 10, and the default sampling number Nbase = 3. The evaluator model is set as same with the generator model by default. We set the beam width at 32 for the beam search process of MITS. We also run MITS-F, the variant algorithm of MITS by fully expanding the search tree without beam search, which allows us to analyze the trade-off between computational efficiency and reasoning performance. We set = 32 for weighted majority voting for both of MITS and MITS-F. To guarantee reproducible inference process, we searched set of hyperparameters, which is detailed in Appendix B.1."
        },
        {
            "title": "4.2 MAIN RESULTS",
            "content": "Baselines. We compare MITS with three types of baselines: single CoT prompting, multiple CoT sampling and tree-search methods. Since MITS only leverages inference phase of language models, (1) For single CoT prompting, we use we focus on training-free baselines for fair comparison. CoT prompting (Wei et al., 2022; Kojima et al., 2022). (2) For multiple CoT sampling, we choose the widely adopted CoT with self-consistency method (CoT-SC; Wang et al., 2023), and employ majority voting to select the final prediction. We report the performances of sampling 32 times. (3) For tree-search methods, we include three strong baseline methods, Tree-of-thought (ToT; Yao et al., 2023), RAP (Hao et al., 2023) and rStar (Qi et al., 2025). We run ToT with Breath-First Search (BFS) to expand the tree. RAP and rStar utilize Monte-Carlo Tree Search algorithm (MCTS; Kocsis & Szepesvári, 2006) for tree search process, with 32 rollouts performed for each method. Performances on diverse reasoning datasets. Here we present the performances of MITS against several baselines on diverse reasoning datasets in Table 1. We start by evaluating the effectiveness of MITS on general reasoning benchmarks. We highlight three observations. (1) MITS demonstrates substantial improvements over previous approaches across diverse tasks. For example, on StrategyQA dataset with QWEN2.5-3B, MITS achieves 68.45% accuracy compared to CoTs 47.34%, representing remarkable +21.11% improvement. Similarly, on ARC-Challenge with QWEN2.57B, MITS reaches 92.55% accuracy, significantly outperforming the strongest baseline rStar by +5.31%. This substantial gain indicates that PMI-based scoring provides more effective guidance for reasoning path selection. (2) MITS consistently outperforms existing methods across different model scales and reasoning domains. Unlike baseline methods that exhibit varying effectiveness across different tasks, MITS maintains strong results across logical reasoning (StrategyQA), scientific reasoning (ARC-Challenge), and commonsense reasoning (CommonsenseQA). For instance, while RAP achieves competitive results on some tasks, it falls significantly behind MITS on StrategyQA (60.56% vs 68.45% on QWEN2.5-3B). (3) The comparison between MITS and MITS-F reveals the value of beam search in reasoning. While MITS-F shows improvements on CommonsenseQA, it generally underperforms MITS on StrategyQA (e.g., 74.73% vs 75.76% on QWEN2.57B). This indicates that PMI-based beam search can enhance performance by filtering out inferior reasoning paths early, preventing exploration from being derailed by low-quality branches. Computational efficiency comparison. To validate the computational efficiency of MITS, we conduct experiments on the StrategyQA dataset using QWEN2.5-3B, comparing against CoT-SC, RAP, and rStar under identical inference settings. We measure wall-clock time from inference start to reasoning completion and report the average time that each method needs to solve problem. Table 2: Compute efficiency comparison. Table 2 presents the results, revealing significant efficiency advantages for MITS. While CoT-SC achieves the fastest time (2.75s), it delivers the lowest accuracy (56.12%). MCTS-based methods RAP and rStar require substantially more computation time (203.42s and 815.67s respectively, which are 3.2 and 12.7 slower than MITS) with limited accuracy improvements (60.56% and 65.32%). In contrast, MITS achieves the highest accuracy (68.45%) with moderate computational cost (64.41s), demonstrating the optimal accuracy-per-time trade-off among all methods. These results validate our claim that PMI-based scoring achieves superior reasoning performance with significantly reduced computational overhead compared to previous methods. CoT-SC RAP rStar MITS 2.75 203.42 815.67 64.41 Method Time (s) Acc. (%) 56.12 60.56 65.32 68.45 4.3 ABLATION STUDY Effectiveness under different number of rollouts. MCTS-based baseline methods such as RAP and rStar construct the search tree through rollout policy of MCTS algorithm. Increasing the number of rollouts expands the set of candidate solution trajectories and can lead to performance improvements, but it also incurs higher inference costs. Here we compare the accuracies of CoTSC, RAP, rStar and MITS under different number of rollouts on the ARC-Challenge dataset as shown in Figure 3. For CoT-SC and MITS, we define number of rollouts same as the number of reasoning chains finally collected. We highlight two key findings: (1) With as few as two rollouts,"
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Performance comparison across different rollouts on the ARC-Challenge dataset. MITS not only demonstrates superior performance in early stages with fewer rollouts, but also continues to improve with increased computational budget. Table 3: Results on ARC-Challenge with different pairs of generator and evaluator models. Table 4: Results on the StrategyQA dataset with model QWEN2.5-3B and PHI-3.5-MINI using different aggregations for PMI scoring. Generator Evaluator QWEN2.5-3B QWEN2.5-7B QWEN2.5-1.5B QWEN2.5-3B QWEN2.5-7B 86.78 89.86 90.61 87.20 88.24 91.68 Models Aggregation QWEN2.5-3B PHI-3.5-MINI Sum Average 65.89 68.45 63.26 66.68 MITS achieves notable improvements in reasoning accuracy, underscoring the effectiveness of PMIbased scoring; (2) While rStar and CoT-SC exhibit continued performance gains with additional rollouts, RAP occasionally shows performance degradation as the number of rollouts increases. We hypothesize that this degradation stems from RAPs limited action space, which constrains the potential of its MCTS framework. Effectiveness of different evaluators. As we coordinate two models generator and evaluator for MITS, how different evaluators impact the PMI score calculation process are under explored. Therefore, we conduct experiments on the ARC-Challenge dataset using Qwen series models as evaluators, with sizes ranging from 1.5B to 7B, as shown in Table 3. From the table we can observe that more powerful evaluator models usually yield better results. For example, take QWEN2.53B as the generator model, QWEN2.5-1.5B as evaluator can only has accuracy of 86.78%, while QWEN2.5-7B can have performance of 90.61%. This is likely because stronger evaluators provide more accurate probability estimates, leading to more reliable PMI values. Effectiveness of Weighted Average Voting. We conduct the ablation study on the Weighted Average Voting technique introduced in Section 3.3, evaluating the impact of selecting different numbers of top-scoring reasoning paths for final answer selection. Using model QWEN2.5-3B, we run MITS with beam width of 32 across all three datasets and vary from 1 to 32. The corresponding performance results are reported in Table 5. The results show consistent performance improvements as increases, with the steepest gains observed at small values (e.g., ARC-Challenge improves from 84.31% to 89.12% as increases from 1 to 8). Optimal values are dataset-dependent: StrategyQA peaks at = 16 while ARC-Challenge and CommonsenseQA achieve best results at = 32. Performance stabilizes as value becomes larger, indicating diminishing returns from incorporating additional reasoning paths. Ablation on different PMI variants. In section 4.1, we mention to normalize PMI by averaging its accumulative value over the total steps to mitigate the length bias. Therefore, we conduct an ablation study comparing these two PMI aggregation approaches: (1) Sum aggregation, which accumulates PMI values across all reasoning steps, and (2) Average aggregation, which divides the cumulative PMI score by the number of steps. Table 4 presents results on the StrategyQA dataset with models QWEN2.5-3B and PHI-3.5-MINI. The results show that The average-aggregated (length-"
        },
        {
            "title": "Preprint",
            "content": "Table 5: Performance across different values of top-K for weighted majority voting with QWEN2.53B. Best results for each dataset are in bold. Dataset Top-K 1 2 8 16 32 StrategyQA ARC-Challenge CommonsenseQA 63.34 84.31 74.78 65.41 85.94 75. 67.44 87.24 76.64 68.04 89.12 77.02 68.86 89.86 78.49 68.23 90.68 78.83 normalized) PMI consistently outperforms the sum-based approach, with improvements of +2.56% on QWEN2.5-3B and +3.42% on PHI-3.5-MINI. This demonstrates that normalizing by path length prevents certain reasoning paths from being unfairly penalized, enabling fair comparison across paths of different lengths, which promotes the performance simultaneously."
        },
        {
            "title": "5 RELATED WORK",
            "content": "Prompting for LLM Reasoning. Since the introduction of Chain-of-Thought prompting (Kojima et al., 2022; Wei et al., 2022), the field has witnessed rapid development of prompting methods which focus on designing instructions and pipelines to elicit the potential reasoning capabilities within LLMs. Recent work make progress through several ways, such as decomposing question into sub-questions to guide the reasoning process (Zhou et al., 2022; Khot et al., 2023; Yang et al., 2023; Zhou et al., 2025) and demonstration selection (Zhang et al., 2023; Shum et al., 2023; Diao et al., 2024; Shi et al., 2025; Yang et al., 2025b), which aims to select high-quality exemplars for better prompting performances. Higher-order thinking approaches, exemplified by abstract and analogical reasoning (Webb et al., 2023; Zheng et al., 2024; Yasunaga et al., 2024; Yu et al., 2024; Liu et al., 2025), demonstrate effectiveness as well, compared to concrete demonstrations appended in prompts. Other approaches also include code-integrated prompting (Gao et al., 2023; Chen et al., 2023; Surís et al., 2023; Li et al., 2024; Zhou et al., 2024), which makes LLMs reason by coding, leading to more structured way to express logical thinking processes. These methods aim to improve single-round inference performance and are orthogonal to ours. Tree Search for LLM Reasoning. Recent advancements have shown that sampling diverse reasoning paths can significantly enhance performances compared to one-time greedy decoding in both theoretical and empirical ways (Brown et al., 2024; Snell et al., 2024), featured by Chain-of-Thought with Self-Consistency (CoT-SC) (Wang et al., 2023). Taking one step further, more work guide the reasoning process into tree structures (Yao et al., 2023; Xie et al., 2023), among which the most pioneering work is Tree-of-Thought (Yao et al., 2023). There are also works such as building search tree with beam search (Xie et al., 2023). Recently, Monte-Carlo Tree Search (MCTS) is regarded powerful algorithm for advancing LLM reasoning, especially from the perspective of inferencetime scaling (Hao et al., 2023; Qi et al., 2025; Ding et al., 2024). Moreover, tree structures are also deployed to help LLMs in planning tasks (Hu et al., 2024; Gui et al., 2025)."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we proposed Mutual Information Tree Search (MITS), principled framework that leverages pointwise mutual information (PMI) to guide LLM reasoning without expensive lookahead simulations. MITS introduces three key innovations: PMI-based scoring for reasoning path evaluation, entropy-based dynamic sampling for adaptive compute allocation, and weighted average voting for robust answer selection. Through comprehensive experiments across diverse reasoning datasets, MITS substantially outperforms several strong baseline methods by large margin. These results demonstrate that information-theoretic principles can effectively steer LLM reasoning processes, achieving superior performances across diverse reasoning tasks while providing computationally efficient algorithm to traditional tree search methods. Furthermore, we conduct extensive ablation studies, providing analysis and insights for more advanced LLM reasoning techniques."
        },
        {
            "title": "REFERENCES",
            "content": "Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. Shivam Agarwal, Zimin Zhang, Lifan Yuan, Jiawei Han, and Hao Peng. The unreasonable effectiveness of entropy minimization in llm reasoning. arXiv preprint arXiv:2505.15134, 2025. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc Le, Christopher Ré, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/ forum?id=YfZ4ZPt8zd. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. URL https://arxiv.org/abs/1803.05457. Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, et al. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617, 2025. Shizhe Diao, Pengcheng Wang, Yong Lin, Rui Pan, Xiang Liu, and Tong Zhang. Active prompting with chain-of-thought for large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 13301350, 2024. Ruomeng Ding, Chaoyun Zhang, Lu Wang, Yong Xu, Minghua Ma, Wei Zhang, Si Qin, Saravan Rajmohan, Qingwei Lin, and Dongmei Zhang. Everything of thoughts: Defying the law of penrose triangle for thought generation. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics: ACL 2024, pp. 16381662, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024. findings-acl.95. URL https://aclanthology.org/2024.findings-acl.95/. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 1076410799. PMLR, 2329 Jul 2023. URL https://proceedings.mlr.press/v202/gao23f.html. PAL: Program-aided language models. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle use laptop? question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346361, 2021. doi: 10.1162/tacl_a_00370. URL https://aclanthology.org/2021.tacl-1.21/. Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, et al. survey on llm-as-a-judge. arXiv preprint arXiv:2411.15594, 2024. Runquan Gui, Zhihai Wang, Jie Wang, Chi Ma, Huiling Zhen, Mingxuan Yuan, Jianye HAO, Defu Lian, Enhong Chen, and Feng Wu. Hypertree planning: Enhancing LLM reasoning via hierarIn Forty-second International Conference on Machine Learning, 2025. URL chical thinking. https://openreview.net/forum?id=45he3Ri6JP. Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy Wang, and Zhiting Hu. Reasoning In Houda Bouamor, Juan Pino, and Kawith language model is planning with world model. lika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 81548173, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.507. URL https://aclanthology.org/2023. emnlp-main.507/."
        },
        {
            "title": "Preprint",
            "content": "Mengkang Hu, Yao Mu, Xinmiao Chelsey Yu, Mingyu Ding, Shiguang Wu, Wenqi Shao, Qiguang Chen, Bin Wang, Yu Qiao, and Ping Luo. Tree-planner: Efficient close-loop task planning with large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=Glcsog6zOe. Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. Decomposed prompting: modular approach for solving complex tasks. In The Eleventh International Conference on Learning Representations, 2023. URL https: //openreview.net/forum?id=_nGgzQjzaRy. Levente Kocsis and Csaba Szepesvári. Bandit based monte-carlo planning. volume 2006, pp. 282 293, 09 2006. ISBN 978-3-540-45375-8. doi: 10.1007/11871842_29. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213, 2022. Xin Lai, Zhuotao Tian, Yukang Chen, Senqiao Yang, Xiangru Peng, and Jiaya Jia. Step-dpo: Stepwise preference optimization for long-chain reasoning of llms. arXiv preprint arXiv:2406.18629, 2024. Chengshu Li, Jacky Liang, Andy Zeng, Xinyun Chen, Karol Hausman, Dorsa Sadigh, Sergey Levine, Li Fei-Fei, Fei Xia, and Brian Ichter. Chain of code: Reasoning with language modelaugmented code emulator. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (eds.), Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp. 2825928277. PMLR, 2127 Jul 2024. URL https://proceedings.mlr. press/v235/li24ar.html. Jiaxi Li, Yiwei Wang, Kai Zhang, Yujun Cai, Bryan Hooi, Nanyun Peng, Kai-Wei Chang, and Jin Lu. Fact or guesswork? evaluating large language models medical knowledge with structured one-hop judgment. arXiv preprint arXiv:2502.14275, 2025. Wenqi Liu, Xuemeng Song, Jiaxi Li, Yinwei Wei, Na Zheng, Jianhua Yin, and Liqiang Nie. Mitigating hallucination through theory-consistent symmetric multimodal preference optimization. arXiv preprint arXiv:2506.11712, 2025. Zhenting Qi, Mingyuan MA, Jiahang Xu, Li Lyna Zhang, Fan Yang, and Mao Yang. Mutual reasoning makes smaller LLMs stronger problem-solver. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id= 6aHUmotXaw. Rulin Shao, Shuyue Stella Li, Rui Xin, Scott Geng, Yiping Wang, Sewoong Oh, Simon Shaolei Du, Nathan Lambert, Sewon Min, Ranjay Krishna, et al. Spurious rewards: Rethinking training signals in rlvr. arXiv preprint arXiv:2506.10947, 2025. Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel R. Bowman, Esin DURMUS, Zac Hatfield-Dodds, Scott Johnston, Shauna Kravec, Timothy Maxwell, Sam McCandlish, Kamal Ndousse, Oliver Rausch, Nicholas Schiefer, Da Yan, Miranda Zhang, and Ethan Perez. Towards understanding sycophancy in language models. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=tvhaxkMKAn. Yucheng Shi, Qiaoyu Tan, Xuansheng Wu, Shaochen Zhong, Kaixiong Zhou, and Ninghao Liu. Retrieval-enhanced knowledge editing in language models for multi-hop question answering. In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management, pp. 20562066, 2024. Yucheng Shi, Quanzheng Li, Jin Sun, Xiang Li, and Ninghao Liu. Enhancing cognition and explainability of multimodal foundation models with self-synthesized data. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/ forum?id=lHbLpwbEyt."
        },
        {
            "title": "Preprint",
            "content": "Kashun Shum, Shizhe Diao, and Tong Zhang. Automatic prompt augmentation and selection with chain-of-thought from labeled data. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 1211312139, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. findings-emnlp.811. URL https://aclanthology.org/2023.findings-emnlp. 811/. Nishad Singhi, Hritik Bansal, Arian Hosseini, Aditya Grover, Kai-Wei Chang, Marcus Rohrbach, and Anna Rohrbach. When to solve, when to verify: Compute-optimal problem solving and generative verification for llm reasoning. arXiv preprint arXiv:2504.01005, 2025. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. Dídac Surís, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 1188811898, October 2023. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 41494158, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1421. URL https://aclanthology.org/N19-1421/. Qwen Team. Qwen2.5: party of foundation models, September 2024. URL https://qwenlm. github.io/blog/qwen2.5/. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language In The Eleventh International Conference on Learning Representations, 2023. URL models. https://openreview.net/forum?id=1PL1NIMMrw. Taylor Webb, Keith Holyoak, and Hongjing Lu. Emergent analogical reasoning in large language models. Nature Human Behaviour, 7(9):15261541, 2023. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Yuxiang Wei, Olivier Duchenne, Jade Copet, Quentin Carbonneaux, Lingming Zhang, Daniel Fried, Gabriel Synnaeve, Rishabh Singh, and Sida Wang. Swe-rl: Advancing llm reasoning via reinforcement learning on open software evolution. arXiv preprint arXiv:2502.18449, 2025. Xuansheng Wu, Wenhao Yu, Xiaoming Zhai, and Ninghao Liu. Self-regularization with latent space explanations for controllable llm-based classification. arXiv preprint arXiv:2502.14133, 2025. Hongyan Xie, Yitong Yao, Yikun Ban, Zixuan Huang, Deqing Wang, Zhenhe Wu, Haoxiang Su, Chao Wang, Shuangyong Song, and Xuelong Li. Mitigating spurious correlations between question and answer via chain-of-thought correctness perception distillation. arXiv preprint arXiv:2509.05602, 2025. Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, James Xu Zhao, Min-Yen Kan, Junxian He, and Michael Xie. Self-evaluation guided beam search for reasoning. Advances in Neural Information Processing Systems, 36:4161841650, 2023. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, arXiv preprint Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv:2505.09388, 2025a. Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. In The Twelfth International Conference on Learning Representations, 2023."
        },
        {
            "title": "Preprint",
            "content": "Tianze Yang, Yucheng Shi, Mengnan Du, Xuansheng Wu, Qiaoyu Tan, Jin Sun, and Ninghao Liu. In Forty-second Concept-centric token interpretation for vector-quantized generative models. International Conference on Machine Learning, 2025b. URL https://openreview.net/ forum?id=sw7ML1IOkk. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822, 2023. Michihiro Yasunaga, Xinyun Chen, Yujia Li, Panupong Pasupat, Jure Leskovec, Percy Liang, Ed H. Chi, and Denny Zhou. Large language models as analogical reasoners. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/ forum?id=AgDICX1h50. Zihao Yi, Jiarui Ouyang, Yuwen Liu, Tianhao Liao, Zhe Xu, and Ying Shen. survey on recent advances in llm-based multi-turn dialogue systems. arXiv preprint arXiv:2402.18013, 2024. Junchi Yu, Ran He, and Zhitao Ying. THOUGHT PROPAGATION: AN ANALOGICAL APIn The PROACH TO COMPLEX REASONING WITH LARGE LANGUAGE MODELS. Twelfth International Conference on Learning Representations, 2024. URL https:// openreview.net/forum?id=SBoRhRCzM3. Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=5NTt8GFjUHkr. Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, and Mengnan Du. Explainability for large language models: survey. ACM Transactions on Intelligent Systems and Technology, 15(2):138, 2024. Huaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen, Heng-Tze Cheng, Ed H. Chi, Quoc Le, and Denny Zhou. Take step back: Evoking reasoning via abstraction in large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=3bq3jsvcQ1. Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, and Hongsheng Li. Solving challenging math word problems using GPT-4 code interpreter with code-based self-verification. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id= c8McWs4Av0. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. Least-to-most prompting enables complex reasoning in large language models. In The Eleventh International Conference on Learning Representations, 2022. Shuang Zhou, Wenya Xie, Jiaxi Li, Zaifu Zhan, Meijia Song, Han Yang, Cheyenna Espinoza, Lindsay Welton, Xinnie Mai, Yanwei Jin, et al. Automating expert-level medical reasoning evaluation of large language models. arXiv preprint arXiv:2507.07988, 2025. Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Shujian Huang, Lingpeng Kong, Jiajun Chen, and Lei Li. Multilingual machine translation with large language models: Empirical results and analysis. In Findings of the Association for Computational Linguistics: NAACL 2024, pp. 2765 2781, 2024."
        },
        {
            "title": "A ALGORITHM DETAILS IN MITS",
            "content": "A.1 DETAILED DERIVATION FOR PMI SCORING In section 3.1, we obtain the recurrent form for the calculation of PMI scores. Here in this appendix subsection, we derive and provide the calculation process for PMI score in recurrence relation, starting from Equation 2. PMI(q; S) = log = log p(S q) p(S) (cid:89) p(si q, s1, . . . , si1) p(si s1, . . . , si1) p(si q, s1, . . . , si1) p(si s1, . . . , si1) (cid:88) i=1 log = = i=1 (cid:88) [log p(si q, s1, . . . , si1) log p(si s1, . . . , si1)]. Therefore, we have the recurrence relation for PMI score calculation presented in Equation 3. i=1 A.2 FORMULA FOR DYNAMIC SAMPLING In section 3.2, building upon the entropy-based partitioning described in the main text, we present the detailed mathematical formulation for computing the sampling adjustment Ni. The proportional control scheme adjusts sampling intensity based on how far the current entropy deviates from the moderate range: Ni = (cid:18) (cid:22) min 2, 2 Vp Hi Hhigh Hhigh Hlow (cid:25)(cid:19) , 0, (cid:18) (cid:22) max 2, 2 Vp (cid:25)(cid:19) Hlow Hi Hhigh Hlow Hi > Hhigh, Hlow Hi Hhigh, (7) , Hi < Hlow. Here Vp is the proportional gain parameter. The normalization factor (Hhigh Hlow) ensures that the adjustment magnitude is scaled relative to the entropy range, making the controller robust to different entropy scales. The final number of samples is computed with boundary constraints Ni = Nbase + Ni, where Nbase = 3 represents the base sample count. In practice, we enable dynamic sampling only when 10, ensuring that sufficiently large number of samples has already been accumulated."
        },
        {
            "title": "B IMPLEMENTATION DETAILS",
            "content": "B.1 HYPERPARAMETERS We conduct comprehensive hyperparameter search to ensure stable and high-quality performances of the MITS algorithm. Here we provide the key parameters during the inference of LLMs. Specifically, we mainly record two groups of hyper-parameters, parameters for basic model inference and parameters for search tree construction. As displayed in Table 6, for critical parameters of basic model inference, we search set of parameters of temperature, top-k, and top-p for model decoding. For search tree construction, the sample number is set to 3, which means by default we sample 3 candidate next-steps given previous steps, as illustrated in section 3. The maximum depth of the search tree is 10. For generation of each candidate step, we set the length limit of 512 tokens. For beam width during the MITS process, we set it to 32."
        },
        {
            "title": "Preprint",
            "content": "Table 6: Hyperparameter Configuration for MITS. Parameter Value Parameters for Basic Inference 0.9 100 0.96 temperature top-k top-p Parameters for Search Tree Construction sample numbers max depth max new tokens beam width 3 10"
        },
        {
            "title": "C CASE STUDY",
            "content": "In this appendix section, we provide some case studies on the datasets StrategyQA, ARC-Challenge, and CommonsenseQA with the backbone model QWEN2.5-3B (both for generator model and evaluator model E). Specifically, we provide the examples in json format and here are the detailed information: \"index\" refers to the index of node (reasoning step) in the built search tree, \"depth\" denotes the depth (level) of the node in the tree, and \"content\" denotes the content of the reasoning step generated by LLMs. To calculate the PMI score, we record the values of conditional probabilities log p(si q, s1, . . . , si1) and log p(si s1, . . . , si1) of step i, and denote them as log p(S q) and log p(S) for brevity, respectively. For the PMI score, we denote it as \"PMI\" and calculate it by simply subtracting two log probabilities log p(S q) log p(S). In the following subsections, we give case studies one by one for each dataset. C.1 STRATEGYQA Prompt. Here is an example prompt with question. You need to answer the question with true or false. Lets think this question step by step. Finally, please make decision from the options or B. When you believe youve reached the final step, please output your final answer in square brackets like [A. True] or [B. False]. Question: Is CAS number 8009-03-8 harmful for rash? A. True B. False Step 1: Reasoning Path. Here is one of the extracted reasoning paths that correctly solves the problem. [ { }, { \"index\": 3, \"depth\": 1, \"content\": \"Identifying CAS Number. CAS Number 8009-03-8 refers to compound known as Toluene diisocyanate (TDI).\", \"log p(Sq)\": -55.45, \"log p(S)\": -85.48, \"PMI\": 30.04 \"index\": 8, \"depth\": 2, \"content\": \"Understanding Toluene diisocyanate. Toluene diisocyanate (TDI) is chemical compound used in the production of polyurethane"
        },
        {
            "title": "Preprint",
            "content": "materials, which can be found in various products like paints, adhesives, and some foam products.\", \"log p(Sq)\": -45.46, \"log p(S)\": -54.48, \"PMI\": 9.02 \"index\": 23, \"depth\": 3, \"content\": \"TDI and Rashes. TDI exposure can cause skin rashes in some individuals, but it is not generally considered to be inherently harmful to the majority of people with rash.\", \"log p(Sq)\": -68.64, \"log p(S)\": -106.24, \"PMI\": 37.60 \"index\": 42, \"depth\": 4, \"content\": \"Conclusion: Based on the information available, TDI is not specifically harmful to rashes for the majority of people. Therefore, the statement that it is harmful for rash is false. The answer is [B. False].\", \"log p(Sq)\": -90.02, \"log p(S)\": -140.89, \"PMI\": 50.87 }, { }, { } ] C.2 ARC-CHALLENGE Prompt. Here is an example prompt with question from the dataset ARC-Challenge. You need to answer the question by choosing one of the given options. Lets think this question step by step. Finally, please make decision from the options. When you believe youve reached the final step, please output your final answer in square brackets like [A. option1], [B. option2], etc. Question: scientist lost the cap to an ammonia bottle, so she moved the ammonia into clean container with lid. What else should she do to be safe? A. Recycle the old ammonia bottle. B. Label the new container clearly. C. Heat the new container to seal the lid. D. Store the new container in the refrigerator. Step 1: Reasoning Path. Here is an example path that correctly solves the question. [ { \"index\": 2, \"depth\": 1, \"content\": \"The main issue is the exposure of ammonia, which is hazardous substance, especially when not properly contained.\", \"log p(Sq)\": -57.87, \"log p(S)\": -79.15, \"PMI\": 21.28 }, { \"index\": 9, \"depth\": 2,"
        },
        {
            "title": "Preprint",
            "content": "\"content\": \"When ammonia is spilled or not properly contained, it can evaporate, leading to inhalation risks and potential health hazards .\", \"log p(Sq)\": -42.67, \"log p(S)\": -51.71, \"PMI\": 9.04 \"index\": 35, \"depth\": 3, \"content\": \"Look at the options. nOption suggests recyling the old ammonia bottle, which is not relevant to the safety concerns of the new container.nOption involves labeling the new container, which is essential. Without proper labeling, others may not realize its contents.nOption suggests heating the new container to seal the lid. This is not effective for sealing container and could actually cause damage.nOption involves storing the container in the refrigerator. While this might help with the smell, it does not enhance the overall safety.\", \"log p(Sq)\": -143.80, \"log p(S)\": -263.64, \"PMI\": 119.84 \"index\": 54, \"depth\": 4, \"content\": \"Determine the most effective safety measure. Option ensures that others, including the scientist herself, can identify the contents and take necessary precautions.nnTherefore, the most appropriate action to ensure safety is [B. Label the new container clearly].\", \"log p(Sq)\": -145.40, \"log p(S)\": -194.64, \"PMI\": 49.24 }, { }, { } ] C.3 COMMONSENSEQA Prompt. Here is an input prompt with example question. You need to answer the question by choosing one of the given options. nLets think this question step by step. Finally, please make decision from the options. When you believe youve reached the final step, please output your final answer in square brackets like [A. option1], [B. option2], etc. Question: The weasel was becoming problem, it kept getting into the chicken eggs kept in the what? A. forrest B. barn C. public office D. out of doors E. freezer Step 1: Reasoning Path. Here is an example path that correctly solves the given question. [ { \"index\": 3, \"depth\": 1,"
        },
        {
            "title": "Preprint",
            "content": "\"content\": \"Context of the Question. The question states that the weasel is causing problem by gaining access to chicken eggs, which implies location where eggs are stored and where weasel could potentially enter.\", \"log p(Sq)\": -86.52, \"log p(S)\": -137.24, \"PMI\": 50.72 \"index\": 8, \"depth\": 2, \"content\": \"Analyze the Options.nA. Forrest: Forrests are wild, natural habitats and are not typically used for storing agricultural products.nB. Barn: Barns are places where agricultural products, including eggs, are commonly stored.nC. Public Office: Public offices are indoor, administrative spaces that are not typically associated with storing agricultural products.nD. Out of doors: Out of doors refers to an open, outdoor area, which is not suitable for storing eggs.nE. Freezer: freezer is cold storage unit designed for keeping food cold, not specifically for storing eggs.\", \"log p(Sq)\": -127.25, \"log p(S)\": -210.57, \"PMI\": 83.32 \"index\": 16, \"depth\": 3, \"content\": \"Determine the Most Likely Answer.nGiven the context of the question and the function of each option, the most logical place where weasel would be able to access chicken eggs is inside barn .\", \"log p(Sq)\": -49.20, \"log p(S)\": -68.81, \"PMI\": 19.61 \"index\": 43, \"depth\": 4, \"content\": \"Therefore, the answer is [B. barn].\", \"log p(Sq)\": -66.43, \"log p(S)\": -89.31, \"PMI\": 22.88 }, { }, { }, { } ]"
        }
    ],
    "affiliations": [
        "The Hong Kong Polytechnic University",
        "University of Georgia"
    ]
}