{
    "paper_title": "Understanding and Mitigating Bottlenecks of State Space Models through the Lens of Recency and Over-smoothing",
    "authors": [
        "Peihao Wang",
        "Ruisi Cai",
        "Yuehao Wang",
        "Jiajun Zhu",
        "Pragya Srivastava",
        "Zhangyang Wang",
        "Pan Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Structured State Space Models (SSMs) have emerged as alternatives to transformers. While SSMs are often regarded as effective in capturing long-sequence dependencies, we rigorously demonstrate that they are inherently limited by strong recency bias. Our empirical studies also reveal that this bias impairs the models' ability to recall distant information and introduces robustness issues. Our scaling experiments then discovered that deeper structures in SSMs can facilitate the learning of long contexts. However, subsequent theoretical analysis reveals that as SSMs increase in depth, they exhibit another inevitable tendency toward over-smoothing, e.g., token representations becoming increasingly indistinguishable. This fundamental dilemma between recency and over-smoothing hinders the scalability of existing SSMs. Inspired by our theoretical findings, we propose to polarize two channels of the state transition matrices in SSMs, setting them to zero and one, respectively, simultaneously addressing recency bias and over-smoothing. Experiments demonstrate that our polarization technique consistently enhances the associative recall accuracy of long-range tokens and unlocks SSMs to benefit further from deeper architectures. All source codes are released at https://github.com/VITA-Group/SSM-Bottleneck."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 1 3 ] . [ 1 8 5 6 0 0 . 1 0 5 2 : r UNDERSTANDING AND MITIGATING BOTTLENECKS OF STATE SPACE MODELS THROUGH THE LENS OF RECENCY AND OVER-SMOOTHING Peihao Wang1, Ruisi Cai1, Yuehao Wang1, Jiajun Zhu1,2, Pragya Srivastava3, Zhangyang Wang1, Pan Li4 1University of Texas at Austin, 2Zhejiang University, 3Google DeepMind, 4Georgia Tech {peihaowang,ruisi.cai,yuehao,atlaswang}@utexas.edu, junnian@zju.edu.cn, pragya8srivastava@gmail.com, panli@gatech.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "Structured State Space Models (SSMs) have emerged as alternatives to transformers. While SSMs are often regarded as effective in capturing longsequence dependencies, we rigorously demonstrate that they are inherently limited by strong recency bias. Our empirical studies also reveal that this bias impairs the models ability to recall distant information and introduces robustness issues. Our scaling experiments then discovered that deeper structures in SSMs can facilitate the learning of long contexts. However, subsequent theoretical analysis reveals that as SSMs increase in depth, they exhibit another inevitable tendency toward over-smoothing, e.g., token representations becoming increasingly indistinguishable. This fundamental dilemma between recency and over-smoothing hinders the scalability of existing SSMs. Inspired by our theoretical findings, we propose to polarize two channels of the state transition matrices in SSMs, setting them to zero and one, respectively, simultaneously addressing recency bias and over-smoothing. Experiments demonstrate that our polarization technique consistently enhances the associative recall accuracy of long-range tokens and unlocks SSMs to benefit further from deeper architectures. All source codes are released at https://github.com/VITA-Group/SSM-Bottleneck."
        },
        {
            "title": "INTRODUCTION",
            "content": "Sequence processing architectures have evolved from RNNs (Hochreiter & Schmidhuber, 1997; Sutskever et al., 2014; Cho et al., 2014; Cho, 2014), to transformers (Vaswani et al., 2017; Devlin et al., 2019; Radford et al., 2018; 2019; Brown et al., 2020), and more recently to State Space Models (SSMs) (Gu et al., 2021a; Gu & Dao, 2023). In particular, SSMs (Gu et al., 2021a; Gu & Dao, 2023; Dao & Gu, 2024) have emerged as compelling alternative to transformers, enabling more efficient handling of long sequences. SSMs operate in two modes: convolution and recurrence (Gu et al., 2021b). During convolutional mode, SSMs assume visibility of the entire sequence and utilize hardware-optimized convolutions to propagate information across all tokens in parallel. This approach avoids the calculation of pairwise correlations inherent in attention mechanisms, thereby accelerating training. Mamba (Gu & Dao, 2023) enabled convolution with parallel scanning algorithm, which yielded more expressive sequence-level mixing without sacrificing efficiency. During recurrent mode, SSMs process one token at time while maintaining compact recurrent hidden state encoding the sequence history. The outputs are sequentially decoded from this hidden state, avoiding storinh all past key-value pairs (Dai et al., 2019) and thus reducing inference memory usage. Furthermore, SSMs have been meticulously tailored to effectively capture long-range dependencies and filter contextual information. These models are grounded in HiPPO theory (Gu et al., 2020), which demonstrates that first-order Ordinary Differential Equation (ODE) can encapsulate long-term memory through designated state matrix known as the HiPPO matrix. Subsequent research (Gu et al., 2021b;a; Gupta et al., 2022; Gu et al., 2022a) has simplified this state matrix to diagonal form, significantly improving computational efficiency while retaining the ability to model longrange dependencies. Mamba (Gu & Dao, 2023) introduced selection mechanism that selectively aggregates pertinent information from the context into the state. Concurrently, linear attention models 1 have been derived from streamlined attention mechanisms (Katharopoulos et al., 2020; Sun et al., 2023; Peng et al., 2023; Yang et al., 2023). Collectively, these advances can be interpreted through unified lens as more structured SSMs (Dao & Gu, 2024). Despite their initial empirical successes, recent findings indicate that SSMs may not match transformers in their ability to recall information from long contexts (Arora et al., 2023; Poli et al., 2024) or in handling more complex retrieval patterns (Park et al., 2024). Furthermore, it has been observed that Mamba continues to underperform compared to transformers on larger scales (Waleffe et al., 2024). These shortcomings have not yet been systematically elucidated. In this paper, we identify two fundamental limitations of SSMs in their ability to model complex long-range dependencies. First, we argue that the long-term memory capabilities of modern SSMs may be misinterpreted. Our analysis reveals that an SSM layer exhibits strong recency bias, limiting tokens to primarily interact with the nearby context. This bias is intrinsic to SSMs and many linear attention models, regardless of the content-informing techniques employed, such as the selection mechanism introduced by Mamba (Gu & Dao, 2023). We further posit that the loss of long-range capabilities may stem from the oversimplification of HiPPO-induced SSMs, which trades efficiency off the performance. To substantiate this claim, we perform long-range retrieval task on an industrial-scale language model (Jiang et al., 2023) based on Mamba. Our test results indicate that Mamba catastrophically forgets distant content once the context length exceeds its memory capacity. Furthermore, we raise novel robustness concern regarding SSMs with recency bias: our empirical outcomes show that Mamba is more susceptible to perturbations on local tokens, making it vulnerable to adversarial attack, as these local tokens can be easily manipulated to serve as backdoors. We then conduct series of scaling experiments with varying context lengths during the pre-training of SSMs. Our results indicate that increasing the model depth is crucial for enhancing its ability to utilize long contexts by expanding the receptive field. However, we observe that depth scaling encounters another bottleneck as performance begins to saturate as depth continues to increase. We analyze the feature dynamics across the SSM layers and theoretically revealed that SSMs inherently function as smoothing operators, leading to over-smoothing in deep architectures (NT & Maehara, 2021; Oono & Suzuki, 2019; Cai & Wang, 2020). As result, token representations become increasingly uniform and indistinguishable with each additional layer. Recognizing such fundamental dilemma between recency and over-smoothing, we introduce unified approach called the polarization, which simultaneously addresses recency bias and over-smoothing. Specifically, we reserve two dedicated channels in the state transition matrices of SSMs and reset them to zero and one, respectively. The all-one channel helps preserve historical information and prevents catastrophic forgetting caused by locality, while the zero channel inhibits excessive fusion of information from past tokens, effectively slowing the smoothing rate. Our experiments on associative recall (Arora et al., 2023) demonstrate that the polarization technique significantly improves recall accuracy by mitigating locality artifacts and can gain more performance when combined with deeper architectures by effectively alleviating over-smoothing."
        },
        {
            "title": "2 PRELIMINARIES",
            "content": "the discrete-time sequence of tokens as = In SSMs (and alike), we represent xT ] RT . For vector-valued input sequences, SSMs process each channel inde- [x1 pendently. To simplify notations, we focus on scalar-valued sequences without loss of generality. The impact of multi-channel inputs will be addressed in the relevant context. SSMs learn to represent and forecast the next token by integrating past information. Formally, SSMs can be viewed as sequence-to-sequence transformation from inputs RT to outputs RT through memory state ht RN , which is iteratively updated with linear recurrence. general form can be written as: ht = Atht1 + tbt(xt), yt = ct(ht), h0 = 0, [T ], (1) where [T ] denotes the time step. Intuitively, At RN extracts information from the previous 1, bt : RN projects every input token to the hidden space, controls how state ht1 much information of the new token will be fused into the hidden memory, and ct : RN decodes 1In most scenarios discussed in this paper, we assume real parameterization by default, as it is the standard approach in the cases of our primary interest, such as language modeling (Gu & Dao, 2023) 2 the hidden state at time to the final prediction. In all SSMs considered in this paper, it is necessary to assume (At, bt, ct, t) only depends on the inputs at the time t. SSMs are trained from end to end to optimize for the parameters {(At, bt, ct, t)}t[T ], for which the different SSMs adopt various types of instantialization. Below, we list some representative examples. S4, DSS, and S4D. The seminal works (Gu et al., 2020; 2021b; 2022b) demonstrate that discretizing time-invariant ODE h(t) = Ah(t) + bx(t) with some special realization of matrix can yield an efficient recurrent network for long-sequence modeling. The follow-up works Gu et al. (2021a) together with Gupta et al. (2022); Gu et al. (2022a) simplifies to be diagonal matrix. Applying the zero-order hold rule for discretization, as suggested by Gupta et al. (2022), we can summarize this series of models in the form of Eq. 1: (S4) At = exp(A), bt(xt) = bxt, c(ht) = cht, = , 2 (2) where (A, b, c, ) are learnable parameters. In particular, is restricted to be diagonal matrix and can be complex valued. However, must have negative real part (Gu et al., 2022a). (0, 1] is often interpreted as the time interval for discretization. We call this family of SSMs S4 following the naming convention in Gu & Dao (2023). Mamba. recent breakthrough Mamba (Gu & Dao, 2023) introduces the selection mechanism to extend S4. Instead of learning (A, b, c, ) in Eq. 2 as free parameters, Mamba conditions (A, b, c, ) on the inputs, which enables each iterative step in Eq. 1 to filter useful token information during the recurrence. Specifically, Mamba computes (At, bt, ct, t) as follows: (Mamba) At = exp(tA), bt(xt) = (WBxt)xt, ct(h) = (WCxt)ht, = σ(Wxt), (3) where R, WB RN , WC RN are learnable weights in addition to A, and σ() denotes softplus activation. When handling multi-dimensional token embeddings, W, WB, WC are extended on the input dimension, different (At, bt, ct, t) are assigned to each token channel and Eq. 3 is running channel-wisely. is varying across different channels, while bt, ct are shared across token channels. That is, if the token embedding dimension is D, then each token channel will be allocated with distinct RD, and shared WB RN D, and WC RN D. In language modeling, has strictly negative real-valued diagonal, which ensures At (0, 1)N . Additionally, Mamba is integrated with the H3 architecture (Fu et al., 2022), wherein the selective SSMs is working with local convolution and sandwiched by two gated connections. Linear Attention. Concurrent with SSMs, there is another line of work streamlining attention to linear time complexity. With slight abuse of terminology, we name all of them collectively as Linear Attention Models (LAMs). We observe that many of them can be written in the form of Eq. 1 such that in the remainder of this paper, we extend the definition of SSMs to include LAMs without introducing ambiguity, as LAMs and SSMs are dual to each other (Dao & Gu, 2024). We leave full summary to Appendix B."
        },
        {
            "title": "3 CAN SSM EFFECTIVELY REPRESENT LONG-RANGE DEPENDENCIES?",
            "content": "3.1 SSMS ARE LOCALLY BIASED In this section, we investigate the ability of SSMs to learn long-range dependencies. Recent studies find that SSMs seem more effective than transformers on this task (Gu et al., 2020; Tay et al., 2020; Li et al., 2022; Gu & Dao, 2023). However, in Sec. 3.1 we theoretically show negative result that an SSM layer is inherent to local bias and loses long-term memory exponentially. In Sec. 3.2, we empirically justify our claim by showing SSMs struggle to retrieve from distant context. We also demonstrate that the local bias may lead to robustness issues in Sec. 3.3. To understand how information is propagated and long-range dependencies are modeled in SSMs, we aim to uncover the relationship between the output at time [T ] and the input token at time t. We define the derivatives yt/xs as the influential score to represent the importance of the s-th 2More rigorously, by zero-order hold, should be parameterized as = (A)1(exp(A) I)b. However, the presented form is more commonly used in practice as in Gu & Dao (2023). input token to the t-th output token. Note that yt/xs is well-defined for every s, [T ] as long as (At.bt, ct, t) are all differentiable in terms of x. Intuitively, if yt/xs is larger, then the s-th input token is more influential on the t-th output token, and vice versa. Below we present formal result regarding the influential score. Theorem 3.1 (Recency of SSMs). Consider an SSM defined in Eq. 1 with {(At, bt, ct, t)}t[T ]. Assume that (i) the input space RT is compact, (ii) {(At, bt, ct, t)}t[T ] are continuous and have continuous derivatives, and (iii) At (0, 1)N are diagonal matrices for all [T ]. Let Amax = maxt[T ],n[N ](At)n,n. Then for arbitrary and every s, [T ] such that < t, yt/xs = O(exp(κ(t s))) for some κ = Θ(log(A1 max)). The proof can be found in Appendix D.1. The first two assumptions are standard and always satisfied. The third assumption also holds for most of SSMs discussed in Sec. 2. Therefore, Theorem 3.1 applies to numerous SSMs including S4 (Gu et al., 2021a; 2022a), Mamba (Gu & Dao, 2023), and many LAMs (Sun et al., 2023; Peng et al., 2023; Yang et al., 2023; Qin et al., 2024; De et al., 2024). Theorem 3.1 states that influential scores between two tokens modeled by SSMs are exponentially diminishing with respect to their relative distance. The decay rate is determined by the maximal values among all Ats elements. The closer Amax is to zero, the faster the influential scores decay. The practical implication is that SSMs are factually recency-biased models. Tokens farther away are under-reaching and forgotten rapidly while the information of closer tokens dominates the final output. This can significantly limit their ability of fitting complex long-range relationships. Figure 1: Visualization of log influential scores log yt/xs versus distance (ts). Empirical Validation. In Fig. 1, we plot the logarithmic influence scores against relative distances. Across various model sizes, Mamba consistently exhibits linear decay rate of influence scores, both at initialization and throughout training. This observation suggests that recency arises from an inherent model bias, as described in Theorem 3.1, rather than solely from capturing data statistics. Moreover, the adopted initialization scheme further amplifies the locality bias. Is the design of decay necessary or desirable? One key observation from our theory is that the parameterization of At within the interval (0, 1) leads to strictly decaying dependencies among tokens based on their relative distances. This design choice appears to be standard practice, and perhaps intentional, in several recently proposed SSMs (Gu & Dao, 2023; Dao & Gu, 2024; Beck et al., 2024; Yang et al., 2023; Peng et al., 2024; De et al., 2024; Ma et al., 2024; Liu et al., 2024; Yang et al., 2024). Interestingly, it has been demonstrated that this intentional decay of long-range dependencies not only avoids degrading the perplexity of language models but also improves generalization for length extrapolation. This observation aligns with the empirical success of soft gating mechanisms adopted in traditional RNNs (Cho, 2014; Cho et al., 2014; Gu et al., 2021b) and the decaying patterns imposed on transformers (Raffel et al., 2020; Press et al., 2021; Sun et al., 2022). We find the constraint At (0, 1)N is likely inherent to SSMs as it plays critical role in ensuring the numerical stability for length generalization during long-context recurrence (Gu et al., 2022a; Yang et al., 2023). Promoting the importance of local tokens could also lead to nearly correct bias, as natural language generation mostly utilizes recent contexts. However, as we will detail in subsequent sections, this design comes with significant drawbacks: it result in substantial loss of long-distance information (Sec. 3.2) and may raise potential security concerns (Sec. 3.3). This observation underscores that the validation perplexity may not fully capture all aspects of models capabilities and can be an inadequate metric for assessing long-range dependencies. 3.2 LOST IN THE DISTANCE: LONG-CONTEXT RETRIEVAL TEST To assess the ability of large language models (LLMs) to effectively utilize long-context data, we evaluate open-source SSM using the Needle in Haystack benchmark and compare its performance 4 Figure 2: Comparison between SSM and Transformer on the Needle in Haystack\" benchmark. The left figure shows the retrieval accuracy of the Mamba-Codestral-7B model, while the right figure presents the retrieval accuracy of the Mistral-7B model. We present heatmap where \"full context length\" refers to the total length of the document, and \"needle position\" denotes the relative position of the statement to be retrieved within the context. See more fine-grained visualization in Appendix E.2. with that of Transformer. In this benchmark, randomly generated statement is embedded within the middle of long document, and the models are tasked with retrieving the statement. By varying the insertion position of the statement, we measure the retrieval accuracy at each location, which reflects the models positional bias. To enforce LLMs using the data within context, instead of recalling information memorized by its model weights, we carefully design the statement with factual error. See detailed examples in Appendix E.2. We compare the retrieval accuracy of the Mamba-Codestral-7B model, representative SSM capable of handling long-context inputs of up to 256k tokens, with Mistral-7B (Jiang et al., 2023), which utilizes transformer architecture. As illustrated in Figure 2, the retrieval accuracy of the Transformer remains stable regardless of the needle position. In contrast, the SSM achieves higher accuracy when the needle is placed closer to the end of the context (i.e., larger needle position values), while its accuracy drops when the needle is located near the beginning of the document. This indicates positional bias towards local tokens in the SSM. 3.3 POTENTIAL RISK ON MODEL ROBUSTNESS We conduct quantitative experiments to show the recency-biased nature of SSMs will lead to potential hazards. The downstream task in this study is image classification on sequences of pixels (Tay et al., 2020), where images are flattened to sequences of pixel tokens and fed to sequence models for classification. We test family of SSMs, including H3 (Fu et al., 2022), RWKV (Peng et al., 2023), and Mamba (Gu & Dao, 2023), and compare them against transformer baseline (Vaswani et al., 2017) on CIFAR-10 dataset. To adapt SSMs for this task, we append learnable class token after the last token of the input sequence. The output state of this class token is then mapped to logits using classifier head. Experiment details are given in Appendix E.3. In the following, two attack patterns on the input data are introduced, which degrade the robustness of SSMs in this task. Adversarial Attack. To assess the bias of SSMs towards corrupted data, we perturb the leading and trailing tokens of input sequences with random noise. In unbiased models, perturbations in both leading and trailing tokens cause similar performance drops. However, in locally biased models, where the class token is appended after the last input token, the trailing tokens are supposed to have greater impacts on classification outcomes than leading tokens. Table 1 presents our experimental results on the CIFAR-10 dataset under two corruption ratios. For each ratio, the same number of Models (no corrupt) [992:1024] [0:32] [928:1024] [0:96] Corrupted region (seq. length = 1024) H3 Transformer RWKV Mamba 0.654 0.580 0.474 0.674 0.569 ( 13.04%) 0.535 ( 7.81%) 0.150 ( 68.35%) 0.126 ( 81.24%) 0.654 ( 0.03%) 0.447 ( 22.95%) 0.466 ( 1.58%) 0.658 ( 2.30%) 0.477 ( 27.07%) 0.431 ( 25.76%) 0.138 ( 70.88%) 0.098 ( 85.46%) 0.650 ( 0.72%) 0.370 ( 36.32%) 0.460 ( 2.91%) 0.647 ( 3.98%) Table 1: Results of adversarial attack experiments on the CIFAR-10 dataset, evaluated using classification accuracy. Each input sequence contains 1,024 tokens. Two corruption ratios (32/1024 and 96/1024) are applied to perturb the leading and trailing tokens, respectively. 5 (a) Attack ratio = 256/1024 (25.00%) (b) Attack ratio = 480/1024 (46.875%) Figure 3: Results of target attack experiments on CIFAR-10, where horse is the target class. (a) and (b) present target attack success rates under two attack ratios. Lower success rates suggest higher robustness in the corresponding attack regions. leading and trailing tokens are corrupted with Gaussian noise. Among all the SSM family methods compared, the performance drops caused by trailing token corruption are significantly larger than those caused by leading token corruption. Notably, for Mamba, perturbing the last 32 out of 1024 tokens results in an 81.24% drop in classification accuracy, whereas corrupting the first 32 tokens only reduces accuracy by 2.30%. In contrast, the transformer baseline shows relatively smaller impacts from trailing token corruption. Instead, our experiments indicate that more informative features from transformers tend to sink in the leading tokens, aligning with the observations in Xiao et al. (2023). Target Attack. Beyond degrading the performance of SSMs by attacking trailing tokens, we also demonstrate that local bias creates backdoor for target attacks. In this scenario, target class is selected, and pixel tokens from that class are used to replace those in images from other classes. The attack succeeds when models mis-classify images from other classes as belonging to the target class. Due to the local bias, trailing tokens are expected to be more effective attack region for SSMs, leading to significantly higher attack success rate compared to leading tokens. Fig. 3 shows the success rate comparisons across different attack regions and ratios. When trailing regions are replaced with pixels from the target class, SSMs achieve much higher success rates than when leading regions are attacked. This phenomenon is observed at both 25% and 47% attack ratios. By comparison, the transformer model possesses greater robustness, maintaining similar success rates between attacks on leading and trailing tokens. Implications for Language Models. While our adversarial attack experiments are conducted on image datasets, the findings have broader implications for language models. System prompts, which are typically group of confidential tokens prepended to user inputs, play critical role in controlling the behavior of language models and preventing undesirable outputs. However, our targeted attack experiments reveal that SSM-based language models are particularly vulnerable to jailbreak attacks (Perez & Ribeiro, 2022; Zou et al., 2023). This is because SSMs prioritize recent information over past tokens, making it easier to bypass system prompts by appending jailbreak instructions at the end of the input. Moreover, our theoretical analysis suggests that fine-tuning LLMs with instructional datasets or human feedback to enforce adherence to system prompts may not resolve this vulnerability, as the recency bias remains inherent to SSM models regardless of weight configurations."
        },
        {
            "title": "4 UNDERSTANDING SCALABILITY BOTTLENECK OF SSMS",
            "content": "4.1 NECESSITY AND LIMITS OF DEPTH SCALING In Sec. 3.1, we have seen that the dependencies between tokens are exponentially decaying with their relative distances in an SSM layer. Consequently, SSMs resemble localized kernels, similar to those employed in various neural architectures such as Convolutional Neural Networks (CNNs) (LeCun et al., 1998) and Graph Neural Networks (GNNs) (Kipf & Welling, 2016). It is reasonable postulation that increasing the number of layers can extend the models receptive field (Goodfellow et al., 2016). We justify this hypothesis via scaling-up experiment with various context lengths and model architectures. We pretrain Mamba using causal language modeling with two context lengths, {2048, 8192}. Besides, we fix the number of layers at {16, 24, 32, 48, 64, 72} and vary the hidden dimension. We defer 6 Figure 4: We empirically observe that deeper models become increasingly advantageous as the context length grows. However, beyond certain depth, the performance of SSMs begins to plateau and eventually declines. more experiment details in Appendix E.4. The validation loss versus the number of parameters is plotted in Fig. 4. Under the 2048 context length, models of different configurations exhibit similar performance, consistent with the findings of Kaplan et al. (2020). However, as the context length increases, the scaling behavior across depth-width configurations begins to diverge. Notably, deeper models outperform shallower ones, likely because deeper architectures can more effectively utilize the extended context to meet the training objectives. Nevertheless, we observe that the performance gain starts to saturate when we keep increasing the depth (cf. the 32-layer and 48-layer models). When the depth of the model continues to increase, the validation perplexity starts to rise, indicating decline in performance (cf. the 64-layer and 72-layer models). In Mamba with 2048 context length, models with more than 48 layers perform worse than 16-layer models. Longer-context models appear to be more tolerant of increased depth, whereas shorter-context models experience rapid performance degradation once the depth exceeds certain threshold. 4.2 UNVEILING OVER-SMOOTHING IN SSMS To explain the depth scaling bottleneck revealed in the previous section, we conduct theoretical and empirical investigation of the feature and state dynamics in SSMs. Our key finding is that token embeddings, after being processed by SSM layers, tend to become increasingly similar, which leads to phenomenon commonly referred to as over-smoothing (NT & Maehara, 2021; Cai & Wang, 2020; Oono & Suzuki, 2019). Over-smoothing occurs when token representations become indistinguishable, rendering the state uninformative. First of all, we warm up by studying continuous-time S4 with constant (A, b, c). Recall that continuous-time S4 layer can be described by group of ODEs: h(t) = Ah(t) + bx(t), y(t) = ch(t). Our analysis starts with the equivalence between convolution and S4 (Gu et al., 2021b). This is, the analytic solution to the time-invariant ODE can be expressed as y(t) = (cid:82) exp(A(t s))bx(s)ds. Now we analyze the filtering property of this convolution operator from the Fourier domain perspective. We define convolutional operator as low-pass filter if it suppresses highfrequency components (see Definition D.3). We summarize the main finding in the following proposition, whose formal version and proof are provided in Appendix D.2.1: Proposition 4.1 (Low-pass filtering of continuous S4). Consider continuous-time S4 with parameters (A, b, c). Assume is diagonal with all values negative. Then y(t) = (cid:82) exp(A(t s))bx(s)ds defines low-pass filter. Proposition 4.1 states that S4 is inherently low-pass filter regardless of how (A, b, c) are trained. Therefore, the high-frequency components of input signals are being constantly removed at each layer. Presumably, stacking many S4 layers might cause over-smoothing when all high-frequency components are suppressed to zero. Now we consider more general scenario when SSMs work on discrete-time regime and (At, bt, ct, t) are time-varying or even data-dependent. Formally, we prove the following result showing the sharpness of input signals will be reduced as well: 7 (a) bt and ht. (b) Mixer output. (c) Block output. Figure 5: Visualization of feature smoothness across layers in pre-trained Mamba and Pythia. The y-axis represents the average pairwise differences among tokens. Mixer outputs (b) solely consider the Mamba or attention module, while Block outputs (c) include all other components (e.g., MLP). Theorem 4.2 (Over-smoothing of SSMs). Consider an SSM specified in Eq. 1 with {(At, bt, ct, t)}t[T ]. Assume an input space RT such that for every , (i) (At)n,n +t 1 for every [N ] and [T ], (ii) mint[T ] bt(xt)n 0 and maxt[T ] bt(xt)n 0 for every [N ]. Let Amin = mint[T ],n[N ](At)n,n. Then for any and the memory states {ht : [T ]} generated by the SSM, we have: ht hs (cid:0)1 AT bt(xt) bs(xs) , (cid:1) max (4) min max t,s[T ] t,s[T ] Proof can be found in Appendix D.2.2. We first justify our assumptions here. (At)n,n + 1 is generic condition to ensure the recurrence of SSMs is non-expansive, which is crucial to guarantee memory states stay numerically stable. The second assumption requires the data to be well-distributed and centered around the origin, which can be easily satisfied by normalization techniques. We find that prevalent SSM models such as (Gu & Dao, 2023; Peng et al., 2023; De et al., 2024; Qin et al., 2024) can easily achieve these two assumptions. Moreover, if (At)n,n + = 1 is always true letting each recurrent update be conservative (Peng et al., 2023; Ma et al., 2022), then we can remove the second assumption as well (see Theorem D.5). Theorem 4.2 examines the relationship between the pairwise distances of memory states and encoded tokens within the sequence. This result indicates that the pairwise discrepancies among memory states are diminished by factor less than one, suggesting that the memories undergo smoothing following the application of an SSM in Eq. 1. We deem that if the memory is losing its discriminative capacity, the intermediate hidden feature space will similarly collapse. Delving deeper, the decay rate is intricately linked to both the context length and the minimal value within {At, [T ]}. As the context length increases, it requires more time to effectively mix all tokens. This can be understood from the message-passing perspective (Gilmer et al., 2017): the message of the first token needs to go through the whole sequence to be mixed with the last token. When Amin approaches one, the decay rate is maximized, as the entire SSM essentially performs uniform pooling over the entire sequence, which smoothens the signal via box-like filter. It is worth noting that the smoothing nature of SSMs is intuitive; one can conceptualize the recurrent operation of SSMs as performing running average of the encoded token signals. (cid:0) (cid:80) ixi2 2 1 2(N 1) i=jxi xj2 2 Empirical Validation. We adopt pairwise distance between tokens to quantify the sharpness (cid:1). E(x) being small means the token (cid:1)/(cid:0) (cid:80) of signal: E(x) = representations are close to each other and become less discriminative. We plot the feature smoothness of 1.4B Mamba in Fig. 5. In Fig. 5a, bt is above ht among all Mamba blocks. This suggests the sharpness of input signals is consistently higher than the sharpness of the memory state output from Mamba, verifying our Theorem 4.2. In addition, Fig. 5b and 5c show the sharpness of Mamba mixer and Mamba block output, which tends to decrease rapidly in deeper layers. We also provide comparison with transformer (of the same size). Although transformers suffer from over-smoothing in theory (Dong et al., 2021; Shi et al., 2022; Wang et al., 2022), we observe that transformers have slower decay of feature sharpness. See theoretical comparison in Appendix C."
        },
        {
            "title": "5 DISCUSSIONS",
            "content": "In this section, we provide further discussions based on our theory while deferring the remaining parts to Appendix due to the page limit. We also introduce more related work in Appendix A. Revisiting HiPPO theory. HiPPO, introduced in (Gu et al., 2020) and extended by Gu et al. (2021b; 2022b), forms the theoretical basis of SSMs. It optimally reconstructs signal up to time by minimizing xt y(t)L2(ω(t)) with respect to measure ω(t) supported on (, t]. The solution projects xt onto basis functions, producing coefficient vector h(t), which synthesizes y(t) via linear combinations. Gu et al. (2020) showed h(t) evolves as h(t) = A(t)h(t) + b(t)x(t), where ω(t) = I[0, t]/t yields closed form A(t) = Ahippo/t. Subsequent works like S4 (Gu et al., 2021a) and Mamba (Gu & Dao, 2023) utilize this matrix as initialization but omitted 1/t, resulting in warped measure ω(t)(s) exp(s t)I[s < t] (Gu et al., 2022b). This measure emphasizes recent history when approximating xt. Hence, findings of Gu et al. (2022b) do not contradict our results but also align with Theorem 3.1. The practical implementations often disconnect from HiPPO theory by neglecting the unitary matrices associated with Ahippo. Whereas, we focus on discrete-domain SSMs, adhering to practical parameterizations. Additionally, Gu et al. (2021b) demonstrates that SSMs expressiveness spans all convolutions and RNNs. We contend that the low-pass filtering property also arises from the parameter simplifications of At (see Proposition 4.1). The effect of selection mechanism. Traditional S4 architectures operate as linear time-invariant systems. To introduce more non-linearity, Mamba (Gu & Dao, 2023) proposes modeling (bt, ct, t) as function of inputs, mechanism known as selection. This is motivated by the selective copying synthetic task, wherein At and bt need to adapt based on content to filter relevant information for memory updates. Despite this adaptation, Theorem 3.1 still holds in scenarios involving the selection mechanism, meaning selective SSMs like Mamba may continue to suffer from recency bias. In the meantime, Theorem 4.2 also applies to Mamba, suggesting that the selective SSMs do not demonstrate higher expressiveness in filtering signals and perform similarly to linear S4 as low-pass filters (Proposition 4.1). Nevertheless, we note that selection can alleviate these issues by adaptively controlling the values in At. According to our theory, the selection mechanism can potentially make the upper bound Amax closer to one and the lower bound Amin closer to zero. However, parameter in Eq. 3 is initialized with negative integers (Gu et al., 2022a), which exacerbates the bound in Theorem 3.1 by accelerating the decay rate of the influence score. Complex parameterization. Our analysis in the main text primarily focuses on the case where (At, bt, ct) are both real-valued. While most modern SSMs adopt real parameterizations, complex parameterizations particularly complex-valued At have been explored in prior works such as Gupta et al. (2022); Goel et al. (2022); Gu et al. (2022a). Notably, we show that both our locality result (Theorem 3.1) and over-smoothing result (Theorem 4.1) remain valid for complex-valued SSMs. We formalize these extensions in Theorems D.2 and D.4. These findings collectively demonstrate that complex parameterization does not eliminate the locality or over-smoothing bias inherent in SSMs. Importance of context scaling. Theorem 4.2 also highlights the importance of context-length scaling to mitigate the over-smoothing issue. As the smoothing rate decreases with increased context length, lengthening training sequences not only relieves over-smoothing but also maximizes the utility of hardware efficiency of SSMs. This is also evidenced by Fig. 4, where models with longer training contexts have better tolerance of deeper architectures. To enable SSMs to fully utilize the context, increasing the model depth is essential. Consequently, synchronized scaling of both model depth and context length is required. Investigating the scaling laws governing these two dimensions presents an intriguing direction for future research."
        },
        {
            "title": "6 MITIGATING RECENCY AND OVER-SMOOTHING VIA POLARIZATION",
            "content": "In this section, we propose simple solution to mitigate recency and over-smoothing simultaneously. First of all, results in Theorem 3.1 and Theorem 4.2 can be interpreted in conjunction. To relieve the smoothening rate, one might aim to minimize the values in At. However, this could inadvertently enhance the locality of SSMs, as decrease in Amax may occur. practical implication of this 9 relationship is that the values in At should be as diverse as possible to simultaneously mitigate the artifacts of recency and over-smoothing. Although Amin 0 and Amax 1 could theoretically occur simultaneously, our empirical findings show that these values are largely concentrated within narrow range. To illustrate this, we visualize the distribution of (Amax Amin) across different channels in Fig. 6. Each bin represents the proportion of channels whose memory state satisfies (Amax Amin) being smaller than the corresponding threshold on the x-axis. Notably, over 60% of channels have (Amax Amin) values smaller than 0.5, indicating that most channels cannot simultaneously achieve Amax 1 and Amax 0. Memory representations will inevitably undergo either exponential diminishing or over-smoothing. Figure 6: Cumulative histogram of (Amax Amin). The height of each bin represents the cumulative proportion of (Amax Amin) less than To this end, we propose to maintain one component or equal to the corresponding value on the x-axis. in At as constant 1, another as constant 0, and others freely learnable. We term this approach as polarization. Polarization ensures that the dimension polarized to zero in the state memory consistently focuses on the current token, counteracting oversmoothing by preventing mixing with previous tokens. Simultaneously, another dimension polarized to one exclusively retains information from past tokens, avoiding locality issues by preserving the complete history. In our implementation (see Appendix E.5), we polarize the first state channel to one (i.e., (At)1,1 = 1) and the last state channel to zero (i.e., (At)N,N = 0). 2 4 256 128 2 2 4 Avg. # Layers # KV Pairs 81.81 82.08 36.00 33.52 98.38 99.23 72.06 71.61 Configurations 94.70 81.35 92.20 99.81 98.41 99.74 Default At Default At (At)1,1 = 1 (At)N,N = 0 (At)N,N = 0 (At)1,1 = 1, (At)N,N = 0 (At)1,1 = 1, (At)N,N = 0 We empirically validate our polarization technique through the associative recall tasks, where Mamba models (Gu & Dao, 2023) are trained over sequences of key-value pairs to retrieve the associated value according to the query from the context. Please refer to Arora et al. (2023) and Appendix E.5 for more details. If an SSM can recall information with higher accuracy from larger number of key-value pairs, then it has better long-context capability. We test the performance of Mamba with neither, one of, or both zeroand one-polarized channels, across different numbers of layers. The empirical results are reported in Tab. 2 (an extended version in Appendix E.5). The key observation is that the default parameterization of At suffers from information retrieval from long context, and deepening the architecture even harms the performance (potentially due to over-smoothing). However, once we introduce channel polarized to one, even shallow Mamba could perform high-accuracy associative recall (row 3). We can also gain the performance by deepening Mamba if the over-smoothing issue can be bypassed by adopting the zero-polarized channel (row 5). Furthermore, if we could apply oneand zero-polarized channels, and in the meanwhile deepening the architecture, we find it achieves the best performance over all settings (row 7). Table 2: Results of polarization. Rows 1-2 have no polarization, rows 3-5 only polarize one channel to either one or zero, and rows 6-7 polarize both channels. 83.63 72.10 81. 56.39 36.55 52.21 83.17 93.43 99.23 99.94 95.54 98.80 54.74 81.56"
        },
        {
            "title": "7 CONCLUSION",
            "content": "In this study, we identify two critical limitations of SSMs. First, we reveal that SSMs exhibit pronounced recency bias, which undermines their ability to model long-range dependencies, recall distant information, and maintain robustness. Second, we find that increasing the depth of SSMs induces over-smoothing, rendering token representations indistinguishable and hindering further performance improvements. To address these challenges, we propose polarization technique that reserves two channels in the state transition matrices, assigning one value of zero and the other value of one. Both theoretical analysis and empirical evaluations demonstrate that polarization significantly enhances the long-range modeling capabilities of SSMs while alleviating over-smoothing."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "PW thanks Bo Liu and Songlin Yang for the insightful discussions when preparing this manuscript. Work was done while JZ and PS were interning at UT Austin. PL is supported by NSF awards IIS-2239565 and IIS-2428777 for this project."
        },
        {
            "title": "REFERENCES",
            "content": "Ameen Ali, Itamar Zimerman, and Lior Wolf. The hidden attention of mamba models. arXiv preprint arXiv:2403.01590, 2024. Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications. arXiv preprint arXiv:2006.05205, 2020. Simran Arora, Sabri Eyuboglu, Aman Timalsina, Isys Johnson, Michael Poli, James Zou, Atri Rudra, and Christopher Ré. Zoology: Measuring and improving recall in efficient language models. arXiv preprint arXiv:2312.04927, 2023. Maximilian Beck, Korbinian Pöppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael Kopp, Günter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. xlstm: Extended long short-term memory. arXiv preprint arXiv:2405.04517, 2024. Assaf Ben-Kish, Itamar Zimerman, Shady Abu-Hussein, Nadav Cohen, Amir Globerson, Lior Wolf, and Raja Giryes. Decimamba: Exploring the length extrapolation potential of mamba. arXiv preprint arXiv:2406.14528, 2024. Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with gradient descent is difficult. IEEE transactions on neural networks, 5(2):157166, 1994. Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pp. 23972430. PMLR, 2023. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Chen Cai and Yusu Wang. note on over-smoothing for graph neural networks. In International Conference on Machine Learning Workshop (ICMLW), 2020. Kyunghyun Cho. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014. Kyunghyun Cho, Bart Van Merriënboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259, 2014. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond fixed-length context. arXiv preprint arXiv:1901.02860, 2019. Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. arXiv preprint arXiv:2405.21060, 2024. Soham De, Samuel Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al. Griffin: Mixing gated linear recurrences with local attention for efficient language models. arXiv preprint arXiv:2402.19427, 2024. 11 Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL, 2019. Francesco Di Giovanni, Lorenzo Giusti, Federico Barbero, Giulia Luise, Pietro Lio, and Michael Bronstein. On over-squashing in message passing neural networks: The impact of width, depth, and topology. In International Conference on Machine Learning, pp. 78657885. PMLR, 2023. Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. Attention is not all you need: Pure attention loses rank doubly exponentially with depth. In International Conference on Machine Learning (ICML), 2021. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. Leo Feng, Frederick Tung, Mohamed Osama Ahmed, Yoshua Bengio, and Hossein Hajimirsadegh. Were rnns all we needed? arXiv preprint arXiv:2410.01201, 2024. Daniel Fu, Tri Dao, Khaled Saab, Armin Thomas, Atri Rudra, and Christopher Ré. Hungry hungry hippos: Towards language modeling with state space models. arXiv preprint arXiv:2212.14052, 2022. Borjan Geshkovski, Cyril Letrouit, Yury Polyanskiy, and Philippe Rigollet. mathematical perspective on transformers. arXiv preprint arXiv:2312.10794, 2023. Justin Gilmer, Samuel Schoenholz, Patrick Riley, Oriol Vinyals, and George Dahl. Neural message passing for quantum chemistry. In International conference on machine learning, pp. 12631272. PMLR, 2017. Karan Goel, Albert Gu, Chris Donahue, and Christopher Ré. Its raw! audio generation with state-space models. In International Conference on Machine Learning, pp. 76167633. PMLR, 2022. Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1. MIT Press, 2016. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Ré. Hippo: Recurrent memory with optimal polynomial projections. Advances in neural information processing systems, 33: 14741487, 2020. Albert Gu, Karan Goel, and Christopher Ré. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021a. Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher Ré. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in neural information processing systems, 34:572585, 2021b. Albert Gu, Karan Goel, Ankit Gupta, and Christopher Ré. On the parameterization and initialization of diagonal state space models. Advances in Neural Information Processing Systems, 35:35971 35983, 2022a. Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher Ré. How to train your hippo: State space models with generalized orthogonal basis projections. arXiv preprint arXiv:2206.12037, 2022b. Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. Advances in Neural Information Processing Systems, 35:2298222994, 2022. Michael Hahn. Theoretical limitations of self-attention in neural sequence models. Transactions of the Association for Computational Linguistics, 8:156171, 2020. 12 Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 17351780, 1997. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Samy Jelassi, David Brandfonbrener, Sham Kakade, and Eran Malach. Repeat after me: Transformers are better than state space models at copying. arXiv preprint arXiv:2402.01032, 2024. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pp. 51565165. PMLR, 2020. Thomas Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016. Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):22782324, 1998. Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, et al. Datacomp-lm: In search of the next generation of training sets for language models. arXiv preprint arXiv:2406.11794, 2024. Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018. Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. What makes convolutional models great on long sequence modeling? arXiv preprint arXiv:2210.09298, 2022. Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, et al. Jamba: hybrid transformermamba language model. arXiv preprint arXiv:2403.19887, 2024. Bo Liu, Rui Wang, Lemeng Wu, Yihao Feng, Peter Stone, and Qiang Liu. Longhorn: State space models are amortized online learners. arXiv preprint arXiv:2407.14207, 2024. Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer. Mega: moving average equipped gated attention. arXiv preprint arXiv:2209.10655, 2022. Xuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Beidi Chen, Lili Yu, Hao Zhang, Jonathan May, Luke Zettlemoyer, Omer Levy, and Chunting Zhou. Megalodon: Efficient llm pretraining and inference with unlimited context length. arXiv preprint arXiv:2404.08801, 2024. William Merrill, Jackson Petty, and Ashish Sabharwal. The illusion of state in state-space models. arXiv preprint arXiv:2404.08819, 2024. Hoang NT and Takanori Maehara. Revisiting graph neural networks: All we have is low-pass filters. In International Conference on Pattern Recognition (ICPR), 2021. Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. arXiv preprint arXiv:2209.11895, 2022. 13 Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for node classification. In International Conference on Learning Representations (ICLR), 2019. Jongho Park, Jaeseung Park, Zheyang Xiong, Nayoung Lee, Jaewoong Cho, Samet Oymak, Kangwook Lee, and Dimitris Papailiopoulos. Can mamba learn how to learn? comparative study on in-context learning tasks. arXiv preprint arXiv:2402.04248, 2024. Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023. Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Teddy Ferdinan, Haowen Hou, Przemysław Kazienko, et al. Eagle and finch: Rwkv with matrix-valued states and dynamic recurrence. arXiv preprint arXiv:2404.05892, 2024. Fábio Perez and Ian Ribeiro. Ignore previous prompt: Attack techniques for language models. arXiv preprint arXiv:2211.09527, 2022. Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Ré. Hyena hierarchy: Towards larger convolutional language models. In International Conference on Machine Learning, pp. 2804328078. PMLR, 2023. Michael Poli, Armin Thomas, Eric Nguyen, Pragaash Ponnusamy, Björn Deiseroth, Kristian Kersting, Taiji Suzuki, Brian Hie, Stefano Ermon, Christopher Ré, et al. Mechanistic design and scaling of hybrid architectures. arXiv preprint arXiv:2403.17844, 2024. Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021. Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. Hgrn2: Gated linear rnns with state expansion. arXiv preprint arXiv:2404.07904, 2024. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter Liu, et al. Exploring the limits of transfer learning with unified text-to-text transformer. J. Mach. Learn. Res., 21(140):167, 2020. Han Shi, Jiahui Gao, Hang Xu, Xiaodan Liang, Zhenguo Li, Lingpeng Kong, Stephen Lee, and James Kwok. Revisiting over-smoothing in bert from the perspective of graph. arXiv preprint arXiv:2202.08625, 2022. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, et al. Learning to (learn at test time): Rnns with expressive hidden states. arXiv preprint arXiv:2407.04620, 2024. Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song, and Furu Wei. length-extrapolatable transformer. arXiv preprint arXiv:2212.10554, 2022. Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: successor to transformer for large language models. arXiv preprint arXiv:2307.08621, 2023. Ilya Sutskever, Oriol Vinyals, and Quoc Le. Sequence to sequence learning with neural networks. Advances in neural information processing systems, 27, 2014. 14 Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: benchmark for efficient transformers. arXiv preprint arXiv:2011.04006, 2020. Jake Topping, Francesco Di Giovanni, Benjamin Paul Chamberlain, Xiaowen Dong, and Michael Bronstein. Understanding over-squashing and bottlenecks on graphs via curvature. arXiv preprint arXiv:2111.14522, 2021. Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan Salakhutdinov. Transformer dissection: unified understanding of transformers attention via the lens of kernel. arXiv preprint arXiv:1908.11775, 2019. Jos Van Der Westhuizen and Joan Lasenby. The unreasonable effectiveness of the forget gate. arXiv preprint arXiv:1804.04849, 2018. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is All You Need. In Proceedings of NeurIPS, 2017. Aaron Voelker, Ivana Kajic, and Chris Eliasmith. Legendre memory units: Continuous-time representation in recurrent neural networks. Advances in neural information processing systems, 32, 2019. Roger Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, Vijay Korthikanti, Tri Dao, Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, et al. An empirical study of mambabased language models. arXiv preprint arXiv:2406.07887, 2024. Peihao Wang, Wenqing Zheng, Tianlong Chen, and Zhangyang Wang. Anti-oversmoothing in deep vision transformers via the fourier domain analysis: From theory to practice. arXiv preprint arXiv:2203.05962, 2022. Shida Wang and Beichen Xue. State-space models with layer-wise nonlinearity are universal approximators with exponential decaying memory. Advances in Neural Information Processing Systems, 36, 2024. Xinyi Wu, Zhengdao Chen, William Wang, and Ali Jadbabaie. non-asymptotic analysis of oversmoothing in graph neural networks. arXiv preprint arXiv:2212.10701, 2022. Xinyi Wu, Amir Ajorlou, Yifei Wang, Stefanie Jegelka, and Ali Jadbabaie. On the role of attention masks and layernorm in transformers. arXiv preprint arXiv:2405.18781, 2024a. Xinyi Wu, Amir Ajorlou, Zihui Wu, and Ali Jadbabaie. Demystifying oversmoothing in attentionbased graph neural networks. Advances in Neural Information Processing Systems, 36, 2024b. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim. Parallelizing linear transformers with the delta rule over sequence length. arXiv preprint arXiv:2406.06484, 2024. Michael Zhang, Kush Bhatia, Hermann Kumbong, and Christopher Ré. The hedgehog & the porcupine: Expressive linear attentions with softmax mimicry. arXiv preprint arXiv:2402.04347, 2024a. Tao Zhang, Xiangtai Li, Haobo Yuan, Shunping Ji, and Shuicheng Yan. Point could mamba: Point cloud learning via state space model. arXiv preprint arXiv:2403.00762, 2024b. Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang. Vision mamba: Efficient visual representation learning with bidirectional state space model. arXiv preprint arXiv:2401.09417, 2024. Andy Zou, Zifan Wang, Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043, 2023."
        },
        {
            "title": "A OTHER RELATED WORK",
            "content": "Despite the empirical success of SSMs in various long-range applications (Lieber et al., 2024; Zhu et al., 2024; Zhang et al., 2024b), their theoretical properties and limitations remain underexplored. Arora et al. (2023) leverages associative recall tasks to theoretically analyze the expressiveness of convolutional SSMs, while advocating for input-dependent kernels. Jelassi et al. (2024) separates the representation capacity of transformers and SSMs via coping tasks. Recent work by Merrill et al. (2024) identifies failure modes of SSMs in state-tracking problems through circuit complexity theory. Hahn (2020) characterize the expressivity of SSMs using linear controlled differential equations, and Ali et al. (2024) reveal structural similarities between selective SSMs and attention mechanisms. While these works provide valuable insights consistent with our findings, none directly examine the long-range modeling capability of SSMs. Ben-Kish et al. (2024) points out that the product of gating matrices (formalized in our Lemma D.1) exhibits locality issues, potentially hindering their ability to model long-range dependencies. However, their analysis lacks formal justification. The most relevant prior work to our study on recency bias is perhaps Wang & Xue (2024), in which Their Theorem 3.13 reveals exponentially decaying memory for SSMs. Distinct from their findings, our primary contribution lies in analyzing nonlinearity and input-dependent mechanisms widely adopted in modern SSMs (Gu & Dao, 2023; Yang et al., 2023; Arora et al., 2023). To the best of our knowledge, our work presents the first counterargument showing that even with input-dependent SSMs, effective context filtering may not be achieved. Instead, these mechanisms impose strict recency bias, inheriting the limitations of linear SSMs as previously highlighted by Wang & Xue (2024). In particular, Wang & Xue (2024) only addresses non-linear activations after performing linear SSMs (i.e., S4), with the state transition matrix At being independent of inputs. Our analysis, however, considers both At and bt as input-dependent, aligning with settings in Mamba and other follow-up works. The approach in Wang & Xue (2024) does not naturally extend to this case, as they assume the linearity of the sequence mixer (cf. Eq. 9 in Wang & Xue (2024)). Furthermore, we conduct finer-grained analysis, quantitatively relating the decay rate to the specific values within the input-dependent gating matrix At. To our best knowledge, none prior work revealed the smoothening nature of SSM operators."
        },
        {
            "title": "B UNIFIED FORMULATION OF SSMS",
            "content": "Linear Attention Models (LAMs) represent vast family of architectures evolving rapidly. We present reformulation of several representative LAMs using the SSM framework described in Eq. 1. This reformulation is expected to extend to models not explicitly covered below, including but not limited to Megalodon (Ma et al., 2022; 2024), Hyena (Poli et al., 2023), HGRN2 (Qin et al., 2024), TTT (Sun et al., 2024), Longhorn (Liu et al., 2024), xLSTM (Beck et al., 2024), and minLSTMs/minGRUs (Feng et al., 2024). Linear Attention. Attention notoriously suffers from quadratic computation and memory complexities. To address this limitation, Tsai et al. (2019); Katharopoulos et al. (2020) finds that if we express self-attention as linear dot-product of kernel feature maps, then we can interchange the order of matrix products to achieve linear time complexity. This finding leads to series of LAMs. Ignoring the denominator therein, we can formalize linear attention via the recurrence in Eq. 1: (LA) At = I, bt(xt) = k(xt), ct(ht) = q(xt)ht, = v(xt), where k, : RN are the kernel feature maps and : transforms the input features. Considering multi-channel inputs, k, q, and become functions processing vectors, and for each channel, the and are shared, while being specified individually. The roles of k, q, are similar to the key, query, and value transformations in standard transformer (Vaswani et al., 2017). While k, q, are often chosen as linear mappings, many other options exist. Katharopoulos et al. (2020) adds 1+ELU after linear mapping, Choromanski et al. (2020) leverages orthogonal random Fourier features with hyperbolic cosine as activations, and Zhang et al. (2024a) finds MLP with exponential activation effective. Retentive Networks (RetNet). RetNet (Sun et al., 2023) is another variant of LAMs, proposed as successor to transformers given its remarkable performance. Each layer of RetNet consists of key, 16 query, and value transformation, akin to linear attention. In addition, it imposes new decaying term over the past states. We find RetNet can be summarized via our formulation in Eq. 1: (RetNet) At = γI, bt(xt) = k(xt), ct(ht) = q(xt)ht, = v(xt), where γ (0, 1) is (learnable) scalar, k, : RN , : are linear functions. Similar to Mamba (Gu & Dao, 2023), RetNet shares bt and ct across channels while assigning distinct for each channel when handling multi-channel inputs. Gated Linear Attention (GLA). GLA (Yang et al., 2023) introduces gating mechanism, originally from RNNs (Van Der Westhuizen & Lasenby, 2018), to LAMs. Its computational mechanism can be encompassed by our formulation in Eq. 1: (GLA) At = diag(α(xt)), bt(xt) = k(xt), ct(ht) = q(xt)ht, = v(xt), where α : (0, 1)N converts input to gating logits, k, : RN , : are linear key, query, value mappings. When the inputs are multi-dimensional, we share α across channels while assigning each channel with separate k, q, and extending their input dimension accordingly. Linear attention (Katharopoulos et al., 2020) can be regarded as GLA with constant At, while RetNet (Sun et al., 2023) can be formulated as GLA with input-independent At (Liu et al., 2024). RWKV. RWKV is series of models that linearize attention computation (Peng et al., 2023; 2024). We focus on RWKV-4 (Peng et al., 2023) and demonstrate it can also be reformulated into the structure of SSMs: (RWKV) At = exp(w)I exp(w) + exp(k(xt)) , bt(xt) = v(xt), ct(ht) = q(xt)ht, = exp(k(xt)) exp(w) + exp(k(xt)) , where R+ is learnable coefficient, : and q, : RN are linear mappings. If the inputs are vector-valued, becomes vector while k, q, turns into functions that take vectors as inputs. One important property of RWKV is that (At)n,n + = 1. Later in RWKV-5 and RWKV-6 (Peng et al., 2024), the normalizer is removed for numerical stability. Griffin. The recurrent unit in Griffin (De et al., 2024) can be re-formulated as kind of SSMs: (Griffin) At = diag (α(xt)), bt(xt) = diag (i(xt)), ct(ht) = ht, = diag (cid:16)(cid:112)1 α(xt)2 (cid:17) , where i(xt) = sigmoid(Wxxt + bx) is an input gate, α is computed in log-space: log α(xt) = ξ softplus(Γ) sigmoid(Waxt + ba), is Hadamard product, ξ is constant, and Γ, Wa, ba, Wx, bx are learnable parameters. In particular, the dimension of ht in Griffin is equal to the dimension of xt. If we consider single-channel xt, then At, bt, and are all scalar-valued."
        },
        {
            "title": "C DEFERRED DISCUSSIONS",
            "content": "Extended discussion with HiPPO theory. HiPPO established in (Gu et al., 2020), extended by Gu et al. (2021b; 2022b) is the theoretical foundation of SSMs. Consider signal and its reconstruction y(t) up to time t. To optimally memorizes the history of using y(t), HiPPO minimizes xt y(t)L2(ω(t)) w.r.t. measure ω(t) supported on (, t]. The solution is to project the history of before time onto basis functions (e.g. Legendre polynomials), which yields time-continuous coefficient vector h(t), and y(t) can be synthesized by linearly combining the basis using h(t). Gu et al. (2020) shows that the evolution of h(t) follows an ODE h(t) = A(t)h(t) + b(t)x(t). In particular, Gu et al. (2020) chooses uniform measure over the past history ω(t) = I[0, t]/t, which places no approximation bias over the time horizon in contrast to an earlier work (Voelker et al., 2019). As result, A(t) in Eq. 1 can be written in closed form: A(t) = Ahippo/t, where Ahippo is time-independent constant called the HiPPO matrix. Its various forms have been used as initialization in subsequent works including S4 (Gu et al., 2021a) and Mamba (Gu & Dao, 2023). While HiPPO theory seems to guarantee the long-rangeness for SSMs, the actual form of A(t) employed in S4 and 17 Mamba drops the normalizer 1/t. Gu et al. (2022b) shows that this change causes warp of measure from uniform to ω(t)(s) exp(s t)I(, t]. We note that this warped measure assigns more importance to recent history, and thus, our Theorem 3.1 does not contradict HiPPO theory but also matches the findings in Gu et al. (2022b). We also point out that when adopting the diagonalized form of Ahippo (Gu et al., 2021a; Gupta et al., 2022; Gu et al., 2022a), the unitary matrices decomposed from Ahippo is sometimes not applied to bt and ct, which introduces disconnect between HiPPO theory and its practical implementation. Our paper directly studies the discrete-domain SSMs and aligns with the parameterization used in practice. Another less-discussed property of SSMs is their approximation power for broad family of operators. In Gu et al. (2021b), SSMs are shown to possess expressiveness that encompasses both convolutions and RNNs. It is worth noting that the original HiPPO-based SSM is not necessarily low-pass filter. Rather, it is the successive simplifications in the parameterization of At that impart the smoothing characteristics of SSMs (see Proposition 4.1). Does hungry hungry hippos help? The key innovation of Hungry Hungry Hippos (H3) (Fu et al., 2022) lies in the introduction of self-gating connections and locally shifting convolutions to improve in-context recall for state space models (SSMs). This design has quickly become standard backbone for various SSMs (Gu & Dao, 2023; Beck et al., 2024). However, we question its effectiveness in addressing the local rangeness issue in SSMs. The gating mechanism operates at the token level, which impacts the bound in Theorem 3.1 only by constant factor. Additionally, the introduced convolutions typically use small kernels, which are insufficient to mitigate the exponentially decaying relevance between tokens. As we empirically show in Fig. 2, while Mamba with H3 performs adequately in associative recall tasks when the state size is sufficiently large, locality bias begins to emerge as the number of key-value pairs exceeds the models memory capacity. This highlights the limitations of the architecture in handling long-range dependencies under constrained memory. Does gradient vanish in SSMs? Vanishing gradients refer to challenge in RNNs, where backpropagation-based learning is impeded due to gradient magnitudes decaying exponentially over time. The diminishing dependencies among distant tokens are fundamental cause of this issue (Bengio et al., 1994). SSMs were initially proposed to address this limitation by explicitly modeling long-range dependencies, as highlighted in (Gu et al., 2020; 2021b). Subsequent work, such as Mamba, extends this approach by adopting their initialization alongside newly proposed selection mechanisms, which are widely believed to enhance these capabilities. Our Theorem 3.1 lies in theoretically challenging this assumption. We demonstrate that modern SSMs still suffer from the recency bias, which not only undermines their ability to capture long-term dependencies but also potentially exacerbates the vanishing gradient problem. Connection with over-squashing theory in GNNs. We can compare recency of SSMs to oversquashing in GNNs. The influential score defined in Sec. 3.1 is also used for over-squashing analysis in Graph Neural Networks (GNNs) to identify information bottleneck (Topping et al., 2021; Di Giovanni et al., 2023). The sensitivity analysis in Topping et al. (2021) demonstrates similar exponentially decaying dependencies among graph nodes, dependent on the underlying graph topology. We postulate that propagating information from long distances remains challenging for SSMs because the model needs to encapsulate all history information into fixed-dimension hidden vector, which is also observed as one major problem with RNNs (Bengio et al., 1994; Alon & Yahav, 2020; Sutskever et al., 2014; Cho et al., 2014; Cho, 2014). Connection with over-smoothing theory in GNNs and transformers. Over-smoothing issues were first identified in GNNs (Li et al., 2018; NT & Maehara, 2021; Oono & Suzuki, 2019; Cai & Wang, 2020; Wu et al., 2022; 2024b) and later explored in transformers (Dong et al., 2021; Wang et al., 2022; Shi et al., 2022; Wu et al., 2024a; Geshkovski et al., 2023). In both cases, over-smoothing manifests as feature representations becoming increasingly uniform with greater model depth. To the best of our knowledge, our work is the first to uncover this phenomenon in SSMs. In GNNs, the over-smoothing effect typically follows linear decay rate governed by the second-largest eigenvalue of the adjacency matrix: O(λL 2 ), where denotes the number of layers (Oono & Suzuki, 2019; Cai & Wang, 2020). More closely related to our setting, Wu et al. (2024a) analyzes the convergence rate of feature smoothness in causal attention, demonstrating rate of min)L/T ), where ˆAmin is the minimal value in attention maps. In comparison, our result O((1 ˆAT min)L), indicating that the smoothening speed of SSMs is in Theorem 4.2 shows rate of O((1 AT 18 faster than that of transformers. Our Fig. 5 supports this claim, as the transformer-based architecture exhibits mild decrease of sharpness at deeper layers, whereas SSMs show constantly steeper decay slope. Moreover, due to the inherent locality of SSMs (Theorem 3.1), achieving effective long-range interactions necessitates deeper architectures. In contrast, transformers, which allow arbitrary long-range interactions among tokens, do not have the same requirement. This distinction is partially supported by the observation that modern SSMs often adopt architectures that are roughly twice as deep as transformers. Consequently, depth-scaling limitations are more critical for SSMs than for transformers."
        },
        {
            "title": "D PROOFS",
            "content": "D.1 EXPONENTIALLY DECAYING DEPENDENCY WITH RELATIVE DISTANCE In this section, we extend and prove Theorem 3.1 in more general case where inputs and parameters are all complex-valued. Below we by default assume that At CN , bt : CN , ct : CN C, and R. First of all, we present the following auxiliary lemma, reformulating SSM recurrence in an explicit parallel form. Lemma D.1 (Parallel form). For any {(At, bt, ct, t)}t[T ] and CT , CT computed via an SSM defined in Eq. 1 is equal to: ht = t1 (cid:88) (cid:32) (cid:89) s= r=s+1 (cid:33) Ar sbs(xs) + tbt(xt), yt = ct(ht), [T ]. (5) Proof. Proof by induction. First let us examine the base case when = 1: h1 = 1b1(x1), indicating that Eq. 5 holds trivially. Now we make inductive hypothesis that Eq. 5 holds for some [T 1]. Then for time step + 1 [T ]: ht+1 = At+1h + t+1bt+1(xt+1) (cid:32)t1 (cid:88) (cid:32) (cid:89) (cid:33) = At+1 s= r=s+1 Ar sbs(xs) + tbt(xt) + t+1bt+1(xt+1) (cid:33) (cid:89) r=s+1 (cid:33) t1 (cid:88) s=1 t1 (cid:88) s=1 (cid:88) = = = (cid:32) At+1 (cid:32) t+1 (cid:89) r=s+1 (cid:32) t+1 (cid:89) Ar sbs(xs) + (cid:33) (cid:33) Ar sbs(xs) + At+1tbt(xt) + t+1bt+1(xt+1) (cid:32) t+1 (cid:89) r=t+1 (cid:33) Ar tbt(xt) + t+1bt+1(xt+1) Ar sbs(xs) + t+1bt+1(xt+1), s=1 r=s+1 which also satisfies Eq. 5. Then we conclude the proof by induction. remark of Lemma D.1. Lemma D.1 provides an alternative perspective on how SSMs compute the outputs. The predicted value for the t-th token is obtained via decoding weighted aggregation over representations of all past tokens. The encoding and decoding stage is element-wise independent of the context. Whereas, the weight associated with each past token in the summation reflects the pairwise relationship, playing similar role to attention weights in transformers (Dao & Gu, 2024; Ali et al., 2024). The weight corresponding to one past token is calculated as the cumulative product (cid:81) Ar, where [s + 1, t] traverses from the past token (at time s) to the target token (at time t). Assume At (0, 1)N , which is satisfied by most of SSMs discussed in Sec. 2, we can show that ((cid:81)t r=s+1 Ar)n,n for any < < and [N ]. By this interpretation, SSMs assign strictly higher attention to the nearer tokens than the further tokens. r=s+1 Ar)n,n < ((cid:81)s Below we formally state and prove the complex version of Theorem 3.1. Theorem 3.1 is straightforward corollary of Theorem D.2. 19 Theorem D.2 (Recency of SSMs). Consider an SSM defined in Eq. 1 with {(At, bt, ct, t)}t[T ]. Assume that: (i) The input space CT is compact. (ii) {(At, bt, ct, t)}t[T ] and (cid:110)(cid:16) At xt , bt xt , ct xt , xt (cid:17)(cid:111) t[T ] are continuous. (iii) At is diagonal and 0 < (At)n,n < 1 for all [T ], [N ]. Let Amax = maxt[T ],n[N ](At)n,n. Then for arbitrary and every s, [T ] such that < t, (cid:12) (cid:12) (cid:12) (cid:12) yt xs (cid:12) (cid:12) (cid:12) (cid:12) exp (κ(t s)) , for some constant > 0 independent of and s, and κ = log(A1 max). Proof of Theorem 3.1. First of all, we note that by compactness of input space (Assumption (i)) and continuity (Assumption (ii)), (At, bt, ct, t) and are all bounded by some constant for every [T ]. (cid:16) At xt , xt , bt xt , ct xt (cid:17) By Lemma D.1, Eq. 1 can be expressed in the closed form as follows: ht = t1 (cid:88) (cid:32) (cid:89) i=1 r=i+1 (cid:33) Ar ibi(xi) + tbt(xt) exp exp (cid:32) (cid:88) r=i+ (cid:32) (cid:88) r=i+1 = = t1 (cid:88) i=1 s1 (cid:88) i=1 (cid:124) (cid:33) log Ar ibi(xi) + tbt(xt) (cid:33) log Ar ibi(xi) + (cid:123)(cid:122) ut (cid:125) t1 (cid:88) i=s (cid:124) (cid:32) (cid:88) exp (cid:33) log Ar ibi(xi) +tbt(xt), r=i+1 (cid:123)(cid:122) vt (cid:125) where the second equality is by simply rewriting the cumulative product as an exponential of cumulative summation of logarithms. The logarithmic terms are well defined and constantly negative due to Assumption (iii). We decompose ht into three components, where the first term, denoted as ut, depends on xs only through {Ar}t r=s, the second term, denoted as vt, only relies on xs via {ibi}t i=s, while the remaining part is independent of xs. Now we tackle each component separately. First, we simplify ut/xs as: ut xs = = = = xs s1 (cid:88) i=1 s1 (cid:88) i=1 s1 (cid:88) i=1 (cid:34)s1 (cid:88) exp (cid:32) (cid:88) (cid:33) (cid:35) log Ar ibi(xi) i=1 (cid:32) xs exp r=i+1 (cid:32) (cid:88) r=i+ (cid:33)(cid:33) log Ar ibi(xi) exp exp (cid:32) (cid:88) r=i+1 (cid:32) (cid:88) r=i+1 log Ar (cid:33) (cid:32) (cid:88) r=i+1 (cid:33) log Ar xs ibi(xi) log Ar (cid:33) (cid:18) A1 (cid:19) As xs ibi(xi), 20 where the last steps follow from the basic chain rule. Then we can bound its ℓ1 norm by: (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:18) 1 Ar ibi(xi)n Ar xs ut xs log Ar (cid:13) (cid:13) (cid:13) (cid:13)1 s1 (cid:88) (cid:88) (cid:32) (cid:13) (cid:13) (cid:13) (cid:13) exp (cid:88) n,n (cid:33) (cid:19) n=1 i=1 (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) n,n (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:33) (cid:88) C1 exp log(Ar)n,n r=i+1 (cid:32) (cid:88) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) s1 (cid:88) n=1 i=1 r=i+1 s1 (cid:88) C1N exp (log Amax(t i)) i=1 1 exp ( log Amax(s 1)) 1 A1 max exp (log Amax(t 1)) exp (log Amax(t s)) exp (log Amax(t 1)) A1 max 1 max(t s)(cid:1) . exp (cid:0) log A1 = C1N = C1N C1N max 1 A1 (6) (7) (8) (9) (10) We elaborate on the derivation step by step. Eq. 6 is due to triangular inequality. To obtain Eq. 7, we note that A1 , Ar/xs, i, bi are all element-wisely bounded, thus, we can extract their uniform upper bound C1 > 0 out of the summation Eq. 8 can be derived by applying the supremum Amax over all {(At)n,n}t[T ],n[N ]. Eq. 9 follows from the summation of geometric series. And finally, inequality in Eq. 10 holds by dropping negative term. Similarly, we rewrite vt/xs as below: (cid:34)t1 (cid:88) vt xs = xs (cid:32) (cid:88) exp (cid:33) (cid:35) log Ar ibi(xi) i=s r=i+1 = t1 (cid:88) i=s exp (cid:32) (cid:88) r=i+ (cid:33) log Ar (ibi(xi)) xs = exp (cid:32) (cid:88) r=s+1 (cid:33) log Ar (sbs(xs)) xs , by which we can yield the following upper bound on its ℓ1 norm: (cid:12) (cid:12) (cid:12) (cid:12) vt xs log Ar (cid:32) (cid:88) (cid:13) (cid:13) (cid:13) (cid:13) exp (cid:88) (cid:33) (cid:13) (cid:13) (cid:13) (cid:13)1 n= (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) n,n (sbs(xs)n) xs (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:88) r=s+1 (cid:32) (cid:88) C2 exp (cid:33) log(Ar)n,n (11) n=1 r=s+1 (12) where Eq. 11 is obtained by applying uniform upper bound C2 > 0 over (sbs(xs)n)/xs. Eq. 12 is induced by leveraging the supremum Amax over all {(At)n,n}t[T ],n[N ]. C2 exp (cid:0) log A1 max(t s)(cid:1) , Combining Eqs. 10 and 12, we have: (cid:13) (cid:13) (cid:13) (cid:13) (cid:18) C1N A1 ht xs (cid:13) (cid:13) (cid:13) (cid:13)1 max 1 (cid:19) + C2 exp (cid:0) log A1 max(t s)(cid:1) . (13) Finally, we conclude the proof based on Eq. 13: (cid:12) (cid:12) (cid:12) (cid:12) ht xs yt xs (cid:12) (cid:12) (cid:12) (cid:12) (cid:13) (cid:13) (cid:13) (cid:13) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) = ct(ht) ht (cid:18) C1N A1 = exp (κ(t s)) max 1 C3 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) ht xs ct(ht) ht (cid:19) + C2 exp (cid:0) log A1 (cid:13) (cid:13) (cid:13) (cid:13)1 max(t s)(cid:1) , where we use Hölder inequality in the first equation and upper bound ct(ht)/ht again via constant C3 > 0 due to continuity (ct(ht) is composition of series of continuous functions as in Assumption (ii)). This is as desired after letting > 0 absorb all constants and κ = log A1 max. D.2 OVER-SMOOTHING IN SSMS D.2.1 LOW-PASS FILTERING PROPERTIES In this section, we formally state and prove Proposition 4.1. To begin with, we define low-pass filters as below: Definition D.3 (Low-pass Filter). Suppose z(t) : is absolutely integrable. Then the Fourier transform Z(ω) : exists and Z(ω) (cid:82) z(t) exp(iωt)dt. We say z(t) is an (ϵ, Ω)-low-pass filter for some 0 < ϵ < 1 and Ω > 0 if Z(ω) ϵ for every ω Ω. Furthermore, we say z(t) is low-pass filter if for every 0 < ϵ < 1, there exists Ω > 0 such that z(t) is an (ϵ, Ω)-low-pass filter. Note that our definition of low-pass filters focuses on its effect on removing high-frequency components. By Definition D.3, it is equivalent to say system performs low-pass filtering if and only if the responses converge to zero as the frequency increases. Fixing ϵ > 0 small enough, we can say (ϵ, Ω)-low-pass filter has cut-off frequency at ω = Ω. Now we present formal version of Proposition 4.1 as below, which quantifies the cut-off frequency of filter induced by continuous-time S4. Note that Proposition D.4 holds for complex-valued (A, b, c). Below we assume CN , CN , CN by default. Proposition D.4 (Formal version of Proposition 4.1). Consider continuous-time S4 with parameters (A, b, c). Assume CN is diagonal and its diagonal values have negative real parts. Then y(t) = (cid:82) exp(A(t s))bx(s)ds is an (ϵ, O(1/ϵ))-low-pass filter for any ϵ > 0 sufficiently small. Proof. First of all, let us rewrite the filter induced by S4 as below: z(t) = (cid:88) n=1 cnbn exp(An,nt). Then we apply Fourier transform on z(t): (cid:90) Z(ω) = z(t)eiωtdt = (cid:90) (cid:88) n= cnbn exp(An,nt) exp(iωt)dt = = = (cid:88) n= (cid:88) n=1 (cid:88) n=1 (cid:90) (cid:90) cnbn cnbn exp(An,nt) exp(iωt)dt exp((An,n iω)t)dt cnbn iω An,n , where the last integral converges due to ℜ(An,n) < 0. Then we can upper bound the magnitude of Z(ω) as: Z(ω) = (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:88) n=1 cnbn iω An,n (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:88) n=1 (cid:12) (cid:12) (cid:12) (cid:12) cnbn iω An,n (cid:12) (cid:12) (cid:12) (cid:12) (cid:88) n=1 cnbn ω An,n . We consider ϵ > 0 small enough, thus, it is sufficient to consider the scenario when ω > Amax maxn[N ] An,n. In this regime, we have: Z(ω) (cid:33) cnbn (cid:32) (cid:88) n= 1 ω Amax b2c2 ω Amax , 22 where the last two inequalities are due to Hölders inequality. It is sufficient to set: Ω = b2c2 ϵ + Amax, to let Z(ω) ϵ for every ω Ω hold. D.2.2 GENERALIZED SMOOTHENING PROPERTIES In this section, we define generalized smoothening operators as those that narrow distances between token features. We provide the following results that formalize this point. Theorem D.5 below extends the result of Theorem 4.2 to include the case when (At)n,n + = 1 for every [N ], [T ], which is more aligned with Ma et al. (2022; 2024); Peng et al. (2023) in practice. Theorem D.5 (Complete version of Theorem 4.2). Consider an SSM specified in Eq. 1 with {(At, bt, ct, t)}t[T ]. Assume an input space RT such that for every , either of the following conditions is satisfied: (i) (At)n,n + = 1 for every [N ] and [T ], or (ii) (At)n,n + 1 for every [N ], [T ], and mint[T ] bt(xt)n 0 and maxt[T ] bt(xt)n 0 for every [N ] . Let Amin = mint[T ],n[N ](At)n,n. Then for any and the memory states {ht : [T ]} generated by the SSM, we have: max t,s[T ] ht hs (cid:0)1 AT min (cid:1) max t,s[T ] bt(xt) bs(xs) , Proof. To simplify the proof, we first only consider the dynamics of one channel in the memory state. We denote αt = (At)n,n, zt = bt(xt)n, and st = ht,n for some [N ]. Additionally, we define the following two quantities: = min t[T ] zt, = max t[T ] zt When Assumption (ii) holds, we know that 0 and 0. Suppose z1 = pm + (1 p)M for some [0, 1]. Furthermore, let = 1 p. Now we consider the following dynamics with s0 = 0: (14) st = αtst1 + tzt, [T ], Next, we can show the result below (proved later): Lemma D.6. We have the following inequality for Eq. 14: (1 At1 minq)m + At1 minqM st At1 minpm + (1 At1 minp)M, [T ], if either of the following conditions holds: (i) αt + = 1; (ii) αt + < 1 and 0 . Note that the two conditions in Lemma D.6 corresponds to Assumptions (i) and (ii) in Theorem D.5, respectively. Thus we can upper and lower bound the minimum and maximum of memory states {st : [T ]} through: min t[T ] st min t[T ] (1 At1 minq)m + At1 minqM as we are moving the convex combination towards the smaller end (i.e., 1 AT 1 and similarly, min > 1 At minq), = (1 AT 1 min q)m + AT 1 min qM, max t[T ] st max t[T ] AT At1 minpm + (1 At1 minp)M min pm + (1 AT 1 min p)M. as we are now relaxing the convex combination towards the larger end (i.e., 1 AT 1 Henceforth, we can upper bound: min > 1 At1 minp) AT 1 min pm + (1 AT 1 min )(M m), = (1 AT min p)M (1 An1 minq)m An1 minqM using the fact that + = 1. Now we can apply the above result to all memory channels. Assigning each channel with mn = mint[T ] bt(xt)n, Mn = maxt[T ] bt(xt)n and accordingly, where [N ]. We can yield: n) (cid:1) (Mn mn) (cid:0)1 AT 1 ht hs max n[N ] max t,s[T ] n, (M min max n[N ] (cid:0)1 AT min (cid:1) max n[N ] (Mn mn) = (cid:0)1 AT 1 min = (cid:0)1 AT 1 min (cid:1) max n[N ] (cid:1) max n[N ] (cid:18) (cid:19) max t[T ] bt(xt) min t[T ] bt(xt) max t,s[T ] (bt(xt) bs(xs)) (cid:0)1 AT 1 min (cid:1) max t,s[T ] bt(xt) bs(xs) , where we exchange the two maximum in the last step. Finally, we prove an auxiliary result we used before. This result generalizes Lemma 3 in Wu et al. (2024a) in the scenario when αt + < 1 and 0 . Proof of Lemma D.6. We first prove the side that is less than. Note that the desired inequality trivially holds for the base case s1. Then we conduct the following inductive steps. Suppose the desired inequality holds for some [T 1], namely: sr Ar1 minpm + (1 Ar1 minp)M. Then we show that for time step + 1 [T ], sr+1 = Ar+1sr + r+1zr+1 Ar+1 Ar+1 Amin = Ar (cid:0)Ar1 (cid:0)Ar1 (cid:0)Ar1 minpm + (1 Ar minpm + (1 Ar1 minpm + (1 Ar1 minpm + (1 Ar1 minp)M, where we substitute the upper bounds of st (by the inductive hypothesis) and zt in Eq. 15, Eq. 16 holds because either r+1 = 1 Ar+1 (by Assumption (i)) or r+1 1 Ar+1 and 0 (by Assumption (ii)), Eq. 17 is satisfied as we are lowering down the coefficient for the smaller end in the convex combination. This concludes the proof by induction. minp(cid:1) ) + r+1M minp(cid:1) ) + (1 Ar+1)M minp(cid:1) ) + (1 Amin)M (17) (16) (15) The proof for the greater than side follows from symmetric argument. We provide brief proof for completeness. The base case s1 trivially satisfies the desired inequality. Consider an inductive step where sr for some [T 1] also satisfies the desired inequality. Then we have: sr+1 = Ar+1sr + r+1zr+1 Ar+1 Ar+1 Amin = (1 Ar (cid:0)(1 Ar1 (cid:0)(1 Ar1 (cid:0)(1 Ar1 minq)m + Ar min)qm + Ar1 min)qm + Ar1 min)qm + Ar1 minqM, minqM )(cid:1) + r+1m minqM )(cid:1) + (1 Ar+1)m minqM )(cid:1) + (1 Amin)m which concludes the proof by induction. 24 (a) Influential scores. (b) Influential score ratios. Figure 7: Visualization of influential scores for Mamba and Transformer models under 150M and 1.4B parameters. (a) illustrates the influence score on the last output token by the preceding input tokens. (b) shows Mamba-to-transformer ratios of influential scores within the nearest 128 tokens."
        },
        {
            "title": "E EXTENDED EXPERIMENTS",
            "content": "E.1 INFLUENTIAL SCORES: TRANSFORMERS VS. MAMBA We further compare how influence scores vary with the relative distance between Transformers and Mamba. For our study, we select Pythia (Biderman et al., 2023), which integrates Rotary Position Encoding (RoPE) (Su et al., 2024) into the attention computation. Fig. 7a illustrates the influence scores of each token contributing to the last token. Pythia demonstrates the well-known lost in the middle pattern, where tokens at the beginning and end have high influence scores, while those in the middle exhibit lower scores. In contrast, Mambas highly influential tokens are more concentrated in the nearest positions. We also compare the decay rate of influence scores for these two models at local tokens. Each curve as shown in Figure 7b represents the ratio of Mambas influence scores to those of Pythia of various model sizes. The dotted curves are fitted with an exponential function. Our findings reveal that Mamba exhibits an exponentially faster decay speed compared to Pythia. This suggests that Mamba places greater emphasis on local tokens than transformers. E.2 NEEDLE IN HAYSTACK TEST Our experiments of testing positional bias for Mamba in Sec. 3.2 are based on an open-source project LLMTest_NeedleInAHaystack 3. To validate the retrieval capability of the models while preventing them from relying on memorized information stored in their model weights, we carefully design the inserted statements to contain factual errors. Several examples of such statements are provided in Figure 8. For instance, we insert the statement, The capital of France is Madrid. and then test the models retrieval ability by asking the question, What is the capital of France? While the correct answer, Paris, is likely memorized by the LLM, if the model correctly outputs Madrid based on the context provided, it demonstrates that the model is successfully using the contextual information rather than relying on pre-existing knowledge. This approach ensures that the evaluation focuses on the models ability to retrieve and process information from the input context. We also add an instruction ignore the fact, only use information in the context to answer the questions to facilitate this behavior. We provide the fine-grained visualization result in Figure 9. E.3 CIFAR-10 IMAGE CLASSIFICATION Here we present experiment details in Sec. 3.3, where we conduct image classification on the CIFAR-10 dataset to study locality bias in SSMs. Specifically, 32 32 RGB images in the dataset are 3https://github.com/gkamradt/LLMTest_NeedleInAHaystack 25 Figure 8: An illustration of our synthetic data. Figure 9: Detailed comparison between SSM and Transformer on the Needle in Haystack\" benchmark. The upper figure shows the retrieval accuracy of the Mamba-Codestral-7B model, while the lower figure presents the retrieval accuracy of the Mistral-7B model. We present heatmap where \"full context length\" refers to the total length of the document, and \"needle position\" denotes the relative position of the statement to be retrieved within the context. flattened into sequences with shape of (1024, 3), where 1024 represents the sequence length and 3 corresponds to the RGB channels of the pixel tokens. These pixel tokens are then linearly projected into H-dimensional features, which are then input into SSM or transformer mixers. In addition to pixel tokens, we insert class token at the last position of the input sequence. The output state of the class token will be processed by one-layer classifier head to generate the final logits. Note that while the ViT architecture (Dosovitskiy et al., 2020) places the class token at the first position of the input sequence, this design is incompatible with SSMs, which rely on causal sequence modeling. In SSMs, the class token must be positioned last to aggregate features from the entire sequence. We position the class token as the last token to establish long-range dependencies between 26 Table 3: Extended results of adversarial attack experiments on the CIFAR-10 dataset. Classification accuracy is used as the metric. Corrupted region (seq. length = 1024) Models H3 Transformer RWKV Mamba (no corrupt) 0.654 0.580 0.474 0.674 [1014:1024] 0.629 0.571 0.194 0.348 [0:10] 0.654 0.500 0.470 0.664 [768:1024] 0.394 0.249 0.107 0.099 [0:256] 0.639 0.263 0.448 0.597 [512:544] 0.603 0.498 0.405 0. [480:576] 0.543 0.347 0.392 0.446 global image features and the leading pixel tokens. Alternative methods for aggregating features across the entire sequence, such as mean pooling (Gu et al., 2021a; Tay et al., 2020) or placing the class token in the middle of the sequence (Zhu et al., 2024), work more robustly in general but do not fit the needs for our arguments on locality. In addition, our image classification setup differs from Tay et al. (2020), where an 8-bit pixel intensity lookup table is used as the token embedder. Instead, we employ linear projection to map RGB pixel colors into H-dimensional features. For fair comparison, the same hyperparameters are used across all models: learning rate = 0.001, weight decay = 0.1, number of layers = 3, feature dimension = 32, and number of states = 64. Each model is trained for 100 epochs. The models and training pipelines are built on Arora et al. (2023). No perturbations are imposed on the input sequences in the training stage. Adversarial Attack. To introduce perturbations to test data for adversarial attack, we first define corruption length K, which is small relative to the entire sequence length. We then replace the leading and trailing tokens with random Gaussian noise. In our experiments, is set to 32 and 96, corresponding to one row and three rows of pixels, respectively. Table 3 shows more results under other corruption regions. Target Attack. For the target attack experiments, target class is first selected. For each image from the other classes, an image from the target class is randomly sampled, and its leading and trailing pixels are used to replace the corresponding pixels in the original image. We test two attack ratios: 256/1024 and 480/1024. Replacing fewer than 256 pixels generally does not result in considerable success rates based on our trials. In our main text, we show success rates when horse is the target class. Similar patterns are also observed across other classes. Fig. 10 shows the average success rates obtained by setting each class as the target. Figure 10: Overall success rate of our target attack experiments on CIFAR-10, calculated by averaging the attack success rates obtained when each class is individually set as the target class. E.4 SCALING LAW WITH VARYING DEPTH In this section, we elaborate on experiment details for Sec. 4.1. We perform casual language modeling (Radford et al., 2018) with Mamba and report the validation perplexity. For each curve in Fig. 4, we fix the depth of the model and vary the number of parameters from 140M to 550M by adjusting the hidden dimension accordingly. The depth is chosen from {16, 24, 32, 48, 64, 72}. All other model configurations keep the default values: the memory state size = 16, the intermediate SSM 27 dimension is two times the hidden dimension, and the intermediate rank for predicting time step is the hidden dimension divided by 16. We adopt DataComp-LM (Li et al., 2024) as our training corpus for its high data quality. The evaluation set is created by holding out subset of 10M tokens from the training data. We follow the training recipe provided in Appendix E.2.1 of Gu & Dao (2023), which roughly follows the Chinchilla scaling laws (Hoffmann et al., 2022) to allocate the number of tokens for each model size. We test two block sizes {2048, 8192}. During training, we fix the number of tokens in one training batch (# sequences sequence length) as 0.5M. The number of training steps is computed by the total number of tokens divided by batch size (# tokens / # tokens in one batch). We use AdamW as the optimizer with gradient clip value 1.0, weight decay 0.1, and linear learning warmup with cosine decay to 1e-5, following Gu & Dao (2023). The peak learning rate is 5 the GPT3 values (Brown et al., 2020) for different model sizes. We summarize the training hyperparameters for varying-sized models in Tab. 4. # Params"
        },
        {
            "title": "Training steps",
            "content": "Peak LR Batch Size (in tokens) # Tokens 100-250M 250-400M 400-550M 4800 13500 20000 3e-3 1.5e-3 1.25e-3 0.5M 0.5M 0.5M 2.5B 7B 10B Table 4: Summary of training settings for varying-sized Mamba. The settings are following Chinchilla law (Hoffmann et al., 2022) and consistent with Gu & Dao (2023). E.5 SSM POLARIZATION AND ASSOCIATIVE RECALL EXPERIMENTS In this section, we provide implementation details for our proposed polarization technique. Also, we introduce experiment details for the associative recall tasks we used to validate our techniques. E.5.1 POLARIZED MAMBA Recall that we polarize channel in At with constant zero and another with constant one. Note that in the implementation of Mamba, At is parameterized by At = exp(tA), where is learnable parameters. We change the forward pass of Mamba by prepending zero and appending number negatively large enough (i.e., -1000). So the preand post-exponential and At become: (cid:35) (cid:35) (cid:34)1 (cid:34)0 , At exp(tA) 0 We also study two variants: 0-polarized and 1-polarized Mamba by only pretending or appending one or zero to At to single out the effects of one and zero gating, respectively. The gradient flow also complies with the original one, meaning polarization does not change the optimization dynamics of Mamba. Below we show that the polarized At does not influence the gradient in terms of the time interval t. First of all, let Gt = , then ℓ tbt(xt) tbt(xt) ℓ = Gt + = Gt + 1 (cid:88) n=2 1 (cid:88) n=2 ℓ (At)n,n (At)n,n ℓ (At)n,n (At)n,n + + ℓ (At)1,1 (At)1,1 + ℓ (At)N,N (At)N,N ℓ ℓ (At)1,1 exp(t(At)1,1) (At)1,1 (cid:124) (cid:123)(cid:122) (cid:125) =0 + ℓ (At)N,N exp(t(At)N,N ) (cid:123)(cid:122) (cid:125) (cid:124) 0 (At)N,N Gt + 1 (cid:88) n= ℓ (At)n,n (At)n,n , where the term exp(t(At)N,N )(At)N,N 0 is achieved by taking (At)N,N . The component directly contributed by At is not affected regardless of the polarization. 28 Configurations # Layers Recency Over-smoothing Default At Default At (At)1,1 = 1 (At)N,N = 0 (At)N,N = 0 (At)1,1 = 1, (At)N,N = 0 (At)1,1 = 1, (At)N,N = 0 2 4 2 2 2 4 # KV Pairs 64 128 256 98.38 99. 99.81 98.41 99.74 99.23 99.94 81.81 82.08 94.70 81.35 92.20 95.54 98.80 36.00 33. 56.39 36.55 52.21 54.74 81.56 Avg. 72.06 71.61 83.63 72.10 81.38 83.17 93. Table 5: Extension to Tab. 5. We note the extent of locality and over-smoothing for each configuration. We consider 1-polarization mitigates locality most significantly, while deepening architecture only relieves recency mildly but deteriorates over-smoothing. 0-polarization alleviates over-smoothening and unleash the benefits by depth scaling. E.5.2 ASSOCIATIVE RECALL EXPERIMENTS Associative Recall (AR) is the task of retrieving specific value tied to given key. This ability is particularly important for in-context learning, where model leverages previously presented information to adapt to new inputs (Olsson et al., 2022; Arora et al., 2023). In our experiments, we use Mamba as the representative SSM, following the setup described in Arora et al. (2023). The input sequences to the model consist of two parts. The first part is set of key-value pairs, randomly sampled from vocabulary, with keys and values drawn independently. The second part contains random keys, selected from those present in the first part (without repetition), and placed according to power-law distribution at random. Any remaining slots in the sequence are filled with zero token. The target sequence contains the values corresponding to the positions of the respective key occurrences, while all other tokens are excluded from loss supervision and accuracy evaluation. We employ training strategy with mixed sequence lengths and varying numbers of key-value pairs. Specifically, the total sequence length varies across {64, 128, 256, 512, 1024}, with {12.5%, 25%, 50%} of the sequence allocated to key-value pairs and the remainder to queried keys. Each configuration includes 20,000 examples, resulting in total of 300,000 training examples. For evaluation, the sequence length is fixed at 1024, while the number of key-value pairs varies among 64, 128, 256. We use batch size of 128 and learning rate of 1e-3 throughout the experiments. We provide an extended table for the results in Tab. 5."
        }
    ],
    "affiliations": [
        "Georgia Tech",
        "Google DeepMind",
        "University of Texas at Austin",
        "Zhejiang University"
    ]
}