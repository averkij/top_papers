{
    "paper_title": "See, Hear, and Understand: Benchmarking Audiovisual Human Speech Understanding in Multimodal Large Language Models",
    "authors": [
        "Le Thien Phuc Nguyen",
        "Zhuoran Yu",
        "Samuel Low Yu Hang",
        "Subin An",
        "Jeongik Lee",
        "Yohan Ban",
        "SeungEun Chung",
        "Thanh-Huy Nguyen",
        "JuWan Maeng",
        "Soochahn Lee",
        "Yong Jae Lee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal large language models (MLLMs) are expected to jointly interpret vision, audio, and language, yet existing video benchmarks rarely assess fine-grained reasoning about human speech. Many tasks remain visually solvable or only coarsely evaluate speech, offering limited insight into whether models can align who speaks, what is said, and when it occurs. We introduce AV-SpeakerBench, a curated benchmark of 3,212 multiple-choice questions focused on speaker-centric audiovisual reasoning in real-world videos. It features: (1) a speaker-centered formulation that treats speakers-not scenes-as the core reasoning unit; (2) fusion-grounded question design embedding audiovisual dependencies into question semantics; and (3) expert-curated annotations ensuring temporal precision and cross-modal validity. Comprehensive evaluations show that the Gemini family consistently outperforms open-source systems, with Gemini 2.5 Pro achieving the best results. Among open models, Qwen3-Omni-30B approaches Gemini 2.0 Flash but remains far behind Gemini 2.5 Pro, primarily due to weaker audiovisual fusion rather than visual perception. We believe AV-SpeakerBench establishes a rigorous foundation for advancing fine-grained audiovisual reasoning in future multimodal systems."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 1 3 2 2 0 . 2 1 5 2 : r See, Hear, and Understand: Benchmarking Audiovisual Human Speech Understanding in Multimodal Large Language Models Le Thien Phuc Nguyen1* Zhuoran Yu1* Samuel Low Yu Hang2 Subin An2 Jeongkik Lee2 Yohan Ban2 SeungEun Chung2 Thanh-Huy Nguyen3 Juwan Maeng Soochahn Lee2 Yong Jae Lee1 1University of WisconsinMadison 2Kookmin University 3Carnegie Mellon University https://plnguyen2908.github.io/AV-SpeakerBench-project-page/"
        },
        {
            "title": "Abstract",
            "content": "Multimodal large language models (MLLMs) are expected to jointly interpret vision, audio, and language, yet existing video benchmarks rarely assess fine-grained reasoning about human speech. Many tasks remain visually solvable or only coarsely evaluate speech, offering limited insight into whether models can align who speaks, what is said, and when it occurs. We introduce AV-SpeakerBench, curated benchmark of 3,212 multiple-choice questions focused on speaker-centric audiovisual reasoning in realworld videos. It features: (1) speaker-centered formulation that treats speakersnot scenesas the core reasoning unit; (2) fusion-grounded question design embedding audiovisual dependencies into question semantics; and (3) expert-curated annotations ensuring temporal precision and cross-modal validity. Comprehensive evaluations show that the Gemini family consistently outperforms opensource systems, with Gemini 2.5 Pro achieving the best results. Among open models, Qwen3-Omni-30B approaches Gemini 2.0 Flash but remains far behind Gemini 2.5 Pro, primarily due to weaker audiovisual fusion rather than visual perception. We believe AV-SpeakerBench establishes rigorous foundation for advancing fine-grained audiovisual reasoning in future multimodal systems. 1. Introduction Multimodal large language models (MLLMs) have rapidly evolved in recent years, extending language models into image [7, 35, 54, 69], video [34, 54, 66], and audio understanding [18, 28, 64]. As this evolution continues, recent efforts have moved beyond pairwise modality fusion toward building omni-models that aim to jointly process vision, audio, and language in unified architecture [22, 51, 59, 60]. Such capability is essential for real-world applications like *Equal Contribution Figure 1. Motivation of AV-SpeakerBench. Existing video benchmarks often contain visually solvable questionssuch as counting visible peoplewhere state-of-the-art multimodal models can answer correctly even when the audio stream is muted (left; examples from Video-MME [13]). In contrast, questions in AVSpeakerBench (right) are explicitly designed to require audiovisual fusion: the correct answer depends on who speaks, when they speak, and how speech events unfold over time. video dialog agents, meeting transcription systems, and humanAI interaction platforms, where the model must see, hear, and reason over multiple signals simultaneously. However, evaluating whether current models can truly integrate multiple modalitiesrather than treat one as the dominant sourceremains an open challenge. In particular, audiovisual speaker perception has been long-standing research problem [26, 27, 46, 47, 49, 55], yet modern MLLMs are rarely evaluated on this ability. This gap arises for two reasons. First, existing speaker datasets are built around closed-set labels or framelevel supervision, making them incompatible with openended, language-based evaluation. Second, current video QA benchmarks seldom focus on speaker-level reasoning: many questions can be solved using single modality [13, 52], while others emphasize coarse audiovisual matching or non-speech acoustic events [30, 61, 70]. As result, current benchmarks do not systematically evaluate whether multimodal models can jointly determine who is 1 speaking, what is being said, and in what visual context. To address this gap, we introduce AV-SpeakerBench, new benchmark for evaluating fine-grained audiovisual reasoning centered on human speakers in real-world video. AV-SpeakerBench is explicitly designed to test whether multimodal models can jointly interpret visual, auditory, and linguistic information at the speaker levela capability not captured by existing datasets. Below, we outline the benchmarks key design principles. Speaker-centric task formulation. AV-SpeakerBench frames every question around the speaker as the fundamental reasoning unit, shifting evaluation from scene-level understanding to human-centered audiovisual grounding. Each video includes multiple visible individuals, enabling identity-based questions that require models to determine who speaks, when, and under which visual context. By spanning diverse conversational settings and speaker configurationsfactors known to increase difficulty in audiovisual perception [26, 46, 47, 49, 55]the benchmark tests whether models can reliably resolve speaker behavior amid visually complex and varied conversational dynamics. Fusion-driven question design. AV-SpeakerBench uses four-choice MCQ format in which auditory and visual cues are jointly encoded in both questions and answer options. This design ensures that solving each item requires cross-modal fusionfor example, associating spoken phrases with visible speakers, interpreting speech in relation to visual events, or resolving multiple voices in shared scene. Together, these constructions reflect how audiovisual understanding naturally relies on coordinating what is heard with what is seen. High-quality human annotation. All clips are manually selected and temporally segmented to isolate moments where speech-driven interaction occurs. Annotators then identify segments that genuinely require audiovisual reasoningsuch as aligning an utterance with the correct visible speakerand compose MCQs grounded in these spans. Each clip and question undergoes multi-stage expert review to ensure temporal precision and cross-modal validity. Dataset summary and evaluation scope. Our IRBapproved AV-SpeakerBench contains 3,212 curated questionanswer pairs across 12 task types, all centered on speakers as the core reasoning unit. These tasks span temporal localization, speaker identification, speech-content reasoning, utterance counting, paralinguistic attribute comparison and so on, collectively evaluating models ability to integrate recognition, grounding, and temporal reasoning across audio and vision rather than depend on static or unimodal cues. Experimental findings and implications. Comprehensive evaluation across open-source and proprietary models reveals clear performance gap between current MLLMs and human accuracy. Among open-source models, only recent omni-directional approachessuch as Qwen3Omni [60]show substantial progress, with the 30B variant reaching parity with Gemini 2.0 Flash [15]. However, Gemini 2.5 Pro [15] remains the strongest overall, outperforming all others across nearly all tasks. Our analysis (Section 4.3) shows that this advantage comes primarily from stronger audiovisual fusion: Gemini 2.5 Pro consistently gains 1020% from audio inputs, whereas Qwen3-Omnis improvements are modest or sometimes negative. systematic error-pattern study further reveals that weaknesses in audio perception and temporal grounding are the dominant sources of failure. Together, these findings show that speaker-centered multimodal reasoning remains core barrier to human-level audiovisual understanding. 2. Related Work Multimodal Understanding Benchmarks. Multimodal evaluation has expanded from image benchmarkssuch as VQA [2, 20], chart/document understanding [42, 43, 50, 56], and general capability suites [11, 29, 37]to video benchmarks that introduce temporal structure and richer scene context. Different video datasets emphasize different abilities: long-form narrative reasoning [41], temporal ordering [3, 38], procedural understanding [53, 57], egocentric perception [8, 21], world modeling [24], and broad capability aggregation [12, 32]. While existing video benchmarks broaden multimodal evaluation, they rarely require fine-grained integration between visual signals and human speech, nor do they target speaker-centric reasoning. AVSpeakerBench fills this gap by providing dedicated benchmark for speaker-level audiovisual understanding. Audiovisual Understanding Benchmarks. Audiovisual speech understanding has been studied through both nonspeech and speech-focused datasets. Non-speech benchmarks such as AudioSet [17] and VGGSound [4] target event-level acoustic classification, while speech datasets cover specific facets of communication: VoxCeleb [6, 45] for speaker identity, LRS/LRS3 [1] for transcription, and AVA-ActiveSpeaker / ASW [27, 49] for frame-level speaking and audibility labels. However, these resources rely on closed-set labels or low-level annotations and thus do not evaluate the open-ended multimodal reasoning abilities expected of modern MLLMs. Recent work has begun exploring audiovisual reasoning for multimodal models, but mainly through task formulations that differ fundamentally from speaker-centric reasoning. VGGSounder [70] reannotates VGGSound [4] for multi-label audio event classification; AV-Odyssey [19] focuses on audiovisual matching taskssuch as pairing images with audio clips or selecting which video aligns with given soundrather than reasoning about spoken content. WorldSense [24] similarly frames audiovisual understand2 Benchmarks Modalities #Videos #QA Anno. Audiovisual Reasoning Speaker-centric Reasoning AV Attribution AV Temporal Localization SpeechSpeaker Attribution Speech temporal localization Speech Grounding MSRVTT-QA [58] ActivityNet-QA [62] MVBench [32] Video-Bench [48] EgoSchema [41] Video-MME [12] MMBench-Video [9] AVQA [61] OmniBench [33] AV-Odyssey [19] WorldSense [25] Daily-Omni [68] AV-SpeakerBench (Ours) V A+V A+I A+I A+V A+V A+V 2,990 800 3,641 5,917 5,063 900 609 57,000 220 1,662 648 72,821 8,000 4,000 17,036 A&M A&M 5,063 2,700 1,998 57,335 1,142 4,555 3,172 1,197 M A&M 2,051 3,212 Table 1. Comparison of multimodal video QA benchmarks. means audio, means video, means image, means subtitle. Anno. is the annotation type (A: automatic, M: manual). ing as matching between visual scenes and acoustic cues (e.g., music style or sound summary), and its tasks span wide range of audio types without centering human speech; OmniBench pairs static images with narrated audio for temporal reconstruction; and Daily-Omni targets high-level scene understanding rather than modeling speaker identity or spoken utterances. We compare AV-SpeakerBench with other benchmarks in Table 1. Across these benchmarks, audio is used to characterize scenes, events, or background context, but not to bind speech to visible speakers. AV-SpeakerBench instead adopts speaker-centric formulation that requires aligning spoken content with the people who produce it. Multimodal Large Language Models. Multimodal LLMs have progressed from imagetext models such as BLIP-2 [31], InstructBLIP [7], and LLaVA [35, 36] to video-oriented systems like Video-LLaMA [5, 65], VITA [10], and PandaGPT [51], which introduce temporal encoding for multi-frame reasoning. Recent omni-modal modelsincluding Gemini [16], Qwen-Omni [60], Phi-4 Multimodal [44], StreamOmni [67], Unified-IO 2 [40], OLA / OneLLM [23, 39], and AnyGPT [63]aim to unify audio, video, and language through instruction tuning and shared cross-modal alignment. However, existing evaluations largely emphasize visual QA or coarse audiovisual matching, providing limited insight into whether these models can perform fine-grained, speaker-centric audiovisual reasoning. AV-SpeakerBench directly targets this gap by assessing whether models can jointly resolve who is speaking, what is said, and when it occurs within natural multi-speaker video. 3. AV-SpeakerBench Recent multimodal video benchmarks such as VideoMME [13] have advanced the evaluation of large multimodal models on wide range of video understanding tasks. However, many existing questions remain largely visionsolvable, such as What is the total number of people in the video? or What clothes is the singer wearing?, where the correct answer can be inferred from visual frames alone without attending to the audio stream. Moreover, only few benchmarks include human speech in their audiovisual evaluation [30, 61, 70], and those that do remain at coarse categorical level. For example, VGGSounder [70] extends VGGSound with multi-label audiovisual classification, where human speech is represented by broad categories such as male speech, female speech, or singing. Such tasks capture the presence of speech but not its linguistic or speakerspecific content, leaving open whether models can reason about who speaks and what is said in natural audiovisual scenes. Our goal in constructing AV-SpeakerBench is to fill this gap by focusing on speaker-centric audiovisual reasoning. Each question in our benchmark is designed to require joint interpretation of visual and auditory cues in natural human speech scenes. In the following sections, we describe the benchmarks design principles, data curation and annotation pipeline, and dataset statistics. 3.1. Fusion-Driven Question Design The central goal of AV-SpeakerBench is to evaluate audiovisual fusion along the temporal dimensionthat is, whether model can coherently align what is seen and heard as events unfold. All questions adopt unified multiple-choice (MCQ) format to standardize evaluation and to encode auditory and visual cues compactly within the question and its answer options. Each item is written so that the correct response requires the model to integrate auditory and visual evidence to reason over temporally ordered actions and utterances. In our formulation, audiovisual dependency is embedded directly in the textual construction of the question and its options. The fusion of modalities appears in several complementary ways, includingbut not limited to (1) linking spoken phrase to visible identity (e.g., When does the man wearing black T-shirt say Whats going on?); (2) cross-conditioning one modality on temporal cues from the Figure 2. Top: Examples of audiovisual reasoning questions in AV-SpeakerBench. Each question illustrates distinct way in which audiovisual dependency is enforcedthrough spoken-phrase grounding, visual event conditioning, cross-modal temporal localization, or multi-speaker coordinationensuring that the correct answer cannot be inferred from single modality. Bottom: Dataset Distribution. We present the distribution of videos by duration, task category, and visual complexity (measured by the number of unique visible people). Together, these statistics highlight the diversity of conversational scenes and reasoning types represented in AV-SpeakerBench. otherfor example, using visual events to localize speech (e.g, What does the woman sitting on the porch swing say just before she drinks from her glass?, or conversely, using auditory events to anchor visual reasoning; At the moment the man with the black headband says, We are not cool, how many people are visible?); and (3) combining auditory and visual cues in multi-speaker settings (e.g., After the man in the grey shirt wiggles his fingers, until the end of the video, how many times is red line mentioned by all speakers?). Through this formulation, each question enforces temporally grounded multimodal reasoning, ensuring that successful models must align voices, appearances, and contextual events rather than rely on static visual or textual shortcuts. Representative examples of different formulation types are illustrated in Figure 2-top, and complete set of per-task examples is provided in the Appendix D. 3.2. Dataset Construction Video sources. We collect all video data from YouTube, prioritizing content that naturally features human speech and multi-party interaction. The majority of clips are sampled from movie clips, as these provide rich audiovisual dynamics such as turn-taking, facial gestures, and speaker transitions within short temporal windows. To diversify speaking style, visual layout, and recording conditions, we additionally include material from game shows, street interviews, sport interviews, group interviews, podcasts, and vlogs. All videos are taken from official channels where individuals are clearly and knowingly recorded on camera. Clip selection and task-oriented sampling. Unlike datasets where annotators label pre-defined clips, our process begins with segment selection. Annotators watch full videos and identify 530s clips that satisfy task requirements and contain meaningful audiovisual variation. Choosing the right moment is crucial: segments with too few speakers (e.g, monologue) or unchanged conversational dynamics make temporal or speaker-grounded questions nearly deterministic (e.g., if only two speakers are visible, Who speaks after the woman says How are you? offers little diagnostic value). Similarly, an anchor condition that does not shift who speaks fails to introduce real temporal grounding. This filtering ensures that all selected clips support genuine temporal localization and cross-modal reasoning rather than superficial pattern matching. Annotation and refinement. For each selected clip, annotators compose the question and four answer options following detailed task guidelines specifying the required modality pairings. Distractors are drawn from entities, actions, or speech events within the same clip; when insufficient, they are formed by recombining visible or auditory attributes to maintain contextual realism. All annotators are experienced researchers rather than crowdsourced workers, ensuring close adherence to multimodal reasoning criteria. Each annotation undergoes multi-stage refinement pipeline: (1) an initial review by separate researcher for clarity and multimodal validity; (2) linguistic and structural polishing using language model; and (3) final verification round by at least two additional researchers other than the original annotator. During this process, ambiguous, inconsistent, or trivially solvable examples are filtered out. This rigorous curation pipeline ensures that all remaining questions exhibit temporal sensitivity, speaker grounding, and robust audiovisual reasoning, providing balanced and reliable basis for evaluation. Annotation details can be found in Appendix A. 3.3. Dataset Statistics After refinement, AV-SpeakerBench comprises 3,212 MCQ pairs across 12 task types. All tasks are speaker-centric but probe different aspects of audiovisual understanding, including temporal localization, speaker identification, speech-content retrieval, utterance counting, paralinguistic attribute comparison, and so on. Together, they introduce diverse compositional and temporal challenges that require coordinated multimodal reasoning rather than reliance on static or unimodal cues. Appendix provides illustrative examples for each task type. We visualize dataset-wide statistics in Figure 2-bottom, including the distributions of question types, video durations, and speaker counts. The dataset spans wide range of question formulations and task complexities, reflecting the natural diversity of audiovisual reasoning scenarios. We ensure at least 200 validated examples per task to maintain balanced coverage across reasoning types. Speakerrelated tasks contain more examples, since temporal reasoning about who speaks and when can draw on cues from either modalityfor instance, determining whether someone speaks after an audio or visual event. In contrast, tasks centered purely on visual or auditory recognition must rely on the opposite modality to remain genuinely multimodal, which limits the diversity of question formulations rather than their total count. Although speaker-related tasks therefore include more examples, they also encompass wider variety of reasoning patterns, and we report per-task accuracy throughout all evaluations to avoid bias toward these categories. We further analyze the dataset by measuring its visual complexity, quantified by the number of unique visible speakers per clipa standard indicator of difficulty in human-centric audiovisual benchmarks [27, 46, 49]. AVSpeakerBench covers wide range of interaction densities, from short dyadic exchanges to multi-party conversations, providing balanced yet challenging evaluation environment for multimodal large language models. 3.4. Interpretation and Scope of Evaluation Although all question types in AV-SpeakerBench are designed to require audiovisual reasoning in principle, we recognize that strong models may occasionally infer answers from visual cues alonesuch as mouth movement or gestureswithout explicitly relying on the audio stream. We regard this as natural and desirable behavior rather than flaw: humans similarly infer speech activity from silent visual context. Our benchmark therefore evaluates whether model can integrate both modalities when available, rather than penalizing instances where one modality alone happens to suffice. In practice, audio information consistently simplifies reasoning and boosts accuracy, but models achieving correct answers through robust visual understanding remain within the intended evaluation scope. 4. Experiments 4.1. Experiment Setup Model Selection. We evaluate multimodal LLMs that natively support audiovideo inputs. Among proprietary systems available to us, Gemini [16] is the only model offering true A+V processing; we report both the thinking and nonthinking versions of Gemini 2.5 Flash, and only the thinking version of Gemini 2.5 Pro (the only mode provided). For open-source models, we include all publicly available A+Vcapable LLMs across omni-modal and video-first architectures: Qwen2.5-Omni [59], Qwen3-Omni [60], VideoLLaMA [65], Video-LLaMA2 [5], Unified-IO 2 [40], OLA [39], OneLLM [23], VITA [10], VITA-1.5 [14], [44], and PandaGPT [51], Phi-4 Multimodal-Instruct AnyGPT [63]. For Qwen3-Omni-30B, we report only the non-thinking variant, as the thinking mode takes about 5 minutes per query and would place full benchmark run beyond our budget. All models are evaluated with their native A+V interfaces using identical inputs and prompts; further details are provided in Appendix B. Task Type Detection Recognition Counting Attribute Recognition Activity Recognition Counting Recognition Duration Pitch Rate Intensity Counting Overall Speaker-centric Visual-centric Audio-centric Human Performance Gemini 2.5 Pro Thinking [15] Gemini 2.5 Flash Thinking [15] Gemini 2.5 Flash [15] Gemini 2.5 Flash-Lite [15] Gemini 2.0 Flash [15] Gemini 2.0 Flash-Lite [15] Video-LLaMA [65] Video-LLaMA [65] Video-LLaMA2 [5] PandaGPT [51] PandaGPT [51] Unified-IO 2 large [40] Unified-IO 2 xl [40] Unified-IO 2 xxl [40] Phi-4 Multimodal [44] OneLLM [22] AnyGPT [63] VITA [10] VITA-1.5 [14] Qwen2.5-Omni Qwen2.5-Omni [59] Qwen3-Omni [60] 7B 13B 7B 7B 13B 1B 3B 7B 5.6B 7B 7B 7B 7B 3B 7B 30B 96.02 93. 94.28 93.14 93.20 94.15 96.52 90. 93.20 91.39 94.17 93.40 93.74 81.73 74.71 69.79 45.90 60.19 56. 29.51 30.91 34.19 28.34 20.61 24.36 28.81 26.70 37.70 30.44 24.12 32.08 32.08 44.91 47.54 61.83 74.15 70.62 68.24 52.44 63.51 57.35 26.07 29.86 36.02 20.61 16.82 27.49 28.44 27.25 41.71 23.70 4.27 34.60 36.73 41.23 41.23 54.74 74.13 60.95 50.75 39.10 47.51 38. 29.35 25.37 31.25 27.36 28.36 23.88 31.25 24.63 28.02 20.65 24.88 29.85 32.59 33.83 34.83 46.77 Proprietary Models 72.55 65.59 61.76 48.53 54.9 49.02 73.30 70.39 68.45 51.94 63.59 55.83 Open-sourced Audiovisual Models 27.23 27.23 35.29 16.18 0.98 32.52 37.38 37.38 20.87 1.94 Open-sourced Omni Models 24.51 30.39 30.39 46.95 30.88 0.98 37.25 38.24 44.61 42.65 56.86 23.79 26.21 22.82 33.50 25.24 19.90 36.41 35.44 45.63 38.83 58.74 62.93 65.85 51.22 43.41 45.85 45. 25.37 29.76 41.46 32.20 25.37 22.93 23.41 29.76 38.05 27.8 19.51 28.78 35.12 44.88 43.41 40.49 77.11 78.61 71.64 55.72 71.14 67.66 31.84 27.86 29.85 33.83 5.97 21.89 24.88 23.28 45.77 25.37 15.42 32.84 32.34 45.77 53.23 68.16 78.81 69.92 58.47 51.69 56.78 52. 28.81 23.31 40.25 19.07 22.46 22.03 30.93 35.59 38.56 1.69 0.00 37.29 45.34 50.00 47.03 59.32 67.48 66.5 60.68 51.46 62.62 52.43 31.55 27.67 49.03 24.27 22.82 25.24 22.82 25.24 49.03 24.27 0.00 38.83 43.20 42.23 51.94 58.74 69.86 67.46 60.29 46.89 55.98 51. 28.23 33.49 44.02 17.70 25.84 31.58 26.32 33.49 42.58 28.23 0.48 35.89 44.98 32.54 42.11 55.98 71.84 65.05 59.71 54.37 55.83 56.31 27.67 30.58 45.15 14.56 23.30 31.55 30.58 37.38 37.38 25.73 2.43 37.86 36.89 39.32 43.20 66.02 63.89 58.33 40.97 36.11 41.67 33. 21.53 27.08 31.25 15.63 15.63 34.38 31.25 28.8 26.04 34.72 22.92 28.13 29.51 26.39 29.17 34.72 73.04 67.84 60.27 47.23 53.21 51.43 28.21 29.11 37.67 22.88 18.37 26.15 27.52 24.97 38.45 24.97 12.67 33.66 36.27 38.23 42.31 54.14 Table 2. Evaluation Results of AV-SpeakerBench. Our evaluation covers proprietary models, open-sourced audiovisual and omni models. Frame Sampling. We follow each models default Concretely, we sample 8 temporal sampling policy. frames for Video-LLaMA [65], Video-LLaMA2 [5], Phi-4 Multimodal-Instruct [44], Unified-IO 2 [40], and AnyGPT [63]; for PandaGPT [51] and OneLLM [22]; 1 fps capped at 100 frames for VITA [10] and VITA-1.5 [14]; and 1 fps for Gemini [16], Qwen2.5Omni [59], and Qwen3-Omni [60]. Unless otherwise noted, frames are sampled uniformly within each clip. 5 frames 4.2. Results The full evaluation results are presented in Table 2; all results are reported in MCQ accuracy. Here, we summarize the main takeaways. Existing models remain far from human performance. Human evaluation reaches an overall accuracy of 93.74%, confirming that the questions are clear and naturally solvIn comparison, the able through audiovisual reasoning. strongest modelGemini 2.5 Proachieves only 73.04%, leaving gap of over 20 percentage points. This indicates that current multimodal LLMs still have substantial room for improvement on fine-grained, temporally grounded speaker-centric reasoning. Gemini models demonstrate stronger performance than open-source models. Gemini 2.5 Pro (thinking) delivers the best results on AV-SpeakerBench, outperforming all other models on 11 of the 12 tasks and achieving an overall accuracy of 73.04%. Gemini 2.5 Flash (thinking) improves upon the non-thinking Flash variant by 7.57%, yet still falls notably short of 2.5 Pro. Since both variants employ the thinking process, this performance gap primarily reflects differences in the underlying model strength, with possible additional gains from how the thinking mechanism interacts with Pros larger capacity. Only recent Omni-series models achieve non-trivial performance on AV-SpeakerBench. Earlier opensource multimodal modelssuch as Video-LLaMA2 [5], PandaGPT [51], OneLLM [22], Unified-IO [40], and AnyGPT [63]perform poorly on AV-SpeakerBench, often approaching random-guessing accuracy despite supporting both audio and video inputs. Recent Omni-series models, including Phi-4-Multimodal [44], VITA/VITA-1.5 [10, 14], Qwen2.5-Omni [59], Qwen3-Omni [60], and OLa [39], demonstrate substantial improvements, marking the first generation of open-source models capable of handling finegrained audiovisual reasoning. Among them, Qwen3Omni-30B is the strongest, exceeding all other open-source models and even slightly surpassing Gemini 2.0 Flash. However, it still remains far behind the Gemini 2.5 family, underscoring the difficulty of speaker-centric audiovisual reasoning even for the most advanced open-source LLMs. 4.3. Analysis Performance gaps reflect differences in audiovisual fusion capability. From Table 2, we observe that Gemini 2.5 Pro consistently outperforms Qwen3-Omni-30B across all categories on AV-SpeakerBench. We investigate this difference by comparing each models performance un6 (a) Modality ablation across task types. Gemini 2.5 Pro demonstrates consistent multimodal gains across most tasks, whereas Qwen3-Omni 30B exhibits limited or even negative audio contributions in certain tasks. (b) Error type distribution across benchmark categories. Most errors occur in audio perception and temporal reasoning tasks. Figure 3. Multimodal ablation and error analysis. der vision-only and audiovisual input settings (Figure 3a). Gemini 2.5 Pro exhibits consistent gains of roughly 1020 percentage points across all tasks when both modalities are available, indicating stable and effective fusion. In contrast, Qwen3-Omni-30B achieves much smaller gainsand in some tasks, even negative differencessuggesting that audio input does not reliably improve its reasoning. These results indicate that Geminis advantage primarily arises from stronger temporal alignment and cross-modal integration, whereas future progress in speaker-centric audiovisual reasoning will depend on improving fusion robustness rather than merely scaling unimodal perception. Why advanced models may answer some audiovisual questions using only vision. Although AV-SpeakerBench is designed to require audiovisual reasoning, we observe that advanced models can occasionally answer certain questions using only visual cues. Human speech naturally produces observable signalssuch as mouth motion, gaze shifts, and conversational gesturesthat can provide partial evidence about who is speaking or when speech occurs. As result, strong models may exploit these cues even in the absence of audio. Figure 4a shows an example where Gemini 2.5 Pro correctly identifies the most active speaker under vision-only input by tracking sustained mouth movement and gesturing patterns. However, these cues are not always reliable. In Figure 4b, the same vision-only setting leads the model to wrong answer because the visible gestures mislead it about the speakers rate of speech. When audio is provided (Figure 4c), the model correctly resolves the ambiguity, demonstrating that the task genuinely requires multimodal fusion. Importantly, these visual cues reflect natural properties of human communication rather than dataset bias. AVSpeakerBench does not explicitly penalize unimodal soNumber of Unique People Gemini 2.5 Pro Thinking [16] Gemini 2.5 Flash Thinking [15] Qwen2.5-Omni 7B [59] Qwen3-Omni 30B [60] 2 74.8 71.8 46.7 58. 3 4 74.1 68.4 40.7 52.9 74.1 66.8 42.4 52.0 5 70.9 65.1 40.2 54. Table 3. Accuracy by the number of visible people. Models generally decrease in accuracy as visual complexity increases. lutionsif model can reliably infer speech activity from vision alone, this reflects genuine capability (e.g., implicit lip-reading or motion-level reasoning). However, as Figures 4bc illustrate, fusion offers clear advantage: audio resolves ambiguities that visual cues alone cannot, and robust performance across the benchmark consistently requires integrating both modalities. Our goal is therefore not to suppress unimodal cues, but to design tasks where multimodal fusion provides the most reliable and general path to correct reasoning. Failure cases for Gemini 2.5 Pro. To better understand the remaining challenges for Gemini 2.5 Pro on AVSpeakerBench, we qualitatively examined five failure cases per task and categorized them into four major types: (1) Visual/Audio perception errormisperceiving information within single modality, such as misidentifying gesture or mishearing spoken phrase; (2) Cross-modal attribution errorrecognizing unimodal content correctly but mismatching it across modalities, e.g., attributing an utterance to the wrong speaker; (3) Temporal grounding erroridentifying the correct event but applying an incorrect temporal relation, such as reasoning over the after segment when the question asks for before; and (4) Temporal localization errorselecting the wrong segment in audio or video, leading to misaligned reasoning. As summarized in 7 Figure 4. Qualitative examples of Gemini 2.5 Pro reasoning traces on AV-SpeakerBench. Green and red highlight colors indicate the models correct and incorrect reasoning, respectively. (a) Vision-only example answered correctly: the model identifies the correct speaker by tracking the duration and consistency of mouth movement and conversational gestures, which serve as natural visual cues for inferring who is speaking. (b) Vision-only example answered incorrectly: the model incorrectly associates slower gestures with slower speech, leading to wrong prediction. (c) The same example as (b) but with audio input: the model correctly identifies the faster speaker once speech-rate evidence becomes available, confirming that the question requires true audiovisual fusion. (d) Vision + audio example answered incorrectly: the model predicts that only one woman speaks while both women say Okay after the event. Eventually, all three speakers talk after the event, showing residual difficulty in temporal alignment and speaker disambiguation. Figure 3b, most failures arise from audio perception and temporal grounding, indicating that even advanced models struggle to parse overlapping speech and accurately anchor reasoning in time. Figure 4d further illustrates an audio perception error where the model misses one of two speakers uttering the same word in close succession. Additional examples for each error type are provided in Appendix C. Evaluation by visual complexity. To examine how visual complexity impacts model performance, we regroup questions by the number of visible faces, as shown in Table 3. All models show lower accuracy as the number of people increases, indicating that multi-person scenes create substantial challenges for speaker identification, temporal association, and audiovisual grounding. 5. Ethical Statement The dataset protocol was reviewed by the institutional review board (IRB), which issued minimal-risk determination. The certificate will be included in the appendix upon acceptance to preserve review anonymity. We define our dataset as collection of publicly available YouTube videos paired with precise timestamps and human-written annotations. The benchmark is used solely for evaluation, not model training, and thus poses minimal risk of bias amplification or unintended memorization. The dataset will be released under the CC BY-NCSA 4.0 license, which restricts use to non-commercial research and prohibits applications involving facial recognition, surveillance, or biometric identification. Individuals featured in referenced videos may request removal, and we will promptly withdraw the corresponding segments from future releases. 6. Conclusion We introduced AV-SpeakerBench, benchmark for evaluating fine-grained, speaker-centric audiovisual reasoning in real-world video. Unlike prior multimodal datasets focus on sceneor event-level understanding, AV-SpeakerBench centers on human speakers and explicitly requires integrating auditory and visual cues. Our experiments show that while Gemini 2.5 Pro that exhibits strong fusion and temporal grounding, other models still face clear limitations in multimodal integration, underscoring the need for more robust fusion mechanisms in future multimodal systems. We hope this benchmark provides foundation for advancing multimodal models toward genuine audiovisual understandingwhere language, vision, and speech are jointly reasoned about within the same conversational context."
        },
        {
            "title": "References",
            "content": "[1] Triantafyllos Afouras, Joon Son Chung, and Andrew Zisserman. Lrs3-ted: large-scale dataset for visual speech recognition, 2018. 2 [2] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 2425 2433, 2015. 2 [3] Mu Cai, Reuben Tan, Jianrui Zhang, Bocheng Zou, Kai Zhang, Feng Yao, Fangrui Zhu, Jing Gu, Yiwu Zhong, Yuzhang Shang, et al. Temporalbench: Benchmarking finegrained temporal understanding for multimodal video models. arXiv preprint arXiv:2410.10818, 2024. 2 [4] Honglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew Zisserman. Vggsound: large-scale audio-visual dataset, 2020. 2 [5] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, and Lidong Bing. Videollama 2: Advancing spatial-temporal modeling and audio understanding in videollms. arXiv preprint arXiv:2406.07476, 2024. 3, 5, 6 [6] J. S. Chung, A. Nagrani, and A. Zisserman. Voxceleb2: Deep speaker recognition. In INTERSPEECH, 2018. 2 [7] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose visionlanguage models with instruction tuning. Advances in neural information processing systems, 36:4925049267, 2023. 1, 3 [8] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, et al. Scaling egocentric vision: The epic-kitchens dataset. In Proceedings of the European conference on computer vision (ECCV), pages 720736, 2018. 2 [9] Xinyu Fang, Kangrui Mao, Haodong Duan, Xiangyu Zhao, Yining Li, Dahua Lin, and Kai Chen. Mmbench-video: long-form multi-shot benchmark for holistic video understanding. In Advances in Neural Information Processing Systems, pages 8909889124. Curran Associates, Inc., 2024. 3 [10] Chaoyou Fu, Haojia Lin, Zuwei Long, Yunhang Shen, Meng Zhao, Yifan Zhang, Shaoqi Dong, Xiong Wang, Di Yin, Long Ma, et al. Vita: Towards open-source interactive omni multimodal llm. arXiv preprint arXiv:2408.05211, 2024. 3, 5, 6 [11] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. In The Thirtyninth Annual Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2025. 2 [12] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, Peixian Chen, Yanwei Li, Shaohui Lin, Sirui Zhao, Ke Li, Tong Xu, Xiawu Zheng, Enhong Chen, Caifeng Shan, Ran He, and Xing Sun. Videomme: The first-ever comprehensive evaluation benchmark In Proceedings of of multi-modal llms in video analysis. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2410824118, 2025. 2, 3 [13] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2410824118, 2025. 1, [14] Chaoyou Fu, Haojia Lin, Xiong Wang, Yi-Fan Zhang, Yunhang Shen, Xiaoyu Liu, Yangze Li, Zuwei Long, Heting Gao, Ke Li, et al. Vita-1.5: Towards gpt-4o level arXiv preprint real-time vision and speech interaction. arXiv:2501.01957, 2025. 5, 6 [15] Google Gemini Team. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities, 2025. 2, 6, 7 [16] Google Gemini Team. Gemini: family of highly capable multimodal models, 2025. 3, 5, 6, 7 [17] Jort F. Gemmeke, Daniel P. W. Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R. Channing Moore, Manoj Plakal, and Marvin Ritter. Audio set: An ontology and humanIn Proc. IEEE ICASSP labeled dataset for audio events. 2017, New Orleans, LA, 2017. 2 [18] Sreyan Ghosh, Sonal Kumar, Ashish Seth, Chandra Kiran Reddy Evuru, Utkarsh Tyagi, Sakshi, Oriol Nieto, Ramani Duraiswami, and Dinesh Manocha. Gama: large audio-language model with advanced audio understanding and complex reasoning abilities. arXiv preprint arXiv:2406.11768, 2024. [19] Kaixiong Gong, Kaituo Feng, Bohao Li, Yibing Wang, Mofan Cheng, Shijia Yang, Jiaming Han, Benyou Wang, Yutong Bai, Zhuoran Yang, and Xiangyu Yue. Av-odyssey bench: Can your multimodal llms really understand audio-visual information?, 2024. 2, 3 [20] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answerIn Proceedings of the IEEE conference on computer ing. vision and pattern recognition, pages 69046913, 2017. 2 [21] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: In Around the world in 3,000 hours of egocentric video. 9 Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1899519012, 2022. 2 [22] Jiaming Han, Kaixiong Gong, Yiyuan Zhang, Jiaqi Wang, Kaipeng Zhang, Dahua Lin, Yu Qiao, Peng Gao, and Xiangyu Yue. Onellm: One framework to align all modalities with language. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 26584 26595, 2024. 1, [23] Jiaming Han, Kaixiong Gong, Yiyuan Zhang, Jiaqi Wang, Kaipeng Zhang, Dahua Lin, Yu Qiao, Peng Gao, and Xiangyu Yue. Onellm: One framework to align all modalities with language, 2025. 3, 5 [24] Jack Hong, Shilin Yan, Jiayin Cai, Xiaolong Jiang, Yao Hu, and Weidi Xie. Worldsense: Evaluating real-world omnimodal understanding for multimodal llms. arXiv preprint arXiv:2502.04326, 2025. 2 [25] Jack Hong, Shilin Yan, Jiayin Cai, Xiaolong Jiang, Yao Hu, and Weidi Xie. Worldsense: Evaluating real-world omnimodal understanding for multimodal llms, 2025. 3 [26] Chaeyoung Jung, Suyeon Lee, Kihyun Nam, Kyeongha Rho, You Jin Kim, Youngjoon Jang, and Joon Son Chung. Talknce: Improving active speaker detection with talk-aware In ICASSP 2024-2024 IEEE Internacontrastive learning. tional Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 83918395. IEEE, 2024. 1, 2 [27] You Jin Kim, Hee-Soo Heo, Soyeon Choe, Soo-Whan Chung, Yoohwan Kwon, Bong-Jin Lee, Youngki Kwon, and Joon Son Chung. Look whos talking: Active speaker detection in the wild. arXiv preprint arXiv:2108.07640, 2021. 1, 2, 5 [28] Zhifeng Kong, Arushi Goel, Rohan Badlani, Wei Ping, Rafael Valle, and Bryan Catanzaro. Audio flamingo: novel audio language model with few-shot learning and dialogue abilities. arXiv preprint arXiv:2402.01831, 2024. [29] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. 2 [30] Guangyao Li, Yake Wei, Yapeng Tian, Chenliang Xu, JiLearning to answer questions Rong Wen, and Di Hu. In Proceedings of in dynamic audio-visual scenarios. the IEEE/CVF conference on computer vision and pattern recognition, pages 1910819118, 2022. 1, 3 [31] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with In Infrozen image encoders and large language models. ternational conference on machine learning, pages 19730 19742. PMLR, 2023. 3 [32] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, Limin Wang, and Yu Qiao. Mvbench: comprehensive multiIn Proceedings of modal video understanding benchmark. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2219522206, 2024. 2, 3 [33] Yizhi Li, Ge Zhang, Yinghao Ma, Ruibin Yuan, Kang Zhu, Hangyu Guo, Yiming Liang, Jiaheng Liu, Zekun Wang, Jian Yang, Siwei Wu, Xingwei Qu, Jinjie Shi, Xinyue Zhang, Zhenzhu Yang, Xiangzhou Wang, Zhaoxiang Zhang, Zachary Liu, Emmanouil Benetos, Wenhao Huang, and Chenghua Lin. Omnibench: Towards the future of universal omni-language models, 2025. 3 [34] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 59715984, 2024. [35] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. 1, 3 [36] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In CVPR, 2024. 3 [37] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pages 216233. Springer, 2024. 2 [38] Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. Tempcompass: Do video llms really understand videos? arXiv preprint arXiv:2403.00476, 2024. 2 [39] Zuyan Liu, Yuhao Dong, Jiahui Wang, Ziwei Liu, Winston Hu, Jiwen Lu, and Yongming Rao. Ola: Pushing the frontiers of omni-modal language model, 2025. 3, 5, 6 [40] Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, and Aniruddha Kembhavi. Unified-io 2: Scaling autoregressive multimodal models with vision, language, audio, and action, 2023. 3, 5, [41] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very longform video language understanding. In Advances in Neural Information Processing Systems, pages 4621246244. Curran Associates, Inc., 2023. 2, 3 [42] Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. In Findings of the association for computational linguistics: ACL 2022, pages 22632279, 2022. 2 [43] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. 2 [44] Microsoft. Phi-4-mini technical report: Compact yet powerful multimodal language models via mixture-of-loras, 2025. 3, 5, 6 [45] A. Nagrani, J. S. Chung, and A. Zisserman. Voxceleb: In INTERa large-scale speaker identification dataset. SPEECH, 2017. [46] Le Thien Phuc Nguyen, Zhuoran Yu, Khoa Quang Nhat Cao, Yuwei Guo, Tu Ho Manh Pham, Tuan Tai Nguyen, Toan Ngo Duc Vo, Lucas Poon, Soochahn Lee, and Yong Jae Lee. Unitalk: Towards universal active speaker detection in real world scenarios. arXiv preprint arXiv:2505.21954, 2025. 1, 2, 5 10 [47] Le Thien Phuc Nguyen, Zhuoran Yu, and Yong Jae Lee. Laser: Lip landmark assisted speaker detection for robustness. arXiv preprint arXiv:2501.11899, 2025. 1, 2 [48] Munan Ning, Bin Zhu, Yujia Xie, Bin Lin, Jiaxi Cui, Lu Yuan, Dongdong Chen, and Li Yuan. Video-bench: comprehensive benchmark and toolkit for evaluating video-based large language models, 2023. 3 [49] Joseph Roth, Sourish Chaudhuri, Ondrej Klejch, Radhika Marvin, Andrew Gallagher, Liat Kaver, Sharadh Ramaswamy, Arkadiusz Stopczynski, Cordelia Schmid, Zhonghua Xi, et al. Ava active speaker: An audio-visual dataset for active speaker detection. In ICASSP 2020-2020 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 44924496. IEEE, 2020. 1, 2, 5 [50] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 83178326, 2019. [51] Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. Pandagpt: One model to instruction-follow them all. arXiv preprint arXiv:2305.16355, 2023. 1, 3, 5, 6 [52] Guangzhi Sun, Wenyi Yu, Changli Tang, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, Yuxuan Wang, and Chao Zhang. video-salmonn: Speech-enhanced audio-visual large language models. arXiv preprint arXiv:2406.15704, 2024. 1 [53] Yansong Tang, Dajun Ding, Yongming Rao, Yu Zheng, Danyang Zhang, Lili Zhao, Jiwen Lu, and Jie Zhou. Coin: large-scale dataset for comprehensive instructional video In Proceedings of the IEEE/CVF Conference analysis. on Computer Vision and Pattern Recognition, pages 1207 1216, 2019. 2 [54] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 1 [55] Xizi Wang, Feng Cheng, and Gedas Bertasius. Loconet: Long-short context network for active speaker detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1846218472, 2024. 1, 2 [56] Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, et al. Charxiv: Charting gaps in realistic chart understanding in multimodal llms. Advances in Neural Information Processing Systems, 37:113569113697, 2024. 2 [57] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to explaining In Proceedings of the IEEE/CVF contemporal actions. ference on computer vision and pattern recognition, pages 97779786, 2021. 2 [58] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question answering via gradually refined attention over appearance and motion. In Proceedings of the 25th ACM International Conference on Multimedia, page 16451653, New York, NY, USA, 2017. Association for Computing Machinery. 3 [59] Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, arXiv preprint et al. Qwen2. 5-omni technical report. arXiv:2503.20215, 2025. 1, 5, 6, 7 [60] Jin Xu, Zhifang Guo, Hangrui Hu, Yunfei Chu, Xiong Wang, Jinzheng He, Yuxuan Wang, Xian Shi, Ting He, Xinfa Zhu, et al. Qwen3-omni technical report. arXiv preprint arXiv:2509.17765, 2025. 1, 2, 3, 5, 6, 7 [61] Pinci Yang, Xin Wang, Xuguang Duan, Hong Chen, Runze Hou, Cong Jin, and Wenwu Zhu. Avqa: dataset for audio-visual question answering on videos. In Proceedings of the 30th ACM International Conference on Multimedia, page 34803491, New York, NY, USA, 2022. Association for Computing Machinery. 1, 3 [62] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa: dataset for understanding complex web videos via question answering. Proceedings of the AAAI Conference on Artificial Intelligence, 33(01):91279134, 2019. 3 [63] Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan, Ge Zhang, Linyang Li, Hang Yan, Jie Fu, Tao Gui, Tianxiang Sun, YuGang Jiang, and Xipeng Qiu. Anygpt: Unified multimodal llm with discrete sequence modeling, 2025. 3, 5, [64] Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng Qiu. Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities. arXiv preprint arXiv:2305.11000, 2023. 1 [65] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. 3, 5, 6 [66] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. 1 [67] Shaolei Zhang, Shoutao Guo, Qingkai Fang, Yan Zhou, and Yang Feng. Stream-omni: Simultaneous multimodal interactions with large language-vision-speech model. arXiv preprint arXiv:2506.13642, 2025. 3 [68] Ziwei Zhou, Rui Wang, and Zuxuan Wu. Daily-omni: Towards audio-visual reasoning with temporal alignment across modalities, 2025. 3 [69] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 1 [70] Daniil Zverev, Thaddaus Wiedemer, Ameya Prabhu, Matthias Bethge, Wieland Brendel, and Koepke. Vggsounder: Audio-visual evaluations for foundation models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10271037, 2025. 1, 2, See, Hear, and Understand: Benchmarking Audiovisual Human Speech Understanding in Multimodal Large Language Models"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Annotation Details This section outlines the details of our annotation process. We first describe the interface used to collect audiovisual annotations, followed by the task guidelines provided to annotators to ensure consistency across examples. We then outline the criteria used to select experienced annotators for this work. Finally, we include trivially solvable case to illustrate the intended task clarity and establish lower bound on annotation difficulty. A.1. Annotation Interface Figure 5 shows the web-based interface used to collect multiple-choice audiovisual annotations. Each example is presented as self-contained card with video player at the top and all annotation fields displayed below. The card includes immutable metadata (e.g., video id, video type, category, sub category, task id) and an approved flag, followed by the natural-language question and four answer options (AD). Annotators watch the clip, specify the temporal span used for reasoning via the start time and end time fields, select the correct answer, and provide short explanation in the reason for the answer field, typically referencing visual or auditory evidence. The researchers responsible for review will examine the reason together with the question and answer choices to determine whether question should be refined or approved. A.2. Annotation Guideline A.2.1. General Guidelines We provide annotators with set of general guidelines to ensure consistent and highquality audiovisual question creation. Annotators work from pool of English-language YouTube videos and follow standardized workflow: each clip is added to the interface, 5-to-30-second segment is selected, and single multiple-choice question is authored according to the assigned task type. Annotators record the answer, and supply brief justification. Video Selection and Clip Duration. We provide the following constraints to maintain data quality: Captions. Select videos without persistent on-screen subtitles or captions that would trivially reveal the spoken content. Content safety. Exclude clips containing extreme or explicit violence or gore. Mild or non-gratuitous physical conflict is allowed, but highly graphic or disturbing content should be avoided. Language. All selected videos must be in English. Identity visibility. Ensure that the queried identity is clearly visible in the foreground and is visually distinguishable from other people in the scene. Discriminative attributes. When designing questions about specific person, choose attributes (e.g., clothing, position, actions) that clearly distinguish that person from others in the clip. Contextual distractors. Construct all answer options from entities, actions, or speech events within the same clip. When there are not enough distinct candidates, form distractors by recombining visible or auditory attributes (e.g., clothes, positions, or phrases) so that each option remains contextually plausible and balanced. Clip length. Localize each question to contiguous 5 30 segment, depending on the tasks requirements, trimming longer spans to the minimal window that still supports correct reasoning. A.2.2. Task-specific Guideline We provide task-specific requirements in Table 5. Annotators apply these templates when constructing multiplechoice questions. The annotators will add few questions for us to provide feedback before they scale up the data annotation. A.3. Annotator Selection To ensure high-quality labels for tasks requiring finegrained audiovisual reasoning, we restrict annotation to experienced researchers rather than crowd workers. Our annotator team has prior research experience in video understanding, speech processing, or multimodal large language models, enabling them to identify and select clips that are meaningful and informative for the tasks. This careful selection process is crucial for producing labels of sufficient quality and reliability, which would be difficult to achieve with crowd-sourced annotators. In addition, researchers are trained on the full task taxonomy (see Section A.2) before annotation. This ensures that annotators understand the distinction between trivially solvable cases and those requiring joint audiovisual inference, and could apply task definitions precisely. A.4. Eliminating Trivially Solvable Questions key step in our quality-control pipeline is identifying and removing trivially solvable questionsitems whose an1 Figure 5. Annotation interface for ratecomparison tasks. The interface presents annotators with the video clip, metadata (video ID, category, task type), the question, all answer choices, and the selected response. Annotators also specify the temporal window used for judgment and provide brief justification. The examples shown correspond to (left) lowest rate of speech, (middle) highest rate of speech, and (right) lowest rate of speech for different time span within the same video. These examples illustrate how annotators validate temporal reasoning by explicitly grounding answers in the video timeline. swers can be inferred without reference to specific temporal or spatial cues in the clip. These questions arise when the structure of the video or the phrasing of the question makes the correct answer globally obvious, regardless of any localized event. Typical examples include questions where the target event occurs throughout the video or is otherwise easily predictable, providing little meaningful challenge for the fine-grained audiovisual reasoning our tasks are designed to assess. Persistent speaker patterns: e.g., one person speaks continuously throughout the clip, making questions such as Does the man speak when event happens? solvable without checking the requested moment. Constant scene composition: e.g., the number of visible people remains unchanged across the entire segment, allowing How many people are visible when event occurs? to be answered without performing momentspecific counting. Globally obvious answers: cases where the question refers to specific moment or segment, but the answer can be determined from property that holds across the entire clip rather than from the localized event. Always-on video captions: some videos contain burnedin subtitles throughout the entire clip. In such cases, questions involving spoken content (e.g., What does the man say after event X?) become answerable by simply reading the captions rather than performing speech recognition or audiovisual alignment. Therefore, these questions fail to evaluate the intended modality. Figure 6 shows two representative examples. In the first, only single person is visible across the entire segment, so the answer to moment-specific visibility question is globally obvious. In the second, the spoken line appears verbatim as burned-in captions on the screen, allowing the correct answer to be selected without listening to or timing the utterance. Such items do not evaluate the intended audio visual capabilities and are therefore removed. 2 Figure 6. Examples of trivially solvable questions removed during filtering. (Top) moment-specific visibility question becomes trivial because only one person is visible throughout the entire clip, making the answer recoverable without grounding to the referenced utterance. (Bottom) speech-content question becomes trivial because the spoken line appears as burned-in captions, allowing the answer to be selected without performing audio-based reasoning. B. Detailed Evaluation This section provides additional details on our evaluation protocol beyond what is reported in the main paper. We first specify the exact prompts and answer format used when querying multimodal language models on our multiplechoice audiovisual questions (Sec B.1). We then describe our procedure for measuring human performance, including annotator recruitment, instructions, and aggregation of responses (Sec B.2), which serves as reference ceiling for model accuracy on our benchmark. B.1. Evaluation Prompt At test time, we query models with single unified prompt for all tasks. Figure 7 illustrates the format. Each instance consists of (i) the video input (uniformly sampled frames plus the full audio track), (ii) fixed instruction, and (iii) multiple-choice question with four options labeled AD. The instruction reads: Select the best answer to the following multiple-choice question based on the video. Respond with only the letter (A, B, C, or D) of the correct option. The question and answer block then specifies the task (e.g., speech intensity, speaker counting) and lists four candidate answers in natural language. We parse the models prediction by mapping it to the corresponding option among A, B, C, or D. Any response that does not contain valid option is counted as incorrect. This strict, letter-only forFigure 7. Evaluation prompt used for multimodal LLMs. For each example we show short video (represented here by keyframes and the waveform), fixed natural-language instruction, and multiple-choice question with four options (AD). Models must answer by outputting only the letter of the correct option. mat helps create deterministic results through standardized instructions across all sub-benchmarks, following prior work [13]. B.2. Human Performance To obtain reliable human upper bound, we conduct separate human evaluation that is fully decoupled from the dataset annotation workflow. None of the human evaluators participated in authoring or reviewing the dataset questions. The evaluation team consists of ten undergraduate and masters students, all with strong English proficiency. Each evaluator answered roughly 300 questions, providing broad, non-overlapping coverage of the test set. This design follows common practice in recent multimodal benchmarks and ensures that the reported human accuracy reflects genuine task difficulty rather than annotator familiarity. Evaluation Interface and Protocol. Figure 8 shows the interface used for human evaluation. The interface presents the video clip, accompanied by playback controls (Play, Pause, Replay), followed by the multiple-choice question exactly as posed to the model. Evaluators select their answer from four options (AD) without access to transcripts, subtitles, or any auxiliary textual resources. To keep track on the number of speakers in each question, the interface also includes control prompt asking evaluators to report the total number of people visible in the video. Each evaluator completes their assigned questions through the interface in random order. After submission, the interface immediately advances to the next item, preventing revisiting or revision. Evaluators do not have access 3 (a) Cross-modality attribution. In Figure 9a, the question asks how many unique people speak after the host waves his hand toward the screen. The video shows that only the host and woman respond verbally. However, the model attributes the utterance to the man standing on the left, and further hallucinates an additional off-screen male speaker. This error reflects failure to align the acoustic evidence (voice timbre and timing) with the correct visual identity, leading to incorrect speaker attribution and overcounting of speakers. (b) Audio and visual perception. The example in Figure 9b targets the person with the lowest voice pitch among several women. The model first misperceives the visual scene, describing non-existent woman in dark green off-the-shoulder top instead of the actual woman in blue dress. It then claims that the woman in khaki sweater has the lowest pitch, even though her pitch is not the lowest in the clip. This case jointly exposes incorrect visual (wrong clothing description) and incorrect audio perception (wrong comparative pitch), showing that errors in one modality can reinforce errors in the other. (c) Temporal grounding. The example in Figure 9c probes whether the model can restrict its reasoning to short temporal window. The question asks how many people are visible, even partially, when the man with cup in his hand speaks at the specific frame that the man starts saying the phrase. The model correctly identifies this time span as the anchor, but then counts people who appear before (a person walking in the background) and after (a person farther back near the cars). The answer is therefore based on people outside the annotated window, illustrating temporal grounding error: the anchor is found, but the visual evidence is aggregated over broader interval than the question specifies. (d) Temporal localization. In Figure 9d, the example focuses on the number of times remember it is mentioned by all speakers from the beginning of the clip until specific action: the man in the blue sweater putting on the headphones. In the video, the relevant action occurs at approximately 00:08, and the correct count is determined by utterances up to that moment. The model, however, localizes the anchor around 00:10 in its reasoning trace, shifting the cut-off point later in time. This mis-localization changes which utterances are counted and leads to an incorrect answer, highlighting that even small temporal offsets can cause large reasoning errors in speech-counting tasks. Figure 8. Human evaluation interface. Evaluators watch the video clip, then answer the corresponding multiple-choice question (AD). No transcript or subtitle is provided. The interface also includes an optional refinement tag and control question asking for the total number of people visible in the video. This setup ensures that human performance is independent of annotation and directly comparable to model outputs. to ground-truth labels or the responses of other participants. The interface additionally provides an optional refinement tag (e.g., trivial error) that allows evaluators to flag questions with clear issues, such as mismatches between the question and answer options, or between the video and the question. These tags are used for data sanity checks and are not shown to models or used in computing human accuracy. In practice, no refinement tags were submitted during the human evaluation process. This protocol ensures that human performance is (i) independent of the annotation process, (ii) based solely on audiovisual information, and (iii) directly comparable to model predictions under the same multiple-choice setting. C. Qualitative Analysis by Error Pattern Figure 9 presents qualitative examples of Gemini 2.5 Pros reasoning traces on AV-SpeakerBench, organized by four In all cases, the model has representative error patterns. access to the full video and audio segment, yet its thinking reveals where the audiovisual reasoning breaks down. 4 Figure 9. Qualitative examples of Gemini 2.5 Pro reasoning traces on AV-SpeakerBench. Green and red highlight colors indicate the models correct and incorrect reasoning, respectively. The figure above contains representative failure cases spanning four key error patterns: (a) cross-modality attribution, (b) audio and visual perception, (c) temporal grounding, and (d) temporal localization. Detailed analyses are provided in the subsection. D. Question Examples To make the task definitions concrete, we visualize representative multiplechoice questionanswer pairs from AVSpeakerBench. Each example is shown as panel containing (from top to bottom) strip of video frames, the corresponding audio waveform, naturallanguage question, and four answer options. Across all tasks, the model is evaluated by its accuracy in selecting the correct option, given the paired audiovisual clip. Figure 10 groups the speaker-centric tasks: Speaker Detection, Speaker Recognition, and Speaker Counting. In these tasks, the evaluation focuses on whether the model can correctly associate speech segments with the corresponding visible speaker(s), distinguishing who is speaking, when they speak, and how many distinct speakers are active. Figure 11 illustrates speakervisual tasks such as Activity Recognition and visually grounded counting. Here, the model must reason jointly about speaking person and their on-screen actions or surrounding context (e.g., who performs an action while or around speaking, or how often visually specified person speaks), coupling appearance, motion, and speech. Figure 12 showcases speech reasoning tasks: Speech Recognition, Speech Counting, and Speech Duration. The evaluation in these tasks requires recovering short spoken phrases, counting phrases, and compare the duration of speech. Finally, Figure 13 presents speech attribute tasks, including and comparative speech questions on rate, pitch, and intensity. These examples emphasize fine-grained acoustic reasoning: the model must use the audio waveform to com5 Speaker-centric Visual-centric Audio-centric Task Type Detection Recognition Counting Attribute Recognition Activity Recognition Counting Recognition Duration Pitch Rate Intensity Counting Overall Human Performance Gemini 2.5 Pro Thinking [15] Gemini 3 Pro Thinking 96.02 81.73 85. 93.13 74.15 79.86 94.28 74.13 73.13 93.14 72.55 80. 93.20 73.30 79.13 94.15 62.93 71.71 96.52 77.11 87. 90.68 93.20 91.39 94.17 78.81 75.85 67.48 70. 69.86 78.47 71.84 75.24 93.40 63.89 70.14 93.74 73.04 77. Table 4. Evaluation Results of Gemini 3 Pro (Thinking) on AV-SpeakerBench. pare how different people speak (e.g., how fast, how high or loud). For compactness, each figure arranges three examples per row and one row per task group, while maintaining consistent interface across all panels. Together, these four figures highlight the diversity of natural-language formulations and reasoning skills targeted by AV-SpeakerBench, spanning identity, actions, counting, appearance, speech, and attributes within unified audiovisual QA framework. E. Performance of Gemini 3 Pro on AV-"
        },
        {
            "title": "SpeakerBench",
            "content": "We evaluate the recently released Gemini 3 Pro (Thinking)1 and compare it with Gemini 2.5 Pro on AV-SpeakerBench. As shown in Table 4, Gemini 3 Pro attains an average accuracy of 77.62%, outperforming Gemini 2.5 Pro (73.04%) by +4.6 points. The improvements are broad and most pronounced in tasks requiring fine-grained audiovisual grounding, including speaker-centric understanding, visual reasoning, speech recognition, and prosody-related perception. However, several categoriesmost notably counting and speech-attribute reasoningremain far from solved, and substantial gap persists relative to human performance (93.74%). Overall, Gemini 3 Pro represents meaningful progress but still falls short of demonstrating robust and generalizable audiovisual reasoning. 1Released after the main paper deadline. 6 Figure 10. Visualization of speaker tasks. Top: Speaker Detection. Middle: Speaker Recognition. Bottom: Speaker Counting. Figure 11. Visualization of speaker-visual tasks. Top: Activity Recognition. Middle: Visual Counting. Bottom: Attribute Recognition. 8 Figure 12. Visualization of speech tasks. Top: Speech Counting. Middle: Speech Duration. Bottom: Speech Recognition. 9 Figure 13. Visualization of speech attribute tasks. Top: Speech Intensity. Middle: Speech Pitch. Bottom: Speech Rate. Table 5. Task-specific goals and requirements for all annotation tasks. Task Name Requirement"
        },
        {
            "title": "Speaker Detection",
            "content": "Goal: Test whether models can detect whether person is currently speaking among multiple visible people. Select 530 clip with at least two visible individuals, where only one person is clearly speaking during short moment. Choose target person (e.g, the man in black suit), and short utterance as an audio anchor or short event as visual anchor. Formulate question such as: Does the [person] speak or Does the [person] speak before/after/when [utterance/event] occurs? Provide four answer options, with 2 No and 2 Yes. For each No option, put visual reference (e.g, No, he just stares at the opposite person). For each Yes option, put an audio reference (e.g, Yes, he talks about his plan). If there is not enough audio/visual reference, please create plausible distractors that fit the scene context. Speaker Recognition Goal: Evaluate whether models can link an utterance to the correct visible speaker. Select 530 clip with multiple visible speakers. Choose short utterance as an audio anchor or short event as visual anchor. Formulate question using: Who speaks right after/before/when [person] says [utterance]?, Who responds to [person] saying [utterance]?, or Who says [utterance]? Provide four answer options referring to different individuals (e.g., the man in the black suit, the woman in the red dress). If fewer than three people appear, create plausible distractors that fit the scene. Speaker Counting Goal: Assess whether models can count how many distinct people speak in segment. Select 1030 clip with multiple visible people and several distinct spoken turns. Choose short utterance as an audio anchor or short event as visual anchor. Formulate question such as: From the start of the video until [utterance/event], how many people speak, out of the people in the video? or After [utterance/event] until the end of the video, how many people speak, out of the people in the video? Provide four numerical answer options in the format out of (e.g., 1 out of 2, 2 out of 3); the first number must be the true count of speakers in the specified interval, and the second number must be the number of visible people in that segment. Ensure that speakers are visually distinguishable and separated by clear turn-taking; create plausible nearby counts as distractors. Attribute Recognition Goal: Test whether models can connect speakers appearance to their speech right before or after an anchor. Select 1030 clip with multiple visible speakers. Choose short utterance as an audio anchor. Formulate question such as: What is the appearance of the person who says [immediately before/after/when] the person says [utterance]? or What does the person who says [utterance] wear? Ask about attributes like clothes, clothing shape, clothing color, hair, hairstyle, or hair color. Provide four answer options describing different plausible appearances; exactly one must match the correct speaker. If fewer than four distinct appearances are visible, create plausible distractors consistent with the scene. 11 Task Name Requirement"
        },
        {
            "title": "Activity Recognition",
            "content": "Goal: Measure whether models can associate speaker with actions that happen before/after/while they speak. Select 1030 clip with multiple visible speakers and at least one clear action (e.g., standing up, waving, pointing). Choose short utterance as an audio anchor. Formulate question such as: What does the person do [immediately before/after/when] the person says [utterance]? or When does the person say [utterance]? For the first question type, encode the activity (e.g., stands up, raises hand) into the four answer choices. For the second question type, encode [before/after/when] + [activity] inside the four answer choices. If fewer than three distinct activities appear, create plausible distractors consistent with the scene."
        },
        {
            "title": "Visual Counting",
            "content": "Goal: Assess whether models can count visible entities conditioned on an audio anchor. Speech Recognition Select 1030 clip with multiple visible people. Choose short utterance as the reference time. Formulate question such as: After/Before/When the [utterance] occurs, how many people are visible, even partly? Provide four numerical answer options (e.g., 1, 2, 3, 4) or phrases that clearly encode the count. Ensure all counted entities are clearly visible in the frame; create plausible distractors using nearby counts. Goal: Evaluate whether models can recognize the spoken content in the clip. Select 1030 clip with multiple visible speakers. Choose short visual event as an anchor. Formulate question such as: What does the person say [just before/after/when] the [event] occurs? Provide four answer options corresponding to different possible transcripts; exactly one must match the spoken phrase (allow light paraphrasing only for the correct option). Ensure distractor phrases are grammatical and plausible in the context of the video. Speech Duration Goal: Check whether models can reason about relative speaking time. Select 520 clip where at least two people each speak for clearly different durations. Choose an event as visual anchor. Formulate question such as: Of those who speak, who speaks the most/least?, From the start of the video, until [event], who speaks the most/least?, or After [event], until the end of the video, who speaks the most/least? Provide four answer options referring to different individuals (e.g., the man in the black suit, the woman in the red dress). Ensure the duration difference is perceptible (at least 12 s); construct plausible but incorrect visual options as distractors. Speech Pitch Goal: Test whether models can distinguish speakers by relative pitch. Select 5-10 clip where at least two speakers have clearly different pitch ranges. Formulate question such as: Among those who speak, who has the highest/lowest pitch? Provide four answer options referring to different individuals in the scene (e.g., the man in the black suit, the woman in the red dress). If fewer than three speakers are present, create plausible distractors based on other visible characters that could plausibly speak. Task Name"
        },
        {
            "title": "Speech Rate",
            "content": "Requirement Goal: Measure whether models can compare how fast different speakers talk. Select 510 clip with at least two speakers whose speaking rates differ noticeably. Formulate question such as: Among those who speak, who has the highest/lowest rate of speech? Provide four answer options referring to different speakers (e.g., by clothing or position). Ensure the difference in speaking rate is clear (e.g., more syllables in similar time), and create plausible distractors when needed."
        },
        {
            "title": "Speech Intensity",
            "content": "Goal: Assess whether models can reason about speech loudness. Select 510 clip where at least two speakers talk with clearly different loudness levels (e.g., one shouting, one speaking softly). Formulate question such as: Among those who speak, who has the highest/lowest voice? Provide four answer options referring to different individuals (e.g., the man in the black suit, the woman in the red dress). Avoid clips where microphone distance alone explains loudness, unless this is visually clear; create plausible distractors as needed. Speech Counting Goal: Evaluate whether models can count number of phrases or keywrods. Select 530 clip with multiple short utterances or turns of speech. Choose an event as visual anchor. Formulate question such as: From the start of the video, until just before [event], how many times [phrase] is mentioned by [person/everyone]? or After [event], until the end of the video, how many times [phrase] is mentioned by [person/everyone]? Provide four numerical answer options; exactly one must match the true count. Ensure utterances are separated enough to be countable (clear pauses or turn-taking), and use nearby counts (e.g., 1, n, + 1, + 2) as distractors."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "Kookmin University",
        "University of Wisconsin-Madison"
    ]
}