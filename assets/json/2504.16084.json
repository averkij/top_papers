{
    "paper_title": "TTRL: Test-Time Reinforcement Learning",
    "authors": [
        "Yuxin Zuo",
        "Kaiyan Zhang",
        "Shang Qu",
        "Li Sheng",
        "Xuekai Zhu",
        "Biqing Qi",
        "Youbang Sun",
        "Ganqu Cui",
        "Ning Ding",
        "Bowen Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper investigates Reinforcement Learning (RL) on data without explicit labels for reasoning tasks in Large Language Models (LLMs). The core challenge of the problem is reward estimation during inference while not having access to ground-truth information. While this setting appears elusive, we find that common practices in Test-Time Scaling (TTS), such as majority voting, yield surprisingly effective rewards suitable for driving RL training. In this work, we introduce Test-Time Reinforcement Learning (TTRL), a novel method for training LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs by utilizing the priors in the pre-trained models. Our experiments demonstrate that TTRL consistently improves performance across a variety of tasks and models. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by approximately 159% on the AIME 2024 with only unlabeled test data. Furthermore, although TTRL is only supervised by the Maj@N metric, TTRL has demonstrated performance to consistently surpass the upper limit of the initial model, and approach the performance of models trained directly on test data with ground-truth labels. Our experimental findings validate the general effectiveness of TTRL across various tasks, and highlight TTRL's potential for broader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL"
        },
        {
            "title": "Start",
            "content": "TTRL: Test-Time Reinforcement Learning TTRL: Test-Time Reinforcement Learning Yuxin Zuo1 Kaiyan Zhang1 Shang Qu1,2 Biqing Qi2 Youbang Sun1 Ganqu Cui2 Ning Ding1,2 Bowen Zhou1,2 1Tsinghua University Li Sheng1,2 Xuekai Zhu1 2Shanghai AI Lab https://github.com/PRIME-RL/TTRL"
        },
        {
            "title": "Abstract",
            "content": "This paper investigates Reinforcement Learning (RL) on data without explicit labels for reasoning tasks in Large Language Models (LLMs). The core challenge of the problem is reward estimation during inference while not having access to ground-truth information. While this setting appears elusive, we find that common practices in Test-Time Scaling (TTS), such as majority voting, yield surprisingly effective rewards suitable for driving RL training. In this work, we introduce Test-Time Reinforcement Learning (TTRL), novel method for training LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs by utilizing the priors in the pre-trained models. Our experiments demonstrate that TTRL consistently improves performance across variety of tasks and models. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by approximately 159% on the AIME 2024 with only unlabeled test data. Furthermore, although TTRL is only supervised by the Maj@N metric, TTRL has demonstrated performance to consistently surpass the upper limit of the initial model, and approach the performance of models trained directly on test data with ground-truth labels. Our experimental findings validate the general effectiveness of TTRL across various tasks, and highlight TTRLs potential for broader tasks and domains. 5 2 0 2 2 2 ] . [ 1 4 8 0 6 1 . 4 0 5 2 : r Figure 1: Performance and Position of TTRL. Equal Contribution. Kaiyan Zhang (zhang-ky22@mails.tsinghua.edu.cn) and Ganqu Cui lead the project. : Corresponding authors. 1 TTRL: Test-Time Reinforcement Learning"
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Test-Time Reinforcement Learning (TTRL) 2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 Majority Voting Reward Function . . . . . . . . . . . . . . . . . . . . . . . . . 3 Experiments 3.1 Experimental Setup . 3.2 Main Results . . . . . 3.3 Training Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Discussions 4.1 Q1: How Well Can TTRL Perform? . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Q2: Why Does TTRL Work? . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 4 4 5 5 6 7 8 9 4.3 Q3: When Might TTRL Fail? . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 5 Related Works 5.1 Test-Time Scaling . 5.2 RL for Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 Conclusion 7 Limitations and Future Works Terminology A.1 Test-Time Training (TTT) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Test-Time Inference (TTI) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Hyper-parameters 12 12 12 13 13 18 18 19 2 TTRL: Test-Time Reinforcement Learning"
        },
        {
            "title": "Introduction",
            "content": "Test-Time Scaling (TTS) (Zhang et al., 2025b; Balachandran et al., 2025) represents an emerging trend for enhancing the reasoning capabilities of Large Language Models (LLMs). Recent studies (Snell et al., 2024; Liu et al., 2025a) suggest that TTS is more computationally efficient than scaling during pre-training (Kaplan et al., 2020), enabling superior performance with equivalent computational investment. Numerous studies have explored enhancing TTS through reward models (Lightman et al., 2023; Yuan et al., 2024; Zhang et al., 2025a; Zhao et al., 2025), employing strategies such as majority voting (Stiennon et al., 2020; Nakano et al., 2021) and Monte Carlo tree search during the decoding phase. Recent leading Large Reasoning Models (LRMs), such as DeepSeek-R1 (Guo et al., 2025) and OpenAIs o1 (El-Kishky et al., 2025), highlight that Reinforcement Learning (RL) plays crucial role in enhancing long chain-of-thought (Wei et al., 2022) thinking. However, LRMs still struggle to solve streams of unlabeled, newly arriving data. For instance, while OpenAI o3 achieves 75.7% success rate on ARC-AGI-1, it solves only 4% of problems on ARC-AGI-2 (2025) 1. These studies on TTS have clearly demonstrated discrepancy between train-time and test-time behavior, especially in RL-based approaches that focus on train-time. However, applying RL solely on large-scale training data is severely insufficient for handling novel features or distributional changes in newly emerging and highly complex inputs. Recently, Test-Time Training (TTT) methods, which allow model parameters to be updated at test-time using the incoming test data (Sun et al., 2019; 2024; Behrouz et al., 2024; Aky urek et al., 2024), have attracted increasing attention. These methods provide natural and promising direction for fully advancing TTS by fine-tuning the model at test-time using RL, thereby improving its generalization to unseen data. However, this introduces another critical challenge: How to obtain rewards or verifier for RL at test-time? As real-world tasks grow in complexity and quantity, annotating such data at scale for RL becomes increasingly impractical. This presents substantial obstacle to the continual learning of leading models. To address these issues, we introduce Test-Time Reinforcement Learning (TTRL), which performs test-time training through RL. TTRL employs repeated sampling strategies in the rollout phase to accurately estimate the label and compute rule-based rewards, thereby enabling RL on unlabeled data. By incorporating effective majority voting rewards, TTRL facilitates efficient and stable RL in the absence of ground truth labels. As previously highlighted, the emergence of more challenging tasks will inevitably lead to larger proportions of unlabeled data. TTRL directly addresses the problem of training models without explicit supervision, investigating models ability to explore and learn under this challenging yet critical setting. Essentially, TTRL enables the model to generate its own experiences, estimate rewards, and improve its performance over time. In experiments, applying TTRL to Qwen2.5-Math-7B results in performance improvement on AIME 2024 of 159% (13.3 to 43.3), with an average gain of 84% across AMC, AIME, and MATH-500. These improvements are achieved through self-evolution without any labeled training data, and further generalize to other tasks. TTRL not only enhances performance on pass@1 but also improves TTS through majority voting. Moreover, our preliminary experiments suggest that TTRL is effective across models of different scales and types, and that it can be integrated with existing RL algorithms. We also found that TTRL exhibits favorable characteristics such as high performance ceiling. These observations highlight its potential to substantially reduce reliance on human annotations, enabling continual learning and scaling RL to large-scale unsupervised training. Below are several key takeaways: Takeaways 1. Majority voting provides effective reward estimation for TTRL ( 3). 2. TTRL can exceed its own training signal and upper limit Maj@N, and closely mirrors the performance of direct training on the test data with ground-truth ( 4.1). 3. It is possible to achieve efficient and stable RL in an unsupervised manner ( 4.2). 1https://arcprize.org/ 3 TTRL: Test-Time Reinforcement Learning Figure 2: TTRL combines both Test-Time Scaling (TTS) and Test-Time Training (TTT)."
        },
        {
            "title": "2 Test-Time Reinforcement Learning (TTRL)",
            "content": "Unlike traditional RL, where the agent learns from known reward signals, TTRL operates on unlabeled test data. In other words, the model must learn and adapt without access to explicit supervision. Our task is defined as follows: We study the problem of training pre-trained model during test time using RL without ground-truth labels. We call this setting Test-Time Reinforcement Learning. 2.1 Methodology Figure 2 illustrates how our approach, TTRL, tackles this challenge. Given state represented by the prompt x, the model acts by producing an output sampled from policy πθ(y x) parameterized by θ. To construct reward signal without ground-truth labels, we generate multiple candidate outputs {y1, y2, . . . , yN} from the model through repeated sampling. consensus output is derived, for instance, by majority voting or another aggregation method, serving as proxy for the optimal action. The environment then provides reward r(y, y) based on the alignment between the sampled action and the consensus action y. The RL objective is thus to maximize the expected reward: (1) max θ yπθ (x)[r(y, y)], and parameters θ are updated through gradient ascent: θ θ + ηθ yπθ (x)[r(y, y)], (2) where η denotes the learning rate. This approach enables the model to adapt during inference, effectively improving its performance on distribution-shifted inputs without the need for labeled data. 2.2 Majority Voting Reward Function The majority voting reward is determined by first estimating label through majority voting. This estimated label is then used to calculate rule-based rewards, which serve as the final rewards. Given question x, we first input into the LLM to generate set of outputs. An answer extractor then processes these outputs to obtain the corresponding predicted answers, denoted as = { ˆyi}N i=1. We first follow Equation 4 over to estimate label, with majority voting as the scoring function s(y, x) to get y, the most frequently occurring prediction in P. The majority-voted prediction is then used as the estimated label to 4 TTRL: Test-Time Reinforcement Learning compute rule-based rewards (Guo et al., 2025). The reward function is: R( ˆyi, y) = (cid:26)1, if ˆyi = y, 0, otherwise. (3) Listing 1: The pseudo-code of the majority voting reward function. from collections import Counter def majority_voting_reward_fn ( outputs ): \"\"\" Assigns reward of 1 to each output whose extracted answer matches the majority answer , otherwise 0. \"\"\" # Extract answers from each output answers = [ extract_answer ( output ) for output in outputs ] # Find the majority answer counts = Counter ( answers ) majority_answer , _ = counts . most_common (1) [0] # Assign rewards : 1 if matches majority , else 0 rewards = [1 if ans == majority_answer else 0 for ans in answers ] return rewards outputs = llm . generate ( problem , =N ) rewards = majority_voting_reward_fn ( outputs ) 1 2 3 4 6 7 8 9 10 12 13 14 15 16"
        },
        {
            "title": "3 Experiments",
            "content": "3.1 Experimental Setup Models To evaluate the generality of TTRL across different backbone models, we conduct experiments using both the base and instruct models. We experiment on the base models Qwen2.5-Math-1.5B and Qwen2.5-Math-7B (Yang et al., 2024) to evaluate whether TTRL scales effectively. For instruct models, we use LLaMA-3.1-8B-Instruct (Grattafiori et al., 2024) to validate the effectiveness of TTRL across different model families. Benchmarks We evaluate TTRL on 3 mathematical reasoning benchmarks: AIME 2024 (Li et al., 2024), AMC (Li et al., 2024), and MATH-500 (Hendrycks et al., 2021). After applying TTRL on each benchmark individually, we evaluate using greedy decoding to report pass@1 to ensure fair comparison with previous works. We also use the decoding parameters of DeepSeek-R1 (Guo et al., 2025) and report the Avg@64 score in Figure 6 to provide more reliable evaluation. Baselines Since the use of TTT for enhanced reasoning has not been previously explored, we primarily compare it with the backbone model to validate whether TTRL can achieve effective improvements through self-evolution. For the two Base models, we also include comparisons with their Instruct versions that have undergone large-scale post-training. In addition, we include for reference current leading R1-Zero-Like models with similar backbones, which are extensively trained using RL: DeepSeek-R1-Distill-1.5B&7B (Guo et al., 2025), SimpleRL-Zero-7B (Zeng et al., 2025), PRIME-Zero-7B (Cui et al., 2025), OpenReasonerZero-7B (Hu et al., 2025b), Oat-Zero-1.5B&7B (Liu et al., 2025b), and LIMR (Li et al., 2025). Note that TTRL has different setup from the previous models, which makes the comparison seem unfair. Implementation Details We independently apply GRPO (Shao et al., 2024) on each benchmark to implement TTRL. For hyperparameters, we use constant learning rate of 5 107 TTRL: Test-Time Reinforcement Learning Table 1: Main results of TTRL on each task. indicates results from Dr. GRPO (Liu et al., 2025b). Our training data size matches the corresponding benchmark dataset size. Name Qwen2.5-Math-1.5B w/ TTRL Qwen2.5-Math-1.5B-Instruct DeepSeek-R1-Distill-1.5B@3k DeepSeek-R1-Distill-1.5B@8k Oat-Zero-1.5B Qwen2.5-Math-7B AIME AMC MATH-500 Avg 28.5 32.5 33.0 53.0 +20.5 63.1% 80.0 +47.0 142.4% 51.0 +22.5 79.0% 48.2 21.7 49.4 53.0 38.6 74.2 52.2 77.4 74.2 50.6 44.1 25.5 48.9 49.1 35.3 20. 20.0 0 0 10.0 2.5 20.0 20.0 16.7 w/ TTRL 43.3 +26.6 159.3% 67.5 +28.9 74.9% 84.2 +33.6 66.4% 65.0 +29.7 84.1% Qwen2.5-Math-7B-Instruct DeepSeek-R1-Distill-7B@3k SimpleRL-Zero-7B PRIME-Zero-7B OpenReasoner-Zero-7B@3k Oat-Zero-7B LIMR-7B LLaMA-3.1-8B-Instruct w/ TTRL 16.7 10.0 26.7 16.7 13.3 43.3 32.5 3.3 3.3 0 0 53.0 26.2 60.2 62.7 47.0 62.7 63.8 19. 83.6 60.1 78.2 83.8 79.2 80.0 78.0 47.8 51.1 32.1 55.0 54.4 46.5 62.0 58.1 23.5 32.5 +13.2 68.4% 61.8 +14.0 29.3% 32.5 +9.0 38.5% Labeled Data - 3.1M 800K 800K 8.9K - 3.1M 800K 8.9K 230K 129K 8.9K 1.4K - and adopt the AdamW optimizer for the policy model. For rollout, we sample 64 responses (32 for MATH-500) using temperature of 1.0 for voting-based label estimation and downsample 16 responses per prompt for training. Evidence shows that our vote-then-sample strategy effectively reduces computational costs while still achieving strong performance. The maximum number of tokens for generation is set to 3072. The KL coefficient is set to 0 across all experiments. We set the number of episodes to 40, 50, and 60 for MATH, AMC, and AIME, respectively, based on the dataset size and task complexity. 3.2 Main Results TTRL performs well on most tasks and models. Despite relying solely on self-evolution using unlabeled test data, TTRL achieves performance comparable to existing RL-based models trained on large-scale labeled datasets. As shown in Table 1, on the highly challenging mathematical reasoning benchmark AIME 2024, TTRL achieves substantial improvement of 159.3%, surpassing all models trained on large-scale datasets. Furthermore, when applied to Qwen2.5-Math-7B, TTRL yields an average improvement of 84.1% across three benchmarks. TTRL naturally scales. Another noteworthy observation is that as the model size increases (from 1.5B to 7B), performance gains on both AIME 2024 and AMC increase, highlighting the natural scaling behavior of TTRL: larger models can produce more accurate majority voting rewards during self-improvement, which leads to more effective learning on new data. However, LLaMA-3.1-8B-Instruct and Qwen2.5-Math-1.5B fail to achieve meaningful gains on AIME 2024 through TTRL, likely due to their limited capacity. In contrast, Qwen2.5-Math-7Bs greater model capacity and more sufficient knowledge allows it to benefit from self-improvement, resulting in clear performance gains. We discuss this in more detail in Section 4.3. 6 TTRL: Test-Time Reinforcement Learning Figure 3: Out-of-distribution performance before and after TTRL. (a) Accuracy Curve. (b) Entropy Curve. Figure 4: Comparison over steps of different RL algorithms, GRPO vs PPO on MATH-500. TTRL generalizes well beyond the target task. We perform TTRL on each benchmark and further evaluate on others, with Qwen2.5-Math-7B as the backbone. Figure 3 shows the results. Despite the out-of-distribution nature of this setting, TTRL achieves substantial improvements acorss all benchmarks. This suggests that TTRL does not rely on overfitting, which would lead to trade-offs on other tasks, but instead acquires generalizable gains during self-improvement. TTRL is compatible with different RL algorithms. Figure 4 presents the results. We apply TTRL using PPO (Schulman et al., 2017) on MATH-500 to assess its compatibility with different reinforcement learning algorithms. The performance trajectories of PPO and GRPO are closely aligned. Compared to GRPO, PPO yields more stable outcomes while achieving similar overall performance. 3.3 Training Dynamics Given the absence of ground-truth labels in the test data, evaluating the performance of TTRL throughout the training process presents challenge. To mitigate this limitation, we introduce set of training-time metrics specifically designed to monitor and assess the effectiveness of TTRL. These metrics inform the selection of the optimal checkpoint and provide valuable insights regarding training dynamics. Figure 5 shows two curves of TTRL on AIME 2024 with Qwen2.5-Math-7B as an example. Entropy: Measures the uncertainty of the models generation. Majority Voting Reward: Rule-based rewards computed from the majority-voted label. Majority Ratio: The frequency of the most common answer within rollout. 7 TTRL: Test-Time Reinforcement Learning (a) Accuracy Curve. (b) Entropy Curve. Figure 5: The entropy and accuracy curves of TTRL on AIME 2024 with Qwen2.5-Math-7B. Furthermore, we define several metrics that rely on access to ground-truth labels, which allow for deeper analysis of the models behavior during training: Label Accuracy (Maj@N): Indicates whether the estimated label matches ground-truth. Reward Accuracy: Indicates the proportion of rewards computed from the estimated label that match the rewards computed from the ground-truth label. Ground-Truth Ratio: The frequency of the ground-truth answer within rollout."
        },
        {
            "title": "4 Discussions",
            "content": "4.1 Q1: How Well Can TTRL Perform? Takeaways 1. TTRL not only surpasses its training signal and the intuitive upper bound Maj@N of the initial model, but also approaches the performance of direct RL trained with labeled test data. This improvement may be attributed to TTRLs use of RL for test-time training: by converting voting-based pseudo-labels into rewards, it enhances the effective supervision quality (e.g., accuracy; see Q2 4.2) while decoupling learning from the limitations imposed by Maj@N. 2. The empirical upper bound of TTRL is training on the test data (i.e., training on test data), highlighting its potential advantages in efficacy over standard trainingevaluation protocols. 3. For challenging tasks, TTRL can reach the empirical upper bound using only 1.5B model. This demonstrates that LLMs can now efficiently self-evolve through TTRL, enabling unbounded lifelong learning on large-scale datasets. We analyze the potential performance of TTRL using two upper bounds. The first upper bound is Maj@N, which is used to compute rewards during TTRL training. The second upper bound is direct training on benchmark datasets, which assumes access to ground-truth labels and thus leaks label information to the policy model. TTRL is Supervised by Maj@N Yet Surpasses It. Since TTRL uses the models own majorityvoted outputs for RL, this voting-based performance of its base model can be intuitively regarded as an upper bound of the final performance. However, we observe surprising phenomenon: after training, the model not only matches but even surpasses this expected upper bound, implying that it exceeds the quality of its own outputs, which form its supervisory signals. Figure 6 shows results of TTRL on Qwen2.5-Math-7B. It can be observed that TTRL Avg@64 outperforms Qwen2.5-Math-7B Maj@64 across all benchmarks, exceeding our expectations by considerable margin. Furthermore, the performance of TTRL improves substantially when Majority Voting is applied. This suggests that language 8 TTRL: Test-Time Reinforcement Learning Figure 6: Majority voting performance comparison between backbone and TTRL model. models can improve through training on their own generations. More notably, through self-reinforcing loop, the model lifts itself up by its own bootstraps, evolving beyond the anticipated performance ceiling. TTRLs Performance Gains Approach Training on the Benchmark. The motivation of TTRL is to estimate labels using majority voting to obtain more accurate rewards, facilitating effective self-improvement through RL on the data without ground-truth labels. Therefore, natural upper bound of TTRL is performing RL directly on the test data, denoted as RL (leakage). Although this setting is rarely adopted or studied due to the issue of information leakage, it represents the most efficient way to improve performance on the particular dataset, with efficiency that far exceeds traditional training-evaluation paradigms. We use Qwen2.5-Math-7B to perform both TTRL and RL (leakage) on MATH-500 and conduct evaluations. Figure 7 shows results. Surprisingly, we find that the performance curve of TTRL closely approaches that of RL (leakage). This suggests that: Figure 7: Comparison of RL (Leakage) vs TTRL. 1. TTRL can achieve level of self-improvement comparable to that of supervised learning (even in the information leakage scenario) through RL in an unsupervised setting. This indicates its substantial efficiency and performance gains. 2. TTRL provides evidence that even small LLMs can now effectively self-improve on input-only challenging tasks through RL, enabling continual learning. Results on Qwen2.5-Math-1.5B further support this observation: starting from subpar performance of 33.0 on MATH-500, the model improved by 142.4% to reach 80.0, demonstrating clear self-improvement through TTRL. 4.2 Q2: Why Does TTRL Work? This section presents progressive analysis of the factors enabling TTRL to achieve stable and effective RL under unsupervised conditions. Our analysis covers two key aspects: label estimation and reward calculation. Label Estimations. direct difference between TTRL and standard RL algorithms is that TTRL involves label estimation, which introduces reward inaccuracies. We believe that TTRL works despite these inaccuracies due to the following two reasons. (i) Existing studies 9 TTRL: Test-Time Reinforcement Learning Figure 8: Comparison of Majority Ratio, Label Accuracy, and Reward Accuracy. have shown that RL can tolerate certain degree of reward inaccuracy. Moreover, RL tends to generalize better than supervised fine-tuning (SFT), which often relies on memorizing training data (Chu et al., 2025). In RL, rewards are typically vague and serve primarily as directional signals for exploration, leading to RLs robustness to reward noise (Razin et al., 2025). (ii) Prior work has also examined what constitutes good reward model from an optimization perspective, revealing that more accurate reward models are not necessarily better teachers (Wang et al., 2020). Therefore, reward signals estimated by the policy model itself may offer more suitable guidance for learning. Reward Calculations. When the model is capable of estimating accurate labels via majority voting, the subsequently estimated reward is generally reliable. However, natural question arises: Why does TTRL remain effective even when the model fails to estimate accurate labels on challenging benchmarks such as AIME 2024? The most fundamental reason lies in the definition of rewards in RL. Rule-based rewards are assigned based on whether the predicted answer matches the label. Therefore, even if an estimated label is not the ground-truth, as long as it differs from an incorrectly predicted answer, the system can still assign correct negative reward. To provide more detailed case study, we examine the performance of TTRL on the AIME 2024 on Qwen2.5-Math-7B. Figure 8 presents the variation curves of the three metrics. We identify two main reasons why TTRL remains effective on AIME 2024: First, rewards are denser than labels, allowing for more opportunities to recover useful learning signals even when the estimated label is inaccurate. For instance, even if the predicted label is incorrect, alternative outputs within the same rollout may still yield correct or high-quality rewards, as shown in Figure 9. This makes the overall reward signal more robust to errors in pseudo-label estimation. Second, an interesting phenomenon is that when the model has weaker capability, the rewards given by TTRL may be more accurate. For the base model, the most frequently predicted answer accounts for only 16.6% of all predictions (Figure 8). Therefore, even when the labels are not accurately estimated, most labels can still receive the correct rewards because the models responses are highly diverse and consistently incorrect, as shown in Figure 9. Moreover, the poorer the models performance, the more mis10 Figure 9: toy case. We illustrate basic numerical prediction scenario to compare reward computation under two conditions: when the model incorrectly estimates the label versus when the groundtruth label is used. As shown on the left, although the estimated label is incorrect, some of the incorrect predictions still differ from the wrong label and therefore receive the correct reward (denoted as 0). TTRL: Test-Time Reinforcement Learning Table 2: Performance of TTRL across the five difficulty levels of MATH-500. Name MATH-500-L1 MATH-500-L2 MATH-500-L3 MATH-500-L4 MATH-500-L5 Metric Accuracy Backbone w/ TTRL Response Len. Backbone w/ TTRL 25.9 71.2 +45.4 175.3% 2,339.2 624.3 1,715.0 73.3% 33.0 76.2 +43.2 130.8% 2,125.1 614.4 1,510.6 71.1% 36.3 76.3 +40.0 110.2% 2,120.6 672.3 1,448.3 68.3% 32.5 58.7 +26.2 80.4% 1,775.1 783.5 991.6 55.9% 22.3 39.2 +16.8 75.3% 1,751.3 985.3 766.0 43.7% takes it tends to make, which paradoxically leads to more accurate reward estimation. An empirical observation supporting this view is the comparison between Label Accuracy and Reward Accuracy, as shown in Figure 8. While most label accuracy values fluctuate between 20% and 50%, the reward accuracy starts at an impressive 92%. This high reward accuracy offers reliable foundation for effective self-improvement on test data. 4.3 Q3: When Might TTRL Fail? At the algorithmic level, TTRL is not fundamentally different from existing RL algorithms and therefore inherits several of their characteristics, such as sensitivity to data difficulty, strong reliance on priors, and risk of collapse under certain conditions. At the implementation level, these issues are further amplified by the constraints of TTRL, which estimates labels via majority voting and operates exclusively on test data that is both sparse and previously unseen, potentially resulting in failures in certain scenarios. In our preliminary experiments, we identified two potential issues: Lack of Prior Knowledge on Target Task. Prior knowledge plays crucial role in RL, often determining the success or failure of the TTRL learning process2. This is mainly because the test data generally exhibits higher difficulty and introduces new features, but TTRL does not incorporate mechanisms such as data filtering to support curriculum learning. Therefore, for the same backbone, TTRL fails if the models prior knowledge is insufficient to handle the complexity of the data. We hypothesize that the absence of performance improvement on AIME 2024 for Qwen2.5-Math-1.5B and LLaMA-3.1-8B-Instruct can be attributed to this factor, considering that TTRL yields notable gains on other less complex benchmarks when applied to the same backbone. To further validate this hypothesis, we conduct an ablation study on MATH-500. We divide MATH-500 into five subsets according to its annotated difficulty levels, ranging from 1 to 5, and apply TTRL to each subset independently, using Qwen2.5-Math-1.5B. We then compare the results to those of the backbone, as shown in Table 2. We observe that as the question difficulty increases, both the performance improvement and length reduction ratios tend to decrease. This suggests that the available prior knowledge of the backbone is insufficient to support learning on more challenging questions. Inappropriate RL Hyperparameters. Hyperparameter settings play crucial role in Figure 10: Failed attempts. We compare the curves under settings with appropriate parameters versus those with suboptimal temperature and training batch size. The results show that the performance is particularly sensitive to the training batch size in such settings. 2https://ysymyth.github.io/The-Second-Half/ 11 TTRL: Test-Time Reinforcement Learning RL training, varying across projects 3 and often leading to training failures. The influence of hyperparameters is further amplified in TTRL due to potential noise in reward estimation and the characteristics of the test data. Figure 10 presents comparison of several unsuccessful attempts on AIME 2024. Both of these failed attempts exhibit persistently high entropy that does not diminish over the course of training, consistent with findings of prior work He et al. (2025). In our preliminary experiments, we identified two key hyperparameters that can critically affect training stability and success: Temperature: Setting the temperature to 1.0, as opposed to 0.6, increases the models output entropy. This promotes more extensive exploration and allows the model to make better use of its prior knowledge for self-improvement, which is particularly important when addressing challenging benchmarks. Episodes: Given the substantial variation in size and difficulty across datasets, smaller and more difficult datasets need more episodes to achieve sufficient exploration."
        },
        {
            "title": "5 Related Works",
            "content": "5.1 Test-Time Scaling Test-Time Scaling (TTS) is designed to enhance the capabilities of Large Language Models (LLMs) in handling complex tasks by increasing computational resources at test time. Prior research (Snell et al., 2024; Liu et al., 2025a) indicates that TTS is more efficient than scaling during pre-training (Kaplan et al., 2020). Therefore, reallocating the same computational resources from pre-training to test-time could yield greater improvements in model performance. Current studies on TTS fall into two categories (Welleck et al., 2024): parallel generation and sequential generation. Parallel generation involves LLMs producing multiple candidate responses (self-consistency (Wang et al., 2022; Chen et al., 2023), best-ofN (Stiennon et al., 2020; Nakano et al., 2021)), decision steps (Monte Carlo Tree Search (Zhou et al., 2023; Xie et al., 2024)), or tokens (Reward-guided Search (Deng & Raffel, 2023; Khanov et al., 2024)) during inference. Subsequently, an aggregation strategy is applied to integrate these candidates, commonly using process reward models (Lightman et al., 2023; Wang et al., 2023; Zhang et al., 2025a). Concurrently, sequential generation focuses on extending the LLMs output to include longer responses with reflective and chain-of-thought processes (Wei et al., 2022; Madaan et al., 2023). Although prompting techniques are widely adopted, they are often constrained by the capabilities of the underlying models. Notably, DeepSeek-R1 (Guo et al., 2025) is representative advancement in this area, achieving extended reasoning capabilities in pre-trained language models through outcome-based reinforcement learning (RL), more specifically group relative policy optimization (Shao et al., 2024). Compared to the first approach, which requires intensive process-level supervision (Yuan et al., 2024), the second approach is more scalable due to its reliance on rule-based rewards. Beyond the aforementioned methods that focus on scaling test-time inference computation, another approach to increasing test-time compute is Test-Time Training (TTT). We introduce the relationship between these terminologies in the Appendix A. While prior work has primarily focused on applications such as video generation and understanding (Hardt & Sun, 2024; Dalal et al., 2025), and to some extent on large language models (Wang et al., 2025; Aky urek et al., 2024), the integration of test-time scaling with reinforcement learning remains largely underexplored. 5.2 RL for Reasoning Reinforcement Learning (RL) (Sutton et al., 1998) plays critical role in enhancing the instruction-following capabilities of Large Language Models (LLMs), particularly through approaches like Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022). RLHF aligns base models with human preferences using algorithms such as Proximal 3https://github.com/TsinghuaC3I/Awesome-RL-Reasoning-Recipes 12 TTRL: Test-Time Reinforcement Learning Policy Optimization (PPO) (Schulman et al., 2017), where preference modeling is essential. Recently, Large Reasoning Models (LRMs), such as DeepSeek-R1 (Guo et al., 2025), have demonstrated the significance of RL in improving reasoning abilities using rule-based rewards, as exemplified by GRPO (Shao et al., 2024). Unlike RLHF, which is tailored to open-domain instructions, GRPO is specifically designed to elicit long Chain-of-Thought (CoT) (Wei et al., 2022) reasoning in mathematical problem solving. Recent studies have focused primarily on improving the training stability of rule-based RL methods like GRPO and PPO (Cui et al., 2025; Yu et al., 2025; Liu et al., 2025b). However, these methods typically train LLMs only on supervised training data, while inference involves generating extended CoT reasoning on unseen test problems. Moreover, current RL approaches (Hu et al., 2025a; Wei et al., 2025b) depend on verifiable outputssuch as solutions in mathematics or codethat can provide reliable reward signals. Such conditions are often impractical in real-world agentic tasks where verification is difficult or impossible (Wei et al., 2025a). Emerging research (Silver & Sutton, 2025) highlights paradigm shift from learning solely from human-annotated data to learning from experiential interactions. In this context, it becomes increasingly important for policy models to generate and label their own training data through real-world interactions. TTRL offers preliminary attempt at RL with self-labeled rewards, advancing toward learning from streams of experience."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we propose Test-Time Reinforcement Learning (TTRL), novel framework for training large language models with Reinforcement Learning (RL) on test data without access to ground-truth labels. key component of TTRL is its majority voting reward function, which generates rule-based rewards based on consensus among model predictions. Our experiments demonstrate the strong potential of TTRL, achieving consistent improvements across variety of models and tasks. We view TTRL as preliminary step toward RL with self-labeled rewards, marking an important direction of learning from continuous streams of experience."
        },
        {
            "title": "7 Limitations and Future Works",
            "content": "Limitations This work represents an initial exploration of test-time reinforcement learning using self-labeled rewards. While our experimental results are promising, there are several aspects that require further investigation. In particular, we plan to conduct more in-depth analysis of the impact of prior knowledge and hyperparameter configurations, both of which play critical roles in reinforcement learning dynamics. We will provide comprehensive discussions and ablation studies in future revisions of this paper. Future Works Building on our findings, we identify several directions for future research: Theoretical Analysis: Developing formal convergence analysis of TTRL, particularly focusing on its ability to optimize toward the two upper bounds in 4.1. Online Learning with Streaming Data: Extending TTRL to real-time learning scenarios, where models interact with continuously arriving data and adapt dynamically, that is Test-Time Adaptation (Liang et al., 2025). Large-Scale Self-Supervised RL Training: Scaling up TTRL to massive datasets and models to explore its potential in self-supervised regimes without human-labeled data. Agentic Tasks and Scientific Discovery: Applying TTRL to more complex, open-ended domains such as agentic tasks and multi-step scientific reasoning. 13 TTRL: Test-Time Reinforcement Learning"
        },
        {
            "title": "References",
            "content": "Ekin Aky urek, Mehul Damani, Linlu Qiu, Han Guo, Yoon Kim, and Jacob Andreas. The surprising effectiveness of test-time training for abstract reasoning. arXiv preprint arXiv:2411.07279, 2024. Vidhisha Balachandran, Jingya Chen, Lingjiao Chen, Shivam Garg, Neel Joshi, Yash Lara, John Langford, Besmira Nushi, Vibhav Vineet, Yue Wu, et al. Inference-time scaling for complex tasks: Where we stand and what lies ahead. arXiv preprint arXiv:2504.00294, 2025. Ali Behrouz, Peilin Zhong, and Vahab Mirrokni. Titans: Learning to memorize at test time. arXiv preprint arXiv:2501.00663, 2024. Xinyun Chen, Renat Aksitov, Uri Alon, Jie Ren, Kefan Xiao, Pengcheng Yin, Sushant Prakash, Charles Sutton, Xuezhi Wang, and Denny Zhou. Universal self-consistency for large language model generation. arXiv preprint arXiv:2311.17311, 2023. Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: comparative study of foundation model post-training. arXiv preprint arXiv:2501.17161, 2025. Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, et al. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025. Karan Dalal, Daniel Koceja, Gashon Hussein, Jiarui Xu, Yue Zhao, Youjin Song, Shihao Han, Ka Chun Cheung, Jan Kautz, Carlos Guestrin, et al. One-minute video generation with test-time training. arXiv preprint arXiv:2504.05298, 2025. Haikang Deng and Colin Raffel. Reward-augmented decoding: Efficient controlled text generation with unidirectional reward model. arXiv preprint arXiv:2310.09520, 2023. Ahmed El-Kishky, Alexander Wei, Andre Saraiva, Borys Minaev, Daniel Selsam, David Dohan, Francis Song, Hunter Lightman, Ignasi Clavera, Jakub Pachocki, et al. Competitive programming with large reasoning models. arXiv preprint arXiv:2502.06807, 2025. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Moritz Hardt and Yu Sun. Test-time training on nearest neighbors for large language models, 2024. URL https://arxiv.org/abs/2305.18466. Jujie He, Jiacai Liu, Chris Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Jiacheng Xu, Wei Shen, Siyuan Li, Liang Xiaoyu Zhang, Fuxiang Zhang, and Yahui Zhou. Zeng, Tianwen Wei, Cheng Cheng, Bo An, Yang Liu, https://capricious-hydrogen-41c.notion.site/ Skywork open reasoner series. Skywork-Open-Reaonser-Series-1d0bc9ae823a80459b46c149e4f51680, 2025. Notion Blog. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Jian Hu, Xibin Wu, Zilin Zhu, Xianyu, Weixun Wang, Dehao Zhang, and Yu Cao. Openrlhf: An easy-to-use, scalable and high-performance rlhf framework. arXiv preprint arXiv:2405.11143, 2024. 14 TTRL: Test-Time Reinforcement Learning Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model. arXiv preprint arXiv:2503.24290, 2025a. Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model, 2025b. URL https://arxiv.org/abs/2503.24290. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Maxim Khanov, Jirayu Burapacheep, and Yixuan Li. Args: Alignment as reward-guided search. arXiv preprint arXiv:2402.01694, 2024. Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13:9, 2024. Xuefeng Li, Haoyang Zou, and Pengfei Liu. Limr: Less is more for rl scaling. arXiv preprint arXiv:2502.11886, 2025. Jian Liang, Ran He, and Tieniu Tan. comprehensive survey on test-time adaptation under distribution shifts. International Journal of Computer Vision, 133(1):3164, 2025. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. Runze Liu, Junqi Gao, Jian Zhao, Kaiyan Zhang, Xiu Li, Biqing Qi, Wanli Ouyang, and Bowen Zhou. Can 1b llm surpass 405b llm? rethinking compute-optimal test-time scaling. arXiv preprint arXiv:2502.06703, 2025a. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025b. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36: 4653446594, 2023. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browserassisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. Noam Razin, Zixuan Wang, Hubert Strauss, Stanley Wei, Jason Lee, and Sanjeev Arora. What makes reward model good teacher? an optimization perspective. arXiv preprint arXiv:2503.15477, 2025. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. David Silver and Richard Sutton. Welcome to the era of experience. Google AI, 2025. TTRL: Test-Time Reinforcement Learning Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize with human feedback. Advances in neural information processing systems, 33:30083021, 2020. Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Testtime training for out-of-distribution generalization. Arxiv, 2019. Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, et al. Learning to (learn at test time): Rnns with expressive hidden states. arXiv preprint arXiv:2407.04620, 2024. Richard Sutton, Andrew Barto, et al. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998. Jingkang Wang, Yang Liu, and Bo Li. Reinforcement learning with perturbed rewards. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 62026209, 2020. Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. arXiv preprint arXiv:2312.08935, 2023. Renhao Wang, Yu Sun, Arnuv Tandon, Yossi Gandelsman, Xinlei Chen, Alexei Efros, and Xiaolong Wang. Test-time training on video streams. Journal of Machine Learning Research, 26(9):129, 2025. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. Browsecomp: simple yet challenging benchmark for browsing agents. OpenAI, 2025a. Yuxiang Wei, Olivier Duchenne, Jade Copet, Quentin Carbonneaux, Lingming Zhang, Daniel Fried, Gabriel Synnaeve, Rishabh Singh, and Sida Wang. Swe-rl: Advancing llm reasoning via reinforcement learning on open software evolution. arXiv preprint arXiv:2502.18449, 2025b. Sean Welleck, Amanda Bertsch, Matthew Finlayson, Hailey Schoelkopf, Alex Xie, Graham Neubig, Ilia Kulikov, and Zaid Harchaoui. From decoding to meta-generation: Inferencetime algorithms for large language models. arXiv preprint arXiv:2406.16838, 2024. Yuxi Xie, Anirudh Goyal, Wenyue Zheng, Min-Yen Kan, Timothy Lillicrap, Kenji Kawaguchi, and Michael Shieh. Monte carlo tree search boosts reasoning via iterative preference learning. arXiv preprint arXiv:2405.00451, 2024. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. 16 TTRL: Test-Time Reinforcement Learning Lifan Yuan, Wendi Li, Huayu Chen, Ganqu Cui, Ning Ding, Kaiyan Zhang, Bowen Zhou, Zhiyuan Liu, and Hao Peng. Free process rewards without process labels. arXiv preprint arXiv:2412.01981, 2024. Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild, 2025. URL https://arxiv.org/abs/2503.18892. Kaiyan Zhang, Jiayuan Zhang, Haoxin Li, Xuekai Zhu, Ermo Hua, Xingtai Lv, Ning Ding, Biqing Qi, and Bowen Zhou. Openprm: Building open-domain process-based reward models with preference trees. In The Thirteenth International Conference on Learning Representations, 2025a. Qiyuan Zhang, Fuyuan Lyu, Zexu Sun, Lei Wang, Weixu Zhang, Zhihan Guo, Yufei Wang, Irwin King, Xue Liu, and Chen Ma. What, how, where, and how well? survey on test-time scaling in large language models. arXiv preprint arXiv:2503.24235, 2025b. Jian Zhao, Runze Liu, Kaiyan Zhang, Zhimu Zhou, Junqi Gao, Dong Li, Jiafei Lyu, Zhouyi Qian, Biqing Qi, Xiu Li, et al. Genprm: Scaling test-time compute of process reward models via generative reasoning. arXiv preprint arXiv:2504.00891, 2025. Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. Language agent tree search unifies reasoning acting and planning in language models. arXiv preprint arXiv:2310.04406, 2023. 17 TTRL: Test-Time Reinforcement Learning"
        },
        {
            "title": "A Terminology",
            "content": "Test-Time Scaling refers to increasing computational resources during test time, which can be categorized into test-time training and test-time inference. These two approaches are complementary. We will provide introduction below. Table 3: Terminology relationship. Name Category Methods Test-Time Scaling (TTS) Test-Time Training (TTT) Test-Time Inference (TTI) Test-Time Reinforcement Learning (TTRL) Majority Voting, Best-of-N A.1 Test-Time Training (TTT) Test-Time Training (TTT) is technique for adapting pre-trained model at inference time to improve generalization under distribution shifts. Let fθ denote model trained on source domain Ds = {(xi, yi)}i = 1N, where xi , yi Y, and θ represents the learned parameters. During standard inference, the model is evaluated on test samples xt Dt with fixed parameters θ, where Dt = Ds. In contrast, TTT allows the model to adapt to each test sample xt by minimizing an auxiliary self-supervised loss Laux, without access to labels yt. The model parameters are updated online with the auxiliary task, which is typically designed to be label-free and consistent with the main task. A.2 Test-Time Inference (TTI) Test-Time Inference (TTI) refers to the strategy of enhancing the performance of large language model during inference by allocating additional computational resources. Formally, let fθ denote language model with parameters θ, and let be an input prompt. The model generates an output by sampling from the conditional distribution pθ(y x). TTI techniques aim to improve the quality of by employing methods such as generating multiple candidate outputs and selecting the best one based on scoring function, or by refining the output through iterative processes (Welleck et al., 2024). One common approach involves generating candidate outputs {y1, y2, . . . , yN} and selecting the optimal output using scoring function s(y, x): = arg max yi s(yi, x) (4) The scoring function s(y, x) can be instantiated in various ways, such as: 1. Majority Voting (MV): Selecting the most frequent output among the candidates. 2. Best-of-N (BoN): Using reward models to score each candidate, then selecting the highest-scoring one. 3. Weighted BoN: Integrating MV and BoN strategies to leverage their respective strengths. 18 TTRL: Test-Time Reinforcement Learning Hyper-parameters To facilitate full reproducibility of our work, we provide comprehensive training recipe mostly based on OpenRLHF (Hu et al., 2024), using TTRL on AIME 2024 with Qwen2.5-Math-7B as an illustrative example. Table 4: Training configuration of TTRL with Qwen2.5-Math-7B. Method TTRL Backbone PPO Trainer Batch Sizes Lengths Optimizations Hyperparameters nvotes = 64 nrollout = 16 Temperature = 1.0 Advantage Estimator = group norm Qwen2.5-Math-7B Actor LR = 5 107 Critic LR = 9 106 γ = 1.0, λ = 1.0 Initial KL Coefficient = 0. train batch size = 16 rollout batch size = 16 micro train batch size = 4 micro rollout batch size = 4 Prompt Max Length = 1024 Generate Max Length = 3072 bf16, adam offload, gradient checkpointing, packing samples, flash attn Training Schedule Episodes = 60 Epochs ="
        }
    ],
    "affiliations": [
        "Shanghai AI Lab",
        "Tsinghua University"
    ]
}