{
    "paper_title": "Zero-1-to-A: Zero-Shot One Image to Animatable Head Avatars Using Video Diffusion",
    "authors": [
        "Zhou Zhenglin",
        "Ma Fan",
        "Fan Hehe",
        "Chua Tat-Seng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Animatable head avatar generation typically requires extensive data for training. To reduce the data requirements, a natural solution is to leverage existing data-free static avatar generation methods, such as pre-trained diffusion models with score distillation sampling (SDS), which align avatars with pseudo ground-truth outputs from the diffusion model. However, directly distilling 4D avatars from video diffusion often leads to over-smooth results due to spatial and temporal inconsistencies in the generated video. To address this issue, we propose Zero-1-to-A, a robust method that synthesizes a spatial and temporal consistency dataset for 4D avatar reconstruction using the video diffusion model. Specifically, Zero-1-to-A iteratively constructs video datasets and optimizes animatable avatars in a progressive manner, ensuring that avatar quality increases smoothly and consistently throughout the learning process. This progressive learning involves two stages: (1) Spatial Consistency Learning fixes expressions and learns from front-to-side views, and (2) Temporal Consistency Learning fixes views and learns from relaxed to exaggerated expressions, generating 4D avatars in a simple-to-complex manner. Extensive experiments demonstrate that Zero-1-to-A improves fidelity, animation quality, and rendering speed compared to existing diffusion-based methods, providing a solution for lifelike avatar creation. Code is publicly available at: https://github.com/ZhenglinZhou/Zero-1-to-A."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 1 5 8 5 1 . 3 0 5 2 : r Zero-1-to-A: Zero-Shot One Image to Animatable Head Avatars Using Video Diffusion Zhenglin Zhou1,2, Fan Ma2, Hehe Fan2,, Tat-Seng Chua3 1 State Key Laboratory of Brain-machine Intelligence, Zhejiang University 2 ReLER, CCAI, Zhejiang University 3 National University of Singapore {zhenglinzhou, mafan, hehefan}@zju.edu.cn dcscts@nus.edu.sg"
        },
        {
            "title": "Abstract",
            "content": "Animatable head avatar generation typically requires extensive data for training. To reduce the data requirements, natural solution is to leverage existing data-free static avatar generation methods, such as pre-trained diffusion models with score distillation sampling (SDS), which align avatars with pseudo ground-truth outputs from the diffusion model. However, directly distilling 4D avatars from video diffusion often leads to over-smooth results due to spatial and temporal inconsistencies in the generated video. To address this issue, we propose Zero-1-to-A, robust method that synthesizes spatial and temporal consistency dataset for 4D avatar reconstruction using the video diffusion model. Specifically, Zero-1-to-A iteratively constructs video datasets and optimizes animatable avatars in progressive manner, ensuring that avatar quality increases smoothly and consistently throughout the learning process. (1) SpaThis progressive learning involves two stages: tial Consistency Learning fixes expressions and learns from front-to-side views, and (2) Temporal Consistency Learning fixes views and learns from relaxed to exaggerated expressions, generating 4D avatars in simple-to-complex manner. Extensive experiments demonstrate that Zero-1-to-A improves fidelity, animation quality, and rendering speed compared to existing diffusion-based methods, providing solution for lifelike avatar creation. Code is publicly available at: https://github.com/ZhenglinZhou/Zero-1-to-A. 1. Introduction Head avatars are lifelike digital representations of humans created by computer graphics and deep learning. They can mimic human expressions, movements, and interactions, serving as speakers, assistants, characters, etc. in AR/VR, films and games. Corresponding author. Figure 1. 4D Avatar generation with SDS loss [58] and our Zero-1-to-A. Video diffusion often suffers from spatial and temporal inconsistencies. (a) The SDS loss, aligning avatar with output from video diffusion, produces over-smooth results. (b) Zero1-to-A addresses this issue by synthesizing spatially and temporally consistent datasets for avatar reconstruction. It introduces an updatable dataset to cache video diffusion results and establishes mutually beneficial relationship between avatar generation and dataset construction to further enhance consistency. Head avatar generation typically relies on large amounts of real or synthetic human data [39, 74, 78, 79, 87, 89, 92, 94], which are not easy to collect. Recently, score distillation-based methods [30, 35, 42, 45, 46, 49, 58, 70, 71, 77] have revolutionized this field by enabling the 3D/4D head avatar generation using pre-trained image diffusion models (e.g. Stable Diffusion [63]) without the need of human data. Existing methods mostly focus on textconditioned generation, which is expressive but lacks the precision and control of visual inputs [4, 23, 44, 69, 90]. Image inputs are critical for realistic 4D avatar generation but are still underexplored. Recent advances in video diffusion [6, 13, 53, 73] have made it feasible to generate dynamic head avatars by distilling from video diffusion models. However, as shown in Fig. 7, video diffusion suffers from spatial and temporal inconsistencies. Essentially, SDS aims to align the images rendered by the 3D model with pseudo ground-truth outputs generated by the diffusion model [42]. Consequently, this instability in the distillation process often leads to overFigure 2. Image to animatable avatars generation by Zero-1-to-A. Without manually annotated data, our method can distill high-fidelity 4D avatars with real-time rendering speed from pre-trained video diffusion using only one image input. smooth results, compromising the quality and fidelity of the avatars (refer to SDS Loss in Fig. 8). In this paper, we propose Zero-1-to-A, novel approach for generating 4D avatars from single image using pretrained video diffusion model. Zero-1-to-A first introduces SymGEN which incorporates an updatable dataset to cache video diffusion results, reducing inconsistencies, and establishing mutually beneficial cycle between avatar generation and dataset construction to enhance consistency. Additionally, Zero-1-to-A employs Progressive Learning strategy that advances from simple to complex scenarios. This strategy decouples video diffusion generation into: (1) Spatial Consistency Learning, fixing expressions and learning from front-to-side views, and (2) Temporal Consistency Learning, fixing the camera and learning from relaxed to exaggerated expressions. Compared to classic random learning, progressive learning generates consistent results in simple views and expressions and thus better initializes the avatar. As avatar quality improves, it further guides video diffusion to achieve greater consistency in more challenging cases, resulting in the smooth and continuous enhancement of avatar quality. We evaluate Zero-1-to-A across various head avatar generation tasks, validating our design choices through comparisons with ablated variants of our method. Additionally, we qualitatively compare Zero-1-to-A to state-of-the-art headspecialized approaches [23, 44, 90]. Comprehensive experiments show that Zero-1-to-A significantly enhances fidelity, animation quality, and rendering speed over existing diffusion-based methods, offering robust solution for lifelike avatar creation. Overall, our contributions can be summarized as follows: We present SymGEN, novel method that gradually synthesizes spatial and temporal consistency datasets for 4D avatar reconstruction using the video diffusion model. We propose Progressive Learning strategy, decoupled learning strategy with Spatial and Temporal Consistency Learning that ensures stable initialization and smooth quality enhancement. Extensive experiments show that Zero-1-to-A achieves superior fidelity, animation quality, and rendering speed in 4D avatar generation compared with baseline methods. 2. Related Work Object Generation. Recent advances in multi-modal alignment models [48, 52, 62, 82] and generative models [27, 65, 72] have led to remarkable progress in text-to-image generation [28, 57, 63]. Building on this, Neural Radiance Fields (NeRF) [3, 55] and 3D Gaussian Splatting [36] have opened new avenues for 3D-aware generation by enabling 3D scene reconstruction from only 2D multi-view images. Leveraging these techniques, recent methods integrate knowledge from text-to-image models to generate 3D content guided by text prompts [30, 35, 42, 58, 70, 71, 77, 91]. Meanwhile, the image-conditioned generation has made significant progress [45, 46, 49]. Moreover, the recent advancement of 3D generation also inspired multiple applications, including scenes generation [16, 29], 3D editing [25, 33], and avatar generation [8, 11, 32, 80]. Avatar Generation. Traditional 3D head avatar generation methods are based on statistical models such as 3DMM [5] and FLAME [40], while recent approaches leverage 3Daware Generative Adversarial Networks (GANs)[2, 9, 10, 64, 76, 86]. Advances in dynamic scene representation [7, 20, 21] have further improved animatable head avatar reconstruction. Given monocular or multi-view videos, methods like [38, 60, 79, 81, 88, 89, 94] can reconstruct photorealistic head avatars and animate them based on FLAME. However, these approaches rely heavily on large datasets of real or synthetic human data [14, 18, 39, 74, 78, 79, 87, 89, 92, 94], which are challenging to collect. Recently, diffusion-based methods have transformed this field by enabling 3D/4D head avatar generation using pre-trained image diffusion models without requiring human data. Most of these methods focus on text-conditioned generation [4, 23, 44, 69, 90], leaving image-conditioned generation for Figure 3. Framework of SymGEN. Our method simultaneously builds both the dataset and avatar from scratch through video diffusion. It establishes mutually beneficial relationship between dataset construction and avatar reconstruction, iteratively updating the synthesized dataset and training the head avatar on the updated dataset to achieve unified results. realistic 4D avatar creation is largely underexplored. 4.1. Symbiotic Generation: Dataset and Avatar 3. Preliminaries In this section, we present brief overview of the image-to4D avatar generation. The generation process can be seen as distilling knowledge from video diffusion model into learnable 4D representation. Below, we briefly introduce the animatable Gaussian head and portrait video diffusion techniques used in our method. Animatable Gaussian Head [60, 90] is 4D avatar representation that combines the animatable head model FLAME [40] with 3D Gaussian splatting (3DGS) [36] to enable high-quality texture and geometry modeling. Specifically, each 3D Gaussian point is rigged to the FLAME mesh, allowing the Gaussians to deform consistently with the mesh. As result, the 4D avatar can be effectively animated using FLAMEs pose and expression parameters and efficiently rendered via the differentiable tile rasterizer [36]. Portrait Video Diffusion. Recent methods [13, 53, 73] extend Stable Diffusion (SD) [63] for portrait video generation, creating videos where the identity matches the reference image and motion aligns with animation signals. Key modules include an appearance net for identity injection via self-attention in UNet, motion injection module for motion control using ControlNet [84], temporal attention with transformers for cross-frame consistency, and an image prompt injection module replacing CLIPs text encoder with an image encoder to adapt SD for portrait animation. 4. Method Zero-1-to-A is an image-to-4D avatar generation method. Given reference portrait, our goal is to generate an animatable head avatar using pre-trained video diffusion. The generation pipeline has two key components, including (1) the symbiotic generation in Sec. 4.1, and (2) the progressive learning strategy in Sec. 4.2. Implementation details are discussed in Sec. 4.3. We address the score distillation bottleneck with straightforward pipeline: reconstruction. Intuitively, we use video diffusion to generate portrait dataset with diverse expressions and poses, which serves as pseudo ground-truth for animatable avatar reconstruction. However, this vanilla implementation often results in poor quality (refer to Onetime Dataset Update in Fig. 8). We attribute this limitation to spatial and temporal inconsistencies introduced by video diffusion in the generated dataset. To this end, we propose Symbiotic GENeration, termed as SymGEN, building mutually beneficial relationship between dataset construction and avatar reconstruction, as illustrated in Fig. 3. By integrating head avatars into video generation, we enhance dataset quality in spatial and temporal consistency, which in turn refines the detail and fidelity of the avatars trained on this dataset. Additionally, we use iterative dataset update [25] that iteratively updates the training dataset and globally consolidates by training the head avatar on the updated dataset. Let Dn = Avatar-Driven Dataset Enhancement. {(Vi, Pi, Ei)}n i=1 be the dataset, where is the number of data. Each Vi is pseudo ground-truth video with corresponding camera sequences Pi and expression sequences Ei. Given sample (Vi, Pi, Ei), we render video from the head avatar and extract its Mediapipe facial landmark map [50]. We then encode the rendered video into latent code z0 using the VAE encoder of the video diffusion, and apply DDIM inversion [42, 56, 66, 75] to obtain its corresponding Gaussian noise zT for texture guidance. Using the facial landmark map for geometry guidance, we denoise zT to obtain refined latent code ˆz0 , which is decoded to produce the improved video ˆVi. Finally, we update the dataset by replacing Vi with ˆVi. Dataset-Refined Avatar Reconstruction. We then reconstruct the animatable head avatar on the refined synthesized dataset Dn. In specific, we train the avatar with combination of L1 and perception loss [85] LLPIPS. Following [60], we incorporate position loss Lpos and scaling loss Figure 4. Pipeline of Progressive Learning. It sequences learning from simple to complex, facilitating symbiotic generation to create consistent avatars from inconsistent video diffusion. This process divides 4D avatar generation into: (1) Spatial Consistency Learning: (2) Temporal Consistency Learning: learn from relaxed to hyperbole progressing from frontal to side views with fixed expression. expressions under fixed camera. Ls to align 3D Gaussians with FLAME, which could effectively avoid outliers. Formally, the reconstruction loss can be formulated as: = λ1L1 + λlpipsLLPIPS + λposLpos + λsLs, (1) where λ1 = 10, λlpips = 10, λpos = 0.1 and λs = 10. Iterative Mutual Enhancement. Following [25], we alternate between avatar-driven dataset enhancement and dataset-refined avatar reconstruction to achieve global consistency. Each iteration involves randomly selecting sample from dataset Dn for avatar reconstruction. Meanwhile, dataset update will be performed every = 30 iterations. 4.2. Progressive Learning: Simple to Complex chicken or the egg dilemma exists in the symbiotic generation. Initially, low-quality guidance from the avatar results in inconsistent updates to the dataset, which in turn leads to inaccurate reconstructions of the avatar, thus creating negative cycle. To break this cycle, we adopt progressive learning strategy: from simple to complex. It is inspired by the observation that video diffusion yields more consistent results under simple camera poses and relaxed expressions. Therefore, we begin with basic camera poses and expressions (e.g., smiling in frontal views) to prevent inconsistencies introduced by video diffusion. As the initial head avatar improves, its rendered videos can more effectively guide the video generation under complex expressions and diverse camera angles, thereby enhancing both the dataset and the avatar quality. Motivated by this insight, we further decouple spatial and temporal in video diffusion into two stages learning strategy, shown in Fig. 4: 1) Spatial Consistency Learning: we fix the expression, progressively learning with camera change from front view to side view. 2) Temporal Consistency Learning: we fix the camera, progressively learning with expression change from relaxed to hyperbole. Spatial Consistency Learning: Front to Side. Let Dns be the spatial dataset of length ns samples. For each sample, the fixed expression is associated with an action unit from ARKit blendshape [31], representing basic expression like eye closure and mouth opening. We then synthesize camera trajectory ˆP = {ˆpi}nf i=1 spanning nf video frames. The trajectory begins with the front view, ends with randomly sampled side view, and includes interpolations between these viewpoints. During training, we progressively update the = {pi}nf i=1 based on the pre-defined camera trajectory ˆP to gradually introduce more complex views as training iterations increase. Formally, we have: pi = ˆpmin(i,j), ds = min( + 1, nf ), (2) where represents the training iteration and ds defines the iteration interval. The variable ensures that the camera view gradually transitions across the trajectory, initially focusing on the front view and gradually incorporating more challenging views. Temporal Consistency Learning: Relax to Hyperbole. Let Dnt be the temporal dataset with nt samples. We use fixed near-frontal camera pose for each sample in Dnt to focus on temporally expression change. The temporal dataset begins with simple, relaxed expressions synthesized from [83], which are easier for the avatar to learn and help mitigate temporal inconsistencies. As training progresses, we augment the dataset with real-world expressions Ereal, sampling from talk show videos [83]. Compared to the synthesized expressions, these real-world expression sequences are more exaggerated and challenging, providing greater variability and realism. This transition is akin to test-time training [41], allowing the avatar to adapt to more challenging and realistic scenarios, thereby enhancing its robustness and real-world applicability. Figure 5. Comparison with Static Head Avatar Generation Methods. From top to bottom: Doctor Strange, Two-Face from DC, Vincent van Gogh, and Black Panther from Marvel. Guided by image prompts, our approach captures rich details and demonstrates superior performance in texture and geometry. 4.3. Implementation Details Symbiotic Generation. We use animatable Gaussian head in [60] as our 4D avatar. Following [90], we initialize each mesh triangle with 10 evenly distributed 3D Gaussians for faster convergence. Additionally, we increase the initial opacity of 3D Gaussians near the eyelid, effectively preventing undesired transparency and enabling natural eye closure. Our method is compatible with portrait video diffusion models driven by facial landmark maps [13, 53, 73]. In our default setup, we use the video diffusion model from [53] with default classifier-free guidance (CFG) [26] weight of = 3.5. Dnsyn Progressive Learning. The entire training consists of 10, 000 iterations. Initially, only the spatial dataset (Dn = Dns ) with ns = 20 samples and progressively update with ds = 1, 000 iterations. After ks = 5, 000 iterations, the temporal dataset with only synthetic expressions is added (Dn = Dns ) with nsyn = 10. After kt = 8, 000 iterations, we expand the temporal dataset by adding nreal = Dnsyn+nreal 10 samples of real-world data (Dn = Dns ). We construct the spatial subset by sampled side view with camera distance range of [8, 11], fovy range of [40, 70], an elevation range of [10, 10], and an azimuth range of [20, 160]. Furthermore, we construct the temporal subset by sampled fixed camera pose with camera distance range of [8.5, 9.5], fovy range of [50, 60], an elevation range of [10, 10], and an azimuth range of [60, 120]. Training. The framework is implemented in PyTorch and threestudio [22]. We train head avatars with resolution of 512 and batch size of nf = 8. The entire optimization takes around five hours on single NVIDIA A6000 (48GB) GPU. We use the Adam optimizer [37], with betas of [0.9, 0.99], and the learning rates of animatable Gaussian head is set following [90]. 5. Experiment In this section, we first evaluate our method on range of head avatar generation tasks, then validate our design choices through comparisons with ablated variants, and lastly analyze the limitations of our approach. Additional experimental details and results are provided in the supplementary material. 5.1. Avatar Generation As shown in Fig. 1, our method is able to generate 4D avatars with fidelity texture and high rendering speed with only one image input. Meanwhile, it can handle various portrait styles, such as realism, comic, cartoon, etc. As result, we can effectively control the avatar style, such as Joker in realism and comic style. Following [90], we evaluate the quality of head avatars with state-of-the-art diffusion-based methods in two settings. Figure 6. Comparison with Dynamic Head Avatar Generation Methods. Yellow circles highlight mouth expression artifacts. Rendering speed on the same device is shown in the black box. The FLAME mesh of the avatar is visualized bottom left. Our method excels in animation quality and rendering speed compared to prior methods. Figure 7. Comparison with Portrait Video Diffusion Methods. Symbiotic generation enhances portrait video diffusion with improved 3D consistency, temporal smoothness, and expression accuracy. In contrast, traditional portrait video diffusion shows spatial inconsistencies, noted by incorrect eye positioning in side views (green boxes), and temporal inconsistencies, highlighted by significant changes with minor facial expressions (blue boxes). Evaluation of Static Head Avatars. We compare our method with seven avatar generation methods [12, 23, 44, 54, 58, 71, 90]. Note that HeadSculpt [23], HeadArtist [44], and HeadStudio [90] are head-specialized methods. As shown in Fig. 5, our method demonstrates superior texture and geometry quality. Guided by image prompts, our approach captures rich details, such as the intricate features of Two-Face from DC Comics. Following [44, 90], we report the average CLIP score for 10 portraits in Tab. 1. Compared to state-of-the-art methods, our approach shows 0.1 and 0.05 improvement in ViT-L/14 and ViT-B/32 evaluations, respectively, highlighting its effectiveness in high-fidelity Figure 8. Ablation Study. Progressive learning is crucial for creating consistent avatars from inconsistent video diffusion, with spatial consistency improving eye and mouth for effective avatar control (green boxes), and temporal consistency enhancing model generalization to new expressions (blue boxes). Table 1. Quantitative Evaluation. Evaluating the coherence of generations with their caption using different CLIP models. CLIP-Score DreamFusion [58] LatentNeRF [54] Fantasia3D [12] ProlificDreamer [71] HeadSculpt [23] HeadArtist [44] HeadStudio [90] Ours ViT-L/14 ViT-B/16 ViT-B/32 0.302 0.299 0.304 0.320 0.306 0.318 0.322 0.320 0.300 0.303 0.300 0.308 0.305 0.313 0.317 0.322 0.244 0.248 0.267 0.268 0.264 0.272 0.275 0.285 avatar generation. Evaluation of Dynamic Head Avatars. We show the comparison with TADA [43] and HeadStudio [90] in Fig. 6. Our method shows better performance in both animation and rendering speed. Specifically, compared to HeadStudio [90], our method reduces artifacts during animation, as highlighted by the blue boxes. Additionally, our approach achieves faster rendering speeds (e.g., 76 FPS vs. 48 FPS in the Geralt). This suggests that symbiotic generation provides more stable and accurate supervision than score distillation-based loss, enabling more precise digital human modeling with fewer Gaussian points. 5.2. Ablation Study Evaluation on Challenge Cases. We evaluate Zero-1-to-A on challenging cases, including side views, closed eyes, and facial occlusions. As shown in Fig. 9, our method demonstrates its effectiveness and robustness by reconstructing full faces from side views, restoring eyes from closed ones, and generating 4D avatars unaffected by occlusions, ensuring seamless animation. The Effect of Symbiotic Generation. To demonstrate the effectiveness of symbiotic generation, we compare it with the portrait video diffusion methods [53, 73], as illustrated in Fig. 7. These baseline methods suffer from both spatial and temporal inconsistencies. As highlighted in the green boxes, there is an incorrect eye positioning in side views, leading to spatial inconsistencies. Additionally, as indicated by the blue boxes, minor changes in facial expression can lead to noticeable variations in adjacent frames. In contrast, our method leverages SymGEN to learn 4D avatar from these video diffusion methods. Compared to the baselines, our approach achieves superior 3D consistency, temporal smoothness, and expression control. The Effect of Progressive Learning. As shown in Fig. 8, we evaluate the effectiveness of each component in progressive learning. Video Diffusion + SDS Loss. Using only SDS loss [58] with video diffusion produces over-smooth avatars that lack textural detail. We attribute this to the spatial and temporal inconsistency in video diffusion, leading to subpar results, which forces us to explore other solutions for diffusionbased generation. One-time Dataset Update. In this setup, we use video diffusion to generate images with diverse expressions and poses, Figure 9. Challenge Cases. Our method exhibits robustness in effectively handling side views (left), eye closure (middle), and facial occlusions (right). Each pair shows the driving expression and animation result (right), and the top row contains reference images. serving as pseudo ground-truths for animatable avatar reconstruction. The additional dataset helps to alleviate inconsistencies; however, lacking symbiotic generation and progressive learning, the reconstructed avatar shows overly smooth textures and reduced animation quality, particularly in the eyes and mouth. w/o Progressive Learning. In this setup, we utilize the base symbiotic generation with random learning strategy, where expression and camera sequences are randomly sampled. While iterative dataset updates [25] improve convergence, the animation quality degrades due to inconsistencies introduced by video diffusion. Notably, misalignments in the eyes and teeth of the avatar result in inaccurate expression control. These results highlight the importance of progressive learning in constructing consistent avatar from inconsistent video diffusion outputs. w/o Temporal Consistency Learning. In this setup, only spatial consistency learning is applied. Experimental results show improved eye and mouth alignment. With progressive strategy, starting from frontal views to side views, the model learns reasonably accurate eye and mouth, allowing for effective avatar control. However, without temporal consistency learning, the avatar lacks generalizability for animation. For hyperbole expressions (highlighted in the blue boxes), significant artifacts appear in mouth deformation due to the absence of temporal smoothness. w/o Spatial Consistency Learning.: Applying only temporal consistency learning improves generalization and reduces artifacts in hyperbole expressions. However, misaligned eyes (highlighted in the green boxes) indicate the necessity of spatial consistency learning. These findings also suggest that incorporating challenging samples (e.g., large angles and hyperbole expressions) into temporal consistency learning, akin to test-time training [68], could further enhance generalization under distribution shifts. Figure 10. Limitation. The animatable Gaussian head [60] aligns Gaussians with the FLAME mesh, limiting the modeling of elements beyond the head. 5.3. Limitations Our method excels in texture and geometry, and maintains improved spatial-temporal consistency in facial animation. However, it encounters challenges in effectively modeling elements beyond the head. As shown in Fig. 10, it inadequately models man with an afro hairstyle. This limitation arises from the animatable Gaussian head [60, 90], which constrains Gaussians to align with the FLAME mesh, leading to the blurred afro. Additionally, edge blur may result from underfitting and labeling ambiguities. To mitigate these issues, we recommend additional rapid reconstruction on key frames and incorporating complementary representations such as Gaussian hair [51]. 6. Conclusion In this paper, we proposed Zero-1-to-A, novel method for generating high-fidelity 4D avatars from single image using pre-trained video diffusion models. Zero-1-to-A tackles spatial and temporal inconsistencies by iteratively synthesizing consistent datasets and employing Progressive Learning strategy, which ensures stable initialization and smooth quality improvement. Experiments show that Zero1-to-A outperforms existing methods in fidelity, animation quality, and rendering speed, providing robust and dataefficient solution for lifelike avatar creation."
        },
        {
            "title": "Acknowledgements",
            "content": "This work was supported in part by the National Natural Science Foundation of China (62472381), Fundamental Research Funds for the Zhejiang Provincial Universities (2262024-00208), and the Natural Science Foundation of Zhejiang Province (LDT23F02023F02)."
        },
        {
            "title": "References",
            "content": "[1] Playht. 1 [2] Sizhe An, Hongyi Xu, Yichun Shi, Guoxian Song, Umit Y. Ogras, and Linjie Luo. Panohead: Geometry-aware 3d fullhead synthesis in 360. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2095020959, 2023. 2 [3] Jonathan Barron, Ben Mildenhall, Dor Verbin, Pratul Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded In Proceedings of the anti-aliased neural radiance fields. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 54705479, 2022. 2 [4] Alexander Bergman, Wang Yifan, and Gordon Wetzstein. Articulated 3d head avatar generation using text-to-image diffusion models. arXiv preprint arXiv:2307.04859, 2023. 1, 2 [5] Volker Blanz and Thomas Vetter. morphable model for the synthesis of 3D faces. In SIGGRAPH, 1999. 2 [6] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [7] Ang Cao and Justin Johnson. Hexplane: fast representation for dynamic scenes. CVPR, 2023. 2 [8] Yukang Cao, Yan-Pei Cao, Kai Han, Ying Shan, and KwanYee Wong. Dreamavatar: Text-and-shape guided 3d human avatar generation via diffusion models. arXiv preprint arXiv:2304.00916, 2023. 2 [9] Eric Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, and Gordon Wetzstein. pi-gan: Periodic implicit generative adversarial networks for 3d-aware image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 57995809, 2021. 2 [10] Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero Karras, and Gordon Wetzstein. Efficient geometry-aware In Proceedings of 3D generative adversarial networks. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 2 [11] Ling-Hao Chen, Jiawei Zhang, Yewen Li, Yiren Pang, Xiaobo Xia, and Tongliang Liu. Humanmac: Masked motion In ICCV, pages completion for human motion prediction. 95449555, 2023. [12] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and appearance for highIn Proceedings of the quality text-to-3d content creation. IEEE/CVF International Conference on Computer Vision (ICCV), 2023. 6, 7 [13] Zhiyuan Chen, Jiajiong Cao, Zhiquan Chen, Yuming Li, and Chenguang Ma. Echomimic: Lifelike audio-driven portrait animations through editable landmark conditions. arXiv preprint arXiv:2407.08136, 2024. 1, 3, 5 [14] Xuangeng Chu and Tatsuya Harada. Generalizable and animatable gaussian head avatar. In NeurIPS, 2024. 2 [15] Xuangeng Chu, Yu Li, Ailing Zeng, Tianyu Yang, Lijian Lin, Yunfei Liu, and Tatsuya Harada. GPAvatar: Generalizable and precise head avatar from image(s). In ICLR, 2024. 1 [16] Dana Cohen-Bar, Elad Richardson, Gal Metzer, Raja Giryes, and Daniel Cohen-Or. Set-the-scene: Global-local training for generating controllable nerf scenes. arXiv preprint arXiv:2303.13450, 2023. [17] Radek Danecek, Michael J. Black, and Timo Bolkart. EMOCA: Emotion driven monocular face capture and anIn Conference on Computer Vision and Pattern imation. Recognition (CVPR), pages 2031120322, 2022. 1 [18] Yu Deng, Duomin Wang, Xiaohang Ren, Xingyu Chen, and Baoyuan Wang. Portrait4d: Learning one-shot 4d head avatar synthesis using synthetic data. In CVPR, pages 7119 7130, 2024. 2 [19] Yao Feng, Haiwen Feng, Michael J. Black, and Timo Bolkart. Learning an animatable detailed 3D face model from in-the-wild images. ACM Transactions on Graphics, (Proc. SIGGRAPH), 40(8), 2021. 1 [20] Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahbæk Warburg, Benjamin Recht, and Angjoo Kanazawa. K-planes: Explicit radiance fields in space, time, and appearance. In CVPR, 2023. 2 [21] Chen Gao, Ayush Saraf, Johannes Kopf, and Jia-Bin Huang. Dynamic view synthesis from dynamic monocular video. In Proceedings of the IEEE International Conference on Computer Vision, 2021. 2 [22] Yuan-Chen Guo, Ying-Tian Liu, Ruizhi Shao, Christian Laforte, Vikram Voleti, Guan Luo, Chia-Hao Chen, ZiXin Zou, Chen Wang, Yan-Pei Cao, and Song-Hai Zhang. threestudio: unified framework for 3d content generation. https://github.com/threestudioproject/ threestudio, 2023. [23] Xiao Han, Yukang Cao, Kai Han, Xiatian Zhu, Jiankang Deng, Yi-Zhe Song, Tao Xiang, and Kwan-Yee Wong. arXiv Headsculpt: Crafting 3d head avatars with text. preprint arXiv:2306.03038, 2023. 1, 2, 6, 7 [24] Jinkun Hao, Junshu Tang, Jiangning Zhang, Ran Yi, Yijia Hong, Moran Li, Weijian Cao, Yating Wang, and Lizhuang Ma. Portrait3d: 3d head generation from single in-the-wild portrait image. arXiv preprint arXiv:2406.16710, 2024. 3, 4 [25] Ayaan Haque, Matthew Tancik, Alexei Efros, Aleksander Holynski, and Angjoo Kanazawa. Instruct-nerf2nerf: EditIn Proceedings of the ing 3d scenes with instructions. IEEE/CVF International Conference on Computer Vision (ICCV), 2023. 2, 3, 4, 8, 1 [26] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 5 [27] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems (NeurIPS), 33:68406851, 2020. 2 [28] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Imagen Poole, Mohammad Norouzi, David Fleet, et al. video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. 2 [29] Lukas Hollein, Ang Cao, Andrew Owens, Justin Johnson, Text2room: Extracting textured and Matthias Nießner. 3d meshes from 2d text-to-image models. arXiv preprint arXiv:2303.11989, 2023. 2 [30] Susung Hong, Donghoon Ahn, and Seungryong Kim. Debiasing scores and prompts of 2d diffusion for robust text-to-3d generation. arXiv preprint arXiv:2303.15413, 2023. 1, [31] Apple Inc. Arkit blendshapes. 4 [32] Ruixiang Jiang, Can Wang, Jingbo Zhang, Menglei Chai, Mingming He, Dongdong Chen, and Jing Liao. Avatarcraft: Transforming text into neural human avatars with arXiv preprint parameterized shape and pose control. arXiv:2303.17606, 2023. 2 [33] Hiromichi Kamata, Yuiko Sakuma, Akio Hayakawa, Masato Instruct 3d-to-3d: Text inarXiv preprint Ishii, and Takuya Narihira. struction guided 3d-to-3d conversion. arXiv:2303.15780, 2023. 2 [34] Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 44014410, 2019. 4 [35] Oren Katzir, Or Patashnik, Daniel Cohen-Or, and Dani arXiv preprint Lischinski. Noise-free score distillation. arXiv:2310.17590, 2023. 1, [36] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42 (4), 2023. 2, 3 [37] Diederik Kingma and Jimmy Ba. Adam: method for arXiv preprint arXiv:1412.6980, stochastic optimization. 2014. 5 [38] Tobias Kirschstein, Simon Giebenhain, and Matthias Nießner. Diffusionavatars: Deferred diffusion for highfidelity 3d head avatars. arXiv preprint arXiv:2311.18635, 2023. 2 [39] Tobias Kirschstein, Shenhan Qian, Simon Giebenhain, Tim Walter, and Matthias Nießner. Nersemble: Multi-view radiance field reconstruction of human heads. ACM Trans. Graph., 42(4), 2023. 1, [40] Tianye Li, Timo Bolkart, Michael Black, Hao Li, and Javier Romero. Learning model of facial shape and expression from 4d scans. ACM Trans. Graph., 36(6):1941, 2017. 2, 3, 1 [41] Jian Liang, Ran He, and Tieniu Tan. comprehensive survey on test-time adaptation under distribution shifts. IJCV, 133 (1):3164, 2025. 4 [42] Yixun Liang, Xin Yang, Jiantao Lin, Haodong Li, Xiaogang Xu, and Yingcong Chen. Luciddreamer: Towards highfidelity text-to-3d generation via interval score matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 65176526, 2024. 1, 2, 3 [43] Tingting Liao, Hongwei Yi, Yuliang Xiu, Jiaxaing Tang, Black. arXiv preprint Justus Thies, text to animatable digital avatars. and Michael Yangyi Huang, Tada! arXiv:2308.10899, 2023. 7 [44] Hongyu Liu, Xuan Wang, Ziyu Wan, Yujun Shen, Yibing Song, Jing Liao, and Qifeng Chen. Headartist: Textconditioned 3d head generation with self score distillation. arXiv preprint arXiv:2312.07539, 2023. 1, 2, 6, 7, 4 [45] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Mukund Varma T, Zexiang Xu, and Hao Su. One-2-3-45: Any single image to 3d mesh in 45 seconds without per-shape optimization. Advances in Neural Information Processing Systems, 36, 2024. 1, 2 [46] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-toIn Proceedings of 3: Zero-shot one image to 3d object. the IEEE/CVF international conference on computer vision, pages 92989309, 2023. 1, 2, 3 [47] Shuaicheng Liu, Lu Yuan, Ping Tan, and Jian Sun. Bundled camera paths for video stabilization. TOG, 32(4):110, 2013. [48] Xiaohao Liu, Xiaobo Xia, Zhuo Huang, and Tat-Seng Chua. Towards modality generalization: benchmark and prospective analysis. arXiv preprint arXiv:2412.18277, 2024. 2 [49] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3d: Single image to 3d using cross-domain diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 99709980, 2024. 1, 2, 3 [50] Camillo Lugaresi, Jiuqiang Tang, Hadon Nash, Chris McClanahan, Esha Uboweja, Michael Hays, Fan Zhang, ChuoLing Chang, Ming Guang Yong, Juhyun Lee, et al. Mediapipe: framework for building perception pipelines. arXiv preprint arXiv:1906.08172, 2019. 3 [51] Haimin Luo, Min Ouyang, Zijun Zhao, Suyi Jiang, Longwen Zhang, Qixuan Zhang, Wei Yang, Lan Xu, and Jingyi Yu. Gaussianhair: Hair modeling and rendering with light-aware gaussians. arXiv preprint arXiv:2402.10483, 2024. 8 [52] Run Luo, Haonan Zhang, Longze Chen, Ting-En Lin, Xiong Liu, Yuchuan Wu, Min Yang, Minzheng Wang, Pengpeng Zeng, Lianli Gao, et al. Mmevol: Empowering multimodal large language models with evol-instruct. arXiv preprint arXiv:2409.05840, 2024. 2 [53] Yue Ma, Hongyu Liu, Hongfa Wang, Heng Pan, Yingqing He, Junkun Yuan, Ailing Zeng, Chengfei Cai, Heung-Yeung Shum, Wei Liu, et al. Follow-your-emoji: Fine-controllable and expressive freestyle portrait animation. arXiv preprint arXiv:2406.01900, 2024. 1, 3, 5, 7, 2, 4 [54] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. Latent-nerf for shape-guided generation of 3d shapes and textures. arXiv preprint arXiv:2211.07600, 2022. 6, 7 [55] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In Proceedings of the European Conference on Computer Vision (ECCV), 2020. [56] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real imIn Proceedings of ages using guided diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 60386047, 2023. 3 [57] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. 2 [58] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. 1, 2, 6, 7 [59] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren, Aliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Skorokhodov, Peter Wonka, Sergey Tulyakov, et al. Magic123: One image to high-quality 3d object generation using both 2d and 3d diffusion priors. arXiv preprint arXiv:2306.17843, 2023. 3 [60] Shenhan Qian, Tobias Kirschstein, Liam Schoneveld, Davide Davoli, Simon Giebenhain, and Matthias Nießner. Gaussianavatars: Photorealistic head avatars with rigged 3d gaussians. arXiv preprint arXiv:2312.02069, 2023. 2, 3, 5, 8 [61] Xuebin Qin, Zichen Zhang, Chenyang Huang, Masood Dehghan, Osmar Zaiane, and Martin Jagersand. U2-net: Going deeper with nested u-structure for salient object detection. Pattern Recognition, 106:107404, 2020. 1 [62] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language superIn Proceedings of the International Conference on vision. Machine Learning (ICML), pages 87488763, 2021. 2 [63] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1068410695, 2022. 1, 2, 3 [64] Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas Geiger. Graf: Generative radiance fields for 3d-aware image synthesis. Advances in Neural Information Processing Systems, 33:2015420166, 2020. [65] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using In International Confernonequilibrium thermodynamics. ence on Machine Learning, pages 22562265. PMLR, 2015. 2 [66] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. 3 and Stefano Ermon. arXiv preprint [67] Jingxiang Sun, Bo Zhang, Ruizhi Shao, Lizhen Wang, Wen Liu, Zhenda Xie, and Yebin Liu. Dreamcraft3d: Hierarchical 3d generation with bootstrapped diffusion prior. arXiv preprint arXiv:2310.16818, 2023. [68] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Test-time training with selfIn Efros, and Moritz Hardt. supervision for generalization under distribution shifts. ICML, pages 92299248. PMLR, 2020. 8 [69] Duotun Wang, Hengyu Meng, Zeyu Cai, Zhijing Shao, Qianxi Liu, Lin Wang, Mingming Fan, Ying Shan, Xiaohang Zhan, and Zeyu Wang. Headevolver: Text to head avatars arXiv preprint via locally learnable mesh deformation. arXiv:2403.09326, 2024. 1, 2 [70] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A. Yeh, and Greg Shakhnarovich. Score jacobian chaining: Lifting In Propretrained 2d diffusion models for 3d generation. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 1, 2 [71] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. arXiv preprint arXiv:2305.16213, 2023. 1, 2, 6, 7 [72] Zhaoqing Wang, Xiaobo Xia, Runnan Chen, Dongdong Yu, Changhu Wang, Mingming Gong, and Tongliang Liu. LavinarXiv preprint dit: Large vision diffusion transformer. arXiv:2411.11505, 2024. [73] Huawei Wei, Zejun Yang, and Zhisheng Wang. Aniportrait: Audio-driven synthesis of photorealistic portrait animations, 2024. 1, 3, 5, 7, 2, 4 [74] Erroll Wood, Tadas Baltruˇsaitis, Charlie Hewitt, Sebastian Dziadzio, Thomas Cashman, and Jamie Shotton. Fake it till you make it: face analysis in the wild using synthetic data alone. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 36813691, 2021. 1, 2 [75] Jing Wu, Jia-Wang Bian, Xinghui Li, Guangrun Wang, Ian Reid, Philip Torr, and Victor Prisacariu. GaussCtrl: MultiView Consistent Text-Driven 3D Gaussian Splatting Editing. ECCV, 2024. 3 [76] Yiqian Wu, Hao Xu, Xiangjun Tang, Xien Chen, Siyun Tang, Zhebin Zhang, Chen Li, and Xiaogang Jin. Portrait3d: Textguided high-quality 3d portrait generation using pyramid representation and gans prior. ACM Trans. Graph., 43(4), 2024. 2 [77] Zike Wu, Pan Zhou, Xuanyu Yi, Xiaoding Yuan, and Hanwang Zhang. Consistent3d: Towards consistent high-fidelity text-to-3d generation with deterministic sampling prior. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 98929902, 2024. 1, 2 [78] Liangbin Xie, Xintao Wang, Honglun Zhang, Chao Dong, and Ying Shan. Vfhq: high-quality dataset and benchIn The IEEE Conmark for video face super-resolution. ference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2022. 1, [79] Yuelang Xu, Lizhen Wang, Xiaochen Zhao, Hongwen Zhang, and Yebin Liu. Avatarmav: Fast 3d head avatar In ACM reconstruction using motion-aware neural voxels. SIGGRAPH 2023 Conference Proceedings, 2023. 1, 2 [93] Wojciech Zielonka, Timo Bolkart, and Justus Thies. Towards metrical reconstruction of human faces. In European Conference on Computer Vision, 2022. 1 [94] Wojciech Zielonka, Timo Bolkart, and Justus Thies. Instant In Proceedings of the IEEE/CVF volumetric head avatars. Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 1, 2 [80] Yuanyou Xu, Zongxin Yang, and Yi Yang. Seeavatar: Photorealistic text-to-3d avatar generation with constrained geometry and appearance. arXiv preprint arXiv:2312.08889, 2023. 2 [81] Yichao Yan, Zanwei Zhou, Zi Wang, Jingnan Gao, and Xiaokang Yang. Dialoguenerf: Towards realistic avatar faceto-face conversation video generation. Visual Intelligence, 2 (1):24, 2024. 2 [82] Yi Yang, Yueting Zhuang, and Yunhe Pan. Multiple knowledge representation for big data artificial intelligence: framework, applications, and case studies. Frontiers of Information Technology & Electronic Engineering, 22(12):1551 1558, 2021. [83] Hongwei Yi, Hualin Liang, Yifei Liu, Qiong Cao, Yandong Wen, Timo Bolkart, Dacheng Tao, and Michael Black. Generating holistic 3d human motion from speech. In CVPR, 2023. 4, 1 [84] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023. 3 [85] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of In Proceedings of deep features as perceptual metric. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 3 [86] Xuanmeng Zhang, Zhedong Zheng, Daiheng Gao, Bang Zhang, Yi Yang, and Tat-Seng Chua. Multi-view consistent generative adversarial networks for compositional 3d-aware image synthesis. International Journal of Computer Vision, 131(8):22192242, 2023. 2 [87] Zhimeng Zhang, Lincheng Li, Yu Ding, and Changjie Fan. Flow-guided one-shot talking face generation with In Proceedings of high-resolution audio-visual dataset. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 36613670, 2021. 1, 2 [88] Yufeng Zheng, Victoria Fernandez Abrevaya, Marcel C. Buhler, Xu Chen, Michael J. Black, and Otmar Hilliges. Avatar: Implicit morphable head avatars from videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [89] Yufeng Zheng, Wang Yifan, Gordon Wetzstein, Michael J. Black, and Otmar Hilliges. Pointavatar: Deformable pointIn Proceedings of the based head avatars from videos. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 1, 2 [90] Zhenglin Zhou, Fan Ma, Hehe Fan, Zongxin Yang, and Yi Yang. Headstudio: Text to animatable head avatars with 3d gaussian splatting. In ECCV, 2024. 1, 2, 3, 5, 6, 7, 8, 4 [91] Zhenglin Zhou, Xiaobo Xia, Fan Ma, Hehe Fan, Yi Yang, and Tat-Seng Chua. Dreamdpo: Aligning text-to-3d generation with human preferences via direct preference optimization. arXiv preprint arXiv:2502.04370, 2025. 2 [92] Hao Zhu, Wayne Wu, Wentao Zhu, Liming Jiang, Siwei Tang, Li Zhang, Ziwei Liu, and Chen Change Loy. CelebVHQ: large-scale video facial attributes dataset. In ECCV, 2022. 1, 2 Zero-1-to-A: Zero-Shot One Image to Animatable Head Avatars Using Video Diffusion"
        },
        {
            "title": "Supplementary Material",
            "content": "7. Introduction In this supplementary material, we provide additional details and insights into the work presented in the paper. Sec. 8 details the image preprocessing steps and explores potential applications for generating avatars. Sec. 9 discusses related solutions, highlighting their limitations, the differences from our approach, and the advantages these differences bring. Sec. 10 presents comprehensive experimental results, including qualitative comparisons with image-to-3D methods, quantitative evaluations against video diffusion models, and various ablation studies. Furthermore, we visualize the spatial and temporal inconsistencies in video diffusion models and demonstrate the improvements introduced by our method (Sec. 9.2). In the ablation study, we evaluate the adaptability of our method to different video diffusion models and its applicability to in-the-wild datasets. 8. Additional Implementation Details Pre-process. Based on [15, 17, 61], the preprocessor removes the background and estimates the pose and FLAME parameters of input portrait. As shown in Fig. 11, the preprocess of each image contains two steps: 1) Background Removal: Given portrait image, we first use Rembg [61] to remove the background and only retain the foreground portrait; 2) Pose Estimation: Then, we use MICA [93] and EMOCA [17] to estimate the FLAME shape, expression In training, we use the and pose parameters of portrait. carved image as the input of video diffusion and initialize the learnable shape parameter with estimated FLAME shape. Application. Once optimized, the parameters of the animatable Gaussian head are fixed, enabling real-time animation and rendering of the avatar using motion and camera sequences. These motion sequences incorporate expression and pose parameters from FLAME 2020 [40]. Following HeadStudio [90], we employ advanced models such as face-to-FLAME [17, 19, 93], speech-to-FLAME [83], and text-to-speech [1] to convert video, speech, and text into FLAME animation inputs. As result, the generated avatar supports multimodal control, demonstrating practical applications in real-world scenarios. Figure 11. Pre-process. Given portrait image, we first remove its background and then estimate the FLAME parameter. Figure 12. 2D Image Generation with SDS-based Loss. From left to right: reference image, SDS [58], ISM [42], NFSD [35] and video diffusion generation [73]. 9. Discussion 9.1. Discussion with Related Solutions DreamFusion v.s. Zero-1-to-A. In Fig. 12, we compare 2D image generation results using SDS [58], ISM [42], and NFSD [35]. Notably, NFSD employs negative prompts to suppress unwanted noise in the diffusion score. We implement this by treating reference image with data augmentation (e.g., blur, brightness adjustment, Gaussian noise) as negative prompts. Compared to video diffusion generation, results from SDS-based loss exhibit issues of oversmoothing and over-saturation. We attribute this to additional temporal modules introduced in portrait video diffusion, which may adversely affect score distillation. This limitation motivates us to explore alternative solutions. Instruct-NeRF2NeRF (IN2N) Zero-1-to-A. IN2N [25] introduces 3D editing method called iterative dataset update, which alternates between editing the ground-truth dataset and optimizing the 3D scene. In contrast, Zero-1-to-A is 4D generation method that progressively builds pseudo ground-truth dataset while optimizing the 4D avatar. Different from the editing task, the lack of consistent input in the generation task creates negative cycle in iterative dataset update, leading to incorrect convergence (e.g., misaligned eyes and inability to open the mouth, as shown in the fifth column of Fig. 8). To v.s. Figure 13. Visualization of Spatial and Temporal Inconsistencies in Video Diffusion Models. Portrait video diffusion exhibits spatial inconsistencies, such as incorrect eye positioning in side views (green boxes), and temporal inconsistencies, evident in significant changes triggered by minor facial expressions (blue boxes). Table 2. Quantitative Evaluation of Avatar Animation. We temporal smoothness, and rendering evaluate ID consistency, speed, demonstrating that our method is able to enhance the performance of portrait video diffusion. Face Animation AniPortrait [73] Follow-Your-Emoji [53] Ours (w. [53]) ID 0.5081 0.4988 0.5000 Motion 0.8410 0.8934 0.9187 Speed 0.52 FPS 0.56 FPS 71 FPS address this, we propose simple-to-complex progressive learning strategy that breaks this cycle and significantly improves generation performance. 9.2. Discussion on the Motivation In Fig. 13, we show the spatial and temporal inconsistencies in video diffusion models and demonstrate the improvements achieved by our method. On the left of Fig. 13, spatial inconsistencies are shown by fixing the expression and varying the camera pose. Ideally, the portraits expression should remain unchanged. However, as the camera pose shifts, the iris incorrectly looks left, and teeth that were initially absent appear (highlighted in green boxes). On the right, temporal inconsistencies are illustrated by fixing the camera pose and varying the expression. Ideally, the portrait should deform smoothly and accurately. Instead, even with minor changes, such as gradually opening the mouth, the generated video exhibits abrupt and incorrect variations (highlighted in blue boxes). With SymGEN, we achieve improvements in video generation under large pose changes and exaggerated expressions, resulting in spatially and temporally consistent pseudo-ground truth dataset. In summary, video diffusion models [53, 73] suffer from Figure 14. Comparisons with Image-to-3D Methods. Our method delivers comparable performance in texture reconstruction while achieving superior 3D consistency. Figure 15. Comparisons with Portrait3D [24]. Our method matches the performance of Portrait3D while providing animatable avatars, enabling wider range of applications. severe spatial and temporal inconsistencies, making them unsuitable for direct 4D avatar reconstruction. Our proposed SymGEN framework iteratively constructs consistent dataset, enabling the reconstruction of 4D avatars. 10. Additional Experiments 10.1. Comparisons with Image-to-3D Methods Figure 16. Evaluation on Different Video Diffusion Models. Our method demonstrates its effectiveness by seamlessly adapting to various video diffusion models. Magic123 [59], DreamCraft3D [67], and Wonder3D [49]. We reproduce Zero-1-to-3, Magic123, and DreamCraft3D using threestudio and implement Wonder3D with NeuS following the official guidelines. The results show that In Fig. 14, we compare our method with diffusionbased image-to-3D approaches, including Zero-1-to-3 [46], https://github.com/threestudio-project/threestudio https://github.com/xxlong0/Wonder3D our Zero-1-to-A delivers comparable texture fidelity and superior geometry reconstruction, leveraging head prior model. Comparisons with Portrait3D. Portrait3D [24] is diffusion-based image-to-avatar method. However, as the code is not yet open-sourced, we could not reproduce its results for the avatar generation benchmark [44, 90]. In Fig. 15, we compare our method with Portrait3D using results captured from its official project. Our method achieves comparable performance while offering animatable avatars, enabling broader applications than Portrait3D. 10.2. Comparisons with Video Diffusion Methods. In Tab. 2, we quantitatively evaluate ID consistency, temporal smoothness, and rendering speed. ID consistency (ID) is measured using cosine similarity of identity embeddings, while temporal smoothness (Motion) is evaluated using stability score based on frequency analysis of estimated 2D motion. Higher low-frequency energy indicates greater video stability (details in [47]). The evaluation is conducted on 18 samples and 300 frames from real-world portrait videos [83]. Our method demonstrates improved ID consistency, temporal smoothness, and rendering speed, highlighting the effectiveness of Zero-1-to-A. 10.3. Additional Ablations Evaluation on Different Video Diffusion Models. In Fig. 16, we compare results using different video diffusion models (AniPortrait [73] and Follow-Your-Emoji [53]). Notably, using AniPortrait achieves better color fidelity to the reference image than using Follow-Your-Emoji. Our method adapts seamlessly to various video diffusion models, effectively generating animatable avatars and demonstrating robust performance. Evaluation on in-the-wild Cases. We further evaluate our method on wide range of in-the-wild cases. Specifically, we sampled multiple portraits from the FFHQ dataset [34], with results shown in Fig. 17. The results demonstrate that our approach is broadly applicable across genders and diverse ethnic groups. https://jinkun-hao.github.io/Portrait3D/ Figure 17. Evaluation on in-the-wild Cases."
        }
    ],
    "affiliations": [
        "National University of Singapore",
        "ReLER, CCAI, Zhejiang University",
        "State Key Laboratory of Brain-machine Intelligence, Zhejiang University"
    ]
}