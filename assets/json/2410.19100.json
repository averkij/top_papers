{
    "paper_title": "VideoWebArena: Evaluating Long Context Multimodal Agents with Video Understanding Web Tasks",
    "authors": [
        "Lawrence Jang",
        "Yinheng Li",
        "Charles Ding",
        "Justin Lin",
        "Paul Pu Liang",
        "Dan Zhao",
        "Rogerio Bonatti",
        "Kazuhito Koishida"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Videos are often used to learn or extract the necessary information to complete tasks in ways different than what text and static imagery alone can provide. However, many existing agent benchmarks neglect long-context video understanding, instead focusing on text or static image inputs. To bridge this gap, we introduce VideoWebArena (VideoWA), a benchmark for evaluating the capabilities of long-context multimodal agents for video understanding. VideoWA consists of 2,021 web agent tasks based on manually crafted video tutorials, which total almost four hours of content. For our benchmark, we define a taxonomy of long-context video-based agent tasks with two main areas of focus: skill retention and factual retention. While skill retention tasks evaluate whether an agent can use a given human demonstration to complete a task efficiently, the factual retention task evaluates whether an agent can retrieve instruction-relevant information from a video to complete a task. We find that the best model achieves 13.3% success on factual retention tasks and 45.8% on factual retention QA pairs, far below human performance at 73.9% and 79.3%, respectively. On skill retention tasks, long-context models perform worse with tutorials than without, exhibiting a 5% performance decrease in WebArena tasks and a 10.3% decrease in VisualWebArena tasks. Our work highlights the need to improve the agentic abilities of long-context multimodal models and provides a testbed for future development with long-context video agents."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 4 2 ] . [ 1 0 0 1 9 1 . 0 1 4 2 : r Preprint. VIDEOWEBARENA: EVALUATING LONG CONTEXT MULTIMODAL AGENTS WITH VIDEO UNDERSTANDING WEB TASKS Lawrence Jang14, Yinheng Li4, Charles Ding1, Justin Lin1, Paul Pu Liang2, Dan Zhao234, Rogerio Bonatti4, Kazuhito Koishida4 1Carnegie Mellon University, 2Massachusetts Institute of Technology, 3New York University, 4Microsoft"
        },
        {
            "title": "ABSTRACT",
            "content": "Videos are often used to learn or extract the necessary information to complete tasks in ways different than what text and static imagery alone can provide. However, many existing agent benchmarks neglect long-context video understanding, instead focusing on text or static image inputs. To bridge this gap, we introduce VideoWebArena (VideoWA), benchmark for evaluating the capabilities of long-context multimodal agents for video understanding. VideoWA consists of 2,021 web agent tasks based on manually crafted video tutorials, which total almost four hours of content. For our benchmark, we define taxonomy of long-context video-based agent tasks with two main areas of focus: skill retention and factual retention. While skill retention tasks evaluate whether an agent can use given human demonstration to complete task efficiently, the factual retention task evaluates whether an agent can retrieve instruction-relevant information from video to complete task. We find that the best model achieves 13.3% success on factual retention tasks and 45.8% on factual retention QA pairs, far below human performance at 73.9% and 79.3%, respectively. On skill retention tasks, long-context models perform worse with tutorials than without, exhibiting 5% performance decrease in WebArena tasks and 10.3% decrease in VisualWebArena tasks. Our work highlights the need to improve the agentic abilities of long-context multimodal models and provides testbed for future development with long-context video agents."
        },
        {
            "title": "INTRODUCTION",
            "content": "Humans often use videos to complete daily tasks, whether to learn from tutorials or retrieve information from within one or several videos. As we build AI assistants, these multimodal agents must also possess similar capabilities to understand and process videos to accomplish tasks or learn how to accomplish workflow, plan, and make decisions. While videos can provide rich source of information, capturing spatial and temporal dynamics that images or text alone may not convey, integrating video input into multimodal models introduces unique challenges relating to temporal coherence, context retention, or efficient information retrieval over lengthy, extended sequences. These challenges can be further compounded when models are deployed as autonomous agents operating in complex environments. In these scenarios, the ability of models to maintain long-term memory, perform informational retrieval, and adapt to new information continuously is critical for tasks that require sustained engagement over time. Recent improvements in long-context understanding of large video-capable vision language models (e.g., LLaVaNeXt, LongVILA) have enabled agents to process and understand more information than before, including long video understanding. However, from an evaluative perspective, there remains significant gap in existing benchmarks that can comprehensively evaluate the agenticn capabilities of these models across diverse multimodal scenarios, particularly those involving video inputs. The requirement for agents to operate across varying modalities and time frames makes developing and properly evaluating long-context multimodal models essential. Existing benchmarks (Wu et al., 2024; 1 Preprint. Mangalam et al., 2023; Li et al., 2024; Patraucean et al., 2023) often fall short in testing for long-term memory retention and multimodal integration within an agent workflow, as they usually focus on only one component. This limits our understanding of how long context multimodal models can generalize and perform in real-world settings as agents. Figure 1: Overview of VideoWebArena. VideoWebArena is visually grounded benchmark that tests the video understanding of agentic models across various realistic domains and environments, mirroring real-life tasks. All tasks require video input and consist of Q/A to test agentic abilities in video information retrieval, video understanding, and more. Contributions. To address this gap, we present VideoWebArena, novel, open-source video-based benchmark that evaluates multimodal models agentic ability to process, understand, and utilize long-context video inputs to accomplish various tasks. Our contributions can be summarized as follows. We present VideoWebArenaa benchmark focusing specifically on evaluating models ability to process long video sequences alongside text and images to complete tasks that require memory retention, information retrieval, multimodal reasoning, and skill retention. VideoWebArena consists of 2,021 tasks and details approximately 4 hours of video content. Of these 2,021 tasks, VideoWebArena consists of 400 factual retention tasks, which test agents abilities to retrieve information from video to perform tasks, and 1,621 skill retention tasks, which test agents abilities to use tutorials in-context to perform tasks more efficiently. We evaluate several video-capable state-of-the-art LLMs, namely GPT-4o and Gemini 1.5 Pro, on our benchmark, providing an overview of these models current long-context video understanding capabilities. Our results show that while these models can serve in limited capacity as video-capable agents, these models are still far reach from human levels of performance, highlighting wide gap in the information retrieval and agentic abilities of current state-of-the-art long-context models. Our code, benchmark, and relevant documentation are available at videowebarena.github.io."
        },
        {
            "title": "2 BACKGROUND",
            "content": "Large Vision Language Models. Large vision language models (VLMs) have been popular study subjects for incorporating video input and can be characterized by their learning mechanisms or overall architectural design. Popular state-of-the-art (SOTA) models like the GPT-4 (OpenAI, 2024) family of models, Claude (AI Anthropic, 2024), and Gemini (Google, 2023) are now able to handle not just text but also visual and even audio input. Generally, similar to LLMs, VLM architectures typically revolve around two typesmodels with either joint encoder-decoder architecture such as LLaVA and its variations (Liu et al., 2023) or decoder-only architecture. Encoder-decoder VLMs tend to project different modalities through shallow neural network or fully connected layer to link modalities. Decoder-style models typically rely on decoder-only LLM that processes raw inputs (e.g., text tokens, image patches, etc.) such as 2 Preprint. VILA (Lin et al., 2023) along and its variants such as VILA2 (Fang et al., 2024) and X-VILA (Ye et al., 2024). As multimodal understanding with long-context capability becomes more important in processing more input information like video data, models like LongVILA (Xue et al., 2024) can now process much larger number of video frames in its input for longer videos. Agents & Tools. Recent works have turned to adapting large multimodal models like VLMs into autonomous agents, given their capabilities in visual question answering and high-level reasoning. Many approaches have been further developed to improve or supplement the agentic capabilities of these large models, ranging from data collection and synthesis for specific kinds of training (Lai et al., 2024; Patel et al., 2024) or for use at inference time (Pan et al., 2024; Fu et al., 2024; Koh et al., 2024b; Sarch et al., 2024; Wang et al., 2024). Similar works have shown how certain fine-tuning (Furuta et al., 2024) or prompting techniques (e.g., with Set-of-Marks (Yang et al., 2023), (Chi et al., 2024)) can improve performance in navigating settings like web pages to accomplish tasks. In contrast, others attempt to improve how agents themselves can dynamically compose/search for policies (Sodhi et al., 2024). Agent Benchmarks. As more works adapt LLMs and VLMs into agents, other works have focused on evaluating such efforts. These benchmarks can range across variety of settings: from general web browsing (Yao et al., 2022; Deng et al., 2023; Zhou et al., 2024; Koh et al., 2024a), where agents are evaluated on their abilities to navigate the web and accomplish specific tasks, to mobile environments, where agents are expected to perform tasks within mobile OS simulation like Android (Zhang et al., 2024a; Rawles et al., 2024). Other general environments attempt to emulate an OS or computing environment like MMInA (Zhang et al., 2024b), OSWorld (Xie et al., 2024), and Windows Agent Arena (Bonatti et al., 2024) where agents must navigate across multiple computer applications online and offline. Custom environments like WorkArena (Drouin et al., 2024) instead target more specific platforms like ServiceNow in constructing tasks for agent evaluation. Long-Context Video Benchmarks. For large multimodal models, long-context capabilities are essential for detailed planning, action, and understanding, especially for the video modality. There have been multiple benchmarks dedicated towards video understanding, with shorter video inputs (Li et al., 2024), around few minutes, and longer video inputs (Wu et al., 2024; Patraucean et al., 2023), up to over an hour. Video understanding benchmarks dedicated to temporal reasoning and thematic reasoning (Xiao et al., 2021; Tapaswi et al., 2016; Lei et al., 2019) also exist. Many of these benchmarks cover subsets and define categories of video understanding tasks related to spatial reasoning, causal reasoning, and temporal reasoning."
        },
        {
            "title": "3.1 SUMMARY & OVERVIEW",
            "content": "VideoWA centers around six key thematic environments created by VisualWebArena (Koh et al., 2024a) and WebArena (Zhou et al., 2024): Reddit, Classifieds, Shopping, Shopping Admin, Map, and Gitlab. Tables 1 and 2 for finer characterization of the tasks and videos within the benchmark. These domains websites are locally hosted since the docker images for each website are publicly available online. There is an Amazon Machine Image and instructions dedicated to hosting these websites on an EC2 instance; we refer readers to the codebase for further information. By doing this, we can make our benchmark realistic but reproducible, leveraging data and code from real and popular websites on the internet. We refer readers to WebArena (Zhou et al., 2024) and VisualWebArena (Koh et al., 2024a) for more information on each site and their setup."
        },
        {
            "title": "3.2.1 FRAMEWORK",
            "content": "We can define an agents trajectory on our tasks as partially observable Markov decision process (POMDP) (S, O, A, T, R) with state space S, observation space O, action space containing actions a, transition function : S, and reward function : R. Given current 3 Preprint. Variable # Videos Total Duration Min Duration Max Duration Avg. Duration Avg. # Factual Retention Tasks per Video Avg. # Skill Retention Tasks per Video Avg. # Videos per Domain Value 74 03:48:19 01:16 10:41 03:05 5.4 19.6 12.3 Table 1: Video statistics for VideoWebArena. Domain Reddit # Factual Tasks # Skill Tasks # Total Tasks 87 (22%) 206 (13%) 293 (14%) Classifieds 60 (15%) 320 (20%) 380 (19%) Shopping 121 (30%) 654 (40%) 775 (38%) Shopping (Admin) 47 (12%) 182 (11%) 229 (11%) GitLab 70 (18%) 191 (12%) 261 (13%) Map Total 15 (4%) 68 (4%) 83 (4%) 400 (100%) 1621 (100%) 2021 (100%) Table 2: Distribution of tasks for VideoWebArena broken down between tasks that test skill retention and factual retention. Figure 2: Left: VideoWebArena Video Difficulty Task Distribution. Right: VideoWebArena Agent Difficulty Task Distribution. Figure 3: VideoWebArena Factual Retention Task Counts for Each Category. The categories are non-exclusive. One task can fall under multiple video perception categories. observation ot O, an agent generates executable action at A, resulting in new state st+1 and new partial observation ot+1 O. The reward function : [0, 1] returns non-zero value at the final step if the agent state achieves the task objective and zero otherwise. We list the available reward function in Table 8. 4 Preprint. Action Type Description click [elem] hover [elem] type [elem] [text] press [key comb] new tab tab focus [index] tab close goto [url] go back go forward scroll [updown] clear [elem] upload [file path] [elem] stop [answer] Click on element elem. Hover on element elem. Type text on element elem. Press key combination. Open new tab. Focus on the i-th tab. Close current tab. Open url. Click the back button. Click the forward button. Scroll up or down the page. Clear text box element. Upload local file using upload button. End the task with an output. Table 3: List of Action Types and Descriptions"
        },
        {
            "title": "3.3 OBSERVATION SPACE",
            "content": "The observation space for the VideoWA environment is strictly predicated on the Set-of-Marks observation space in VisualWebArena. The environment uses executable JavaScript code at each step to extract the interactable HTML elements from the webpage and present them in top-down order. Similarly, the JavaScript code extracts the bounding boxes of each interactable element and screenshot of the webpage with bounding boxes over the interactable elements is generated for input to the agent along with the text state representation. At each time step, the agent is presented with the overlayed Set-of-Marks screenshot and text observation space, along with the chosen video information to be put into context."
        },
        {
            "title": "3.3.1 ACTION SPACE",
            "content": "The action space for the agents in the VideoWA environment can be seen in Table 3. The agents are prompted to generate single action from the action space at each time step. Each action is associated with Playwright Python code that automatically performs the action within the browser. The elem parameter in the action represents the unique Set-of-Marks element that can be interacted with from the observation space provided through the environments JavaScript code."
        },
        {
            "title": "3.4 TASK DESIGN",
            "content": "The taxonomy covers two subsets of tasks skill retention and factual retention inspired by real-world use cases. We illustrate the taxonomy breakdown in Figure 6. We define skill retention as the ability to learn from and use given human demonstration to efficiently complete task For example, using YouTube tutorials or screen recordings of expert demonstrations to learn how to perform task is form of skill retention. On the other hand, factual retention is the ability to retrieve information relevant to users specific question/task present in video that may not be the videos main focus (e.g., an incidental detail). For example, one might want to buy the shoes particular NBA player is wearing that are shown within short duration of much longer basketball highlights video. To complete the task, the model must extract not only information about the specific player but also their shoes, even if this information is secondary to the main content of the video. We present example tasks in Table 4 and an example of an agent on stylized task in Figure 7. Each task has an intent, which is the objective of the task. For all of the newly created factual retention tasks, there is also an intermediate intent, video-based question that must be answered correctly to have the information necessary to complete the task. Each task also has an automatic evaluator function for both intent and intermediate intent that returns score of 0 or 1 based on the environment and response given by the LLM agent. Each task also has an agentic difficulty, distributed between easy, medium, and hard. The agentic difficulty for each task signifies the complexity of the action sequence needed to complete an intent successfully. For agentic difficulty, we classify task as easy if it can be completed in 1-3 steps, medium if it can be completed in 4-9 steps, and hard if it can be completed in more than 9 steps. This classification is adopted from VisualWebArena (Koh et al., 2024a). Figure 2 provides more detailed breakdown of task difficulty. 5 Preprint."
        },
        {
            "title": "Buy Cheapest Item Skill Retention",
            "content": "cheapest the Buy red from blanket Blankets & Throws. N/A"
        },
        {
            "title": "Leave a Comment Audio Perception",
            "content": "for the Search company the person said they work at in the video and find first the posts comments. did company What the person in the video say they work for?"
        },
        {
            "title": "Full Video\nUnderstanding",
            "content": "Follow repos the video. all visited the in What are the names of all the visited repos?"
        },
        {
            "title": "See Listing Ratings Visual Reasoning",
            "content": "page the Find that the shows zipcode of the 2nd destination in the video. Take me to the first red vehicle listing that appears in the video. What was the name of the 2nd destination used in the video? What was the name of the first red vehicle listing that appears in the video? Table 4: Examples of Each Task in the VideoWebArena Taxonomy: Given video tutorial, the agent is asked to perform the intent. The intermediate intent tests the multimodal agents ability to extract the necessary information to perform the task from the video. Skill retention tasks do not have intermediate intents as they do not require recalling specific information that factual retention tasks will require."
        },
        {
            "title": "3.5 VIDEO CREATION AND SKILL RETENTION TASKS",
            "content": "Our benchmark contains 74 unique videos, totaling almost 4 hours of video content (see Table 1 for details)all of our video tutorials are based on tasks in WebArena and VisualWebArena. We provide our videos online through YouTube channel and Google Drive link containing the zip file of all the videos. We formulated these videos by accumulating all the feasible intent templates in WebArena and VisualWebArena. We take 297 unique templates from VisualWebArena and 220 unique templates from WebArena, totaling 1621 total intents. Further details can be found in Appendix A.1 We map each of our video tutorials to the respective tasks in the WebArena and VisualWebArena task set to create skill retention tasks. We then create 400 original factual retention tasks based on these same tutorials. We had three of the papers authors create videos and corresponding tasks for each video they created. We then conducted cross-validation quality assurance with an author who did not make the video/tasks to ensure the task was understandable and able to be completed. We conduct human performance tests similarly, having an author who didnt create the video or tasks attempt the task and have it evaluated by third author. Further details on task creation and human evaluation can be found in A.2 and A.3. 6 Preprint."
        },
        {
            "title": "3.6 FACTUAL RETENTION TASKS",
            "content": "For factual retention, we further divide this category into four finer sub-categories: (1) Visual Perception (OCR, Spatial Reasoning), (2) Audio Perception, (3) Full Video Understanding (i.e., tasks that require information across several parts of the video), and (4) Temporal Reasoning (i.e., tasks that require understanding the video with respect to time). One key difference between the factual and skill retention tasks is the intermediate intent and evaluation we create for each factual retention task. The intermediate intent is the video-based question that must be answered correctly to have the information necessary to complete the task. This is intended to decouple the evaluation of agentic abilities in long context video models for video information retrieval tasks; by checking if the model can extract information necessary to complete the task from the video and evaluating that separately from the agents success, this process can pinpoint the failure modes of the model, whether they come from generating agent actions or video processing. Additionally, we provide video difficulty ratings for all intermediate intents, distributed between easy, medium, and hard. The video difficulty ratings signify the complexity of returning the correct answer for given tasks intermediate intent. Easy tasks require returning one piece of information and can be solved with less than 3 frames, medium tasks require returning 2 to 3 things and can be solved with less than half the video, and hard tasks require returning more than 3 things and require watching more than half the video. We provide breakdown of each task type in our benchmark in Table 4. VideoWA contains 111 unique intent templates across the 400 intents in the factual retention task set."
        },
        {
            "title": "3.7 TASK EVALUATION",
            "content": "Each task has an eval and intermediate eval function. We import the automatic functionality from VisualWebArena (Koh et al., 2024a) and WebArena (Zhou et al., 2024) to evaluate our agent tasks. For the intermediate intent evaluation, we use the string-based existing functionality evaluators to assess the agents response to the video-based question. All of our tasks have final evaluation function (i.e., evaluator) that determines an agents reward on each task. The reward is typically binary, returning zero or unity depending on whether the agent performs the task unsuccessfully or successfully. Reward values are determined by evaluating the state of the environment at the end of the agents trajectory to determine if said state matches the correct state corresponding to the correct task execution."
        },
        {
            "title": "4 BASELINE AGENTS",
            "content": "We evaluate our benchmark using three different types of baseline agents with multimodal models as backbone. At each step, the agent is given the task objective, 2 in-context examples, current state s, and the input video to the objective as context to generate one action."
        },
        {
            "title": "4.1 VIDEO IN-CONTEXT AGENT",
            "content": "We define video input agent that takes the video in at every time step to generate actions. We provide the whole video in-context to the model with the Gemini model. The Gemini model automatically processes the audio, eliminating the need to process audio secretly. The specific prompts we use are in Appendix D. We use Set-of-Marks on the website HTML page, the Set-of-Marks element tree string, and the prompt along with the video as input to the model."
        },
        {
            "title": "4.2 VIDEO FRAMES IN-CONTEXT AGENT",
            "content": "We define video input agent that takes set amount of video frames at every time step along with the video audio to generate actions. To obtain the information from the video, we follow the practice from (Wu et al., 2024). We sample 1 frame per second (max 60 frames) for the video and include them into the context for the LLM. In addition, we use OpenAIs Whisper (Radford et al., 2022) to transcribe the audio and append it to the context. In this way, we can still pass the information from the video to our LLM. This may not be perfect method as the video information remains missing during framing sampling. However, since most LLMs in the market only support image and text input, it is essential to experiment with this setting. We use GPT-4o, and the prompt can be seen in 7 Preprint. Figure 4: VideoWebArena Baseline Agent Framework: We use 3 baseline agents: 1.) Video Summary Agent, where the video summary is fed in-context. 2.) Video Frame Agent, where set number of frames and audio transcription is fed in-context. 3.) Video Agent, where the video is fed in as an .mov file in-context. The video information is put in-context along with the Set-of-Marks state representation to generate singular action, following the multimodal SoM agent in VisualWebArena (Koh et al., 2024a). Appendix D. We again use Set-of-Marks on the website HTML page, the Set-of-Marks element tree string, and the prompt along with 60 video frames and audio transcriptions as input to the model."
        },
        {
            "title": "4.3 VIDEO SUMMARY IN-CONTEXT AGENT",
            "content": "We define summary in-context agent that takes video summary related to the objective at hand in-context at every time step to generate actions. To obtain this summary, we call GPT-4o and feed the video using 60 frames and the Whisper transcription into the model and prompt it to summarize the video concerning the task at hand. Again, our prompt can be seen in Appendix D. Similarly, the summary agent also uses Set-of-Marks for the observation space and generates actions in the aforementioned action space."
        },
        {
            "title": "5.1 MODEL PERFORMANCE",
            "content": "From Table 5, Table 6 and Table 7, we see varying degrees of agentic performance across the videocapable Gemini and GPT family of models; however, we note several consistent trends across LLM agent results. We comprehensively outline the failure modes in Appendix B. There is no winning baseline agent or model family across skill and factual retention tasks. For factual retention tasks, the summary agent performs the best in task success at 13.3% while the 30 and 100 Frame GPT-4o Agent perform the best in intermediate intent success at 45.8%. For skill retention tasks, we see that long-context models with tutorials actually perform significantly worse than models without tutorials, suggesting that the tutorials introduce negative noise that hurt action selection. Although intermediate scores tend to be higher than final scores, this does not necessarily translate to task success. This is constant failure mode of the long-context agents, as they can perform the necessary VQA to extract the necessary information for the task at hand but fall short due to hallucinations, action grounding, and high-level planning errors. For example, in Figure 5, the LLM agent successfully identifies the item to buy from the video. Still, it does not successfully plan and complete the intent. We tested on smaller subset of tasks with the GPT4-o agent and tested on the full set of tasks with the Gemini agent due to compute constraints. 8 Preprint. Model Task Domain Final Score Intermediate Score # Steps (Avg) Gemini 1.5 Pro Video Agent GPT4-o Summary Agent GPT4-o Frame Agent (30 Frames) GPT4-o Frame Agent (60 Frames) GPT4-o Frame Agent (100 Frames) Human Performance Classifieds Gitlab Map Reddit Shopping (admin) Shopping Total Classifieds Gitlab Map Reddit Shopping (admin) Shopping Total Classifieds Gitlab Map Reddit Shopping (admin) Shopping Total Classifieds Gitlab Map Reddit Shopping (admin) Shopping Total Classifieds Gitlab Map Reddit Shopping (admin) Shopping Total Classifieds Gitlab Map Reddit Shopping (admin) Shopping Total 6.7% 5.7% 6.7% 3.4% 8.5% 10.0% 7.0% 10.0% 14.2% 26.7% 11.5% 8.5% 15.7% 13.3% 18.3% 5.7% 26.7% 6.9% 8.5% 12.4% 11.0% 10.0% 5.7% 26.7% 2.3% 4.3% 5.0% 6.0% 13.3% 7.1% 20.0% 5.7% 8.5% 10.7% 9.5% 61.5% 81.3% 69.2% 81.8% 68.4% 75.0% 73.9% 41.7% 35.7% 73.3% 39.0% 48.9% 24.7% 37.0% 40.0% 34.7% 66.7% 39.0% 29.1% 33.8% 36.8% 46.6% 50.0% 73.3% 42.5% 57.4% 37.2% 45.8% 30.0% 55.7% 60.0% 44.8% 48.9% 38.0% 43.5% 41.6% 58.6% 53.3% 43.7% 51% 38.8% 45.8% 69.2% 81.3% 76.9% 86.4% 73.7% 82.1% 79.3% 17.1 18.5 9.9 18.2 23.7 21.6 19.4 9.7 13.0 3.8 13.8 13.7 14.3 12.8 9.3 11.8 4.7 11.6 16.8 19.5 14.0 9.5 13.4 3.5 11.2 13.6 16.9 13. 7.64 14.8 3.8 11.6 14.4 16.4 13.0 7.9 7.1 4.8 9.0 5.1 5.0 6.4 Table 5: Results on VideoWebArena Factual Retention Tasks. Performance of GPT4-o, Gemini 1.5 Pro, and human performance on 400 factual retention tasks broken down by task domain. Final scores indicate the overall task performance (i.e., if the task is completed successfully in its entirety), while intermediate scores measure the performance on the intermediate intents. Model WebArena Final Score Steps VisualWebArena Final Score Steps GPT4-o (No Tutorial) GPT4-o Summary Agent (Tutorial) GPT4-o Frame Agent (Tutorial) Human Performance (No Tutorial) Human Performance (Tutorial) 14.9% 13.8% 9.9% 82.6% 93.1% - 13.9 11.4 12.0 6.1 19.8% 11.6% 9.5% 72.7% 88.6% - 12.4 12.5 12.4 8.2 Table 6: Results on VideoWebArena Skill Retention Tasks. Overall performance comparison of GPT4-o and human performance on skill retention tasks. Human performance shows tutorials should help task performance success and efficiency. However, adding tutorials in-context to the model does not necessarily help, but in fact hurts performance by significant margin. See the failure modes in Appendix for more analysis. Dashes (-) indicate that data is unavailable for that particular metric."
        },
        {
            "title": "5.2 HUMAN PERFORMANCE",
            "content": "To understand the level of human performance expected on the tasks within VideoWA, three authors attempted the tasks and provide intermediate answers for random sample of each unique task 9 Preprint."
        },
        {
            "title": "Task Category",
            "content": "GPT-4o Summary GPT-4o (30 Frames) GPT-4o (60 Frames) GPT-4o (100 Frames) Gemini 1.5 Pro"
        },
        {
            "title": "Visual Perception Task Success Rate\nAudio Perception Task Success Rate\nFull Video Understanding Task Success Rate\nTemporal Reasoning Task Success Rate\nAgentic Easy Task Success Rate\nAgentic Medium Task Success Rate\nAgentic Hard Task Success Rate\nVisual Perception Intermediate Success Rate\nAudio Perception Intermediate Success Rate\nFull Video Understanding Intermediate Success Rate\nTemporal Reasoning Intermediate Success Rate\nVideo Easy Intermediate Success Rate\nVideo Medium Intermediate Success Rate\nVideo Hard Intermediate Success Rate",
            "content": "14.1% 14.8% 15.5% 13.7% 19.5% 14.2% 10.8% 32.7 50.0% 34.2 35.9 39.5% 39.4% 32.2% 11.1% 18.1% 10.0% 12.4% 12.8% 13.4% 8.1% 43.9% 60.2% 40.0% 50.5% 52.9% 46.2% 42.4% 6.8% 7.7% 7.2% 6.2% 9.0% 5.7% 6.2% 43.0% 62.8% 40.9% 50.9% 52.2% 50.4% 40.7% 9.3% 12.5% 10.5% 10.4% 13.0% 9.4% 9.1% 43.5% 62.5% 41.2% 50.0% 53.2% 48.3% 41.0% 7.7% 11.1% 6.5% 8.8% 8.3% 7.7% 6.9% 34.0% 67.9% 26.2% 38.9% 47.1% 46.6% 26.1% Table 7: Factual Retention Results Breakdown: Overall performance breakdown of the baseline agents across all task categories and difficulties in the factual retention set. The summary agent has the best task performance, even without having any visual aspect of the video in context. However, it lags behind in the intermediate VQA intents, as the video frame and video agents all perform very similarly on intermediate tasks. template in the factual retention set. Further details on the human evaluation set can be found in Appendix A.3. They also tested 74 unique skill retention tasks, with each task having 2 separate humans attempt the task, one with tutorial and one without. The humans performed actions and recorded steps using the VideoWA action space. Humans achieve success rate of 73.9% on factual retention tasks while only taking an average of 6.4 steps per task  (Table 5)  . Additionally, the intermediate intent and intent performance are linearly correlated while LLMs are not, citing deficiency in the agentic abilities of these models. For skill retention tasks, human performance registers 93.1% on WebArena tasks and 88.6% in VisualWebArena with tutorials, and 82.6% and 72.7% without tutorials. We see an intuitive drop in human performance and efficiency without tutorials. In terms of both task success rates and average number of steps taken, video-capable LLM agents lag behind significantly, further emphasizing the need to improve agentic reasoning with video capacity in todays models."
        },
        {
            "title": "6 DISCUSSION",
            "content": "We present VideoWebArena, rigorous video-based agent benchmark that tests the agentic ability of long-context multimodal models. We define task taxonomy of video-based agent tasks, utilizing wide coverage of task types including skill retention and factual retention to create comprehensive test bed for the setting of video agents. We provide 2021 tasks that are all video-based, along with 74 manually created videos. According to our experiments, the baseline agent does not perform well on most of tasks compared with human performance. There is still long way in developing intelligent agents. For future work, it is important to analyze the failure cases explore better video agent architectures with different LLMs on this benchmark. We hope our environment and benchmark facilitate improvement and additional work on improving long-context multimodal agents."
        },
        {
            "title": "ACKNOWLEDGEMENTS",
            "content": "We would like to thank Jing Yu Koh, Gabriel Sarch, Jennifer Hsia, Trevor Moreci, and Nicholas Chermak for providing valuable feedback on our project and paper."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "The authors are committed to making this work reproducible. Our code is open-sourced and available at https://github.com/ljang0/videowebarena. Our videos are also available through Google Drive and Youtube. Our data details are provided in Section 3 and our models and prompts are specified in Section 4. 10 Preprint."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "Our benchmark is intended for safe and responsible innovations of video-based LLM agents. With the rising popularity of LLM agents and the excitement around their deployment, measures to ensure their safe practical deployment and use cases must be present. The authors are committed to the ethical development of LLM agents. For our paper, we did not use human subjects, find any potentially harmful insights, or any ethical concerns. Our benchmark is self-contained environment to test agents on synthetic tasks."
        },
        {
            "title": "REFERENCES",
            "content": "AI Anthropic. The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card, 2024. Rogerio Bonatti, Dan Zhao, Francesco Bonacci, Dillon Dupont, Sara Abdali, Yinheng Li, Yadong Lu, Justin Wagle, Kazuhito Koishida, Arthur Bucker, Lawrence Jang, and Zack Hui. Windows agent arena: Evaluating multi-modal os agents at scale, 2024. URL https://arxiv.org/abs/ 2409.08264. Wayne Chi, Ameet Talwalkar, and Chris Donahue. The impact of element ordering on lm agent performance, 2024. URL https://arxiv.org/abs/2409.12089. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards generalist agent for the web. In Advances in Neural Information Processing Systems (NeurIPS), 2023. Alexandre Drouin, Maxime Gasse, Massimo Caccia, Issam Laradji, Manuel Del Verme, Tom Marty, Leo Boisvert, Megh Thakkar, Quentin Cappart, David Vazquez, et al. Workarena: How capable are web agents at solving common knowledge work tasks? arXiv preprint arXiv:2403.07718, 2024. Yunhao Fang, Ligeng Zhu, Yao Lu, Yan Wang, Pavlo Molchanov, Jang Hyun Cho, Marco Pavone, Song Han, and Hongxu Yin. Vila2: Vila augmented vila. arXiv preprint arXiv:2407.17453, 2024. Yao Fu, Dong-Ki Kim, Jaekyeom Kim, Sungryull Sohn, Lajanugen Logeswaran, Kyunghoon Bae, and Honglak Lee. Autoguide: Automated generation and selection of state-aware guidelines for large language model agents. arXiv preprint arXiv:2403.08978, 2024. Hiroki Furuta, Kuang-Huei Lee, Ofir Nachum, Yutaka Matsuo, Aleksandra Faust, Shixiang Shane Gu, and Izzeddin Gur. Multimodal web navigation with instruction-finetuned foundation models. In International Conference on Learning Representations (ICLR), 2024. Google. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. In Proceedings of the 2024 Annual Meeting of the Association for Computational Linguistics (ACL), 2024a. Jing Yu Koh, Stephen McAleer, Daniel Fried, and Ruslan Salakhutdinov. Tree search for language model agents, 2024b. URL https://arxiv.org/abs/2407.01476. Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, et al. Autowebglm: Bootstrap and reinforce large language model-based web navigating agent. arXiv preprint arXiv:2404.03648, 2024. Jie Lei, Licheng Yu, Mohit Bansal, and Tamara L. Berg. Tvqa: Localized, compositional video question answering, 2019. URL https://arxiv.org/abs/1809.01696. Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, Limin Wang, and Yu Qiao. Mvbench: comprehensive multi-modal video understanding benchmark, 2024. URL https://arxiv.org/abs/2311.17005. 11 Preprint. Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. arXiv preprint arXiv:2023b, 2023. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Advances in Neural Information Processing Systems (NeurIPS), 2023. Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very long-form video language understanding, 2023. URL https://arxiv.org/ abs/2308.09126. OpenAI. Hello gpt-4o. https://openai.com/index/hello-gpt-4o/, 2024. Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey Levine, and Alane Suhr. Autonomous evaluation and refinement of digital agents, 2024. URL https://arxiv.org/abs/2404.06474. Ajay Patel, Markus Hofmarcher, Claudiu Leoveanu-Condrei, Marius-Constantin Dinu, Chris CallisonBurch, and Sepp Hochreiter. Large language models can self-improve at web agent tasks. arXiv preprint arXiv:2405.20309, 2024. Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adri`a Recasens Continente, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Joseph Heyward, Mateusz Malinowski, Yi Yang, Carl Doersch, Tatiana Matejovicova, Yury Sulsky, Antoine Miech, Alex Frechette, Hanna Klimczak, Raphael Koster, Junlin Zhang, Stephanie Winkler, Yusuf Aytar, Simon Osindero, Dima Damen, Andrew Zisserman, and Joao Carreira. Perception test: diagnostic benchmark for multimodal video models, 2023. URL https://arxiv.org/abs/2305.13786. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision, 2022. URL https://arxiv.org/ abs/2212.04356. Christopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Marybeth Fair, Alice Li, William Bishop, Wei Li, Folawiyo Campbell-Ajala, Daniel Toyama, Robert Berry, Divya Tyamagundlu, Timothy Lillicrap, and Oriana Riva. Androidworld: dynamic benchmarking environment for autonomous agents, 2024. Gabriel Sarch, Lawrence Jang, Michael J. Tarr, William W. Cohen, Kenneth Marino, and Katerina Fragkiadaki. Ical: Continual learning of multimodal agents by transforming trajectories into actionable insights, 2024. URL https://arxiv.org/abs/2406.14596. Paloma Sodhi, SRK Branavan, Yoav Artzi, and Ryan McDonald. Step: Stacked llm policies for web actions. arXiv preprint arXiv:2310.03720v2, 2024. Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba, Raquel Urtasun, and Sanja Fidler. Movieqa: Understanding stories in movies through question-answering, 2016. URL https://arxiv.org/abs/1512.02902. Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4):600612, 2004. doi: 10.1109/TIP.2003.819861. Zora Zhiruo Wang, Jiayuan Mao, Daniel Fried, and Graham Neubig. Agent workflow memory, 2024. URL https://arxiv.org/abs/2409.07429. Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding, 2024. URL https://arxiv.org/abs/2407.15754. Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa:next phase of question-answering to explaining temporal actions, 2021. URL https://arxiv.org/abs/2105.08276. Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. arXiv preprint arXiv:2404.07972, 2024. Preprint. Fuzhao Xue, Yukang Chen, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, Ethan He, Hongxu Yin, Pavlo Molchanov, Jan Kautz, Linxi Fan, Yuke Zhu, Yao Lu, and Song Han. Longvila: Scaling long-context visual language models for long videos, 2024. Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441, 2023. Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. Advances in Neural Information Processing Systems, 35:2074420757, 2022. Hanrong Ye, De-An Huang, Yao Lu, Zhiding Yu, Wei Ping, Andrew Tao, Jan Kautz, Song Han, Dan Xu, Pavlo Molchanov, and Hongxu Yin. X-vila: Cross-modality alignment for large language model. CoRR, abs/2405.19335, 2024. Jiwen Zhang, Jihao Wu, Yihua Teng, Minghui Liao, Nuo Xu, Xiao Xiao, Zhongyu Wei, and Duyu Tang. Android in the zoo: Chain-of-action-thought for gui agents. arXiv preprint arXiv:2403.02713, 2024a. Ziniu Zhang, Shulin Tian, Liangyu Chen, and Ziwei Liu. Mmina: Benchmarking multihop multimodal internet agents. arXiv preprint arXiv:2404.09992, 2024b. Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, et al. Webarena: realistic web environment for building autonomous agents. In International Conference on Learning Representations (ICLR), 2024."
        },
        {
            "title": "A VIDEOWEBARENA DATA DETAILS",
            "content": "A.1 VIDEO CREATION DETAILS Figure 5: Dataset Creation Process walkthrough of the VideoWebArena dataset creation. From 1641 existing tasks in WebArena and VisualWebArena, the authors grouped these tasks by their intent templates. For each intent template, the authors created new video tutorial showing how to perform the tasks. For each video, the authors made at minimum 4 factual retention tasks. This led to 1641 skill retention and 400 factual retention tasks. Three authors of the paper created 74 original video tutorials with audio narrating the actions taken. The three authors evenly divided the videos based off the site the video was based on. We based these 74 tutorials off of 1641 intents in WebArena and VisualWebArena. Each of these tasks are mapped to video, creating skill retention task. Each video creator manually checked all of 13 Preprint. Figure 6: VideoWebArena Task Taxonomy We define taxonomy for all the tasks in our benchmark, namely splitting them into factual and skill retention groups. Under the factual retention group, there are 4 types of tasks: Visual Perception, Audio Perception, Full Video Understanding, and Temporal Reasoning. the tasks that they were to create tutorial for before, then made sure the functionality of the task was shown in the tutorial. We post these videos on Youtube at https://www.youtube.com/ @webarenawarrior. We also provide them online at Google Drive link: https://drive.google. com/file/d/1E1hM2jn1mj5q5d j9mlx n5FkbpZS1Yb/view?usp=drive link. A.2 TASK CREATION DETAILS Every video is mapped to minimum one VisualWebArena or WebArena task, creating skill retention tasks. The authors of each video were also tasked with creating minimum of four factual retention tasks per video, with one task type each from the factual retention taxonomy. The taxonomy can be seen in Figure 6. The authors of each task also are tasked with creating intermediate questions for the factual retention tasks that test if the model can extract the information necessary to complete the task. The authors also create evaluation functions for the intermediate intent and task. Once created, second author verified and completed each task for quality assurance purposes. A.3 HUMAN EVALUATION DETAILS We conducted two sets of human evaluation, one for the skill retention and one for the factual retention tasks. For the factual retention tasks, an author who did not make the task was given the video, along with the intermediate intent and task intent. Each author was given the agent action space and recorded their number of actions as defined by the agent action space. The answers to the intermediate and task intent were then verified by another author. We did human evaluation on subset of the factual retention tasks, simply taking random sample of each unique intent template. We had 111 factual retention tasks for human evaluation. For the skill retention tasks, two authors who did not make the original tutorial were tasked with completing skill retention task. One author was given the video tutorial before and the other author was not. They then recorded their action steps and completed the task, which was then evaluated and verified by third author. We did skill retention human evaluation on singular task per tutorial, totaling to 74 human evaluation skill retention tasks. Given the extremely high success rate for both types of tasks, many of the failures came from human carelessness or interpretation mistakes. A.4 ENVIRONMENT DETAILS We provide table of the VideoWebArena reward functions in Table 8. These are adapted from VisualWebArena (Koh et al., 2024a). 14 Preprint. Evaluator Functions Reward Condition exact match(a, ˆa) must include(a, ˆa) fuzzy match(a, ˆa) must exclude(a, ˆa) eval vqa(img, question, ˆa) eval fuzzy image match(img, ˆimg) 1 if is exactly ˆa. 1 if is in the set ˆa. 1 if and ˆa are deemed semantically equal by an LLM. 1 if is not in the set ˆa. 1 if the output of VQA Model(img, question) contains ˆa. 1 if the SSIM (Wang et al., 2004) between img, ˆimg is higher than given threshold. Table 8: List of VideoWebArena evaluator functions and descriptions: All rewards are binary. We adopt our evaluators from WebArena (Zhou et al., 2024) and VisualWebArena (Koh et al., 2024a)."
        },
        {
            "title": "B FAILURE MODES",
            "content": "B.1 COMMON LLM AGENT FAILURE MODES Many of the basic failures captured in the baseline agents were common repeats of agent errors seen in other agent academic benchmarks. These include hallucinations, where the agent produces nonsensical action unrelated to its context or task at hand. We attribute this to the lack of instruction tuning and model alignment on agentic tasks. Another common failure mode displayed in the baseline agents was failure to do visual grounding. The agents will recognize the correct plan of action, but choose the wrong element with respect to the Set-of-Marks image input and take the wrong action. Action grounding and planning was also common failure mode of the baseline agents. An agent can simply generate the wrong plan or action that will yield unsuccessful trajectories, and not change this plan even with negative feedback from the environment. This suggests using inference time search or memory based methods can be effective to combat these failures. Incorporating self-reflection during inference can also help the agents recover from failures in action grounding and planning. The lack of self-reflection is especially seen when the agent generates the same action repetitively, leading the task to terminate. Even though an action is shown to be unsuccessful towards completing task, an agent will continue to repeatedly attempt the same action to try and complete the task. B.2 LONG-CONTEXT SPECIFIC FAILURE MODES Within our skill and factual retention tasks, there were many failure modes that presented issues relevant to long-context modeling. One constant issue we noticed was failure to adhere to the prompt instructions for generating actions. With the extra noise provided with the video information in-context, the agent did not always adhere to the action generation guidelines provided in the prompt. For example, under the Set-of-Marks elements, click action must be generated using click [elem] where elem is the numeric ID of the SoM element. However, the agent would return click [elem] where elem was the name of the element. This formatting issue persisted for other actions with the longer prompt. common issue for skill retention tasks was the agent began generating multi-action responses when the prompt explicitly says to generate one action. Given the tutorial or summary to complete similar task, the agent would get distracted by the comprehensive plan and generate multiple actions from the video information, straying away from the prompt guidelines. This led to failure to complete tasks. common issue for factual retention tasks was video grounding. Specifically, we could pinpoint that the video-frame and summary agents would simply miss visual information due to the nature of their video processing. Additionally, the video agent also showcased many of these video grounding errors. For example, common task was to Take me to the page in the video when event happened. However, if the frames or summary did not include this page, there was no way for the agent to get to this page or know about its existence. This issue was exacerbated in tasks that required full video understanding or temporal reasoning across the video. This is flaw in the baseline agent setup we proposed. Many of the audio tasks were completed at much higher rate than the video perception tasks, citing that video grounding is larger issue than audio grounding when processing these modalities within videos. We encourage better video understanding agent systems with our benchmark. 15 Preprint."
        },
        {
            "title": "C RESULTS",
            "content": "C.1 ADDITIONAL RESULTS We provide another result breakdown plot at Table 9. This shows the average steps per task type. Table 9: Model Comparison - Average Steps per Task Type"
        },
        {
            "title": "Visual Perception\nAudio Perception\nFull Video Understanding\nTemporal Reasoning\nAgentic Easy\nAgentic Medium\nAgentic Hard\nVideo Easy\nVideo Medium\nVideo Hard",
            "content": "GPT-4o Summary GPT-4o (30 Frames) GPT-4o (60 Frames) GPT-4o (100 Frames) Gemini 1.5 Pro 12.9 10.5 12.5 14.9 12.6 11.6 14.3 11.8 13.1 13. 14.5 10.5 14.4 14.6 10.1 14.7 15.6 13.4 13.2 15.5 13.7 10.2 13.6 13.9 10.1 13.5 14.9 11.9 12.9 14.6 13.2 10.5 13.0 14.5 10.2 12.5 15.4 13.4 11.8 13.8 19.7 17.0 20.3 20.1 20.6 19.2 19.4 19.5 19.5 19.6 Figure 7: Abridged Example of VideoWebArena Task. stylized example of task in VideoWebArena: starting from (a) to (b), the task is defined, and an agent interacts with its visual input to create plan and perform actions. From (b) to (c), it continues its actions and planning along its trajectory for the task before concluding (incorrectly) in (d), where it receives final reward of zero for failing to complete the task correctly."
        },
        {
            "title": "D AGENT PROMPTS",
            "content": "D.1 VIDEO AGENT TASK PROMPT You are an autonomous intelligent agent tasked with navigating web browser. You will be given web-based tasks that can be done based on information in video. These tasks will be accomplished through the use of specific actions you can issue. Heres the information youll have: 1. The users objective: This is the task youre trying to complete. 16 Preprint. 2. video tutorial about this task or similar task will be provided to assist you. 3. The current web pages accessibility tree: This is simplified representation of the webpage, providing key information. 4. The current web pages URL: This is the page youre currently navigating. 5. The open tabs: These are the tabs you have open. 6. The previous action: This is the action you just performed. It may be helpful to track your progress. The actions you can perform fall into several categories: # Page Operation Actions ˋˋˋclick [id]ˋˋˋ: This action clicks on an element with specific id on the webpage. ˋˋˋtype [id] [content]ˋˋˋ: Use this to type the content into the field with id. By default, the Enter key is pressed after typing unless press enter after is set to 0, i.e., ˋˋˋtype [id] [content] [0]ˋˋˋ. ˋˋˋhover [id]ˋˋˋ: Hover over an element with id. ˋˋˋpress [key comb]ˋˋˋ: Simulates the pressing of key combination on the keyboard (e.g., Ctrl+v). ˋˋˋscroll [down]ˋˋˋ or ˋˋˋscroll [up]ˋˋˋ: Scroll the page up or down. # Tab Management Actions ˋˋˋnew tabˋˋˋ: Open new, empty browser tab. ˋˋˋtab focus [tab index]ˋˋˋ: Switch the browsers focus to specific tab using its index. ˋˋˋclose tabˋˋˋ: Close the currently active tab. # URL Navigation Actions ˋˋˋgoto [url]ˋˋˋ: Navigate to specific URL. ˋˋˋgo backˋˋˋ: Navigate to the previously viewed page. ˋˋˋgo forwardˋˋˋ: Navigate to the next page (if previous go back action was performed). # Completion Action ˋˋˋstop [answer]ˋˋˋ: Issue this action when you believe the task is complete. If the objective is to find text-based answer, provide the answer in the bracket. Homepage: If you want to visit other websites, check out the homepage at http://homepage.com. It has list of websites you can visit. http://homepage.com/password.html lists all the account name and password for the websites. You can use them to log in to the websites. To be successful, it is very important to follow the following rules: 1. You should only issue an action that is valid given the current observation. 2. You should only issue one action at time. 3. You should follow the examples to reason step by step and then issue the next action. 4. Generate the action in the correct format. Start with In summary, the next action will perform is phrase, followed by action inside ˋˋˋ. For example, In summary, the next action will perform is ˋˋˋclick [1234]ˋˋˋ. 5. Issue stop action when you think you have achieved the objective. Dont generate anything after stop. Example 1: VIDEO: Shopping Tutorial MOV File OBSERVATION: [28] [A] [Pre-baked Gingerbread House Kit Value Pack, 17 oz., Pack of 2, Total 34 oz.] [] [StaticText] [19.99] [30] [BUTTON] [Add to Cart] [34] [A] [V8 +Energy, Healthy Energy Drink, Steady Energy from Black and Green Tea, Pomegranate Blueberry, 8 Ounce Can, Pack of 24] [] [StaticText] [14.47] [36] [BUTTON] [Add to Cart] [40] [A] [Elmwood Inn Fine Teas, Orange Vanilla Caffeine-free Fruit Infusion, 16-Ounce Pouch] [] [StaticText] [19.36] [42] [BUTTON] [Add to Cart] [43] [A] [Add to Wish List] URL: http://onestopmarket.com OBJECTIVE: Can you take me to the product page for the first item added to the cart in the video? PREVIOUS ACTION: None OUTPUT: Lets think step-by-step. This page is the home page for the One Stop Market and includes the first item added to the cart in the video, which was Elmwood Inn Fine Teas, Orange Vanilla Caffeine-free Fruit Infusion, 16-Ounce Pouch. In the video tutorial, it shows that clicking on the title of the listing will take me to the item page. Therefore, will issue the click action with the item listing. In summary, the next action will perform is ˋˋˋclick [40]ˋˋˋ. Example 2: VIDEO: Reddit Tutorial MOV File OBSERVATION: [StaticText] [Forums] [1] [A] [Forums] [9] [A] [Alphabetical] [] [StaticText] [allentown] [10] [A] [allentown] [] [StaticText] [baltimore] [16] [A] [baltimore] [] [StaticText] [books] [17] [A] [books] [] [StaticText] [boston] [18] [A] [boston] [] [StaticText] [MachineLearning] [52] [A] [MachineLearning] [] [StaticText] [pittsburgh] [78] [A] [pittsburgh] [] [StaticText] [technology] [90] [A] [technology] [] [StaticText] [television] [91] [A] [television] [] [StaticText] [Running Postmill] [105] [A] [Postmill] URL: http://reddit.com OBJECTIVE: Can you take me to the post in the video the user commented on? PREVIOUS ACTION: None OUTPUT: Lets think step-by-step. This page lists all the forums on the website. In the video, the post that the user left comment was under the r/technology forum, so should navigate to that. can navigate to that forum by first clicking on the r/technology link. Therefore, will issue the click action. In summary, the next action will perform is ˋˋˋclick [90]ˋˋˋ. 17 Preprint. D.2 VIDEO FRAME AGENT TASK PROMPT You are an autonomous intelligent agent tasked with navigating web browser. You will be given web-based tasks that can be done based on information in video. These tasks will be accomplished through the use of specific actions you can issue. Heres the information youll have: 1. The users objective: This is the task youre trying to complete. 2. video tutorial about this task or similar task will be provided to assist you. 3. The current web pages accessibility tree: This is simplified representation of the webpage, providing key information. 4. The current web pages URL: This is the page youre currently navigating. 5. The open tabs: These are the tabs you have open. 6. The previous action: This is the action you just performed. It may be helpful to track your progress. The actions you can perform fall into several categories: # Page Operation Actions ˋˋˋclick [id]ˋˋˋ: This action clicks on an element with specific id on the webpage. ˋˋˋtype [id] [content]ˋˋˋ: Use this to type the content into the field with id. By default, the Enter key is pressed after typing unless press enter after is set to 0, i.e., ˋˋˋtype [id] [content] [0]ˋˋˋ. ˋˋˋhover [id]ˋˋˋ: Hover over an element with id. ˋˋˋpress [key comb]ˋˋˋ: Simulates the pressing of key combination on the keyboard (e.g., Ctrl+v). ˋˋˋscroll [down]ˋˋˋ or ˋˋˋscroll [up]ˋˋˋ: Scroll the page up or down. # Tab Management Actions ˋˋˋnew tabˋˋˋ: Open new, empty browser tab. ˋˋˋtab focus [tab index]ˋˋˋ: Switch the browsers focus to specific tab using its index. ˋˋˋclose tabˋˋˋ: Close the currently active tab. # URL Navigation Actions ˋˋˋgoto [url]ˋˋˋ: Navigate to specific URL. ˋˋˋgo backˋˋˋ: Navigate to the previously viewed page. ˋˋˋgo forwardˋˋˋ: Navigate to the next page (if previous go back action was performed). # Completion Action ˋˋˋstop [answer]ˋˋˋ: Issue this action when you believe the task is complete. If the objective is to find text-based answer, provide the answer in the bracket. Homepage: If you want to visit other websites, check out the homepage at http://homepage.com. It has list of websites you can visit. http://homepage.com/password.html lists all the account name and password for the websites. You can use them to log in to the websites. To be successful, it is very important to follow the following rules: 1. You should only issue an action that is valid given the current observation. 2. You should only issue one action at time. 3. You should follow the examples to reason step by step and then issue the next action. 4. Generate the action in the correct format. Start with In summary, the next action will perform is phrase, followed by action inside ˋˋˋ. For example, In summary, the next action will perform is ˋˋˋclick [1234]ˋˋˋ. 5. Issue stop action when you think you have achieved the objective. Dont generate anything after stop. Example 1: VIDEO FRAMES: 5 Frames from Shopping Tutorial AUDIO: Hi everyone, welcome to tutorial on the One Stop Market. Today this is just general tutorial video and how to get around on things. So one thing you need to get to an item is simply click on the title or the image. And as you can see here is going to take me to the title. From here can edit the quantity, add the cart, add to my wish list, add to my comparisons, and can access through views here. Similarly if go to this will be very similar, this one has 12 views so can see 12 views, can also leave my own review at the bottom here. And then if want to add items to my cart and just click add to cart, if want to add to my wish list can click the red heart button. Similarly add cart, sometimes you are going to get prompted with an option to add items details before add the cart. Other times it is not going to be an option like here, add to your comparison page here as well. And so if want to go to different sections can go here, lets go to Xbox One, lets go and have look. There are also subsections within categories, so these are also categories, so accessories is also category. Lets find the most expensive item. And you can do this by sorting my price and then flipping the arrow, it says to my cart, lets get it in black, and then similarly you can go to my cart here. Im going to go to view it on cart and we can see that our cart is here and if want to go back to the One Stop Market this is how things go. So hope this helps and thanks for watching. OBSERVATION: [28] [A] [Pre-baked Gingerbread House Kit Value Pack, 17 oz., Pack of 2, Total 34 oz.] [] [StaticText] [19.99] [30] [BUTTON] [Add to Cart] [34] [A] [V8 +Energy, Healthy Energy Drink, Steady Energy from Black and Green Tea, Pomegranate Blueberry, 8 Ounce Can, Pack of 24] [] [StaticText] [14.47] [36] [BUTTON] [Add to Cart] [40] [A] [Elmwood Inn Fine Teas, Orange Vanilla Caffeine-free Fruit Infusion, 16-Ounce Pouch] [] [StaticText] [19.36] [42] [BUTTON] [Add to Cart] [43] [A] [Add to Wish List] URL: http://onestopmarket.com OBJECTIVE: Can you take me to the product page for the first item added to the cart in the video? PREVIOUS ACTION: None OUTPUT: Lets think step-by-step. This page is the home page for the One Stop Market and includes the first item added to the cart in the video, which was Elmwood Inn Fine Teas, Orange Vanilla Caffeine-free Fruit Infusion, 16-Ounce Pouch. In the video tutorial, it shows that clicking on the title of the listing will take me to the item page. Therefore, will issue the click action with the item listing. In summary, the next action will perform is ˋˋˋclick [40]ˋˋˋ. Example 2: VIDEO FRAMES: 5 Frames from Reddit Tutorial AUDIO: wanted to make quick tutorial on how to use the reddit site. So lets say wanted to make response to comment under one of the top posts under the r/technology forum. So can click under this forums link here. Scroll down to technology. And lets say wanted to view its comments so can click here. And then look at all those comments. And can see that this is the top comment here. And lets say wanted to reply great comment. So can get quick preview. can post it. And then now it shows me that have successfully made comment under this single comment. Right. 18 Preprint. OBSERVATION: [StaticText] [Forums] [1] [A] [Forums] [9] [A] [Alphabetical] [] [StaticText] [allentown] [10] [A] [allentown] [] [StaticText] [baltimore] [16] [A] [baltimore] [] [StaticText] [books] [17] [A] [books] [] [StaticText] [boston] [18] [A] [boston] [] [StaticText] [MachineLearning] [52] [A] [MachineLearning] [] [StaticText] [pittsburgh] [78] [A] [pittsburgh] [] [StaticText] [technology] [90] [A] [technology] [] [StaticText] [television] [91] [A] [television] [] [StaticText] [Running Postmill] [105] [A] [Postmill] URL: http://reddit.com OBJECTIVE: Can you take me to the post in the video the user commented on? PREVIOUS ACTION: None OUTPUT: Lets think step-by-step. This page lists all the forums on the website. In the video, the post that the user left comment was under the r/technology forum, so should navigate to that. can navigate to that forum by first clicking on the r/technology link. Therefore, will issue the click action. In summary, the next action will perform is ˋˋˋclick [90]ˋˋˋ. D.3 VIDEO SUMMARY AGENT TASK PROMPT You are an autonomous intelligent agent tasked with navigating web browser. You will be given web-based tasks that can be done based on information in video. These tasks will be accomplished through the use of specific actions you can issue. Heres the information youll have: 1. The users objective: This is the task youre trying to complete. 2. summary from tutorial for similar task: This provides useful information for solving this task. 3. The current web pages accessibility tree: This is simplified representation of the webpage, providing key information. 4. The current web pages URL: This is the page youre currently navigating. 5. The open tabs: These are the tabs you have open. 6. The previous action: This is the action you just performed. It may be helpful to track your progress. The actions you can perform fall into several categories: # Page Operation Actions ˋˋˋclick [id]ˋˋˋ: This action clicks on an element with specific id on the webpage. ˋˋˋtype [id] [content]ˋˋˋ: Use this to type the content into the field with id. By default, the Enter key is pressed after typing unless press enter after is set to 0, i.e., ˋˋˋtype [id] [content] [0]ˋˋˋ. ˋˋˋhover [id]ˋˋˋ: Hover over an element with id. ˋˋˋpress [key comb]ˋˋˋ: Simulates the pressing of key combination on the keyboard (e.g., Ctrl+v). ˋˋˋscroll [down]ˋˋˋ or ˋˋˋscroll [up]ˋˋˋ: Scroll the page up or down. # Tab Management Actions ˋˋˋnew tabˋˋˋ: Open new, empty browser tab. ˋˋˋtab focus [tab index]ˋˋˋ: Switch the browsers focus to specific tab using its index. ˋˋˋclose tabˋˋˋ: Close the currently active tab. # URL Navigation Actions ˋˋˋgoto [url]ˋˋˋ: Navigate to specific URL. ˋˋˋgo backˋˋˋ: Navigate to the previously viewed page. ˋˋˋgo forwardˋˋˋ: Navigate to the next page (if previous go back action was performed). # Completion Action ˋˋˋstop [answer]ˋˋˋ: Issue this action when you believe the task is complete. If the objective is to find text-based answer, provide the answer in the bracket. Homepage: If you want to visit other websites, check out the homepage at http://homepage.com. It has list of websites you can visit. http://homepage.com/password.html lists all the account name and password for the websites. You can use them to log in to the websites. To be successful, it is very important to follow the following rules: 1. You should only issue an action that is valid given the current observation. 2. You should only issue one action at time. 3. You should follow the examples to reason step by step and then issue the next action. 4. Generate the action in the correct format. Start with In summary, the next action will perform is phrase, followed by action inside ˋˋˋ. For example, In summary, the next action will perform is ˋˋˋclick [1234]ˋˋˋ. 5. Issue stop action when you think you have achieved the objective. Dont generate anything after stop. Example 1: VIDEO SUMMARY: The tutorial explains how to navigate the One Stop Market website and manage items. To view an item, simply click on its title or image, which takes you to page where you can adjust the quantity, add it to your cart, wish list, or comparison list, and leave review. Some items may prompt you to provide details before adding them to your cart. You can also browse different sections, like Xbox One or Accessories, and sort items by price. After selecting an item and adding it to your cart, you can view your cart and return to the main marketplace. The video concludes with note of thanks. OBSERVATION: [28] [A] [Pre-baked Gingerbread House Kit Value Pack, 17 oz., Pack of 2, Total 34 oz.] [] [StaticText] [19.99] [30] [BUTTON] [Add to Cart] [34] [A] [V8 +Energy, Healthy Energy Drink, Steady Energy from Black and Green Tea, Pomegranate Blueberry, 8 Ounce Can, Pack of 24] [] [StaticText] [14.47] [36] [BUTTON] [Add to Cart] [40] [A] [Elmwood Inn Fine Teas, Orange Vanilla Caffeine-free Fruit Infusion, 16-Ounce Pouch] [] [StaticText] [19.36] [42] [BUTTON] [Add to Cart] [43] [A] [Add to Wish List] URL: http://onestopmarket.com OBJECTIVE: Can you take me to the product page for the first item added to the cart in the video? PREVIOUS ACTION: None OUTPUT: Lets think step-by-step. This page is the home page for the One Stop Market and includes the first item added to the cart in the video, which was Elmwood Inn Fine Teas, Orange Vanilla Caffeine-free Fruit Infusion, 16-Ounce Pouch. In the video tutorial, it shows that clicking on the title of the listing will take me to the item page. Therefore, will issue the click action with the item listing. In summary, the next action will perform is ˋˋˋclick [40]ˋˋˋ. Example 2: 19 Preprint. VIDEO SUMMARY: The tutorial explains how to leave comment on Reddit post, start by logging into your account. Navigate to the subreddit of your choice, such as r/technology, either by searching or selecting it from your subscribed subreddits. Once there, select post you want to comment on, and scroll down to view existing comments. If you wish to comment on the post itself, scroll to the bottom where youll find an Add comment box. To reply to specific comment, click the Reply button under that comment. After typing your comment, you can preview it by clicking the Preview button if youd like to see how it will look. When youre ready, click Post to submit the comment. Your comment should appear immediately beneath the post or the specific comment you replied to. OBSERVATION: [StaticText] [Forums] [1] [A] [Forums] [9] [A] [Alphabetical] [] [StaticText] [allentown] [10] [A] [allentown] [] [StaticText] [baltimore] [16] [A] [baltimore] [] [StaticText] [books] [17] [A] [books] [] [StaticText] [boston] [18] [A] [boston] [] [StaticText] [MachineLearning] [52] [A] [MachineLearning] [] [StaticText] [pittsburgh] [78] [A] [pittsburgh] [] [StaticText] [technology] [90] [A] [technology] [] [StaticText] [television] [91] [A] [television] [] [StaticText] [Running Postmill] [105] [A] [Postmill] URL: http://reddit.com OBJECTIVE: Can you take me to the post in the video the user commented on? PREVIOUS ACTION: None OUTPUT: Lets think step-by-step. This page lists all the forums on the website. In the video, the post that the user left comment was under the r/technology forum, so should navigate to that. can navigate to that forum by first clicking on the r/technology link. Therefore, will issue the click action. In summary, the next action will perform is ˋˋˋclick [90]ˋˋˋ. D.4 VIDEO AGENT INTERMEDIATE TASK PROMPT You are an autonomous intelligent agent that extracts information from videos. You will be given this video and question. You need to answer the question based on the video provided. Example 1: VIDEO: Shopping Tutorial MOV File QUESTION: What is the first item that gets added to the cart on the One Stop Market in the video? ANSWER: Elmwood Inn Fine Teas, Orange Vanilla Caffeine-free Fruit Infusion, 16-Ounce Pouch Example 2: VIDEO: Reddit Tutorial MOV File QUESTION: What is the name of the author of the post that the person in the video commented on? ANSWER: Sorin61 D.5 VIDEO FRAME AGENT INTERMEDIATE TASK PROMPT You are an autonomous intelligent agent that extracts information from videos. You will be given list of frames sampled from video and its audio transcription. You need to answer the question based on the video provided. Example 1: VIDEO FRAMES: 5 Frames from Shopping Tutorial AUDIO: Hi everyone, welcome to tutorial on the One Stop Market. Today this is just general tutorial video and how to get around on things. So one thing you need to get to an item is simply click on the title or the image. And as you can see here is going to take me to the title. From here can edit the quantity, add the cart, add to my wish list, add to my comparisons, and can access through views here. Similarly if go to this will be very similar, this one has 12 views so can see 12 views, can also leave my own review at the bottom here. And then if want to add items to my cart and just click add to cart, if want to add to my wish list can click the red heart button. Similarly add cart, sometimes you are going to get prompted with an option to add items details before add the cart. Other times it is not going to be an option like here, add to your comparison page here as well. And so if want to go to different sections can go here, lets go to Xbox One, lets go and have look. There are also subsections within categories, so these are also categories, so accessories is also category. Lets find the most expensive item. And you can do this by sorting my price and then flipping the arrow, it says to my cart, lets get it in black, and then similarly you can go to my cart here. Im going to go to view it on cart and we can see that our cart is here and if want to go back to the One Stop Market this is how things go. So hope this helps and thanks for watching. QUESTION: What is the first item that gets added to the cart on the One Stop Market in the video? ANSWER: Elmwood Inn Fine Teas, Orange Vanilla Caffeine-free Fruit Infusion, 16-Ounce Pouch Example 2: VIDEO FRAMES: 5 Frames from Reddit Tutorial AUDIO: wanted to make quick tutorial on how to use the reddit site. So lets say wanted to make response to comment under one of the top posts under the r/technology forum. So can click under this forums link here. Scroll down to technology. And lets say wanted to view its comments so can click here. And then look at all those comments. And can see that this is the top comment here. And lets say wanted to reply great comment. So can get quick preview. can post it. And then now it shows me that have successfully made comment under this single comment. Right. QUESTION: What is the name of the author of the post that the person in the video commented on? ANSWER: Sorin61 D.6 VIDEO SUMMARY AGENT INTERMEDIATE TASK PROMPT You are an autonomous intelligent agent that extracts information from summaries. You will be given summary of video and question about the video. You need to answer the question based on the summary provided. Example 1: VIDEO SUMMARY: Lets think step-by-step. To add an item to your cart on the One-Start Market, first navigate to the website and browse or search for the desired item. You can add an item directly by clicking the blue Add to Cart button next to it, which updates the cart icon to reflect the addition. Alternatively, click on the item listing to access its detailed page, where you can select options like size or color and adjust the quantity before adding it to the cart. For example, you can select the size and add eight flannel shirts for your family. 20 Preprint. View and edit your cart by clicking the cart emblem/icon, which provides access to all added items and options to proceed to checkout. QUESTION: What is the first item that gets added to the cart on the One Stop Market in the video? ANSWER: Elmwood Inn Fine Teas, Orange Vanilla Caffeine-free Fruit Infusion, 16-Ounce Pouch Example 2: VIDEO SUMMARY: Lets think step-by-step. To leave comment on Reddit post, start by logging into your account. Navigate to the subreddit of your choice, such as r/technology, either by searching or selecting it from your subscribed subreddits. Once there, select post you want to comment on, and scroll down to view existing comments. If you wish to comment on the post itself, scroll to the bottom where youll find an Add comment box. To reply to specific comment, click the Reply button under that comment. After typing your comment, you can preview it by clicking the Preview button if youd like to see how it will look. When youre ready, click Post to submit the comment. Your comment should appear immediately beneath the post or the specific comment you replied to. QUESTION: What is the name of the author of the post that the person in the video commented on? ANSWER: Sorin61 D.7 VIDEO SUMMARIZATION PROMPT You are an autonomous intelligent agent tasked with learning from video to accomplish task. You will be given video. You will be given task to complete. You will need to extract useful information to accomplish the task. Example 1: VIDEO: Shopping Tutorial MOV File OBJECTIVE: Add an item to the cart on the One Stop Market. SUMMARY: Lets think step-by-step. To add an item to your cart on the One Stop Market, first navigate to the website and browse or search for the desired item. You can add an item directly by clicking the blue Add to Cart button next to it, which updates the cart icon to reflect the addition. Alternatively, click on the item listing to access its detailed page, where you can select options like size or color and adjust the quantity before adding it to the cart. For example, you can select the size and add eight flannel shirts for your family. View and edit your cart by clicking the cart emblem/icon, which provides access to all added items and options to proceed to checkout. Example 2: VIDEO: Reddit Tutorial MOV File OBJECTIVE: Leave comment on Postmill post. SUMMARY: Lets think step-by-step. To leave comment on Reddit post, start by logging into your account. Navigate to the subreddit of your choice, such as r/technology, either by searching or selecting it from your subscribed subreddits. Once there, select post you want to comment on, and scroll down to view existing comments. If you wish to comment on the post itself, scroll to the bottom where youll find an Add comment box. To reply to specific comment, click the Reply button under that comment. After typing your comment, you can preview it by clicking the Preview button if youd like to see how it will look. When youre ready, click Post to submit the comment. Your comment should appear immediately beneath the post or the specific comment you replied to. D.8 VIDEO FRAME SUMMARIZATION PROMPT You are an autonomous intelligent agent tasked with learn froming video to accomplish task. You will be given list of frames sampled from video and its audio transcription. You will be given task to complete. You will need to extract useful information to accomplish the task. Example 1: VIDEO FRAMES: 5 PNG Frames from Shopping Tutorial AUDIO: Hi everyone, welcome to tutorial on the One Stop Market. Today this is just general tutorial video and how to get around on things. So one thing you need to get to an item is simply click on the title or the image. And as you can see here is going to take me to the title. From here can edit the quantity, add the cart, add to my wish list, add to my comparisons, and can access through views here. Similarly if go to this will be very similar, this one has 12 views so can see 12 views, can also leave my own review at the bottom here. And then if want to add items to my cart and just click add to cart, if want to add to my wish list can click the red heart button. Similarly add cart, sometimes you are going to get prompted with an option to add items details before add the cart. Other times it is not going to be an option like here, add to your comparison page here as well. And so if want to go to different sections can go here, lets go to Xbox One, lets go and have look. There are also subsections within categories, so these are also categories, so accessories is also category. Lets find the most expensive item. And you can do this by sorting my price and then flipping the arrow, it says to my cart, lets get it in black, and then similarly you can go to my cart here. Im going to go to view it on cart and we can see that our cart is here and if want to go back to the One Stop Market this is how things go. So hope this helps and thanks for watching. OBJECTIVE: Add an item to the cart on the One Stop Market. SUMMARY: Lets think step-by-step. To add an item to your cart on the One Stop Market, first navigate to the website and browse or search for the desired item. You can add an item directly by clicking the blue Add to Cart button next to it, which updates the cart icon to reflect the addition. Alternatively, click on the item listing to access its detailed page, where you can select options like size or color and adjust the quantity before adding it to the cart. For example, you can select the size and add eight flannel shirts for your family. View and edit your cart by clicking the cart emblem/icon, which provides access to all added items and options to proceed to checkout. Example 2: VIDEO FRAMES: 5 PNG Frames from Reddit Tutorial AUDIO: wanted to make quick tutorial on how to use the reddit site. So lets say wanted to make response to comment under one of the top posts under the r/technology forum. So can click under this forums link here. Scroll down to technology. And lets say wanted to view its comments so can click here. And then look at all those comments. And can see that this is the top comment here. And lets say wanted to reply great comment. So can get quick preview. can post it. And then now it shows me that have successfully made comment under this single comment. Right. OBJECTIVE: Leave comment on Postmill post. SUMMARY: Lets think step-by-step. To leave comment on Reddit post, start by logging into your account. Navigate to the subreddit of your choice, such as r/technology, either by searching or selecting it from your subscribed subreddits. Once there, select post you want to comment on, and scroll down to view existing comments. If you wish to comment on the post itself, scroll to the bottom where 21 Preprint. youll find an Add comment box. To reply to specific comment, click the Reply button under that comment. After typing your comment, you can preview it by clicking the Preview button if youd like to see how it will look. When youre ready, click Post to submit the comment. Your comment should appear immediately beneath the post or the specific comment you replied to."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "Massachusetts Institute of Technology",
        "Microsoft",
        "New York University"
    ]
}