{
    "paper_title": "SWEET-RL: Training Multi-Turn LLM Agents on Collaborative Reasoning Tasks",
    "authors": [
        "Yifei Zhou",
        "Song Jiang",
        "Yuandong Tian",
        "Jason Weston",
        "Sergey Levine",
        "Sainbayar Sukhbaatar",
        "Xian Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language model (LLM) agents need to perform multi-turn interactions in real-world tasks. However, existing multi-turn RL algorithms for optimizing LLM agents fail to perform effective credit assignment over multiple turns while leveraging the generalization capabilities of LLMs and it remains unclear how to develop such algorithms. To study this, we first introduce a new benchmark, ColBench, where an LLM agent interacts with a human collaborator over multiple turns to solve realistic tasks in backend programming and frontend design. Building on this benchmark, we propose a novel RL algorithm, SWEET-RL (RL with Step-WisE Evaluation from Training-time information), that uses a carefully designed optimization objective to train a critic model with access to additional training-time information. The critic provides step-level rewards for improving the policy model. Our experiments demonstrate that SWEET-RL achieves a 6% absolute improvement in success and win rates on ColBench compared to other state-of-the-art multi-turn RL algorithms, enabling Llama-3.1-8B to match or exceed the performance of GPT4-o in realistic collaborative content creation."
        },
        {
            "title": "Start",
            "content": "SWEET-RL: Training Multi-Turn LLM Agents on Collaborative Reasoning Tasks Yifei Zhou1,2,, Song Jiang1, Yuandong Tian1, Jason Weston1, Sergey Levine2, Sainbayar Sukhbaatar1,, Xian Li1, 1FAIR at Meta, 2UC Berkeley Work done at Meta, Equal advising Large language model (LLM) agents need to perform multi-turn interactions in real-world tasks. However, existing multi-turn RL algorithms for optimizing LLM agents fail to perform effective credit assignment over multiple turns while leveraging the generalization capabilities of LLMs and it remains unclear how to develop such algorithms. To study this, we first introduce new benchmark, ColBench, where an LLM agent interacts with human collaborator over multiple turns to solve realistic tasks in backend programming and frontend design. Building on this benchmark, we propose novel RL algorithm, SWEET-RL (RL with Step-WisE Evaluation from Training-time information), that uses carefully designed optimization objective to train critic model with access to additional training-time information. The critic provides step-level rewards for improving the policy model. Our experiments demonstrate that SWEET-RL achieves 6% absolute improvement in success and win rates on ColBench compared to other state-of-the-art multi-turn RL algorithms, enabling Llama-3.1-8B to match or exceed the performance of GPT4-o in realistic collaborative content creation. Date: March 20, 2025 Correspondence: Yifei Zhou at yifei_zhou@berkeley Code: https://github.com/facebookresearch/sweet_rl Data: https://huggingface.co/datasets/facebook/collaborative_agent_bench"
        },
        {
            "title": "1 Introduction",
            "content": "5 2 0 2 9 1 ] . [ 1 8 7 4 5 1 . 3 0 5 2 : r Figure 1 (Left) Overview of our ColBench including Backend Programming and Frontend Design that supports cheap and reliable evaluation of multi-turn RL algorithms for agents in realistic settings. (Right) The high-level motivation behind SWEET-RL that uses additional training-time information along with appropriate Bradley-Terry (BT) objective to perform effective credit assignment. Large language models (LLMs) have the potential of serving as decision-making agents capable of executing complex tasks autonomously, such as navigating the web and controling devices (Zhou et al., 2024a,b; Xu 1 et al., 2024; Gur et al., 2021; Bai et al., 2024), writing and maintaining code bases (Jimenez et al., 2024), and serving as personal assistants (Xie et al., 2024a; Jiang et al., 2024), thanks to advances in the reasoning and generalization capabilities of LLMs (OpenAI, 2024; GeminiTeam, 2024; Llama3Team, 2024). However, to achieve the best performance on tasks that involve making sequence of decisions, the agent needs to directly optimize for the multi-turn objective of interest such as success rates, which is more challenging than only imitating the most probable action at each turn as learnt in the next-token-prediction pre-training objective. While natural approach to directly optimize for multi-turn objective is to apply successful algorithms from single-turn Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022; Ziegler et al., 2020; Christiano et al., 2023), such as RAFT (Dong et al., 2023), DPO (Rafailov et al., 2024b) and PPO (Schulman et al., 2017), these methods do not perform explicit credit assignment across turns. Consequently, they may suffer from high variance and poor sample complexity due to the long-horizon nature of complex sequential decision-making tasks (Zhou et al., 2024c). Another alternative is to apply value function learning methods, such as TD-learning (Mnih et al., 2013; Zhou et al., 2024c; Snell et al., 2023). Yet that requires training new task-specific value head on top of LLM representations, which may not generalize well with limited fine-tuning data (Figure 3). As result, it is still unclear what is the most effective multi-turn RL algorithm that fully takes advantage of the reasoning capabilities of LLMs for training general, capable, and goal-directed agents. To begin to address this challenge, we first notice that benchmark for validating multi-turn RL algorithms for realistic LLM agents is required to meet the following three criteria: 1) sufficient task diversity for RL training without overfitting, 2) sufficient task complexity that challenges the reasoning and generalization capability of agents, and 3) minimum engineering overhead for fast research prototyping. However, as shown in Table 1, none of the existing benchmarks satisfy all of the three necessary criteria. To address this gap, our first contribution in this work is to develop benchmark, Collaborative Agent Benchmark (ColBench), designed to support research on multi-turn RL algorithms for realistic LLM agent scenarios. Our benchmark focuses on the realistic domain of artifact creation, where the goal for agents is to interact with humans to produce final artifact (e.g., code, web pages, or slides) that fulfills human expectations. To solve such tasks, the agent must act to understand the intent of the human collaborator and reason with the missing pieces, as only limited information is provided in each turn for complex and potentially multi-modal artifacts like code and web pages. To facilitate rapid iteration and cost-effective evaluation, we employ LLMs as human simulators, where we crucially also provide the ground-truth artifacts to ensure faithful simulations in their responses. For reliable evaluations, we have developed series of functional evaluators that measure the similarity between the agent-produced artifact and the ground-truth. Examples of tasks in ColBench are shown in Figure 1(left) and full trajectories in Appendix E. For multi-turn RL algorithm to perform effective credit assignments in such LLM agent settings, it needs to incorporate solutions to the following realistic challenges. Firstly, the agent is acting in partially observable environment where some of the important task-relevant information is not directly revealed to the agent. In such cases, the agent needs to be properly awarded for information-seeking behaviors in the stochastic environment. Moreover, there is only limited amount of data available during fine-tuning for large diverse set of tasks that may show up at test time. Therefore, the learning objective of the algorithm needs to effectively take advantage of the reasoning capabilities of LLMs for the best generalization performance. For the first challenge, we observe that additional training-time information, such as the final outcome and the reference solution, may be available during training. The knowledge of additional training-time information can offer shortcut for credit assignments for an agent performing information-seeking behaviors without such knowledge. natural approach to leveraging this training-time information is to train value function that predicts the expected utility of each action as scalar value. However, this introduces fine-tuning objective that significantly differs from the next-token prediction pre-training objective of LLMs, which leads to inferior reasoning and generalization performance (Figure 3). With these observations, the second contribution of this work is an easy-to-implement yet highly effective RL algorithm, SWEET-RL (RL with Step-WisE Evaluation from Training-Time Information) as depicted in Figure 1(right). SWEET-RL improves credit assignments by providing the critic with training-time information that is inaccessible to the actor. Our novel turn-level critic takes advantage of this asymmetric observation spaces for the critic and actor. Furthermore, we propose directly learning the advantage function, which characterizes the effectiveness of each action at the current state, avoiding the need of first training value function that predicts the expected utility of the current state and action. Finally, we also propose 2 Table 1 Comparisons between ColBench and existing benchmarks for multi-turn LLM agents. As shown in the table, no existing benchmarks satisfy all of the three criterions necessary for developing efficient RL algorithms for fine-tuning LLM agents: 1) sufficient task diversity for RL training without overfitting, 2) sufficient task complexity that challenges the reasoning and generalization capability of agents, and 3) minimum engineering overhead for fast research prototyping. RL Complex Training Reasoning Overhead Min Web/Device-Control Agents (Zhou et al., 2024a; Xie et al., 2024b) SWEBench (Jimenez et al., 2024; Pan et al., 2024) Travel Planner (Xie et al., 2024a) LLF Benchmark (Cheng et al., 2023) AgentBench (Liu et al., 2023) Mint (Wang et al., 2024b) Dialop (Lin et al., 2024) LMRL Gym (Abdulhai et al., 2023) RL4VLM (Zhai et al., 2024) ColBench (ours) Yes Yes No No No No No Yes Yes Yes Yes Yes Yes No Yes Yes Yes No No Yes No No Yes Yes Yes Yes Yes Yes Yes Yes parameterizing the advantage function by the mean log probability of the action at each turn and training this advantage function through the Bradley-Terry objective at the trajectory level. We find such an objective aligns better with pre-trained LLMs compared to the common practice of training value head on top of the hidden states of LLMs, leading to superior generalization results. In our experiments, we find that the use of asymmetric information during training and appropriate learning objectives result in superior multi-turn agent on both realistic Backend Programming and Frontend Design tasks from ColBench, with 6% absolute success and win rates gains compared to other SOTA algorithms. As result, the performance of Llama-3.1-8B (Llama3Team, 2024) can match or even surpass the performance of SOTA proprietary models including GPT-4o and o1-mini (OpenAI, 2024)."
        },
        {
            "title": "2 Related Work",
            "content": "Benchmarks for LLM Agents. While many recent benchmarks have been proposed to evaluate the capabilities of LLM agents in various settings, such as software engineering (Jimenez et al., 2024; Liu et al., 2023), web navigation (Zhou et al., 2024a; Koh et al., 2024; Deng et al., 2023; Yao et al., 2023), device control (Rawles et al., 2023, 2024; Xie et al., 2024b), and travel planning (Xie et al., 2024a), most of them tend to focus on evaluation of state-of-the-art generalist LLMs without providing research-friendly interactive environment and set of training tasks to study multi-turn RL algorithm. While LMRL Gym (Abdulhai et al., 2023) and RL4VLM (Zhai et al., 2024) offers this flexibility for comparing different multi-turn RL algorithms, the task settings focus on narrower domains and do not require the model to have strong reasoning capabilities. As shown in Table 1, there is no existing LLM agent benchmark that provides the flexibility for testing multi-turn RL algorithms on reasoning-intensive tasks with minimum engineering overhead. In contrast, ColBench is the first benchmark designed to support research efforts in multi-turn RL algorithms on reasoning-intensive tasks, focusing on the realistic domain of artifact creation with reliable functional verifiers. Multi-turn RL algorithms for LLM Agents. Unlike single-turn scenarios such as single-turn preference optimization (Christiano et al., 2023; Ziegler et al., 2020; Casper et al., 2023; Xu et al., 2023) where it suffices for LLMs to produce single response without further interactions with the environment, multi-turn RL (Zhou et al., 2024c; Kumar et al., 2024; Abdulhai et al., 2023) captures realistic agent scenarios where LLM agents need to make sequence of actions to complete the task, such as operating unix terminal (Liu et al., 2023) and navigating through the web (Zhou et al., 2024a). While some early works directly applied successful methods from single-turn RL, such as REINFORCE (Sutton et al., 1999; Wu and Hu, 2018), DPO (Xiong et al., 2024; Song et al., 2024), and PPO (Schulman et al., 2017; Szot et al., 2024), they often suffer from high variance when the horizon gets longer, resulting in poor performance. While recent works have applied more advanced techniques from the deep RL literature such as Bellman bootstrapping (Zhou et al., 2024c; Snell et al., 2023) and Path Consistency (Wang et al., 2024a; Liu et al., 2024; Nachum et al., 2017) to reduce 3 long-horizon variance, our work makes an important advancement to take advantage of the oft-neglected additional training-time information and make corresponding adjustments to the optimization objective for improved assignment. Finally, while some prior works apply an asymmetric actor-critic structure to perform sim-to-real transfer in robotics where the critic observes the latent state and the actor observes RGB inputs (Pinto et al., 2017; Wilson and Hermans, 2020; Salter et al., 2019), less has been studied in terms of how such techniques can be applied in reasoning-intensive LLM tasks. Process reward models. The use of step-wise critic resembles the notion of process reward model (PRM) in the reasoning literature (Lightman et al., 2023; Uesato et al., 2022). PRMs evaluate the correctness of each reasoning step and can be trained from automated supervision (Luo et al., 2024; Setlur et al., 2024; Yuan et al., 2024; Hwang et al., 2024) without costly human-annotated process labels. Once PRM is trained, it can be used for searching with more test-time compute (Snell et al., 2024; Yuan et al., 2024) or accelerating the explorations in on-policy RL (Setlur et al., 2024; Shao et al., 2024; Lin et al., 2025) (i.e. when the policy is trained on the online collected trajectories by itself). In contrast, in our work, the step-wise critic is mainly used to perform credit assignment as an intermediate reward proxy to directly optimize the policy without the need to collect additional interaction data. This benefit is important in LLM agent tasks where collecting on-policy data involves expensive interactions with an external environment."
        },
        {
            "title": "3 Collaborative Agent Benchmark (ColBench)",
            "content": "In this section, we first outline the foundational design principles of our ColBench to address challenges in developing multi-turn RL algorithms for LLM agents, followed by detailed explanation of the two specific tasks: Backend Programming and Frontend Design."
        },
        {
            "title": "3.1 Design Principles\n(1) Sufficient task complexity that challenges the rea- soning and generalization capability of agents. As the\nultimate goal is to enable the LLM agents to complete tasks on our behalf in the real world, where the agents\nneed to address complicated challenges in out-of-distribution scenarios, it is essential that the benchmark\nreflects such realistic reasoning and generalization challenges. To achieve this, ColBench is designed to align\nwith realistic artifact creation tasks, where the objective of collaboration is to produce tangible outputs such\nas code or web pages. In order to do well, it is necessary for agents to dive deep into the structure of the code\nand nuanced differences in visual design for unseen requests and a potentially sub-optimal collaborator.",
            "content": "(2) Minimum overhead for fast research prototyping. To achieve this, we ground each collaboration task with the goal to re-create the exact same product as the reference artifact. In this way, the human collaborator can be easily simulated by an LLM with access to the reference artifact to faithfully answer the uncertainties from the LLM agent. Furthermore, the presence of reference artifact allows ColBench to employ objective and functional evaluation metrics that assess the similarity between the final collaborative product and the reference artifact. As result, the only requirement needed for setting up ColBench is API access for some LLM calls and some Python packages for running code and rendering HTML web pages. Although in real-world scenarios the human collaborator might only have general idea of the desired final product, having access to clear reference artifact is reasonable assumption that significantly enhances reliability. (3) Sufficient task diversity for RL training without overfitting. As the best LLMs are often trained on huge amount of data, it is essential that our benchmark contains enough tasks to understand the scalability of different multi-turn RL algorithms while at the same time ensuring the reliability of simulation and evaluation. Therefore, we designed ColBench to be highly scalable, with more than 10k different procedurally-generated tasks, which also can be easily expanded as needed by incorporating more existing artifacts such as code and web pages. The difficulty of ColBench can be easily adjusted by simply creating more collaboration tasks with more complicated code bases and web pages. Next, we will describe detailed setups of ColBench, including Backend Programming and Frontend Design."
        },
        {
            "title": "3.2 Backend Programming Collaborations\nTask description. In this task, the agent is required to collaborate with the human simulator to write a custom\nPython function (up to 50 lines). In the beginning of the collaboration, the agent is provided with a high-level",
            "content": "4 description and the signature of the function. However, many concrete details, such as what conditions should be considered and what to do at edge cases, are not provided, and the agent has to reason and decide what clarifications are needed from the human simulator. The human simulator will provide brief explanation in natural language to each clarification question based on the reference code visible only to the human simulator, but it will not write code. The interactions are limited to 10 back-and-forth rounds between the agent and the human simulator. The interaction ends either when the agent has decided to give the final solution or the maximum number of rounds has been reached. The success of the agent is evaluated by 10 hidden unit tests for each function for 0/1 reward only at the end of each collaboration. Data generation. Python functions, high-level descriptions, and unit tests are generated by prompting Llama3.1-70B-Instruct (Llama3Team, 2024) to extract python function as inspired by an Internet excerpt from DCLM (Li et al., 2024). Only the tasks where the generated python functions can pass their corresponding unit tests are kept. We generate 10k such tasks in the train set and 1k tasks in the test set where tasks in the test set are manually inspected by authors. 15k offline train trajectories are generated by zero-shot prompting Llama-3.1-8B-Instruct as agent and Llama-3.1-70B-Instruct as human simulator."
        },
        {
            "title": "3.3 Frontend Design Collaborations\nTask description. In this task, the agent is required to collaborate with the human simulator to design a web\npage by writing an HTML snippet (around 100 lines). At the beginning of the collaboration, the agent is\nprovided with a high-level description of the web page. Again, many concrete details such as the layout and\ncolor palette of the web page are missing and only visible to the human simulator. At each round, the agent\nhas a chance to write an HTML solution and it will be rendered by the web browser. The human simulator\nwill be able to examine the rendered web page from the agent and the reference web page, then describe their\ndifferences to the agent. Similar to the backend programming collaboration, the interaction ends either when\nthe agent has decided to give the final solution or the maximum number of 10 rounds has been reached. The\nperformance of the agent is evaluated by the cosine similarity of CLIP (Radford et al., 2021) embeddings\nbetween the agent solution and reference web page, the best metric found in prior works (Si et al., 2024) for\nfrontend design. It serves as a reward within the range of 0 to 1 only at the end of the collaboration.",
            "content": "Data generation. The tasks containing reference web pages and high-level descriptions are from WebSight (Laurençon et al., 2024). We include 10k such tasks for training and 500 for the test set. The test set tasks are manually inspected by the authors. We generate 6k offline train trajectories by zero-shot prompting Llama-3.1-8B as agent and Qwen2-VL-72B (Yang et al., 2024) as human simulator."
        },
        {
            "title": "4 SWEET-RL: RL with Step-WisE Evaluation from Training-Time Information",
            "content": "To introduce our method, we will begin by first defining the terminology for multi-turn RL on ColBench. Then, we will motivate the two-stage training procedure of SWEET-RL for first training step-wise critic with additional training-time information and using it as per-step reward model to train the actor with careful algorithmic choices. An overview of the two-stage training procedure is presented in Figure 2. Figure 2 textbfAn overview of the training procedure of SWEET-RL. At high level, we first apply Bradley-Terry objective to directly train step-wise advantage function with access to additional training-time information. Once the advantage function is trained, we perform policy improvement by using the advantage function as reward model for each turn."
        },
        {
            "title": "4.1 Problem Setup\nWe frame the problem of collaboration between humans and agents as a finite-horizon partially observable\nMarkov decision process (POMDP) M = {O, C, A, T , µ1, R, N }. Here, O and C are the observable and hidden\nparts of the state space. In the beginning of each episode an initial instruction o1 ∈ O and some hidden\ntraining-time information c ∈ C (e.g., the reference solution) are drawn from the initial state distribution µ1.\nThe hidden training-time information c remains unchanged during the episode.",
            "content": "At the t-th turn, the observation ot for the agent includes all the interaction history, and the agent needs to take an action at by outputting response consisting of tokens a1:L . After taking an action, the user will respond to the agent, and the new state for the agent (represented by the transition function ) is derived by appending the latest interaction to the interaction history. At each step, the agent has chance to receive scalar reward r(ot, at, c) R. The episode ends either when the agent outputs termination token or when maximum number of = rounds of interactions has been reached. The objective of RL is to train policy that can generate token sequences maximizing the cumulative rewards (cid:80)N t=1 r(ot, at, c) throughout rollout (we assume no reward decay for simplicity). We consider the offline setting of learning from past interaction dataset as online interactions with humans may be costly to obtain. The Q-function for policy π represents the expected cumulative reward of specific action at the current step, (cid:104)(cid:80)N followed by adherence to policy π: Qπ(ot, at, c) = Eπ . The value function of π, π(ot, c), is defined as the expected Q-value, Eatπ[Qπ(ot, at, c)], where actions at are sampled from π. The advantage function Aπ(ot, at, c) indicates the relative benefit of taking action at in state (ot, c) and is calculated as the difference between the Q-value and the states value under the policy: Aπ(ot, at, c) = Qπ(ot, at, c) π(ot, c). We directly model the turn-wise advantage function using parameters θ, and use that advantage Aθ to train the policy parameterized by ϕ to generate tokens a1:L (cid:105) t=t r(ot, at, c) within each turn."
        },
        {
            "title": "4.2 Learning turn-wise advantage functions\nTo perform explicit credit assignments in reasoning-intensive tasks, some prior works have explored learning\nan explicit value function first and derive the advantage of each individual action from the learnt value\nfunction (Bai et al., 2024; Zhou et al., 2024c; Snell et al., 2023). However, in our experiments we found\nthat such value functions do not generalize well when only a limited number of samples for fine-tuning are\navailable as shown in subsection 5.4. We hypothesize this is because learning an accurate value function in\nreasoning-intensive tasks is itself a hard task and does not effectively take advantage of the reasoning and\ngeneralization capability of pre-trained LLMs.",
            "content": "Since the ultimate goal of performing credit assignment is to derive the advantages for each action which may be easier task for LLMs compared to estimating the expected future returns, we propose to directly learn the advantage function for each turn-wise action at. Inspired by the success of preference optimization in funetuning LLMs (Christiano et al., 2023; Ziegler et al., 2020), we propose to train the the turn-wise advantage function from preference pairs of trajectories. Given two trajectories under the same task with additional training-time information c, we label them chosen τ + and rejected τ judged by their cumulative rewards. This allows us employ the Bradley-Terry objective (Bradley and Terry, 1952; Rafailov et al., 2024a) for fine-tuning (cid:34) JBT = log σ (cid:32) (cid:88) βr(o+ , a+ , c) βr(o , t , c) (cid:88) (cid:33)(cid:35) , (1) where o+ objective using the advantage function: and , , a+ are from τ + and τ respectively, and β is hyperparameter. We can rewrite this (cid:34) JA(θ) = log σ (cid:32) (cid:88) βAθ(o+ , a+ , c) βAθ(o , , c) (cid:88) (cid:33)(cid:35) . (2) Intuitively, similar to the objective of single-turn RLHF to learn high reward for each chosen response and low reward for each rejected response, the effect of Equation 2 is to increase the advantage for each action in the chosen trajectory and lower the advantage for each action in the rejected trajectory. For completeness, we provide theoretical derivation in Appendix B. To further align the learning objective with next-token-prediction pre-training, we parameterize the advantage function by re-purposing the existing language model head of the LLM: Aθ(ot, at, h) ="
        },
        {
            "title": "1\nL",
            "content": "(cid:34) (cid:88) l=1 log πθ(al πref(al tot, a1:l1 tot, a1:l1 , c) , c) (cid:35) (3) where πθ is the LLM model that we train to act as the advantage function, while πref is frozen initial seed model. We find it important to include 1 to normalize the advantage by the length of the response to stabilize training."
        },
        {
            "title": "4.3 Optimizing the agent by turn-wise advantage\nOur key observation is that while our final policy πϕ cannot condition on the hidden information h, such\ninformation is available during training time. Since the advantage LLM πθ will only be used during training,\nit can take c as input as in Equation 3. Intuitively, many realistic problems such as collaboration and math\nreasoning have some hidden training-time information like reference solutions. If the turn-wise advantage\nfunction has access to such training-time information, it should be in a better position to judge whether the\naction taken by the policy is on the right track.",
            "content": "Therefore, we provide additional training-time information to the turn-wise advantage function while only the interaction history ot is provided to the policy, resulting in an asymmetric actor-critic structure. In principle, any successful algorithm from the RLHF literature can be used to optimize the per-turn policy πϕ by treating the interaction histories as prompts and the turn-wise advantage function Aθ as the reward model. In this stage of training the policy, no interaction from human collaborators is needed. For simplicity we choose to use DPO (Rafailov et al., 2024b) for training. For each turn we first sample candidate actions from the current policy given interaction history ot, and rank them by the learnt turn-wise advantage function to obtain chosen and rejected actions. We then optimize the policy for each turn using the standard DPO loss: Jπ(ϕ) = log σ (cid:18) β log πϕ(a+ot) log πref(a+ot) β log πϕ(aot) log πref(aot) (cid:19) . (4) In practice, for each turn we sample 16 candidate actions and take random actions from top-50% quantile as chosen and from the bottom-50% quantile as rejected."
        },
        {
            "title": "5 Experiments",
            "content": "The purpose of our experiments is to validate the effectiveness of SWEET-RL as multi-turn RL algorithm that trains LLM agents for complex collaborative tasks. Specifically, they are designed to answer the following questions: (1) How do existing generalist models and multi-turn RL algorithms perform for collaborative tasks on ColBench? (2) How does SWEET-RLs performance compare with other SOTA multi-turn RL algorithms for training LLM agents on reasoning-heavy tasks? (3) How does the use of asymmetric information help with credit assignments? (4) What are the best algorithmic choices for effectively taking advantage of LLMs reasoning and generalization capability to perform credit assignments? (5) How does SWEET-RL scale as the number of training samples increase compared to baselines?"
        },
        {
            "title": "5.1 Experimental Setup\nBaseline comparisons. We compare SWEET-RL with a variety of SOTA LLMs and multi-turn RL algorithms\non ColBench. We consider Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct (Llama3Team, 2024) as represen-\ntatives of SOTA open-source LLMs and GPT4-O and O1-Mini as representatives of SOTA proprietary LLMs.\nWe test these models in both a single-turn and a collaborative setting to understand how LLM agents can\nbenefit from multi-turn collaborative interactions on ColBench. We compare different RL algorithms based\non Llama-3.1-8B-Instruct. We first consider a simple yet effective baseline Rejection Fine-Tuning widely used\nfor LLM agent fine-tuning (Zhou et al., 2024b; Dong et al., 2023), where Supervised Fine-Tuning (SFT) is",
            "content": "7 Table 2 Comparisons of different LLMs and multi-turn RL algorithms on ColBench. SWEET-RL is able to achieve more than 6% performance gain over other multi-turn RL algorithms, enabling Llama-3.1-8B-Instruct to be on par with larger proprietary models. Backend Programming Frontend Design % Tests Passed Success Rate Cosine Similarity Win Rate Single-Turn SOTA LLMs Llama-3.1-8B-Instruct Llama-3.1-8B-Instruct Llama-3.1-70B-Instruct O1-Mini GPT-4O Llama-3.1-70B-Instruct GPT-4O O1-Mini Zero-Shot Rejection Fine-Tuning Multi-Turn DPO SWEET-RL (ours) 11.8 24.2 22.4 27. 48.0 54.6 43.2 34.2 40.9 48.0 56.8 6.9 14.8 13.1 16.2 35.0 40.4 30.3 22.4 28.2 34.4 40.4 63.1 61.8 70.2 68. 73.7 78.1 77.5 72.4 75.2 76.9 77.7 13.6 13.2 23.8 23.8 39.8 50.0 47.2 33.8 38.6 42.8 48.2 performed on successful trajectories to minimize the negative log-likelihood loss. Furthermore, we consider recent effective baseline Multi-Turn DPO that applies variant of DPO (Rafailov et al., 2024b) to the multi-turn setting (Xiong et al., 2024; Song et al., 2024). Multi-Turn DPO first constructs contrastive trajectory pairs where the chosen trajectory achieves higher trajectory reward compared to the rejected trajectory, and uses the DPO loss to directly optimize the policy without using critic. SWEET-RL first trains turn-wise advantage model using the same model architecture with access to trainingtime information and then optimizes the policy with respect to rewards given by the turn-level advantage model as described in section 4. We use the reference code and web page as training-time information for Backend Programming and Frontend Design respectively. Due to the multi-modal nature of the reference web page, we instantiate the advantage LLM with similar sized VLM Qwen2-VL-7B-Instruct (Yang et al., 2024) with regression head on top of the mean representations of all visual and text tokens. Note that our experiments focus on the setting of learning from historically collected data (offline setting), and thus RL algorithms like PPO (Schulman et al., 2017) and REINFORCE (Williams, 2004) that require on-policy data collection do not apply. This is because in the real world, online human collaboration data requires extensive human annotations and can be costly to obtain. Evaluation metrics. Each task on Backend Programming comes with 10 unit tests, and we report the average percentage of tests passed and task success rate where all 10 unit tests for task are passed. We report the average cosine similarity of the final web page and the reference web page measured by the image representations from Clip-vit-base-patch32 (Radford et al., 2021) for Frontend Design. To provide more interpretable metric, we also include the win rate with respect to GPT-4O, where the model that lands on web page closer to the reference web page as measured by cosine similarity wins for each task."
        },
        {
            "title": "5.2 Comparisons on ColBench\nTable 2 shows the performance comparison of different SOTA LLMs and multi-turn RL algorithms across\ndifferent tasks on ColBench. First, comparing “single-turn” results and the other collaborative results, we\nnote that multi-turn collaborations can greatly enhance the performance of LLM agents for artifact creations\nby more closely aligning the final product with the reference “expectations” of human simulators. If the\nagent has to directly produce the final product in a single turn (top rows), even the best-performing GPT-4O\ncan only achieve 16.2%. In contrast, the success rates for all models are doubled (e.g., the success rate for\nLlama-3.1-8B-Instruct increases from 6.9% to 22.4%) if they are given the chance to interact with human\nsimulators for several turns and gather more information. Nonetheless, multi-turn collaboration remains a\nchallenging task even for proprietary LLMs like GPT-4O and O1-Mini, which can only achieve a success rate\nof 40.4% and 30.3%, respectively. Despite the improvements of O1-Mini on symbolic reasoning tasks such as\nmath and coding, we observe that these improvements do not directly result in taking a better strategy for\nmulti-turn collaborative agents, indicating that downstream fine-tuning is still necessary for LLMs to optimize",
            "content": "8 collaboration with humans."
        },
        {
            "title": "5.4 Analysis\nWith the advantage of SWEET-RL over baselines presented in Table 2, this section presents analytical results\nto understand this advantage and compare alternative designs.",
            "content": "Figure 3 (a) Scaling curve of Best-of-N sampling with respect to different step reward model on Backend Programming. Results show that SWEET can best tell good actions on turn-wise basis, resulting in the best scaling with respect to Best-of-N sampling. Note that this curve is different from test-time scaling curve because SWEET exploits additional training-time information. (b) Performance scaling of different multi-turn RL algorithms with respect to the amount of fine-tuning data on Backend Programming. While SWEET-RL takes more data initially to learn reliable critic, it quickly catches up and achieves better converging performance. How should we use training-time information to help credit assignment? We carry out Best-of-N sampling experiments on Backend Programming to directly compare the capability of different methods to perform credit assignments. At each turn, candidate actions are sampled from fixed actor Llama-3.1-8B-Instruct and different methods are used to choose the best action to be executed. We compare our reward model in SWEET-RL with three other natural choices: 1) SWEET-RL w/o Training-Time Information the same as SWEET-RL except that it only has the interaction history as inputs without access to the reference solution, 2) LLM-as-a-Judge uses Llama-3.1-8B-Instruct to compare pairwise the quality of each action based on the interaction history and reference solution, and 3) SWEET-RL w/ Value Function where regular classification head on top of Llama-3.1-8B-Instruct backbone is trained to predict the expected success rate given the interaction history and the reference solution with binary classification loss. Performance comparisons are presented in Figure 3(a) and qualitative comparisons are presented in Appendix D. First of all, we observe that the use of training-time information can significantly enhance the capability to perform credit assignment, evidenced by the huge performance gap between SWEET-RL and SWEET-RL w/o Training-Time Information. While Best-of-N sampling with respect to fixed LLM-as-a-Judge can result in some improvements over the zeroshot success rate, this improvement is limited. Qualitatively, we found that fixed LLM-as-a-Judge can easily get distracted by the length and format of the response without actually attending to its utility for task success. Finally, while being standard practice in the deep RL literature, the use of value function fails to achieve comparable scaling performance compared to SWEET-RL. This shows the importance of the careful 9 RL algorithmic choices of SWEET-RL and that the go-to practice of training value function may generalize poorly in unseen tasks. What are the best parameterization choices for the critic to perform credit assignment? In Table 3, we perform ablation experiments to understand the effect of different parameterizations for the advantage function of the critic. In particular, we consider two alternative parameterizations: 1) SWEET-RL w/ Regression Head uses regression head on top of the mean pooled representations of the last hidden state across all tokens, 2) SWEET-RL w/o Training-Time Information only uses the interaction history as inputs without access to the reference solution, and 3) SWEET-RL w/o Normalization where we do not perform the normalization step of division by the number of tokens in Equation 3. As shown in Table 3, similar to the conclusion from the previous section, the use of regression head does not generalize well compared to SWEET-RL and training-time information significantly improves the performance. Without the additional normalization step, we find that the trained actor quickly collapses by generating shorter and shorter responses, showing the importance of carefully chosen parameterizations for multi-turn RL algorithms. Table 3 Ablation study on different parameterizations of the critic. Results show that the parameterization of using the mean log probability significantly outperform the other natural choices. % Tests Passed Success Rate Rejection Fine-Tuning Multi-Turn DPO SWEET-RL SWEET-RL w/ Regression Head SWEET-RL w/o Train-Time Info. SWEET-RL w/o Normalization 40.9 48.0 56.8 45.3 44.0 4.2 28.2 34.4 40. 36.2 31.2 3.6 How does SWEET-RL scale with the amount of fine-tuning data? Additionally, we compare the scaling performance of SWEET-RL compared to the multi-turn RL baselines Rejection Fine-Tuning and Multi-Turn DPO. Results are presented in Figure 3(b). Although SWEET-RL requires more data to train reliable critic for performing credit assignment where it under-performs Multi-Turn DPO with 3k fine-tuning samples available, SWEET-RL quickly catches up with more samples once the turn-wise critic is trained and results in significantly improved converging performance. How does SWEET-RL work for different model architectures and off-policy data? Finally, Table 4 presents additional comparison experiments on Backend Programming using stronger base model Llama-3.1-70BInstruct to study how SWEET-RL works across different model architectures. We use the same offline data generated by Llama-3.1-8B as Table 2 to understand the effectiveness of different methods for taking advantage of off-policy data (i.e. offline generated by different model). We first observe that Rejection Fine-Tuning fails to learn from data generated by worse model, with the success rate dropping from 35.0% to 31.9%. This is potentially because the objective of Rejection Fine-Tuning forces Llama-3.1-70B-Instruct to imitate sub-optimal trajectories generated from the worse Llama-3.1-8B-Instruct word-by-word. While Multi-turn DPO is able to achieve big improvement even using off-policy data generated from an inferior model (35.0% to 41.8% in success rate), SWEET-RL still maintains similar gap of 3.8% through performing explicit credit assignments with training-time information (41.8% compared to 45.6% in success rate). Table 4 Comparison results on Backend Programming using Llama-3.1-70B-Instruct as the base model. Results show that SWEET-RL achieves similar gain over the baselines when using stronger Llama-3.1-70B-Instruct as the base model. Llama-3.1-70B-Instruct % Tests Passed Success Rate Zeroshot Rejection Fine-Tuning Multi-Turn DPO SWEET-RL 48.0 45.5 56.7 60.2 35.0 31.9 41.8 45."
        },
        {
            "title": "6 Conclusion",
            "content": "To advance the development of effective multi-turn RL algorithms that perform effective credit assignments, this paper first introduces benchmark, ColBench, focusing on the realistic domain of collaborative artifact creation. ColBench is the first LLM agent benchmark designed to validate multi-turn RL algorithms for reasoning-intensive tasks with minimum engineering overhead. Building upon ColBench, we develop novel multi-turn RL algorithm, SWEET-RL, leveraging additional training-time information and appropriate algorithmic choices, achieving significantly improved performance over SOTA baselines in this domain. Our experiment results on ColBench show that off-the-shelf deep RL methods for multi-turn LLM agents can lead to poor generalization performance. While SWEET-RL serves as preliminary step for closing this gap, there are lot of future research opportunities to develop better multi-turn RL algorithm in this important area of LLM agents."
        },
        {
            "title": "Impact Statement",
            "content": "This paper advances the development of more effective multi-turn RL algorithms and better human-agent collaborations. An effective collaborative LLM may significantly improve human productivity in many areas such as content creation. However, various safety concerns may arise as LLM agents take over more tasks from humans where they might be subject to malicious attacks or conduct unexpected behaviors. We leave this important direction for future research."
        },
        {
            "title": "References",
            "content": "Marwa Abdulhai, Isadora White, Charlie Snell, Charles Sun, Joey Hong, Yuexiang Zhai, Kelvin Xu, and Sergey Levine. Lmrl gym: Benchmarks for multi-turn reinforcement learning with language models, 2023. https://arxiv.org/abs/ 2311.18232. Alekh Agarwal, Nan Jiang, and Sham Kakade. Reinforcement learning: Theory and algorithms. 2019. Hao Bai, Yifei Zhou, Mert Cemri, Jiayi Pan, Alane Suhr, Sergey Levine, and Aviral Kumar. Digirl: Training in-the-wild device-control agents with autonomous reinforcement learning, 2024. https://arxiv.org/abs/2406.11896. Ralph Allan Bradley and Milton E. Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39:324, 1952. https://api.semanticscholar.org/CorpusID:125209808. Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, Tony Wang, Samuel Marks, Charbel-Raphaël Segerie, Micah Carroll, Andi Peng, Phillip Christoffersen, Mehul Damani, Stewart Slocum, Usman Anwar, Anand Siththaranjan, Max Nadeau, Eric J. Michaud, Jacob Pfau, Dmitrii Krasheninnikov, Xin Chen, Lauro Langosco, Peter Hase, Erdem Bıyık, Anca Dragan, David Krueger, Dorsa Sadigh, and Dylan Hadfield-Menell. Open problems and fundamental limitations of reinforcement learning from human feedback, 2023. https://arxiv.org/abs/2307.15217. Ching-An Cheng, Andrey Kolobov, Dipendra Misra, Allen Nie, and Adith Swaminathan. Llf-bench: Benchmark for interactive learning from language feedback, 2023. https://arxiv.org/abs/2312.06853. Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences, 2023. https://arxiv.org/abs/1706.03741. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards generalist agent for the web, 2023. https://arxiv.org/abs/2306.06070. Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. Raft: Reward ranked finetuning for generative foundation model alignment, 2023. https://arxiv.org/abs/2304.06767. GeminiTeam. Gemini: family of highly capable multimodal models, 2024. https://arxiv.org/abs/2312.11805. Izzeddin Gur, Hiroki Furuta, Austin V. Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. real-world webagent with planning, long context understanding, and program synthesis, 2021. Hyeonbin Hwang, Doyoung Kim, Seungone Kim, Seonghyeon Ye, and Minjoon Seo. Self-explore: Enhancing mathematical reasoning in language models with fine-grained rewards, 2024. https://arxiv.org/abs/2404.10346. Song Jiang, Da JU, Andrew Cohen, Sasha Mitts, Aaron Foss, Justine Kao, Xian Li, and Yuandong Tian. Towards full delegation: Designing ideal agentic behaviors for travel planning, 2024. https://arxiv.org/abs/2411.13904. Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues?, 2024. https://arxiv.org/abs/2310.06770. Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks, 2024. https://arxiv.org/abs/2401.13649. Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, Lei Zhang, Kay McKinney, Disha Shrivastava, Cosmin Paduraru, George Tucker, Doina Precup, Feryal Behbahani, and Aleksandra Faust. Training language models to self-correct via reinforcement learning, 2024. https://arxiv.org/abs/2409.12917. Hugo Laurençon, Léo Tronchon, and Victor Sanh. Unlocking the conversion of web screenshots into html code with the websight dataset, 2024. https://arxiv.org/abs/2403.09029. Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas Muennighoff, Reinhard Heckel, Jean Mercat, Mayee Chen, Suchin Gururangan, Mitchell Wortsman, Alon Albalak, Yonatan Bitton, Marianna Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba Ghosh, Josh Gardner, Maciej Kilian, Hanlin Zhang, Rulin Shao, Sarah Pratt, Sunny Sanyal, Gabriel Ilharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan, Jieyu Zhang, Khyathi Chandu, Thao Nguyen, Igor Vasiljevic, Sham Kakade, Shuran Song, Sujay Sanghavi, Fartash Faghri, Sewoong Oh, Luke Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari, Alexander Toshev, Stephanie Wang, Dirk Groeneveld, Luca Soldaini, Pang Wei Koh, Jenia Jitsev, Thomas Kollar, Alexandros G. Dimakis, Yair Carmon, Achal Dave, Ludwig Schmidt, and Vaishaal Shankar. Datacomp-lm: In search of the next generation of training sets for language models, 2024. https://arxiv.org/abs/2406.11794. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step, 2023. https://arxiv.org/abs/2305.20050. Jessy Lin, Nicholas Tomlin, Jacob Andreas, and Jason Eisner. Decision-oriented dialogue for human-ai collaboration, 2024. https://arxiv.org/abs/2305.20076. Yen-Ting Lin, Di Jin, Tengyu Xu, Tianhao Wu, Sainbayar Sukhbaatar, Chen Zhu, Yun He, Yun-Nung Chen, Jason Weston, Yuandong Tian, et al. Step-KTO: Optimizing mathematical reasoning through stepwise binary feedback. arXiv preprint arXiv:2501.10799, 2025. Guanlin Liu, Kaixuan Ji, Renjie Zheng, Zheng Wu, Chen Dun, Quanquan Gu, and Lin Yan. Enhancing multi-step reasoning abilities of language models through direct q-function optimization, 2024. https://arxiv.org/abs/2410. 09302. Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. Agentbench: Evaluating llms as agents, 2023. https://arxiv.org/abs/2308.03688. Llama3Team. The llama 3 herd of models, 2024. https://arxiv.org/abs/2407.21783. Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Meiqi Guo, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, and Abhinav Rastogi. Improve mathematical reasoning in language models by automated process supervision, 2024. https://arxiv.org/abs/2406.06592. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning, 2013. https://arxiv.org/abs/1312.5602. Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the gap between value and policy based reinforcement learning, 2017. https://arxiv.org/abs/1702.08892. OpenAI. Gpt-4 technical report, 2024. https://arxiv.org/abs/2303.08774. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, 12 Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022. https://arxiv.org/abs/2203.02155. Jiayi Pan, Xingyao Wang, Graham Neubig, Navdeep Jaitly, Heng Ji, Alane Suhr, and Yizhe Zhang. Training software engineering agents and verifiers with swe-gym, 2024. https://arxiv.org/abs/2412.21139. Richard Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho, He He, Sainbayar Sukhbaatar, and Jason Weston. Iterative reasoning preference optimization, 2024. https://arxiv.org/abs/2404.19733. Lerrel Pinto, Marcin Andrychowicz, Peter Welinder, Wojciech Zaremba, and Pieter Abbeel. Asymmetric actor critic for image-based robot learning, 2017. https://arxiv.org/abs/1710.06542. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. https://arxiv.org/abs/2103.00020. Rafael Rafailov, Joey Hejna, Ryan Park, and Chelsea Finn. From to q: Your language model is secretly q-function, 2024a. https://arxiv.org/abs/2404.12358. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model, 2024b. https://arxiv.org/abs/2305.18290. Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. Android in the wild: large-scale dataset for android device control, 2023. https://arxiv.org/abs/2307.10088. Christopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Marybeth Fair, Alice Li, William Bishop, Wei Li, Folawiyo Campbell-Ajala, Daniel Toyama, Robert Berry, Divya Tyamagundlu, Timothy Lillicrap, and Oriana Riva. Androidworld: dynamic benchmarking environment for autonomous agents, 2024. https://arxiv.org/abs/2405.14573. Sasha Salter, Dushyant Rao, Markus Wulfmeier, Raia Hadsell, and Ingmar Posner. Attention privileged reinforcement learning for domain transfer. ArXiv, abs/1911.08363, 2019. https://api.semanticscholar.org/CorpusID:215835372. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. https://arxiv.org/abs/1707.06347. Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, and Aviral Kumar. Rewarding progress: Scaling automated process verifiers for llm reasoning, 2024. https://arxiv.org/abs/2410.08146. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. https://arxiv.org/abs/2402.03300. Chenglei Si, Yanzhe Zhang, Zhengyuan Yang, Ruibo Liu, and Diyi Yang. Design2code: How far are we from automating front-end engineering?, 2024. https://arxiv.org/abs/2403.03163. Charlie Snell, Ilya Kostrikov, Yi Su, Mengjiao Yang, and Sergey Levine. Offline rl for natural language generation with implicit language learning, 2023. https://arxiv.org/abs/2206.11871. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters, 2024. https://arxiv.org/abs/2408.03314. Yifan Song, Da Yin, Xiang Yue, Jie Huang, Sujian Li, and Bill Yuchen Lin. Trial and error: Exploration-based trajectory optimization for llm agents, 2024. https://arxiv.org/abs/2403.02502. Richard Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In S. Solla, T. Leen, and K. Müller, editors, Advances in Neural Information Processing Systems, volume 12. MIT Press, 1999. https://proceedings.neurips.cc/paper_files/paper/1999/file/ 464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf. Andrew Szot, Max Schwarzer, Harsh Agrawal, Bogdan Mazoure, Walter Talbott, Katherine Metcalf, Natalie Mackraz, Devon Hjelm, and Alexander Toshev. Large language models as generalizable policies for embodied tasks, 2024. https://arxiv.org/abs/2310.17722. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with processand outcome-based feedback, 2022. https: //arxiv.org/abs/2211.14275. 13 Huaijie Wang, Shibo Hao, Hanze Dong, Shenao Zhang, Yilin Bao, Ziran Yang, and Yi Wu. Offline reinforcement learning for llm multi-step reasoning, 2024a. https://arxiv.org/abs/2412.16145. Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, and Heng Ji. Mint: Evaluating llms in multi-turn interaction with tools and language feedback, 2024b. https://arxiv.org/abs/2309.10691. Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8:229256, 2004. https://api.semanticscholar.org/CorpusID:2332513. Matthew Wilson and Tucker Hermans. Learning to manipulate object collections using grounded state representations, 2020. https://arxiv.org/abs/1909.07876. Yuxiang Wu and Baotian Hu. Learning to extract coherent summary via deep reinforcement learning, 2018. https: //arxiv.org/abs/1804.07036. Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, and Yu Su. Travelplanner: benchmark for real-world planning with language agents, 2024a. https://arxiv.org/abs/2402.01622. Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng Xu, Shuyan Zhou, Silvio Savarese, Caiming Xiong, Victor Zhong, and Tao Yu. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments, 2024b. https://arxiv.org/abs/2404.07972. Wei Xiong, Chengshuai Shi, Jiaming Shen, Aviv Rosenberg, Zhen Qin, Daniele Calandriello, Misha Khalman, Rishabh Joshi, Bilal Piot, Mohammad Saleh, Chi Jin, Tong Zhang, and Tianqi Liu. Building math agents with multi-turn iterative preference learning, 2024. https://arxiv.org/abs/2409.02392. Jing Xu, Andrew Lee, Sainbayar Sukhbaatar, and Jason Weston. Some things are more cringe than others: Preference optimization with the pairwise cringe loss. arXiv preprint arXiv:2312.16682, 2023. Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, and Caiming Xiong. Aguvis: Unified pure vision agents for autonomous gui interaction, 2024. https://arxiv.org/abs/2412.04454. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report, 2024. https://arxiv.org/abs/2407.10671. Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents, 2023. https://arxiv.org/abs/2207.01206. Lifan Yuan, Wendi Li, Huayu Chen, Ganqu Cui, Ning Ding, Kaiyan Zhang, Bowen Zhou, Zhiyuan Liu, and Hao Peng. Free process rewards without process labels, 2024. https://arxiv.org/abs/2412.01981. Yuexiang Zhai, Hao Bai, Zipeng Lin, Jiayi Pan, Shengbang Tong, Yifei Zhou, Alane Suhr, Saining Xie, Yann LeCun, Yi Ma, and Sergey Levine. Fine-tuning large vision-language models as decision-making agents via reinforcement learning, 2024. https://arxiv.org/abs/2405.10292. Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. Webarena: realistic web environment for building autonomous agents, 2024a. https://arxiv.org/abs/2307.13854. Yifei Zhou, Qianlan Yang, Kaixiang Lin, Min Bai, Xiong Zhou, Yu-Xiong Wang, Sergey Levine, and Erran Li. Proposer-agent-evaluator(pae): Autonomous skill discovery for foundation model internet agents, 2024b. https: //arxiv.org/abs/2412.13194. Yifei Zhou, Andrea Zanette, Jiayi Pan, Sergey Levine, and Aviral Kumar. Archer: Training language model agents via hierarchical multi-turn rl, 2024c. https://arxiv.org/abs/2402.19446. Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences, 2020. https://arxiv.org/abs/1909.08593."
        },
        {
            "title": "A Hyperparameters",
            "content": "For completeness and reproducibility, we present all hyperparameters used in SWEET-RL and all baselines in Table 5. In general, we found that the performances of both Multi-Turn DPO and SWEET-RL are consistent with respect to the hyperparameters in the DPO loss objective such as learning rate and beta, and the inclusion of negative-log-likelihood coefficient of 0.01 helps in most cases (Pang et al. (2024) also found this to be useful). Table 5 Hyperparameters for SWEET-RL and baseline methods for all experiments. Backend Programming Frontend Design Rejection Fine-Tuning Multi-Turn DPO SWEET-RL learning rate batch size epochs learning rate beta negative-log-likelihood loss coefficient batch size epochs critic learning rate critic beta critic negative-log-likelihood loss coefficient critic batch size critic epochs actor lr actor beta actor negative-log-likelihood loss coefficient actor batch size actor epochs 2e-7 32 4 2e-7 0.1 0.01 8 4 2e-7 0.1 0.01 8 4 2e-7 0.1 0.01 8 1 2e-7 32 8 2e-7 0.1 0.01 8 8 2e-7 0.1 0.01 8 8 2e-7 0.1 0.01"
        },
        {
            "title": "B Theoretical Justifications",
            "content": "In this section, we will provide some theoretical justifications for the derivation of our method. We first show that the trajectory-level Bradley-Terry objective can be similarly used for learning the advantage function: Lemma B.1. For an MDP with each state (o, c) and for any π : (cid:55) A, assume the transition function (o, a, c) is deterministic for any (o, a, c) then the following holds τ +, τ : (cid:34) log σ (cid:32) (cid:88) βr(o+ , a+ , c) (cid:33)(cid:35) (cid:34) βr(o , , c) = log σ (cid:32) (cid:88) (cid:88) βAπ(o+ , a+ , c) (cid:33)(cid:35) βAπ(o , , c) (cid:88) Proof. To show this, we would like to prove: (cid:88) r(ot, at, c) = Aπ(ot, at, c) (cid:88) We will prove this by telescoping: = = = = (cid:88) (cid:88) Aπ(ot, at, c) [Qπ(ot, at, c) π(ot, c)] 1 (cid:88) (cid:104) r(ot, at, c) + Eo t+1T (ot,at,c)V π(o t+1, c) π(ot, c) (cid:105) + r(oN , aN , c) t=1 (cid:88) t=1 (cid:88) t= r(ot, at, c) + 1 (cid:88) t=1 r(ot, at, c), (cid:104) Eo t+1T (ot,at,c)V π(o t+1, c) π(ot, c) (cid:105) 15 where the last equality follows by the assumption of deterministic transition. Additionally, we would like to also provide theoretical justification for using an asymmetric critic for optimizing the actor with different observation space. Intuitively, although for each sample such trainingtime advantage function may give different judgement compared to its regular counterpart, the following lemma shows that the policy gradient (Williams, 2004) estimated from an advantage function with training-time information is unbiased averaged over all samples. Lemma B.2. For an MDP with each state (o, c) and for any π : (cid:55) A, let dπ (ot, at, c) to be joint state-action occupancy distribution at step t, the following two estimators are both unbiased estimators of the policy gradient of π: Eτ π (cid:33) r(ot, at, c) = (cid:32) (cid:88) t= (cid:88) t=1 Eot,atAπ(ot, at) log π(atot) = (cid:88) t=1 Eot,at,cAπ(ot, at, c) log π(atot) Proof. The proof of this lemma is similar to the standard policy gradient analysis (Agarwal et al., 2019). Eτ π (cid:32) (cid:88) t=1 (cid:33) r(ot, at, c) =EcEo1 π(o1) =EcEo1V π(o1) =EcEo1 (cid:34) (cid:88) π(a1o1)Qπ(o1, a1) (cid:35) =EcEo1 =EcEo1 (cid:34) (cid:88) a1 (cid:34) (cid:88) a1 (π(a1o1))Qπ(o1, a1) + (cid:35) π(a1o1)Qπ(o1, a1) (cid:88) a1 π(a1o1)( log π(a1o1))Qπ(o1, a1) + (cid:35) π(a1o1)Qπ(o1, a1) (cid:88) =EcEo1 (cid:34) Ea1 ( log π(a1o1))Qπ(o1, a1) + (cid:88) (cid:35) π(a1o1)Qπ(o1, a1) a1 =EcEo1 [Ea1 ( log π(a1o1))Qπ(o1, a1) + Ea1 Eo2V π(o2)] =Ec (cid:88) t=1 Eot,atQπ(ot, at) log π(atot). To proceed, we need to first show useful equality: Eot,atV π(ot) log π(atot) =Eot =Eot (cid:88) (cid:88) π(atot)V π(ot) log π(atot) π(ot)π(atot) =EotV π(ot) (cid:88) π(atot) =EotV π(ot)1 =0 16 Backend Programming Prompt You are helpful LLM agent. Your task is to help human user to resolve their problem, in particular python programming. 1) Note that the problem is highly personalized so you need to explicitly gather information by asking questions to the human user about some hidden information and implicit constraints. YOU SHOULD TRY TO ASK CLARIFICATION QUESTIONS. 2) Note that you should not ask human users complicated questions as they will only answer questions briefly in two sentences. 3) When you have gathered enough information to answer, say \"I WANT TO ANSWER:\" in the beginning of your response and provide your final answer. 4) Note that you can only interact with the human users WITHIN 10 back-and-forth rounds and you have to provide your final answer before the conversation ends. 5) You should be as concise as possible in your response to human. \"I WANT TO ANSWER:\" should be included in your response to human if you think that you have gathered enough information for addressing this problem. Directly output the raw python code after \"I WANT TO ANSWER:\". Complete only the immediate agent response in this dialogue: Figure 4 The prompt used for testing different models on Backend Programming task. Therefore, we can use the advantage function instead of the Q-function in the expression of policy gradients: Eτ π (cid:32) (cid:88) t=1 (cid:33) r(ot, at, c) =Ec =Ec =Ec (cid:88) t= (cid:88) t=1 (cid:88) t=1 Eot,atQπ(ot, at) log π(atot) Eot,at(Qπ(ot, at) π(ot, at)) log π(atot) Eot,atAπ(ot, at) log π(atot) = = (cid:88) t=1 (cid:88) t=1 Eot,at ( log π(atot)) EcAπ(ot, at) Eot,at ( log π(atot)) EcAπ(ot, at, c) =Ec (cid:88) t= Eot,at ( log π(atot)) Aπ(ot, at, c), where the second last equation follows from the fact that Ecdπ (ot,at)Aπ(ot, at, c) = Aπ(ot, at)."
        },
        {
            "title": "C Prompts",
            "content": "For completeness, we have included the prompt that we used for testing different models on Backend Programming in Figure 4 and on Frontend Programming in Figure 5."
        },
        {
            "title": "D Qualitative Comparisons of Different Credit Assignment Methods",
            "content": "We present qualitative comparison results of different credit assignment in Figure 6. First of all, we observe that LLM-as-a-Judge can easily get distracted by the length and format of the response without actually attending to its utility for task success. Furthermore, while being natural practice in deep RL literature, the use of value function fails to reasonably predit the expected future utility in unseen tasks. In particular, it 17 Frontend Design Prompt You are helpful LLM agent. Your task is to help human user to code complete website with good design in HTML and Tailwind CSS. Write the code inside tag <html>. Write real and long sentences about the business. You dont have to include images, but if you do, use only this source https://picsum.photos/id/48/W/H, by replacing and with the width and height of the image. Keep the id the same to only use id 48 image. 1) Note that the problem is highly personalized so you need to go through few rounds of revisions. 2) When you have gathered enough information to answer, say \"I WANT TO ANSWER:\" in the beginning of your response and provide your final answer. 3) Note that you can only interact with the human users WITHIN 10 back-and-forth rounds and you have to provide your final answer before the conversation ends. 4) You will be judged both by the quality of the final answer and the efficiency of the conversation. 5) You can include ONLY ONE snippet raw html and Tailwind css code (wrapped in html tag)in your response to human user to ask how is the proposed design different from what the human user wants. This snippet of raw html and Tailwind css code (WRAPPED IN html TAG) will be rendered for the human to see screenshot of the webpage. The human user will respond by comparing your rendered webpage with the webpage that the human user has in mind. 6) You need to make sure that your html webpage looks exactly as the human user wants, including the overall layout, navigation bars, background color etc. 7) The human user can only see your rendered image and provide suggestions based on the rendered image, and not any text questions. First output your thought on your remaining uncertainties about the understanding of the problem and user preferences such as name of the function, input format, output format, and etc. Then say \"OUTPUT: n\" followed by your proposal html. Figure 5 The prompt used for testing different models on Frontend Design task. predicts that the first candidate response has probability of 97% to lead to the final success of the agent despite being only the second turn out of 10 turns and this candidate response being phrased in very confusing way. In contrast, SWEET-RL is able to tell the advantage of the second response with higher score because it is important in this task for the agent to figure out that the returned list can contain duplicate objects. This shows the importance of the RL algorithmic choices of SWEET-RL and that the go-to practice of training value function may generalize arbitrarily poorly in unseen tasks. Figure 6 Qualitative comparisons between different credit assignment methods. fixed LLM-as-a-Judge can be easily distracted by length and formats of the actions without considering their actual utility. value function generalizes poorly to unseen tasks. In contrast, SWEET can attend to the actual utility of the action for task success and generalize well."
        },
        {
            "title": "E Full Qualitative Examples",
            "content": "To demonstrate the level of difficulty of tasks in ColBench and provide qualitative comparisons of different models, we have included examples of full trajectories in this section. In particular, in Figure 7, Figure 8, Figure 9, Figure 10, we have provided full trajectories on Backend Programming for SWEET-RL Llama-3.1-8B-Instruct, Zeroshot Llama-3.1-8B-Instruct, and Zeroshot GPT4-O. While zeroshot baselines do try to propose some critical questions to seek more information from the human collaborator, they quickly jump into conclusions without gathering enough information, thus resulting in wrong answer. Such failure modes exist even for stronger general-purpose LLMs like GPT4-O, indicating that task-specific tuning may always be necessary despite the improvement in the capability of the base model. In contrast, SWEET-RL Llama-3.1-8B-Instruct learnt back-and-forth information-seeking behaviors and only answered the question once all information has been collected. Surprisingly, we found that RL training also results in some emergent behaviors such as reasoning with longer chain-of-thought and even self-corrections as shown in the last response from the agent in Figure 8. We also include full trajectory example on Frontend Design with SWEET-RL Llama-3.1-8B-Instruct in Figure 11, Figure 12, Figure 13, Figure 14, Figure 15, Figure 16. We would like to note the significant complexity of this task where the agent needs to reason about HTML code over an extended horizon (up to 16k tokens), as HTML code snippet is included in the response of each turn. After SWEET-RL training, the LLM agent has learnt nuanced collaborative and reward-maximizing behaviors where it first proposes scratch solution to gather coarse-grained feedback and only perform fine-grained edits in the end. 19 Figure 7 Example full trajectory for Backend Programming with SWEET-RL Llama-3.1-8B-Instruct. After training, the LLM agent has learnt back-and-forth information seeking behaviors before giving the final answer. Figure 8 Example full trajectory for Backend Programming with SWEET-RL Llama-3.1-8B-Instruct (Continued). After training, the LLM agent has learnt back-and-forth information seeking behaviors before giving the final answer. 21 Figure 9 Example full trajectory for Backend Programming with Zeroshot Llama-3.1-8B-Instruct. While the agent has asked few questions, it quickly jumps into conclusions, resulting in wrong final answer. 22 Figure 10 Example full trajectory for Backend Programming with Zeroshot GPT-4O. While the agent does propose critical questions to the human collaborator, it also has the issue of jumping into conclusions. Figure 11 Example full trajectory for Frontend Design with SWEET-RL Llama-3.1-8B-Instruct (1). After training, the LLM agent has learnt sophisticated task-specific strategies to optimize the final reward. 24 Figure 12 Example full trajectory for Frontend Design with SWEET-RL Llama-3.1-8B-Instruct (2). After training, the LLM agent has learnt sophisticated task-specific strategies to optimize the final reward. 25 Figure 13 Example full trajectory for Frontend Design with SWEET-RL Llama-3.1-8B-Instruct (3). After training, the LLM agent has learnt sophisticated task-specific strategies to optimize the final reward. Figure 14 Example full trajectory for Frontend Design with SWEET-RL Llama-3.1-8B-Instruct (4). After training, the LLM agent has learnt sophisticated task-specific strategies to optimize the final reward. 27 Figure 15 Example full trajectory for Frontend Design with SWEET-RL Llama-3.1-8B-Instruct (5). After training, the LLM agent has learnt sophisticated task-specific strategies to optimize the final reward. 28 Figure 16 Example full trajectory for Frontend Design with SWEET-RL Llama-3.1-8B-Instruct (6). After training, the LLM agent has learnt sophisticated task-specific strategies to optimize the final reward."
        }
    ],
    "affiliations": [
        "FAIR at Meta",
        "UC Berkeley"
    ]
}