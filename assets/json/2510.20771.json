{
    "paper_title": "AlphaFlow: Understanding and Improving MeanFlow Models",
    "authors": [
        "Huijie Zhang",
        "Aliaksandr Siarohin",
        "Willi Menapace",
        "Michael Vasilkovsky",
        "Sergey Tulyakov",
        "Qing Qu",
        "Ivan Skorokhodov"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "MeanFlow has recently emerged as a powerful framework for few-step generative modeling trained from scratch, but its success is not yet fully understood. In this work, we show that the MeanFlow objective naturally decomposes into two parts: trajectory flow matching and trajectory consistency. Through gradient analysis, we find that these terms are strongly negatively correlated, causing optimization conflict and slow convergence. Motivated by these insights, we introduce $\\alpha$-Flow, a broad family of objectives that unifies trajectory flow matching, Shortcut Model, and MeanFlow under one formulation. By adopting a curriculum strategy that smoothly anneals from trajectory flow matching to MeanFlow, $\\alpha$-Flow disentangles the conflicting objectives, and achieves better convergence. When trained from scratch on class-conditional ImageNet-1K 256x256 with vanilla DiT backbones, $\\alpha$-Flow consistently outperforms MeanFlow across scales and settings. Our largest $\\alpha$-Flow-XL/2+ model achieves new state-of-the-art results using vanilla DiT backbones, with FID scores of 2.58 (1-NFE) and 2.15 (2-NFE)."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 1 7 7 0 2 . 0 1 5 2 : r ALPHAFLOW: UNDERSTANDING AND IMPROVING MEANFLOW MODELS Huijie Zhang 1,2 Aliaksandr Siarohin1 Willi Menapace1 Michael Vasilkovsky1 Sergey Tulyakov1 Qing Qu2 1Snap Inc. 2Department of EECS, University of Michigan Ivan Skorokhodov1 Figure 1: Uncurated samples (seeds 1-8) from the DiT-XL/2 model for MeanFlow Geng et al. (2025a) and α-Flow (our proposed method) produced with 1 (upper) and 2 (lower) sampling steps for ImageNet-1K 2562."
        },
        {
            "title": "ABSTRACT",
            "content": "MeanFlow has recently emerged as powerful framework for few-step generative modeling trained from scratch, but its success is not yet fully understood. In this work, we show that the MeanFlow objective naturally decomposes into two parts: trajectory flow matching and trajectory consistency. Through gradient analysis, we find that these terms are strongly negatively correlated, causing optimization conflict and slow convergence. Motivated by these insights, we introduce α-Flow, broad family of objectives that unifies trajectory flow matching, Shortcut Model, and MeanFlow under one formulation. By adopting curriculum strategy that smoothly anneals from trajectory flow matching to MeanFlow, α-Flow disentangles the conflicting objectives, and achieves better convergence. When trained from scratch on class-conditional ImageNet-1K 256256 with vanilla DiT backbones, α-Flow consistently outperforms MeanFlow across scales and settings. Our largest α-Flow-XL/2+ model achieves new state-of-the-art results using vanilla DiT backbones, with FID scores of 2.58 (1NFE) and 2.15 (2-NFE). The source code and pre-trained checkpoints are available on https://github.com/snap-research/alphaflow."
        },
        {
            "title": "INTRODUCTION",
            "content": "Diffusion models (Sohl-Dickstein et al., 2015) have emerged as the leading paradigm for generative modeling of visual data (Dhariwal & Nichol, 2021; Rombach et al., 2022; Brooks et al., 2024). However, their widespread use is limited by slow inference, as generating high-fidelity samples Work done during an internship at Snap Inc. 1 typically requires large number of denoising steps. This computational bottleneck has spurred extensive research into designing efficient diffusion-based generators that are able to operate in very few steps while preserving high generation quality (Salimans & Ho, 2022; Sauer et al., 2024; Song et al., 2023; Song & Dhariwal, 2024; Lu & Song, 2025; Geng et al., 2025b; Frans et al., 2025; Geng et al., 2025a). Early attempts reduce the inference time of diffusion models through distilling pre-trained multistep model into few-step one (Salimans & Ho, 2022; Sauer et al., 2024). The subsequent development of consistency models (Song et al., 2023; Song & Dhariwal, 2024; Lu & Song, 2025) enabled training from scratch for few-step generative models. However, significant performance gap still remains between existing few-step and multi-step diffusion models. The recently introduced MeanFlow framework (Geng et al., 2025a) enables more stable training and better classifier-free guidance (Ho & Salimans, 2022) integration, significantly bridging the gap between few-step and multi-step from-scratch trained diffusion models. Despite its practical success, there still lacks clear understanding of why MeanFlow performs better, which hinders further improvements and the design of stronger few-step models. In this work, we provide deeper understanding of why MeanFlow works, revealing that its traintrajectory flow matching and trajectory ing objective can be decomposed into two components: consistency. Our gradient analysis shows that these two components are strongly negatively correlated during training, leading to instability and slow convergence in joint optimization. We further demonstrate that the previous heuristic adoption of border-case flow matching supervision is crucial: it actually acts as surrogate loss for trajectory flow matching and mitigates gradient conflict. However, over 75% of MeanFlows computation is spent on this border-case supervision, which is not its primary focus. This raises an open question: can we design more efficient techniques to optimize MeanFlow objective, without such computational overhead? Motivated by these observations, we introduce α-Flow, new broad family of objectives for few-step flow models. This framework unifies trajectory flow matching, Shortcut Models Frans et al. (2025), and MeanFlow under single unified formulation. By employing curriculum learning strategy that smoothly transitions from trajectory flow matching to MeanFlow, α-Flow better disentangles the optimization of trajectory flow matching and trajectory consistency, reduces reliance on border-case flow matching supervision, and achieves better convergence. By training vanilla DiT-(Peebles & Xie, 2023) models from scratch with α-Flow on class-conditional ImageNet-1K 2562, we obtain consistently stronger performance across both smalland large-scale settings compared with MeanFlow, for both one-step and few-step generation. Our largest DiTXL/2+ model establishes new state-of-the-art results among all from-scratch trained models with the vanilla DiT backbone and training pipeline, achieving FID scores of 2.58 (1-NFE) and 2.15 (2-NFE)."
        },
        {
            "title": "2 PRELIMINARIES",
            "content": "Diffusion models and flow matching. Diffusion model (Ho et al., 2020; Song & Ermon, 2019; Rombach et al., 2022) define forward process that progressively adds noise to data sample [0, 1]. Specifically, given training data, the forward process pdata(x) over continuous timestep perturbs into noisy version zt = βtx + σtϵ where ϵ (0, I), βt and σt are pre-defined scheduler parameters that depend on t, such that z0 = and z1 = ϵ. Flow matching (Liu et al., 2023; Lipman et al., 2023) is deterministic alternative that defines the forward process as straightline path between the noise distribution and the data distribution, setting βt = 1 and σt = t. neural network vθ(zt, t) is trained to model the ground-truth vector field dzt/dt along this trajectory zt by minimizing the objective: (1) where vt v(zt, x. To generate new sample, the probability flow ODE (PF-ODE) dz/dt = vθ(zt, t) is solved from = 1 to = 0, starting with an initial value z1 x) = dzt/dt(cid:12) (cid:12)x = ϵ (0, I). vθ(zt, t) FM (θ) = Et,x,zt[ 2] vt One primary challenge of diffusion models is the slow sampling speed. To address this, several methods have been proposed to enable high-quality generation with significantly fewer steps. 2 Consistency model (CM). (Song et al., 2023) enables one-step generation by training neural network fθ(zt, t) to directly map the noisy input zt to clean samples x. The core idea is to enforce consistency property at any two nearby timesteps and s, by minimizing the difference between the models output. Depending on the := s, the training objective can be categorized into: Discrete-time Consistency Training (CT) (Geng et al., 2025b; Song et al., 2023; Song & Dhariwal, 2024) minimizes the following discrete time CT loss CTd: CTd(θ) = Et,s,zt (cid:104) 2 2 and fθ := stopgrad (fθ). While smaller values where 0 of reduce the discretization error and improve performance, they might also lead to training instability (Song et al., 2023; Geng et al., 2025b). This necessitates carefully designed scheduler for to ensure good performance and stability during training. fθ (zs, s) 1, zs = zt < t (2) (cid:105) , fθ(zt, t) Continuous-time CT (Lu & Song, 2025; Song et al., 2023) eliminates the discretization error by the continuous time CT loss CTc: CTc(θ) = 2Et,zt (cid:20) θ (zt, t) dfθ (zt, t) dt (cid:21) , (3) CTd(θ)/t. However, esSong et al. (2023) theoretically show that timating dfθ (zt,t) relies on the Jacobian-vector product (JVP) operation, which causes potential issues of scalability and efficiency in modern deep learning frameworks (Wang et al., 2025b; Peng et al., 2025). CTc(θ) = limt0 θ dt θ Consistency trajectory model (CTM). (Kim et al., 2024; Zhou et al., 2025; Frans et al., 2025; Geng et al., 2025a) generalize Consistency Models (CMs) by training neural network uθ(zt, r, t) to enforce consistency across trajectory from to with 0 1. This allows jumping from (0, 1] to any < during inference, enabling multi-step generation. To train CTM from any scratch: Shortcut model (Frans et al., 2025) enforces consistency by ensuring that single shortcut step from to is consistent with two consecutive shortcut steps of half the size. The training objective is: where zs = zt (cid:104) SC(θ) = t,r,zt s) (t uθ(zt, r, t) uθ (zt, s, t)/2 uθ (zt, s, t) and = (t + r)/2. 1 tr (cid:82) v(zτ , τ )dτ , with training objective given by: MeanFlow (Geng et al., 2025a) trains the model uθ(zt, r, t) to estimate the mean velocity uθ (zs, r, s) /2 (cid:105) 2 2 , (4) MF(θ) = t,r,zt (cid:34)(cid:13) (cid:13) (cid:13) (cid:13) uθ(zt, r, t) vt + (t r) duθ (zt, r, t) dt (cid:35) . (cid:13) 2 (cid:13) (cid:13) (cid:13) 2 (5) In practice, MeanFlow significantly outperforms other one/few-step diffusion and flow models. Yet, there has been little analysis explaining why it works so effectively. To shed light on this, we analyze MeanFlow training in the next section."
        },
        {
            "title": "3 ANALYZING MEANFLOW TRAINING",
            "content": "An intriguing aspect of MeanFlow is the noise distribution used during training: Geng et al. (2025a) empirically found that the best results are achieved when setting = for 75% of the samples. This might look counter-intuitive, since we are interested in learning the average velocity on [r, t] interval to perform large trajectory leaps during inference, so why spending the majority of the training computation on fitting this border case that corresponds to vanilla flow matching supervision? In this section, we show that the MeanFlow loss on its own can be interpreted as velocity consistency training with extra flow matching supervision, and analyze the interaction of these two objectives. 3 (a) Gradient similarity (b) LTFM under different LFM ratios. (c) LTCc under different LFM ratios. Figure 2: MeanFlow training analysis. (a) Shows the cosine similarity between the gradients of two loss pairs ( TCc vs. TFM when MeanFlow trained with 0% and 75% of TCc when MeanFlow trained with 0% and 75% of FM) throughout training. (b) Evaluated FM. (c) Evaluated TFM and TCc vs. L L FM. L"
        },
        {
            "title": "3.1 UNDERSTANDING THE OBJECTIVE",
            "content": "Through algebraic manipulations, the original MeanFlow loss the following equivalent form (see Section D.1): MF in Eq. (5) can be rewritten into MF(θ) = t,r,zt (cid:124) (cid:2) uθ(zt, r, t) vt 2 2 (cid:123)(cid:122) Trajectory flow matching LTFM (cid:20) 2 (t (cid:3) (cid:125) + t,r,zt (cid:124) r) θ (zt, r, t) (cid:123)(cid:122) Trajectory consistency LTCc duθ (zt, r, t) dt (cid:21) (cid:125) +C, (6) where is constant independent of θ. In this decomposition, the first term TFM corresponds to flow matching loss but with an additional modeling input parameter r, so we refer to it as TCc, denoted as trajectory consistency loss, acts as trajectory flow matching. The second term r)-reweighted continuous consistency loss 1, but also without boundary condition (Song (t et al., 2023). This decomposition highlights that the MeanFlow objective can be interpreted as consistency (trajectory) model with extra flow matching supervision. L TCc does not have any boundary condition. An interesting property of this decomposition is that In comparison, Song et al. (2023) enforces such condition for vanilla consistency models using z0-prediction parameterization: without it, the model would quickly converge to trivial solution (e.g., constant output). In the MeanFlow case, this collapse does not occur, which suggests that TCc. We believe that the absence of an explicit TFM implicitly provides the boundary condition for TCc easier to optimize and gives it much larger solution space. boundary condition makes Another important observation here is that trajectory flow matching involves random t, which differs from the = case used during training by Geng et al. (2025a). To clarify this distinction, we directly compare trajectory flow matching ( TFM) with vanilla flow matching, which we denote as FM when using the u-prediction parameterization: L TFM t,r,zt (cid:2) uθ(zt, r, t) (cid:3) , vt 2 2 FM t,r,ztr=t (cid:2) uθ(zt, r, t) (cid:3) 2 vt (7) TFM arises from the decomposition of the MeanFlow loss, while FM corresponds to the Here, objective used in Geng et al. (2025a) for joint training. From this formulation, several observations = t) slice of the joint distribution follow. First, p(t, r). Second, if the network is independent of r, then marginalizing out yields FM, reducing the objective to vanilla flow matching. TFM, active only on the p(t, FM is part of TFM L 3.2 EMPIRICAL ANALYSIS With the decomposition in Equation (6), how does In this section, we analyze the gradients of these losses and examine how extra affects FM interact with the two decomposed terms? FM minimization TCc individually. We conduct detailed experiments by training MeanFlow with TFM and L 1Similarly to the proof in Remark 10 of Song et al. (2023), one can show that this term is equivalent to minimizing the difference between uθ(zt, r, t) and uθ (ztt, r, t) as 0. 4 the DiT-B/2 (Peebles & Xie, 2023) architecture on ImageNet-1K 2562 (Deng et al., 2009) for 400K steps. Additional experiment settings are in Section E. TFM and We first analyze the training dynamics by measuring the cosine similarity between the gradients TCc during training. As shown in Figure 2a, these two gradients are strongly negaTCc TCc , without any boundTFM whose manifold is very TCc manifold, distracting from tively correlated, with similarity typically below jointly is inherently difficult. We hypothesize this stems from the fact that ary condition, has very large optimal solution manifold, compared to narrow. Thus the optimization process is getting pulled towards the reaching narrow intersection. 0.4. This reveals that optimizing TFM and L TFM, TCc = 0. Consequently, the gradient TFM compared to pure MeanFlow training. Second, Given this gradient conflict, the question arises: why does joint training with tify two key reasons: First, as subset of FM directly reduces confirmed in Figure 2b, where allocating 75% of the training budget to the overall FM help? We idenTFM. This is empirically FM significantly lowers FM applies only at = t, where TFM gradient. TCc) is consistently higher than This is demonstrated in Figure 2a, which shows that cos( cos( TCc component doesnt seem to be affected and can even be lower when allocating 75% of the training budget to TCc is relatively easy to optimize, even near the TCc ), that is strongly negative for more than 95% of the training. Surprisingly, FM, as shown in Figure 2c. Which again hints at the fact that TCc than the FM interferes less with TFM optimum. TFM, L L L FM, L In conclusion, our analysis reveals three important observations: MF can be decomposed into trajectory flow matching TFM and trajectory consistency objectives, whose gradients are strongly negatively correlated during training. TCc does not have necessary boundary condition on its own, implying that TFM serves TCc as an implicit boundary condition for it. FM acts as surrogate loss for TCc . Trajectory consistency loss TFM, but with significantly less gradient conflict with the 4 α-FLOW MODELS As we showed in the previous section, the TCc. While the introduction of the TFM, this approach dedicates significant portion of training to an objective that is not of our primary interest. This raises key question: Can we more efficiently optimize MF without this computational overhead? To answer this, we introduce our α-Flow loss, new family of training objectives for flow-based models. FM loss serves as an effective surrogate for optimizing TFM loss is difficult to optimize jointly with the TFM when optimizing L 4.1 α-FLOW: UNIFYING ONE, FEW, AND MANY-STEP FLOW-BASED MODELS Definition 1. The α-Flow loss α is defined as: α(θ) t,r,zt (cid:104) α1 uθ(zt, r, t) (α vs,t + (1 α) uθ(zs, r, s)) (cid:105) , 2 2 (8) [0, 1] is the start and end timestep, is the intermediate timestep: = α where t, α) vs,t is the trajectory value at this t, α timestep s. Here, vs,t is the shift velocity used to estimate the intermediate variable zs from zt. (0, 1] is the consistency step ratio, and zs = zt + (t + (1 s) The α-Flow loss is visualized in Figure 3e. Intuitively, it enforces trajectory consistency between and by introducing an additional s, which is an interpolation between t, with ratio α. More importantly, this definition generalizes previously introduced training objectives such as trajectory flow matching, Shortcut Model training, and MeanFlow training: Theorem 1. The α-Flow loss unifies flow matching, Shortcut Models, and MeanFlow: TFM(θ) = α=1(θ) with vs,t = vt. 5 (a) Discrete CT (b) Continuous CT (c) Shortcut Model (d) MeanFlow (e) α-Flow Figure 3: Comparison of training trajectories for various few-step diffusion and flow-based models. SC(θ) = 2 Lα=1/2(θ) with vs,t = uθ (zt, s, t). α0(θ) with vs,t = vt. θ MF(θ) = θ Moreover, if one considers z0-parametrized network uθ(zt, 0, t) = (zt α incorporates discrete and continuous consistency training as well. Specifically, with vs,t = vt and fθ(zt, t)) /t = ˆz0, 0: fθ(zt, t)) /t = ˆz0, vs,t = vt and 0: (0, t). Algorithm 1 α-Flow: Training. With uθ(zt, 0, t) = (zt θ CTd(θ) = α=δ(θ) for δ CTc(θ) = α0(θ). L θ This theorem reveals that the ratio α is the key hyperparameter that unifies seemingly different methods, which controls the relative position of the intermediate timestep within the (r, t) interval. By annealing α from 1 to 0, we obtain family of models in the interpolation between trajectory flow matching and MeanFlow. Notably, discrete CT is special case 0. Unlike disof α-Flow with crete CT, α-Flow requires no complex timestep partitioning: once and are sampled, is immediately determined with fixed α. 4.2 α-FLOW MODELS # fn(z, r, t): function to predict # x: training batch, k: training iterations t, = sample r() alpha = sample alpha(k) = alpha * + (1 - alpha) * = randn like(x) zt = (1 - t) * + * = - if alpha == 0: u, dudt = jvp(fn, (zt, r, t), (v, 0, 1)) u_tgt = - (t - r) * dudt else : = fn(zt, r, t) zs = zt - (t - s) * u_tgt = alpha * + (1 - alpha) * fn(zs, r, s) error = - stopgrad(u_tgt) loss = metric(error) Algorithm 2 α-Flow: Curriculum Schedule # k_s, k_e: start/end schedule iterations, # gamma: temperature parameter # k: current iteration, eta: clamping value The α-Flow loss enables curriculum learning strategy that progressively transitions from the trajectory flow matching to MeanFlow objective. This approach better disentangles the optimization of the trajectory flow matching and consistency losses, could potentially reduce reliance on the flow matching objective, and leads to better convergence. The detailed curriculum learning can be summarized into three phases: scale = 1 / (k_e - k_s) offset = - (k_s + k_e) / 2 / (k_e - k_s) alpha = 1 - sigmoid((scale * + offset) * gamma) alpha = 1 if alpha > (1eta) else (0 if alpha < eta else alpha) Trajectory flow matching pretraining (α = 1). To speed-up convergence toward narrow TFM manifold, we prioritize optimizing trajectory flow matching in the early training phase. Additionally, as low-variance objective, trajectory flow matching quickly establishes reliable noise-todata mapping, providing good initialization for subsequent few-step refinement. Notably, this pretraining strategy is aligned with previous diffusion model pretraining strategy applied on consistency model (Geng et al., 2025b), while we start from different motivations and generalize it into the α-Flow framework. α-Flow transition (α (0, 1)). Once the model has solid foundation, we transition from trajectory flow matching to the MeanFlow objective. We accomplish this with curriculum learning approach where we progressively decrease the consistency step ratio from 1 to 0. This gradual shift is inspired by the discrete CT (Song et al., 2023). It effectively transitions from the high-bias, low-variance objective to the high-variance, low-bias one, leading to improved convergence. MeanFlow fine-tuning (α 0). In the final stage, we focus entirely on the MeanFlow training objective. Unlike the original paper, our improved early-stage optimization of trajectory flow matching significantly reduces the need for the flow matching loss (as shown in Table 2 (b)) and achieves significantly better few-step generation quality. MF or α when α > 0. The overall training code of α-Flow is shown in Algorithm 1, where we first sample t, and obtain the α from the schedule. Based on whether α = 0 or not, α-Flow will use either α to train the model. α-Flow applies the same training details as MeanFlow when training MF (except lower ratio of flow matching). Below, we only show the difference: the schedule of α as well as the design space of Schedule. To schedule the training, we use sigmoid function, α = Sigmoidkske,γ,η (k), which depends on the training iteration k. The function is defined by its starting and ending iterations, ks, ke, temperature parameter γ (set to be 25) and clamping value η. The specific implementation can be found in Algorithm 2. Figure 5 provides visualization of this scheduler, while Section 5.2 conducts an ablation study over its parameters. Clamping value. Geng et al. (2025b) show that when = approaches 0, the performance of few-step CT model will first increase and then decrease. For α-Flow, we observe similar phenomenon: by training α-Flow with fixed α, as α approaches 0, the 1-step generation performance will first increase then decrease. Detailed experiments are shown in Table 5 (c). From the exper103. Thus, we set clamping value iment, the optimal performance is achieved when α = 5 103 for the schedule. α will be set to 0 when α < η. We also use the same clamping value η = 5 to set α to 1 when α > 1 In the unifying space of α-Flow loss, all other few-step models set vs,t = vt Training objective. except the shortcut model which uses vs,t = uθ(zt, s, t). Additionally, we are interested in seeing whether we need exponential moving average (EMA) for θ. With ablation study in Table 5 (a), we set vs,t = vt and do not use EMA for θ. Adaptive loss weight. MeanFlow (Geng et al., 2025a) demonstrates the effectiveness of adaptive 2 2 + c) loss. Basically, let where = 103. And the adaptively weighted loss is sg(ω) 2 2. Theoretically, we derived an 2 equivalent adaptive loss weight ω = α/( 2 + c) for α. We defer the derivation in Section G.2. With ablation study in Table 5 (b), we demonstrate the derived adaptive loss weight is better than other loss weights. 2 2 denote the squared L2 loss. The adaptive loss weight ω = 1/( η, as when α is close to 1, α but more efficient. TFM is similar to L Classifier-free guidance (CFG). We apply similar CFG training strategy as MeanFlow, by setting vs,t in Equation (8) with vs,t = c) + (1 κ) ) denotes the class-condition uθ (zt, t, (with class c) and class-unconditional prediction. Detailed settings of w, κ are deferred to Section F. ), where w, κ are the guidance scale, uθ ( uθ (zt, t, c), uθ ( x) + κ v(zt, Sampling. We employ both consistency sampling (Song et al., 2023) and ODE sampling for twostep generation. Implementation details are provided in Algorithm 3. Empirically, we observe that consistency sampling outperforms ODE sampling for larger models with better convergence. Consequently, we adopt ODE sampling for all DiT-B/2 architectures and consistency sampling for all DiT-XL/2 architectures, with additional ablation studies on DiT-XL/2 presented in Figure 4."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "In this section, we employ α-Flow on real image datasets ImageNet-1K 2562 Deng et al. (2009). We use exactly the same DiT Peebles & Xie (2023) architecture as MeanFlow Geng et al. (2025a). For evaluation, we use Frechet Inception Distance (FID) Heusel et al. (2017), Frechet DINOv2 Oquab et al. (2023). We evaluate model performance for both 1 and 2 Number of Function Evaluations (NFE=1, NFE=2). We implement our models in the latent space of the Stable Diffusion Variational Autoencoder (SD-VAE) 2. More details on the experiments settings are in Section F. 2The EMA version in https://huggingface.co/stabilityai/sd-vae-ft-mse 7 Method Source Params Epochs Frans et al. (2025) Zhou et al. (2025) 675M Shortcut-XL/2 676M IMM-XL/2 MeanFlow-XL/2 Geng et al. (2025a) 676M MeanFlow-XL/2+ Geng et al. (2025a) 676M FACM-XL/2 Peng et al. (2025) 160 3840 240 1000 675M 800 + 250 FACM-XL/2 FACM-XL/2 MeanFlow-B/2 MeanFlow-XL/2 α-Flow-B/2 α-Flow-XL/2 α-Flow-XL/2+ Our reproduction Our methods 675M 675M 131M 676M 131M 676M 676M 120 2 240 2 240 240 240 240 240+60 NFE 1 NFE 2 FID FDD FID FDD FID 10.60 8.05 3.43 3.88 2.93 2.20 2.07 9.54 410.4 7.31 362.0 6.59 327.7 4.73 278.6 6.04 312.3 5.17 232.1 3.47 185.8 2.46 108.7 2.26 5.40 287.1 5.01 231.8 2.95 164.6 2.34 105.7 2.16 2.58 148.4 2.15 96.8 1.95 256. The table reports the results for Table 1: Class-conditional generation on ImageNet-256 few-step diffusion/flow matching-based methods trained from scratch. 2 indicates that FACM requires roughly twice the computation per epoch compared to other methods. For direct epochto-epoch comparison, α-Flow-XL/2, MeanFlow-XL/2 and FACM-XL/2 are each trained for 240 epochs. α-Flow-XL/2+ is fine-tuned version of α-Flow-XL/2, trained for extra 60 epochs with batch size of 1024. FID scores are evaluated with the balanced class sampling (see Section I). Schedule NFE 1 NFE 2 FID FDD FID FDD Model NFE 1 NFE 2 % = Schedule FID FDD FID FDD Constant0.0 44.4 844.1 42.1 836. Trajectory flow matching iterations Sigmoid0K100K 44.3 860.3 40.8 826.9 Sigmoid50K150K 44.1 846.8 39.9 811.6 Sigmoid100K200K 42.4 828.0 38.3 795.4 Sigmoid150K250K 41.3 818.8 38.1 793.1 Transition iterations Sigmoid200K200K 41.4 794.4 38.8 796.7 Sigmoid150K250K 41.3 818.8 38.1 793.1 40.0 785.4 37.1 782.9 Sigmoid0K400K 0% 25% 50% 75% Constant0.0 46.0 879.6 44.3 867.7 Sigmoid0K400K 40.4 822.5 38.9 811.8 Constant0.0 44.4 844.1 42.1 836.3 Sigmoid0K400K 40.0 785.4 37.1 782.9 Constant0.0 43.9 844.1 42.1 836.3 Sigmoid0K400K 40.2 781.0 37.1 775.0 Constant0.0 43.1 819.2 38.5 787.6 Sigmoid0K400K 42.2 810.5 36.2 754. (a) Consistency step ratio schedule. (b) Flow matching ratio. Table 2: Ablation study on ImageNet-1K 2562 for α-Flow-B/2. 5.1 COMPARISON WITH BASELINE In Table 1, we compare α-Flow with previous few-step Diffusion and Flow models, demonstrating its superior performance for 1-NFE and 2-NFE generation. Across models trained for 240 epochs, α-Flow-XL/2 achieves 2.95 FID (164.6 FDD), representing relative improvement of 15% (12%) over MeanFlow-XL/2 and 70% (60%) over FACM-XL/2. Our best model, α-Flow-XL/2+, sets new state-of-the-art 1-NFE generation with an impressive FID of 2.58 (148.4 FDD), compared with all the other few-step Diffusion and Flow models trained over the SD-VAE. Furthermore, for 2-NFE generation, α-Flow-XL/2+ achieves 2.15 FID (96.8 FDD), outperforms all these baseline methods. Its particularly notable that it surpasses FACM-XL/2s 2.07 FID (achieved with class-balanced sampling) by reaching 1.95 FID with only 23% of the training epochs. Uncurated samples, shown in Figure 1 and Section K, visually confirm these results. Specifically in Figure 1, α-Flow-XL/2 generates more images with better quality, as highlighted in green. 5.2 ABLATION STUDY Consistency step ratio schedule. In Table 2 (a), we evaluate our α-Flow framework trained with various sigmoid schedules, as visualized in Figure 5. For these experiments, the flow matching ratio is fixed at 25%. We first analyze the impact of the trajectory flow matching pretraining du8 Figure 4: Comparing ODE vs consistency sampling for MeanFlow and α-Flow models. Figure 5: Visualization of consistency step ratio schedule. ration. By fixing ke ks to 100K iterations, we progressively increase ks from 0K to 150K. As the pretraining duration increases, α-Flows performance consistently improves across all metrics. The best-performing schedule, Sigmoid150K250K, significantly outperforms the baseline MeanFlow (Constant0.0). This suggests that optimizing trajectory flow matching is more crucial than optimizing MeanFlow in the early training stages for achieving superior few-step flow modeling. This finding aligns with our empirical analysis, which shows that because the gradients of the trajectory flow matching and consistency losses conflict, it is more efficient to exclusively optimize the trajectory flow matching objective for faster initial convergence. Next, we investigate the effect of the transition duration. With the midpoint (ks + ke)/2 fixed at 200K iterations, we vary the total transition iterations from 0 to 400K. Our results indicate that longer, smoother transition leads to better generation quality. This highlights the importance of gradually reducing the bias of the training objective by smoothly transitioning between trajectory flow matching and MeanFlow. Flow matching ratio. In Table 2 (b), we compare our α-Flow framework with the MeanFlow baseline across various flow matching ratios (%r = t). Our results show that α-Flow consistently outperforms MeanFlow for all evaluated ratios, confirming the effectiveness of our proposed method. key finding is that α-Flow achieves its best 1-NFE performance at relatively low flow matching ratio. Specifically, it reaches the best FID of 40.0 at 25 % of = and the best FDD of 781.0 at 50 % of = t, while MeanFlow requires higher ratio of 75% to achieve its best FID of 43.1 and FDD of 819.2. This aligns with our motivation: by pretraining on trajectory flow matching, α-Flow is less reliant on the flow matching objective and can focus more on the overall MeanFlow objective, leading to superior one-step generation quality. Furthermore, we observe that for α-Flow, the flow matching ratio presents clear trade-off between 1-NFE and 2-NFE performance. For instance, the 75% ratio yields worse NFE=1 but better NFE=2 generation results compared to the 50%-ratio version. This indicates that higher proportion of flow matching improves the models ability to generate images in slightly higher number of steps. Sampling. As shown in Figure 4, we compare ODE sampling (solid line) and consistency sampling (dotted line) for 2-NFE generation across different intermediate sampling timesteps, using MeanFlow-XL/2, α-Flow-XL/2, and α-Flow-XL/2+. The results show that consistency sampling yields better generation performance for both α-Flow-XL/2 and α-Flow-XL/2+, achieving the best FID scores of 2.09 at timestep 0.4 and 2.28 at timestep 0.45, respectively. In contrast, ODE sampling performs better for MeanFlow-XL/2, which attains its best FID of 2.39 at timestep 0.35. In Table 1, we select intermediate sampling timesteps that balance FID and FDD; see Table 3 for details."
        },
        {
            "title": "6 CONCLUSION",
            "content": "Our work provided principled analysis of the MeanFlow framework, analyzing its objective and establishing the necessity of flow matching supervision during training. Motivated by this understanding, we proposed the α-Flow objective as generalization of MeanFlow loss, allowing us to train consistently stronger few-step image generation models from scratch."
        },
        {
            "title": "7 REPRODUCIBILITY STATEMENT",
            "content": "We are committed to ensuring the reproducibility of our results. To this end, we include all the necessary implementation details in Section F, ensuring that our methodology can be faithfully reproduced. We will publicly release our source training, inference, and evaluation code, as well as the pre-trained checkpoints for ImageNet-1K 2562."
        },
        {
            "title": "REFERENCES",
            "content": "Michael Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. arXiv preprint arXiv:2209.15571, 2022. David Berthelot, Arnaud Autef, Jierui Lin, Dian Ang Yap, Shuangfei Zhai, Siyuan Hu, Daniel Zheng, Walter Talbott, and Eric Gu. Tract: Denoising diffusion models with transitive closure time-distillation. arXiv preprint arXiv:2303.04248, 2023. Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. URL https://openai.com/research/ video-generation-models-as-world-simulators. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248255. Ieee, 2009. Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. Kevin Frans, Danijar Hafner, Sergey Levine, and Pieter Abbeel. One step diffusion via shortcut models. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=OlzB6LnXcS. Zhengyang Geng, Mingyang Deng, Xingjian Bai, Zico Kolter, and Kaiming He. Mean flows for one-step generative modeling. arXiv preprint arXiv:2505.13447, 2025a. Zhengyang Geng, Ashwini Pokle, Weijian Luo, Justin Lin, and Zico Kolter. Consistency models In The Thirteenth International Conference on Learning Representations, 2025b. made easy. URL https://openreview.net/forum?id=xQVxo9dSID. Yi Guo, Wei Wang, Zhihang Yuan, Rong Cao, Kuan Chen, Zhengyang Chen, Yuanyuan Huo, Yang Zhang, Yuping Wang, Shouda Liu, et al. Splitmeanflow: Interval splitting consistency in few-step generative modeling. arXiv preprint arXiv:2507.16884, 2025. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusionbased generative models. Advances in neural information processing systems, 35:2656526577, 2022. 10 Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine. Analyzing and improving the training dynamics of diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2417424184, 2024. Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, and Stefano Ermon. Consistency trajectory models: Learning probability flow ODE trajectory of diffusion. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=ymjI8feDTD. Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Tuomas Kynkaanniemi, Tero Karras, Miika Aittala, Timo Aila, and Jaakko Lehtinen. The role of imagenet classes in fr echet inception distance. arXiv preprint arXiv:2203.06026, 2022. Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=PqvMRDCJT9t. Xingchao Liu, Chengyue Gong, and qiang liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=XVjTT1nw5z. Cheng Lu and Yang Song. Simplifying, stabilizing and scaling continuous-time consistency models. In The Thirteenth International Conference on Learning Representations, 2025. URL https: //openreview.net/forum?id=LyJi5ugyJx. Martin Marek, Sanae Lotfi, Aditya Somasundaram, Andrew Gordon Wilson, and Micah Goldblum. Small batch size training for language models: When vanilla sgd works, and why gradient accumulation is wasteful. arXiv preprint arXiv:2507.07101, 2025. Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An empirical model of large-batch training. arXiv preprint arXiv:1812.06162, 2018. Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 41954205, 2023. Yansong Peng, Kai Zhu, Yu Liu, Pingyu Wu, Hebei Li, Xiaoyan Sun, and Feng Wu. Flow-anchored consistency models. arXiv preprint arXiv:2507.03738, 2025. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PmLR, 2021. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512, 2022. Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. In European Conference on Computer Vision, pp. 87103. Springer, 2024. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pp. 22562265. pmlr, 2015. 11 Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021a. URL https://openreview.net/ forum?id=St1giarCHLP. Yang Song and Prafulla Dhariwal. In The Twelfth International Conference on Learning Representations, 2024. URL https:// openreview.net/forum?id=WNzy9bRDvG. Improved techniques for training consistency models. Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben In InternaPoole. Score-based generative modeling through stochastic differential equations. tional Conference on Learning Representations, 2021b. URL https://openreview.net/ forum?id=PxTIG12RRHS. Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 3221132252. PMLR, 2329 Jul 2023. Peng Sun, Yi Jiang, and Tao Lin. Unified continuous generative models. arXiv preprint arXiv:2505.07447, 2025. Fu-Yun Wang, Zhaoyang Huang, Alexander Bergman, Dazhong Shen, Peng Gao, Michael Lingelbach, Keqiang Sun, Weikang Bian, Guanglu Song, Yu Liu, et al. Phased consistency models. Advances in neural information processing systems, 37:8395184009, 2024. Zidong Wang, Yiyuan Zhang, Xiaoyu Yue, Xiangyu Yue, Yangguang Li, Wanli Ouyang, and Lei Bai. Transition models: Rethinking the generative learning objective. 2025a. Zidong Wang, Yiyuan Zhang, Xiaoyu Yue, Xiangyu Yue, Yangguang Li, Wanli Ouyang, and arXiv preprint Lei Bai. Transition models: Rethinking the generative learning objective. arXiv:2509.04394, 2025b. Ling Yang, Zixiang Zhang, Zhilong Zhang, Xingchao Liu, Minkai Xu, Wentao Zhang, Chenlin Meng, Stefano Ermon, and Bin Cui. Consistency flow matching: Defining straight flows with velocity consistency. arXiv preprint arXiv:2407.02398, 2024. Tianwei Yin, Michael Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and Bill Freeman. Improved distribution matching distillation for fast image synthesis. Advances in neural information processing systems, 37:4745547487, 2024a. Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 66136623, 2024b. Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=DJSZGGZYVi. Linqi Zhou, Stefano Ermon, and Jiaming Song. In Forty-second International Conference on Machine Learning, 2025. URL https://openreview.net/ forum?id=pwNSUo7yUb. Inductive moment matching. Mingyuan Zhou, Huangjie Zheng, Zhendong Wang, Mingzhang Yin, and Hai Huang. Score identity distillation: Exponentially fast distillation of pretrained diffusion models for one-step generation. In Forty-first International Conference on Machine Learning, 2024."
        },
        {
            "title": "A RELATED WORK",
            "content": "Diffusion Models. Diffusion models have become dominant paradigm in generative modeling for vision domains (Sohl-Dickstein et al., 2015; Song & Ermon, 2019; Ho et al., 2020; Song et al., 2021a;b; Dhariwal & Nichol, 2021). The classical diffusion framework defines forward noising process and corresponding reverse process that model learns to approximate. Early works such as DDPM (Ho et al., 2020) and score-based generative modeling (Song & Ermon, 2019) demonstrated high-quality image generation, later extended to continuous-time SDEs and ODEs (Song et al., 2021b). (Dhariwal & Nichol, 2021) further improved sample fidelity with larger architectures and classifier guidance. More recently, the community has explored flow-based parameterizations that directly learn continuous velocity fields (Liu et al., 2023; Lipman et al., 2023; Albergo & VandenEijnden, 2022). These flow matching approaches simplify training, unify scoreand likelihoodbased models, and are used in large-scale systems such as Stable Diffusion 3 (Esser et al., 2024). Few-step Diffusion. Despite their quality, diffusion models are computationally expensive due to iterative sampling. large body of work accelerates sampling to few steps or even one step. Distillation-based approaches include progressive distillation (Salimans & Ho, 2022; Berthelot et al., 2023), and often incorporate adversarial objectives (Yin et al., 2024b;a; Zhou et al., 2024; Sauer et al., 2024). UCGM (Sun et al., 2025) develops unified training scheme for multi-step and fewstep diffusion-based methods. closer research direction (which our method follows as well) includes the methods which are trained from scratch and support fewand even one-step generation by design. Consistency Models (CMs) (Song et al., 2023) learn to map noisy inputs directly to clean data by enforcing selfconsistency. Extensions improve stability and scalability (Song & Dhariwal, 2024; Lu & Song, 2025; Geng et al., 2025b). Trajectory-based methods learn the dynamics of the entire denoising process, enabling arbitrary jumps along the diffusion path. PCM (Wang et al., 2024) scale consistency distillation to large scale models and optimize with preselected time intervals. Shortcut diffusion models (Frans et al., 2025) learn direct mappings with shortcut constraints. MeanFlow (Geng et al., 2025a) predicts time-averaged velocities with continuous consistency, while Guo et al. (2025) explore this idea for discrete consistency. Hybrid approaches combine consistency and flow matching: Consistency-FM (Yang et al., 2024) enforces velocity self-consistency, FACM (Peng et al., 2025) anchors consistency to flow objectives, and IMM (Zhou et al., 2025) matches the output distributions via moment matching instead of exact outputs. Consistency Trajectory Models (CTM) (Kim et al., 2024) generalize consistency training to support transitions between any two timesteps, combining one-step generation with progressive refinement. Transition Models (TiM) (Wang et al., 2025a) derive an exact continuous-time dynamics equation for arbitrary-step transitions. These methods achieve oneto few-step sampling with steadily improving fidelity."
        },
        {
            "title": "B LIMITATIONS",
            "content": "Our α-Flow loss enables high-quality training of discrete MeanFlow models without requiring 0) remains JVP computation. However, in practice, the continuous objective (i.e., setting α important, likely due to the biasvariance trade-off inherent in the consistency objective (Song et al., 2023; Song & Dhariwal, 2024). We occasionally observed unstable training in large-scale models with guidance integration, both for the vanilla MeanFlow model and our α-Flow variant. Thus, our framework should not be viewed as silver bullet for addressing the well-known instability issues of consistency models Geng et al. (2025b). The α-Flow objective uses pure flow matching supervision up to ks iterations, after which the consistency objective is applied. Before this point, the models few-step performance is weak, which can make progress harder to monitor. Our gradient analysis provides actionable insights but remains empirical; it does not fully explain, from theoretical perspective, why flow matching is so critical for consistency. Although we motivate larger batch sizes for fine-tuning by the high variance of the consistency loss, the observed improvements (see Table 4) may instead reflect that small batches are more 13 sensitive to hyperparameters (Marek et al., 2025), and that beyond certain size, batch-size scaling exhibits diminishing returns (McCandlish et al., 2018)."
        },
        {
            "title": "C FAILED EXPERIMENTS",
            "content": "We also wish to share with the community several experiments that did not succeed during the course of this project. Some of these directions were likely underexplored on our side, while others may represent genuine dead-ends. Nevertheless, we believe documenting them may serve as useful reference for future work. We devoted several weeks to exploring decomposed training of the MeanFlow objective with individually tuned weighting functions for each term, drawing inspiration from EDM Karras et al. (2022) to map out the design space. Unfortunately, every configuration we attempted produced worse results than the default adaptive loss heuristic, which was particularly frustrating outcome. Consistency sampling (see Figure 4) did not provide the improvements we had anticipated. In0.5, which coincides with the default terestingly, the optimal midpoint consistently emerged at MeanFlow setting. We suspect this effect is related to the training distribution, which has mode slightly lower 0.5. Following the original work, we employed logit-normal distribution with location parameter 0.4. We experimented with LoRA fine-tuning and introduced separate prediction heads for vanilla velocity and mean velocity. Neither approach yielded promising results. We conducted roughly 50 ablations on the train-time noise schedule for vanilla MeanFlow models. None resulted in noticeably better performance, even when factorizing the joint distribution p(t, r) into p(t)p(r t) and exploring alternative supervision distributions for flow matching in parallel. We investigated additional representation alignment losses Yu et al. (2025) with the aim of accelerating convergence in MeanFlow models. However, the observed gains were insufficient to justify the added complexity of the training framework. We also experimented with different EMA schedules, but these attempts did not lead to meaningful improvements."
        },
        {
            "title": "D PROOFS OF THINGS",
            "content": "D.1 LOSS DECOMPOSITION Proof. The MeanFlow loss is given by: (cid:34)(cid:13) (cid:13) (cid:13) (cid:13) MF(θ) = Et,r,zt uθ(zt, r, t) vt + (t r) duθ (zt, r, t) dt (cid:35) (cid:13) 2 (cid:13) (cid:13) (cid:13) (unpacking the norm and regrouping terms yields) uθ(zt, r, t) (cid:123)(cid:122) LTFM(θ) vt 2 2 (cid:20) (cid:105) (cid:125) + Et,r,zt (cid:124) (t r) = Et,r,zt (cid:124) (cid:104) (cid:34) + Et,r,zt 2 (t r) v(zt, (cid:124) D.2 α LOSS UNIFICATION x) duθ (zt, r, t) dt (cid:123)(cid:122) Does not depend on θ + (t θ (zt, r, t) (cid:123)(cid:122) LTCc (θ) (cid:13) (cid:13) (cid:13) (cid:13) r)2 duθ (zt, r, t) dt (cid:21) (cid:125) duθ (zt, r, t) dt (cid:13) 2 (cid:13) (cid:13) (cid:13) 2 (cid:35) (cid:125) (9) (10) (11) Proof of theorem 1. The proof for flow matching and shortcut models is straightforward. We will only show the proof for the third bullet point. For brevity, lets set = s and α = 14 . r α(θ) = Et,r,zt (cid:20) t (cid:13) (cid:13) (cid:13) (cid:13) uθ(zt, r, t) vt t t (cid:20) (i) = Et,r,zt (cid:13) (cid:13) (cid:13) (cid:13) uθ(zt, r, t) (cid:18) vt t uθ (ztt, r, (cid:35) , t) (cid:13) 2 (cid:13) (cid:13) (cid:13) 2 (ii) = Et,r,zt (cid:20) r (cid:18) uθ (zt, r, t) duθ (zt, r, t) dt (cid:13) (cid:13) (cid:13) (cid:13) uθ(zt, r, t) uθ (zt, r, t) vt (t r) duθ (zt, r, t) dt + (cid:0)2t(cid:1) r uθ (zt, r, t) + (cid:35) , (cid:19)(cid:13) 2 (cid:13) (cid:13) (cid:13) 2 (12) (cid:0)2t(cid:1) (cid:35) , (cid:19)(cid:13) 2 (cid:13) (cid:13) (cid:13) 2 where (i) uses the Taylor expansion over uθ (ztt, r, uθ (ztt, r, t) = uθ (zt, r, t) + (cid:0)2t(cid:1) , t): duθ (zt, r, t) dt (cid:0)2t(cid:1). Thus, duθ (zt, r, t) dt α(θ) θ 2t = and (ii) uses the fact that lim α0 θ α(θ) = lim Et,r,zt t0 t (cid:20) 2 (cid:18) = lim t0 vt = lim t0 Et,r,zt (cid:20) 2 (cid:18) vt (t (cid:19) (cid:18) t (cid:20) 2 MF(θ), =Et,r,zt = θ θ uθ(zt, r, t) (cid:18) uθ(zt, r, t) r) duθ (zt, r, t) dt (t uθ (zt, r, t) + θ uθ(zt, r, t) (cid:18) (cid:19) uθ (zt, r, t) (cid:19)(cid:19)(cid:21) (cid:0)2t(cid:1) , r) duθ(zt, r, t) dt uθ (zt, r, t) + (cid:0)2t(cid:1) (cid:19)(cid:21) , θ uθ(zt, r, t) (cid:18) uθ(zt, r, t) vt + (t r) duθ (zt, r, t) dt (cid:19)(cid:21) , (13) Proof of equivalence with consistency model. By setting vs,t = vt, = 0 and uθ(zt, 0, t) = (zt fθ(zt, t)) /t, = and α = , we have: α(θ) = Et,r,zt uθ(zt, r, t) vt (cid:20) (cid:13) (cid:13) (cid:13) (cid:13) t r uθ (ztt, r, (cid:35) , t) (cid:13) 2 (cid:13) (cid:13) (cid:13) 2 (i) Et,zt (cid:20) (cid:13) (cid:13) (cid:13) (cid:13) zt fθ(zt, t) t vt ztt (ii) = Et,zt (cid:20) 1 tt fθ(zt, t) fθ (ztt, 15 (14) t) (cid:35) , (cid:13) 2 (cid:13) (cid:13) (cid:13) 2 fθ (ztt, (cid:21) (iii) = 2 t) CTd (θ), where (i) plug in the reparameterization and = 0, (ii) uses the fact that zt = ztt +t α(θ) could be reparameterized to . Since the discrete CT uses timestep partition to determine and t, the (iii) holds for special timestep partition given fixed α. From Theorem 2 in Lipman et al. (2023), because uθ(zt, r, t) is when = α independent of θ, we have: CTd (θ) with loss weighting function vt. Thus 1 tt MF(θ) = Et,r,zt = Et,r,zt (cid:34)(cid:13) (cid:13) (cid:13) (cid:13) (cid:34)(cid:13) (cid:13) (cid:13) (cid:13) uθ(zt, r, t) uθ(zt, r, t) vt + (t r) duθ(zt, r, t) dt (cid:35) , (cid:13) 2 (cid:13) (cid:13) (cid:13) v(zt, t) + (t r) duθ (zt, r, t) dt (cid:35) (cid:13) 2 (cid:13) (cid:13) (cid:13) + C, with constant independent of θ. Thus, v(zt, t) + (t r) duθ(zt, r, t) dt (cid:19)(cid:21) , θ MF(θ) (cid:20) = Et,r,zt (cid:20) 2 Et,zt (i) θ uθ(zt, r, t) θ fθ(zt, t) (cid:18) v(zt, t) (cid:18) uθ(zt, r, t) (cid:18) zt fθ(zt, t) dfθ (zt, t) dt dfθ (zt, t) dt = (cid:19) (cid:21) = Et,zt (cid:20) 2 1 θ fθ(zt, t) v(zt, t)+ fθ(zt, t)) (cid:19)(cid:21) , CTc (θ), (zt θ (15) (16) where (i) plug in the reparameterization and = 0, and use the fact that: (cid:18) v(zt, t) (zt = (cid:18) (cid:19) duθ(zt, = 0, t) dt dfθ (zt, t) dt 1 t2 (cid:19) fθ (zt, t)) (17) Thus θ MF(θ) could be reparameterized to CTc (θ) with loss weighting function 1 ."
        },
        {
            "title": "E ANALYSIS DETAILS",
            "content": "The detailed implementation of DiT-B/2 is provided in Table 3, where we adopt the DiT-B/2-non-cfg setting. For loss evaluation, at each checkpoint we use batch size of 128 and run 1000 iterations to compute the mean loss along with its 5% and 95% percentiles, which are reported in the figure. To TCc on measure the cosine similarity between different losses, we calculate the same batch and then compute their pairwise cosine similarities. This procedure is also repeated over 1000 iterations to obtain the mean similarity and its 5% and 95% percentiles, as shown in the figure. FM, and L TFM,"
        },
        {
            "title": "F IMPLEMENTATION DETAILS",
            "content": "Implementation details are shown in Table 3."
        },
        {
            "title": "G ADDITIONAL EXPERIMENTS",
            "content": "G.1 ABLATION STUDY OVER BATCH SIZE Training diffusion/flow-based models can be challenging due to the high variance of their gradients. Past research Zhou et al. (2025); Karras et al. (2022; 2024) often used large batch sizes (1024 or even 4096) to mitigate this issue. In this section, we fine-tune MeanFlow-XL/2 model (with implementation details in Table 3) for an additional 60 epochs using large batch size. 16 Table 3: Configurations on ImageNet 256 the main text. 256. B/2-non-cfg is our ablation and analysis model in Configs DiT-B/2-non-cfg DiT-B/2 DiT-XL/2 DiT-XL/2+ Network Architectures Params (M) FLOPs (G) Depth Hidden dim Heads Patch size Training hyperparameters Training steps Batch size for training Fine-tuning steps Batch size for fine-tuning Dropout Optimizer lr schedule lr Adam (β1, β2) Weight decay EMA half-life Gradient clipping norm Autoencoder used α-Flow hyperparameters Ratio of = (r, t) sampler vs,t Whether to use EMA for uθ Adaptive weight Schedule of α γ ks ke η CFG training κ CFG triggered if is in Whether use EMA for CFG 2-NFE Sampling Method Intermediate timestep 131 23.1 12 768 12 22 400K 256 131 23.1 12 768 12 2 1.2M 256 676 119.0 28 1152 16 2 2 1.2M 256 0.0 Adam Kingma & Ba (2014) constant 0.0001 (0.9, 0.95) 0.0 6931 16 sd-vae-ft-ema 676 119.0 28 1152 16 2 2 1.2M 256 75K 50% vt No Table 2 (b) Table 5 (a) Table 5 (a) Table 5 (b) 25 Table 2 (b) Table 5 (c) 25% 50% logitnorm(0.4, 1.0) vt No vt No ω = α/ (cid:0)2 2 + c(cid:1) 25 0 1.2M 5 103 25 600K 1M 5 103 600K 1M 5 103 1.0 1.0 [0.0, 1.0] No 0.2 0.92 [0.0, 0.75] No 0.2 0.92 [0.0, 0.75] No ODE 0.5 ODE 0.5 consistency consistency 0.55 0. As shown in Table 4, batch size of 512 achieved the best 1-NFE FID of 3.05 and FDD of 164.3. batch size of 1024, however, yielded the best FDD of 93.4. Overall, batch size of 1024 performed well across all metrics, so we designate this configuration as MeanFlow-XL/2+. The same setting is applied to fine-tune the MeanFlow-XL/2 model, leading to the MeanFlow-XL/2+ results in Table 1. Our proposed α-Flow-XL/2+ model outperforms MeanFlow-XL/2+ in several key metrics: 1-NFE FID (2.58 vs. 3.06), 1-NFE FDD (148.4 vs. 165.7) and 2-NFE FID (2.15 vs. 2.16), only worse in 2-NFE FDD (96.8 vs. 93.4). These results demonstrate the overall effectiveness of our α-Flow method. Notably, the results in Table 4 are obtained using labels sampled from the ImageNet dataset distribution, whereas the results in Table 1 use randomly generated labels. In general, sampling labels from the ImageNet distribution leads to lower FID scores compared to using random labels. 17 Algorithm 3 α-Flow: Sampling # 1 = t1 > t2 > ... > tN = 0 :sequence of timesteps = randn like(x) for in range(N): = + 1 if consistency sampling: = - tn * fn(z, r=0, t=tn) = + tm * randn like(x) elif ODE sampling: = - (tn - tm) * fn(z, r=tm, t=tn) Batch Size 256 512 1024 2048 4096 NFE NFE 2 FID FDD FID FDD 3.13 167.2 2.31 97.1 3.05 164.3 2.21 95.2 3.06 165.7 2.16 93.4 3.29 169.6 2.10 96.6 3.13 168.9 2.16 95.1 Table 4: Ablation study over the fine-tuning batch size using the data distribution over class labels. vs,t uθ uθ vt vt uθ FID FDD Loss weight EMA 188.1 Non-EMA 319.0 202.8 59. EMA Non-EMA 1761.6 4009.9 1832.3 964.6 ω = 1 ω = 1/ (cid:0)2 ω = 1/ (cid:0)2 ω = α/ (cid:0)2 2 + c(cid:1)0.5 2 + c(cid:1) 2 + c(cid:1) FID 59.2 55.0 52.2 49. FDD 964.6 918.5 883.6 845.2 (a) Reformulate the training objective. (b) Adaptive loss. α 102 5 103 2 103 1 10 FID 49.7 46.2 50.3 57.2 FDD 845.2 860.8 833.0 863.7 Method Shortcut Model = v(zt, tx) + Adaptive loss + α = 0.005 MeanFlow FID 59.8 59.2 49.7 45.6 43. FDD 1017.3 964.6 845.2 857.8 822.3 (c) Consistency step ratio. (d) Overall ablation study. Table 5: Ablation study over α-Flow. G.2 ABLATION STUDY OVER α-FLOW DESIGN SPACE (0, 1). We use DiTThis section contains an ablation study on α-Flow, specifically for α B/2-non-cfg model (see Table 3) that is pre-trained on flow matching for 200k iterations and then fine-tuned on α-Flow for another 200k iterations. Across all experiments, α remains constant, and the ratio of = is 25 %. Training objective. Here, we set α = 102. Table 5(a) shows that the model only converge when vs,t was set to vt and without using EMA for uθ . This is key difference from Shortcut Models Frans et al. (2025), which set vs,t = uθ . We suspect their objective only works when α is larger (e.g., 0.5). Adaptive loss. Geng et al. (2025a) uses an adaptive weight: ω = 1/ (cid:0) From Equation (12), we could derive limα0 MF + c). MF. When α is close 0, we approximate α + c) = α/α. This gives us new adaptive weight, ω = 1/ ( 2 + c(cid:1) as both and α is very small. As shown in Table 5(b), this new weight performs 2 MF as α/ (cid:0) better empirically, especially compared to the original MeanFlow adaptive weight. 2 + c(cid:1) = 1/ ( 2 α/α + c) α = α α/ ( L Consistency step ratio. Ablating the α in Table 5 (c) reveals that α = 5 consistency step ratio. This value was then used as the clamping value for our schedule. 103 to be the optimal Table 5(d) shows that by combining these improvements, our discrete α-Flow approach significantly reduces the performance gap between Shortcut models and the MeanFlow model."
        },
        {
            "title": "H LLM USAGE",
            "content": "As requested by the ICLR 2026 policy3, we disclose the usage of Large Language Models in this section. LLMs were primarily used in two capacities: Coding assistance for experiments. LLMs provided code auto-completion functionality to ease the process of implementing and analyzing the experiments. Writing assistance for paper writing. We used LLMs to assist with grammar and phrasing validation while working on the submission."
        },
        {
            "title": "I RANDOM VS BALANCED CLASSES FOR FID COMPUTATION",
            "content": "We treat EDM series (Karras et al., 2022; 2024) as the standard in FID (Heusel et al., 2017) evaluations, which use randomly sampled class label (from 0 to 999) for each sample in constructing 50,000 synthetic examples with the model. We found curious way to decrease the FID values by up to 10% by using balanced class sampling: instead of using 50,000 independently sampled random classes, one can generate 50 samples for each of 1000 classes. This greatly improves FID results, but not FDD (i.e., Frechet Distance in the DINOv2 (Oquab et al., 2023) feature space) or FCD (Kynkaanniemi et al., 2022) (i.e., Frechet Distance in the CLIP-L-based (Radford et al., 2021) feature space). Since it is not standard practice in the community, we only report it separately from the random class sampling results and with the appropriate notice. But we emphasize that it might be more reasonable way to evaluate FID since it reduces the variance (we are less likely to sample an unlucky set of classes). We provide the results for it in Table 6. Method Class sampling Params Epochs NFE NFE 2 FID FDD FCD FID FDD FCD MeanFlow-XL/2 Random [1..1000] 676M 240 3.47 185.8 3.39 2.46 108.7 2.40 α-Flow-XL/2 (ours) Random [1..1000] 676M 240 2.95 164.6 3.14 2.32 105.7 2.42 α-Flow-XL/2+ (ours) Random [1..1000] 676M 240+60 2.58 148.4 3.07 2.15 96.8 2.31 MeanFlow-XL/2 α-Flow-XL/2 (ours) α-Flow-XL/2+ (ours) 3.33 182.8 3.34 2.26 106.1 2.36 676M 240 676M 240 2.81 162.4 3.10 2.16 103.2 2.37 676M 240+60 2.44 147.2 3.04 1.95 94.6 2.30 Balanced Balanced Balanced Table 6: Balanced vs random class sampling for FID, FDD and FCD. It is curious to observe that while it greatly improves FID results, FDD and FCD are barely affected. We believe that this constitutes one more reason for the community to switch from FID to more robust metrics which correlate better with human perception, like FDD and FCD. 3https://iclr.cc/Conferences/2026/AuthorGuide"
        },
        {
            "title": "J ADDITIONAL EXPLORATION OF THE MEANFLOW LOSS",
            "content": "(a) cos (LFM , LTCc ) (b) cos (LTFM, LFM ) (c) cos (LTFM, LMF) (d) cos (LFM , LMF) (e) cos (LTCc , LMF) (f) cos (LTCc , LTFM) Figure 6: Average cosine similarities between the gradients of different losses ( for DiT-B/2 MeanFlow model trained with 0% and 75% of flow matching. TFM, FM, CTc, MF)"
        },
        {
            "title": "K ADDITIONAL VISUALIZATIONS",
            "content": "i 2 / - o a - 2 / - o - α - + 2 / - o - α - 2 / - o - n i 2 / - o - α - + 2 / - o - α - Figure 7: Uncurated samples (seeds 1-16) for Class 15 (robin) for NFE=1. Figure 8: Uncurated samples (seeds 1-16) for Class 15 (robin) for NFE=2. 2 / - o a - 2 / - o - α - + 2 / - o - α - 2 / - o - n i 2 / - o - α - + 2 / - o - α - Figure 9: Uncurated samples (seeds 1-16) for Class 29 (axolotl) for NFE=1. Figure 10: Uncurated samples (seeds 1-16) for Class 29 (axolotl) for NFE=2. 2 / - o a - 2 / - o - α - + 2 / - o - α - 2 / - o - n i 2 / - o - α - + 2 / - o - α - Figure 11: Uncurated samples (seeds 1-16) for Class 33 (loggerhead) for NFE=1. Figure 12: Uncurated samples (seeds 1-16) for Class 33 (loggerhead) for NFE=2. 2 / - o a - 2 / - o - α - + 2 / - o - α - 2 / - o - n i 2 / - o - α - + 2 / - o - α - Figure 13: Uncurated samples (seeds 1-16) for Class 88 (macaw) for NFE=1. Figure 14: Uncurated samples (seeds 1-16) for Class 88 (macaw) for NFE=2. 2 / - o a - 2 / - o - α - + 2 / - o - α - 2 / - o - n i 2 / - o - α - + 2 / - o - α - Figure 15: Uncurated samples (seeds 1-16) for Class 89 (cockatoo) for NFE=1. Figure 16: Uncurated samples (seeds 1-16) for Class 89 (cockatoo) for NFE=2. 2 / - o a - 2 / - o - α - + 2 / - o - α - 2 / - o - n i 2 / - o - α - + 2 / - o - α - Figure 17: Uncurated samples (seeds 1-16) for Class 127 (white stork) for NFE=1. Figure 18: Uncurated samples (seeds 1-16) for Class 127 (white stork) for NFE=2. 2 / - o a - 2 / - o - α - + 2 / - o - α - 2 / - o - n i 2 / - o - α - + 2 / - o - α - Figure 19: Uncurated samples (seeds 1-16) for Class 279 (arctic fox) for NFE=1. Figure 20: Uncurated samples (seeds 1-16) for Class 279 (arctic fox) for NFE=2. 2 / - o a - 2 / - o - α - + 2 / - o - α - 2 / - o - n i 2 / - o - α - + 2 / - o - α - Figure 21: Uncurated samples (seeds 1-16) for Class 980 (volcano) for NFE=1. Figure 22: Uncurated samples (seeds 1-16) for Class 980 (volcano) for NFE=2. 2 / - o a - 2 / - o - α - + 2 / - o - α - 2 / - o - n i 2 / - o - α - + 2 / - o - α - Figure 23: Uncurated samples (seeds 1-16) for Class 975 (lakeside) for NFE=1. Figure 24: Uncurated samples (seeds 1-16) for Class 975 (lakeside) for NFE=2."
        }
    ],
    "affiliations": [
        "Department of EECS, University of Michigan",
        "Snap Inc."
    ]
}