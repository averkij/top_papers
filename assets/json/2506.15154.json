{
    "paper_title": "SonicVerse: Multi-Task Learning for Music Feature-Informed Captioning",
    "authors": [
        "Anuradha Chopra",
        "Abhinaba Roy",
        "Dorien Herremans"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Detailed captions that accurately reflect the characteristics of a music piece can enrich music databases and drive forward research in music AI. This paper introduces a multi-task music captioning model, SonicVerse, that integrates caption generation with auxiliary music feature detection tasks such as key detection, vocals detection, and more, so as to directly capture both low-level acoustic details as well as high-level musical attributes. The key contribution is a projection-based architecture that transforms audio input into language tokens, while simultaneously detecting music features through dedicated auxiliary heads. The outputs of these heads are also projected into language tokens, to enhance the captioning input. This framework not only produces rich, descriptive captions for short music fragments but also directly enables the generation of detailed time-informed descriptions for longer music pieces, by chaining the outputs using a large-language model. To train the model, we extended the MusicBench dataset by annotating it with music features using MIRFLEX, a modular music feature extractor, resulting in paired audio, captions and music feature data. Experimental results show that incorporating features in this way improves the quality and detail of the generated captions."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 4 5 1 5 1 . 6 0 5 2 : r SonicVerse: Multi-Task Learning for Music Feature-Informed Captioning Anuradha Chopra1, Abhinaba Roy1, and Dorien Herremans1 1Singapore University of Technology and Design anuradha_chopra@mymail.sutd.edu.sg, {abhinaba_roy, dorien_herremans}@sutd.edu.sg"
        },
        {
            "title": "Abstract",
            "content": "Detailed captions that accurately reflect the characteristics of music piece can enrich music databases and drive forward research in music AI. This paper introduces multi-task music captioning model, SonicVerse, that integrates caption generation with auxiliary music feature detection tasks such as key detection, vocals detection, and more, so as to directly capture both low-level acoustic details as well as high-level musical attributes. The key contribution is projection-based architecture that transforms audio input into language tokens, while simultaneously detecting music features through dedicated auxiliary heads. The outputs of these heads are also projected into language tokens, to enhance the captioning input. This framework not only produces rich, descriptive captions for short music fragments but also directly enables the generation of detailed time-informed descriptions for longer music pieces, by chaining the outputs using large-language model. To train the model, we extended the MusicBench dataset by annotating it with music features using MIRFLEX, modular music feature extractor, resulting in paired audio, captions and music feature data. Experimental results show that incorporating features in this way improves the quality and detail of the generated captions."
        },
        {
            "title": "Introduction",
            "content": "Music captioning represents challenging task that has been the focus of research efforts in recent years (Manco et al., 2021). An automated system for accurate music captioning would be helpful in myriad of ways. First, by allowing music feature capturing through centralized system that outputs in organic formats, opening up the complex domain of music theory to broader audience. Second, by potentially generating training datasets for music-to-text models, text-to-music models, and, music question-answering tasks (Liu et al., 2024). system that produces detailed captions, describing both the musical characteristics of piece and how they evolve over time, would allow the generation of datasets for text-to-music models, which can accept prompts that include the desired temporal evolution within pieces and convert them into generated compositions. Ironically, this task remains an open challenge due to the lack of an extensive dataset. number of approaches have attempted to mitigate this issue by augmenting smaller existing datasets with predefined modifications (Doh et al., 2023) (Melechovsky et al., 2024). Currently, the largest opensource dataset with music and matching text captions is JamendoMaxCaps Roy et al. (2025), which contains over 362k songs. Other datasets are often much smaller, such as MusicBench containing around 50k audio samples (Melechovsky et al., 2024). Generative AI models typically need millions of data points, hence this motivates our study to create captioning system that can generate captions with both music technical as well as more general musical feature descriptions for musical pieces. We propose that by training model to generate detailed captions on short segments, these can then Proceedings of the 6th Conference on AI Music Creativity (AIMC 2025), Brussels, Belgium, September 10th-12th Figure 1: Overview of the proposed caption generation framework. be chained using Large Language Model to produce comprehensive captions that encompass the musical features as well as the temporal evolution of the piece. Existing music captioning systems predominantly focus on generating captions that capture high-level, qualitative features such as the mood and potential setting suitable for the music. We refer to these as \"soft features\", in contrast to \"hard features\" such as key, instrumentation, vocals, or other technical musical components. Limited prior work has specifically targeted the extraction and inclusion of hard musical features in caption generation. Pseudo captioning techniques such as LP-MusicCaps Doh et al. (2023) would only extract features and then use an LLM to rewrite them into caption. Or to create the MusicBench dataset Melechovsky et al. (2024), music features where first extracted and merged with the original caption using an LLM. They found that training the model on such an enriched dataset improved the model to include dedicated musical feature sin the captions. Directly integrating features into captioning model has only been done by Gardner et al. (2023), however, the accuracy of the music features extracted by some these systems has not been extensively evaluated. Open-source captioning systems lack the development of comprehensive approach that generates captions incorporating detailed music theory components. Therefore, the creation of system trained on creative commons data, capable of capturing concrete musical features and seamlessly integrating them into descriptive natural language outputs remains an open research challenge. To address these questions, this paper proposes novel caption generation model that is trained on comprehensive dataset featuring labels for wide range of concrete musical features, such as key, instrumentation, genre, mood, vocals, beats, and chords. The model employs multi-task projector that operates on the music embeddings to output feature extractor-guided tokens for input into large language model for caption generation. By utilizing multi-task learning algorithm with auxiliary heads to predict the individual feature-related information, the model is able to convert these extracted features into language tokens that can be effectively incorporated into the captioning process. Furthermore, the music embeddings are also directly converted into language tokens, enabling the capture of more abstract, qualitative \"soft\" features to enhance the descriptiveness of the generated captions. Experiments conducted in this study demonstrate that the incorporation of these music feature extractors within the token projection model leads to significant improvements in the quality of the generated captions, as measured by n-gram metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and METEOR (Banerjee and Lavie, 2005). Finally, We explore how we can generate temporally-informed long captions for full-length pieces of music by leveraging LLM-chaining. The primary contributions of the paper are as follows: 1. We propose novel multi-task learning framework that jointly learns music captioning and musical feature prediction. This approach enables effective training on smaller, open-source datasets by leveraging auxiliary supervision. 2 2. We introduce chaining mechanism with large language models (LLMs) to generate temporally-aware, coherent captions for long-form music. This allows our model to describe the musical progression over time using fine-grained clip-level captions. 3. Our architecture eliminates the need for external music feature extractors by integrating feature prediction directly into the captioning pipeline. The full model, SonicVerse, is released as open source to support reproducibility and future research1. The rest of the paper is structured as follows. Section 2 discusses the related work. Section 3 details the method and architecture of our model. Followed by section 4 that describes the experimental setup, including the dataset and metrics used. Finally, Section 5 discusses the results followed by conclusion."
        },
        {
            "title": "2 Related Work",
            "content": "Music captioning techniques can be viewed as sub-domain of the broader field of audio captioning. Consequently, many methods developed for music captioning draw from general audio description approaches. Existing research can broadly be classified into three categories: pseudo-caption generation using music tags and large language models (LLMs), encoder-decoder frameworks for cross-modal translation, and token projection methods that integrate music representations into LLMs. One class of methods focuses on generating pseudo-captions from music feature labels and tags, which are reformulated into structured prompts for LLMs to generate final captions (Doh et al., 2023). These approaches do not process raw audio themselves and instead rely entirely on dedicated feature extractors. recent example is LP-MusicCaps (Doh et al., 2023), which addresses the lack of annotated data by using GPT-3.5 Turbo to create synthetic dataset of over 2.2 million pseudocaptions for over 500k audio clips. This dataset comprises subsets such as LP-MusicCaps-MC, built from 10-second clips in the MusicCaps dataset (Agostinelli et al., 2023); LP-MusicCaps-MTT, using 30-second clips from Magna-Tag-A-Tune (Law et al., 2009); and LP-MusicCaps-MSD, drawing on the Million Song Dataset (Dataset, 2011). Captions are generated from features through prompts for the four predefined tasks: writing, summarisation, paraphrasing, and attribute prediction. They are evaluated using both automatic metrics (BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGE-L (Lin, 2004), BERT-Score (Zhang et al., 2019)) and human listening tests. cross-modal model trained on LP-MusicCaps has demonstrated strong zero-shot and transfer learning performance (Doh et al., 2023). However, the quality of the generated captions is fundamentally bounded by the prompt design, the accuracy of the music feature prediction models, and the inherent capabilities of the LLM, since these systems lack direct audio input and therefore lack any direct acoustic cues. second group of methods uses encoder-decoder frameworks to learn mappings between music and text representations through latent spaces. For example, the Synaesthesia model (Kuang et al., 2022) employs dataset of 1,955 Western classical recordings paired with professional descriptions to train cross-modal translation model. Audio and text are encoded separately and aligned using group topology preservation loss (Kuang et al., 2022), encouraging semantic consistency. While the method benefits from aligned professional annotations, its reliance on classical music limits generalization to other genres. Furthermore, the authors point out that the generated captions occasionally exhibit formulaic patterns and grammatical inconsistencies. third and increasingly prominent category involves projecting encoded music features into the token space of LLMs, allowing for joint inference over both audio and text inputs. MusiLingo (Deng et al., 2023), for instance, uses the MERT-330M encoder (Li et al., 2023) to extract music representations, which are then projected into token embeddings for frozen Vicuna-7B model. Pretraining is performed on the LP-MusicCaps-MSD dataset (Doh et al., 2023) for captioning, followed by fine-tuning on music question-answering using the MusicInstruct dataset (Deng et al., 2023). Similarly, the Music Understanding LLaMA (MU-LLaMA) model (Liu et al., 2024) maps MERT-encoded audio (Li et al., 2023) through custom adapter into the input space of Metas LLaMA model. The model is used for augmenting captioning datasets such as MusicCaps (Doh et al., 2023) and MagnaTagATune (Law et al., 2009) with synthetic Q&A pairs generated by MosaicML 1The model with online demo is available at https://github.com/AMAAI-Lab/SonicVerse MPT-7B (MosaicML NLP Team et al., 2023). The augmented dataset is used to train multi modal interface for captioning and reasoning over music. Several other models build on similar principles. For instance, SALMONN (Tang et al., 2023) combines Whisper (Radford et al., 2023) and BEATs (Chen et al., 2022) encoders with windowlevel Q-former to extend LLMs for general audio understanding, including speech and music tasks. The LLaRK model (Gardner et al., 2023), on the other hand, incorporates explicit music attributes like key, tempo, and instrumentation. It uses Jukebox-5B (Kamuni et al., 2024) as the encoder and multimodal projection layer for the incorporation of music features along with the music encodings. However, LLaRKs weights are not publicly released. Other notable work includes BLAP (Bootstrapping LanguageAudio Pre-training) (Lanzendörfer et al., 2025), which aims to match the performance of large-scale captioning systems using significantly less data. This model combines the CLAP encoder (Wu* et al., 2023) with frozen Flan-T5 model and Q-Former for latent space alignment. While it can achieve good comparative results with lesser data and smaller architecture, it tends to produce concise captions with lesser details, as compared to the reference MusicCaps. FUTGA (Wu et al., 2024) introduces temporally-enhanced generative augmentation to generate fine-grained, segment-level captions by aligning synthetic labels with human annotations from HarmonixSet (Nieto et al., 2019). This is achieved by automatically annotating smaller segments of twoto five-minute audio clips and aligning synthetic captions with human annotations from HarmonixSet (Nieto et al., 2019), FUTGA enables time-localized descriptions that aid retrieval and segment-aware generation tasks. While capable of identifying structural segments of music such as intro, verse, chorus, etc. the weights for BEATs (Chen et al., 2022) are unavailable publicly at the time of writing. Finally, the recent QWEN2-Audio model by Huawei (Chu et al., 2024), is large scale audio language model (LALM), which was trained on large amount of speech (370k hours), sound (10k hours), and music (140k hours) data. The model uses an encoder based on Whisper-large-v2 and frozen Qwen-7B LLM, and is evaluated on the AIR-Bench Chat-Benchmark-Music dataset (Yang et al., 2024). This large model outperforms other LALMs such as SALMONN (Tang et al., 2023). We note that the evaluation process does not capture accuracy of the music features detected, but rather measures for overall descriptiveness and coherence. Although MusiLingo (Deng et al., 2023), MU-LLaMA (Liu et al., 2024), and BLAP (Lanzendörfer et al., 2025) provide publicly available weights, none offer an explicit music-feature extraction module. SALMONN (Tang et al., 2023) and FUTGA (Wu et al., 2024) depend on the BEATs encoder (Chen et al., 2022), whose weights are not currently available on their hosted links. LLaRK (Gardner et al., 2023) focuses on structured music attributes but does not release its projection layers, and QWEN2-Audio (Chu et al., 2024) relies on large proprietary datasets and unpublished code. In addition, many of these models, with the exception of FUTGA Wu et al. (2024), do not generate time-informed long form captions for full length audio. In this work, we aim to address these gaps by releasing both our model and its weights. Our architecture (see Figure!2) explicitly targets the detection and integration of detailed musical features and is trained exclusively on creative commons datasets, ensuring full reproducibility as well as legality. Moreover, it also provides mechanism to caption short chunks of long music piece, and chain it together, to retrieve long captions that capture the temporal evolution of the input audio with general description as well as music features."
        },
        {
            "title": "3 Proposed music captioning model",
            "content": "In this section, we outline the details of the proposed SonicVerse architecture. Our aim is to capture high-level music features, such as instrumentation and key, during the captioning task. To achieve this, we employ multi-task learning approach that integrates the latent representations of these concrete music features directly into the captioning pipeline. The proposed model learns to predict various musical features with multitask setup, and projects them as input for the language model. Concurrently, the original music encodings are also projected to form input language model tokens. These inputs are concatenated and the resulting dual representation provides latent representations of the music clip as whole for learning general features, while also incorporating the specific music features to guide the captioning process. Finally, these projected tokens are concatenated with the query tokens and passed into pre-trained large language model to generate the final caption. 4 Figure 2: Overview of the proposed SonicVerse architecture. Mathematically, let RT denote the input music clip, where is the number of audio frames and the feature dimension. Our model processes this input and produces caption using: Caption = LLM ([zcontent zfeature q]) (1) where zcontent RM and zfeature RN are language tokens derived from music content and music features respectively, represents textual query tokens, and is the embedding dimension of the pre-trained language model. The variable is the number of tokens representing the music content and the number of tokens representing the textual query. The overall architecture of our music captioning model consists of three primary components: 1) Music Encoder, 2) Multi-tasking Projector, and 3) Pre-trained Large Language Model. We describe the design choices and justification for each of these elements below. 3.1 Music Encoder To select the encoder, we first consider the ability to extract information required for language token prediction, from audio clips, as well as capturing musical attributes such as instruments, genre, mood/theme, key, vocals/instrumental, and vocals gender. Second, we looked at the ability to preserve feature information while still producing efficient embeddings. As the audio encoder, we chose the use MERT (Music undERstanding model with large-scale selfsupervised Training) (Li et al., 2023). MERT leverages multi-task learning, using RVQ-VAE for acoustic information and CQT for musical information. It consists of transformer architecture with self-attention, to model long range dependencies and hierarchical music patterns. MERT has been validated on 14 diverse MIR (music information retrieval) tasks and achieved state-of-the-art performance. The MERT encoder outputs the hierarchical embedding H: = MERT(x) RLT where = 13 is the number of representation layers, is the temporal resolution, and = 768 is the embedding dimension. Each layer captures dedicated acoustic information, which could be related to musical features such as tonal key or genre. For instance, the authors of MERT report that the higher layers do not perform particularly well for the genre detection task (Li et al., 2023) and suggest empirically choosing layers for tasks or using learnable weighted average2. (2) 3.2 Multi-task projector The multi-layered output of the MERT encoder, and its ability to capture diverse musical attributes make it natural choice for our music captioning architecture. The projector component of our model plays crucial role in leveraging these rich music representations to inform the captioning process. The module passes each fixed length audio clip through the MERT encoder in Equation 2 to obtain the embeddings H. These MERT embeddings are then fed into multi-task projection architecture consisting of music content projector (Hcontent) and music feature projector (Hfeature) pathway as detailed in Figure 2. First, the music embedding projector computes learned layer-weighted average combined with time-averaged representation of the MERT features and passes it through parallel fully connected layers and GeLU units. The approach to use MERT embeddings as input to the feature projector using learned layer-weighted average was selected based on the suggestions of the MERT authors3. The shape of the MERT embeddings is initially D. After time averaging, the shape of the embeddings becomes D, thus eliminating the time dimension, resulting in fixed number of tokens regardless of the input audio length. The learned weighted-averaged time-averaged embeddings are passed through set of parallel multilayer perceptrons with GeLU activations, MLPcontent. The result is set of tokens, zcontent, suitable as input for the frozen LLM. Hcontent = (cid:88) ℓ=1 αℓ H[ℓ], with (cid:88) ℓ=1 αℓ = 1, αℓ 0 zcontent = MLPcontent 1 (cid:88) t=1 Hcontent[t] (3) (4) Here, αℓ represents the learned weights for the representation layer ℓ which is L, while H[ℓ] represents the ℓth layer of H. The tokens, zcontent, while not representative of textual word embeddings directly, are learned against the frozen LLMs embedding by training on known captions. This ensures that they are learned such that they are aligned with English words that describe the music content. Secondly, the music feature projector is multi-task network that captures discrete musical attributes via shared backbone. For this shared backbone, we also compute the learned layer-weighted average combined with time-averaged representation of the MERT features, represented by Hshared. Hshared = (cid:88) ℓ= βℓ H[ℓ], with (cid:88) ℓ=1 βℓ = 1 (5) 2https://huggingface.co/m-a-p/MERT-v0 3https://huggingface.co/m-a-p/MERT-v 6 In Equation 5, βℓ represents the learned weights for the representation layer ℓ which is L, while H[ℓ] represents the ℓth layer of H. Let the total number of music features that are predicted in the music feature extractor be K. Each task head fk (for = 1, . . . , K) predicts set of probabilities, ˆyk for specific music feature: ˆyk = σ fk"
        },
        {
            "title": "1\nT ′",
            "content": "T (cid:88) t=1 Hshared[t] , ˆyk [0, 1]Ck (6) Next, each music feature vector is passed through set of parallel multi-layer perceptrons with GeLU activations, represented by MLPfeat. The output of each multi-layer perceptron is an individual input token for the frozen LLM. All output tokens are concatenated to form set of tokens zfeature, that represent the predicted music features, projected into the language space. These output tokens are learnt against the frozen LLMs embedding, and become aligned with English words that describe the music features, as they are trained with known English captions. zfeature = concat (MLPfeat (ˆyk)) , [1, K] (7) The latent vectors zcontent and zfeature are then combined with with the textual query tokens and fed into the pre-trained LLM (Mistral-7) (Jiang et al., 2023) to generate the captions. Finally, the training objective is defined as the weighted sum of the captioning and feature classification losses: = λcap Lcap + (cid:88) k= λk Lk (8) where: Lcap is the cross-entropy loss for captioning. Lk = BCEWithLogits(ˆyk, yk) is the binary classification loss for task k. λcap, λk are task weighting hyperparameters. The key innovation in this projector architecture is the incorporation of music feature extractor heads within the music-to-language token projection, which directly infuses concrete musical attributes into the token representations to guide the captioning process. 3.3 LLM-enabled chaining The above architecture generates captions for short fragments of music (10s or 30s segments in our training dataset). We further leverage these captions to generate elaborate captions that describe the temporal evolution throughout long piece of music, as shown in Figure 1. To achieve this, the full-length song is first clipped into fixed length (10 second) chunks, which are input into the model described above. After retrieving captions for all of the chunks, they are combined with curated prompt that requests the generation of detailed description of the entire song, while retaining musical feature information. The prompt is fed into state-of-the-art LLM. In our experiments, we opted to use GPT-4 (Achiam et al., 2023) for the generation of single long caption that captures the music feature details and their evolution over the length of the music piece. Mathematically, each audio segment is divided into fixed length clips {x(i)}N chunk level caption c(i). i=1, generating The chunk level captions are concatenated into structured prompt to give the long caption. c(i) = SonicVerse(x(i)) The prompt used to generate the caption through chaining is given below. LongCaption = GPT4(p) 7 (9) (10) Given the following chronological 10-second chunk descriptions of single piece, write one flowing, detailed description of the entire song its structure, instrumentation, and standout moments. Mention transition points in terms of time stamps. If the description of certain chunks does not seem to fit with those for the chunks before and after, treat those as bad descriptions with lower accuracy and do not incorporate the information. Retain concrete musical attributes such as key, chords, tempo. Chunks for {song_name}\" : 1. 0 to 10 seconds: {Chunk 1 caption} 2. 10 to 20 seconds: {Chunk 2 caption} ... Full song description:"
        },
        {
            "title": "4 Experimental setup",
            "content": "We discuss the dataset used, baselines, and implementation details in this section. 4.1 Dataset We use three datasets for training the proposed architecture. First, we use the Jamendo dataset (Bogdanov et al., 2019), which consists of collection of around 55k music clips (30s) with tags for instrument, mood/theme and genre. We extend this dataset to include labels for vocals, vocals gender, and key, using MIRFLEX (Chopra et al., 2024). Secondly, for general music-language alignment, we train the model using the Magna-Tag-A-Tune dataset (Law et al., 2009), which consists of around 25k music clips (30s), with paired captions generated using MosiacML (MosaicML NLP Team et al., 2023) for the training of the captioning MuLLaMA (Liu et al., 2024). We extend this dataset using MIRFLEX (Chopra et al., 2024) to get labels for instrument, mood / theme, genre, key, vocals and vocals gender. Finally, we use the MusicBench training set which consists of around 26k clips (after excluding those samples that are part of MusicCaps eval set) with paired captions (10s). These captions explicitly include music features. This dataset was also extended using MIRFLEX to obtain labels for instrument, mood / theme, genre, key, vocals and vocals gender. 4.2 Baselines To assess the impact of incorporating explicit music feature representations within our music-to-token projector, we performed an ablation study, comparing two baselines: Baseline A: Control baseline uses only the music context projector, Baseline : Music feature projector augmented baseline extends the control baseline by integrating the music feature extraction heads followed by the music feature to language token projection, thus creating our proposed SonicVerse. The section below describes the training procedures and evaluation metrics used for both baselines. 4.3 Implementation details We fixed the number of tokens + to 60 across all experiments. In the control baseline (Baseline A), single MLP projector is used to map aggregated music embeddings directly to language tokens. In the music feature projector augmented baseline (Baseline B) we allocated = 35 tokens for the music embedding projection and Nk = 5 tokens for each task head, namely key detection, instrument detection, mood / theme detection, genre detection, vocals and vocals gender detection. Both baselines are pretrained on the Magna-Tag-a-Tune (Law et al., 2009) + MosiacML datasets (MosaicML NLP Team et al., 2023), and then finetuned on the MusicBench Training Set (Melechovsky et al., 2024). Because the datasets lack ground-truth music feature labels, we augment them using MIRFLEX (Chopra et al., 2024), which generated labels for key, instrument, mood/theme, genre, vocals and vocals gender, chords as well as downbeat timings. This process provides us with dataset of aligned triplets of audio, text caption, and feature labels for the multi-task training of Baseline B. 8 We note that using the MIRFLEX feature extractors rather than ground truth music features might introduce some noise and bias into the dataset. However, the final evaluation of the model is done on the ground trust caption, which is not dependent on the extracted features. When we evaluate the accuracy of the presence of music features in the generated captions in Section 3, we compare to these to the original captions without referring to MIRFLEX. Specific to Baseline B, the feature extractor is first pretrained with the Jamendo dataset (Bogdanov et al., 2019) extended using MIRFLEX (Chopra et al., 2024) and finetuned on augmented music features in the Magna-tag-atune (Law et al., 2009) + MosiacML dataset. weighted sum of the losses for each task is used as the back-propagated loss, with weight λk = 0.2for each task (instrument detection, mood detection, genre detection, key detection, vocals detection). During the captioning pretraining step, we employ the extended Magna-Tag-aTune dataset which is comprised of 30s music clips. The loss is weighted with λcap = 1.0 for the captioning loss and λk = 0.1 for all individual feature extraction task individually. The captioning model is then finetuned on the extended MusicBench dataset. To evaluate the performance of both configurations, we used the traditional metrics from natural language processing (NLP) as well as designed some music-specific metrics."
        },
        {
            "title": "4.4 Evaluation metrics",
            "content": "All text caption (NLP) evaluations were performed on the MusicCaps evaluation set (Agostinelli et al., 2023), while all music-feature specific evaluations were performed on the MusicBench test set (Melechovsky et al., 2024), following the MusicCaps split (Agostinelli et al., 2023). NLP Metrics Similar to other captioning papers (Doh et al., 2023; Kuang et al., 2022; Deng et al., 2023; Liu et al., 2024; Tang et al., 2023), we used metrics that are generally used for assessing translation accuracy and sentence similarity. To assess the accuracy of the generated captions against ground-truth annotations we used the following key metrics: BLEU Score (Papineni et al., 2002) quantifies the overlap between the predicted and reference captions by matching n-grams (up to certain length). BLEU-4 Score (Papineni et al., 2002) focuses on 4-gram comparison making it more suitable for longer, complex sentences and phrases. METEOR Score (Banerjee and Lavie, 2005) aligns predictions with references, using synonyms, stemming and exact matching. It penalises longer captions that do not match significantly. ROUGE Score (Lin, 2004) measures the overlap of n-grams, word sequences and word pairs, prioritising recall over precision score. It measures how much of the reference appears in the prediction. BERT Score (Zhang et al., 2019) measures semantic similarity through the use of BERT embeddings. It captures the context and meaning rather than matching words. Music Metrics While the NLP metrics may give us measure to compare similarity of captions, they do not explicitly capture how much of the music features are accurately mentioned in the caption. To enable this evaluation, we designed metrics that provide insight in the overlap between the (correct) mention of the key, instrumentation, and vocals between the caption and audio. These metrics are obtained using state-of-the-art LLM (GPT-4) with the below prompt that requests comparison of the reported features across the prediction and reference. You are tasked with comparing two descriptions of musical piece. Evaluate the following aspects: 1. Key Match: Does the musical key specified in the prediction match that in the reference? If missing in reference, respond n/a. 2. Instrumentation Match: Do the instruments described in the prediction correspond to those in the reference? If missing in reference, respond n/a. 9 3. Genre Match: Does the genre implied by the description in the prediction match that in the reference? If missing in reference, respond n/a. 4. Mood/Theme Match: Does the mood or theme of the music in the prediction match that in the reference? If missing in reference, respond n/a. 5. Vocal Presence Match: Does the presence or absence of vocals in the prediction match that in the reference? 6. Vocal Gender Match: If vocals are present, does the gender of the vocals (male or female) in the prediction match that in the reference? If missing in reference, respond n/a Return your answer as JSON object with the following keys: key_match, instrument_match, genre_match, mood_match, vocal_presence_match, vocal_gender_match. Values should be yes, no, or n/a if the attribute is not applicable. --- Prediction: {prediction_text} Reference: {reference_text} The resulting accuracy for each feature is then calculated as the total number of matches (output yes) divided by the total number of valid pairs (output yes or no, excluding all n/a output)."
        },
        {
            "title": "5 Results",
            "content": "In this section, we report results of our ablation study, as well as compare the performance of our model with state-of-the-art models, and analyse an example output of caption for full-length song. 5.1 Ablation study In order to show the importance of guiding the caption with the use of auxiliary music feature detection tasks, we conduct an ablation study using the baselines described in Section 4.2. The results are shown in the Table 1. The inclusion of music featureprojected tokens in Baseline leads to superior performance across all evaluated NLP metrics except METEOR (Banerjee and Lavie, 2005), suggesting that our feature-augmented projector better captures musical nuances in the generated captions. Table 1: Ablation study results on the MusicBench test set (MusicCaps split) Model BLEU BLEU-4 METEOR ROUGE BERT Baseline Baseline (SonicVerse) 0.3456 0.3484 0.1799 0.1824 0.2507 0.2506 0.2621 0.2622 0.8716 0. 5.2 Comparison with state-of-the-art We present comparison with state-of-the-art models in Table 2 in terms of NLP metrics. All of the models are evaluated on the MusicCaps Test dataset. The results were taken from the respective papers, except BLAP and QWEN2-Audio. The latter two models were run to obtain the values. When examining the table, we should keep in mind that models such as QWEN2-Audio were trained on large private datasets (potentially including our test set MusicCaps). The table thus serves as rough benchmark. It is good to observe that SonicVerse can outperform most models, even recent ones such as BLAP that are also trained on limited datasets. This confirms that our proposed model is able to generate high-quality music captions. 10 Table 2: Results of state-of-the-art music captioning models on the MusicCaps test set. Model BLEU BLEU-4 METEOR ROUGE BERT LP-MusicCaps MusiLingo SALMONN LLARK QWEN2-Audio BLAP SonicVerse - 0.308 - 0.28 0.3427 0.1024 0.3484 0.0605 - 0.055 0.14 0.2109 0.0406 0.1824 0.2239 0.216 - 0.28 0.2584 0.0746 0.2506 0.1303 0.217 0.218 0.25 0.2405 0.0688 0.2622 0.8451 0.868 - - 0.8816 0.8502 0. The metrics above, however, capture only the NLP similarities. As discussed in the previous section, we have designed dedicated metrics to assess if the caption accurately captures specific musical aspects such as key, instrument, genre, mood, vocals and gender of vocals. Table 3 shows the result of these metrics on the MusicBench test set for QWEN2-Audio, BLAP, as well as SonicVerse. We chose to compare with QWEN2-Audio as it is large model representing the state-of-the-art trained on private data. BLAP, on the other hand, represents smaller, open model trained on open data. The MusicBench test set was selected here, as annotations for the music features are available. In terms of captioning the correct key, SonicVerse outperforms all other models, however, as expected the model trained on internal data (QWEN2-Audio) takes the lead when it comes to most other features. The proposed SonicVerse does show competitive performance when comparing to the other model trained on open data (BLAP) as it outperforms on all but one of the music metrics. This again confirms the ability of our model to successfully capture music features in the captions, facilitated through the multi-task mechanism. Table 3: Results of the music metrics on the MusicBench test set (MusicCaps split). Model BLAP QWEN2-Audio SonicVerse Key 0.2162 0.2966 0. Instrument Genre Mood Vocals Gender 0.3231 0.6582 0.4337 0.2987 0.5486 0.4329 0.4643 0.6389 0. 0.7656 0.8808 0.8051 0.7797 0.7654 0.7764 5.3 Case study LLM-enabled caption chaining To demonstrate the capability of our proposed SonicVerse framework to generate long form captions, we examine case study. We use the popular music piece Bohemian rhapsody by Queen and feed it through our pipeline. The resulting caption is given below, and clearly captures the temporal evaluation through the piece. By training our model to recognise detailed musical features of short 10s fragments, this example demonstrates that LLM chaining can then be used to expand those detailed captions into temporally informed caption. For instance, the first segments mentions the choir part of Bohemian Rhapsody, whereas after 80s the song morphs into love song. This is very apt description of the feeling of those segments and how they change over time. In addition, our framework offers flexibility as the resulting output caption can easily be customized tweaked to focus on certain aspects or have certain length by changing the LLM instruction. \"Bohemian Rhapsody\" is dynamic and genre-blending piece that takes listeners on musical journey. The song begins with gospel-inspired section, featuring choir of female vocalists singing harmoniously in major key, accompanied by keyboard playing the melody. The tempo is brisk at 171.0 beats per minute, creating an uplifting and spiritual atmosphere. Around the 60-second mark, the song transitions into slower tempo, introducing male vocalist singing soft melody accompanied by piano and cello playing single note. The song takes on calming and soothing atmosphere, perfect for romantic movie soundtrack. The chord progression shifts to Bb, A#, Bb, and the beat counts to 4, with tempo of 70.0 beats per minute. At 80 seconds, the song morphs into slow tempo country love song with female vocalist singing melodically. The song is accompanied by keyboard playing the melody, bass guitar playing the root notes, and an acoustic guitar playing the melody. The song has romantic and emotional feel to it, with chord progression of B, Gm, F#m, B, Gm, F#m, and tempo of 117.0 beats per minute. At the 100-second mark, the song transitions into bluesy atmosphere with male vocalist singing in higher key. The melody 11 is played by harmonica and the rhythm is provided by an acoustic drum. The bass plays the root notes of the chords, and the chord progression is Bb, Eb, Bb, Eb, Bb, Eb. The tempo remains at 117.0 beats per minute. At 170 seconds, the song takes turn into the realm of instrumental rock, characterized by medium tempo with groovy electric guitar lead, punchy snare, groovy bass guitar, shimmering hi hats, punchy kick and snare hits, and groovy electric guitar solo. The song is in 4/4 time signature and has chord progression of Cm, Fm, Cm, Fm, Cm, Fm, Cm, Fm, Cm. At 200 seconds, the song becomes lively and energetic with fast tempo and catchy tune. The male vocalists sing in unison, creating harmonious and engaging melody. The song is filled with enthusiasm and passion, making it perfect addition to any party or celebration. The beat is quick and lively, with tempo of 170.0 beats per minute. The song is in the key of minor. At 260 seconds, the song transitions into heavy metal/rock song with fast tempo and aggressive electric guitar lead melody. The bass guitar plays the root notes of the chords while the acoustic drums provide simple beat. The atmosphere is intense and energetic, perfect for rock bar. The song is in the key of A# minor and has chord progression of A#m7, Dm7, A#m7, Dm7, A#m7, Dm7, A#m7. At 320 seconds, the song transitions into mellow and emotional soft rock song with male vocalist singing melodically. The piano and electric guitar play the main melody while the bass guitar and acoustic drum beat provide simple and slow rhythm. The song has sentimental and mellow atmosphere, making it perfect for romantic movie. The chord progression is Gm, C#m, Gm, C#m, Gm, C#m, Gm, C#m, Gm, C#m, Gm, C#m, Gm, C#m, Gm. At 340 seconds, the song transitions into slow tempo instrumental song with melancholic melody played on the piano. The music is emotional and sentimental, creating poignant atmosphere. Finally, at 360 seconds, the song concludes with medium tempo instrumental featuring loud crash of timpani and the sound of marimba in the background. The song is in the key of minor, with beat of 2 and chord sequence of E. The tempo of this final section is Andante, providing dramatic and climactic end to this musical journey.\" We invite the reader to explore to more examples of generated captions on our GitHub page4, where we have also made the model available as open source."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduce SonicVerse, an open-source multi-task music captioning system that integrates caption generation with auxiliary music feature detection to produce richer and more informative descriptions of musical content. By projecting both raw audio and detected features into shared language space, our model captures not only low-level acoustic attributes but also high-level musical semantics. By training the proposed multitask captioning model on the MagnaTag-A-Tune dataset (Law et al., 2009) with MosiacML (MosaicML NLP Team et al., 2023) captions and MIRFLEX-based (Chopra et al., 2024) features, and finetuning on the MusicBench (Melechovsky et al., 2024) dataset expanded with MIRFLEX-based features, we can explore the synergy between captioning and feature detection in unified framework, thus enabling our captioning model to be trained on smaller, open dataset. Experimental results confirm that this multi-task approach enhances caption quality as well as descriptive depth. The resulting model outperforms state-of-the-art models trained on open data. Beyond captioning short audio fragments, our framework facilitates scalable captioning of longer musical works through chaining strategies using large language model. This opens up promising directions for modeling the temporal structure of music. The resulting SonicVerse is made available online5 and as HuggingFace Spaces demo."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "This work has received support from SUTDs Kickstart Initiative under grant number SKI 2021_04_06 and MOE under grant number MOE-T2EP20124-0014."
        },
        {
            "title": "References",
            "content": "Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. (2023). Gpt-4 technical report. arXiv:2303.08774. 4https://github.com/AMAAI-Lab/sonicverse 5https://github.com/AMAAI-Lab/sonicverse 12 Agostinelli, A., Denk, T. I., Borsos, Z., Engel, J., Verzetti, M., Caillon, A., Huang, Q., Jansen, A., Roberts, A., Tagliasacchi, M., et al. (2023). Musiclm: Generating music from text. arXiv:2301.11325. Banerjee, S. and Lavie, A. (2005). Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pages 6572. Bogdanov, D., Won, M., Tovstogan, P., Porter, A., and Serra, X. (2019). The mtg-jamendo dataset for automatic music tagging. ICML. Chen, S., Wu, Y., Wang, C., Liu, S., Tompkins, D., Chen, Z., and Wei, F. (2022). Beats: Audio pre-training with acoustic tokenizers. arXiv:2212.09058. Chopra, A., Roy, A., and Herremans, D. (2024). Mirflex: Music information retrieval feature library for extraction. arXiv:2411.00469. Chu, Y., Xu, J., Yang, Q., Wei, H., Wei, X., Guo, Z., Leng, Y., Lv, Y., He, J., Lin, J., et al. (2024). Qwen2-audio technical report. arXiv:2407.10759. Dataset, M. S. (2011). Million song dataset. Deng, Z., Ma, Y., Liu, Y., Guo, R., Zhang, G., Chen, W., Huang, W., and Benetos, E. (2023). Musilingo: Bridging music and text with pre-trained language models for music captioning and query response. arXiv:2309.08730. Doh, S., Choi, K., Lee, J., and Nam, J. (2023). Lp-musiccaps: Llm-based pseudo music captioning. In Proceedings of the 24th International Society for Music Information Retrieval Conference (ISMIR), pages 409416, Milan, Italy. Gardner, J., Durand, S., Stoller, D., and Bittner, R. M. (2023). Llark: multimodal foundation model for music. arXiv:2310.07160. Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. (2023). Mistral 7b. arXiv:2310.06825. Kamuni, N., Jindal, M., Soni, A., Mallreddy, S. R., and Macha, S. C. (2024). Exploring jukebox: novel audio representation for music genre identification in mir. In 2024 3rd International Conference on Artificial Intelligence For Internet of Things (AIIoT), pages 16. IEEE. Kuang, Z., Zong, S., Zhang, J., Chen, J., and Liu, H. (2022). Music-to-text synaesthesia: Generating descriptive text from music recordings. Lanzendörfer, L. A., Pinkl, C., Perraudin, N., and Wattenhofer, R. (2025). Bootstrapping languageaudio pre-training for music captioning. In ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE. Law, E., West, K., Mandel, M. I., Bay, M., and Downie, J. S. (2009). Evaluation of algorithms using games: The case of music tagging. In ISMIR, pages 387392. Citeseer. Li, Y., Yuan, R., Zhang, G., Ma, Y., Chen, X., Yin, H., Lin, C., Ragni, A., Benetos, E., Gyenge, N., et al. (2023). Mert: Acoustic music understanding model with large-scale self-supervised training. arXiv:2306.00107. Lin, C.-Y. (2004). Rouge: package for automatic evaluation of summaries. In Text summarization branches out, pages 7481. Liu, S., Hussain, A. S., Sun, C., and Shan, Y. (2024). Music understanding llama: Advancing text-to-music generation with question answering and captioning. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 286290. IEEE. Manco, I., Benetos, E., Quinton, E., and Fazekas, G. (2021). Muscaps: Generating captions for music audio. In 2021 International Joint Conference on Neural Networks (IJCNN), pages 18. IEEE. 13 Melechovsky, J., Guo, Z., Ghosal, D., Majumder, N., Herremans, D., and Poria, S. (2024). Mustango: In Proceedings of the 2024 Conference of the Toward controllable text-to-music generation. North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 82868309. MosaicML NLP Team et al. (2023). Introducing mpt-7b: new standard for open-source, commercially usable llms. DataBricks (May, 2023) www. mosaicml. com/blog/mpt-7b. Nieto, O., McCallum, M. C., Davies, M. E., Robertson, A., Stark, A. M., and Egozy, E. (2019). The harmonix set: Beats, downbeats, and functional segment annotations of western popular music. In ISMIR, pages 565572. Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2002). Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311318. Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., and Sutskever, I. (2023). Robust speech recognition via large-scale weak supervision. In International conference on machine learning, pages 2849228518. PMLR. Roy, A., Liu, R., Lu, T., and Herremans, D. (2025). Jamendomaxcaps: large scale music-caption dataset with imputed metadata. In Proceedings of the International Joint Conference on Neural Networks (IJCNN), Rome, Italy. Tang, C., Yu, W., Sun, G., Chen, X., Tan, T., Li, W., Lu, L., Ma, Z., and Zhang, C. (2023). Salmonn: Towards generic hearing abilities for large language models. arXiv:2310.13289. Wu, J., Novack, Z., Namburi, A., Dai, J., Dong, H.-W., Xie, Z., Chen, C., and McAuley, J. (2024). Futga: Towards fine-grained music understanding through temporally-enhanced generative augmentation. arXiv:2407.20445. Wu*, Y., Chen*, K., Zhang*, T., Hui*, Y., Berg-Kirkpatrick, T., and Dubnov, S. (2023). Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation. In IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP. Yang, Q., Xu, J., Liu, W., Chu, Y., Jiang, Z., Zhou, X., Leng, Y., Lv, Y., Zhao, Z., Zhou, C., et al. (2024). Air-bench: Benchmarking large audio-language models via generative comprehension. arXiv:2402.07729. Zhang, T., Kishore, V., Wu, F., Weinberger, K. Q., and Artzi, Y. (2019). Bertscore: Evaluating text generation with bert. arXiv:1904.09675."
        }
    ],
    "affiliations": [
        "Singapore University of Technology and Design"
    ]
}