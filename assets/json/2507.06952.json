{
    "paper_title": "What Has a Foundation Model Found? Using Inductive Bias to Probe for World Models",
    "authors": [
        "Keyon Vafa",
        "Peter G. Chang",
        "Ashesh Rambachan",
        "Sendhil Mullainathan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Foundation models are premised on the idea that sequence prediction can uncover deeper domain understanding, much like how Kepler's predictions of planetary motion later led to the discovery of Newtonian mechanics. However, evaluating whether these models truly capture deeper structure remains a challenge. We develop a technique for evaluating foundation models that examines how they adapt to synthetic datasets generated from some postulated world model. Our technique measures whether the foundation model's inductive bias aligns with the world model, and so we refer to it as an inductive bias probe. Across multiple domains, we find that foundation models can excel at their training tasks yet fail to develop inductive biases towards the underlying world model when adapted to new tasks. We particularly find that foundation models trained on orbital trajectories consistently fail to apply Newtonian mechanics when adapted to new physics tasks. Further analysis reveals that these models behave as if they develop task-specific heuristics that fail to generalize."
        },
        {
            "title": "Start",
            "content": "What Has Foundation Model Found? Using Inductive Bias to Probe for World Models Keyon Vafa 1 Peter G. Chang 2 Ashesh Rambachan 2 Sendhil Mullainathan 2 5 2 0 2 0 1 ] . [ 2 2 5 9 6 0 . 7 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Foundation models are premised on the idea that sequence prediction can uncover deeper domain understanding, much like how Keplers predictions of planetary motion later led to the discovery of Newtonian mechanics. However, evaluating whether these models truly capture deeper structure remains challenge. We develop technique for evaluating foundation models that examines how they adapt to synthetic datasets generated from some postulated world model. Our technique measures whether the foundation models inductive bias aligns with the world model, and so we refer to it as an inductive bias probe. Across multiple domains, we find that foundation models can excel at their training tasks yet fail to develop inductive biases towards the underlying world model when adapted to new tasks. We particularly find that foundation models trained on orbital trajectories consistently fail to apply Newtonian mechanics when adapted to new physics tasks. Further analysis reveals that these models behave as if they develop task-specific heuristics that fail to generalize. 1. Introduction The promise of foundation models relies on central presumption: that learning to predict sequences can uncover deeper truths, or optimistically, even world model. While this idea is new in one sense, it is old in another. Hundreds of years ago, astronomers like Kepler discovered geometric patterns that could pinpoint the future locations of planets in the night sky. Newton would later expand on this progress to develop Newtonian mechanics, fundamental laws that could not only predict the movement of planets but also explain physical properties across the universe (Koestler, 1959; Gin1Harvard University 2MIT. Correspondence to: Keyon Vafa <kvafa@g.harvard.edu>. Proceedings of the 42 nd International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025 by the author(s). 1 gerich, 2004). This path from predicting sequences to understanding the deeper mechanisms that underlie them is not unique to physics. In biology, animal breeders noticed patterns in the traits of offspring long before their predictive insights inspired Mendel to develop theory of genetics. How would we know if foundation models have also made the leap from making accurate predictions to developing reliable world models? This paper develops framework for answering this question. Specifically, we create procedure that, when given foundation model and world model, tests whether the foundation model has learned that world model. We call this technique an inductive bias probe, and it is built on simple insight: the implicit world model of foundation model is revealed by how it extrapolates from small amount of information. This is inspired by how scientists use world models to make inferences from small amounts of data. Similarly, the inductive bias of foundation model reveals its world model. We first demonstrate this procedure using an example from physics. Specifically, we aim to replicate Keplers and Newtons experiments, albeit replacing the physicist with foundation model of orbital mechanics. Much like Kepler, the model is able to predict orbital trajectories, even for solar systems it has not seen. What would it mean for this foundation models inductive bias to be toward Newtonian mechanics? We demonstrate one tangible way to test this: we fine-tune the foundation model on small dataset where the output is exactly the force vector (a cornerstone of Newtonian mechanics) at each point in the trajectory. If the foundation models world model is toward Newtonian mechanics, it should have an inductive bias towards these force vectors. In contrast, Figure 1 shows that the model produces poor force vectors. More extremely, when we perform this exercise at larger scale across many solar systems, the laws of gravity it uses to generalize bear no resemblance to Newtons law  (Table 1)  . We further apply inductive bias probes in other domains with known world model: lattice problems and Othello games (Liu et al., 2022; Hazineh et al., 2023; Nanda et al., 2023b; Vafa et al., 2024). Across these domains, we find that neural sequence models have weak inductive biases What Has Foundation Model Found? Using Inductive Bias to Probe for World Models Figure 1: Each pair of panels illustrates the trajectory of planet in the solar system and its gravitational force vectors, comparing the true Newtonian forces (left) to the predicted forces (right) from transformer foundation model pretrained on orbital sequences and fine-tuned to predict forces. While the model excels at generating accurate predictions of planetary trajectories, it does not have an inductive bias toward true Newtonian mechanics; moreover, its force predictions recover nonsensical force law, as revealed by symbolic regression. toward the given world models. We also highlight practical implication: models that perform better on inductive bias probes have better performance when theyre fine-tuned to perform new tasks that rely on the underlying world model. Data and tasks. Let denote an input and denote some output. dataset = {(x1, y1), . . . , (xn, yn)} is collection of input-output pairs. task : is mapping between inputs and outputs. Taken together, our results provide direction for understanding the deficiencies of foundation models: if models inductive bias isnt toward known model of reality, what is it toward? We explore this question by examining whether these foundation models have alternative inductive biases. Our analysis reveals that these models instead behave as if they develop task-specific heuristics that fail to generalize. For physics, rather than learning one universal physical law, the foundation model applies different, seemingly nonsensical laws depending on the task its being applied to. In lattice and Othello, models have an inductive bias toward the set of legal next-tokens (e.g. boards legal next moves) rather than the world model itself. 2. Framework In this section, we lay out our framework for evaluating whether foundation model has learned postulated world model. We develop an inductive bias probe, which is procedure that evaluates the foundation models behavior as it adapts to new tasks. Foundation models: foundation model is learning algorithm which, when given dataset D, returns prediction function (cid:98)mD : that relates the input to the outputs. Foundation models can take many forms; for example, (cid:98)mD could be some pre-trained model that is fine-tuned on the dataset D, or it can be an LLM that is supplied in-context. World model: postulated world model is summarized by state space Φ and mapping ϕ : Φ that associates each input with some state ϕ(x) Φ. dataset is consistent with the world model if for each (x, y) D, the output is deterministic function of the state, = g(ϕ(x)) for some : Φ Y. 2.1. Comparing foundation models to world models There is challenge in defining what it means for foundation model to recover world model: foundation models and world models operate in different spaces. foundation model uses datasets to output predictions given inputs, whereas world model describes state structure implicit in that data. What Has Foundation Model Found? Using Inductive Bias to Probe for World Models Figure 2: An inductive bias probe measures whether foundation model has an inductive bias toward given world model. The probe involves repeatedly fitting foundation model to small, synthetic datasets and comparing the functions it learns to the functions in the given world model. One approach would be to mechanistically probe the foundation model, e.g. by comparing its weight-level representations to the postulated states in the world model. However, understanding the internal mechanisms of large models is challenging (Olah, 2022) and even then may not reflect how model actually behaves on new data (Casper et al., 2023). Another approach is to study the models behavior statically, on single task (Toshniwal et al., 2022; Vafa et al., 2024), but this doesnt capture how foundation models are used in the real world: as tools for new tasks. We take different approach, motivated by the no-freelunch theorem (Wolpert, 1996). Loosely speaking, the no-free-lunch theorem states that no learning algorithm can perform better than another one on average if any function could have generated the data it is applied to. Given limited data, learning algorithms must extrapolate to unseen inputs, and if any underlying function is possible, any such extrapolation must be equally good or bad. This means that every learning algorithm is better for some collection of possible functions those functions that it tends to learn when extrapolating from limited data. The functions that learning algorithm tends to learn represent its inductive bias. The idea of inductive bias offers connection between foundation models and world models. world model is restriction on the possible functions from inputs to outputs: only those that obey its state structure are possible. Consequently, foundation model that has learned postulated world model should have an inductive bias towards functions that obey the world models state structure. For example, physicists may train foundation model on sequences of planetary orbits. Since planetary orbits obey Newtonian mechanics, they might hope the model has an inductive bias toward functions of Newtonian mechanics (e.g. predicting the force vector between two planets). We develop an inductive bias probe for testing whether foundation models inductive bias matches the postulated world models state structure. The inductive bias probe repeatedly applies foundation model to synthetic datasets consistent with the world model and studies the extrapolated functions together (Figure 2). In each such simulation, we do not calculate the accuracy of the resulting extrapolations since there is no one accurate function; multiple ways to extrapolate may be allowed by the true world model. Instead, we evaluate whether the extrapolations resemble those that are allowed by the true world model. 2.2. Special case: finite state space and binary outputs. To provide more intuition for the inductive bias probe, we first consider the special case of binary output = {0, 1} and postulated world model with finite state space Φ. The two metrics we introduce in this setting are special cases of the general inductive bias probe defined in the next section. The inductive bias probe evaluates whether foundation models inductive bias is towards postulated world model. At high level, the probe repeatedly applies the foundation model to synthetic datasets consistent with the postulated world model and each time evaluates its predictions on held-out inputs. If the foundation models inductive bias is towards the postulated world model, its extrapolations should have two properties. First, the foundation models predictions should respect state: if two inputs x, map to the same state (ϕ(x) = ϕ(x)), the foundation model should have the same predicted outputs ( (cid:98)mD(x) = (cid:98)mD(x)) when applied across synthetic datasets. If not, it means that the foundation model fits functions that do not belong to the world model. Second, the foundation models predictions should distinguish state: if two inputs x, map to different 3 What Has Foundation Model Found? Using Inductive Bias to Probe for World Models Figure 3: An illustration of the inductive bias probe when the given world model has finite state space. Each row represents function and each column represents an input xi, with inputs belonging to the same state grouped together. The shading illustrates each functions value at the corresponding input. foundation model has low R-IB (middle) if it learns functions that divide states, while foundation model has low D-IB (right) if it learns function that merge states. states (ϕ(x) = ϕ(x)), the foundational model should typically have different predicted outputs ( (cid:98)mD(x) = (cid:98)mD(x)) across synthetic datasets. If not, then the foundation model does not fit functions that fully cover the world models allowable functions. These properties can be measured using two metrics. Let 1(y, y) denote the indicator for whether = y. We specify sampling distribution over consistent datasets PD and sampling distribution over inputs (Xi, Xj) PX PX . The foundation models inductive bias towards respecting state (R-IB) is EXi,Xj ,D[1( (cid:98)mD(Xi), (cid:98)mD(Xj)) ϕ(Xi) = ϕ(Xj)]. R-IB measures the similarity between the foundation models extrapolations on inputs in the same state under the postulated world model: higher R-IB indicates more similar predictions for the same states. The foundation models inductive bias towards distinguishing state (D-IB) is (1) 1 EXi,Xj ,D[1( (cid:98)mD(Xi), (cid:98)mD(Xj)) ϕ(Xi) = ϕ(Xj)]. (2) D-IB measures whether inputs that belong to different states under the postulated world model nonetheless receive consistently similar predictions by the foundation model: higher DIB indicates more dissimilar predictions for different states. Figure 3 illustrates both metrics. Together, R-IB and D-IB provide contrasting perspectives on foundation models implicit world model, analogous to precision and recall in binary classification. For example, while it is trivial for foundation model to achieve high RIB by making the same prediction on every input, its D-IB will suffer. Both metrics are needed to contrast foundation models inductive bias with the postulated world model. In this sense, the inductive bias probe captures behavior of foundation model that is not captured by standard probe tests (Nanda et al., 2023b), which measures how well simple predictive model (e.g., linear model) can predict state from foundation models intermediate representation. By contrast, the inductive bias probe directly analyzes how the foundation model behaves when adapted to synthetic tasks from the postulated world model. When there are many distinct state mappings that are predictable from foundation models internal representation, standard probes cannot distinguish which is actually being used by the model. Moreover, the standard probe is sensitive to how state is mechanistically represented by the chosen world model. For example, Nanda et al. (2023b) find that different representations of the Othello game board (one based on the standard board and another that inverts the board based on whose turn it is) lead to different results by standard probes. By contrast, because inductive bias probes only depend on state equality, they are insensitive to equivalent representations. To implement the inductive bias probe, practitioner must supply sampling distribution over consistent datasets PD and sampling distribution over inputs PX . In our experiments with finite state space and binary outputs (see Section 4), we sample consistent datasets by assigning each unique state the output 0 or 1 uniformly at random. 2.3. Inductive bias probe We now describe the inductive bias probe allowing for general outputs, state spaces, and tasks. For example, for sequences of two planets orbiting one another, the states could correspond to their relative positions, relative velocities, and the masses of each planet under Newtonian mechanics. We further introduce collection of admissible func4 What Has Foundation Model Found? Using Inductive Bias to Probe for World Models tions on state that govern the relationship between the state space and the output under the world model with each : Φ Y. For example, in some settings, we may expect the output to vary smoothly with the state, in which case could be the collection of K-Lipshitz functions. dataset is now consistent with the world model if for each (x, y) D, = g(ϕ(x)) for some G. Given sampling distribution over consistent datasets PD and sampling distribution over inputs PX , the inductive bias probe repeatedly applies the foundation model to sampled datasets, and then evaluates its predictions on heldout inputs. It measures how predictable the foundation models predicted outputs for one input are from those of another input across many synthetic datasets. The intuition is unchanged: inputs in similar states should be more predictable from one another than inputs from different states. We next formalize this property. Extrapolative predictability. We further specify family of predictors with such that : and loss function over outputs ℓ : R+. We define the extrapolative predictability between two inputs as (cid:98)I(xi, xj) = min hH EDP [ℓ(h( (cid:98)mD(xi)), (cid:98)mD(xj))], (3) which measures how predictable the foundation models predicted outputs for one input are from the other. Higher values of extrapolative predictability indicate higher levels of predictability. If foundation model behaves as if it extrapolates based on the postulated world model, the extrapolative predictability should be larger for inputs with more similar states. Oracle foundation model. As calibration, we calculate the extrapolative predictability for an oracle foundation model that is given access to the true state space Φ and admissible functions G. When applied to consistent dataset D, the oracle foundation model returns = arg min gG 1 (cid:80) (xi,yi)D ℓ(g(ϕ(xi)), yi). (4) (The loss function used here need not be the same as the loss function used to calculate extrapolative predictability.) The oracle extrapolative predictability is (xi, xj) = min hH EDP [ℓ(h(m D(xi)), D(xj))]. (5) Inductive bias towards the world model. The inductive bias probe compares the foundation models extrapolative predictability to that of the oracle. Specifically, the foundation models inductive bias towards the world model is defined as, for any 0 s, IB(s, s) = EXi,Xj [(cid:98)I(Xi, Xj) (Xi, Xj) s]. (6) 5 Figure 4: Inductive bias probe performance (Equation 6) for transformer pretrained on orbital trajectories. 45-degree line would indicate perfect inductive bias toward an oracle that extrapolates based on the Newtonian state vector. We calculate this quantity over grid of values 0 = s0 < s1 < < sm, visualizing how IB(s, s) varies over the grid. The foundation models inductive bias towards the world model can be interpreted like calibration curve: if the foundation model behaves like the oracle when applied to many small datasets, then IB(s, s) should lie on the 45degree line in this visualization (as illustrated in Figure 4). R-IB and D-IB are special cases of the foundation models inductive bias towards the world model (Equation 6). Consider the case in which the output is binary, Φ is finite, and is the collection of all mappings. Provided PD places positive probability on all possible consistent datasets and is limited to the identity function, there are only two possible values for the oracles extrapolative predictability, which occur when ϕ(xi) = ϕ(xj) and when ϕ(xi) = ϕ(xj). Consequently, the foundation models inductive bias towards the world model reduces to R-IB in in the former case (Equation 1) and D-IB (up to sign change) in the latter case (Equation 2). 3. Orbital Mechanics We illustrate these ideas by testing whether transformer trained to predict the locations of planets in motion has recovered Newtonian mechanics. We first train model to predict the location of planets across solar systems. Despite the models ability to accurately predict the future trajectories of planets, the inductive bias probe reveals that it has low inductive bias toward Newtonian mechanics. This is corroborated by the fact that when the model is fine-tuned to predict planets force vector cornerstone of Newtonian mechanics its predictions imply nonsensical law of gravitation. We find that the model has recovered piecemeal heuristics rather than compact world model; it recovers different law of gravitation depending on the slice of data it is applied to. Background. For centuries, astronomers and physicists have worked on predicting the orbits of planets around the What Has Foundation Model Found? Using Inductive Bias to Probe for World Models sun. groundbreaking model was offered by the astronomer Johannes Kepler in the 17th century. His model was based on geometric patterns: for example, that the orbit of each planet followed an ellipse with the sun at one of its foci. While the model could predict orbits with near-perfect level of precision, it couldnt explain why the planets obeyed these geometric orbits or be applied to new problems beyond predicting trajectories. 2017) to predict the next token of each sequence in the training set. We experimented between using a) continuous coordinates (and MSE loss) and b) discretized coordinates (with cross-entropy loss), finding the latter worked better. We discretize each position vector of each body in the solar system by creating 7K bins per coordinate (x, y), where the coordinates spans from -50 to 50 AU. We train for 25 epochs using 8 H100 GPUs. See Appendix for more training details. Later, Isaac Newton expanded on this model using new laws of motion, now known as Newtonian mechanics. These laws involved computing properties of the set of planets in motion, such as their relative velocities and masses. Using these properties, he could derive Keplers earlier laws for orbital trajectories, but also go beyond, understanding and formalizing other concepts like force and gravity. From Kepler to Newton, scientists were able to move beyond good predictive models of sequences to deeper understanding of them. In this section, we test whether transformer that can predict sequences of orbital trajectories is merely good sequence model, or whether it has also made the transition to providing world model. Data and pre-training. We first simulate dataset of sequences, where each sequence describes planets in motion around sun. To do this, we randomly sample initial conditions (e.g. the masses and positions of the planets and their initial relative velocities) to target the shape of orbits observed in known exoplanets (Kipping, 2013). We simulate each planets trajectory around the sun using Newtons laws of motion; because planet masses are much smaller than the suns, interactions between planets are minimal, so we omit them. To convert orbits into sequences, we record (x, y) coordinates of each planet and the sun across regular intervals, and interleave all the locations into single sequence with 1,000 observations. This means that each sequence denotes different solar system. We consider two different types of time intervals: fixed intervals, which uses the same 6-month interval for each sequence, and varied intervals, for which random half of the sequences use 6-month intervals and the other half use 1week intervals, with special token at the beginning indicating the interval length. For example, in solar system with planets and varied intervals, the first timestep encodes the interval length, the next observations are the (x, y) coordinates for each planet at the first point in time, and the next are the coordinates for each planet the appropriate timestep later, etc. We consider two training set sizes: 2B tokens (across 1M sequences) for fixed intervals and 20B tokens (across 10M sequences) for the varied intervals. We find similar results for models trained on each, so we report the results for the varied interval model unless noted otherwise. We train 109M parameter transformer (Vaswani et al., We evaluate model predictions on held-out data. The model makes good predictions: its R2 is above 0.9999, and it significantly outperforms baseline models that always predict the most recent position or the per-orbit mean  (Table 7)  . It can also generate long orbits with high degree of accuracy. Has the model recovered Newtonian mechanics? The transformers predictions reflect very good sequence model. But has it recovered Newtonian mechanics? To test this, we note that Newtonian mechanics dictate that each observation in sequence of orbits is governed by state vector consisting of the masses, relative velocities, and relative positions of each planet. Given the current state of trajectory, the next position of an orbit is deterministic. This is our world model; if foundation models inductive bias depends on Newtonian mechanics, it must be extrapolating based on this state vector. We use the inductive bias probe described in Section 2 to assess the models inductive biases. We create 100 synthetic datasets where the outputs are linear functions of the state of the sequence. We then fine-tune the transformer by training it to predict these functions. We measure the models extrapolative predictability across inputs (Equation 3) by considering to consist of the identity and the loss function ℓ to be MSE. We evaluate Equation 6 by comparing the model to an oracle that extrapolates based on state directly (we consider both linear models and 2-layer neural networks for the oracle, finding similar results). The inductive bias toward simple functions of Newtonian state is poor; see Figure 4 for visualization of the fixed-interval model. In other words, the models inductive bias is not toward Newtonian state; when it has to extrapolate, it makes similar predictions for orbits with very different states and different predictions for orbits with very similar states. For implementation details, see Appendix B.1. To understand the degree to which the model fails to apply Newtonian mechanics, we test its ability to predict specific quantities derived from Newtonian mechanics. Specifically, we consider each planets force vector, simple transformation of state given by Newtons law of gravitation: = m1m2 r2 er, which relates the force between planet and the sun to their masses m1, m2 and their squared distance r2 (in the direction er of its relative position). The force vector can be computed for each observation in sequence; force is simple transformation of state, so the 6 What Has Foundation Model Found? Using Inductive Bias to Probe for World Models Ground-truth law Estimated laws (cid:16) 1 sin(r0.24) m1m2 r2 sin(cid:0) Galaxy 1 Galaxy 2 cos(cid:0)cos(2.19 m1)(cid:1) Galaxy 3 cos(cid:0)sin( 0.48 m1 Galaxy 4 sin(cid:0)r + 8569.2 + 1 m1 Galaxy 5 cos(cid:0)cos(em2 )(cid:1) )(cid:1) (cid:1) (cid:1) + 1.45 (cid:17) 1 1 +m Table 1: Force equations recovered via symbolic regression of transformer pretrained on orbital mechanics and finetuned to different galaxy samples. The model recovers different equations for each sample, never recovering the true law. predictions of model that has recovered Newtonian mechanics should obey this law. We test this by creating sequence-to-sequence dataset where each input is trajectory and each output is the force vector on the planet implied by the state of the orbit. We first fine-tune the pretrained transformer to predict the force vector on orbits from our solar system, providing 1% of the true forces as training data. Figure 1 shows these force predictions are poor. To assess how close the model is to recovering Newtons law of gravitation, we further fine-tune it to predict the force magnitude on larger dataset of 10K solar systems. We then perform symbolic regression (using the PySR software (Cranmer, 2023)) of the predicted force magnitudes on the true values of m1, m2, and r. symbolic regression is method to search for symbolic expression that optimizes regression-like objective (Cranmer et al., 2020). When the symbolic regression is applied to the transformers predictions, the physical law is nonsensical (Figure 1). In contrast, an oracle trained on the true state predicts the force vectors well and symbolic regression recovers the true physical law (Figure 7 in Appendix C). See Appendix for implementation details and Appendix for similar experiment with LLMs. How can model perform so well at predicting orbit locations without having inductive biases towards the laws of physics that govern them? We study this question by applying the fine-tuned models force predictions to five different sets of randomly sampled galaxies (each consisting of many solar systems). We then perform symbolic regression on the force magnitude for each sample. The symbolic regression finds different implied law of gravitation for each sample  (Table 1)  . In contrast, the oracle trained on true state recovers the same (correct) law for each galaxy. These results show that rather than building single universal law, the transformer extrapolates as if it constructs different laws for each sample. 7 4. Other Applications We now apply the inductive bias probe to evaluate the extent to which foundation models obey known world models in other domains. Evaluating world models requires studying domains where theres state structure and ground-truth state is known. We study two such types of datasets: lattice problems and the board game Othello. Lattice. One common type of structure to assess models against is spatial structure, or lattices (Vafa et al., 2024; Liu et al., 2022). We study lattice setting that simulates an agent moving along line segment with finite number of positions. There is true state space consisting of states: Φ = {1, 2, . . . , S}. The language consists of sequences with three tokens: Σ = {L, , R}. The initial state of the sequence is 1. For token σ = R, the state increases by 1, while the state decreases by 1 for σ = and stays the same for σ =. When the state is 1, the state is at the boundary, so σ = is not valid token; similarly, when the state is S, σ = is not valid token. All tokens are valid for all other states. We randomly generate sequences of length 100 over the language by sampling move uniformly at random over the set of valid moves for each timestep. We consider different versions of the lattice problem, varying the number of states from 2 to 5. We consider sequences taken from training set containing 10M tokens, along with 100k hold-out tokens. Othello. We also study the board game Othello, common testbed for evaluating the world models of sequence models (Li et al., 2023; Nanda et al., 2023b; Hazineh et al., 2023; Vafa et al., 2024). The game consists of two players taking turns placing tiles on an 8x8 board. Each game of Othello is tokenized into sequence of at most 60 moves, where each token indicates which of the 60 squares the most recent tile was placed on (the middle four tiles are always occupied). The true state space Φ corresponds to all 8x8 boards and the mapping ϕ converts game sequences into states. We consider game sequences taken from training set containing 7.7M tokens, along with 300K hold-out tokens. Models. We study the properties for five classes of pretrained sequence models: RNNs (Elman, 1990), LSTMs (Hochreiter, 1997), transformers (Vaswani et al., 2017), Mamba (Gu & Dao, 2023), and Mamba-2 (Dao & Gu, 2024). We train each model using next-token prediction for each domain. By way of comparison, we also compare these pretrained models to untrained models that fine-tune from random initialization. See Appendix for more information. All pre-trained models perform well at next-token prediction, generating outputs that appear to obey state. Following Toshniwal et al. (2022), we measure the fraction of models top predictions that are legal in the underlying What Has Foundation Model Found? Using Inductive Bias to Probe for World Models Lattice (5 States) Othello Pre-training R-IB () D-IB () R-IB () D-IB () RNN (Elman, 1990) Untrained NTP trained 0.346 (0.026) 0.574 (0.026) 0.749 (0.027) 0.803 (0.032) 0.240 (0.019) 0.558 (0.021) 0.987 (0.002) 0.845 (0.019) LSTM (Hochreiter, 1997) Untrained NTP trained 0.456 (0.028) 0.782 (0.021) 0.718 (0.031) 0.921 (0.030) 0.506 (0.028) 0.649 (0.030) 0.672 (0.032) 0.448 (0.035) Transformer Untrained (Vaswani et al., 2017) NTP trained 0.268 (0.027) 0.483 (0.031) 0.742 (0.028) 0.677 (0.034) 0.714 (0.022) 0.668 (0.023) 0.840 (0.021) 0.593 (0.034) Mamba (Gu & Dao, 2023) Untrained NTP trained 0.260 (0.026) 0.571 (0.023) 0.771 (0.027) 0.866 (0.029) 0.342 (0.019) 0.597 (0.024) 0.933 (0.013) 0.734 (0.028) Mamba-2 (Dao & Gu, 2024) Untrained NTP trained 0.244 (0.026) 0.617 (0.021) 0.785 (0.026) 0.864 (0.029) 0.490 (0.020) 0.590 (0.022) 0.936 (0.008) 0.732 (0.027) Table 2: The inductive bias towards respecting state (R-IB) and inductive bias towards distinguishing state (D-IB) metrics (1 is perfect performance, 0 is equivalent to noninformative model). NTP-trained represents model pre-trained on next-token prediction, while untrained refers to model trained on the same synthetic tasks, initialized from scratch. architectures based on recurrent or state-space models. The results for Othello are depicted in Table 2. Here, all models perform worse than on the lattice problems, indicating poor inductive bias. Despite generating legal moves nearly 100% of the time when pretrained to play Othello, these models dont use the board as an inductive bias on new tasks. To understand the implications of these results, we study how different models transfer to new functions of state (the board). Specifically, we take the Othello dataset and construct new sequence-to-sequence datasets. The input sequence for each dataset is the original game transcript, and we consider three different output transformations that are functions of state. In Majority Tiles, each element of the output is 1 or 0 indicating where there are more black or white tiles in the board implied by the sequence so far. In Board Balance, each element of the output sequence indicates whether black has more pieces in the top half of the board or in the bottom half of the board. Finally, in Edge Balance, the output measures whether black has more pieces along the edge squares of the board. Each of these functions is deterministic function of state (the board), so foundation models that have inductive bias toward state should be better at transfer. The results are depicted in Table 9. The last row shows the (unsigned) correlation for each metric and the ratio, R-IB 1D-IB that summarizes the inductive bias measures in Table 2. There is strong correlation across all metrics; models that do better on inductive bias metrics transfer better to these functions of state. What are the inductive biases? These results show that models can perform well at predicting token sequences without appearing to learn the underlying world model. This raises the question: If foundation models inductive bias isnt toward given world model, what is it toward? Here, we consider one hypothesis motivated by the nextFigure 5: Inductive bias probe results (R-IB and D-IB) for the lattice problem as function of the underlying number of states. different model is pre-trained on data consistent with each number of states and its inductive bias for that state structure is recorded using the metrics in Section 2. state. Table 6 in Appendix shows the results. All models do very well across all datasets, e.g. every models top prediction is legal 90% of the time for Othello and legal 100% of the time for lattice problem with five states. Inductive bias probe results. We measure each models inductive bias using the procedure from Section 2 to assess the inductive bias of these models. The procedure involves fine-tuning each model to small datasets of randomly generated outputs and assessing whether the models inductive bias as measured by its extrapolations obeys state structure. We use the discrete version of the procedure for both models. The results for the lattice problem are depicted in Figure 5. While models have high inductive biases when the number of states is small, as the number of states increases, the inductive biases drop off. Notably, the transformer model consistently does worse than the other models, all of which have What Has Foundation Model Found? Using Inductive Bias to Probe for World Models Figure 6: On the left, true Othello board implied by sequence, and on the right, the predicted board from model fine-tuned to predict boards. Although the prediction has errors, the set of predicted next tokens exactly matches the true board. On the right, metrics about board reconstruction during fine-tuning. Consistently, even as Mamba models struggle to recover full boards, they recover them well enough such that the sets of valid next moves match those in the true boards. token pretraining objective: that when foundation models are applied to new tasks, they group together sequences with distinct states for which the set of legal next tokens are nevertheless equivalent. For example, in the board game Othello, two distinct boards can have the same set of allowable next moves. Therefore, models inductive bias might be toward boards with the same sets of allowable next moves rather than the true board itself. To first demonstrate this concept with Othello, we fine-tune foundation model pretrained on next-token prediction to predict the true board of each sequence. We record two metrics when we fine-tune: 1) whether the predicted board exactly matches the true board, and 2) whether the set of valid moves in the predicted board matches the set of valid moves in the true board. The results are depicted in Figure 6: surprisingly, even when the predicted board is incorrect, the set of legal moves frequently matches the set of legal moves from the true board. Rather than recovering the full board, the foundation model is often recovering enough of the board to calculate legal next moves. To quantify this hypothesis generally, we modify the inductive bias probe to test whether models inductive bias is toward next-token partitions of state. Recall that D-IB measures how similar extrapolations for two points with different states are one from another. If model is extrapolating based on which next-tokens are legal, sequences in different states that happen to have the same legal next tokens will have more similar predictions than sequences in different states that have different legal next tokens. let denote the next-token coarsening Specifically, of the state space such that q(x) = q(x) if and only if NextTokens(ϕ(x)) = NextTokens(ϕ(x)), where NextTokens(s) is the set of valid next tokens for state s. We decompose D-IB into two quantities. First, define Same(Xi, Xj) as the event that ϕ(Xi) = ϕ(Xj) but q(Xi) = q(Xj). We then define, D-IBq= = 1 [1( (cid:98)mD(Xi), (cid:98)mD(Xj)) Same(Xi, Xj)] , which measures how predictable the extrapolations for inputs associated with different states that have the same legal next tokens are. Similarly, define Diff(Xi, Xj) as the event that ϕ(Xi) = ϕ(Xj) and q(Xi) = q(Xj). Analogously, D-IBq= = 1 [1( (cid:98)mD(Xi), (cid:98)mD(Xj)) Diff(Xi, Xj)] , which measures how predictable the extrapolations for inputs associated with different states that have different legal next tokens are. If distinct-state inputs with the same legal next tokens are more predictable than distinct-state inputs with different legal next tokens (i.e., D-IBq= < D-IBq=), then it suggests the model extrapolates based on the nexttoken partition rather than the true board state. We compute these refined metrics for lattice and Othello. Each has natural definition of legal next moves (corresponding to boundaries and game rules). The results are depicted in Table 8. For all models, the gap between D-IBq= and D-IBq= is statistically significant, suggesting that models are grouping together distinct states with the same sets of legal next tokens. 5. Related Work This paper studies whether predictive models form world models (LeCun, 2022). One strand of world model research studies whether the outputs of fixed model accord with known world model by studying the fixed models outputs (Vafa et al., 2024). For example, one way that Toshniwal et al. (2022) and Li et al. (2023) study world models is by assessing whether model trained on sequential game data always predicts legal moves in the underlying game. The question we study is different yet related question: rather than studying the world model properties of fixed model, 9 What Has Foundation Model Found? Using Inductive Bias to Probe for World Models we study what it means to test if learning algorithm foundation model has world model embodied in it. Another strand of the literature assesses whether models parametric representations encode world models without directly studying learning properties. For example, common method uses probes or sparse autoencoders (SAEs) (Trenton Bricken et al., 2023) to assess whether an intermediate representation used by neural network is predictive of state (Hewitt & Liang, 2019; Li et al., 2021; Abdou et al., 2021; Jin & Rinard, 2023; Li et al., 2023; Spies et al., 2025; Karvonen, 2024). However, there are open questions about the reliability of probes (Belinkov, 2022), such as appropriate function complexity (Alain & Bengio, 2018; Cao et al., 2021; Li et al., 2023). Our method sidesteps these issues by asking how model learns, rather than whats encoded in its fixed representations. The methods in this paper are also related to the study of mechanistic interpretability of ML models (Nanda et al., 2023a; Cunningham et al., 2023; Bereska & Gavves, 2024). Closely related to us, jylin04 & Rager (2024) and Nikankin et al. (2025) find that GPT model trained on Othello and math tasks, respectively, performs internal computations corresponding to bags of heuristics rather than coherent world model. While our procedures differ in aim, these findings support our analysis of the Othello model relying on heuristics, rather than state, as its inductive bias (McCoy et al., 2019). Recent work developing foundation models in scientific domains such as like protein folding, gene regulation, and molecular chemistry (Chowdhury et al., 2022; Benegas et al., 2023; Boiko et al., 2023; Jablonka et al., 2024) use predictive models as steppingstones toward uncovering deeper principles. Our orbital mechanics example relates specifically to the large body of work studying AI and physics (Hao et al., 2022; Wu & Tegmark, 2019). It is most closely related to works studying whether AI models can uncover physical laws (Chen et al., 2022; Belyshev et al., 2024; Kansky et al., 2017; Gurnee & Tegmark, 2024). We adapt tools from this literature such as using symbolic regressions for interpretability to study the inductive biases of algorithms (Liu & Tegmark, 2021; Wu & Tegmark, 2019). 6. Conclusion The promise of foundation models is that sequence prediction can uncover deeper understanding of underlying mechanisms. We develop framework for evaluating whether foundation model has learned postulated world model by measuring its inductive biases when transferring to new tasks. Our empirical results reveal that while many sequence models excel at next-token prediction tasks, they often have limited inductive bias toward genuine world models. Rather than learning coherent world models, we find that these models may be relying on coarsened state representations or non-parsimonious representations. As described in Section 2, our metrics require specifying world model to test foundation model against. That world model must be specified aligns with other examples in this literature (Li et al., 2023; Vafa et al., 2024), but it is limitation for analysts searching for the exact representation the model is using. While we propose strategies for testing candidates (e.g. next-token partitions), future work should prioritize methods for automatically constructing the world model implicit in the foundation models behavior."
        },
        {
            "title": "Acknowledgments",
            "content": "Keyon Vafa is supported by the Harvard Data Science Initiative."
        },
        {
            "title": "References",
            "content": "Abdou, M., Kulmizev, A., Hershcovich, D., Frank, S., Pavlick, E., and Søgaard, A. Can language models encode perceptual structure without grounding? case study in color. arXiv preprint arXiv:2109.06129, 2021. Alain, G. and Bengio, Y. Understanding intermediate layers using linear classifier probes. 2018. arXiv preprint arXiv:1610.01644, 2018. Belinkov, Y. Probing classifiers: Promises, shortcomings, and advances. Computational Linguistics, 48(1):207219, 2022. Belyshev, A., Kovrigin, A., and Ustyuzhanin, A. Beyond dynamics: learning to discover conservation principles. Machine Learning: Science and Technology, 5(2):025055, 2024. Benegas, G., Batra, S. S., and Song, Y. S. DNA language models are powerful predictors of genome-wide variant effects. Proceedings of the National Academy of Sciences, 120(44):e2311219120, 2023. Bereska, L. and Gavves, E. Mechanistic interpretability for ai safetya review. arXiv preprint arXiv:2404.14082, 2024. Boiko, D. A., MacKnight, R., Kline, B., and Gomes, G. Autonomous chemical research with large language models. Nature, 624(7992):570578, 2023. Cao, S., Sanh, V., and Rush, A. M. Low-complexity arXiv preprint probing via finding subnetworks. arXiv:2104.03514, 2021. 10 What Has Foundation Model Found? Using Inductive Bias to Probe for World Models Casper, S., Li, Y., Li, J., Bu, T., Zhang, K., Hariharan, K., and Hadfield-Menell, D. Red Teaming Deep Neural Networks with Feature Synthesis Tools, September 2023. Chen, B., Huang, K., Raghupathi, S., Chandratreya, I., Du, Q., and Lipson, H. Automated discovery of fundamental variables hidden in experimental data. Nature Computational Science, 2(7):433442, 2022. Chowdhury, R., Bouatta, N., Biswas, S., Floristean, C., Kharkar, A., Roy, K., Rochereau, C., Ahdritz, G., Zhang, J., Church, G. M., Sorger, P. K., and AlQuraishi, M. Single-sequence protein structure prediction using language model and deep learning. Nature Biotechnology, 40(11):16171623, 2022. Cranmer, M. Interpretable machine learning for science with pysr and symbolicregression.jl, 2023. Cranmer, M., Sanchez-Gonzalez, A., Battaglia, P., Xu, R., Cranmer, K., Spergel, D., and Ho, S. Discovering Symbolic Models from Deep Learning with Inductive Biases, November 2020. Cunningham, H., Ewart, A., Riggs, L., Huben, R., and Sharkey, L. Sparse autoencoders find highly interpretable features in language models. arXiv preprint arXiv:2309.08600, 2023. Dao, T. and Gu, A. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. arXiv preprint arXiv:2405.21060, 2024. Elman, J. L. Finding structure in time. Cognitive science, 14(2):179211, 1990. Gingerich, O. The book nobody read: Chasing the revolutions of Nicolaus Copernicus. Bloomsbury Publishing USA, 2004. Gu, A. and Dao, T. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. Gurnee, W. and Tegmark, M. Language Models Represent Space and Time, March 2024. Hao, Z., Liu, S., Zhang, Y., Ying, C., Feng, Y., Su, H., and Zhu, J. Physics-informed machine learning: survey on problems, methods and applications. arXiv preprint arXiv:2211.08064, 2022. Hazineh, D. S., Zhang, Z., and Chiu, J. Linear latent world models in simple transformers: case study on OthelloGPT. arXiv preprint arXiv:2310.07582, 2023. Hochreiter, S. Long short-term memory. Neural Computation MIT-Press, 1997. Jablonka, K. M., Schwaller, P., Ortega-Guerrero, A., and Smit, B. Leveraging large language models for predictive chemistry. Nature Machine Intelligence, pp. 19, 2024. Jin, C. and Rinard, M. Evidence of meaning in lanarXiv preprint guage models trained on programs. arXiv:2305.11169, 2023. jylin04, learned JackS, A. K. and Rager, C. bag Othjul https://www.lesswrong. ellogpt 2024. com/posts/gcpNuEZnxAPayaKBY/ othellogpt-learned-a-bag-of-heuristics-1. Posted on LessWrong. heuristics,"
        },
        {
            "title": "URL",
            "content": "of Kansky, K., Silver, T., Mely, D. A., Eldawy, M., LazaroGredilla, M., Lou, X., Dorfman, N., Sidor, S., Phoenix, S., and George, D. Schema Networks: Zero-shot Transfer with Generative Causal Model of Intuitive Physics. In Proceedings of the 34th International Conference on Machine Learning, pp. 18091818. PMLR, July 2017. Karvonen, A. Emergent World Models and Latent Variable Estimation in Chess-Playing Language Models, July 2024. Kingma, D. P. and Ba, J. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Kipping, D. M. Parametrizing the exoplanet eccentricity distribution with the beta distribution. Monthly Notices of the Royal Astronomical Society: Letters, 434(1):L51 L55, 2013. Koestler, A. The Sleepwalkers: History of Mans Changing Vision of the Universe. Hutchinson, London, 1959. First edition. LeCun, Y. path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review, 62(1):162, 2022. Li, B. Z., Nye, M., and Andreas, J. Implicit representations of meaning in neural language models. arXiv preprint arXiv:2106.00737, 2021. Li, K., Hopkins, A. K., Bau, D., Viegas, F., Pfister, H., and Wattenberg, M. Emergent world representations: Exploring sequence model trained on synthetic task. In International Conference on Learning Representations, 2023. Hewitt, J. and Liang, P. Designing and interpreting probes with control tasks. arXiv preprint arXiv:1909.03368, 2019. Liu, B., Ash, J. T., Goel, S., Krishnamurthy, A., and Zhang, C. Transformers learn shortcuts to automata. arXiv preprint arXiv:2210.10749, 2022. What Has Foundation Model Found? Using Inductive Bias to Probe for World Models Liu, Z. and Tegmark, M. Ai poincare: Machine learning conservation laws from trajectories. arXiv:2011.04698, 2021. Wu, T. and Tegmark, M. Toward an artificial intelligence physicist for unsupervised learning. Physical Review E, 100(3):033311, 2019. McCoy, R. T., Pavlick, E., and Linzen, T. Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference. In Korhonen, A., Traum, D., and M`arquez, L. (eds.), Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 34283448, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1334. Nanda, N., Chan, L., Lieberum, T., Smith, J., and Steinhardt, J. Progress measures for grokking via mechanistic interpretability. arXiv preprint arXiv:2301.05217, 2023a. Nanda, N., Lee, A., and Wattenberg, M. Emergent linear representations in world models of self-supervised sequence models. arXiv preprint arXiv:2309.00941, 2023b. Nikankin, Y., Reusch, A., Mueller, A., and Belinkov, Y. Arithmetic Without Algorithms: Language Models Solve Math With Bag of Heuristics, May 2025. Olah, C. Mechanistic Interpretability, Variables, and the Importance of Interpretable Bases. https://www.transformercircuits.pub/2022/mech-interp-essay, June 2022. Spies, A. F., Edwards, W., Ivanitskiy, M. I., Skapars, A., Rauker, T., Inoue, K., Russo, A., and Shanahan, M. Transformers Use Causal World Models in Maze-Solving Tasks, March 2025. Toshniwal, S., Wiseman, S., Livescu, K., and Gimpel, K. Chess as testbed for language model state tracking. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 1138511393, 2022. Trenton Bricken, Adly Templeton, B. C. et al. Towards Monosemanticity: Decomposing Language Models With Dictionary Learning. https://transformercircuits.pub/2023/monosemantic-features/index.html, 2023. Vafa, K., Chen, J. Y., Kleinberg, J., Mullainathan, S., and Rambachan, A. Evaluating the world model implicit in generative model. arXiv preprint arXiv:2406.03689, 2024. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. In Neural Information Processing Systems, 2017. Wolpert, D. H. The lack of priori distinctions between learning algorithms. Neural Computation, 8(7):1341 1390, 10 1996. ISSN 0899-7667. doi: 10.1162/neco. 1996.8.7.1341. URL https://doi.org/10.1162/ neco.1996.8.7.1341. 12 What Has Foundation Model Found? Using Inductive Bias to Probe for World Models A. Model and Training Details We use the following specifications for each model: RNN (Elman, 1990): For Othello, We use 6 uni-directional RNN layers with 768 embedding dimensions. For the lattice experiments, the architecture is the same except we use only 2 layers because it optimizes to better in-sample and out-of-sample loss. LSTM (Hochreiter, 1997): We use the same specification as for the RNN, except we use LSTM layers. Transformer (Vaswani et al., 2017): We use transformer decoder architecture, with 12 layers, 12 attention heads, and 768 embedding dimensions. Mamba (Gu & Dao, 2023): We first encode inputs with 768-dimension embedding layer. We then pass inputs through 24 Mamba layers (analogous to 12 layers in transformer due to how Mamba layers are defined). We use 768 embedding dimensions, 16 for the SSM state expansion factor, 2 for the block expansion factor, and 4 for the convolutional width. Mamba-2 (Dao & Gu, 2024): We use the same architecture as for Mamba except the mixer in each block is Mamba-2 module. We use the same specifications as well: 768 embedding dimensions, an SSM state expansion factor of 16, block expansion factor of 2, and convolutional width of 4. We use Adam (Kingma & Ba, 2014) to optimize each model. We use learning rate of 6e-4 and decay the learning rate with with 2000 warmup iterations. We use weight decay of 0.1 and gradient clipping at 1 for each model. When we pre-train models on next-token prediction, we include head to predict next tokens (tying its parameter weights to the initial embedding layer parameters). For physics dataset generation, we use the following sampling strategy. For each solar system, we sample the number of planets from Unif([1, 2, . . . , 10]), the eccentricity from Beta(α = 0.867, β = 3.03) following Kipping (2013), the semi-major axis from Unif(0.3, 42), in astronomical units (AU), the mass of each planet from LogUniform(107, 103), the mass of the star from Unif(0.5, 5). These distributions ensure that our solar system is within the training distribution of the model. In order to generate sequences, we randomly generate initial conditions and solve Keplers equation to obtain each trajectory. We simulate 1000 timesteps with 6-month intervals for each sequence. B. Metric Implementation Details B.1. Physics To compute the empirical approximations of Equation 6, we follow the following procedure. First, we create 100 datasets of 100 examples, D1, . . . , D100. For each dataset Di, we sample 100 sequences uniformly at random among the set of data points and consider their corresponding sequences of state-vectors. First, we randomly sample 50 matrices of dimension (6 1) from standard Gaussian. We consider the linear projection of each state-vector using each of the 50 matrices, and choose the one that maximizes the Spearman correlation between pairwise Euclidian distances in the 6D state space and the projected 1D space. We randomly sample projected point from each sequence, leading to Di of size 100. We then fine-tune model separately for each dataset, resulting in 100 fine-tuned models ˆm(; D1), . . . , ˆm(; D100). We then calculate the associated prediction functions across all inputs xi from the same hold-out dataset, resulting in new datasets of the form {(xi, ˆm(xi; D1)}, . . . , {(xi, ˆm(xi; D100)}. To compute the metrics, we first randomly sample 2,000 examples from all inputs, xk1, . . . xk100, compute the pairwise Euclidean distance among the Oracle (a linear map or 2 layer MLP with 5 nodes in each hidden layer) predictions on the inputs, and divide the range of predictions into 20 equally-spaced bins. For all the points that lie in each bin, we compute the mean pairwise Euclidiean distance among the model predictions. The resulting figure is shown in Figure 4. B.2. Lattice and Othello To compute the empirical approximations of Equation 1 and Equation 2, we follow the following procedure. First, we create 100 datasets of 100 examples, D1, . . . , D100. For each dataset, we sample sequences uniformly at random among the set of 13 What Has Foundation Model Found? Using Inductive Bias to Probe for World Models data points and sample outputs from Bernoulli(0.5) distribution. In our construction we make sure that any two sequences with the same state are mapped to the same output variable. We then fine-tune model separately for each dataset, resulting in 100 fine-tuned models ˆm(; D1), . . . , ˆm(; D100). We then calculate the associated prediction functions across all inputs xi from the same hold-out dataset, resulting in new datasets of the form {(xi, ˆm(xi; D1)}, . . . , {(xi, ˆm(xi; D100)}. To compute the metrics, we first randomly sample 2,000 examples from all inputs, xk1, . . . xk100, then measure the average predictive loss for all pairs (xki, xkj ) with the same state, ϕ(xki) = ϕ(xkj ) (R-IB): R-IB EDDtest (cid:104) (cid:105) Ei,j:ϕ(xki )=ϕ(xkj ) [m(xi; D) = m(xj; D)] and the average predictive loss for all pairs (xki, xkj ) with different states, ϕ(xki) = ϕ(xkj ) (D-IB): (cid:104) (cid:105) Ei,j:ϕ(xki )=ϕ(xkj ) [m(xi; D) = m(xj; D)] D-IB 1 EDDtest (7) (8) We rescale them so that the value of 0 corresponds to perfect accuracy and 1 corresponds to random guessing (large values for both indicate the model exhibits stronger inductive bias towards the state). For the lattice example, we use state space consisting of states: Φ = {1, 2, . . . , k}. The inputs xi for extrapolation are taken from 1,000 random sequences of valid moves, each of length 100, for total of 100,000 sub-sequences of moves. Our procedure for Othello follows the same steps as for the lattice example, except the state is 64-dimensional board instead of single categorical variable. Note that for Othello, if we randomly sample sequences from game transcripts, it is exceedingly likely that we end up with dataset in which two sequences lead to the same state if and only if they are permutations of one another. This implies that non-sophisticated model that detects unique permutations of sequences would appear to have high inductive bias towards the state. To prevent this, we first construct all valid Othello game openings of depth 10, randomly choose board that appears many times in this dataset, then use all possible valid permutations of any sequence of moves that leads to that board as our input dataset. Note that since all sequences will be permutations of one another, the non-sophisticated model would no longer be able to distinguish different states. We end up with an input dataset of 210 Othello openings, each of length 10, for total of 2,100 subsequence of moves. C. Force Prediction Implementation Details Here we describe more implementation details for the force prediction experiments. Force vector prediction. To create Figure 1, we fine-tuned the transformer to predict force vectors in two-body gravitational systems. We keep force vectors as continuous, and normalize the force vectors in each sequence so the maximum force vector in each sequence is unit length. We specifically fine-tune the model on the 8 sequences consisting of the trajectories in our solar system, randomly using 1% of the observations in each sequence as labeled force vector data for the model. We fine-tune the model to minimize MSE for 10,000 steps. We consider learning rate grid between 1e-6 and 5e-4, finding that 2e-4 has the best validation loss. We keep the checkpoint with the lowest held-out loss. The model is then extrapolated to make predictions across the remainder of the points in each sequence. For comparison, we perform the same procedure for an oracle model that predicts force vectors based on the true state matrices. Specifically, using the same sampling procedure, the oracle fits k-nearest neighbor model with = 2 based on Euclidean distance with the true state. We then use this model to predict force vectors for the remainder of the points in the solar system. The oracle predictions are depicted in Figure 7. These results show that it is feasible for model to make accurate predictions if it is extrapolating based on the correct world model. Force magnitude prediction and symbolic regression. We use symbolic regression to assess how close the recovered force equation is to the true law. To simplify, we use the force magnitude rather than the full vector for these experiments (the vector is always in the direction of the sun). Here, we dont normalize the force magnitudes per solar system in order to preserve the force magnitudes dependence on the suns mass. We start by creating training set that includes 9K two-body problems sampled using the sampling strategy in Appendix A. We create test set of 1K sequences of two-body problems. We additionally ensure that the model is always extrapolating to sequences where it has seen partial information by adding two randomly sampled timestep observations of each test set sequence to the training set. Because m1m2/r2, this means that the only factor changing within the sequence is the 14 What Has Foundation Model Found? Using Inductive Bias to Probe for World Models Figure 7: Each pair of panels illustrates the trajectory of planet in the solar system and its gravitational force vectors, comparing the true Newtonian forces (left) to the predicted forces from an oracle model that predicts force vectors based on the true state matrices. symbolic regression recovers the true gravitational law from its predictions. r2 term. Additionally, instead of imputing predictions on the full test set, we select the 5,000 timesteps across the 1,000 sequences that have the most similar states to states in the training set (using Euclidean distance). This ensures that the model is extrapolating to states that are similar to the ones it is trained on. We fine-tune the transformer on the training set for 10,000 steps with batch size of 64, keeping the checkpoint with the lowest held-out MSE. We impute the models predictions on 1,000 randomly sampled points from the test set. We fit symbolic regression to these predictions using the PySR library (Cranmer, 2023). Specifically we constrain our search space to have max size of 20 and we consider two binary operators (addition and multiplication) along with 4 unary operators (sine, cosine, exponentiation, and inverse). We use loss function that applies 0 penalty if the model is within 1e-8 of the magnitude and otherwise penalizes based on the absolute distance. We choose the model with the best score across three random restarts of 100 iterations each. We perform this symbolic procedure procedure five times, each time randomly sampling 1,000 different points from the test set to correspond to different galaxy. The symbolic regression returns different equations for each sample, never recovering the true law  (Table 1)  . To make sure this procedure is feasible when model is extrapolating based on true state, we also consider an oracle model that is given true state. Specifically, we use the same data and fit k-nearest neighbor model with = 2 based on Euclidean distance to the true state. We then use this model to predict the same held-out points as above and fit symbolic regressions in the same manner. In contrast to the transformer results in Table 1, we find that this procedure recovers the true gravitational law for all five sampled galaxies. D. LLM Physics Experiments Throughout this paper, we train foundation models on domain-specific data. Here, we consider large language models (LLMs) as foundation models for physics. While LLMs arent trained on the same domain-specific trajectories we use, they are trained on large quantities of text that contain information about physics and orbital trajectories. We consider three advanced reasoning models: o3 (from OpenAI), Claude 4 Sonnet (from Anthropic), and Gemini 2.5 Pro (from Google). Fine-tuning these models is infeasible because theyre proprietary and running the full inductive bias probe is 15 What Has Foundation Model Found? Using Inductive Bias to Probe for World Models Figure 8: Comparing LLM magnitude predictions to the true magnitude across timesteps for 5 randomly sampled solar systems. Each LLM is provided the full trajectory and random 2% sample of force magnitudes, and is prompted to impute the remaining outcomes. Ground-truth law Estimated laws m1m2 r2 o3 Claude Sonnet 4 Gemini 2.5 Pro F m1 1 m20.50 Table 3: Force equations recovered via symbolic regression of LLMs predicting force magnitudes. expensive because it involves applying the model to many new datasets. Instead, we run small-scale experiment assessing each models ability to predict the force magnitude of orbital trajectories. Rather than fine-tuning models, we provide them with information in-context, and study their extrapolation behavior. Specifically, we sample 5 random solar systems with 450 observations each. For each solar system, we provide each LLM with prompt that describes the structure of the data, also including the true force magnitudes for 10 randomly selected observations. We instruct the LLM to predict the outputs for the remaining data points (we do not provide any information in the prompt indicating that the outputs correspond to forces). See Figure 9 for an example of the prompt. We collect the magnitude inferences for each solar system (2,250 observations per LLM). Figure 8 shows the predicted force magnitudes for each solar system for each model. Most of the results are poor, which is further corroborated by symbolic regressions  (Table 3)  . Interestingly, the symbolic regressions are simpler than the ones found for the domain-specific foundation models. However, this may be due to differences in experimental setup (e.g. using fewer solar systems for the LLM due to cost concerns). What Has Foundation Model Found? Using Inductive Bias to Probe for World Models LLM Prompt You are physics expert. You are given sequence of coordinates and outcomes. The coordinates are the positions of planet in 2-body solar system. The planet is orbiting the sun. The sun is at the origin. Here is sequence of observations. Some of them are unknown. Your job is to predict the outcomes for the unknown timesteps. Timestep: 0, Coordinates: (-26.08, -6.98), Outcome: Unk Timestep: 1, Coordinates: (-26.08, -6.99), Outcome: Unk Timestep: 2, Coordinates: (-26.06, -7.01), Outcome: 2.907672751462087e-05 Timestep: 3, Coordinates: (-26.06, -7.04), Outcome: Unk Timestep: 4, Coordinates: (-26.05, -7.05), Outcome: Unk Timestep: 5, Coordinates: (-26.04, -7.08), Outcome: 2.9093407647451386e-05 Timestep: 6, Coordinates: (-26.04, -7.09), Outcome: Unk Timestep: 7, Coordinates: (-26.02, -7.12), Outcome: Unk Timestep: 8, Coordinates: (-26.02, -7.14), Outcome: Unk Timestep: 9, Coordinates: (-26.01, -7.16), Outcome: Unk ... Timestep: 449, Coordinates: (-20.28, -15.66), Outcome: Unk You can reason all youd like, but your answer should end with \"ANSWER: \" followed by the predicted outcomes for all of the timesteps, even the unknown ones. You should structure your predictions as dict, where each key is timestep and each value is the prediction. You should make predictions for all of the timesteps, even the ones that are known. Here is an example of the output format: ANSWER: { 0: 1.0e-8, 1: 1.0e-8, 2: 1.0e-8, ... 449: 1.0e-8, } Figure 9: Example prompt used in the LLM physics experiments. E. Inductive Bias Ablations On the Othello dataset, we perform ablation of the IB metrics on the number of fine-tuning iterations  (Table 4)  , keeping the number of fine-tuning examples fixed to 100, and the number of fine-tuning examples  (Table 5)  , keeping the number of fine-tuning iterations fixed to 100. F. Additional Transfer Results Table 9 shows the full transfer learning results described in Section 4. G. Next Token Performance Table 6 shows results for the next-token test (Toshniwal et al., 2022; Li et al., 2023) for the pre-trained models on the lattice and Othello models. It measures the share of top model predictions that are true for the underlying state. All models learn good next token predictions that appear to obey state. Table 7 shows results for physics. Across 200 held-out trajectories, we autoregressively generate the models predicted trajectory given the first 50 steps. Then, we compute the MSE of the predicted trajectory, 1, 5, 10 steps from the 50th step. We include the MSE of baseline that always predicts the most recent timestep. What Has Foundation Model Found? Using Inductive Bias to Probe for World Models # iterations 10 50 100 R-IB () D-IB () R-IB () D-IB () R-IB () D-IB () R-IB () D-IB () RNN LSTM 0.623 (0.023) 0.738 (0.030) 0.560 (0.022) 0.837 (0.020) 0.558 (0.021) 0.845 (0.019) 0.561 (0.022) 0.854 (0.018) 0.680 (0.027) 0.491 (0.037) 0.652 (0.030) 0.445 (0.035) 0.649 (0.030) 0.448 (0.035) 0.653 (0.030) 0.451 (0.035) Transformer 0.735 (0.020) 0.605 (0.035) 0.665 (0.024) 0.583 (0.035) 0.668 (0.023) 0.593 (0.034) 0.680 (0.021) 0.620 (0.030) Mamba 0.626 (0.024) 0.704 (0.030) 0.595 (0.024) 0.731 (0.028) 0.597 (0.024) 0.734 (0.028) 0.600 (0.024) 0.732 (0.029) Mamba-2 0.586 (0.023) 0.720 (0.028) 0.592 (0.022) 0.728 (0.027) 0.590 (0.022) 0.732 (0.027) 0.583 (0.023) 0.746 (0.027) Table 4: Results for ablating the number of iterations of fine-tuning. # examples 10 50 100 500 R-IB () D-IB () R-IB () D-IB () R-IB () D-IB () R-IB () D-IB () RNN LSTM 0.725 (0.023) 0.491 (0.037) 0.622 (0.024) 0.698 (0.029) 0.558 (0.021) 0.845 (0.019) 0.466 (0.020) 0.934 (0.010) 0.838 (0.023) 0.302 (0.038) 0.693 (0.032) 0.409 (0.037) 0.649 (0.030) 0.448 (0.035) 0.710 (0.026) 0.457 (0.034) Transformer 0.843 (0.023) 0.284 (0.035) 0.707 (0.024) 0.541 (0.034) 0.668 (0.023) 0.593 (0.034) 0.549 (0.021) 0.796 (0.021) Mamba 0.705 (0.024) 0.547 (0.035) 0.622 (0.025) 0.653 (0.032) 0.597 (0.024) 0.734 (0.028) 0.482 (0.021) 0.843 (0.022) Mamba-2 0.665 (0.026) 0.609 (0.037) 0.636 (0.022) 0.660 (0.030) 0.590 (0.022) 0.732 (0.027) 0.534 (0.021) 0.829 (0.019) Table 5: Results for ablating the number of examples used for fine-tuning. Lattice Othello RNN LSTM Transformer Mamba Mamba-2 1.00 1.00 1.00 1.00 1.00 0.905 0.907 0.915 0.890 0.901 Table 6: Results for the next token test (Toshniwal et al., 2022; Li et al., 2023) for models pre-trained on next-token prediction. # steps out Per-orbit mean Previous position Transformer 1 (1.64 0.09) 101 (3.70 0.30) 104 (1.04 0.14) 107 5 (1.49 0.09) 101 (7.88 0.80) 104 (1.07 0.11) 107 100 (6.72 0.73) 102 (7.74 0.89) 103 (3.75 0.56) 107 Table 7: Orbit trajectory prediction performance (MSE) for models pre-trained on next-token prediction. Each column shows prediction accuracy when forecasting planetary positions 1, 5, or 100 time steps ahead from position 500 in the sequence. We compare the transformer model (trained on 6-month intervals) to two simple baselines (one that always predicts planets position at the previous timestep, and another that uses the per-orbit mean). All results are evaluated on held-out test trajectories. 18 What Has Foundation Model Found? Using Inductive Bias to Probe for World Models Lattice Othello D-IBq= D-IBq= D-IBq= D-IBq= RNN LSTM Transformer Mamba Mamba-2 0.740 (0.042) 0.873 (0.051) 0.626 (0.037) 0.764 (0.040) 0.778 (0.042) 0.844 (0.034) 0.952 (0.034) 0.710 (0.037) 0.933 (0.035) 0.920 (0.033) 0.580 (0.029) 0.447 (0.035) 0.445 (0.032) 0.552 (0.032) 0.548 (0.031) 0.845 (0.019) 0.448 (0.035) 0.594 (0.034) 0.734 (0.028) 0.732 (0.027) Table 8: Metrics for assessing whether models inductive bias is toward its legal next-token partition. Low values of D-IBq= and high values of D-IBq= suggest that failures to differentiate state are driven by the models having an inductive bias toward the legal next-token partition. Majority Tiles Board Balance Edge Balance Pretraining NLL () ACC () NLL () ACC () NLL () ACC () RNN LSTM Untrained NTP trained 0.362 (0.002) 0.308 (0.003) 0.825 (0.001) 0.857 (0.002) 0.290 (0.002) 0.210 (0.004) 0.865 (0.001) 0.907 (0.002) 0.087 (0.001) 0.077 (0.002) 0.963 (0.001) 0.967 (0.001) Untrained NTP trained 0.349 (0.002) 0.275 (0.003) 0.831 (0.001) 0.880 (0.002) 0.234 (0.002) 0.173 (0.003) 0.892 (0.001) 0.923 (0.001) 0.079 (0.001) 0.043 (0.002) 0.965 (0.001) 0.981 (0.001) Transformer Untrained NTP trained 0.413 (0.002) 0.249 (0.003) 0.798 (0.001) 0.889 (0.002) 0.259 (0.002) 0.170 (0.003) 0.880 (0.001) 0.924 (0.002) 0.073 (0.001) 0.048 (0.003) 0.968 (0.000) 0.983 (0.001) Mamba MambaUntrained NTP trained 0.297 (0.002) 0.260 (0.003) 0.866 (0.001) 0.882 (0.002) 0.215 (0.002) 0.169 (0.003) 0.904 (0.001) 0.928 (0.001) 0.058 (0.001) 0.039 (0.003) 0.976 (0.000) 0.986 (0.001) Untrained NTP trained 0.297 (0.002) 0.241 (0.004) 0.864 (0.001) 0.901 (0.002) 0.218 (0.002) 0.165 (0.003) 0.903 (0.001) 0.925 (0.001) 0.065 (0.001) 0.028 (0.003) 0.972 (0.000) 0.990 (0.001) IB Correlation 0.441 0. 0.759 0.752 0.577 0.543 Table 9: Results showing transfer performance across new functions of state. NLL represents negative log-likelihood (lower is better), and ACC represents accuracy (higher is better). IB Correlation measures the (unsigned) correlation between each column of results to the ratios of the inductive bias metrics in Table 2, R-IB 1D-IB . Transfer learning results are correlated to the inductive bias metrics; models with low inductive bias perform worse at transfer. H. What are models using to extrapolate? Here we describe how we compute the decomposition of D-IB into D-IBq= and D-IBq=. For lattice, we coarsen the state-space by defining mapping from the ground-truth state-space (of size = 5) to pseudo-state-space of size 3. The mapping is defined as {1} 1, {2, . . . , 1} 2, {N } 3. For Othello, we coarsen the state-space by defining mapping from board state to the set of legal next moves possible from the state. Notice that this mapping is many-to-one: as the pair of boards in Figure 6 demonstrate, there can be many boards that share the same set of legal next moves. Then, we measure the expected extrapolative predictability of random pair of sequences that have different states but the same pseudo-state (D-IBq=) and random pair of sequences that have both different states and also different pseudo-states (D-IBq=), as defined in Section 4. The results are shown in Table 8. Note that across all models, D-IBq= is smaller than D-IBq=. In other words, among sequences with different states, extrapolations on sequences that share the same legal next tokens are more predictable from each other than those on sequences that do not."
        }
    ],
    "affiliations": [
        "Harvard University",
        "MIT"
    ]
}