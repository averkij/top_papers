{
    "paper_title": "LiveXiv -- A Multi-Modal Live Benchmark Based on Arxiv Papers Content",
    "authors": [
        "Nimrod Shabtay",
        "Felipe Maia Polo",
        "Sivan Doveh",
        "Wei Lin",
        "M. Jehanzeb Mirza",
        "Leshem Chosen",
        "Mikhail Yurochkin",
        "Yuekai Sun",
        "Assaf Arbelle",
        "Leonid Karlinsky",
        "Raja Giryes"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The large-scale training of multi-modal models on data scraped from the web has shown outstanding utility in infusing these models with the required world knowledge to perform effectively on multiple downstream tasks. However, one downside of scraping data from the web can be the potential sacrifice of the benchmarks on which the abilities of these models are often evaluated. To safeguard against test data contamination and to truly test the abilities of these foundation models we propose LiveXiv: A scalable evolving live benchmark based on scientific ArXiv papers. LiveXiv accesses domain-specific manuscripts at any given timestamp and proposes to automatically generate visual question-answer pairs (VQA). This is done without any human-in-the-loop, using the multi-modal content in the manuscripts, like graphs, charts, and tables. Moreover, we introduce an efficient evaluation approach that estimates the performance of all models on the evolving benchmark using evaluations of only a subset of models. This significantly reduces the overall evaluation cost. We benchmark multiple open and proprietary Large Multi-modal Models (LMMs) on the first version of our benchmark, showing its challenging nature and exposing the models true abilities, avoiding contamination. Lastly, in our commitment to high quality, we have collected and evaluated a manually verified subset. By comparing its overall results to our automatic annotations, we have found that the performance variance is indeed minimal (<2.5%). Our dataset is available online on HuggingFace, and our code will be available here."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 5 1 ] . [ 2 3 8 7 0 1 . 0 1 4 2 : r Preprint, Under review LIVEXIV - MULTI-MODAL LIVE BENCHMARK BASED ON ARXIV PAPERS CONTENT Nimrod Shabtay1,2, Felipe Maia Polo3, Sivan Doveh2, Wei Lin4, M. Jehanzeb Mirza5, Leshem Chosen6, Mikhail Yurochkin6, Yuekai Sun3, Assaf Arbelle2, Leonid Karlinsky6, Raja Giryes1 1 Faculty of Engineering Tel-Aviv University, 2 IBM Research, 3 Department of Statistics, University of Michigan, USA 4 JKU Linz, Austria, 5 TU Graz, Austria. 6 MIT-IBM"
        },
        {
            "title": "ABSTRACT",
            "content": "The large-scale training of multi-modal models on data scraped from the web has shown outstanding utility in infusing these models with the required world knowledge to perform effectively on multiple downstream tasks. However, one downside of scraping data from the web can be the potential sacrifice of the benchmarks on which the abilities of these models are often evaluated. To safeguard against test data contamination and to truly test the abilities of these foundation models we propose LiveXiv: scalable evolving live benchmark based on scientific ArXiv papers. LiveXiv accesses domainspecific manuscripts at any given timestamp and proposes to automatically generate visual question-answer pairs (VQA). This is done without any human-in-the-loop, using the multi-modal content in the manuscripts, like graphs, charts, and tables. Moreover, we introduce an efficient evaluation approach that estimates the performance of all models on the evolving benchmark using evaluations of only subset of models. This significantly reduces the overall evaluation cost. We benchmark multiple open and proprietary Large Multi-modal Models (LMMs) on the first version of our benchmark, showing its challenging nature and exposing the models true abilities, avoiding contamination. Lastly, in our commitment to high quality, we have collected and evaluated manually verified subset. By comparing its overall results to our automatic annotations, we have found that the performance variance is indeed minimal (<2.5%). Our dataset is available online on HuggingFace, and our code will be available here."
        },
        {
            "title": "INTRODUCTION",
            "content": "Figure 1: Static benchmark contamination. As training data increases, the risk for test set contamination grows and static benchmarks becomes saturated, reflecting falsely improved capabilities. The internet, with its vast and ever-growing repository of information, serves as rich data source for training Large Language Models (LLMs) (Brown et al., 2020; OpenAI, 2023; Chiang et al., 2023; Raffel et al., 2019; Touvron et al., 2023a;b; Dubey et al., 2024) and Large Multi-modal Models (LMMs) (OpenAI, 2023; Liu et al., 2023c; Li et al., 2024c; Zhu et al., 2023; Chen et al., 2023a; Alayrac et al., 2022; Radford et al., 2021a). This diverse and continuously updated data fits precisely the need to cover varying knowledge in scale in the training data. Training on such data enables the models to achieve superhuman performance across wide range of tasks on multiple common benchmarks (Fu et al., 2023; Yue et al., 2024; Li et al., 2024d; Liu et al., 2023d). 1 Preprint, Under review Figure 2: We propose LiveXiv, new method for generating Live multi-modal dataset for Visual Question-Answering based on ArXiv content. Our pipeline automatically generates scalable and reliable questions along with an efficient evaluation method to reduce the computational and logistic overheads required for continually evaluating past and present models on new versions of the dataset. We hypothesize that portion of LLMs reported improvements are due to data contamination (Figure 1) and pose the following question: To what extent does the potential for test set contamination during large-scale training affect our perception of the abilities of LMMs? One possible way to safeguard against the contamination of static benchmarks is to design live benchmark that can continuously harness data from the web and turn it into an ever-evolving benchmark to test the abilities of these models. live benchmark may be used in one of the following ways: (a) Expand the dataset over time and evaluate the models overall knowledge over all collected data, while taking into account that the data might be contaminated. (b) Use only the latest version to assess model capabilities while keeping data contamination risk minimal. While we focus on (b), we share key properties of our efficient evaluation method that is applicable to both cases. Although, live benchmark is promising direction, it still comes with its fair share of challenges. live benchmark should ideally be updated frequently, consistently, and automatically, i.e. it should be able to scrape the data from the web and formulate it into benchmark for automated evaluations. Furthermore, as the benchmark is ever-evolving, each time new version arrives, all the participant models need to be re-evaluated, making the update procedure prohibitively expensive both in time and compute. This requires methodology for efficient evaluation of these models on continuously updating benchmark. Such methodology should ease the computational burden of evaluating all the models on each new version of the dataset and reduce the logistic overhead of maintaining inaccessible old models. In this work, we take step in this direction and propose LiveXiv novel fully automated multi-modal live benchmark that focuses on scientific domains. LiveXiv starts with scraping category-specific (e.g. cs.CV, eess.SY, q-bio.BM, etc.) manuscripts from ArXiv and generates visual question answers from figures, charts, and tables present in these manuscripts through capable multi-modal model, namely, GPT-4o. As it is challenging to directly feed information-rich PDF documents to GPT-4o, as pre-processing step, we extract relevant information from the papers by processing it with structured document parsing pipeline (Team, 2022) to obtain pertinent information like placements of figures, charts, tables, and the text in the captions or in the tables. This information is used to extract, e.g. by cropping, relevant information from the manuscripts, which is fed to GPT-4o to generate visual questions and answers. Although very capable, GPT-4o is still prone to errors, e.g. due to hallucinations, and may even generate questions that can be answered without visual information. Thus to mitigate these issues, we add an extensive filtering stage that automatically filters questions requiring only textual information to answer them, and reduce hallucinations through obtaining agreement about the generated questions with another capable multi-modal model, namely, Claude. After the extensive filtering, we obtain large corpora of VQA pairs which are incorporated into our LiveXiv live benchmark. Over time, the benchmark is expected to grow, either in the size of the dataset or the amount of models to be evaluated, which increases the required resources for evaluation. Moreover, comparing new model to existing models at different times requires re-evaluating the existing models over the latest version of the dataset, which can cause additional overhead for continuous evaluation and comparison to prior works. To make the evaluations on LiveXiv feasible, we take inspiration from Maia Polo et al. (2024a;b) and propose method to approximate the performance of the existing models in new versions of LiveXiv just by re-evaluation small portion of them. Figure 2 provides conceptualized overview of our approach. We summarize our contributions as follows: (a) We propose scalable live benchmark without any human in the loop that automatically harnesses data from online scientific manuscripts, generates multiple VQA 2 Preprint, Under review pairs, filters these questions to reduce errors, and formulates them in the form of benchmark to test the evolving landscape of LMMs; (b) We introduce an efficient evaluation pipeline that requires LMMs to be tested only on fraction of the data to infer its performance on the latest version of the benchmark, reducing the overall needed evaluations by 70%; (c) We benchmark multiple open and proprietary LMMs on the first version of our benchmark highlighting its challenging nature and providing interesting insights about the models behavior when evaluated on less contaminated data."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Large multi-modal Models (LMMs). LMMs have shown significant advancements in enabling billionparameter scale LLMs to perform multi-modal tasks such as image captioning, visual reasoning, and visual question answering. Academia and industry have endeavored to develop LMMs targeting the multi-modal competence of advanced proprietary models like GPT4o (OpenAI, 2023) and Claude (cla, 2024). InstructBLIP performs instruction tuning on the pre-trained BLIP-2 (Li et al., 2023) covering 11 vision-language tasks. The LLaVA series models (Liu et al., 2023c;a;b; Li et al., 2024b) develop the pipeline of collection of instruction-following data and visual instruction tuning with enhanced vision capabilities. The internLM-XComposer (IXC) series (Dong et al., 2024a;b) target free-form vision-language composition and multilingual comprehension. Models from Idefics release (Laurencon et al., 2024b;a) benefit from the massive collection of instruction-following data from over 50 vision-language databases, enhancing capabilities of OCR, document understanding, and visual reasoning. In this work, we include 17 top-performing LMMs in our multi-modal live benchmark LiveXiv, covering both open-sourced and proprietary representatives. Static evaluation benchmarks for LMMs. Most existing LMM benchmarks offer static evaluation with fixed questions and answers (Fu et al., 2023; Yue et al., 2024; Li et al., 2024d; Liu et al., 2023d; Huang et al., 2024; Lin et al., 2024; Zhang et al., 2024b). MME (Fu et al., 2023) offers evaluation of perception and cognition on 14 tasks and MMMU (Yue et al., 2024) includes 11.5K questions from college exams, quizzes and text books from six major disciplines. Although these benchmarks cover large variety of multi-modal domain knowledge, evaluation on them is faced with two hazards: the excessive evaluation cost and test data contamination. In this work, we tackle both challenges by proposing suite that enables efficient evaluation on contamination-free live benchmark. Contamination-free benchmarks. As large foundation models like LLMs and LMMs are trained on combined sources of tremendous amount of web data or repurposed version of existing open-sourced datasets, there is high risk of overlap between training data and samples from evaluation benchmarks. Reported evidence and analysis show impact of data contamination on evaluation benchmarks for LLMs (Wei et al., 2023; Zhang et al., 2024a; Cobbe et al., 2021; Roberts et al., 2023; Jain et al., 2024) and LMMs (Chen et al., 2024), indicating the significance of contamination-free evaluation benchmarks. For LLMs, LMSys Chatbot Arena (Chiang et al., 2024) and AI2 WildVision (Lu et al., 2024) create user-focused platform that provides contamination-free environment for proper evaluations. However, it is expensive to collect tens of thousands of human preferences on the compared language models. Furthermore, Seal Benchmark (AI, 2024) proposes private questions paired with human evaluations. Srivastava et al. (2024) update the questions in the MATH dataset (Hendrycks et al., 2021) by changing numbers in the math questions. LiveBench White et al. (2024) collects frequently updated questions from diverse information sources e.g.math competitions, arXiv papers and news articles and more challenging versions of existing benchmark tasks. Concurrently, LiveCodeBench (Jain et al., 2024) contributes live benchmark on broader code-related capabilities. Note that these datasets focus on language data only. For LMMs, Vibe-Eval (Padlewski et al., 2024) and LLaVA-Wilder (Li et al., 2024a) perform contamination check on the collected samples that reflect real-world user requests. Most related to our work, the LMMs-Eval LiveBench (Zhang et al., 2024b) collects images from sources of new websites and online forums and employs proprietary LMMs for design and revision of questions. However, the LMMs-Eval LiveBench requires human manual verification of questions which impedes the scalability. Furthermore, it contains only open-ended questions that require LMM-as-a-judge which is time-consuming, susceptible to judge biases, and difficult to scale. In comparison, our LiveXiv constructs fully-automated data collection pipeline which generates multiple-choice questions which are challenging to the top-performing LMMs. Efficient benchmarks. With the increasing amount of tasks and samples in current benchmarks, evaluation of the full suite is time-consuming and cost-intensive. Efforts are underway to develop efficient benchmarks that reduce computation costs without sacrificing reliability. For LLMs, Perlitz et al. (2023) proposed the first systematic study of the effects of language model benchmark designs on reliability and efficiency, Preprint, Under review Figure 3: Our live dataset generation consists of several stages. We first extract the images and their corresponding metadata (i.e. captions and table contents), then we classifying the figures into categories using meta-prompting. All the extracted data is then fed to GPT4o to generate multiple questions-answer pairs per image. Since generative models are prone to errors, we apply several filtering steps, using an LLM and LMM to ensure that our dataset is truly multi-modal and reliable. and applied efficient benchmark practices on the HELM benchmark (Liang et al., 2022), leading to 100 computation reduction with minimal loss on reliability. Lifelong benchmarks (Prabhu et al., 2024) has an ever-expanding pool of test samples for the categories in CIFAR10 (Krizhevsky & Hinton, 2009) and ImageNet (Deng et al., 2009); to make this design economically feasible, it reuses past model evaluations on sample set through dynamic programming to enable efficient evaluation of new incoming models, drastically reducing the evaluation cost. Most related to our work, tinyBenchmarks (Maia Polo et al., 2024a) and PromptEval (Maia Polo et al., 2024b) propose using Item Response Theory (IRT) (Lord et al., 1968) to estimate the performance of LLMs on unseen samples, making efficient evaluation possible by only conducting small fraction of the total number of evaluations. Inspired by the last two works, we leverage IRT to estimate the performance of older models in new batches of data. More specifically, at each version of LiveXiv, we choose small core set of models (5) previously added to the leaderboard and re-evaluate them on the new data. Depending on their responses to the new samples, we estimate the performance of the remaining old models on the new benchmark version."
        },
        {
            "title": "3 LIVEXIV",
            "content": "At higher level, our automated LiveXiv is created by first obtaining the domain-specific scientific manuscripts from ArXiv at any given timestamp. Then, to obtain pertinent information from the manuscripts, we pass them through structured document parsing pipeline and then generate visual question answers through capable LMM (Section 3.1). However, the generated questions can contain errors due to hallucinations or might be too straightforward to answer. Thus, to mitigate these issues, we offer an extensive filtering stage (Section 3.2). To evaluate the benchmark, we propose an efficient evaluation framework to infer the overall performance on the benchmark using only small subset of evaluations, making the evaluations extremely resource-efficient (Section 3.3). The data acquisition and filtering steps are schematically visualized in Figure 3. 3.1 DATA ACQUISITION AND VQA GENERATION We start with the data acquisition phase, then pre-process the data to obtain the required metadata (e.g. placements of figures, captions, etc.), and then generate the first iteration of VQA from the multi-modal data (figures and tables) from the manuscripts. Data Acquisition: At any given timestamp, we begin by acquiring only ArXiv papers which have non-exclusive license to distribute from predefined domains such as Computer Science (cs.AI, cs.CV), Electrical Engineering (eess.SP, eess.SY), and Quantitative Biology (q-bio.BM, q-bio.GN). However, these manuscripts contain lot of information that might not be necessary for the task of VQA data generation. Thus, to extract pertinent information we require pre-processing step. Pre-processing: The downloaded PDFs undergo structured document parsing pipeline using the DeepSearch toolkit (Team, 2022), which extracts comprehensive layout of each document, including the positions of figures, tables, captions, and other elements. This structured layout forms the basis for extracting the multi-modal data required for subsequent tasks. To enrich the dataset with additional metadata not captured by the parsing pipeline, we employ meta-prompting approach with CLIP (Radford et al., 2021b), similar to the method used by Mirza et al. (2024). Specifically, we classify the figures into three distinct categories: Block Diagram, Chart, and Qualitative visual examples which facilitates more granular, domain-specific evaluation of LMM performance. Preprint, Under review VQA Generation: For Visual Question Answering (VQA), we construct pairs of figures and their corresponding captions, and for generating VQA from the data present in the tables, we obtain (e.g. crop) images of tables accompanied by their corresponding data. The VQA process involves two steps using GPT-4o. First, we input the figure and its caption to GPT-4o to generate detailed description of the figure, employing Chain-of-Thought (CoT) approach (Wei et al., 2022). Next, the detailed description and figure are fed back into GPT-4o, with prompts adapted from ConMe (Huang et al., 2024) to suit our scientific use case, enabling the generation of relevant VQA questions. For questions from the tables, we utilize the tables content directly, presenting both the image of the table and its data in markdown format to GPT-4o to produce questions that require common-sense reasoning and data manipulation. The automated nature of this process ensures robust and comprehensive evaluation framework for LMMs, tailored to scientific literature specifics. Detailed prompt templates can be found in Appendix A.4."
        },
        {
            "title": "3.2 FILTERING PHASE",
            "content": "Even though GPT-4o is powerful and has been reported to outperform humans on many different benchmarks (OpenAI, 2023), still it is prone to errors and sometimes can even result in VQA pairs that are answerable without requiring the visual information. Thus, to ensure that the benchmark remains competitive and also has minimum errors, we propose an extensive automatic filtering step. At higher level, the filtering phase consists of two main parts, each designed to mitigate separate issue that can arise due to the automatic dataset generation. Blind test with an LLM: To ensure that the generated VQA pairs are truly multi-modal, we pass them through Large Language Model (LLM) without providing any associated images or image descriptions. This process, referred to as blind test, aims to identify questions that the LLM can answer correctly even in the absence of visual context, indicating they are not truly multi-modal. To ensure robustness, this blind evaluation is repeated multiple times to eliminate any potential lucky guesses by the LLM. Questions that are consistently answered correctly by the LLM are filtered out, resulting in the removal of approximately 30% of the generated questions. This step ensures that the remaining questions in the dataset are inherently multi-modal and cannot be answered solely based on linguistic context. The filtered dataset thus represents more challenging benchmark for evaluating multi-modal capabilities of vision-language models. Agreement between disjoint models: Generative models, including LMMs, are prone to hallucination, where the model generates incorrect or not grounded information. In our case, these hallucinations can lead to erroneous VQA pairs. To address this issue, we introduce an additional filtering step. All questions that pass the initial blind test are reviewed along with their generated answers by different LMM, in this case Claude-Sonnet (cla, 2024), which is provided with the image, question, and the ground truth answer which were all generated by GPT-4o. This second model is asked to either agree or disagree with the generated answer, considering the visual context. We point out that agreement between models is nuanced process; incorporating more models to validate answers may lead to the exclusion of difficult questions, thereby diluting the difficulty of the dataset. Therefore, we limit this validation step to models with comparable performance to the generation model (i.e. GPT-4o). Our preliminary manual evaluation on subset of the dataset indicates that this agreement step significantly reduces the proportion of incorrect ground-truth (GT) questions, with reduction of 38.5%, while minimally impacting the retention of high-quality question-GT pairs, with only 6.15% removal of valid pairs. This refinement ensures that the final dataset is both challenging and accurate for the evaluation of LMMs multi-modal reasoning capabilities. The generated corpus of data is ready to be incorporated into LiveXiv and can be updated automatically without any human intervention. 3.3 EFFICIENT EVALUATION Since LiveXiv is dynamic benchmark, evaluation can be costly: ideally, whenever new version of the benchmark is released, all models must be re-evaluated on the updated data, which can pose an engineering challenge and become computationally expensive when handling dozens of models. In this section, we describe our approach to efficient evaluation, which avoids re-evaluating all models at each step, making LiveXivs maintenance economically feasible. Our idea is based on Item Response Theory (IRT) (Cai et al., 2016; Van der Linden, 2018; Lord et al., 1968; Maia Polo et al., 2024b;a), collection 5 Preprint, Under review of statistical models traditionally used in psychometrics and educational assessment. We briefly give some background on IRT and detail how we use it for our evaluations. 3.3.1 ITEM RESPONSE THEORY (IRT) We use the IRT model to predict the probability of certain LMM answering correctly on sample (question) j. In mathematical terms, let Yij {0,1} denote the correctness on sample when responded by LMM i: Yij Bernoulli(µ(θi,βj)), where θi is an LMM-specific parameter, βj is sample-specific parameter, and µ is function that maps those parameters to the probability of correctness. In this work, we follow Maia Polo et al. (2024b) and assume the parameters live in the real line while µ induces logistic regression model. In more detail, we assume P(Yij =1;θi,βj)= 1 1+exp[(θiβj)] . (1) Here, θi can be interpreted as the skill level of LMM while βj is seen as the hardness of sample j. By equation 1, if θi is much greater (resp. smaller) than βj, then the probability P(Yij =1;θi,βj) will be close to one (resp. zero). This version of the IRT model is known as the Rasch model (Georg, 1960; Chen et al., 2023b), and it is widely used in fields such as recommendation systems (Starke et al., 2017), educational testing (Clements et al., 2008), and evaluation of language models (Maia Polo et al., 2024b). Moreover, it has similar formulation to the popular Bradley-Terry model (Bradley & Terry, 1952) used in Chatbot Arena (Chiang et al., 2024), popular and dynamic benchmark for AI-powered chatbots. We fit the Rasch model using maximum likelihood estimation as in Chen et al. (2023b) and Maia Polo et al. (2024b). 3.3.2 EFFICIENT EVALUATION WITH IRT We can estimate old model scores on new data without reevaluating those models. Let It and Jt represent sets of non-negative integers corresponding to LMMs and samples at time t0. We assume that It It+1 since the set of available models does not shrink over time, and Jt1 Jt2 = for t1 =t2 because samples are not repeated across different time steps. Let the set of evaluated models at time be denoted by ˆIt. For t>0, we assume that ItIt1 is proper subset of ˆIt, meaning that all newly introduced models are evaluated on the new batch of samples along with some previously existing models. At t=0, we assume that ˆIt =It, meaning all models are evaluated on all samples. Furthermore, we assume that ˆIt is much smaller than It when t>0 so computing power and evaluation time can be saved. Our goal at time > 0 is to estimate the performance of model / ˆIt on the set of samples Jt, using only the correctness scores Dt = {Yij : (i,j) Ωt}, where Ωt ttˆIt Jt. Specifically, we aim to approximate Sit = 1 Jt Yij by estimating its expectation jJt (cid:80) E[Sit]= 1 Jt (cid:88) jJt P(Yij =1;θi,βj). (2) For moment, let us assume that Ωt is known. Using Dt, we can estimate the skill parameters θis of all models in It and the difficulty parameters βjs of all samples in ttJt; we denote these estimates as ˆθis and ˆβjs. Finally, we obtain an approximation for equation 2, ˆE[Sit], by substituting θi and βjs by their estimates. The estimator ˆE[Sit] is known as the Performance-IRT estimator (Maia Polo et al., 2024b;a). Now, we provide method to obtain Ωt assuming Ωt1 is given; in summary, we need to decide which models in It1 are going to be in ˆIt. Our approach to choosing which models are going to be re-evaluated is inspired by the concept of optimal design of tests (Van der Linden, 2017, Chapter 9) but in which we choose LMMs instead of samples. First, we set budget mt, representing the maximum number of models to be re-evaluated at time step t. Second, assuming that the level of difficulty of the new samples Jt is not very different from the ones in Jt1, we choose set of mt representative samples in Jt1 by ordering ˆβjs and choosing equally spaced samples, based on their quantiles, from the 5th to the 95th percentiles; this will give us questions with variety of difficulties, excluding outliers. For example, if mt = 3 we would choose questions with difficulties in the 5th, 50th, and 95th percentiles. Denote the chosen core 6 Preprint, Under review set of samples as {j0,,jmt1} and, for each one of these samples jk, we choose model in It1 such that the following Fisher information criterion (cid:16) Yijk =1;ˆθi,ˆβjk Fjk(i)=P (cid:17)(cid:104) (cid:16) Yijk =1;ˆθi,ˆβjk 1P (cid:17)(cid:105) is maximized. The model that maximizes Fjk is maximally informative about the parameter of sample jk and, consequently, about all samples with similar difficulty levels in the new version of LiveXiv; this will help us estimate the difficulties of new samples. We note that some models in It1 might not be available at step t, e.g., due to deprecation; when choosing models, we do not consider them, but note that we can still estimate their performance on the new batches of data. Moreover, the model selection procedure can also take convenience into account; for example, if two models have very similar Fisher information, we opt for the one that is cheaper to evaluate. In our experiments, we show that re-evaluating 5 models at each step is enough for good performance prediction. When the total number of models is 50, for example, we expect this procedure to save us at least 10 computing resources considering that we can opt to re-evaluate cheaper models if that does not imply big loss in terms of Fjk."
        },
        {
            "title": "4 RESULTS & ANALYSIS",
            "content": "This section presents the results obtained on the first iteration of LiveXiv. First, we start by describing the experimental settings. Then, we present the results and finally conclude with detailed analysis of our dataset. 4.1 EXPERIMENTAL SETTINGS Evaluation Protocol: After the generation of the question-answer pairs from our automated pipeline explained in Section 3, we transform the benchmark to multiple-choice questions. We resort to the generate inference employed extensively by previous works, such as Li et al. (2024d); Huang et al. (2024); Liu et al. (2023d). The model is prompted to choose the letter corresponding to the correct choice and answer with the letter directly. The output letter is then compared with the ground truth and the accuracy is measured. We report the average accuracy over all the samples evaluated in Table 1. For ease of assimilation and to obtain insights into what type of data the models flourish at, we provide the results from data generated on tables and figures separately. The data generated from figures is labeled as part of Visual Question and Answers (VQA) and the data from the tables is labeled as Table Question and Answers (TQA). Examples for the multiple-choice formulation of the question-answer pairs are added to the Appendix Section A.2. Size of dataset: The current version of our LiveXiv consists of 7328 questions on figures, and 9000 questions on tables, both are generated from 250 papers (25 papers from 10 domains). Overall our first version of the dataset has 16328 questions in total. Thanks to the continual growth in the number of publications in our target domains and the fully automatic nature of our proposed LiveXiv pipeline for benchmark data generation, we will grow LiveXiv by adding an equal-sized large amount of new VQA & TQA data (around 7K VQA and 9K TQA) every month. Such large-scale updates might be significantly more difficult for benchmarks relying on manual data collection for live updates (Zhang et al., 2024b). Models: We extensively evaluate our benchmark by employing total of 17 LMMs. Specifically, we employ 5 models from the LLaVA family of models including LLaVA 1.5-7B and LLaVA 1.5-13B (Liu et al., 2023c), LLaVA-1.6-7B and LLaVA 1.6-34B (Liu et al., 2023b) and LLaVA One-Vision (Li et al., 2024b). Furthermore, we employ IntstructBLIP (Dai et al., 2023), InternVL2-2B and InternVL2-8B (Chen et al., 2023c), InternLM-Xcomposer2-4KHD (Dong et al., 2024b) and InternLM-Xcomposer2.5 (Chen et al., 2023c), Mantis (Jiang et al., 2024), Phi3v (Abdin et al., 2024), Idefics2 (Laurencon et al., 2024b) and Idefics3 (Laurencon et al., 2024a), Qwen2-VL (Wang et al., 2024) and API models Claude-Sonnet (cla, 2024) and GPT4o (OpenAI, 2023) for our evaluations. These models have been chosen because of their varying characteristics and strong performance on multiple current benchmarks. All the models (except GPT-4o and Cloude-Sonnet) are accessed from the huggingface API, which makes our framework modular for an extension to more models as they are being added to the hub in the future. 4.2 EXPERIMENTAL RESULTS Results on entire dataset: We evaluated 17 large multi-modal models (LMMs) across two prominent tasks, VQA and TQA. Table 1 provides detailed summary of the performance across both tasks. One interesting 7 Preprint, Under review Table 1: VQA and TQA average accuracy across ArXiv taxonomy (the number of samples is in brackets). VQA Accuracy InstructBLIP-7B LLaVA-1.5-7B LLaVA-1.6-Mistral-7B Mantis-LLama3-8B LLaVA-1.5-13B Idefics2-8B IXC2-4KHD-7B IXC2.5-7B InternVL2-2B LLaVA-1.6-34B Idefics3 LLaVA-OneVision-7B Phi3v GPT-4o InternVL2-8B Qwen2-VL Claude-Sonnet TQA Accuracy InstructBLIP-7B LLaVA-1.6-Mistral-7B Mantis-LLama3-8B LLaVA-1.5-7B LLaVA-1.5-13B Idefics2-8B InternVL2-2B IXC2-4KHD-7B IXC2.5-7B LLaVA-OneVision-7B Phi3v Idefics3 LLaVA-1.6-34B GPT-4o Qwen2-VL InternVL2-8B Claude-Sonnet eess.SP q-bio.BM q-bio.CB (900) (651) (840) cs.AI eess.SY cs.CV cs.RO q-bio.GN cs.LG q-bio.TO Mean (7328) (685) (735) (720) (647) (844) (634) (672) 21.2 29.0 28.1 32.3 32.6 35.6 33.0 46.2 48.4 48.4 54.4 53.1 60.1 64.1 64.5 68.0 78.9 25.2 27.8 28.7 28.6 29.4 38.4 36.7 46.1 48.1 45.6 50.6 49.7 54.4 55.9 56.9 62.4 72.3 19.5 29.5 28.6 32.7 31.5 35.9 33.0 48.2 50.4 47.4 52.3 51.8 59.9 58.8 61.4 71.8 77.4 24.5 31.9 33.9 33.7 33.4 40.7 40.1 53.3 53.4 55.9 57.2 57.2 64.5 62.9 67.0 67.2 77.7 23.4 30.5 31.0 30.2 33.2 40.5 35.8 50.5 50.5 52.5 57.0 52.8 61.8 64.4 65.3 69.3 78.4 21.3 31.0 31.4 36.9 35.9 38.6 45.7 45.1 46.3 51.8 53.3 57.2 56.0 60.1 59.9 63.3 69. 22.6 34.9 33.3 32.6 35.7 39.6 44.5 47.0 54.2 54.9 54.6 57.6 58.5 60.3 65.3 64.6 74.1 24.9 29.1 27.0 29.2 30.6 30.3 37.9 47.9 48.4 47.9 51.5 51.6 58.9 55.2 58.4 64.5 72.9 24.1 29.3 27.9 30.8 30.0 36.9 35.8 49.4 48.2 47.9 51.5 51.1 56.0 59.0 61.4 63.7 76.4 21.1 32.8 29.5 34.9 32.2 38.8 36.1 46.8 50.9 50.2 56.6 59.1 58.2 64.4 65.6 71.9 75.9 23.6 30.4 29.9 32.1 32.3 37.6 37.7 48.1 49.8 50.0 53.7 53.9 58.7 60.3 62.3 66.6 75.4 eess.SP q-bio.BM q-bio.CB cs.AI eess.SY cs.CV cs.RO q-bio.GN cs.LG q-bio.TO Mean (9000) (1121) (1195) (1624) (1069) (472) (426) (697) (932) (570) (894) 18.1 24.9 31.9 31.2 30.5 37.1 41.1 42.3 42.7 49.5 47.2 46.2 51.4 50.7 57.5 60.3 84.0 16.6 20.9 26.8 28.0 28.9 35.9 40.8 40.5 44.2 49.4 48.1 47.8 48.7 51.8 57.5 59.6 81. 20.2 25.4 29.7 30.0 33.1 43.2 46.8 48.4 53.7 55.7 55.5 53.9 54.8 56.2 65.3 67.3 80.3 21.8 23.5 29.2 30.0 31.5 39.2 38.4 39.8 48.9 49.2 48.9 50.3 52.8 54.3 57.5 59.7 84.5 18.9 25.0 36.4 33.5 35.6 42.8 47.2 50.8 58.3 59.7 57.8 57.8 57.8 62.3 67.2 70.1 85.6 20.7 21.8 29.0 29.1 31.5 35.0 37.0 39.2 44.7 48.9 47.2 47.7 48.7 50.8 60.1 62.6 84.0 22.8 24.9 30.5 32.5 33.0 40.0 41.9 44.6 51.6 50.7 51.9 53.2 51.4 56.1 61.8 64.6 86.5 16.9 22.6 27.6 29.9 29.8 38.7 42.8 47.5 52.0 49.2 51.7 51.2 51.0 56.3 60.8 61.1 82. 18.5 24.9 30.4 30.0 29.4 37.0 41.3 36.7 49.2 46.6 48.0 50.5 51.5 55.1 59.1 59.2 86.4 18.2 25.4 29.1 30.3 30.8 38.9 42.6 40.7 52.0 50.8 51.8 52.7 56.2 55.0 61.4 65.0 82.3 19.1 23.5 29.3 30.0 30.9 38.2 41.5 42.1 49.1 50.2 50.2 50.6 51.8 54.5 60.2 62.1 83.5 observation is the Claudes superior performance across the board. This substantial performance gap suggests that Claudes architecture and underlying methodologies are particularly well-suited for both VQA and TQA tasks. The results align with other relatively close benchmarks, DocVQA (Mathew et al., 2021), ChartQA (Masry et al., 2022) and AI2D (Kembhavi et al., 2016), where we see similar trend, Claude has significantly higher performance over the runner-up models such as Qwen2-VL, GPT-4o and InternVL28B. See Table 4 for more details. However, notable caveat is that Claude plays an integral role in the question-filtering process, which may introduce potential bias in favor of questions it is predisposed to solve effectively. This implies that while Claudes overall performance remains strong, the evaluation might not fully reflect its robustness to novel or more diverse question types outside the scope of this filtering. We further observe that newer models, such as InternVL2-8B and Qwen2-VL, consistently outperform older models like LLaVA-1.6 and Idefics2, suggesting rapid advancements in LMM development over the past few months. This trend highlights the continual improvement in both architecture and training paradigms, leading to better generalization across multi-modal tasks. Zooming into the domain-specific performance using an ArXiv-based taxonomy, we evaluate each models effectiveness in distinct scientific fields such as biology, electrical engineering, and mathematics. Our results show that certain models, particularly the newer architectures, exhibit higher degree of robustness across diverse domains, highlighting that the models training data might already have potential contamination issues. Conversely, for VQA, models in the Intern-VL2 and the LLaVA families appear to be more sensitive to domain shifts, performing inconsistently across different scientific areas, as oppose to the more recent models like Qwen2-VL, Claude and GPT4o, see Figures 5, 6 for more details. For TQA, its not the case, probably since the questions test more specific skills such retrieval and arithmetic manipulations, see Figures 7, 8. This domain-specific sensitivity emphasizes the need for further refinements in LMMs, especially when applied to specialized scientific knowledge domains. Overall, this analysis not only underscores the ongoing evolution of LMMs but also highlights areas for further investigation, especially concerning model adaptability to diverse content domains and the potential biases introduced by models. Contamination free effect: Interestingly, focusing on new data that came after the LMMs were trained, allows LiveXiv to provide new, contamination-free, perspective on the relative performance ranking between strong LMMs. For example, taking the official results from original publications and computing the average 8 Preprint, Under review Table 2: Performance change between LiveXiv and manually verified subset averaged across all evaluated models. LiveXiv is robust, thanks to excessive filtering steps which keep the labeling errors low. LiveXiv Verified Subset Absolute Avg. VQA TQA 46.734 45.101 47.273 46.028 2.336 2.105 ranking of the LMMs from Table 1 over the long-established DocVQA (Mathew et al., 2021), ChartQA (Masry et al., 2022) and AI2D (Kembhavi et al., 2016) benchmarks , and comparing those to average rankings provided by LiveXiv  (Table 1)  , we observe some significant ranking changes. e.g. GPT-4o drops almost 2 points and IXC2.5 and IXC2-4KHD drop over 4 points in average ranking, see Table 5 for all the details. Performance on manually filtered dataset: To further verify our proposed automated question-answer generation and filtering methodology and to obtain measure of errors in the generated data, we manually verified subset of 1000 samples (500 for both, VQA and TQA) and evaluated all models on this subset. Table 2 presents the results for VQA and TQA on the filtered subset. We see that on average the performance only fluctuates by 2.3% and 2.1% for VQA and TQA when comparing the results obtained by all the models on the entire dataset and the manually verified subset. These results hint that our automated question-answer generation pipeline and the filtering methodology is quite robust. Detailed results can be found at the Appendix, Tables 6 and 7. Efficient evaluations of LMMs: In this section, we empirically validate the effectiveness of our proposed efficient re-evaluation method for LMMs. Dynamic benchmarks like LiveXiv present challenge in terms of evaluation costs since each time new version of the benchmark is released, all models should be re-evaluated on the updated data. This process, however, can become computationally prohibitive when dealing with numerous models. Our goal is to demonstrate that by re-evaluating only small subset of models on the new version of LiveXiv, we can still reliably predict the performance of the remaining models. For this experiment, we focus on either VQA or TQA (but not both simultaneously) and consider the 10 ArXiv domains. We chronologically split each domains papers and samples into training and test sets where the test sets contain 85% of the more recent papers and samples. The training set represents hypothetical first version of LiveXiv, while the test set simulates second version for which we would like to perform the efficient updates. All 17 LMMs are fully evaluated on the first version, but only 5 are re-evaluated on the second version using the model selection methodology detailed in Section 3.3. An IRT model is then fit to the full observed data, and we predict the performance of the non-re-evaluated models on each ArXiv domain and the overall benchmark using empirical versions of equation 2. Figure 4 presents these results, with domain-specific outcomes on the left and full benchmark results on the right. We report both the mean absolute error (MAE) ( mean absolute deviation) for the test models when predicting their accuracy and Spearmans rank correlation across all 17 LMMs on the second LiveXiv version when comparing VQA TQA MAE Rank Corr. MAE Rank Corr. 2.31.5 eess.SP q-bio.BM 3.52.4 q-bio.CB 2.00.9 3.21.3 cs.AI eess.SY 1.91.3 2.01.2 cs.CV 2.01.3 cs.RO q-bio.GN 2.71.6 2.11.6 q-bio.TO 2.51.5 cs.LG 97.1 95.1 97.1 97.3 95.3 98.0 97.5 96.8 97.3 97.3 2.61.5 2.00.9 2.21.6 2.01.0 2.61.6 2.01.5 4.81.8 3.51.4 3.91.4 2.81.3 98.3 97.8 98.0 97.8 95.6 97.1 94.8 97.5 95.1 98.8 Figure 4: Performance prediction results of our efficient re-evaluation method on the hypothetical second (next) version of the LiveXiv benchmark (please see text for details). The table on the left shows the mean absolute error (MAE) and Spearman rank correlation when comparing true and predicted accuracies across individual ArXiv domains, while the graph on the right presents the overall benchmark performance. The results demonstrate that re-evaluating only 5 out of 17 models is sufficient to accurately predict the performance of the remaining models, as well as maintain high rank correlation, validating the effectiveness of our approach. 9 Preprint, Under review Table 3: LiveXiv accuracy (%) on different categories of question and partitions averaged over all evaluated models. Data Analysis Reasoning Attribute Localization Reading Arithmetic Charts Block Diagram Qualitative VQA TQA 46.93 46.02 47.95 63.61 46.18 68. 41.91 51.66 47.83 59.35 46.87 35.56 44.17 - 52.69 - 48.60 - real accuracy and predicted accuracy. These results suggest that re-evaluating just 5 models is likely to be sufficient for accurately predicting the performance of the remaining models and the ranking of all models. In Appendix A.3, we present additional experiments to further validate the effectiveness of our method. Specifically, we (i) examine different numbers of re-evaluated models, (ii) show that accuracy prediction error negatively correlates with test sample size, and (iii) test our approach on MM-LiveBench (Zhang et al., 2024b). The second point suggests that in real-world applications, we expect our efficient evaluation strategy to achieve lower MAE than those reported in Figure 4, given that test datasets will be larger and unaffected by data splitting."
        },
        {
            "title": "4.3 ANALYSIS AND ABLATIONS",
            "content": "To analyze various aspects of LiveXiv we provide an extensive ablation study. We start by providing an analysis of the results from different models obtained w.r.t the language content partitions, then provide results for different models w.r.t the visual data partitions. Language analysis - performance according to question type. To discover error slices of models for an analysis of mistakes they commonly make, we classify the questions present in the benchmark into one of the following categories: reasoning, data analysis, reading, localization, attribute, and arithmetic. To achieve this classification, we employ the Llama-3.1 (Meta, 2024) LLM and prompt the model with the question and the list of categories to choose for this question. The prompt is provided in the Appendix Figure 19. Table 3 summarizes the results for all the models. We see that the performance of these models on the arithmetic partition is the lowest on average as compared to other partitions highlighting room for potential improvement. We also provide the detailed results for all models on these partitions for VQA and TQA in Tables 9 and 10 of the Appendix. Vision analysis - performance according to figure type. For more fine-grained analysis of LMM performance on different types of visual data present in our benchmark, we first categorize the data through Meta-Prompting for CLIP, proposed by Mirza et al. (2024), in zero-shot classification setup. Specifically, we classify the image content into three categories of figures: Block diagrams, Qualitative visual results, and Charts. We summarize the results in Table 3. Detailed results for each models performance can be found in Table 8 in the Appendix. The results reveal significant variance in performance across figure types for nearly all models. In most cases, block diagrams are the most favorable category for models. However, InternLM-Xcomposer2-4KHD-7B (Dong et al., 2024b) stands out by achieving the highest accuracy on Qualitative figures. Overall, Charts emerge as the most challenging figure type on average, suggesting lack of sufficient examples in the training data for this category. This kind of analysis can be further expanded to include more categories and discover error slices on which different models struggle so that potential targeted improvements can be designed for these models to mitigate the shortcomings."
        },
        {
            "title": "5 LIMITATIONS AND CONCLUSIONS",
            "content": "Limitations. LiveXiv relies on capable proprietary LMMs in order to be fully automatic, and with high quality. However, relying on proprietary LMMs is limitation since we do not have full control over the models, they can change through time and might affect LiveXiv. Nevertheless, we commonly expect them to continuously improve leading to positive impact on LiveXiv effectiveness. Conclusions. We propose LiveXiv, an ever-evolving, fully automatic, multi-modal benchmark focused on scientific domains to tackle test set contamination issues and consequently allow new (contamination-free) perspective on relative ranking of advanced LMMs. We utilize ArXiv, as the data source, carefully and extensively crafting quality dataset to evaluate LMMs. To significantly reduce the computational and logistical overhead of maintaining the dataset throughout time and models, we propose an efficient evaluation method that can save more than 70% of the evaluated models on each dataset version. Our method can be extended to other archives such as BioRXiv to extend our dataset to new domains. 10 Preprint, Under review possible future direction is to evaluate data contamination on past versions of the benchmark, using comparison of the efficient evaluation vs. full naive evaluation."
        },
        {
            "title": "6 ETHICS STATEMENT",
            "content": "This work introduces LiveXiv, live multi-modal benchmark for evaluating LMMs using scientific ArXiv papers. By relying solely on publicly available ArXiv manuscripts with proper licenses, we ensure compliance with copyright and distribution policies. The automated generation of Visual Question Answering (VQA) and Table Question Answering (TQA) pairs enables scalable evaluation of LMMs without human involvement, minimizing the risk of human biases in data collection. However, we acknowledge the potential for unintentional biases within the models or dataset itself. Continuous evaluation and refinement are necessary to mitigate these biases and promote the responsible deployment of LMMs in wider applications."
        },
        {
            "title": "REFERENCES",
            "content": "Introducing the next generation of claude. https://www.anthropic.com/news/ claude-3-family, 2024. Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. Scale AI. Seal leaderboards. https://scale.com/leaderboard, 2024. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: Visual Language Model for Few-Shot Learning. arXiv:2204.14198, 2022. Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. arXiv:2005.14165, 2020. Li Cai, Kilchan Choi, Mark Hansen, and Lauren Harrell. Item response theory. Annual Review of Statistics and Its Application, 3:297321, 2016. Jun Chen, Deyao Zhu1 Xiaoqian Shen1 Xiang Li, Zechun Liu2 Pengchuan Zhang, Raghuraman Krishnamoorthi2 Vikas Chandra2 Yunyang Xiong, and Mohamed Elhoseiny. MiniGPT-v2: Large Language Model as Unified Interface for Vision-Language Multi-task Learning. arXiv preprint arXiv:2310.09478, 2023a. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024. Yunxiao Chen, Chengcheng Li, Jing Ouyang, and Gongjun Xu. Statistical inference for noisy incomplete binary matrix. Journal of Machine Learning Research, 24(95):166, 2023b. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023c. 11 Preprint, Under review Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph Gonzalez, et al. Chatbot arena: An open platform for evaluating llms by human preference. arXiv preprint arXiv:2403.04132, 2024. Douglas Clements, Julie Sarama, and Xiufeng Liu. Development of measure of early mathematics achievement using the rasch model: The research-based early maths assessment. Educational Psychology, 28(4):457482, 2008. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale InstructBLIP: Towards General-purpose Vision-Language Models with Fung, and Steven Hoi. Instruction Tuning. In NeurIPS, 2023. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: large-scale hierarchical image database. In Proc. CVPR, 2009. Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, et al. Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model. arXiv preprint arXiv:2401.16420, 2024a. Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Songyang Zhang, Haodong Duan, Wenwei Zhang, Yining Li, et al. Internlm-xcomposer2-4khd: pioneering large visionlanguage model handling resolutions from 336 pixels to 4k hd. arXiv preprint arXiv:2404.06512, 2024b. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. Rasch Georg. Probabilistic models for some intelligence and attainment tests. Copenhagen: Institute of Education Research, 1960. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Irene Huang, Wei Lin, Jehanzeb Mirza, Jacob Hansen, Sivan Doveh, Victor Ion Butoi, Roei Herzig, Assaf Arbelle, Hilde Kuhene, Trevor Darrel, et al. Conme: Rethinking evaluation of compositional reasoning for modern vlms. arXiv preprint arXiv:2406.08164, 2024. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max Ku, Qian Liu, and Wenhu Chen. Mantis: Interleaved multi-image instruction tuning. arXiv preprint arXiv:2405.01483, 2024. Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pp. 235251. Springer, 2016. Alex Krizhevsky and Geoffrey Hinton. Learning Multiple Layers of Features from Tiny Images. Technical report, Department of Computer Science, University of Toronto, 2009. Preprint, Under review Hugo Laurencon, Andres Marafioti, Victor Sanh, and Leo Tronchon. Building and better understanding vision-language models: insights and future directions. arXiv preprint arXiv:2408.12637, 2024a. Hugo Laurencon, Leo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models? arXiv preprint arXiv:2405.02246, 2024b. Bo Li, Kaichen Zhang, Hao Zhang, Dong Guo, Renrui Zhang, Feng Li, Yuanhan Zhang, Ziwei Liu, and Chunyuan Li. Llava-next: Stronger llms supercharge multimodal capabilities in the wild, 2024a. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024b. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. LLaVA-OneVision: Easy Visual Task Transfer. arXiv preprint arXiv:2408.03326, 2024c. Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench: Benchmarking multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1329913308, 2024d. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping Language-Image Pretraining with Frozen Image Encoders and Large Language Models. arXiv preprint arXiv:2301.12597, 2023. Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022. Wei Lin, Muhammad Jehanzeb Mirza, Sivan Doveh, Rogerio Feris, Raja Giryes, Sepp Hochreiter, and Leonid Karlinsky. Comparison visual instruction tuning. arXiv preprint arXiv:2406.09240, 2024. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved Baselines with Visual Instruction Tuning. arXiv:2310.03744, 2023a. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. LLaVA-Next (LLaVA 1.6). arXiv:2310.03744, 2023b. URL https://llava-vl.github.io/blog/2024-01-30-llava-next/. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual Instruction Tuning. In NeurIPS, 2023c. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023d. FM Lord, MR Novick, and Allan Birnbaum. Statistical theories of mental test scores. 1968. Yujie Lu, Dongfu Jiang, Wenhu Chen, William Yang Wang, Yejin Choi, and Bill Yuchen Lin. Wildvision: Evaluating vision-language models in the wild with human preferences. arXiv preprint arXiv:2406.11069, 2024. Felipe Maia Polo, Lucas Weber, Leshem Choshen, Yuekai Sun, Gongjun Xu, and Mikhail Yurochkin. tinybenchmarks: evaluating llms with fewer examples. arXiv preprint arXiv:2402.14992, 2024a. Felipe Maia Polo, Ronald Xu, Lucas Weber, Mırian Silva, Onkar Bhardwaj, Leshem Choshen, Allysson Flavio Melo de Oliveira, Yuekai Sun, and Mikhail Yurochkin. Efficient multi-prompt evaluation of llms. arXiv preprint arXiv:2405.17202, 2024b. Ahmed Masry, Do Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: benchIn Findings of the mark for question answering about charts with visual and logical reasoning. Association for Computational Linguistics: ACL 2022, pp. 22632279, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-acl.177. URL https://aclanthology.org/2022.findings-acl.177. 13 Preprint, Under review Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pp. 22002209, 2021. Meta. Introducing meta llama 3: The most capable openly available llm to date. https: //ai.meta.com/blog/meta-llama-3, 2024. M. Jehanzeb Mirza, Leonid Karlinsky, Wei Lin, Sivan Doveh, , Jakub Micorek, Mateusz Kozinski, Hilde Kuhene, and Horst Possegger. Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs. In Proceedings of the European Conference on Computer Vision (ECCV), 2024. OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Piotr Padlewski, Max Bain, Matthew Henderson, Zhongkai Zhu, Nishant Relan, Hai Pham, Donovan Ong, Kaloyan Aleksiev, Aitor Ormazabal, Samuel Phua, et al. Vibe-eval: hard evaluation suite for measuring progress of multimodal language models. arXiv preprint arXiv:2405.02287, 2024. Yotam Perlitz, Elron Bandel, Ariel Gera, Ofir Arviv, Liat Ein-Dor, Eyal Shnarch, Noam Slonim, Michal Shmueli-Scheuer, and Leshem Choshen. Efficient benchmarking (of language models). arXiv preprint arXiv:2308.11696, 2023. Ameya Prabhu, Vishaal Udandarao, Philip Torr, Matthias Bethge, Adel Bibi, and Samuel Albanie. Lifelong benchmarks: Efficient model evaluation in an era of rapid progress. arXiv preprint arXiv:2402.19472, 2024. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning Transferable Visual Models from Natural Language Supervision. In Proc. ICML, 2021a. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PMLR, 2021b. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the Limits of Transfer Learning with Unified Text-to-Text Transformer. arXiv:1910.10683, 2019. Manley Roberts, Himanshu Thakur, Christine Herlihy, Colin White, and Samuel Dooley. To the cutoff... In The Twelfth International and beyond? longitudinal perspective on llm data contamination. Conference on Learning Representations, 2023. Saurabh Srivastava, Anto PV, Shashank Menon, Ajay Sukumar, Alan Philipose, Stevin Prince, Sooraj Thomas, et al. Functional benchmarks for robust evaluation of reasoning performance, and the reasoning gap. arXiv preprint arXiv:2402.19450, 2024. Alain Starke, Martijn Willemsen, and Chris Snijders. Effective user interface designs to increase energy-efficient behavior in rasch-based energy recommender system. In Proceedings of the eleventh ACM conference on recommender systems, pp. 6573, 2017. Deep Search Team. Deep Search Toolkit, 6 2022. URL https://github.com/DS4SD/ deepsearch-toolkit. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and Efficient Foundation Language Models. arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open Foundation and Fine-Tuned Chat Models. arXiv preprint arXiv:2307.09288, 2023b. Wim Van der Linden. Handbook of item response theory: Volume 3: Applications. CRC press, 2017. Wim Van der Linden. Handbook of item response theory: Three volume set. CRC Press, 2018. 14 Preprint, Under review Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng Cheng, Weiwei Lu, Rui Hu, et al. Skywork: more open bilingual foundation model. arXiv preprint arXiv:2310.19341, 2023. Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Ben Feuer, Siddhartha Jain, Ravid Shwartz-Ziv, Neel Jain, Khalid Saifullah, Siddartha Naidu, Chinmay Hegde, Yann LeCun, Tom Goldstein, Willie Neiswanger, and Micah Goldblum. Livebench: challenging, contamination-free llm benchmark. 2024. URL arXivpreprintarXiv:2406.19314. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95569567, 2024. Hugh Zhang, Jeff Da, Dean Lee, Vaughn Robinson, Catherine Wu, Will Song, Tiffany Zhao, Pranav Raja, Dylan Slack, Qin Lyu, et al. careful examination of large language model performance on grade school arithmetic. arXiv preprint arXiv:2405.00332, 2024a. Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, et al. Lmms-eval: Reality check on the evaluation of large multimodal models. arXiv preprint arXiv:2407.12772, 2024b. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. MiniGPT-4: Enhancing VisionLanguage Understanding with Advanced Large Language Models. arXiv preprint arXiv:2304.10592, 2023. Preprint, Under review"
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 ANALYSIS & ABLATIONS Figure 5: Domain sensitivity according to domains. We visualize the performance of each model across all domains. Clear trends revealed where old models or models with small LLM are under-fitting and perform worse across all domains. In the middle we have the mid-level models that are sensitive to the domain, indicating their lack of generalization across domain without any additional training. Lastly the newest models (open-source and proprietary) are robust to domain shifts and present stable performance across the domains. Figure 6: LMMs performance based on domain. To complement our analysis form Figure 5 we visualize the statistical properties of each domain. One clear trend is that across all modesl, the performance on cs.CV and cs.AI is the most concentrated, hinting lower variance between models. 16 Preprint, Under review Figure 7: Domain sensitivity according to domains. As opposed to the high variance some models demonstrated in Figure 5, in TQA the tasks and he visual content are more limited thus shrunken the performance variance greatly. Figure 8: LMMs performance based on domain. The domains are very similar in their statistical properties showing high variance in performance. This is probably due to wide range of models that differ significantly in their performance. 17 Preprint, Under review Table 4: Average results for relatively close benchmarks (DocVQA, ChartQA and AI2D) Average performance InstructBLIP-7B LLaVA-1.6-Mistral-7B Mantis LLaVA-1.5-7B LLaVA-1.5-13B Idefics2 InternVL2-2B IXC2-4KHD-7B IXC2.5-7B LLaVA-OneVision-7B Phi3v Idefics3 LLaVA-1.6-34B GPT-4o Qwen2-VL InternVL2-8B Claude-Sonnet 41.83 64.33 51.65 49.23 55.23 73.15 79.07 84.00 84.90 81.70 78.23 82.10 76.83 90.90 86.83 86.23 93.57 Table 5: Average ranking on static benchmarks (ChartQA, DocVQA and AI2D) and LiveXiv. We can see from the ranking difference column that some models have significant drop (negative difference) in the relative ranking in LiveXiv compared to the static datasets. The gap is highlighting potential risk of test data contamination when using static (frozen in time) benchmark datasets. Model Static datasets LiveXiv Difference (static - livexiv) InstructBLIP-7B LLaVA-1.6-Mistral-7B Mantis LLaVA-1.5-7B LLaVA-1.5-13B Idefics2 InternVL2-2B IXC2-4KHD-7B IXC2.5-7B LLaVA-OneVision-7B Phi3v Idefics3 LLaVA-1.6-34B GPT-4o Qwen2-VL InternVL2-8B Claude-Sonnet 15.33 13.67 14.50 14.33 12.67 12.00 9.00 6.33 5.00 8.00 9.00 8.50 9.33 2.33 3.33 3.33 1. 17.00 16.00 14.50 14.50 13.00 12.00 10.00 10.50 9.50 6.50 6.00 6.50 6.50 4.00 3.00 2.00 1.00 -1.67 -2.33 0.00 -0.17 -0.33 0.00 -1.00 -4.17 -4.50 1.50 3.00 2.00 2.83 -1.67 0.33 1.33 0.00 We provide additional details regarding the ablations: A.1.1 PERFORMANCE CHANGE COMPARED TO MANUALLY CURATED SUBSET We provide detailed table for VQA performance compared to manually curated subset of 500 samples. Preprint, Under review Table 6: VQA Performance change between LiveXiv and manually curated subset. Model LiveXiv (%) Manual (%) Performance Change LLaVA-1.5-7B InternVL2-2B LLaVA-OneVision-Qwen2-7B InternVL2-8B LLaVA-1.5-13B InternLM-Xcomposer2-4KHD-7B LLaVA-1.6-34B LLaVA-1.6-Mistral-7B InstructBLIP-7B InternLM-Xcomposer2.5-7B Mantis-LLama3-8B Phi3v Idefics2-8B Claude-Sonnet Qwen2-VL GPT-4o Idefics3 Average (absolute) change 29.983 49.548 52.864 61.558 31.859 36.801 49.196 29.163 23.216 47.839 32.094 58.141 36.851 75.942 66.248 60.303 52.881 28.654 48.654 56.154 66.154 30.385 33.654 53.269 26.346 21.346 50.769 28.654 58.654 36.731 79.615 71.346 60.577 52.692 -1.329 -0.894 3.290 4.596 -1.475 -3.147 4.073 -2.816 -1.870 2.930 -3.440 0.513 -0.120 3.673 5.098 0.274 -0.189 2.336 We provide detailed table for TQA performance compared to manually curated subset of 500 samples. Table 7: TQA Performance change between LiveXiv and manually curated subset. Model LiveXiv (%) Manual (%) Performance Change InstructBLIP-7B InternLM-Xcomposer2.5-7B InternVL2-8B LLaVA-1.6-Mistral-7B LLaVA-OneVision-Qwen2-7B LLaVA-1.5-13B LLaVA-1.5-7B LLaVA-1.6-34B Mantis-LLama3-8B Phi3v InternLM-Xcomposer2-4KHD-7B Idefics2-8B InternVL2-2B Claude-Sonnet Qwen2-VL GPT-4o Idefics3 Average (absolute) change 19.1 49.1 62.1 23.5 50.2 30.9 30.0 51.8 29.3 50.2 42.1 38.2 41.5 83.5 60.2 54.5 50.6 18.5 45.9 65.3 23.2 51.6 31.2 29.6 52.2 28.0 54.1 41.7 42.0 39.5 89.2 58.3 55.7 56. -0.6 -3.2 3.2 -0.3 1.4 0.3 -0.3 0.4 -1.3 4.0 -0.4 3.8 -2.0 5.6 -1.9 1.3 5.7 2.105 A.1.2 FIGURE TYPE We provide detailed table for VQA performance according to figure type content. We divide the performance to the following figure types: Chart, Block Diagram and Qualitative. 19 Preprint, Under review Table 8: Performance of LMMs over different figure types from the VQA set (the amount of samples for each figure type is in brackets)."
        },
        {
            "title": "Model",
            "content": "Chart (4354) block diagram Qualitative (2110) (864) InstructBLIP-7B InternLM-Xcomposer2.5-7B InternVL2-8B LLaVA-1.6-Mistral-7B LLaVA-OneVision-Qwen2-7B LLaVA-1.5-13B LLaVA-1.5-7B LLaVA-1.6-34B Mantis-LLama3-8B Phi3v InternLM-Xcomposer2-4KHD-7B Idefics2-8B InternVL2-2B Claude-Sonnet Qwen2-VL GPT-4o idefics3 22.9 46.7 59.1 27.3 48.7 29.3 28.1 44.6 29.3 56.1 32.9 34.3 47.2 73.7 63.1 56.5 51. 22.8 53.5 68.4 33.6 62.8 35.4 33.1 60.0 36.2 65.5 43.7 43.0 54.9 81.3 73.9 68.6 59.0 22.5 41.7 63.8 33.4 57.9 40.2 36.0 52.7 36.1 55.2 47.1 41.1 50.0 69.1 66.0 59.5 54.1 A.1.3 QUESTION CATEGORY We provide detailed tables for VQA and TQA performance according to the category of the questions as classified by an LLM. We divide the performance to the following categories: Data Analysis, Attribute, Reasoning, Reading, Localization and Arithmetic Table 9: VQA Performance by Question Categories (the amount of samples for each category is in brackets). Model InstrcutBLIP InterLM-XC-2.5 InternVL2-8B LLaVA1.6-7B LLaVA-OneVision LLaVA1.5-13B LLaVA1.5-7B LLaVA1.6-34B Mantis Phi3v InterLM-XC-4Khd Idefics2 InternVL2-2B Claude-Sonnet Qwen2-VL GPT4o Idefics3 Data Analysis Reasoning Attribute Localization Reading Arithmetic (154) (2291) (1596) (1470) (872) (903) 21.16 47.52 61.74 28.91 52.57 32.92 30.56 50.87 32.50 58.05 37.51 37.79 50.26 74.78 66.93 61.41 52.34 29.29 43.43 63.64 31.31 54.55 25.25 28.28 43.43 33.33 63.64 43.43 39.39 46.46 78.79 66.67 60.61 63.64 22.77 48.51 65.35 30.69 60.40 25.74 29.70 45.54 30.69 59.41 39.60 35.64 44.55 67.33 68.32 59.41 51.49 31.25 31.25 56.25 12.50 56.25 43.75 25.00 37.50 43.75 43.75 18.75 31.25 43.75 81.25 43.75 62.50 50.00 23.87 50.46 62.54 30.00 56.76 31.85 29.71 49.60 31.97 58.15 38.67 39.54 51.45 75.49 68.73 59.83 54. 23.01 47.28 62.41 30.43 52.85 32.58 30.89 50.00 31.80 59.07 37.09 36.34 48.62 75.64 65.01 59.76 54.00 A.2 DETAILED EXAMPLES FOR VQA AND TQA GENERATION Here we present full and detailed examples of our flow from ArXiv papers until constructing verified multi-choice Q&A. Figure 9 shows the full example for generating questions from figures (VQA). Figure 10 shows the full examples for TQA. 20 Preprint, Under review Table 10: TQA Performance by Question Categories (the amount of samples for each category is in brackets). Model InstructBLIP-7B InternLM-Xcomposer2.5-7B InternVL2-8B LLaVA-1.6-Mistral-7B LLaVA-OneVision-Qwen2-7B LLaVA-1.5-13B LLaVA-1.5-7B LLaVA-1.6-34B Mantis-LLama3-8B Phi3v InternLM-Xcomposer2-4KHD-7B Idefics2-8B InternVL2-2B Claude-Sonnet Qwen2-VL GPT-4o Idefics3 Data Analysis Reasoning Attribute Localization Reading Arithmetic (3934) (2582) (2127) (121) (123) (23) 24.7 54.9 58.6 30.4 47.5 31.6 30.8 46.4 28.7 52.2 45.4 35.0 36.9 85.1 64.6 57.5 52.0 27.6 75.6 73.2 39.0 72.4 49.6 39.0 69.1 39.0 76.4 69.1 57.7 57.7 90.2 86.2 79.7 79.7 34.7 79.3 78.5 52.1 80.2 47.9 42.1 76.9 46.3 77.7 77.7 57.9 70.2 91.7 90.1 86.8 77.7 43.5 60.9 65.2 30.4 60.9 30.4 43.5 47.8 21.7 60.9 60.9 43.5 30.4 87.0 65.2 69.6 56. 20.4 74.6 79.8 32.0 69.1 32.4 31.6 66.8 32.4 72.4 66.2 51.1 62.2 91.0 82.0 73.0 72.0 13.6 29.7 54.1 13.0 40.3 28.5 27.9 46.1 27.3 35.2 25.0 32.4 32.1 78.1 44.0 40.6 36.6 Figure 9: detailed example for VQA questions generation. A.3 EXPLORING MORE DETAILS ON EFFICIENT EVALUATION A.3.1 EXTRA RESULTS FOR LIVEXIV We start showing what would happen if our method for efficient evaluation is applied setting the number of re-evaluated models to be 3 or 8. As expected, Figures 11 and 12 show that overall performance is positively related to the number of re-evaluated models. We found that re-evaluating 5 models offers good trade-off. 21 Preprint, Under review Figure 10: detailed example for TQA question generation. VQA TQA MAE Rank Corr. MAE Rank Corr. 3.21.8 eess.SP q-bio.BM 4.32.4 q-bio.CB 4.62.0 3.21.1 cs.AI eess.SY 2.71.8 2.91.3 cs.CV 3.31.8 cs.RO q-bio.GN 4.11.8 2.92.0 q-bio.TO 4.13.1 cs.LG 97.3 96.3 97.1 97.3 96.1 98.2 98.3 97.5 97.1 97.1 3.62.6 3.71.5 3.32.4 3.21.8 3.52.0 3.62.1 6.12.6 4.11.4 5.32.0 4.01.6 98.3 97.8 98.3 97.8 94.9 95.8 93.6 96.6 95.1 98.8 Figure 11: Performance prediction results of our efficient re-evaluation method on the hypothetical second version of the LiveXiv benchmark when re-evaluating 3 models. VQA TQA MAE Rank Corr. MAE Rank Corr. 2.31.5 eess.SP q-bio.BM 4.12.5 q-bio.CB 1.30.8 2.31.1 cs.AI eess.SY 2.11.0 2.01.2 cs.CV 1.91.4 cs.RO q-bio.GN 2.31.0 2.31.3 q-bio.TO 3.52.2 cs.LG 98.3 96.3 99.8 98.0 98.5 99.4 99.0 98.0 99.3 98. 2.31.2 1.81.0 2.01.6 1.60.9 2.10.9 2.51.8 3.91.6 3.21.6 3.01.3 1.81.2 99.3 98.3 98.3 97.3 97.5 97.8 97.3 95.8 97.3 99.3 Figure 12: Performance prediction results of our efficient re-evaluation method on the hypothetical second version of the LiveXiv benchmark when re-evaluating 8 models. 22 Preprint, Under review Figure 13: Number of testing samples negatively correlates with prediction error, suggesting that our efficient evaluation strategy will perform even better in practical situations in which test sets are larger. The plots represent the cases for 3, 5, and 8 re-evaluated models. In Figure 13, we can see that the number of testing samples negatively correlates with prediction error, suggesting that our efficient evaluation strategy achieves lower MAE than those reported in Figure 4 in real application, given that test datasets will be larger and unaffected by data splitting. A.3.2 EFFICIENT EVALUATION ON MM-LIVEBENCH In this section, we challenge our efficient evaluation method, by examining its performance over another type of multi-modal live dataset Zhang et al. (2024b). The dataset has 3 versions (May 2024, June 2024, and July 2024), and each version has roughly 250-300 samples of open-ended questions scraped from newspapers. To evaluate our method we use GPT-4o to convert the open-ended questions into closed-form of questions where the true answer is rephrased and 3 more negative answers are proposed. Then we evaluate 13 LMMs over all the dataset versions. We use the first version as training set and we predict the performance over the new concatenated sets using our IRT-based method. Figure 14 shows that our method still performs well on different benchmark when re-evaluating only 5 models. Figure 14: The results for MM-LiveBench are optimistic and we check that our method could be successfully applied in this other context. A.4 PROMPT TEMPLATE FOR QA GENERATIONS This is figure from scientific paper with the following caption: {text_desc}. Please describe the image in as much details as possible. For all the details you are confident about include everything you see, and be as specific as possible , such as existing numbers, describing objects, attributes ... Figure 15: Prompt template for general detailed caption. Preprint, Under review Compositional reasoning defines the understanding of attributes, relations and word order significance. good vision -language model should be able to accurately answer composition reasoning questions about an image. Your task is to fool vision -language model by generating challenging compositional reasoning questions about the figure. Given the image and the description you generated: {detailed_description}, generate {n_questions} diverse and challenging compositional reasoning questions which vision-language model would incorrectly answer. For each question include the following: - compositional reasoning question - correct answer - 3 hard negative options. should differ only subtly from the correct answer but still be clearly incorrect given the image, and the question. The goal is for vision-language model to choose the negative option over the positive option when you asked to answer the question in binary Each negative option multiple choice format. in your answer and make sure there is indeed only single correct Only include questions you are confident answer and the others are false answers. as string in the format [{\"Q\":<question>, \"a\":<correct answer"
        },
        {
            "title": "Format your response",
            "content": ">, \"n1\":<negative option 1>, \"n2\":<negative option 2>, ...}]. Figure 16: Prompt template for visual question-answering. Document and table understanding defines the understanding of values, metrics and perform arithmetic operations over numerical values and commonsense reasoning . good language model should be able to accurately answer {commonsense_reasoning / arithmetic manipulation} questions from given table. Your task is to fool language model by generating challenging table {commonsense_reasoning / arithmetic manipulation } questions about the table. Given the table: {table_content} Generate {n_questions} diverse and challenging {commonsense_reasoning / arithmetic manipulation} questions on the table questions which language model would incorrectly answer .For each question include the following: - question - correct answer - 3 hard negative options. Each negative option should differ only subtly from the correct answer but still be clearly incorrect given the figure, caption and the question. The goal is for language model to choose the negative option over the positive option when you asked to answer the question in binary multiple choice format. Only include questions you are confident in your answer and make sure there is indeed only single correct answer and the others are false answers. Format your response as string in the format [{\"Q\":<question>, \"a\":<correct answer >, \"n1\":<negative option 1>, \"n2\":<negative option 2>, ...}]. Figure 17: Prompt template for table question-answering. Think step by step before answering. For the given image and question: {question} write only the words yes or no if think the option {correct_answer } is indeed the correct answer out of {options} for this question? Figure 18: Prompt template for agreement filtering. 24 Preprint, Under review You are an insightful assistant, for the question/options pair provided by the user, pick question category from the list below: Question category: - attribute: the question asks about the presence or visibility of an attribute of an object (e.g. \"What is the color of circles in plot (a)?\" \"[A. Blue, B. White, C. Green, D. Red]\") - reasoning: the question asks about understanding the figure (e.g \"What is the object inside the red box?\" \"[A. Bottle, B. Table, C. Tree, D. Nothing]\") - localization: the question asks about the presence or visibility at specific location in the image (e.g \"On which subplot does the scatter is the most spread?\" \"[A. Top-Left, B. Bottom-Right, C. Middle-Left, D. Top-Right]\") - reading: the question asks about reading some text from the figure (e.g \"What is name of the method presneted as green line?\" \"[A. GPSK, B. FDAH, C. TQWA, D.Ours]\") - arithmetic: the questions asks about mathematical arithmetic of numbers (e.g if the maximium accuracy of SIFT would be doubled? what would be the value?\" \"[A. 2, B. 4 , C. 100, D. 50]) - data analysis: the question asks about understanding of graph (e.g, \"Which values intersect at T=2?\" \"[\"A. N1, B. N2, C. N3, D. N4]\") Respond with JSON object with the following format: {\"Question category\": \"category\"} Figure 19: Prompt template for question categories analysis."
        }
    ],
    "affiliations": [
        "Department of Statistics, University of Michigan, USA",
        "Faculty of Engineering Tel-Aviv University",
        "IBM Research",
        "JKU Linz, Austria",
        "MIT-IBM",
        "TU Graz, Austria"
    ]
}