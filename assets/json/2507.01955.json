{
    "paper_title": "How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation Models on Standard Computer Vision Tasks",
    "authors": [
        "Rahul Ramachandran",
        "Ali Garjani",
        "Roman Bachmann",
        "Andrei Atanov",
        "Oğuzhan Fatih Kar",
        "Amir Zamir"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal foundation models, such as GPT-4o, have recently made remarkable progress, but it is not clear where exactly these models stand in terms of understanding vision. In this paper, we benchmark the performance of popular multimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0 Flash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision tasks (semantic segmentation, object detection, image classification, depth and surface normal prediction) using established datasets (e.g., COCO, ImageNet and its variants, etc). The main challenges to performing this are: 1) most models are trained to output text and cannot natively express versatile domains, such as segments or 3D geometry, and 2) many leading models are proprietary and accessible only at an API level, i.e., there is no weight access to adapt them. We address these challenges by translating standard vision tasks into equivalent text-promptable and API-compatible tasks via prompt chaining to create a standardized benchmarking framework. We observe that 1) the models are not close to the state-of-the-art specialist models at any task. However, 2) they are respectable generalists; this is remarkable as they are presumably trained on primarily image-text-based tasks. 3) They perform semantic tasks notably better than geometric ones. 4) While the prompt-chaining techniques affect performance, better models exhibit less sensitivity to prompt variations. 5) GPT-4o performs the best among non-reasoning models, securing the top position in 4 out of 6 tasks, 6) reasoning models, e.g. o3, show improvements in geometric tasks, and 7) a preliminary analysis of models with native image generation, like the latest GPT-4o, shows they exhibit quirks like hallucinations and spatial misalignments."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 5 5 9 1 0 . 7 0 5 2 : r How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation Models on Standard Computer Vision Tasks Rahul Ramachandran Andrei Atanov* Ali Garjani Oguzhan Fatih Kar* Roman Bachmann Amir Zamir* Swiss Federal Institute of Technology Lausanne (EPFL) https://fm-vision-evals.epfl.ch"
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Multimodal foundation models, such as GPT-4o, have recently made remarkable progress, but it is not clear where exactly these models stand in terms of understanding vision. In this paper, we benchmark the performance of popular multimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0 Flash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision tasks (semantic segmentation, object detection, image classification, depth and surface normal prediction) and using established datasets (e.g., COCO, ImageNet and its variants, etc). The main challenges to performing this are: 1) most models are trained to output text and cannot natively express versatile domains, such as segments or 3D geometry, and 2) many leading models are proprietary and accessible only at an API level, i.e., there is no weight access to adapt them. We address these challenges by translating standard vision tasks into equivalent text-promptable and API-compatible tasks via prompt chaining to create standardized benchmarking framework. We observe that 1) the models are not close to the stateof-the-art specialist models at any task. However, 2) they are respectable generalists; this is remarkable as they are presumably trained on primarily image-text-based tasks. 3) They perform semantic tasks notably better than geometric ones. 4) While the prompt-chaining techniques affect performance, better models exhibit less sensitivity to prompt variations. 5) GPT-4o performs the best among non-reasoning models, securing the top position in 4 out of 6 tasks, 6) reasoning models, e.g. o3, show improvements in geometric tasks, and 7) preliminary analysis of models with native image generation, like the latest GPT-4o, shows they tend to create semantic recreations instead of precise edits, which results in hallucinations and spatial misalignments. Equal technical advising. Figure 1. We benchmark multimodal foundation models (MFMs) on established datasets using prompt chaining. Top: The performance of the MFMs on several classical computer vision tasks. We compare MFMs with specialist models both directly and by calibrating for the chosen structure and constraints of the used prompt chain. (+chain; see Sec. 4). Bottom: GPT-4os predictions for each task. Multimodal foundation models (MFMs), such as GPT4o, Gemini 1.5 Pro and 2.0 Flash, and Claude 3.5 Sonnet [6, 44, 49], have advanced significantly in recent months, demonstrating impressive capabilities in their public releases [44]. However, while the community has extensively 1 investigated their language proficiency [12, 15, 24, 50], the extent of their vision capabilities remains underexplored. We still lack well-calibrated quantitative understanding of their performance on established vision tasks and datasets, particularly across diverse axes of vision, e.g., semantics, 3D, grouping, etc. Most of the existing vision benchmarks of MFMs primarily target text (e.g., VQA) or tasks closely tied to text, like classification [20, 47, 56, 57, 64, 70]. While they provide valuable insights, several key limitations persist. First, it is unclear how much solving these benchmarks truly depends on the visual input, and some were shown to mainly measure the language capabilities of MFMs while overlooking the vision component [56]. Second, they all require the model to output text, making it hard to compare the vision capabilities of MFMs on vision-only tasks and against vision specialist models. Third, they do not shed light on other aspects of visual understanding, such as 3D geometry, grouping, or segmentation, that are less text-oriented. We address these limitations by evaluating MFMs on well-established vision tasks and datasets developed by the community. Specifically, we test GPT-4o, o4-mini, Claude 3.5 Sonnet, Gemini 2.0 Flash, Gemini 1.5 Pro, Qwen2-VL, and Llama 3.2 on classification, object detection, semantic segmentation, grouping, depth prediction, and surface normal prediction using COCO [39], Hypersim [51], as well as ImageNet [52] and its variants [23, 25, 33, 48, 59]. Most of these tasks, however, require dense pixel-wise predictions that are not readily compatible with the default text output of most MFMs. Furthermore, direct prompting usually leads to varying and often weak performance across tasks, hence it may not represent the actual visual understanding capabilities of MFMs (see Sec. 4.2 and App. E). To address these challenges, we split each task into multiple sub-tasks, each of which can be solved in textual form via prompting (see Sec. 3). This results in promptchaining framework that can be applied to any MFM with text interface (e.g., ChatBot APIs) to solve standard vision tasks. Specifically, our proposed approach allows MFMs to 1) detect bounding boxes, 2) generate complete segmentation masks for complex scenes, 3) extract semantic entities from images similar to SAM [36], 4) estimate dense depth and surface normal maps. Please see Fig. 1 for an overview. We emphasize that this prompt-chaining framework is not proposed as an alternative methodology for solving vision tasks, nor do we suggest that MFMs should adopt such approaches in practice. Instead, our framework serves specifically as standardized method to measure and benchmark the vision capabilities of any MFM that can input images and output text. Crucially, this enables quantifiable and holistic understanding of MFMs vision capabilities on established benchmarks and direct comparison with vision-only models. We find that the current generation of MFMs achieves good performance in most cases and are respectable as generalists, with GPT-4o scoring the best in 4 out of 6 tasks. However, they still lag behind task-specific state-of-the-art vision models in all tasks. In particular, we find that the MFMs perform geometric tasks significantly worse than semantic ones. Furthermore, we perform detailed prompt sensitivity analysis for each task and find that the performance varies for different prompts, though better models exhibit less sensitivity. To enable further research in this direction and enable the community to benchmark future MFMs on vision tasks, we will open-source our evaluation and prompt chaining tool set upon acceptance. 2. Related Work Advances in MFMs. There has been remarkable progress in MFMs [2, 5, 6, 8, 9, 17, 37, 40, 43, 44, 46, 49, 54, 55, 60, 61] (see [69, 71] for surveys), leading to strong performance across wide range of tasks that require joint vision and linguistic capabilities such as captioning, visual question answering, and instruction following. Despite the progress, it is unclear how well these models perform tasks that require dense visual understanding, which is our main focus. Benchmarking vision capabilities of MFMs. Many works investigate the vision capabilities of MFMs by developing VQA-style benchmarks that combine visual and textual inputs to generate textual outputs [4, 20, 30, 38, 40, 47, 56, 57, 70]. While these approaches offer valuable insights, they are incompatible with vision-only models, making direct comparisons difficult. In contrast, we evaluate MFMs on standard vision tasks, enabling direct comparison with strong vision specialists to track MFMs progress. [56] evaluates MFMs on vision datasets [10, 39, 72] by repurposing dataset annotations into text format. We differ by translating MFM outputs into the annotation format instead, e.g., segmentation maps. To the best of our knowledge, this is the first approach that enables an apples-to-apples comparison with vision specialist models using standard task-specific metrics and qualitative analyses in the tasks native output space. Prompting MFMs. Various prompting techniques have been developed for MFMs [34, 62, 68, 73]. We follow similar strategy and decompose complex vision tasks into simpler sub-tasks that MFMs can handle. Several works developed prompting techniques to unlock the vision capabilities of MFMs [26, 6466]. related work is DetToolChain [65], which develops prompting mechanism for object detection; however, at the time of writing, its approach is not reproducible. We differ by 1) focusing on wider range of tasks including semantic and geometric ones 2) for several MFMs including closedand open-weight ones 3) with simpler yet effective and cost-efficient prompt chaining mechanism. 3. Prompt Chaining for Solving Vision Tasks In this section, we describe the developed prompt chaining techniques that enable MFMs to solve standard computer vision tasks. These techniques are based on the main idea of breaking the original task into multiple simpler sub-tasks that can be solved in language format, e.g., identifying whether an object is present in patch of an image. We then solve each sub-task by prompting the MFM. To guide the choice of how to split each task into sub-tasks, we rely on our early key observation that most MFMs are relatively strong at image classification (see, e.g., Tab. 1) and, therefore, split each task into multiple classification sub-tasks. We provide the pseudo-code for each technique in App. D. Image classification. Here, the MFM is presented with list of all possible classes and tasked with assigning the image to the correct class. Following [30], we group multiple images into single sequence and classify them jointly for efficiency, as we observed no significant decrease in accuracy. Object detection. The goal is to predict bounding box coordinates that tightly localize the objects in the image. Similar to [67], our initial attempts showed that many MFMs fail at predicting the coordinates directly. Thus, we develop prompt chaining method and divide the original task into two stages. The first stage has single sub-task to identify all present objects in the image, similar to classification. In the second stage, for each object, we regress its coordinates via recursive zooming (see Fig. 2). Specifically, we divide the image into grid cells and ask the model to identify whether (a part of) the object is present in each cell. We then discard cells without objects, reducing the search space. We apply this process recursively, progressively eliminating irrelevant regions of the image until only the object of interest remains present in the image. We use two grid resolutions: coarse grid for quick downsampling and finer grid for precise edge refinement, allowing us to reduce the number of steps. Semantic segmentation. The goal is to assign single semantic class to each pixel in an image. Instead of perpixel querying that is prohibitively expensive, we split the image into pixel groups using an unsupervised superpixel clustering algorithm [1] and assign single label per group to decrease the number of API calls (or forward passes). Using superpixels is common approach to segment an image into smaller, homogeneous regions based on lowlevel image features, such as color or texture [53]. In all our comparisons, we account for potential biases introduced by superpixelation (and other approximations in prompting) by introducing calibration control baselines (see Sec. 4.) After dividing the image into superpixels, we classify them in batches to decrease the overall cost, as in the classification task. Similar to the object detection algorithm, this approach utilizes the strength of MFMs as strong image classifiers. To maintain consistency across different batches of superpixels, we include predictions for the previously obtained batches as part of the chain, which we found to improve the models performance. In our early experiments, we found that simply outlining individual superpixels on an input image leads to poor performance. This aligns with other works [20, 64] that found that MFMs have blurry vision and struggle with fine-grained details and localization. To address this, we provide the MFM with the crops of each superpixel at multiple scales, which we found to improve the performance significantly. Please see Fig. 3 for an overview. Grouping. Given an image and query (or anchor) point on it, the goal is to identify other pixels that belong to the same object or background. Unlike semantic segmentation, there is no fixed, pre-defined set of classes, which makes this task more challenging. To tackle this task, as before, we use superpixels and the MFMs capability to determine visual similarity [20]. We construct graph where each superpixel is node, and edges connect neighboring superpixels. We then identify the superpixel containing the query point and explore adjacent superpixels. The model decides whether each adjacent superpixel belongs to the same object as the initial superpixel. The selected superpixels are then merged with the initial one to form the next input cluster. This process continues until no more superpixels are added. Please see Fig. 4 for an overview. Depth prediction. As predicting 3D from single 2D image is inherently ambiguous, we perform relative depth prediction by querying the model to rank different parts of the image according to their distance from the camera. As per-pixel querying is infeasible, we adopt region-wise comparison strategy inspired from [75]. To identify suitable regions for comparison, we first segment the image into superpixels. We then randomly sample pairs of superpixels and query the MFM to rank these pairs based on relative depth (see Fig. 5). These pairwise rankings are then globalized by minimizing the objective function from [75], which encourages assigning larger values to superpixels ranked deeper than those ranked shallower in the pairwise comparisons (see App. D.3). We then use the values found by the optimization method to rank all superpixels. For simplicity, we assume that all pixels within superpixel share the same depth rank, allowing us to extend the superpixel-level depth predictions to pixelwise ranking across the entire image (control baselines are included in evaluations). Surface normal prediction. We follow ranking approach similar to that used for depth. We use standard basis vectors relative to the camera (right, up, and forward) as reference directions, and for each randomly sampled pair of superpixels, we query the MFM to determine their relative alignment with each basis vector (see Fig. 13 in the supplementary). After we obtain the pairwise comparisons for each direction, we globalize them using the same algorithm used for depth [75]. This results in three distinct surface normal maps, one for Figure 2. Object detection algorithm. At each step, we divide the image into 3 3 grid of crops and query each for the presence of the target object (Sheep in the figure) through the model. Grid cells without the object are discarded, and the process is repeated until the object is fully located. Further details are provided in App. D.1. *Summary of the actual prompt; see the full prompt in the supplementary. Figure 3. Semantic segmentation algorithm. We divide the image into superpixels and create multi-scale pyramids of superpixels. The pyramids are then classified using the MFM in sequential manner to produce the complete segmentation map. multi-scale pyramid consists of 3 layers: crop of the superpixel, some context surrounding the crop, and the full image. In practice, we batch multiple superpixels into sequences and classify them jointly. *Summary of the actual prompt; see the full prompt in the supplementary. each basis direction. Similar to depth, we assume uniformity within superpixels and assign the same rank to all pixels within each superpixel group (control baselines are included in the evaluations). 3.1. Accounting for algorithmic constraints Using superpixels and the zooming algorithm can impose constraints on MFMs performance. We address this in two ways. First, we introduce targeted control baselines in all our experiments (see Sec. 4) to ensure fair and calibrated comparisons with other vision models. Second, in App. E.8, we demonstrate that the proposed prompting chains do not inherently pose hard upper bound on the performance and that employing more fine-grained prompt chains consistently improves results. 4. Experiments We present the experimental results for different tasks and MFMs. First, we describe our setting, including the choice of the datasets and models. Then, we discuss our main results. We provide qualitative examples for all tasks in Fig. 6. Finally, we provide further analysis and ablations in Sec. 4.2. Please see the supplementary for additional results. Evaluated MFMs. We evaluate several closed-weight MFMs, namely GPT-4o [44], Gemini 2.0 Flash [22], Gemini 1.5 Pro [49], and Claude 3.5 Sonnet [6] by querying them via their APIs. We also include Qwen2-VL-72B [61] and Llama 3.2 90B [3, 41] as recent open-weight models that yield competitive results with GPT-4o and Claude 3.5 on several benchmarks. In addition, we evaluate recent multimodal 4 Figure 4. Grouping algorithm. Given an image and query point, we first divide the image into superpixels and select the superpixel the query point falls into. At each step, the model is asked to identify the adjacent superpixels that belong to the same object as the one covered by the cluster. The selected superpixels are then merged with the cluster to form the next steps input cluster. *Summary of the actual prompt; see the full prompt in the supplementary. Figure 5. Depth prediction algorithm. We randomly select pairs of superpixels. Each pair is given to the model to perform pairwise depth comparison. The resulting pairwise ranks are then globalized by minimizing an objective function to generate relative depth map, which can then be scaled to obtain classical evaluation metrics. We perform surface normal estimation in similar manner, see Fig. 13 in the supplementary. *Summary of the actual prompt; see the full prompt in the supplementary. reasoning models such as o1, o3, and o4-mini [43, 46]. o1 and o3 are evaluated on smaller representative subsets due to cost constraints, while o4-mini is evaluated across the full benchmark. For each model and task, we choose the best prompt out of several candidates based on small validation set and use it to obtain the final results on the test set. Datasets. We use the following common vision datasets for evaluations: Image classification. We use standard benchmarks including ImageNet [52] and ImageNet-v2 [48]. To test robustness, we include ImageNet-R [25], ImageNet-S [59], and two corruption benchmarks from RobustBench [16], specifically, ImageNet-C [23] and ImageNet-3DCC [33]. Object detection. We use the COCO [39] validation set and choose images containing only single instance of each present class, resulting in 1.7K examples. Semantic segmentation & grouping. We use random subset of 500 COCO [39] validation images for semantic segmentation for cost-efficiency. For grouping, we filter 100 images from the COCO validation set by measuring the consistency of SAM [35] predictions between different query points within every instance. More details are provided in the supplementary. Depth & surface normal prediction. We use Hypersim [51] and randomly subsample 100 validation images from it for cost-efficiency. 5 Figure 6. Qualitative results. Visual comparisons showing the performance of MFMs across each task. We find that all models perform relatively better on semantic tasks than geometric ones. For surface normal visualizations, we combine the per-axis normalized predictions and project them onto the unit sphere. Please see the supplementary for additional qualitative results. Baselines. We include the following control baselines to contextualize the performance of MFMs and account for the impact of superpixelation and other design choices: Vision Specialist. We report the performance of leading vision models for each task. We specify each model used in the corresponding task sections. In addition to the state-ofthe-art models, we benchmark 4M-21 [7, 42] as zero-shot vision baseline for an unbiased comparison. Although 4M21 is an MFM, we treat it as vision specialist because it is specifically trained for solving these tasks. Overall, these baselines indicate the current state of the (specialized) computer vision models. Vision Specialist + Chain. This control baseline applies the same algorithmic constraints to the vision specialist as those experienced by MFMs, such as superpixels and recursive zooming. This control baseline provides fair and calibrated comparison between vision specialists and MFMs, ensuring that our benchmark remains accurate in relative and ordinal sense. Oracle + Chain. This baseline shows the performance of the prompt chain if the MFM gave the ground-truth answer at each sub-task. It isolates the performance of MFMs from the limitations of the chosen granularity level for the prompt chaining algorithm. Note that this is not hard upper bound, and we can alleviate these limitations by using more fine-grained chains (see App. E.8.) Blind Guess. We prompt the model with blank image, revealing potential biases and assessing whether it genuinely utilizes the image content for its predictions. 4.1. Evaluation results Image classification. The classification results across all datasets are summarized in Tab. 1. We use Model Soups ViT-G [63] as the vision specialist, and we also include OpenCLIP [14] to assess zero-shot capabilities. Although MFMs do not reach the performance levels of vision specialists, they demonstrate strong results across the benchmarks and show resilience to image corruptions and natural distribution shifts. Notably, GPT-4o and Gemini 2.0 Flash stand out with particularly strong performance, followed by Gemini 1.5 Pro, Claude 3.5 Sonnet, o4-mini, Qwen2-VL, and Llama 3.2. We also observe that o4-mini is especially sensitive to the batch size used during inference, with performance improving at smaller batch sizes (see ablation in the supplementary). Model ImageNet ImageNet-V2 Corruptions Domain Shift 2DCC 3DCC ImageNet-R ImageNet Sketch Model Soups ViT-G OpenCLIP GPT-4o o4-mini Gemini 2.0 Flash Gemini 1.5 Pro Claude 3.5 Sonnet Qwen2-VL Llama 3.2 90.94 84.37 77.20 55.90 74.78 73.88 62.85 55.54 49.15 84.22 78. 71.57 46.99 75.79 69.76 54.45 49.39 48.21 - 66.96 62.46 37.22 55.67 56.14 40.76 38.92 34.45 - 65.95 61.13 36.68 56.92 56.22 41.41 36.45 34.37 95.46 93. 84.38 56.05 82.05 71.42 70.36 66.31 65.05 74.23 73.24 67.30 45.18 69.43 57.15 57.42 51.18 47.11 Table 1. Image classification. We compare the top-1 acc. () of the MFMs with vision specialists, Model Soups [63] and OpenCLIP [14]. Although their performance falls short of the top specialist models, MFMs, particularly GPT-4o, demonstrate competitive results across broad range of benchmarks. Object detection. The results are summarized in Tab. 2. We use DETR [11] and Co-DETR [74], state-of-the-art COCO model, as the vision specialists, and 4M-21 as zero-shot baseline. We observe that all MFMs lag behind the vision models, with GPT-4o achieving the highest performance, significantly outperforming other MFMs. For the Specialist + Chain baselines, we apply the same recursive algorithm, but at each stage we only keep the grid cells that intersect with the original bounding boxes predicted by the specialist. As mentioned earlier, this calibration confirms that the gap between MFMs and specialists remains significant. Finally, we assess the performance of the Oracle + Chain baseline. We evaluated two variants: one using GPT-4os class predictions, and another using the ground-truth class 6 labels. The first baseline examines the outcome if GPT-4o correctly selects the grid cells at each step of the chain, while the second assumes both correct class predictions and accurate grid cell selection. These provide the upper bounds for both the grid search component and the overall pipeline when using specific grid resolution to calibrate the performance. Baselines Model AP50 () AP75 () AP () Vision Specialists MFMs Control Co-DETR Co-DETR + Chain DETR DETR + Chain 4M-21 4M-21 + Chain GPT-4o o4-mini Gemini 2.0 Flash Gemini 1.5 Pro Claude 3.5 Sonnet Qwen2-VL Llama 3.2 91.30 90.06 73.31 72.33 59.54 55.46 60.62 42.90 44.17 39.75 31.69 35.62 31. 86.17 52.78 63.61 38.36 51.57 30.48 31.97 22.18 15.83 15.27 12.13 12.82 8.40 80.23 51.54 58.67 39.36 47.71 30.74 31.87 22.60 19.85 18.11 14.78 15.27 12.83 Oracle + Chain (pred. class) Oracle + Chain (full) Blind guess 75.44 92.18 <0. 41.31 49.33 <0.01 41.56 50.14 <0.01 Table 2. Object Detection: We compare the average precision of MFMs against vision specialists, DETR [11] and Co-DETR [74]. We also employ 4M-21 [7] as zero-shot baseline. Like the classification task, vision specialists significantly outperform MFMs, yet they perform reasonably well, with GPT-4o achieving the best performance among MFMs. Semantic segmentation. Table 3 and Fig. 6 show that MFMs achieve nontrivial performance, but still lag behind the vision specialist, i.e., OneFormer [29]. Similar to object detection, we include the baseline of constraining the performance of the vision specialist using the chain algorithm: we assign the majority class prediction to each superpixel and flood-fill the entire superpixel with that class. Baselines Model mIoU () Pixel acc. () Vision Specialists MFMs Baselines OneFormer OneFormer + Chain 4M-21 4M-21 + Chain GPT-4o o4-mini Gemini 2.0 Flash Gemini 1.5 Pro Claude 3.5 Sonnet Qwen2-VL Llama 3.2 Oracle + Chain Blind guess 65.52 60.64 54.31 52.72 44.89 39.19 43.04 40.46 32.05 33.59 36.63 83.41 0.03 83.26 81.69 79.66 78. 68.60 64.26 66.15 64.88 58.41 56.36 59.95 94.68 0.29 Table 3. Semantic Segmentation. We compare the mIoU and pixel acc. of MFMs against OneFormer [29] and 4M-21. The results show that the MFMs have nontrivial segmentation capabilities, with GPT-4o performing the best, as shown qualitatively in Fig. 6. Grouping. Table 4 shows that MFMs have varying performance on this task, and GPT-4o performs the best and achieves good overall performance, as can also be seen in Fig. 6. All MFMs still lag behind the vision specialist SAM [35]. SAM SAM + Chain GPT-4o o4-mini Gemini 2.0 Flash Gemini 1.5 Pro Claude 3.5 Sonnet Qwen2-VL Llama 3. Oracle + Chain mIoU () 80.12 72.32 59.06 46. 55.25 44.13 41.68 21.64 25.69 81. Table 4. Grouping. We compare the MFMs against SAM [36] for the grouping task. GPT-4o demonstrates particularly strong performance among MFMs, followed by Gemini 2.0 Flash. Depth prediction. The results are summarized in Tab. 5. Alongside standard metrics, we also report 1) the Spearman correlation coefficient (ρ), which serves as relative metric by measuring the correlation between the groundtruth depth ranking of the pixels and the predicted ranking and 2) accuracy, which reflects the percentage of correct pairwise depth comparisons. While MFMs demonstrate nontrivial performance and outperform the blind guess, there remains significant gap compared to the vision specialist, Omnidata [18, 32], which is more pronounced compared to the semantic tasks. Notably, o4-mini achieves the strongest performance among MFMs on depth, despite trailing behind some models in semantic tasks. Other reasoning-focused models like o1 and o3 also show promising results on the subset they were evaluated, indicating growing capabilities in 3D understanding. Similar to previous tasks, we analyze the performance using the Oracle + Chain baseline, which assumes 100% accuracy in all pairwise comparisons, and the Omnidata + Chain baseline, which uses depth predictions from Omnidata to perform these pairwise comparisons. Due to the coarse granularity of the evaluation, the two baselines closely match each other. Importantly, since MFMs perform notably worse than the baselines, they are not bounded by the coarseness, and, therefore, our conclusions hold. Baselines Method Higher is better Lower is better δ1 δ2 δ3 ρ Accuracy AbsRel Vision Specialists MFMs Control Omnidata Omnidata + Chain 4M-21 4M-21 + Chain GPT-4o o4-mini Gemini 2.0 Flash Gemini 1.5 Pro Claude 3.5 Sonnet Qwen2-VL Llama 3. Oracle + Chain Blind Guess 0.768 0.568 0.636 0.565 0.461 0.467 0.461 0.458 0.428 0.432 0.458 0.571 0.375 0.867 0.772 0.814 0.774 0.716 0.718 0.715 0.709 0.693 0.698 0. 0.774 0.628 0.911 0.864 0.888 0.865 0.840 0.841 0.839 0.835 0.830 0.831 0.835 0.863 0.773 0.95 0.81 0.89 0.81 0.54 0.58 0.59 0.51 0.49 0.44 0. 0.83 0.25 - 93.74 - 88.25 70.59 74.08 71.11 66.78 68.09 65.88 67.51 100.00 54.24 0.375 0.528 0.406 0.529 0.618 0.595 0.615 0.628 0.654 0.638 0. 0.528 0.758 Table 5. Depth prediction. MFMs can coarsely estimate depth, but their gap to vision specialists is larger than in semantic tasks. All MFMs perform similarly, with GPT-4o and o4-mini slightly ahead. Surface normal prediction. To assess performance, we employ Spearmans rank correlation coefficient, ρi, measuring the correlation between ground truth and predicted pixel 7 alignments along each basis direction i. Alignment for pixel is measured as the dot product of the surface normal with the direction i. Tab. 6 demonstrates that most MFMs struggle with the task: all models fail to achieve positive correlation along the left-right direction, with Gemini performing below random chance for two directional components, revealing consistent bias in its understanding of these directions. Notably, o4mini outperforms all other MFMs across the surface normal prediction metrics, indicating stronger geometric reasoning. This trend extends to other recent reasoning models, with o1 and o3 also showing strong performance on the evaluated subset. Similarly, we show in the supplementary that other MFMs not directly trained for reasoning improve their performance on the up-down ambiguity resolution with CoT prompting [62]. Similar to depth, the results suggest that while MFMs have limited 3D visual understanding, newer reasoning models like o1, o3, and o4-mini exhibit promising progress."
        },
        {
            "title": "Control",
            "content": "Omnidata Omnidata + Chain 4M-21 4M-21 + Chain GPT-4o o4-mini Gemini 2.0 Flash Gemini 1.5 Pro Claude 3.5 Sonnet Qwen2-VL Llama 3.2 Oracle + Chain Blind guess ρx 0.80 0.64 0.71 0.65 -0.14 0.22 -0.39 -0.17 -0.19 0.09 0. 0.64 -0.48 ρy 0.83 0.70 0.74 0.70 0.57 0.61 -0.04 -0.57 0.61 -0.07 -0.42 0.70 -0.61 ρz 0.78 0.58 0.65 0.56 0.40 0.46 0.02 0.04 0.40 0.02 0.22 0.60 0.11 Table 6. Surface normal prediction. MFMs underperform on this task, especially on the x-axis. o4-mini is strongest among them, while Gemini performs poorly, often near chance. Reasoning models. As discussed earlier, we evaluate o1 and o3 on smaller representative subset of images (see supplementary for details). For comparison, we also include GPT-4o as baseline, along with evaluations of o4-mini at varying levels of reasoning effort (low, medium, and high). The results are presented in Fig. 7. o1 and o3 slightly outperform GPT-4o on most semantic tasks, and significantly outperform it on geometric tasks. The comparison between GPT-4o and o4-mini on this subset remains consistent with earlier observations: GPT-4o excels in semantic tasks, while o4-mini performs better on geometric tasks. We find no clear trend with the reasoning effort; medium and high reasoning improve results over low, but not always. We refer the reader to the supplementary for further details and experiments. Figure 7. Evaluation of reasoning models. The performance of o1, o3, and o4-mini (at varying levels of reasoning effort) is compared against GPT-4o on representative subset of images. The reasoning models exhibit particularly strong performance in geometric tasks. GPT-4o with image generation capability. Recent MFMs, such as the updated GPT-4o [45], can generate dense image outputs rather than being restricted solely to output text. This capability represents promising advancement for comprehensively evaluating diverse vision tasks. However, these new image generation features exhibit several limitations, some of which we illustrate in Fig. 8. Specifically, we observe that generated outputs frequently constitute semantic recreations rather than per-pixel edits, which results in various hallucinations and spatial misalignments. This presents challenges in directly applying this model to vision tasks, which we leave to future work to address. Additionally, even outputs with fewer hallucinations show several inaccuracies. We provide further qualitative examples and preliminary quantitative analyses in the supplementary material. 4.2. Analysis Prompt chaining vs direct prompting. We analyze the impact of using prompt chains versus directly asking the models to solve tasks in single prompt in Tab. 7. Specifically, for bounding box regression, we directly query GPT-4o for coordinates, while for semantic segmentation, we mark image regions and request corresponding semantic labels. The results indicate clear performance boost from using the proposed prompt chains. Please see App. for detailed discussion, qualitative visuals, and ablations. Prompt sensitivity. The choice of prompt can greatly influence the performance of model, and we have made reasonable efforts to optimize prompts for each model. We evaluate the MFMs across various prompts to assess their sensitivity 8 Figure 8. Failure cases of GPT-4o with image generation capability. Despite the models promising capabilities, limitations remain. Here, we highlight some typical failure modes: hallucinations (marked in dotted blue) and inaccurate predictions (marked in dotted green). to word choice and prompt structure. We then select the most effective prompt on small validation set for the final results presented in Sec. 4. comprehensive analysis is provided in App. H, showing that there is some variation in performance with different prompts, and the performance is generally less prompt-dependent for better-performing models, e.g., GPT-4o. Task Direct prompting Prompt Chaining (Ours) Segmentation (mIoU )* Object Detection (AP50 ) 25.79 17.69 41.67 60. Table 7. Prompt chaining ablation. We compare the performance of the prompt chaining algorithm with direct prompting techniques on GPT-4o for semantic segmentation and object detection. As demonstrated, the models performance on both tasks was significantly enhanced by prompt chaining. * Segmentation is performed on subset of 100 images. In-the-wild evaluations. Previously, we used standard vision datasets like ImageNet and COCO in our evaluations. Given the opaqueness of the training data used in training the MFMs, one cannot be confident that those images were not used in training. This, so called data contamination, problem is broad concern for the community regarding most MFMs [28]. To assess to what extent the MFMs generalize to entirely novel data and ensure our evaluations are not distorted by potential data contamination, we curated collection of images released online after the specific model APIs were launched [19, 58], which the MFMs could not have encountered before their knowledge cutoff date. The results are provided in App. E.9 and show good generalization performance to the in-the-wild samples. Cost analysis. key consideration is the cost associated with prompting the MFMs; therefore, we provide detailed prompting costs for the prompt chains in App. I. 5. Limitations and Conclusions We present benchmark to investigate the vision capabilities of MFMs by translating standard computer vision tasks into an API-compatible format that can be solved via prompt chaining. Our results show that current MFMs have relatively stronger performance in semantic tasks compared to geometric tasks, and GPT-4o is generally the bestperforming model, followed by Gemini 2.0 Flash and 1.5 Pro, o4-mini, Claude 3.5 Sonnet, Llama 3.2, and Qwen2-VL-72B. Despite the impressive recent advancement in their capabilities, this current generation of MFMs lags significantly behind vision specialists for all tasks, even when those specialists are evaluated under the same prompt chaining constraints. This suggests plenty of room for improvement in model development. Notably, recent reasoning models such as o1, o3, and o4-mini show promising performance on geometric tasks, indicating growing capabilities in 3D understanding that complement their already strong semantic performance. Below we discuss some limitations of our work and some future directions. Inference cost. Evaluating an MFM on classical vision tasks can be computationally expensive per sample compared to other benchmark types like VQA or captioning, due to the multiple API calls involved. While we acknowledge this as shortcoming, we emphasize that the framework is designed as one-time benchmarking tool for assessing visual capabilities, rather than efficient querying for downstream applications. While efficiency is important, it is orthogonal to our objective and promising direction for future work. Optimality of the proposed prompt chains. Research into advanced prompting techniques has the potential to further enhance the performance of MFMs on classical vision tasks, beyond what is shown in this paper. Nonetheless, while the final design appears simple, our proposed prompt chains are carefully designed, emerge from vast search space, and consistently improve upon direct prompting (Sec. 4.2 and App. E). It is important to emphasize that we are not proposing this approach as production-ready method for solving these vision tasks with MFMs. Indeed, we expect that future MFMs will likely close the gap to specialist vision models by training directly on these tasks. Our prompt-chaining framework, however, provides benchmark for evaluating broader class of MFMs that require only image input and text output capabilities. Through that, we establish the first benchmark for comparing diverse range of MFMs, both against each other and against specialist vision models."
        },
        {
            "title": "References",
            "content": "[1] Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine Süsstrunk. Slic superpixels compared to state-of-the-art superpixel methods. IEEE transactions on pattern analysis and machine intelligence, 34(11): 22742282, 2012. 3 [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 2 [3] Meta AI. The llama 3 herd of models, 2024. 4 [4] Haider Al-Tahan, Quentin Garrido, Randall Balestriero, Diane Bouchacourt, Caner Hazirbas, and Mark Ibrahim. Unibench: Visual reasoning requires rethinking visionlanguage beyond scaling. arXiv preprint arXiv:2408.04810, 2024. 2 [5] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:2371623736, 2022. 2 [6] Anthropic. https : Introducing claude 3.5 sonnet. / / www . anthropic . com / news / claude - 3 - 5 - sonnet, 2024. Accessed: 2024-09-23. 1, 2, 4 [7] Roman Bachmann, Oguzhan Fatih Kar, David Mizrahi, Ali Garjani, Mingfei Gao, David Griffiths, Jiaming Hu, Afshin Dehghan, and Amir Zamir. 4m-21: An any-to-any vision arXiv preprint model for tens of tasks and modalities. arXiv:2406.09406, 2024. 6, 7, 36, [8] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. 2 [9] Lucas Beyer, Andreas Steiner, André Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, et al. Paligemma: versatile 3b vlm for transfer. arXiv preprint arXiv:2407.07726, 2024. 2 [10] Garrick Brazil, Abhinav Kumar, Julian Straub, Nikhila Ravi, Justin Johnson, and Georgia Gkioxari. Omni3d: large benchmark and model for 3d object detection in the wild. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1315413164, 2023. 2 [11] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-toend object detection with transformers. In Computer Vision ECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part 16, pages 213229. Springer, 2020. 6, 7 [12] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. 2 [13] Wei-Lin Chen, Cheng-Kuang Wu, Yun-Nung Chen, and HsinHsi Chen. Self-icl: Zero-shot in-context learning with selfgenerated demonstrations, 2023. [14] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 28182829, 2023. 6 [15] Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph Gonzalez, et al. Chatbot arena: An open platform for evaluating llms by human preference. arXiv preprint arXiv:2403.04132, 2024. 2 [16] Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein. Robustbench: standardized adversarial robustness benchmark. arXiv preprint arXiv:2010.09670, 2020. 5 [17] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. InstructBLIP: Towards general-purpose vision-language models with instruction tuning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. 2 [18] Ainaz Eftekhar, Alexander Sax, Roman Bachmann, Jitendra Malik, and Amir Roshan Zamir. Omnidata: scalable pipeline for making multi-task mid-level vision datasets from 3d scans. 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 1076610776, 2021. 7, 38 [19] Flickr. Find your inspiration. https://www.flickr. com/, 2024. Accessed: 2024-09-23. 9, 28, 36, 37, 38 [20] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. arXiv preprint arXiv:2404.12390, 2024. 2, 3 [21] Google. Explore vision capabilities with the gemini api. https://ai.google.dev/geminiapi/docs/ vision?lang=python, 2024. Accessed: 2024-09-23. 22 [22] Google DeepMind. https : / / deepmind . google / technologies / gemini / flash/, 2024. 4 Gemini 2.0 flash. [23] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. arXiv preprint arXiv:1903.12261, 2019. 2, 5 [24] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. 2 [25] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF international conference on computer vision, pages 83408349, 2021. 2, 5 [26] Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah Smith, and Ranjay Krishna. Visual 10 sketchpad: Sketching as visual chain of thought for multimodal language models. arXiv preprint arXiv:2406.09403, 2024. 2 [27] Huggingface. https : / / Llama multiple images. huggingface.co/meta-llama/Llama-3.2-11BVision-Instruct/discussions/43. 26 [28] Alon Jacovi, Avi Caciularu, Omer Goldman, and Yoav Goldberg. Stop uploading test data in plain text: Practical strategies for mitigating data contamination by evaluation benchmarks. arXiv preprint arXiv:2305.10160, 2023. 9 [29] Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita Orlov, and Humphrey Shi. Oneformer: One transformer to rule universal image segmentation, 2022. 7 [30] Yixing Jiang, Jeremy Irvin, Ji Hun Wang, Muhammad Ahmed Chaudhry, Jonathan Chen, and Andrew Ng. Many-shot in-context learning in multimodal foundation models. arXiv preprint arXiv:2405.09798, 2024. 2, [31] Yixing Jiang, Jeremy Irvin, Ji Hun Wang, Muhammad Ahmed Chaudhry, Jonathan H. Chen, and Andrew Y. Ng. Many-shot in-context learning in multimodal foundation models, 2024. 28 [32] Oguzhan Fatih Kar, Teresa Yeo, Andrei Atanov, and Amir Zamir. 3d common corruptions and data augmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1896318974, 2022. 7 [33] Oguzhan Fatih Kar, Teresa Yeo, and Amir Zamir. 3d common corruptions for object recognition. In ICML 2022 Shift Happens Workshop, 2022. 2, 5 [34] Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. Decomposed prompting: modular approach for solving complex tasks. arXiv preprint arXiv:2210.02406, 2022. 2 [35] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and Ross B. Girshick. Segment anything. ArXiv, abs/2304.02643, 2023. 5, 7 [36] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023. 2, 7 [37] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. [38] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Evaluating object hallucinaarXiv preprint Zhao, and Ji-Rong Wen. tion in large vision-language models. arXiv:2305.10355, 2023. 2 [39] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. Microsoft COCO: Common objects in context. In European Conference on Computer Vision, 2014. 2, 5 [40] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 2 [41] Meta. Llama 90b. https://www.llama.com/docs/ model - cards - and - prompt - formats / llama3 _ 2/. 4 [42] David Mizrahi, Roman Bachmann, Oguzhan Fatih Kar, Teresa Yeo, Mingfei Gao, Afshin Dehghan, and Amir Zamir. 4M: In Advances in Massively multimodal masked modeling. Neural Information Processing Systems, 2023. 6 [43] OpenAI. Introducing openai o1. https://openai.com/ o1/, 2024. 2, 5 [44] OpenAI. Hello gpt-4o. https://openai.com/index/ hello-gpt-4o/, 2024. Accessed: 2024-09-23. 1, 2, 4 [45] OpenAI. Introducing 4o image generation, 2025. 8 [46] OpenAI. Introducing openai o3 and o4-mini. https:// openai.com/index/introducingo3ando4mini/, 2025. 2, 5 [47] Pooyan Rahmanzadehgervi, Logan Bolton, Mohammad Reza Taesiri, and Anh Totti Nguyen. Vision language models are blind. arXiv preprint arXiv:2407.06581, 2024. 2 [48] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In International conference on machine learning, pages 53895400. PMLR, 2019. 2, [49] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 1, 2, 4 [50] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023. 2 [51] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb, and Joshua Susskind. Hypersim: photorealistic synthetic dataset for holistic indoor scene understanding. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1091210922, 2021. 2, 5 [52] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet large scale visual recognition challenge. International Journal of Computer Vision, 115:211 252, 2014. 2, 5 [53] David Stutz, Alexander Hermans, and Bastian Leibe. Superpixels: An evaluation of the state-of-the-art. Computer Vision and Image Understanding, 166:127, 2018. 3 [54] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. 2 [55] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [56] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng 11 [70] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. 2 [71] Duzhen Zhang, Yahan Yu, Chenxing Li, Jiahua Dong, Dan Su, Chenhui Chu, and Dong Yu. Mm-llms: Recent advances in multimodal large language models. arXiv preprint arXiv:2401.13601, 2024. 2 [72] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ADE20K dataset. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 51225130, 2017. 2 [73] Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022. 2 [74] Zhuofan Zong, Guanglu Song, and Yu Liu. Detrs with collaborative hybrid assignments training, 2023. 6, 7 [75] Daniel Zoran, Phillip Isola, Dilip Krishnan, and William Freeman. Learning ordinal relationships for mid-level vision. In Proceedings of the IEEE international conference on computer vision, pages 388396, 2015. 3, 19 Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024. 2 [57] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. arXiv preprint arXiv:2401.06209, 2024. 2 [58] Unsplash. The internets source for visuals. https:// unsplash.com/, 2024. Accessed: 2024-09-23. 9, 28, 36, 37, 38 [59] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric Xing. Learning robust global representations by penalizing local predictive power. Advances in Neural Information Processing Systems, 32, 2019. 2, [60] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. Git: generative image-to-text transformer for vision and language. arXiv preprint arXiv:2205.14100, 2022. 2 [61] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 2, 4 [62] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-ofthought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824 24837, 2022. 2, 8 [63] Mitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig Schmidt. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time, 2022. 6 [64] Penghao Wu and Saining Xie. V*: Guided visual search as core mechanism in multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1308413094, 2024. 2, 3 [65] Yixuan Wu, Yizhou Wang, Shixiang Tang, Wenhao Wu, Tong He, Wanli Ouyang, Jian Wu, and Philip Torr. Dettoolchain: new prompting paradigm to unleash detection ability of mllm. arXiv preprint arXiv:2403.12488, 2024. 2, 21 [66] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441, 2023. 2, 22, [67] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v(ision), 2023. 3 [68] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36, 2024. 2 [69] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. survey on multimodal large language models. arXiv preprint arXiv:2306.13549, 2023."
        },
        {
            "title": "Appendix\nTable of Contents",
            "content": "A. Overview Video & Interactive Visualizations B. Code & Full Prompts C. Qualitative Examples D. Additional Details on Prompt Chaining . . . . D.1. Object detection . . D.2. Segmentation . . . . D.3. Depth estimation . D.4. Surface normal estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E. Additional experimental details and results . . . . . . . . . . . . . . . . E.1. Object detection . . . E.2. Semantic segmentation . . . . E.3. Grouping . . . E.4. Depth prediction . . . . E.5. Surface normal prediction . . . E.6. Experiments with Llama . E.7. Blind guess . . . . E.8. Finer-grained prompt chain . . E.9. In-the-wild evaluations . . . . . . . . . F. Reasoning Results F.1. Ablating the batch size . . F.2. Detailed experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G. Additional Experiments with 4o Image Generation G.1. Prompting methodology . . G.2. Preliminary quantitative analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . H. Prompt Sensitivity Analysis . Prompting Costs A. Overview Video & Interactive Visualizations 13 13 13 18 . 18 . 18 . 19 . 20 21 . 21 . 22 . 24 . 26 . 26 . 26 . 27 . 28 . 28 28 . 28 . 31 . 31 . 33 33 33 . . . . . . . . . . . . . . . . . narrated overview video including the papers method, quantitative and qualitative results, as well as interactive visualizations, is provided at https://fm-vision-evals.epfl.ch. B. Code & Full Prompts We provide our code and full prompts at https://github.com/EPFL-VILAB/fm-vision-evals. C. Qualitative Examples We provide additional qualitatives in Figures 9, 10, 11 and 12 to show each models performance on different tasks. 13 Figure 9. Additional qualitative results for MFM predictions on different tasks. 14 Figure 10. Additional qualitative results for MFM predictions on different tasks. 15 Figure 11. Qualitative results for reasoning model predictions on different tasks next to GPT-4o prediction. 16 Figure 12. Additional qualitative results for GPT-4o image generation predictions across tasks. Notice the various failure cases such as spatial misalignment in these examples, as outlined in the main paper Fig. 8."
        },
        {
            "title": "Precision Recall",
            "content": "Strategy 1 Strategy 2 GPT-4o Gemini 1.5 Pro Claude 3.5 Sonnet GPT-4o Gemini 1.5 Pro Claude 3.5 Sonnet 97.5 90.5 84.27 89.05 84.37 78. 75.75 83.81 81.24 88.37 89.3 85.94 Table 8. Classification for Object Detection: The results clearly show the precision-recall trade-off between using the two strategies for multi-label classification. D. Additional Details on Prompt Chaining D.1. Object detection Different variations of classification for object detection. As discussed in Section 3, the first stage of the object detection pipeline involves identifying all the objects present in the image. We attempt the following two strategies for the multi-label classification task: The first strategy simply provides the model with the entire image, asking it to identify all present classes. The second strategy divides the image into five regions: four quadrants and center crop. The model is asked to identify the classes present in the five regions in independent queries. With each query, the full image is provided for additional context. The final prediction is obtained by taking the union of the classes identified across all regions (see Algorithm 1 in the appendix for detailed pseudocode). This approach typically improves recall but may reduce precision, reflecting trade-off between the two strategies. The precision-recall trade-off for the models is described in Tab. 8. To pick the best classification strategy for the models, we run the oracle on the predicted labels on small subset and choose the one that yields the highest AP. After we find the object labels, we run the procedure described in Algorithm 2 to regress the bounding boxes. Algorithm 1 Region-based Image Classification regions DivideIntoRegions(image) allClasses for region regions do 1: procedure REGIONBASEDCLASSIFICATION(image) 2: 3: 4: 5: 6: classes QueryMFM(image, region) allClasses allClasses classes end for return allClasses 7: 8: 9: end procedure 10: procedure DIVIDEINTOREGIONS(image) 11: 12: quadrants DivideIntoQuadrants(image) center ExtractCenterCrop(image) return quadrants {center} 13: 14: end procedure In Alg. 2, we explored batching the grid-cells and querying them independently. While batching didnt affect results for most MFMs, it significantly deteriorated the performance for Gemini, so we opted to use independent queries for Gemini instead. D.2. Segmentation The procedures for supervised segmentation and grouping are described in Algorithm 3 and Algorithm 4 respectively. 18 Algorithm 2 Recursive Grid-Search while search space can be reduced do 1: procedure COARSEGRIDSEARCH(image, object, gridStructure) 2: 3: 4: 5: cells DivideIntoGrid(image, gridStructure) relevantCells {c cells : QueryMFM(c, object) = TRUE} image CropToRelevantCells(image, relevantCells) 6: end while return image as bbox 7: 8: 9: end procedure 10: procedure QUERYMFM(cell, object) 11: 12: end procedure return MFM classification of object presence in cell Algorithm 3 Superpixel Segmentation 1: procedure SEMSEGMENTATION(image, batchSize, scaleList) 2: 3: 4: 5: 6: superpixels SLIC(image) classif iedSuperpixels history for 1 to length(superpixels) step batchSize do batch GetBatch(superpixels, i, batchSize) pyramid CreateSemanticPyramid( 7: 8: 9: 10: 11: 12: image, batch, scaleList) batchClasses ClassifyBatch( pyramid, history) classif iedSuperpixels classif iedSuperpixels batchClasses history UpdateHistory( history, batchClasses) end for segmentedImage FloodFillSuperpixels( image, classif iedSuperpixels) return segmentedImage 13: 14: end procedure D.3. Depth estimation The procedure for depth estimation is given in Algorithm 5. crucial part of the algorithm involves optimizing the objective to obtain the overall depth rankings. To formulate the objective for globalizing the pairwise depth rankings, we repurpose the objective in [75]. Given the vector of global rankings RN , we first consider instances where superpixel is predicted to be at greater depth than superpixel j. The corresponding objective is formulated as: Lgt(x) = (cid:88) i,j (xi xj 1)2 (1) This objective encourages xi, ranked at greater depth than xj, to take on higher values. Similarly, an analogous objective Llt can be defined for superpixels xi predicted to be at depth less than xj. Following [75], we include smoothness regularization term to stabilize the depth estimations: Ls(x) = (xi xj)2 (cid:88) i,j 19 (2) Algorithm 4 BFS Segmentation 1: procedure INSTANCEGROUPING(image, queryP oint, batchSize, scaleList) 2: 3: superpixels SLIC(image) graph ConstructSuperpixelGraph(superpixels) startN ode FindSuperpixelContaining( superpixels, queryP oint) 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: cluster {startN ode} queue new Queue() queue.enqueue(startN ode) visited {startN ode} while not queue.isEmpty() do batch GetBatchFromQueue(queue, batchSize) batchP yr CreateSemanticPyramid( image, batch, scaleList) clusterP yr CreateSemanticPyramid( image, cluster, scaleList) newM embers QueryMFM( batchP yr, clusterP yr) cluster cluster newM embers queue, visited UpdateQueueAndVisited( graph, newM embers, visited) end while return cluster 16: 17: 18: end procedure This regularization is applied over pairs of adjacent superpixels and j, promoting smooth transitions between their depth values. The final objective that needs to be minimized is weighted sum of the above terms: = min (λgtLgt + λltLlt + λsLs) (3) where λgt, λlt, and λs are the weight parameters. For our experiments, we select λgt = λlt = 1 and λs based on the performance on smaller validation set. To obtain metric depth estimates, we assume access to ground-truth depth values for the purpose of scaling. Specifically, after floodfilling the values of x, we generate complete relative depth map d. Given the ground-truth depth map d, we optimize the following objective to determine the appropriate scale and shift parameters: (s, t) = arg min s,t (cid:88) i=1 (sdi + i )2 (4) where is the total number of pixels in the image. By solving this optimization problem, we can then scale and shift the relative depth map to align it with the metric depth. D.4. Surface normal estimation The procedure for surface normal estimation is detailed in Algorithm 6 and Fig. 13. While the model makes binary decisions regarding whether one depth is less than or greater than another, we have found that enabling the model to also consider equality predictions enhances the accuracy of surface normal estimations. To incorporate this into our approach, we introduce the following term for cases where superpixels xi and xj are predicted to be at equal depth: Leq(x) = (cid:88) (xi xj)2 (5) i,j for pairs of superpixels xi and xj predicted to lie at an equal depth. For weights, we choose λeq = λlt = λgt = 1 and, as before, we pick λs based on the performance on smaller validation set. 20 Algorithm 5 Depth Estimation 1: procedure ESTIMATEDEPTH(image, numP airs) 2: 3: 4: superpixels SLIC(image) pairwiseRankings for 1 to numP airs do 5: 6: 7: 8: 9: 10: pair SampleRandomPair(superpixels) ranking QueryMFM(pair) pairwiseRankings pairwiseRankings {ranking} end for globalRankings MinimizeObjective( pairwiseRankings) depthM ap AssignDepthToPixels( image, superpixels, globalRankings) return depthM ap 11: 12: end procedure Algorithm 6 Surface Normal Estimation superpixels SLIC(image) pairwiseAlign {} for 1 to numP airs do 1: procedure ESTIMATENORMALS(image, numP airs, bases) 2: 3: 4: 5: 6: pair SampleRandomPair(superpixels) for in bases do 7: 8: 9: 10: 11: 12: 13: 14: alignment QueryMFM(pair, b) pairwiseAlign[b] pairwiseAlign[b] {alignment} end for end for normalM aps {} for in bases do globalAlign MinimizeGlobalObjective( pairwiseAlign[b]) normalM aps[b] AssignAlignmentToPix( image, superpixels, globalAlign) end for return normalM aps 15: 16: 17: end procedure To visualize surface normals, we take the per-axis predictions and normalize them to [0,1], after which we project them onto the unit sphere. We directly interpret the three channels as RGB values. Note that since the per-axis normalized surface normal predictions do not present absolute directional information with respect to the camera, the colors might not match the ground truth visualizations. E. Additional experimental details and results E.1. Object detection We evaluate additional baselines for GPT-4o in Tab. 9. In these experiments, the classification component of the pipeline remains unchanged, while the grid search is replaced with alternative methods. The results are clear: GPT-4o struggles with directly regressing bounding box coordinates. To address this, we experimented with overlaying rulers on the images to assist in bounding box regression, following insights from [65], but we found minimal improvement. The various visual prompts we Figure 13. Surface normal prediction algorithm. Similar to depth in Fig. 5, we randomly select superpixels and give them to the model to perform pairwise comparison. The superpixels are compared based on their alignment with the basis vectors relative to the camera. The pairwise ranks are globalized to obtain the final result. *Summary of the actual prompt; see the full prompt in the supplementary. Figure 14. We ablate different ruler types as visual aids for object detection. tried are displayed in Fig. 14, and the numbers we obtained on subset of 100 COCO images are summarised in Tab. 10. Additionally, we evaluate direct bounding box regression with Gemini, Qwen2-VL, and Claude (see Tab.9), as some of these models have demonstrated this capability [21]. The results indicate substantial variance in performance: while Gemini and Qwen2-VL localize bounding boxes effectively, GPT-4o and Claude underperform considerably. Interestingly, despite improvements in Gemini and Qwen2-VL, they still lag behind the specialist models and do not surpass GPT-4o when using the chain algorithm. E.2. Semantic segmentation We depict various marker types used for segmentation in Fig. 15. Furthermore, we conduct an ablation study on the marker type and the context provided during classification, as shown in Tab. 11. The numbers highlight the importance of contextual information within the semantic pyramid. Removing the context layer leads to performance drop of over 10 mIoU. Additionally, the direct strategy of marking directly on the image and then classifying results in 16 mIoU difference, indicating that MFMs currently lack the ability to localize precisely. We also investigate the impact of omitting the finest level of the semantic pyramidthe crop. While the mIoU value does not decrease much, qualitative analysis reveals that this omission hampers the models ability to capture finer image details. This is shown in Fig. 16. We also conduct ablation studies on the effect of the models performance when the semantic pyramid is omitted. The visual markers in Fig. 15 do not work well and do not allow batching, so we borrow visual marker similar to the one used in [66]"
        },
        {
            "title": "Method",
            "content": "AP50 AP75 AP GPT-4o (Direct Regression) Gemini 2.0 Flash (Direct Regression) Gemini 1.5 Pro (Direct Regression) Claude 3.5 Sonnet (Direct Regression) Qwen2-VL (Direct Regression) 17.69 38.77 55.11 17.97 44.10 1.69 10.80 31.23 2.13 23.71 5.08 15.66 31.33 6.03 24. GPT-4o (Regression with Ruler) 15.95 2.60 4.99 Table 9. Additional experiments with MFMs on object detection. Direct bounding box regression is ineffective for GPT-4o and Claude 3.5 Sonnet, while Gemini 1.5 Pro and Qwen2-VL perform better. For all models, the most effective prompt was selected from set of options based on validation set performance, similar to the approach used for prompt chains. Visual Prompt AP50 AP AP Ruler 1 Ruler 2 Ruler 3 21.19 22.59 19.06 4.09 7.85 4.86 7.60 9.20 8.09 Table 10. Rulers for Object Detection: The results indicate that visual markers such as rulers are ineffective in aiding GPT-4o for bounding box regression. Numbers obtained are on subset of 100 COCO Images. Figure 15. The curve, rectangle, and point marker types we ablated for the segmentation task. Category Visual Prompts Contextual Ablations Ablation Curve Rectangle Point Without Crop Without Context Best Direct mIoU Pixel Accuracy 41.62 41.67 41.68 40.97 31.25 25.79 67.43 69.74 68.83 69.84 61.66 55. Table 11. Ablation study on semantic segmentation. The results show that GPT-4o is robust to the choice of visual prompt. The substantial performance drop (16 mIoU) observed upon removal of the semantic pyramid shows the critical role of the contextual information used in the sub-task. (see Fig. 17). Tab. 12 shows the results for different marker types [66]. Clearly, the models performance drops considerably when deprived of the crops. We note that the marks we use differ from the ones used in [66] in two ways: The marks obtained in [66] already correspond to semantic entities, while we use superpixels as proxy for this. Extracting full semantic mask requires discerning finer-grained details, so the marks we use typically correspond to smaller regions in the image. 23 Figure 16. Semantic Segmentation predictions with different layers of the semantic pyramid. From left to right: 1. The RGB Image. 2. The predicted mask when no crops are given, and markings on the full image are directly used. The model is unable to make out fine details. 3. The predicted mask when the top of the semantic pyramid is removed. The model misses out on predicting some finer details (for instance, the gaps in the bench and the handbag). 4. The predicted mask when the middle layer (the context) is removed. The model makes some wrong predictions. 5. The mask with the full pyramid of information. Figure 17. The marker styles used for directly querying semantic entities from the full image. Visual Marker mIoU Pixel Accuracy Curve Rectangle Number 20.70 18.24 21.13 50.34 47.80 50.00 Table 12. Ablation on Direct Segmentation: The numbers clearly show that omitting the extra information provided by the crops greatly impacts the models performance. The numbers shown are for subset of 30 images. The prompt was selected from set of options based on validation set performance. E.3. Grouping For the grouping task, we filter out 100 COCO images that contain instances that are well-posed for this task. The wellposedness of an instance for grouping is measured by how consistent the SAM predictions are for the instance. To calculate the consistency of predictions for an instance, we sample random points inside the instance and use SAM to obtain an instance mask for each point individually, as well as global mask by querying all points together. The mIoU between individual masks 24 Figure 18. Ambiguous instances: If cats ear is marked, is the object the cat or the cats ear? Images on the left: Grouping without explicit reference to objectness\". Images on the right: Grouping obtained using the \"apostrophe-s\" test. and the global mask is used as the consistency metric. Finally, the images that contain instances with consistency value above given threshold are selected and randomly sampled to create the evaluation set. E.3.1. What constitutes an object? Determining the granularity of what qualifies as an object in grouping task is often ambiguous. For instance, if persons nose is highlighted, should the object be considered the nose alone, or the entire person? Both interpretations are valid, leading to potential inconsistencies. To address this, we propose prompting method that refines the granularity of objectness. By instructing the model to interpret the highlighted instance as possessive nounexpressed through the apostrophe-s structurethe model is encouraged to group coarser objects. For example, when prompted with persons nose, the model is guided to interpret the object as the person, rather than the nose alone. This approach is illustrated in Fig. 18. While this method is not universally effective, it often resolves ambiguity by clarifying the relationship between parts and wholes. We provide the full prompt in the supplementary material."
        },
        {
            "title": "Method",
            "content": "Higher is better Lower is better δ1 δ2 δ3 ρ"
        },
        {
            "title": "Curve\nRectangle\nPoint",
            "content": "0.550 0.534 0.525 0.822 0.807 0.802 0.935 0.931 0.928 53.75 51.68 51.89 70.43 69.28 62.07 0.332 0.341 0. Table 13. Ablation study on depth estimation. GPT-4o performs the best when curves are used as the visual marker."
        },
        {
            "title": "Samples",
            "content": "Higher is better Lower is better 100 100 200 200 δ1 δ2 δ 0.571 0.597 0.571 0.593 0.774 0.785 0.773 0.788 0.863 0.867 0.867 0.869 ρ 0.83 0.86 0.83 0.86 200 400"
        },
        {
            "title": "AbsRel",
            "content": "0.528 0.514 0.501 0.502 Table 14. Oracle depth results with different numbers of superpixels and comparisons made during chaining. Method ρx ρy ρz Curve Rectangle Point -4.89 -13.99 2.42 58.00 58.84 51.26 39.28 39.65 39.59 Accuracyx Accuracyy Accuracyz 67.95 69.25 67.55 49.02 45.75 49. 66.9 67.83 67.2 Table 15. Ablation study on surface normal estimation. GPT-4o is relatively robust to different visual marker choices. E.4. Depth prediction We conduct an ablation study on the choice of visual markers in Tab. 13. Please also see Tab. 14 for additional oracle evaluations. E.5. Surface normal prediction We conduct an ablation study on the choice of visual markers in Tab. 15. E.6. Experiments with Llama Unlike the other models, Llama employs different prompt chains for object detection, grouping, and segmentation due to its current limitations with handling multiple images [27]. Specifically: For object detection, we provide the full image with the corresponding grid cell marked instead of providing crop of the grid cell with the full image. For semantic segmentation, we provide the full image with the corresponding superpixel marked, instead of providing set of crops per superpixel. For grouping, we highlight the superpixel corresponding to the initial cluster in red and the superpixel of the query point in blue. The model is tasked with determining whether the blue region belongs to the same entity as the red region. As with the other models, we experimented with multiple prompts and selected the best-performing one based on smaller validation set across all tasks. Surprisingly, Llama does well in segmentation despite not being provided with any crops. comparative evaluation against other MFMs on smaller subset using identical prompts is shown in Tab. 16. Notably, Llama surpasses all other models in this setup. An additional interesting finding is Llamas unique capability to achieve positive correlation in the x-direction for the surface normals task, result not observed with the other MFMs. Figure 19. The blind guesses made by GPT-4o on different tasks. Model mIoU Pixel Accuracy MFMs GPT-4o Gemini 1.5 Pro Claude 3.5 Sonnet Llama-3.2-90B 19.77 22.98 20.00 25. 54.31 61.04 55.69 61.66 Table 16. The performance of Llama compared with the other models on smaller subset, in the absence of crops. E.7. Blind guess As mentioned in Section 4, useful way to analyze the potential biases of the MFM, and to gauge the degree to which it uses the visual content is blind guess, or prompting the image with blank image. In particular: For object detection, we ask the model to imagine the classes present. After this, we ask it to provide reasonable coordinates for the objects based on its world knowledge. For semantic segmentation, we mark rectangle in white image and force the model to predict class. We ask the model to use the location to make an educated guess. For depth, we ask the model to imagine an indoor setting. We mark two rectangles and force the model to predict that one is at greater depth than the other. For normals, we repeat the procedure for depth for each direction. The results for GPT-4o are visualized in Fig. 19, and reveal several interesting insights. For object detection, the model chooses common classes like person and car. Additionally, it seems to grasp the relative sizes of objects reasonably well, as indicated by its tendency to make the car and the bench longer. For semantic segmentation, the model makes reasonable guesses. For instance, it guesses \"sky-merged\" and \"airplane\" at the top of the image, \"person\" near the middle, \"dog,\" \"cat,\" and \"floor\" near the bottom. For depth estimation, GPT-4o exhibits \"ceiling bias\" and consistently infers that the top right corner is located at greater relative depth. We observe that this bias is reflected in several of the models predictions as well, where the ceiling is consistently assumed to be at greater depth. For surface normals, the model uses the relative locations of the rectangles to form judgments. For instance, in the direction, it infers that the right rectangle aligns more towards the right. In the direction, it consistently infers that the bounding box at greater coordinate aligns more with the positive direction. While Chain-of-Thought (CoT) reasoning is able to break this bias along the direction for GPT-4o, the left-right bias persists when actual images are presented. E.8. Finer-grained prompt chain natural question is whether the performance of our MFMs can be further enhanced by refining the granularity of the prompt chain. In other words, can we improve performance by increasing the number of superpixels and comparisons or by using thinner outer grid cells? To explore this, we first examine the effect of finer-grained prompt chain on the Oracle + Chain\" and Specialist + Chain\" baselines. As shown in Fig. 20, these baselines exhibit steady performance improvements as the prompt chain is refined. Figure 20. Performance improvements of the Oracle + Chain\" and Specialist + Chain\" baselines on full datasets when using finer-grained prompt chain. Ten iterations were used for object detection. To determine whether this trend extends to the MFMs, we conducted small-scale experiment using GPT-4o on the same tasks. As illustrated in Fig. 21, although GPT-4o shows modest improvements with finer-grained prompt chain, its performance quickly saturates due to misclassifications. This observation supports our decision to adopt coarser granularity for the MFMs, confirming that our original settings are sufficient to capture the performance gap. We reiterate here that the control baselines in our paper serve as calibration tools rather than performance ceilings. They mitigate the sub-optimality of the prompting method and prevent exaggerated conclusions about MFMs significantly lagging behind specialists. Our benchmark is designed to be accurate in relative and ordinal sense. E.9. In-the-wild evaluations Please see Fig. 24, 25 and 26 for qualitative evaluation of MFMs on in-the-wild samples [19, 58]. F. Reasoning Results F.1. Ablating the batch size As noted in Sec. 4.1, o4-mini is especially sensitive to the batch size used during inference. While our main evaluations used batch size of 100 for consistency across models, we ablate this choice on subset of 500 ImageNet samples in Fig. 22. As shown, o4-mini suffers substantial degradation in classification accuracy as the batch size increases, across all reasoning effort settings. In contrast and surprisingly, both o1 and o3 demonstrate improved performance with larger batch sizes, consistent with trends observed in prior work on in-context learning for LLMs [13, 31]. Figure 21. GPT-4os performance improvements with finer-grained prompt chain plateau, showing that our original settings are adequate. Figure 22. Effect of batch size on classification accuracy. We ablate the classification performance of o1, o3, o4-mini and GPT-4o on subset of 500 ImageNet images under varying batch sizes. While o1 and o3 benefit from larger batch sizes, o4-mini shows pronounced drop in accuracy. 29 F.2. Detailed experiments As noted in the main paper, we evaluate o1 and o3 on smaller, representative subsets of data. To construct informative subsets for classification, object detection, segmentation, and grouping, we compute the Kendall τ rank correlation between the performance rankings of non-reasoning models on candidate subsets and their full-dataset rankings, over multiple bootstrap runs. For each task, we choose the smallest subset size that meets task-specific correlation threshold, yielding 1,000 samples for classification, 200 for detection, 50 for segmentation, and 30 for grouping. These subsets are deemed informative because they preserve the relative ranking of non-reasoning models. For depth and surface normal prediction, we select the 10 most challenging samples for GPT-4o (where it achieves the lowest Spearman correlation) due to higher evaluation costs."
        },
        {
            "title": "ImageNet",
            "content": "ImageNet-V2 2DCC 3DCC ImageNet-R ImageNet-Sketch o4-mini (low) o4-mini (medium) o4-mini (high) o1 o3 GPT-4o 50.7 58.4 59.6 77.90 78.00 76.70 49.2 70.50 74.10 72.10 35.6 62.40 62.60 63. 36.4 60.20 61.70 61.40 58.2 85.3 87.2 86.2 46.4 65.50 69.10 66.10 Table 17. Classification. Accuracy scores on the standard classification datasets. o3 consistently outperforms GPT-4o and achieves the highest overall scores. In contrast, o4-mini lags behind."
        },
        {
            "title": "Model",
            "content": "AP50 () AP75 () AP () o4-mini (low) o4-mini (medium) o4-mini (high) o1 o3 GPT-4o 48.40 47.56 48.10 66.70 64.89 64.11 29.71 29.77 26.49 40.52 40.73 42.61 27.65 27.11 26.07 37.77 38.57 38.17 Table 18. Object Detection. All models other than o4-mini achieve comparable performance, with o1 and o3 slightly outperforming GPT-4o."
        },
        {
            "title": "Model",
            "content": "mIoU () Pixel acc. () o4-mini (low) o4-mini (medium) o4-mini (high) o1 o3 GPT-4o 29.67 28.86 28.66 40.24 38.18 35.66 63.94 63.82 62.81 72.92 72.75 69.59 Table 19. Segmentation. o1 and o3 outperform GPT-4o in both mIoU and pixel accuracy. o4-mini underperforms across all reasoning levels. The performance of o1, o3, GPT-4o, and o4-mini, under varying levels of reasoning effort, is summarized in Tables 17 through 22, covering classification, object detection, segmentation, grouping, depth, and surface normal prediction. On semantic tasks, all models perform comparably, with o1 and o3 showing slightly stronger results in classification, object detection, and segmentation. For geometric tasks, performance drops across the board due to the difficulty of the selected samples. However, the reasoning models consistently outperform GPT-4o. In particular, all reasoning models achieve positive 30 Model δ1() δ2() δ3() ρ() Accuracy() AbsRel() o4-mini (low) o4-mini (medium) o4-mini (high) o1 o3 GPT-4o 0.467 0.465 0.462 0.456 0.490 0. 0.726 0.711 0.724 0.716 0.738 0.732 0.857 0.852 0.864 0.873 0.855 0.881 -0.009 0.070 -0.029 0.079 0.250 -0.050 55.15 56.85 53.65 57.20 63.50 52.00 1.07 0.953 1.090 0.914 0.819 1.121 Table 20. Depth Prediction. All models struggle with this difficult subset. However, o3 consistently outperforms the others across most metrics."
        },
        {
            "title": "Model",
            "content": "mIoU() o4-mini (low) o4-mini (medium) o4-mini (high) o1 o3 GPT-4o 47.41 44.59 46.36 52.61 55.06 59.64 Table 21. Grouping. GPT-4o outperforms all reasoning-based models on this semantic task. Among the reasoning models, o3 performs best. Model o4-mini (low) o4-mini (medium) o4-mini (high) o1 o3 GPT-4o ρx 0.18 0.39 0.38 0.24 0.48 -0.30 ρy 0.12 0.24 0.23 0.23 0.36 0.09 ρz 0.30 0.32 0.31 0.19 0.28 0. Table 22. Surface Normals. Reasoning models outperform GPT-4o, particularly along the horizontal (x) direction, where GPT-4o shows strong negative correlation. o3 achieves the highest scores across all axes. correlation along the horizontal axis in surface normal prediction, correcting common failure mode in GPT-4o (see Fig. 11, where the horizontal gradient is flipped). Qualitative comparisons for all tasks are shown in Fig. 11. G. Additional Experiments with 4o Image Generation G.1. Prompting methodology As discussed in the main paper, GPT-4o generates full image recreations rather than edits, often resulting in spatial misalignments. To enable consistent comparisons, we first zero-pad all input images to square dimensions and, after generation, crop the relevant regions to align with the original input. Below, we outline task-specific prompting details. Preliminary quantitative results for grouping, depth, and surface normals, evaluated at full scale, are provided in App. G.2. Grouping. The prompt used for object-based segmentation is shown in Listing 1. The model is given an image with red point marking location on an object and is instructed to return the same image with the entire object filled in solid red. To extract the predicted mask, we use HSV thresholding and post-process it by retaining only the largest connected component, which effectively removes small artifacts and hallucinated regions. Although this simple postprocessing often yields reasonable masks, more advanced postprocessing techniques could improve results further. We leave such refinements to future work. (cid:7) You are given an input RGB image where small red circle marks point on an object. Your task is to return the **exact same image**, but with the **entire object that contains the marked point filled in solid red**. Do not add any other markings, text, or overlays; only apply the red fill to the object. (cid:6) (cid:4) (cid:5) Listing 1. Grouping prompt for GPT-4o image generation 31 Depth. The prompt used for depth prediction is provided in Listing 2. The model is instructed to produce grayscale rendering of the input image, where closer regions appear darker and farther regions lighter. (cid:7) Generate **pure grayscale depth map** from the input image. The grayscale values must encode depth as (cid:4) follows: - **White (255 intensity)** represents points that are **closest** to the camera (minimum depth). - **Black (0 intensity)** represents points that are **farthest** from the camera (maximum depth). - All other points must be shaded **monotonically between black and white**, based solely on their distance from the camera. This map must not contain any colors, textures, or artistic effects: only smooth grayscale transitions that accurately reflect increasing depth, with darker shades at greater distances. (cid:6) Listing 2. Depth prediction prompt for GPT-4o image generation Surface Normals. The prompt used for surface normal prediction is shown in Listing 3. The model is asked to generate surface normal map in the standard RGB encoding used in computer graphics. Conventionally, red corresponds to the leftright (x) axis (left = 0, right = 1), green to the updown (y) axis (down = 0, up = 1), and blue to the depth (z) axis (inward = 0, outward = 1). (cid:7) You are vision model that, given an input RGB image, must predict per-pixel surface normal map and render it as an RGB image using the standard normal-map color scheme. Requirements: 1. **Output format** - Directly generate an image. - Produce raw image (same dimensions as input) whose pixel colors encode the normals as above. - Do **not** add any annotations, text overlays, or alpha channels: only the RGB channels. 2. **Normal-map encoding** - For each pixel estimate its surface normal vector - Display the orientation of the surface normal vector using the standard color scheme used in computer graphics. (cid:6) G.1.1. Prompt sensitivity Listing 3. Normals prediction prompt for GPT-4o image generation (cid:5) (cid:4) (cid:5) We observe that GPT-4os image generation is susceptible to changes in the prompt. To illustrate this brittleness, we test two modified prompts. For depth prediction, we invert the color mapping in Listing 2, asking the model to render near points as dark and far points as white. For surface normals, rather than producing single RGB normal map, we prompt the model for three separate grayscale images, each representing alignment with one of the x, y, or axes, similar to our prompt-chaining setup. As shown in Tab. 23, these small changes lead to substantial degradation in performance. (a) Depth prediction Prompt δ1 () δ2 () δ3 () ρ () AbsRel () Original Altered 0.562 0.399 0.849 0.694 0.942 0.854 0.448 -0. 0.303 0.474 (b) Surface normal prediction Prompt Original Altered ρx 0.31 -0. ρy 0.35 0.38 ρz 0.14 -0.23 Table 23. Performance for different prompts. Comparison between the original and an altered prompt for depth and surface normal prediction. G.2. Preliminary quantitative analysis The full-scale quantitative results are shown in Tab. 24. While the performance is non-trivial, it currently falls short of what is achieved by GPT-4o using the prompt chain. We view the refinement of prompts and decoding strategies for image generation as an important direction for future work. Notably, grouping predictions are impacted by spatial misalignment, hallucinated regions, and incorrect markings. Depth predictions, on the other hand, occasionally suffer from an inverted rendering of the depth gradient, which significantly affects correlation-based metrics. We showcase representative qualitative results for grouping, depth, and surface normals in Fig. 12, highlighting successes and common failure modes."
        },
        {
            "title": "Depth Prediction",
            "content": "Surface Normal Prediction"
        },
        {
            "title": "Metric",
            "content": "mIoU δ1 δ2 δ3 ρ AbsRel () ρx ρy ρz Value () 28.56 0.485 0.735 0.848 0.52 0. 0.09 0.44 0.17 Table 24. GPT-4o image generation performance across three tasks. H. Prompt Sensitivity Analysis In Fig. 23, we evaluate the non-reasoning models for each task considering different prompting techniques. We observe that GPT-4o generally shows lower sensitivity to different prompts on most of the tasks compared to other MFMs. For surface normals, we interestingly observe that the predictions greatly improve in the and directions, when GPT-4o and Claude are asked to reason in the prompt (see Tab. 25). Model Prompt ρx () ρy () ρz () GPT-4o Claude 3.5 Sonnet Prompt 1 Prompt 2 Prompt 3 Prompt 4 Prompt Prompt 1 Prompt 2 Prompt 3 Prompt 4 Prompt 5 -0.36 -0.29 0.07 -0.08 -0.34 -0.34 -0.18 -0.09 -0.06 -0.06 -0.63 -0.45 0.55 0.65 0.45 -0.56 -0.08 0.68 0.66 0.56 0.04 -0.10 0.44 0.43 0. 0.00 -0.11 0.41 0.35 0.35 Table 25. Prompt sensitivity for surface normal prediction. Correlations under five different prompts for GPT-4o and Claude 3.5 Sonnet. CoT prompting (in bold) greatly improves ρy and ρz. I. Prompting Costs At the time of writing, the API pricing for GPT-4o (gpt-4o-2024-08-06) was $2.50 per million input tokens and $10.00 per million output tokens. For Gemini 1.5 Pro (gemini-1.5-pro-001), the corresponding rates were $3.50 (input) and $10.50 (output), and for Claude 3.5 Sonnet (claude-3-5-sonnet-20240620), $3.00 and $15.00, respectively. 33 Figure 23. Sensitivity of MFMs to different prompting techniques. We observe that GPT-4o showcases lower sensitivity on most tasks compared to other MFMs. The lower-cost o4-mini-2025-04-16 was priced at $1.10 (input) and $4.40 (output), while o1-2024-12-17 and o3-2025-04-16 were substantially more expensive at $15.00/$60.00 and $10.00/$40.00, respectively. The costs for the scaled-up experiments are documented in Tab. 26, and the prompting costs for the reasoning model are presented in Tab. 27. The primary reason for cost fluctuations across tasks is how each MFM tokenizes images. The notably higher costs for object detection with Gemini 1.5 Pro stem from the independent calls required in the prompt chain. For reasoning models, especially on the surface normal task, major contributor is the large number of generated reasoning tokens. These costs reflect the constraints of current APIs and are not indicative of how such tasks would be solved in practical 34 deployments. As discussed in the main paper, our framework is intended for standardized one-time evaluation (and not for efficient task execution) with MFMs."
        },
        {
            "title": "Task",
            "content": "GPT-4o Gemini 1.5 Pro Claude 3.5 Sonnet"
        },
        {
            "title": "Object Detection",
            "content": "223.8 185."
        },
        {
            "title": "Semantic Segmentation",
            "content": "232."
        },
        {
            "title": "Normals",
            "content": "22.1 57.4 130.1 298.6 610.8 450. 47.4 52.4 50.1 142.8 155.0 227. 42.0 198.2 209.9 Table 26. Prompting costs for scaled-up experiments (in $). Task Classification o1* 41.0 o3* 22.5 Object Detection 220. 104.0 Semantic Segmentation 200.0 Grouping Depth Normals 82.2 96.4 306.2 96.0 48.9 31. 85.4 o4-mini 50.0 102.2 115.0 25. 62.0 194.0 Table 27. Prompting costs for the reasoning models (in $).*Reported costs are for experiments conducted on the subset. 35 Figure 24. Qualitative results of evaluating MFMs for object detection on in-the-wild examples [19, 58]. We compare against 4M-21 [7] as vision specialist. Figure 25. Qualitative results of evaluating MFMs for semantic segmentation on in-the-wild examples [19, 58]. We compare against 4M-21 [7] as vision specialist. 37 Figure 26. Qualitative results of evaluating MFMs for depth prediction on in-the-wild examples [19, 58]. We compare against the Omnidata [18] depth estimator as vision specialist."
        }
    ],
    "affiliations": [
        "Swiss Federal Institute of Technology Lausanne (EPFL)"
    ]
}