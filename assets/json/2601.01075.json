{
    "paper_title": "Flow Equivariant World Models: Memory for Partially Observed Dynamic Environments",
    "authors": [
        "Hansen Jin Lillemark",
        "Benhao Huang",
        "Fangneng Zhan",
        "Yilun Du",
        "Thomas Anderson Keller"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Embodied systems experience the world as 'a symphony of flows': a combination of many continuous streams of sensory input coupled to self-motion, interwoven with the dynamics of external objects. These streams obey smooth, time-parameterized symmetries, which combine through a precisely structured algebra; yet most neural network world models ignore this structure and instead repeatedly re-learn the same transformations from data. In this work, we introduce 'Flow Equivariant World Models', a framework in which both self-motion and external object motion are unified as one-parameter Lie group 'flows'. We leverage this unification to implement group equivariance with respect to these transformations, thereby providing a stable latent world representation over hundreds of timesteps. On both 2D and 3D partially observed video world modeling benchmarks, we demonstrate that Flow Equivariant World Models significantly outperform comparable state-of-the-art diffusion-based and memory-augmented world modeling architectures -- particularly when there are predictable world dynamics outside the agent's current field of view. We show that flow equivariance is particularly beneficial for long rollouts, generalizing far beyond the training horizon. By structuring world model representations with respect to internal and external motion, flow equivariance charts a scalable route to data efficient, symmetry-guided, embodied intelligence. Project link: https://flowequivariantworldmodels.github.io."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 3 ] . [ 1 5 7 0 1 0 . 1 0 6 2 : r Preprint FLOW EQUIVARIANT WORLD MODELS: MEMORY FOR PARTIALLY OBSERVED DYNAMIC ENVIRONMENTS Hansen Jin Lillemark1,2 hlillemark@ucsd.edu Fangneng Zhan4 fnzhan@seas.harvard.edu Benhao Huang3 benhaoh@andrew.cmu.edu Yilun Du1 ydu@seas.harvard.edu T. Anderson Keller1 t.anderson.keller@gmail.com 1Kempner Institute, Harvard University 3ML, Carnegie Mellon University 2CSE, UC San Diego 4SEAS, Harvard University"
        },
        {
            "title": "ABSTRACT",
            "content": "Embodied systems experience the world as symphony of flows: combination of many continuous streams of sensory input coupled to self-motion, interwoven with the dynamics of external objects. These streams obey smooth, timeparameterized symmetries, which combine through precisely structured algebra; yet most neural network world models ignore this structure and instead repeatedly re-learn the same transformations from data. In this work, we introduce Flow Equivariant World Models, framework in which both self-motion and external object motion are unified as one-parameter Lie group flows. We leverage this unification to implement group equivariance with respect to these transformations, thereby providing stable latent world representation over hundreds of timesteps. On both 2D and 3D partially observed video world modeling benchmarks, we demonstrate that Flow Equivariant World Models significantly outperform comparable state-of-the-art diffusion-based and memory-augmented world modeling architectures particularly when there are predictable world dynamics outside the agents current field of view. We show that flow equivariance is particularly beneficial for long rollouts, generalizing far beyond the training horizon. By structuring world model representations with respect to internal and external motion, flow equivariance charts scalable route to data efficient, symmetry-guided, embodied intelligence. Project page"
        },
        {
            "title": "INTRODUCTION",
            "content": "As embodied agents in dynamic world, our survival critically depends on our ability to accurately model our surrounding environment, our own self-motion through it, and the dynamics of moving to coordinate an attack, an agent must bodies within it. natural example is pack hunting: accurately estimate the location and velocity of target while simultaneously predicting the motion of other pack animals. However, these world states are not simply provided to the agent in the form of an omniscient global view; instead, the agent is provided with restricted first-person field of view that simultaneously shifts and rotates with the agents own self-motion. The result is highly entangled stream of stimulus flows that yields only fraction of the full environments information at any point in time. Despite this, biological agents appear to navigate such partially observed dynamic environments effortlessly, as if they have latent map of the environment perfectly flowing in unison with the global world state. In this work, we study the task of partially observed dynamical world modeling (visualized in Fig. 1), combined with the inherent self-motion of embodied agents, and investigate if we might be able to account for both external and internal sources of visual variation in geometrically structured Equal contribution 1 Preprint Figure 1: Partially observable dynamic world modeling. The agent observes dynamics, turns away, then turns back to the original viewpoint. Flow Equivariant World Models (FloWM) can successfully integrate dynamics through time in stable manner, while existing work hallucinates. manner. Specifically, we find that both internal and external motion can be understood as mathematical flows, enabling both sources of variation to be handled exactly as time-parameterized symmetries through the framework of flow equivariance (Keller, 2025). We demonstrate that we can construct Flow Equivariant World Models that handle self-generated motion in precisely structured manner, while simultaneously capturing the motion of external objects, even if they are moving outside the agents field of view. We show that this yields substantially improved video world modeling performance and generalization to significantly longer sequences than those seen during training, highlighting the benefits of precise spatial and dynamical structure in world models."
        },
        {
            "title": "2 BACKGROUND",
            "content": "To build world models with structured representations of environment dynamics, we rely on recent work in both world modeling and equivariance, which we briefly overview here. More thorough information on related work is available in Section 5 and Appendix B. World Modeling. world model can be described at high level as system affording the ability to predict not only the future state of an environment given initial conditions, but also how that state may evolve differently when acted upon by an agent (Ha & Schmidhuber, 2018). Recent work on world models has focused on representing and predicting the future world state as video, primarily using large-scale latent diffusion transformer models. While these models achieve impressive perceptual quality and scale well with growing data and compute, we argue here that their current form inherently lacks the ability to predict long-horizon dynamics in partially observable environments, thus fundamentally limiting their ability to be used for real-world downstream tasks. Partial observability is defined as setting when the agents observation does not contain the full information of the worlds state. This problem is particularly relevant to world modeling: to accurately make predictions of the future, the model must retrieve all relevant information from previous observations, no matter how long ago it was observed, and bring it to the future prediction. Moving beyond fixed length video generation, recent autoregressive methods such as History-Guided Diffusion Forcing allow extending the transformer self-attention window over many past observation frames to retain self consistency (Song et al., 2025). But inevitably, as the number of observation frames grows, information must be discarded through sliding-window attention or some other approximation. This problem is exacerbated by the cost of spatiotemporal attention over highly redundant signal such as video. Once the past observation has left the context window (true partial observability with respect to the self attention window), it has been lost; turning around will reveal an entirely new hallucinated scene. Furthermore, relying on information from stale observation frames can be detrimental to modeling the natural dynamics of the evolving world around us. 2 Preprint Figure 2: Comparing World Modeling Frameworks. a) Standard autoregressive video diffusion evicts frames beyond the sliding window. b) Information dependencies between past observations and generated frames cause inconsistency without memory. c) Existing memory solutions are viewdependent, and thus cannot predict dynamic scenes consistently. d) FloWM remembers past observations in the spatial latent memory, and continually updates them via internal dynamics. Recent work has explored augmenting video diffusion models with different forms of latent memory that persist across time; however, the focus has primarily been on consistency in static 3D scenes, without unified framework for modeling partially observed dynamics. In contrast, we argue that natural way to build world models is with recurrent flow equivariant memory at the core, evolving and shifting to represent both the dynamics of the world and the actions of the agent seamlessly. Such memory enables prediction of future states in precise motion-symmetric manner while maintaining important information for extended observation windows. visual comparison between modern sliding-window autoregressive transformers, existing memory solutions, and our model (FloWM) is available in Figure 2. Equivariance. neural network ϕ is said to be equivariant if its output, ϕ(f ), changes in structured, predictable manner when the input is transformed by an element of the group G, i.e. ϕ(g ) = ϕ(f ) G. One way to construct equivariant neural networks is through structured weight sharing (Cohen & Welling, 2016; Ravanbakhsh et al., 2017). This structure reduces the number of parameters that need to be learned in an artificial neural network while simultaneously improving performance by incorporating known symmetries from the data distribution. For example, in the setting of molecular dynamics simulation, introducing equivariance with respect to 3-dimensional translations, rotations, and reflections (the group E(3), known symmetry of the laws of physics) increases data efficiency by up to three orders of magnitude (Batzner et al., 2022)."
        },
        {
            "title": "3 FLOW EQUIVARIANT WORLD MODELS",
            "content": "In this section, we begin with review of Flow Equivariance, followed by an introduction of generalized form of the recurrence relation capable of supporting complex tasks. Then, we present instantiations of our general framework for 2D and 3D partially observed dynamic world modeling. 3.1 GENERALIZED FLOW EQUIVARIANCE Flow Equivariance. Recently, Keller (2025) introduced the concept of flow equivariance, extending existing static group equivariance to time-parameterized sequence transformations (flows), such as 2d visual motion. These flows are generated by vector fields ν, and written as ψt(ν) G. The flow ψt(ν) maps from some initial group element g0 to new element gt (i.e. ψt(ν) g0 = gt), and we can therefore informally think of ψt(ν) as time-parameterized group element when g0 is fixed. For example, if we think of ν as particular velocity, then ψt(ν) corresponds to the spatial displacement resulting from integrating ν for time. Formally, flow ψt(ν) : is subgroup of Lie group G, generated by corresponding Lie algebra element ν g, and parameterized by single value often interpreted as time. sequence-to-sequence model Φ, mapping from (f0, . . . , fT ) (cid:55) (y0, . . . , yT ) is then said to be flow equivariant if, when the input sequence undergoes flow, the output sequence also transforms according to the action of flow, i.e. (cid:1) (1) where the action of the flow on signal ft over the group is defined as the left action: ψt(ν) ft(g) := ft(ψt(ν)1 g). In the case of partial observability, we can modify Equation 1 = ψt(ν) Φ (cid:0){fi}T Φ (cid:0){ψi(ν) fi}T t, i=0 i=0 (cid:1) 3 Preprint to describe flow transformations acting on global world state, rather than directly on observations, since objects can move while unobserved and flow equivariant world models should still represent this action faithfully. Formally, we can express this by adding an additional observation function O(wi) which maps from the global world state (wi) to the agents current view (fi), and redefining flow equivariance with respect to this world state: Φ (cid:0){O (ψi(ν) wi)}T i=0 (cid:1) = ψt(ν) Φ (cid:0){O (wi)}T i= (cid:1) t, (2) In practice, this is mainly formality, but means output of the sequence model Φ is now defined to be equivariant with respect to the full world state, implying that it is structured to represent more than just the current observation but instead faithful memory map of the dynamic environment. To achieve flow equivariance, Keller (2025) demonstrated that it is sufficient to perform computation in the co-moving reference frame of the input. In other words, for simple Recurrent Neural Network (RNN), the hidden state must flow in unison with the input, i.e. ht+t = σ (ψt(ν) ht + ft) . (3) To achieve equivariance with respect to set of multiple flows (ν ), Flow Equivariant RNNs possess multiple hidden state velocity channels, each flowing according to their own vector fields ν (denoted as ht(ν)), illustrated as stacked rows in Fig. 3(a). Because of the fact that the elements of the Lie algebra combine in structured manner, it is then possible to show that when the input sequence is acted on by flow ψ(ˆν), the hidden state outputs also flow, and these velocity channels permute according to the difference between their velocity and the input velocity (ν ˆν): ht[ψ(ˆν) ](ν) = ψt1(ˆν) ht[f ](ν ˆν) t. (4) In the following subsection, we will propose that in order to gain the efficiency and robustness benefits of equivariance in the world modeling setting, the hidden state or memory of world model can be group-structured with respect to both the group of the agents actions, and the group which defines the abstract motions of other objects in the world. We can then act on this memory with the inverse of the representation of the agents action in the output space (T 1 action), introducing equivariance of this memory with respect to self-motion, while the internal velocity channels handle equivariance with respect to external motion. Fundamentally, this self-motion equivariance enforces the closure of group operations, such that if set of actions brings an agent back to previously observed location, the representation will necessarily be the same. As we will show empirically, this addresses the above described challenge of modeling out-ofview dynamics. Generalized Flow Equivariant Recurrence Relation. To support more complex tasks, such as 3D partially observed world modeling, we introduce an abstract version of the flow equivariant recurrence relation which supports arbitrary encoders and update operations. Specifically, we define our abstract observation encoder as Eθ[ft; ht], function of the current observation ft and the prior hidden state ht; and we define our abstract recurrent update operation as ht+1 = Uθ[ht; ot], function of the encoded observation (ot = Eθ[ft; ht]) and the past hidden state. The internal velocity channels flow for one timestep via the action of ψ1. Putting them together, the new generalized flow equivariant recurrence relation can then be written in as: Figure 3: Visualization of the Simple Recurrent FloWM on MNIST World. FloWM Recurrence relation. Velocity channels are plotted as rows, with the read-in and read-out part of the hidden state in blue. ht+1(ν) = ψ1(ν) Uθ (cid:2)ht(ν); Eθ [ft; ht] (ν)(cid:3) (5) 4 Preprint To prove that this is indeed still flow equivariant, we require the following properties for the encoder and update mechanism. Specifically, both the encoder and update operations must be equivariant with respect to transformations on their inputs: Eθ [g ft; ht] = Eθ [ft; ht] & Uθ [g ht; ot] = Uθ [ht; ot] (6) Secondly, we also require that the Encoder performs trivial lift of the input to all velocity channels, such that: Eθ [ft; ht] (ν) = Eθ [ft; ht] (ˆν) ν, ˆν g. In Appendix Section A, we prove formally that this framework indeed retains the flow equivariance properties of the original Flow Equivariant RNN, given that these assertions hold. We note that flow equivariance is typically defined fully observed environments, and we therefore maintain this in our proofs. Self-Motion Equivariance. In this work, we leverage the fact that motion is relative (i.e. selfmotion is equivalent to the motion of the input) to additionally achieve equivariance to self-motion in unified manner with the core difference being that self-motion is accompanied by known action variable (at) between the intervening observations. This additional information allows us to build world model which operates in the co-moving reference frame of the agent, thereby achieving self-motion equivariance, without any additional velocity channels we call this model FloWM. Specifically, given the action variable at, denoting the action of the agent between observations ft and ft+1, we transform the hidden state of the network to flow according to the latent group representation of the action. We assert the representation of the action on this hidden state is known, denoted Tat, resulting in the following Self-Motion Flow Equivariant Recurrence Relation: Next Latent Prediction Internal Flow Transform Latent Defined over ν (cid:122) (cid:125)(cid:124) (cid:123) ht+1(ν) = 1 at (cid:124)(cid:123)(cid:122)(cid:125) Self Action Transform (cid:122) (cid:125)(cid:124) (cid:123) ψ1(ν) Uθ (cid:124)(cid:123)(cid:122)(cid:125) Update Memory (cid:122) (cid:125)(cid:124) (cid:123) (cid:2) (cid:3). ht(ν); Eθ[ft, ht] (ν) (cid:125) (cid:124) (cid:123)(cid:122) Encoder Output (7) In the case when the action space is the 2D translation group (such as in our MNIST World experiments in the following section), the representation Tat takes the form of another Action Flow (ψ1(at)) describing the visual flow induced by the action in the agents reference frame. By the properties of flows, this then combines with the Internal Flows (ψ1(ν)) of the velocity channels, yielding simple combined flow, ψ1(ν at), in the recurrence. When the action space is more sophisticated, such as involving rotations, the representation acts directly on the spatial dimensions and velocity channels of the hidden state itself. In the following paragraphs we describe precisely how these abstract elements are instantiated for each of the datasets we explore in this study. 3.2 INSTANTIATIONS FOR 2D / 3D PARTIALLY OBSERVED DYNAMIC WORLD MODELING Simple Recurrent FloWM. For the first set of experiments, to validate our framework in 2D environment, we extend the model of Keller (2025) with the unified self-motion equivariance introduced above (depicted in Figure 3). Explicitly, this yields the following recurrence: ht+1(ν) = ψ1(ν at) σ(cid:0)W ht(ν) + pad( ft)(cid:1), where ht, and ft denote convolutions over the hidden state and input spatial dimensions. To model partial observability, we simply write-in-to (denoted pad()), and read-out-from, fixed window size < world size portion of the hidden state (blue dashed square in Fig. 3(a)), letting the rest of the hidden state flow around the agents field of view according to ψ1(ν at). In particular, the hidden state is windowed at each timestep, pixel-wise max-pooled over velocity channels and passed through decoder gθ to predict the next observation, explicitly: (cid:0)maxν(window(ht+1))(cid:1). We see that this is equivalent to an instantiation of our general ˆft+1 = gθ framework with Eθ[ft; ht](ν) = ft, and Uθ[ht; ot] = σ(W ht + pad(ot)) where all operations are equivariant to translation, and thus satisfy the conditions of Equation 6. (8) Transformer-Based FloWM. To extend our FloWM framework to work with more complex datasets, such as our second set of experiments involving 3D world with other moving elements, 5 Preprint Figure 4: Transformer-Based FloWM. a) Image observation ft at time and FoV selected map latents ht are passed through ViT encoder Eθ. Latent map ht is fully learned, visualized as map here for clarity. b) Write to ht at the FoV locations, then transform latent map according to known action at and internal flow ψ1(ν), producing ht+1. c) Decode using cross attention over FoV of ht+1 with ViT decoder Dθ to predict next image ˆft+1. we construct second instantiation of the FloWM with Vision Transformer (ViT) (Dosovitskiy et al., 2021) based encoder and decoder, depicted in Fig. 4 (note that it is still recurrent model, but each steps update computed with transformer, rather than simple addition of liner maps). Specifically, in this setting ht is set of spatially organized token embeddings that act as group-structured latent map. In the spirit of Ha & Schmidhuber (2018), we set this map to be top-down 2D abstract version of the true 3-dimensional environment the agent inhabits. We denote this set of tokens ht := {h(x,y) (x, y) [0, ) [0, H)} where (x, y) are the spatial coordinates of the token. Most importantly, this map is group-structured with respect to the agents action group (2D translation and 90-degree rotation), and the group of external object motion (2D translation), giving us known form of the representation of these group elements in the latent map, Tg. The goal of the encoder Eθ[ft; ht], instantiated as ViT, is then to take the tokens of the map corresponding to the current field of view, and update them using the image patch tokens (patchify(ft)). Explicitly: Eθ[ft; ht] = ViT[concat[patchify(ft); FoV(ht)]] = ot, where FoV(ht) returns fixed subset of ht corresponding to the 2D triangular wedge field of view of the agent, depicted in Fig. 4. We highlight that the coordinates of this set are fixed since the map is always egocentric, shifting and rotating around the agent in the center. The update operation Uθ[ht; ot] then simply performs gated combination of the output of the encoder (map token latent positions only), and the previous values of the hidden state FoV(ht). Explicitly, matching our framework: Uθ[ht; ot](x,y) = (cid:40) (1 α) h(x,y) h(x,y) + α o(x,y) if x, FoV otherwise , (9) ; o(x,y) where α = σ(W concat[h(x,y) ]) for some learnable gating weights W. We can see that the update operation Uθ is indeed equivariant with respect to shifts or rotations of the spatial coordinates of its inputs, satisfying equation 6 for Uθ. However, since the encoder must map from the 3D first person point of view of the agent, to an abstract top-down map, it is highly non-trivial to make this transformation exactly action equivariant by design (without relying on explicit depth unprojection). Therefore, instead, we simply treat the output of the encoder as if it were equivariant in the recurrence relation, and anticipate that the transformation 1 ψ1(ν) between timesteps will at encourage the encoder to learn to become equivariant, as has been demonstrated in prior work (Keller & Welling, 2022; Keurti et al., 2024). As we will demonstrate empirically in the following section, in practice this appears to hold. We provide more model details in Appendix E, and in Section 5 we review related models that have similarly structured representations with respect to self-motion, but may be seen as special cases of this framework without input-flow equivariance. 6 Preprint"
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "In this section we present our 2D and 3D partially observable dynamic world modeling benchmarks, and compare the proposed FloWM framework against state-of-the-art video diffusion world models and ablations of our FloWM. Our results on these datasets validate that unified self-motion and external flow equivariance are useful for modeling dynamics out of the field of view, whereas current world model formulations struggle due to their lack of unified memory and inability to model dynamics naturally. Links to model and dataset code are available on the project website here. 4.1 DIFFUSION-BASED BASELINES Diffusion Forcing Transformer. Due to its claims of long term consistency and flexible autoregressive inference, we chose History-guided Diffusion Forcing as the baseline training method, using latent diffusion with CogVideoX-style transformer backbone, which we will call here DFoT (Song et al., 2025; Chen et al., 2024; Yang et al., 2025). For each dataset, we first train spatial downsampling VAE to encode video frames into latent representation before being fed to diffusion model. Unlike FloWM recurrent models, and due to the diffusion forcing objective, during training for DFoT we make no distinction between observation and prediction frames, and train on fixed length sequences in the self-attention window. To condition on actions, we follow CogVideoX-style action conditioning, including the embedded action sequence in the self-attention window. During inference, we maintain context frames while denoising the prediction frames; the prediction frames begin at full noise and are gradually denoised until they are clean with the diffusion model, and then the sliding window advances by the number of predicted frames. More details, discussion, and training settings for DFoT can be found in Appendix G.2. Diffusion Forcing State Space Model. As representative comparative work on memoryaugmented video diffusion models, we compare against recently proposed approach (Po et al., 2025) that combines short horizon Diffusion forcing transformer (for local consistency) with blockwise scan State Space Model module (for long horizon memory) (Dao & Gu, 2024). This model employs the same VAE as DFoT across all datasets to operate in latent space. Differing from DFoT, there are some number of clean context frames during training, and the prediction frames are predicted according to the diffusion forcing objective, encouraging the model to leverage the clean context (Po et al., 2025). During inference, we follow the same procedure as in DFoT, and refer to this model as DFoT-SSM. Further details are provided in Appendix G.4. 4.2 2D MNIST WORLD BENCHMARK Dataset To test our architecture on partially observable dynamic world modeling, we propose simple MNIST World dataset. The world is 2D black canvas with multiple MNIST digits moving at random constant velocities. The agent is provided view of the world, smaller than the world size, yielding partial observability. At each discrete timestep, the world evolves according to the velocity of each object, and the agent takes random action (relative (x, y) offset) to move its viewpoint. The edges roll, so digit moving off the screen to the left will reappear on the right. Given 50 observation frames, the training task is to predict the dynamics played out for 20 future frames, integrating future self-motion (given) and world dynamics. To test length generalization, we additionally run inference with 50 observation rames and 150 prediction frames. We include ablations on data subsets with different combinations of partial observability, object dynamics, and self-motion in Appendix D. Results On the MNIST World dataset, we train and evaluate the Simple Recurrent FloWM introduced in Section 3.2, which includes velocity channels (VC) and self-motion equivariance (SME). We also include ablations FloWM (no VC), FloWM (no VC, no SME), and the diffusion baselines mentioned above. We note here that FloWM (no VC, no SME) is just simple convolutional RNN. More training and model details are available in Appendices and G.2. At each timestep, we calculate various quality metrics for the predicted frames for each model with respect to the ground truth, reported in Table 1. Example rollouts and full world view visualizations are available in Fig. 5(a). Predictions from the FloWM remain consistent with the motion of objects out of its view for 150 timesteps past the observation window, well beyond its training prediction horizon of 20 timesteps, while FloWM (no VC, no SME) fails. We find that the FloWM (no VC) can still somewhat learn to 7 Preprint Figure 5: Dynamic MNIST World Prediction Rollouts a) Timesteps 0 to 49 are given as observations. Models are trained to predict up to = 69. Note that FloWM does not diverge even at timestep 199, while baselines slowly degrade in image quality or lose track of the digits. b) MSE over different length rollouts show length generalization. c) Learning efficiency of the FloWM. Table 1: Rollout performance on 2D Dynamic MNIST World with partial observability. Models are evaluated by conditioning on 50 context frames, then generating 20 (training length) and 150 (length generalization) future frames respectively. Model MSE PSNR SSIM 150 20 150 20 150 FloWM (Ours) (no VC) (no SME) (no SME, no VC) 0.0005 0.0018 32.99 27.56 0.9900 0.9813 0.0041 0.0334 23.83 14.77 0.9576 0.7729 0.1234 0.1317 9.088 8.805 0.0366 0.0127 0.1233 0.1333 9.091 8.751 0.0374 0.0146 (+ action-concat) 0.1125 0.1359 9.491 8.669 0.0623 0.0149 0.1448 0.2111 8.394 6.755 0.4045 0.2434 0.1277 0.1688 8.940 7.726 0.4550 0.3146 0.1656 0.1654 7.810 7.814 0.3573 0.3564 DFoT DFoT-SSM All-Black Baseline model unobserved dynamics, especially within its training window, but drifts over time; length extrapolation abilities are presented in Fig. 5(b). We further find that models combining SME and VC require orders of magnitude less training steps to converge compared with those without these priors, shown in Fig. 5(c). The DFoT models predictions quickly diverge from the ground truth, even within its training window, just generating plausible digit-like artifacts. The DFoT-SSM models predictions show the digits slowly fading to black. Through additional results in Appendix D, we explore how the DFoT model can sometimes handle partial observability, object dynamics, and self-motion individually, but not in any combination. 4.3 3D DYNAMIC BLOCK WORLD BENCHMARK Dataset Reasoning about the dynamics of the 3D world from 2D image observations requires approximating unprojection of egocentric views to world-centric representation. To validate FloWM on this more difficult setting, we further introduce simple 3D dataset, built in the Miniworld environment (Chevalier-Boisvert et al., 2023). An agent is spawned in random position in square room, along with colored blocks initialized with random positions and velocities. At each timestep, the blocks evolve according to their velocities, and the agent takes one of four discrete actions: turn left, turn right, move straight, or do nothing. The blocks bounce when encountering wall, making the task of modeling dynamics out of view significantly harder. The data-generating agent follows biased random exploration strategy, sometimes pausing to observe the rooms dynamics from next to the wall. Videos of the agents observations are collected; similar to the MNIST World setup, during training the world model is given 50 observation frames as context and must predict the dynamics evolving over the next 90 steps, given the agents actions. More details, including ablations 8 Preprint Table 2: Rollout metrics on 3D Dynamic Block World. Given 70 context frames, models are evaluated on generating 70 (training objective) and 210 (length extrapolation) future prediction frames. Model MSE PSNR SSIM 70 210 70 70 210 FloWM (Ours) 0.000603 0.001539 32.19 28.13 0.9673 0.9525 0.007615 0.009614 21.18 20.17 0.9045 0.8935 (no VC) (no SME, no VC) 0.009579 0.012625 20.19 18.99 0.8782 0.8631 0.011759 0.021684 19.30 16.64 0.9377 0.8885 0.022616 0.022570 16.46 16.46 0.8877 0.8879 DFoT DFoT-SSM Figure 6: Mean model prediction error per timestep on 3D Dynamic Block World Figure 7: Dynamic Block World Prediction Visualizations. Timesteps 0 to 49 are given as observations for this visualization. Note that FloWM stays consistent until the final frame, while the baselines hallucinate object position and color. The ablations are unable to remain consistent. involving static version of Block World are included in Appendix C, and more training details are available in Appendix E. Results On the 3D Dynamic Block World dataset, we compare our Transformer-Based FloWM from Section 3.2 and Fig. 4 with the diffusion baseline, also including the ablations FloWM (no VC) and FloWM (no VC, no SME). We report the metrics on rollouts of 70 and 210 prediction frames, given 70 frames of context in Table 21. Example rollouts are visualized in Figure 7 and on the website here. Similar to the 2D experiments, we observe the FloWMs predictions are able to remain consistent for as many as 210 frames of future prediction, while the baselines and ablations are not. The average prediction error through time is represented in Figure 7, demonstrating FloWMs consistent rollouts through long horizons. Perceptually, DFoT and SSM model predictions frequently hallucinate new objects and forget old ones, aligning with the hypothesis that their architectures are not well suited for partially observable dynamic environments. In addition to the standard, simpler 3D Dynamic Block World dataset, we evaluate our model and baselines on significantly more visually difficult Textured 3D Dynamic Block World dataset, where block, wall, and floor textures are randomly assigned for each example. The same gap holds, demonstrating that FloWM is additionally applicable to more visually realistic settings. Results for this set are available in Appendix C."
        },
        {
            "title": "5 RELATED WORK",
            "content": "Generative World Modeling with Memory. As mentioned in Section 2, there have been few prior works that augment diffusion video models to improve long horizon consistency and go be1We report results on 70 frames of context to match the DFoT-SSM training requirements better, and find either 50 or 70 frames of context produce similar results; further explanation is in Appendix G.4. 9 Preprint yond the limited context window of transformer-based diffusion models. Here, we will discuss the ones most relevant to this work in detail, and leave broader related work to Appendix B. To begin, all work mentioned here focuses on static scenes only, and conditions on actions through token-based conditioning, lacking any way to harmoniously integrate the agents actions over time. The baseline DFoT-SSM model incorporates recurrent SSM backbone for the purposes of long-context consistency and memory; however, the SSM is primarily remembering viewpoint-dependent observations, similar to extending the self-attention context window, and does not have any explicit computation steps for predicting the state of the world out of the view of the agent (Po et al., 2025; Dao & Gu, 2024). As shown in our work, these drawbacks limit the ability for that class of model to perform accurate dynamics prediction. Another recently developed, separate model, WORLDMEM augments diffusion video models by placing past image observations in memory bank for later retrieval based on the camera position (Xiao et al., 2025). An existing ablation of their model in their paper involving camera view prediction may encourage the model to learn how to integrate its global position in time, similar to the effect of self-motion equivariance in our work, but their memory mechanism is fundamentally different, relying on self-attention to integrate information for self-consistency. Their retrieval mechanism also memorizes viewpoint variant information and would be ill suited for dynamic prediction. Recent work from Zhou et al. (2025) maintains 3d voxel map of the environment, later retrieved to condition diffusion generation. However, their method relies on depth unprojection, the voxels are updated via max pooling instead of more flexible recurrence relation, and are prohibitively expensive for large hidden state sizes. Equivariant World Modeling. Perhaps most related to our proposed FloWM are world modeling frameworks with similarly structured map memories. For example, Neural Map (Parisotto & Salakhutdinov, 2017) introduced spatially organized 2D memory that stores observations at estimated agent coordinates. The storage location of these observations is shifted precisely according to the agents actions, yielding an effectively equivariant allocentric latent map. In Section 5 of the paper, the authors describe an egocentric version of their model which can in fact be seen as special case of our FloWM, specifically equivalent to the ablation without velocity channels. The authors demonstrate that their allocentric map enables long-term recall and generalization in navigation tasks. In similar vein, EgoMap (Beeching et al., 2020) leverages inverse perspective transformations to map from observations in 3D environments to top-down egocentric map. This work also explicitly transforms the latent map in an action-conditioned manner, although the transformation is learned with Spatial Transformer Network, making it only approximately equivariant. Our work can be seen to formalize these early models in the framework of group theory, allowing us to extend the action space beyond just spatial translation to any Lie group and any world space. For example, our framework can theoretically support full 3-dimensional neural maps without problem, following the framework of flow equivariance. Finally, there are few other works that discuss equivariant world modeling, but are less precisely related to our own. Specifically, (van der Pol et al., 2021) was one of the first works to build equivariant policy and value networks for reinforcement learning, but not with respect to motion, instead with respect to the symmetries of the environment (such as static rotations or translations). More recent work (Park et al., 2022; Ghaemi et al., 2025) proposes to approach the goal of building equivariant world models in more approximate manner by conditioning or encouraging equivariance through training losses, rather than our approach which builds it in explicitly."
        },
        {
            "title": "6 LIMITATIONS & FUTURE WORK.",
            "content": "Our current instantiations of FloWM target environments where both the agent and objects undergo relatively simple, rigid motions under known action parameterization. This setting lets us isolate the role of the flow equivariant memory under partial observability, but it does not yet capture richer non-rigid or semantic actions (e.g. articulated bodies, deforming objects, or discrete semantic actions such as open door or pick up object). Extending the flow equivariant recurrence to actions that live in more expressive latent groups or hierarchies, and to domains where agent actions are semantic rather than purely geometric, is an important direction for future work. Our experiments also focus on deterministic dynamics given an action sequence, and we train FloWM with single-step reconstruction loss to predict single future rollout. We view the ability to model deterministic trajectories as prerequisite to eventually model stochastic ones. The same 10 Preprint flow equivariant latent map could in principle be combined with stochastic latent variables to enable stochastic dynamic prediction, representing another interesting line of future work. Training and inference compute requirements for FloWM remain within the same order of magnitude as the strongest baselines despite maintaining recurrent state. We report compute metrics and suggest several future directions to further improve efficiency in Appendix H. There are few architectural limitations to note in this current study. As noted, our 3D FloWM ViT Encoder is not analytically equivariant with respect to 3D transformations. While the model appears to learn this equivariance over time, and therefore still benefits from the group-structured hidden state, we observe that this results in slower learning initially until point where approximate equivariance appears learned, and the model can leverage the velocity channels properly. Future work that incorporates proper analytically equivariant 3D encoder would likely observe significantly faster training speeds and lower loss, akin to the performance we report on the 2D dataset. Second, Flow Equivariance to date has only been developed with respect to discrete sets of flows , while real world velocities may span continuous range. However, prior work has repeatedly demonstrated empirically that even equivariance to small discretized groups yields performance improvements on data that is symmetric with respect to the full group (Cohen & Welling, 2016; Kuipers & Bekkers, 2023). Recent theoretical work has further characterized the value of such approximate or partial group equivariance, demonstrating the value that such methods hold even if not exact (Petrache & Trivedi, 2025). Future work to extend flow equivariance to continuous velocities would regardless still hold significant value. Lastly, our latent world map is egocentric and fixed in spatial extent and resolution. While the textured Block World results indicate that FloWM can handle increased visual complexity, scaling to more realistic, open world scenes will likely require variable sized maps and stronger perceptual backbones. Finally, we believe complementary line of work is to combine FloWM with insights from nongenerative world models. In this paper, we refer to embodied world models as an egocentric sensory stream paired with self motion actions, and evaluate FloWM purely as predictive video model. In the future, the flow equivariant latent memory that we introduce in this paper could be used as representation backbone within JEPA or TDMPC2 style world model to improve long horizon dynamics predictions (LeCun & Courant, 2022; Hansen et al., 2024). Together, they could represent major step forward to tackle downstream embodied tasks such as autonomous driving, robotic manipulation, or game environments through planning over the model predictions or latent space."
        },
        {
            "title": "7 CONCLUSION",
            "content": "In this work, we have introduced Flow Equivariant World Models, new framework unifying both internally and externally generated motion for more accurate and efficient world modeling in partially observable settings with dynamic objects out of the agents view. Our results on both the 2D and 3D datasets with these properties demonstrate the potential of flow equivariance, and highlight the limitations with current state-of-the-art diffusion-based video world models. Specifically, we find that flow equivariant world models are able to represent motion in structured symmetric manner, permitting faster learning, lower error, fewer hallucinations, and more stable rollouts far beyond the training length. We believe this work lays the theoretical groundwork along with empirical validation to support the potential for novel symmetry-structured approach for efficient and effective world modeling."
        },
        {
            "title": "8 ACKNOWLEDGMENTS",
            "content": "The authors would like to thank Domas Buracas, Zhiyi Li, Nicklas Hansen, Nathan Cloos, Christian Shewmake, William Chung, and Kirill Dubovitskiy for their helpful discussion and feedback about early versions of this work. This work has been made possible in part by gift from the Chan Zuckerberg Initiative Foundation to establish the Kempner Institute for the Study of Natural and Artificial Intelligence at Harvard University. 11 Preprint"
        },
        {
            "title": "REFERENCES",
            "content": "Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025. Mido Assran, Adrien Bardes, David Fan, Quentin Garrido, Russell Howes, Mojtaba, Komeili, Matthew Muckley, Ammar Rizvi, Claire Roberts, Koustuv Sinha, Artem Zholus, Sergio Arnaud, Abha Gejji, Ada Martin, Francois Robert Hogan, Daniel Dugas, Piotr Bojanowski, Vasil Khalidov, Patrick Labatut, Francisco Massa, Marc Szafraniec, Kapil Krishnakumar, Yong Li, Xiaodong Ma, Sarath Chandar, Franziska Meier, Yann LeCun, Michael Rabbat, and Nicolas Ballas. V-jepa 2: Self-supervised video models enable understanding, prediction and planning, 2025. URL https://arxiv.org/abs/2506.09985. Philip J. Ball, Jakob Bauer, Frank Belletti, Bethanie Brownfield, Ariel Ephrat, Shlomi Fruchter, Agrim Gupta, Kristian Holsheimer, Aleksander Holynski, Jiri Hron, Christos Kaplanis, Marjorie Limont, Matt McGill, Yanko Oliveira, Jack Parker-Holder, Frank Perbet, Guy Scully, Jeremy Shar, Stephen Spencer, Omer Tov, Ruben Villegas, Emma Wang, Jessica Yung, Cip Baetu, Jordi Berbel, David Bridson, Jake Bruce, Gavin Buttimore, Sarah Chakera, Bilva Chandra, Paul Collins, Alex Cullum, Bogdan Damoc, Vibha Dasagi, Maxime Gazeau, Charles Gbadamosi, Woohyun Han, Ed Hirst, Ashyana Kachra, Lucie Kerley, Kristian Kjems, Eva Knoepfel, Vika Koriakin, Jessica Lo, Cong Lu, Zeb Mehring, Alex Moufarek, Henna Nandwani, Valeria Oliveira, Fabio Pardo, Jane Park, Andrew Pierson, Ben Poole, Helen Ran, Tim Salimans, Manuel Sanchez, Igor Saprykin, Amy Shen, Sailesh Sidhwani, Duncan Smith, Joe Stanton, Hamish Tomlinson, Dimple Vijaykumar, Luyu Wang, Piers Wingfield, Nat Wong, Keyang Xu, Christopher Yew, Nick Young, Vadim Zubov, Douglas Eck, Dumitru Erhan, Koray Kavukcuoglu, Demis Hassabis, Zoubin Gharamani, Raia Hadsell, Aaron van den Oord, Inbar Mosseri, Adrian Bolton, Satinder Singh, and Tim Rocktaschel. Genie 3: new frontier for world models. 2025. Simon Batzner, Albert Musaelian, Lixin Sun, Mario Geiger, Jonathan P. Mailoa, Mordechai Kornbluth, Nicola Molinari, Tess E. Smidt, and Boris Kozinsky. E(3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials. Nature Communications, 13 (1):2453, 2022. doi: 10.1038/s41467-022-29939-5. URL https://doi.org/10.1038/ s41467-022-29939-5. Edward Beeching, Christian Wolf, Jilles Dibangoye, and Olivier Simonin. Egomap: Projective mapping and structured egocentric memory for deep rl, 2020. URL https://arxiv.org/ abs/2002.02286. Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. URL https://openai.com/research/ video-generation-models-as-world-simulators. Stanley H. Chan. Tutorial on diffusion models for imaging and vision, 2025. URL https:// arxiv.org/abs/2403.18103. Boyuan Chen, Diego Marti Monso, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion, 2024. URL https://arxiv.org/abs/2407.01392. Maxime Chevalier-Boisvert, Bolun Dai, Mark Towers, Rodrigo de Lazcano, Lucas Willems, Salem Lahlou, Suman Pal, Pablo Samuel Castro, and Jordan Terry. Minigrid & miniworld: Modular & customizable reinforcement learning environments for goal-oriented tasks, 2023. URL https: //arxiv.org/abs/2306.13831. Taco Cohen and Max Welling. Group equivariant convolutional networks. In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pp. 29902999, New York, New York, USA, 2022 Jun 2016. PMLR. URL https://proceedings.mlr.press/ v48/cohenc16.html. Preprint Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. arXiv preprint arXiv:2405.21060, 2024. Etched Decart, Quinn McIntyre, Spruce Campbell, Xinlei Chen, and Robert Wachen. Oasis: universe in transformer. URL: https://oasis-model. github. io, 2024. William Dorrell, Peter E. Latham, Timothy E. J. Behrens, and James C. R. Whittington. Actionable neural representations: Grid cells from minimal constraints, 2023. URL https://arxiv. org/abs/2209.15563. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale, 2021. URL https://arxiv.org/abs/2010.11929. Ruili Feng, Han Zhang, Zhantao Yang, Jie Xiao, Zhilei Shu, Zhiheng Liu, Andy Zheng, Yukun Huang, Yu Liu, and Hongyang Zhang. The matrix: Infinite-horizon world generation with realtime moving control, 2024. URL https://arxiv.org/abs/2412.03568. Hafez Ghaemi, Eilif Muller, and Shahab Bakhtiari. seq-jepa: Autoregressive predictive learning of invariant-equivariant world models, 2025. URL https://arxiv.org/abs/2505.03176. Junliang Guo, Yang Ye, Tianyu He, Haoyu Wu, Yushu Jiang, Tim Pearce, and Jiang Bian. Mineworld: real-time and open-source interactive world model on minecraft, 2025. URL https://arxiv.org/abs/2504.08388. Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Li Fei-Fei, Irfan Essa, Lu Jiang, and Jose Lezama. Photorealistic video generation with diffusion models, 2023. URL https: //arxiv.org/abs/2312.06662. David Ha and Jurgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2(3), 2018. Nicklas Hansen, Hao Su, and Xiaolong Wang. Td-mpc2: Scalable, robust world models for continuous control, 2024. URL https://arxiv.org/abs/2310.16828. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners, 2021. URL https://arxiv.org/abs/2111. 06377. Xianglong He, Chunli Peng, Zexiang Liu, Boyang Wang, Yifan Zhang, Qi Cui, Fei Kang, Biao Jiang, Mengyin An, Yangyang Ren, et al. Matrix-game 2.0: An open-source, real-time, and streaming interactive world model. arXiv preprint arXiv:2508.13009, 2025. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance, 2022. URL https://arxiv. org/abs/2207.12598. Emiel Hoogeboom, Thomas Mensink, Jonathan Heek, Kay Lamerigts, Ruiqi Gao, and Tim Salimans. Simpler diffusion (sid2): 1.5 fid on imagenet512 with pixel-space diffusion. arXiv preprint arXiv:2410.19324, 2024. Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, and Eli Shechtman. Self forcing: Bridging the train-test gap in autoregressive video diffusion, 2025. URL https://arxiv.org/abs/ 2506.08009. Georg Keller, Tobias Bonhoeffer, and Mark Hubener. Sensorimotor mismatch signals in primary visual cortex of the behaving mouse. Neuron, 74(5):809815, 2012. T. Anderson Keller. Flow equivariant recurrent neural networks, 2025. URL https://arxiv. org/abs/2507.14793. T. Anderson Keller and Max Welling. Topographic vaes learn equivariant capsules, 2022. URL https://arxiv.org/abs/2109.01394. 13 Preprint Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering, 2023. URL https://arxiv.org/abs/2308. 04079. Hamza Keurti, Hsiao-Ru Pan, Michel Besserve, Benjamin F. Grewe, and Bernhard Scholkopf. Homomorphism autoencoder learning group structured representations from observed transitions, 2024. URL https://arxiv.org/abs/2207.12067. Diederik P. Kingma and Ruiqi Gao. Understanding diffusion objectives as the elbo with simple data augmentation, 2023. URL https://arxiv.org/abs/2303.00848. Thijs Kuipers and Erik Bekkers. Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis, pp. 252261. 10 2023. ISBN 978-3-031-43897-4. doi: 10.1007/978-3-031-43898-1 25. Yann LeCun and Courant. path towards autonomous machine intelligence version 0.9.2, 202206-27. 2022. URL https://api.semanticscholar.org/CorpusID:251881108. Marcus Leinweber, Daniel Ward, Jan Sobczak, Alexander Attinger, and Georg Keller. sensorimotor circuit in mouse cortex for visual flow predictions. Neuron, 95(6):14201432, 2017. Isabella Liu, Hao Su, and Xiaolong Wang. Dynamic gaussians mesh: Consistent mesh reconstruction from dynamic scenes, 2025. URL https://arxiv.org/abs/2404.12379. Eric Martin and Chris Cundy. Parallelizing linear recurrent neural nets over sequence length, 2018. URL https://arxiv.org/abs/1709.04057. Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis, 2020. URL https://arxiv.org/abs/2003.08934. Emilio Parisotto and Ruslan Salakhutdinov. Neural map: Structured memory for deep reinforcement learning, 2017. URL https://arxiv.org/abs/1702.08360. Jung Yeon Park, Ondrej Biza, Linfeng Zhao, Jan Willem van de Meent, and Robin Walters. Learning symmetric embeddings for equivariant world models, 2022. URL https://arxiv.org/ abs/2204.11371. William Peebles and Saining Xie. Scalable diffusion models with transformers, 2023. URL https: //arxiv.org/abs/2212.09748. Mircea Petrache and Shubhendu Trivedi. Approximation-generalization trade-offs under (approximate) group equivariance, 2025. URL https://arxiv.org/abs/2305.17592. Ryan Po, Yotam Nitzan, Richard Zhang, Berlin Chen, Tri Dao, Eli Shechtman, Gordon Wetzstein, and Xun Huang. Long-context state-space video world models, 2025. Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural radiance fields for dynamic scenes, 2020. URL https://arxiv.org/abs/2011.13961. Siamak Ravanbakhsh, Jeff Schneider, and Barnabas Poczos. Equivariance through parametersharing. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 28922901. PMLR, 0611 Aug 2017. URL https://proceedings.mlr.press/v70/ ravanbakhsh17a.html. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models, 2022. URL https://arxiv.org/ abs/2112.10752. Nedko Savov, Naser Kazemi, Deheng Zhang, Danda Pani Paudel, Xi Wang, and Luc Van Gool. arXiv preprint to diffusion world models. Statespacediffuser: Bringing long context arXiv:2505.22246, 2025. Jimmy T. H. Smith, Andrew Warrington, and Scott W. Linderman. Simplified state space layers for sequence modeling, 2023. URL https://arxiv.org/abs/2208.04933. 14 Preprint Kiwhan Song, Boyuan Chen, Max Simchowitz, Yilun Du, Russ Tedrake, and Vincent Sitzmann. History-guided video diffusion, 2025. URL https://arxiv.org/abs/2502.06764. Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges, 2019. URL https://arxiv.org/abs/1812.01717. Elise van der Pol, Daniel E. Worrall, Herke van Hoof, Frans A. Oliehoek, and Max Welling. Mdp homomorphic networks: Group symmetries in reinforcement learning, 2021. URL https: //arxiv.org/abs/2006.16908. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2023. URL https://arxiv. org/abs/1706.03762. Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang. 4d gaussian splatting for real-time dynamic scene rendering, 2024. URL https://arxiv.org/abs/2310.08528. Tong Wu, Shuai Yang, Ryan Po, Yinghao Xu, Ziwei Liu, Dahua Lin, and Gordon Wetzstein. Video world models with long-term spatial memory, 2025. URL https://arxiv.org/abs/ 2506.05284. Jiannan Xiang, Guangyi Liu, Yi Gu, Qiyue Gao, Yuting Ning, Yuheng Zha, Zeyu Feng, Tianhua Tao, Shibo Hao, Yemin Shi, Zhengzhong Liu, Eric P. Xing, and Zhiting Hu. Pandora: Towards general world model with natural language actions and video states, 2024. URL https:// arxiv.org/abs/2406.09455. Jiannan Xiang, Yi Gu, Zihan Liu, Zeyu Feng, Qiyue Gao, Yiyan Hu, Benhao Huang, Guangyi Liu, Yichi Yang, Kun Zhou, Davit Abrahamyan, Arif Ahmad, Ganesh Bannur, Junrong Chen, Kimi Chen, Mingkai Deng, Ruobing Han, Xinqi Huang, Haoqiang Kang, Zheqi Liu, Enze Ma, Hector Ren, Yashowardhan Shinde, Rohan Shingre, Ramsundar Tanikella, Kaiming Tao, Dequan Yang, Xinle Yu, Cong Zeng, Binglin Zhou, Zhengzhong Liu, Zhiting Hu, and Eric P. Xing. Pan: world model for general, interactable, and long-horizon world simulation, 2025. URL https: //arxiv.org/abs/2511.09057. Zeqi Xiao, Yushi Lan, Yifan Zhou, Wenqi Ouyang, Shuai Yang, Yanhong Zeng, and Xingang Pan. Worldmem: Long-term consistent world simulation with memory, 2025. URL https: //arxiv.org/abs/2504.12369. Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Yuxuan Zhang, Weihan Wang, Yean Cheng, Bin Xu, Xiaotao Gu, Yuxiao Dong, and Jie Tang. Cogvideox: Text-to-video diffusion models with an expert transformer, 2025. URL https://arxiv.org/abs/2408.06072. Tianwei Yin, Qiang Zhang, Richard Zhang, William T. Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From slow bidirectional to fast autoregressive video diffusion models, 2025. URL https://arxiv.org/abs/2412.07772. Lvmin Zhang, Shengqu Cai, Muyang Li, Gordon Wetzstein, and Maneesh Agrawala. Frame context packing and drift prevention in next-frame-prediction video diffusion models, 2025a. URL https://arxiv.org/abs/2504.12626. Peiyuan Zhang, Yongqi Chen, Haofeng Huang, Will Lin, Zhengzhong Liu, Ion Stoica, Eric Xing, and Hao Zhang. Vsa: Faster video diffusion with trainable sparse attention, 2025b. URL https: //arxiv.org/abs/2505.13389. Xingguang Zhong, Yue Pan, Jens Behley, and Cyrill Stachniss. Shine-mapping: Large-scale 3d mapping using sparse hierarchical implicit neural representations, 2023. URL https: //arxiv.org/abs/2210.02299. Siyuan Zhou, Yilun Du, Yuncong Yang, Lei Han, Peihao Chen, Dit-Yan Yeung, and Chuang Gan. Learning 3d persistent embodied world models. arXiv preprint arXiv:2505.05495, 2025. 15 Preprint Zihan Zhu, Songyou Peng, Viktor Larsson, Weiwei Xu, Hujun Bao, Zhaopeng Cui, Martin R. Oswald, and Marc Pollefeys. Nice-slam: Neural implicit scalable encoding for slam, 2022. URL https://arxiv.org/abs/2112.12130. 16 Preprint"
        },
        {
            "title": "A GENERALIZED FLOW EQUIVARIANCE PROOF",
            "content": "In this section, we prove by induction that the generalized Flow Equivariant Recurrence Relation of Equation 5 is indeed flow equivariant, following the proof technique of Keller (2025). First, to restate the problem, we wish to show that for the recurrence defined as follows: ht+1(ν) = ψ1(ν) Uθ (cid:2)ht(ν); Eθ [ft, ht] (ν)(cid:3), (10) if we assume that: 1. The encoder satisfies the trivial lift condition to the velocity field: Eθ [ft; ht] (ν) = Eθ [ft; ht] (ˆν) ν, ˆν 2. The encoder and decoder are both group equivariant with respect to their arguments: Eθ [g ft; ht] = Eθ [ft; ht] & Uθ [g ht; ot] = Uθ [ht; ot] 3. The hidden state is initialized to be constant along the flow dimension and invariant to the flow action: h0(ν) = h0(ν) ν, ν and ψ1(ν) h0(ν) = h0(ν) ν V, then, the follow flow equivariance commutation relation holds: ht[ψ(ˆν) ](ν) = ψt(ˆν) ht[f ](ν ˆν) t. (11) Theorem (The Generalized Flow Equivariant Recurrence Relation of Eqn. 10 is Flow Equivariant). Let h[f ] FK(Y, Z) be the output of the generalized flow equivariant recurrence relation as defined in Equation 10, with hidden-state initialization invariant to the group action and constant in the flow dimension, i.e. h0(ν, g) = h0(ν, g) ν, ν and ψ1(ν) h0(ν, g) = h0(ν, g) ν V, G. Then, h[f ] is flow equivariant with the following representation of the action of the flow in the output space for 1: (ψ(ˆν) h[f ])t(ν, g) = ht[f ](ν ˆν, ψt(ˆν)1 g) (12) We note for the sake of completeness, that this then implies the following equivariance relations: ht[ψ(ˆν) ](ν, g) = ht[f ](ν ˆν, ψt(ˆν)1 g) = ψt(ˆν) ht[f ](ν ˆν, g) (13) Furthermore, in all settings in this work, refers to the 2D translation group, either indexing pixel coordinates (MNIST), or 2D latent map coordinates (Block world) referred to as (x, y) in the main text. We see that, different from the work of Keller (2025), since the generalized recurrence relation in Eqn. 10 applies the flow update after the input has been combined with the hidden state (i.e. outside the Uθ operator), the commutation relation in Eqn. 11 now has index on ψt, instead of 1 as written in equation 4. Proof. (Theorem, Generalized Flow Equivariance) Base Case: The base case is trivially true from the initial condition: h0[ψ(ˆν) f<0](ν, g) = h0[f<0](ν, g) (by initial cond. being independent of input) = h0[f<0](ν ˆν, ψt(ˆν)1 g) (by constant init.) (14) (15) Inductive Step: Assuming ht[ψ(ˆν) ](ν, g) = ψt(ˆν) ht[f ](ν ˆν, g) ν V, G, for some 0, we wish to prove this also holds for + 1: Using the Generalized Flow Recurrence (Eqn. 10) on the transformed input, we get: 17 Preprint (cid:16) ht+1[ψ(ˆν)f ](ν, g) = ψ1(ν)Uθ ht[ψ(ˆν)f ](ν, g) ; Eθ[(ψt(ˆν)ft), ht[ψ(ˆν)f ]](ν) (cid:17) (16) (cid:16) (by inductive hyp.) = ψ1(ν)Uθ ψt(ˆν) ht[f ](ν ˆν, g) ; Eθ[(ψt(ˆν)ft), ψ(ˆν)t ht[f ]](ν) (cid:17) (17) (cid:16) (trivial lift in ν) = ψ1(ν)Uθ ψt(ˆν) ht[f ](ν ˆν, g) ; Eθ[(ψt(ˆν)ft), ψ(ˆν)t ht[f ]](ν ˆν) (cid:17) (cid:16) (equivariance of Eθ) = ψ1(ν)Uθ ψt(ˆν) ht[f ](ν ˆν, g) ; ψt(ˆν)Eθ[ft, ht[f ]](ν ˆν) (cid:17) (cid:16) (equivariance of Uθ) = ψ1(ν)ψt(ˆν)Uθ ht[f ](ν ˆν, g) ; Eθ[ft, ht[f ]](ν ˆν) (cid:17) (cid:16) (flow composition) = ψt+1(ˆν)ψ1(ν ˆν)Uθ ht[f ](ν ˆν, g) ; Eθ[ft, ht[f ]](ν ˆν) (cid:17) (by Eqn. 10) = ψt+1(ˆν)ht+1[f ](ν ˆν, g) = ht+1[f ](cid:0)ν ˆν, ψt+1(ˆν)1 g(cid:1). (18) (19) (20) (21) (22) (23) Thus, assuming the inductive hypothesis for time implies the desired relation at time t+1; together with the base case this completes the induction and proves the Theorem."
        },
        {
            "title": "B ADDITIONAL RELATED WORK",
            "content": "World Models World modeling is commonly framed as learning to predict how an environment evolves over time, conditioned on an agents actions, enabling imagination for data-efficient policy learning and model-based planning (Ha & Schmidhuber, 2018). Recent progress in large scale generative modeling has extended this paradigm to video prediction, yielding generative world models that directly predict future observations in pixel or latent video space (Ball et al., 2025; Agarwal et al., 2025; Xiang et al., 2024; He et al., 2025; Guo et al., 2025; Xiang et al., 2025). When successful, these models capture both the stochasticity and multimodality of environment dynamics, and can be queried to simulate counterfactual futures under different action sequences. Our work also falls in this category; the prediction target is the agents visual observations. However, we focus on the regime where accurate prediction requires persistent memory of scene state across long horizons and changing viewpoints. dominant current approach to generative world modeling uses transformer backbones trained with diffusion-style objectives, extending to long rollouts via sliding window inference (Chen et al., 2024; Feng et al., 2024; Yin et al., 2025; Huang et al., 2025). In particular, History-Guided Diffusion Forcing, which extends video diffusion to stable autoregressive rollouts, denoises multiple frames jointly and can leverage attention over recent context to improve short-range temporal coherence and conditioning fidelity (represented in the DFoT baseline we benchmark against) (Song et al., 2025). However, in partially observed settings, long horizon prediction requires retrieving and updating information that may have been observed far in the past (e.g. objects that moved out of view). Under the common sliding window attention regime, context is necessarily truncated as rollouts grow, so information that leaves the window becomes inaccessible to the model to generate new frames. The model must remain consistent with past observations while simultaneously reflecting state changes that occurred out of view, yet neither requirement is naturally supported once the relevant evidence is outside the attention window (as visualized in Figure 2). Consequently, despite strong perceptual quality, windowed diffusion transformer simulators struggle to maintain globally consistent, stateful predictions over long horizons, especially in scenes that have out of view dynamics. In contrast, many non-generative world models learn compact latent dynamics tailored for control rather than reconstructing future observations. Representative approaches such as TDMPC2 and V-JEPA style predictive learning maintain recurrent latent state and optimize objectives that encourage predictability and useful abstractions for downstream decision making (Hansen et al., 2024; 18 Preprint LeCun & Courant, 2022; Assran et al., 2025). These methods can be highly effective for planning in continuous control, but they typically do not aim to produce observation space rollouts, and their benchmarks and objectives often place less emphasis on long horizon, out-of-view state tracking in visually rich, partially observed environments. This gap leads to the question we put forward in this work: how to build an action-consistent world model with persistent state that can stably represent and evolve both self-motion and external object motion over hundreds of steps, enabling accurate prediction even when the relevant dynamics occur outside the current field of view. Memory in generative world models Local temporal consistency in generative world models can often be maintained using self attention over recent history (e.g. through the history guided diffusion forcing scheme mentioned in the previous paragraph), but this mechanism degrades once rollouts exceed the models effective attention window. The comprehensive history of past observations cannot all be kept in context; though there is significant work on long context extensions, such as sparse attention or compression techniques, we believe that more principled solutions are needed to represent the world properly (Zhang et al., 2025a;b). In partially observed environments (which we argue all realistic environments eventually are), long horizon prediction requires persistent and updateable state that can carry information forward even when relevant evidence is no longer incontext. Tracking objects that move out of view and continuing to evolve their state according to previously observed dynamics is an example of this. growing set of methods therefore augment transformer backbones with explicit memory, yet many existing proposals either (i) primarily address static scene consistency, (ii) store viewpoint-variant observations rather than world-centric state, or (iii) introduce memory structures whose capacity or cost scales poorly with horizon. One representative design (and the baseline we chose in our experiments) combines local attention window for short-range coherence with an auxiliary SSM memory module that compresses past observations (Po et al., 2025; Savov et al., 2025). While such hybrids can stabilize generation over modest horizons, in practice we observed they often learn to memorize camera-conditioned appearance rather than maintaining canonical scene state. This viewpoint-variant storage becomes brittle under viewpoint changes and especially under dynamics that evolve out of view, since the memory must simultaneously (a) retain evidence from past viewpoints and (b) support correct state updates when the scene changes without direct observation. Moreover, when the auxiliary module has limited functional capacity (e.g. linear state updates), it can further constrain the complexity of dynamics and viewpoint variation that can be retained over long horizons. Other memory strategies trade compression for retrieval. For instance, database-style approaches such as WORLDMEM store past frames (or features) and retrieve relevant context based on fieldof-view overlap (Xiao et al., 2025). While retrieval can recover visually similar evidence, storing raw viewpoints can be inadequate for dynamic environments where the correct future state may differ from any previously observed frame. Furthermore, the memory footprint typically grows with the number of observations unless aggressive pruning or summarization is introduced. third class maintains explicit geometric memory, such as pointcloud or voxel-based representations. These representations can promote viewpoint invariant consistency, but they are expensive to maintain at scale (Zhou et al., 2025). Further, recent method that separates dynamic from static only remembers the static portion through its pointcloud memory; the dynamic portion is generated anew, conditioned on local frames, so it is unable to handle out of view dynamics well (Wu et al., 2025). More generally, many existing 3D memory constructions are designed for view synthesis with largely static memory, making it challenging to represent and update long horizon dynamics under partial observability. Together, these limitations motivate memory mechanisms that are explicitly world-centric, consistent with action conditioning, and dynamically updateable over long horizons. An additional intuition figure for partially observable dynamic world modeling in 2d environments (such as our MNIST World benchmark) is available in Figure 8. Novel View Synthesis and 3D Priors separate line of work in novel view synthesis and 3D reconstruction, such as NeRFs and 3D Gaussian Splatting, focuses on building explicit scene representations that enable photorealistic rendering from new camera poses (Mildenhall et al., 2020; Kerbl et al., 2023). These methods provide strong geometric and multi-view priors, often yielding impressive viewpoint consistency because rendering is mediated by an underlying 3D representation rather than purely by autoregressive appearance modeling. Extensions to dynamic settings (e.g. 4D vari19 Preprint Figure 8: Partially observable dynamic world modeling in 2d environments. Grayed out areas are not visible to the agent at time t. The agent moves its view each timestep via action at and must predict future states after an observation phase, conditioned on its own future action sequence. ants that model time varying geometry or appearance) further support rerendering observed motions from novel viewpoints (Pumarola et al., 2020; Liu et al., 2025; Wu et al., 2024). However, these approaches are typically formulated as reconstruction or retargeted rendering problems conditioned on observed frames, rather than as action-conditioned future prediction under partial observability. In other words, they are primarily designed to visualize or interpolate dynamics that are present in the captured data, not to predict how an environment will evolve forward in time when key parts of the state may be unobserved and must be remembered and updated. Further, their representations are often not semantic, so they cannot be used to learn generalized prediction models over future states of the world. World model evaluation on long horizon memory and dynamics Evaluation of generative world models is often centered on perceptual similarity and sample quality using metrics such as PSNR, SSIM, and distributional scores like FVD (Unterthiner et al., 2019). While useful, these metrics can underemphasize the specific failure modes that arise in long horizon, partially observed prediction: model may remain visually plausible yet drift in state consistency, forget out-of-view objects, hallucinate new objects, or violate deterministic dynamics in ways that are not strongly penalized by short horizon or ambiguity-tolerant benchmarks. In this work we therefore target more diagnostic regime: deterministic dynamics under controlled partial observability, where the correct future is well defined and errors can be attributed to deficiencies in memory and state tracking rather than to inherent stochasticity. We view deterministic dynamics modeling as first step in modeling all classes of dynamics properly, since it is subset of more general stochastic dynamics. Concretely, we contribute controlled dataset and protocol that determines whether world model can (i) retain information beyond local context window, (ii) update its hidden state when dynamics evolve out of view, and (iii) maintain viewpoint-consistent predictions that reflect an underlying world centric state over long horizons. Equivariant World Models Equivariant models respect the symmetry of their data, ensuring that structured transformations in the input induce predictable changes in the models internal state. This inductive bias has indeed been found to be valuable in prior work on world modeling. Specifically, although not explicitly framed as equivariant, one of the most related world modeling architectures to our proposed self-motion equivariance is the Neural Map (Parisotto & Salakhutdinov, 2017). This work introduced spatially organized 2D memory that stores observations at estimated agent coordinates. The storage location of these observations is shifted precisely according to the agents actions, yielding an effectively equivariant allocentric latent map. In Section 5 of the paper, the authors describe an egocentric version of their model which can in fact be seen as special case of our FloWM, specifically equivalent to the ablation without velocity channels. The authors demonstrate that their allocentric map enables long-term recall and generalization in navigation tasks. In similar vein, EgoMap (Beeching et al., 2020) built on this by leveraging inverse perspective transformations to map from observations in 3D environments to top-down egocentric map. This work also explicitly transforms the latent map in an action-conditioned manner, although the transformation is learned with Spatial Transformer Network, making it only approximately equivariant. Our work can be seen to formalize these early models in the framework of group theory, allowing us to extend the action space beyond just spatial translation to any Lie group and any world space. For example, our 20 Preprint framework can theoretically support full 3-dimensional neural maps without problem, following the framework of flow equivariance. Finally, there are few other works that discuss equivariant world modeling, but are less precisely related to our own. Specifically, (van der Pol et al., 2021) was one of the first works to build equivariant policy and value networks for reinforcement learning, but not with respect to motion, instead with respect to the symmetries of the environment (such as static rotations or translations). More recent work (Park et al., 2022; Ghaemi et al., 2025) proposes to approach the goal of building equivariant world models in more approximate manner by conditioning or encouraging equivariance through training losses, rather than our approach which builds it in explicitly. Overall, we find all of these approaches to be complementary to our own and are excited for their combined potential. Neuroscience. Excitingly, in the neuroscience literature, there is evidence for predictive processing in the mammalian visual system which is function of self-motion signals. For example, Keller et al. (2012) and Leinweber et al. (2017) have found that responses in visual cortex are strongly modulated by self-motion signals, and mismatch between predicted and experienced stimuli. Similarly, it is known that position coding in the hippocampus through place cells and grid cells forms an equivariant map through phase coding of agent location. Explicitly, the phase of given place cells spike shifts equivariantly with respect to the agents forward motion along linear track. Further computational and theoretical work has demonstrated that grid-like activations emerge automatically through the enforcement of equivariance in cell responses Dorrell et al. (2023). We believe this work suggests that there may indeed be biological mechanism for encouraging the visual system to be G-equivariant map from stimuli to type of latent G-space."
        },
        {
            "title": "C BLOCK WORLD ADDITIONAL DATASET DETAILS AND RESULTS",
            "content": "C.1 BLOCK WORLD DATASET DETAILS Here, we describe dataset generation and parameter settings for the Dynamic (presented in the main text), Textured Dynamic, and Static subsets of Block World. The dataset generation parameters are described in Table 3. Each dataset example has video of shape [num frames, channels, height, width], where channels is 3 for RGB; and an accompanying actions list of shape [num frames], for the discrete actions taken by the agent: left, right, forward, or do nothing. Left and right correspond to 90 degree rotation such that the agent stays aligned with the grid. Each dataset subset contains 10,000 videos for training, and 1,000 videos for validation. The world size describes the number of coordinates in the world able to be occupied by the agent or block. The agent is spawned in random location within the world. The Dynamic Block World dataset subset has blocks with randomized colors chosen from among blue, green, yellow, red, purple, and gray. There are no collisions between the blocks, or between the agent and the blocks, but the blocks bounce off the wall and change direction, making the motion nonlinear. Figure 9: Textured 3D Block World Rollouts. Here we visualize additional qualitative rollout results on the Textured 3D Block World split. Note the hallucinated objects in both DFoT and DFoT-SSM as time goes on, whereas FloWM remains consistent. 21 Preprint Table 3: Generation parameters for Block World dataset subsets. Data Subset Static Block World Dynamic Block World Textured Dynamic Block World World Size 15x15 15x15 15x15 Observation Resolution 128x128 128x128 128x128 # Blocks 6-10 6-10 6-10 Block Velocity Range, and 0 -1 to +1 (no diagonals) -1 to +1 (no diagonals) Table 4: Rollout results on Textured 3D Dynamic Block World. Models are evaluated by generating 70 and 210 future frames conditioned on 70 context frames. Model MSE PSNR SSIM 70 210 FloWM (Ours) DFoT DFoT-SSM 0.000826 0.005581 0.009658 0.000926 0.009050 0. 70 30.83 22.53 20.15 210 30.33 20.43 19.15 70 0.9388 0.8960 0.8581 0.9376 0.8577 0.8384 The textured dataset has the same dynamics behavior as the Dynamic Block World dataset, but with the following randomizations: (i) textures are randomized for the wall, chosen between brick, dark wood panel, wood panel; (ii) textures are randomized for the floor, chosen between cardboard, grass, concrete; (iii) textures are randomized for the blocks, chosen between metal grill, airduct grate, cinder blocks, and ceiling tiles; they remain colored randomly; (iv) 1/3 of the time, the object is sphere instead of block. In combination, these all add significant visual complexity and randomness to the environment. The static subset just has the velocity of the objects initialized to 0; the agents exploration pattern is the same. We used Miniworld for this environment, and would like to thank the authors and contributors of Miniworld for creating flexible and useful 3d simulator environment for agents (Chevalier-Boisvert et al., 2023). C.2 TEXTURED BLOCK WORLD RESULTS In this section we report additional results on the Textured Dynamic Block World dataset described above. The focus of this work is on the introduction of the flow equivariant framework for modeling partially observable dynamics, rather than the strength of the visual encoder / decoder. However the fact that FloWM can retain its performance improvements over the baselines in this settings suggests it can serve as step forward for general framework capable of encoding visually realistic scenes as well. The results are available in Table 4. Example rollouts are visible on the project website here. Frames of the rollouts are visible in Figure 9. The metric numbers are not easily comparable to the other dataset split due to the different dataset statistics, but the relative distance is still clearly noticeable. We evaluate with 70 context frames to match the training regime of DFoT-SSM; more details are available in Appendix G.4, C.3 STATIC BLOCK WORLD RESULTS In this section we report additional results on the static Block World dataset described above. Results are available in Table 5. As with the static MNIST World dataset, in this setting, the default configuration of FloWM with velocity channels only adds noise to the model, since the velocity channels have no external motion to model. Therefore, it is unsurprising that the FloWM (no VC) achieves the best metric scores. Despite the environment being static, DFoT and DFoT-SSM still struggle with keeping consistent with information that may have left their immediate context window. We hypothesize that due to the baseline models lack of spatial memory, combined with the randomized number of blocks per environment, these models are unable to consistently remember where the blocks are. Here we also evaluate with 70 context frames to match the training regime of DFoT-SSM; more details are available in Appendix G.4, 22 Preprint Table 5: Rollout performance on 3D Static Block World with partial observability. Models are evaluated by generating 70 or 210 future frames, conditioned on 70 context frames. Model MSE PSNR SSIM 70 70 210 70 210 FloWM (Ours) 0.000860 0.000937 30.65 30.28 0.9675 0.9629 0.000789 0.000641 31.03 31.93 0.9800 0.9803 (no VC) (no SME, no VC) 0.015230 0.015816 18.17 18.01 0.8586 0.8510 0.011686 0.021069 19.32 16.76 0.9378 0.8910 0.004763 0.007985 23.22 20.98 0.9743 0. DFoT DFoT-SSM"
        },
        {
            "title": "D MNIST WORLD ADDITIONAL DATASET DETAILS AND RESULTS",
            "content": "D.1 MNIST WORLD DATASET DETAILS Data Subset dynamic fo no sm dynamic fo static po dynamic po Self-Motion Dynamics No Yes Yes Yes Yes Yes No Yes Partially Observable No No Yes Yes Table 6: MNIST world data subsets demonstrating scaling difficulty in self-motion, dynamics, and partial observability. Here, we describe dataset generation and parameter settings for our ablations on self-motion, dynamics, and partial observability in the MNIST World setting. The subsets are succinctly described in Table 6, and the generation parameters in Table 7. subset is described as partially observable if the world size is larger than the window size. We also scale the number of digits by the size of the world. Each dataset example has video of shape [num frames, channels, height, width], where channels is 1; and an accompanying actions list of shape [num frames, 2], for the and translation of the agent view at each timestep. The dynamic fo no sm subset just has dynamics and is fully observable; the dynamic fo subset has dynamics and is fully observable, but also has self-motion; the static po subset is partially observable, and the agent has self-motion, but the digits do not move; and finally, the dynamic po subset includes partial observability, agent movement, and dynamics. In the main text, we report all results on just the dynamic po subset. For all subsets with dynamics, each digit is given an integer velocity for and in the digit velocity range (e.g., -2 to 2). For each dataset subset with self-motion, at each step during the observation and prediction phase, random integer is chosen in and to be the agents view translation, bounded by the self-motion range (e.g., -10 to 10). For each dataset, objects that move across the boundary reappear on the other side as circular pad. Each dataset subset contains 180,000 videos in the training set, and 8,000 videos in the validation set. Results on each of these data subsets for FloWM and baseline models are described below. Data Subset dynamic fo no sm dynamic fo static po dynamic po World Size 32 32 50 50 Window Size 32 32 32 # Digits 3 3 5 5 Self-motion Range 0 10 10 10 Digit Velocity Range, and -2 to +2 -2 to +2 0 -2 to +2 Table 7: Generation parameters for MNIST World dataset subsets. D.2 MNIST WORLD ADDITIONAL RESULTS Here we report additional results on the MNIST World subsets described above. We evaluate FloWM and FloWM ablations described in Appendix F. We compare to the DFoT and DFoT-SSM baselines as described in Appendix G.4. 23 Preprint Figure 10: Rollout Error (MSE) vs. Forward Prediction Steps for all data subsets of MNIST World. The dynamic subset is replicated from the main text for ease of comparison. The error (MSE) between the predicted future observations (rollout) and the ground truth is plotted for each baseline in Figure 10 as function of forward prediction timestep (x-axis). Metrics are reported over the first 20 timesteps (the training length) and over the full 150 timesteps (length generalization) in Tables 9, 8, 10. Due to being constructed with different number of digits, metrics between the data subsets are not necessarily directly comparable. We provide the All-Black Baseline (model that only predicts 0 for future observations) as form of normalization for comparison. All models are able to do reasonably well on the simplest fully observable dataset with no selfmotion; note here the DFoT is doing latent diffusion, so there is small amount of error error from the decoding step, contributing baseline MSE of around 0.02, see Appendix G.5 for more details. This setup aligns with the typical setting of world modeling, where the information that the model needs is expected to be in the attention window. The other dataset splits do not follow this assumption, and the results align with expectations about the models capabilities. The DFoT does relatively better on the static static po compared to the dynamic po dataset, due to not having to model dynamics, but the models outputs still diverge from the ground truth quickly. For dataset where the velocity channels are redundant, i.e. static po, the FloWM (no VC) does slightly better than FloWM. Further note that the FloWM (no VC) is able to have low error on most of the tasks, though with much higher value than FloWM as errors accumulate due to not having the velocity channels to encode flow equivariantly. Taken together, the ablations suggest that self-motion equivariance is key to solving the problem, and that input flow equivariance via velocity channels helps with exactness and convergence time, with the tradeoff of larger hidden state activation size. 24 Preprint Table 8: Rollout performance on 2D Dynamic MNIST World with full observability while without self-motion. Models are trained to generate 20 future frames given 50 context frames, and evaluated by generating 20 (training length) and 150 (length generalization) future frames respectively, conditioned on 50 context frames. Model MSE PSNR SSIM 20 150 20 20 150 0.00009 0.00104 40.61 29.83 0.9982 0.9752 FloWM (Ours) 0.00006 0.00173 41.89 27.61 0.9981 0.9797 (no SME) (no SME, no VC) 0.00031 0.02845 35.05 15.46 0.9950 0.7746 0.00026 0.00925 35.87 20.34 0.9877 0.9061 (no VC) 0.00015 0.01349 38.36 18.70 0.9973 0.8099 (action concat) 0.02128 0.03601 16.72 14.44 0.8961 0.8205 0.9399 0.2907 0.01298 0.22098 18.87 DFoT DFoT-SSM 6.56 Table 9: Rollout performance on 2D Dynamic MNIST World with full observability. Models are trained to generate 20 future frames given 50 context frames, and evaluated by generating 20 (training length) and 150 (length generalization) future frames respectively, conditioned on 50 context frames. Model MSE PSNR SSIM 20 20 150 20 150 0.00013 0.00136 39.01 28.65 0.9977 0.9763 FloWM (Ours) 0.0355 0.0105 (no SME) 0.15595 0.16167 0.0351 0.0141 (no SME, no VC) 0.15597 0.18251 0.9982 0.1718 (no VC) 0.0796 0.0294 (action concat) 0.2086 0.0833 0.6170 0.1090 8.07 8.07 0.00010 0.26278 40.21 8.72 0.13425 0.15148 0.22194 0.29699 6.54 0.08162 0.25271 10. 7.91 7.39 5.80 8.20 5.27 5.97 DFoT DFoT-SSM FLOWM EXPERIMENT DETAILS: 3D DYNAMIC BLOCK WORLD On the 3D Dynamic Block World Dataset, FloWM is built with 6-layer ViT encoders and decoders with 8 attention heads per layer, and an embedding dimension of 256. E.1 RECURRENCE The hidden state ht RV ChidHworldWworld has velocity channels (indexed by the elements ν ), and Chid = 256 hidden state channels (the same as the ViT token embedding dimension). The spatial dimensions of the hidden state are set to two times the world size for each dataset, meaning Hworld = Wworld = 32, so that the agent can be robust to being spawned anywhere. The hidden state is initialized to all zeros for the first timestep, i.e. h0 = 0. E.2 VELOCITY CHANNELS On the 3D Block World dataset, we add velocity channels only up to 1 in both the and dimensions of the map with no diagonal velocities, and include zero velocity channel. Thus in total, = 5 for the FloWM. Each channel is flowed by its corresponding velocity field, denoted by ψ1(ν)ht(ν) for each of the velocities ν, matching the velocity of the blocks in the real environment. The actions of the agent then induce an additional flow of the hidden state, which we implement via the inverse of the representation of the action Tat = 1 at . In practice, this is implemented by performing roll operation on the hidden state by exactly 1 element for forward action, and +/ 90-degree rotation for left or right actions respectively. Doing so allows the models representation of the world to stay accurate with respect to the transformation it has just done. Intuitively, if an agent has representation of an object in front of it, then turns to the left, the world should shift to the right, opposite of the movement of the turn. 25 Preprint Table 10: Rollout performance on 2D Static MNIST World with partial observability. Models are trained to generate 20 future frames given 50 context frames, and evaluated by generating 20 (training length) and 150 (length generalization) future frames respectively, conditioned on 50 context frames. Model MSE PSNR SSIM 20 20 150 20 150 FloWM (Ours) 0.00013 0.00467 38.78 23.31 0.9970 0.8053 0.0460 0.0116 0.12097 0.12790 (no SME) (no SME, no VC) 0.12086 0.13046 0.0520 0.0144 0.00002 0.00004 47.63 44.49 0.9994 0.9993 (no VC) 0.1595 0.1354 0.09473 0.11860 10.24 (action concat) 0.5199 0.2257 0.10505 0.22641 9.79 0.7274 0.3397 0.05398 0.15706 12.68 DFoT DFoT-SSM 9.26 6.45 8.04 8.93 8.85 9.17 9.18 E.3 TRANSFORMER DETAILS The ViT Encoder takes in both image input tokens and the FoV-selected map latent tokens, and processes them together in the self-attention window. We use patch size of 16 to patchify the image before sin-cos absolute position embedding is added (Vaswani et al., 2023). The map latent tokens are also added together with sin-cos absolute position embedding based on the position in the 2d map. Notably, these two are different embeddings and represent different spatial coordinates, one is of the image patch location, and the other is of the world location relative to the agent. The ViT Decoder starts with copy of learnable [MASK] token at each patch location in the image, added with position embeddings representing the patch location in pixel space. Then it uses cross attention between the selected tokens in the Field of View of the updated hidden state FoV(ht+1) to make prediction of the observation at the next timestep, ˆft+1. E.4 TRAINING LOSS DETAILS To train FloWM, as well as the ablated versions, we provide the model with 50 observation frames as input, and train the model to predict the next 90 observations conditioned on the corresponding future action sequence. Specifically, we minimize the mean squared error (MSE) between the output of the model and the ground truth sequence, averaged over the prediction frames 50 to 139: LM SE = 1 90 139 (cid:88) t= ft ˆft2 2. (24) The models are trained with the Adam optimizer with learning rate of 1e 4, batch size of 16. We use teacher forcing ratio of 12.5%. They are each trained for 150k steps, or until converged. More details are available in Table 11. FLOWM EXPERIMENT DETAILS: MNIST WORLD The Simple Recurrent version of the Flow Equivariant World Model for 2d settings is built as simple sequence-to-sequence RNN with small CNN encoders/decoders to model MNIST digit features. Full code is available on the project page. For completeness, we repeat the Simple Recurrent FloWM recurrence relation below: ht+1(ν) = σ(cid:0)ψ1(ν at) ht(ν) + pad(U ft)(cid:1). (25) F.1 RECURRENCE The hidden state ht RV ChidHworldWworld has velocity channels (indexed by the elements ν ), and Chid = 64 hidden state channels. The spatial dimensions of the hidden state are set to match the world size for each dataset. For the partially observed world, this means Hworld = Wworld = 50 (where the window size is set to 3232), while for the fully observed world, Hworld = Wworld = 32. The hidden state is initialized to all zeros for the first timestep, i.e. h0 = 0. 26 Preprint Table 11: Block World ViT FloWM Configurations. Component Option Training Model Learning rate Effective batch size Training steps GPU usage Teacher Forcing Hidden channels Encoder depth Encoder heads Decoder dim Decoder depth Decoder heads Patch size Params Patch size Value 1e-4 16 150k 2H100 0.125 256 6 8 256 6 8 16 10M 16 The hidden state is processed between timesteps by convolutional kernel W. This kernel has the potential to span between velocity channels, and therefore model acceleration or more complex dynamics than static velocities. In this work, since our dataset has no such dynamics (we only have constant object velocities), we safely ignore the inter-velocity convolution terms, and simply set to be 3 3 convolutional kernel, with 64 input and output channels, circular padding, and no bias. We refer the interested reader to Keller (2025) for details on the form of the full flow-equivariant convolution that could be equally used in this model. The hidden state is finally passed through non-linearity σ to complete the update to the next timestep. In this work, for MNIST World, we use ReLU. F.2 VELOCITY CHANNELS In this work, for MNIST World, we add velocity channels up to 2 in both the and dimensions of the image. Explicitly, = {(2, 2), (2, 1), . . . (0, 0) . . . (2, 2)}. Thus in total, = 25 for the FloWM. Each channel is flowed by its corresponding velocity field (defined by ψ(ν)) at each step. This is denoted by ψ1(ν) ht(ν). The actions of the agent then induce an additional flow of the visual stimulus. In order to be equivariant with respect to this flow in addition to the flows in , we simply additionally flow each hidden state by the corresponding inverse of the action flow ψ(at). In total this gives the combined flow for each flow channel ψ1(ν at). In practice, this is implemented by performing roll operation on the hidden state by exactly (ν at) pixels. F.3 ENCODER The encoder is simply single convolutional layer, with 3 3 kernel U, 1 input channel, and 64 output channels. The convolution uses circular padding, and no bias. The observation at timestep (ft), is thus processed by the encoder (U ft) yielding the processed observation of the agent. Given this observation is only partial observation of the full world, we must pad this observation to match the world size, and the size of the hidden state. We denote this operation as pad in the recurrence relation, and simply pad the boundary of the output of the encoder with 0 to match the world-size (size of the hidden state). F.4 DECODER We learn the parameters of the FloWM by training it to predict future observations from its hidden state and the corresponding sequence of future actions. To compute this prediction, we take consistent window size crop from the center of the hidden state, corresponding to the same location where the encoder writes-in. We denote this crop window(ht+1). To then enable the model to predict each pixels velocity independently, we perform pixel-wise max-pool over the dimension (velocity channels) before passing the result to decoder gθ. Specifically: Preprint (cid:0)max (window(ht+1))(cid:1). The decoder gθ is simple 2 layer convolutional neural netˆft+1 = gθ work with 3 3 convolutional kernels, 64 hidden channels, and ReLU non-linearity between the layers. ν F.5 ABLATION: NO VELOCITY CHANNELS To construct the ablated version of the FloWM with no velocity channels, we simply set = {(0, 0)}. Since the original FloWM model simply max-pools over velocity channels, the decoder already only takes single velocity channel as input, so no other portions of the model need to change. We note that this model is identical to simple convolutional recurrent neural network with self-motion equivariance. Explicitly: ht+1 = σ(cid:0)ψ1(at) ht + pad(U ft)(cid:1). (26) F.6 ABLATION: NO SELF-MOTION EQUIVARIANCE To construct the ablated version of the FloWM with no self-motion equivariance, we simply remove the term at from the flow of the recurrence relation. Explicitly: ht+1(ν) = σ(cid:0)ψ1(ν) ht(ν) + pad(U ft)(cid:1). (27) We note that this is equivalent to the original FERNN model with the addition of the partialobservability modifications (padding the input and windowing the hidden state for readout). F.7 ABLATION: NO VELOCITY CHANNELS + NO SELF-MOTION EQUIVARIANCE To ablate both velocity channels and self-motion equivariance, we reach simple convolutional RNN: ht+1 = σ(cid:0)W ht + pad(U ft)(cid:1). (28) F.8 ABLATION: CONV-RNN + ACTION CONCAT In the appendix, we additionally include version of the model with no velocity channels, no selfmotion equivariance, but with action conditioning for both the input and hidden state. Specifically, we concatenate the current action to the hidden state vector and the input image as two additional channels (corresponding to the and components of the action translation vector), and change the number of input channels for both convolutions correspondingly. Explicitly: ht+1 = σ(cid:0)W concat(ht, at) + pad(U concat(ft, at))(cid:1). (29) Empirically, we find that this additional conditioning marginally improves the model performance; however, the model is still clearly unable to learn the precise equivariance that the FloWM has built-in. F.9 TRAINING DETAILS To train the FloWM, as well as the ablated versions, we provide the model with 50 observation frames as input, and train the model to predict the next 20 observations conditioned on the corresponding action sequence. Specifically, we minimize the mean squared error (MSE) between the output of the model and the ground truth sequence, averaged over the 20 frames (from frame 50 to 69): LM SE = 1 20 69 (cid:88) t=50 ft ˆft2 2. (30) The models are trained with the Adam optimizer with learning rate of 1e 4, batch size of 32, and gradient clipping by norm with value of 1.0. They are each trained for 50 epochs, or until converged. Some models, such as the FloWM with self-motion equivariance but no velocity channels, took longer than 50 epochs to converge, and thus training was extended to 100 epochs. All FloWM models (and ablations) have roughly 75K trainable parameters. 28 Preprint"
        },
        {
            "title": "G BASELINE DETAILS",
            "content": "G.1 VIDEO DIFFUSION TRANSFORMERS Diffusion Transformer based video generation models are the most prominent so-called world models today (Peebles & Xie, 2023; Brooks et al., 2024). Training follows similar formula with diffusion image generation pipelines, requiring attention over the temporal dimension to retain temporal consistency. For video data, diffusion models are typically trained within the latent space of variational autoencoder (VAE) (Rombach et al., 2022; Gupta et al., 2023), where raw video frames are first compressed into compact latent representation. The ability for these video diffusion models to generate impressively realistic videos has led to an increased interest for their use as world models, and there is growing focus in ensuring the spatiotemporal consistency of these models as world simulators. Due to the size complexity of the input token space, to generate long videos, researchers have turned to autoregressive sampling and sliding window attention; though ubiquitously used, we speculate that the drawbacks of this method for inference, where there is no hidden state passed between generation rounds after the window shifts, is major reason that DiT baselines fails on the simple task presented in this work. G.2 DIFFUSION FORCING TRANSFORMER BASELINE Due to its claims of long term consistency and flexible inference abilities, for our baseline we chose History-guided Diffusion Forcing training scheme, using latent diffusion with CogVideoX-style transformer backbone, which we will call here DFoT (Song et al., 2025; Chen et al., 2024; Yang et al., 2025; Rombach et al., 2022). Models for state of the art video world modeling today have similar training formulas and architectures for the backbone (Xiang et al., 2024; Ball et al., 2025; Decart et al., 2024; Agarwal et al., 2025). We first trained spatial downsampling VAE on frames of the MNIST-world data subsets, then pass input video frames through the VAE to form latent representation before it reaches the diffusion model. Following the standard diffusion forcing training scheme, each frame during training is corrupted with independent gaussian noise, and the training target is to predict some form of the ground truth from these noisy frames. Song et al. (2025) showed that using this training schedule allows for the history image frames to be prepended to the noisy frames as context in the same self attention window, with zero (or some minimal) noise level, called History Guidance. For DFoT models, unlike FloWM recurrent models, during training we make no distinction between observation and prediction frames, and train on length 70 sequences in the self-attention window, where each frames tokens receive independent gaussian noise. During inference, we utilize History Guidance with 70 frames in the attention window to provide image context for consistent generation. Specifically, the 50 observation frames are given minimal noise, and the 20 prediction frames all begin at full noise; then the entire set of frames is passed through the model multiple times according to the scheduler to complete denoising the target frames to get clean frames as outputs. Specifically, each latent frame in the sequence xt xτ is assigned an independent noise level kt [0, 1]. Each frame (more precisely, each collection of spatial tokens corresponding to single frame) is noised according to the following equation: xkt = αktx0 + σktϵt, ϵt (0, 1), (31) where αkt and σkt denote the signal and noise scaling factors, respectively, determined by the chosen variance schedule. The diffusion model ϵθ takes in as input sequence of noise levels, kτ , and the sequence of independently noised inputs xkτ τ . The model is trained to minimize the following diffusion loss: Ekτ ,xτ ,ϵτ (cid:104)(cid:13) (cid:13)ϵτ ϵθ (cid:0)xkτ τ , kτ (cid:1)(cid:13) (cid:13) 2(cid:105) . (32) For more information on diffusion models in general, please see Chan (2025). To run inference for generation of longer videos (as in the length extrapolation experiments), we use sliding window approach, matching the number of frames seen during training in the self-attention window. Specifically, we keep 50 context frames, shift the window ahead by 20 frames after each chunk is done denoising, and use the newly generated frames as context for the next generation round. 29 Preprint Table 12: DFoT configurations for different datasets. Section and key are organized hierarchically in the first column. Config Training Effective batch size Learning rate Warmup steps Weight decay Training steps GPU usage Optimizer Training strategy Precision Diffusion Objective Sampling steps Noise schedule Loss weighting Model Total parameters # attention heads Head dimension # layers Time embed dimension Condition embed dimension Inference History guidance Context frames Sampler MnistWorld BlockWorld 128 2e-4 (linear warmup) 2,000 1e-3 245k 1L40S Adam, betas=(0.9, 0.99) Distributed Data Parallel Bfloat16 32 2e-4 (linear warmup) 2,000 1e-3 300k 2L40S Adam, betas=(0.9, 0.99) Distributed Data Parallel Bfloat16 v-prediction 50 cosine sigmoid 95.3 12 64 10 256 768 v-prediction 50 cosine sigmoid 95.3 12 64 10 256 stabilized conditional (level = 0.02) 50 DDIM stabilized conditional (level = 0.02) 70 DDIM G.3 DFOT TRAINING DETAILS We train separate DFoT model for each MNIST World data subset to separate out its abilities. We embed actions using simple MLP embedder, and concatenate it to the video tokens, following CogVideoX. Our 96M parameter DFoTs validation loss and validation metrics converge after 245k steps on 1 NVIDIA L40S 48GB GPU with batch size of 128 on MNIST World, and after 300k steps on 1 NVIDIA L40S 48GB GPU with batch size of 32 on Block World. More training hyperparameters are reported in Table 12. G.4 DFOT-SSM TRAINING DETAILS We train separate DFoT-SSM model for each data subset to separate out its abilities. We embed actions using simple MLP embedder, and concatenate it to the video tokens, following CogVideoX. Our 97.8M parameter DFoT-SSMs validation loss and validation metrics converge after 200k steps on 2 NVIDIA L40S 48GB GPU with an effective batch size of 128 on MnistWorld, and 300k steps on 2 NVIDIA L40S 48GB GPU with an effective batch size of 32 on Block World. More training hyperparameters are reported in Table 13. For the Blockworld dataset, we allow all models to have 140 frames available during training, but the models use them differently. FloWM uses 50 context frames and calculates the loss by predicting the next 90. For the sliding window inference to work for the DFoT-SSM, 70 frames of context is better, so that each time the window slides it can slide forward by 70 frames. So during inference, to match the information given to each model, we evaluate all with 70 frames of context, despite the 30 Preprint Table 13: DFoT-SSM configurations. Classifier-free guidance (Ho & Salimans, 2022) for conditioning is not used during inference; though the models have been trained to allow for it, we find their instruction following ability not to be limiting factor. Loss weighting uses sigmoid reweighting proposed by Kingma & Gao (2023) and adopted by Hoogeboom et al. (2024). History guidance follows the stabilized conditional method (level = 0.02) from Song et al. (2025); please refer to their codebase for details. Config Training Effective batch size Learning rate Warmup steps Weight decay Training steps GPU usage Optimizer Training strategy Precision Context frames Diffusion Objective Sampling steps Noise schedule Loss weighting Model Total parameters # attention heads Head dimension # layers Time embed dimension Condition embed dimension Inference History guidance Context frames Sampler MnistWorld Block World 128 2e-4 (linear warmup) 2,000 1e-3 200k 1L40S Adam, betas=(0.9, 0.99) Distributed Data Parallel Bfloat16 50 32 2e-4 (linear warmup) 2,000 1e-3 300k 2L40S Adam, betas=(0.9, 0.99) Distributed Data Parallel Bfloat16 70 v-prediction 50 cosine sigmoid 97.8 12 64 10 256 768 v-prediction 50 cosine sigmoid 97.8 12 64 10 256 768 stabilized conditional (level = 0.02) 50 DDIM stabilized conditional (level = 0.02) 70 DDIM fact that FloWM was trained with 50 frames of context only. We didnt find significant difference in results between the two settings, but report with 70 frames of context for all models for fairness. G.5 VAE TRAINING DETAILS Following standard practice, we use VAE to perform latent diffusion; doing diffusion on pixels instead could offer perceptually different results, but we do not believe it would alter the results of the model. We train our 8x spatial downsampling VAE on sample frames from mix of all of the data subsets, such that all combinations of overlapping MNIST digits are within the training distribution. Our 20M parameter VAEs validation loss converges at about 90k steps for MNIST World, using an effective batch size of 256 across 4 NVIDIA L40S 48GB GPUs with learning rate of 4e-4. We utilize Masked Autoencoder Vision Transformer based VAE (He et al., 2021). We directly apply the VAE code from Oasis (Decart et al., 2024), including an additional discriminator loss that helps with visual quality; please refer to their work for more details. The reconstruction MSE accuracy reaches 0.02 for MNIST World, so any DFoT MSE can be expected to be 0.02 higher than if trained on pixels; we believe this should not affect convergence behavior of the DFoT models on the downstream task. During diffusion training, for our VAE with latent dimension 4, and spatial downsampling ratio 8, input videos of shape [num frames, channels, height, 31 Preprint Table 14: VAE configurations for MNIST World. The input size from the dataset is 32 32. Component Option Value Training Model Learning rate Effective batch size Precision Strategy Warmup steps Training epochs GPU usage Optimizer (AE) Optimizer (Disc) Total parameters Encoder dim Encoder depth Encoder heads Decoder dim Decoder depth Decoder heads Patch size 4e-4 256 Float16 mixed precision Distributed Data Parallel 10,000 172 4L40S Adam, betas=(0.5, 0.9) Adam, betas=(0.5, 0.9) 19.7 384 4 12 384 7 12 8 Latent Latent dim Temporal downsample 4 1 Table 15: VAE configurations for Block World. The input size from the dataset is 128 128. Component Option Value Training Model Learning rate Effective batch size Precision Strategy Warmup steps Training epochs GPU usage Optimizer (AE) Optimizer (Disc) Total parameters Encoder dim Encoder depth Encoder heads Decoder dim Decoder depth Decoder heads Patch size 4e-4 256 Float16 mixed precision Distributed Data Parallel 10,000 40 4L40S Adam, betas=(0.5, 0.9) Adam, betas=(0.5, 0.9) 80.7 576 5 12 576 15 12 16 Latent Latent dim Temporal downsample 8 1 width] are converted to shape [num frames, 4, height // 8, width // 8]. For Block World, the reconstruction error is less than 0.003, and we train the model for 300k steps with latent dimension 8 and spatial downsampling ratio 16. More training hyperparameters are reported in Table 14 and Table 15. Preprint Table 16: Training compute on 3D Dynamic Block World. FLOPs are estimated from per-sample forward/backward GFLOPs measured at batch size 1 and 140 frames on 1 H100 GPU, then scaled by the actual batch size and number of training steps. Model Batch size Steps Total compute (EFLOPs) FloWM (Ours) DFoT SSM 16 32 32 150k 300k 300k 27.5 15.7 10.5 Table 17: Per step compute and runtime on Block World with batch size 1 and 2 frames on 1 H100 GPU. Model Forward GFLOPs Backward GFLOPs Forward Time (ms) Backward Time (ms) FloWM (Ours) DFoT SSM 31.18 4.94 5.00 99.50 9.91 10. 104.47 101.05 90.93 109.59 47.52 0.85 72.35 2.37 106.94 4.70 89.85 0."
        },
        {
            "title": "H COMPUTE RESOURCE COMPARISON",
            "content": "H.1 TRAINING AND INFERENCE COMPUTE COMPARISONS To contextualize the computational footprint of FloWM, we report training compute (in EFLOPs) and inference wall-clock time for FloWM and our baselines. All measurements are obtained on single NVIDIA H100 GPU. Training compute (reported in Table 16) is estimated by first measuring the forward and backward GFLOPs for batch size of 1, and then scaling by the actual batch size and number of optimization steps. Under this common protocol, FloWM requires roughly 1.7 to 2.6 more training FLOPs than DFoT and DFoT-SSM to reach convergence, but remains within the same order of magnitude while delivering substantially more stable long-horizon predictions. Inference wall clock time is measured as the per-step latency for predicting single frame with batch size 1 (reported in Table 17). These per-frame runtimes are also of the same order of magnitude across all models. Unlike fully feedforward architectures, FloWM maintains recurrent world state. Therefore it cannot be parallelized over sequence length in its current form, and inference cost grows linearly with the number of predicted frames. We view this linear dependence as deliberate tradeoff in exchange for preserving coherent latent map over long horizons, which appears important for accurately representing hidden dynamics under partial observability. In our view, this computational tradeoff is well worth it in order to attain the capability to predict the dynamics of the world consistently. H.2 LIMITATIONS AND OPPORTUNITIES FOR EFFICIENCY Our current implementation is not heavily optimized, and there are several possible future avenues to reduce computational cost without changing the core modeling assumptions. First, decoupling the encoder from the hidden state update would allow processing of input frames in parallel before writing to the latent map, increasing input level parallelism. Second, designing the recurrent update to be linear and associative would, in combination with such decoupling, make it amenable to parallel scan algorithms similar to those used in modern state-space models Martin & Cundy (2018); Smith et al. (2023). This would enable parallelization over sequence length while retaining recurrent structure. Third, the latent map is currently updated via dense gated operation over the entire estimated field of view, even though the true change in information per timestep is typically sparse. Introducing sparse or multiscale updates could substantially reduce per-step computation while preserving the benefits of persistent world memory. Implementation could be inspired by other hierarchical mapping methods such as NICE-SLAM and SHINE-Mapping (Zhu et al., 2022; Zhong et al., 2023). Overall, these directions suggest that the additional compute required by FloWM is not fundamental to the architecture, but rather reflects design choices that can be systematically optimized in future work. Preprint"
        },
        {
            "title": "I LLM USAGE",
            "content": "In this work, we occasionally used LLMs for polishing writing, including for sentence rewriting suggestions and finding word synonyms."
        }
    ],
    "affiliations": [
        "CSE, UC San Diego",
        "Kempner Institute, Harvard University",
        "ML, Carnegie Mellon University",
        "SEAS, Harvard University"
    ]
}