{
    "paper_title": "Critical Tokens Matter: Token-Level Contrastive Estimation Enhances LLM's Reasoning Capability",
    "authors": [
        "Zicheng Lin",
        "Tian Liang",
        "Jiahao Xu",
        "Xing Wang",
        "Ruilin Luo",
        "Chufan Shi",
        "Siheng Li",
        "Yujiu Yang",
        "Zhaopeng Tu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have exhibited remarkable performance on reasoning tasks. They utilize autoregressive token generation to construct reasoning trajectories, enabling the development of a coherent chain of thought. In this work, we explore the impact of individual tokens on the final outcomes of reasoning tasks. We identify the existence of ``critical tokens'' that lead to incorrect reasoning trajectories in LLMs. Specifically, we find that LLMs tend to produce positive outcomes when forced to decode other tokens instead of critical tokens. Motivated by this observation, we propose a novel approach - cDPO - designed to automatically recognize and conduct token-level rewards for the critical tokens during the alignment process. Specifically, we develop a contrastive estimation approach to automatically identify critical tokens. It is achieved by comparing the generation likelihood of positive and negative models. To achieve this, we separately fine-tune the positive and negative models on various reasoning trajectories, consequently, they are capable of identifying identify critical tokens within incorrect trajectories that contribute to erroneous outcomes. Moreover, to further align the model with the critical token information during the alignment process, we extend the conventional DPO algorithms to token-level DPO and utilize the differential likelihood from the aforementioned positive and negative model as important weight for token-level DPO learning.Experimental results on GSM8K and MATH500 benchmarks with two-widely used models Llama-3 (8B and 70B) and deepseek-math (7B) demonstrate the effectiveness of the propsoed approach cDPO."
        },
        {
            "title": "Start",
            "content": "Critical Tokens Matter: Token-Level Contrastive Estimation Enhances LLMs Reasoning Capability Zicheng Lin 1 * Tian Liang 2 * Jiahao Xu 2 * Xing Wang 2 Ruilin Luo 1 Chufan Shi 1 Siheng Li 1 Yujiu Yang 1 Zhaopeng Tu 2 4 2 0 2 2 ] . [ 2 3 4 9 9 1 . 1 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have exhibited remarkable performance on reasoning tasks. They utilize autoregressive token generation to construct reasoning trajectories, enabling the development of coherent chain of thought. In this work, we explore the impact of individual tokens on the final outcomes of reasoning tasks. We identify the existence of critical tokens that lead to incorrect reasoning trajectories in LLMs. Specifically, we find that LLMs tend to produce positive outcomes when forced to decode other tokens instead of critical tokens. Motivated by this observation, we propose novel approach - cDPO - designed to automatically recognize and conduct token-level rewards for the critical tokens during the alignment process. Specifically, we develop contrastive estimation approach to automatically identify critical tokens. It is achieved by comparing the generation likelihood of positive and negative models. To achieve this, we separately fine-tune the positive and negative models on various reasoning trajectories, consequently, they are capable of identifying identify critical tokens within incorrect trajectories that contribute to erroneous outcomes. Moreover, to further align the model with the critical token information during the alignment process, we extend the conventional DPO algorithms to token-level DPO and utilize the differential likelihood from the aforementioned positive and negative model as important weight for token-level DPO learning. Experimental results on GSM8K and MATH500 benchmarks with two-widely used models Llama-3 (8B and 70B) and deepseek-math (7B) demonstrate the effectiveness of the propsoed approach cDPO. *Equal contribution. Work done when Zicheng was interning at Tencent AI Lab. 1Tsinghua University 2Tencent AI Lab. Correspondence to: Yujiu Yang <yang.yujiu@sz.tsinghua.edu.cn>, Zhaopeng Tu <zptu@tencent.com>. Figure 1. Impact of critical tokens on reasoning accuracy. The With Critical Token line shows that, without intervention, repeated sampling from the original trajectory consistently fails to produce correct trajectories. In contrast, the Without Critical Token line demonstrates that replacing the identified critical token with an alternative drastically increases the likelihood of correct reasoning outcomes. This underscores the significant role of critical tokens in incorrect reasoning trajectories. 1. Introduction Aligning large language models (LLMs) with human preferences represents crucial challenge in the current research. It is the process of fine-tuning pre-trained LLMs on various instruction data, and thereby, the model would be consistent with human values, preferences, and instructions. This alignment paradigm has made significant progress and is widely applied in all downstream applications of LLMs. Among all the contributions made to alignment algorithms (Christiano et al., 2017; Schulman et al., 2017; Ziegler et al., 2019; Ouyang et al., 2022; Bai et al., 2022), Direct Preference Optimization (DPO) (Rafailov et al., 2024) is one of the most representative algorithms. It utilizes the LLM itself as secret reward model and conducts preference optimization on preference pair of positive and negative examples. Since then, various contributions have been made to further advance the DPO development of LLM alignment (Pal et al., 2024; Amini et al., 2024; Azar et al., 2024; Lai et al., 2024; Pang et al., 2024; Tang et al., 2024; Zeng et al., 2024). 1 Figure 2. Illustration of the impact of critical tokens on reasoning trajectories. The token owed leads to incorrect logical deductions, resulting in erroneous answers. In contrast, decoding alternative tokens like paid significantly improves reasoning accuracy, enabling the model to produce correct answers. Despite DPOs success in general instruction tuning tasks, it still faces challenges when applied to reasoning and mathematical tasks. Studies have explored this issue through an optimization lens, pinpointing that these algorithms often reduce the generation likelihood of positive examples in reasoning. The consequent approach is to conduct preference optimization while maintaining high generation likelihoods exclusively for positive examples. (Pang et al., 2024; Pal et al., 2024; Feng et al., 2024). In contrast to the optimization approach, this paper highlights another insight that originates from the unique characteristic of reasoning tasks. We identify the existence of critical tokens within incorrect reasoning trajectories. Specifically, the critical token is type of token that plays crucial role in leading to incorrect outcomes. More specifically, force decoding to avoid those critical tokens of an incorrect trajectory would lead to correct outcomes. From human perspective, there is also consistent observation that small changes in operators and logical elements in mathematical reasoning often lead to completely different results, while other aspects of reasoning can be very robust. This finding further demonstrates the significantly uneven distribution of token importance in reasoning tasks (Lin et al., 2024). To address such an issue, in this paper, we present cDPO, an automatic token-level preference optimization framework for reasoning tasks. Since critical tokens largely determine the final outcome of reasoning tasks, the outcome information could be inversely used to estimate critical tokens as well. Motivated by this, we separately train two models: one on positive trajectories and the other on negative trajectories, to capture such outcome information. The contrastive ratio of the two models likelihoods provides natural indicator for critical tokens. More specifically, we subtract the generation likelihood of the positive model from the negative model, the differential result demonstrates the token-level disagreement between the two models, which highlight the critical tokens within the negative trajectory. Empirical experiments show that tokens detected from such contrastive estimation align well with the force-decoding outcomes. We further utilize such differential likelihoods as token-level reward signals during the preference optimization process for better alignment of reasoning tasks. Experimental results show that our proposed cDPO demonstrates superior performance across various baseline strategies. In summary, our contributions are three-fold: We identify critical tokens within reasoning trajectories, which are the root cause of incorrect outcomes in reasoning tasks. We propose cDPO - token-level preference optimization algorithm - to automatically identify and provide token-level reward on critical tokens. Empirical results demonstrate that our proposed method outperforms both example-level and step-level baselines across all benchmark with < 0.005. 2 2. Estimating Critical Tokens In this section, we mainly demonstrate the identification of critical tokens of incorrect reasoning trajectory. We further demonstrate novel proposed contrastive estimation strategy to automatically identify those tokens. Critical Tokens Affect Reasoning Mathematical reasoning tasks emphasize logical and sequential deduction to arrive at solutions. However, within incorrect reasoning trajectories, certain tokens play pivotal role in driving the trajectory toward an incorrect outcome. These tokens disrupt the logical flow, misrepresent relationships, or introduce computational errors that significantly impact the final result. Unlike other tokens that may have negligible effect on the reasoning process, these critical tokens serve as pivotal points of failure. Identifying these tokens is essential, as avoiding or correcting them can often lead to correct outcomes, even within an incorrect trajectory. As shown in Fig. 2, the token owed is responsible for most of the incorrect reasoning trajectories, as it misleads the logical deduction process. Conversely, forcing the model to decode alternative tokens, such as paid significantly improves the likelihood of producing correct final result. To further validate the existence of critical tokens and analyze their influence on reasoning trajectories, we conducted an experiment using LLama-3-8B on 100 incorrect reasoning paths sampled from the GSM8K dataset. Specifically, we performed the following experiment: Token Score Calculation via Resampling: For each token in an incorrect trajectory, we resampled 64 times while keeping the token unchanged. Based on the correctness of the generated completions, we calculated score to quantify the influence of each token on the trajectory. The first token with score of 0, indicating that it consistently led to incorrect outcomes, was designated as the critical token. Forced Decoding with Alternative Tokens: After identifying the critical token in each trajectory, we replaced it by forcing the model to decode an alternative token. This was achieved by resampling the critical token 64 times using the models probability distribution. We then evaluated the correctness of these modified continuations and calculated the Pass@k metric. As shown in Fig. 1, the forced decoding strategy significantly improves the likelihood of correct solution. Specifically, the Pass@k curve demonstrates rapid increase in accuracy, with Pass@1 at 0.31 and Pass@64 reaching 0.90. This result highlights the pivotal role that critical tokens play in determining the outcome of reasoning trajectories. SimFigure 3. Contrastive estimation identifies critical tokens. This figure illustrates how contrastive estimation identifies critical tokens in incorrect trajectories by comparing the likelihoods of tokens generated by positive model and negative model. ply avoiding these tokens can dramatically enhance performance, even when the previous context remains unchanged. Contrastively Estimating Critical Tokens Experimental results demonstrate that critical tokens are pivotal in determining incorrect final outcomes. While methods such as resampling or Monte Carlo Tree Search (Wang et al., 2024; Luo et al., 2024) can identify critical tokens within incorrect trajectories, they incur prohibitively high sampling costs and face significant scalability challenges. Furthermore, the absence of human-annotated, token-level datasets limits the feasibility of training dedicated detection model. Existing methods (Guo et al., 2023; Yoon et al., 2024) rely on external models for token-level annotations, which can provide effective supervision signals but are both costly and constrained by the capabilities of the external models. To achieve automatic detection of critical tokens, we propose method called contrastive estimation. This approach Figure 4. Overview of aligning LLMs with critical tokens. The pipeline consists of two steps: (1) fine-tuning positive and negative models on correct and incorrect reasoning trajectories, and (2) applying contrastive estimation for cDPO. leverages models trained to learn patterns from both correct and incorrect reasoning trajectories. By comparing the token-level likelihoods generated by two separately trained models, one trained on correct reasoning trajectories and the other on incorrect ones, contrastive estimation is possible to effectively identify critical tokens that contribute to incorrect outcomes. As shown in Fig. 3, the contrastive estimation probability naturally highlight tokens that contribute to incorrect reasoning outcomes. log st = (1+β) log p(ytx, y<t)β log q(ytx, y<t)log (1) where β is scaling hyperparameter, and denote the positive model (trained on correct trajectories) and the negative model (trained on incorrect trajectories), and log is the partition function in the softmax computation. Specifically, for each token within an incorrect reasoning trajectory, low st indicates low likelihood under the correct pattern and high likelihood under the incorrect pattern, signifying the presence of critical tokens. 3. cDPO The previous section has demonstrated the existence of critical tokens within an incorrect reasoning trajectory, and one could identify those tokens through contrastive estimation. Following this, natural idea is to align such information during the preference optimization. Consequently, in this section, we first train the positive and negative models separately. We further extend the DPO from example-level to token-level and utilize such contrastive signals as tokenlevel rewards for preference optimization. Model Training for Contrastive Estimation Building on the aforementioned strategy of contrastive estimation, the primary objective is to develop models that can effectively estimate wide range of both correct and incorrect reasoning distribution. As shown in step 1 of Fig. 4, to train those models, we collect wide range of reasoning trajectories based on the sampling strategy: Given dataset = {(xi, yi)}M i=1, we utilize pre-trained LLM to firstly decode reasoning trajectories with times sampling. Then, we verify the outcome results based on the groud-truth labels yi, which yields ki positive reasoning trajectories and ki negative reasoning trajectories, which is denoted as: Dp = {(xi, {yp Dn = {(xi, {yn i,j}k i,j}N j=1)}M i=1 j=k+1)}M i=1 For positive model, we randomly select one correct trajectory since we expect the model to be decisive with its own correct reasoning paths; while for negative model, we select top-p percent of incorrect trajectories, since we expect the model would identify various types of error patterns. Finally, we train our model on those two datasets separately. Preparing Token-level Preference Dataset To identify and leverage critical tokens, we focus on computing scores 4 for tokens within incorrect trajectories. Critical tokens in these trajectories play key role in causing errors, while other tokens may still be correct. Assigning token-level scores in the incorrect trajectory enables precise penalization of critical tokens without adversely affecting correct ones. On the other hand, scoring correct trajectories to encourage specific tokens, where most tokens are already correct, could unintentionally penalize other valid tokens and result in undesirable distribution shifts in DPO (Rafailov et al., 2024; Xu et al., 2024). Therefore, in constructing the token-level preference dataset, we focus exclusively on scoring tokens within incorrect trajectories. As shown in step 2 of Fig. 4, for each prompt xi, we construct positive and negative pairs (yp , yn ) by randomly selecting one correct trajectory as yp and the most frequent incorrect trajectory as yn . When it comes to estimating critical tokens within negative examples, we utilize Eq. 1 to automatically annotate the contrastive likelihood sn = [sn ] of each token, where Ti is the total length of negative sample yi. Consequently, we obtain the preference dataset with contrastive likelihoods, which is Dce = {(xi, yp i,1, ...sn i,t, ...sn i,Ti )}M i=1. , sn , yn Critical Token Rewards in DPO DPO algorithm utilizes the policy model itself as reward model. The objective of which could be formulated as: ϕ(x, y) = γ log πθ(y x) πref(y x) ℓDPO = (cid:88) i=1 log σ(ϕ(xi, yp ) ϕ(xi, yn )) (2) (3) where πθ(x)and πref(yx) denotes the policy model and the reference model respectively. γ is the coefficient for the KL divergence penalty, and ϕ(x, y) represents an implicit reward function (ignore the partition function). We further extend the sample-level DPO into token-level DPO. First, we modify the reward function ϕ(x, y) by incorporating the token-level scores sn as follows: ϕs(x, y, s) = γ (cid:88) t=1 (1 st) log πθ(ytx, y<t) πref(ytx, y<t) ℓcDPO = (cid:88) i=1 log σ(ϕ(xi, yp ) ϕs(xi, yn , sn )) (4) (5) where is the total length of the response y, and st represents the token-level reward score in cDPO for the t-th token. Lower values of st indicate tokens that are more influential in producing incorrect outcomes. By weighting each tokens contribution with 1 st , the model penalizes the probability of generating these critical tokens. This 5 Figure 5. The accuracy across models on GSM8K for critical tokens identified by contrastive estimation. The results highlight the effectiveness of contrastive estimation in identifying critical tokens and demonstrate that cDPO achieves the highest performance by leveraging token-level signal from Contrastive Estimation. token-level approach ensures that the model effectively reduces the likelihood of generating critical tokens, thereby improving the overall accuracy of the responses. 4. Experiments 4.1. Setup Model We conducted experiments on range of models, including the general-purpose models Llama-3-8B-base and Llama-3-70B-base (Dubey et al., 2024), as well as the domain-specific model DeepSeek-math-7B-base (Shao et al., 2024). Dataset We use two widely adopted math reasoning datasets for training and evaluation: GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021). For training, we sample from all questions in the training set to generate the data. For evaluation, we utilize the MATH500 subset, which is uniformly sampled and has distribution of difficulty levels and subjects that matches the full MATH test set, as demonstrated in Lightman et al. (2023). Additionally, for both training sampling and evaluation, we apply the few-shot prompt approach from Fu et al. (2023). Baseline Methods For comparison, we evaluated multiple baseline methods using the data generated from the process described in Section 3. For Supervised Fine-Tuning (SFT), we fine-tuned the model using the positive response set Dp. For preference optimization (PO) methods, we utilized the token-level annotated pair-wise preference dataset Dce. The baselines we compared include: GSM8K MATH500 Method"
        },
        {
            "title": "Baseline",
            "content": "+ SFT + DPO (Rafailov et al., 2024) + TokenDPO (Zeng et al., 2024) + DPO (Rafailov et al., 2024) + RPO (Pang et al., 2024) + cDPO (Ours) Llama-"
        },
        {
            "title": "DeepSeek",
            "content": "8B 56.4 61.2 59.7 62.3 70B 80.4 82.1 87.8 83. 59.6 67.5 67.9* 88.9 89.7 90.8* math-7B 64.1 67.1 66.5 69.6 63.1 68.9 72.9* Llama-"
        },
        {
            "title": "DeepSeek",
            "content": "Avg. 67.0 70.1 71.3 71.7 8B 16.8 17.2 17.0 17. 70B 42.2 43.0 41.2 42.2 70.5 75.4 77.2* 15.4 18.4 19.6* 39.8 43.8 45.6* math-7B 31.4 32.6 33.4 32.4 33.0 34.8 35.0* Avg. 30. 30.9 30.5 30.8 29.4 32.3 33.4* Table 1. Experimental results on GSM8K and MATH500 datasets. Our proposed method surpasses all the strong baselines at large margin on individual settings and average performance. * denotes the significance test where < 0.005. DPO We experimented with two different starting points for training: the base model and the SFT model. TDPO is token-level approach that enhances KL divergence regulation by incorporating forward KL divergence constraints at the token level. The SFT model is used as the starting point for training. We implemented TDPO using the publicly available implementation. RPO introduces an additional negative log-likelihood term, enhancing performance on inference tasks. We implemented it using HuggingFaces implementation and performed one iteration of training, starting with the base model. Contrastive Estimation In our contrastive estimation setup, we sampled each problem = 64 times and selected the top-p = 50% of incorrect trajectories to train the negative model q(). During the estimation process, we set the hyperparameter β to 1.0. Training Details We use LoRA adapters (Hu et al., 2022) to train the model. For both the positive and negative models, we train for 1 epoch with learning rate of 3e-4. For preference optimization training, we set γ = 1.0 and train for 3 epochs with learning rate of 2e-5 for all baseline methods. For our cDPO approach, since the token-level scores range between 0 and 1 (whereas in DPO, the scores were all 1), we simply increase the learning rate to 4e-5. 4.2. Main Results Table 1 presents the experimental results for various methods across the GSM8K and MATH500. Our proposed method consistently outperforms all baselines and other methods, achieving the highest scores across both datasets. Specifically, our approach reaches the highest scores with Llama-3 (90.8 for 70B) and DeepSeek (72.9). These results highlight the effectiveness of our method in leveraging the strengths of both large-scale models (Llama-3) and taskspecific models (DeepSeek). Similarly, on the MATH500 dataset, our method attains an average score of 33.4, marking significant improvement over the baseline (30.1) and other enhanced methods such as SFT and RPO. Notably, our approach yields the highest individual score with Llama-3 (45.6 for 70B) and performs robustly across all model configurations. The consistent performance improvements observed across various settings underscore the superiority of our method compared to existing techniques. The significance tests, which were conducted to verify the statistical reliability of these results, confirm the competitive advantage of our proposed approach. 4.3. Analysis Contrastive Estimation Can Identify Critical Tokens To evaluate the effectiveness of contrastive estimation in identifying critical tokens, we conducted experiments on GSM8K using aforementioned token-level preference dataset Dce. For each model, we selected 100 incorrect reasoning paths and assessed these paths using Contrastive Estimation. The tokens with the lowest scores in these paths were designated as critical tokens. For each critical token, we performed two types of sampling: With Critical Token: The model continued generating sequences critical token while keeping it unchanged. We sampled 64 completions. For the GSM8K dataset, our method achieves remarkable average score of 77.2, surpassing the Baseline and notable improvements such as those incorporating SFT and DPO. Without Critical Token: The critical token was replaced by forcing the model to decode an alternative token, and 64 completions were sampled under this condition. 6 The accuracy results for both settings, as well as the performance of our proposed method, cDPO, are shown in Figure 5. From the results, we observe that replacing critical tokens (Without Critical Token) consistently improves accuracy across all models compared to keeping them unchanged (With Critical Token). For example, in the DeepSeekmath-7B, the accuracy increases from 19.99% (With Critical Token) to 35.11% (Without Critical Token). Similar improvements are seen in Llama-3-8B and Llama-3-70B, where accuracy rises from 15.94% to 39.19% and from 27.08% to 39.28%, respectively. These results demonstrate that contrastive estimation effectively identifies critical tokens strongly associated with incorrect reasoning outcomes. Moreover, our proposed cDPO method outperforms both settings by achieving significantly higher accuracies on all models, demonstrating its capability to align reasoning paths effectively. Specifically, cDPO achieves accuracies of 65.69%, 69.09%, and 84.53% on DeepSeek-math-7B, Llama-3-8B, and Llama-3-70B, respectively. These results validate the ability of contrastive estimation to identify critical tokens in reasoning tasks and highlight the effectiveness of cDPO in leveraging this insight to enhance model performance."
        },
        {
            "title": "Method",
            "content": "GSM8K (top-25%) Llama-3-8B (top-50%) (top-75%) 67.85 67.93 66.33 Table 2. Accuracy comparison on GSM8K using cDPO with different thresholds (top-p) for selecting incorrect answer trajectories. Impact of Incorrect Trajectories Selection Thresholds To capture diverse error patterns, as described earlier, we select trajectories from the top-p incorrect set. Here, we explore the impact of different values on performance. The results in Table 2 indicate that selecting the top-50% of incorrect answer trajectories yields the best performance on the GSM8K, achieving an accuracy of 67.93%. This suggests that using moderate threshold (top-50%) allows the model to capture balanced range of critical error patterns without introducing excessive noise from less representative errors. In contrast, setting the threshold to top-25% or top75% results in slightly lower performance, with accuracies of 67.85% and 66.33%, respectively. smaller selection may fail to fully cover all critical errors, while larger selection may introduce non-critical errors, thus diminishing the models ability to effectively learn from critical tokens. Log Probability During Training To further investigate the effect of cDPO on training dynamics, Fig. 6 presents Figure 6. Log probabilities of chosen and rejected sequences during training on the GSM8K dataset using DPO, RPO, and cDPO. The solid lines represent chosen sequences, while the dashed lines represent rejected sequences. The figure demonstrates how cDPO achieves greater separation between chosen and rejected sequences compared to DPO and RPO. the log probability trends for chosen and rejected sequences over training steps on the GSM8K dataset using the Llama-38B model across DPO (from base model), RPO, and cDPO. As shown in Fig. 6, our proposed cDPO method achieves clear separation between chosen and rejected sequences by significantly increasing the log probability of correct trajectories while sharply reducing the likelihood of incorrect ones. In comparison, RPO (DPO + NLL) raises the probability of correct trajectories through the introduction of an NLL loss term, but its reduction of incorrect response probabilities is less pronounced. DPO, on the other hand, achieves substantial decrease in the probability of generating incorrect trajectories but at the cost of also lowering the probability of correct trajectories. This indicates that cDPO offers balanced approach, effectively enhancing correct output probability while minimizing critical errors, surpassing the performance of both DPO and RPO. 5. Related Work 5.1. Contrastive Estimation Contrastive estimation is technique used primarily in statistical modeling and machine learning to estimate model parameters by contrasting observed data with artificially constructed noise data. The core idea is to improve parameter estimation by comparing the likelihood of the observed data against the likelihood of other, less plausible data. line of work improves the performance of contrastive estimation (Gutmann & Hyvarinen, 2010; Bose et al., 2018; He et al., 2020; Denize et al., 2023). 7 Contrastive Decoding More specifically, our approach is quite related to contrastive decoding (CD), which is one of the downstream applications of contrastive estimation. Contrastive Decoding (Li et al., 2023) implements strategy that contrasts the token distribution likelihoods between expert and amateur models during decoding. As outlined in OBrien & Lewis (2023), This approach avoids the selection of high-probability but low-quality tokens, thereby ensuring the fluency and coherence of the generated text. Subsequent studies further highlight CDs potential in improving factuality (Zhang et al., 2023; Yang et al., 2024), knowledge editing (Bi et al., 2024), safety (Zhao et al., 2024), and reasoning (OBrien & Lewis, 2023; Shi et al., 2024). Specifically, Zhang et al. (2023) developed the Induce-then-Contrast Decoding (ICD) strategy, which improves factuality by contrasting original LLM predictions with those from hallucinatory LLMs. Similarly, Yang et al. (2024) introduced hallucinatory/truthful comparators during decoding to mitigate hallucinations. Zhao et al. (2024) proposed Decoding by Contrasting Knowledge (DeCK) to editing of stubborn knowledge. Zhao et al. (2024) proposes Adversarial Contrastive Decoding (ACD), prompt-based approach that enhances safety performance without training; Additionally, OBrien & Lewis (2023) demonstrated that CD improves performance on reasoning tasks, effectively preventing common reasoning errors such as missing steps or semantic misunderstandings (Wang et al., 2023). Furthermore, Shi et al. (2024) showed that unchosen experts in Mixture-of-Experts (MoE) model can be used for CD, thereby improving the reasoning capabilities of models. Different from those works that focus on the inference of contrastive decoding, in this paper, we mainly utilize contrastive decoding ideas to identify the critical tokens that significantly affect the correctness of reasoning process. 5.2. Reinforcement Learning Algorithm Reinforcement Learning from Human Feedback (RLHF) has demonstrated significant improvements in alignment of LLMs. In the process of RLHF, the reward model learns human preferences and subsequently utilized to guide policy optimization. Proximal Policy Optimization (PPO) is commonly used in this process and set of improvement further optimize training efficiency and robustness (Zheng et al., 2023; Casper et al., 2023; Gao et al., 2023; Li et al., 2024; Santacroce et al., 2023; Rame et al., 2024). Notably, Rafailov et al. (2024) introduces Direct Preference Optimization (DPO), which increases the relative probability of preferred responses over dispreferred ones, thereby avoiding the need to explicitly learn reward function. Subsequently, several works have been devoted to further refining DPO from various perspectives. Azar et al. (2024) developed Identity Preference Optimization (IPO) to ad8 dress DPOs susceptibility to overfitting. Amini et al. (2024) proposed DPO with an offset (ODPO) to address the limitation of DPOs binary preference data, which fails to account for the varying degrees of preference between responses. Through success, one of the significant challenges of DPO still remains, the sparse nature of reward signals at the sequence level fails to point out subtle differences at the token level. Consequently, it causes the failure modes of DPO optimization (Pang et al., 2024; Pal et al., 2024; Feng et al., 2024). To address this, Lai et al. (2024) utilize human or GPT-4 validation to identify incorrect reasoning steps; Guo et al. (2023); Yoon et al. (2024) engage external LLMs to revise given responses, deriving token-level preferences from the comparison of preand post-revision changes. In contrast to those works, this paper aims to design an automatic process supervision strategy, that is fully free of human annotation and could be easily scaled up. specifically, We utilize contrastive estimation to identify critical token, providing token-level signal for preference optimization, which can effectively enhence LLMs reasoning capability. 6. Conclusion We identify the critical token that plays vital role within LLMs reasoning trajectory. Specifically, we find that, for an incorrect reasoning trajectory, when critical tokens are decoded, LLMs tends to produce negative outcomes, while if LLMs are forced to decode other tokens instead of critical tokens within that trajectory, they tend to produce positive results. Moreover, other tokens do not exhibit this decisive property. Consequently, we propose contrastive estimation strategy to identify the critical tokens of the negative trajectories and design token-level contrastive DPO algorithm for learning the critical token information. Experimental results demonstrate that our proposed method significantly outperforms the baseline strategies such as DPO and RPO, and yields 77.2% and 33.4% performance on GSM8k and MATH500 - two widely used benchmarks respectively."
        },
        {
            "title": "Impact Statement",
            "content": "In our research, we focus exclusively on developing models for solving mathematical problems, which inherently minimizes common ethical concerns typically associated with AI applications in broader domains. The primary function of our models is to enhance computational accuracy and efficiency in mathematical tasks, ensuring that any potential bias, privacy issues, or harmful outputs are effectively nonexistent. Since our dataset consists solely of mathematical questions, it does not involve personal, sensitive, or controversial information that could lead to ethical dilemmas."
        },
        {
            "title": "References",
            "content": "Amini, A., Vieira, T., and Cotterell, R. Direct prefarXiv preprint erence optimization with an offset. arXiv:2402.10571, 2024. Azar, M. G., Guo, Z. D., Piot, B., Munos, R., Rowland, M., Valko, M., and Calandriello, D. general theoretical paradigm to understand learning from human preferences. In International Conference on Artificial Intelligence and Statistics, pp. 44474455, 2024. Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. Bi, B., Liu, S., Mei, L., Wang, Y., Ji, P., and Cheng, X. Decoding by contrasting knowledge: Enhancing llms confidence on edited facts. arXiv preprint arXiv:2405.11613, 2024. Bose, A. J., Ling, H., and Cao, Y. Adversarial contrastive estimation. arXiv preprint arXiv:1805.03642, 2018. Casper, S., Davies, X., Shi, C., Gilbert, T. K., Scheurer, J., Rando, J., Freedman, R., Korbak, T., Lindner, D., Freire, P., et al. Open problems and fundamental limitations of reinforcement learning from human feedback. arXiv preprint arXiv:2307.15217, 2023. Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Denize, J., Rabarisoa, J., Orcesi, A., Herault, R., and Canu, S. Similarity contrastive estimation for self-supervised soft contrastive learning. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 27062716, 2023. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Feng, D., Qin, B., Huang, C., Zhang, Z., and Lei, W. Towards analyzing and understanding the limitations arXiv preprint of dpo: theoretical perspective. arXiv:2404.04626, 2024. Fu, Y., Ou, L., Chen, M., Wan, Y., Peng, H., and Khot, T. Chain-of-thought hub: continuous effort to measure large language models reasoning performance. arXiv preprint arXiv:2305.17306, 2023. Gao, L., Schulman, J., and Hilton, J. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pp. 1083510866, 2023. Guo, G., Zhao, R., Tang, T., Zhao, W. X., and Wen, J.- R. Beyond imitation: Leveraging fine-grained quality signals for alignment. arXiv preprint arXiv:2311.04072, 2023. Gutmann, M. and Hyvarinen, A. Noise-contrastive estimation: new estimation principle for unnormalized statistical models. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 297304. JMLR Workshop and Conference Proceedings, 2010. He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 97299738, 2020. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. Hu, E. J., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W., et al. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. Lai, X., Tian, Z., Chen, Y., Yang, S., Peng, X., and Jia, J. Step-dpo: Step-wise preference optimization for long-chain reasoning of llms. arXiv preprint arXiv:2406.18629, 2024. Li, X. L., Holtzman, A., Fried, D., Liang, P., Eisner, J., Hashimoto, T. B., Zettlemoyer, L., and Lewis, M. Contrastive decoding: Open-ended text generation as optimization. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1228612312, 2023. Li, Z., Xu, T., Zhang, Y., Lin, Z., Yu, Y., Sun, R., and Luo, Z.-Q. Remax: simple, effective, and efficient reinforcement learning method for aligning large language models. In Forty-first International Conference on Machine Learning, 2024. 9 Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. Shi, C., Yang, C., Zhu, X., Wang, J., Wu, T., Li, S., Cai, D., Yang, Y., and Meng, Y. Unchosen experts can contribute too: Unleashing moe models power by self-contrast. arXiv preprint arXiv:2405.14507, 2024. Lin, Z., Gou, Z., Gong, Y., Liu, X., Shen, Y., Xu, R., Lin, C., Yang, Y., Jiao, J., Duan, N., et al. Rho-1: Not all tokens are what you need. arXiv preprint arXiv:2404.07965, 2024. Luo, L., Liu, Y., Liu, R., Phatale, S., Lara, H., Li, Y., Shu, L., Zhu, Y., Meng, L., Sun, J., et al. Improve mathematical reasoning in language models by automated process supervision. arXiv preprint arXiv:2406.06592, 2024. OBrien, S. and Lewis, M. Contrastive decoding improves reasoning in large language models. arXiv preprint arXiv:2309.09117, 2023. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. Pal, A., Karkhanis, D., Dooley, S., Roberts, M., Naidu, S., and White, C. Smaug: Fixing failure modes of preference optimisation with dpo-positive. arXiv preprint arXiv:2402.13228, 2024. Pang, R. Y., Yuan, W., Cho, K., He, H., Sukhbaatar, S., and Weston, J. Iterative reasoning preference optimization. arXiv preprint arXiv:2404.19733, 2024. Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. Rame, A., Vieillard, N., Hussenot, L., Dadashi, R., Cideron, G., Bachem, O., and Ferret, J. Warm: On the benefits of weight averaged reward models. In Forty-first International Conference on Machine Learning, 2024. Santacroce, M., Lu, Y., Yu, H., Li, Y., and Shen, Y. Efficient rlhf: Reducing the memory usage of ppo. arXiv preprint arXiv:2309.00754, 2023. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Zhang, M., Li, Y., Wu, Y., and Guo, D. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Tang, Y., Guo, Z. D., Zheng, Z., Calandriello, D., Munos, R., Rowland, M., Richemond, P. H., Valko, M., Pires, B. A., and Piot, B. Generalized preference optimization: unified approach to offline alignment. arXiv preprint arXiv:2402.05749, 2024. Wang, B., Min, S., Deng, X., Shen, J., Wu, Y., Zettlemoyer, L., and Sun, H. Towards understanding chain-of-thought prompting: An empirical study of what matters. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 27172739, 2023. Wang, P., Li, L., Shao, Z., Xu, R., Dai, D., Li, Y., Chen, D., Wu, Y., and Sui, Z. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 94269439, 2024. Xu, S., Fu, W., Gao, J., Ye, W., Liu, W., Mei, Z., Wang, G., Yu, C., and Wu, Y. Is dpo superior to ppo for llm alignment? comprehensive study. In Forty-first International Conference on Machine Learning, 2024. Yang, D., Xiao, D., Wei, J., Li, M., Chen, Z., Li, K., and Zhang, L. Improving factuality in large language models via decoding-time hallucinatory and truthful comparators. arXiv preprint arXiv:2408.12325, 2024. Yoon, E., Yoon, H. S., Eom, S., Han, G., Nam, D., Jo, D., On, K.-W., Hasegawa-Johnson, M., Kim, S., and Yoo, C. Tlcr: Token-level continuous reward for finegrained reinforcement learning from human feedback. In Findings of the Association for Computational Linguistics ACL 2024, pp. 1496914981, 2024. Zeng, Y., Liu, G., Ma, W., Yang, N., Zhang, H., and Wang, J. Token-level direct preference optimization. arXiv preprint arXiv:2404.11999, 2024. Zhang, Y., Cui, L., Bi, W., and Shi, S. Alleviating hallucinations of large language models through induced hallucinations. arXiv preprint arXiv:2312.15710, 2023. Zhao, Z., Zhang, X., Xu, K., Hu, X., Zhang, R., Du, Z., Guo, Q., and Chen, Y. Adversarial contrastive decoding: Boosting safety alignment of large language models via opposite prompt optimization. arXiv preprint arXiv:2406.16743, 2024. 10 Zheng, R., Dou, S., Gao, S., Hua, Y., Shen, W., Wang, B., Liu, Y., Jin, S., Liu, Q., Zhou, Y., et al. Secrets of rlhf in large language models part i: Ppo. arXiv preprint arXiv:2307.04964, 2023. Ziegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D., Christiano, P., and Irving, G. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019."
        }
    ],
    "affiliations": []
}