{
    "paper_title": "Focal Guidance: Unlocking Controllability from Semantic-Weak Layers in Video Diffusion Models",
    "authors": [
        "Yuanyang Yin",
        "Yufan Deng",
        "Shenghai Yuan",
        "Kaipeng Zhang",
        "Xiao Yang",
        "Feng Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The task of Image-to-Video (I2V) generation aims to synthesize a video from a reference image and a text prompt. This requires diffusion models to reconcile high-frequency visual constraints and low-frequency textual guidance during the denoising process. However, while existing I2V models prioritize visual consistency, how to effectively couple this dual guidance to ensure strong adherence to the text prompt remains underexplored. In this work, we observe that in Diffusion Transformer (DiT)-based I2V models, certain intermediate layers exhibit weak semantic responses (termed Semantic-Weak Layers), as indicated by a measurable drop in text-visual similarity. We attribute this to a phenomenon called Condition Isolation, where attention to visual features becomes partially detached from text guidance and overly relies on learned visual priors. To address this, we propose Focal Guidance (FG), which enhances the controllability from Semantic-Weak Layers. FG comprises two mechanisms: (1) Fine-grained Semantic Guidance (FSG) leverages CLIP to identify key regions in the reference frame and uses them as anchors to guide Semantic-Weak Layers. (2) Attention Cache transfers attention maps from semantically responsive layers to Semantic-Weak Layers, injecting explicit semantic signals and alleviating their over-reliance on the model's learned visual priors, thereby enhancing adherence to textual instructions. To further validate our approach and address the lack of evaluation in this direction, we introduce a benchmark for assessing instruction following in I2V models. On this benchmark, Focal Guidance proves its effectiveness and generalizability, raising the total score on Wan2.1-I2V to 0.7250 (+3.97\\%) and boosting the MMDiT-based HunyuanVideo-I2V to 0.5571 (+7.44\\%)."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 2 1 ] . [ 1 7 8 2 7 0 . 1 0 6 2 : r Focal Guidance: Unlocking Controllability from Semantic-Weak Layers in Video Diffusion Models Yuanyang Yin1,2,3 Yufan Deng3 Kaipeng Zhang2 Xiao Yang3 Shenghai Yuan3 Feng Zhao1* 1MoE Key Lab of BIPC, USTC 2Shanghai Innovation Institute 3ByteDance China"
        },
        {
            "title": "Abstract",
            "content": "The task of Image-to-Video (I2V) generation aims to synthesize video from reference image and text prompt. This requires diffusion models to reconcile high-frequency visual constraints and low-frequency textual guidance during the denoising process. However, while existing I2V models prioritize visual consistency, how to effectively couple this dual guidance to ensure strong adherence to the text prompt remains underexplored. In this work, we observe that in Diffusion Transformer (DiT)-based I2V models, certain intermediate layers exhibit weak semantic responses (termed Semantic-Weak Layers), as indicated by measurable drop in text-visual similarity. We attribute this to phenomenon called Condition Isolation, where attention to visual features becomes partially detached from text guidance and overly relies on learned visual priors. To address this, we propose Focal Guidance (FG), which enhances the controllability from Semantic-Weak Layers. FG comprises two mechanisms: (1) Fine-grained Semantic Guidance (FSG) leverages CLIP to identify key regions in the reference frame and uses them as anchors to guide Semantic-Weak Layers. (2) Attention Cache transfers attention maps from semantically responsive layers to Semantic-Weak Layers, injecting explicit semantic signals and alleviating their over-reliance on the models learned visual priors, thereby enhancing adherence to textual instructions. To further validate our approach and address the lack of evaluation in this direction, we introduce benchmark for assessing instruction following in I2V models. On this benchmark, Focal Guidance proves its effectiveness and generalizability, raising the total score on Wan2.1-I2V to 0.7250 (+3.97%) and boosting the MMDiT-based HunyuanVideo-I2V to 0.5571 (+7.44%). 1. Introduction Propelled by diffusion Transformers (DiT) [21, 33, 35, 41, 51, 52], the field of Text-to-Video (T2V) generation has achieved remarkable progress [24, 18, 25, 26, 31, 34, 42, 53, 57, 69, 71]. Building upon this, the pursuit of finergrained controllability has led researchers to extend the paradigm from text-driven synthesis to video generation conditioned on both starting image and text prompt, known as the Image-to-Video (I2V) task [17, 23, 26, 39, 42, 57, 65, 73]. As direct extension of the T2V paradigm, I2V task usually incorporates starting frame as visual anchor to ensure high fidelity in subject appearance while text prompt guides the dynamic evolution of the video content. Pioneering works such as WAN [57] and HunyuanVideo [26] have already validated the efficacy of the I2V framework, showcasing its significant potential for producing high-fidelity videos with controllable dynamics. Despite its promise, central challenge in Image-to-Video (I2V) generation lies in harmonizing the conditioning signals from the initial frame and the text prompt during the denoising process. Ideally, the model must preserve high-frequency visual details (e.g. subject identity, texture, and style) from the reference image while faithfully executing the motion and semantic transformations dictated by the text. However, even the state-of-the-art I2V models [26, 42, 57, 66] struggle to maintain this delicate balance, frequently prioritizing the visual condition and internal priors over textual directives (as shown in Figure 4). Current I2V research, however, has predominantly focused on enhancing temporal consistency and aesthetic quality, leaving the fundamental issue of prompt adherence relatively under-explored. Attempts to address this problem are often indirect and limited to the training phase. For instance, some methods initialize I2V models with weights from T2V model, hoping to inherit its strong textresponsiveness [26, 42, 57, 66]. Others employ techniques like crafting prompts that first describe the reference image in detail to encourage alignment between the two modalities [8]. Recent interpretability studies in Transformer and DiT-based generation models have shown that semantic responsiveness often varies across layers and that text-based conditioning signals can be partially overshadowed by visual priors [5, 6, 54, 70]. Therefore, principled understanding 1 Figure 1. Visualization of semantic alignment within the Wan2.1-I2V [57], quantified by the cosine similarity between visual features and keyword textual features. The features are sampled at evenly spaced inference steps and network layers. The heatmap reveals that the initial and final layers exhibit stronger and more accurate alignment with the target words, while several intermediate layers show noticeably degraded and noisy responses. of the underlying causes of prompt mis-alignment becomes essential. To move beyond current limitations in controllability and unlock the true potential of I2V, it is crucial to first diagnose and then rectify the underlying causes of this phenomenon. Our investigation reveals that poor prompt adherence in DiT-based I2V models originates from the emergence of Semantic-Weak Layers where Morans of textvisual similarity sharply declines from 0.76 to 0.19, indicating collapse in semantic alignment (see Figure 2 and Figure 1). These layers undermine text-driven guidance during denoising, ultimately impairing the models ability to follow semantic instructions. key factor that exacerbates this phenomenon is Condition Isolation where the three primary conditioning signals (the VAE-encoded reference frame, image encoder features, and text embeddings) are injected into the model in relatively isolated manner. This lack of finegrained alignment increases the likelihood that specific layers fail to establish precise correspondence between textual concepts and their visual counterparts in the initial frame, thereby reinforcing the tendency toward Semantic-Weak Layers and weakening prompt adherence. Based on these findings, we propose Focal Guidance, lightweight and principled framework that unlocks semantic controllability in DiT-based I2V models. It consists of two complementary mechanisms (shown in Figure 3 ). Finegrained Semantic Guidance (FSG) mitigates conditioning isolation by explicitly aligning textual keywords with their corresponding visual regions in the reference frame, enhancing cross-modal consistency. Attention Cache (AC) transfers structured attention patterns from semantically responsive layers to weaker ones, reinforcing textual guidance where it tends to collapse. Together, these mechanisms reestablish coherent textvisual correspondence across layers, significantly improving prompt adherence. To facilitate rigorous evaluation, we further introduce benchmark dedicated to assessing instruction-following in I2V models. Our contributions are summarized as follows: We identify Condition Isolation as the root cause of Semantic-Weak Layers, which in turn leads to poor prompt adherence in DiT-based I2V models, providing foundation for understanding controllability loss. We propose Focal Guidance, lightweight framework that directly addresses these issues through Fine-grained Semantic Guidance and the Attention Cache, enabling fine-grained semantic control. We introduce new benchmark for evaluating instructionfollowing in I2V models. On this benchmark, Focal Guidance boosts the performance of leading open-source models: improving Wan2.1-I2V by +3.97% and the MMDiTbased HunyuanVideo-I2V by +7.44%. 2. Related Work Image-to-Video Generation Models Image-to-Video (I2V) generation aims to synthesize dynamic video sequence from single static image, enabling richer digital content creation and visual storytelling. Early I2V methods primarily relied on motion priors, reference videos, or modeling specific physical phenomena such as fluids and 0All visual examples are from public benchmark datasets. in generative models. Across architectures like Transformers and ViTs, semantic responsiveness is non-uniform, with middle layers often being the least selective [5, 6]. Similarly, in diffusion models, U-Net mid-blocks exhibit weaker semantic expression [54], and textual signals can be overshadowed by visual priors, causing condition detachment [70]. This issue persists in DiT-based models, which show nonuniform text-visual similarity and delayed semantic emergence [19, 56]. While existing attention interventions offer general correctives [7], they do not address specific root cause. While these disparate findings hint at common problem, our work provides the first systematic diagnosis in the I2V domain. We identify Semantic-Weak Layers, trace their origin to mechanism we term Condition Isolation, and introduce Focal Guidance, targeted intervention built on this diagnosis to restore controllability. 3. Background and Problem Formulation In this section we introduce the fundamental principles of diffusion models then analyze two key issues, Semantic-weak Layers and Conditioning Isolation in I2V models, which directly affect the controllability. 3.1. Rectified Flow for Video Generation Recent state-of-the-art video generation models have increasingly adopted Rectified Flow [14, 30], an Ordinary Differential Equation (ODE) based generative framework known for its efficient sampling and straight training paths. Given video RF HW 3, it is first encoded by VAE encoder into latent representation z0 = E(v) RF W C, where the spatial dimensions are typically downsampled. Rectified Flow defines linear interpolation path between the data latent z0 and standard Gaussian noise sample z1 (0, I). For any time [0, 1], an intermediate latent zt on this path is given by: zt = (1 t)z1 + tz0. (1) The model is trained to predict the velocity field along this path. The objective is to minimize the difference between the predicted velocity vθ(zt, t, c) and the paths ground-truth constant velocity, which is (z0 z1): = Ez0,z1,c,t (cid:2)vθ(zt, t, c) (z0 z1)2 2 (cid:3) , (2) where represents the conditioning information (e.g., text and image embeddings). During inference, one starts with random noise sample z1 (0, I) and integrates the learned velocity field vθ from = 1 to = 0 using numerical ODE solver to obtain the final data latent 0. 3.2. Issues in Current DiT-based I2V Models (a) Morans of similarity between visual and textual features. (b) Standard deviation of similarity between visual and textual features. Figure 2. Statistical analysis of visual-textual similarity across 50 samples. We evaluate the semantic responsiveness of DiT layers by measuring Morans ( Figure 2a) and standard deviation ( Figure 2b) of normalized visual-textual similarity maps. Consistent with the results in Fig. 1, the initial and final layers show stronger and more stable responses to textual keywords, while intermediate layers exhibit weakened semantic alignment. hair [10, 11, 38, 40, 47, 49, 50, 60, 64, 74, 75], which constrained their generality and flexibility. With the emergence of U-Net-based diffusion models, approaches such as [9, 16, 27] introduced conditional control over the first frame by fusing the input image features with noise, while subsequent works like [65, 72] further improved conditioning through cross-attention layers, enhancing video fidelity and consistency. Leveraging advances from Text-to-Video (T2V) generation with DiT-based architectures [15, 32, 37], contemporary I2V methods [17, 23, 26, 39, 42, 57, 65, 73] build upon pre-trained T2V models to achieve higher visual fidelity, controllability, and temporal coherence, demonstrating the strong potential of the I2V paradigm. Controllable Video Generation Controllable video generation methods exploit explicit guidance through different signals: bounding boxes to guide object motion and appearance [13, 24, 28, 36, 58, 62, 68], trajectories for specific paths [43, 48, 63], or 3D camera parameters for perspective control [22, 59, 61, 67]. While effective, these approaches require precise external signals and labeled data, leaving intrinsic controllability of base I2V models underexplored. Interpretability and Conditioning in Generative Models Interpretability research reveals inconsistent conditioning Current DiT-based Image-to-Video (I2V) models have demonstrated remarkable progress in generating videos. 3 Nevertheless, they still face fundamental limitations that hinder their controllability, particularly in integrating the initial image and text prompt. At the core of these limitations lies the emergence of Semantic-Weak Layers, which over-rely on DiTs internal priors and weaken the textual influence. key structural reason behind this phenomenon is Conditioning Isolation, which restricts the interaction between visual and textual conditions. Together, Conditioning Isolation increases the likelihood of Semantic-Weak Layers, thereby reducing the I2V models ability to generate videos that are both visually consistent and text-faithful. Conditioning Isolation One major structural factor contributing to the emergence of the Semantic-Weak layers is the relatively independent injection of multiple conditioning signals, namely the VAE-encoded reference image zref RF W C, visual condition features cimg RN Dv extracted by an image encoder, and textual condition features ctext RM Dt obtained from text encoder. In cross-attentionbased architectures [57], zref is concatenated with the first-frame noise latent along the channel dimension, while ctext and cimg are injected via cross-attention mechanism. In MMDiT-style designs [26, 66], all condition tokens are concatenated along the token dimension before attention. Although these designs permit interaction within the Transformer layers, the three modalities originate from heterogeneous representation spaces: zref encodes high-frequency spatial details, ctext provides low-frequency semantic guidance, and cimg captures mid-level visual semantics. Without explicit pre-alignment, the model must learn spatialsemantic correspondences solely through generic attention weights, which is inherently difficult. As result, semantic entities in ctext often fail to align with their spatial counterparts in zref, producing weak grounding at the initial frame (as shown in Figure 1). This weak grounding propagates through temporal denoising, creating fertile ground for semantic-weak layers to emerge. Semantic-Weak Layers As direct consequence, current DiT-based I2V models exhibit semantic-weak layers which respond weakly to the text prompt. This weak semantic responsiveness suggests that the model, lacking strong textual guidance, may consequently default to its learned internal priors (i.e., generic motion patterns and stylistic biases learned from the large-scale pre-training dataset, rather than the specific instructions in the prompt). This weak semantic responsiveness reduces the constraint of textual instructions during denoising, leading to misalignment between the intended text-driven transformations and the generated video content as shown in Figure 5. To quantify semantic responsiveness, we evaluate each layers attention to the text prompt using two complementary metrics: Morans [12] and Standard Deviation of the normalized similarity maps between visual and textual features. Morans measures the spatial autocorrelation indicates whether the latent representation at given layer exhibits clear and spatially coherent response to the text feature. Let Al RF W denote the normalized similarity map between the visual features zl and the text features ctext at layer l. For each frame {1, . . . , }, we extract its 2D similarity map A(f ) and flatten it into {x(f ) . The Morans for given frame is computed as: RH }H i=1 (cid:80) (f ) = i,j wij(x(f ) (cid:80) i(x(f ) x(f ))(x(f ) x(f ))2 x(f )) , (3) where x(f ) is the mean attention value within frame , and wij is an element of spatial weight matrix, where wij = 1 if pixels and are adjacent (using 8-connectivity), and wij = 0 otherwise. The layer-wise Morans is then obtained by averaging across all frames: Il = 1 (cid:88) =1 (f ) . (4) We then define the layers as semantic-weak layers based on their lower Morans values. As complementary measure, we use the standard deviation of the normalized similarity to assess the distinctiveness of the text-conditioned attention patterns. higher standard deviation indicates more focused and less uniform attention pattern, reflecting more pronounced semantic significance. For each layer, we compute its score by averaging the standard deviations across all frames: Stdl = 1 (cid:88) =1 σ(A(f ) ), (5) where σ() denotes the standard deviation operator. 4. Method: Focal Guidance Framework This section presents Focal Guidance, framework addressing controllability failures stemming from Semantic-Weak Layers in DiT-based I2V models via two mechanisms: Finegrained Semantic Guidance (FSG), which couples multimodal conditions to reduce Conditioning Isolation, and Attention Cache, which transfers structured semantic attention from strong to weak layers to enhance semantic guidance. 4.1. Fine-grained Semantic Guidance (FSG) FSG is designed to alleviate the conditioning isolation observed in Semantic-Weak Layers by explicitly coupling textual concepts with their corresponding visual regions in the reference frame. Unlike conventional approaches that rely 4 Figure 3. Overview of the Focal Guidance framework. FG consists of two main components: Fine-grained Semantic Guidance and Attention Cache. (a) Fine-grained Semantic Guidance enhances the accuracy of information conditioning and reduces the models learning complexity by coupling the fine-grained relationships among the VAE-encoded reference frame, image encoder features, and text conditions. (b) Attention Cache leverages the semantic-responsive layers attention patterns to guide the injection of conditions into layers with weak semantic responses. solely on the Transformer to learn these associations implicitly, FSG injects visual anchors into both the text and visual features, thereby establishing fine-grained cross-modal correspondence before attention computation as shown in Figure 3(a). TextVisual Anchor Binding For each selected keyword from Eq. (6), we first project both the textual embedding tk RDt (from the language model e.g. T5) and its corresponding visual anchor vanchor,k RDv into the shared DiT latent space: Keyword Selection via TextImage Similarity Given text prompt and reference image Iref, we first employ the visual encoder in the I2V model Φimg() to extract spatial visual tokens from the second-to-last layer, denoted as n=1 RDv . Since the visual encoder is aligned cimg = {vn}N with text space (e.g. CLIP), we then use the associated text encoder Φtext(), to convert the prompt into text tokens m=1 RDv . For each text token tm, we compute its {tm}M negative cosine similarity [29] with every spatial position in cimg: Sm,n = mvn tm2 vn2 , (6) where vn denotes the visual token at spatial position n. Text words are selected into the keyword set if their maximum similarity maxn Sm,n exceeds predefined threshold τsel. The visual anchors Vanchor = are computed as weighted sums of the visual tokens vn based on the similarity scores Sk,n. We then inject the visual anchors into text and the visual features of the Smentic-Weak layers, thereby connecting the isolated conditions. n=1 Sk,n vn (cid:110)(cid:80)N kK (cid:111) ˆtk = Pt(tk), ˆvk,anchor = Pv(vanchor,k). (7) Then ˆtk and ˆvanchor are processed by each layer to produce query (Q), key (K), and value (V ) vectors. Let text = text ˆvanchor denote the value vectors of the text token and the visual anchor, respectively. We enhance the text tokens value by additive fusion: = vis ˆtk and vis text text + λtxt vis , (8) where λtxt controls the injection strength. This design enriches the text tokens content representation with spatially grounded visual cues while keeping its query and key vectors unchanged, ensuring the stability of attention patterns. Visual Anchor Injection into Latent Features Let zref denote the reference frame within the DiT layers hidden state. For each K, its corresponding spatial region Rk in zref is determined based on the similarity map Sk,n, where threshold is applied to extract the valid area. The visual anchor value representation Vvis is then directly injected into the latent feature map as follows: ref z(u,v) z(u,v) ref + λlat w(u,v) Vvis , (u, v) Rk, (9) Figure 4. Qualitative comparison of controllability across mainstream open-source I2V models. Existing methods often fail to reliably ground the text instruction in the first-frame reference, leading to instruction non-compliance and hallucinated (or duplicated) visual elements. Our FG approach strengthens textreference alignment, enabling more accurate instruction following and improved controllability.(All visual examples in this paper are from public benchmark datasets.) where w(u,v) denotes the normalized similarity weight at spatial location (u, v), and λlat controls the strength of the injection. This operation plants localized control signals into the generative latent space, ensuring that key objects are preserved and semantically aligned at each step. 4.2. Attention Cache Fine-grained Semantic Guidance resolves the issue of isolated condition information by establishing fine-grained binding between the text and the reference frame. This reduces the difficulty of coupling different modal conditions, but the process still relies on the self-modeling capacity of the current layer. To further enhance instructionfollowing ability, we propose Attention Cache mechanism that reuses attention from semantic-responsive layers to guide the Semantic-Weak layers. Specifically, the attention cache captures the similarity maps between the text and visual features at the semanticresponsive layers, which record their attention to the text conditioning, and uses it to guide the semantic-weak layers, as shown in Figure 3(b). Attention Aggregation For each layer at time step t, the similarity map Al is computed to quantify the cosine similarity between the keyword values RKF W (k)l and the visual features zl : At l,k(u, v) = (k)l t(u, v) zl 2 zl t(u, v)2 (k)l , (10) where zl (u, v)and (k)l is keyword values. t(u, v) is the visual features at spatial position We compute weighted sum of the attention maps across layers to get the attention cache: At cache = (cid:88) l=1 αlAt l, (11) where αl is the weight for the similarity map at layer (set to 0 for semantic-weak layers), and for other layers, αl = 1 Lm , where is the number of semantic-weak layers. Applying Attention Cache to Semantic-Weak Layers During both training and inference, the Attention Cache is utilized to guide the attention mechanism in semantic-weak layers. For each Sementic-Weak layer lw, we apply At cache to more accurately inject the text condition into the semantically corresponding regions. This reduces the tendency of these layers to rely solely on the visual priors for denoising, which would otherwise weaken the constraint imposed by the text condition. 6 Figure 5. Qualitative ablations on Wan2.1-I2V. We randomly sample cases along three dimensionsHuman Motion, Dynamic Attributes, and Human Interaction. With FG, textreference alignment is strengthened, motions and attributes follow instructions more faithfully. Specifically, for each keyword (obtained from the FSG procedure Equation (6)), we apply threshold to At cache,k, retaining only the positions with similarity greater than predefined threshold τcache: At cache,k 1{At cache,k>τcache} At (12) where 1{Sk,i>τcache} is an indicator function that sets all values below the threshold τcache to zero, ensuring that only the most relevant regions are preserved. cache,k , In line with the procedure in FSG, instead of directly using the text condition for localization, we employ the visual anchors value representation Vvis as guiding reference to assist in the injection of semantic information into SemanticWeak layers. The visual features zt of Semantic-Weak lw layers are updated by At zt lw cache,k Vvis , where corresponds to the k-th keyword. cache: + λcache At zt lw (13) 5. Experiments In this section, we quantitatively assess the effectiveness of Focal Guidance (FG) on two state-of-the-art opensource I2V models: Wan2.1-I2V (CrossDiT-based)[57] and HunyuanVideo-I2V (MMDiT-based)[26] under small-scale post-training that fine-tunes the Semantic-Weak layers. To address the current gap in evaluation metrics for I2V models, we introduce new benchmark designed specifically to assess the instruction-following capabilities of image to video generation models. The benchmark evaluates models across three key dimensions: dynamic attributes, human motion, and human interaction. 5.1. Experimental Setup Implementation Details We utilize an internally video dataset of 12K samples with accurate captions for fine-tuning. FG aims to enhance I2V model controllability with minimal post-training on limited data, and is model-agnostic, applicable to any I2V model. We evaluate FG on the CrossDiT-based Wan2.1-I2V [57] and the MMDiT-based HunyuanVideo-I2V [26], with full implementation details provided in Appendix A. Metric To fill the gap in existing I2V evaluation methods, we propose comprehensive benchmark assessing controllability across three dimensions: dynamic attributes, human motion, and human interaction. Each dimension is supported by manually annotated datasets and evaluated using videobased VQA framework [76] (see Appendix B). In addition, we adopt Subject Consistency and Background Consistency from the vbench2 beta i2v benchmark [76] to measure visual consistency with the reference frame. The final score is computed as the average across all dimensions, providing holistic measure of model performance. 5.2. Main Results To ensure fairness, all quantitative results are averaged over five random seeds. We evaluate FG on Wan I2V-14B using the vbench2 beta i2v benchmark [76]. As shown in Table 2, we make two key observations: 1) Additional post-training data has little impact on performance, confirming that FGs gains are not due to extra data; 2) Existing I2V metrics, which focus on video quality and consistency, fail to capture the improvements in model responsiveness to textual inMethod Param I2V Subject I2V Background Dynamic Attributes Human Motion Human Interaction Total Score CogVideoX-I2V Open-Sora Plan v1.3 LTX-Video Wan2.1-I2V Wan2.2-TI2V HunyuanVideo-I2V SkyReels-V2-I2V FG+Wan2.1-I2V FG+HunyuanVideo-I2V 5B 2.7B 13B 14B 5B 13B 14B 14B 13B 0.9658 0.9630 0.9845 0.9685 0.9858 0.9886 0.9867 0.9787 0.9781 0.9893 0.9870 0.9941 0.9942 0. 0.1279 0.1047 0.2558 0.3512 0.1512 0.1698 0.0465 0.6100 0.4300 0.4800 0.6920 0.7000 0.2600 0.7100 0.4500 0.4400 0.3500 0.4880 0.3700 0.1800 0.3200 0.6265 0.5832 0.6119 0.6973 0.6402 0.5185 0.6110 0.9694 (+0.09%) 0.9867 (-0.19%) 0.9875 (+0.05%) 0.9937 (-0.05%) 0.3860 (+9.91%) 0.2270 (+33.69%) 0.7500 (+8.38%) 0.3480 (+33.85%) 0.5320 (+9.02%) 0.2300 (+27.78%) 0.7250 (+3.97%) 0.5571 (+7.44%) Table 1. Quantitative comparison across open-source I2V models. Best scores are in bold; second best are underlined. The Total Score is the arithmetic mean of five metrics (I2V Subject, I2V Background, Dynamic Attributes, Human Motion, Human Interaction). Our Fine-grained Guidance (FG) delivers clear controllability gains across two mainstream architectures while preserving subject/background fidelity in image-to-video generation. Method Subject Consistency Background Consistency Motion Smoothness Dynamic Degree Aesthetic Quality Imaging Quality I2V Subject I2V Background Wan2.1-I2V Wan2.1-I2V w/ post-training FG+Wan2.1-I2V w/ zero-shot FG+Wan2.1-I2V w/ post-training 0.9375 0.9367 (-0.09%) 0.9388 (+0.14%) 0.9372 (-0.03%) 0.9691 0.9750 (+0.61%) 0.9741 (+0.52%) 0.9732 (+0.42%) 0.9765 0.9764 (-0.01%) 0.9764 (-0.01%) 0.9765(+0.00%) 0.5935 0.6423 (+8.22%) 0.5935(+0.00%) 0.6260 (+5.48%) 0.6324 0.6398 (+1.17%) 0.6412 (+1.39%) 0.6432 (+1.71%) 0.7089 0.7067 (-0.31%) 0.7088 (-0.01%) 0.7052 (-0.52%) 0.9685 0.9698 (+0.13%) 0.9711 (+0.27%) 0.9694 (+0.09%) 0.9870 0.9886 (+0.16%) 0.9889 (+0.19%) 0.9875 (+0.05%) Table 2. Impact of post-training data on conventional I2V metrics. Our post-training does not yield noticeable improvements on these metrics, which primarily focus on aesthetics and consistency, while lacking measures of instruction-following ability. structions. As shown in Figure 5, FG-Wan2.1 demonstrates higher responsiveness than Wan2.1, though this improvement is not reflected in traditional metrics, which emphasize reference-frame fidelity over instruction adherence. We retain the vbench beta i2v [76] consistency metrics I2V Subject and I2V Background to measure fidelity to the first-frame reference, and use Dynamic Attributes, Human Motion, and Human Interaction to assess instruction following. The average values across these five dimensions are then considered as the overall score. As shown in Table 1, Wan2.1-I2V with FG achieves the strongest semantic control, improving the Total Score by 3.97% (0.69730.7250). FG is also effective on the MMDiT-based HunyuanVideoI2V, where combining FG raises the Total Score by 7.44% (0.51850.5571). Method Dynamic Attributes Human Motion Human Interaction Wan2.1 Wan2.1 w/ post-training FG+Wan2.1 w/ zero-shot Wan2.1 w/ AC(post-training) Wan2.1 w/ FSG(post-training) FG+Wan2.1 w/ post-training 0.3512 0.3628 0.3512 0.3827 0.3804 0.3860 0.6920 0.6980 0.7020 0.7160 0.7280 0.7500 0.4880 0.5140 0.5220 0.5280 0.5240 0.5320 Table 3. Ablation study results on Wan2.1-I2V. Best scores are in bold and second best are underlined. FG achieves significant performance gains with minimal post-training. 5.3. Ablation Study We conduct comprehensive ablation study to disentangle the contributions of FG. We evaluate its impact on Dynamic Attributes, Human Motion, and Human Interaction against the Wan2.1-I2V baseline, with results summarized in Table 3, leading to the following conclusions: Limited Gains from Standard Post-training. Posttraining with small, general-purpose dataset yields marginal improvements (e.g., Human Motion: 0.6920 0.6980), establishing baseline but showing limited effectiveness due to the scale and general nature of the data. FG Delivers Strong Gains without Fine-tuning. Our FG module, without any post-training, significantly boosts performance on Human Motion (0.7020) and Human Interaction (0.5220), demonstrating its strong intrinsic capability to enhance semantic control. FG and Post-training are Synergistic. The full model combining both FG and post-training achieves the best performance, confirming the complementarity of explicit guidance and data-driven learning. 6. Conclusion, Limitation and Future Work We analyze the DiT-based I2V models and identify key issue: while most layers respond well to semantic instructions, certain layers called Semantic-Weak Layers are less sensitive to text prompts. This weak responsiveness limits the models ability to generate videos aligned with text, causing over-reliance on visual priors. To address this, we propose Focal Guidance which correct this issue and improves text controllability. We also design benchmark to automatically assess how well videos align with their corresponding prompts. While FSGs effectiveness is influenced by the underlying image encoder and the models base capabilities. If the base model is weak, FSG will be constrained, as it relies on accurate semantic injection through the Attention Cache and the models fundamental capabilities."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 12 [2] Wenbo Bao, Wei-Sheng Lai, Chao Ma, Xiaoyun Zhang, Zhiyong Gao, and Ming-Hsuan Yang. Depth-aware video frame interpolation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 37033712, 2019. 1 [3] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Yuanzhen Li, Tomer Michaeli, et al. Lumiere: space-time diffusion model for video generation. arXiv preprint arXiv:2401.12945, 2024. [4] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2256322575, 2023. 1 [5] Walid Bousselham, Angie Boggust, Sofian Chaybouti, Hendrik Strobelt, and Hilde Kuehne. Legrad: An explainability method for vision transformers via feature formation sensitivity. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2033620345, 2025. 1, 3 [6] Hila Chefer, Shir Gur, and Lior Wolf. Transformer interpretability beyond attention visualization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 782791, 2021. 1, [7] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. ACM transactions on Graphics (TOG), 42(4):110, 2023. 3 [8] Guibin Chen, Dixuan Lin, Jiangping Yang, Chunze Lin, Junchen Zhu, Mingyuan Fan, Hao Zhang, Sheng Chen, Zheng Chen, Chengcheng Ma, et al. Skyreels-v2: Infinite-length film generative model. arXiv preprint arXiv:2504.13074, 2025. 1 [9] Xinyuan Chen, Yaohui Wang, Lingjun Zhang, Shaobin Zhuang, Xin Ma, Jiashuo Yu, Yali Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Seine: Short-to-long video diffusion model for generative transition and prediction. In The Twelfth International Conference on Learning Representations, 2023. 3 [10] Chia-Chi Cheng, Hung-Yu Chen, and Wei-Chen Chiu. Time flies: Animating still image with time-lapse video as refIn Proceedings of the IEEE/CVF Conference on erence. Computer Vision and Pattern Recognition, pages 56415650, 2020. 3 [11] Yung-Yu Chuang, Dan Goldman, Ke Colin Zheng, Brian Curless, David Salesin, and Richard Szeliski. Animating pictures with stochastic motion textures. In ACM SIGGRAPH 2005 Papers, pages 853860. ACM, 2005. 3 [12] Csaba David, Kristof Giber, Katalin Kerti-Szigeti, Mihaly Kollo, Zoltan Nusser, and Laszlo Acsady. An image segmentation method based on the spatial correlation coefficient of local morans i. bioRxiv, pages 202305, 2023. 4 [13] Yufan Deng, Xun Guo, Yuanyang Yin, Jacob Zhiyuan Fang, Yiding Yang, Yizhi Wang, Shenghai Yuan, Angtian Wang, Bo Liu, Haibin Huang, et al. Magref: Masked guidance for anyreference video generation. arXiv preprint arXiv:2505.23742, 2025. [14] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis, 2024. 3 [15] Peng Gao, Le Zhuo, Ziyi Lin, Chris Liu, Junsong Chen, Ruoyi Du, Enze Xie, Xu Luo, Longtian Qiu, Yuhang Zhang, et al. Lumina-t2x: Transforming text into any modality, resolution, and duration via flow-based large diffusion transformers. arXiv preprint arXiv:2405.05945, 2024. 3 [16] Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi Yin, Devi Parikh, and Ishan Misra. Emu video: Factorizing text-to-video generation by explicit image conditioning. https://arxiv.org/abs/2311.10709, 2024. 3 [17] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. 1, 3 [18] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, et al. Ltx-video: Realtime video latent diffusion. arXiv preprint arXiv:2501.00103, 2024. 1 [19] Alec Helbling, Tuna Han Salih Meral, Ben Hoover, Pinar Yanardag, and Duen Horng Chau. Conceptattention: Diffusion transformers learn highly interpretable features. arXiv preprint arXiv:2502.04320, 2025. [20] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 13 [21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 1 [22] Chen Hou and Zhibo Chen. Training-free camera control for video generation. arXiv preprint arXiv:2406.10126, 2024. 3 [23] Yaosi Hu, Chong Luo, and Zhenzhong Chen. Make it move: controllable image-to-video generation with text descriptions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1821918228, 2022. 1, 3 [24] Hsin-Ping Huang, Yu-Chuan Su, Deqing Sun, Lu Jiang, Xuhui Jia, Yukun Zhu, and Ming-Hsuan Yang. Fine-grained controllable video generation via object appearance and context. In 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 36983708. IEEE, 2025. 3 [25] Yuming Jiang, Shuai Yang, Tong Liang Koh, Wayne Wu, Chen Change Loy, and Ziwei Liu. Text2performer: TextIn Proceedings of the driven human video generation. IEEE/CVF International Conference on Computer Vision, pages 2269022700, 2023. 1 [26] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 1, 3, 4, 7, 12 [27] Guojun Lei, Chi Wang, Hong Li, Rong Zhang, Yikai Wang, and Weiwei Xu. Animateanything: Consistent and controllable animation for video generation. arXiv preprint arXiv:2411.10836, 2024. 3 [28] Pengxiang Li, Kai Chen, Zhili Liu, Ruiyuan Gao, Lanqing Hong, Dit-Yan Yeung, Huchuan Lu, and Xu Jia. Trackdiffusion: Tracklet-conditioned video generation via diffusion models. In 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 35393548. IEEE, 2025. 3 [29] Yi Li, Hualiang Wang, Yiqun Duan, and Xiaomeng Li. Clip surgery for better explainability with enhancement in openvocabulary tasks. arXiv e-prints, pages arXiv2304, 2023. 5 [30] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. [31] Yu-Lun Liu, Yi-Tung Liao, Yen-Yu Lin, and Yung-Yu Chuang. Deep video frame interpolation using cyclic frame generation. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 87948802, 2019. 1 [32] Haoyu Lu, Guoxing Yang, Nanyi Fei, Yuqi Huo, Zhiwu Lu, Ping Luo, and Mingyu Ding. Vdt: General-purpose video diffusion transformers via mask modeling. arXiv preprint arXiv:2305.13311, 2023. 3 [33] He Lyu, Ningyu Sha, Shuyang Qin, Ming Yan, Yuying Xie, and Rongrong Wang. Advances in neural information processing systems. Advances in neural information processing systems, 32, 2019. 1 [34] Guoqing Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan, Shengming Yin, Changyi Wan, Ranchen Ming, Xiaoniu Song, Xing Chen, et al. Step-video-t2v technical report: The practice, challenges, and future of video foundation model. arXiv preprint arXiv:2502.10248, 2025. 1 [35] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. arXiv preprint arXiv:2401.08740, 2024. 1 [36] Wan-Duo Kurt Ma, John Lewis, and Bastiaan Kleijn. Trailblazer: Trajectory control for diffusion-based video generation. In SIGGRAPH Asia 2024 Conference Papers, pages 111, 2024. [37] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024. 3 [38] Aniruddha Mahapatra and Kuldeep Kulkarni. Controllable animation of fluid elements in still images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 36673676, 2022. 3 [39] Haomiao Ni, Changhao Shi, Kai Li, Sharon Huang, and Martin Renqiang Min. Conditional image-to-video generation with latent flow diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1844418455, 2023. 1, 3 [40] Makoto Okabe, Ken Anjyo, Takeo Igarashi, and Hans-Peter Seidel. Animating pictures of fluid using video examples. In Computer Graphics Forum, pages 677686. Wiley Online Library, 2009. 3 [41] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. 1 [42] Xiangyu Peng, Zangwei Zheng, Chenhui Shen, Tom Young, Xinying Guo, Binluo Wang, Hang Xu, Hongxin Liu, Mingyan Jiang, Wenjun Li, et al. Open-sora 2.0: Training commercialarXiv preprint level video generation model in 200 k. arXiv:2503.09642, 2025. 1, [43] Haonan Qiu, Zhaoxi Chen, Zhouxia Wang, Yingqing He, Menghan Xia, and Ziwei Liu. Freetraj: Tuning-free trajectory control in video diffusion models. arXiv preprint arXiv:2406.16863, 2024. 3 [44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 13 [45] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. [46] Axel Sauer, Kashyap Chitta, Jens Muller, and Andreas Geiger. Projected gans converge faster. Advances in Neural Information Processing Systems, 34:1748017492, 2021. 13 [47] Yoav Shalev and Lior Wolf. Image animation with perturbed masks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 36473656, 2022. 3 [48] Xiaoyu Shi, Zhaoyang Huang, Fu-Yun Wang, Weikang Bian, Dasong Li, Yi Zhang, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, et al. Motion-i2v: Consistent and controllable image-to-video generation with explicit motion In ACM SIGGRAPH 2024 Conference Papers, modeling. pages 111, 2024. 3 [49] Aliaksandr Siarohin, Stephane Lathuili`ere, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. First order motion model for image animation. Advances in neural information processing systems, 32, 2019. [50] Aliaksandr Siarohin, Oliver Woodford, Jian Ren, Menglei Chai, and Sergey Tulyakov. Motion representations for articulated animation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 13653 13662, 2021. 3 [51] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using In International confernonequilibrium thermodynamics. 10 ence on machine learning, pages 22562265. pmlr, 2015. 1 [52] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. 1 [53] Yu Tian, Jian Ren, Menglei Chai, Kyle Olszewski, Xi Peng, Dimitris Metaxas, and Sergey Tulyakov. good image generator is what you need for high-resolution video synthesis. In International Conference on Learning Representations, 2021. [54] Berk Tinaz, Zalan Fabian, and Mahdi Soltanolkotabi. Emergence and evolution of interpretable concepts in diffusion models. arXiv preprint arXiv:2504.15473, 2025. 1, 3 [55] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. 13 [56] Pedro Velez, Luisa Polanıa, Yi Yang, Chuhan Zhang, Rishabh Kabra, Anurag Arnab, and Mehdi SM Sajjadi. From image to video: An empirical study of diffusion representations. arXiv preprint arXiv:2502.07001, 2025. 3 [57] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 1, 2, 3, 4, 7, 12 [58] Jiawei Wang, Yuchen Zhang, Jiaxin Zou, Yan Zeng, Guoqiang Wei, Liping Yuan, and Hang Li. Boximator: Generating rich and controllable motions for video synthesis. arXiv preprint arXiv:2402.01566, 2024. 3 [59] Xi Wang, Robin Courant, Marc Christie, and Vicky Kalogeiton. Akira: Augmentation kit on rays for optical video generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 26092619, 2025. 3 [60] Yaohui Wang, Di Yang, Francois Bremond, and Antitza Dantcheva. image animator: Learning to animate images via latent space navigation. arXiv preprint arXiv:2203.09043, 2022. 3 Latent [61] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. [62] Jianzong Wu, Xiangtai Li, Yanhong Zeng, Jiangning Zhang, Qianyu Zhou, Yining Li, Yunhai Tong, and Kai Chen. Motionbooth: Motion-aware customized text-to-video generation. Advances in Neural Information Processing Systems, 37:3432234348, 2024. 3 [63] Weijia Wu, Zhuang Li, Yuchao Gu, Rui Zhao, Yefei He, David Junhao Zhang, Mike Zheng Shou, Yan Li, Tingting Gao, and Di Zhang. Draganything: Motion control for anything using entity representation. In European Conference on Computer Vision, pages 331348. Springer, 2024. 3 [64] Wenpeng Xiao, Wentao Liu, Yitong Wang, Bernard Ghanem, and Bing Li. Automatic animation of hair blowing in still portrait photos. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2296322975, 2023. 3 [65] Jinbo Xing, Menghan Xia, Yong Zhang, Haomiao Chen, Wendao Yu, Hong Liu, Guibo Liu, Xin Wang, Yanyan Shan, and Tin-Tin Wong. Dynamicrafter: Animating open-domain images with video diffusion priors. In European Conference on Computer Vision, pages 399417. Springer, 2024. 1, 3 [66] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 1, 4 [67] Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, Tien-Tsin Wong, Ying Shan, and Yonghong Tian. Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis. arXiv preprint arXiv:2409.02048, 2024. 3 [68] Shenghai Yuan, Jinfa Huang, Xianyi He, Yunyang Ge, Yujun Shi, Liuhan Chen, Jiebo Luo, and Li Yuan. Identitypreserving text-to-video generation by frequency decomposition. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1297812988, 2025. 3 [69] Shenghai Yuan, Jinfa Huang, Yujun Shi, Yongqi Xu, Ruijie Zhu, Bin Lin, Xinhua Cheng, Li Yuan, and Jiebo Luo. Magictime: Time-lapse video generation models as metamorphic simulators. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. [70] Zhixuan Zeng, Yuhao Chen, and Alexander Wong. Decoding diffusion: scalable framework for unsupervised analysis of latent space biases and representations using natural language prompts. arXiv preprint arXiv:2410.21314, 2024. 1, 3 [71] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng Shou. Show-1: Marrying pixel and latent diffusion models for text-to-video generation. arXiv preprint arXiv:2309.15818, 2023. 1 [72] David Junhao Zhang, Dongxu Li, Hung Le, Mike Zheng Shou, Caiming Xiong, and Doyen Sahoo. Moonshot: Towards controllable video generation and editing with multimodal conditions. arXiv preprint arXiv:2401.01827, 2024. 3 [73] Yiming Zhang, Zhening Xing, Yanhong Zeng, Youqing Fang, and Kai Chen. Pia: Your personalized image animator via plug-and-play modules in text-to-image models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 77477756, 2024. 1, 3 [74] Jian Zhao and Hui Zhang. Thin-plate spline motion model for image animation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 36573666, 2022. 3 [75] Ruiqi Zhao, Tianyi Wu, and Guodong Guo. Sparse to dense motion transfer for face image animation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 19912000, 2021. 3 [76] Dian Zheng, Ziqi Huang, Hongbo Liu, Kai Zou, Yinan He, Fan Zhang, Yuanhan Zhang, Jingwen He, Wei-Shi Zheng, Yu Qiao, et al. Vbench-2.0: Advancing video generation benchmark suite for intrinsic faithfulness. arXiv preprint arXiv:2503.21755, 2025. 7, 8, 13,"
        },
        {
            "title": "Appendix",
            "content": "Figure .1. Illustrative qualitative examples generated by FG-Wan2.1-I2V 14B across three dimensions: human motion, human interaction, and dynamic attribute changes. These cases demonstrate the models ability to produce realistic, temporally consistent, and semantically coherent video outputs under diverse scenarios. A. Experimental Setup Dataset As an efficient method to unlock controllability in I2V models, FG aims to enhance model generation control with minimal post-training on limited data. We utilize an internally curated dataset of 12K samples with accurate captions with accurate captions, generated using Qwen2.5-VL-32B [1]. The training objective is to teach the model this conditioning injection paradigm while preserving its original capabilities. This approach is model-agnostic, meaning it is applicable regardless of the underlying model or dataset. Implementation Details. We evaluate FG on the CrossDiT-based Wan2.1-I2V [57] and the MMDiT-based HunyuanVideoI2V [26]. For Wan2.1-I2V, we adopt the Wan I2V-14B-480P configuration and fine-tune the cross-attention layers in the Semantic-Weak Layers (layers 1126) using batch size of 8 and learning rate of 1e-5, while applying FG throughout. For HunyuanVideo-I2V, we inject visual anchors via CLIP encoder and apply FG to the single-stream layers 1732, training with batch size of 8 and learning rate of 1e-4. 12 Figure A.2. Visualization of reference images in our benchmark. We manually annotate subject bounding boxes on the original-resolution images and derive two canonical crops 16:9 and 1:1. Based on these annotations we can generate adaptive resolution reference images for image to video generation. B. Controllability Evaluation and Dataset Annotation Metric Design. Current evaluation metrics for Image-to-Video (I2V) generation primarily focus on visual quality and subject consistency [20, 4446, 55, 76], with limited attention to controllability. To fill this gap, we introduce three evaluation dimensions targeting instruction following: dynamic attributes, human motion, and human interaction. These dimensions enable comprehensive assessment of how well generated videos adhere to both textual and visual conditions, promoting semantically grounded alignment between the text prompt and the reference image. To accurately evaluate whether the actions or attributes in the first-frame image are faithfully generated according to the text, we adopt video-based multi-question answering (VQA) framework [76]. Constrained by the first-frame reference, I2V has lower content freedom than T2V, and this framework mitigates evaluation noise while ensuring consistency across prompts. For each prompt, we design multiple complementary (and occasionally slightly redundant) questions to robustly check instruction following: Answer = (cid:88) i=1 VQA(Qi, S), (14) where is the set of questions, is the video, and denotes the semantic structure of the prompt. The evaluation score is determined by whether all Figure B.3. Resolution statistics of reference images. 13 answers are correct. Dataset Annotation and Cropping. For the three evaluation dimensionsDynamic Attributes, Human Motion, and Human Interactionwe manually annotated datasets comprising 86, 100, and 100 imageprompt pairs, respectively, yielding 258, 278, and 303 corresponding questions. Following VBench-I2V [76], we annotate each image with the subjects bounding box and apply an aspect-ratioaware cropping protocol to ensure the subject remains visible in all crops. Specifically: (i) for portrait images (height > width), we first apply 16:9 crop and then 1:1 crop; (ii) for landscape images (width > height), we first apply 1:1 crop and then 16:9 crop. We maintain the original image resolutions; their distribution is shown in Figure B.3. C. Qualitative Results We present additional qualitative results on the best-performing model, FG-Wan2.1-I2V 14B, along three dimensions: human motion, human interaction, and dynamic attributes (as shown in Figure .1. These examples further illustrate the models strengths and its ability to generate realistic, coherent, and temporally expressive videos under various scenarios."
        }
    ],
    "affiliations": [
        "ByteDance",
        "MoE Key Lab of BIPC, USTC",
        "Shanghai Innovation Institute"
    ]
}