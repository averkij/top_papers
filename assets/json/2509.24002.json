{
    "paper_title": "MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP Use",
    "authors": [
        "Zijian Wu",
        "Xiangyan Liu",
        "Xinyuan Zhang",
        "Lingjun Chen",
        "Fanqing Meng",
        "Lingxiao Du",
        "Yiran Zhao",
        "Fanshi Zhang",
        "Yaoqi Ye",
        "Jiawei Wang",
        "Zirui Wang",
        "Jinjie Ni",
        "Yufan Yang",
        "Arvin Xu",
        "Michael Qizhe Shieh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "MCP standardizes how LLMs interact with external systems, forming the foundation for general agents. However, existing MCP benchmarks remain narrow in scope: they focus on read-heavy tasks or tasks with limited interaction depth, and fail to capture the complexity and realism of real-world workflows. To address this gap, we propose MCPMark, a benchmark designed to evaluate MCP use in a more realistic and comprehensive manner. It consists of $127$ high-quality tasks collaboratively created by domain experts and AI agents. Each task begins with a curated initial state and includes a programmatic script for automatic verification. These tasks demand richer and more diverse interactions with the environment, involving a broad range of create, read, update, and delete (CRUD) operations. We conduct a comprehensive evaluation of cutting-edge LLMs using a minimal agent framework that operates in a tool-calling loop. Empirical results show that the best-performing model, gpt-5-medium, reaches only $52.56$\\% pass@1 and $33.86$\\% pass^4, while other widely regarded strong models, including claude-sonnet-4 and o3, fall below $30$\\% pass@1 and $15$\\% pass^4. On average, LLMs require $16.2$ execution turns and $17.4$ tool calls per task, significantly surpassing those in previous MCP benchmarks and highlighting the stress-testing nature of MCPMark."
        },
        {
            "title": "Start",
            "content": "MCPMARK: BENCHMARK FOR STRESS-TESTING"
        },
        {
            "title": "REALISTIC AND COMPREHENSIVE MCP USE",
            "content": "Zijian Wu1,,, Xiangyan Liu1,,, Xinyuan Zhang2,, Lingjun Chen1,, Fanqing Meng4,, Lingxiao Du5,, Yiran Zhao1,, Fanshi Zhang2,3,, Yaoqi Ye1, Jiawei Wang2, Zirui Wang2, Jinjie Ni1, Yufan Yang2,3, Arvin Xu2,3,, Michael Qizhe Shieh1,"
        },
        {
            "title": "ABSTRACT",
            "content": "The MCP standardizes how LLMs interact with external systems, forming the foundation for general agents. However, existing MCP benchmarks remain narrow in scope: they focus on read-heavy tasks or tasks with limited interaction depth, and fail to capture the complexity and realism of real-world workflows. To address this gap, we propose MCPMark, benchmark designed to evaluate MCP use in more realistic and comprehensive manner. It consists of 127 high-quality tasks collaboratively created by domain experts and AI agents. Each task begins with curated initial state and includes programmatic script for automatic verification. These tasks demand richer and more diverse interactions with the environment, involving broad range of create, read, update, and delete (CRUD) operations. We conduct comprehensive evaluation of cutting-edge LLMs using minimal agent framework that operates in tool-calling loop. Empirical results show that the best-performing model, gpt-5-medium, reaches only 52.56% pass@1 and 33.86% pass^4, while other widely regarded strong models, including claude-sonnet-4 and o3, fall below 30% pass@1 and 15% pass^4. On average, LLMs require 16.2 execution turns and 17.4 tool calls per task, significantly surpassing those in previous MCP benchmarks and highlighting the stress-testing nature of MCPMark. mcpmark.ai eval-sys/mcpmark Figure 1: MCPMark evaluation pipeline with full state tracking. Each task begins from curated initial state with specific task instruction. The MCPMark-Agent then executes tool-calling loop, followed by programmatic verifier that evaluates whether all required checks are satisfied. 5 2 0 2 8 ] . [ 1 2 0 0 4 2 . 9 0 5 2 : r Student leads; listed in random order University of Singapore Correspond to: {zijian.wu, liu.xiangyan}@u.nus.edu 3 LobeHub 2 EvalSys Equal contribution Equal advising 4 Shanghai Jiao Tong University 1 TRAIL, National 5 Fudan University"
        },
        {
            "title": "Technical Report",
            "content": "Figure 2: Representative task instances, showing initial states (Top) and task instructions (Bottom). Examples include: Login with Cloudflare Turnstile in Playwright; CI/CD setup with ESLint in GitHub; weekend planner using tagged queries in Notion; schema design for project tracking in PostgreSQL; and contact extraction to CSV in Filesystem. All tasks show complex, multi-step workflows typical of real-world usage."
        },
        {
            "title": "INTRODUCTION",
            "content": "The Model Context Protocol (MCP) (Anthropic, 2024) is standardized interface that connects large language models (LLMs) (Comanici et al., 2025; OpenAI, 2025c; Team, 2025) with external systems such as tools, APIs, databases, and contextual resources. By standardizing the way LLMs access and operate on these systems, MCP allows agents to function more effectively with eyes and hands in real environments, and many see it as foundational layer for AI in the agentic era (Hou et al., 2025). Despite growing use in practice, existing MCP benchmarks remain limited: tasks often involve shallow or read-heavy interactions (Liu et al., 2025; Yin et al., 2025; Mo et al., 2025; Luo et al., 2025), leading to narrow range of task patterns. As result, they fail to capture the complex, multi-step workflows typical of real-world usage. This makes it difficult to probe the performance boundariesespecially in assessing whether current models and agents possess the necessary capabilities, such as reasoning, planning, long-context processing, and tool use, to tackle realistic and demanding agent tasks. To address these gaps, we introduce MCPMark, benchmark designed to simulate realistic user scenarios within mirrored or isolated container environments, accompanied by reliable automated evaluation. Specifically, MCPMark spans five representative MCP environments: Notion, GitHub, Filesystem, PostgreSQL and Playwright. Figure 1 presents an overview of the evaluation pipeline, where each task comprises three components: task instruction, initial state, and programmatic verification script. Figure 2 provides examples of task instructions and initial states. For task creation, after selecting or designing an initial state, task instruction and verification script are developed through humanAI collaborative pipeline, where domain experts and language agents iteratively co-design and refine each task. After this pipeline, we apply expert cross-review and community-level validation to ensure clarity, realism, and quality. Compared to existing MCP benchmarks, MCPMark offers significantly broader coverage of create, read, update, and delete (CRUD) operations across diverse workflows. In total, MCPMark comprises total of 127 tasks and 38 unique initial states, with 20 to 30 tasks in each MCP environment. To fairly evaluate model performance on these tasks, we introduce MCPMark-Agent, minimal and general agent framework that executes models through standardized tool-calling loop. MCPMark-Agent integrates with variety of MCP servers and model providers, enabling"
        },
        {
            "title": "Technical Report",
            "content": "Table 1: Benchmark Comparison. and"
        },
        {
            "title": "Benchmark",
            "content": "(OpenAI, automated"
        },
        {
            "title": "Average\nTurns",
            "content": "consistent evaluation grounded in the programmatic infrastructure defined by MCPMark. Comprehensive experiments on state-of-the-art models demonstrate the benchmarks difficulty. the best-performing model, Specifically, gpt-5-medium 2025c), achieves only 52.56% pass@1 and 33.86% pass^4, while other leading models such as claude-sonnet-4 (Anthropic, 2025a) and o3 (OpenAI, 2025d) fall below 30% pass@1 and 15% pass^4. On average, each task requires 16.2 execution turns and 17.4 tool calls, with some models such as kimi-k2-instruct (Team et al., 2025) averaging over 20 turns per task. Overall, as shown in Table 1, prior MCP benchmarks are limited by task patterns or verification rigor. In contrast, MCPMark integrates CRUD-diverse tasks, programmatic verification, and longer workflows, offering closer alignment with real-world MCP use and workflow complexity. Synthetic MCPEval LiveMCPBench CRUD-diverse LLM-as-judge Programmatic Read-heavy MCP-Universe LLM-as-judge N/A LiveMCP-101 CRUD-diverse Programmatic N/A 3.2 6.8 5."
        },
        {
            "title": "Hybrid",
            "content": "16.2 In addition, our evaluation reveals several consistent patterns that underscore the distinctive properties of the benchmark. First, the benchmark demonstrates its intrinsic difficulty through consistently low performance on the pass^4, which more convincingly reflects real-world conditions than commonly used metrics like pass@1 or pass@4 (Yao et al., 2024), emphasizing the challenge of solving tasks reliably and consistently across multiple runs. Second, performance varies substantially across different MCP environments, suggesting notable environment gap. This variation arises from differences in data availability and simulation fidelity: tasks involving local services such as the Filesystem are generally easier to emulate and more commonly represented in training data, whereas remote services like Notion require more complex, underrepresented interaction patterns that are harder to reproduce. Finally, the benchmark emphasizes efficient tool use: successful completions tend to involve fewer, more targeted tool calls, while failure cases often exhibit repetitive or exploratory interactions that fail to make meaningful progress. Collectively, these patterns show that MCPMark effectively surfaces key challenges in stability, generalization, and planning across diverse multi-component environments."
        },
        {
            "title": "2 MCPMARK: STRESS-TESTING COMPREHENSIVE MCP USE",
            "content": "In this section, we provide detailed introduction to MCPMark, including the benchmark construction process, the associated evaluation framework, and an overview of the benchmark."
        },
        {
            "title": "2.1 BENCHMARK CONSTRUCTION",
            "content": "MCP services and initial states. MCPMark integrates 5 MCP servers that span diverse and practical application environments. partial overview of each MCP tool set is shown in Figure 3 (right). Moreover, unlike prior work that uses generic or minimally initialized environments as task starting states (Liu et al., 2025; Luo et al., 2025; Yin et al., 2025), we carefully design initial states that reflect realistic and comprehensive usage scenarios, serving as the starting points for the tasks. Specifically: Notion connects to the official remote API for creating, editing, and querying both documents and databases. Initial states are instantiated from widely adopted templates. GitHub leverages the official remote API to support project management and Git operations, including CI/CD, issues, branches, pull requests, and commits. Initial states are derived from repositories with realistic development histories and configurations. Filesystem supports file I/O, directory organization, metadata inspection, and search. Initial states are curated folder structures that mirror everyday user scenarios. PostgreSQL provides access to relational database, with tools for schema exploration and SQL query execution. Initial states are representative template databases with realistic schemas. Playwright enables browser automation, offering commands for navigation, form completion, data extraction, and generating screenshots or PDF exports. Initial states come from two sources: self-authored webpages designed to test specific functionalities (e.g., login through Cloudflare) and localhost webpages adapted from WebArena (Zhou et al., 2023)."
        },
        {
            "title": "Technical Report",
            "content": "Figure 3: Task distribution and tool set overview of MCPMark. Left: 127 tasks distributed across 5 MCP servers and 38 curated initial states. Right: toolset per server, covering commonly used functionalities, with full support for CRUD operations in each corresponding MCP environment. Task creation pipeline. Each task in MCPMark is grounded in an initial state of the respective environment (e.g., template Notion page or designated website) and consists of natural language instruction paired with an automatic verification script. Constructing tasks of this form is difficult if we rely solely on humans or solely on agents. To address this, we design humanAI collaborative pipeline that pairs human experts with two agents: task creation agent and task execution agent. The pipeline proceeds in four steps: I. Exploration: Given an initial environment state, human expert and the task creation agent jointly explore the environment, guided by high-level instruction or topic informed by expertise and real-world experience. This stage aims to capture both wide overview of the environment and deep, specific context that will later support realistic and well-grounded task creation. II. Evolvement: The task creation agent proposes new task instruction or refines an existing one by introducing additional complexity. This may include removing unnecessary instructions, increasing the difficulty of information seeking, raising the processing burden (e.g., through longer input content), or requiring more interaction steps. The human expert ensures that the task remains practical, verifiable, and sufficiently challenging. III. Verification: The task creation agent drafts programmatic verification script. The human expert then completes the task with assistance from the task execution agent. Afterward, the verification script is executed and iteratively refined until it is fully consistent with the task instruction. To ensure reliability, the human expert also adjusts the final environment state to validate whether the script correctly detects both successful and unsuccessful outcomes. IV. Iteration: Steps II. and III. are repeated to progressively increase task difficulty, while preserving automatic verifiability and maintaining realism through authentic user scenarios. Overall, even with agent assistance, constructing each sample remains labor-intensive. Involving 10 experts with diverse backgroundsincluding computer science PhD students, front-end designers, full-stack & AI infra engineers, and AI investorseach task takes 3 5 hours of focused expert effort. While most tasks are built through the standard pipeline, experts occasionally leverage their accumulated experience or domain knowledge to directly write natural language instructions. In these cases, the task creation agent is bypassed, but the verification scripts are still generated and refined within the same pipeline. We defer the prompts and guidelines used in the task creation pipeline to Appendix A. Quality control. All tasks underwent cross-review by human experts and month-long community check to ensure clarity, consistency, and alignment with real-world application scenarios. In particular, for tasks that no model solved correctly, we conducted additional verification to ensure their validity. This process ensures that the benchmark remains challenging yet practical, and that evaluation outcomes are unambiguous."
        },
        {
            "title": "2.2 BENCHMARK OVERVIEW",
            "content": "Dataset statistics. We create total of 127 tasks across 5 MCP servers30 for Filesystem, 28 for Notion, 25 for Playwright, 23 for GitHub, and 21 for PostgreSQLbased on 38 curated initial states. On average, the task instructions contain 288.6 words, and the corresponding verification scripts consist of 209.8 lines of code. The detailed task distribution is presented in Figure 3 (left), while the corresponding toolsets for each MCP are shown in Figure 3 (right). Task characteristics. The tasks span wide range of realistic workflows, including updating nested properties in Notion, managing commits and pull requests in GitHub, automating interactive forms in Playwright, organizing complex directory structures in the Filesystem, and executing transactional updates in PostgreSQL. Five representative tasks, one from each MCP, are shown in Figure 2. Collectively, these tasks provide diverse CRUD coverage and reflect the challenges of authentic multi-step workflows across varied application scenarios."
        },
        {
            "title": "2.3 EVALUATION FRAMEWORK",
            "content": "State tracking and management. MCPMark executes all tasks within sandboxed environments that enforce explicit state tracking, design choice that ensures safety, reproducibility, and fair comparison across models. Each evaluation follows consistent lifecycle: ① tasks begin from well-defined initial state that mirrors realistic application scenarios, ② proceed with agent execution based on task instructions, and ③ conclude with an automatic verification script that programmatically checks whether the final environment satisfies the task requirements. After verification, ④ the environment is reset to its original state, preventing side effects and enabling repeated evaluation under identical conditions. Evaluation Agent. To standardize evaluation, we provide MCPMark-Agent, lightweight and general-purpose agent framework. It is built on LiteLLM together with the Model Context Protocol Python SDK to support compatibility and extensibility. Specifically, MCP servers are configured through the SDK, and their tools are exposed to the agent. LiteLLM then (1) converts the tools into the OpenAI function-call format and (2) routes requests to the official APIs of different providers, thereby ensuring execution that reflects each models native capabilities. During task evaluation, the agent follows tool-calling loop in which the model iteratively invokes MCP tools, interprets responses from MCP servers, and adjusts its actions. The loop terminates once the model produces final response without further tool calls. Although this agent framework is deliberately basic and omits optimizations that may be desirable in production systems (which we leave for future work), this design avoids task-specific heuristics and model-specific biases, thereby providing clearer measure of models intrinsic agentic capabilities in MCP environments."
        },
        {
            "title": "3 EXPERIMENTS",
            "content": "In this section, we describe the experimental setup, introduce the evaluated models and metrics, and present results and analyses on different environment, reasoning efforts, and failure patterns."
        },
        {
            "title": "3.1 EXPERIMENTAL SETUP",
            "content": "Models. We test range of state-of-the-art proprietary and open-source models, primarily accessed through LiteLLM. Proprietary models include gpt-5 (OpenAI, 2025c) with different reasoning effort levels (low, medium, high) and smaller variants (mini and nano), as well as earlier gpt-4.1 (OpenAI, 2025b) variants. We also evaluate claude-opus-4.1, claude-sonnet-4, grok-4, grok-code-fast-1, o3, o4-mini, qwen3-max, gemini-2.5-flash, and gemini-2.5-pro (Anthropic, 2025b;a; xAI, 2025; OpenAI, 2025d; Comanici et al., 2025). On the open-source side, we evaluate qwen3-coder-plus, kimi-k2-instruct, deepseek-v3.1, glm-4.5, and gpt-oss-120b (Team, 2025; Team et al., 2025; Liu et al., 2024; Zai, 2025; OpenAI, 2025a). We do not test small open-source models ( 100B) due to the difficulty of the benchmark. https://github.com/BerriAI/litellm https://github.com/modelcontextprotocol/python-sdk"
        },
        {
            "title": "Technical Report",
            "content": "Metrics. We use three complementary metrics to measure agent performance: pass@1, pass@4, and pass^4. Pass@1, captures the single-run success rate, i.e., the proportion of tasks successfully in one single attempt. Pass@4 measures success when allowing up to 4 independent runs, indicating whether repeated attempts improve coverage of difficult cases. Pass^4 is stricter measure: task is counted as correct only if all four independent runs succeed, making it strong indicator of model consistency and stability under stochastic generation (Yao et al., 2024). Implementation Details. We use MCPMark-Agent as the unified framework to benchmark MCP use across models. While specialized agent designs could further improve performance, we leave such optimizations as important future work. Each run is limited to maximum of 100 turns with 3600-second timeout. Unless otherwise specified, all models are evaluated under their default inference settings (e.g., temperature, top-p, reasoning effort). The agent supports two execution paths: general path via LiteLLM with function-calling tools and native path with direct tool support for certain models (e.g., Anthropic API for extended thinking mode). For MCP server selection, we generally choose the most commonly used ones (see Appendix for details)."
        },
        {
            "title": "3.2 MAIN RESULTS",
            "content": "We evaluate all 127 tasks using MCPMark-Agent, reporting pass@1, pass@4, and pass^4 metrics. Unless otherwise specified, pass@1 scores are averaged over four independent runs and reported as mean std. The full taskmodel results are provided in Appendix C, giving per-task detail beyond the overall metrics. Detailed results by MCP service are reported in Appendix D, and representative trajectories appear in Appendix E. MCPMark remains challenging for frontier models. Table 2 shows that the best-performing model, gpt-5-medium, reaches only 52.56% pass@1, while qwen3-coder-plus, the strongest opensource model, achieves 24.80%. Most proprietary models fall within the 15% to 30% range on pass@1, and several open-source models perform below 10%. Moreover, Table 9 highlights the high interaction demands of the benchmark: for example, qwen3-max and kimi-k2-instruct average 23.85/26.95 turns with 23.02/26.22 tool calls, respectively. These results underscore that MCPMark remains highly challenging benchmark for current frontier models. Models generally perform better on local service tasks. We observe from Table 2 that performance varies significantly across MCP services, showing clear divide between local and remote environments. Local services such as PostgreSQL, Filesystem, and Playwright achieve substantially higher success rates, with gpt-5-medium reaching 76.19%, 57.50%, and 43.00% pass@1 respectively. Remote services like Notion and GitHub remain challenging, with most models achieving below 25% pass@1. This gap likely stems from data availability: local services are easier to simulate and collect training data for, while remote service APIs require authentic interaction traces that are expensive to curate at scale. These results suggest that data remains key to enabling better MCP use. Robustness lags far behind. Table 2 demonstrates that pass@4 provides substantial gains, with gpt-5-medium and claude-sonnet-4 achieving 68.50% and 44.88% compared to just 52.56% and 28.15% for pass@1. However, the performance at pass^4 drops sharply to 33.86% and 12.60%, respectively, underscoring the models inconsistency and instability across runs. Similar discrepancies are observed across other models, with pass@4 often exceeding 30% while pass^4 remains in the 5% to 15% range, suggesting that while repeated attempts improve success, robustness under multi-turn tool use in MCP contexts remains common challengea shortcoming that poses significant risks for real-world deployment where reliability across runs is essential. More turns do not necessarily yield better performance. Figure 4 highlights distinct tool-calling behaviors across models. In particular, the efficiency-accuracy correlation shows that stronger models succeed through better decision making and targeted exploration, not blind trial-and-error. Notably, kimi-k2-instruct often enters an overcalling mode, exceeding 30 turns with diminishing success ratesindicating the model might get stuck or loop without effective information retrieval. In contrast, gpt-5-medium achieves the highest pass@1 while maintaining reasonable turn budgets, demonstrating that success arises from efficient decision-making rather than exhaustive tool calls. Turn counts also vary significantly across MCP services (see Appendix for details). Cost is not proxy for performance. Figure 22 shows that higher cost does not lead to higher accuracy. Some of the most expensive runs achieve lower pass@1, while several lower-cost runs"
        },
        {
            "title": "Technical Report",
            "content": "Table 2: Model comparison across MCPs. Pass@1 is computed as the average over four independent runs, with the superscript showing the standard deviation; each MCP service value is also averaged over four runs. Within each model group (Proprietary / Open-Source), the best result is marked in bold and the second best result is underlined. For GPT-5 series models, explicit suffixes (e.g., -medium) indicate the reasoning effort setting; for all models, results correspond to their default reasoning effort if supported. Abbreviations of MCP services are: FS = Filesystem, GH = GitHub, NT = Notion, PW = Playwright, PG = PostgreSQL."
        },
        {
            "title": "Metrics",
            "content": "FS GH NT PW PG pass@ pass@4 pass^4 gpt-5-medium grok-4 claude-opus-4.1 claude-sonnetgpt-5-mini-medium o3 grok-code-fast-1 qwen3-max o4-mini gemini-2.5-pro gemini-2.5-flash gpt-4.1 gpt-5-nano-medium gpt-4.1-mini gpt-4.1-nano qwen3-coder-plus kimi-k2-instruct deepseek-v3.1 glm-4.5 gpt-oss-120b 57.50 50. 33.33 27.50 33.33 35.83 23.33 10. 25.00 24.17 8.33 12.50 6.67 3. 0.00 13.33 14.17 15.83 7.50 5."
        },
        {
            "title": "Proprietary Models",
            "content": "41.96 2.68 35.71 21.43 16.07 24. 2.68 16.96 20.54 4.46 6.25 6. 3.57 1.79 0.00 43.00 35.00 24. 26.00 12.00 15.00 25.00 8.00 12. 15.00 6.00 8.00 0.00 0.00 0. Open-Source Models 19.64 8.04 12.50 21.43 3. 30.00 30.00 7.00 13.00 3.00 47. 14.13 21.74 16.30 18.48 14.13 8. 14.13 14.13 9.78 15.22 7.61 7. 6.52 0.00 19.57 16.30 9.78 22. 4.35 76.19 58.33 33.33 53.57 61. 36.90 47.62 44.05 11.90 26.19 10. 4.76 15.48 9.52 0.00 47.62 47. 42.86 14.29 7.14 52.56 1.29 31.69 2.91 29.92 0.00 28.15 2.57 27.36 3.12 25.39 2.04 20.47 3.39 17.72 1.31 17.32 2.30 15.75 0.56 9.06 0.68 8.07 0.65 6.30 2.01 3.94 0.96 0.00 0.00 24.80 2.05 21.85 1.16 16.73 1.41 15.55 1.16 4.72 0.96 68. 44.88 44.88 45.67 43.31 30. 22.83 31.50 29.92 18.11 12.60 11. 7.09 0.00 40.94 31.50 28.35 24. 13.39 33.86 18.11 12.60 9. 12.60 9.45 11.02 6.30 4.72 3. 3.15 1.57 1.57 0.00 12.60 12. 7.87 6.30 0.00 reach stronger results. Table 9 reports per-task averages and further shows that costs vary widely even when the number of turns is similar. Higher cost alone does not imply better results."
        },
        {
            "title": "4 ANALYSIS",
            "content": "In this section, we investigate two aspects that shape model performance on MCPMark: the role of reasoning effort in agent generalization, and the types of failures that prevent successful execution."
        },
        {
            "title": "4.1 REASONING MODE AND EFFORT",
            "content": "We study how models benefit from different levels of reasoning effort, which are typically reflected in the number of consumed thinking tokens before issuing tool calls. Table 3 reports results for the gpt-5 series and claude-sonnet-4 across different effort settings. Model perspective. The gpt-5 series benefits from increased reasoning effort at moderate and large scales, though effects diverge by size. For gpt-5, medium effort raises pass@1 to 52.56% from 46.85% at low effort. gpt-5-mini shows even stronger relative gains, improving from 8.27% to 30.32% between low and high. By contrast, gpt-5-nano shows only marginal changes around 4% to 6%, suggesting models of this scale lack the capacity to exploit additional reasoning tokens."
        },
        {
            "title": "Technical Report",
            "content": "Figure 4: Turns distribution. Each point is one run (gray = fail). Plots show the turn distribution of successes; color encodes pass@1. Stronger models finish with fewer, better-targeted calls. Table 3: Reasoning effort. Comparison of gpt-5 series models and claude-sonnet-4 under different reasoning effort settings. Pass@1 is reported as mean with standard deviation (4 runs). Each model expands into its supported reasoning effort settings. Best values in each column are bolded."
        },
        {
            "title": "Overall",
            "content": "FS GH NT PW PG gpt-"
        },
        {
            "title": "Low",
            "content": "gpt-5-mini"
        },
        {
            "title": "Low",
            "content": "gpt-5-nano"
        },
        {
            "title": "Medium",
            "content": "claude-sonnet-"
        },
        {
            "title": "High",
            "content": "N/A"
        },
        {
            "title": "High",
            "content": "46.85 3.31 52.56 1.29 51.57 2.91 8.27 1.51 27.36 3.60 30.32 1.98 4.33 1.36 6.30 2.32 5.12 2.36 28.15 2.97 27.36 1.97 28.35 2.73 54.17 7.88 57.50 4.19 52.50 4.19 12.50 5.69 33.33 7.20 35.00 8. 12.50 4.19 6.67 6.09 5.83 5.69 27.50 3.19 23.33 5.44 23.33 4.71 27.17 2.17 47.83 9.39 50.00 2.51 8.70 3.55 18.48 8.96 19.57 2.51 0.00 0.00 7.61 2.17 8.70 3.55 16.30 6.52 25.00 4.16 28.26 2. 36.61 8.93 41.96 3.42 44.64 2.06 5.36 6.19 16.07 6.84 20.54 15.0 0.00 0.00 3.57 0.00 0.89 1.79 21.43 5.83 22.32 3.42 19.64 9.45 45.00 2.00 43.00 6.00 42.00 5.16 1.00 2.00 12.00 7.30 15.00 6. 0.00 0.00 0.00 0.00 2.00 2.31 26.00 6.93 22.00 4.00 26.00 2.31 73.81 4.76 76.19 8.69 72.62 4.56 14.29 3.89 61.90 6.73 66.67 3.89 8.33 4.56 15.48 5.99 9.52 3.89 53.57 7.14 48.81 8.13 50.00 8. claude-sonnet-4 is similarly insensitive, remaining stable around 27% to 28%. These results indicate that translating additional reasoning steps into better MCP use is non-trivial and likely depends on models base capacity and training approach. MCP perspective. Reasoning effort selectively improves generalization in agentic tasks. Remote services benefit most: GitHub performance nearly doubles from 27.17% to 50.00% between low and high effort for gpt-5, while Notion rises from 36.61% to 44.64%. Local services remain stable, with PostgreSQL at 72% to 76% and Filesystem varying under 5 percentage points. We interpret this discrepancy as stemming from differences in training coverage. Remote services typically have limited exposure due to rate limits and access restrictions, making the tasks harder and requiring stronger generalization at test-time. Reasoning helps bridge this gap by enabling models to extrapolate to unseen cases, aligning with recent discussions (Yao et al., 2023b; Yao, 2025) that highlight language generalizes through reasoning in agents."
        },
        {
            "title": "4.2 FAILURE BREAKDOWN",
            "content": "Introduction. We classify failures into two categories to ease presentation: implicit and explicit. Implicit failures occur when the task completes successfully but the output does not meet the required specifications. These often stem from issues such as reasoning errors, suboptimal planning, ineffective"
        },
        {
            "title": "Technical Report",
            "content": "Figure 5: Failure breakdown across models. Failures are categorized as either implicit (task completes normally but fails verification) or explicit (e.g., context window overflow, turn limit exceeded, abandoned, premature stop, or malformed tool calls). tool usage, or difficulty handling long contexts, which may interact to cause complex failures that are difficult to attribute to single factor. In contrast, explicit failures can be directly linked to specific issues. These include context window overflow (input exceeding the models processing length), turn limit exceeded (the model exhausts its allowed interaction steps), abandoned tasks (model decides the task is infeasible), premature stop (model halts without completing or making necessary tool calls), and malformed tool calls (invalid parameters or improperly structured payloads). Observations. As seen in Figure 5, implicit failures account for the majority of errors across all models, often exceeding 50%. Models like gpt-5-high and kimi-k2-instruct show over 80% implicit failures, indicating they generally complete tasks without obvious breakdowns, with errors being more subtle and capability-driven. In contrast, gemini-2.5-flash and gpt-4.1 have lower implicit failure rates (52% and 66%, respectively), suggesting more explicit causes. For explicit failures, gemini-2.5-flash and gpt-4.1 mainly experience abandoned or premature stop errors, reflecting weaker reasoning and planning. gemini-2.5-flash also shows higher incidence of malformed tool calls (around 10%), possibly due to mismatches in tool-call conventions or insufficient training. gpt-5-high has more context window overflow errors, indicating difficulties with long-context handling, while kimi-k2-instruct faces frequent turn limit exceeded errors, often due to repetitive tool-calling loops. These results suggest that explicit errors are model-specific, highlighting the need for targeted improvements in reasoning, context management, and tool use."
        },
        {
            "title": "5 RELATED WORK",
            "content": "LLM Agents. With the development of large language models (LLMs) (Team, 2025; Anthropic, 2025a; Team et al., 2025; OpenAI, 2025c; Comanici et al., 2025), LLM agents have progressed from early prompting methods such as ReAct (Yao et al., 2023b), which integrated reasoning traces with tool actions, to more structured designs like MetaGPT (Hong et al., 2024) that coordinate multi-agent collaboration through explicit role assignment. This evolution has been supported by research on tool use (Schick et al., 2023; Qin et al., 2023; Patil et al., 2024), which explore when and how models should call APIs, as well as planning and reflection methods (Yao et al., 2023a; Shinn et al., 2023; Wang et al., 2024a) that improve robustness in multi-step workflows. Multi-agent frameworks (Wu et al., 2024; Li et al., 2023; Chen et al., 2023) further demonstrate the benefits of coordinated division of labor. In applied domains, coding agents (Yang et al., 2024; Wang et al., 2024b) enable real repository interaction; GUI and computer-use agents are advanced by benchmarks (Zhou et al., 2023; Deng et al., 2023; Xie et al., 2024); and deep research efforts are represented by initiatives (Wei et al., 2025; Starace et al., 2025; Du et al., 2025). Together, these developments illustrate the trend toward general agents that can operate across heterogeneous systems and contexts, naturally pointing to the need for standardized protocols such as the Model Context Protocol (MCP) (Anthropic, 2024) that provide unifying interface for tool and environment integration. Benchmarks for evaluating MCP use. Recent work has begun to systematically benchmark agent performance in MCP-enabled settings (Yan et al., 2025; Liu et al., 2025; Mo et al., 2025; Gao et al., 2025). MCP-Universe (Luo et al., 2025) constructed tasks across multiple domains and evaluators, revealing the difficulty models face with long and dynamic workflows. LiveMCP-101 (Yin et al., 2025) focused on multi-tool interaction and execution-plan validation, while MCP-AgentBench (Guo"
        },
        {
            "title": "Technical Report",
            "content": "et al., 2025) scaled up evaluation with hundreds of tasks spanning diverse servers and tools. These efforts primarily emphasize broad tool coverage or easier execution but leave gaps in assessing high-fidelity workflows tied to realistic application environments. Our proposed MCPMark addresses this by designing tasks with diverse CRUD operations in containerized settings to ensure safety and reproducibility. Each task is paired with programmatic verification scripts and full environment state tracking, enabling reliable and fine-grained evaluation."
        },
        {
            "title": "6 DISCUSSION ON LIMITATIONS AND FUTURE DIRECTIONS",
            "content": "We identify three critical directions for future progress, spanning the language model, the agent framework, and the server-side tools. First, agents must evolve from simple reactive tool use to more sophisticated reasoning. As our analysis shows, success depends on making fewer but smarter decisions rather than more attempts, and reasoning can enable better generalization in agents. Second, achieving long-horizon task completion will require major gains in context efficiency. The challenge is not just the models context window but the agents ability to manage an ever-growing history, suggesting need for better summarization strategies and more concise tool outputs. Finally, for these systems to be trusted in the real world, they need profound leap in execution stability. The observed inconsistency across multiple runs highlights core unreliability that can only be solved by building agents with robust error-handling and self-correction capabilities. We believe that MCPMark provides concrete testbed to measure progress along these essential research axes. Alongside developing more capable agentic systems, the benchmarks that measure them must also evolve. Our task creation pipeline, while ensuring task quality, is difficult to scale. This creates bottleneck for producing the large-scale training data needed to advance the field. Furthermore, the steep difficulty of many tasks in MCPMark limits its utility for evaluating and guiding the development of smaller, more efficient models. Future work on the benchmark should therefore focus on introducing more fine-grained difficulty gradient, potentially through semi-automated task generation and reduced task execution chain. Additionally, to better reflect real-world complexity, the benchmark could be expanded to include tasks with ambiguous user intent. This would test an agents ability to ask clarifying questions or infer the users actual intent. Finally, incorporating wider variety of MCP servers could also help challenge agents across more diverse set of digital tools."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "This section outlines how we address the ethical considerations involved in the construction of our benchmark, which includes several key components that could raise ethical concerns: Initial State of MCP Environment: Each initial state and environment used in the benchmark is provided with the appropriate license information (see Appendix for details). few environments were self-curated, and for these, we have ensured transparency and compliance with relevant licensing requirements, promoting ethical usage. Task Curation: All tasks included in the benchmark were collaboratively annotated by both experts and AI agents. The experts involved in the curation process have been properly recognized as co-authors in the author list, ensuring that their contributions are duly acknowledged. Additionally, the licenses for the agents used, including Claude Code (License) and Cursor (License), are provided to ensure that all resources are used responsibly and in accordance with the relevant licensing terms for research purposes. MCP Servers: The licenses for each specific MCP server used in the benchmark are provided in Appendix B. This ensures that all external systems and tools are properly licensed for research and evaluation purposes. By adhering to these practices, we ensure that high ethical standards are maintained throughout the construction of the benchmark, and that all resources are used responsibly and in accordance with relevant regulations. REFERENCES Anthropic. Introducing the model context protocol. https://www.anthropic.com/news/ model-context-protocol, November 2024. Accessed: 2025-06-30. Anthropic. Claude opus 4.1. https://www.anthropic.com/news/claude-opus-4-1, August 2025a. Accessed: 2025-08-06. Anthropic. Introducing claude 4. https://www.anthropic.com/news/claude-4, May 2025b. Accessed: 2025-07-28. Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi Lu, Ruobing Xie, et al. Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents. arXiv preprint arXiv:2308.10848, 2(4):6, 2023. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards generalist agent for the web. Advances in Neural Information Processing Systems, 36:2809128114, 2023. Mingxuan Du, Benfeng Xu, Chiwei Zhu, Xiaorui Wang, and Zhendong Mao. Deepresearch bench: comprehensive benchmark for deep research agents. arXiv preprint arXiv:2506.11763, 2025. Xuanqi Gao, Siyi Xie, Juan Zhai, Shqing Ma, and Chao Shen. Mcp-radar: multi-dimensional arXiv preprint benchmark for evaluating tool use capabilities in large language models. arXiv:2505.16700, 2025. Zikang Guo, Benfeng Xu, Chiwei Zhu, Wentao Hong, Xiaorui Wang, and Zhendong Mao. Mcpagentbench: Evaluating real-world language agent performance with mcp-mediated tools. arXiv preprint arXiv:2509.09734, 2025. Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Ceyao Zhang, Jinlin Wang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, et al. Metagpt: Meta programming for multi-agent collaborative framework. International Conference on Learning Representations, ICLR, 2024."
        },
        {
            "title": "Technical Report",
            "content": "Xinyi Hou, Yanjie Zhao, Shenao Wang, and Haoyu Wang. Model context protocol (mcp): Landscape, security threats, and future research directions. arXiv preprint arXiv:2503.23278, 2025. Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for\" mind\" exploration of large language model society. Advances in Neural Information Processing Systems, 36:5199152008, 2023. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. Zhiwei Liu, Jielin Qiu, Shiyu Wang, Jianguo Zhang, Zuxin Liu, Roshan Ram, Haolin Chen, Weiran Yao, Huan Wang, Shelby Heinecke, et al. Mcpeval: Automatic mcp-based deep evaluation for ai agent models. arXiv preprint arXiv:2507.12806, 2025. Ziyang Luo, Zhiqi Shen, Wenzhuo Yang, Zirui Zhao, Prathyusha Jwalapuram, Amrita Saha, Doyen Sahoo, Silvio Savarese, Caiming Xiong, and Junnan Li. Mcp-universe: Benchmarking large language models with real-world model context protocol servers. arXiv preprint arXiv:2508.14704, 2025. Guozhao Mo, Wenliang Zhong, Jiawei Chen, Xuanang Chen, Yaojie Lu, Hongyu Lin, Ben He, Xianpei Han, and Le Sun. Livemcpbench: Can agents navigate an ocean of mcp tools? arXiv preprint arXiv:2508.01780, 2025. OpenAI. Introducing gpt-oss. https://openai.com/index/introducing-gpt-oss/, August 2025a. Accessed: 2025-08-14. OpenAI. Introducing gpt-4.1 in the api. https://openai.com/index/gpt-4-1/, April 2025b. Accessed: 2025-07-28. OpenAI. Gpt-5 system card. https://cdn.openai.com/gpt-5-system-card.pdf, August 2025c. Accessed: 2025-08-13. OpenAI. Introducing openai o3 and o4-mini. https://openai.com/index/ introducing-o3-and-o4-mini/, April 2025d. Accessed: 2025-07-28. Shishir Patil, Tianjun Zhang, Xin Wang, and Joseph Gonzalez. Gorilla: Large language model connected with massive apis. Advances in Neural Information Processing Systems, 37: 126544126565, 2024. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789, 2023. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36:6853968551, 2023. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:86348652, 2023. Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon Maksin, Rachel Dias, Evan Mays, Benjamin Kinsella, Wyatt Thompson, et al. Paperbench: Evaluating ais ability to replicate ai research. arXiv preprint arXiv:2504.01848, 2025. Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, et al. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534, 2025. Qwen Team. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388."
        },
        {
            "title": "Technical Report",
            "content": "Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. Executable code actions elicit better llm agents. In Forty-first International Conference on Machine Learning, 2024a. Xingyao Wang, Boxuan Li, Yufan Song, Frank Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, et al. Openhands: An open platform for ai software developers as generalist agents. arXiv preprint arXiv:2407.16741, 2024b. Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. Browsecomp: simple yet challenging benchmark for browsing agents. arXiv preprint arXiv:2504.12516, 2025. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, et al. Autogen: Enabling next-gen llm applications via multi-agent conversations. In First Conference on Language Modeling, 2024. xAI. Grok 4. https://x.ai/news/grok-4, July 2025. Accessed: 2025-07-28. Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. Advances in Neural Information Processing Systems, 37:5204052094, 2024. Yunhe Yan, Shihe Wang, Jiajun Du, Yexuan Yang, Yuxuan Shan, Qichen Qiu, Xianqing Jia, Xinge Wang, Xin Yuan, Xu Han, et al. Mcpworld: unified benchmarking testbed for api, gui, and hybrid computer use agents. arXiv preprint arXiv:2506.07672, 2025. John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. Swe-agent: Agent-computer interfaces enable automated software engineering. Advances in Neural Information Processing Systems, 37:5052850652, 2024. Shunyu Yao. The second half. https://ysymyth.github.io/The-Second-Half/, 2025. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822, 2023a. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023b. Shunyu Yao, Noah Shinn, Pedram Razavi, and Karthik Narasimhan. τ -bench: benchmark for tool-agent-user interaction in real-world domains. arXiv preprint arXiv:2406.12045, 2024. Ming Yin, Dinghan Shen, Silei Xu, Jianbing Han, Sixun Dong, Mian Zhang, Yebowen Hu, Shujian Liu, Simin Ma, Song Wang, et al. Livemcp-101: Stress testing and diagnosing mcp-enabled agents on challenging queries. arXiv preprint arXiv:2508.15760, 2025. Zai. Glm-4.5: Reasoning, coding, and agentic abililties. https://z.ai/blog/glm-4.5, July 2025. Accessed: 2025-07-28. Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023."
        },
        {
            "title": "B MCP servers",
            "content": "C Task-Level Results across Models"
        },
        {
            "title": "H Initial States Selection and Licenses",
            "content": "H.1 Notion Templates . . H.2 GitHub Repositories H.3 Playwright Usage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . H.4 Filesystem Components . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . H.5 PostgreSQL Databases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 17 18 19 24 40 41 41 41 42"
        },
        {
            "title": "A DETAILS OF THE TASK CREATION PIPELINE",
            "content": "We use Playwright as an example to illustrate the guideline for human experts and the initial instruction/prompt for the task creation agent. These are simplified for reference. Guideline (Playwright) Step 1. Select the starting environment Pick website or web app as the initial state. Prefer staging or test instance to avoid side effects. Examples: Reddit-like forum or Shopping Admin dashboard. Step 2. Configure the agent environment In Cursor or Claude Code, set up the MCP server stack and include the Playwright MCP server so the agent can control browser. Step 3. Define an initial question or topic Write seed question or topic that will guide agent exploration and task creation. It can be broad or moderately specific. Step 4. Create and refine the task Step 4.1. Exploration with the agent Have the agent read the initial instruction (which includes the seed question), then explore the target site together with the agent. Based on the collected context, ask the agent to propose task that fits the objectives and requirements. Step 4.2. Provide feedback to improve the task Guide the agent to revise the task as needed. Examples: If verification is weak: This task is not sufficiently verifiable. Please revise it to make verification clearer and more reliable. If exploration lacks coverage: You can explore deeper to collect more diverse and detailed information. If subtasks feel disconnected: Make the subtasks integrated rather than unrelated. Step 4.3. Save the task Store the task description and the verification script as separate files. Use consistent folder structure based on category and name. Follow well-structured prior examples for formatting. Step 4.4. Human-in-the-loop adjustments Iterate between the agent and the reviewer until both the task description and the verification script meet quality standards. Step 5. Execute and verify Run the task with Playwright MCP to reach the final state, then run the verification script. Stress-test the checker to confirm: Step 5.1. The task is executable end to end. Step 5.2. Pass or fail is clear and objective. Step 5.3. The script flags both correct and incorrect outcomes, including edge cases. Step 6. Assess difficulty (optional) If the task and checker pass, consider whether difficulty is high enough to test the model. Adjust scope or constraints if needed. Notes. These steps target experts working with Cursor or Claude Code. They are guidelines. If issues appear, collaborate with colleagues to debug efficiently."
        },
        {
            "title": "Technical Report",
            "content": "Initial Instruction for Task Creation Agent (Playwright) Your job is to: 1. First explore the web environment to understand available MCP tools and capabilities. 2. Generate one challenging, verifiable, and realistic task based on your collected information. 3. Focus your exploration and task generation on the following specific topic or question: Use this as guiding theme for creating more targeted and relevant tasks. Ensure the task addresses different aspects or components related to this requirement. Playwright MCP Tools Reference: <playwright_mcp_doc> [contents of docs/playwright-mcp-introduction.md go here] </playwright_mcp_doc> Output Format: { \"tasks\": [ { \"task_id\": \"task_1\", \"description\": \"Clear, conversational task description\", \"difficulty\": \"hard\", \"verification_criteria\": [\"criterion 1\", \"criterion 2\"], \"expected_mcp_calls\": [\"browser_navigate\", \"browser_snapshot\", \"browser_click\"], \"estimated_complexity\": \"high\" } ] } Based on the given web application environment, write one challenging, verifiable, and realistic browser automation task that aligns with users actual web interaction workflows. The goal is to evaluate an Agents ability to use Playwright MCP tools effectively. Requirements: Difficulty: The task should be really hard ... (omitted) Verifiability: Avoid open-ended outcomes ... (omitted) Authenticity: Describe the task in natural, conversational tone ... (omitted) Context Awareness: Leverage page structure, form elements, navigation patterns, ... (omitted) Start by exploring the web application environment using MCP tools to understand the current structure, interactive elements, and user workflows, then generate task that combines: 1. Your real-time MCP exploration findings. 2. The specific website structure and interactive elements you discover. 3. focus on browser automation operations that require multiple Playwright MCP tools rather than only content reading. 4. The specific focus area: <seed_topic>. Please explore thoroughly before creating the task. Consider: Form elements and input fields. Navigation patterns and menu structures. Dynamic content and interactive features. User workflow patterns. Authentication and session management. Data submission and validation processes."
        },
        {
            "title": "B MCP SERVERS",
            "content": "We relied on five Model Context Protocol (MCP) servers in our setup. Below we summarize their functionality, invocation, repository, and license. Filesystem. The filesystem server provides local read, write, and directory operations over the host file system. It is invoked as @modelcontextprotocol/server-filesystem. The implementation is hosted at github.com/modelcontextprotocol/servers under the MIT License. GitHub. The GitHub server integrates with the GitHub API to manage repositories, issues, and pull requests. The endpoint used is https://api.githubcopilot.com/mcp/. The code is available at github.com/github/github-mcp-server, released under the MIT License. Notion. The Notion server allows interaction with Notion databases and pages. It is invoked as @notionhq/notion-mcp-server. The official repository is github.com/makenotion/notionmcp-server, licensed under the MIT License. The Playwright server enables browser automation and scripted web workIt is started using @playwright/mcp@latest. The source code is provided at Playwright. flows. github.com/microsoft/playwright-mcp, distributed under the Apache License 2.0. PostgreSQL. The PostgreSQL server provides access to relational database through SQL queries. It is launched with postgres-mcp -access-mode=unrestricted. The implementation is maintained at github.com/crystaldba/postgres-mcp, and is released under the MIT License."
        },
        {
            "title": "Technical Report",
            "content": "C TASK-LEVEL RESULTS ACROSS MODELS To facilitate fine-grained analysis, we include taskmodel success matrix, shown in Fig. 6. This complements the aggregate metrics with per-task view across models. Figure 6: Taskmodel success matrix. Each cell shows the number of successful runs (04) for the taskmodel pair."
        },
        {
            "title": "D DETAILED MCP BENCHMARK RESULTS",
            "content": "Tables 2 and 9 presented the overall success rates and usage statistics, aggregated across all MCPs. Here we provide the corresponding breakdown by individual MCP from Table 4 to Table 8. #Input and #Output are measured in thousands of tokens (K), and Cost is reported in USD. For success metrics, bold and underline indicate the best and second-best results, respectively. For usage statistics, bold and underline denote the largest and second-largest values, without implying better performance. Table 4: Filesystem MCP benchmark results."
        },
        {
            "title": "Metrics",
            "content": "Per-Task Avg Usage Pass@1 Pass@4 Pass^4 # Input # Output"
        },
        {
            "title": "Turns Tool Calls",
            "content": "gpt-5-medium grok-4 o3 gpt-5-mini-medium claude-opus-4.1 claude-sonneto4-mini gemini-2.5-pro grok-code-fast-1 gpt-4.1 gemini-2.5-flash gpt-5-nano-medium gpt-4.1-mini gpt-4.1-nano deepseek-v3.1 kimi-k2-instruct-0905 qwen3-coder-plus qwen3-max glm-4.5 gpt-oss-120b 57.50 3.63 50.83 6.40 35.83 2.76 33.33 6.24 33.33 0.00 27.50 2.76 25.00 2.89 24.17 3.63 23.33 7.45 12.50 1.44 8.33 1.67 6.67 5.27 3.33 0.00 0.00 0.00 15.83 1.44 14.17 1.44 13.33 6.67 10.83 1.44 7.50 1.44 5.83 4."
        },
        {
            "title": "Proprietary Models",
            "content": "76.67 73.33 50.00 53.33 50. 36.67 43.33 40.00 20.00 13.33 16. 3.33 0.00 36.67 26.67 26.67 10. 6.67 13.33 10.00 10.00 3. 6.67 0.00 3.33 0.00 215.96 247. 689.64 398.34 272.17 302.21 293.34 214. 276.40 143.95 67.64 462.74 196.15 116. Open-Source Models 26.67 23.33 26.67 13.33 13. 16.67 6.67 6.67 3.33 10.00 3. 0.00 421.33 696.79 972.41 389.56 193. 19.75 17.38 10.70 17.79 12.58 4. 4.00 15.89 7.75 2.36 1.81 7. 19.53 1.63 1.32 3.38 4.47 4. 2.87 3.92 1.08 0.44 0.90 1. 0.12 4.41 0.97 0.39 0.65 0. 0.30 0.04 0.03 0.08 0.01 0. 0.43 0.20 0.48 0.07 10.06 10. 28.79 14.84 16.37 16.02 20.88 14. 16.38 9.28 6.50 20.75 15.50 12. 23.83 26.27 28.23 19.27 16.39 < 0. 4.62 21.07 16.87 27.80 36.93 15. 15.08 19.88 14.72 16.77 18.48 11. 27.76 19.57 15.32 23.12 25.70 27. 18.39 17.09 3."
        },
        {
            "title": "Technical Report",
            "content": "Table 5: GitHub MCP benchmark results. Model"
        },
        {
            "title": "Metrics",
            "content": "Per-Task Avg Usage Pass@1 Pass@4 Pass^4 # Input # Output Cost Turns Tool Calls Proprietary Models gpt-5-medium claude-opus-4.1 gpt-5-mini-medium claude-sonnetgemini-2.5-flash grok-4 o4-mini o3 gemini-2.5-pro grok-code-fastgpt-5-nano-medium gpt-4.1 gpt-4.1-mini gpt-4.1-nano glm-4.5 qwen3-coder-plus kimi-k2-instruct-0905 qwen3-max deepseek-v3.1 gpt-oss-120b 47.83 8.13 21.74 0.00 18.48 7.76 16.30 5.65 15.22 2.17 14.13 3.61 14.13 6.43 14.13 3.61 9.78 1.88 8.70 5.32 7.61 1.88 7.61 1.88 6.52 6.52 0.00 0.00 22.83 6.43 19.57 6.52 16.30 1.88 14.13 3.61 9.78 1.88 4.35 3. 659.73 620.63 614.68 696.81 20.57 5. 7.71 4.44 1107.04 12.70 1.03 9. 0.17 2.16 0.36 2.44 0.60 0. 0.52 0.16 0.05 0.91 0.19 0. 0.16 0.40 0.62 1.63 0.21 < 0. 14.33 10.78 13.92 11.16 10.46 12. 10.92 9.20 5.45 17.85 15.15 9. 12.00 9.27 11.92 19.12 23.68 26. 9.46 4.62 21.23 10.13 17.28 10. 17.71 16.76 10.08 8.24 6.29 17. 17.63 14.97 14.63 11.04 11.04 18. 23.23 25.78 9.22 3.62 1.93 8. 3.56 5.75 6.50 26.77 2.49 1. 2.59 3.65 3.36 8.25 2.55 2. 1.41 65.22 17.39 34.78 30. 21.74 21.74 26.09 21.74 21.74 17. 13.04 8.70 17.39 0.00 4. 8.70 8.70 8.70 4.35 4.35 0. 4.35 0.00 4.35 0.00 0.00 804. 510.13 451.18 173.43 751.41 751.62 445. 466.70 312.86 Open-Source Models 34.78 34.78 26. 17.39 13.04 8.70 13.04 13.04 8. 4.35 8.70 0.00 482.00 1987.14 995. 1348.13 362.36 76."
        },
        {
            "title": "Technical Report",
            "content": "Table 6: Notion MCP benchmark results. Model"
        },
        {
            "title": "Metrics",
            "content": "Per-Task Avg Usage Pass@1 Pass@4 Pass^4 # Input # Output Cost Turns Tool Calls gpt-5-medium claude-opus-4.1 o3 claude-sonnet-4 o4-mini gpt-5-mini-medium gemini-2.5-flash gpt-4.1 gemini-2.5-pro gpt-5-nano-medium grokgrok-code-fast-1 gpt-4.1-mini gpt-4.1-nano glm-4.5 qwen3-coder-plus qwen3-max deepseek-v3.1 kimi-k2-instruct-0905 gpt-oss-120b 41.96 2.96 35.71 0.00 24.11 3.89 21.43 5.05 20.54 5.85 16.07 5.92 6.25 4.64 6.25 1.55 4.46 2.96 3.57 0.00 2.68 1.55 2.68 1.55 1.79 1.79 0.00 0.00 21.43 2.53 19.64 6.44 16.96 4.64 12.50 3.09 8.04 2.96 3.57 2.53 Proprietary Models 50.00 32.14 46.43 39.29 42. 32.14 21.43 14.29 7.14 3.57 3. 3.57 3.57 0.00 7.14 7. 7.14 3.57 0.00 0.00 0.00 3. 0.00 0.00 0.00 0.00 375.04 638. 224.93 646.64 267.63 705.09 201.00 135. 212.92 204.32 678.64 561.49 262.75 93. Open-Source Models 32.14 39.29 25.00 28.57 10. 14.29 10.71 7.14 3.57 0.00 3. 0.00 625.97 796.73 973.92 503.35 1117. 68.31 31.62 3.93 9.47 4.24 25. 12.34 6.58 1.37 7.13 32.08 13. 7.26 1.35 1.40 5.04 2.75 3. 2.20 5.20 1.72 0.79 9.87 0. 2.00 0.41 0.20 0.08 0.28 0. 0.02 2.23 0.12 0.11 12.94 17. 13.72 19.71 15.29 14.60 6.11 8. 7.12 7.46 20.14 20.27 12.57 < 0. 9.64 0.21 0.16 1.19 0.29 0. 22.15 21.07 26.57 17.94 33.55 < 0. 5.49 21.60 16.04 12.72 18.71 14. 17.28 9.61 11.82 8.67 8.74 24. 20.09 14.56 10.93 21.17 20.23 25. 17.40 32.72 4."
        },
        {
            "title": "Technical Report",
            "content": "Table 7: Playwright MCP benchmark results."
        },
        {
            "title": "Metrics",
            "content": "Per-Task Avg Usage Pass@1 Pass@4 Pass^4 # Input # Output Cost"
        },
        {
            "title": "Proprietary Models",
            "content": "gpt-5-medium grok-4 claude-sonnet-4 grok-code-fast-1 claude-opus-4.1 gemini-2.5-pro o3 o4-mini gpt-5-mini-medium gpt-4.1 gemini-2.5-flash gpt-5-nano-medium gpt-4.1-mini gpt-4.1-nano qwen3-coder-plus kimi-k2-instruct-0905 glm-4.5 qwen3-max deepseek-v3.1 gpt-oss-120b 43.00 5.20 35.00 7.68 26.00 6.00 25.00 1.73 24.00 0.00 15.00 1.73 15.00 5.20 12.00 2.83 12.00 6.32 8.00 2.83 6.00 2.00 0.00 0.00 0.00 0.00 0.00 0.00 30.00 4.47 30.00 6.00 13.00 3.32 8.00 0.00 7.00 3.32 3.00 1.73 56.00 48. 36.00 36.00 32.00 32.00 28. 24.00 12.00 12.00 0.00 0.00 0. 36.00 20.00 8.00 8.00 4. 8.00 0.00 4.00 4.00 0.00 0. 0.00 0.00 1807.17 1264.91 1241.92 1157. 1146.05 1696.44 556.30 862.51 1814.94 859. 3838.93 21.79 6.64 3.52 7.17 2. 5.58 4.46 18.07 8.55 0.86 8. 711.95 17.71 4959.14 389.80 2.48 3. 3.78 0.24 17.41 4.32 1.15 1. 0.47 1.73 1.17 0.04 1.99 0. 0.57 0.82 0.20 2.76 0.47 0. 23.78 20.05 19.80 18.23 19.04 19. 16.30 17.70 22.75 13.80 26.33 18. 31.33 13.51 21.21 20.64 15.36 27. 19.09 7.21 22.96 23.02 19.12 18. 18.40 18.33 15.40 16.93 22.04 15. 38.78 17.55 31.52 13.61 20.40 19. 14.61 27.41 20.78 6.26 3.28 0. 2.39 2.17 2.76 1.16 1.77 1. Open-Source Models 48.00 40.00 20.00 12.00 16. 4.00 8.00 20.00 4.00 4.00 0. 0.00 2851.57 1358.02 582.73 2297.67 836. 139."
        },
        {
            "title": "Technical Report",
            "content": "Table 8: PostgreSQL MCP benchmark results."
        },
        {
            "title": "Metrics",
            "content": "Per-Task Avg Usage Pass@1 Pass@4 Pass^4 # Input # Output"
        },
        {
            "title": "Turns Tool Calls",
            "content": "gpt-5-medium gpt-5-mini-medium grok-4 claude-sonnet-4 grok-code-fast-1 claude-opus-4.1 gemini-2.5-pro gpt-5-nano-medium o4-mini gemini-2.5-flash gpt-4.1-mini gpt-4.1 gpt-4.1-nano qwen3-coder-plus kimi-k2-instruct-0905 qwen3-max deepseek-v3. glm-4.5 gpt-oss-120b 76.19 7.53 61.90 5.83 58.33 7.81 53.57 6.19 47.62 4.76 36.90 3.95 33.33 0.00 26.19 7.90 15.48 5.19 11.90 4.12 10.71 6.19 9.52 3.37 4.76 0.00 0.00 0.00 47.62 5.83 47.62 4.76 44.05 2.06 42.86 7.53 14.29 7.53 7.14 2."
        },
        {
            "title": "Proprietary Models",
            "content": "100.00 90.48 80.95 71.43 61.90 66. 47.62 28.57 19.05 23.81 14. 4.76 0.00 47.62 28.57 38.10 38. 28.57 14.29 9.52 4.76 4. 4.76 4.76 4.76 0.00 113.35 115. 186.07 331.10 226.41 63.56 260.68 39. 105.02 15.92 46.08 46.63 55.11 71. Open-Source Models 61.90 66.67 52.38 61.90 23. 23.81 38.10 28.57 38.10 28.57 0. 0.00 573.90 441.16 192.13 316.60 204. 21.36 17.04 9.27 8.23 7.54 5. 4.72 9.80 8.91 23.04 5.76 9. 1.78 1.20 2.43 5.13 5.38 4. 4.65 5.14 1.42 0.31 0.05 0. 1.11 0.05 0.16 4.64 0.23 0. 0.04 0.04 0.02 0.12 < 0.01 0. 0.28 0.26 0.19 0.07 13.37 11. 17.89 26.80 19.70 10.71 24.86 7. 9.46 5.06 8.76 9.77 8.12 8. 29.00 30.21 18.88 26.48 25.39 < 0. 5.07 12.45 10.77 17.08 25.81 18. 9.71 23.86 6.45 10.15 4.06 11. 11.61 10.54 10.18 28.00 29.25 17. 25.49 24.40 4."
        },
        {
            "title": "E CASE STUDIES BY MCP",
            "content": "Figure 7: Task sheet and initial directory tree for the Filesystem case; trajectories are in Figures 89."
        },
        {
            "title": "Technical Report",
            "content": "Figure 8: Successful run by claude-sonnet-4: extracts contacts, writes CSV and answer file, verifier passes."
        },
        {
            "title": "Technical Report",
            "content": "Figure 9: Failed run by gemini-2.5-pro: files are created but CSV/answer content is incorrect, verifier fails."
        },
        {
            "title": "Technical Report",
            "content": "Figure 10: Task sheet and initial repository snapshot for the GitHub case; trajectories are in Figures 11 12."
        },
        {
            "title": "Technical Report",
            "content": "Figure 11: Successful run by gpt-5-medium: branch, ESLint config, workflow, and PR are created; CI run fixes lint errors; verifier passes."
        },
        {
            "title": "Technical Report",
            "content": "Figure 12: Failed run by qwen3-coder-plus: partial setup leaves artifacts or CI incomplete, verifier fails."
        },
        {
            "title": "Technical Report",
            "content": "Figure 13: Task sheet and initial Notion page/databases for the Notion case; trajectories are in Figures 1415."
        },
        {
            "title": "Technical Report",
            "content": "Figure 14: Successful run by claude-opus-4.1: updates callout and retags database items consistently, verifier passes."
        },
        {
            "title": "Technical Report",
            "content": "Figure 15: Failed run by deepseek-v3.1: performs partial edits but misses required tag updates, verifier fails."
        },
        {
            "title": "Technical Report",
            "content": "Figure 16: Task sheet and initial login page for the Playwright case; trajectories are in Figures 1718."
        },
        {
            "title": "Technical Report",
            "content": "Figure 17: Successful run by o3: navigates login, fills credentials, passes Turnstile, reaches authenticated state, verifier passes."
        },
        {
            "title": "Technical Report",
            "content": "Figure 18: Failed run by grok-4: credentials entered but Turnstile not solved, verifier fails."
        },
        {
            "title": "Technical Report",
            "content": "Figure 19: Task sheet and initial schema for the PostgreSQL case; trajectories are in Figs. 2021."
        },
        {
            "title": "Technical Report",
            "content": "Figure 20: Successful run by grok-code-fast-1: creates/updates tracking tables, adds indexes and seed rows, verifier passes."
        },
        {
            "title": "Technical Report",
            "content": "Figure 21: Failed run by grok-4: schema work incomplete and required rows/indexes missing, verifier fails."
        },
        {
            "title": "F COST AND TURN DISTRIBUTION",
            "content": "Figure 22: Cost-performance map per run. The shaded area highlights runs with higher performance at lower cost. Table 9: Usage stats. Per-task averages: input/output tokens (K), cost (USD), turns, tool calls. Model Per-Task Avg Usage # Input # Output Cost Turns Tool Calls Proprietary Models claude-opus-4.1 grokclaude-sonnet-4 gemini-2.5-pro qwen3-max gpt-5-medium o3 gpt-4. o4-mini gpt-4.1-mini gemini-2.5-flash gpt-5-mini-medium grok-code-fast-1 gpt-4.1-nano gpt-5-nano-medium 586.07 633.51 639.37 469.65 1034. 627.66 414.23 323.00 393.10 1172.70 1024.09 737. 590.50 193.37 447.99 5.14 8.42 4. 7.02 2.99 21.91 8.59 1.55 15. 1.90 8.80 10.31 5.65 1.64 23. Open-Source Models kimi-k2-instruct qwen3-coder-plus deepseek-v3.1 glm-4.5 gpt-oss-120b 931.50 1421.47 493.05 419.66 64.50 5.01 3.51 2. 4.09 1.37 9.18 2.03 1.99 1.28 1. 1.00 0.90 0.66 0.50 0.47 0. 0.20 0.13 0.02 0.03 0.57 0.29 0. 0.14 0.01 17.43 16.25 18.48 10. 23.85 14.71 16.47 9.94 14.60 16.39 11. 15.67 18.42 10.78 14.50 26.95 23.75 19. 18.14 5.40 16.57 19.84 17.62 11. 23.02 20.16 15.50 14.42 13.68 18.61 17. 21.78 18.19 12.39 16.81 26.22 22.84 19. 17.62 4."
        },
        {
            "title": "G TURN DISTRIBUTIONS ACROSS MCP SERVICES",
            "content": "In this section, we provide per-service turn distributions for the five MCPs in MCPMark from Figure 23 to Figure 27. These plots complement the overall turn analysis in Figure 4 and illustrate how turn requirements differ by service. Figure 23: Turn distribution per task on the Filesystem MCP. Figure 24: Turn distribution per task on the Notion MCP. Figure 25: Turn distribution per task on the GitHub MCP."
        },
        {
            "title": "Technical Report",
            "content": "Figure 26: Turn distribution per task on the PostgreSQL MCP. Figure 27: Turn distribution per task on the Playwright MCP."
        },
        {
            "title": "H INITIAL STATES SELECTION AND LICENSES",
            "content": "This section provides an overview of the initial states selection, including Notion templates, GitHub repositories, PostgreSQL databases, Playwright websites, and Filesystem components, along with their corresponding licenses. H.1 NOTION TEMPLATES We utilized 9 publicly available Notion templates from the Notion Template Marketplace for benchmarking purposes. According to Notions Marketplace Guidelines & Terms, templates are provided under non-exclusive license for use within the users workspace as long as an active Notion subscription is maintained. Redistribution or resale is prohibited. Our use of these templates was limited to internal research and benchmarking, in compliance with the licensing conditions. H.2 GITHUB REPOSITORIES Several GitHub repositories were utilized during the research. Below is summary of the repositories and their respective licenses: anthropics/claude-code: Anthropic PBC. All rights reserved. Use is subject to Anthropics Commercial Terms of Service. openai/harmony: Apache License 2.0. missing-semester/missing-semester: CC BY-NC-SA 4.0."
        },
        {
            "title": "Technical Report",
            "content": "Table 10: Notion templates used in this research benchmark."
        },
        {
            "title": "Japan Travel Planner",
            "content": "# Template 1 Online Resume 2 3 Company in-a-Box 4 Computer Science Student Dashboard 5 6 7"
        },
        {
            "title": "Standard Operating Procedure\nTeam Projects\nPython Roadmap\nToronto Guide\nIT Trouble Shooting Hub",
            "content": "codecrafters-io/build-your-own-x: CodeCrafters, Inc. has waived all copyright and related or neighboring rights to this work. hiyouga/EasyR1: Apache License 2.0. mcpmark-cicd: Written by authors and hosted via GitHub. H.3 PLAYWRIGHT USAGE We utilized environments reddit, shopping, and shopping_admin from the web-arenax/webarena repository, which is licensed under the Apache License 2.0. These modules were incorporated for testing and evaluation purposes within the benchmarking setup. Other websites were written by authors and hosted via Vercel. H.4 FILESYSTEM COMPONENTS The following filesystem components were used as part of our research environment: (1) desktop, desktop_template, file_context, file_property, folder_structure, papers, and student_database were collected from the authors own local environment or files synthesized using LLMs. (2) legal_document refers to legal document on NVCA financing, which can be accessed at CooleyGo . (3) threestudio and votenet are open-source projects utilized from GitHub repositories. Specifically, votenet (MIT License), and threestudio (Apache License 2.0). H.5 POSTGRESQL DATABASES We utilized the following PostgreSQL databases, which are publicly available with their corresponding licenses: chinook: MIT License, and Apache License 2.0. employees: CC BY-SA 3.0, and Apache License 2.0. lego: CC0 1.0 Universal (Public Domain Dedication), and Apache License 2.0. sports: Apache License 2.0. dvdrental: MIT License."
        }
    ],
    "affiliations": [
        "EvalSys",
        "Fudan University",
        "LobeHub",
        "National University of Singapore",
        "Shanghai Jiao Tong University"
    ]
}