{
    "paper_title": "StrandHead: Text to Strand-Disentangled 3D Head Avatars Using Hair Geometric Priors",
    "authors": [
        "Xiaokun Sun",
        "Zeyu Cai",
        "Zhenyu Zhang",
        "Ying Tai",
        "Jian Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While haircut indicates distinct personality, existing avatar generation methods fail to model practical hair due to the general or entangled representation. We propose StrandHead, a novel text to 3D head avatar generation method capable of generating disentangled 3D hair with strand representation. Without using 3D data for supervision, we demonstrate that realistic hair strands can be generated from prompts by distilling 2D generative diffusion models. To this end, we propose a series of reliable priors on shape initialization, geometric primitives, and statistical haircut features, leading to a stable optimization and text-aligned performance. Extensive experiments show that StrandHead achieves the state-of-the-art reality and diversity of generated 3D head and hair. The generated 3D hair can also be easily implemented in the Unreal Engine for physical simulation and other applications. The code will be available at https://xiaokunsun.github.io/StrandHead.github.io."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 6 1 ] . [ 1 6 8 5 1 1 . 2 1 4 2 : r StrandHead: Text to Strand-Disentangled 3D Head Avatars Using Hair Geometric Priors Xiaokun Sun1, Zeyu Cai2, Zhenyu Zhang1, Ying Tai1, Jian Yang 1Nanjing University 2The Hong Kong University of Science and Technology (Guangzhou) xiaokun sun@smail.nju.edu.cn, zcai701@connect.hkust-gz.edu.cn, zhangjesse@foxmail.com, {yingtai, csjyang}@nju.edu.cn https://xiaokunsun.github.io/StrandHead.github.io Figure 1. We propose StrandHead, text-driven framework for generating strand-disentangled 3D head avatars that feature high-fidelity facial details and strand-based hair. By accurately capturing the internal geometry of hair strands, our approach seamlessly supports flexible hairstyle transfer and editing, as well as physics-based rendering and simulation."
        },
        {
            "title": "Abstract",
            "content": "can also be easily implemented in the Unreal Engine for physical simulation and other applications. While haircut indicates distinct personality, existing avatar generation methods fail to model practical hair due to the general or entangled representation. We propose StrandHead, novel text to 3D head avatar generation method capable of generating disentangled 3D hair with strand representation. Without using 3D data for supervision, we demonstrate that realistic hair strands can be generated from prompts by distilling 2D generative diffusion models. To this end, we propose series of reliable priors on shape initialization, geometric primitives, and statistical haircut features, leading to stable optimization and text-aligned performance. Extensive experiments show that StrandHead achieves the state-of-the-art reality and diversity of generated 3D head and hair. The generated 3D hair 1. Introduction Creating 3D head avatars is crucial for many applications including digital telepresence, gaming, movies, and AR/VR. Traditional approaches heavily depend on manual efforts which is time-consuming and labor-intensive. Recent advances [55, 79, 84, 93] achieve automatic 3D head generation with the learning-based paradigm. Although these methods provide reliable 3D head geometry, they require high-cost 3D training data, which significantly constrains their generalization capabilities and limits the potential applications. With the rapid development of text-driven generation 1 methods [50, 57], creating 3D head avatars from given prompts becomes possible. Recent studies [1, 12, 89, 99] build upon the standard text-to-3D generation frameworks, achieving low-cost and high-fidelity performance by integrating with 3D parametric head models [4, 26, 82]. While focusing on facial geometry and texture modeling, these methods utilize holistic meshes or NeRF [39] to represent 3D haircut, failing to capture the internal geometric structure of hair strands, i.e., 3D curves. This limitation not only significantly reduces the realism of the generated avatars but also makes them incompatible with existing strand-based rendering and simulation systems [2, 9, 10]. To model 3D haircuts, recent efforts achieve strand-level hairstyle reconstruction [58, 66, 94] or generation [13, 97] using VAEs or parametric models. Despite their impressive performance, they require constrained multi-view images or manually latent space searching, failing to freely create 3D hair under semantics of textual prompts. The most recent work HAAR [67] pioneers text-to-strand generation by training text-conditioned hair map diffusion model. However, due to the high cost and limited scale of available 3D hair data, HAAR struggles to generate hairstyles outside of the training set. In addition, HAAR does not model hair texture or geometry that adapts to specific generated human head, which limits its applicability in downstream tasks. In this paper, we propose StrandHead, novel framework that generates strand-disentangled 3D head avatars from text descriptions. As illustrated in Fig. 1, the generated head avatars exhibit detailed 3D faces with high-fidelity hair strands and diverse 3D haircuts, enabling seamless and flexible haircut transfer, editing, and physical-based simulation. Without requiring any hair training data, we create 3D hair by distilling the 2D generative diffusion models. This is achieved by employing series of key insights on hair geometric priors. First, inspired by the cylindrical structure of hair strands [19, 35], we propose novel differentiable prismatization algorithm. This algorithm facilitates the efficient, flexible, and differentiable transformation of strands into watertight prismatic meshes with customizable thickness and defined lateral edges, enabling the use of mesh-based renderers, loss functions, and models for detailed strand-level modeling tasks. Second, leveraging the distribution patterns of 3D hair orientation and curvature, we propose reliable consistency regularization losses for robust generation. These losses enhance the cosine similarity between neighboring strand orientations and constrain the global hair curvature. Extensive experiments demonstrate that StrandHead outperforms the state-of-theart (SOTA) methods in both head and hair generation tasks, and supports flexible haircut transfer and editing, as well as physical-based rendering and simulation. Our main contributions are summarized as follows: We propose StrandHead, novel framework for generating high-fidelity 3D head avatars with disentangled strand-based hair. To the best of our knowledge, StrandHead is the first work to generate strand-disentangled 3D head from pre-trained 2D diffusion models. We propose differentiable prismatization algorithm that converts hair strands into watertight prismatic meshes, facilitating the use of mesh-based renderers, losses, algorithms, and models in strand-level deep learning tasks. Inspired by statistical hair features, we propose orientation consistency and curvature regularization losses to supervise both local and global hair shape, enabling reliable and realistic hairstyle generation. 2. Related Work Text-to-3D General Object Generation. Inspired by the success of text-to-image (T2I) generation [43, 53, 54, 57, 59, 91], many studies have explored using pre-trained vision-language models [52, 57] to achieve text-guided zero-shot 3D content generation. Early methods [18, 38, 40, 61, 71] employ the CLIP model [52] to supervise the alignment of 3D representations with prompts. DreamFusion [50] introduces the Score Distillation Sampling (SDS) loss, significantly enhancing the fidelity of generated 3D content by leveraging more powerful pre-trained diffusion models [57]. Subsequent works further advance text-driven 3D generation by improving 3D representations [5, 6, 85], optimization strategies [29, 70, 86], SDS loss [28, 75], and diffusion models [27, 51, 65]. Despite these advancements, current approaches focused on general content generation do not fully leverage the extensive prior knowledge of human heads and hair, limiting their ability to generate realistic, high-quality 3D head avatars with strand-based hair. Text-to-3D Head Avatar Generation. CLIPFace [1], T2P [92] and Describe3D [78] pioneer zero-shot text-driven 3D head generation by combining CLIP [52] with 3D parametric head models [4, 8, 26, 82]. With the introduction of SDS loss [50], DreamFace [90] and FaceG2E [80] leverage pre-trained T2I diffusion models to greatly improve generation fidelity within specific domains. HeadEvolver [72] enhances the expressiveness of head mesh deformations by introducing vector fields, while HumanNorm [17], HeadSculpt [12], and HeadArtist [30] employ DMTet [63] instead of traditional meshes to capture geometric details. HeadStudio [99] incorporates 3DGS [20] to produce realistic and animatable 3D heads. Despite these advancements, these approaches treat the head and hair as holistic model, limiting support for downstream applications such as hairstyle transfer and editing. To enable flexible head-hair-disentangled generation, TECA [89] independently represents the head with mesh and the hair with NeRF [39]. However, these methods focus only on the realistic external appearance of hair without modeling its internal geometric structure, restricting their application for 2 Task Method Text-to-Head Text-to-Hair [12, 17, 30, 99] [89] [28, 51, 65, 85] [67] Ours Head-HairDecoupled Strand-Based Hair Geometry & Texture No Training Data Text-to-Head-Hair Table 1. Comparison with current related methods. strand-level physics-based rendering and simulation. Strand-Based Hair Creation. Automatic strand-based hair modeling has attracted substantial attention from both industry [2, 9, 10] and academia [3, 14, 34, 87], as it enables downstream physics-based rendering and simulation Such methods can be roughly divided into two categories: (1) reconstructing hair from input images or videos and (2) generating hair based on control conditions. Works on the former one [15, 21, 33, 36, 42, 58, 60, 64, 66, 69, 76, 77, 83, 88, 9496, 98] relies on classical 3D reconstruction frameworks [20, 37, 39, 46, 62] and incorporates hairspecific features [45] to realize accurate 3D strand reconstruction. However, these methods depend on constrained multi-view images, making hair creation costly and challenging in less controlled settings. Recently, 3D hair generation approaches [13, 97] have emerged using GANs or parametric model, but they struggle with controllable generation. The most relevant work to ours is HAAR [67], which pioneers text-guided strand generation using diffusion models. However, HAAR relies on high-cost 3D hair data which is limited in diversity, restricting its ability to generate novel haircut beyond the training data. Compared to previous methods, StrandHead generate 3D heads with fine geometry and lifelike textures, as well as realistic textured strand-based hairstyles, and the entire framework does not require any hair training data. We summarize the main differences between our approach and related approaches in Tab. 1. 3. Method 3.1. Preliminaries FLAME [26] is 3D parametric head model that represents the human head shape, pose, and expression using compact set of parameters. Given set of shape parameters β, pose parameters θhead, and expression parameters ψexp, FLAME can create 3D head mesh . Score Distillation Sampling (SDS) [50] uses pre-trained T2I model to guide the optimization of 3D representation based on the input text. For an input text prompt y, 3D representation parameterized by θ, and T2I diffusion model with parameters ϕ, the SDS loss is defined as follows: θLSDS = Et,ϵ (cid:20) w(t)(ϵϕ(xt; y, t) ϵ) (cid:21) θ , (1) where = g(θ) is the image rendered from θ by differentiable renderer g, is the time step, xt = + ϵ is noised version of x, ϵϕ(xt; y, t) is the denoised image, and w(t) is weighting function. For simplicity, we omit w(t) in the following formulas. 3 DMTet [63] is hybrid representation that marrys signed distance function (SDF) and differentiable Marching Tetrahedral layer to convert implicit SDF to an explicit mesh. Thanks to multi-resolution hash encoding [41], the SDF can be implemented in lightweight manner. Neural Scalp Textures (NST) [58] is an efficient hair representation where each pixel stores feature vector conveying the shape information of single strand at the corresponding scalp location. With the help of pre-trained hair strand generator [58] G, the low-dimensional 2D neural scalp texture can be decoded into high-dimensional, dense 3D strand polylines = {si}Ns i=1, where Ns is the number of strand polylines, and si = {pi j=1 is polyline formed by Np 3D points. This process can be formulated as follows: = G(T ). j}Np 3.2. Overview Given text prompt, StrandHead aims to create highquality 3D head avatar with strand-disentangled textured hair. Fig. 2 provides an overview of our pipeline. We first produce high-fidelity and reasonable 3D bald head (Sec. 3.3). Then, we generate realistic, reasonable strandbased hair with lifelike appearance by introducing differentiable prismatization algorithm, two prior-driven losses and strand-aware texture field (Sec. 3.4). 3.3. Bald Head Generation Head Geometry Modeling. To sculpt detailed geometry, we follow HeadArtist [30] and HeadSculpt [12] using DMTet [63] as our 3D representation and initialize it with the FLAME mesh Minit. After initialization, we leverage Nvdiffrastt [22] and SDS loss to model the geometry and texture of the human head in decoupled manner. We employ series of human-specialized diffusion models proposed by HumanNorm [17] to further improve the fidelity of the generated head. Specifically, we optimize the head DMTet parameterized by θh using the following losses: θh Lhn SDS = Et,ϵ θh Lhd SDS = Et,ϵ (cid:20) (ϵϕhn (nh (cid:20) (ϵϕhd (dh t; yh, t) ϵ) t; yh, t) ϵ) (cid:21) (cid:21) , , nh θh dh θh (2) (3) where yh is the input bald head text, ϕhn and ϕhd are the human-specialized normal-adapted and depth-adapted diffusion models, respectively, and nh and dh are the rendered normal and depth maps of the bald head. Additionally, inspired by Barbie [68], we impose FLAME-evolving prior loss to prevent unnatural geometry (e.g., overly large ears or overly pointed head tops). In particular, we freeze the FLAME shape parameters β but improve Minit to evolvable ˆMinit by adding learnable vertexwise offsets, and periodically fit ˆMinit to the head DMTet every δ iterations, enabling the learned offsets to model Figure 2. Strandhead consists of two stages: (a) Under the constraints of the human-specific diffusion model and the FLAME-volving prior loss, StrandHead generates detailed and reasonable bald head. (b) By introducing differentiable prismatization algorithm, orientation consistency loss and curvature regularization loss inspired by hair geometric priors, StrandHead achieves diverse and realistic strandaccurate hair creation without any requiring hair training data. complex facial features (e.g., beards and wrinkles, see the Supp. Mat. for loss details). During evolution, ˆMinit gradually captures rich geometric details, and provide reliable and diverse head prior for geometry generation using the following formula: (cid:88) Lprior = pP (cid:13) (cid:13) (cid:13)sθh (p) ˆMinit (p) (cid:13) 2 (cid:13) (cid:13) 2 , (4) where sθh and ˆMinit are the SDF functions of the head DMTet and ˆMinit, and is set of randomly sampled points. In summary, the loss function for optimizing the head geometry is as follows: Lhead-geo = Lhn SDS + Lhd SDS + λpriorLprior. (5) Head Texture Modeling. Given the generated head geometry generated, we fix it and utilize texture field ψh which maps query position to its color to generate head appearance. Specifically, we construct this field using MLP network with multi-resolution hash encoding [41], and we optimize it with the following loss function: ψh Lhc SDS = Et,ϵ (cid:20) (ϵϕhc (ch t; nh, yh, t) ϵ) (cid:21) , ch ψh (6) where ϕhc is human-specialized normal-conditioned diffusion model, and ch represents the rendered color image of the generated head. Since the vanilla SDS loss often leads to color oversaturation, we replace it with the following MSDS loss [17] to further enhance the textures realism in later iterations of texture optimization: ψh Lhc SDS = Et,ϵ (cid:20) (H(ch t; nh, yh, t) ϵ) (cid:21) , ch ψh (7) where H() denotes the multi-step operation function. 3.4. Strand-Based Hair Generation Hair Initialization. To achieve accurate and diverse hair initialization, we utilize ChatGPT [44] to select the 20 most representative hairstyles from the USC-HairSalon Dataset [15]. We then use the following loss to optimize NST to fit the selected hairstyle: Lfit = Ns(cid:88) Np (cid:88) i=1 j= ˆpi pi j2 +λori(1 ˆoi oi j)+λcurˆci ci j1, oi j)/pi and pi = oi j, and ci j+1 pi j+1 pi (8) where ˆpi are the position of the j-th point on the i-th polyline of the GT and generated hair, respectively, and ˆoi j, j, ˆci oi denote their orientation and curvature, respectively. Here, orientation oi = (pi j2 represents the direction of change in strand position, and curvature ci j12 denote the rate of change in strand orientation. Given the hair description, we select an optimal pre-trained NST as initialization and further optimize it in the following stages. Differentiable Prismatization. Actually, as there is no reliable strand-based differentiable renderer, performing endto-end hair strands optimization becomes challenging. Further, since we have no constrained multi-view images, we cannot directly model the hair geometry and appearance using 3DGS [20] or line-based renderer [58], as existing hair reconstruction methods [33, 58, 69, 88]. feasible alternative is to first convert the strands into quad meshes [87], similar to NeuralHaircut [66], and then leverage mesh optimization frameworks to model hair strands. However, this non-watertight stripe-like mesh easily produces ambiguous normal maps or excessively thin sides, which significantly reduce the stability of optimization (Sec. 4.4). Therefore, more stable strand-to-mesh conversion method with superior optimization properties is urgently necessary. To this end, we propose novel differentiable prismatization algorithm inspired by the internal structure of hair (i.e., hair fiber is dielectric cylinder covered with tilted scales and with pigmented interior [19, 35]). This algorithm can efficiently and differentiably convert strands into watertight prismatic meshes with arbitrary thickness and lateral edges to approximate the cylindrical structure of hair, enabling the use of mesh-based renderers and algorithms for strand-level modeling. Specifically, given hair strand s, our differentiable prismization algorithm converts 4 Figure 3. Illustration of the process of converting hair strand into an octagonal prism mesh using the differentiable prismatization algorithm. it into watertight prismatic mesh with lateral edges and radius through the following five steps: 1. Compute the Initial Normal Vector: Determine normal to the hair strand by taking the cross product of its orientation with non-collinear reference point (typically the center of the head). 2. Generate Rotated Normals: Rotate this normal around the axis defined by the strands orientation times, each by 360 , to produce normals. 3. Translate to Form Lateral Edges: Translate along each of these normals by R, generating lateral edges of the prism. 4. Construct Lateral Faces: Connect the adjacent lateral edgesvertices to form the lateral faces of the prism. 5. Construct Top and Bottom Faces: Connect the vertices at the ends of the lateral edges to form the top and bottom faces of the prism, completing the conversion from hair strand to watertight prismatic mesh. Figure 3 shows an example of converting hair strand into Importantly, the proposed difan octagonal prism mesh. ferentiable prismatization algorithm can be easily implemented on GPU, achieving flexible and fast prismatization of hair strands and paving new way for hair modeling. The algorithm details are shown in the Supp. Mat. Hair Geometry and Texture Modeling. With the help of our proposed differentiable prismatization algorithm, we create the hair geometry and texture in two stages as Sec. 3.3 using the following losses: Lhn SDS = Et,ϵ (cid:20) (ϵϕhn (nh+s ; yh, t) ϵ) Lhd SDS = Et,ϵ (cid:20) (ϵϕhd (dh+s ; yh, t) ϵ) (9) (cid:21) , (cid:21) , (10) nh+s dh+s ψs Lhc SDS = Et,ϵ (cid:20) (ϵϕhc (ch+s ; nh+s, yh, t) ϵ) (cid:21) , ch+s ψs ψs Lhc SDS = Et,ϵ (cid:20) (H(ch+s ; nh+s, yh, t) ϵ) ch+s ψs (11) (cid:21) , (12) where nh+s, dh+s, and ch+s are the rendered normal, depth, and color maps of the head θh with strand-based hair prismatic mesh Mp = DP (G(T )), respectively. Here DP () denotes the differentiable prismatization operation function, ψs is the strand texture field. Prior-Driven Losses. However, due to the excessive flexibility of hair strands, relying solely on the SDS loss leads Figure 4. The distribution of Oori and Cmean in the USC-HairSalon dataset [15]. The results indicate that (1) neighboring strand orientations are highly consistent; (2) strand curvature is strongly and positively related to the haircut curliness. to unnatural strand orientations and unrealistic hairstyles (Sec. 4.4). Further, the lack of constrained multi-view images prevents us from leveraging hair-specific features [45] to provide powerful supervision as in previous works [58]. To address this issue, we propose two straightforward yet solid prior-driven losses based on observations of the hair strand orientation and curvature to ensure the rationality of the generated hair strands. Specifically, we observe two geometric properties of hair: (1) neighboring strand orientations are highly consistent, and (2) strand curvature is strongly and positively correlated with the curliness of the hairstyle. To validate the properties, we calculate the cosine similarity of adjacent hair strand orientations CSori and the average curvature Cmean of 343 hairstyles in the USCHairSalon dataset [15] using the following equations and visualize their distributions and some examples in Fig. 4: Ns(cid:88) Np (cid:88) (cid:88) ok oi A(i) , (13) CSori = 1 NsNp i=1 j=1 kA(i) Cmean = 1 NsNp Ns(cid:88) Np (cid:88) i=1 j=1 ci j, (14) where A(i) is the set of adjacent strands for the i-th strand, and A(i) is the number of neighboring strands. As illustrated, over 95% of hairstyles exhibit an orientation similarity above 0.9, and the average curvature is significantly positively related to curliness, which successfully validates the properties. Based on these observations, we introduce orientation consistency loss and curvature regularization to guide the local and global strand shapes, formulated as follows: 5 Figure 5. Examples of high-fidelity and diverse 3D heads and strand-accurate haircuts generated by our method. The upper visualization includes rendered color and normal maps of the head and hair prism meshes. The lower visualization shows the physics-based rendering result using Blender [10]. Please zoom in for detailed views, and refer to the Supp. Mat. for video demonstrations. Lori = 1 CSori, Lcur = Cmean Ctarget1, (15) (16) where Ctarget represents the target average curvature set according to the input hair description. These two losses ensure the rationality of generated strands by supervising the consistency of orientations between adjacent strands and the overall hairstyles curvature. Additionally, we introduce series of geometry-aware losses Lbbox, Lf ace, and Lcolli, which prevent the hair from exceeding the bounding box, obscuring the face, and colliding with the head, respectively. Details are provided in the Supp. Mat. The final loss function for optimizing hair geometry is expressed as follows: SDSLhn SDS + λoriLori + λcurLcur SDS + λhn + λbboxLbbox + λfaceLface + λcolliLcolli. Lhair-geo = Lhd (17) Strand-Aware Texture Field. Since the complexity of hair textures, basic texture field cannot fully capture the lifelike appearance of strands (Sec. 4.4). Therefore, we propose strand-aware texture field that better models high-frequency color variations. Specifically, we introduce two improvements to the basic texture field: first, we replace Euclidean coordinates with scalp UV coordinates which are more uniformly distributed. Second, we incorporate strand orientations as additional input information to model orientationdependent texture. These enhancements enhance the re6 alism of the generated results by better creating the highfrequency appearance variations, but also ensure consistent colors across different faces of single prismatic mesh since the input features are strand-based. For more details, please refer to the Supp. Mat. 3.5. Implementation Details Our approach utilizes PyTorch [47] and ThreeStudio [11] for implementation. Experiments are carried out on an Ubuntu server with A6000 GPUs. Starting with textual description of head with hair, such as man with brown hair. we initially create bald head using the prompt bald man and then generate hair based on man with brown hair. Further implementation details are available in the Supp. Mat. 4. Experiments Some examples of 3D head models with strand-based hair generated by StrandHead are shown in Fig. 5. Due to space limitations, only the key experimental settings and results are presented here. For more results and details, please refer to our Supp. Mat. 4.1. Experimental Settings Baselines. We compare StrandHead with the SOTA methods for head avatar and haircut generation. The baseline Figure 6. Qualitative comparisons with the SOTA methods. Since TECA [89] uses the vanilla NeRF to represent hair, rendering normals is not supported. HAAR [67] generates only the geometry of hair strands, so we first convert the strands into prismatic meshes using differentiable prismatization and then utilize TEXTure [56] to generate texture for visualization and comparison. Method BLIP-VQA BLIP2-VQA GQP TAP HeadArtist HeadStudio HumanNorm TECA StrandHead (Ours) MVDream GaussianDreamer LucidDreamer RichDreamer TECA HAAR StrandHead (Ours) 0.7667 0.7833 0.7000 0.7333 0.8500 0.9000 0.3333 0.8000 0.8333 0.7000 0.6333 0. 0.9667 0.8833 0.9500 0.9500 0.9667 0.8333 0.3667 0.9333 0.7667 0.7667 0.2000 0.9000 1.00 3.33 7.67 34.33 53.67 24.67 5.33 5.33 4.00 1.67 1.33 57.67 2.33 3.67 7.67 28.33 58.00 20.00 3.00 5.00 5.67 3.67 2.33 60. Table 2. Quantitative comparisons with the SOTA methods. The best and second-best results are highlighted in bolded and underlined, respectively. GQP: generation quality preference (%), TAP: text-image alignment preference (%). ing the fine-grained alignment between 3D content and input prompts, as further validated by our experiments in the Supp. Mat. To address this limitation, and drawing inspiration from Progressive3D [7] and Barbie [68], we use BLIPVQA [23, 24] and BLIP2-VQA [23, 25] to evaluate both current methods and StrandHeads generative capabilities. Specifically, we convert each prompt into two questions to separately verify head and hair. We then feed the rendered image of the generated 3D content into the VQA model, asking each question in sequence and using the probability of yes response as our evaluation metric. For example, the head prompt man with black hair is transformed into Is the person in the picture man? and Does the person in the picture have black hair? The hair prompt Black hair becomes Is the object in the picture black hair? Finally, we conduct user study by randomly selecting 10 generated examples and asking 30 volunteers to assess (1) generation quality and (2) text-image alignment, and select the best methods. 4.2. Comparisons of Head Generation As illustrated in the upper section of Tab. 2, our approach outperforms all comparison methods across every evaluaFigure 7. Qualitative comparison with HAAR [67]. For better visual comparison, we interpolate the hairstyles to approximately 10,000 strands and apply consistent appearance. Since HAAR does not model heads, its generated results frequently display hairhead collisions, highlighted within the black box. methods for head generation include text-to-holistic-head works (HeadArtist [30], HumanNorm [17], and HeadStudio [99]) and text-to-decoupled-head work (TECA [89]). For text-to-hair generation, we consider general text-to-3D methods (MVDream [65], GaussianDreamer [85], LucidDreamer [28], and RichDreamer [51]), strand-specific textto-hair methods (HAAR [67]), and hair-disentangled textto-head methods (TECA [89]). Dataset Construction. To create the dataset, we use ChatGPT[44] to randomly generate 30 text descriptions of heads with hair. These include 30 descriptions for assessing head generation and 30 for evaluating hair generation. The full list of prompts can be found in the Supp. Mat. Evaluation Metrics. Current text-to-3D methods typically use CLIP-based metrics to assess text-image alignment and the quality of generated outputs. However, research has shown [16, 32] that CLIP-based metrics fall short in captur7 Figure 8. Ablation study on (a) strand-level optimization, (b) orientation consistency loss, (c) curvature regularization loss, (d) differentiable prismatization, and (e) strand-aware texture field. tion metric. The qualitative results on the left side of Fig. 6 further highlight the advantages of our method. Compared to other head generation methods, the head avatars generated by StrandHead exhibit not only more refined facial geometry and texture details but separable strand-accurate hair with plausible appearance that integrates seamlessly with physics-based rendering and simulation systems. To the best of our knowledge, StrandHead is the first head generation framework to achieve strand-level hair modeling, capability that holds substantial potential for advancing human-centric 3D AIGC applications in the industry. 4.3. Comparisons of Hair Generation As shown in the lower part of Tab. 2, our approach surpasses other methods across most evaluation metrics, and is only slightly lower than LucidDreamer (the latest open-source text-to-3D method) on BLIP2-VQA, ranking the second. The qualitative comparisons on the right side of Fig. 6 display sample generation results. Compared to general textto-3D methods, our approach more precisely captures the internal structure of hair strands, resulting in realistic haircut geometry and appearance without incorporating incorrect content such as parts of the human head. Unlike the text-to-strand approach HAAR [67], our approach does not require 3D hair training data, enabling it to better generate uncommon hairstyles (e.g., slicked-back and side-swept hairstyles), as further validated by the physicsbased rendering comparison in Fig. 7. Additionally, StrandHead can generate both hair textures and head models, which not only extends the range of downstream applications but also avoids hair-head collisions (see Fig. 7). 4.4. Ablation Study Effect of Strand-Level Optimization. As illustrated in Fig. 8-(a), under dual constraints of SDS loss and priordriven losses, the optimized hair better aligns with the text and integrates harmoniously with the human head. Effect of Orientation Consistency Loss. Fig. 8-(b) demonstrates that incorporating Lcur provides strong guidance for generating realistic and natural hair. Effect of Curvature Regularization Loss. As shown in Fig. 8-(c), our Lcur preserves the desired curliness by conFigure 9. Applications of StrandHead. straining the strand curvature. Effect of Differentiable Prismatization. As shown in Fig. 8-(d), compared to the quad mesh used by NeuralHaircut [66] our proposed differentiable prismatization algorithm enables more stable gradient backpropagation for reliable strand-based hair optimization, while also avoiding abnormal normal issues caused by non-watertight meshes. Effect of Strand-Aware Texture Field. As shown in Fig. 8-(e), our proposed strand-aware texture field accurately models high-frequency appearance details by switching coordinate spaces and incorporating orientation information, resulting in more realistic strand textures. 4.5. Applications As illustrated in Fig. 9-(a), StrandHead enables the flexible combination of any head with any hairstyle, greatly enhancing the playability and reusability of head generation. Furthermore, due to our precise strand-level hair modeling, we can not only edit hairstyles by manipulating strand geometry and texture but also leverage mature industrial software to achieve physics-based realistic rendering and simulation, as shown in Fig. 9-(b) and (c). 5. Conclusion In this paper, novel framework StrandHead is proposed to generate realistic hair-disentangled 3D head avatar using text prompts. StrandHead generates disentangled strandbased hair only from the pre-trained 2D generative diffusion models, requiring no extra 3D hair data for supervision. StrandHead achieves this goal mainly using two approaches: (1) proposing novel differentiable prismatization method that transforms the hair strands into meshes of octagonal prisms, and (2) using the prior-driven losses inspired by the statistical features of 3D hair data. The former one produces geometry more similar to the original hair strands, and makes the optimization available to meshbased renderers with significantly less ambiguity. Besides, the latter one well constrains the optimization with reliable guidance to generate reasonable 3D haircut. Extensive experiments demonstrate that StrandHead obtains the stateof-the-art performance on the generated 3D head avatars and haircuts. The generated results are easily implemented in industrial softwares to produce physical simulation, and high-fidelity rendering."
        },
        {
            "title": "References",
            "content": "[1] Shivangi Aneja, Justus Thies, Angela Dai, and Matthias Nießner. Clipface: Text-guided editing of textured 3d morphable models. ACM SIGGRAIPH, 2022. 2 [2] Inc. Autodesk. Autodesk maya - 3d animation and modhttps : / / www . autodesk . com / eling software. products/maya, 2024. 2, 3 [3] Gaurav Bhokare, Eisen Montalvo, Elie Diaz, and Cem YukIn ACM sel. Real-time hair rendering with hair meshes. SIGGRAIPH, 2024. 3 [4] Volker Blanz and Thomas Vetter. morphable model for the synthesis of 3d faces. In Seminal Graphics Papers: Pushing the Boundaries, Volume 2, 2023. [5] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and appearance for highIn Int. Conf. Comput. quality text-to-3d content creation. Vis., 2023. 2 [6] Zilong Chen, Feng Wang, and Huaping Liu. Text-to-3d using gaussian splatting. In IEEE Conf. Comput. Vis. Pattern Recog., 2024. 2 [7] Xinhua Cheng, Tianyu Yang, Jianan Wang, Yu Li, Lei Zhang, Jian Zhang, and Li Yuan. Progressive3d: Progressively local editing for text-to-3d content creation with complex semantic prompts. In Int. Conf. Learn. Represent., 2023. 7 [8] Hang Dai, Nick Pears, William Smith, and Christian Duncan. Statistical modeling of craniofacial shape and texture. Int. J. Comput. Vis., 2020. 2 [9] Inc. Epic Games. Unreal engine - real-time 3d creation tool. https://www.unrealengine.com/, 2024. 2, 3 [10] Blender Foundation. Blender - 3d modelling and rendering software. https://www.blender.org/, 2024. 2, 3, 6 [11] Yuan-Chen Guo, Ying-Tian Liu, Ruizhi Shao, Christian Laforte, Vikram Voleti, Guan Luo, Chia-Hao Chen, ZiXin Zou, Chen Wang, Yan-Pei Cao, and Song-Hai Zhang. threestudio: unified framework for 3d content generation. https://github.com/threestudioproject/ threestudio, 2023. 6 [12] Xiaoping Han, Yukang Cao, K. Han, Xiatian Zhu, Jiankang Deng, Yi-Zhe Song, Tao Xiang, and Kwan-Yee K. Wong. Headsculpt: Crafting 3d head avatars with text. In Adv. Neural Inform. Process. Syst., 2023. 2, [13] Chengan He, Xin Sun, Zhixin Shu, Fujun Luan, Soren Pirk, Jorge Alejandro Amador Herrera, Dominik Michels, Tuanfeng Wang, Meng Zhang, Holly Rushmeier, and Yi Zhou. Perm: parametric representation for multi-style 3d hair modeling. arXiv preprint arXiv:2407.19451, 2024. 2, 3 [14] Jerry Hsu, Tongtong Wang, Zherong Pan, Xifeng Gao, Cem Yuksel, and Kui Wu. Real-time physically guided hair interpolation. ACM Trans. Graph., 2024. 3 [15] Liwen Hu, Chongyang Ma, Linjie Luo, and Hao Li. Singleview hair modeling using hairstyle database. ACM Trans. Graph., 2015. 3, 4, 5 [16] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: comprehensive benchmark for open-world compositional text-to-image generation. In Adv. Neural Inform. Process. Syst., 2023. 7 [17] Xin Huang, Ruizhi Shao, Qi Zhang, Hongwen Zhang, Ying Feng, Yebin Liu, and Qing Wang. Humannorm: Learning normal diffusion model for high-quality and realistic 3d human generation. In IEEE Conf. Comput. Vis. Pattern Recog., 2024. 2, 3, 4, 7 [18] Ajay Jain, Ben Mildenhall, Jonathan Barron, Pieter Abbeel, and Ben Poole. Zero-shot text-guided object generation with dream fields. In IEEE Conf. Comput. Vis. Pattern Recog., 2022. 2 [19] James Kajiya and Timothy Kay. Rendering fur with three dimensional textures. ACM Siggraph Computer Graphics, 1989. 2, [20] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 2023. 2, 3, 4 [21] Zhiyi Kuang, Yiyang Chen, Hongbo Fu, Kun Zhou, and Youyi Zheng. Deepmvshair: Deep hair modeling from sparse views. In ACM SIGGRAPH Asia, 2022. 3 [22] Samuli Laine, Janne Hellsten, Tero Karras, Yeongho Seol, Jaakko Lehtinen, and Timo Aila. Modular primitives for ACM Trans. high-performance differentiable rendering. Graph., 2020. 3 [23] Dongxu Li, Junnan Li, Hung Le, Guangsen Wang, Silvio Savarese, and Steven C.H. Hoi. LAVIS: one-stop library for language-vision intelligence. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), 2023. 7 [24] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International Conference on Machine Learning, 2022. 7 [25] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with In Infrozen image encoders and large language models. ternational Conference on Machine Learning, 2023. 7 [26] Tianye Li, Timo Bolkart, Michael Black, Hao Li, and Javier Romero. Learning model of facial shape and expression from 4d scans. ACM Trans. Graph., 2017. 2, 3 [27] Weiyu Li, Rui Chen, Xuelin Chen, and Ping Tan. Sweetdreamer: Aligning geometric priors in 2d diffusion for consistent text-to-3d. arXiv preprint arXiv:2310.02596, 2023. 2 [28] Yixun Liang, Xin Yang, Jiantao Lin, Haodong Li, Xiaogang Xu, and Yingcong Chen. Luciddreamer: Towards highfidelity text-to-3d generation via interval score matching. In IEEE Conf. Comput. Vis. Pattern Recog., 2024. 2, 3, 7 [29] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In IEEE Conf. Comput. Vis. Pattern Recog., 2023. [30] Hongyu Liu, Xuan Wang, Ziyu Wan, Yujun Shen, Yibing Song, Jing Liao, and Qifeng Chen. Headartist: Textconditioned 3d head generation with self score distillation. In ACM SIGGRAIPH, 2024. 2, 3, 7 [31] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael Black. Smpl: skinned multiperson linear model. ACM Trans. Graph., 2015. 9 [32] Yujie Lu, Xianjun Yang, Xiujun Li, Xin Eric Wang, and William Yang Wang. Llmscore: Unveiling the power of large language models in text-to-image synthesis evaluation. In Adv. Neural Inform. Process. Syst., 2024. 7 [33] Haimin Luo, Min Ouyang, Zijun Zhao, Suyi Jiang, Longwen Zhang, Qixuan Zhang, Wei Yang, Lan Xu, and Jingyi Yu. Gaussianhair: Hair modeling and rendering with light-aware gaussians. arXiv preprint arXiv:2402.10483, 2024. 3, 4 [34] Ryota Maeda, Kenshi Takayama, and Takafumi Taketomi. Refinement of hair geometry by strand integration. In Comput. Graph. Forum, 2023. 3 [35] Stephen Marschner, Henrik Wann Jensen, Mike Cammarano, Steve Worley, and Pat Hanrahan. Light scattering from human hair fibers. ACM Trans. Graph., 2003. 2, 4 [36] Givi Meishvili, James Clemoes, Charlie Hewitt, Zafiirah Hosenie, Xian Xiao, Martin de La Gorce, Tibor Takacs, Tadas Baltrusaitis, Antonio Criminisi, Chyna McRae, et al. Hairmony: Fairness-aware hairstyle classification. arXiv preprint arXiv:2410.11528, 2024. [37] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Occupancy networks: Learning 3d reconstruction in function space. In IEEE Conf. Comput. Vis. Pattern Recog., 2019. 3 [38] Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and Rana Hanocka. Text2mesh: Text-driven neural stylization In IEEE Conf. Comput. Vis. Pattern Recog., for meshes. 2022. 2 [39] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In Eur. Conf. Comput. Vis., 2020. 2, 3 [40] Nasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky, and Tiberiu Popa. Clip-mesh: Generating textured meshes from text using pretrained image-text models. In ACM SIGGRAPH Asia, 2022. 2 [41] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with multiresolution hash encoding. ACM Trans. Graph., 2022. 3, 4 [42] Giljoo Nam, Chenglei Wu, Min Kim, and Yaser Sheikh. In IEEE Conf. Strand-accurate multi-view hair capture. Comput. Vis. Pattern Recog., 2019. [43] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. 2 [44] OpenAI. Chatgpt. https://openai.com/, 2024. 4, 7 [45] Sylvain Paris, Hector Briceno, and Francois Sillion. Capture of hair geometry from multiple images. ACM Trans. Graph., 2004. 3, 5 [46] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. In IEEE Conf. Comput. Vis. Pattern Recog., 2019. 3 [47] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In Adv. Neural Inform. Process. Syst., 2019. 6 [48] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and Michael Black. Expressive body capture: 3d hands, face, and body from single image. In IEEE Conf. Comput. Vis. Pattern Recog., 2019. [49] Pascal Paysan, Reinhard Knothe, Brian Amberg, Sami Romdhani, and Thomas Vetter. 3d face model for pose In 2009 sixth and illumination invariant face recognition. IEEE international conference on advanced video and signal based surveillance, 2009. [50] Ben Poole, Ajay Jain, Jonathan Barron, and Ben MildenIn Int. hall. Dreamfusion: Text-to-3d using 2d diffusion. Conf. Learn. Represent., 2022. 2, 3 [51] Lingteng Qiu, Guanying Chen, Xiaodong Gu, Qi Zuo, Mutian Xu, Yushuang Wu, Weihao Yuan, Zilong Dong, Liefeng Bo, and Xiaoguang Han. Richdreamer: generalizable normal-depth diffusion model for detail richness in text-to3d. In IEEE Conf. Comput. Vis. Pattern Recog., 2024. 2, 3, 7 [52] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, 2021. 2 [53] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, 2021. [54] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. 2 [55] Anurag Ranjan, Timo Bolkart, Soubhik Sanyal, and Michael Black. Generating 3d faces using convolutional mesh autoencoders. In Eur. Conf. Comput. Vis., 2018. 1 [56] Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, and Daniel Cohen-Or. Texture: Text-guided texturing of 3d shapes. In ACM SIGGRAIPH, 2023. 7 [57] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In IEEE Conf. Comput. Vis. Pattern Recog., 2022. 2 [58] Radu Alexandru Rosu, Shunsuke Saito, Ziyan Wang, Chenglei Wu, Sven Behnke, and Giljoo Nam. Neural strands: Learning hair geometry and appearance from multi-view images. In Eur. Conf. Comput. Vis., 2022. 2, 3, 4, 5 [59] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep In Adv. Neural Inform. Process. language understanding. Syst., 2022. 2 [60] Shunsuke Saito, Liwen Hu, Chongyang Ma, Hikaru Ibayashi, Linjie Luo, and Hao Li. 3d hair synthesis using 10 volumetric variational autoencoders. ACM Trans. Graph., 2018. 3 [61] Aditya Sanghi, Hang Chu, Joseph Lambourne, Ye Wang, Chin-Yi Cheng, Marco Fumero, and Kamal Rahimi Malekshan. Clip-forge: Towards zero-shot text-to-shape generation. In IEEE Conf. Comput. Vis. Pattern Recog., 2022. 2 [62] Johannes Schonberger, Enliang Zheng, Jan-Michael Frahm, and Marc Pollefeys. Pixelwise view selection for unstructured multi-view stereo. In Eur. Conf. Comput. Vis., 2016. 3 [63] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and Sanja Fidler. Deep marching tetrahedra: hybrid representation for high-resolution 3d shape synthesis. In Adv. Neural Inform. Process. Syst., 2021. 2, [64] Yuefan Shen, Shunsuke Saito, Ziyan Wang, Olivier Maury, Chenglei Wu, Jessica Hodgins, Youyi Zheng, and Giljoo Nam. Ct2hair: High-fidelity 3d hair modeling using computed tomography. ACM Trans. Graph., 2023. 3 [65] Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. In Int. Conf. Learn. Represent., 2023. 2, 3, 7 [66] Vanessa Sklyarova, Jenya Chelishev, Andreea Dogaru, Igor Medvedev, Victor Lempitsky, and Egor Zakharov. Neural In haircut: Prior-guided strand-based hair reconstruction. IEEE Conf. Comput. Vis. Pattern Recog., 2023. 2, 3, 4, 8 [67] Vanessa Sklyarova, Egor Zakharov, Otmar Hilliges, Michael Black, and Justus Thies. Text-conditioned generative model of 3d strand-based human hairstyles. In IEEE Conf. Comput. Vis. Pattern Recog., 2024. 2, 3, 7, 8 [68] Xiaokun Sun, Zhenyu Zhang, Ying Tai, Qian Wang, Hao Tang, Zili Yi, and Jian Yang. Barbie: Text to barbie-style 3d avatars. arXiv preprint arXiv:2408.09126, 2024. 3, 7 [69] Yusuke Takimoto, Hikari Takehara, Hiroyuki Sato, Zihao Zhu, and Bo Zheng. Dr. hair: Reconstructing scalpconnected hair strands without pre-training via differentiable rendering of line segments. In IEEE Conf. Comput. Vis. Pattern Recog., 2024. 3, 4 [70] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. In Int. Conf. Learn. Represent., 2023. 2 [71] Can Wang, Menglei Chai, Mingming He, Dongdong Chen, and Jing Liao. Clip-nerf: Text-and-image driven manipulation of neural radiance fields. In IEEE Conf. Comput. Vis. Pattern Recog., 2022. 2 [72] Duotun Wang, Hengyu Meng, Zeyu Cai, Zhijing Shao, Qianxi Liu, Lin Wang, Mingming Fan, Xiaohang Zhan, and Zeyu Wang. Headevolver: Text to head avatars via expressive and attribute-preserving mesh deformation. arXiv preprint arXiv:2403.09326, 2024. [73] Lizhen Wang, Zhiyuan Chen, Tao Yu, Chenguang Ma, Liang Li, and Yebin Liu. Faceverse: fine-grained and detailcontrollable 3d face morphable model from hybrid dataset. In IEEE Conf. Comput. Vis. Pattern Recog., 2022. [74] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. In Adv. Neural Inform. Process. Syst., 2021. [75] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. In Adv. Neural Inform. Process. Syst., 2024. 2 [76] Keyu Wu, Yifan Ye, Lingchen Yang, Hongbo Fu, Kun Zhou, and Youyi Zheng. Neuralhdhair: Automatic high-fidelity hair modeling from single image using implicit neural repIn IEEE Conf. Comput. Vis. Pattern Recog., resentations. 2022. 3 [77] Keyu Wu, Lingchen Yang, Zhiyi Kuang, Yao Feng, Xutao Han, Yuefan Shen, Hongbo Fu, Kun Zhou, and Youyi Zheng. Monohair: High-fidelity hair modeling from monocular In IEEE Conf. Comput. Vis. Pattern Recog., 2024. video. 3 [78] Menghua Wu, Hao Zhu, Linjiang Huang, Yi Zhuang, Yuanxun Lu, and Xun Cao. High-fidelity 3d face generation from natural language descriptions. IEEE Conf. Comput. Vis. Pattern Recog., 2023. [79] Sijing Wu, Yichao Yan, Yunhao Li, Yuhao Cheng, Wenhan Zhu, Ke Gao, Xiaobo Li, and Guangtao Zhai. Ganhead: ToIn IEEE wards generative animatable neural head avatars. Conf. Comput. Vis. Pattern Recog., 2023. 1 [80] Yunjie Wu, Yapeng Meng, Zhipeng Hu, Lincheng Li, Haoqian Wu, Kun Zhou, Weiwei Xu, and Xin Yu. Text-guided 3d face synthesis-from generation to editing. In IEEE Conf. Comput. Vis. Pattern Recog., 2024. 2 [81] Yuanyou Xu, Zongxin Yang, and Yi Yang. Seeavatar: Photorealistic text-to-3d avatar generation with constrained geometry and appearance. arXiv preprint arXiv:2312.08889, 2023. [82] Haotian Yang, Hao Zhu, Yanru Wang, Mingkai Huang, Qiu Shen, Ruigang Yang, and Xun Cao. Facescape: large-scale high quality 3d face dataset and detailed riggable 3d face prediction. In IEEE Conf. Comput. Vis. Pattern Recog., 2020. 2 [83] Lingchen Yang, Zefeng Shi, Youyi Zheng, and Kun Zhou. Dynamic hair modeling from monocular videos using deep neural networks. ACM Trans. Graph., 2019. 3 [84] Tarun Yenamandra, Ayush Tewari, Florian Bernard, HansPeter Seidel, Mohamed Elgharib, Daniel Cremers, and i3dmm: Deep implicit 3d morphable Christian Theobalt. model of human heads. In IEEE Conf. Comput. Vis. Pattern Recog., 2021. [85] Taoran Yi, Jiemin Fang, Junjie Wang, Guanjun Wu, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Qi Tian, and Xinggang Wang. Gaussiandreamer: Fast generation from text to 3d gaussians by bridging 2d and 3d diffusion models. In IEEE Conf. Comput. Vis. Pattern Recog., 2024. 2, 3, 7 [86] Taoran Yi, Jiemin Fang, Zanwei Zhou, Junjie Wang, Guanjun Wu, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Xinggang Wang, and Qi Tian. Gaussiandreamerpro: Text to manipulable 3d gaussians with highly enhanced quality. arXiv preprint arXiv:2406.18462, 2024. 2 [87] Cem Yuksel, Scott Schaefer, and John Keyser. Hair meshes. ACM Trans. Graph., 2009. 3, 4 [88] Egor Zakharov, Vanessa Sklyarova, Michael Black, Giljoo Nam, Justus Thies, and Otmar Hilliges. Human hair reconstruction with strand-aligned 3d gaussians. In Eur. Conf. Comput. Vis., 2024. 3, 4 [89] Hao Zhang, Yao Feng, Peter Kulits, Yandong Wen, Justus Thies, and Michael J. Black. Teca: Text-guided generation In International and editing of compositional 3d avatars. Conference on 3D Vision, 2024. 2, 3, 7 [90] Longwen Zhang, Qiwei Qiu, Hongyang Lin, Qixuan Zhang, Cheng Shi, Wei Yang, Ye Shi, Sibei Yang, Lan Xu, and Jingyi Yu. Dreamface: Progressive generation of animatable 3d faces under text guidance. ACM Trans. Graph., 2023. 2 [91] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Int. Conf. Comput. Vis., 2023. 2 [92] Rui Zhao, Wei Li, Zhipeng Hu, Lincheng Li, Zhengxia Zou, Zhen Xia Shi, and Changjie Fan. Zero-shot text-to-parameter translation for game character auto-creation. In IEEE Conf. Comput. Vis. Pattern Recog., 2023. 2 [93] Mingwu Zheng, Hongyu Yang, Di Huang, and Liming Chen. Imface: nonlinear 3d morphable face model with implicit neural representations. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022. 1 [94] Yujian Zheng, Zirong Jin, Moran Li, Haibin Huang, Chongyang Ma, Shuguang Cui, and Xiaoguang Han. Hairstep: Transfer synthetic to real using strand and depth maps for single-view 3d hair modeling. In IEEE Conf. Comput. Vis. Pattern Recog., 2023. 2, [95] Yujian Zheng, Yuda Qiu, Leyang Jin, Chongyang Ma, Haibin Huang, Di Zhang, Pengfei Wan, and Xiaoguang Han. Towards unified 3d hair reconstruction from single-view portraits. arXiv preprint arXiv:2409.16863, 2024. [96] Yi Zhou, Liwen Hu, Jun Xing, Weikai Chen, Han-Wei Kung, Xin Tong, and Hao Li. Hairnet: Single-view hair reconstruction using convolutional neural networks. In Eur. Conf. Comput. Vis., 2018. 3 [97] Yuxiao Zhou, Menglei Chai, Alessandro Pepe, Markus Gross, and Thabo Beeler. Groomgen: high-quality generative hair model using hierarchical latent representations. ACM Trans. Graph., 2023. 2, 3 [98] Yuxiao Zhou, Menglei Chai, Daoye Wang, Sebastian Winberg, Erroll Wood, Kripasindhu Sarkar, Markus Gross, and Thabo Beeler. Groomcap: High-fidelity prior-free hair capture. arXiv preprint arXiv:2409.00831, 2024. 3 [99] Zhen Zhou, Fan Ma, Hehe Fan, and Yi Yang. Headstudio: Text to animatable head avatars with 3d gaussian splatting. In Eur. Conf. Comput. Vis., 2024. 2, 3,"
        }
    ],
    "affiliations": [
        "Nanjing University",
        "The Hong Kong University of Science and Technology (Guangzhou)"
    ]
}