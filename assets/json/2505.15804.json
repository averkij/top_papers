{
    "paper_title": "STAR-R1: Spatial TrAnsformation Reasoning by Reinforcing Multimodal LLMs",
    "authors": [
        "Zongzhao Li",
        "Zongyang Ma",
        "Mingze Li",
        "Songyou Li",
        "Yu Rong",
        "Tingyang Xu",
        "Ziqi Zhang",
        "Deli Zhao",
        "Wenbing Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities across diverse tasks, yet they lag significantly behind humans in spatial reasoning. We investigate this gap through Transformation-Driven Visual Reasoning (TVR), a challenging task requiring identification of object transformations across images under varying viewpoints. While traditional Supervised Fine-Tuning (SFT) fails to generate coherent reasoning paths in cross-view settings, sparse-reward Reinforcement Learning (RL) suffers from inefficient exploration and slow convergence. To address these limitations, we propose STAR-R1, a novel framework that integrates a single-stage RL paradigm with a fine-grained reward mechanism tailored for TVR. Specifically, STAR-R1 rewards partial correctness while penalizing excessive enumeration and passive inaction, enabling efficient exploration and precise reasoning. Comprehensive evaluations demonstrate that STAR-R1 achieves state-of-the-art performance across all 11 metrics, outperforming SFT by 23% in cross-view scenarios. Further analysis reveals STAR-R1's anthropomorphic behavior and highlights its unique ability to compare all objects for improving spatial reasoning. Our work provides critical insights in advancing the research of MLLMs and reasoning models. The codes, model weights, and data will be publicly available at https://github.com/zongzhao23/STAR-R1."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 2 4 0 8 5 1 . 5 0 5 2 : r STAR-R1: Spatial TrAnsformation Reasoning by"
        },
        {
            "title": "Reinforcing Multimodal LLMs",
            "content": "Zongzhao Li1 Zongyang Ma2 Mingze Li1 Songyou Li1 Yu Rong3,4 Tingyang Xu3,4 Ziqi Zhang2 Deli Zhao3,4 Wenbing Huang1 1Gaoling School of Artificial Intelligence, Renmin University of China 2MAIS, Institute of Automation, Chinese Academy of Sciences 3DAMO Academy, Alibaba Group, Hangzhou, China 4Hupan Lab, Hangzhou, China lizongzhao2023@ruc.edu.cn, mazongyang2020@ia.ac.cn hwenbing@126.com"
        },
        {
            "title": "Abstract",
            "content": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities across diverse tasks, yet they lag significantly behind humans in spatial reasoning. We investigate this gap through Transformation-Driven Visual Reasoning (TVR), challenging task requiring identification of object transformations across images under varying viewpoints. While traditional Supervised Fine-Tuning (SFT) fails to generate coherent reasoning paths in cross-view settings, sparse-reward Reinforcement Learning (RL) suffers from inefficient exploration and slow convergence. To address these limitations, we propose STAR-R1, novel framework that integrates single-stage RL paradigm with fine-grained reward mechanism tailored for TVR. Specifically, STAR-R1 rewards partial correctness while penalizing excessive enumeration and passive inaction, enabling efficient exploration and precise reasoning. Comprehensive evaluations demonstrate that STAR-R1 achieves state-of-the-art performance across all 11 metrics, outperforming SFT by 23% in cross-view scenarios. Further analysis reveals STAR-R1s anthropomorphic behavior and highlights its unique ability to compare all objects for improving spatial reasoning. Our work provides critical insights in advancing the research of MLLMs and reasoning models. The codes, model weights, and data will be publicly available at https://github.com/zongzhao23/STAR-R1."
        },
        {
            "title": "Introduction",
            "content": "Multimodal Large Language Models (MLLMs) [4, 46, 5, 11, 10, 43, 9, 55, 2, 39, 48, 8] have demonstrated remarkable progress in recent years, excelling at tasks such as visual question answering [15, 58, 59], text-to-image generation [16, 22], and video understanding [27, 61, 47, 14, 7]. Despite these advances, existing MLLMs still struggle with spatial reasoning [33, 41]a fundamental aspect of human intelligence. Given this critical gap in emulating human-like spatial cognition, it becomes imperative to develop targeted framework for assessing such aspect. To this end, this paper takes the challenging task of Transformation Driven Visual Reasoning (TVR) [21] as starting point to investigate MLLMs capabilities in spatial understanding. By comparing two given images containing multiple objects, TVR involves series of continuous thinking and reasoning steps to derive the correct answer, including identifying which object is transformed and what transformation is applied to this object. Moreover, objects in the two images may be presented in different viewpoints, which further complicates the task and poses difficulties for existing MLLMs. Our experiments show Equal contribution. Corresponding author. Preprint. Under review. Figure 1: Comparison of addressing the TVR task by STAR-SFT and STAR-R1. STAR-SFT finetunes MLLMs with supervised instructions, while STAR-R1 employs RL-guided thinking. that, the powerful closed-source commercial model GPT-4o [23] achieves only 23.5% accuracy on this task, further demonstrating that this problem poses significant challenge to existing MLLMs. While Supervised Fine-Tuning (SFT) can improve MLLMs performance on TVR, this approach remains insufficient. Since successfully solving TVR requires step-by-step Chain-of-Thought (CoT) [49] reasoning, merely replicating annotations of final object and attribute changes fails to yield robust performance, particularly in view-shifting scenarios. Recently, training LLM by Reinforcement Learning (RL), exemplified by DeepSeek-R1-Zero [18], has demonstrated significant advantages in generating the correct CoT, offering promising solution for the TVR task. However, the naive GRPO method [37] in DeepSeek-R1-Zero, designed for tasks like Mathematical Problem Solving (MPS) [12, 20, 19, 60, 50], uses sparse reward based on the binary verification signal. When directly adapted to the TVR task, it results in low exploration efficiency and slow convergence. In the MPS task, the final answer, such as the option of the multiple-choice question or the value of the fill-in-the-blank question, has only two possibilities: correct or incorrect, without intermediate state of partial correctness. In contrast, the final answer in the TVR task is sequence composed of multiple objects with multiple transformations. During early training, the model often produces partially correct responses; for instance, it may correctly identify the object whose attribute has changed but fail to specify the exact transformation applied. Under binary reward setting, the model rarely receives positive feedback, hindering its ability to determine effective exploration directions. To address these challenges, we propose to integrate dense reward plus punishment into GRPO, thereby constructing new method dubbed Spatial TrAnsformation Reasoning with R1 (STAR-R1). STAR-R1 is built upon two core designs. On the one hand, it introduces dense rewards by accounting for partial answer correctness, progressively assigning fine-grained rewards to three-level answers. Specifically, for the first-level answer, the model identifies the objects with modified attributes, receiving minimal reward. For the second-level answer, it correctly predicts the changed attributes, earning medium reward. For the third-level answer, the model accurately identifies the full objectattribute-transformation triplet, receiving the highest reward. On the other hand, STAR-R1 penalizes the model for either excessively enumerating object-attribute-change triplets to maximize rewards or remaining inactive to avoid incorrect predictions, ensuring active exploration without shortcuts. This mechanism design instills into the model the learning philosophy that prioritizes attempting potentially incorrect answers over overlooking possible correct ones. Through rigorous data filtering and sampling pipeline, we select training/test samples from the original TRANCE dataset [21]. To comprehensively evaluate existing MLLMs on the TVR task, our test set includes samples whose initial and final images are captured either from the same view (In-Domain) or from more challenging different views (Out-of-Domain), totaling 4.5K samples. Extensive evaluations on both closed-source commercial and open-source MLLMs demonstrate 2 that the proposed STAR-R1 not only significantly outperforms other models across all types of attribute transformations, but also exceeds the performance of SFT methods under the same training settings. Notably, in Out-of-Domain (OOD) tasks, STAR-R1 achieves substantial 23% performance advantage over SFT-based approaches. Furthermore, through in-depth analysis of experimental results, we reveal STAR-R1s anthropomorphic behavior and identify the core reason for RLs superiority: it majorly compares all objects between the initial and final scenes, thereby improving reasoning in OOD scenarios. We also observe an intriguing phenomenon where the models response length initially decreases and then increases during training, indicating the specificity of the TVR task compared to other common reasoning tasks. Our contributions are summarized as follows: We propose STAR-R1, multimodal reasoning model trained via single-stage pure RL paradigm akin to DeepSeek-R1-Zero. By integrating customized reward function for RL optimization, STAR-R1 exhibits superior reasoning capabilities in spatial understanding. In the TVR task, STAR-R1 achieves the best performance across all eleven evaluation metrics. Moreover, in the cross-view OOD task setting, it delivers 23% TAcc gain over SFT, validating the R1-Zero paradigms potential in unlocking reasoning capabilities of the TVR task. Through in-depth analysis of experimental results, we uncover STAR-R1s anthropomorphic behavior and elucidate the mechanistic advantages of RL over SFT. We also systematically investigate the dynamic changes in response length. These findings provide unique insights for advancing MLLMs in spatial reasoning."
        },
        {
            "title": "2 Related Work",
            "content": "Multimodal Large Language Model. MLLMs have made rapid advancements recently, achieving significant improvements in various image [15, 32] and video understanding [27, 31] tasks. The early LLaVA series models [29, 28, 26, 25] are typical representatives of MLLMs, which use simple MLP as the projectioner to map the CLIP-encoded [36, 40] visual features into the semantic space of Large Language Models (LLMs) [44, 45, 17, 3, 54], enabling the visual-language joint understanding. This architecture is subsequently widely adopted by more powerful models such as the QwenVL series [4, 46, 5] and InternVL series [11, 10, 43], significantly narrowing the performance gap between open-source MLLMs and commercial-grade closed-source benchmark models like GPT-4o [23] and Gemini series [42]. Despite these advancements, current MLLMs still struggle with complex reasoning tasks, especially those involving spatial reasoning [33, 41]. To this end, we treat the TVR task as pivotal point, concentrating on exploring effective methods for enhancing the spatial reasoning capabilities of MLLMs. Multimodal Reasoning Model. Research on the reasoning capabilities of MLLMs has attracted significant attention in recent years. Early studies primarily employed Chain-of-Thought (CoT) finetuning to enhance model reasoning abilities [52, 53], while the recent success of DeepSeek-R1 [18] has promoted the application of rule-based Reinforcement Learning in this field [6, 38]. Notably, MMEureka [34] adopts an online filtering strategy and two-stage multimodal RL training framework to achieve significant improvements in mathematical reasoning tasks; LMM-R1 [35], which enhances multimodal reasoning capabilities through two-phase training paradigm combining text-only RL and multimodal RL; and Video-R1 [13], which achieves breakthroughs in video reasoning tasks via twostage approach involving supervised fine-tuning (SFT) followed by RL training. However, existing methods exhibit several limitations: (1) most rely on naive GRPO strategies, (2) they predominantly adopt two-stage training paradigms, and (3) there is lack of task-specific customization and in-depth exploration of pure RL training (e.g., like DeepSeek-R1-Zero). To address these gaps, we introduces tailored rule-based RL strategy for the TVR task and proposes single-stage, pure RL training paradigm. Our approach not only significantly enhances multimodal reasoning performance but also provides novel theoretical insights into RL training mechanisms."
        },
        {
            "title": "3 Our Method: STAR-R1",
            "content": "3.1 Task Formulation As illustrated in Fig. 2, the TVR task is formally defined in this way: given an initial image initial, final image final, and question Q, the model ϕ is required to generate sequence of transformations 3 Figure 2: The STAR-R1 Framework. Given initial/final images and question, STAR-R1 establishes object correspondences by systematically comparing all objects across both images. It generates Stepby-step reasoning in <think></think> tags before delivering the final answer in <answer></answer> tags. We demonstrate the reward function of STAR-R1 using partially correct case study. Correct/incorrect answers are shown in blue and red respectively. as the output answer. Formally, this generation process can be expressed as: = ϕ(I initial, final, Q). (1) Here, = {t1, t2, . . . , tn} denotes sequence of transformations. To avoid the enormous description space introduced by pixel-level transformations, we adopt the methodology from the previous work [21], defining single-attribute transformation of single object as an atomic operation. In form, ti = (indexi, attributei, valuei), which indicates modifying attributei to valuei, of the object with indexi. Given the multi-step nature of transformations, MLLMs must meticulously analyze fine-grained differences between the initial and final images and perform step-by-step reasoning to derive the correct transformation sequence. 3.2 The Design of Reward Function Reward function, serves as direct feedback from the environment and plays pivotal role in reinforcement learning. sparse reward space can lead to inefficient exploration by the model, often requiring extended training periods to discover optimal decision paths. An excessively dense reward, on the contrary, may induce the model to adopt shortcut strategies for reward hacking. To facilitate active and efficient exploration of the solution space, our STAR-R1 integrates dense reward and punishment mechanisms into the original GRPO framework [37]. Particularly, we incorporate not only format rewards for structural constraints but also dense reward and punishment mechanisms to provide precise reward signals. The design principles of these two reward components are as follows. Format Reward The format reward enforces structural constraints on model outputs, mandating that reasoning processes should appear within <think></think> tags while final answers must be enclosed in <answer></answer> tags. Compliance of this formate yields reward Rformat of 1, whereas violations result in 0. Accuracy Reward In existing studies, Reinforcement Learning with Verifiable Rewards (RLVR) primarily employs binary verification signals (correct or incorrect), which can be easily computed by simple rule-based verifiers. This paradigm has been validated in multiple studies and has demonstrated promising results across various scenarios [51, 34]. However, in the TVR task, directly applying binary verification signalsthe model only receives reward if all transformations are correct would lead to inefficient exploration and suboptimal model performance. Therefore, our model STAR-R1 first refines the reward granularity to each individual transformation and then decomposes the reward into each component of the transformation. Additionally, it carries out punishment mechanisms to guide the model toward correct and proactive exploration, ensuring both efficient exploration and high prediction accuracy. 4 For any transformation ti = (indexi, attributei, valuei) in the models output set = {t1, t2, . . . , tn}, if its object index, attribute, and value exactly match corresponding ˆti in the ground-truth transformations ˆT = {ˆt1, ˆt2, . . . , ˆtn}, the model is deemed to have perfectly inferred the transformation and thus receives the highest reward (reward = 5.0). If only the object index and attribute match but the value is incorrect, the model demonstrates accurate identification and association capabilities, which are critical for forming correct reasoning chains; hence, partial reward (reward = 1.5) is granted. match solely in the object index reflects preliminary identification ability, and minimal reward (reward = 0.5) is provided to encourage iterative model improvement. The positive reward described above can be formally defined as: Rpos(ti) = 5.0, 1.5, 0.5, if all indexi, attributei, and valuei match; if indexi and attributei match; if only indexi matches. (2) i=1 Rpos(ti). By accumulating the rewards of all tis, we obtain Rpos = (cid:80)n Building upon the positive reward design, we further introduce dual punishment mechanism to effectively constrain the models exploratory behavior. Relying solely on positive rewards may lead the model to engage in reward hacking by exhaustively enumerating all possible (indexi, attributei, valuei) triplets. To mitigate this, after positive reward calculation, we further examine the predictions to verify whether the attribute transformations are inconsistent with the final image state. Each mistaken prediction incurs punishment (reward = 1.0), yielding total reward as nmiss for nmiss mistaken predictions. Furthermore, to encourage comprehensive exploration of all possible transformations, the system will impose punishment (reward = (ˆn n)), when the number of predicted transformations is less than the ground truth ˆn. This mechanism teaches the model to prioritize attempting potentially correct answers over avoiding possible errors, thereby promoting continuous experimentation, in-depth analysis, and precise evaluation of each objects attribute changes, ultimately leading to steady improvements in accuracy. The punishment reward described above can be formally defined as: Rpun = (cid:26)nmis (ˆn n), nmis, if < ˆn; otherwise. (3) Finally, we sum Rpos and Rpun to obtain Racc = Rpos + Rpun. 3.3 The Learning Algorithm We employ GRPO as the reinforcement learning algorithm for STAR-R1, since GRPO samples and scores group of responses based on the reward function, then calculates each responses relative advantage within the group. Compared to traditional Proximal Policy Optimization (PPO), GRPO eliminates the need for training an additional critic model by estimating the baseline from group scores, thereby significantly improving training efficiency. Formally, given an initial image initial, final image final, and question description Q, the model first generates group of responses {T1, T2, . . . , TG}. STAR-R1 then computes the format reward and accuracy reward Racc Rformat for each response, and sums them to obtain set of rewards . The advantage ˆAg for each response is then {R1, R2, . . . , RG}, where Rg = Rformat calculated by normalizing the rewards with respect to the group statistics: ˆAg = RgµG , where µG and σG are the mean and the standard deviation of the rewards in the group. + Racc σG Finally, based on the computed advantages and KL penalty term that constrains the divergence between the trained policy and the reference policy, we obtain the following training objective, which is maximized to optimize the policy model: JGRPO(θ) = qP (Q),{Tg}G g=1πθold (T q) 1 (cid:88) g=1 1 oi (cid:104) min (cid:16) Rg ˆAg, clip (Rg, 1 ϵ, 1 + ϵ) ˆAg (cid:17) (cid:105) βDKL[πθπref ] , (4) 5 where rg = πθ(Tgq) πθold (Tgq) is the probability ratio for observation Tg under the current policy πθ relative to the old policy πθold , πref denotes the reference policy, and β is hyperparameter controlling the strength of the KL penalty."
        },
        {
            "title": "4 Experiments",
            "content": "Dataset. As challenging multimodal reasoning task, TVR requires the model to infer and describe single-step or multi-step transformations (i.e., changes in object attributes) between given initial and final images. To better reflect real-world scenarios, in addition to simpler tasks where both initial and final images share the same central viewpoint, TVR also includes more challenging cases where the model must match corresponding objects across different viewpoints (e.g., initial image from the central view while final image from the left/right view) and identify the transformations. After cleaning and filtering the original TRANCE dataset [21], we randomly sample 9k training examples with both initial and final images from the central view. From the remaining data, we randomly sample 4.5k test examples comprising initial images from the central view, paired with final images from left, center, and right views. We provide further details in Appendix A. Implementations and Baselines. We employ Qwen2.5-VL-7B as the base model for our main experiments, and conduct all experiments on single computation node equipped with 8H20 GPUs. Unlike existing approaches [13, 34] that require multi-stage training, our STAR-R1 demonstrates excellent reasoning capability through direct RL training without any supervised fine-tuning (SFT) as prerequisite. Due to current computational constraints, all RL experiments in this paper (unless otherwise specified) are trained for 2 epochs. To comprehensively evaluate the reasoning capabilities of STAR-R1, we conduct extensive comparisons with numerous existing models, including both closed-source (e.g., Qwen-VL [4, 46, 5], InterVL [11, 10, 43]) and open-source (e.g., GPT-4o [23], Gemini[42]) alternatives. To obtain comprehensive, intuitive, and in-depth understanding of the models reasoning capabilities, we employ multiple evaluation metrics. We first execute all transformations from the models response on the initial image state, obtaining the predicted final state Sfinal. By comparing this with the ground truth final state ˆSfinal, we compute the following two categories of metrics: Sample-level Metrics. (1) Attribute Accuracy: measuring whether each attribute (color, shape, size, material) of all objects is predicted correctly. (2) Total Accuracy (TAcc): measuring whether all four attributes of all transformations are predicted correctly. (3) Attribute Difference (Diff): counting the number of differing attributes between Sfinal and ˆSfinal. (4) Normalized Difference (Norm Diff): dividing Diff by the total number of transformations, eliminating scale effect. Population-level Metrics. To evaluate how object number influences complexity and challenge, we categorize samples into four groups based on object count: {1-3, 4-6, 7-8, 9-10}. For each group, we calculate Object Total Accuracy: the total accuracy within each category. 4.1 Main results As shown in Table 1, our model STAR-R1 achieves state-of-the-art performance across all eleven evaluation metrics when compared with advanced MLLMs. This demonstrates both the exceptional capability of STAR-R1 and the remarkable potential of the reinforcement learning paradigm for visual reasoning tasks. Specifically, we have the following observations: (1) For all metrics, STAR-R1 shows significant improvement over the base model Qwen-2.5-VL-7B. It substantially outperforms not only InternVL2 with comparable model scale, but also larger-scale models like QWen-2.5-VL-7B. Remarkably, in terms of TAcc, STAR-R1 surpasses the closed-source commercial models GPT-4o and Gemini-1.5-pro by 37.9% and 45.5%, respectively. (2) Compared to the SFT paradigm, our pure RL approach without any cold-start data achieves approximately 13% performance gain of TAcc, providing strong evidence that RL can significantly enhance model capabilities for complex visual reasoning scenarios. (3) For the four Attribute Accuracy metrics, we observe comparable performance across all attributes. This indicates robust stability in our learning framework without exhibiting catastrophic forgetting or attribute dominance. (4) For the four population-level metrics, model performance shows steep decline as the number of objects increases. This trend reflects the compounding challenges in complex scenes, where difficulties accumulate across the entire reasoning pipeline from object identification to object association and finally to change perception. 6 Table 1: Results on TVR task. Bold indicates the best results. Model Param TAcc Diff NDiff Attribute Accuracy Transformation Number Color Shape Size Metarial Num3 Num6 Num8 Num10 GPT-4o [23] Gemini-2.0-flash [42] Gemini-1.5-Pro [42] Deepseek-VL [30] Phi3.5V [1] mPLUG-owl3 [57] MiniCPM-V2.6 [56] LLaVA-OneVision [25] Pixtral [2] InternVL2.5 [10] InternVL3 [62] Qwen2-VL [46] Qwen2-VL [46] Qwen2.5-VL [5] Qwen2.5-VL [5] Qwen2.5-VL [5] STAR-SFT STAR-R1 - - - 7B 7B 7B 7B 7B 12B 8B 7B 2B 7B 3B 7B 32B 7B 7B 23.5 20.9 15.9 0.9 0.9 1.0 1.0 1.1 3.0 3.3 3.7 0.3 1.2 2.3 3.8 9. 48.7 61.4 1.96 1.97 2.46 5.02 4.91 4.82 4.78 4.67 3.02 2.80 2.62 5.23 4.11 3.53 3.41 3.21 1.17 0.77 0.76 0.79 1.00 2.23 2.15 2.14 2.11 2.06 1.44 1.14 1.08 2.40 1.84 1.51 1.46 1. 0.57 0.31 Closed-Source Models 61.7 63.3 54.4 69.3 73.3 72.2 Open-Source Models 18.8 20.1 19.6 22.5 22.4 30.3 33.6 35.3 14.9 23.8 25.6 26.2 41. 75.5 81.3 38.6 37.9 39.1 40.2 43.3 53.6 59.3 58.2 45.0 45.6 47.2 56.5 57.3 72.1 83.2 62.2 59.8 54.6 36.4 39.2 40.3 41.3 42.5 54.7 56.0 58.5 48.4 45.3 52.2 52.8 50.6 80.2 86. 61.0 58.0 48.9 33.2 41.1 40.7 38.3 40.6 52.2 51.0 54.7 54.1 42.0 44.5 45.9 36.6 79.3 85.5 49.3 42.4 37.4 4.1 4.8 5.1 5.4 5.8 10.8 11.2 13.3 1.0 6.2 8.8 14.5 27.5 82.4 91. 27.3 24.5 18.5 14.6 16.8 8.6 0.1 0.2 0.1 0.1 0.1 1.9 2.7 2.6 0.3 0.1 1.5 2.5 9.8 0.1 0.1 0.1 0.1 0.1 0.7 0.9 0.9 0.0 0.1 0.5 0.9 2.5 9.6 6.0 4.7 0.0 0.0 0.0 0.1 0.0 0.3 0.4 0.4 0.0 0.1 0.3 0.3 1. 53.3 70.7 37.9 54.2 30.0 37.5 4.2 Anthropomorphic Behavior: RL VS SFT In this section, we conduct thorough comparison between SFT and RL training paradigms by evaluating their performance on both In-Domain (ID) and Out-Of-Domain (OOD) test samples, where ID samples contain initial and final images from the same center viewpoint while OOD samples contain images from different viewpoints (center-to-left/right). The OOD setting presents greater challenge, as models must distinguish genuine object transformations from viewpoint-induced variations. As shown in Table 2, SFT achieves better accuracy on ID data, but RL demonstrates remarkable superiority in the OOD setting, outperforming SFT by 23%, highlighting its exceptional generalization capability for reasoning in unseen views. ID OOD Method 0.83 0.39 0.08 0.14 30.9 1.65 53.9 0.96 TAcc Diff NDiff TAcc Diff NDiff STAR-SFT 84.2 0.22 76.3 0.38 STAR-R1 Table 2: Results in In-Domain (ID) and Out-OfDomain (OOD) scenarios. Further case studies reveal that RL models superior performance in the OOD scenario stems fundamentally from its capability to systematically compare each object between initial and final images. As illustrated in Fig. 3, the STAR-SFT model typically performs cursory comparisons of only few objects. This approach suffices for ID scenarios with consistent viewpoints, where differences are often immediately apparent. However, in OOD scenarios, viewpoint transitions necessitate cross-verification through multiple object comparisons to reliably identify transformations, mirroring human reasoning processes. For example, STAR-SFT misidentifies objects 1, 4, and 3 in the final image as objects 4, 2, and 1, respectively. This suggests that the model fails to effectively account for viewpoint-induced object displacement, erroneously retaining the object IDs from the initial viewpoint in the transformed viewpoint, thus leading to incorrect object correspondence and significant degradation in prediction accuracy. In contrast, STAR-R1 meticulously examine all objects, including untransformed ones. In multi-object scenes, this comprehensive comparison enables precise crossview mapping by referring to untransformed objects, thereby preventing performance degradation from mismatches and significantly improving robustness to viewpoint changes. Importantly, during the whole process, our prompts do not include any information related to view invariance/variation. Quantitatively, while processing consistent-viewpoint scenes (ID), RL model performs full object comparisons in 67% of cases, but this ratio increases to 81% for viewpoint-shifting scenarios (OOD). This phenomenon indicates that the model, like humans, adapts its verification intensity based on situational complexity, performing more exhaustive comparisons when viewpoint changes demand higher confidence in object correspondences. Additional case studies are provided in Appendix for further reference. Furthermore, we apply RL training to the STAR-SFT model and observe series of intriguing experimental results and phenomena, with the findings presented in Appendix C. 7 Figure 3: case study comparing the reasoning processes of STAR-SFT and STAR-R1. STAR-SFT performs only cursory comparisons, failing to detect view changes, which results in incorrect object matching and answers. In contrast, STAR-R1 systematically compares all objects step-by-step, identifies the view shift, and accurately tracks object correspondences, producing correct results. Blue indicates correct answers while red denotes incorrect ones. 4.3 Training Curve of Response Length We analyze the training curve of the response length to further reveal the underlying insights. For additional details regarding the training curves, please refer to Appendix B. In Fig. 4, the models response length initially undergoes rapid decline, followed by gradual increase before eventually stabilizing. We observe that during the early phase, up until the point where the response length reaches its minimum, the model actively explores various reasoning strategies while progressively refining its language. For instance, the models output transitions from verbose and multi-object descriptions, such as: \"The object with index 3 in the first image is gray cylinder of size large. In the second image, it has changed to metallic gold. This suggests transformation in color and material,\" to more concise and single-object reasoning, such as: \"The small brown sphere changed its color to gray.\" At this stage, however, the model realizes that focusing on only few objects may lead to incorrect mappings between objects across the two images or the omission of critical information, leaving room for further improvement in prediction accuracy. Consequently, while maintaining its refined reasoning style for individual objects, the model gradually explores new and stable reasoning path, systematically comparing the states of all objects between the two images. This approach ultimately achieves high accuracy while keeping the reasoning length at an appropriate level. Figure 4: Dynamics of model response length. Zoom in to examine specific examples. 4.4 Ablation Studies In this section, to comprehensively investigate the impact of various factors on model performance, we conduct ablation studies on each component of the reward function, the volume of training data, the maximum response length limit, and the model size. 8 Table 3: Ablation studies on reward design. Transformation Number Component TAcc Diff NDiff Table 4: Ablation studies on data volume. Num3 Num6 Num8 Num10 Volume TAcc Diff NDiff Transformation Number Num3 Num6 Num8 Num10 58.0 0.90 w/o obj 56.8 0.96 w/o attr 58.2 0.97 w/o up w/o pun 54.3 1.05 naive grpo 54.5 1.02 61.4 0.77 STAR-R1 0.37 0.40 0.41 0.44 0.43 0.31 89.8 87.4 90.2 85.5 85.2 91.0 66.5 64.6 68.5 61.9 63.7 70.7 49.5 49.3 49.6 46.8 46.1 54. 34.6 34.1 33.0 31.4 31.1 37.5 1,000 3,000 5,000 7,000 9,000 13.8 2.32 23.7 1.97 40.7 1.49 52.6 1.08 61.4 0.77 0.95 0.88 0.65 0.46 0.31 29.0 49.0 75.1 82.7 91.0 16.2 25.3 47.0 59.6 70. 9.0 17.2 30.1 45.0 54.2 4.9 10.3 19.5 31.2 37.5 Reward Design We conducted systematic ablation studies on all components of the reward function in STAR-R1 to evaluate their contributions to model performance, with experimental results presented in Table 3. Our key observations are: (1) The models performance degrades when either the object reward (Row 1) or attribute reward (Row 2) is removed, demonstrating that dense rewards facilitate more efficient exploration and improve answer accuracy. (2) Removing the punishment constraint for under-prediction of transformations (when Npred < Ngt) leads to significant accuracy deterioration (Row 3). This reveals that the punishment mechanism, which encourages exploration and prevents model inaction due to risk aversion, enables the model to search for better reasoning paths. (3) Eliminating the wrong-answer punishment causes the model to reward hacking by exhaustively enumerating all possible (object, attribute, value) triplets, leading to meaningless responses. To further validate the importance of the punishment design, we replace all punishment constraints with unified negative absolute difference between predicted and ground-truth transformation counts: (n ˆn). As shown in Row 4, this yields markedly worse performance, confirming that timely punishment for incorrect answers is crucial for correcting the models exploration path. We additionally conduct experiments with Naive GRPO (Row 5), which demonstrates limited suitability for TVR tasks, yielding merely subpar results. Training Data To evaluate the effect of training sizes, we randomly sample four subsets for training from the whole training set, containing 1000, 3000, 5000, and 7000 samples, respectively, and evaluate them on the original test set. The experimental results shown in Table 4 demonstrate that as the volume of training data increases, model performance improves continuously, but the growth rate of accuracy exhibits an initial increase followed by subsequent decline. This suggests that insufficient data in the early stages constrains model learning, while expanded datasets facilitate phase of rapid knowledge acquisition, leading to significantly accelerated performance gains. Method Table 5: Ablation studies on model size. Model Size To assess the influence of model size, we perform experiments by replacing the base model with Qwen-2.5VL-3B. The experimental results are presented in Table 5. It is evident that the 3B model also exhibits significant improvement in prediction accuracy after RL training, demonstrating the effectiveness of our method. However, under the same training settings, the performance gain of the 3B model is considerably smaller than that of the 7B model. This suggests that the capability of the base model plays crucial role in the post-training phase, determining the models ability to learn and adapt to new knowledge and scenarios. stronger base model can achieve higher upper bound of reasoning performance. Qwen2.5-VL-7B 3.8 3.41 1.46 61.4 0.77 0.31 STAR-R1-7B Qwen2.5-VL-3B 2.3 3.53 1.51 28.6 1.84 0.79 STAR-R1-3B Num3 Num6 Num8 Num10 Transformation Number TAcc Diff NDiff 8.8 55.2 0.3 13.5 1.5 32.4 0.5 20.4 14.5 91.0 0.3 37. 2.5 70.7 0.9 54."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we present STAR-R1, pure RL-trained model equipped with our new reward design for the challenging spatial reasoning taskTVR. By integrating dense rewards for partial correctness with punishment rewards for penalizing shortcuts, STAR-R1 enables robust learning, achieving superior performance across all eleven evaluation metrics. Results reveal mechanistic advantage of RL over SFT stems from comprehensively comparing objects in the scene, especially in cross-view out-of-domain scenarios. Further analysis uncovers STAR-R1s anthropomorphic behavior and non-linear training dynamic of response length, offering insights into RLs capability in complex 9 visual reasoning. This work validates the potential of the R1-Zero paradigm for advanced reasoning tasks. Our findings may pave the way for future research in enhancing MLLMs spatial cognition, emphasizing RLs capability to unlock human-like reasoning through structured exploration and fine-grained feedback."
        },
        {
            "title": "References",
            "content": "[1] Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. [2] Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Baptiste Bout, Devendra Chaplot, Jessica Chudnovsky, Diogo Costa, Baudouin De Monicault, Saurabh Garg, Theophile Gervet, et al. Pixtral 12b. arXiv preprint arXiv:2410.07073, 2024. [3] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. [4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 1(2):3, 2023. [5] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [6] Liang Chen, Lei Li, Haozhe Zhao, Yifan Song, and Vinci. R1-v: Reinforcing super generalization ability in vision-language models with less than $3. https://github.com/Deep-Agent/ R1-V, 2025. Accessed: 2025-02-02. [7] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024. [8] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. [9] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. [10] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. Science China Information Sciences, 67(12):220101, 2024. [11] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2418524198, 2024. [12] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [13] Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025. 10 [14] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. [15] Chaoyou Fu, Yi-Fan Zhang, Shukang Yin, Bo Li, Xinyu Fang, Sirui Zhao, Haodong Duan, Xing Sun, Ziwei Liu, Liang Wang, et al. Mme-survey: comprehensive survey on evaluation of multimodal llms. arXiv preprint arXiv:2411.15296, 2024. [16] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36:5213252152, 2023. [17] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [18] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [19] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. [20] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. [21] Xin Hong, Yanyan Lan, Liang Pang, Jiafeng Guo, and Xueqi Cheng. Transformation driven visual reasoning. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, pages 69036912, 2021. [22] Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. Ella: Equip diffusion models with llm for enhanced semantic alignment. arXiv preprint arXiv:2403.05135, 2024. [23] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [24] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [25] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [26] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024. [27] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2219522206, 2024. [28] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. [29] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. 11 [30] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, et al. Deepseek-vl: towards real-world vision-language understanding. arXiv preprint arXiv:2403.05525, 2024. [31] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very long-form video language understanding. Advances in Neural Information Processing Systems, 36:4621246244, 2023. [32] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. [33] Julius Mayer, Mohamad Ballout, Serwan Jassim, Farbod Nosrat Nezami, and Elia Bruni. ivispar an interactive visual-spatial reasoning benchmark for vlms. arXiv preprint arXiv:2502.03214, 2025. [34] Meng, Du, Liu, Zhou, Lu, Fu, Shi, Wang, He, Zhang, et al. Mm-eureka: Exploring visual aha moment with rule-based large-scale reinforcement learning. arXiv preprint arXiv:2503.07365, 2025. [35] Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, and Xu Yang. Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl. arXiv preprint arXiv:2503.07536, 2025. [36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [37] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [38] Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. [39] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. 2023. [40] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques for clip at scale. arXiv preprint arXiv:2303.15389, 2023. [41] Kexian Tang, Junyao Gao, Yanhong Zeng, Haodong Duan, Yanan Sun, Zhening Xing, Wenran Liu, Kaifeng Lyu, and Kai Chen. Lego-puzzles: How good are mllms at multi-step spatial reasoning? arXiv preprint arXiv:2503.19990, 2025. [42] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [43] OpenGVLab Team. Internvl2: Better than the bestexpanding performance boundaries of open-source multimodal models with the progressive scaling strategy, 2024. [44] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [45] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 12 [46] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [47] Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Xiaotao Gu, Shiyu Huang, Bin Xu, Yuxiao Dong, et al. Lvbench: An extreme long video understanding benchmark. arXiv preprint arXiv:2406.08035, 2024. [48] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. [49] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [50] Tianwen Wei, Jian Luan, Wei Liu, Shuang Dong, and Bin Wang. Cmath: Can your language model pass chinese elementary school math test? arXiv preprint arXiv:2306.16636, 2023. [51] Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2502.14768, 2025. [52] Guowei Xu, Peng Jin, Li Hao, Yibing Song, Lichao Sun, and Li Yuan. Llava-o1: Let vision language models reason step-by-step. arXiv preprint arXiv:2411.10440, 2024. [53] Haotian Xu, Xing Wu, Weinong Wang, Zhongzhi Li, Da Zheng, Boyuan Chen, Yi Hu, Shijia Kang, Jiaming Ji, Yingying Zhang, et al. Redstar: Does scaling long-cot data unlock better slow-reasoning systems? arXiv preprint arXiv:2501.11284, 2025. [54] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [55] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. [56] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. [57] Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl3: Towards long image-sequence understanding in multi-modal large language models. arXiv preprint arXiv:2408.04840, 2024. [58] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. [59] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal In Proceedings of the IEEE/CVF understanding and reasoning benchmark for expert agi. Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. [60] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: human-centric benchmark for evaluating foundation models. arXiv preprint arXiv:2304.06364, 2023. [61] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264, 2024. [62] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025."
        },
        {
            "title": "A Dataset and Experimental Setup",
            "content": "We first clean the TRANCE dataset [21] by removing samples containing redundant transformations. Then, we randomly sample 9,000 and 4,500 samples as the training set and test set, respectively, ensuring that the number of samples corresponding to each transformation length (ranging from 1 to 4) is equally distributed in both the training and test sets. We employ Qwen2.5-VL-7B as our base model and utilize vLLM [24] as the training framework. All experiments (including RL and SFT) are conducted on single node equipped with 8 H20 GPUs. Except for the training strategy mentioned in Appendix C, all other training runs adopt single-stage procedure with 2 epochs, batch size of 1 per GPU, and gradient accumulation over 2 steps, yielding total of 1,286 training steps."
        },
        {
            "title": "B Training Curves",
            "content": "Figure 5: Training curves about accuracy reward and format reward. We illustrate the evolution of Total Accuracy for STAR-RL and STAR-SFT models on OOD test data across training steps. The training dynamics plotted in Fig. 7 further show that SFT quickly plateaus around 1,200 steps on OOD data and eventually degrades with further training, whereas RL maintains steady performance growth throughout the training process, suggesting its unique ability to progressively unlock the models reasoning capabilities for reasoning in complex scenarios. Additionally, we plot further curves to analyze the training dynamics of STAR-R1. As shown in the right panel of Fig. 5, we present the format reward curve of the STAR-R1. It can be observed that for models with strong foundational capabilities, such as Qwen-2.5-VL-7B, they already exhibit excellent instruction-following performance in the TVR task from the beginning, with most responses adhering to the required format. Thus, no additional customization for format reward is necessary. Moreover, in the left panel of Fig. 5, we illustrate the accuracy reward curve. The models accuracy reward steadily increases with training progression, ultimately stabilizing at high value. This indicates that our designed reward function effectively encourages the model to engage in accurate, proactive, and efficient exploration, progressively unlocking the multimodal reasoning capabilities of the base model. Figure 6: Dynamics of the total accuracy as well as the attribute accuracy for the four types throughout the training process. 14 As shown in Fig. 6, these five curves illustrate the dynamics of the Total Accuracy as well as the Attribute Accuracy for the four types throughout the training process. All accuracy curves exhibit steady increase during training and eventually stabilize at high level of prediction accuracy. This demonstrates that our STAR-R1 effectively guides the model to efficiently explore the solution space and gradually develop stronger reasoning capabilities. Table 6: Comparison of RL vs. SFT performance on ID and OOD settings. ID OOD Method TAcc Diff NDiff TAcc Diff NDiff STAR-SFT 84.2 0.22 STAR-SFT&RL 87.7 0.19 76.3 0.38 STAR-R 0.08 0.07 0.14 30.9 1.65 36.5 1.53 53.9 0.96 0.83 0.71 0.39 Figure 7: Accuracy curves of RL and SFT. Training STAR-SFT with RL We additionally apply RL training to the SFT model. In Row 2 of Table 6, results demonstrate that further training the model with RL not only achieves continuous improvement in ID performance but, more importantly, exhibits significant progress on OOD tasks. This indicates that RL successfully breaks through the performance bottleneck encountered during SFT training on OOD data. Furthermore, we observed that the two-phase training approach of STAR-SFT&RL do not ultimately achieve the same performance on OOD data as pure RL training. We hypothesize that this may be due to imitation learning in the SFT phase locking the models reasoning patterns, such that the subsequent RL training phase using the same amount of data can only marginally adjust the models reasoning. This adjustment leads the model to attempt comparing more objects. To validate this conjecture, we further analyze the difference between the number of objects compared in each response and the total number of objects in the scene. The average difference for STAR-SFT is -3.3, while for STAR-SFT&RL, it is -3.0. This indicates slight increase in the number of objects compared during the reasoning process of the STAR-SFT&RL. Consequently, this results in modest improvement in OOD accuracy, though it still falls short of the performance achieved by the pure RL training."
        },
        {
            "title": "D More Cases",
            "content": "In this section, we provide detailed presentation of our problem prompt (shown in Fig. 8) along with additional case studies. The {ObjectFeature} in Fig. 8 will be substituted by all features corresponding to each object in the initial image of every sample. An example is provided below: {idx: 0; color: yellow; material: metal; shape: cylinder; size: medium}, {idx: 1; color: gray; material: rubber; shape: sphere; size: medium}, {idx: 2; color: blue; material: rubber; shape: cylinder; size: medium}, {idx: 3; color: yellow; material: metal; shape: sphere; size: medium}, {idx: 4; color: blue; material: rubber; shape: cube; size: large}, {idx: 5; color: brown; material: rubber; shape: sphere; size: medium}, {idx: 6; color: brown; material: rubber; shape: sphere; size: large}. The case studies shown in Figs. 9 to 11 further substantiate the argument presented in Section 4.2 of the main paper, namely that STAR-SFT merely engages in imitation learning and does not perceive changes in point of view. It erroneously associates objects located at the same positions in the initial and final images (referring to their positions within the images, not their positions in the real-world scene) as the same object, thereby significantly impairing the models reasoning performance. In contrast, STAR-R1 demonstrates anthropomorphic behavior by systematically comparing the states of all objects between the initial and final images during its reasoning process to ascertain their correspondence. Consequently, without any explicit mention of viewpoint changes in the problem 15 prompts, STAR-R1 successfully detects the perspective shift and achieves significantly higher answer accuracy than STAR-SFT. Since the STAR-SFT model fails to perceive changes in viewpoint, it mistakenly assumes that objects at the same position in two images are the same object. In Fig. 9, the STAR-SFT model incorrectly identifies objects numbered 0, 3, and 5 as objects numbered 3, 5, and 6. In Fig. 10, it misidentifies objects numbered 1, 0, and 5 as objects numbered 0, 5, and 4. In Fig. 11, the model confuses objects numbered 3, 4, and 0 with objects numbered 7, 3, and 4. In contrast, by comparing the states of all objects in both images, STAR-R1 correctly matches the same objects across the two images and ultimately outputs the correct answer. Figure 8: The problem prompt. 16 Figure 9: case study comparing the reasoning processes of STAR-SFT and STAR-R1. Blue indicates correct answers while red denotes incorrect ones. Figure 10: case study comparing the reasoning processes of STAR-SFT and STAR-R1. Blue indicates correct answers while red denotes incorrect ones. 17 Figure 11: case study comparing the reasoning processes of STAR-SFT and STAR-R1. Blue indicates correct answers while red denotes incorrect ones."
        },
        {
            "title": "E Limitations",
            "content": "This work studies the problem of concurrent object transformations at single timestamp. In contrast, real-world scenarios often involve temporally extended sequences of transformations with latent dependencies. Extending our approach to such long-term settings, accounting for temporal causality and state memory, is key direction for future research."
        }
    ],
    "affiliations": [
        "DAMO Academy, Alibaba Group, Hangzhou, China",
        "Gaoling School of Artificial Intelligence, Renmin University of China",
        "Hupan Lab, Hangzhou, China",
        "MAIS, Institute of Automation, Chinese Academy of Sciences"
    ]
}