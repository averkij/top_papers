{
    "paper_title": "Met$^2$Net: A Decoupled Two-Stage Spatio-Temporal Forecasting Model for Complex Meteorological Systems",
    "authors": [
        "Shaohan Li",
        "Hao Yang",
        "Min Chen",
        "Xiaolin Qin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The increasing frequency of extreme weather events due to global climate change urges accurate weather prediction. Recently, great advances have been made by the \\textbf{end-to-end methods}, thanks to deep learning techniques, but they face limitations of \\textit{representation inconsistency} in multivariable integration and struggle to effectively capture the dependency between variables, which is required in complex weather systems. Treating different variables as distinct modalities and applying a \\textbf{two-stage training approach} from multimodal models can partially alleviate this issue, but due to the inconformity in training tasks between the two stages, the results are often suboptimal. To address these challenges, we propose an implicit two-stage training method, configuring separate encoders and decoders for each variable. In detailed, in the first stage, the Translator is frozen while the Encoders and Decoders learn a shared latent space, in the second stage, the Encoders and Decoders are frozen, and the Translator captures inter-variable interactions for prediction. Besides, by introducing a self-attention mechanism for multivariable fusion in the latent space, the performance achieves further improvements. Empirically, extensive experiments show the state-of-the-art performance of our method. Specifically, it reduces the MSE for near-surface air temperature and relative humidity predictions by 28.82\\% and 23.39\\%, respectively. The source code is available at https://github.com/ShremG/Met2Net."
        },
        {
            "title": "Start",
            "content": "Met2Net: Decoupled Two-Stage Spatio-Temporal Forecasting Model for Complex Meteorological Systems Shaohan Li1,2 Hao Yang1 Min Chen1,2,3 Xiaolin Qin2,3 1Chengdu University of Information Technology 2Chengdu Institute of Computer Applications, Chinese Academy of Sciences 3University of Chinese Academy of Sciences 5 2 0 2 3 2 ] . [ 1 9 8 1 7 1 . 7 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "The increasing frequency of extreme weather events due to global climate change urges accurate weather prediction. Recently, great advances have been made by the end-to-end methods, thanks to deep learning techniques, but they face limitations of representation inconsistency in multivariable integration and struggle to effectively capture the dependency between variables, which is required in complex weather systems. Treating different variables as distinct modalities and applying two-stage training approach from multimodal models can partially alleviate this issue, but due to the inconformity in training tasks between the two stages, the results are often suboptimal. To address these challenges, we propose an implicit twostage training method, configuring separate encoders and decoders for each variable. In detailed, in the first stage, the Translator is frozen while the Encoders and Decoders learn shared latent space, in the second stage, the Encoders and Decoders are frozen, and the Translator captures inter-variable interactions for prediction. Besides, by introducing self-attention mechanism for multivariable fusion in the latent space, the performance achieves further improvements. Empirically, extensive experiments show the state-of-the-art performance of our method. Specifically, it reduces the MSE for near-surface air temperature and relative humidity predictions by 28.82% and 23.39%, respectively. The source code is available at https:// github.com/ShremG/Met2Net. 1. Introduction Global climate change and the increasing frequency of extreme weather events make accurate weather prediction crucial [10]. While traditional statistical and physical models are effective in some areas, they often struggle with complex nonlinear relationships and high-dimensional *Equal contribution. Corresonding authors. (a) Data distributions. (b) First-order difference. (c) T2M variable. (d) TCC variable. Figure 1. (a) Spatiotemporal distributions of the meteorological variables TCC and T2M highlight their heterogeneity. (b) Firstorder differences illustrate variations in trends and rates. (c) and (d) show that incorporating multiple variables did not enhance and even degraded TAUs performance, while the two-stage training scheme also produced suboptimal results. data. Recently, advancements in artificial intelligence have driven the rapid development of spatiotemporal prediction models, significantly improving weather forecasting accuracy [16, 17, 31, 32, 50, 51]. The success of these methods is attributed not only to their focusing on temporal nonstationarity but also to their incorporation of spatial dependency to capture more complex weather patterns. However, variables such as temperature, humidity, wind speed, and precipitation interact with each other, forming complex climate system [1, 5, 7, 8, 12]. The prevailing methods aim to analyze single variable, fail to capture these dynamic relationships, thus leading to prediction errors. Therefore, integrating multiple meteorological variables enables the model to better understand weather patterns, identify potential extreme weather events, and provide more reliable warnings and decision support in reate representations from noise [36, 38, 63], continuing the reconstruction process. However, in spatiotemporal prediction, while the first stage aligns with generative training, the second stage shifts to prediction task rather than denoising reconstruction. This mismatch hinders the effectiveness of learned representations, leading to suboptimal accuracy, as shown in Figure 1c and 1d. To address this issue, we propose Met2Net, decoupled two-stage spatio-temporal forecasting model for complex meteorological systems, as shown in Figure 2c. Specifically, to eliminate representation inconsistency and task inconformity, we freeze the gradients of the Translator in the first stage and those of the Encoder and Decoder in the second stage, ensuring alignment between training objectives. Momentum updates [18] adjust the frozen components in both stages, maintaining stability and enabling smooth parameter adaptation. Additionally, prediction loss in the latent space ensures unified learning objective, keeping both stages focused on forecasting rather than shifting from reconstruction to prediction. This design allows Met2Net to function within single continuous training cycle, eliminating explicit stage separation. To further improve performance, dedicated encoders and decoders preserve variable-specific features, while self-attention mechanism in the Translator enables effective multivariable integration. By addressing both representation inconsistency and task inconformity, Met2Net achieves more accurate and stable multivariate spatiotemporal prediction. To summarize, our contribution in this work is as follows: We propose an implicit two-stage training paradigm that freezes module gradients at specific stages and employs momentum updates. By introducing prediction loss in the latent space, the approach effectively aligns training objectives, enhancing spatiotemporal prediction. We treat each meteorological variable as an independent modality, using separate encoders and decoders, and introduce self-attention mechanism in the Translator to capture inter-variable relationships. We construct general multivariate spatiotemporal prediction dataset to evaluate the robustness and generalization of our method. Empirical results show that Met2Net achieves SOTA performance, reducing the MSE of T2M and by 28.82% and 23.39%, resectively. Additionally, it improves hurricane trajectory prediction accuracy, demonstrating its effectiveness in weather forecasting. 2. Related works Spatiotemporal predictive learning. The advancements in recurrent-based models have greatly enhanced our understanding of spatiotemporal predictive learning. The ConvLSTM model [48] combines convolutional networks with LSTM to capture spatiotemporal patterns. Following this, PredRNN [53] and PredRNN++ [54] used spatiotemporal (a) E2E. (b) Generative. (c) Ours. Figure 2. (a) End-to-End (E2E) training strategy. (b) Generative model training strategy, where task inconsistency exists between the two stages. (c) Met2Net training strategy, which ensures task consistency across both stages. The Translator module is responsible for learning spatiotemporal features. The Translator module is responsible for learning spatiotemporal features. sponse to climate change [8, 12]. During implementation, we found that simply incorporating multiple meteorological variables into the existing spatiotemporal prediction framework did not significantly improve accuracy; in some cases, it even decreased. As shown in Figure 1c and 1d, the experimental results confirm this performance decline. Different from image data with RGB channel, which could be regarded as multi-variables, the meteorological variables demonstrate high degree of divergence [21, 52]. As an illustration, we present the distribution of each variables values across spatial and temporal dimensions, as shown in Figure 1a, 1b. (Detailed experimental setting is given in Appendix.) These distribution divergences make it challenging for simple end-to-end training to effectively integrate these variables [9, 37, 41], leading to (i) representation inconsistency. The reason is that when single model processes highly divergent data, it can easily lead to loss of the original characteristics of the data as features across different dimensions are forcibly integrated into the same space, resulting in an inconsistency between the representation and the input variables. To address representation inconsistency, two-stage training approaches, inspired by multi-modal generation models [11, 47, 59], treat different meteorological variables as distinct modalities. This strategy, widely used in modern generative models like Latent Diffusion [47] and autoregressive for image [30, 62] and video [24, 61], maps diverse modalities into shared latent space, transforming heterogeneity into homogeneity [43] and enhancing model performance. However, its application to spatiotemporal prediction remains rare. When we adapted this two-stage strategy, results were unsatisfactory (Figure 1c, 1d), which we attribute to (ii) task inconformity. In generative tasks, the first stage typically employs Auto-encoders or Variational Auto-encoders [6, 27] to reconstruct data, mapping it into latent space. The second stage then uses models like diffusion to generLSTM (ST-LSTM) and gradient highway units to better capture temporal dependencies and reduce gradient vanishing. MIM [56] uses the difference between hidden states to improve handling of nonstationarity. E3D-LSTM [55] adds 3D convolutions to LSTM to improve feature extraction across time and space. PhyDNet [16] separates partial differential equation (PDE) dynamics from unknown factors using recurrent physical unit. MAU [4] introduces motion-aware unit to capture motion-related information. Lastly, PredRNNv2 [58] uses curriculum learning strategy and memory decoupling loss to improve performance. Unlike recurrent-based methods, which have high computational costs in spatiotemporal prediction, SimVP [14] introduces non-recurrent spatiotemporal prediction framework that reduces computational costs while achieving competitive performance. Subsequently, non-recurrent models such as TAT [39], TAU [50], DMVFN [20], and Wast [40] made further progress by incorporating triplet attention, temporal attention, dynamic multiscale voxel flow, and 3D wavelet framework. However, these methods are based on singlevariable spatiotemporal training, and although they have achieved top-tier performance in weather prediction, they fail to fully exploit the potential of multivariable data in weather forecasting. To address this, we propose solution that independently encodes and decodes each variable and introduce an implicit two-stage training strategy. Weather forecasting. Weather forecasting, as specific application of spatiotemporal prediction, has also benefited from these advancements, leading to significant improvements in prediction accuracy and computational efficiency. Han et al. [17] applied the SimVP model for radar extrapolation and achieved competitive performance in precipitation prediction scenarios. Chen et al. [8] introduced the TAU model, integrating satellite data to alleviate the issue of severe abrupt changes in precipitation scenarios. Eusebi et al. [12] utilized PINN to fuse barometric pressure for hurricane reconstruction. Jiang et al. [22] employed GNN to integrate multiple variables, further enhancing the accuracy of wind speed predictions. Models such as Pangu [1], FuXi [7], and Fengwu [5] incorporate wide range of meteorological variables, achieving near-perfect predictions of typhoon paths and highlighting their capability for mediumrange forecasting. This underscores the necessity and effectiveness of integrating multiple variables and multimodal data in weather forecasting. Two stage training methods. Two-stage training is widely used in generative tasks, where models like Stable Diffusion [47], MAGVIT [62], and VideoPoet [24] map diverse modalities into shared latent space for improved representation learning. Typically, the first stage employs Auto-encoders (AE) or Variational Auto-encoders (VAE) [6, 27] for data reconstruction, while the second stage utilizes diffusion models [36, 38, 63] to generate latent representations from noise. Recent research on twostage training focuses on enhancing the generality of representations learned in the first stage through large-scale pretraining [3, 15, 18, 42]. While this improves transferability across tasks, it requires extensive data, limiting its feasibility in scenarios with constrained datasets. However, in spatiotemporal prediction, the challenge lies not only in obtaining general representations but also in addressing task inconformity, where the second stage shifts from reconstruction to forecasting. This misalignment reduces the effectiveness of learned representations for prediction, leading to suboptimal accuracy. Our work tackles this issue by designing training strategy that better aligns the objectives of both stages, improving prediction performance without requiring large-scale pretraining. 3. Method 3.1. Preliminaries i}t t+1 = {yt+1 We formally define the multivariable spatiotemporal predictive learning problem as follows. Given multivaritT +1 = {xt able time sequence tT +1 at time with the past frames, we aim to predict the subsequent frames t+T from time + 1, represents collection of variables, each with channels C, height H, and width . In practice, we represent the time setT +1 RT CHW and quences as tensors, i.e., t+T t+1 RT CHW . For example, the wind variable has two channels, and V, and its dimensions can be represented as RT 12HW . }t+T t+1 In this paper, we denote the Encoder by E, the Decoder by D, and the Translator by . We denote the true input data by and the label by . The reconstructed or predicted corresponding data are represented by and , respectively. Zx represents the encoded state of processed by E, and Zy represents the encoded state of processed by E. denotes the predicted state of Zx processed by . 3.2. Overview As shown in Figure 3, training is divided into two stages. In the first stage, the Translator is frozen while the Encoder and Decoder are trained to focus on spatial feature compression and reconstruction. In the second stage, the Encoder and Decoder are frozen, and latent space prediction loss is introduced to train the Translator for inter-variable relationship modeling and prediction. During inference, only the fully trained modules are used, excluding those with frozen gradients. Thus, while this approach adds parameters and computation during training, the parameter count and computational load during inference remain the same as standard end-to-end model. To better handle multi-variable sceFigure 3. Training pipeline and Inference pipeline. The training pipeline consists of two stages: In Stage I, the Translator (blue snowflake icon) is frozen while the Encoder and Decoder (orange flame icon) are trained. In Stage II, the Encoder and Decoder are frozen, and the Translator is trained. Momentum updates, represented by dashed arrows, are applied to the frozen parameters at the end of each stage. Multiple encoders and decoders are used for different meteorological variables, enabling independent feature extraction and reconstruction. The inference pipeline uses the trained Encoder, Translator, and Decoder to generate the final output. narios, each variable has its own Encoder-Decoder pair. 3.3. Implicit two-stage training strategy Inspired by the two-stage training approach used in multimodal models in the generative domain [24, 47, 62], we propose an implicit two-stage training strategy. This strategy involves freezing the gradients of different components at various stages within single training process and using momentum updates to adjust parameters. While ensuring consistency of objectives between the two stages, this method retains the advantages of traditional two-stage training strategies, such as effective module transferability and efficient modality fusion. In implementation, we should stop gradients of Translator (T ) during the first stage and gradients of Encoder (E) & Decoder (D) during the second stage. However, the Translator and Encoder & Decoder cannot be optimized alternatively with naive stop-gradient operation. To address this issue, we introduce the momentum update method to update the corresponding parameters. Formally, denoting the parameters of fm and as θm and θ, we update θm by: θm αθm + (1 α)θ (1) Here α [0, 1] is momentum coefficient. represents the components E, D, and whose parameters are updated through backpropagation, while fm represents the corresponding components with frozen gradients. During the training process, the objective function for the first stage is consistent with the end-to-end model, aiming to minimize the gap between and . The objective for the second stage is to minimize the gap between and Zy. Ultimately, our loss function can be expressed as: = L1(Y , ) + L2(Z (2) y, Zy) where L1 and L2 represent the loss functions for the first and second stages, respectively, and both are defined as Mean Squared Error (MSE) in this study. represents the target data, is the models output data, is the output of the second stage , and Zy is the output of after being processed by E. 3.4. Multiple meteorological variables fusion In meteorological forecasting, integrating multiple variables is essential in that meteorological factors, as different physical quantities, are mutually coupled and influence each other within an actual geophysics dynamic system [22, 26, 33, 35, 57]. Merging different variables can be regarded as multimodal fusion, and we thus, enlighten by the latest techniques in multimodal fusion [2, 19, 24, 44], design independent and for each meteorological variable and then used in the latent space to achieve the fusion and prediction. Each meteorological variable is assigned pair of and with the same configuration. For and D, we use twodimensional architecture. independently compresses each variable into the latent space along the spatial dimensions, while restores the embeddings from the latent space back to the original space. Formally, denoting the two input meteorological variables as X1, X2, the fusion steps read Z1 = E1(X1), Z2 = E2(X2) = Stack(Z1, Z2) ˆZ = (Z) 1 = D1(Slice( ˆZ )), ˆX ˆX 2 = D2(Slice( ˆZ )) (3) (4) (5) (6) Here, Stack() and Slice() represent stacking and slicing Table 1. Quantitative comparison of predictions across multiple meteorological variables, including UV10, T2M, TCC, and R, measured using MSE, MAE, and RMSE metrics. # Params denote the parameters of the model during the inference stage. represents the number of models required to predict the four variables. indicates lower is better. Method Date #Params. UV10 T2M TCC ConvLSTM[48] NeurIPS2015 NeurIPS2017 PredRNN[53] ICML2018 PredRNN++[54] NeurIPS2021 MAU[4] CVPR2022 SimVP[14] CVPR2022 ConvNeXt[34] NeurIPS2022 HorNet[45] CVPR2023 TAU[50] AAAI2024 Wast[40] Met2Net Ours (M) 14.98 23.57 38.40 5.46 14.67 10.09 12.42 12.22 14.46 8.65 4 4 4 4 4 4 4 4 4 1 MSE MAE RMSE MSE MAE RMSE MSE MAE RMSE MSE MAE RMSE 5.9280 1.8976 0.9215 6.1330 1.8810 0.9068 6.7820 1.8727 0.9019 5.8760 1.9001 0.9194 5.8610 1.9993 0.9510 5.7600 1.6914 0.8698 5.6640 1.5539 0.8254 5.6420 1.5925 0.8426 5.5690 4.9382 35.1460 4.0120 37.6110 4.0960 45.9930 4.7310 34.5290 4.0040 34.3550 3.9940 33.1790 3.9280 32.0810 3.8260 31.8310 3.8180 3.6940 24.3854 3. 0.0494 0.1542 0.0550 0.1588 0.0548 0.1544 0.0496 0.1516 0.0477 0.1503 0.0474 0.1487 0.0469 0.1475 0.0472 0.1460 0.1452 0.0422 0.1370 1.5210 0.7949 1.3310 0.7246 1.4580 0.7676 1.2510 0.7036 1.2380 0.7037 1.2770 0.7220 1.2010 0.6906 1.1620 0.6707 1.0980 0.6338 0.8271 0.5770 0.2223 0.2346 0.2341 0.2226 0.2183 0.2178 0.2166 0.2173 0.2150 0.2054 1.2330 1.1540 1.2070 1.1190 1.1130 1.1300 1.0960 1.0780 1.0440 0.9094 1.3775 1.3715 1.3685 1.3784 1.4140 1.3006 1.2466 1.2619 - 1.2270 1.5055 0. - - - - Table 2. The statistics of datasets. The training or testing set has Ntrain or Ntest samples, composed by or images with the shape (C, H, ). The weather refers to the multivariate prediction dataset. The subscript Ldenotes low spatial resolution, HA represents high-altitude variables, and indicates high spatial resolution. The MvMmfnist is dataset we constructed for the general multivariate spatiotemporal prediction scenario. RN HW is the value term, extracted by another 2D-CNN, Za RN HW is the fused embeddings. We denote the above operations using VA() (Variable Attention). Next, we use spatiotemporal feature extractor written as ST() to extract spatiotemporal features and map them into predictions, as WeatherL WeatherHA WeatherH ERA5 UV10 T2M TaxiBJ MvMmfnist"
        },
        {
            "title": "Ntest",
            "content": "52559 54019 52559 43801 52559 52559 19627 10000 17495 2883 17495 8737 17495 17495 1334 10000 5 12 5 4 1 1"
        },
        {
            "title": "H W T",
            "content": "32 32 64 128 32 32 32 64 64 64 128 128 64 64 32 64 12 4 12 12 12 12 4 10 12 4 12 12 12 12 4 10 operations, respectively. ˆX diction results for the two meteorological variables. 2 represent the models pre1, ˆX 3.5. Translator To model the complex dynamic dependencies of multiple meteorological variables, we utilize an attention mechanism to effectively capture the correlations between variables [13] and achieve effective decoupling between multiple variables. Before processing with the , the embeddings of each variable for all time steps need to be arranged sequentially along the channel dimension. For spatiotemporal signal with variables, the length of , and the spatial resolution of (H, ), which is denoted by RN HW , the attention matrix over variable axis reads as: = softmax( QK ), (7) where Q, RN DHW are query and key, which are is scaling extracted by two different 2D-CNNs, and term. After the softmax() function, RN demonstrates the correlations between different variables. Subsequently, the fusion is performed by Za = AV , where (Z) = ST(VA(Z)) (8) In this paper, we utilize TAU Block[50] as the practical implementation of ST(). We discuss the impact of using different blocks on the results in the appendix. 4. Experiment In this section, we present experiments demonstrating the effectiveness of our method across two tasks: multivariate weather prediction and general spatiotemporal prediction. For multivariate weather prediction, we tested lowand high-resolution spatial data as well as high-altitude data, and we conducted hurricane trajectory prediction experiment using real ERA5 data. For general spatiotemporal prediction, both single-variable and multivariate experiments were conducted to assess the methods generalization and versatility. The datasets used are listed in Table 2. The datasets WeatherL and WeatherH use UV10 (10meter and components of wind), T2M (2-meter temperature), TCC (total cloud cover), and (relative humidity), for total of four variables, with the two components of wind considered as single variable. The WeatherHA dataset uses (u component of wind), (v component of wind), (temperature), and (relative humidity), for total of four variables. Additionally, the ERA5 dataset includes MSL (mean sea level pressure), U10, V10, and T2M, also consisting of four variables. For the ERA5 dataset, we performed spatial cropping within the range of 110E142E and 12.25N44N. The dataset was split into three subsets: 20172021 for training, 2022 for validation, and 2023 for testing. (a) t=1. (b) t=12. Figure 4. Visualization of prediction results for different lead times. (a) Results at forecast time of 1 hour. The background in white represents the absolute error (Y ) for each model. (b) Results at forecast time of 12 hours. Across both 1-hour and 12-hour forecasts, the error of the TAU model is consistently higher than that of our Met2Net model. Table 3. Quantitative comparison of predictions across multiple high-altitude variable meteorological variables, including R, T, U, and V, measured using MSE, MAE, and RMSE metrics. indicates lower is better. Method Date T ConvLSTM[48] NeurIPS2015 368.15 13.49 NeurIPS2017 354.57 13.17 PredRNN[53] ICML2018 PredRNN++[54] 363.15 13.25 NeurIPS2021 363.36 13.50 MAU[4] 370.03 13.58 CVPR2022 SimVP[14] CVPR2022 ConvNeXt[34] 367.39 13.52 NeurIPS2022 353.02 13.02 HorNet[45] 342.63 12.80 CVPR2023 TAU[50] Met2Net 337.98 12.74 Ours MSE MAE RMSE MSE MAE RMSE MSE MAE RMSE MSE MAE RMSE 30.789 3.8238 5.5488 28.973 3.6617 5.3827 29.872 3.7067 5.4655 27.929 3.6700 5.2848 29.094 3.7614 5.3939 31.326 3.8435 5.5969 30.028 3.7148 5.4798 25.456 3.4723 5.0454 23.900 3.3597 4.8888 6.3030 1.7695 2.5107 5.5966 1.6411 2.3657 5.6471 1.6433 2.3763 5.6287 1.6810 2.3725 6.1068 1.7554 2.4712 6.1749 1.7448 2.4849 5.5856 1.6198 2.3634 4.9042 1.5341 2.2145 4.1376 1.3688 2. 30.002 3.8923 5.4774 27.484 3.6776 5.2425 28.396 3.7322 5.3288 27.582 3.7409 5.2519 28.782 3.8435 5.3649 29.764 3.8688 5.4556 28.192 3.7142 5.3096 24.719 3.5060 4.9719 22.832 3.3449 4.7783 19.19 18.830 19.056 19.062 19.236 19.168 18.789 18.510 18.38 The optimizer used was Adam [23], with training batch size of 16, running for 50 epochs, and momentum parameter α set to 0.999. We evaluated the test set using three metrics: MSE, MAE, and RMSE. 4.2. Multiple Meteorological Variables Prediction Low-Resolution. Table 1 presents quantitative comparison between our method and existing models for lowresolution multivariable meteorological prediction. Our method achieves SOTA performance across all major metrics, including MSE, MAE, and RMSE. Compared to TAU, our approach reduces MSE by 5.46%, 28.82%, 10.59%, and 23.39% for the four variables, respectively. To illustrate these differences more intuitively, Figure 4 presents visual comparison between models, where Met2Net demonstrates smaller spatial prediction errors across the four variables, highlighting its advantage in accuracy. High-Altitude Variable. To further validate the robustness of our approach under varying atmospheric conditions, we conducted an experiment on high-altitude meteorological prediction. Table 3 compares the performance of different models in this task, showing that our method maintains SOTA performance. This experiment evaluates model performance at three altitude levels150, 500, and 850 hPausing 6-hour intervals. Specifically, the previous Figure 5. Met2Net effectively resolves representation inconsistency by maintaining higher CKA values across layers compared to TAU. The TAU model exhibits gradual increase in CKA values through the Translator section but maintains relatively lower CKA values overall. In contrast, Met2Net sustains higher CKA values across all layers, particularly in the Encoder and Translator sections, indicating improved representation consistency. 4.1. Setup The baseline experiments were conducted using the OpenSTL [51] library, with all meteorological variable data sourced from the WeatherBench [46] dataset. The settings followed the OpenSTL configuration, where each variable inputs 12 frames and predicts the subsequent 12 frames. Table 4. Quantitative comparison of predictions across multiple high-resolution meteorological variables, including UV10, T2M, TCC, and R, measured using MSE, MAE, and RMSE metrics. indicates lower is better. Method Date UV10 T2M TCC ConvLSTM[48] NeurIPS2015 1.5691 0.8653 1.2518 NeurIPS2017 1.4083 0.8037 1.1859 PredRNN[53] NeurIPS2021 1.5781 0.8578 1.2554 MAU[4] 1.7654 0.9332 1.3279 SimVP[14] CVPR2022 1.4804 0.8399 1.2160 ConvNeXt[34] CVPR2022 1.3038 0.7887 1.1411 CVPR2023 TAU[50] Met2Net 1.2745 0.7835 1.1283 Ours MSE MAE RMSE MSE MAE RMSE MSE MAE RMSE MSE MAE RMSE 28.9580 3.6594 5.3769 30.1590 3.7334 5.4875 30.7881 3.7989 5.5442 32.7849 4.0132 5.7217 31.9944 3.8841 5.6518 28.2291 3.6307 5.3090 21.5415 3.1606 4. 0.0450 0.1431 0.2121 0.0482 0.1444 0.2195 0.0446 0.1451 0.2111 0.0448 0.1451 0.2115 0.0457 0.1455 0.2136 0.0434 0.1403 0.2082 0.0378 0.1270 0.1944 1.1385 0.6774 1.0654 1.1862 0.6695 1.0873 1.2493 0.7097 1.1161 1.2750 0.7333 1.1278 1.2155 0.7048 1.1010 1.0599 0.6592 1.0282 0.8936 0.6194 0.9446 Table 5. Quantitative comparison of predictions performance on the ERA5 dataset, including MSL, U10, V10, and T2M, measured using MSE, MAE, and R2 metrics. indicates lower is better, while indicates higher is better. MSL MAE Method Date U10 V10 T2M MSE R2 MSE MAE R2 MSE MAE R2 MSE MAE R2 ConvLSTM[48] NeurIPS2015 16577.42 90.8390 0.9677 1.7236 0.9238 0.9397 1.8728 0.9813 0.9060 2.0504 0.9236 0.9873 ConvNeXt[34] CVPR2022 15618.98 92.7848 0.9696 1.5358 0.8495 0.9462 1.6811 0.9020 0.9156 1.2860 0.7347 0.9920 NeurIPS2022 14585.94 88.7772 0.9716 1.4964 0.8443 0.9476 1.6184 0.8896 0.9188 1.1962 0.7087 0.9926 HorNet[45] 18617.73 100.8613 0.9638 1.5582 0.8756 0.9454 1.6547 0.9084 0.9170 1.4031 0.7618 0.9913 ICLR2024 MogaNet[29] 16541.85 93.7000 0.9678 1.4979 0.8404 0.9476 1.6121 0.8824 0.9191 1.2808 0.7205 0.9920 CVPR2023 TAU[50] Met2Net 70.3976 0.9812 1.2070 0.7518 0.9577 1.3124 0.7954 0.9341 0.8176 0.5866 0.9949 9645.28 Ours presents quantitative comparison across multiple highresolution variablesUV10, T2M, TCC, and Rusing MSE, MAE, and RMSE as evaluation metrics. Met2Net demonstrates competitive performance across all metrics, achieving the lowest error values, which underscores its robustness and accuracy in high-resolution prediction tasks. Tracking Tropical Cyclones. Our method was evaluated on Typhoon Mawars trajectory prediction using the ERA5 dataset, which includes MSL, U10, V10, and T2M variables. As shown in Figure 6, our model closely follows the ground truth while outperforming the TAU model, particularly in the early and mid-stages. Notably, it achieves higher accuracy with 3-hour lead time while using fewer parameters. The quantitative results in Table 5 further confirm its superiority, with the lowest MSE and MAE and the highest R2 across all variables, demonstrating its efficiency and effectiveness in tropical cyclone forecasting. Further analysis. To demonstrate the effectiveness of our method in addressing representation inconsistency, we analyzed the Centered Kernel Alignment (CKA) [25] values across layers for two meteorological variables: T2M and TCC. As shown in Figure 5, this analysis compares CKA values for both variables in the TAU and Ours models, focusing on the Encoder, Translator, and Decoder sections. CKA values, which measure similarity between layer representations, reflect each models alignment with the internal structure of T2M and TCC, where higher values indicate stronger representation consistency. The TAU model shows gradual increase in CKA values through the Translator section, whereas the Ours model maintains consistently higher values, particularly in the Encoder and TransFigure 6. Comparison of predicted and ground truth tracks of Typhoon Mawar (3-hour lead time). The figure shows the observed trajectory alongside predictions from TAU and Met2Net. Met2Net more closely follows the ground truth, demonstrating improved accuracy in typhoon trajectory forecasting. frames (representing one days data) are used to predict the next 4 frames, simulating forecast of the following days weather based on current data. High-Resolution. To assess the models ability to capture fine-scale meteorological patterns, we conducted an experiment on high-resolution meteorological prediction. Table 4 lator sections. This suggests that the Ours model better preserves representation consistency across variables. Table 7. Quantitative comparison on the Taxibj. The subscript in BaselineS indicates the single-variable model. Table 6. Quantitative comparison on the MvMmfnist. The learning rate was optimized by selecting the best value from {1e4, 1e3, 5e3}. Method SimVP ConvNeXt TAU Met2Net MSE MAE 397.10 155.52 434.63 174.97 385.13 147.69 328.67 132.89 SSIM 0.8812 0.8682 0.8833 0.9086 PSNR 19.2825 18.7304 19.5075 19. 4.3. General Spatiotemporal Prediction To evaluate the generalization capability of our model, we conducted experiments on single-variable and multivariable spatiotemporal prediction datasets. TaxiBJ was chosen for single-variable prediction, representing urban traffic flow, while three-channel MvMmfnist dataset was created for multivariable prediction. These experiments aim to assess the models ability to handle diverse data types and spatiotemporal patterns, validating its applicability across different domains. MvMmfnist. The dataset simulates the complexity of multivariate spatiotemporal prediction scenarios. It contains three channels, each functioning as an independent moving dataset. To better represent real-world scenarios, such as multivariate weather prediction, the channels are interrelated yet retain some independence. Specifically, all channels share the same movement trajectory: the second channels moving object is from the Fashion MNIST [60] dataset instead of MNIST [49], while the third channel is generated by flipping the foreground and background of the first channel. This design introduces both unique features and interrelationships among the channels. Experiments on this dataset, with results shown in Table 6, indicate that the Met2Net achieved state-of-the-art performance, demonstrating its effectiveness in multivariate spatiotemporal prediction scenarios. All models were trained using the Adam optimizer for 200 epochs, with the best-performing epoch selected for evaluation, ensuring fair comparison. TaxiBJ. Table 7 shows experimental results on the TaxiBJ dataset, where the only difference between our method, Met2Net, and the TAU model is the training strategy, with all other configurations kept identical. Results indicate that Met2Net achieves comparable performance to TAU across all metrics, including MSE, MAE, SSIM, and PSNR. This similarity suggests that our training approach provides advantages in multivariable prediction tasks without compromising performance in single-variable scenarios. Method ConvLSTM[48] PredRNN[53] PredRNN++[54] MAU[4] SimVP[14] MogaNet[28] HorNet[45] TAU[50] Met2NetS MSE MAE 15.32 0.3358 15.31 0.3194 15.37 0.3348 15.26 0.3268 15.45 0.3282 15.06 0.3114 15.01 0.3186 14.93 0.3108 14.82 0. SSIM 0.9836 0.9838 0.9834 0.9834 0.9835 0.9847 0.9843 0.9848 0.9851 PSNR 39.45 39.51 39.47 39.52 39.45 39.70 39.66 39.74 39.81 Table 8. Ablation study of the proposed method on MSE. MED (Multiple Encoder and Decoder). ITS (Implicit two-stage). The subscript in BaselineS indicates the single-variable model. Method BaselineS Baseline +MED +VA() +ITS UV10 1.5925 1.8347 1.8327 1.6287 1.5055 T2M 1.1620 2.2247 1.6708 1.5432 0. TCC 0.0472 0.0439 0.0445 0.0428 0.0422 31.8310 29.0749 28.8003 26.6849 24.3854 4.4. Ablation Study Table 8 presents ablation study results for the proposed method in multivariable prediction. Using the TAU model as baseline for multivariate meteorological forecasting, results indicate that model performance declines when handling multiple variables compared to single-variable predictions. This suggests that complex variable interactions challenge model precision, highlighting the need for optimized multivariable fusion strategies. Starting from the baseline model, we progressively introduced Multi-Encoder-Decoder (MED), Variable Attention (VA), and Implicit Two-Stage Training (ITS). Results show consistent MSE decrease across all variables, with substantial improvements after incorporating MED and VA, indicating that these enhancements decouple variable relationships and significantly improve accuracy. Further gains with ITS confirm the effectiveness of our training strategy. 5. Conclusion In this paper, we proposed novel implicit two-stage training paradigm to improve spatiotemporal prediction tasks involving multiple meteorological variables. By treating each variable as an independent modality and incorporating separate encoders, decoders, and self-attention mechanism, our framework, Met2Net, effectively captures complex interactions between variables and enhances prediction accuracy. We also constructed general multivariate spatiotemporal prediction dataset, demonstrating the robustness and generalization of our method across different tasks."
        },
        {
            "title": "Acknowledgement",
            "content": "This research was partly supported by the Sichuan Science and Technology Achievement Transfer and Transformation Demonstration Project (2024ZHCG0026), the Sichuan Science and Technology Program (2024NSFJQ0035), and the Talents Program supported by the Organization Department of the Sichuan Provincial Party Committee."
        },
        {
            "title": "References",
            "content": "[1] Kaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu, and Qi Tian. Accurate medium-range global weather forecasting with 3d neural networks. Nature, 619(7970): 533538, 2023. 1, 3 [2] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2256322575, 2023. 4 [3] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. Advances in neural information processing systems, 33:9912 9924, 2020. 3 [4] Zheng Chang, Xinfeng Zhang, Shanshe Wang, Siwei Ma, Yan Ye, Xiang Xinguang, and Wen Gao. Mau: motionaware unit for video prediction and beyond. Advances in Neural Information Processing Systems, 34:2695026962, 2021. 3, 5, 6, 7, 8 [5] Kang Chen, Tao Han, Junchao Gong, Lei Bai, Fenghua Ling, Jing-Jia Luo, Xi Chen, Leiming Ma, Tianning Zhang, Rui Su, et al. Fengwu: Pushing the skillful global mediumrange weather forecast beyond 10 days lead. arXiv preprint arXiv:2304.02948, 2023. 1, 3 [6] Kai Chen, Zhili Liu, Lanqing Hong, Hang Xu, Zhenguo Li, and Dit-Yan Yeung. Mixed autoencoder for selfsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2274222751, 2023. 2, [7] Lei Chen, Xiaohui Zhong, Feng Zhang, Yuan Cheng, Yinghui Xu, Yuan Qi, and Hao Li. Fuxi: cascade machine learning forecasting system for 15-day global weather forecast. npj Climate and Atmospheric Science, 6(1):190, 2023. 1, 3 [8] Min Chen, Hao Yang, Shaohan Li, and Xiaolin Qin. Staa: Spatiotemporal alignment attention for short-term precipitation forecasting. IEEE Geoscience and Remote Sensing Letters, 21:15, 2024. 1, 2, 3 [9] Shengchao Chen, Guodong Long, Tao Shen, and Jing Jiang. Prompt federated learning for weather forecasting: Toward foundation models on meteorological data. arXiv preprint arXiv:2301.09152, 2023. 2 [11] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming In Protransformers for high-resolution image synthesis. ceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. 2 [12] Ryan Eusebi, Gabriel Vecchi, Ching-Yao Lai, and Mingjing Tong. Realistic tropical cyclone wind and pressure fields can be reconstructed from sparse data using deep learning. Communications Earth & Environment, 5(1):8, 2024. 1, 2, 3 [13] Tryambak Gangopadhyay, Sin Yong Tan, Zhanhong Jiang, Rui Meng, and Soumik Sarkar. Spatiotemporal attention for multivariate time series prediction and interpretation. In ICASSP 2021-2021 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 3560 3564. IEEE, 2021. 5 [14] Zhangyang Gao, Cheng Tan, Lirong Wu, and Stan Li. Simvp: Simpler yet better video prediction. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 31703180, 2022. 3, 5, 6, 7, [15] Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems, 33:2127121284, 2020. 3 [16] Vincent Le Guen and Nicolas Thome. Disentangling physical dynamics from unknown factors for unsupervised video In Proceedings of the IEEE/CVF conference prediction. on computer vision and pattern recognition, pages 11474 11484, 2020. 1, 3 [17] Daehyeon Han, Minki Choo, Jungho Im, Yeji Shin, Juhyun Lee, and Sihun Jung. Precipitation nowcasting using ground radar data and simpler yet better video prediction deep learning. GIScience & Remote Sensing, 60(1):2203363, 2023. 1, 3 [18] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 97299738, 2020. 2, 3 [19] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Mohammad Norouzi, David Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. 4 [20] Xiaotao Hu, Zhewei Huang, Ailin Huang, Jun Xu, and Shuchang Zhou. dynamic multi-scale voxel flow network for video prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 61216131, 2023. [21] Yaxuan Huang, Bin Guo, Haoxuan Sun, Huijie Liu, and Song Xi Chen. Relative importance of meteorological variables on air quality and role of boundary layer height. Atmospheric Environment, 267:118737, 2021. 2 [10] James Douris and Geunhye Kim. The atlas of mortality and economic losses from weather, climate and water extremes (1970-2019). 2021. 1 [22] Wenjun Jiang, Bo Liu, Yang Liang, Huanxiang Gao, Pengfei Lin, Dongqin Zhang, and Gang Hu. Applicability analysis of transformer to wind speed forecasting by novel deep learning framework with multiple atmospheric variables. Applied Energy, 353:122155, 2024. 3, 4 [23] Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization. In International Conference on Learning Representations, 2015. 6 [24] Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jose Lezama, Jonathan Huang, Rachel Hornung, Hartwig Adam, Hassan Akbari, Yair Alon, Vighnesh Birodkar, et al. Videopoet: large language model for zero-shot video generation. arXiv preprint arXiv:2312.14125, 2023. 2, 3, [25] Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural network represenIn International conference on machine tations revisited. learning, pages 35193529. PMLR, 2019. 7 [26] Remi Lam, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter Wirnsberger, Meire Fortunato, Ferran Alet, Suman Ravuri, Timo Ewalds, Zach Eaton-Rosen, Weihua Hu, et al. Learning skillful medium-range global weather forecasting. Science, 382(6677):14161421, 2023. 4 [27] Pengzhi Li, Yan Pei, and Jianqiang Li. comprehensive survey on design and application of autoencoder in deep learning. Applied Soft Computing, 138:110176, 2023. 2, 3 [28] Siyuan Li, Zedong Wang, Zicheng Liu, Cheng Tan, Haitao Lin, Di Wu, Zhiyuan Chen, Jiangbin Zheng, and Stan Li. Moganet: Multi-order gated aggregation network. In International Conference on Learning Representations, 2024. 8 [29] Siyuan Li, Zedong Wang, Zicheng Liu, Cheng Tan, Haitao Lin, Di Wu, Zhiyuan Chen, Jiangbin Zheng, and Stan Li. Moganet: Multi-order gated aggregation network. In ICLR, 2024. 7 [30] Siyuan Li, Luyuan Zhang, Zedong Wang, Juanxi Tian, Cheng Tan, Zicheng Liu, Chang Yu, Qingsong Xie, Haonan Lu, Haoqian Wang, and Zhen Lei. Mergevq: unified framework for visual generation and representation with disentangled token merging and quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. 2 [31] Haitao Lin, Zhangyang Gao, Yongjie Xu, Lirong Wu, Ling Li, and Stan Li. Conditional local convolution for spatiotemporal meteorological forecasting. In Proceedings of the AAAI conference on artificial intelligence, pages 74707478, 2022. [32] XuDong Ling, ChaoRong Li, LiHong Zhu, FengQing Qin, Ping Zhu, and Yuanyuan Huang. Spacetime separable latent diffusion model with intensity structure information for precipitation nowcasting. IEEE Transactions on Geoscience and Remote Sensing, 2024. 1 [33] Xingdou Liu, Li Zhang, Jiangong Wang, Yue Zhou, and Wei Gan. unified multi-step wind speed forecasting framework based on numerical weather prediction grids and wind farm monitoring data. Renewable Energy, 211:948963, 2023. 4 [34] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1197611986, 2022. 5, 6, 7 [35] Minbo Ma, Peng Xie, Fei Teng, Bin Wang, Shenggong Ji, Junbo Zhang, and Tianrui Li. Histgnn: Hierarchical spatiotemporal graph neural network for weather forecasting. Information Sciences, 648:119580, 2023. 4 [36] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024. 2, 3 [37] Fadji Maina, Erica Siirila-Woodburn, and Pouya Vahmani. Sensitivity of meteorological-forcing resolution on hydrologic variables. Hydrology and Earth System Sciences, 24 (7):34513474, 2020. 2 [38] Haomiao Ni, Changhao Shi, Kai Li, Sharon Huang, and Martin Renqiang Min. Conditional image-to-video generIn Proceedings of ation with latent flow diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1844418455, 2023. 2, [39] Xuesong Nie, Xi Chen, Haoyuan Jin, Zhihang Zhu, Yunfeng Yan, and Donglian Qi. Triplet attention transformer for spatiotemporal predictive learning. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 70367045, 2024. 3 [40] Xuesong Nie, Yunfeng Yan, Siyuan Li, Cheng Tan, Xi Chen, Haoyuan Jin, Zhihang Zhu, Stan Li, and Donglian Qi. Wavelet-driven spatiotemporal predictive learning: BridgIn Proceedings of the ing frequency and time variations. AAAI Conference on Artificial Intelligence, pages 4334 4342, 2024. 3, 5 [41] Edward Appau Nketiah, Li Chenlong, Jing Yingchuan, and Simon Appah Aram. Recurrent neural network modeling of multivariate time series and its application in temperature forecasting. Plos one, 18(5):e0285713, 2023. 2 [42] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 3 [43] Qingyao Qiao, Hamidreza Eskandari, Hassan Saadatmand, and Mohammad Ali Sahraei. An interpretable multi-stage forecasting framework for energy consumption and co2 emissions for the transportation sector. Energy, 286:129499, 2024. 2 [44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. [45] Yongming Rao, Wenliang Zhao, Yansong Tang, Jie Zhou, Ser Nam Lim, and Jiwen Lu. Hornet: Efficient highorder spatial interactions with recursive gated convolutions. Advances in Neural Information Processing Systems, 35: 1035310366, 2022. 5, 6, 7, 8 [46] Stephan Rasp, Peter Dueben, Sebastian Scher, Jonathan Weyn, Soukayna Mouatadid, and Nils Thuerey. Weatherbench: benchmark data set for data-driven weather forecasting. Journal of Advances in Modeling Earth Systems, 12 (11):e2020MS002203, 2020. 6 [59] Jialong Wu, Shaofeng Yin, Ningya Feng, Xu He, Dong Li, Jianye Hao, and Mingsheng Long. ivideogpt: Interactive videogpts are scalable world models. arXiv preprint arXiv:2405.15223, 2024. 2 [60] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashionmnist: novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017. 8 [61] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using vq-vae and transformers. arXiv preprint arXiv:2104.10157, 2021. 2 [62] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jose Lezama, Han Zhang, Huiwen Chang, Alexander Hauptmann, MingHsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: In Proceedings of Masked generative video transformer. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1045910469, 2023. 2, 3, [63] Sihyun Yu, Kihyuk Sohn, Subin Kim, and Jinwoo Shin. Video probabilistic diffusion models in projected latent space. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1845618466, 2023. 2, 3 [47] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2, 3, 4 [48] Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-chun Woo. Convolutional lstm network: machine learning approach for precipitation nowcasting. Advances in neural information processing systems, 28, 2015. 2, 5, 6, 7, 8 [49] Nitish Srivastava, Elman Mansimov, and Ruslan Salakhudinov. Unsupervised learning of video representations using In International conference on machine learning, lstms. pages 843852. PMLR, 2015. 8 [50] Cheng Tan, Zhangyang Gao, Lirong Wu, Yongjie Xu, Jun Xia, Siyuan Li, and Stan Li. Temporal attention unit: ToIn Prowards efficient spatiotemporal predictive learning. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1877018782, 2023. 1, 3, 5, 6, 7, 8 [51] Cheng Tan, Siyuan Li, Zhangyang Gao, Wenfei Guan, Zedong Wang, Zicheng Liu, Lirong Wu, and Stan Li. Openstl: comprehensive benchmark of spatio-temporal predictive learning. Advances in Neural Information Processing Systems, 36:6981969831, 2023. 1, [52] Meron Teferi Taye and Ellen Dyer. Hydrologic extremes in changing climate: review of extremes in east africa. Current Climate Change Reports, 10(1):111, 2024. 2 [53] Yunbo Wang, Mingsheng Long, Jianmin Wang, Zhifeng Gao, and Philip Yu. Predrnn: Recurrent neural networks for predictive learning using spatiotemporal lstms. Advances in neural information processing systems, 30, 2017. 2, 5, 6, 7, 8 [54] Yunbo Wang, Zhifeng Gao, Mingsheng Long, Jianmin Wang, and Yu Philip. Predrnn++: Towards resolution of the deep-in-time dilemma in spatiotemporal predictive learning. In International conference on machine learning, pages 51235132. PMLR, 2018. 2, 5, 6, 8 [55] Yunbo Wang, Lu Jiang, Ming-Hsuan Yang, Li-Jia Li, Mingsheng Long, and Li Fei-Fei. Eidetic 3d lstm: model for video prediction and beyond. In International conference on learning representations, 2018. 3 [56] Yunbo Wang, Jianjin Zhang, Hongyu Zhu, Mingsheng Long, Jianmin Wang, and Philip Yu. Memory in memory: predictive neural network for learning higher-order nonstationarity from spatiotemporal dynamics. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 91549162, 2019. 3 [57] Yun Wang, Runmin Zou, Fang Liu, Lingjun Zhang, and Qianyi Liu. review of wind speed and wind power forecasting with deep neural networks. Applied Energy, 304: 117766, 2021. [58] Yunbo Wang, Haixu Wu, Jianjin Zhang, Zhifeng Gao, Jianmin Wang, Yu Philip, and Mingsheng Long. Predrnn: recurrent neural network for spatiotemporal predictive learnIEEE Transactions on Pattern Analysis and Machine ing. Intelligence, 45(2):22082225, 2022. 3 Met2Net: Decoupled Two-Stage Spatio-Temporal Forecasting Model for Complex Meteorological Systems"
        },
        {
            "title": "Supplementary Material",
            "content": "Algorithm 1 Pseudocode of Implicit Two-Stage Process in PyTorch-like Style Integrated Within Inference Pipeline. Algorithm 2 Pseudocode of Implicit Two-Stage Process in PyTorch-like Style Integrated Within Training Pipeline. # E1, E2: encoders. # D1, D2: decoders. # H: translator. for in loader: # load minibatch with samples x1, x2 = slice(x) # Slicing Operations in Python z1_x = E1(x1) # Independent encoding of Variables z2_x = E2(x2) z_x = torch.stack(z1_x,z2_x) # Spatio-Temporal learnning and variable fusion z_y = H(z_x) z1_y, z2_y = slice(z_y) y1 = D1(z1_y) # Independent decoding of Variables y2 = D2(z2_y) y_pre = torch.stack(y1, y2) 6. Appendix 6.1. Experimental Setup for Variable Distributions Data Distributions. The experiment utilized the 2018 T2M (2-meter air temperature) and TCC (Total Cloud Cover) data from the WeatherBench dataset to analyze spatial and temporal distribution patterns. The data were normalized to the range [0, 1] for comparability. Spatial analysis was based on grid data from single time step to capture geographic variability, while temporal analysis focused on the time series data of single grid point. First-Order Differences. The experiment analyzed firstorder differences of 2018 T2M and TCC data from the WeatherBench dataset. Temporal differences were calculated between steps, and spatial differences from adjacent grid points along the and dimensions. Differences were standardized to zero mean and unit variance, with outliers exceeding three standard deviations removed. 6.2. Pseudocode of Inference Pipeline In Algorithm 1, we present the pseudocode of our method within inference pipeline. For simplicity, we demonstrate the case with only two variables. 6.3. Pseudocode of Trainning Pipeline In Algorithm 2, we present the pseudocode of our method within training pipeline. For simplicity, we demonstrate the case with only two variables. # E1, E2, E1_m, E2_m: encoder that applies gradient updates and momentum updates to two different variables. # D1, D2, D1_m, D2_m: decoder that applies gradient updates and momentum updates to two different variables. # H, H_m: translator that gradient updates and momentum updates. # a: momentum E1_m.params = E1.params # initialize E2_m.params = E2.params # initialize D1_m.params = D1.params # initialize D2_m.params = D2.params # initialize H_m.params = H.params # initialize for x,y in loader: # load minibatch x,y with samples # stage 1 x1, x2 = slice(x) # Slicing Operations in Python z1_x = E1(x1) # Independent encoding of Variables z2_x = E2(x2) z_x = torch.stack(z1_x,z2_x) # Spatio-Temporal learnning and variable fusion z_y = H_m(z_x) z1_y, z2_y = slice(z_y) y1 = D1(z1_y) # Independent decoding of Variables y2 = D2(z2_y) y_rec = torch.stack(y1, y2) # momentum update E1_m.params = a*E1_m.params +(1-a)*E1.params E2_m.params = a*E2_m.params +(1-a)*E2.params D1_m.params = a*D1_m.params +(1-a)*D1.params D2_m.params = a*D2_m.params +(1-a)*D2.params loss_rec = MSE(y_rec,y) # stage 2 x1, x2 = slice(x) # Slicing Operations in Python z1_x = E1_m(x1) # use momentum updates module z2_x = E2_m(x2) z_x = torch.stack(z1_x, z2_x) # Spatio-Temporal learnning and variable fusion z_y_pre = H(z_x) z1_y, z2_y = slice(z_y) y1 = D1_m(z1_y) y2 = D2_m(z2_y) y_pre= torch.stack(y1, y2) y1, y2 = slice(y) z1_y = E1_m(y1) # use momentum updates module z2_y = E2_m(y2) z_y = torch.stack(z1_y, z2_y) # momentum update H_m.params = a*H_m.params +(1-a)*H.params loss_pre = MSE(z_y_pre,z_y) loss = loss_rec + loss_pre # Adam update: query network loss.backward() 6.4. Impact of different blocks in translator 6.6. Single meteorological variables prediction We tested different blocks within the Translator of our framework, as shown in Table 9. The results indicate that while the TAU block remains competitive choice, our method consistently outperforms the baseline methods across all tested blocks. This demonstrates the robustness of our framework in handling various blocks. Regardless of the block selected, our method maintains superior performance, validating the effectiveness of the proposed framework across different configurations. To validate the applicability and effectiveness of our method, we conducted single-variable prediction experiments. Table 10 presents quantitative comparison results for UV10 and T2M, showing that our method outperforms existing models across all key metrics. Although singlevariable accuracy is lower than multi-variable predictions  (Table 1)  , this highlights the effectiveness of our multivariable fusion approach and the importance of considering multiple variables in meteorological forecasting. Table 9. Impact of using different Blocks in the translator on T2M and TCC prediction performance. The light gray background indicates results not applied in our framework. The white background indicates results obtained using different translators within our framework. Method HorNet TAU Wast ConvNext SimVPv2 PoolFormer Hornet Moga TAU MSE 1.2010 1.1620 1.0980 1.0238 0.9215 0.9493 0.8778 1.0314 0. T2M MAE 0.6906 0.6707 0.6338 0.6598 0.6148 0.6271 0.5987 0.6643 0.5770 RMSE MSE 1.0960 1.0780 1. 1.0105 0.9588 0.9730 0.9358 1.0141 0.9094 0.0469 0.0472 - 0.0440 0.0425 0.0435 0.0423 0.0445 0.0422 TCC MAE 0.1475 0.1460 0.1452 0.1426 0.1367 0.1426 0.1388 0.1455 0. RMSE 0.2166 0.2173 0.2150 0.2096 0.2061 0.2085 0.2055 0.2109 0.2054 6.5. Performance evolves with time steps. As shown in Figure 7, the performance of both models (TAU and our method) varies with the prediction time steps for different variables (T2M and TCC). Although the performance of both models declines as the time step increases, our method consistently outperforms TAU, exhibiting slower growth in MSE and more stable PCC. (a) T2M on MSE. (b) T2M on PCC. (c) TCC on MSE. (d) TCC on PCC. Figure 7. Performance comparison of T2M and TCC prediction using MSE and PCC across different prediction time steps. Table 10. Quantitative comparison on the UV10 and T2M variables. The subscript in Baselines indicates the single-variable model."
        },
        {
            "title": "Method",
            "content": "UV10 T2M MAE ConvLSTM 0.9215 0.9019 PredRNN++ 0.9510 SimVP 0.8698 ConvNeXt 0.8426 TAU Met2NetS 0.8197 RMSE MAE 0.7949 1.3775 0.7866 1.3685 0.7037 1.4091 0.7220 1.3006 0.6607 1.2619 0.6536 1.2518 RMSE 1.2330 1.2070 1.1130 1.1300 1.0780 1.0753 6.7. Additional metrics and resource comparison We report both the anomaly correlation coefficient (ACC) and the resource consumption of different models on the cropped ERA5 dataset, as presented in Table 11. All experiments are conducted under the same setting with fp32 precision and batch size 16 on single NVIDIA RTX 4090 GPU. Met2Net achieves the highest forecasting accuracy across all variables while maintaining moderate parameter count and competitive efficiency in terms of computation and memory usage. Table 11. ACC and resource comparison on cropped ERA5. Method Params FLOPs Mem Time MSL U10 V10 T2M (M) (G) (MiB) (Min) ACC ConvLSTM 7.44 12.83 MogaNet 12.29 TAU Met2Net 8.90 135.0 4398 2:42 0.9671 0.9073 0.9427 0.9293 18.9 14408 1:37 0.9690 0.9181 0.9492 0.9533 18.3 11942 1:21 0.9652 0.9097 0.9432 0.9510 119.0 23078 2:24 0.9803 0.9340 0.9590 0. Note: All experiments were conducted on single NVIDIA RTX 4090 GPU. Mem indicates the peak GPU memory usage with fp32 and batch size of 16; Time refers to the training time per epoch. 6.8. Scalability under increased variable input To evaluate the scalability of the proposed method, we expand the number of input meteorological variables in the eatherL setting by introducing three additional physical fields: total precipitation (TP), geopotential height (Z), and top-of-atmosphere incoming shortwave radiation (TISR). Correspondingly, we increase the encoderdecoder pairs The attention map reveals several meaningful patterns. For example, the model places strong attention between U10 and V10, and between T2M and TCC, which are physically correlated. This indicates that the translator can adaptively capture variable-specific influences, enhancing both forecasting performance and model interpretability. 6.10. Additional tracking tropical cyclones (a) Predicted and ground truth tracks of Typhoon MAWAR (1-Hour lead time). (b) Predicted and ground truth tracks of Typhoon DOKSURI (3-Hour lead time). Figure 9. Tracking tropical cyclones. 6.11. Additional visualization results from 4 to 8, while maintaining the same translator architecture. We compare the forecasting performance of TAU and Met2Net under varying numbers of encoderdecoder pairs. The results are summarized in Table 12. Table 12. Forecasting performance with increased variables (UV10 and TCC) under different encoderdecoder configurations."
        },
        {
            "title": "Method",
            "content": "TAU1 Met2Net4 TAU8 Met2Net8 # (M) MSE 1.5925 12.2 1.5055 8.7 1.7345 12.2 1.4740 8.9 UV"
        },
        {
            "title": "TCC",
            "content": "RMSE MSE 0.0472 1.2619 0.0422 1.2270 0.0444 1.3161 0.0417 1.2129 RMSE 0.2173 0.2054 0.2107 0.2043 The results show that Met2Net maintains superior forecasting accuracy while scaling to more variables, with only marginal increase in parameter count (from 8.65M to 8.87M). In contrast, TAU exhibits performance drop despite using the same number of encoders. This demonstrates that Met2Net is well-suited for scalable spatiotemporal modeling in multi-variable settings. 6.9. Cross-Variable Attention Analysis To better understand the inter-variable dependencies captured by the translator module, we analyze the crossvariable attention weights learned during the forecasting process. In our model, each meteorological variable is encoded independently as token, and the translator performs self-attention over these variable tokens to enable dynamic information aggregation. Figure 8 presents the averaged attention map across all samples and heads. Each row represents the target variable being predicted, and each column indicates the source variable being attended to. The values are normalized attention weights, reflecting how much each variable contributes to the others. Figure 8. Cross-variable attention map extracted from the translator module. Brighter colors indicate stronger attention. Variables include: MSL, U10, V10, T2M, TP, Z, TISR, and TCC. (a) t=1. (b) t=12. Figure 10. Visualization of prediction results for different lead times. (a) Results at forecast time of 1 hour. The background in white represents the absolute error ( GT-Prediction ) for each model. (b) Results at forecast time of 12 hours. (a) t=1. (b) t=12. Figure 11. Visualization of prediction results for different lead times. (a) Results at forecast time of 1 hour. The background in white represents the absolute error ( GT-Prediction ) for each model. (b) Results at forecast time of 12 hours. (a) t=1. (b) t=12. Figure 12. Visualization of prediction results for different lead times. (a) Results at forecast time of 1 hour. The background in white represents the absolute error ( GT-Prediction ) for each model. (b) Results at forecast time of 12 hours. (a) t=1. (b) t=12. Figure 13. Visualization of prediction results for different lead times. (a) Results at forecast time of 1 hour. The background in white represents the absolute error ( GT-Prediction ) for each model. (b) Results at forecast time of 12 hours. (a) t=1. (b) t=10. Figure 14. Visualization of prediction results for different lead times on the Mv Mmfnist dataset. The last two columns represent the absolute error ( GT - Prediction ) for each model. (a) Results at forecast time of 1 frame. (b) Results at forecast time of 10 frame. (a) t=1. (b) t=10. Figure 15. Visualization of prediction results for different lead times on the Mv Mmfnist dataset. The last two columns represent the absolute error ( GT - Prediction ) for each model. (a) Results at forecast time of 1 frame. (b) Results at forecast time of 10 frame. (a) t=1. (b) t=10. Figure 16. Visualization of prediction results for different lead times on the Mv Mmfnist dataset. The last two columns represent the absolute error ( GT - Prediction ) for each model. (a) Results at forecast time of 1 frame. (b) Results at forecast time of 10 frame."
        }
    ],
    "affiliations": [
        "Chengdu Institute of Computer Applications, Chinese Academy of Sciences",
        "Chengdu University of Information Technology",
        "University of Chinese Academy of Sciences"
    ]
}