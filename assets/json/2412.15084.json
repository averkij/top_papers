{
    "paper_title": "AceMath: Advancing Frontier Math Reasoning with Post-Training and Reward Modeling",
    "authors": [
        "Zihan Liu",
        "Yang Chen",
        "Mohammad Shoeybi",
        "Bryan Catanzaro",
        "Wei Ping"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this paper, we introduce AceMath, a suite of frontier math models that excel in solving complex math problems, along with highly effective reward models capable of evaluating generated solutions and reliably identifying the correct ones. To develop the instruction-tuned math models, we propose a supervised fine-tuning (SFT) process that first achieves competitive performance across general domains, followed by targeted fine-tuning for the math domain using a carefully curated set of prompts and synthetically generated responses. The resulting model, AceMath-72B-Instruct greatly outperforms Qwen2.5-Math-72B-Instruct, GPT-4o and Claude-3.5 Sonnet. To develop math-specialized reward model, we first construct AceMath-RewardBench, a comprehensive and robust benchmark for evaluating math reward models across diverse problems and difficulty levels. After that, we present a systematic approach to build our math reward models. The resulting model, AceMath-72B-RM, consistently outperforms state-of-the-art reward models. Furthermore, when combining AceMath-72B-Instruct with AceMath-72B-RM, we achieve the highest average rm@8 score across the math reasoning benchmarks. We will release model weights, training data, and evaluation benchmarks at: https://research.nvidia.com/labs/adlr/acemath"
        },
        {
            "title": "Start",
            "content": "AceMath: Advancing Frontier Math Reasoning with Post-Training and Reward Modeling Zihan Liu * 1 Yang Chen * 1 Mohammad Shoeybi 1 Bryan Catanzaro 1 Wei Ping 1 4 2 0 2 9 1 ] . [ 1 4 8 0 5 1 . 2 1 4 2 : r Abstract In this paper, we introduce AceMath, suite of frontier math models that excel in solving complex math problems, along with highly effective reward models capable of evaluating generated solutions and reliably identifying the correct ones. To develop the instruction-tuned math models, we propose supervised fine-tuning (SFT) process that first achieves competitive performance across general domains, followed by targeted finetuning for the math domain using carefully curated set of prompts and synthetically generated responses. The resulting model, AceMath-72BInstruct greatly outperforms Qwen2.5-Math-72BInstruct, GPT-4o and Claude-3.5 Sonnet. To develop math-specialized reward model, we first construct AceMath-RewardBench, comprehensive and robust benchmark for evaluating math reward models across diverse problems and difficulty levels. After that, we present systematic approach to build our math reward models. The resulting model, AceMath-72B-RM, consistently outperforms state-of-the-art reward models. Furthermore, when combining AceMath-72BInstruct with AceMath-72B-RM, we achieve the highest average rm@8 score across the math reasoning benchmarks. 1. Introduction Over the past year, the open large language model (LLM) community has made remarkable progress in advancing the key capabilities of LLMs, including multi-turn conversation (Chiang et al., 2023; Dubey et al., 2024), coding (Guo et al., 2024; Hui et al., 2024), multimodal functionalities (Dai et al., 2024; Chen et al., 2024), retrievalaugmented generation (RAG) (Liu et al., 2024c), and mathematical reasoning (Azerbayev et al., 2023; Shao et al., 2024; *Equal contribution 1NVIDIA. Correspondence to: Zihan Liu <zihanl@nvidia.com>, Yang Chen <yachen@nvidia.com>, Wei Ping <wping@nvidia.com>. Technical Report. Copyright 2024 by the author(s). Mistral, 2024; Yang et al., 2024b). Among these capabilities, mathematics is recognized as fundamental aspect of intelligence. It can serve as reliable benchmark due to its objective, consistent, verifiable nature. Consequently, solving math problems is widely regarded as critical testbed for evaluating an LLMs ability to tackle challenging tasks that require complex, numerical and multi-step logical reasoning (e.g., Cobbe et al., 2021; Hendrycks et al., 2021a; Lightman et al., 2023). Previous studies have convincingly demonstrated that mathspecialized LLMs significantly outperform general-purpose LLMs on challenging mathematical benchmarks (Azerbayev et al., 2023; Shao et al., 2024; Mistral, 2024; Yang et al., 2024b). These math-specialized models, including the corresponding reward models (a.k.a. verifiers), are not only valuable to the mathematics and science communities (e.g., Tao, 2023), but they also provide valuable insights into data collection and serve as synthetic data generation tools, contributing to the advancement of future iterations of general-purpose LLMs. The improved mathematical reasoning capabilities of mathspecialized LLMs are generally acquired through both the i) During concontinued pre-training and post-training: tinued pre-training stage, the models are initialized with general-purpose base pretrained LLMs (e.g., Llama-3.170B (Dubey et al., 2024)), and continually trained on extensive collections of mathematical corpora, often comprising hundreds of billions of tokens sourced from Common Crawl (Shao et al., 2024), ArXiv papers (Azerbayev et al., 2023), and synthetically generated datasets (Yang et al., 2024b; Akter et al., 2024). In this stage, losses are calculated on every token within the corpus. ii) In the post-training phase, the continually pretrained math base LLMs (e.g., Qwen2.5-Math-72B (Yang et al., 2024b)) are fine-tuned using large datasets of mathematical prompt-response pairs. In this stage, losses are computed only on the response tokens, allowing the models to refine their ability to generate accurate answers given the prompts or problem descriptions. In this work, we push the limits of math reasoning with posttraining and reward modeling based on open weights base LLMs and math base LLMs. We establish state-of-the-art supervised fine-tuning (SFT) and reward modeling (RM) Advancing Frontier Math Reasoning with Post-Training and Reward Modeling Figure 1. AceMath versus leading open-weights and proprietary LLMs on math reasoning benchmarks. Additionally, we report rm@8 accuracy (best of 8) with our reward model AceMath-72B-RM and use the official reported numbers from Qwen2.5-Math. processes for building math-specialized models, while also sharing key insights gained from our comprehensive studies. Specifically, we make the following contributions: 1. We introduce SFT process designed to first achieve competitive performance across general domains, including multidisciplinary topics, coding, and math. Building on this, the general SFT model is further fine-tuned in math domain using meticulously curated set of prompts and synthetically generated responses. Leveraging the highquality training data, the resulting model, AceMath-7BInstruct, largely outperforms the previous best-in-class Qwen2.5-Math-7B-Instruct (pass@1: 67.2 vs. 62.9) on variety of math reasoning benchmarks (detailed results in Figure 1), while coming close to the performance of 10 larger Qwen2.5-Math-72B-Instruct (67.2 vs. 68.2). Notably, our AceMath-72B-Instruct outperforms the stateof-the-art Qwen2.5-Math-72B-Instruct by margin (71.8 vs. 68.2). 2. We conducted systematic investigation of training techniques for building math-specialized reward models, focusing on key aspects such as the construction of positive-negative pairs, training objectives, and the elimination of stylistic biases from specific LLMs. Leveraging the insights gained from this exploration, our AceMath-72B-RM consistently outperforms state-of-theart reward models, including Qwen2.5-Math-RM-72B and Skywork-o1-Open-PRM-Qwen-2.5-7B (Skyworko1, 2024), in the math domain. Moreover, when combining AceMath-72B-Instruct with AceMath-72B-RM, we achieve the highest average rm@8 score across seven math reasoning benchmarks (see Figure 1), setting new standard for performance in this field. 3. We will open source the model weights for AceMathInstruct and AceMath-RM, along with the complete training data used across all stages of their development. We also release AceMath-RewardBench, comprehensive benchmark for evaluating math reward models, offering diverse datasets, varying difficulty levels, and robustness to variations in response styles. We organize the rest of this paper as follows. In 2, we introduce the related work. In 3, we introduce the SFT processes, including the details on the training data curation. We present our reward model training in 4. We conclude the paper in 5 2. Related Work 2.1. Continued Pre-training on Math Corpus Many studies have investigated the integration of large-scale mathematical data for pre-training LLMs to enhance their math capabilities (Shen et al., 2021; Wang et al., 2023; Zhang et al., 2024a; Ying et al., 2024; Akter et al., 2024; Hui 2 Advancing Frontier Math Reasoning with Post-Training and Reward Modeling et al., 2024). Additionally, some research has focused on developing math-specialized LLMs by continuing the pretraining of general-purpose LLM with an extensive math corpus, sourced from math-related web texts, encyclopedias, exam questions, and synthetic mathematical data (Shao et al., 2024; Yang et al., 2024b). These works demonstrate that this additional math-focused pre-training significantly enhances the models ability to solve math problems, benefiting not only the pre-trained base model but also subsequent instruct models after post-training. 2.2. Supervised Fine-Tuning Numerous supervised fine-tuning (SFT) datasets have been developed to enhance pretrained LLMs with versatile capability, such as instruction following (Chiang et al., 2023; The-Vicuna-Team, 2023; Lian et al., 2023; Mukherjee et al., 2023; Teknium, 2023; Peng et al., 2023; Yuan et al., 2024), coding (Glaive-AI, 2023; Wei et al., 2024; Luo et al., 2023), and mathematical problem-solving (Yue et al., 2024a;b; Yu et al., 2023; Mitra et al., 2024; Li et al., 2024b). Due to the high cost of human-annotated data, synthetic data generation has become an essential component of SFT data construction, including both prompt and response augmentation (Yu et al., 2023; Xu et al., 2024; Luo et al., 2023; Li et al., 2024a; Toshniwal et al., 2024). Taking this further, math-instructed models have been developed to advance LLM performance in the mathematics domain (Shao et al., 2024; Toshniwal et al., 2024; Yang et al., 2024b) by utilizing math-specific pretrained models as backbones and vast amounts of synthetic posttraining data tailored to mathematics. For example, OpenMathInstruct (Toshniwal et al., 2024) shows that mathspecialized SFT with extensive synthetic data on the Llama3.1 base model significantly outperforms the corresponding Llama3.1 instruct model on mathematical benchmarks. In addition, Qwen2.5-Math (Yang et al., 2024b) demonstrates that 7B math-instruct model can achieve math reasoning capabilities comparable to GPT-4o. 2.3. Reward Modeling Training reward models for mathematical verification often involves discriminative approaches, such as binary classification to distinguish correct solutions from incorrect ones (Cobbe et al., 2021). Alternatively, preferencebased methods are employed, leveraging techniques like the Bradley-Terry loss (Bradley & Terry, 1952; Ouyang et al., 2022) or regression loss to rank solutions, as demonstrated in models like HelpSteer (Wang et al., 2024e;d). In contrast, generative reward models, such as LLM-as-ajudge (Zheng et al., 2023) prompt LLMs to act as verifiers using predefined rubrics and grading templates (Bai et al., 2022), GenRM (Zhang et al., 2024c) leverages Chain-of3 Thought reasoning (Wei et al., 2022), and Critic-RM (Yu et al., 2024) uses critic before predicting reward. Our work on outcome reward model mainly focuses on robustness against style biases (Liu et al., 2024b) by sampling diverse model responses for training. Beyond outcomebased reward models, process reward models (PRMs) provide step-by-step evaluations of model responses (Uesato et al., 2022; Lightman et al., 2023). For example, MathShepherd (Wang et al., 2024b) introduces an automated sampling method to construct large-scale process supervision data for training, following by further developments in step-wise supervision labeling (Dong et al., 2024), including PAV (Setlur et al., 2024), OmegaPRM (Luo et al., 2024), ER-PRM (Zhang et al., 2024b), AutoPSV (Lu et al., 2024) and ProcessBench (Zheng et al., 2024). 3. Supervised Fine-tuning 3.1. Overview Providing strong initialization point is crucial for the model to begin math-focused SFT effectively. Previous works (Shao et al., 2024; Yang et al., 2024b) have demonstrated that continual pre-training of LLMs with large math corpus provides more effective initialization for subsequent math post-training. Taking this further, we explore whether conducting general SFT on pre-trained LLM can serves as even better initialization for the subsequent mathspecific SFT. The idea is that performing SFT on generalpurpose tasks helps the model develop strong capabilities for following instructions and reasoning (e.g., knowledgerelated). This foundation, in turn, makes it easier for the model to acquire math problem-solving skills from mathfocused SFT data. The details of curating general SFT data can be found in 3.2.1. The next-step is constructing math-specific SFT data. It is crucial to develop diverse set of math prompts accompanied by unified, step-by-step, and accurate solutions. The details of curating math SFT data can be found in 3.2.2. Figure 2 depicts the summary of the SFT data. The details of how we leverage general and math SFT data for the training can be found in 3.3. 3.2. Data Curation 3.2.1. GENERAL SFT DATA Our goal is to build general SFT model that serve as strong starting point for the subsequent math-specific SFT. This general SFT model should excel at following instructions and answer wide range of questions, including those related to math and coding. Advancing Frontier Math Reasoning with Post-Training and Reward Modeling Prompt Construction To achieve this goal, we collect prompts from diverse range of open-source datasets, categorized as follows: General domain: ShareGPT (Chiang et al., 2023; TheVicuna-Team, 2023), SlimOrca (Lian et al., 2023; Mukherjee et al., 2023), EvolInstruct (Xu et al., 2024), GPTeacher (Teknium, 2023), AlpacaGPT4 (Peng et al., 2023), and UltraInteract (Yuan et al., 2024); Coding domain: Magicoder (Wei et al., 2024), WizardCoder (Luo et al., 2023), GlaiveCodeAssistant (GlaiveAI, 2023), and CodeSFT (Adler et al., 2024); Math domain: NuminaMath (Li et al., 2024b), OrcaMathWordProblems (Mitra et al., 2024), MathInstruct (Yue et al., 2024a), and MetaMathQA (Yu et al., 2023), as well as our synthetic data (details in 3.2.2). Since different data sources could have prompt overlaps, we conduct data deduplication to eliminate duplicate prompts that are identical when converted to lowercase. After deduplication, we retain the prompt set unfiltered to preserve the diversity of prompts. Response Construction After collecting the prompts, our goal is to construct high-quality responses in consistent format so that models can learn more effectively. Therefore, we avoid using the original open-source responses for these prompts, as they may lack quality and have inconsistent formats due to being sourced from different curators or generated by different models. We use GPT-4o-mini (20240718) to generate responses for collected prompts in coding and general domains. GPT-4o-mini is selected for its strong performance across different tasks and instructions, as well as its compact size, which makes it both time-efficient and cost-efficient for producing large volume of generated responses. We put the details of constructing responses for math SFT prompts in 3.2.2. We generate single response for each prompt using greedy decoding, ultimately accumulating around 1.2 million coding SFT samples (0.67 billion tokens) and 0.7 million samples (0.55 billion tokens) in the general domain. And, we take around 1.2 million samples (0.95 billion tokens) from the math SFT data (described in 3.2.2) for the general SFT. 3.2.2. MATH SFT DATA The goal is to construct diverse set of math prompts accompanied by unified, step-by-step, and accurate solutions. Initial Prompts We first take math prompts from general SFT data, drawing specifically from open-source datasets: NuminaMath (Li et al., 2024b), OrcaMathWordProblems (Mitra et al., 2024), MathInstruct (Yue et al., 2024a), and MetaMathQA (Yu et al., 2023). These prompts cover wide range of math problems, spanning grade school, high school, college-level, and Olympiad-level math challenges. After that, we perform data deduplication to remove duplicate prompts as before. Finally, we collect over 1.3 million initial prompts. Synthetic Prompt Generation Furthermore, we generate additional synthetic prompts to enrich the diversity of our math prompt collection. This process involves two key steps: 1) leveraging diverse seed prompts to inspire powerful instruct model to generate entirely new, potentially more challenging or uncommon prompts, and 2) ensuring that the generated prompts are solvable, as unsolvable prompts can lead to incorrect answers, which may degrade performance when used for training. Therefore, we select NuminaMath as our seed prompt source due to its broad coverage of math questions across various difficulty levels. Then, we apply two strategies inspired by Xu et al. (2024): in-breadth evolution for generating more rare prompts and in-depth evolution for generating more challenging ones. For synthetic prompt generation, we utilize GPT-4o-mini (2024-0718). It is crucial to filter out low-quality synthetic prompts. In particular, we find that one type of in-depth evolution, which involves adding constraints to existing prompts to generate new ones, can sometimes produce unsolvable or overly challenging questions. This, in turn, may result in incorrect answers being included in the training data, ultimately degrading model performance (see ablation studies in 3.6.4). As result, we exclude this type of prompt augmentation. Moreover, we filter out the synthetic prompts exceeding 300 words, as excessively lengthy math-related prompts are often problematic or unsolvable. Finally, we refine the synthetic math prompts to approximately one million by filtering out 500K, ensuring more curated dataset for training. As result, we have total collection of over 2.3 million math prompts (1.3M initial prompts + 1M synthetic prompts). We provide the details about the synthetic prompt generation in Appendix C. Response Construction We utilize Qwen2.5-Math-72BInstruct for generating responses to math prompts, given its state-of-the-art performance across various math benchmarks. We add the instruction, Please reason step by step, and put your final answer within boxed{}. to the prompt to ensures the responses are presented in clear, step-bystep format with consistent style. We generate single response for each of the over 2.3M prompts and ensure consistency in the response format by selecting only those responses (along with their prompts) that adhere to uniform structure (e.g., starting the response with summary of the question and having the final answer within boxed). Additionally, responses exceeding 4 Advancing Frontier Math Reasoning with Post-Training and Reward Modeling 2,500 words are excluded, along with their prompts, as excessive response length often indicates verbose or incorrect solution, or an unfinished response. Furthermore, while Qwen2.5-Math-72B-Instruct demonstrates strong capabilities, it occasionally produces repetitive strings (e.g., repeating the same text until reaching the maximum output length). We detect and remove such patterns, along with their corresponding prompts. Although these cases represent only small fraction of the dataset, they can negatively impact the final performance and are carefully filtered during the curation process. After filtering, we obtain total of around 2.3 million math SFT samples (1.83 billion tokens), of which around 1.2 million are utilized in the general SFT. Qwen2.5-Math-72B-Instruct can still generate incorrect solutions which may negatively impact model training. To mitigate this, we focus on identifying samples with accurate final answers to create higher-quality dataset for training. Our approach involves cross-checking answers generated by different models and treating solutions with consistent outcomes as highly likely to be correct. Specifically, we leverage another strong model, GPT-4o-mini (2024-0718), to generate responses. Since GPT-4o-mini is comparatively weaker in mathematics than Qwen2.5-Math-72B-Instruct, we generate two responses per prompt and consider answers consistent across both responses as potentially correct. Finally, we compare these answers with those from Qwen2.5Math-72B-Instruct, and select matched final answers as high-quality solutions for training, which results in total size of 800K math SFT samples. 3.2.3. DATA DECONTAMINATION Data decontamination is essential in SFT to ensure unbiased evaluation and to prevent models from memorizing test samples. Following Yang et al. (2024b), we conduct data decontamination for math SFT prompts. The process begins with text normalization and the removal of irrelevant punctuation for each math prompt. Next, we filter out the prompt that has 13-gram overlap with the test data and the longest common subsequence exceeding 60% of its length. For the rest of non-math SFT prompts, we simply filter out those with 13-gram overlap with test samples. 3.3. Training Strategy 3.3.1. GENERAL SFT STRATEGY Among general tasks, solving complex coding and math problems stands out as particularly challenging, and many general instruct models often struggle with them. To address this and develop more effective general SFT model, we introduce two-stage training approach. In stage-1, the model is trained on large dataset specifically curated for code and math SFT tasks, providing Figure 2. The proportion of total SFT tokens for math, coding, and other categories. strong foundation in these areas. Stage-2 expands the scope by incorporating balanced mix of code, math, and other general SFT data, broadening the models capabilities and enhance the overall performance. We organize the constructed general SFT data (around three million samples) to support this two-stage training. For stage-1, the majority of the coding and math samples are selected, leading to total of around 2 million SFT samples. Stage-2 training utilizes the remaining coding and math SFT samples, subset of the stage-1 data, along with all other general SFT samples, resulting in total of around 1.6 million samples. For math SFT samples used in stage-2 training, we select only the cross-checked high-quality data where the final answers provided by GPT-4o-mini and Qwen2.5Math-72B-Instruct align, as detailed in 3.2.2. This strategy ensures that stage-2 training integrates additional, diverse, and high-quality coding and math SFT samples, thereby fostering more robust model. 3.3.2. MATH SFT STRATEGY We take the base (or math-base) model trained on our general SFT data as the starting point for the math SFT. In order to achieve diverse and high-quality math SFT data, we merge all samples from NuminaMath (Li et al., 2024b), subset of samples from our synthetic prompts, and the 800K math SFT samples that are cross-checked between GPT-4o-mini and Qwen2.5-Math-72B-Instruct (as described in 3.2.2). We remove duplicate samples with identical prompts, resulting in total of 1.6 million samples for math SFT. We find that this training blend leads to better results than directly utilize all 2.3 million math SFT samples for training (this ablation study can be found in 3.6.3). 3.3.3. SFT DATA SUMMARY Figure 2 provides an overview of the distribution of total SFT tokens across math, coding, and other categories, along with details on the utilization of math SFT samples. In total, there are approximately 2.3 million math SFT samples 5 Advancing Frontier Math Reasoning with Post-Training and Reward Modeling Models HumanEval MBPP GSM8K MATH MMLU MMLU Pro Avg. DeepSeek-Coder-7B-Instruct-v1.5 DeepSeek-Coder-7B-Base + Two-Stage SFT (Ours) Llama3.1-8B-Instruct Llama3.1-8B-Base + Two-Stage SFT (Ours) Qwen2.5-1.5B-Instruct Qwen2.5-1.5B-Base + Two-Stage SFT (Ours) Qwen2.5-7B-Instruct Qwen2.5-7B-Base + Two-Stage SFT (Ours) Qwen2.5-72B-Instruct Qwen2.5-72B-Base + Two-Stage SFT (Ours) 64.10 78. 72.60 81.10 61.60 73.17 84.80 85.37 86.60 89.63 64.60 73.54 69.60 74. 63.20 65.76 79.20 74.32 88.20 83.66 72.60 82.56 84.50 90.45 73.20 80. 91.60 93.10 95.80 96.36 34.10 55.62 51.90 64.42 55.20 60.34 75.50 76. 83.10 84.50 49.50 54.65 69.40 68.31 58.37 58.17 74.51 74.68 84.67 83. - 33.28 48.30 43.27 32.40 33.78 56.30 54.50 71.10 66.10 - 62. 66.05 70.38 57.33 61.94 76.99 76.40 84.91 84.02 Table 1. Results of our general SFT models. We apply our proposed two-stage training strategy to conduct SFT on various base models. These finetuned models are then compared against the corresponding instruct baselines that are built upon the same base models. Models HumanEval MBPP GSM8K MATH MMLU MMLU Pro Avg. Llama3.1-8B-Base + Two-Stage SFT Llama3.1-8B-Base + Single-Stage SFT w/ all general SFT data Llama3.1-8B-Base + Single-Stage SFT w/ only stage-2 data Qwen2.5-7B-Base + Two-Stage SFT Qwen2.5-7B-Base + Single-Stage SFT w/ all general SFT data Qwen2.5-7B-Base + Single-Stage SFT w/ only stage-2 data 81.10 78.66 73.78 85.37 83.54 83.54 74.71 69.26 67. 74.32 75.49 73.15 90.45 87.79 88.17 93.10 91.96 92.27 64.42 56.80 55.84 76.40 75.04 75.12 68.31 67.62 67. 74.68 73.96 74.26 43.27 42.64 42.85 54.50 53.36 53.06 70.38 67.13 65.91 76.40 75.56 75.23 Table 2. Ablation studies of our general SFT models regarding the effectiveness of the two-stage training strategy. (1.83 billion tokens), 1.2 million coding SFT samples (0.67 billion tokens), and 0.7 million samples in other categories (0.55 billion tokens). Among the math SFT samples, 1.2 million (0.95 billion tokens) are used for general SFT, while 1.6 million (1.29 billion tokens) are utilized for math SFT. 3.3.4. TRAINING DETAILS All SFT models are trained using the AdamW optimizer (Kingma, 2014; Loshchilov, 2017). We use learning rate of 5e-6 for the general SFT and 3e-6 for the math SFT. global batch size of 128 is used across all model sizes, except for the 72B model, where it is increased to 256. We conduct one epoch of training with maximum sequence length of 4096 for both general SFT and math SFT. 3.4. Benchmarks 3.4.1. GENERAL SFT BENCHMARKS We evaluate our general SFT models on diverse set of widely used benchmarks. These benchmarks consist of coding tasks, including HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021), mathematical reasoning, including GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021b), as well as general knowledge domains, including MMLU (Hendrycks et al., 2020) and MMLU Pro (Wang et al., 2024c). We conduct standard 5-shot evaluations for MMLU and MMLU Pro, and use 0-shot evaluations for the remaining benchmarks. 3.4.2. MATHEMATICAL BENCHMARKS We follow the evaluation setting in Qwen2.5-Math (Yang et al., 2024b) for assessing English mathematical tasks. Beyond the commonly used GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021b) benchmarks, we also evaluate our models on broader set of mathematical benchmarks, including Minerva Math (Lewkowycz et al., 2022), GaoKao 2023 En (Liao et al., 2024), Olympiad Bench (He et al., 2024), College Math (Tang et al., 2024), and MMLU STEM (Hendrycks et al., 2020). These benchmarks comprehensively assess wide range of mathematical reasoning capabilities, from grade school arithmetic to advanced college-level problems and Olympic-level challenges. Other than the above datasets, we further evaluate our models on AMC 20231 and AIME 20242. Although these benchmarks are highly challenging math competition benchmarks, they are quite limited in size, with AMC 2023 containing only 40 test samples and AIME 2024 comprising just 30. Following Yang et al. (2024b), we evaluate these benchmarks separately and present the results in Appendix A. We conduct 5-shot evaluations for MMLU STEM, and use 0-shot evaluations for the remaining benchmarks. Note that for all benchmarks except for Math and GSM8K, we do not use any training dataset or synthetic dataset de1https://huggingface.co/datasets/AI-MO/ aimo-validation-amc 2https://huggingface.co/datasets/AI-MO/ aimo-validation-aime 6 Advancing Frontier Math Reasoning with Post-Training and Reward Modeling Models GSM8K MATH Minerva Math GaoKao 2023 En Olympiad Bench College Math MMLU STEM GPT-4o (2024-0806) Claude-3.5 Sonnet (2024-1022) Llama3.1-70B-Instruct Llama3.1-405B-Instruct OpenMath2-Llama3.1-8B Qwen2.5-Math-1.5B-Instruct Qwen2.5-Math-7B-Instruct Qwen2.5-Math-72B-Instruct AceMath-1.5B-Instruct (Ours) AceMath-7B-Instruct (Ours) AceMath-72B-Instruct (Ours) 92.90 96.40 94.10 96.80 91.70 84.80 95.20 95.90 86.95 93.71 96.44 81.10 75.90 65.70 73.80 67.80 75.80 83.60 85.90 76.84 83.14 86. 50.74 48.16 34.20 54.04 16.91 29.40 37.10 44.10 41.54 51.11 56.99 67.50 64.94 54.00 62.08 53.76 65.50 66.80 71.90 64.42 68.05 72.21 43.30 37.93 27.70 34.81 28.00 38.10 41.60 49.00 33.78 42.22 48. 48.50 48.47 42.50 49.25 46.13 47.70 46.80 49.50 54.36 56.64 57.24 87.99 85.06 80.40 83.10 46.02 57.50 71.90 80.80 62.04 75.32 85.44 Avg. 67.43 65.27 56.94 64.84 50.08 56.97 63.29 68. 59.99 67.17 71.84 Table 3. Greedy decoding (pass@1) results of math instruct models on math benchmarks. Our AceMath-1.5B/7B/72B-Instruct models are built upon the Qwen2.5-Math-1.5B/7B/72B-base models. AceMath-72B-Instruct greatly surpasses the previous state-of-the-art math-instruct model, Qwen2.5-Math-72B-Instruct. rived from it. This ensures more reliable and valid evaluation of our models on these benchmarks. 3.5. Results of General SFT Models 3.5.1. MAIN RESULTS As shown in Table 1, we apply our proposed two-stage training strategy to conduct SFT on various base models, including DeepSeekCoder-7B (Guo et al., 2024), Llama3.18B (Dubey et al., 2024), and Qwen2.5-1.5B/7B/72B (Yang et al., 2024a). We compare our finetuned general SFT models to the corresponding instruct baselines that are built upon the same base models. We observe that our general SFT brings significant improvements across different models, such as DeepSeek-Coder-7B, Llama3.1-8B, and Qwen2.5-1.5B, with an average score improvement of over 4%. Notably, results on DeepSeek-Coder show that our SFT achieves particularly pronounced gains, with an average score increase of approximately 10% or more in coding and math tasks. When compared to more advanced models like Qwen2.5-7B-Instruct and Qwen2.5-72B-instruct, our SFT delivers comparable performance. These findings highlight the effectiveness and strong generalization capabilities of our constructed general SFT dataset. 3.5.2. EFFECTIVENESS OF TWO-STAGE TRAINING As shown in Table 2, we study the effectiveness of two-stage training strategy. For comparison, we use two base models from distinct families (Qwen2.5 and Llama3.1) and conduct single-stage training using either all general SFT data or only the stage-2 SFT data. We observe that our two-stage training consistently outperforms single-stage training. Interestingly, we find notable improvements (more than 3% average score) on relatively weaker base model (e.g., Llama3.1-8B) compared to stronger one (e.g., Qwen2.5-7B). This highlights the importance of incorporating extensive coding and math data during training to enhance the models ability to handle complex coding and math tasks. We conjecture that the Qwen2.5 models already leverage substantial math and coding SFT data during pretraining, which reduces the effectiveness of an additional stage-1 SFT focused on these areas. 3.6. Results of AceMath-Instruct 3.6.1. MAIN RESULTS In Table 3, we compare our AceMath-Instruct models against several strong baselines for greedy decoding, including Qwen2.5-Math-1.5B/7B/72B-Instruct (Yang et al., 2024b), GPT-4o (OpenAI, 2024a), and Claude-3.5 Sonnet (Anthropic, 2024). Specifically, our AceMath1.5B/7B/72B-Instruct models are built upon the Qwen2.5Math-1.5B/7B/72B-base models, which also serve as the foundation for Qwen2.5-Math-1.5B/7B/72B-Instruct. We find that AceMath-1.5B, 7B, and 72B-Instruct achieve significantly better performance compared to the corresponding Qwen2.5-Math-1.5B, 7B, and 72B-Instruct models. Our best model, AceMath-72B-Instruct, achieves significant average improvement of 3.68 over the previous state-ofthe-art, Qwen2.5-Math-72B-Instruct. This highlights the superior quality and generalizability of our constructed math SFT data. Moreover, we find that our 7B model, AceMath-7B-Instruct, demonstrate superior or comparable performance compared to several advanced instruct models, including Llama3.1405B-Instruct, GPT-4o, and Claude-3.5 Sonnet. And, it comes close to matching the performance of the significantly larger Qwen2.5-Math-72B-Instruct, with only slight difference in the average score (68.16 vs. 67.17). 7 Advancing Frontier Math Reasoning with Post-Training and Reward Modeling Models GSM8K MATH Minerva Math GaoKao 2023 En Olympiad Bench College Math MMLU STEM Avg. AceMath-Instruct Only Qwen2.5-Math-72B-Instruct Only GPT-4o-mini Skipping general SFT AceMath-Instruct Only Qwen2.5-Math-72B-Instruct Only GPT-4o-mini Math SFT with all math samples Math SFT with only cross-checked samples Skipping general SFT Backbone: Llama3.1-8B-Base 91.51 91.13 90.83 91.81 69.06 69.66 68.12 68. 31.99 33.82 36.03 31.99 Backbone: Qwen2.5-7B-Base 93.56 92.80 91.66 93.40 92.72 93.03 43.38 41.91 43.75 42.28 41.54 40.44 77.10 76.96 74.14 77.12 76.76 77.52 59.74 60.26 60.26 59.48 65.19 63.64 64.42 65.19 65.97 62. AceMath-Instruct Math SFT with all math samples Skipping general SFT Backbone: Qwen2.5-Math-72B-Base 96.44 96.29 95.75 86.10 86.06 85.52 56.99 55.15 56. 72.21 70.13 71.43 32.00 30.37 31.70 31.11 37.78 38.07 39.26 37.78 36.74 37.19 48.44 46.67 45.33 49.08 49.86 48.05 48.40 54.90 54.93 52.27 54.05 54.33 54. 57.24 57.49 56.71 67.94 66.21 66.50 62.76 77.41 75.64 76.03 75.33 76.78 75.77 85.44 84.96 84.42 57.33 57.33 57.36 56.32 64.19 63.42 63.08 63.59 63.55 63. 71.84 70.96 70.77 Table 4. Ablation Studies on training data and strategies across various backbone models for training our AceMath-Instruct models. The ablation studies can be categorized into three parts: 1) evaluating the effectiveness of using either GPT-4o-mini responses or Qwen2.5-Math-72B-Instruct responses individually; 2) analyzing the effectiveness of different math-specific samples for math SFT; and 3) assessing the impact of conducting general SFT prior to math SFT. tinues the pre-training of DeepSeek-Coder-7B-Base using large math corpus. The math instruct baseline is DeepSeekMath-7B-RL (Shao et al., 2024), which is developed from DeepSeek-Math-7B-Base. For Qwen2.5-1.5/7B/72B, the base models are Qwen2.5-1.5/7B/72B-Base, while the math base models are Qwen2.5-Math-1.5/7B/72B-Base, with the baselines being Qwen2.5-Math-1.5/7B/72B-Instruct. We find that as the model size increases, the performance of our models with base models as backbones approaches that of models with math base as backbones. Specifically, when the Qwen2.5-(Math)-72B-Base is used, the performance gap between Ours (Base) and Ours (Math Base) becomes very marginal (71.84 vs. 71.13). We conjecture that larger models inherently possess better math problem-solving and generalization capability, which diminishes the need for continual pre-training. This finding extends across different model families. Additionally, when comparing models of sizes between 1.5B and 7B, the performance gap between Ours (Base) and Ours (Math Base) is smaller for 7B models (i.e., DeepSeek-7B and Qwen2.5-7B) than it is for Qwen2.5-1.5B. Moreover, we observe that except for Qwen2.5-1.5B, all the models from Ours (Base) outperform the corresponding math-instruct models that use stronger math base models as backbones. This further indicates that smaller models (e.g., 1.5B) rely more on continual pre-training with large math corpus to enhance their math problem-solving capability (full results can be found in Appendix B). 3.6.3. ABLATION STUDIES ON TRAINING STRATEGY In Table 4, we conduct ablation studies on training data and strategies across various backbone models for training our Figure 3. Studies on the impact of using either the base model or the math base model as the backbone on the performance of our AceMath-Instruct models. We compare our models against the corresponding math-instruct baselines across different model types and sizes. Results are the average scores of greedy decoding over the math benchmarks. 3.6.2. BACKBONE MODEL: BASE VS. MATH-BASE In Figure 3, we study the impact of using either the base model (e.g., Qwen2.5-7B-Base) or the math base model (e.g., Qwen2.5-Math-7B-Base) as the backbone on the performance of our AceMath-Instruct models. This study is crucial, as it helps us understand the importance of continual pre-training on large math corpus (i.e., building math base models) for improving the performance on solving math questions after post-training. For DeepSeek-7B, Ours (Base) uses the DeepSeekCoder-7B-Base (Guo et al., 2024) as the backbone model, while Ours (Math Base) uses the DeepSeek-Math-7BBase (Shao et al., 2024) as the backbone model, which conAdvancing Frontier Math Reasoning with Post-Training and Reward Modeling Models AceMath-Instruct Removing all synthetic data Using extra low-quality synthetic data Average 64.19 62.53 62. Table 5. Ablation studies on the synthetic data, exploring the effects of removing all synthetic math SFT data and incorporating additional low-quality synthetic math SFT data. The backbone of AceMath-Instruct is Qwen2.5-7B-Base. Results are average across the seven math benchmark. AceMath-Instruct models. First, we explore the effectiveness of using either GPT-4omini responses or Qwen2.5-Math-72B-Instruct responses individually. Given that our best-performing models leverage responses from both, we analyze the impact of relying solely on one model when constructing general and math SFT data. Notably, even when only GPT-4o-mini responses are available, we achieve strong performance, with just 1% average score drop when Qwen2.5-7B-Base serves as the backbone model. Furthermore, with Llama3.1-8B-Base as the backbone, using responses from GPT-4o-mini, Qwen2.5Math-72B-Instruct, or their combination (AceMath-Instruct) yields comparable results. This indicates that the robustness of our data construction process which minimizes dependence on super powerful math expert models for generating synthetic data. Second, we analyze the effectiveness of different mathspecific samples for math SFT. To study this, we compare AceMath-Instruct trained with 1.6 million math SFT samples (details in 3.3.2) to models trained using all available math SFT samples (2.3 million) or only cross-checked highquality samples (800K). We find that simply increasing the quantity of data or exclusively using high-quality samples does not yield better outcomes. Instead, combining crosschecked high-quality data with additional samples that include diverse range of math questions produces superior results. Third, we study the impact of conducting general SFT before transitioning to math SFT. To explore this, we skip the general SFT step, and conduct math SFT directly using all math-specific samples. We observe that skipping general SFT typically results in an average score drop of approximately 1%, even when using math-base model (e.g., Qwen2.5-Math-72B-Base) as the backbone. The results highlight the effectiveness of conducting general SFT prior to math SFT. 3.6.4. ABLATION STUDIES ON SYNTHETIC DATA As shown in Table 5, we study how synthetic math SFT data affects the results. We compare AceMath-Instruct against two scenarios: one where all one million synthetic data samples are removed and another where an additional 500K low-quality synthetic data are included for training (e.g., lengthy prompts and one type of in-depth evolution that adds constraints). Details of the synthetic math SFT data can be found in 3.2.2. In both scenarios, we observe decline in results, underscoring the importance of not only generating synthetic data but also carefully selecting it for training. Effectively leveraging appropriate synthetic data proves essential for achieving optimal performance. 4. Reward Model Training We train math reward model for AceMath-Instruct, aiming to select more accurate solutions and better reasoning paths. To ensure broad applicability across variety of language models, we curate diverse training dataset. The following sections detail our training methodology, evaluation protocols, and empirical results. 4.1. Reward Training Data Synthesis 4.1.1. INITIAL DATASET CONSTRUCTION We utilize portion of the math SFT dataset (350K) from 3.2.2 to use the prompts and the answers generated by gpt-4o-mini (OpenAI, 2024b) as reference labels. To capture the diversity of model-generated reasoning steps and potential different kinds of reasoning mistakes, we sample four model responses per LLM from set of 14 LLMs, including Llama2-7b-chat (Touvron et al., 2023), Llama3.18/70B-Instruct (Dubey et al., 2024), DeepSeek-math-7binstruct (Shao et al., 2024), Mistral-7B/Mathstral-7B (Jiang et al., 2023), Gemma-2/27b-it (Gemma et al., 2024), and Qwen2/2.5-1.5/7/72B-Instruct (Yang et al., 2024b). We then annotate the model solutions as correct or incorrect by comparing them against the referenced labels using the Qwen-math evaluation toolkit. 3 This process initializes pool of correct and incorrect candidate responses for each problem, which we treat as positive and negative samples that can be further sampled to create paired responses for training. 4.1.2. RESPONSE SCORING AND SELECTION Mathematical problem answers encompass wide range of formats with diverse representations (e.g., [frac{1}{2}, 1/2, 0.5] and [1e-5, 110ˆ{-5}]), and heuristic math evaluation toolkits using SymPy and latex2sympy2 may inevitably result in false negative candidates (i.e., correct answers annotated as incorrect). Such examples in the negative candidates could introduce noise and adversely affect model training. Therefore, instead of randomly sample responses from all candidates, we rank the candidates and apply 3https://github.com/QwenLM/Qwen2.5-Math/ tree/main/evaluation Advancing Frontier Math Reasoning with Post-Training and Reward Modeling score-sorted sampling strategy. Specifically, we use the math reward model Qwen2.5-Math-RM-72B to rank positive and negative candidates for each problem based on their scores. We then randomly sample from top-k positive and bottom-k negative candidates, with set to 14 based on preliminary experiments. Compared to random sampling from all candidates, our ablation study in Table 8 demonstrates the benefits of the score-sorted sampling strategy. In conclusion, we sample total of six response candidates (positive + negative) for each problem, ensuring balanced number of positive and negative responses, and filter out problems where all responses are either correct or incorrect. 4.1.3. ADDRESSING STYLISTIC BIASES LLMs can generate different styles of chain-of-thought reasoning paths when prompted in the zero-shot setting or with few-shot examples (Wei et al., 2022). We observe significant shorter and simple reasoning paths in model outputs for datasets such as MMLU (Hendrycks et al., 2021a) as the model follows the simple 5-shot examples provided in the instruction. To improve reward model performance on such output styles, we create training data using the few-shot prompting approach to generate simple and short reasoning paths for 2,000 multiple-choice problems. In addition, as our ultimate goal is to develop reward model for the AceMath-Instruct model family, we sample set of 30,000 problems and use AceMath-(1.5/7/72B)-Instruct checkpoints to generated responses to create positive and negative pairs for training. In conclusion, our final training dataset consists of 356K problems, each paired with total of six responses (k positive and 6 negative). 4.2. Reward Training Strategy Our reward model architecture adopts outcome reward approach, which introduces linear layer at the top of the language model to project the last token representation into scalar value. We initialize the backbone of the reward model using supervised fine-tuned model (i.e., AceMath-Instruct). Following the training objective established in Qwen2.5Math (Yang et al., 2024b), we construct problem-response pairs with positive (correct) candidates and 6 negative (incorrect) candidates. We compute the list-wise BradleyTerry loss (Bradley & Terry, 1952), which demonstrates computational efficiency compared to pair-wise approaches as shown in Table 8. Lrm(θ) = 1 (6 k) E(x,ypos,yneg) (cid:104) log (cid:0)σ(rθ(x, ypos) rθ(x, yneg))(cid:1)(cid:105) Here, rθ(x, y) represents the output score of the reward model rθ, where denotes the problem and represents the response candidate. The loss function is designed to optimize the models ability to discriminate between correct and incorrect responses by maximizing the margin between positive and negative candidate scores. 4.3. Reward Evaluation Benchmarks 4.3.1. ACEMATH-REWARDBENCH Existing math reward benchmarks lack diversity, both in the types of candidate solutions and the range of difficulty levels in the math questions. To address this, we construct math reward model evaluation benchmark, AceMathRewardBench, which contains 7 datasets and use 8 different LLMs to generate solutions for robust evaluation. The benchmark use the best-of-N (BoN or rm@n) metric, methodology extensively used in literature (Cobbe et al., 2021; Lightman et al., 2023; Yang et al., 2024b). The primary objective of the reward model is to select the highest reward scored model response from candidate set of and calculate the corresponding problem-solving rate for each math benchmark (7 datasets) used in 3.4.2. We adopt rm@8 metric following the Qwen2.5-Math evaluation protocol, optimizing computational efficiency during the inference stage. To ensure robust and statistically reliable benchmark performance, we implement two design principles: 1) diverse model distribution: we sample 8 responses from each model in set of mathematical and generalpurpose LLMs (i.e., Qwen2.5-Math-7/72B-Instruct (Yang et al., 2024b), Qwen2-Math-7/72B-Instruct (Yang et al., 2024a), Llama-3.1-8/70B-Instruct (Dubey et al., 2024), DeepSeek-Math-7B-Instruct (Shao et al., 2024), Mathtral7B-v0.1 (Jiang et al., 2023)), mitigating potential modelspecific style biases; 2) we compute accuracy metrics by averaging results across 100 random seeds, reducing result variance and enhancing reproducibility. In total, each problem in the benchmark contains total of 64 candidate responses from 8 LLMs. We then randomly sample 8 responses from these 64 candidates, compute the rm@8 result, and average the final accuracy over 100 random seeds. Different from the Math SFT evaluation, we use the MATH500 (Lightman et al., 2023), subset of 500 problems sampled from the MATH dataset (Hendrycks et al., 2021b) following the prior work such as PRM800K (Lightman et al., 2023) and RewardBench (Lambert et al., 2024). 4.3.2. REWARDBENCH (MATH500) AND REWARDMATH Apart from our own benchmarks, we also evaluate on RewardBench (Lambert et al., 2024) (MATH500) and RewardMath (Kim et al., 2024) to report the accuracy of selecting the correct solution from list of candidates for each problem in MATH500 (Lightman et al., 2023). The primary difference between these two benchmarks lies in the candi10 Advancing Frontier Math Reasoning with Post-Training and Reward Modeling Model GSM8K MATH Minerva Math GaoKao 2023 En Olympiad Bench College Math MMLU STEM majority@8 Internlm2-7b-reward Internlm2-20b-reward Skywork-Reward-Llama-3.1-8B-v0.2 Skywork-Reward-Gemma-2-27B-v0.2 Skywork-o1-Open-PRM-Qwen-2.5-7B Qwen2.5-Math-RM-72B AceMath-7B-RM (Ours) AceMath-72B-RM (Ours) pass@8 (Oracle) 96.22 95.26 95.10 95.64 95.94 96.92 96.61 96.66 97.23 98.86 83.11 78.96 76.53 74.16 74.90 86.64 86. 85.47 86.72 91.84 41.20 36.25 37.69 39.11 39.37 41.00 43.60 41.96 45.06 56.18 68.21 67.51 66.63 67.16 66.96 72.34 73. 73.82 74.69 82.09 42.69 40.49 40.12 39.10 39.07 46.50 47.21 46.81 49.23 59.00 45.01 43.88 42.57 44.58 45.46 46.30 47. 46.37 46.79 56.38 78.21 75.42 70.60 76.52 78.20 74.01 84.24 80.78 87.01 96.15 Avg. 64.95 62.54 61.32 62.32 62.84 66.24 68.46 67.41 69.53 77.21 Table 6. Reward model evaluation on AceMath-RewardBench. The average results (rm@8) of reward models on math benchmarks, randomly sample 8 responses from 64 candidates with 100 random seeds. Response candidates are generated from pool of 8 LLMs (Qwen{2/2.5}-Math-{7/72}B-Instruct, Llama-3.1-{8/70}B-Instruct, Mathtral-7B-v0.1, deepseek-math-7b-instruct). date sets: RewardBench uses one correct (human-written) solution and one incorrect candidate (generated by GPT-4), while RewardMath uses one correct (a GPT-4 rewrite) and nine incorrect candidates (generated by models). Kim et al. (2024) highlight significant distributional shift between human-written solutions and machine-generated ones as the prior ones are typically shorter, more concise, and contain fewer details. This difference in style and content may partially explain why saturated accuracy exceeding 95% on the RewardBench. To address this limitation and better assess the robustness of reward models, they propose RewardMath, which introduces more challenging evaluation setup and show that most reward models struggle significantly with this new benchmark, achieving accuracies of only around 30% or lower. 4.4. Experiments of Reward models 4.4.1. HYPERPARAMETERS We use the AceMath-7B/72B-Instruct model as the backbone to train the outcome reward model: AceMath-RM7/72B. The model is trained using AdamW (Kingma, 2014; Loshchilov, 2017) for 2 epochs with learning rate of {5e-6, 2e-6}, using cosine learning rate scheduler and an effective batch size of 256. Training is conducted on 8 H100 GPUs for the 7B model and 256 H100 GPUs for the 72B model. 4.4.2. BASELINES For mathematical reward modeling, we compare with current state-of-the-art outcome reward model Qwen2.5-MathRM-72B (Yang et al., 2024b) and process reward model Skywork-o1-Open-PRM-Qwen-2.5-7B (Skywork-o1, 2024). We also include majority@8 (majority voting) baseline and the pass@8 (any one of the 8 is correct) as an oracle reward model to measure the upper bound of this benchmark. Additionally, we incorporate general reward models topranked on RewardBench, including Skywork-Reward (Liu et al., 2024a) and Internlm2-reward (Cai et al., 2024). It is noteworthy that while these models are not exclusively trained for mathematical domains, substantial portion of their training data encompasses mathematical content. For instance, Skywork-Reward (Liu et al., 2024a) uses 50% math data for training. 4.4.3. RESULTS ON ACEMATH-REWARDBENCH In Table 6, we show that our AceMath-72B-RM achieves the state-of-the-art rm@8 accuracy on average of AceMathRewardBench, outperforming the Qwen2.5-Math-RM-72B by 1% absolute (69.53 vs 68.46) and on 6 out of 7 datasets. We show the 7B variant achieves 67.41 accuracy on average and demonstrates the benefits of model size scaling from 7B to 72B, especially on datasets require college-level STEM knowledge such as Minerva Math (41.96 45.06) and MMLU STEM (80.78 87.01). Comparing to other reward model baselines, the 7B outperform Internlm2 and Skywork-Reward by large margin as our benchmark reveal these reward model even underperform the majority voting baseline. Nevertherless, we note that there remains considerable room for improvement as indicated by the gap between the reward model and pass@8 oracle accuracy. 4.4.4. RESULTS ON REWARDBENCH AND REWARDMATH In Table 7, we demonstrate that our AceMath-72B-RM achieves state-of-the-art accuracy on RewardMATH. While many reward models (e.g., ArmoRM (Wang et al., 2024a), Internlm2) achieve 95%+ accuracy on the RewardBench MATH500 split, their accuracy drops significantly on Re11 Advancing Frontier Math Reasoning with Post-Training and Reward Modeling Model Random LLM-as-a-Judge Claude-3.5-Sonnet GPT-4o-2024-05-13 Classifier-based Math-Shepherd-Mistral-7B ArmoRM-Llama3-8B-v0.1 Skywork-Reward-Llama-3.1-8B Internlm2-20b-reward Internlm2-7b-reward Skywork-o1-Open-PRM-7B Qwen2.5-Math-RM-72B AceMath-7B-RM (Ours) AceMath-72B-RM (Ours) RewardBench MATH RewardMath MATH500 50.00 70.70 72.50 94.41 98.70 96.87 95.10 94.90 78.52 95.97 92.62 97.09 10. 15.32 25.98 17.18 20.50 22.15 33.95 37.27 51.34 68.53 57.76 68.94 Table 7. The accuracy of reward models on RewardBench (MATH500) (Lambert et al., 2024) and RewardMATH (Kim et al., 2024). : Results are copied from RewardMATH. Bold: top-1. Underline: top-2 accuracy. Model AceMath-7B-RM Backbone: Qwen2.5-Math-7B-Instruct Data: Random sampling Loss: Pairwise BT Loss: Cross-entropy Classification Loss: MSE Regression AceMath-72B-RM Backbone: Qwen2.5-Math-72B-Instruct Loss: Cross-entropy Classification AceMath-RewardBench 67.41 66.93 67.07 67.33 66.93 66.79 69.53 69.09 68. Table 8. Ablation study of AceMath-7/72B-RM on AceMathRewardBench (Backbone: AceMath-7/72B-Instruct; Data: reward score-sorted sampling; Loss: listwise Bradley-Terry. wardMATH, ranging from only 20% to 37%. We found Skywork-PRM model performs much better on RewardMATH (51.34) but worse on RewardBench (78.5). This may be due to the lack of reasoning steps typically found in human-written solutions, and as result, our AceMath7B-RM outperforms it on both benchmarks. In conclusion, these evaluation results highlight the benefits of training on diverse, model-generated solutions to mitigate, though not entirely eliminate, out-of-distribution generalization challenges. 4.4.5. ABLATION STUDIES In Table 8, we conduct ablation studies on the model backbone, data sampling method, and different loss functions used to train the reward model. First, we found that using AceMath-7B-Instruct as the backbone model for training the model consistently outperforms Qwen2.5-Math-7B-Instruct on average of 7 datasets, with similar performance gap Figure 4. rm@k evaluation on average accuracy of 7 datasets for AceMath-7B-Instruct. observed at the 72B scale. Secondly, we observed that employing reward score-sorted sampling (4.1.2) during the data construction process improves average accuracy compared to random sampling. This highlights the benefits of filtering out noisy labels when heuristic evaluation toolkits produce false negative errors. Lastly, we experimented with different loss functions. We found that using pairwise Bradley-Terry loss achieves comparable accuracy to the listwise loss approach, however requiring 3.7 more training time using 8 H100 GPUs. Additionally, training classifier using cross-entropy loss or regression model using mean squared error (MSE) loss both resulted in lower accuracy. similar performance gap was also observed at the 72B scale for the cross-entropy classification approach. Since the data is constructed for the listwise BT approach, where each problem consists of six responses, this also leads to 3.8 times more compute hours on 8 GPUs. 4.4.6. RESULTS ON RM@k In Figure 4, we present comparison between AceMath72B-RM and Qwen2.5-Math-RM-72B on rm@k (k = 8, 16, 32, 64, 128) across the seven datasets listed in Table 6, using samples generated by AceMath-7B-Instruct. We report the average accuracy across these seven datasets, each with 10 different random seeds. First, we find that using AceMath-72B-RM to score the outputs from AceMath-7B-Instruct consistently improves the average accuracy, increasing from 72.6 to 74.4 as rises from 8 to 128. Second, we observe that AceMath-RM consistently outperforms Qwen2.5-Math-RM in scoring outputs generated from AceMath-7B-Instruct, and this improvement becomes larger as increases. Furthermore, we compare the performance of AceMath12 Advancing Frontier Math Reasoning with Post-Training and Reward Modeling Figure 5. Learning curves for reward model training. All models are trained from Qwen2.5-Instruct family. 72B-RM paired with AceMath-Instruct to Qwen2.5-MathRM-72B paired with Qwen2.5-Math-Instruct. As shown in Figure 1, the AceMath combination consistently outperforms its Qwen2.5 counterpart in terms of rm@8, on average, for both the 7B and 72B models. Remarkably, we find that the our AceMath-7B model even outperforms the Qwen2.5-Math-72B in rm@8, showing the potential of smaller model when paired with carefully designed reward model. 4.4.7. LEARNING CURVES OF REWARD MODEL TRAINING In Figure 5, we aim to understand how reward modeling accuracy improves as we increase model size and use additional data. We find distinct patterns in the interplay between model size and data scaling. In general on simpler dataset such as GSM8K, all model sizes (ranging from 0.5B to 32B parameters) exhibit steady improvements as training proceeds, with larger models achieving higher accuracy. In contrast, on the more challenging datasets, which requires college-level knowledge, such as Minerva Math, MMLU STEM, and OlympiadBench, model size emerges as critical factor: smaller models (e.g., 0.5B, 1.5B) show negligible improvement despite increased data, whereas larger models (e.g., 14B, 32B) achieve better accuracy gains. These results suggest that increasing model size provides the greatest benefit, whereas the advantages of increasing data appear less pronounced. Our experiments use Qwen2.5-Instruct (Yang et al., 2024a) model family. instead of Qwen2.5-MathInstruct, as it provides more comprehensive set of models with different sizes. All models are trained for one epoch only. 5. Conclusion In this work, we present AceMath, series of frontierclass math instruct and reward models. We demonstrate that our AceMath-7B-Instruct significantly surpasses the previous best-in-class Qwen2.5-Math-7B-Instruct across comprehensive math reasoning benchmarks, and it performs slightly worse than 10 larger Qwen2.5-Math-72Instruct (67.2 vs. 68.2). Remarkably, our AceMath-72BInstruct greatly outperforms Qwen2.5-Math-72-Instruct, GPT-4o and Claude-3.5 Sonnet. Additionally, we construct AceMath-RewardBench, comprehensive benchmark designed to evaluate math reward models across diverse range of datasets and difficulty levels. We show that our AceMath-72B-RM consistently outperforms state-of-theart reward models, including Qwen2.5-Math-RM-72B and Skywork-o1-Open-PRM-Qwen-2.5-7B, on various math reward benchmarks. Furthermore, when combining AceMath72B-Instruct with AceMath-72B-RM, we achieve the highest average rm@8 score across the math reasoning benchmarks. To advance open research in the field, we will open source the model weights for both AceMath-Instruct and AceMath-RM, along with the complete training data used throughout their development."
        },
        {
            "title": "References",
            "content": "Adler, B., Agarwal, N., Aithal, A., Anh, D. H., Bhattacharya, P., Brundyn, A., Casper, J., Catanzaro, B., Clay, S., Cohen, J., et al. Nemotron-4 340b technical report. arXiv preprint arXiv:2406.11704, 2024. Akter, S. N., Prabhumoye, S., Kamalu, J., Satheesh, S., Nyberg, E., Patwary, M., Shoeybi, M., and Catanzaro, B. 13 Advancing Frontier Math Reasoning with Post-Training and Reward Modeling Mind: Math informed synthetic dialogues for pretraining llms. arXiv preprint arXiv:2410.12881, 2024. Anthropic. URL claude-3-5-sonnet. Introducing claude 3.5 sonnet. 2024. https://www.anthropic.com/news/ Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Azerbayev, Z., Schoelkopf, H., Paster, K., Santos, M. D., McAleer, S., Jiang, A. Q., Deng, J., Biderman, S., and Welleck, S. Llemma: An open language model for mathematics. arXiv preprint arXiv:2310.10631, 2023. Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKinnon, C., et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022. Dong, H., Xiong, W., Pang, B., Wang, H., Zhao, H., Zhou, Y., Jiang, N., Sahoo, D., Xiong, C., and Zhang, T. Rlhf workflow: From reward modeling to online rlhf. TMLR, 2024. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The Llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Gemma, Riviere, M., Pathak, S., Sessa, P. G., Hardin, C., Bhupatiraju, S., Hussenot, L., Mesnard, T., Shahriari, B., Rame, A., et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. Glaive-AI. GlaiveCodeAssistant, 2023."
        },
        {
            "title": "URL",
            "content": "https://huggingface.co/datasets/ glaiveai/glaive-code-assistant-v2. Bradley, R. A. and Terry, M. E. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. ISSN 00063444, 14643510. URL http://www.jstor. org/stable/2334029. Guo, D., Zhu, Q., Yang, D., Xie, Z., Dong, K., Zhang, W., Chen, G., Bi, X., Li, Y., et al. Deepseek-coder: When the large language model meets programmingthe rise of code intelligence. arXiv preprint arXiv:2401.14196, 2024. Cai, Z., Cao, M., Chen, H., Chen, K., Chen, K., Chen, X., Chen, X., Chen, Z., Chen, Z., Chu, P., et al. Internlm2 technical report. arXiv preprint arXiv:2403.17297, 2024. Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. D. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Chen, Z., Wang, W., Tian, H., Ye, S., Gao, Z., Cui, E., Tong, W., Hu, K., Luo, J., Ma, Z., et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E., Stoica, I., and Xing, E. P. Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality, 2023. URL https://lmsys.org/blog/ 2023-03-30-vicuna/. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. He, C., Luo, R., Bai, Y., Hu, S., Thai, Z. L., Shen, J., Hu, J., Han, X., Huang, Y., Zhang, Y., et al. Olympiadbench: challenging benchmark for promoting agi with olympiadlevel bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2020. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021a. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021b. Hui, B., Yang, J., Cui, Z., Yang, J., Liu, D., Zhang, L., Liu, T., Zhang, J., Yu, B., Lu, K., et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. Dai, W., Lee, N., Wang, B., Yang, Z., Liu, Z., Barker, J., Rintamaki, T., Shoeybi, M., Catanzaro, B., and Ping, W. NVLM: Open frontier-class multimodal LLMs. arXiv preprint arXiv:2409.11402, 2024. Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. 14 Advancing Frontier Math Reasoning with Post-Training and Reward Modeling Kim, S., Kang, D., Kwon, T., Chae, H., Won, J., Lee, D., and Yeo, J. Evaluating robustness of reward models for mathematical reasoning. arXiv preprint arXiv:2410.01729, 2024. Liu, Y., Yao, Z., Min, R., Cao, Y., Hou, L., and Li, J. Rmbench: Benchmarking reward models of language models with subtlety and style. arXiv preprint arXiv:2410.16184, 2024b. Kingma, D. P. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Lambert, N., Pyatkin, V., Morrison, J., Miranda, L., Lin, B. Y., Chandu, K., Dziri, N., Kumar, S., Zick, T., Choi, Y., et al. Rewardbench: Evaluating reward models for language modeling. arXiv preprint arXiv:2403.13787, 2024. Lewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H., Ramasesh, V., Slone, A., Anil, C., Schlag, I., Gutman-Solo, T., et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:38433857, 2022. Li, C., Yuan, Z., Yuan, H., Dong, G., Lu, K., Wu, J., Tan, C., Wang, X., and Zhou, C. Mugglemath: Assessing the impact of query and response augmentation on math reasoning. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1023010258, 2024a. Li, J., Beeching, E., Tunstall, L., Lipkin, B., Soletskyi, R., Huang, S. C., Rasul, K., Yu, L., Jiang, A., Shen, Z., Qin, Z., Dong, B., Zhou, L., Fleureau, Y., Lample, G., and Polu, S. Numina- [https://huggingface.co/AI-MO/ math. NuminaMath-CoT](https://github.com/ project-numina/aimo-progress-prize/ blob/main/report/numina_dataset.pdf), 2024b. Lian, W., Wang, G., Goodson, B., Pentland, E., Cook, A., Vong, C., and Teknium. Slimorca: An open dataset of gpt-4 augmented flan reasoning traces, with verification, 2023. URL https://https://huggingface. co/Open-Orca/SlimOrca. Liao, M., Luo, W., Li, C., Wu, J., and Fan, K. Mario: Math reasoning with code interpreter outputa reproducible pipeline. arXiv preprint arXiv:2401.08190, 2024. Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. Liu, Z., Ping, W., Roy, R., Xu, P., Lee, C., Shoeybi, M., and Catanzaro, B. ChatQA: Surpassing gpt-4 on conversational QA and RAG. In NeurIPS, 2024c. Loshchilov, I. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Lu, J., Dou, Z., Wang, H., Cao, Z., Dai, J., Wan, Y., Huang, Y., and Guo, Z. Autocv: Empowering reasoning with automated process labeling via confidence variation. arXiv preprint arXiv:2405.16802, 2024. Luo, L., Liu, Y., Liu, R., Phatale, S., Lara, H., Li, Y., Shu, L., Zhu, Y., Meng, L., Sun, J., et al. Improve mathematical reasoning in language models by automated process supervision. arXiv preprint arXiv:2406.06592, 2024. Luo, Z., Xu, C., Zhao, P., Sun, Q., Geng, X., Hu, W., Tao, C., Ma, J., Lin, Q., and Jiang, D. Wizardcoder: Empowering code large language models with evol-instruct. arXiv preprint arXiv:2306.08568, 2023. Mistral. MathΣtral, 2024. URL https://mistral. ai/news/mathstral/. Mitra, A., Khanpour, H., Rosset, C., and Awadallah, A. Orca-math: Unlocking the potential of slms in grade school math. arXiv preprint arXiv:2402.14830, 2024. Mukherjee, S., Mitra, A., Jawahar, G., Agarwal, S., Palangi, H., and Awadallah, A. Orca: Progressive learning from complex explanation traces of gpt-4. arXiv preprint arXiv:2306.02707, 2023. OpenAI. Hello GPT-4o, 2024a. URL https://openai. com/index/hello-gpt-4o/. OpenAI. GPT-4o mini: advancing cost-efficient intelligence, 2024b. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. Peng, B., Li, C., He, P., Galley, M., and Gao, J. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023. Liu, C. Y., Zeng, L., Liu, J., Yan, R., He, J., Wang, C., Yan, S., Liu, Y., and Zhou, Y. Skywork-reward: Bag of tricks for reward modeling in llms. arXiv preprint arXiv:2410.18451, 2024a. Setlur, A., Nagpal, C., Fisch, A., Geng, X., Eisenstein, J., Agarwal, R., Agarwal, A., Berant, J., and Kumar, A. Rewarding progress: Scaling automated process verifiers for llm reasoning. arXiv preprint arXiv:2410.08146, 2024. 15 Advancing Frontier Math Reasoning with Post-Training and Reward Modeling Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Zhang, M., Li, Y., Wu, Y., and Guo, D. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Shen, J. T., Yamashita, M., Prihar, E., Heffernan, N., Wu, X., Graff, B., and Lee, D. Mathbert: pre-trained language model for general nlp tasks in mathematics education. In NeurIPS 2021 Math AI for Education Workshop, 2021. Skywork-o1. https:// Skywork-o1 open series. huggingface.co/Skywork, November 2024. URL https://huggingface.co/Skywork. Tang, Z., Zhang, X., Wang, B., and Wei, F. Mathscale: Scaling instruction tuning for mathematical reasoning. In Forty-first International Conference on Machine Learning, 2024. Tao, T. Embracing change and resetting expectations, URL https://unlocked.microsoft. 2023. com/ai-anthology/terence-tao/. Teknium. GPTeacher-General-Instruct, 2023. URL https://huggingface.co/datasets/ teknium/GPTeacher-General-Instruct. The-Vicuna-Team. ShareGPT-Vicuna, 2023. https://huggingface.co/datasets/ anon8231489123/ShareGPT_Vicuna_ unfiltered. URL Thailand, August 2024b. Association for Computational Linguistics. Wang, Y., Ma, X., Zhang, G., Ni, Y., Chandra, A., Guo, S., Ren, W., Arulraj, A., He, X., Jiang, Z., et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. arXiv preprint arXiv:2406.01574, 2024c. Wang, Z., Xia, R., and Liu, P. Generative ai for math: Part imathpile: billion-token-scale pretraining corpus for math. arXiv preprint arXiv:2312.17120, 2023. Wang, Z., Bukharin, A., Delalleau, O., Egert, D., Shen, G., Zeng, J., Kuchaiev, O., and Dong, Y. Helpsteer2preference: Complementing ratings with preferences, 2024d. URL https://arxiv.org/abs/2410. 01257. Wang, Z., Dong, Y., Delalleau, O., Zeng, J., Shen, G., Egert, D., Zhang, J. J., Sreedhar, M. N., and Kuchaiev, O. Helpsteer2: Open-source dataset for training top-performing reward models, 2024e. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Toshniwal, S., Du, W., Moshkov, I., Kisacanin, B., Ayrapetyan, A., and Gitman, I. Openmathinstruct-2: Accelerating ai for math with massive open-source instruction data. arXiv preprint arXiv:2410.01560, 2024. Wei, Y., Wang, Z., Liu, J., Ding, Y., and Zhang, L. Magicoder: Empowering code generation with oss-instruct. In Forty-first International Conference on Machine Learning, 2024. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023. Uesato, J., Kushman, N., Kumar, R., Song, F., Siegel, N., Wang, L., Creswell, A., Irving, G., and Higgins, I. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022. Wang, H., Xiong, W., Xie, T., Zhao, H., and Zhang, T. Interpretable preferences via multi-objective reward modeling and mixture-of-experts. In EMNLP, 2024a. Wang, P., Li, L., Shao, Z., Xu, R., Dai, D., Li, Y., Chen, D., Wu, Y., and Sui, Z. Math-shepherd: Verify and reinforce LLMs step-by-step without human annotations. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Bangkok, Xu, C., Sun, Q., Zheng, K., Geng, X., Zhao, P., Feng, J., Tao, C., Lin, Q., and Jiang, D. WizardLM: Empowering large pre-trained language models to follow complex instructions. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=CfXh93NDgH. Yang, A., Yang, B., Hui, B., Zheng, B., Yu, B., Zhou, C., Li, C., Li, C., Liu, D., Huang, F., et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024a. Yang, A., Zhang, B., Hui, B., Gao, B., Yu, B., Li, C., Liu, D., Tu, J., Zhou, J., Lin, J., et al. Qwen2.5-math technical report: Toward mathematical expert model via selfimprovement. arXiv preprint arXiv:2409.12122, 2024b. Ying, H., Zhang, S., Li, L., Zhou, Z., Shao, Y., Fei, Z., Ma, Y., Hong, J., Liu, K., Wang, Z., et al. InternLM-Math: Open math large language models toward verifiable reasoning. arXiv preprint arXiv:2402.06332, 2024. Advancing Frontier Math Reasoning with Post-Training and Reward Modeling Yu, L., Jiang, W., Shi, H., Yu, J., Liu, Z., Zhang, Y., Kwok, J. T., Li, Z., Weller, A., and Liu, W. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023. Yu, Y., Chen, Z., Zhang, A., Tan, L., Zhu, C., Pang, R. Y., Qian, Y., Wang, X., Gururangan, S., Zhang, C., et al. Selfgenerated critiques boost reward modeling for language models. arXiv preprint arXiv:2411.16646, 2024. Yuan, L., Cui, G., Wang, H., Ding, N., Wang, X., Deng, J., Shan, B., Chen, H., Xie, R., Lin, Y., et al. Advancing llm reasoning generalists with preference trees. arXiv preprint arXiv:2404.02078, 2024. Yue, X., Qu, X., Zhang, G., Fu, Y., Huang, W., Sun, H., Su, Y., and Chen, W. Mammoth: Building math generalist models through hybrid instruction tuning. ICLR, 2024a. Yue, X., Zheng, T., Zhang, G., and Chen, W. Mammoth2: Scaling instructions from the web. NeurIPS, 2024b. Zhang, F., Li, C., Henkel, O., Xing, W., Baral, S., Heffernan, N., and Li, H. Math-llms: Ai cyberinfrastructure with pretrained transformers for math education. International Journal of Artificial Intelligence in Education, pp. 124, 2024a. Zhang, H., Wang, P., Diao, S., Lin, Y., Pan, R., Dong, H., Zhang, D., Molchanov, P., and Zhang, T. Entropyregularized process reward model, 2024b. Zhang, L., Hosseini, A., Bansal, H., Kazemi, M., Kumar, A., and Agarwal, R. Generative verifiers: Reward modeling as next-token prediction. arXiv preprint arXiv:2408.15240, 2024c. Zheng, C., Zhang, Z., Zhang, B., Lin, R., Lu, K., Yu, B., Liu, D., Zhou, J., and Lin, J. Processbench: Identifying process errors in mathematical reasoning. arXiv preprint arXiv:2412.06559, 2024. Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36: 4659546623, 2023. Advancing Frontier Math Reasoning with Post-Training and Reward Modeling A. AIME 2024 & AMC 2023 Results Models AIME 2024 AMC 2023 Llama-3.1-405B-Instruct Claude 3.5 Sonnet (2024-1022) OpenMath2-Llama3.1-8B OpenMath2-Llama3.1-70B Qwen2.5-Math-1.5B-Instruct Qwen2.5-Math-7B-Instruct Qwen2.5-Math-72B-Instruct AceMath-1.5B-Instruct AceMath-7B-Instruct AceMath-72B-Instruct 5/30 4/30 3/30 4/30 3/30 5/30 9/30 4/30 6/30 6/30 20/40 21/40 16/40 20/40 24/40 25/40 28/40 25/40 26/40 28/40 Table 9. Greedy decoding results of AceMath-Instruct on AIME 2024 and AMC 2023. Table 9 shows the greedy decoding results on AIME 2024 and AMC 2023. We find that the AceMath-1.5B/7B-Instruct models slightly outperform Qwen2.5-Math-1.5B/7B-Instruct on both datasets, while AceMath-72B-Instruct falls short of Qwen2.5-Math-72B-Instructs performance on AIME 2024. Given that AIME 2024 contains challenging math problems comparable to pre-Olympiad levels, these results indicate that there is room to improve AceMath-Instruct to better address varying levels of mathematical difficulty. B. AceMath-Instruct Using Different Backbone Models Models GSM8K MATH Minerva Math GaoKao 2023 En Olympiad Bench College Math MMLU STEM DeepSeek-Math-7B-RL Ours (backbone: DeepSeek-Coder-7B-Base) Ours (backbone: DeepSeek-Math-7B-Base) Llama-3.1-8B-Instruct OpenMath2-Llama3.1-8B Ours (backbone: Llama3.1-8B-Base) Qwen2.5-Math-1.5B-Instruct Ours (backbone: Qwen2.5-1.5B-Base) Ours (backbone: Qwen2.5-Math-1.5B-Base) Qwen2.5-Math-7B-Instruct Ours (backbone: Qwen2.5-7B-Base) Ours (backbone: Qwen2.5-Math-7B-Base) Qwen2.5-Math-72B-Instruct Ours (backbone: Qwen2.5-72B-Base) Ours (backbone: Qwen2.5-Math-72B-Base) 88.20 83.85 85.06 84.50 91.70 91.51 84.80 80.89 86.95 95.20 93.56 93.71 95.90 95.99 96. 52.40 59.72 66.86 51.90 67.80 69.06 75.80 64.59 76.84 83.60 77.10 83.14 85.90 85.06 86.10 20.60 29.78 40. 21.70 16.91 31.99 29.40 30.51 41.54 37.10 43.38 51.11 44.10 54.04 56.99 43.60 53.51 56.62 38.40 53.76 59. 65.50 53.25 64.42 66.80 65.19 68.05 71.90 73.25 72.21 19.00 24.59 29.63 15.40 28.00 32.00 38.10 27.11 33. 41.60 37.78 42.22 49.00 46.96 48.44 37.50 44.64 48.94 33.80 46.13 49.08 47.70 47.80 54.36 46.80 54.90 56. 49.50 57.10 57.24 64.80 55.95 65.53 60.50 46.02 67.94 57.50 58.62 62.04 71.90 77.41 75.32 80.80 85.48 85. Avg. 46.59 50.29 56.10 43.74 50.08 57.33 56.97 51.82 59.99 63.29 64.19 67.17 68.16 71.13 71. Table 10. Greedy decoding results of AceMath-Instruct across different backbone models. Table 10 shows the full results of AceMath-Instruct using various models as backbone models. Additionally, we include the results for Llama3.1-8B-Base as the backbone model and compare our model to OpenMath2-Llama3.1-8B (Toshniwal et al., 2024) that also uses Llama3.1-8B-Base as its backbone model. We find that except for our 1.5B model based on Qwen2.5-1.5B-Base, all our models, including those built on base models, outperform their respective strong baselines, often by significant margin. C. Synthetic Prompt Generation for Math SFT In this section, we describe the prompt we provided to GPT-4o-mini (2024-0718) for generating synthetic prompts tailored to math SFT. We utilize the in-breath evolution and in-depth evolution prompts inspired from Xu et al. (2024). 18 Advancing Frontier Math Reasoning with Post-Training and Reward Modeling C.1. In-Breath Evolution We use the following prompt to generate more diverse math questions. You are good math question creator. Your objective is to draw inspiration from the #Given MATH Question# to create brand new math question. This new math question should be distinctly different from the #Given MATH Question# and be even more unique. The length and difficulty level of the #Created MATH Question# should be similar to those of the #Given MATH Question#. The #Created MATH Question# must be solvable and understandable by humans. #Given MATH Question#: {given_math_question} #Created MATH Question#: C.2. In-Depth Evolution We use the following prompt to generate more challenging math questions. You are good math question creator. Your objective is to draw inspiration from the #Given MATH Question# to create brand new math question. This new math question should be more complex and challenging than the #Given MATH Question#. The #Created MATH Question# must be solvable and understandable by humans. #Given MATH Question#: {given_math_question} #Created MATH Question#: Moreover, we find that the following prompt that requires to add constraints to the given prompt could result in unsolvable or overly challenging math questions. This, in turn, can lead to incorrect answers being included in the training data, ultimately degrading model performance. You are good math question creator. Your objective is to rewrite the #Given MATH Question# into brand new but more complex version. You can complicate the #Given MATH Question# by introducing additional constraints and requirements. The #Created MATH Question# must be solvable and understandable by humans. #Given MATH Question#: {given_math_question} #Created MATH Question#:"
        }
    ],
    "affiliations": [
        "NVIDIA"
    ]
}