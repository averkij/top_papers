{
    "paper_title": "ORIGEN: Zero-Shot 3D Orientation Grounding in Text-to-Image Generation",
    "authors": [
        "Yunhong Min",
        "Daehyeon Choi",
        "Kyeongmin Yeo",
        "Jihyun Lee",
        "Minhyuk Sung"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce ORIGEN, the first zero-shot method for 3D orientation grounding in text-to-image generation across multiple objects and diverse categories. While previous work on spatial grounding in image generation has mainly focused on 2D positioning, it lacks control over 3D orientation. To address this, we propose a reward-guided sampling approach using a pretrained discriminative model for 3D orientation estimation and a one-step text-to-image generative flow model. While gradient-ascent-based optimization is a natural choice for reward-based guidance, it struggles to maintain image realism. Instead, we adopt a sampling-based approach using Langevin dynamics, which extends gradient ascent by simply injecting random noise--requiring just a single additional line of code. Additionally, we introduce adaptive time rescaling based on the reward function to accelerate convergence. Our experiments show that ORIGEN outperforms both training-based and test-time guidance methods across quantitative metrics and user studies."
        },
        {
            "title": "Start",
            "content": "ORIGEN: Zero-Shot 3D Orientation Grounding in Text-to-Image Generation Yunhong Min Daehyeon Choi"
        },
        {
            "title": "Jihyun Lee Minhyuk Sung",
            "content": "KAIST {dbsghd363,daehyeonchoi,aaaaa,jyun.lee,mhsung}@kaist.ac.kr 5 2 0 2 8 2 ] . [ 1 4 9 1 2 2 . 3 0 5 2 : r Figure 1. 3D orientaion-grounded text-to-image generation results of ORIGEN. We present ORIGEN, the first zero-shot method for 3D orientation grounding in text-to-image generation across multiple objects and diverse categories. ORIGEN generates high-quality images that are accurately aligned with the grounding orientation conditions, indicated by the colored arrows, and the input text prompts."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction We introduce ORIGEN, the first zero-shot method for 3D orientation grounding in text-to-image generation across multiple objects and diverse categories. While previous work on spatial grounding in image generation has mainly focused on 2D positioning, it lacks control over 3D orientation. To address this, we propose rewardguided sampling approach using pretrained discriminative model for 3D orientation estimation and onestep text-to-image generative flow model. While gradientascent-based optimization is natural choice for rewardbased guidance, it struggles to maintain image realism. Instead, we adopt sampling-based approach using Langevin dynamics, which extends gradient ascent by simply injecting random noiserequiring just single additional line of code. Additionally, we introduce adaptive time rescaling based on the reward function to accelerate convergence. Our experiments show that ORIGEN outperforms both training-based and test-time guidance methods across quantitative metrics and user studies. Project Page: https://origen2025.github.io. *Equal contribution. Controllability is key aspect of generative models, enabling precise, user-driven outputs. In image generation, spatial grounding ensures structured and semantically meaningful results by incorporating conditions that cannot be fully specified through text alone. Recent research integrating spatial instructions, such as bounding boxes [3, 36, 37, 40, 50] and segmentation masks [14, 14, 40], has shown promising results. While these works have advanced 2D spatial control, particularly positional constraints, 3D spatial grounding remains largely unexplored. In particular, orientation is essential for defining an objects spatial pose [8, 22, 23, 60, 62, 65, 66], yet its integration into conditioning remains an open challenge. few existing methods, such as Zero-1-to-3 [46] and Continuous 3D Words [11], support orientation-conditioned image generation. However, Zero-1-to-3 enables only relative orientation control with respect to reference foreground image, while Continuous 3D Words is limited to single-object images and supports only half-front azimuth control. Moreover, all these models lack realism because they are trained on synthetic data, i.e., multi-view renderings of centered 3D objects, as real-world training images with accurate per-object orientation annotations are not publicly available. In addition, OrientDream [24] supports orientation control via text prompts, but it is restricted to four primitive azimuths (front, left, back, right) and is also limited to single-object images. To overcome these limitations, we propose ORIGEN, the first method for generalizable 3D orientation grounding in real-world images across multiple objects and diverse categories. We introduce zero-shot approach that leverages test-time guidance from OrientAnything [67], foundational discriminative model for 3D orientation estimation. Specifically, using pretrained one-step text-to-image generative model [34] that maps latent vector to real image, along with reward function defined by the discriminative model, our goal is to find latent vector whose corresponding real image yields high reward. natural approach for this search is gradient-ascentbased optimization [16], but it struggles to keep the latent distribution aligned with the prior (a standard Gaussian), leading to loss of realism in the generated images. To address this, we introduce sampling-based approach that balances reward maximization with adherence to the prior latent distribution. Specifically, we propose novel method that simulates Langevin dynamics, where the drift term is determined by our orientation-grounding reward. We further show that its EulerMaruyama discretization simplifies to surprisingly simple formulationan extension of standard gradient ascent with random noise injection, which can be implemented in single line of code. To further enhance efficiency, we introduce novel time-rescaling method that adjusts timesteps based on the current reward value, accelerating convergence. Since no existing method has quantitatively evaluated 3D orientation grounding in text-to-image generation (except for user studies by Cheng et al. [11]), we curate benchmark based on the MS-COCO dataset [42], mixing and matching object classes and orientations to create images with single or multiple orientation-grounded objects. We demonstrate that ORIGEN significantly outperforms previous orientation-conditioned image generative models [11, 46] on both our benchmark and user studies. Since prior models cannot condition on multiple objects (whereas ORIGEN can, as shown in Fig. 1), comparisons are conducted under single-object conditioning. Additionally, we perform experiments to further validate the superior performance of our method over text-to-image generative models with orientation-specific prompts and other training-free guided sampling strategies. Overall, our main contributions are: We present ORIGEN, the first method for 3D orientation grounding in text-to-image generation for multiple objects across diverse categories. We introduce novel reward-guided sampling approach based on Langevin dynamics that provides theoretical guarantee for convergence while simply adding single line of code. We also propose reward-adaptive time rescaling method that further accelerates convergence. We show that ORIGEN achieves significantly better 3D orientation grounding than existing orientationconditioned image generative models [11, 46], text-toimage generative models [34, 57, 73] with orientationspecific prompts, and training-free guided sampling strategies. 2. Related Work Viewpoint or Orientation Control. Several works have focused on controlling the global viewpoint of the entire image. For example, Burgess et al. [10] propose viewmapping network that predicts word embedding to control the viewpoint in text-to-image generation. Kumari et al. [33] enable model customization to modify object properties via text prompts, with added viewpoint control. However, these methods cannot individually control the orientation of foreground objects. Other works have attempted to control single-object orientation in image generation. For instance, Liu et al. [46] introduce an image diffusion model that controls the relative orientation of an object with respect to its reference image. Huang et al. [24] propose an orientation-conditioned image diffusion model for sampling multi-view object images for text-to-3D generation. The most recent work in this domain, Cheng et al. [11], aim to control object attributes, including azimuth, through continuous word embeddings. However, these methods rely on training-based approaches using single-object synthetic training images, limiting their generalizability across multiple objects and diverse categories. We additionally note that few works address image generation conditioned on depth [6, 28, 75] or 3D bounding boxes [15], but they do not allow direct control of object orientations for example, 3D bounding boxes have front-back ambiguities. Training-Free Guided Generation. number of training-free methods have been proposed for guided generation tasks. DPS [13], MPGD [20], and PiGDM [58] update the noisy data point at each step using given reward function. Yu et al. [74] and Ye et al. [72] take this further by introducing rewinding, where intermediate data points are regenerated by reversing the generative denoising process. The core principle of this approach is leveraging the conditional expectation of the clean image from noisy image at an intermediate step. The expected future reward can also be computed from the expected clean image, allowing gradient ascent to update the noisy image. However, key limitation of these methods is that the expected clean image from diffusion model is often too blurry to accurately predict future rewards. 2 While distilled or fine-tuned diffusion [27, 41, 59] and flow models [48, 49] can mitigate this issue, their straightened trajectories lead to insufficiently small updates to the noisy image during gradient ascent at intermediate timesteps. Recent approaches [5, 16, 19, 64] attempt to address this limitation by updating the initial noise rather than the intermediate noise. Notably, Eyring et al. [16] introduce one-step generative model to efficiently iterate the initial noise update through one-step generation and future reward computation. However, gradient ascent with respect to the initial noise often suffers from local optima and leads to deviations from real images, even with heuristic regularization [55]. To overcome this limitation, we propose novel sampling-based approach rather than an optimization-based one, leveraging Langevin dynamics to effectively balance reward maximization and realism while maintaining simple implementation. Training-Based Guided Generation. For controlling image generation, ControlNet [75] and IP-Adapter [71] are commonly used to utilize pretrained generative model to control for various conditions, though they require training data. For 3D orientation grounding, no public training data is available, and collecting such data would be particularly challenging, especially for multi-object grounding, due to the need for diversity. To address this, recent works have explored several reward-based fine-tuning approaches [7, 17, 52, 68, 70], but these methods require extensive computational resources. Instead of fine-tuning, we propose test-time reward-guided framework that leverages discriminative foundational model for guidance. 3. ORIGEN We present ORIGEN, zero-shot method for 3D orientation grounding in text-to-image generation. To the best of our knowledge, this is the first 3D orientation grounding method for multiple objects across diverse categories. 3.1. Problem Formulation and Overview Our goal is 3D orientation grounding in text-to-image generation using one-step text-to-image generative flow model [44] Fθ, which directly maps latent space to the image space. Let = Fθ(x, c) denote an image generated by Fθ given an input text and latent sampled from prior distribution = (0, I). The input text prompts the generation of an image containing set of desired objects (e.g., person in brown suit is directing dog). For each of the objects that appear in c, we associate set of object phrases = {wi}N i=1 (e.g., {dog, person}) and corresponding set of 3D orientation grounding conditions Φ = {ϕi}N i=1. Following the convention [67], each object orientation ϕi is parameterized by its azimuth angle [0, 360), polar angle ϕpo ϕaz gle ϕro [0, 360). [0, 180), and rotation anAs there is no training dataset consisting of diverse realworld images with accurate per-object orientation annotations, we propose zero-shot approach that leverages test-time guidance from the recently proposed foundational model for image-based orientation estimation: OrientAnything [67]. In particular, given this orientation estimator and reward function defined based on it (which we will discuss in detail in Sec. 3.2), our goal is to find latent sample that maximizes the reward of its corresponding image, expressed as R(Fθ(x, c), W, Φ). For simplicity, we define the pullback of the reward function as ˆR = Fθ and, unless otherwise specified, omit and Φ in the notation, writing the objective simply as ˆR(x) in the following sections. straightforward approach to maximizing ˆR is gradient ascent, where the latent sample is iteratively optimized. The update rule at each optimization step can be written as: xi+1 = xi + η ˆR(xi), (1) where η is step size. However, this gradient ascent in the latent space may pose several challenges: (1) the latent sample may get stuck in local maxima [54, 69] before achieving the desired orientation alignment, (2) the modeseeking nature of gradient ascent can reduce sample diversity [26], and (3) may deviate from the prior latent distribution (0, I), resulting in unrealistic images [17, 63]. Although the recent reward-guided noise optimization method (ReNO [16]) employs norm-based regularization [5, 56] to enforce the latent to be close to the prior distribution, it still suffers from local optima, leading to suboptimal orientation grounding results (see Sec. 4 for experimental comparisons and Appendix for detailed analysis). To address these issues, we reformulate the problem as sampling problem rather than an optimization problem. Specifically, we aim to sample from target distribution that maximizes the expected reward, while ensuring remains close to the original latent distribution: = arg max Exp[ ˆR(cid:0)x(cid:1)] αDKL(p q), (2) where α is constant that controls the regularization strength, and DKL( ) is the Kullback-Leibler divergence [32]. This objective is closely related to those considered in existing fine-tuning-based approaches for reward maximization [17, 35, 53]. However, the key difference is that, while these methods define the target distribution for the output images, we define it for the latent samples, setting as the prior distribution (0, I). This formulation enables the derivation of an effective and simple sampling approach based on Langevin dynamics, which we discuss in Sec. 3.3. We first introduce our reward function designed for 3D orientation grounding (Sec. 3.2) and propose reward-guided Langevin dynamics to effectively sample from our target distribution (Sec. 3.3). Lastly, we introduce rewardadaptive time rescaling to speed up sampling convergence by incorporating time rescaling (Sec. 3.4). 3.2. Orientation Grounding Reward To define our reward function R, we leverage foundational orientation estimation model, OrientAnything [67], (henceforth denoted as D) and measure how well the orientations of the objects estimated by from the generated image align with the grounding orientation conditions Φ. Since represents the orientation as probability distribution discretized into one-degree intervals, we define based on the negative KL divergence between the estimated and target probability distributions of discretized orientations: (cid:88) (cid:16) i=1 jS (cid:88) 1 DKL R(I, W, Φ) = Dj(cid:0)Crop(I, wi)(cid:1) (cid:13) (cid:17) (cid:13) Π(ϕj (cid:13) ) (3) where Crop(I, wi) is an image center-cropping function1 for the object specified by the phrase wi W, with its bounding box estimated from using an open-set object detection model [47]. We define the set of orientation components as = {az, po, ro}, corresponding to azimuth, polar, and rotation angles, respectively. For each component S, Dj(cid:0)Crop(I, wi)(cid:1) denotes the predicted probability distribution for the j-th orientation component of the i-th object, while Π(ϕj ) denotes the target orientation distribution, instantiated as Gaussian distribution centered at ϕj , directly following OrientAnything [67]2. Note that our orientation grounding reward in Eq. 3 simply takes the mean of the reward values computed for objects, inherently supporting orientation grounding for an arbitrary number of objects. We highlight that all existing methods for orientation-conditioned image generation [11, 24, 46] do not support multi-object orientation grounding, as they require training data with image and per-object orientation annotation pairs currently absent in the literature. 3.3. Reward-Guided Langevin Dynamics We now introduce method to effectively sample latent from our target distribution in Eq. 2. To address the limitations of vanilla gradient ascent for reward maximization (as discussed in Sec. 3.1), we propose enhancing the exploration of the sampling space of by injecting stochasticity, which is known to help avoid local optima or saddle points [54, 69]. In particular, we show that simulating 1Note that OrientAnything [67] requires an input image of roughly centered single object. 2It uses Gaussian distribution with manually set variances instead of one-hot distribution, as it better captures the correlations between adjacent angles. Langevin dynamics, in which the drift term is determined by our reward function (Sec. 3.2), can sample that effectively aligns with the grounding orientation conditions. Proposition 1. Reward-Guided Langevin Dynamics. Let = (0, I) denote the prior distribution, ˆR(x) be the pullback of differentiable reward function, and wt denote the standard Wiener process. As , the stationary distribution of the following Langevin dynamics dxt = (cid:32) α ˆR(xt) xt + 1 (cid:33) dt + dwt, (4) coincides with the optimal distribution of Eq. 2. , The proof of Proposition 1 is provided in Appendix B. This demonstrates that simulating the Langevin stochastic differential equation (SDE) (Eq. 4) in the latent space ensures samples are drawn from the target distribution q, balancing reward maximization with proximity to the prior distribution (Eq. 2). Using EulerMaruyama discretization [18], we express its discrete-time approximation as: xi+1 xi + (cid:32) α ˆR(xi) xi + 1 2 (cid:33) δt + δtϵi, (5) where ϵi (0, I). By introducing the substitutions γ δt and η 1 2α , the above expression (Eq. 5) simplifies to the following intuitive update rule: xi+1 = (cid:112)1 γ (xi + γη ˆR(xi)) + γϵi. (6) Notably, this final update step (Eq. 6) is surprisingly simple. Compared to the update step in standard gradient ascent (Eq. 1), only the stochastic noise term and scaling factor are additionally introduced, which require just single additional line of code. This update rule integrates exploration through noise while explicitly ensuring proximity to prior distribution. 3.4. Reward-Adaptive Time Rescaling While our reward-guided Langevin dynamics already enables effective sampling of for 3D orientation grounding, we additionally introduce reward-adaptive time rescaling to further accelerate convergence via time rescaling. In Leroy et al. [39], time-rescaled SDE is introduced with monitor function that adaptively controls the step size, 2 G( ˆR(x))dt to prealong with correction drift term 1 serve the stationary distribution of the original SDE. By defining new time variable τ and the corresponding pro2 G( ˆR(xτ ))dτ , cess xτ = xt(τ ) with correction drift term 1 our time-rescaled version of reward-guided Langevin SDE 4 in Eq. 4 is expressed as: dxτ = G( ˆR(xτ )) (cid:18) 1 2 xτ + η ˆR(xτ ) (cid:19) dτ G( ˆR(xτ ))dτ + (cid:113) + 1 2 G( ˆR(xτ ))d wτ , (7) (cid:113) where the original time increment is rescaled according to dt = G( ˆR(xτ ))dτ and dwt = G( ˆR(xτ ))d wτ . detailed derivation and convergence analysis of Eq. 7 are provided in Appendix C. Defining γ(xτ ) = G( ˆR(xτ ))dτ , we obtain the following time-rescaled update rule via EulerMaruyama discretization: xi+1 = (cid:112)1 γ(xi) 1 2 (cid:16) xi + γ(xi)η ˆR(xi) γ(xi) log G( ˆR(xi)) + (cid:112)γ(xi) ϵi. + (cid:17) Algorithm 1 ORIGEN Require: (Prompt), (Object phrase set), Φ (Grounding orientation set), Fθ (One-step T2I model), (Reward function), (Monitor function), (# of optimization steps), η (Gradient scaling factor), γ (Step size) 1: x0 (0, I) 2: for = 0 to 1 do 3: 4: 5: 6: ˆRi = R(Fθ(xi, c), W, Φ) Reward computation γi = G( ˆRi)γ Timestep rescaling ϵi (0, I) xi+1 = (cid:16) (cid:17) xi + γiη ˆRi 1 γi 2 γi log G( ˆRi) + + γiϵi Update step 7: end for 8: return Fθ(xM , c) Ground Truth ReNO [16] ORIGEN ORIGEN (8) Regarding the design of the monitor function, existing work [38] suggests setting the step size inversely proportional to the squared norm of the drift coefficients in Eq. 4. However, this approach requires computing the Hessian of the reward function when evaluating the correction term, which is computationally expensive. Alternatively, we propose the following simple, reward-adaptive monitor function: G( ˆR(x)) = smin tanh(k ˆR(x)) (smax smin), (9) 3 , 4 where the hyperparmeters smin, smax, and are set to 1 3 , and 6 5 in our experiments, respectively. This function adaptively scales the step size by assigning smaller steps in highreward and larger steps in low-reward, thereby improving convergence speed and accuracy. Please refer to Fig. in Appendix that provides the visualization of our monitor function G( ˆR(x)). In Fig. 2, we present toy experiment demonstrating the effectiveness of our Langevin dynamics and rewardadaptive time rescaling compared to gradient ascent with regularization (ReNO [16]). See Appendix for details on the toy experiment setup. The top row shows the ground truth target latent distribution (leftmost) alongside latent samples generated by different methods, and the bottom row displays the corresponding data distributions. While ReNO [16] fails to accurately capture the target distribution due to its mode-seeking behavior (2nd column, as discussed in Sec. 3.1), our method successfully aligns with it (3rd column), and time rescaling further accelerates convergence (4th column). Our optimization procedure is outlined in Alg. 1. Note that setting G( ˆR(x)) = 1 reverts the method to the uniformly time-rescaled form. Figure 2. Toy experiment results. The top row shows latent space samples (red), while the bottom row shows the corresponding data space samples (blue). From left to right, each column represents: (1) the ground truth target distribution from Eq. (2); (2) results of ReNO [16]; (3) results of ours with uniform time scaling; and (4) results of ours with reward-adaptive time rescaling. 4. Experiments 4.1. Datasets To the best of our knowledge, no existing benchmark has been proposed to evaluate 3D orientation grounding in textto-image generation, aside from user studies conducted by Cheng et al. [11]. To address this, we introduce three benchmarks based on the MS-COCO dataset [42], that consist of diverse text prompts, image and bounding boxes. MS-COCO-Single. For comparison with previous orientation-grounding methods [11, 46] that can condition on only single object, we construct the MS-COCO-Single dataset. From MS-COCO validation set [42], we filter out (1) object classes for which orientation cannot be clearly defined (e.g., objects with front-back symmetry) and (2) image captions lacking explicit object references. Since the current orientation-to-image generation model [11] is only capable of controlling the front 180 range of azimuths, we further filter the samples to only include those within this 5 Orientation Zero-1-to-3 [46] C3DW [11] ORIGEN (Ours) Orientation Zero-1-to-3 [46] C3DW [11] ORIGEN (Ours) Figure 3. Qualitative comparisons on MS-COCO-Single benchmark (Sec. 4.3). Compared to the existing orientation-to-image models [11, 46], ORIGEN generates the most realistic images, which also best align with the grounding conditions, indicated by the overlapped arrow in each image. Orientation SD-Turbo [73] SDXL-Turbo [57] FLUX-Schnell [34] Zero-1-to-3 [46] C3DW [11] ORIGEN (Ours) Figure 4. Qualitative comparisons on MS-COCO-NView benchmark (Sec. 4.4). ORIGEN generates high-fidelity images with the best grounding accuracy. Note that C3DW [11] does not support back-view generation, as it only supports the control of the front 180 of azimuth angles. range. This procedure yields 252 text-image pairs covering 25 distinct object classes. Using the provided bounding boxes, we cropped the foreground objects and fed them into OrientAnything [67], an orientation estimation model, to obtain pseudo-GT grounding orientations. Building upon this dataset, MS-COCO-Single was constructed by mixmatching the image captions and grounding orientations, ultimately forming dataset consisting of 25 object classes, each with 40 samples, totaling 1K samples. Please refer Tab. in Appendix to check object classes we used. MS-COCO-NView. One may consider achieving orientation grounding through text prompts for specific views. To further compare our method with such cases, we construct an extended dataset, MS-COCO-NView, based on the 252 text-image pairs from MS-COCO-Single. Specifically, by generating four grounding orientations front, left, right, back for each sample, we enable direct comparison with existing text-to-image models by appending these directional cues to prompts (e.g., adding facing left to the end of the prompt). In our experiments, we defined ground-truth 6 Orientation ORIGEN (Ours) Orientation ORIGEN (Ours) Orientation ORIGEN (Ours) Orientation ORIGEN (Ours) Figure 5. Qualitative results on the MS-COCO-Multi benchmark (Sec. 4.5) ORIGEN achieves accurate grounding while generalizing across multiple objects. (GT) orientations on horizontal plane with four azimuth angles: 0 (front), 90 (left), 180 (back), and 270 (right). We consider two cases: (1) 3-View configuration that covers the front, left, and right views within 180 frontal range (to further compare with C3DW [11]), (2) 4-View configuration that additionally includes back view. MS-COCO-Multi. For more qualitative results, we construct MS-COCO-Multi following an approach to that in MS-COCO-Single. Since our base dataset [42] lacks samples composed solely of objects with clear orientation, we mix-match object classes and grounding orientations from the 252 text-image pairs in MS-COCO-Single, forming dataset consisting of 371 samples, each containing varying number of objects. We annotated prompts by concatenating individual object phrases (e.g., cat, and dog.). 4.2. Evaluation Metrics We measure orientation grounding accuracy using two metrics: 1) the absolute error on azimuth angles3 between the predicted and grounding object orientations, and 2) Acc.@22.5, the angular accuracy within tolerance of 22.5, following Wang et al. [67]. For evaluation, we use OrientAnything [67] to predict the 3D orientation from the generated images. Since OrientAnythings predictions may not be perfect, we also conduct user study in Sec. 4.6 to validate the results. Along with grounding accuracy, we also evaluate text-to-image alignment using CLIP Score [21] and VQA-Score [43], as well as human preference using PickScore [31]. 3We perform comparisons only on azimuth angle, as existing methods [11, 46] do not support the control over polar and rotation angles. Note that our results for all azimuth, polar, and rotation angles are provided in Appendix E. 4.3. Results on MS-COCO-Single Baselines. We compare ORIGEN with two types of baselines: (1) orientation-to-image generation methods [11, 46] and (2) training-free guided generation methods [16, 74]. For orientation-to-image generation methods, we include Continuous 3D Words (C3DW) [11] and Zero-1-to-3 [46], following the baselines used in the most recent work in this field [11]. For training-free guided generation methods, we consider FreeDoM [74], which updates the intermediate samples at each step of the multi-step sampling process based on the expected reward computed on the foreseen clean samples, and ReNO [16], which optimizes the initial latent through vanilla gradient ascent using one-step models. To ensure fair comparisons, we keep the number of function evaluations (NFEs) consistent (NFEs = 50) across different guided generation methods. For example, in FreeDoM, we set the rewind iterations to 5 and inference steps to 10 to match 50 NFEs. Results. In Tab. 1a, we show our quantitative comparison results on the MS-COCO-Single benchmark. ORIGEN significantly outperforms all the baselines in orientation alignment, showing comparable performance in text-to-image alignment. Our qualitative comparisons are also shown in Fig. 3, demonstrating that ORIGEN generates high-quality images that align with the grounding orientation conditions and input text prompts. Note that C3DW [11] is trained on synthetic data (i.e., multi-view renderings of 3D object) to learn orientation-to-image generation. Thus, it has limited generalizability to real-world images and the output images lack realism. Zero-1-to-3 [46] is also trained on singleobject images but without backgrounds, requiring additional background image composition (also used in the evaluation of C3DW [11]) that may introduce unnatrual artifacts. The existing methods on guided generation methods also achieve suboptimal results compared to ORIGEN. Notably, FreeDoM [74] achieves the worst results among the guided 7 generation methods. This is because controlling the object orientation requires modifying the low-frequency structures of the image which are known to be determined at early stages of sampling [12], as discussed in Sec. 2. ReNO [16] also achieves suboptimal results compared to ours, as it performs latent optimization based on vanilla gradient ascent which is prone to local optima (as discussed in Sec. 3.1). Overall, our method achieves the best results both with and without time rescaling, demonstrating its ability to effectively maximize rewards while avoiding over-optimization. Table 1. Quantitative comparisons on 3D orientation grounded image generation. Best and second-best results are highlighted in bold and underlined, respectively. (a) Comparisons on MS-COCO-Single (Sec. 4.3). ORIGEN denotes ours without reward-adaptive time rescaling, where ORIGEN represents our full method. Id Model Orientation Alignment Text Alignment Acc.@22.5 Abs. Err. CLIP VQA PickScore 1 C3DW [11] 2 Zero-1-to-3 [46] 3 FreeDoM [74] 4 ReNO [16] (Ours) 5 ORIGEN 6 ORIGEN (Ours) 0.426 0.499 0.741 0.796 0.854 0.871 64.77 59.03 20.90 20.56 18.28 17.41 0.220 0.272 0.259 0.247 0.265 0. 0.439 0.663 0.728 0.663 0.732 0.735 0.197 0.213 0.225 0.212 0.224 0.224 (b) Comparisons on MS-COCO-NView (Sec. 4.4). Id Model 3-view Alignment 4-view Alignment Acc.@22.5 Abs. Err. Acc.@22.5 Abs. Err. 1 SD-Turbo [73] 2 SDXL-Turbo [57] 3 FLUX-Schnell [34] 4 C3DW [11] 5 Zero-1-to-3 [46] 6 ORIGEN (Ours) 0.257 0.189 0.312 0.504 0.366 0.824 75.09 78.44 75.04 53.53 68.70 20.99 0.244 0.196 0.424 - 0.321 0.866 78.47 81.88 60.26 - 75.10 17. (c) Comparisons on MS-COCO-Multi (Sec. 4.5). Model Orientation Alignment Text alignment Acc.@22.5 Abs. Err. CLIP VQA PickScore ORIGEN (Ours) 0.634 34.27 0.281 0.764 0. 4.4. Results on MS-COCO-NView Baselines. We also compare ORIGEN on the MS-COCONView benchmark with the text-to-image (T2I) generation models, where their orientation condition is provided via the input text prompt. As baselines, we consider several onestep T2I generative models: SD-Turbo [73], SDXL-Turbo [57], and FLUX-Schnell [34]. In particular, we appended phrase that specifies the object orientation at the end of each caption in MS-COCO-NView dataset. For more comprehensive comparisons, we also included the orientation-to-image baseline models (C3DW [11] and Zero-1-to-3 [46]) considered in Sec. 4.3 in this experiment as well. Results. As shown in Tab. 1b, ORIGEN significantly outperforms all baseline models in orientation alignment. Note that, although FLUX-Schnell [34] achieves the highest alignment among the vanilla T2I models, ORIGEN surpasses it by more than 2.5 times in the 3-view alignment setting (82.4% vs. 31.2%) and more than 2 times in the 4-view alignment setting (86.6% vs. 42.4%). This substantial margin highlights the inherent ambiguity and lack of precise control in vanilla T2I approaches, as orientation information embedded within textual descriptions is less explicit and reliable compared to direct orientation guidance. We further demonstrate the advantage of ORIGEN through qualitative comparisons in Fig. 4. Vanilla T2I models frequently fail to adhere strictly to the desired orientation, even with directional phrases in text prompts. In contrast, ORIGEN consistently generates images accurately aligned with the specified orientations. Please refer to Appendix that shows ORIGEN also achieves comparable text alignment results compared to the vanilla T2I models. 4.5. Results on MS-COCO-Multi We additionally show the results of ORIGEN on the MSCOCO-Multi benchmark. Since no baseline is capable of multi-object orientation grounding, we exclusively assess our method. Our quantitative results are shown in Tab. 1c and the qualitative examples are shown in Fig. 5. These results further demonstrate that our approach is seamlessly generalizable to multiple objects by simply averaging the orientation grounding reward across multiple objects. Table 2. User Study Results. 3D orientation-grounded text-toimage generation results of ORIGEN was preferred by 58.18% of the participants on Amazon Mechanical Turk [9], significantly outperforming the baselines [11, 46]. Method Zero-1-to-3 [46] C3DW [11] ORIGEN (Ours) User Preferences (%) 20.58 21.24 58.18 4.6. User Study Our previous quantitative evaluations were performed by comparing the grounding orientations and orientations estimated from the generated images using OrientAnything [67]. While its orientation estimation performance is highly robust (as seen in all of our qualitative results), we additionally conduct user study to further validate the effectiveness of our method based on human evaluation performed by 100 participants on Amazon Mechanical Turk [9]. Each participant was presented with the grounding orientation, the input prompt, and the images generated by (1) Zero-1-to-3 [46], (2) C3DW [11], and (3) ORIGEN. Then, they were asked to select the image that best matches both the grounding orientations and the input text prompt directly following the user study settings in C3DW [11]. In Tab. 2, ORIGEN was preferred by 58.18% of the participants, clearly outperforming the baselines. For more details of this user study, refer to Appendix G. 5. Conclusion We presented ORIGEN, the first 3D orientation grounding method for text-to-image generation across multiple objects and diverse categories. To enable test-time guidance with 8 pretrained discriminative model, we proposed novel sampling approach based on Langevin dynamics, which adds just one line of code to gradient ascent while theoretically guaranteeing balance between orientation conditioning and image realism. Additionally, we introduced reward-adaptive time rescaling method to accelerate convergence. Our experiments demonstrated significantly superior performance compared to previous fine-tuning-based methods, along with capabilities that were previously infeasible: conditioning on multiple objects, elevations, rotations, and 360 azimuths."
        },
        {
            "title": "References",
            "content": "[1] Omri Avrahami, Thomas Hayes, Oran Gafni, Sonal Gupta, Yaniv Taigman, Devi Parikh, Dani Lischinski, Ohad Fried, and Xi Yin. Spatext: Spatio-textual representation for controllable image generation. In CVPR, 2023. [2] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, et al. ediff-i: Text-to-image diffusion models with an ensemble of expert denoisers. CoRR, abs/2211.01324, 2022. [3] Arpit Bansal, Hong-Min Chu, Avi Schwarzschild, Soumyadip Sengupta, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Universal guidance for diffusion models. In CVPR, 2023. [4] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. Multidiffusion: Fusing diffusion paths for controlled image generation. In ICML, 2023. [5] Heli Ben-Hamu, Omri Puny, Itai Gat, Brian Karrer, Uriel Singer, and Yaron Lipman. D-flow: Differentiating through flows for controlled generation. 2024. [6] Shariq Farooq Bhat, Niloy Mitra, and Peter Wonka. Loosecontrol: Lifting controlnet for generalized depth conditioning. In SIGGRAPH, pages 111, 2024. [7] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. In ICLR, 2024. [8] Eric Brachmann, Alexander Krull, Frank Michel, Stefan Gumhold, Jamie Shotton, and Carsten Rother. Learning 6d object pose estimation using 3d object coordinates. In ECCV, 2014. [9] Michael Buhrmester, Tracy Kwang, and Samuel Gosling. Amazons mechanical turk: new source of inexpensive, yet high-quality, data? Perspectives on Psychological Science, 6 (1):35, 2011. [10] James Burgess, Kuan-Chieh Wang, and Serena Yeung-Levy. Viewpoint textual inversion: Discovering scene representations and 3d view control in 2d diffusion models. In ECCV, pages 416435. Springer, 2024. [11] Ta-Ying Cheng, Matheus Gadelha, Thibault Groueix, Matthew Fisher, Radomir Mech, Andrew Markham, and Niki Trigoni. Learning continuous 3d words for text-toimage generation. In CVPR, 2024. In Proceedings of oritized training of diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1147211481, 2022. [13] Hyungjin Chung, Jeongsol Kim, Michael Mccann, Marc Klasky, and Jong Chul Ye. Diffusion posterior sampling for general noisy inverse problems. arXiv preprint arXiv:2209.14687, 2022. [14] Guillaume Couairon, Marlene Careil, Matthieu Cord, Stephane Lathuiliere, and Jakob Verbeek. Zero-shot spatial In layout conditioning for text-to-image diffusion models. ICCV, 2023. [15] Abdelrahman Eldesokey and Peter Wonka. Build-a-scene: Interactive 3d layout control for diffusion-based image generation. arXiv preprint arXiv:2408.14819, 2024. [16] Luca Eyring, Shyamgopal Karthik, Karsten Roth, Alexey Dosovitskiy, and Zeynep Akata. Reno: Enhancing one-step text-to-image models through reward-based noise optimization. In NeurIPS, 2025. [17] Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Dpok: Reinforcement learning for fine-tuning text-to-image diffusion models. In NeurIPS, pages 7985879885, 2023. [18] Francesco Gianfelici. Numerical solutions of stochastic differential equations (kloeden, pk and platen, e.; 2008)[book IEEE Transactions on Neural Networks, 19(11): reviews]. 19901991, 2008. [19] Xiefan Guo, Jinlin Liu, Miaomiao Cui, Jiankai Li, Hongyu Yang, and Di Huang. Initno: Boosting text-to-image diffusion models via initial noise optimization. In CVPR, pages 93809389, 2024. [20] Yutong He, Naoki Murata, Chieh-Hsin Lai, Yuhta Takida, Toshimitsu Uesaka, Dongjun Kim, Wei-Hsiang Liao, Yuki Mitsufuji, Zico Kolter, Ruslan Salakhutdinov, et al. arXiv preprint Manifold preserving guided diffusion. arXiv:2311.16424, 2023. [21] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021. [22] Tomaˇs Hodaˇn, Jiˇrı Matas, and ˇStˇepan Obdrˇzalek. On evaluation of 6d object pose estimation. In ECCV, 2016. [23] Yinlin Hu, Joachim Hugonot, Pascal Fua, and Mathieu Salzmann. Segmentation-driven 6d object pose estimation. In CVPR, 2019. [24] Yuzhong Huang, Zhong Li, Zhang Chen, Zhiyuan Ren, Guosheng Lin, Fred Morstatter, and Yi Xu. Orientdream: Streamlining text-to-3d generation with explicit orientation control. arXiv preprint arXiv:2406.10000, 2024. [25] HuggingFace. OpenAI-CLIP, 2022. [26] Rohit Jena, Ali Taghibakhshi, Sahil Jain, Gerald Shen, Nima Tajbakhsh, and Arash Vahdat. Elucidating optimal rewarddiversity tradeoffs in text-to-image diffusion models. arXiv preprint arXiv:2409.06493, 2024. [12] Jooyoung Choi, Jungbeom Lee, Chaehun Shin, Sungwon Kim, Hyunwoo Kim, and Sungroh Yoon. Perception pri- [27] Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki 9 Mitsufuji, and Stefano Ermon. Consistency trajectory models: Learning probability flow ode trajectory of diffusion. arXiv preprint arXiv:2310.02279, 2023. [28] Gyeongnyeon Kim, Wooseok Jang, Gyuseong Lee, Susung Hong, Junyoung Seo, and Seungryong Kim. Dag: Depthaware guidance with denoising diffusion probabilistic models. arXiv preprint arXiv:2212.08861, 2022. [29] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and In Proceedings of the Ross Girshick. Segment anything. IEEE/CVF International Conference on Computer Vision (ICCV), pages 40154026, 2023. [30] S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi. Optimization by simulated annealing. Science, 220(4598):671680, 1983. [31] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36: 3665236663, 2023. [32] Solomon Kullback and Richard A. Leibler. On information and sufficiency. Annals of Mathematical Statistics, 22(1): 7986, 1951. [33] Nupur Kumari, Grace Su, Richard Zhang, Taesung Park, Eli Shechtman, and Jun-Yan Zhu. Customizing text-to-image diffusion with object viewpoint control. In SIGGRAPH Asia, pages 113, 2024. [34] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. [35] Hao Lang, Fei Huang, and Yongbin Li. Fine-tuning language models with reward learning on policy. arXiv preprint arXiv:2403.19279, 2024. [36] Phillip Lee and Minhyuk Sung. Reground: Improving textual and spatial grounding at no cost. In ECCV, 2024. [37] Yuseung Lee, TaeHoon Yoon, and Minhyuk Sung. Groundit: Grounding diffusion transformers via noisy patch transplantation. In NeurIPS, 2024. [38] A. Leroy, B. Leimkuhler, J. Latz, and D. J. Higham. Adaptive stepsize algorithms for langevin dynamics. SIAM Journal on Scientific Computing, 46(6):A3574A3598, 2024. [39] Alix Leroy, Benedict Leimkuhler, and Desmond Higham. Adaptive stepsize algorithms for langevin dynamics. SIAM Journal on Scientific Computing, 46(6):A3574A3598, 2024. Jonas Latz, [40] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In CVPR, 2023. [41] Shanchuan Lin, Anran Wang, and Xiao Yang. SdxlProgressive adversarial diffusion distillation. lightning: arXiv preprint arXiv:2402.13929, 2024. [42] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer visionECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part 13, pages 740755. Springer, 2014. [43] Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig, Pengchuan Zhang, and Deva Ramanan. Evaluating text-to-visual generation with image-to-text generation. In European Conference on Computer Vision, pages 366384. Springer, 2024. [44] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [45] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [46] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: In ICCV, pages 9298 Zero-shot one image to 3d object. 9309, 2023. [47] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with In grounded pre-training for open-set object detection. ECCV, 2024. [48] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. [49] Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, et al. Instaflow: One step is enough for high-quality diffusionbased text-to-image generation. In The Twelfth International Conference on Learning Representations, 2023. [50] Wan-Duo Kurt Ma, Avisek Lahiri, John Lewis, Thomas Leung, and Bastiaan Kleijn. Directed diffusion: Direct control of object placement through attention guidance. In AAAI, 2024. [51] Grigorios A. Pavliotis. Stochastic Processes and Applications: Diffusion Processes, the Fokker-Planck and Langevin Equations. Springer, 2014. [52] Mihir Prabhudesai, Anirudh Goyal, Deepak Pathak, and Katerina Fragkiadaki. Aligning text-to-image diffusion models with reward backpropagation. [53] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In NeurIPS, 2023. [54] Maxim Raginsky, Alexander Rakhlin, and Matus Telgarsky. Non-convex learning via stochastic gradient langevin dynamics: nonasymptotic analysis. In Conference on Learning Theory, pages 16741703. PMLR, 2017. [55] Dvir Samuel, Rami Ben-Ari, Nir Darshan, Haggai Maron, and Gal Chechik. Norm-guided latent space exploration for text-to-image generation. 2023. [56] Dvir Samuel, Rami Ben-Ari, Simon Raviv, Nir Darshan, and Gal Chechik. Generating images of rare concepts using pretrained diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 46954703, 2024. [57] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin In European Rombach. Adversarial diffusion distillation. Conference on Computer Vision, pages 87103. Springer, 2024. [72] Haotian Ye, Haowei Lin, Jiaqi Han, Minkai Xu, Sheng Liu, Yitao Liang, Jianzhu Ma, James Zou, and Stefano Ermon. Tfg: Unified training-free guidance for diffusion models. In NeurIPS, pages 2237022417, 2025. [73] Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 66136623, 2024. [74] Jiwen Yu, Yinhuai Wang, Chen Zhao, Bernard Ghanem, and Jian Zhang. FreeDoM: Training-free energy-guided conditional diffusion model. In ICCV, pages 2317423184, 2023. [75] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 38363847, 2023. [76] Bernt Øksendal. Stochastic Differential Equations: An Introduction with Applications. Springer-Verlag Berlin Heidelberg, 6 edition, 2003. [58] Jiaming Song, Arash Vahdat, Morteza Mardani, and Jan Kautz. Pseudoinverse-guided diffusion models for inverse problems. In ICLR, 2023. [59] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. 2023. [60] Martin Sundermeyer, Zoltan-Csaba Marton, Maximilian Durner, Manuel Brucker, and Rudolph Triebel. Implicit 3d orientation learning for 6d object detection from rgb images. In ECCV, 2018. [61] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor Lempitsky. Resolution-robust large mask inpainting with fourier convolutions. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 2149 2159, 2022. [62] Bugra Tekin, Sudipta Sinha, and Pascal Fua. Real-time In CVPR, seamless single shot 6d object pose prediction. 2018. [63] Masatoshi Uehara, Yulai Zhao, Kevin Black, Ehsan Hajiramezanali, Gabriele Scalia, Nathaniel Lee Diamant, Alex Tseng, Tommaso Biancalani, and Sergey Levine. Finetuning of continuous-time diffusion models as entropyregularized control. arXiv preprint arXiv:2402.15194, 2024. [64] Bram Wallace, Akash Gokul, Stefano Ermon, and Nikhil Naik. End-to-end diffusion latent optimization improves classifier guidance. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 72807290, 2023. [65] Chen Wang, Danfei Xu, Yuke Zhu, Roberto Martın-Martın, Cewu Lu, Li Fei-Fei, and Silvio Savarese. Densefusion: 6d object pose estimation by iterative dense fusion. In CVPR, 2019. [66] He Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin, Shuran Song, and Leonidas Guibas. Normalized object coordinate space for category-level 6d object pose and size estimation. In CVPR, 2019. [67] Zehan Wang, Ziang Zhang, Tianyu Pang, Chao Du, Hengshuang Zhao, and Zhou Zhao. Orient anything: Learning robust object orientation estimation from rendering 3d models. arXiv, 2024. [68] Fanyue Wei, Wei Zeng, Zhenyang Li, Dawei Yin, Lixin Duan, and Wen Li. Powerful and flexible: Personalized textto-image generation via reinforcement learning. In European Conference on Computer Vision, pages 394410. Springer, 2024. [69] Max Welling and Yee Teh. Bayesian learning via stochasIn ICML, pages 681688. tic gradient langevin dynamics. Citeseer, 2011. [70] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for textto-image generation. In NeurIPS, 2023. [71] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023."
        },
        {
            "title": "Appendix",
            "content": "A. Analysis on Norm-Based Regularization In this section, we analyze the limitations of norm-based regularization in reward-guided noise optimization by highlighting its mode-seeking property, which can lead to overoptimization and convergence to local maxima. Consider the following reward maximization process with norm-based regularization [16, 55]: xt+1 = xt + η1 ˆR(xt) + η2 log y(xt), (10) where ˆR = Fθ is the pullback of the reward function and y( ) represents the probability density of χd distribution. We can view both ˆR() and y( ) as components of single reward, allowing us to rewrite the update as xt+1 = xt + η1 ˆR(xt) + η2 log y(xt) = xt + η1 ˆR(xt) (cid:104) + η2 (d 1) log xt2 1 2 = xt + η1 ˆR(xt) + η2K(xt) xt2(cid:105) = xt + Φ(xt). (11) where Φ(x) = η1 ˆR(x) + η2K(x). This optimization process is an Euler discretization (with δt = 1) of the ODE dxt = 1 2 log q(xt)dt + dwt, (13) then q(x) is the unique stationary distribution. Using Eq. 2, we obtain 1 2α Integrating this with respect to gives log q(x) = + 1 2 1 ˆR(xt). (14) q(x) q(x) exp (cid:32) ˆR(x) α (cid:33) , (15) where q(x) is standard Gaussian distribution. Finally, following existing approach [53], we easily arrive at = arg max Exp[ ˆR(x)] αDKL(pq), (16) which matches the expression presented in Eq. 2. C. Analysis on Reward-Adaptive Time-"
        },
        {
            "title": "Rescaled SDE",
            "content": "In this section, we analyze the effect of positiondependent step size on reward-guided Langevin dynamics. We first illustrate why naive time-rescaling approach fails to preserve the desired stationary distribution, and how to fix it with an additional correction term, eventually leading to Eq. 8. Our derivation follows the approach of Leroy et al. [39]. C.1. Non-Convergence of Direct Time-Rescaled"
        },
        {
            "title": "SDE",
            "content": "dx dt = Φ(x), (12) Consider the following SDE: which represents deterministic gradient flow, whose stationary measure is weighted sum of Dirac deltas located at the maximizers of Φ(x) = η1 ˆR(x) + η2K(x) = 0. However, as illustrated in Fig. 2, this deterministic gradient ascent does not directly prevent the iterates from deviating substantially from the original latent distribution. Also, the process may collapse to local maximaeven ones where the reward is not sufficiently high [30]. Our empricial observations indicate that this approach is ineffective for our application, presumably because the reward function defined over the latent exhibits many local maxima, causing the deterministic ascent to converge to suboptimal solutions. B. Proofs Proof of Proposition 1. Recall overdamped Langevin dynamics: to the SDE: the standard result from if xt evolves according dxt = b(xt) dt + σ(xt) dwt, (17) with drift b(xt) and diffusion coefficient σ(xt). Its probability density ρ(x, t) evolves according to the FokkerPlanck equation [76]: 1 ρ(x, t) = [b(x)ρ(x, t)] + 2(cid:2)σ2(x)ρ(x, t)(cid:3). (18) Thus, the stationary distribution must lie in the kernel of the corresponding FokkerPlanck operator [51]: Lρ(x) = (cid:16) (cid:17) b(x) ρ(x) 2(cid:104) (cid:105) σ2(x) ρ(x) . (19) + 1 2 Now, consider the reward-guided Langevin SDE in Eq. 4: (cid:18) dxt = 1 2 xt + η ˆR(xt) (cid:19) dt + dwt, (20) which has the stationary distribution q(x) given by Eq. 2. By comparing with the general form, we identify: rescaled process converges to the same stationary distribution as the original SDE. The modified SDE becomes + η ˆR(x), σ(x) = 1. (21) dxτ = G(xτ ) (cid:18) 1 2 xτ + η ˆR(xτ ) (cid:19) dτ b(x) = 1 2 Hence, Lq(x) = (cid:17) + η ˆR(x) (cid:104)(cid:16) 2(cid:104) 1 2 q(x) (cid:105) (cid:105) q(x) (22) (23) (24) + 1 2 = 0. Next, we introduce monitor function G(x) > 0 and define new time variable τ by dt = G(cid:0)xt(τ ) (cid:1) dτ. (25) 1 2 + G(xτ )dτ + (cid:112)G(xτ ) Wτ . (29) The corresponding Fokker-Planck operator corr for this corrected time-rescaled SDE now annihilates the original stationary distribution q(x): Defining the time-rescaled process xτ = xt(τ ), we rewrite the SDE as corrq(x) = (cid:104) (cid:16) G(x) 1 2 (cid:17) + η ˆR(x) q(x) (cid:19) (cid:18) 1 2 dxτ = G(xτ ) xτ + η ˆR(xτ ) dτ + (cid:112)G(xτ ) Wτ . (26) In this rescaled SDE, the new drift and diffusion coefficients are 1 2 (cid:104) = 1 2 = 0. + G(x) q(x) (cid:105) + 2(cid:104) (cid:105) G(x)q(x) (cid:105) G(x) q(x) 1 2 1 2 (cid:104) (cid:105) G(x) q(x) (30) a(x) = G(x) (cid:18) 1 2 + η ˆR(x) (cid:19) , σ(x) = (cid:112)G(x). (27) Applying the corresponding FokkerPlanck operator to q(x), we obtain Lq(x) = (cid:104) G(x) (cid:16) + η ˆR(x) (cid:17) (cid:105) q(x) 1 + G(x)q(x) 2(cid:104) 1 2 (cid:104) a(x) q(x) (cid:105) + = + G(x) q(x) (cid:105) (cid:104) = (cid:105) a(x) q(x) (cid:105) + + 2a(x) q(x) (cid:105) 1 2 1 2 (cid:104) G(x) q(x) (cid:104) G(x) q(x) = (cid:104) 1 2 G(x) q(x) (cid:105) = 0. (28) which is nonzero in general. Therefore, q(x) is not annihilated by L, implying that the time-rescaled SDE does not converge to the desired target distribution in Eq. 2. C.2. Time-Rescaled SDE with Correction Drift"
        },
        {
            "title": "Term",
            "content": "Convergence guarantee to original stationary distribution. Following previous work [38], we can add correction drift term 1 2 G(xτ ) to Eq. 26 to ensure that the 13 Consequently q(x) remains in the kernel of the corr, showing that this corrected time-rescaled SDE preserves the original invariant distribution. Convergence speed of time rescaling approach. To verify the effectiveness of our time-rescaling approach, we analyzed the average number of iterations required to reach the desired reward level during reward-guided Langevin dynamics on MS-COCO-Single. Without time rescaling, our method required an average of 14.18 iterations to reach the target reward, whereas applying reward-adaptive time rescaling reduced this to 12.88 iterations, demonstrating improved convergence speed. Notably, the computational overhead introduced by adaptive rescaling is negligible, as the term log G( ˆRi) in Alg. 1 can be efficiently computed via the chain rule, using the precomputed reward gradient ˆRi. D. Setup for the Toy Experiment We train rectified flow model [48] on 2D domain, where the source distribution is (0, I) and the target distribution is mixture of two Gaussians, (µ1, σ1I) and (µ2, σ2I), with µ1 = (4, 0)T , µ2 = (4, 0)T , σ1 = 0.3, and σ2 = 0.9. The velocity prediction network consists of four hidden MLP layers of width 128. We first train an initial model and distill it twice, resulting in 3-rectified flow model. The reward function is defined as R(x, y) = exp (cid:18) (x 4)2 + y2 2 (cid:19) (cid:18) + 0.1 exp (x + 4)2 + y2 2 (cid:19) 1. (31) In Fig. 2, all methods use the same total number of sampling steps. Class Number Class Name (a) Start page of the user study. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23"
        },
        {
            "title": "Airplane\nBear\nBench\nBicycle\nBird\nBoat\nBus\nCar\nCat\nChair\nCow\nDog\nElephant\nGiraffe\nHorse\nLaptop\nMotorcycle\nPerson\nSheep\nTeddy bear\nToilet\nTrain\nTruck\nTv\nZebra",
            "content": "(b) Main test page of the user study. Figure A. Screenshots of our user study. verse orientation conditions. This demonstrates the robustness of our approach, maintaining high orientation alignment performance even when evaluated on more general set of samples. F. Text-to-Image Alignment Results on MSCOCO-NView In Tab. C, we report the additional quantitative results on text-to-image alignment on the MS-COCO-NView benchmark. As noted in Sec. 4.4, ORIGEN achieves comparable text alignment results compared to the vanilla T2I models, while achieving significantly more accurate orientation grounding. Table A. Selected object classes in our dataset. G. User Study Examples E. General Orientation Controllability We provide our extensive evaluation on general curated MSCOCO-Single dataset. As discussed in Sec. 4.1, we filter out non-clear object classes and image captions, but we do not apply filtering on the front range and do not fix the polar and rotation angles. Upon this, we mix-match object classes and grounding orientations, forming dataset consisting of 25 object classes, each with 40 samples, totaling 1K samples. We evaluated on same metrics as in Sec. 4.2, including polar and rotation accuracy, within tolerance of 5.0 as well as their absolute errors, following Wang et al. [67]. As shown in Tab. B, ORIGEN generalizes well on diIn this section, we provide details of the user study. To assess user preferences, we conducted an evaluation comparing images generated by Zero-1-to-3 [46], C3DW [11], and ORIGEN using MS-COCO-Single as the benchmark. The study was conducted on Amazon Mechanical Turk (AMT). For each object class, one sample was randomly selected, resulting in total of 25 questions. The orientations used to generate the images were intuitively visualized and presented alongside their corresponding prompts. As shown in Fig. A, participants were asked to respond to the following question: Considering the orientation and prompt conditions, which image most closely follows ALL the conditions below? Each user study session included 25 test samples along with 5 vigilance tests, which were randomly interTable B. Quantitative Results on General Orientation Controllability. ORIGEN maintains high accuracy even when evaluated on more general set of samples. Model Orientation Alignment Text Alignment Azi, Acc.@22.5 Azi, Abs. Err. Pol, Acc.@5 Pol, Abs. Err. Rot, Acc.@5 Rot, Abs. Err. CLIP VQA PickScore ORIGEN (Ours) 0.777 24.96 0.575 12.46 0.969 1. 0.263 0.710 0.219 denotes Table C. Quantitative comparisons on 3D orientation grounded image generation on MS-COCO-NView dataset. ORIGEN ours without reward-adaptive time rescaling, where ORIGEN represents our full method. Best and second-best results are highlighted in bold and underlined, respectively. Id Model 3-view Alignment 4-view Alignment Acc.@22.5 Abs. Err. CLIP VQA PickScore Acc.@22.5 Abs. Err. CLIP VQA PickScore 1 SD-Turbo [73] 2 SDXL-Turbo [57] 3 FLUX-Schnell [34] 4 C3DW [11] 5 Zero-1-to-3 [46] 6 ORIGEN (Ours) 0.257 0.189 0.312 0.504 0.366 0.824 75.09 78.44 75.04 53.53 68.70 20.99 0.261 0.265 0.268 0.187 0.266 0.262 0.721 0.722 0.739 0.334 0.646 0.721 0.223 0.227 0.230 0.188 0.210 0.220 0.244 0.196 0.424 - 0.321 0. 78.47 81.88 60.26 - 75.10 17.45 0.262 0.266 0.267 - 0.264 0.262 0.717 0.723 0.739 - 0.642 0.720 0.223 0.227 0.229 - 0.209 0.220 L) [67] to measure the Orientation Grounding Reward, as detailed in Sec. 3.2. For all experiments, we set γ = 0.3 and η = 1 in Alg. 1, as this configuration provides favorable balance between image quality and computational cost when evaluating with 50 NFEs. As described in Sec. 3.4, we additionally visualize our monitor function G( ˆR(x)) from Eq. 9 in Fig. B, which demonstrates how the function adaptively scales the step sizeassigning smaller steps in high-reward regions and larger steps in low-reward regions thereby improving convergence speed. About Zero-1-to-3 [46]. For the Zero-1-to-3 [46] baseline, we generated the base image using FLUX-Schnell with 4 steps and encouraged the model to generate front-facing objects by adding the facing front prompt. The model was then conditioned on the target azimuth while keeping the polar angle fixed at 0 to generate novel foreground views, which were later composited with the background. The foreground was segmented using SAM[29], and missing background pixels were inpainted using LaMa [61] before composition. About C3DW [11]. For C3DW [11], we utilized the orientation & illumination model checkpoint provided in the official implementation. This model is only capable of controlling azimuth angles for half-front views and was trained with scalar orientation values ranging from 0.0 to 0.5, where 0.0 corresponds to 90, and 0.5 corresponds to -90, with intermediate orientations obtained through linear interpolation. Using this mapping, we converted ground truth (GT) orientations into the corresponding input values for the model. Handling Back-Facing Generation. We empirically observed that our base model struggled to generate backfacing images when the prompt did not explicitly contain view information. We hypothesized that this issue arises because high-reward samples lie within an extremely sparse Figure B. Plot of our monitor function G( ˆR(x)). spersed. The final results were derived from responses of valid participants who correctly answered at least three vigilance tests, leading to total of 55 valid participants out of 100. No additional eligibility restrictions were imposed. H. Implementation Details Following the convention of OrientAnything [67], we set the standard deviation hyperparameters for the azimuth, polar, and rotation distributions to 20, 2, and 1, respectively, to transform discrete angles into the orientation probability distribution Π. For image quality evaluation, we utilized the official implementation of VQA-Score [43], and assessed CLIP Score [21], VQA-Score [43], and Pickscore [31] using OpenAIs CLIP-ViT-L-14-336 [25], LLaVA-v1.513b [45], and Pickscore-v1 [31] models, respectively. About ORIGEN. We employed FLUX-Schnell [34] as our one-step T2I generative model and OrientAnything (ViT15 Orientation Zero-1-to-3 [46] C3DW [11] ORIGEN (Ours) Orientation Zero-1-to-3 [46] C3DW [11] ORIGEN (Ours) Figure C. Additional Qualitative results on the MS-COCO-Single benchmark. region of the conditional probability space, making them inherently challenging to sample, even when employing our proposed approach. To address this issue, we explicitly added the phrase facing back in facing back cases (i.e., where 90 < ϕaz < 270) and optimized the noise accordingly. With this simple adjustment, the model effectively generated both facing front and back images while maintaining high-quality outputs. I. More Qualitative Comparisons on Single"
        },
        {
            "title": "Object Orientation Grounding",
            "content": "In the following, Fig. and Fig. present additional qualitative comparisons on the MS-COCO-Single and MSCOCO-NView benchmarks. J. More Qualitative Results on Multi Object"
        },
        {
            "title": "Orientation Grounding",
            "content": "We report more qualitative results on multi object orientation grounding (MS-COCO-Multi) in Fig. E. 16 Orientation SD-Turbo [73] SDXL-Turbo [57] FLUX-Schnell [34] Zero-1-to-3 [46] C3DW [11] ORIGEN (Ours) Figure D. Qualitative comparisons on MS-COCO-4-View benchmark. Orientation ORIGEN (Ours) Orientation ORIGEN (Ours) Orientation ORIGEN (Ours) Orientation ORIGEN (Ours) Figure E. Additional Qualitative results on the MS-COCO-Multi benchmark."
        }
    ],
    "affiliations": [
        "KAIST"
    ]
}