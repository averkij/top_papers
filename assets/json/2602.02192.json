{
    "paper_title": "ECHO-2: A Large-Scale Distributed Rollout Framework for Cost-Efficient Reinforcement Learning",
    "authors": [
        "Jie Xiao",
        "Meng Chen",
        "Qingnan Ren",
        "Jingwei Song",
        "Jiaqi Huang",
        "Yangshen Deng",
        "Chris Tong",
        "Wanyi Chen",
        "Suli Wang",
        "Ziqian Bi",
        "Shuo Lu",
        "Yiqun Duan",
        "Xu Wang",
        "Rymon Yu",
        "Ween Yang",
        "Lynn Ai",
        "Eric Yang",
        "Bill Shi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning (RL) is a critical stage in post-training large language models (LLMs), involving repeated interaction between rollout generation, reward evaluation, and centralized learning. Distributing rollout execution offers opportunities to leverage more cost-efficient inference resources, but introduces challenges in wide-area coordination and policy dissemination. We present ECHO-2, a distributed RL framework for post-training with remote inference workers and non-negligible dissemination latency. ECHO-2 combines centralized learning with distributed rollouts and treats bounded policy staleness as a user-controlled parameter, enabling rollout generation, dissemination, and training to overlap. We introduce an overlap-based capacity model that relates training time, dissemination latency, and rollout throughput, yielding a practical provisioning rule for sustaining learner utilization. To mitigate dissemination bottlenecks and lower cost, ECHO-2 employs peer-assisted pipelined broadcast and cost-aware activation of heterogeneous workers. Experiments on GRPO post-training of 4B and 8B models under real wide-area bandwidth regimes show that ECHO-2 significantly improves cost efficiency while preserving RL reward comparable to strong baselines."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 0 1 ] . [ 3 2 9 1 2 0 . 2 0 6 2 : r ECHO-2: Large-Scale Distributed Rollout Framework for Cost-Efficient Reinforcement Learning Jie Xiao1, Meng Chen2, Qingnan Ren1, Jingwei Song3, Jiaqi Huang1, Yangshen Deng4, Chris Tong1, Wanyi Chen1, Suli Wang5, Ziqian Bi1, Shuo Lu1, Yiqun Duan1, Xu Wang1, Rymon Yu1, Ween Yang1, Lynn Ai1, Eric Yang1, Bill Shi1 1Gradient, 2Fudan University, 3The University of Hong Kong, 4University of Edinburgh, 5Technical University of Darmstadt tianyu@gradient.network Equal contribution, Corresponding author Abstract Reinforcement learning (RL) is critical stage in post-training large language models (LLMs), involving repeated interaction between rollout generation, reward evaluation, and centralized learning. Distributing rollout execution offers opportunities to leverage more cost-efficient inference resources, but introduces challenges in wide-area coordination and policy dissemination. We present ECHO-2, distributed RL framework for post-training with remote inference workers and non-negligible dissemination latency. ECHO-2 combines centralized learning with distributed rollouts and treats bounded policy staleness as user-controlled parameter, enabling rollout generation, dissemination, and training to overlap. We introduce an overlap-based capacity model that relates training time, dissemination latency, and rollout throughput, yielding practical provisioning rule for sustaining learner utilization. To mitigate dissemination bottlenecks and lower cost, ECHO-2 employs peerassisted pipelined broadcast and cost-aware activation of heterogeneous workers. Experiments on GRPO post-training of 4B and 8B models under real wide-area bandwidth regimes show that ECHO-2 significantly improves cost efficiency while preserving RL reward comparable to strong baselines."
        },
        {
            "title": "Introduction",
            "content": "Reinforcement learning (RL) has become central component of the post-training pipeline for large language models (LLMs), enabling improvements in reasoning, tool use, safety alignment, and preference optimization at scale [13]. While modern RL algorithms such as PPO [4] and GRPO [5] have significantly improved stability and sample efficiency, the system design of LLM RL remains largely conventional. Most existing pipelines assume centralized deployment in which learners and rollout workers are co-located inside the data center, and training proceeds in tightly coupled iteration cycles [6]. This design increasingly conflicts with the cost structure of contemporary RL workloads. In many RLHF-style pipelines, rollout generation dominates wall-clock time, often accounting for the majority of the total training duration due to test-time scaling, while the learner remains intermittently idle [68]. Despite this imbalance, rollouts are typically executed on the same expensive GPU clusters used for learning, even though they consist primarily of forward passes and reward evaluation. As result, scaling RL post-training frequently translates into disproportionately high financial cost, limiting accessibility and slowing experimentation for rollout-heavy 1 tasks. Recent systems [711] have begun to relax strict synchronization in RL pipelines. However, these systems assume controlled training environments, where rollout workers and learners are deployed within the same administrative domain, connected by provisioned high-bandwidth interconnects, or organized as multidatacenter GPU clusters [10]. While asynchronous execution improves utilization under these assumptions, it does not fundamentally change the cost structure of RL post-training, as rollout generation remains tightly coupled to expensive centrally managed infrastructure. In contrast, distributed inference resources are usually abundant and cheaper, consisting of independent and loosely managed inference nodes such as geographically distributed cloud instances or opportunistic compute resources [12, 13]. RL training needs trajectories wherever they come from, so we can leverage cheap resources to aggregate enough rollouts to saturate training. However, decentralization naturally introduces new system characteristics, including heterogeneous throughput, wide-area communication latency, and dynamic availability. These effects are not artifacts of poor engineering, but inherent consequences of operating across heterogeneous and wide-area environments. Naively extending centralized synchronous or asynchronous RL designs to such settings lead to severe inefficiencies, including training bubbles and excessive overprovisioning. These observations motivate different research question: How can we reduce the cost of RL post-training by moving rollout generation away from centralized GPU clusters to distributed inference resources, while keeping centralized learner continuously utilized? To address this problem, we present ECHO-2, distributed RL framework built on simple architectural principle: centralized learning with distributed rollouts. Policy optimization runs on small, stable set of data-center GPUs, while rollout generation is offloaded to heterogeneous pool of Parallax [14] inference workers connected over wide-area networks. By decoupling rollout generation from centralized training infrastructure, ECHO-2 exposes new opportunities for cost reduction. ECHO-2 achieves this through two complementary mechanisms. First, it adopts bounded-staleness execution model: the learner may consume rollouts generated by policies that lag behind the current learner parameters within maximum staleness [7, 8, 15], where is user-specified staleness budget (S=1 allows one-training-step stale rollouts; larger permits proportionally higher delay). Bounded staleness provides temporal slack that absorbs wide-area latency and other overhead, enabling the overlap of rollout generation, policy dissemination, and training without stalling the learner. Second, ECHO-2 explicitly designs policy dissemination using peer-assisted broadcast: workers are organized in tree topology with multiple levels in limited-bandwidth environment. And they immediately forward newly received snapshots and start generating rollouts as soon as possible, leveraging aggregate fleet bandwidth to reduce tail broadcast latency. Importantly, once broadcast is pipelined in this way, bounded staleness is no longer merely mechanism to mask broadcast delay; it becomes system-level control parameter that trades rollout cost against training stability by determining how much the system can rely on cheaper workers while still saturating the learner. This perspective yields concrete provisioning problem: given staleness budget S, how much effective rollout capacity is required to keep the learner continuously busy under remote inference devices and wide-area networks? ECHO-2 addresses this with simple provisioning rule that relates measurable per-step training time, dissemination latency, and per-worker rollout throughput to the aggregate rollout capacity needed for continuous learning. Beyond provisioning, ECHO-2 exposes task-agnostic system abstraction that disaggregates rollout, learning, and data/reward handling into independent planes, enabling new RL workloads to be integrated by supplying datasets and reward logic without entangling algorithm code with infrastructure decisions. Using Parallax [14] as an inference-serving backend relieves ECHO-2 from having to deploy the model across heterogeneous resources. We evaluate ECHO-2 on standard GRPO post-training of 4B and 8B models across distributed rollout pools and wide-area bandwidth regimes. Our results show that ECHO-2 substantially reduces end-to-end training cost while maintaining RL quality comparable to strong centralized baselines. 2 In summary, we make the following contributions: distributed inference RL architecture for cost-efficient post-training. We propose system architecture that separates centralized learning from distributed rollout inference, enabling RL posttraining to reduce cost by offloading rollout generation from data-center GPU clusters to distributed resources. Overlap-aware execution and peer-assisted broadcast. We design system mechanisms that enable overlapping rollout inference, policy dissemination, and training across distributed rollout workers and centralized trainer via simple provisioning rule. ECHO-2 bounds policy staleness by user-specified budget and employs peer-assisted broadcast to reduce dissemination tail latency. Three-plane disaggregation of rollout, learning, and data. ECHO-2 cleanly decouples rollout inference, policy optimization, and data handling into independent execution planes, enabling flexible integration of new RL tasks. End-to-end evaluation on LLM RL workloads. Through extensive end-to-end experiments, we show that ECHO-2 substantially reduces the cost of RL post-training while maintaining learning quality, making large-scale RL more accessible under realistic resource constraints."
        },
        {
            "title": "2.1 RL Post-Training",
            "content": "Reinforcement learning is widely used in LLM post-training to improve reasoning, tool use, safety alignment, and preference optimization [16, 17]. Most practical pipelines iterate over three stages: (i) rollout generation under policy snapshot, (ii) reward evaluation for generated responses, and (iii) policy optimization using objectives such as PPO [4] or GRPO [5]. While the learning objective and update rule are algorithmic, the end-to-end efficiency and cost of RL post-training are heavily shaped by system-level choices."
        },
        {
            "title": "2.2 RL Post-Training Methods",
            "content": "State-of-the-art RL post-training frameworks are predominantly deployed in centralized settings. Recent systems such as verl [6] provide highly optimized centralized pipelines that achieve high throughput under data-center conditions through careful parallelization and coordination. To reduce learner idle time within centralized deployments, several systems adopt asynchronous rollout streaming. AReaL [7] streams rollouts to improve utilization, and AReaL-Hex [10, 18] extends this line with improved support for heterogeneous GPUs and communication optimizations. These systems primarily target controlled environments (single or multidatacenter clusters) and do not directly address distributed rollout execution over wide-area networks. Recent efforts also explore fully distributed training settings, including training nodes and rollout workers, where fully asynchronous reinforcement learning is used across globally distributed network, e.g., INTELLECT-2 [19]. In contrast, our focus is on hybrid setting: we retain centralized learning on stable training cluster while distributing inference across geographically distributed resources. This hybrid setting shifts the focus to leveraging low-cost, wide-area distributed inference workers for rollout generation. Moreover, the reinforcement learning framework should support wide range of environments and tasks; it is not feasible to dive into the underlying framework for every experiment. So, usability is crucial for an RL service [20], and ECHO-2 also provides user-friendly APIs to facilitate data handling for customized tasks."
        },
        {
            "title": "3 Design for Cost-Efficient Distributed RL",
            "content": "This section presents the design choice, system-level execution, and scheduling mechanisms that enable efficient RL training under wide-area inference workers."
        },
        {
            "title": "3.1 Overview",
            "content": "Rollout workers may differ in throughput and availability, and model dissemination over wide-area networks incurs non-negligible and variable latency. Enforcing fully synchronous, on-policy execution would require 3 Figure 1 Asynchronous RL execution in ECHO-2 with maximum bounded staleness = 3 and publication period κ = 2. The rollout, generation, and learner update proceed concurrently. Rollout workers uses the latest policy snapshot to generate trajectories into the replay buffer. The learner consumes trajectories from replay buffer and broadcasts new version of policy to rollout workers in each κ training steps. ECHO-2 generates rollouts at higher rate than it consumes during training. either rollout workers or the learner to idle, wasting expensive training resources and erasing the cost advantage of cheap compute. ECHO-2 is built on simple but underexploited observation: for modern LLM RL objectives, small amount of policy delay is often practically tolerable, and can be traded for substantially better system efficiency. Prior asynchronous RL systems have shown that bounded policy lag can improve utilization by hiding execution variability [7, 8, 10, 21] without harming training quality and model accuracy. ECHO-2 takes this idea one step further: rather than treating staleness as an artifact to be minimized, we treat it as first-class budget that makes distributed rollouts usable at low cost. We adopt an asynchronous execution model in which rollout generation, policy dissemination, and training proceed concurrently. As natural consequence, rollouts may be generated under policy snapshot that lags behind the learners current parameters. ECHO-2 explicitly bounds this lag: the learner may consume rollouts whose policy version is at most training steps older than the learner state, where is user-specified staleness budget. In this paper, we recognize policy version updates after each training step, which includes two model updates. ECHO-2 treats as user-specified staleness budget, allowing the learner to consume rollouts that are up to staleness, and we only need to broadcast every κ training step to preserve the outdated, as shown in figure 1. This bounded staleness creates temporal slack, decoupling the learner from wide-area rollout pool. It turns hard synchronization constraint into resource provisioning problem: given publication period κ and staleness budget S, how much aggregate rollout throughput is needed to keep the learner saturated? section 3.3 answers this question with an overlap condition that yields simple capacity rule in terms of measurable quantities of training time, distributed overhead, rollouts required for training step and rollout throughput (Ttrain, Tbcast, R, {µi}), where Tbcast includes communication latency and the model reload overhead. However, this overlap condition critically depends on how dissemination is realized in practice. In wide-area settings, snapshot delivery time may exhibit large tail latency, and naive push-to-all strategy makes Tbcast sensitive to the learner uplink and rollout worker downlink. ECHO-2 therefore treats broadcast as an engineered primitive: workers forward snapshots upon receipt and begin generating rollouts immediately after local installation, reducing the learner-visible broadcast latency Tbcast (details in section 4.2). By making Tbcast both measurable and reducible, we shrink the amount of temporal slack required to hide 4 communication, and shift the main role of toward controlling the cost-quality trade-off rather than just masking network latency."
        },
        {
            "title": "3.2 Execution Model and Notation",
            "content": "We formalize the execution model of distributed rollouts and centralized training. The learner runs on centralized training cluster and performs policy optimization steps. Each update consumes fixed number of completed trajectories. We denote by the number of rollouts required per learner update, and by Ttrain the wall-clock time per learner update. The learner periodically publishes immutable policy snapshots that rollout workers use for generation. We denote by κ the publication period in learner updates: new snapshot is published once every κ updates. Larger κ amortizes dissemination overhead but reduces snapshot freshness. We define policy staleness as follows. Let the learner state at the beginning of training step be version vt, and suppose the rollouts used to form the training batch at step are generated by snapshot version vx. The staleness of step is (t) and the maximum staleness over the run is max maxt (t). Users specify staleness budget S, and the system is configured to satisfy max S. We denote by Tbcast the learner-visible time for newly published snapshot to become available to generate rollout. The network latency is measured once we have resource pool. Let denote the set of available rollout workers. Each worker is characterized by: (i) µi, its effective rollout throughput (rollouts/sec), and (ii) ci, its monetary cost per unit time (e.g., $/hour). The throughput µi captures the end-to-end delivery rate of completed and rewarded trajectories into the replay buffer, implicitly incorporating inference time, reward computation, scheduling delay, network latency, and straggler effects. We define the unit throughput cost of worker as: ρi ci µi unit of rollout throughput. , which measures the cost required to supply one"
        },
        {
            "title": "3.3 Overlap Condition and Capacity Requirement",
            "content": "Bounded staleness enables asynchronous execution, but continuous learner utilization is achieved only if rollout generation and policy dissemination can overlap with training. We consider publication period of κ learner training steps, during which the learner consumes κR rollouts and publishes new policy snapshot once. To avoid training bubbles, rollout generation and dissemination must be completed within one publication period: κTtrain Tbcast + κR iA µi (cid:80) , where denotes the active rollout worker set. Rearranging yields an aggregate capacity requirement: (cid:88) iA µi µmin(κ) κR κTtrain Tbcast , κTtrain > Tbcast. (1) (2) This rule collapses heterogeneous worker pool into single measurable requirement on total throughput. Linking (S, κ) via conservative staleness bound. section derives conservative upper bound on the maximum staleness max under this execution model. Under the worst-case assumption that no rollout is generated from newly published snapshot until dissemination completes, the maximum staleness is bounded by cons max κ + (cid:38) Tbcast + µpool Ttrain (cid:39) 1, where µpool (cid:80) iA µi is the aggregate throughput of the active pool. 5 Given staleness budget S, the system chooses κ to satisfy cons typically implies max S. In our settings, the overlap condition (cid:38) Tbcast + µpool Ttrain (cid:39) 2, and thus simple sufficient choice is κ 1, for which we set κ = 1 by default unless otherwise stated. This choice is conservative (it accounts for step discretization and worst-case broadcast delays), while in practice the observed staleness (t) is often smaller due to progressive dissemination and immediate rollout start as described in section 4.2. figure 1 describes it, for = 3, κ = 2 and Tbcast/Ttrain < 1 (true in all our experimental settings), the maximum 3-steps staleness occurs during v3 v4 and v5 v6 training step. section shows that the conservative bound gives cons max 3 in training pipeline."
        },
        {
            "title": "3.4 Cost-Aware Provisioning under Heterogeneous Resources",
            "content": "The capacity rule specifies how much rollout throughput is needed; cost-aware provisioning decides which workers to activate to meet that requirement cheaply. Given candidate worker set W, ECHO-2 selects an active subset that satisfies equation (2) while minimizing cost: min AW (cid:88) iA ci s.t. (cid:88) iA µi µmin(κ). (3) While equation (3) resembles knapsack-style optimization, ECHO-2 adopts simple and practical approximations suitable for online operation. In particular, workers can be ranked by increasing unit throughput cost ρi, and the scheduler activates the cheapest subset whose cumulative throughput exceeds µmin(κ). This greedy strategy aligns with the system objective of minimizing rollout cost while maintaining learner saturation."
        },
        {
            "title": "3.5 Scheduling and Resource Pool Management",
            "content": "In distributed environments, throughput and availability vary over time. ECHO-2 therefore treats provisioning as closed-loop control problem: estimate effective capacity, compare against the required threshold, and adjust the active set. Each worker periodically reports lightweight statistics. The system maintains throughput estimate µi(t) and an availability indicator ai(t) {0, 1}. The effective pool capacity is: µpool(t) (cid:88) ai(t) µi(t). (4) Given measured Ttrain and Tbcast, the scheduler computes µmin(κ) via equation (2) and targets µtarget = γµmin(κ) with γ > 1 to absorb variability. If µpool(t) persistently falls below µtarget, ECHO-2 activates additional low-ρi workers; if capacity exceeds the target by sufficient margin, expensive workers are released."
        },
        {
            "title": "4 System Architecture and Implementation",
            "content": "This section describes how ECHO-2 realizes distributed and cost-efficient RL post-training with centralized learning and distributed rollouts. The design follows three-plane decomposition: Rollout, Learning, and Data, which are connected by versioned, immutable messages and shared replay buffer figure 2. Rollout Plane: distributed fleet of workers that repeatedly generates rewarded trajectories under locally installed snapshot version ˆv and pushes version-tagged results to the buffer. This plane is responsible for realizing the effective throughput µi and immediately forwarding. Learning Plane: centralized learner that consumes trajectories and performs training step with two model updates. It enforces bounded staleness (S) when sampling data, and publishes snapshots once every κ learner updates. 6 Figure 2 System Architecture of ECHO-2. The system adopts three-plane decomposition for cost-efficient distributed RL. The centralized Learning Plane performs policy optimization using data sampled with bounded staleness budget. The Data Plane provides unified interface for task adaptation and manages versioned trajectory storage. The distributed Rollout Plane executes asynchronous generation across workers using pipelined broadcast. Data Plane: task adapters for prompts, trajectory schemas, reward and loss function design. This plane provides task-agnostic interface so new workloads are integrated by swapping datasets and reward logic, without touching scheduling or infrastructure. We outlined the workflow of ECHO-2 in algorithm 1."
        },
        {
            "title": "4.1 Versioned Execution and Bounded Staleness",
            "content": "Policy publication. The learner publishes immutable policy snapshots once every κ update steps (algorithm 1 Line 15-17). Between two publications, the learner may perform multiple updates while workers continue generating rollouts under their most recent installed snapshot. The publication period κ is chosen to respect the staleness budget (default κ 1, and we use κ = 1 unless otherwise stated), which should 2 in ECHO-2. Rollout generation. Each rollout worker maintains local snapshot version ˆv. For each prompt x, the worker samples response πˆv( x), computes reward = R(x, y), and emits trajectory (x, y, r, ˆv) into the buffer (algorithm 1 Line 26-27). Reward computation is performed entirely in the Rollout Plane. We reject items that violate the data format to simply but effectively achieve data integrity. Replay buffer management. The replay buffer stores version-tagged trajectories and supports selective sampling. At learner update index vt, only trajectories with bounded lag are admissible:v vt S. Older trajectories are discarded. This enforces bounded staleness without imposing global synchronization, treating rollouts as stream [10]. Bounded staleness constrains data freshness but does not impose fixed update schedule. The learner advances whenever sufficient eligible data are available. The parameter therefore bounds the maximum policy lag between rollout generation and training, providing temporal slack to absorb latency without modifying the underlying RL objective."
        },
        {
            "title": "4.2 Peer-to-Peer Broadcast and Asynchronous Rollout Start",
            "content": "A naive push-to-all strategy (star topology) achieves minimal communication latency when we have unlimited bandwidth between the training center and remote workers. Otherwise, it makes the learners uplink and tail receivers bottleneck. ECHO-2 organizes tree topology network and reduces dissemination latency under bandwidth constraints, enabling data transmission to leverage the aggregate bandwidth of the rollout fleet. common wide-area regime is that the learner has finite uplink budget B0, while each worker is capped by smaller per-node bandwidth Bw. When B0 Bw with parallel transmit links, ECHO-2 uses simple striped chain design: the learner acts as bandwidth sorter and splits each snapshot of size into disjoint stripes {Dj}N (each G/N ), then streams stripe Dj to first-hop seed A1,j at rate Bw. Each seed forms chain and serves as relay: upon receiving data (chunked), it immediately forwards the same stripe downstream to its unique child A2,j using store-and-forward streaming, which continues along the chain. After pipeline warm-up, dissemination approaches line-rate on each stripe with minimal control overhead, since each worker maintains only one inbound and one outbound flow (fan-out = 1), avoiding complex multi-parent scheduling. j=1 To acquire as many rollouts as possible, upon receiving any new chunks, worker forwards them immediately (algorithm 1 Line 22). Upon completing installation of the new snapshot, it immediately switches its local version ˆv and starts generating rollouts under the new version."
        },
        {
            "title": "4.4 Data Plane Interfaces and Task Integration",
            "content": "The Data Plane defines the task semantics of ECHO-2 while preserving the versioned execution in section 4.1 and the end-to-end loop in algorithm 1. Concretely, it specifies how workload is mapped to immutable, version-tagged trajectory records stored in the replay buffer: τ = (x, y, r, v, Ω), where (x, y) is the prompt-response pair, is the scalar reward, is the snapshot version used to generate y, and Ω is optional task metadata. The buffer indexes τ by version and enforces bounded staleness. task is integrated by implementing Data Plane adapter that (i) constructs prompts x, (ii) defines the reward function used by rollout workers to produce = R(x, y), and (iii) defines how Ω is materialized into learner-side training signals (e.g., masks and normalized advantages) under the chosen objective (e.g., GRPO with KL regularization). Detailed interfaces and an end-to-end example (poker sandbox integration) are provided in section D."
        },
        {
            "title": "5 Experiments",
            "content": "We evaluate ECHO-2 in distributed RL post-training settings and ask three questions: (Q1) Does ECHO-2 reduce the cost to reach target RL quality compared to centralized pipelines? (Q2) Is RL quality robust to bounded staleness in wide-area distributed rollouts? (Q3) Do our overlap model and system mechanisms predict and improve learner utilization under wide-area constraints?"
        },
        {
            "title": "5.1 Experimental Setup",
            "content": "Models. We post-train two base models: Qwen3-4B and Qwen3-8B [22]. Unless otherwise stated, we use the same GRPO hyperparameters across all systems: global batch size = 128, maximum generation length 8192, 8 (a) Costquality on AIME24. (b) Effect of bounded staleness S. (c) Bubble ratio vs rollout workers. Figure 3 Experimental results of ECHO-2 on Qwen3-8B. (a) Costquality efficiency on AIME24 under the WAN setting. Dashed lines indicate computed costs based on steady-state training time and public GPU rental prices (right y-axis). (b) Impact of staleness on RL stability. Performance remains robust for 6, while excessive staleness (S = 11) leads to divergence. (c) Learner bubble ratio as function of the number of rollout workers. Vertical dashed lines denote the theoretical minimum workers. temperature 1.0, top-p 0.95, rollout n: 16. We disable chain-of-thought prompting and evaluate avg@64. Task and reward. Our main task is AIME24 [23] with verifiable final answers. Each rollout receives reward = R(x, y) computed by the dataset corresponding match answer checker. Unless otherwise noted, we report AIME accuracy as the primary RL quality metric. We additionally report results on broader math benchmark suite in section C.2. System deployment. (1) Learning plane. The learner runs on 4 A100 80GB for ECHO-2, 8 A100 for centralized baseline. We measure the steady-state per-update time Ttrain as the median over 5 offline training steps. (2) Rollout plane. To simplify experimental validation, rollouts run on distributed pool of RTX 5090 workers, served by Parallax [14], hardware-agnostic inference service. Network regimes. To study the impact of bandwidth constraints, we cap the learners outbound uplink budget to B0 {unlimited, 300-1000Mbps} and cap each workers download rate to Bw=100Mbps. In experiments, we identify Tbcast as the elapsed time from publication until target fraction 1/γ of active workers have installed the snapshot. This aligns the abstraction in section 3 with the observed dissemination behavior and captures the practical effect of tail receivers. State-of-the-art baselines. (1) Centralized-Sync (verl [6]: synchronized pipeline where rollouts are co-located with the learner on the same data-center GPUs (8 GPUs for training and inference). (2) Centralized-Async (verl-async [6]: streaming/asynchronous baseline within the data center (AReaLstyle [7]) that overlaps rollout generation and learning while assuming high-bandwidth, low-latency connectivity (4 GPUs for training and others for inference). ECHO-2 ablations. (1) ECHO-2-NoP2P: disables peer-assisted broadcast and uses direct learnerworker dissemination. (2) ECHO-2-NoCost: disables cost-aware provisioning and uses random worker activation. Cost and Utilization Metrics. We report dollar costs computed from publicly available rental prices in table 1, using Google Cloud to represent data-center hardware and vast.ai simulate distributed customer-grade resources. Let pA100 and p5090 denote hourly prices. Dollar cost is: Cost$ = (cid:80) g{A100,5090} pg (GPU-hoursg). , where Tidle is the waiting time due to We measure learner bubble ratio (idle fraction) as: insufficient admissible rollouts. Tidle Tidle+Ttrain,active Table 1 Costs and pricing sources. USD/hour denotes the hourly rental price per single GPU, collected on 28/01/2026. GPU type Price Symbol USD / hour Platform A100 80GB RTX 5090 pA100 p5090 $3.06 $0.35 Google Cloud vast.ai Table 2 Ablation summary. We evaluate the impact of removing peer-assisted (P2P) broadcast and cost-aware provisioning (Cost) in ECHO-2. #Mach, Tbcast, and Wait denote the rollout fleet size, dissemination latency, and learner idle time, respectively. Method #Mach Cost/Step Tbcast (s) Wait (s) Full w/o P2P w/o P2P w/o Cost 9 9 10 8.098 8.432 8.630 9.339 1437 1830 1872 1437 0 131.9 84."
        },
        {
            "title": "5.2 Cost-Quality Efficiency",
            "content": "Given the discreteness and variance of AIME accuracy, we interpret costquality efficiency as the cumulative cost required to reach target accuracy threshold. We first ask whether ECHO-2 improves the cost-quality of RL post-training under network setting B0=100Mbps, Bw=1Gbps. We conduct experiments to compare different baselines and plot the evaluation curve in figure 3a. Training time per step (Ttrain) of verl-sync/async and ECHO-2 (S=3/4) are 1508.2s, 1582.3s, 1631.2s, and 1649.3s, respectively. To further summarize the trade-off, by combining Ttrain and prices in steady training pipeline, we can fit = ax line for all methods, as dash lines that shown in figure 3a, and the right y-axis indicates the costs. The Qwen3-4B model (section C.1) shares the same trend and conclusion with Qwen3-8B, ECHO-2 consistently dominates centralized pipelines: at matched AIME accuracy, ECHO-2 reduces cumulative cost by 33.3-36.3%, while at matched cost, ECHO-2 achieves final accuracy by +-0.03 points. This improvement arises because rollout generation can be executed on cheaper distributed GPUs without stalling the centralized learner, as long as the overlap condition in equation (1) is maintained."
        },
        {
            "title": "5.3 RL Quality under Bounded Staleness",
            "content": "We sweep staleness budget to quantify how it trades training stability/quality for system efficiency and cheaper workers. We evaluate whether bounded staleness affects RL quality by sweeping {3, 4, 6, 11} while keeping all other settings fixed (recall that the data staleness in stable pipelined ECHO-2 S). figure 3b shows that moderate staleness does not degrade final quality: for 6, ECHO-2 achieves reward score within 5% fluctuation of the synchronous baseline, with similar convergence trends, and reducing cost. In contrast, overly large staleness (S=11) can lead to instability in standard GRPO, consistent with the intuition that stale data gradually deviates from the current policy distribution."
        },
        {
            "title": "5.4 Validating the Overlap Condition",
            "content": "equation (2) predicts threshold behavior: as rollout capacity increases, learner bubbles should rapidly vanish once the system enters the feasible overlap region. We validate this prediction by sweeping the effective pool size. figure 3c illustrates that, as the pool grows, bubble ratio drops consistently towards zero near the predicted threshold, confirming that the overlap model provides practical provisioning rule. Larger shifts the transition left, showing that staleness acts as an explicit control knob that trades policy freshness for reduced rollout capacity."
        },
        {
            "title": "5.5 Ablation Study",
            "content": "5.5.1 Broadcast under Bandwidth Constraints We evaluate policy dissemination latency under different broadcast strategies as the rollout fleet scales. We measure the learner-visible broadcast time Tbcast as the elapsed time from snapshot publication until target fraction of active workers have fully installed the snapshot and can start generating rollouts under the new version (section 4.2), and we use = 1/γ = 1/1.1 = 0.9 here. 10 Figure 4 Policy broadcast latency Tbcast vs. rollout fleet size. Comparison of three dissemination strategies across different numbers of nodes . Star-Limited (with learner uplink B0 [300, 800]Mbps) suffers from linear latency growth as the learner becomes bandwidth bottleneck. Tree-Pipelined dissemination, by utilizing chunked peer forwarding, maintains near-constant broadcast time that scales efficiently with the fleet size, closely matching the idealized Star-Unlimited baseline. We compare three dissemination settings while sweeping the number of active rollout workers : (i) StarUnlimited, an idealized push-to-all broadcast with no uplink cap at the learner; (ii) Star-Limited, push-to-all with capped learner uplink budgets B0 = 300-800 Mbps and per-worker bandwidth cap Bw = 100 Mbps; and (iii) Tree-Pipelined dissemination that uses chunked store-and-forward forwarding so that workers relay data upon receipt. figure 4 shows that Star-Limited suffers from rapidly increasing broadcast time as grows: with fixed learner uplink budget B0, the learner becomes the bottleneck. In contrast, Tree-Pipeline keeps Tbcast close to Star-Unlimited even under the same caps, by pipelining chunk delivery and exploiting aggregate bandwidth of the rollout fleet through peer forwarding. 5.5.2 Broadcast and Cost-Aware Provisioning We isolate the benefit of cost-aware activation using ECHO-2-NoCost, which uniformly samples capable workers, while still targeting the full-overlap goal. To ensure this ablation is informative, we evaluate under mixed-price rollout pool where workers have heterogeneous costs. We report (i) cost per step, (ii) dissemination latency, and (iii) waiting time between two training steps. table 2 shows that removing peer-assisted broadcast increases communication latency and bubble time, requiring additional machines and costs to reduce the bubble ratio. Disabling cost-aware provisioning increases the cost to reach the same target quality by activating suboptimal workers under heterogeneity. Together, these ablations show that ECHO-2s mechanisms are necessary to achieve the end-to-end cost-efficiency performance in figure 3a. We discuss task-agnostic data plane use case in section D."
        },
        {
            "title": "6 Limitations and Future Work",
            "content": "ECHO-2 relies on the empirical robustness of modern LLM RL objectives to bounded policy lag. While moderate staleness preserves GRPO post-training quality in our experiments, we do not provide formal guarantees, and the safe range may depend on the task and reward signal. Developing theoretical staleness control remains future work. Our peer-assisted broadcast mitigates uplink bottlenecks, whereas our future work will include delta or quantized updates and cache-aware deployment. ECHO-2 focuses on centralized learning with distributed rollouts. Extending the design to multiple or geographically replicated learners is 11 promising, but it introduces new challenges in synchronization and policy consistency, and further validation across wider range of model sizes remains future work."
        },
        {
            "title": "7 Conclusion",
            "content": "We presented ECHO-2, an RL framework for LLM post-training that separates centralized learning from distributed rollouts. By treating bounded staleness as control knob, modeling overlap-based capacity, and on-demand worker activation ECHO-2 enables cost-aware provisioning under wide-area execution. With peer-assisted pipelined broadcast, ECHO-2 reduces dissemination overhead. Experiments on GRPO posttraining of 4B and 8B models show that ECHO-2 significantly lowers training cost while preserving RL quality comparable to baselines."
        },
        {
            "title": "References",
            "content": "[1] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. [2] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [3] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [4] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. [5] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. [6] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, pages 12791297, 2025. [7] Wei Fu, Jiaxuan Gao, Xujie Shen, Chen Zhu, Zhiyu Mei, Chuyi He, Shusheng Xu, Guo Wei, Jun Mei, Jiashu Wang, et al. Areal: large-scale asynchronous reinforcement learning system for language reasoning. arXiv preprint arXiv:2505.24298, 2025. [8] Jingkai He, Tianjian Li, Erhu Feng, Dong Du, Qian Liu, Tao Liu, Yubin Xia, and Haibo Chen. History rhymes: Accelerating llm reinforcement learning with rhymerl. arXiv preprint arXiv:2508.18588, 2025. [9] Michael Noukhovitch, Shengyi Huang, Sophie Xhonneux, Arian Hosseini, Rishabh Agarwal, and Aaron Courville. Asynchronous rlhf: Faster and more efficient off-policy rl for language models. arXiv preprint arXiv:2410.18252, 2024. [10] Yinmin Zhong, Zili Zhang, Xiaoniu Song, Hanpeng Hu, Chao Jin, Bingyang Wu, Nuo Chen, Yukun Chen, Yu Zhou, Changyi Wan, et al. Streamrl: Scalable, heterogeneous, and elastic rl for llms with disaggregated stream generation. arXiv preprint arXiv:2504.15930, 2025. [11] Jie Xiao, Changyuan Fan, Qingnan Ren, Alfred Long, Yuchen Zhang, Rymon Yu, Eric Yang, Lynn Ai, and Shaoduo Gan. Echo: Decoupling inference and training for large-scale rl alignment on heterogeneous swarms. arXiv preprint arXiv:2508.05387, 2025. [12] Alexander Borzunov, Dmitry Baranchuk, Tim Dettmers, Maksim Riabinin, Younes Belkada, Artem Chumachenko, Pavel Samygin, and Colin Raffel. Petals: Collaborative inference and fine-tuning of large models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pages 558568, 2023. [13] Max Ryabinin, Tim Dettmers, Michael Diskin, and Alexander Borzunov. Swarm parallelism: Training large models can be surprisingly communication-efficient. In International Conference on Machine Learning, pages 2941629440. PMLR, 2023. [14] Chris Tong, Youhe Jiang, Gufeng Chen, Tianyi Zhao, Sibian Lu, Wenjie Qu, Eric Yang, Lynn Ai, and Binhang Yuan. Parallax: Efficient llm inference service over decentralized environment. arXiv preprint arXiv:2509.26182, 2025. [15] Runlong Zhou, Lefan Zhang, Shang-Chen Wu, Kelvin Zou, Hanzhi Zhou, Ke Ye, Yihao Feng, Dong Yin, Alex Guillen Garcia, Dmytro Babych, et al. Rlax: Large-scale, distributed reinforcement learning for large language models on tpus. arXiv preprint arXiv:2512.06392, 2025. [16] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022. 13 [17] Timo Kaufmann, Paul Weng, Viktor Bengs, and Eyke Hüllermeier. survey of reinforcement learning from human feedback, 2024. [18] Ran Yan, Youhe Jiang, Tianyuan Wu, Jiaxuan Gao, Zhiyu Mei, Wei Fu, Haohui Mai, Wei Wang, Yi Wu, and Binhang Yuan. Areal-hex: Accommodating asynchronous rl training over heterogeneous gpus. arXiv preprint arXiv:2511.00796, 2025. [19] Prime Intellect Team, Sami Jaghouar, Justus Mattern, Jack Min Ong, Jannik Straube, Manveer Basra, Aaron Pazdera, Kushal Thaman, Matthew Di Ferrante, Felix Gabriel, et al. Intellect-2: reasoning model trained through globally decentralized reinforcement learning. arXiv preprint arXiv:2505.07291, 2025. [20] Xufang Luo, Yuge Zhang, Zhiyuan He, Zilong Wang, Siyun Zhao, Dongsheng Li, Luna K. Qiu, and Yuqing Yang. Agent lightning: Train any ai agents with reinforcement learning, 2025. [21] Haizhong Zheng, Jiawei Zhao, and Beidi Chen. Prosperity before collapse: How far can off-policy rl reach with stale data on llms? arXiv preprint arXiv:2510.01161, 2025. [22] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [23] MAA. American invitational mathematics examination (AIME), 2024. [24] Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, et al. Omni-math: universal olympiad level mathematic benchmark for large language models. arXiv preprint arXiv:2410.07985, 2024. [25] Daman Arora, Himanshu Singh, et al. Have llms advanced enough? challenging problem solving benchmark for large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 75277543, 2023. [26] Jingxuan Fan, Sarah Martinson, Erik Wang, Kaylie Hausknecht, Jonah Brenner, Danxian Liu, Nianli Peng, Corey Wang, and Michael Brenner. Hardmath: benchmark dataset for challenging problems in applied mathematics. arXiv preprint arXiv:2410.09988, 2024. [27] Minh-Thang Luong, Dawsen Hwang, Hoang Nguyen, Golnaz Ghiasi, Yuri Chervonyi, Insuk Seo, Junsu Kim, Garrett Bingham, Jonathan Lee, Swaroop Mishra, et al. Towards robust mathematical reasoning. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 3540635430, 2025. [28] Qwen Team. Qwen3 technical report, 2025. [29] OpenAI. Openai gpt-5 system card, 2025. [30] xAI. Grok 4 model card. Technical report, xAI, 2025. [31] Anthropic. Claude sonnet 4.5 system card. Technical report, Anthropic, 2025. [32] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities, 2025. 14 Worst-Case Staleness Bound under Overlap This appendix derives conservative upper bound on the maximum policy staleness max in ECHO-2, and shows how the overlap condition tightens this bound. A.1 Execution Semantics and Conservative Assumption We consider the following execution semantics, which intentionally model worst-case scenario: Policy snapshots are published every κ learner update steps, at the end of training step. Training batches are formed at the beginning of each step. In the most conservative case, rollout workers generate no trajectories from newly published snapshot until dissemination completes after Tbcast time. After dissemination completes, rollouts from the new policy are generated at aggregated rate µpool. This model intentionally ignores progressive dissemination and early rollout start, and therefore upper-bounds the staleness that can occur in practice. A.2 Baseline Worst-Case Staleness Bound Let denote the number of learner steps elapsed since snapshot is published. By time nTtrain, the number of rollouts generated from the new policy is at most G(n) = µpool max(cid:0)0, nTtrain Tbcast (cid:1). The earliest step at which at least new-policy rollouts are available satisfies G(n) (cid:38) Tbcast + µpool Ttrain (cid:39) . (5) (6) At publication, the learner version advances by κ relative to the previous published snapshot. Since no new-policy rollout can be consumed during the first 1 steps after publication, the maximum staleness under this conservative model is cons max = κ + (cid:38) Tbcast + µpool Ttrain (cid:39) 1. (7) A.3 Tightening the Bound using the Overlap Condition The baseline bound in equation (7) depends on the rollout throughput µpool. We now show that under the overlap condition, this dependence can be tightened. Recall the overlap condition: Rearranging yields κTtrain Tbcast + κR µpool . µpool Ttrain Tbcast κ . Substituting equation (9) into the numerator of equation (7) gives Tbcast + µpool (cid:18) Ttrain + 1 (cid:19) 1 κ Tbcast. 15 (8) (9) (10) Dividing both sides by Ttrain and taking the ceiling, (cid:38) Tbcast + µpool Ttrain (cid:39) (cid:24)(cid:18) 1 + 1 (cid:19) Tbcast Ttrain 1 κ (cid:25) . Substituting back into equation (7) yields tightened bound: cons max κ + (cid:24)(cid:18) 1 (cid:19) Tbcast Ttrain 1 κ (cid:25) . Implication for κ = 2 A.4 For the common case κ = 2, the overlap condition implies Tbcast < 2Ttrain, and thus Therefore, 0 <"
        },
        {
            "title": "Tbcast\nTtrain",
            "content": "< 1. cons max 3. (11) (12) (13) This bound corresponds to worst-case execution in which new-policy rollouts become available only after dissemination completes. In practice, rollout workers begin generating trajectories as soon as they receive the update during dissemination, making the observed staleness typically smaller than this bound. Corollary: Single-Parameter Configuration. Consider the configuration used by ECHO-2, in which the publication period is set to κ = 1. Substituting into the conservative bound yields cons max = 1 + (cid:38) Tbcast + µpool Ttrain (cid:39) 1. If the system satisfies the overlap condition and Tbcast/Ttrain < 1, which holds in all our experimental settings, then (cid:38) Tbcast + µpool Ttrain (cid:39) 2, and therefore cons max S. This result justifies exposing as the sole staleness control parameter in ECHO-2. ECHO-2 execution B.1 Overall Procedure We illustrate the end-to-end execution model of ECHO-2 in algorithm 1, which only includes Rollout Plane and Learning Plane since training process is transparent to Data Plane. B.2 Supplementary System Design B.2.1 Low-Frequency Adjustment The scheduler maintains an active worker set and monitors its aggregate throughput (cid:80) iA aiµi. If capacity persistently falls below µtarget, additional workers with low unit throughput cost ρ are activated; if capacity exceeds the target by sufficient margin, expensive workers are gradually released. This design ensures that the learner remains saturated whenever feasible, while avoiding frequent reconfiguration and unnecessary rollout cost. 16 Algorithm 1 Execution of ECHO-2 1: Shared: replay buffer B, worker pool W, active set 2: Learner state: update index 0 3: Worker state: each worker maintains local snapshot version ˆvi 4: Learning Plane: 5: while training not converged do 6: 7: if scheduling tick or sustained capacity deviation then Estimate µpool = (cid:80) iA aiµi Compute µtarget = γ µmin(κ) using equation (2) Adjust by activating/releasing workers based on ρi (section 4.3) if has at least admissible trajectories with vt then Sample batch from subject to staleness version Perform one policy update (time Ttrain) + 1 if mod κ = 0 then 14: 15: 16: Rollout Plane, on each worker in parallel: 17: while worker is active do 18: Receive and forward snapshot chunks as relay (Sec. 4.2) if newer snapshot is fully installed then Publish snapshot with version and trigger dissemination (section 4.2) Update local version ˆvi vnew Sample and generate πˆvi ( x) and compute reward = R(x, y) Push trajectory (x, y, r, ˆvi, Ω) into 8: 9: 10: 11: 12: 13: 19: 20: 21: 22: Table 3 Reward scores after RL post-training on math reasoning benchmarks. AIME24 reports avg@64, JEE reports avg@8, and OmniMath / HardMath /IMO-A report avg@1 (i.e., Pass@1). IMO-A denotes IMO-answer-400. All results are reported under the same training configuration. Method Model AIME24 OmniMath JEE HardMath IMO-A MEAN initial verl Qwen3-4B Qwen3-8B Qwen3-4B Qwen3-8B ECHO-2 Qwen3-4B Qwen3-8B 25.0 29.1 45.78 47. 45.16 48.8 28.5 28.12 40.65 41.92 41.67 40.31 20.15 18.57 36.82 32. 34.32 39.51 11.15 11.18 24.17 25.33 26.3 26.87 11.0 11.0 23.25 29. 20.75 23.25 19.16 19.59 34.13 35.30 33.64 35."
        },
        {
            "title": "C Supplementary Experiments",
            "content": "C.1 Results of Qwen3-4B In this section, we show cost-quality comparison for Qwen3-4B, which demonstrates similar performance to Qwen3-8B in figure 3c. Moreover, as shown in figure 6, we also conduct empirical experiments of staleness for Qwen3-4B with standard GRPO. C.2 Wide Range Benchmarks table 3 reports reward scores after RL post-training on 5 math reasoning benchmarks: AIME24 [23], OmniMath [24], JEE [25], HardMath [26], and IMO-answer-400 [27]. We compare ECHO-2 (S = 3) with verl under the same reward model and training configuration, using Qwen3-4B and Qwen3-8B as the base models. Across all datasets and both model scales, ECHO-2 maintains reward performance comparable to verl, demonstrating that distributed rollouts with bounded staleness do not degrade RL optimization quality and provide cost-efficiency opportunity, and ECHO-2 realizes it. 17 Figure 5 Costquality on AIME for Qwen3-4B. Figure 6 Effect of bounded staleness on RL quality in ECHO-2 for Qwen3-4B. Beyond Math: Poker Game Alignment via Sandbox Integration To demonstrate the versatility of Echo-2s decoupled Data Plane, we extend our evaluation from static mathematical reasoning to dynamic, interactive environment: No-Limit Texas Holdem. This case study illustrates how Echo-2 adapts to non-standard modalities (game logs and episodic returns) without modifying the underlying Learning Plane or Rollout Plane. Concretely, we only instantiate task-specific Data Plane Adapter that (i) interfaces with poker sandbox, (ii) standardizes raw logs into the canonical rollout schema, and (iii) materializes the additional metadata Ω (e.g., token masks and normalized advantages) required by GRPO, yielding the canonical record τ = (x, y, r, v, Ω) consumed by the shared replay buffer and the learner. Figure 7 Overview of the Echo-2 Poker Game Alignment system. The Orchestrator (Parallax) interfaces with the Sandbox (E) to generate Trajectory Logs (Li). The Log-to-Rollout Converter (C) processes these logs into Training Rollouts (D), which are then used by the Trainer (T ) to update the policy parameters (θ), closing the iterative training loop. D.1 System Overview: Data Plane Instantiation We implement specialized Data Plane Adapter that bridges the raw poker sandbox and Echo-2s training interface. As shown in Figure 7, the pipeline consists of three phases: Sandbox Interaction, Log Standardization, and Reward-Augmented Rollout Generation. The adapter outputs unified rollout tuple that can be consumed directly by the generic Rollout Plane and Learning Plane. Let the policy be an autoregressive language model πθ(y x), where represents the serialized game context and represents the agents decision (betting action text). We denote reference policy (for KL regularization) as πref. 18 One-line task switching via sandbox adapters. key benefit of the decoupled Data Plane is that switching to new interactive task only requires swapping the sandbox adapter configuration, while the Rollout/Learning Planes (and the replay schema τ = (x, y, r, v, Ω)) remain unchanged. Unified Orchestration API (Poker MOBA) 1 # Task-specific: only the Data Plane adapter changes. 2 # Switching tasks is configuration-only: 3 # adapter.env = \"moba://hok_v2\" ; adapter.serializer = \"moba_obs->prompt_v2\" 4 adapter = SandboxAdapter( env=\"poker://nlhe_6max_v1\", # or \"moba://dota2_v3\", \"moba://hok_v \" serializer=\"envstate->chat_prompt_v2\", action_renderer=\"action->text_v1\", reward_spec=\"episodic_return\", metadata_spec=\"grpo:turn_mask+adv_norm\" 5 6 7 9 10 ) 11 12 echo = Echo2(policy=pi_theta, reference=pi_ref) 13 14 echo.set(dataset=adapter.stream(mode=\"offline_or_live\")) 15 echo.set(staleness=3) 16 echo.warmup() 17 echo.run() D.2 Phase 1: Environment Interaction (Sandbox Raw Logs) We deploy sandbox environment simulating poker table. For each episode i, the environment records raw interaction log Li: Li = {(si,t, ai,t, ri,t)}Ti t=1, (14) where: si,t is textual description of the private hand, community cards, pot size, and derived odds (e.g., \"Hand: [Ah, Kd], Board: [Qs, Th, 2c], Pot: 100\"). ai,t is structured action rendered as text (e.g., \"Action: ri,t is the immediate chip delta, i.e., the change in chip stack relative to the previous turn. Raise 50\"). Unlike math tasks where rollouts are generated by the model under training, poker logs may initially come from rule-based baselines or prior model iterations, demonstrating Echo-2s ability to consume off-policy data (and strictly on-policy data if connected to live rollout workers). In ECHO-2, the scalar reward is produced in the Rollout Plane (co-located with environment interaction), while the Data Plane defines and the post-processing rules used to derive Ω for learning. D.3 Phase 2: Standardization and Conversion (Raw Logs Canonical Messages) The core responsibility of the Data Plane is to convert heterogeneous logs Li into unified rollout format compatible with the generic Learning Plane. The adapter transforms the raw log into chat-formatted message sequence Mi by flattening complex game states into standard prompt-response template: Mi = (cid:2)msys i,0 , musr i,0 , masst i,1 , musr i,1 , . . . , masst i,Ti (cid:3), (15) where msys message followed by an assistant action message: i,0 = SystemPrompt encodes global poker rules Prules, and each turn is represented as user state musr i,t = State: si,t, masst i,t = ai,t. (16) 19 Optionally, for bookkeeping we may insert rewards as user messages Reward: signals are ultimately computed from numeric rewards inside the Data Plane. ri,t; however, training We then linearize Mi using the tokenizer chat template and obtain token IDs: xi = (xi,1, . . . , xi,Li) = Tokenize(Mi), (17) along with an attention mask ai {0, 1}Li. Echo-2 uses left padding to batch variable-length episodes. The following implementation demonstrates how raw environment outputs are iteratively converted into the user-assistant message structure: Constructing Canonical Messages 1 # Extract from GemEnvRollout.get_lm_inputs 2 messages = [ {\"role\": \"system\", \"content\": self.system_prompt}, {\"role\": \"user\", \"content\": self.prefix_lookup[env_id]} 3 4 5 ] 6 7 for idx, content in enumerate(env_output[\"history\"]): messages[-1][\"content\"] += f\"nTurn {idx + 1}:n\" 8 9 10 11 13 14 15 16 17 19 20 21 # Flatten State s_{i,t} if \"state\" in content: FORMAT = \"<answer> ... </answer>\" messages[-1][\"content\"] += f\"State:n{content[state]}n Always output: {FORMAT}n\" # Append Assistant Action a_{i,t} if \"llm_response\" in content: messages.append({\"role\": \"assistant\", \"content\": content[\"llm_response\" ]}) # Optional: Insert intermediate rewards for bookkeeping if \"reward\" in content: messages.append({\"role\": \"user\", \"content\": f\"Reward:n{content[reward ]}n\"}) D.4 Phase 3: Turn-Aware Masking and Reward-Augmented Rollouts Poker supervision is sparse and episodic; therefore, the Data Plane additionally computes (i) turn-aware masks that restrict learning to assistant tokens, and (ii) advantages derived from final chip outcomes. D.4.1 Turn-Aware Masking We construct turn indicators using special turn-start token ID τstart (e.g., <im_start> in Qwen-style templates). Define: ui,t = I[xi,t = τstart], ci,t = (cid:88) k=1 ui,k, where ci,t is the chat-turn index of token t. The assistant-response mask is: mresp i,t = I[ci,t > 1] I[ci,t mod 2 = 1], (18) (19) i,t = mresp selecting tokens after the system prompt that belong to assistant turns. We set the loss mask as mloss , so learning is restricted to the agents action tokens. Under next-token prediction, masks are aligned with shifted targets yi,t = xi,t+1. i,t 20 The implementation below corresponds to the calculation of mresp i,t boundaries: and the logic for aligning rewards to turn Turn-Aware Mask Computation 1 def get_masks_and_scores(input_ids, tokenizer, all_scores, enable_response_mask= False): special_token, reward_token = get_special_tokens(tokenizer) # Calculate turn indicators c_{i,t} turn_starts = torch.where(input_ids == special_token, 1, 0) turn_indicators = torch.cumsum(turn_starts, dim=-1) # Generate Response Mask m^{resp}_{i,t} # Selects tokens where turn count is odd (assistant) and > 1 (after sys prompt) response_mask = (turn_indicators % 2 == 1) & (turn_indicators > 1) # Assign scores to the last token of the turn score_tensor = torch.zeros_like(input_ids, dtype=torch.float32) scores = [sum(i) for in all_scores] # Sum of rewards R_i score_tensor[:, -1] = torch.tensor(scores, dtype=torch.float32) # Alignment adjustments for causal masking score_tensor = score_tensor[:, 1:] loss_mask = response_mask[:, :-1] return score_tensor, loss_mask, response_mask 3 4 5 6 7 9 10 11 12 13 15 16 17 18 19 21 D.4.2 Outcome-Based Returns and Group-wise Normalization While poker is high-variance, our primary evaluation metric is the final chip change. For episode i, the trajectory-level return is: Ti(cid:88) ri,t, Ri = (20) t=1 equal to the net profit/loss in chips. We use the trajectory-level return as the scalar reward stored in the record, i.e., ri := Ri. To reduce variance and stabilize policy updates, the Data Plane applies group-wise normalization. For group of episodes (e.g., sharing similar initial private hands or other coarse state descriptors), we compute the normalized advantage: ˆAi = Ri Mean(cid:0){Rj}jG (cid:1) + ϵ Std(cid:0){Rj}jG (cid:1) . (21) (22) We then broadcast ˆAi to the response tokens: so that only assistant tokens receive non-zero advantage. ˆAi,t = ˆAi mresp i,t , This normalization logic supports multiple grouping strategies (e.g., by initial state or batch) to compute the standardized returns used in the GRPO objective: 21 Group-wise Reward Normalization 1 def _normalize_score_tensor(self, score_tensor, env_outputs): 3 4 5 6 7 9 10 11 12 13 15 16 17 18 19 21 22 23 24 # Grouping logic (G) if self.reward_normalization[grouping] == \"state\": group_tags = [out[\"group_id\"] for out in env_outputs] # Calculate Mean and Std for the group # hat{A}_i = (R_i - Mean) / (Std + epsilon) norm_func = lambda x: (x - x.mean(dim=-1, keepdim=True)) / (x.std(dim=-1, keepdim=True) + 1e-6) # Map indices to groups group2index = {} for i, tag in enumerate(group_tags): if tag not in group2index: group2index[tag] = [] group2index[tag].append(i) # Apply normalization per group acc_scores = score_tensor[:, -1] normalized = acc_scores.clone() for group, idxs in group2index.items(): normalized[idxs] = norm_func(normalized[idxs]) score_tensor[:, -1] = normalized return score_tensor D.4.3 GRPO-Style Policy Gradient Objective The adapter emits the canonical version-tagged record τi = (xi, yi, ri, vi, Ωi), where ri = (cid:80)Ti t=1 ri,t is the ) and grouping tags used to compute ˆAi. episode return and Ωi includes task metadata such as (mloss , ˆAi). After sampling τi from the replay buffer, the learner materializes the training tensors (xi, ai, mloss To account for the distribution shift between the sampling policy πsampler and the current learner πlearner, we define the token-level likelihood ratio for token in episode as: , mresp , mresp i ρi,t(θ) = πθ(yi,t xi, yi,<t) πθold(yi,t xi, yi,<t) . (23) The training objective (θ) incorporates truncated importance sampling to stabilize updates when reusing off-policy data from the buffer: (θ) = Eaπsampler(θold) min (cid:124) (cid:18) πlearner(a, θold) πsampler(a, θold) (cid:123)(cid:122) truncated importance ratio , J (θ) , (cid:19) (cid:125) (24) where is hyper-parameter and (θ) denotes the GRPO-style clipped surrogate objective with KL regularization: (θ) = 1 mresp i,t (cid:80) Li1 (cid:88) t=1 mresp i,t min (cid:16) ρi,t(θ) ˆAi,t, clip(cid:0)ρi,t(θ), 1 ϵc, 1 + ϵc (cid:1) ˆAi,t (cid:17) β DKL(πθπref). (25) Intuitively, the truncated ratio prevents gradient instability when the current policy deviates significantly from the data-collection policy. This allows the Learning Plane to robustly leverage diverse experiences from the Data Plane, demonstrating that poker environment support requires only specialized Data Plane instantiation. D.5 Texas Holdem Performance Table 4 Texas Holdem evaluation of different player policies against three rule-based opponents and an LLM opponent (highlighted). The reported metric is the final chip change (net chips at the end of an episode/match relative to the initial stack). Positive values indicate net profit, while negative values indicate net loss. Only Qwen3-0.6B includes GRPO-trained variant (second row, marked +GRPO); all other rows are direct LLM policies without GRPO training. Model Qwen3-0.6B [28] Qwen3-0.6B [28] +GRPO Qwen3-30B-A3B [28] Qwen3-next-80B-A3B-instruct [28] GPT-5 [29] Grok-4 [30] Claude-sonnet-4.5 [31] Gemini-2.5-flash [32] Gemini-2.5-pro [32] Rule-based1 Rule-based2 Rule-based3 0.514 -0.599 0.592 -0. 0.571 -0.195 -0.397 -0.399 -0.216 -0.3915 -0.1155 -0.173 -0.199 0.093 0.1005 -0.1955 -0.2915 -0.055 -0.2165 -0.2055 -0.1225 -0.0055 0.1455 -0.228 0.056 0.0305 -0.04 LLM -1.677 1. 0.4265 0.304 0.266 0.911 0.1145 0.359 0.4445 We evaluate our method on Texas Holdem environment using the final chip change (net chips at the end of match relative to the initial stack; higher is better). In Table 4, columns correspond to different opponentsthree rule-based opponents (Rule-based1Rule-based3) and one LLM opponent (highlighted). Rows correspond to the evaluated player/agent. For Qwen3-0.6B, we report both the base model (first row) and its GRPO-trained variant (second row, marked +GRPO). For all other backbones, we report the performance of their direct LLM policies (i.e., without GRPO training) under the same evaluation protocol. Main result: GRPO improves Qwen3-0.6B against the LLM opponent. For Qwen3-0.6B, GRPO flips the outcome against the LLM opponent from net loss (1.677) to net profit (+1.245), demonstrating that GRPO can substantially improve end-of-game profitability in the most challenging setting. Meanwhile, performance against the three rule-based opponents becomes negative after GRPO, suggesting trade-off that may be addressed by multi-opponent training or more diverse opponent sampling."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Gradient",
        "Technical University of Darmstadt",
        "The University of Hong Kong",
        "University of Edinburgh"
    ]
}