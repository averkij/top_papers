{
    "paper_title": "SitEmb-v1.5: Improved Context-Aware Dense Retrieval for Semantic Association and Long Story Comprehension",
    "authors": [
        "Junjie Wu",
        "Jiangnan Li",
        "Yuqing Li",
        "Lemao Liu",
        "Liyan Xu",
        "Jiwei Li",
        "Dit-Yan Yeung",
        "Jie Zhou",
        "Mo Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Retrieval-augmented generation (RAG) over long documents typically involves splitting the text into smaller chunks, which serve as the basic units for retrieval. However, due to dependencies across the original document, contextual information is often essential for accurately interpreting each chunk. To address this, prior work has explored encoding longer context windows to produce embeddings for longer chunks. Despite these efforts, gains in retrieval and downstream tasks remain limited. This is because (1) longer chunks strain the capacity of embedding models due to the increased amount of information they must encode, and (2) many real-world applications still require returning localized evidence due to constraints on model or human bandwidth. We propose an alternative approach to this challenge by representing short chunks in a way that is conditioned on a broader context window to enhance retrieval performance -- i.e., situating a chunk's meaning within its context. We further show that existing embedding models are not well-equipped to encode such situated context effectively, and thus introduce a new training paradigm and develop the situated embedding models (SitEmb). To evaluate our method, we curate a book-plot retrieval dataset specifically designed to assess situated retrieval capabilities. On this benchmark, our SitEmb-v1 model based on BGE-M3 substantially outperforms state-of-the-art embedding models, including several with up to 7-8B parameters, with only 1B parameters. Our 8B SitEmb-v1.5 model further improves performance by over 10% and shows strong results across different languages and several downstream applications."
        },
        {
            "title": "Start",
            "content": "Preprint. SitEmb-v1.5: Improved Context-Aware Dense Retrieval for Semantic Association and Long Story Comprehension Junjie Wu1, Jiangnan Li2, Yuqing Li3, Lemao Liu2, Liyan Xu2, Jiwei Li4 Dit-Yan Yeung1, Jie Zhou2, Mo Yu2 1HKUST 2WeChat AI, Tencent junjie.wu@connect.ust.hk 3IIE-CAS 4Zhejiang University {jiangnanli,moyumyu}@tencent.com 5 2 0 A 3 ] . [ 1 9 5 9 1 0 . 8 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Retrieval-augmented generation (RAG) over long documents typically involves splitting the text into smaller chunks, which serve as the basic units for retrieval. However, due to dependencies across the original document, contextual information is often essential for accurately interpreting each chunk. To address this, prior work has explored encoding longer context windows to produce embeddings for longer chunks. Despite these efforts, gains in retrieval and downstream tasks remain limited. This is because (1) longer chunks strain the capacity of embedding models due to the increased amount of information they must encode, and (2) many real-world applications still require returning localized evidence due to constraints on model or human bandwidth. We propose an alternative approach to this challenge by representing short chunks in way that is conditioned on broader context window to enhance retrieval performance i.e., situating chunks meaning within its context. We further show that existing embedding models are not wellequipped to encode such situated context effectively, and thus introduce new training paradigm and develop the situated embedding models (SitEmb). To evaluate our method, we curate book-plot retrieval dataset specifically designed to assess situated retrieval capabilities. On this benchmark, our SitEmb-v1 model based on BGE-M3 substantially outperforms state-of-theart embedding models, including several with up to 7-8B parameters, with only 1B parameters. Our 8B SitEmb-v1.5 model further improves performance by over 10% and shows strong results across different languages and several downstream applications."
        },
        {
            "title": "Introduction",
            "content": "Text embedding models (Wang et al., 2024a; Sturua et al., 2024; Nussbaum et al., 2024; Hu et al., 2025) encode textual inputs into vector spaces. These models enable efficient semantic representation and matching, thus are foundational to many applications involving retrieval-augmented generation (RAG) (Lewis et al., 2020), such as code generation (Wang et al., 2024b; Miao et al., 2024), reference generation (Wu et al., 2025), and personal AI assistants (Martin & Johnson, 2023). In these tasks, candidate documents are typically segmented into smaller chunks to facilitate efficient processing. However, since documents often exhibit narrative or logical flow, the meaning of each chunk is highly dependent on its surrounding context. This highlights the need for text embeddings that capture broader contextual information to enable contextaware retrieval. One straightforward approach to this issue is to increase chunk size, allowing each chunk to capture more information. This has motivated wave of recent work on supporting Corresponding Author. 1https://huggingface.co/SituatedEmbedding 1 Preprint. Figure 1: Comparison of the same embedding models that return the same lengths of texts with different chunk sizes on our evaluation task (3). X-axis refers to chunk sizes. For example, when the return text is 5k and the chunk size is 1,024, the retriever returns top-5 chunks. long input sequences in embedding models, either by designing efficient bidirectional models (Chen et al., 2024; Sturua et al., 2024; Nussbaum et al., 2024), or by repurposing powerful unidirectional pre-trained LLMs as embedding generators (Li et al., 2023; Wang et al., 2024a; Moreira et al., 2024; Kim et al., 2024). These models can produce embeddings for sequences of up to 8,192 tokens or more. However, it is often observed that simply enabling longer input windows does not necessarily lead to better embeddings. key reason lies in the limited capacity of embedding vectors embedding models must compress the information in the input text into single vector. Intuitively, the longer the input chunk, the more information it contains, and the more longrange dependencies across arbitrary pairs of chunks within document it needs to capture. This increases the likelihood of critical information loss during compression. Existing models are trained by merely extending the context window, without explicitly learning how to represent such distributed contextual relationships, which leads to counterintuitive outcome: applications built on long-chunk embeddings often underperform those using short-chunk embeddings, despite the latter discarding more contextual information. Figure 1 illustrates this effect on book plot retrieval task (Xu et al., 2024a). When the same total length of text (5k/10k/25k tokens) is retrieved using the same embedding model (Jina or NV-Embed), recall consistently decreases as the documents are segmented into longer chunks. Given the aforementioned challenges, we propose an alternative approach to context-aware embedding: directly incorporating the broader context surrounding each short chunk into its chunk embedding. This allows the model to account for how the chunk is situated within the original document, enabling more contextually informed embeddings. In other words, we aim to situate chunks meaning within its broader context (Yu et al., 2023; Xu et al., 2024a) during the embedding process. We refer to this approach as situated embedding. By doing so, we alleviate the issue of capacity limitations: during encoding, the model only needs to identify and integrate context that is relevant to the target chunk, which is more tractable task than modeling all dependencies across an extended input window. Building on the idea of situated embedding, we first investigate whether existing embedding models can effectively generate situated embeddings. However, we find that situated embedding cannot be achieved simply by prompting existing embedding models, as demonstrated in 4. To address this limitation, we develop dedicated situated embedding model specifically designed to handle this scenario. We achieve this through two techniques: (1) Constructing context-dependent training instances using publicly available user-annotated book notes. Platforms such as Douban2 allow users to write notes anchored to particular book segments. We treat the note as query and the anchor text as groundtruth, framing retrieval task with 1.6M query-candidate pairs. As user notes typically reflect the contextual understanding of surrounding context, it makes context-aware embeddings beneficial for this retrieval task. (2) Promoting context usage through residual learning. In many cases, chunk alone may offer partial (usually ambiguous) clues about its relevance to the query, allowing models 2https://book.douban.com/ 2 Preprint. to exploit shortcuts. To counter this, we employ residual architecture where the situated embedding model is trained to resolve the residual from baseline chunk-only embedding model. This encourages the model to focus on the additional contextual information. To evaluate models context-aware embedding capability, we curate book-plot retrieval task following (Xu et al., 2024a), which has been verified by previous work for its guaranteed requirement of context-aware embedding capability. Experiments demonstrate our situated embedding models superior performance over all the state-of-the-art embedding models, including those with up to 7B parameters and with massive pre-training. We further illustrate the generalizability of the trained models on the Recap Snippet Identification task (Li et al., 2024), distinct task in book understanding beyond standard query-candidate retrieval, and several downstreaming story understanding tasks such as QA and claim verification that require RAG."
        },
        {
            "title": "2 Our Situated Embedding Model",
            "content": "We develop the first model that generates high-quality situated embeddings as follows: Training Data Construction We construct two sets of training data in English and Chinese, corresponding to different usage scenarios of retrieval, long-story comprehension, and semantic association. For the story comprehension purpose, we follow Mou et al. (2021) and build data based on NarrativeQA (Koˇcisk `y et al., 2018). For association-oriented training, we draw on book notes following prior work (Yu et al., 2023; Zhou et al., 2025). Book notes are particularly suitable for this purpose, as they capture human readers divergent thinking while engaging with paragraph, thereby revealing associations between notes and the corresponding text. The construction of this book-note training data is detailed as follows. We collect the notes and their associated anchor texts for 100 most popular books according to Douban. We treat each user note as query and its corresponding anchored segment as chunk, resulting in 1,614,007 querychunk pairs. We reserve all the querychunk pairs from our evaluation books and randomly select 1000 pairs for early-stopping. Given querychunk pair, we define the situated context of the chunk as sequence of its surrounding sentences, including the chunk itself. Specifically, we use the user-underlined texts anchored to their query as the chunks situated context, as these texts naturally align with our definition. Due to variations in user behavior, the lengths of these situated contexts range from 37 tokens to several thousand, making our trained model robust to wide range of context lengths. After this process, each chunk will be contained within one such segment, and we regard the segment as the situated context of the chunk. Residual Learning to Promote Situated Context Usage Prior studies, such as (Ettinger, 2020), have shown that BERT-based models often rely on shallow heuristics or partial, ambiguous clues when matching texts. This behavior hinders the models ability to fully comprehend the entire input, which potentially explains why existing embedding models struggle to utilize long contextual information. To address this limitation, we adopt residual learning framework (He et al., 2016), in which situated embedding model is trained to resolve the residual from baseline chunk-only embedding model, thereby equipping the trained model with deeper understanding of situated context. Specifically, we maintain two models, baseline model Θb that embeds the chunk only and situated model Θs that embeds the chunk situated within the context. For each query-chunk pair in the training data, we treat the chunk as the positive sample, and randomly sample 10 other chunks from the remaining chapters of the same book as negative samples. query is embedded as = qb + qs, where qb and qs are embedding vectors from Θb and Θs, respectively. Similarly, chunk is embedded as = cb + cs. The training loss on each 3 Preprint. Model M3 Jina-v3 Chunk-Only + Situated Context + Situated Summ. Size @10 @20 @50 @10 @20 @50 @10 @20 @50 0.5B 42.55 53.33 66.51 15.68 24.70 41.83 53.94 67.78 0.5B 42.65 52.67 67.92 34.10 44.27 58.33 45.48 56.12 69.90 9.48 E5-Mistral GTE-Qwen2 NV-Embed-v2 Qwen3-Embedding 7B 7B 7B 8B 43.18 51.93 66.65 14.77 24.49 32.68 44.58 54.32 68.27 46.19 55.79 71.15 19.01 29.77 49.34 42.44 49.88 60.85 51.38 60.11 71.16 21.01 30.20 42.18 49.25 58.33 70.85 51.58 61.32 73.47 48.01 58.91 71.86 51.20 61.63 76.10 voyage-context-3 unk 58.54 68.47 79.09 60.46 69.50 82.19 61.37 70.36 82.84 SitEmb-v1-M3 (ours) SitEmb-v1.5-Qwen3 (ours) + book note data 1B 8B 8B 48.79 58.45 73.29 50.85 60.57 76.40 51.47 63.69 76.76 61.66 69.06 79.54 63.32 72.75 83.59 61.26 70.02 81.57 66.81 74.36 84.32 68.98 79.32 86.68 67.81 76.37 84.37 Table 1: Recall results on NDP-v1. The maximum length is set to 8,192. Best results of each setting are boldfaced. query-chunk pair can then be defined as: L(Θb, Θs) = 1 N=10 i= max (cid:0)0, γ + sim( qj, j,i) sim( qj, c+ )(cid:1), (1) where is the index of negative chunk. See Appendix for details of the training process."
        },
        {
            "title": "3 Evaluation Dataset",
            "content": "Xu et al. (2024a) repurpose instances from the PlotRetrieval dataset Xu et al. (2024b) to support the task of contextual retrieval. Their work focuses on single book, demonstrating that incorporating graph-based representation of the book can improve local chunk retrieval. This finding highlights that the plot retrieval task inherently requires situated understanding and retrieval capabilities. Following their work, we repurpose the PlotRetrieval dataset into chunk-level retrieval task and expand the number of evaluation books from 1 to 7. Specifically, we filter books that are too short (i.e., 100,000 tokens), as they can typically be processed in single input window and therefore diminish the utility of RAG. We also exclude books with too few user notes, as well as less popular versions on the reading platform, which tend to have less diverse note styles. This filtering process results in 7 evaluation books containing 1,394 diverse queries, which together constitute the Book Plot Retrieval task (see Appendix B.1 for details on each selected book). When constructing the situated context for each chunk in the Book Plot Retrieval task, we first partition the chunks corresponding book into segments of consecutive sentences util the length reaches 200 tokens. We then sequentially group every 16 consecutive segments. This grouped context serves as the situated context of each chunk inside the group. During evaluation, we report Recall@10, Recall@20, and Recall@50 as the primary metrics. Remark Our plot retrieval task captures an important real-world scenario where localized retrieval results are essential. When users of an online reading app try to recall plot, they typically lack the time or patience to read through long passages. In our setting, each retrieved segment corresponds to about 23 pages as displayed in mobile or digital reading app, aligning with this user requirement. However, while humans can mentally connect these short segments to the broader narrative, the content alone is often insufficient due to missing context, highlighting the need for situated embedding techniques. 4 Preprint."
        },
        {
            "title": "Embeddings",
            "content": "As the first step, we investigate the necessity of training situated embedding model. That is, are existing long-context embedding models capable of generating good situated embeddings? Setup We investigate this question on the NDP-v1 book in our evaluation dataset. Following our approach described in 2, we use the 16 surrounding chunks of each chunk to construct its situated context. We compare the following models3: 1) Long-context BERT models, including BGE-M3 (Chen et al., 2024) and Jina-v3 (Jina-Embeddings-v3) (Sturua et al., 2024). 2) LLM-based embedding models: E5-Mistral (E5-Mistral-7b-Instruct) (Wang et al., 2024a), GTE-Qwen2 (GTE-Qwen2-7b-Instruct) (Li et al., 2023), NV-Embed-v2 (Lee et al., 2024) and Qwen3-Embedding-8B (Zhang et al., 2025). 3) Our trained situated model from 2, including the v1 model based on M3 and the v1.5 model based on Qwen3. For reference, we also compare with the concurrent work on the most advanced commercial late-chunking model voyage-context-3 (Voyage-AI, 2025). Check Appendix for additional details on model usages. Results Table 1 presents the evaluation results, from which we draw the following conclusions: Existing models does not have zero-shot situated embedding capability. When enhancing the contexts to chunks, the performance of all the existing models degrades significantly (i.e., comparing columns of +Situated Context and Chunk-Only). Note that the length of the situated context is well within their claimed maximum context window sizes. In contrast, our situated embedding model can effectively leverage contextual information, and largely surpasses the much larger 7B baselines. The poor results partly sourced from limitation in understanding long inputs. The failure of producing situated embeddings is partly from the existing models (actual) insufficiency of handling long inputs. To see this, we in addition compare with the LLM-generated situated summaries approach (Anthropic, 2024), which prompts an LLM to generate concise summary that reflects how chunk is situated within its broader context as the contextual information. We ask GPT-4o (OpenAI, 2024) to generate the situated summaries and use them in the same way like the situated contexts. Note that we use this setting only for reference, because it does not make fair comparison due to the involvement of much stronger model in the pipeline with high computational cost. From the results, all the models suffer from much smaller degrade when using the summaries instead of original situated contexts, while M3, Jina, E5 and Qwen3 have their results slightly increased. This reflects the fact that the baselines fail to situate the target chunk within long contexts. In comparison, our approach can achieve performance boost for both types of contextual inputs."
        },
        {
            "title": "5 Study II: Analyzing the Robustness of Our SitEmb Models",
            "content": "In this study, we examine two aspects crucial to real-world applications: (1) whether our SitEmb models learn to generalize to new books rather than rely on memorization, and (2) whether they are robust to variations in situated context length. The Impact of Training-Test Book Overlap key concern in evaluating pre-trained language models, including embedding models, is whether models benefit unfairly from training-test overlap. Demonstrating that such overlap does not drive results is particularly important, because model training cannot anticipate all future downstream uses; it is therefore impractical to proactively filter training data against every possible evaluation or user scenario. 3The models are selected based on their strong performance on the MTEB benchmark (Muennighoff et al., 2023). 5 Preprint. Setting Recall @10 @20 @50 w/ NDP w/o NDP 68.98 69.60 79.32 78.72 86.68 87.04 Table 2: Study the impact of training-test book overlap on the NDP-v1 task. We experiment with the Sit-Qwen3 with book-note setting. Situated Context Length [512, 800] [1024, 1600] [2048, 3200] [4096, 6400] [8192, 12800] SitEmb-v1 Recall SitEmb-v1.5 Recall @10 @20 @50 @ @20 @50 51.63 52.29 51.73 50.62 50.52 61.22 61.23 61.56 59.11 59.11 74.48 74.15 75.00 75.16 74.51 69.31 68.76 68.98 69.75 68. 77.51 76.24 79.32 77.45 77.00 86.09 85.78 86.68 86.83 86.47 Table 3: Recall results of our situated embedding models on NDP-v1 with various lengths of situated contexts. The listed lengths correspond to multiples (4/8/16/32/64) of the average segment range observed in books from the book plot retrieval task. The best results are boldfaced and the second best results are underlined. To verify the validity of our evaluation, we constructed controlled experiment that modifies the training data from NarrativeQA with and without any version of the NDP books,4 and then evaluated their performance on NDP-v1. As shown in Table 2, models exposed to the test books during training exhibit no measurable performance gain, indicating that training-test overlap does not materially affect our results. Robustness to Context Length We evaluate the sensitivity of our trained model to variations in context length. Experiment is conducted on the NDP-v1 book used in Table 1, varying the number of sentences per chunk and measuring recall scores with our SitEmbv1-M3 and the SitEmb-v1.5-Qwen models, both trained with book note data. The results in Table 3 demonstrate that the model maintains stable performance across different context lengths. Our choice of 16-segment groups strikes balance between efficiency and accuracy."
        },
        {
            "title": "6 Study III: Contextual Retrieval on the Full Book Plot Retrieval Task",
            "content": "We evaluate our situated embedding model on the full plot retrieval task to assess its effectiveness in enhancing contextual retrieval. We compared our models trained with the QA data (denoted as QA) and book note data (semantic association, denoted as SA). The M3 model fails to improve with the QA training data thus the corresponding results are omitted. As shown in Table 4, incorporating contextual information through situated embeddings significantly improves performance. Our SitEmb-v1-M3 model consistently outperforms chunk-only baselines without our training techniques. The SitEmb-v1.5 models further boost performance by over 10% when trained on QA data and over 15% when trained on QA+SA data. Notably, both variants surpass the performance of the recent commercial late-chunking model voyage-context-3, and show clear advantages over their chunk-only variations. To ensure that the gains of SitEmb-v1-M3 are not merely due to increased model capacity, we also train the same residual architecture on two chunk-only M3 models (Res-M3). It fails to yields improvements over the trained M3 (SA) baseline, indicating that the advantage of 4We use NarrativeQA rather than book-note data, since the queries in the plot retrieval task are originally derived from book notes so the test books have to be removed, making such an overlap experiment implausible in that setting. 6 Preprint. Setting Model Chunk-only M3 (out-of-box) M3 (SA) + Residual Qwen3 (out-of-box) Qwen3 (QA) Qwen3 (QA+SA) Size Recall @10 @20 @50 0.5B 32.92 41.46 55.85 0.5B 42.87 52.91 66.01 1B 43.43 51.74 65.51 8B 43.57 52.94 69.48 8B 51.02 60.78 73.89 8B 60.36 68.87 80.45 Late Chunking voyage-context-3 unk 51.39 60.89 73. Situated SitEmb-v1-M3 (SA) - Residual SitEmb-v1.5-Qwen3 (QA) SitEmb-v1.5-Qwen3 (QA+SA) 1B 45.15 55.66 69.25 0.5B 43.85 54.98 68.93 8B 53.57 63.56 78.64 8B 63.03 72.83 82.70 Table 4: Overall results on book plot retrieval. Check Table 9 and 10 for full results. Model Qwen3 Qwen3 (QA) Qwen3 (QA+SA) SitEmb-v1.5-Qwen3 (QA) SitEmb-v1.5-Qwen3 (QA+SA) Recap R@ P@5 F1@5 33.0 32.0 32.6 32.3 33.6 46. 45.8 46.4 46.6 48.2 37.9 37.0 37.6 37.4 38.9 Table 5: Results on the recap task. our method primarily come from the effective use of contextual information. In addition, training without the residual architecture (- Residual) leads to degraded performance compared to our full SitEmb-v1-M3, further supporting our training design."
        },
        {
            "title": "Identification",
            "content": "Next, we assess the generalizability of our situated embedding model on downstream applications that are not explicitly designed for contextual retrieval and contain only limited portion of context-dependent examples. In this section, we evaluate on distinct task, Recap Snippet Identification task (Li et al., 2024), which aims to identify recap passages for given paragraph. We following their setting of using top-5 retrieved passages. Results Table 5 presents the results. Because this task differs substantially from our training data, it poses challenging transfer setting. Consequently, our trained models without context usage show slight performance drop compared to the original Qwen3 model. However, since recap identification requires embedding capabilities beyond simple similarity matching, our models trained with SA data achieve better generalization to this task. In particular, the SitEmb-v1.5 (QA+SA) model outperforms all others by leveraging contextual information. These results highlight the importance of enhancing semantic association capabilities in embeddings."
        },
        {
            "title": "8 Study V: Downstream Long Story Comprehension Applications",
            "content": "Finally, we evaluate on variaty of story understanding tasks that requires processing inputs exceeding the length limits of many LLMs, including NarrativeQA (Koˇcisk `y et al., 2018), the multichoice QA task from Bench (Zhang et al., 2024), the newly release DetectiveQA (Xu et al., 2025), the public subset of NoCha (Karpinska et al., 2024) and the LongStoryQA-large 7 Preprint. Model NarrativeQA Bench-En.MC DetectiveQA NoCha (Public) LongStoryQA-Large F1 Acc Acc Pair Acc F1 Qwen3 (out-of-box) 27.5/30.8/32.2 75.1/80.4/86.0 62.5/68.7/73.2 42.9/41.3/46.0 52.7/57.9/61.2 Qwen3 (QA) SitEmb-v1.5-Qwen3 (QA) SitEmb-v1.5-Qwen3 (QA+SA) 29.5/31.9/32.4 31.1/32.0/34.4 29.4/31.6/31.8 83.0/84.7/88.7 83.0/86.9/90.0 83.0/85.6/88.2 70.5/78.2/81.8 73.2/78.7/82.3 66.5/74.2/78.3 54.0/52.4/36.5 54.0/55.6/46.0 55.6/52.4/49.2 58.3/59.2/61.4 57.7/58.7/61.9 57.7/59.4/61.5 Table 6: Results on the story QA tasks. We report results with top-3/5/10 retrieved chunks. Model voyage-context-3 Qwen3 (out-of-box) Qwen3 (QA) SitEmb-v1.5-Qwen3 (QA) SitEmb-v1.5-Qwen3 (QA+SA) Answer Recall Clue Recall Final Accuracy Top-3 Top-5 Top-10 Top-3 Top-5 TopTop-3 Top-5 Top-10 36.1 29.6 35.8 42.5 29.4 46.8 37. 50.5 54.5 41.3 63.3 55.5 66.4 69.3 56.7 24.8 23.8 23.7 24.6 26.9 33.8 31. 33.0 34.0 36.4 48.1 46.5 48.0 49.2 51.2 68.7 62.5 70.5 73.2 66.5 73.5 68. 78.2 78.7 74.2 79.8 73.2 81.8 82.3 78.3 Table 7: Study on the effects of improved retrieval on the DetectiveQA dataset, which provides evidence passage annotations. task from CLongEval (Qiu et al., 2024). These tasks cover different genres, both English and Chinese languages, and task types of free-form QA, multi-choice QA and claim verification. We retrieve top-3/5/10 with the compared embedding models and use Qwen2.5-72B (4-bit quantized) model to generate the results. Overall Results Table 6 shows that our SitEmb-v1.5 model trained on QA data consistently outperforms its counterpart without situated embeddings, except on LongStoryQA. It also substantially outperforms the original Qwen3 embedding model, particularly on top-3 and top-5 results. In comparison, our SitEmb model trained on QA+SA yields mixed results relative to the no-context model, but still shows advantages over the original Qwen3. This suggests that existing story comprehension tasks demand limited semantic association capability, making the SitEmb (QA) model well-balanced choice across diverse benchmarks. One notable observation regarding performance degradation with larger retrieved context on NoCha is that, once the key plot is retrieved, additional context tends to consist mainly of distractors, causing an LLM with weaker reasoning ability to lose focus. To verify this, we evaluated the advanced Gemini-2.5-Flash under our QA+SA setting, achieving top-3/5/10 pair accuracies of 55.6/57.1/57.1 without degradation. This confirms that the necessary evidence is saturated within the top-5 results. Fine-Grained Evaluation on Retrieval Results Finally, we perform fine-grained evaluation on the DetectiveQA dataset, which includes human-annotated evidence locations. Specifically, DetectiveQA provides two types of evidence annotations: Answer Evidence, which refers to the text span that directly yields the answer; and Clue Evidence, which refers to the supporting information that connects the evidence to the correct answer, mirroring the logical reasoning steps detective would follow to solve the mystery. For reference, we also compare our results with those of voyage-context-3 in this setting. Table 7 highlights two key findings. First, our SitEmb (QA) model achieves substantial advantage in answer evidence recall over all other models, directly contributing to its higher final answer accuracy. Second, our SitEmb (QA+SA) model shows clear advantage in clue recall. This metric reflects an important aspect of semantic association capability, as clues are often only loosely or implicitly related to the question."
        },
        {
            "title": "9 Conclusion",
            "content": "This paper introduces the situated embedding models, which encodes chunks surrounding contextual information directly into its embedding, enabling deeper understanding of the 8 Preprint. chunk itself. Experiments across multiple long-context understanding tasks demonstrate that situated embeddings provide an effective alternative approach to contextual retrieval, and our proposed model serves as strong first step in advancing this direction."
        },
        {
            "title": "Limitations",
            "content": "While our experiments on several use cases highlight the advantages of embedding models with enhanced semantic association capabilities, results on broader applications are mixed. At this stage, training with QA-only data achieves better overall balance. This suggests that semantic association exists along spectrum from direct relevance to abstract and implicit relations. To excel across diverse scenarios, model must be able to adaptively control its degree of divergence. Achieving this poses challenges for our current LoRA fine-tuning regime, which has limited capacity, and calls for new training objectives that explicitly encourage controllable association through instruction following. Another limitation of our current work is that the models are primarily optimized for narrative data. In future work, we plan to construct training data from broader range of domains to improve generalization."
        },
        {
            "title": "References",
            "content": "Anthropic. Enhancing rag with contextual retrieval. 2024. URL https://github.com/ anthropics/anthropic-cookbook/blob/main/skills/contextual-embeddings/guide. ipynb. Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation, 2024. Allyson Ettinger. What bert is not: Lessons from new suite of psycholinguistic diagnostics for language models. Transactions of the Association for Computational Linguistics, 8:3448, 2020. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770778, 2016. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. URL https://openreview.net/ forum?id=nZeVKeeFYf9. Xinshuo Hu, Zifei Shan, Xinping Zhao, Zetian Sun, Zhenyu Liu, Dongfang Li, Shaolin Ye, Xinyuan Wei, Qian Chen, Baotian Hu, et al. Kalm-embedding: Superior training data brings stronger embedding model. arXiv preprint arXiv:2501.01028, 2025. Marzena Karpinska, Katherine Thai, Kyle Lo, Tanya Goyal, and Mohit Iyyer. One thousand and one pairs: \"novel\" challenge for long-context language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, 2024. URL https://doi.org/10.18653/v1/2024. emnlp-main.948. Jihoon Kwon Sangmo Gu Yejin Kim, Minkyung Cho Jy-yong Sohn Chanyeol, Choi Junseong Kim, and Seolhwa Lee. Linq-embed-mistral: Elevating text retrieval with improved gpt data through task-specific control and quality refinement. linq ai research blog, 2024. Tomáš Koˇcisk `y, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette. The narrativeqa reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317328, 2018. URL https: //aclanthology.org/Q18-1023.pdf. 9 Preprint. Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Nv-embed: Improved techniques for training llms as generalist embedding models. arXiv preprint arXiv:2405.17428, 2024. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrievalaugmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 33:94599474, 2020. Jiangnan Li, Qiujing Wang, Liyan Xu, Wenjie Pang, Mo Yu, Zheng Lin, Weiping Wang, and Jie Zhou. Previously on the stories: Recap snippet identification for story reading. arXiv preprint arXiv:2402.07271, 2024. Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. Towards general text embeddings with multi-stage contrastive learning. arXiv preprint arXiv:2308.03281, 2023. Xueguang Ma, Luyu Gao, Shengyao Zhuang, Jiaqi Samantha Zhan, Jamie Callan, and Jimmy Lin. Tevatron 2.0: Unified document retrieval toolkit across scale, language, and modality. arXiv preprint arXiv:2505.02466, 2025. Raiza Martin and Steven Johnson. Introducing notebooklm. 2023. URL https://blog. google/technology/ai/notebooklm-google-ai. Jing Miao, Charat Thongprayoon, Supawadee Suppadungsuk, Oscar Garcia Valencia, and Wisit Cheungpasitporn. Integrating retrieval-augmented generation with large language models in nephrology: advancing practical applications. Medicina, 60(3):445, 2024. Gabriel de Souza Moreira, Radek Osmulski, Mengyao Xu, Ronay Ak, Benedikt Schifferer, and Even Oldridge. Nv-retriever: Improving text embedding models with effective hard-negative mining. arXiv preprint arXiv:2407.15831, 2024. Xiangyang Mou, Chenghao Yang, Mo Yu, Bingsheng Yao, Xiaoxiao Guo, Saloni Potdar, and Hui Su. Narrative question answering with cutting-edge open-domain qa techniques: comprehensive study. Transactions of the Association for Computational Linguistics, 9: 10321046, 2021. Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. Mteb: Massive text embedding benchmark. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pp. 20142037, 2023. Zach Nussbaum, John Morris, Brandon Duderstadt, and Andriy Mulyar. Nomic embed: Training reproducible long context text embedder. arXiv preprint arXiv:2402.01613, 2024. OpenAI. Hello gpt-4o. 2024. URL https://openai.com/index/hello-gpt-4o/. Zexuan Qiu, Jingjing Li, Shijue Huang, Xiaoqi Jiao, Wanjun Zhong, and Irwin King. Clongeval: chinese benchmark for evaluating long-context large language models. In Findings of the Association for Computational Linguistics: EMNLP 2024, pp. 39854004, 2024. Saba Sturua, Isabelle Mohr, Mohammad Kalim Akram, Michael Günther, Bo Wang, Markus Krimmel, Feng Wang, Georgios Mastrapas, Andreas Koukounas, Andreas Koukounas, Nan Wang, and Han Xiao. jina-embeddings-v3: Multilingual embeddings with task lora, 2024. URL https://arxiv.org/abs/2409.10173. Voyage-AI. Introducing voyage-context-3: focused chunk-level details with global document jul 2025. URL https://blog.voyageai.com/2025/07/23/voyage-context-3/. context, Blog post. Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. Improving text embeddings with large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 11897 11916, 2024a. Preprint. Zora Zhiruo Wang, Akari Asai, Xinyan Velocity Yu, Frank Xu, Yiqing Xie, Graham Neubig, and Daniel Fried. Coderag-bench: Can retrieval augment code generation? arXiv preprint arXiv:2406.14497, 2024b. Junjie Wu, Gefei Gu, Yanan Zheng, Dit-Yan Yeung, and Arman Cohan. Ref-long: Benchmarking the long-context referencing capability of long-context language models. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2386123880, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. URL https://aclanthology.org/2025.acl-long. 1162/. Liyan Xu, Jiangnan Li, Mo Yu, and Jie Zhou. Fine-grained modeling of narrative context: coherence perspective via retrospective questions. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 5822 5838, 2024a. Shicheng Xu, Liang Pang, Jiangnan Li, Mo Yu, Fandong Meng, Huawei Shen, Xueqi Cheng, and Jie Zhou. Plot retrieval as an assessment of abstract semantic association. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop), pp. 146161, 2024b. Zhe Xu, Jiasheng Ye, Xiaoran Liu, Xiangyang Liu, Tianxiang Sun, Zhigeng Liu, Qipeng Guo, Linlin Li, Qun Liu, Xuanjing Huang, and Xipeng Qiu. DetectiveQA: Evaluating long-context reasoning on detective novels. In Workshop on Reasoning and Planning for Large Language Models, 2025. URL https://openreview.net/forum?id=9ExIs5ELlk. Mo Yu, Jiangnan Li, Shunyu Yao, Wenjie Pang, Xiaochen Zhou, Zhou Xiao, Fandong Meng, and Jie Zhou. Personality understanding of fictional characters during book reading. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1478414802, 2023. Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Khai Hao, Xu Han, Zhen Leng Thai, Shuo Wang, Zhiyuan Liu, and Maosong Sun. bench: Extending long context evaluation beyond 100k tokens. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, 2024. URL https://doi.org/10.18653/v1/2024.acl-long. 814. Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, Fei Huang, and Jingren Zhou. Qwen3 embedding: Advancing text embedding and reranking through foundation models, 2025. Chulun Zhou, Qiujing Wang, Mo Yu, Xiaoqian Yue, Rui Lu, Jiangnan Li, Yifan Zhou, Shunchi Zhang, Jie Zhou, and Wai Lam. The essence of contextual understanding in theory of mind: study on question answering with story characters. arXiv preprint arXiv:2501.01705, 2025."
        },
        {
            "title": "A Additional Training Details of Our Situated Embedding Model",
            "content": "In this section, we describe additional details on how we attempt to train the first situated embedding model. A.1 Model Initialization Before the residual learning process described in 2, we initialize two models, Θb and Θs. While Θs is directly initialized from the BGE-M3 embedding model, we perform prior training step on Θb to facilitate more effective residual learning. 11 Preprint. Specifically, we initialize Θb from the same BGE-M3 embedding model as Θs. For each querychunk pair in the training data, we treat the chunk as the positive sample and randomly sample 10 negative chunks from other chapters of the same book. We then obtain the query embedding qb and chunk embedding cb from Θb, and train Θb using the margin-based loss defined in Eq. 1, applied solely to this model. This prior training stage familiarizes Θb with the task of retrieving book chunks based on user notes, thereby providing more informative foundation for the subsequent residual learning phase. A.2 Training Configurations All training procedures in this paper follow consistent configuration. For SitEmb-v1-M3, we use learning rate of 2e-5 and weight decay of 5e-2. The batch size is set to 80, and the maximum input sequence length is 8192 tokens, which corresponds to the input limit of BGE-M3. During training, we employ the development set introduced in 2 for early stopping. The model is evaluated on this set every 180 training steps, and training is terminated once both the training loss and development performance converge. The margin and temperature values used in the loss function are both set to 0.1. All experiments referring to SitEmb-v1-M3 are conducted using two NVIDIA A100 80G GPUs. For the training of SitEmb-v1.5-Qwen3, we equip Qwen3-Embedding-8B (Zhang et al., 2025) with Low-Rank Adaptation (Hu et al., 2022). The rank is set to 128, the alpha is set to 256, and adapters are attached to the query/key/value/output projections in multi-head attention modules, whose dropout rate is set to 0.05. The training schedule moves on using the cosine LR, warming up at the first 10% steps, whose learning rate is set to 1e-4. Unlike SitEmb-v1-M3, which first trains chunk-only encoder and then residually trains situated encoder with the chunk-only one frozen, we fully utilize the causal-masking feature of the decoder in Qwen3-Embedding (i.e., the unidirectional feature that history tokens cannot access future tokens), and train the context-only and situated settings at the same time. Specifically, for the chunk and context encoding, we concatenate them into one sequence in which the chunk comes first and is followed by the context. Due to the unidirectionality, the chunk can only see itself (i.e., the context-only setting), and the context can realize the situated chunk (i.e., the situated setting). The sequence is formed as [CHUNK]<endoftext>The context in which the chunk is situated is given below. Please encode the chunk by being aware of the context. Context:n[CONTEXT]<endoftext>. In this way, the chunk embedding and the situated embedding are obtained by the last pooling of extracting the embedding of the first and the second <endoftext>. We denote the chunk embedding as cb, the situated embedding as cs, and the query embedding as q. To co-train the two settings, the contrastive learning loss is computed using the scores sim(q, cb), sim(q, cs), sim(q, cb) + sim(q, cs), and the temperature value of 0.01. Furthermore, we follow Chen et al. (2024); Ma et al. (2025) to broadcast the computed scores of batches on every GPU to reach bigger batch size. The batch size per GPU is set to 5, and we use 8 pieces of NVIDIA A800 80G GPUs. For every query, we sample positive chunk plus 13 negative chunks from the same book. In this way, each query can see 8 * 5 * 14 = 560 chunks at step. Additionally, the accumulation step is set to 4, and the model will be trained for 5 epochs. The best checkpoint is picked by the result on the NDP-v1 dev set per epoch, which is always from epoch 2. Therefore, we use the checkpoint saved at epoch 2 by default. All evaluating experiments referring to SitEmb-v1.5-Qwen3 are conducted using piece of NVIDIA A100 40G GPU in the data type of bfloat16."
        },
        {
            "title": "B Full Results Decomposed to Books",
            "content": "B.1 Books in the Evaluation Dataset Following the process described in 3, we select 7 books from the PlotRetrieval dataset to construct our evaluation set. The names of these books, along with the corresponding numbers of queries and candidate chunks, are summarized in Table 8. 12 Preprint. Book Queries Candidates Notre-Dame de Paris (NDP)-v1 Notre-Dame de Paris (NDP)-v2 Notre-Dame de Paris (NDP)-v3 Crime and Punishment (C&P) The Adventures of Tom Sawye (TATS) The Red and the Black (TRB) Tess of the dUrbervilles (TDU) 510 153 146 134 173 144 134 1288 1369 1347 1639 154 1294 1093 Table 8: Statistics of books in the evaluation dataset. Instruct: Given user note query, retrieve the passages that are most relevant to the content or context described in the query. Query: {QUERY} Figure 2: The query format of E5-Mistral and GTE-Qwen2 Note that for some English books, the PlotRetrieval dataset includes multiple Chinese translation versions, treating each version as distinct book. We adopt the same setting and denote the three translation versions of Notre-Dame de Paris in the 7 selected books as v1, v2, and v3, respectively. Among them, NDP-v1 is the version used in Table 1."
        },
        {
            "title": "C Details on Running Embedding Models",
            "content": "For all non-LLM embedding models (i.e., BGE-M3 and Jina-v3), we directly use the models to encode queries, chunks, and situated context, with the maximum input length set to 8192 tokens. For E5-Mistral, GTE-Qwen2, and voyage-context-3, we follow the official encoding guidelines provided at https://huggingface.co/intfloat/e5-mistral-7b-instruct, https:// huggingface.co/Alibaba-NLP/gte-Qwen2-7B-instruct, and https://docs.voyageai.com/ docs/contextualized-chunk-embeddings#quickstart, respectively. In both cases, we prepend one-sentence instruction to each query as required, as illustrated in Figure 2. For NV-Embed-v2, we adopt the same input format as E5-Mistral and GTE-Qwen2 when encoding queries. For chunk encoding in the chunk-only setting of Table 1, we omit instructions, consistent with the E5-Mistral and GTE-Qwen2 setups. In all other settings in Table 1 where additional context is included, we follow the official prompt format of NV-Embed-v2 (https://huggingface.co/nvidia/NV-Embed-v2), as shown in Figure 3. When running the latter two columns of experiments in Table 1, we append the situated context or situated summary to each chunk using the delimiters </s> and nn, respectively. The concatenated sequence is then treated as new chunk and encoded as whole. 13 Preprint. Model M3 (out-of-box) M3 (SA) Res-M3 (SA) SitEmb-v1-M3 (SA, No Res) SitEmb-v1-M3 (SA) Book NDP-v1 NDP-v2 NDP-v3 C&P TATS TRB TDU Avg NDP-v1 NDP-v2 NDP-v3 C&P TATS TRB TDU Avg NDP-v1 NDP-v2 NDP-v3 C&P TATS TRB TDU Avg NDP-v1 NDP-v2 NDP-v3 C&P TATS TRB TDU Avg NDP-v1 NDP-v2 NDP-v3 C&P TATS TRB TDU Avg @ 42.55 33.22 43.79 21.64 38.73 23.61 26.87 32.92 48.79 46.58 50.33 36.57 42.20 36.80 38.81 42.87 49.30 48.29 50.33 35.45 45.66 35.42 39.55 43. 48.76 47.26 51.31 37.31 49.42 33.33 39.55 43.85 50.85 48.97 51.63 36.94 47.69 38.19 41.79 45.15 @20 53.33 44.18 51.63 24.63 47.40 34.72 34. 41.46 58.45 56.16 62.09 42.91 56.94 43.06 50.75 52.91 58.73 54.79 59.15 39.93 56.36 40.97 52.24 51.74 58.53 57.88 63.07 45.90 59.25 47.22 52. 54.98 60.57 57.88 64.71 44.40 60.40 47.92 53.73 55.66 @50 66.51 60.62 64.38 45.15 63.01 47.22 44.03 55. 73.29 67.81 73.53 54.85 70.81 58.33 63.43 66.01 71.93 66.78 72.22 52.99 72.25 59.72 62.69 65.51 72.22 69.86 74.84 57.09 76.01 64.58 67.91 68. 76.40 70.89 74.84 57.84 75.14 63.19 66.42 69.25 Table 9: Full results of SitEmb-v1-M3 on book plot retrieval. Your task is to embed passages for retrieval. Your input consists of the target passage and its context. You need to find relevant information from the context to enhance the target passage embedding such that it captures the meanings of the passages situated within the context. context: {CONTEXT} passage: {PASSAGE} Figure 3: Prompt for NV-Embed-v2. 14 Preprint. Model Qwen3-Embedding (out-of-box) Qwen3 (QA) Qwen3 (QA+SA) SitEmb-v1.5-Qwen (QA) SitEmb-v1.5-Qwen (QA+SA) Book NDP-v1 NDP-v2 NDP-v3 C&P TATS TRB TDU Avg NDP-v1 NDP-v2 NDP-v3 C&P TATS TRB TDU Avg NDP-v1 NDP-v2 NDP-v3 C&P TATS TRB TDU Avg NDP-v1 NDP-v2 NDP-v3 C&P TATS TRB TDU Avg NDP-v1 NDP-v2 NDP-v3 C&P TATS TRB TDU Avg @10 51.20 38.36 50.65 35.82 49.71 43.06 36.19 43.57 61.66 52.74 54.9 40.67 58.09 46.53 42. 51.02 66.81 67.47 65.03 49.63 60.69 56.94 55.97 60.36 63.32 58.90 59.48 44.40 58.96 45.14 44.78 53.57 68.98 71.23 66.34 58.58 65.61 50.00 60. 63.03 @20 61.63 47.95 60.78 43.66 60.69 50.00 45.90 52.94 69.06 60.62 70.59 52.61 65.61 56.25 50.74 60. 74.36 75.68 71.90 61.57 73.12 63.54 61.94 68.87 72.75 68.84 70.59 54.10 70.23 57.64 50.75 63.56 79.32 79.45 79.41 68.28 73.70 62.50 67.16 72. @50 76.10 69.86 76.14 60.45 75.14 65.97 62.69 69.48 79.54 78.42 83.66 64.93 77.75 68.75 64.18 73.89 84.32 84.59 87.91 73.51 84.97 73.96 73. 80.45 83.59 80.82 84.64 76.49 83.24 71.53 70.15 78.64 86.68 89.73 87.58 76.49 84.10 77.43 76.87 82.70 Table 10: Full results of SitEmb-v1.5-Qwen on book plot retrieval."
        }
    ],
    "affiliations": [
        "HKUST",
        "IIE-CAS",
        "WeChat AI, Tencent",
        "Zhejiang University"
    ]
}