{
    "paper_title": "NRGBoost: Energy-Based Generative Boosted Trees",
    "authors": [
        "João Bravo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite the rise to dominance of deep learning in unstructured data domains, tree-based methods such as Random Forests (RF) and Gradient Boosted Decision Trees (GBDT) are still the workhorses for handling discriminative tasks on tabular data. We explore generative extensions of these popular algorithms with a focus on explicitly modeling the data density (up to a normalization constant), thus enabling other applications besides sampling. As our main contribution we propose an energy-based generative boosting algorithm that is analogous to the second order boosting implemented in popular packages like XGBoost. We show that, despite producing a generative model capable of handling inference tasks over any input variable, our proposed algorithm can achieve similar discriminative performance to GBDT on a number of real world tabular datasets, outperforming alternative generative approaches. At the same time, we show that it is also competitive with neural network based models for sampling."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 4 ] . [ 1 5 3 5 3 0 . 0 1 4 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "NRGBOOST: ENERGY-BASED GENERATIVE BOOSTED TREES João Bravo Feedzai joao.bravo@feedzai.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Despite the rise to dominance of deep learning in unstructured data domains, treebased methods such as Random Forests (RF) and Gradient Boosted Decision Trees (GBDT) are still the workhorses for handling discriminative tasks on tabular data. We explore generative extensions of these popular algorithms with focus on explicitly modeling the data density (up to normalization constant), thus enabling other applications besides sampling. As our main contribution we propose an energy-based generative boosting algorithm that is analogous to the second order boosting implemented in popular packages like XGBoost. We show that, despite producing generative model capable of handling inference tasks over any input variable, our proposed algorithm can achieve similar discriminative performance to GBDT on number of real world tabular datasets, outperforming alternative generative approaches. At the same time, we show that it is also competitive with neural network based models for sampling."
        },
        {
            "title": "INTRODUCTION",
            "content": "Generative models have achieved tremendous success in computer vision and natural language processing, where the ability to generate synthetic data guided by user prompts opens up many exciting possibilities. While generating synthetic table records does not necessarily enjoy the same wide appeal, this problem has still received considerable attention as potential avenue for bypassing privacy concerns when sharing data. Estimating the data density, p(x), is another typical application of generative models which enables host of different use cases that can be particularly interesting for tabular data. Unlike discriminative models which are trained to perform inference over single target variable, density models can be used more flexibly for inference over different variables or for out of distribution detection. They can also handle inference with missing data in principled way by marginalizing over unobserved variables. The development of generative models for tabular data has mirrored its progression in computer vision with many of its Deep Learning (DL) approaches being adapted to the tabular domain (Jordon et al., 2018; Xu et al., 2019; Fan et al., 2020; Engelmann & Lessmann, 2021; Zhao et al., 2021; Kotelnikov et al., 2023). Unfortunately, these methods are only useful for sampling as they either dont model the density explicitly or cant evaluate it due to untractable marginalization over high dimensional latent variable spaces. Furthermore, despite growing in popularity, DL has still failed to displace tree-based ensemble methods as the tool of choice for handling tabular discriminative tasks with gradient boosting still being found to outperform neural-network-based methods in many real world datasets (Grinsztajn et al., 2022; Borisov et al., 2022a). While there have been recent efforts to extend the success of tree-based models to generative modeling (Correia et al., 2020; Wen & Hang, 2022; Nock & Guillame-Bert, 2022; Watson et al., 2023; Nock & Guillame-Bert, 2023; Jolicoeur-Martineau et al., 2024), direct extensions of Random Forests (RF) and Gradient Boosted Decision Trees (GBDT) are still missing. It is this gap that we try to address, seeking to keep the general algorithmic structure of these popular algorithms but replacing the optimization of their discriminative objective with generative counterpart. Our main contributions in this regard are: We propose NRGBoost, novel energy-based generative boosting model that is trained to maximize local second order approximation to the likelihood at each boosting round."
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Downsampled MNIST samples generated by the best generative models on this dataset. Despite this being simple image dataset, it can be challenging for tabular generative models due to the high dimensionality and complex structure of correlations between features. We find NRGBoost to be the only model that is able to generate passable samples. We propose an amortized sampling approach that greatly reduces the training time of NRGBoost, which, like other energy-based models, is often dominated by sampling. We explore the use of bagged ensembles of Density Estimation Trees (DET) (Ram & Gray, 2011) with feature subsampling as generative counterpart to Random Forests. The longstanding popularity of GBDT models in machine learning practice can, in part, be attributed to the strength of its empirical results and the efficiency of its existing implementations. We therefore focus on an experimental evaluation in real world datasets spanning range of use cases, number of samples and features. On smaller datasets, our implementation of NRGBoost can be trained in few minutes on mid-range consumer CPU and achieves similar discriminative performance to standard GBDT model, while remaining competitive with the best generative models for sampling."
        },
        {
            "title": "2 ENERGY BASED MODELS",
            "content": "An Energy-Based Model (EBM) parametrizes the logarithm of probability density function directly (up to an unspecified normalizing constant): qf (x) = exp (f (x)) Z[f ] . (1) Here (x) : is real function over the input domain.1 We will avoid introducing any parametrization, instead treating the function F(X ) lying in an appropriate function space over the input space as our model parameter directly. Z[f ] = (cid:80) xX exp (f (x)), known as the partition function, is then functional of giving us the necessary normalizing constant. This is the most flexible way one could represent probability density function making essentially no compromises on its structure. The downside to this is that for most interesting choices of F, computing or estimating this normalizing constant is untractable which makes training these models difficult. Their unnormalized nature, however, does not prevent EBMs from being useful in number 1We will assume that is finite and discrete to simplify the notation and exposition but everything is applicable to bounded continuous input spaces, replacing the sums with integrals as appropriate."
        },
        {
            "title": "Preprint",
            "content": "of applications besides sampling. They can be used to perform inference over small enough groups of variables. Partitioning the input space into set of observed variables xo and unobserved variables xu we have qf (xuxo) = exp (f (xu, xo)) exp (f (x u, xo)) (cid:80) , (2) which only involves normalizing (i.e., computing softmax) over the possible values of the unobserved variables. Furthermore, for anomaly or out of distribution detection, knowledge of the normalizing constant is not necessary. One common way to train an energy-based model to approximate data generating distribution, p(x), is to minimize the Kullback-Leibler divergence between and qf , or equivalently, maximize the expected log likelihood functional: L[f ] = Exp log qf (x) = Expf (x) log Z[f ] . (3) This optimization is typically carried out by gradient descent over the parameters of , but due to the intractability of the partition function, one must rely on Markov Chain Monte Carlo (MCMC) sampling to estimate the gradients (Song & Kingma, 2021)."
        },
        {
            "title": "3 NRGBOOST",
            "content": "Expanding the increase in log-likelihood in equation 3 due to variation δf around an energy function up to second order we have L[f + δf ] L[f ] Expδf (x) Exqf δf (x) 1 2 Varxqf δf (x) =: Lf [δf ] . (4) The δf that maximizes this quadratic approximation should thus have large positive difference between the expected value under the data and under qf while having low variance under qf . We note that just like the original log-likelihood, this Taylor expansion is invariant to adding an overall constant to δf . This means that, in maximizing equation 4 we can consider only functions that have zero expectation under qf in which case we can simplify Lf [δf ] as Lf [δf ] = Expδf (x) 1 2 Exqf δf 2(x) . (5) We thus formulate our boosting algorithm as modelling the data density with an additive energy function. At each boosting iteration we improve upon the current energy function ft by finding an optimal step δf that maximizes Lft[δf ]: δf (6) = arg max δf Ht Lft[δf ] , where Ht is an appropriate space of functions (satisfying Exqft δf (x) = 0 if equation 5 is used). The solution to this problem can be interpreted as Newton step in the space of energy functions. Because for an EBM, the Fisher Information matrix with respect to the energy function and the hessian of the expected log-likelihood are the same (see Appendix A), we can also interpret the solution to equation 6 as natural gradient step. This approach is analogous to the second order step implemented in modern gradient boosting libraries such as XGBoost (Chen & Guestrin, 2016) and LightGBM (Ke et al., 2017) and which can be traced back to Friedman et al. (2000). In updating the current iterate, ft+1 = ft + αt δf by an additional scalar step-size αt. This can be interpreted as globalization strategy to account for the fact that the quadratic approximation in equation 4 is not necessarily valid over large steps in function space. common strategy in nonlinear optimization would be to select αt via line search based on the original log-likelihood. Common practice in discriminative boosting however is to interpret this step size as regularization parameter and to select fixed value in ]0, 1] with (more) smaller steps typically outperforming fewer larger ones when it comes to generalization. We choose to adopt hybrid strategy, first selecting an optimal step size by line search and then shrinking it by fixed factor. We find that this typically accelerates convergence allowing the algorithm to take comparatively larger steps that increase the likelihood in the initial phase of boosting. , we scale δf For starting point, f0, we can choose the logarithm of any probability distribution over as long as it is easy to evaluate. Sensible choices are uniform distribution (i.e., 0), the product of marginals for the training set, or any mixture distribution between these two. In Figure 2 we show an example of NRGBoost starting from uniform distribution and learning toy 2D data density."
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Density learned by NRGBoost at different boosting iterations (1, 3, 10 and 100), for toy dataset (shown right), starting from uniform distribution. Weak learners are trees with 16 leaves."
        },
        {
            "title": "3.1 WEAK LEARNERS",
            "content": "As weak learner we will consider piecewise constant functions defined by binary trees over the input space. Letting (cid:83)J j=1 Xj = be the partitioning of the input space induced by the leaves of binary tree whose internal nodes represent split along one dimension into two disjoint partitions, we take as the set of functions such as δf (x) = (cid:88) j=1 wj1Xj (x) , (7) where 1X denotes the indicator function of subset and wj are values associated with each leaf [1..J]. In standard decision tree these values would typically encode an estimate of p(yx Xj), with being special target variable that is never considered for splitting. In our generative approach they encode unconditional densities (or more precisely energies) over each leafs support and every variable can be used for splitting. Note that our functions δf are thus parametrized by the values wj as well the structure of the tree and the variables and values for the split at each node which ultimately determine the Xj. We omit these dependencies for brevity. Replacing the definition in equation 7 in our objective (equation 5) we get the following optimization problem to find the optimal decision tree: max w1,...,wJ ,X1,...,XJ s.t. j=1 (cid:88) j=1 (cid:88) (cid:18) wjP (Xj) (cid:19) j Qf (Xj) 1 2 wjQf (Xj) = 0 , (8) where (Xj) and Qf (Xj) denote the probability of the event Xj under the respective distribution and the constraint ensures that δf has zero expectation under qf . With respect to the leaf weights this is quadratic program whose optimal solution and objective values are respectively given by j = (Xj) Qf (Xj) 1 , (X1, . . . , XJ ) = 1 (cid:88) j=1 . (9) 2(Xj) Qf (Xj) Because carrying out the maximization of this optimal value over the tree structure that determines the Xj is hard, we approximate its solution by greedily growing tree that maximizes it when considering how to split each node individually. parent leaf with support XP is thus split into two child leaves, with disjoint support, XL XR = XP , so as to maximize over all possible partitionings along single dimension, (XP ), the following objective: 2(XR) Qf (XR) max XL,XRP(XP ) 2(XP ) Qf (XP ) 2(XL) Qf (XL) (10) + . Note that, when using parametric weak learners, computing second order step would typically involve solving linear system with full Hessian. As we can see, this is not the case when the"
        },
        {
            "title": "Preprint",
            "content": "weak learners are decision trees, where the optimal value to assign to leaf does not depend on any information from other leaves and, likewise, the optimal objective value is sum of terms, each depending only on information from single leaf. This would have not been the case had we tried to optimize the likelihood functional in Equation 3 directly instead of its quadratic approximation."
        },
        {
            "title": "3.2 AMORTIZED SAMPLING",
            "content": "To compute the leaf values in equation 9 and the splitting criterion in equation 10 we would have to know (X) and be able to compute Qf (X) which is infeasible due to the untractable normalization constant. We therefore estimate these quantities, with recourse to empirical data for (X), and to samples approximately drawn from the model with MCMC. Because even if the input space is not partially discrete, is still discontinuous and constant almost everywhere we cant use gradient based samplers and therefore rely on Gibbs sampling instead. This only requires evaluating each ft along one dimension at time, while keeping all others fixed which can be computed efficiently for tree by traversing it only once. However, since at boosting iteration our energy function is sum of trees, this computation scales linearly with the iteration number. This makes the overall time spent sampling quadratic in the number of boosting iterations and thus precluding us from training models with large number of trees. In order to reduce the burden associated with this sampling, which dominates the runtime of training the model, we propose new sampling approach that leverages the cumulative nature of boosting. The intuition behind this approach is that the set of samples used in the previous boosting round are (approximately) drawn from distribution that is already close to the new model distribution. It could therefore be helpful to keep some of those samples, especially those that conform the best to the new model. Rejection sampling allows us to do just that. The boosting update in terms of the densities takes the following multiplicative form: qt(x) = kt qt1(x) exp (αtδft(x)) . (11) Here, kt is an unknown multiplicative constant and since δft is given by tree, we can easily bound the exponential factor by finding the leaf with the largest value. We can therefore use the previous model, qt1(x), as proposal distribution for which we already have set of samples and keep each sample, x, with an acceptance probability of: (cid:104) αt paccept(x) = exp δft(x) max δft(x) (12) (cid:17)(cid:105) (cid:16) . We note that knowledge of the constant kt is not necessary to compute this probability. Our proposed sampling strategy is to maintain fixed size pool of approximate samples from the model. At the end of each boosting round, we use rejection sampling to remove samples from the pool and draw new samples from the model using Gibbs sampling. Note that q0 is typically simple model for which we can both directly evaluate the desired quantities (i.e., Q0(X) for given partition X) and cheaply draw exact samples from. As such, no samples are required for the first iteration of boosting and for the second we can initialize the sample pool by drawing exact samples from q1 with rejection sampling using q0 as proposal distribution. This approach works better when the range of δft and/or the step sizes αt are small as this leads to larger acceptance probabilities. Note that in practice it can be helpful to independently refresh fixed fraction of samples, pref resh, at each round of boosting in order to encourage more diverse samples between rounds. This can be accomplished by keeping each sample with probability paccept(x)(1 pref resh) instead. 3.3 REGULARIZATION The simplest way to regularize boosting model is to stop training when overfitting is detected by monitoring suitable performance metric on validation set. For NRGBoost this could be the increase in log-likelihood at each boosting round. However, estimating this quantity would require drawing additional validation samples from the model (see Appendix A.1). An alternative viable validation strategy which needs no additional samples is to simply monitor discriminative performance metric. This amounts to monitoring the quality of qf (xixi) instead of the full qf (x). Besides early stopping, the decision trees themselves can be regularized by limiting the depth or total number of leaves in each tree. Additionally, we can rely on other strategies such as disregarding splits"
        },
        {
            "title": "Preprint",
            "content": "that would result in leaf with too little training data, (X), model data, Qf (X), volume (X) or too high of ratio between training and model data (X)/Qf (X). We found the latter to be the most effective of these, not only yielding better generalization performance than other approaches, but also having the added benefit of allowing us to lower bound the acceptance probability of our rejection sampling scheme. Furthermore, as we show in Appendix A.2, limiting this ratio guarantees that small enough step size produces an increase in likelihood at each round of boosting."
        },
        {
            "title": "4 DENSITY ESTIMATION TREES AND DENSITY ESTIMATION FORESTS",
            "content": "Density Estimation Trees (DET) were proposed by Ram & Gray (2011) as an alternative to histograms and kernel density estimation but have received little attention as generative models for sampling or other applications. They model the density function as constant value over the support of each leaf in binary tree, q(x) = (cid:88) j=1 ˆP (Xj) (Xj) 1Xj (x) , (13) with ˆP (X) being an empirical estimate of probability of the event and (X) denoting the volume of X. Note that it is possible to draw an exact sample from this type of model by randomly selecting leaf, [1..J], given probabilities ˆP (Xj), and then drawing sample from uniform distribution over Xj. To fit DET, Ram & Gray (2011) propose optimizing the Integrated Squared Error (ISE) between the data and model distributions which, following similar approach to Section 3.1, leads the following optimization problem when considering how to split leaf node: max XL,XRP(XP ) D(P (XL), (XL)) + D(P (XR), (XR)) D(P (XP ), (XP )) . (14) For the ISE, should be taken as the function DISE(P, ) = 2/V which leads to similar splitting criterion to Equation 14 but replacing the previous models distribution with the volume measure which can be interpreted as the uniform distribution on (up to multiplicative constant). Maximum Likelihood Often generative models are trained to maximize the likelihood of the observed data. This was left for future work in Ram & Gray (2011) but, as we show in Appendix B, can be accomplished by replacing DISE with DKL(P, ) = log (P/V ). This choice of minimization criterion can be seen as analogous to the choice between Gini impurity and Shannon entropy in the computation of the information gain in decision trees. Bagging and Feature Subsampling Following the common approach in decision trees, Ram & Gray (2011) suggest the use of pruning for regularization of DET models. Practice has however evolved to prefer bagging as form of regularization rather than relying on single decision trees. We employ same principle to DETs by fitting many trees on bootstrap samples of the data. We also adopt the common practice from Random Forests of randomly sampling subset of features to consider when splitting any leaf node in order to encourage independence between the different trees in the ensemble. The ensemble model, which we call Density Estimation Forests (DEF) in the sequel, is thus an additive mixture of DETs with uniform weights, therefore still allowing for normalized density computation and exact sampling."
        },
        {
            "title": "5 RELATED WORK",
            "content": "Generative Boosting Most prior work on generative boosting focuses on unstructured data and the use of parametric weak learners and is split between two approaches: (i) Additive methods that model the density function as an additive mixture of weak learners such as Rosset & Segal (2002); Tolstikhin et al. (2017). (ii) Those that take multiplicative approach modeling the density function as an unnormalized product of weak learners. The latter is equivalent to the energy based approach that writes the energy function (log density) as an additive sum of weak learners. Welling et al. (2002) in particular also approach boosting from the point of view of functional optimization of the likelihood or the logistic loss of an energy-based model. However, they rely on first order local approximation"
        },
        {
            "title": "Preprint",
            "content": "of the objective since they focus on parametric weak learners such as restricted Boltzmann machines for which second order step would be impractical. Greedy Multiplicative Boosting Another more direct multiplicative boosting framework was first proposed by Tu (2007). At each boosting round discriminative classifier is trained to distinguish between empirical data and data generated by the current model by estimating the likelihood ratio p(x)/qt(x). This estimated ratio is used as direct multiplicative factor to update the current model qt (after being raised to an appropriate step size). In ideal conditions this greedy procedure would converge in single iteration if step size of 1 would be used. While Tu (2007) does not prescribe particular choice of classifier, Grover & Ermon (2018) proposes similar concept where the ratio is estimated based on an adversarial bound for an -divergence and Cranko & Nock (2019) provides additional analysis on this method. We note that the main difference between this greedy approach and NRGBoost is that the latter attempts to update the current density proportionally to an exponential of the likelihood ratio, exp (αt p(x)/qt(x)), instead of (p(x)/qt(x))αt directly. In Appendix we explore the differences between NRGBoost and this approach when it is adapted to use trees as weak learners. Tree-Based Density Modelling Other authors have proposed tree-based density models similar to DET (Nock & Guillame-Bert, 2022) or additive mixtures of tree-based models (Correia et al., 2020; Wen & Hang, 2022; Watson et al., 2023) but perhaps surprisingly, the natural idea of creating an ensemble of DET models through bagging has not been explored before as far as we are aware. Distinguishing features of some of these alternative approaches are: (i) Not relying on density estimation goal to drive the partitioning of the input space. Correia et al. (2020) leverages standard discriminative Random Forest, therefore giving special treatment to particular input variable whose conditional estimation drives the choice of partitions and Wen & Hang (2022) proposes using mid-point random tree partitioning. (ii) Leveraging more complex models for the data in the leaf of tree instead of uniform density (Correia et al., 2020; Watson et al., 2023). This can allow for the use of trees that are more representative with smaller number of leaves. (iii) Relying on an adversarial framework where the generator and discriminator are both tree (Nock & Guillame-Bert, 2022) or an ensemble of trees (Watson et al., 2023). While this also leads to an iterative approach, unlike with boosting or bagging, the new model trained at each round doesnt add to the previous one but replaces it instead. Other Recent Tree-Based approaches Nock & Guillame-Bert (2023) proposes different ensemble approach where each tree does not have its own leaf values that get added or multiplied to produce the final density, but instead serve to collectively define the partitioning of the input space. To train such models the authors propose framework where, rather than adding new tree to the ensemble at every iteration, the model is initialized with fixed number of tree root nodes and each iteration adds split to an existing leaf node. Finally Jolicoeur-Martineau et al. (2024) propose diffusion model where tree-based model (e.g., GBDT) is used to regress the score function. Being diffusion model, however, means that computing densities is untractable."
        },
        {
            "title": "6 EXPERIMENTS",
            "content": "We evaluate the our proposed methods on 5 tabular datasets from the UCI Machine Learning Repository (Dheeru & Karra Taniskidou, 2017): Abalone (AB), Physicochemical Properties of Protein Tertiary Structure (PR), Adult (AD), MiniBooNE (MBNE) and Covertype (CT) as well as the California Housing (CH) dataset available through scikit-learn (Pedregosa et al., 2011). We also include downsampled version of MNIST (by 2x along each dimension) which allows us to visually assess the quality of individual samples, something that is generally difficult with structured tabular data. More details about these datasets are given in Appendix D.1. We split our experiments into two sections, the first to evaluate the quality of density models directly on single variable inference task and the second to investigate the performance of our proposed models when used for sampling, comparing them against more specialized models."
        },
        {
            "title": "Preprint",
            "content": "Table 1: Discriminative performance of different methods at inferring the value of target variable. We use R2 for regression tasks, AUC for binary classification and accuracy for multiclass classification. The reported values are means and standard errors over 5 cross-validation folds. The best generative method for each dataset is highlighted in bold and other methods that are not significantly worse (as determined by paired t-test at 95% confidence level) are underlined. R2 CH AB AUC Accuracy PR AD MBNE MNIST CT XGBoost 0.552 0.035 0.849 0.009 0.678 0.004 0.927 0.000 0.987 0.000 0.976 0. 0.971 0.001 RFDE ARF DEF (ISE) DEF (KL) NRGBoost 0.071 0.096 0.531 0.032 0.467 0.037 0.482 0.027 0.547 0.036 0.679 0.002 0.938 0.005 0.790 0.003 0.852 0.002 0.948 0.001 Due to the discrete nature of the MNIST dataset and the fact that the ARF algorithm tries to fit continuous distributions at the leaves, we could not obtain reasonable density model for this dataset. 0.862 0.002 0.893 0.002 0.854 0.003 0.892 0.001 0.920 0.001 0.668 0.008 0.968 0.001 0.653 0.011 0.939 0.001 0.974 0. 0.340 0.004 0.758 0.009 0.737 0.008 0.801 0.008 0.850 0.011 0.059 0.007 0.591 0.007 0.566 0.002 0.639 0.004 0.676 0.009 0.302 0.010 - 0.206 0.011 0.487 0.007 0.966 0.001 6.1 SINGLE VARIABLE INFERENCE We test the ability of generative model, trained to learn the density over all input variables, q(x), to infer the value of single one (i.e., we evaluate the quality of its estimate of q(xixi)). For this purpose we pick xi = as the original target of the dataset, noting that the models that we train do not treat this variable in any special way, except for the selection of the best model in validation. As such, we would expect that the models performance in inference over this particular variable is indicative of its strength on any other single variable inference task and also indicative of the quality of the full q(x) from which the conditional probability estimate is derived. We use XGBoost (Chen & Guestrin, 2016) as baseline for what should be achievable by strong discriminative model, noting that it is trained to maximize the discriminative likelihood, Exp log q(xixi), directly, not wasting model capacity in learning other aspects of the full data distribution. We also compare to two other tree-based generative baselines: RFDE (Wen & Hang, 2022) and ARF (FORDE) (Watson et al., 2023). The former allows us to gauge the impact of the guided partitioning used in DEF models over random partitioning of the input space. We use random search to tune the hyperparameters of the XGBoost model and grid search to tune the most important hyperparameters of each generative density model. We employ 5-fold crossvalidation, repeating the hyperparameter tuning on each fold. For the full details of the experimental protocol please refer to Appendix D. We find that NRGBoost outperforms the remaining generative models (see Table 1), even achieving comparable performance to XGBoost on the smaller datasets and with small gap on the three larger ones (MBNE, MNIST, CT). We note also that for the regression datasets, the generative models provide an estimate of the full conditional distribution over the target variable rather than point estimate like XGBoost. While there are other variants of boosting that can do the same (Duan et al., 2020), they rely on parametric assumption about p(yx) that needs to hold for any x. 6.2 SAMPLING We evaluate two different aspects of the quality of generated samples: their utility for training machine learning model and how distinguishable they are from real data. Besides ARF, we compare to TVAE (Xu et al., 2019) and TabDDPM (Kotelnikov et al., 2023), two neural-network based generative models, as well as Forest-Flow (Jolicoeur-Martineau et al., 2024), tree-based diffusion model. Note that these three methods are not capable of density estimation. Machine Learning Efficiency Machine learning (ML) efficiency has been popular way to measure the quality of generative models for sampling (Xu et al., 2019; Borisov et al., 2022b; Kotelnikov et al., 2023). It relies on using samples from generative model to train discriminative model which is then evaluated on real data, and is thus similar to the single variable inference performance that"
        },
        {
            "title": "Preprint",
            "content": "Table 2: Performance of an XGBoost model trained on synthetic data and on real data (for reference). For consistency, we use the same discriminative metrics as in Table 1 for evaluating its performance. Reported values are the averages and standard errors over 5 synthetic datasets generated by the same generative model. The best generative method for each dataset is highlighted in bold and methods that are not significantly worse (as determined by t-test at 95% confidence level) are underlined. Real Data AB 0.554 R2 CH 0.838 0.483 0.006 TVAE 0.539 0.018 TabDDPM Forest-Flow 0.418 0.019 ARF DEF (KL) NRGBoost 0.504 0.020 0.450 0.013 0.528 0.016 0.758 0.005 0.807 0.005 0.716 0.003 0.739 0.003 0.762 0.006 0.801 0. PR 0.682 0.365 0.005 0.596 0.007 0.412 0.009 0.524 0.003 0.498 0.007 0.573 0.008 AUC Accuracy AD 0.927 MBNE MNIST 0.987 0. CT 0.972 0.898 0.001 0.910 0.001 0.879 0.003 0.901 0.001 0.892 0.001 0.914 0.001 0.975 0.000 0.984 0.000 0.964 0.001 0.971 0.001 0.943 0.002 0.977 0. 0.770 0.009 - 0.224 0.010 0.908 0.002 0.230 0.028 0.959 0.001 0.750 0.002 0.818 0.001 0.705 0.002 0.848 0.002 0.753 0.002 0.895 0.001 Figure 3: Joint histogram for the latitude and longitude for the California Housing dataset. we use to compare density models in Section 6.1. In fact, if the density models support covers that of the full data, one would expect the discriminative model to recover the generators q(yx), and therefore its performance, in the limit where infinite generated data is used to train it. We use an XGBoost model as the discriminative model and train it on the same number of training samples as the original data. For the density models, we generate samples from the best model found in the previous section and for TVAE and TabDDM we select their hyperparameters by evaluating the ML Efficiency in the real validation set (full details of the hyperparameter tuning are provided in Appendix D.2). Note that this leaves these models at potential advantage since the hyperparameter selection is based on the metric that is being evaluated. We repeat all experiments 5 times, with different generated datatsets from each model and report the performance of the discriminative model in Table 2. We find that NRGBoost and TabDDPM alternate as the best-performing model depending on the dataset (with two inconclusive cases), and NRGBoost is never ranked lower than second on any dataset. We note also that, despite our best efforts with additional manual tuning, we could not achieve reasonable model with TabDDM on MNIST."
        },
        {
            "title": "Preprint",
            "content": "Table 3: AUC of an XGBoost model trained to distinguish real from generated data (lower means better). Reported values are the averages and standard errors over 5 synthetic datasets generated by the same model. The best generative method for each dataset is highlighted in bold and methods that are not significantly worse (as determined by t-test at 95% confidence level) are underlined. CT MNIST MBNE AD AB CH PR 0.971 0.004 TVAE TabDDPM 0.818 0.015 Forest-Flow 0.987 0.002 ARF DEF (KL) NRGBoost 0.975 0.005 0.823 0.013 0.625 0.017 0.834 0.006 0.667 0.005 0.926 0.002 0.973 0.004 0.751 0.008 0.574 0. 0.940 0.002 0.628 0.004 0.885 0.002 0.795 0.008 0.877 0.002 0.631 0.006 0.898 0.001 0.604 0.002 0.932 0.002 0.992 0.000 0.956 0.002 0.559 0.003 1.000 0.000 0.789 0.002 1.000 0.000 0.998 0.000 1.000 0.000 0.993 0. 1.000 0.000 - 1.000 0.000 1.000 0.000 1.000 0.000 0.943 0.003 0.999 0.000 0.915 0.007 0.985 0.001 0.989 0.001 0.999 0.000 0.724 0.006 Table 4: Wasserstein distance between empirical test distribution and synthetic samples. Reported results are averages and standard errors over 5 repeated experiments. Smaller is better. MNIST PR MBNE AD AB CH CT 0.302 0.005 TVAE TabDDPM 0.500 0.023 Forest-Flow 0.248 0.020 ARF DEF (KL) NRGBoost 0.218 0.015 0.337 0.006 0.502 0.045 0.206 0.010 0.169 0.009 0.205 0.009 0.240 0.009 0.210 0.012 0.178 0.011 0.274 0.008 0.195 0.003 0.231 0.008 0.238 0.006 0.264 0.011 0.225 0.010 1.086 0.023 0.938 0.029 1.243 0. 1.378 0.020 1.696 0.023 1.051 0.013 0.425 0.032 109.5 93.13 4.295 3.697 0.732 0.664 18.29 14.90 22.33 20.11 20.61 0.042 - 20.73 0.574 18.14 0.060 261.0 35.01 15.70 0.188 0.807 0.023 0.646 0.022 0.893 0. 0.709 0.018 0.935 0.024 0.602 0.015 Discriminator Measure Similar to Borisov et al. (2022b) we test the capacity of discriminative model to distinguish between real and generated data. We use the original validation set as the real part of the training data for the discriminator in order to avoid benefiting generative methods that overfit their original training set. new validation set is carved out of the original test set (20%) and used to tune the hyperparameters of an XGBoost model which we use as our discriminator. We report the AUC of this model on the remainder of the real test data in Table 3 which shows that NRGBoost outperforms other methods except on the PR (with an inconclusive result) and MBNE datasets. Statistical Distance Measure As an additional measure of the statistical dissimilarity between data and model distributions we evaluate Wasserstein distance using similar setup to JolicoeurMartineau et al. (2024). Namely, we min-max scale the numerical variables, and one-hot encode the categorical variables and scale them by 1/2. We use L1 distance in the formulation of the optimal transport problem. Since finding the optimal solution has worst case cubic scaling with the sample size, we sub-sample maximum of 5000 samples from the original train or test sets and use an equal number of synthetic samples. We repeat the evaluation for each method and dataset 5 times using different synthetic data and also different subsampling seeds for the real data (where applicable). Our results for the distance between empirical training and test distribution and synthetic samples from each model are reported in Tables 5 and 4 respectively. We found these results to be somewhat sensitive to the choice of normalization of the numericals and categoricals, as well as to the choice of Table 5: Wasserstein distance between empirical train distribution and synthetic samples. Reported results are averages and standard errors over 5 repeated experiments. Smaller is better. MNIST PR MBNE AD AB CH CT 0.299 0.005 TVAE TabDDPM 0.467 0.021 Forest-Flow 0.234 0.020 ARF DEF (KL) NRGBoost 0.199 0.015 0.320 0.006 0.509 0.047 0.195 0.010 0.153 0.007 0.192 0.007 0.233 0.009 0.199 0.012 0.170 0.012 0.274 0.011 0.190 0.005 0.238 0.009 0.232 0.007 0.260 0.010 0.230 0.011 1.075 0.024 0.895 0.019 1.239 0. 1.363 0.018 1.682 0.024 1.028 0.015 0.366 0.055 109.6 93.14 4.214 3.683 0.698 0.684 18.21 14.92 22.25 20.09 20.59 0.041 - 20.73 0.574 18.14 0.060 261.0 15.60 15.60 0.212 0.808 0.029 0.640 0.016 0.896 0. 0.703 0.019 0.933 0.025 0.600 0."
        },
        {
            "title": "Preprint",
            "content": "distance in this normalized space. Despite NRGBoost not being as dominant as in the discriminator measure, it still fares well, obtaining the best average rank over all datasets and experiments. Qualitatively speaking, we find that the samples generated by NRGBoost also look visually similar to the real data in both the MNIST and California datasets (see Figures 1 and 3), being the only generative model that can claim that for the former."
        },
        {
            "title": "7 DISCUSSION",
            "content": "The proposed DEF models are easy to sample from and require no sampling during training. However, we find that, they require deep trees to model the data well which, in turn, also requires larger number of trees in the ensemble to regularize. In our experiments we found that the performance of DEF models was often capped by the maximum number of leaves we allowed (214). In contrast, NRGBoost was able to model the data better while using shallower trees and in fewer number (the models selected on all datasets except for MNIST and CT had 210 leaves). Its main downside is that it can only be sampled from approximately using more expensive MCMC and also requires sampling during the training process. While our fast Gibbs sampling implementation and our proposed sampling approach were able to mitigate the slow training, making these models practical, they can still be cumbersome to use for sampling, particularly if independent samples are required, due to autocorrelation between samples from the same Markov Chain. We argue however that unlike in image or text generation where fast sampling is necessary for an interactive user experience, this can be less of concern for the task of generating synthetic datasets where the one time cost of sampling is not as important as faithfully capturing the data generating distribution. limitation of tree-based models when compared to DL approaches is that they require keeping the full training data in memory. While discretization allows this data to be stored in compact format, this could still limit the applicability of these methods to very large datasets."
        },
        {
            "title": "8 CONCLUSION",
            "content": "We extend the two most popular tree-based discriminative methods for use in generative modeling. We find that our boosting approach, in particular, offers generally good discriminative performance and competitive sampling performance to more specialized alternatives. We hope that these results encourage further research into generative boosting approaches for tabular data, in particular exploring other applications besides sampling that are enabled by (unnormalized) density models."
        },
        {
            "title": "REFERENCES",
            "content": "Kristin Blesch and Marvin Wright. arfpy: python package for density estimation and generative modeling with adversarial random forests. arXiv preprint arXiv:2311.07366, 2023. Vadim Borisov, Tobias Leemann, Kathrin Seßler, Johannes Haug, Martin Pawelczyk, and Gjergji Kasneci. Deep neural networks and tabular data: survey. IEEE transactions on neural networks and learning systems, 2022a. Vadim Borisov, Kathrin Seßler, Tobias Leemann, Martin Pawelczyk, and Gjergji Kasneci. Language models are realistic tabular data generators. arXiv preprint arXiv:2210.06280, 2022b. Tianqi Chen and Carlos Guestrin. XGBoost: Scalable Tree Boosting System. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 785794, August 2016. doi: 10.1145/2939672.2939785. arXiv:1603.02754 [cs]. Alvaro Correia, Robert Peharz, and Cassio de Campos. Joints in random forests. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1140411415. Curran Associates, Inc., 2020. Zac Cranko and Richard Nock. Boosted Density Estimation Remastered. In Proceedings of the 36th International Conference on Machine Learning, pp. 14161425. PMLR, May 2019. ISSN: 2640-3498."
        },
        {
            "title": "Preprint",
            "content": "Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal Processing Magazine, 29(6):141142, 2012. Dua Dheeru and Efi Karra Taniskidou. UCI machine learning repository, 2017. Tony Duan, Avati Anand, Daisy Yi Ding, Khanh Thai, Sanjay Basu, Andrew Ng, and Alejandro Schuler. Ngboost: Natural gradient boosting for probabilistic prediction. In International conference on machine learning, pp. 26902700. PMLR, 2020. Justin Engelmann and Stefan Lessmann. Conditional wasserstein gan-based oversampling of tabular data for imbalanced learning. Expert Systems with Applications, 174:114582, 2021. Ju Fan, Junyou Chen, Tongyu Liu, Yuwei Shen, Guoliang Li, and Xiaoyong Du. Relational data synthesis using generative adversarial networks: design space exploration. Proceedings of the VLDB Endowment, 13(12):19621975, August 2020. ISSN 2150-8097. doi: 10.14778/3407790. 3407802. Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Additive logistic regression: statistical view of boosting (With discussion and rejoinder by the authors). The Annals of Statistics, 28(2): 337407, April 2000. ISSN 0090-5364, 2168-8966. doi: 10.1214/aos/1016218223. Publisher: Institute of Mathematical Statistics. Léo Grinsztajn, Edouard Oyallon, and Gaël Varoquaux. Why do tree-based models still outperform deep learning on typical tabular data? Advances in neural information processing systems, 35: 507520, 2022. Aditya Grover and Stefano Ermon. Boosted generative models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018. Charles R. Harris, K. Jarrod Millman, Stéfan J. van der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fernández del Río, Mark Wiebe, Pearu Peterson, Pierre Gérard-Marchant, Kevin Sheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Array programming with NumPy. Nature, 585(7825):357362, September 2020. doi: 10.1038/s41586-020-2649-2. Alexia Jolicoeur-Martineau, Kilian Fatras, and Tal Kachman. Generating and imputing tabular data via diffusion and flow-based gradient-boosted trees. In International Conference on Artificial Intelligence and Statistics, pp. 12881296. PMLR, 2024. James Jordon, Jinsung Yoon, and Mihaela Van Der Schaar. Pate-gan: Generating synthetic data with differential privacy guarantees. In International conference on learning representations, 2018. Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. LightGBM: Highly Efficient Gradient Boosting Decision Tree. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. Akim Kotelnikov, Dmitry Baranchuk, Ivan Rubachev, and Artem Babenko. Tabddpm: Modelling tabular data with diffusion models. In International Conference on Machine Learning, pp. 17564 17579. PMLR, 2023. Richard Nock and Mathieu Guillame-Bert. Generative trees: Adversarial and copycat. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 1690616951. PMLR, 1723 Jul 2022. Richard Nock and Mathieu Guillame-Bert. Generative forests. arXiv preprint arXiv:2308.03648, 2023. Melissa E. ONeill. Pcg: family of simple fast space-efficient statistically good algorithms for random number generation. Technical Report HMC-CS-2014-0905, Harvey Mudd College, Claremont, CA, September 2014."
        },
        {
            "title": "Preprint",
            "content": "F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:28252830, 2011. Parikshit Ram and Alexander G. Gray. Density estimation trees. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 627635, San Diego California USA, August 2011. ACM. ISBN 978-1-4503-0813-7. doi: 10.1145/2020408. 2020507. Saharon Rosset and Eran Segal. Boosting Density Estimation. In Advances in Neural Information Processing Systems, volume 15. MIT Press, 2002. Yang Song and Diederik Kingma. How to train your energy-based models. arXiv preprint arXiv:2101.03288, 2021. Ilya Tolstikhin, Sylvain Gelly, Olivier Bousquet, Carl-Johann Simon-Gabriel, and Bernhard Schölkopf. Adagan: Boosting generative models. Advances in neural information processing systems, 30, 2017. Zhuowen Tu. Learning Generative Models via Discriminative Approaches. In 2007 IEEE Conference on Computer Vision and Pattern Recognition, pp. 18, June 2007. doi: 10.1109/CVPR.2007. 383035. ISSN: 1063-6919. David S. Watson, Kristin Blesch, Jan Kapar, and Marvin N. Wright. Adversarial random forests for density estimation and generative modeling. In Francisco Ruiz, Jennifer Dy, and Jan-Willem van de Meent (eds.), Proceedings of The 26th International Conference on Artificial Intelligence and Statistics, volume 206 of Proceedings of Machine Learning Research, pp. 53575375. PMLR, 2527 Apr 2023. Max Welling, Richard Zemel, and Geoffrey Hinton. Self Supervised Boosting. In Advances in Neural Information Processing Systems, volume 15. MIT Press, 2002. Hongwei Wen and Hanyuan Hang. Random forest density estimation. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 2370123722. PMLR, 1723 Jul 2022. Lei Xu, Maria Skoularidou, Alfredo Cuesta-Infante, and Kalyan Veeramachaneni. Modeling Tabular data using Conditional GAN. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. Zilong Zhao, Aditya Kunar, Robert Birke, and Lydia Chen. Ctab-gan: Effective table data synthesizing. In Asian Conference on Machine Learning, pp. 97112. PMLR, 2021."
        },
        {
            "title": "Appendix",
            "content": "A Theory A.1 Application to Piecewise Constant Functions . . . . . . . . . . . . . . . . . . . . . A.2 Simple Lower Bound on the Likelihood Increase . . . . . . . . . . . . . . . . . Density Estimation Trees Greedy Tree-Based Multiplicative Boosting Reproducibility D.1 Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 DEF and NRGBoost Implementation Details . . . . . . . . . . . . . . . . . . . . . D.3 Hyperparameter Tuning . D.4 Evaluation Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.5 Computational Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Additional Results E.1 Computational Effort . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2 MCMC Chain Convergence on MNIST . . . . . . . . . . . . . . . . . . . . . . . 15 17 18 19 20 20 21 23 24 24"
        },
        {
            "title": "A THEORY",
            "content": "The expected log-likelihood for an energy-based model (EBM), qf (x) = exp (f (x)) Z[f ] , is given by L[f ] = Exp log qf (x) = Expf (x) log Z[f ] . (15) (16)"
        },
        {
            "title": "The first variation of L can be computed as",
            "content": "δL[f ; g] := dL[f + ϵg] dϵ (cid:12) (cid:12) (cid:12) (cid:12)ϵ=0 = Exp g(x) δ log Z[f ; g] = Exp g(x) Exqf g(x) . (17) This is linear functional of its second argument, g, and can be regarded as directional derivative of at along variation g. The last equality comes from the following computation of the first variation of the log-partition function: δ log Z[f ; g] = = δZ[f ; g] Z[f ] 1 Z[f ] (cid:88) exp (f (x)) g(x) = (cid:88) exp (f (x)) Z[f ] = Exqf g(x) . g(x) (18) (19) (20) (21) Analogous to Hessian, we can differentiate Equation 17 again along second independent variation of yielding symmetric bilinear functional which we will write as δ2L[f ; g, h]. Note that the first term in equation 16 is linear in and thus has no curvature, so we only have to consider the log partition function itself: δ2L[f ; g, h] := 2L[f + ϵg + εh] ϵε (cid:12) (cid:12) (cid:12) (cid:12)(ϵ,ε)=0 = δ2 log Z[f ; g, h] = δ {δ log Z[f ; g]} [f ; h] (cid:88) (cid:41) exp (f (x)) g(x) [f ; h] (cid:40) = δ 1 Z[f ] = = δZ[f ; h] 2[f ] δZ[f ; h] Z[f ] (cid:88) 1 Z[f ] (cid:88) exp (f (x)) g(x)h(x) (25) exp (f (x)) g(x) Exqf g(x) 1 Z[f ] (cid:88) exp (f (x)) g(x)h(x) = Exqf h(x) Exqf g(x) Exqf h(x)g(x) = Covxqf (g(x), h(x)) . (22) (23) (24) (26) (27) (28) Note that this functional is negative semi-definite for all , i.e. δ2L[f ; h, h] 0, meaning that the log-likelihood is concave functional of . Using these results, we can now compute the Taylor expansion of the increment in log-likelihood from change + δf up to second order in δf : Lf [δf ] = δL[f ; δf ] + 1 2 = Expδf (x) Exqf δf (x) δ2L[f ; δf, δf ] 1 2 Varxqf δf (x) . (29) (30)"
        },
        {
            "title": "Preprint",
            "content": "As an aside, defining the functional derivative, δJ[f ] δf (x) , of functional implicitly by: (cid:88) δJ[f ] δf (x) g(x) = δJ[f ; g] , (31) we can formally define, by analogy with the parametric case, the Fisher Information \"Matrix\" (FIM) at as the following bilinear functional of two independent variations and h: [f ; g, h] := (cid:20) Exqf (cid:21) δ2 log qf (x) δf (y)δf (z) (cid:88) y,z g(y)h(z) = (cid:88) y,z δ2 log Z[f ] δf (y)δf (z) g(y)h(z) = δ2 log Z[f ; g, h] . (32) (33) (34) The only difference to the second-order variation of 16 computed in equation 22 would be that the expectation is taken under the model distribution, qf , instead of the data distribution p. However, because the only term in log qf (x) that is non-linear in is the log-partition functional, which is not function of x, this expectation plays no role in the computation and we get the result that the Fisher Information is the same as the negative Hessian of the log-likelihood for these models. A.1 APPLICATION TO PIECEWISE CONSTANT FUNCTIONS Considering weak learner such as δf (x) = (cid:88) j= wj1Xj (x) , where the subsets Xj are disjoint and cover the entire input space, , we have that q(x) (cid:88) j=1 wj1Xj (x) Exqδf (x) = = (cid:88) xX (cid:88) (cid:88) wj q(x) = j=1 xXj (cid:88) j= wjQ(Xj) . Similarly, making use of the fact that 1Xi (x)1Xj (x) = δij1Xi(x), we can compute Exqδf 2(x) = (cid:88) xX q(x) (cid:88) j=1 wj1Xj (x) = (cid:88) j=1 j Q(Xj) . In fact, we can extend this to any ordinary function of δf : Exq (δf (x)) = = = (cid:88) xX (cid:88) q(x) (cid:88) j=1 1Xj (x)g (δf (x)) (cid:88) q(x)g(wj) j=1 xXj (cid:88) j= g(wj)Q(Xj) , where we made use of the fact that the 1Xj constitute partition of unity: 1 = (cid:88) j=1 1Xj (x) . 16 (35) (36) (37) (38) (39) (40) (41) (42)"
        },
        {
            "title": "Preprint",
            "content": "Finally, we can compute the increase in log-likelihood from step + α δf as L[f + α δf ] L[f ] = Exp [α δf (x)] log Z[f + α δf ] + log Z[f ] = αExpδf (x) log Exqf exp(αδf (x)) = α (cid:88) j=1 wjP (Xj) log (cid:88) j=1 Qf (Xj) exp (αwj) , (43) (44) (45) where in equation 44 we made use of the equality: log Z[f + α δf ] log Z[f ] = log (cid:80) exp(f (x) + αδf (x)) Z[f ] = log (cid:88) qf (x) exp(αδf (x)) , (46) and of the result in equation 41 in the final step. This result can be used to conduct line search over the step size using training data and to estimate an increase in likelihood at each round of boosting for the purpose of early stopping, using validation data. A.2 SIMPLE LOWER BOUND ON THE LIKELIHOOD INCREASE We can take the results from the last subsection to show that each boosting round improves the likelihood of the model under ideal conditions. Substituting the NRGBoost leaf values wj = (Xj )/Qf (Xj ) 1 in Equation 45 we get: L[f + α δf ] L[f ] = α (cid:88) j=1 1 log 2 (Xj) Qf (Xj) (cid:88) j=1 (cid:20) Qf (Xj) exp α (cid:18) (Xj) Qf (Xj) (cid:19)(cid:21) 1 = αχ2(P Qf ) log exp (αW ) , (47) (48) where, with some abuse of notation, we denote by χ2(P Qf ) the χ2-divergence between the two discrete distributions over [1..J] induced by the partitioning obtained in the current boosting round. This value is always non-negative, and can only be zero if (Xj) = Qf (Xj) for all leaves. It therefore corresponds to an increase in likelihood that is as large as one can make the difference between the induced and Qf distributions by judicious choice of Xj. Note that this quantity is precisely the objective that NRGBoost tries to greedily maximize with its choice of Xj (see Equation 9). The second term in Equation 48 can be interpreted as the log of moment generating function for centered random variable that takes values in {P (Xj )/Qf (Xj ) 1}j[1..J] (i.e., the set of the leaf values), with corresponding probabilities {Qf (Xj)}j[1..J]. With our proposed regularization approach (see Section 3.3), we limit the ratio (Xj )/Qf (Xj ) in any given Xj to maximum value when partitioning the domain and therefore we have that [1, 1]. Since is bounded, by Hoeffdings Lemma (1963), it is sub-gaussian with variance proxy R2/4 and the log of its moment generating function is thus upper bounded by: log exp (αW ) α2R2 . As result, we have the following lower bound on the log-likelihood increase: L[f + α δf ] L[f ] α (cid:20) χ2(P Qf ) α (cid:21) . R2 8 (49) (50) As long as in the current round of boosting partitioning of the input space can be found that yields non-zero χ2(P Qf ), the likelihood is guaranteed to increase as long as we choose small enough"
        },
        {
            "title": "Preprint",
            "content": "R2 χ2(P Qf )(cid:1). In particular, choosing step size α = 4 step-size: α (cid:0)0, 8 an increase in log-likelihood of at least 2 (cid:2)χ2(P Qf )/R(cid:3)2 We note that this result, while insightful, assumes that one uses the exact probabilities under the model distribution in the NRGBoost update. In practice, we would only be able to approximately estimate these with MCMC. Furthermore, it can only guarantee an increase in the training log-likelihood as we also dont have access to the probabilities under the data generating distribution. We leave further analysis for future work. R2 χ2(P Qf ) produces ."
        },
        {
            "title": "B DENSITY ESTIMATION TREES",
            "content": "Density Estimation Trees (DET) (Ram & Gray, 2011) model the density function as piecewise constant function, q(x) = (cid:88) j=1 vj1Xj (x) , (51) where Xj are given by partitioning of the input space induced by binary tree and the vj are the density values associated with each leaf that, for the time being, we will only require to be such that q(x) sums to one. Ram & Gray (2011) proposes fitting DET models to directly minimize generative objective, the Integrated Squared Error (ISE) between the data generating distribution, p(x) and the model: min qQ (cid:88) xX (p(x) q(x))2 . Noting that is function as in Equation 51 and that (cid:83)J j=1 Xj = , we can rewrite this as min v1,...,vJ ,X1,...,XJ s.t. (cid:88) xX (cid:88) p2(x) + (cid:88) (cid:88) j=1 xXj (cid:88) vj = 1 . j=1 xXj (cid:0)v2 2vjp(x)(cid:1) (52) (53) Since the first term in the objective does not depend on the model this optimization problem can be further simplified as min v1,...,vJ ,X1,...,XJ s.t. (cid:88) j=1 (cid:88) j=1 (cid:0)v2 (Xj) 2vjP (Xj)(cid:1) vjV (Xj) = 1 , (54) where (X) denotes the volume of subset X. Solving this quadratic program for the vj we obtain the following optimal leaf values and objective: = (Xj) (Xj) , ISE (X1, . . . , XJ ) = (cid:88) j=1 2(Xj) Vf (Xj) . (55) One can therefore grow tree by greedily choosing to split parent leaf with support XP into two leaves with supports XL and XR so as to maximize the following criterion: max XL,XRP(XP ) 2(XL) (XL) + 2(XR) (XR) 2(XP ) (XP ) . MAXIMUM LIKELIHOOD To maximize the likelihood, max Exp log q(x) , (56) (57)"
        },
        {
            "title": "Preprint",
            "content": "rather than the ISE one can use the same approach. Here the optimization problem to solve is: max v1,...,vJ ,X1,...,XJ s.t. (cid:88) j=1 (cid:88) j=1 (Xj) log vj vjV (Xj) = 1 . (58) This is, again, easy to solve for vj since it is separable over after removing the constraint using Lagrange multipliers. The optimal leaf values and objective are in this case: = (Xj) (Xj) , (X1, . . . , XJ ) = (cid:88) j=1 (Xj) log (Xj) Vf (Xj) . The only change is therefore to the splitting criterion which should become: max XL,XRP(XP ) (XL) log (XL) (XL) + (XR) log (XR) (XR) (XP ) log (XP ) (XP ) . (59) (60) GREEDY TREE-BASED MULTIPLICATIVE BOOSTING In multiplicative generative boosting an unnormalized current density model, qt1(x), is updated at each boosting round by multiplication with new factor δqαt (x): qt(x) = qt1(x) δqαt (x) . (61) For our proposed NRGBoost, this factor is chosen in order to maximize local quadratic approximation of the log likelihood around qt1 as functional of the log density (see Section 3). The motivation behind the greedy approach of Tu (2007) or Grover & Ermon (2018) is to instead make the update factor δqt(x) proportional to the likelihood ratio rt(x) := p(x)/qt1(x) directly, which under ideal conditions would mean that the method converges immediately when choosing step size αt = 1. In more realistic setting, however, this method has been shown to converge under conditions on the performance of the individual δqt as discriminators between real and generated data (Tu, 2007; Grover & Ermon, 2018; Cranko & Nock, 2019). While in principle this desired rt(x) could be derived from any binary classifier that is trained to predict probability of datapoint being generated (e.g., by training it to minimize strictly proper loss) and Tu (2007) does not prescribe any particular choice, Grover & Ermon (2018) propose relying on the following variational bound of an -divergence to derive an estimator for this ratio: Df (P Qt1) sup uUt (cid:2)Exp u(x) Exqt1f (u(x))(cid:3) . (62) Here denotes the convex conjugate of . This bound is tight, with the optimum being achieved for (x) = (p(x)/qt1(x)), if Ut is capable of representing this function. (f )1 (u (x)) can thus be interpreted as an approximation of the desired rt(x). Adapting this method to use trees as weak learners can be accomplished by considering Ut in Equation 62 to be defined by tree functions = 1/J (cid:80)J j=1 wj1Xj with leaf values wj and leaf supports Xj. At each boosting iteration new tree, can thus be grown to greedily optimize the lower bound in the r.h.s. of Equation 62 and setting δqt(x) = (f )1 (u (x)) which is thus also tree with the same leaf supports and leaf values given by vj := (f )1 (wj). This leads to the seaprable optimization problem: max w1,...,wJ ,X1,...,XJ (cid:88) [P (Xj)wj Q(Xj)f (wj)] . (63) Note that we drop the iteration indices from this point onwards for brevity. Maximizing over wj with the Xj fixed we have that = (P (Xj )/Q(Xj )) which yields the optimal value (cid:19) (cid:88) (cid:20) (Xj)f (cid:18) (Xj) Q(Xj) Q(Xj)(f ) (cid:18) (Xj) Q(Xj) (cid:19)(cid:21) (64) (X1, . . . , Xj) ="
        },
        {
            "title": "Preprint",
            "content": "Table 6: Comparison of splitting criterion and leaf weights for the different versions of boosting. Splitting Criterion Leaf Values (Density) Greedy (KL) Greedy (χ2) NRGBoost log (P/Q) 2/Q 2/Q P/Q P/Q exp (P/Q 1) In turn, this determines the splitting criterion as function of the choice of . Finally, the optimal density values for the leaves are given by = (f )1 (w ) = (Xj) Q(Xj) . (65) It is interesting to note two particular choices of -divergences. For the KL divergence, (t) = log and (t) = 1 + log = (f )1 (t). This leads to JKL(X1, . . . , Xj) = (cid:88) (Xj) log (Xj) Q(Xj) (66) as the splitting criterion. The Pearson χ2 divergence, with (t) = (t 1)2, leads to the same splitting criterion as NRGBoost. Note however that for NRGBoost the leaf values for the multiplicative update of the density are proportional to exp (P (Xj )/Q(Xj )) instead of the ratio directly. Table 6 summarizes these results. Another interesting observation is that DET model can be interpreted as single round of greedy multiplicative boosting starting from uniform initial model. The choice of the ISE as the criterion to optimize the DET corresponds to the choice of Pearsons χ2 divergence and likelihood to the choice of KL divergence."
        },
        {
            "title": "D REPRODUCIBILITY",
            "content": "D.1 DATASETS We use 5 datasets from the UCI Machine Learning Repository (Dheeru & Karra Taniskidou, 2017): Abalone, Physicochemical Properties of Protein Tertiary Structure (from hereon referred to as Protein), Adult, MiniBooNE and Covertype. We also use the California Housing dataset which was downloaded through the Scikit-Learn package Pedregosa et al. (2011) and downsampled version of the MNIST dataset Deng (2012). Table 7 summarizes the main details of these datasets as well as the approximate number of samples used for train/validation/test for each cross-validation fold. Table 7: Dataset Information. We respect the original test sets of each dataset when provided, otherwise we set aside 20% of the original dataset as test set. 20% of the remaining data is set aside as validation set used for hyperparameter tuning. Abbr Name Train + Val Test Num Cat Target Cardinality Abalone California Housing Protein Adult AB CH PR AD MBNE MiniBooNE MNIST MNIST (downsampled) CT Covertype 3342 16512 36584 32560 104051 60000 835 4128 9146 16280 26013 10000 116202 7 8 9 6 50 196 10 1 0 0 8 0 0 2 29 Num Num Continuous Num Continuous Cat Cat Cat Cat 2 2 10 Original test set was respected. D.2 DEF AND NRGBOOST IMPLEMENTATION DETAILS Discretization In our practical implementation of tree based methods we first discretize the input space by binning continuous numerical variables by quantiles. Furthermore we also bin discrete"
        },
        {
            "title": "Preprint",
            "content": "numerical variables in order to keep their cardinalities smaller than 256. This can also be interpreted as establishing priori set of discrete values to consider when splitting on each numerical variable and is done for computational efficiency, being inspired by LightGBM (Ke et al., 2017). Categorical Splitting For splitting on categorical variable we once again take inspiration from LightGBM. Rather than relying on one-vs-all splits we found it better to first order the possible categorical values at leaf according to pre-defined sorting function and then choose the optimal many-vs-many split as if the variable was numerical. The function used to sort the values is the leaf value function. E.g., for splitting on categorical variable xi we order each possible categorical value by ˆP (xi=k,Xi)/ ˆQ(xi=k,Xi) where Xi denotes the leaf support over the remaining variables. Tree Growth Strategy We always grow trees in best first order, i.e., we always split the current leaf node that yields the maximum gain in the chosen objective value. Line Search As mentioned in Section 3, we perform line search to find the optimal step size after each round of boosting in order to maximize the likelihood gain in Equation 45. Because evaluating multiple possible step sizes, αt, is inexpensive, we simply do grid search over 101 different step sizes, split evenly in log space over [103, 10]. Code Our implementation of the proposed tree-based methods is mostly Python code using the NumPy library (Harris et al., 2020). We implement the tree evaluation and Gibbs sampling in C, making use of the PCG library (ONeill, 2014) for random number generation. Our code is available in the supplementary material and we will make both the DEF and NRGBoost methods available in an open-source library. D.3 HYPERPARAMETER TUNING D.3.1 XGBOOST To tune the hyperparameters of XGBoost we use 100 trials of random search with the search space defined in Table 8. Table 8: XGBoost hyperparameter tuning search space. δ(0) denotes point mass distribution at 0. Parameter learning_rate max_leaves min_child_weight reg_lambda reg_alpha max_leaves grow_policy tree_method Distribution or Value LogUniform (cid:0)(cid:2)103, 1.0(cid:3)(cid:1) Uniform ({16, 32, 64, 128, 256, 512, 1024}) LogUniform (cid:0)(cid:2)101, 103(cid:3)(cid:1) 0.5 δ(0) + 0.5 LogUniform (cid:0)(cid:2)103, 10(cid:3)(cid:1) 0.5 δ(0) + 0.5 LogUniform (cid:0)(cid:2)103, 10(cid:3)(cid:1) 0 (we already limit the number of leaves) lossguide hist Each model was trained for 1000 boosting rounds on regression and binary classification tasks. For multi-class classification tasks maximum number of 200 rounds of boosting was used due to the larger size of the datasets and because separate tree is built at every round for each class. The best model was selected based on the validation set, together with the boosting round where the best performance was attained. The test metrics reported correspond to the performance of the selected model at that boosting round on the test set. D.3.2 RFDE We implement the RFDE method (Wen & Hang, 2022) after quantile discretization of the dataset and therefore split at the midpoint of the discretized dimension instead of the original one. When leaf support has odd cardinality over the splitting dimension random choice is made over the two"
        },
        {
            "title": "Preprint",
            "content": "possible splitting values. Finally, the original paper does not mention how to split over categorical domains. We therefore choose to randomly split the possible categorical values for leaf evenly as we found that this yielded slightly better results than random one vs all split. For RFDE models we train total of 1000 trees. The only hyperparameter that we tune is the maximum number of leaves per tree for which we test the values [26, 27, . . . , 214]. For the Adult dataset, due to limitations of our tree evaluation implementation we only test values up to 213. D.3.3 ARF For ARF we used the official python implementation of the algorithm (Blesch & Wright, 2023). We implement the single variable inference for this method ourselves since the python library does not provide density evaluation. Due to memory (and time) concerns we train models with maximum of 100 trees for up to 10 adversarial iterations. We use similar grid-search setup to DEF, tuning only the following two parameters: The maximum number of leaves per tree, for which we test the values [26, 27, . . . , 212, ] ( denotes unconstrained). The minimum number of examples falling on each leaf, for which we test the values [3, 5, 10, 30, 50, 100]. If one value of this parameter doesnt improve upon the previous, we move on to the next value of the maximum number of leaves (i.e., we exit this loop early). D.3.4 DEF We train ensembles with 1000 DET models. Only three hyperparameters are tuned, using three nested loops, each loop running over the possible values of single parameter in pre-defined order. These are, in order of outermost to innermost: The maximum number of the values [16384, 4096, 1024, 256]. If the best model for one value of the maximum number of leaves (over the remaining inner-loop parameters) doesnt improve upon the previous, we stop the hyperparameter tuning. This is done because in most datasets, reducing the number of leaves always leads to worse model for DEF. for which we leaves per tree test The fraction of features to consider when splitting node. We test the values [d1/2, d1/4, 1] with being the dimension of the dataset. If the best model for one value of this parameter doesnt improve upon the best model for the previous value, we move on to the next value of the maximum number of leaves (i.e., we exit this loop early). The minimum number of data points that need to be left in each leaf for split to be considered. We test the values [0, 1, 3, 10, 30]. If one value of this parameter doesnt improve upon the previous, we move on to the next value of fraction of features. D.3.5 NRGBOOST We train NRGBoost models for maximum of 200 rounds of boosting. We only tune two parameters for NRGBoost Models: The maximum number of leaves for which we try the values [64, 256, 1024, 4096] in order, stopping if performance fails to improve from one value to the next. For the CT dataset we also include 16384 in the values to test. The constant factor by which the optimal step size determined by the line search is shrunk at each round of boosting. This is essentially the \"learning rate\" parameter. To tune it we perform Golden-section search for the log of its value using total of 6 evaluations. The range we use is [0.01, 0.5]. For regularization we limit the ratio between empirical data density and model data density on each leaf to maximum of 2. We noticed that smaller values of this regularization parameter tend to work better but that it otherwise plays similar role to the shrinkage factor so we opt to tune only the latter. For sampling we use sample pool of 80000 samples for all datasets except for Covertype where we use 320000. These values are chosen so that the number of samples in the pool are at minimum"
        },
        {
            "title": "Preprint",
            "content": "similar to the training set size which we found to be good rule of thumb. At each round of boosting, samples are removed from the pool according to our rejection sampling approach (using Equation 12) after independently removing 10% at random. These samples are replaced by samples from the current model using Gibbs sampling. The starting point of each NRGBoost model was selected as mixture model between uniform distribution (10%) and the product of training marginals (90%) on the discretized input space. We observed that this mixture coefficient does not have much impact on the results however. D.3.6 TVAE We use the implementation of TVAE from the SDV package.2 To tune its hyperparameters we use 50 trials of random search with the search spaces defined in Table 9. Table 9: TVAE hyperparameter tuning search space. We set both compress_dims and decompress_dims to have the number of layers specified by num_layers, with hidden_dim hidden units in each layer. We use larger batch sizes and smaller number of epochs for the larger datasets (MBNE, MNIST, CT) since these can take signifcantly longer to run single epoch. Parameter epochs batch_size embedding_dim hidden_dim num_layers compress_dims decompress_dims Datasets Distribution or Value small large small large all all all all all Uniform ([100..500]) Uniform ([50..200]) Uniform ({100, 200, . . . , 500}) Uniform ({500, 1000, . . . , 2500}) Uniform ({32, 64, 128, 256, 512}) Uniform ({32, 64, 128, 256, 512}) Uniform ({1, 2, 3}) (hidden_dim,) * num_layers (hidden_dim,) * num_layers D.3.7 TABDDPM We used the official implementation of TabDDPM3 adapted to use our datasets and validation setup. To tune the hyperparameters of TabDDPM we use 50 trials of random search with the same search space that the original authors report in their paper (Kotelnikov et al., 2023). D.3.8 FOREST-FLOW We use the official implementation of Forest-Flow.4 The only hyperparameters with impact on performance that the Forest-Flow documentation recommends tuning are the number of diffusion timesteps for which recommended default value of 50 is offered and the number of noise values per sample for which the default value is 100. Due to the fact that Forest-Flow models already take significantly longer to train than the other models, and that that increasing either value would lead to slower training we opted to use these default values. D.4 EVALUATION SETUP Single variable inference For the single variable inference evaluation, the best models are selected by their discriminative performance on validation set. The entire setup is repeated five times with different cross-validation folds and with different seeds for all sources of randomness. For the Adult and MNIST datasets the test set is fixed but training and validation splits are still rotated. 2https://github.com/sdv-dev/SDV 3https://github.com/yandex-research/tab-ddpm 4https://github.com/SamsungSAILMontreal/ForestDiffusion"
        },
        {
            "title": "Preprint",
            "content": "Sampling For the sampling evaluation we use single train/validation/test split of the real data (corresponding to the first fold in the previous setup) for training the generative models. The density models used are those previously selected based on their single variable inference performance on the validation set. For TVAE and TabDDPM we directly evaluate their ML Efficiency using the validation data. ML Efficiency For each selected model we sample train and validation sets with the same number of samples as those used on the original data. For NRGBoost we generate these samples by running 64 chains in parallel with 100 steps of burn in and downsampling their outputs by 30 (for the smaller datasets) or 10 (for MBNE, MNIST and CT). For every synthetic dataset, an XGBoost model is trained using the best hyperparameters found on the real data and using synthetic validation set to select the best stopping round for XGBoost. The setup is repeated 5 times with different datasets being generated for each method and dataset. Discriminator Measure We create the training, validation and test sets to train an XGBoost model to discriminate between real and generated data using the following process: The original validation set is used as the real part of the training set in order to avoid benefiting generative methods that overfit their training set. The original test set is split 20%/80%. The 20% portion is used as the real part of the validation set and the 80% portion as the real part of the test set. To form the synthetic part of the training, validation and test sets for the smaller datasets (AB, CH, PR, AD) we sample data according to the original number of samples in the train, validation and test splits on the real data respectively. Note that this makes the ratio of real to synthetic data 1:4 in the training set. This is deliberate because for these smaller datasets the original validation has few samples and adding extra synthetic data helps the discriminator. For the larger datasets (MBNE, MNIST, CT) we generate the same number of synthetic samples as there are real samples on each split, therefore making every ratio 1:1 because the discriminator is typically already too powerful without adding additional synthetic data. Because, in contrast to the previous metric, having lower number of effective samples helps rather than hurts we take extra precautions to not generate correlated data with NRGBoost. We draw each sample by running its own independent chain for 100 steps starting from an independent sample from the initial model which is rather slow process. The setup is repeated 5 times with 5 different sets of generated samples from each method. Statistical Distance Measure We follow procedure similar to Jolicoeur-Martineau et al. (2024). We min-max scale the numerical variables, and one-hot encode the categorical variables and scale them by 1/2. We use L1 distance in the formulation of the optimal transport problem and use the python POT library to solve it. We sub-sample maximum of 5000 samples from the original train or test sets and use an equal number of synthetic samples, repeating each experiment 5 times. D.5 COMPUTATIONAL RESOURCES The experiments were run on machine equipped with an AMD Ryzen 7 7700X 8 core CPU and 32 GB of RAM. The comparisons with TVAE and TabDDPM further made use of GeForce RTX 3060 GPU with 12 GB of VRAM."
        },
        {
            "title": "E ADDITIONAL RESULTS",
            "content": "E.1 COMPUTATIONAL EFFORT In Figure 4 we show the training times for NRGBoost as well as the other methods for the best model selected by hyperparameter tuning. We do not report the training times for the RFDE method because it is virtually free when compared to the other methods since the splitting process is random and depends only on the input domain. The data itself is only required for computing leaf probabilities which is inexpensive."
        },
        {
            "title": "Preprint",
            "content": "Figure 4: Wall time required to train the best model for each method. Figure 5: Downsampled MNIST samples generated by Gibbs sampling from NRGBoost model. Each row corresponds to an independent chain initialized with sample from the initial model f0 (first column). Each column represents consecutive sample from the chain. Note that the biggest computational cost for training NRGBoost model is the Gibbs sampling (accounting for roughly 70% of the training time on average). This could potentially be improved by leveraging higher parallelism than what we used in the experiments (16 virtual cores). We note also that we believe there is still plenty of margin for optimizing the tree-fitting code used for DEF models. As such, the results presented are merely indicative. The computational effort for fitting tree-based model should scale linearly both in the number of samples and the number of features and this is the main source of variation for the training times between datasets. However, we note that larger datasets can benefit more from larger models (e.g., with larger number of leaves) which are also slower to train. E.2 MCMC CHAIN CONVERGENCE ON MNIST In Figure E.2 we show the convergence of Gibbs sampler sampling from NRGBoost model. In only few samples each chain appears to have converged to the data manifold after starting at random sample from the initial model (a mixture between the product of training marginals and uniform). Note how consecutive samples are autocorrelated. In particular it can be rare for chain to switch between two different modes of the distribution (e.g., switching digits) even though few such transitions can be observed."
        }
    ],
    "affiliations": [
        "Feedzai"
    ]
}