{
    "paper_title": "Rethinking the Value of Agent-Generated Tests for LLM-Based Software Engineering Agents",
    "authors": [
        "Zhi Chen",
        "Zhensu Sun",
        "Yuling Shi",
        "Chao Peng",
        "Xiaodong Gu",
        "David Lo",
        "Lingxiao Jiang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Model (LLM) code agents increasingly resolve repository-level issues by iteratively editing code, invoking tools, and validating candidate patches. In these workflows, agents often write tests on the fly, a paradigm adopted by many high-ranking agents on the SWE-bench leaderboard. However, we observe that GPT-5.2, which writes almost no new tests, can even achieve performance comparable to top-ranking agents. This raises the critical question: whether such tests meaningfully improve issue resolution or merely mimic human testing practices while consuming a substantial interaction budget. To reveal the impact of agent-written tests, we present an empirical study that analyzes agent trajectories across six state-of-the-art LLMs on SWE-bench Verified. Our results show that while test writing is commonly adopted, but resolved and unresolved tasks within the same model exhibit similar test-writing frequencies Furthermore, these tests typically serve as observational feedback channels, where agents prefer value-revealing print statements significantly more than formal assertion-based checks. Based on these insights, we perform a controlled experiment by revising the prompts of four agents to either increase or reduce test writing. The results suggest that changes in the volume of agent-written tests do not significantly change final outcomes. Taken together, our study reveals that current test-writing practices may provide marginal utility in autonomous software engineering tasks."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 8 ] . [ 1 0 0 9 7 0 . 2 0 6 2 : r Rethinking the Value of Agent-Generated Tests for LLM-Based Software Engineering Agents ZHI CHEN, Singapore Management University, Singapore ZHENSU SUN, Singapore Management University, Singapore YULING SHI, Shanghai Jiao Tong University, China CHAO PENG, ByteDance, China XIAODONG GU, Shanghai Jiao Tong University, China DAVID LO, Singapore Management University, Singapore LINGXIAO JIANG, Singapore Management University, Singapore Large Language Model (LLM) code agents increasingly resolve repository-level issues by iteratively editing code, invoking tools, and validating candidate patches. In these workflows, agents often write tests on the fly, paradigm adopted by many high-ranking agents on the SWE-bench leaderboard. However, we observe that GPT-5.2, which writes almost no new tests, can even achieve performance comparable to top-ranking agents. This raises the critical question: whether such tests meaningfully improve issue resolution or merely mimic human testing practices while consuming substantial interaction budget. To reveal the impact of agent-written tests, we present an empirical study that analyzes agent trajectories across six state-of-the-art LLMs on SWE-bench Verified. Our results show that while test writing is commonly adopted, but resolved and unresolved tasks within the same model exhibit similar test-writing frequencies Furthermore, these tests typically serve as observational feedback channels, where agents prefer valuerevealing print statements significantly more than formal assertion-based checks. Based on these insights, we perform controlled experiment by revising the prompts of four agents to either increase or reduce test writing. The results suggest that changes in the volume of agent-written tests do not significantly change final outcomes. Taken together, our study reveals that current test-writing practices may provide marginal utility in autonomous software engineering tasks. Additional Key Words and Phrases: Large Language Model, Agent-Written Tests, Agent Trajectory Analysis, Software Development Agent ACM Reference Format: Zhi Chen, Zhensu Sun, Yuling Shi, Chao Peng, Xiaodong Gu, David Lo, and Lingxiao Jiang. 2026. Rethinking the Value of Agent-Generated Tests for LLM-Based Software Engineering Agents. In Proceedings of Make sure to enter the correct conference title from your rights confirmation email (Conference acronym XX). ACM, New York, NY, USA, 21 pages. https://doi.org/XXXXXXX.XXXXXXX Corresponding author. Authors Contact Information: Zhi Chen, Singapore Management University, Singapore, Singapore, zhi.chen.2023@smu. edu.sg; Zhensu Sun, Singapore Management University, Singapore, Singapore, zssun@smu.edu.sg; Yuling Shi, Shanghai Jiao Tong University, Shanghai, China, yuling.shi@sjtu.edu.cn; Chao Peng, ByteDance, Beijing, China, chao.peng@acm.org; Xiaodong Gu, Shanghai Jiao Tong University, Shanghai, China, xiaodong.gu@sjtu.edu.cn; David Lo, Singapore Management University, Singapore, Singapore, davidlo@smu.edu.sg; Lingxiao Jiang, Singapore Management University, Singapore, Singapore, lxjiang@smu.edu.sg. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. Conference acronym XX, Woodstock, NY 2026 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/2018/06 https://doi.org/XXXXXXX.XXXXXXX , Vol. 1, No. 1, Article . Publication date: February 2026. 2 Zhi Chen, Zhensu Sun, Yuling Shi, Chao Peng, Xiaodong Gu, David Lo, and Lingxiao Jiang Fig. 1. Overview of our study design. RQ1 profiles emergent testing behaviours (test writing frequency, timing, and execution analysis). RQ2 characterizes the feedback signals encoded in agent-written tests (assertions vs. value-revealing prints) and the types of assertions. RQ3 applies prompt interventions to encourage or discourage writing tests, and measures both outcome impact and efficiency impact."
        },
        {
            "title": "1 Introduction\nCode agents are increasingly used as effective paradigm for resolving software issues, where Large\nLanguage Models (LLMs) [5, 10, 32] are integrated into scaffold of tools and interaction protocols\nto edit real repositories, invoke tools, and attempt to resolve issues end-to-end [11, 13, 16, 33, 34,\n44, 45, 47]. In this paper, a code agent denotes an LLM coupled with external tools and an iterative\nactionâ€“observation loop, and the scaffold refers to the surrounding tool interface and interaction\nprotocol that specifies the agentâ€™s allowed actions and feedback. Among the diverse skills required\nby code agents, testing plays a critical role, which expose regressions, validate hypotheses, and\nprovide a feedback loop during patch development [23, 29, 45, 54].",
            "content": "When operating on repository-level tasks, agents typically use tests as primary validation interface, which come from two main sources. The first is the repositorys existing, human-written test suite, which reflects developer intent and established project conventions [6, 18]. The second is agent-written testsnew test artifacts written by the agent during problem solving that were not present in the original codebase. In contrast to curated human-written tests, agent-written tests are written on the fly during issue resolution, and their reliability depends on the models understanding of the specification, domain knowledge, and the semantics of the target codebase. Agent-written tests can be beneficial by surfacing edge cases and providing actionable feedback for fault localization and patch refinement. However, they can also be harmful if they embed incorrect assumptions or oracles, diverting effort toward satisfying the test rather than resolving the target issue. Moreover, test generation and execution introduce non-trivial overheadconsuming API calls and tokens and increasing context footprintwhich can reduce the remaining budget available for core debugging and patching [20]. When the resulting signals are low-value, this overhead may dilute the agents focus and become net detrimental. To better understand agent-written tests, we conduct preliminary quantitative analysis of agent trajectories on SWE-bench Verified [31] using mini-SWE-agent [40], where testing is optional and not enforced by any hard-coded procedure. We find that agent-written testing is prevalent for several strong models. For example, claude-opus-4.5 (ranked #1 in this setting, 74.4% resolution) , Vol. 1, No. 1, Article . Publication date: February 2026. Rethinking the Value of Agent-Generated Tests for LLM-Based Software Engineering Agents 3 generates at least one new test artifact in about 83% of tasks. Surprisingly, we observe pronounced contrast for gpt-5.2: it achieves comparable resolution rate (71.8%), only 2.6 percentage points below claude-opus-4.5, while generating near-zero new tests (only in 0.6% of tasks). This interesting observation inspires us to core question: Do agent-written tests truly facilitate task resolution, or do models merely mimic learned software development practice while the resulting tests contribute little to the final patch? If the latter holds, then the widespread creation and execution of agent-written tests may represent significant resource waste, consuming significant interaction budget without meaningful gains in task success. Therefore, we argue that systematic empirical study is needed to understand the role of agent-written tests in resolving software issues. Prior work mostly evaluates and benchmarks LLM-generated tests under predefined testing objectives and fixed quality metrics (e.g., unit tests, assertions, or issue-reproducing tests), typically with respect to fixed target program or snapshot of code under test [24, 30, 38, 42, 43, 51, 53]. However, in complex real-world GitHub issue resolution [18, 19], the codebase and candidate patches evolve over time, and test writing and usage arise dynamically as self-directed behaviors rather than pre-specified evaluation objectives. Yet the intrinsic tendency of high-autonomy agents to write and use tests during such issue resolutionand the impact of such agent-written tests on resolution outcomeshas not been systematically studied. This motivates closer empirical investigation of agent-written tests, guided by the following three research questions. Research questions and overview. Guided by the above gap, we conduct systematic empirical study of agent-written tests in GitHub issue resolution. Figure 1 summarizes our study design, which decomposes the problem into three complementary research questions. RQ1 characterizes the agents testing behaviors when test writing is required by the prompt: whether agents write tests, when they introduce them, and how intensively they execute them. RQ2 shifts from behaviors to test content, investigating what feedback signals agent-written tests actually emit at execution time (assertions vs. value-revealing prints) and what types of assertions agents use. RQ3 evaluates impact: by revising prompts to encourage or discourage writing new tests, we measure whether changing test-writing behavior meaningfully alters task resolution outcomes and what efficiency costs (API calls and tokens) these changes incur. Summary of findings. Across models, agent-written testing is best understood as modeldependent process style rather than dependable driver of success. First, RQ1 shows that test writing is widespread behavior among the studied models, while resolved and unresolved trajectories within model exhibit broadly similar test-writing rates. When agent-written tests exist, unsuccessful trajectories tend to spread test writing slightly more over the run and execute tests more frequently. Second, RQ2 reveals that agent-written tests primarily function as an observational feedback channel: value-revealing prints consistently dominate assert-based checks, and assertion forms concentrate on local-property and exact-value checks, with relational/range-style constraints remaining rare. Third, RQ3 provides controlled evidence that large shifts in whether agents write tests translate into only small shifts in task outcomes for most tasks, while efficiency effects can be substantialinducing tests can increase interaction overhead without improving resolution, whereas suppressing tests can materially reduce API calls and token usage with only modest success losses. Contributions. The contribution of this work can be summarized as: behavioral analysis of agent-written tests from code agents. We characterize the agentwritten testing behaviors of base LLM agents, including whether they create new test artifacts, when such test creation occurs within trajectory, and how these tests are executed. Our results show that test writing and execution intensity are largely model-dependent process styles and only , Vol. 1, No. 1, Article . Publication date: February 2026. 4 Zhi Chen, Zhensu Sun, Yuling Shi, Chao Peng, Xiaodong Gu, David Lo, and Lingxiao Jiang weakly align with task success (e.g., some high-performing models resolve many tasks while writing almost no tests). feedback-signal analysis of agent-written tests with four-category assertion categorization. We separate verification-oriented assertions from observational outputs and introduce rule-based AST classifier that maps assertions into four assertion categories. We find that tests largely serve an observational role: value-revealing prints consistently outnumber assertions; assertion usage is dominated by local-property and exact-value checks, whereas relational/rangestyle constraints are uncommon. causal evaluation of agent-written tests on task resolution. Through controlled prompt interventions that either encourage or suppress writing new test files, we quantify the causal effects of agent-written tests on task success and interaction efficiency. We demonstrate that large flips in test-writing status translate into only small changes in resolution outcomes for most tasks, whereas efficiency effects can be substantialinducing tests can increase token and interaction overhead without improving success, while suppressing tests yields large cost reductions with only modest success drops. Paper organization. Section 2 describes our study setting, data collection, and measurement procedures. Section 3 characterizes emergent agent-written testing behaviors as process events whether agents write tests, when they appear, and how intensively they are executed (RQ1). Section 4 shifts to test content by analyzing what feedback signals agent-written tests produce at execution time (assertions vs. value-revealing prints) and what types of assertions they use (RQ2). Section 5 evaluates the outcome and efficiency impacts of agent-written tests via prompt interventions that induce or suppress test writing (RQ3). Section 6 discusses implications, limitations, and future directions. Section 7 reviews related work, and Section 8 concludes the paper."
        },
        {
            "title": "2 Methodology\nIn this section, we introduce the methodology for this study, including the benchmark, studied\nagent and LLMs, the extraction of agent-written tests, and implementation details. Our study is\nguided by three research questions:\nâ€¢ RQ1: What Testing Behaviors Emerge Under a Light Agent Scaffold?\nâ€¢ RQ2: What Feedback Signals Do Agent-Written Tests Provide, and What Types of Assertions Do",
            "content": "They Use? RQ3: Do Agent-Written Tests Truly Affect Task Resolution?"
        },
        {
            "title": "2.2 Agent and its LLMs\nWhile many recent LLM-based agents incorporate curated testing components, such as specialized\nvalidation modules, dedicated test-planning stages, or multi-agent coordination [8, 23, 36, 54], these\nframeworks can confound a modelâ€™s intrinsic tendencies with scaffold-induced constraints. To\nbetter isolate base-model behavior, we adopt mini-SWE-agent [40, 41]. It provides a lightweight\nagent work loop restricted to a standard bash interface: the agent interacts with the repository\nsolely through the bash tool, executing commands in a bash shell (e.g., running python) and using",
            "content": ", Vol. 1, No. 1, Article . Publication date: February 2026. Rethinking the Value of Agent-Generated Tests for LLM-Based Software Engineering Agents 5 standard command-line utilities to inspect and modify files. The model can create executable Python test files on the fly and run them via bash as part of its workflow. Crucially, mini-SWE-agent does not provide additional testing-specific functions or dedicated testing tools (e.g., test planners or structured testing modules), so the testing decisions (whether, when, and how) are left to the model. Accordingly, any observed behaviors (e.g., creating or running test artifacts) can be interpreted as model-native ones. We select diverse set of strong LLMs to capture heterogeneous agent-written testing behaviors under mini-SWE-agent. Specifically, we reference the SWE-bench Bash Only leaderboard1 as of the time of writing (2025-12-11) and identify the top-six model families (rather than top entries, since family may appear with multiple variants on the leaderboard). For each family, we use its highest-ranked model as the representative: claude-opus-4.5 [1] (74.4%), gemini-3-pro-preview [14] (74.2%), gpt-5.2 [32] (71.8%), kimi-k2-thinking [28] (63.4%), minimax-m2 [26] (61.0%), and deepseekv3.2-reasoner [9] (60.0%)."
        },
        {
            "title": "3 RQ1: What Testing Behaviors Emerge Under a Light Agent Scaffold?",
            "content": "Motivation. In high-autonomy setting where testing is optional, agents may or may not write tests during issue resolution. RQ1 establishes descriptive baseline of these emergent testing behaviorswhat tests agents write, when they introduce them, and how intensively they run them. This baseline (i) clarifies what \"testing\" looks like in this setting and (ii) provides grounded behavioral variables for later research questions. Experiment Design. RQ1 uses resolved vs. unresolved trajectories as comparative lens to characterize systematic differences in test-related behaviors. We emphasize that these outcomestratified comparisons are not intended to establish causality regarding task success; rather, they serve as diagnostic tool to surface consistent differences in testing practices between successful and unsuccessful problem-solving processes. RQ1 reports descriptive summaries of three complementary aspects of test-oriented behavior: Frequency (RQ1.1): whether the agent writes tests, and how many. 1https://www.swebench.com/ , Vol. 1, No. 1, Article . Publication date: February 2026. 6 Zhi Chen, Zhensu Sun, Yuling Shi, Chao Peng, Xiaodong Gu, David Lo, and Lingxiao Jiang Timing (RQ1.2): when test writing happens during issue resolution. Execution (RQ1.3): how intensively tests are run, and their outcomes."
        },
        {
            "title": "3.1 RQ1.1 Frequency: Do Agents Write Test Artifacts?",
            "content": "Goal and measurements. We examine whether base LLMs write test artifacts under light scaffold. For each task, we record (i) whether the agent writes at least one test artifact, and (ii) if so, how many distinct test artifacts it writes. We report results separately for resolved and unresolved tasks. Table 1. Per-model test writing rate by execution outcome Model #Tasks Tasks w/ tests (count, %) Mean #tests #Tasks Resolved Unresolved Tasks w/ tests (count, %) All Mean #tests #Tasks Tasks w/ tests (count, %) Mean #tests claude-opus-4-5 gemini-3-pro-preview gpt-5.2 kimi-k2-thinking minimax-m2 deepseek-v3.2-reasoner 372 371 359 317 305 300 314 (84.4%) 235 (63.3%) 3 (0.8%) 309 (97.5%) 302 (99.0%) 277 (92.3%) 3.33 2.02 1.00 3.48 4.82 3.55 128 129 141 183 195 200 101 (78.9%) 73 (56.6%) 0 (0.0%) 178 (97.3%) 191 (97.9%) 169 (84.5%) 4.12 2.16 3.83 5.76 4.08 500 500 500 500 500 500 415 (83.0%) 308 (61.6%) 3 (0.6%) 487 (97.4%) 493 (98.6%) 446 (89.2%) 3.52 2.05 1.00 3.61 5.19 3.75 Notes. Tasks w/ tests (count, %) reports the number (and percentage) of tasks that write at least one test artifact within each outcome split. Mean #tests reports the mean number of distinct test artifacts, computed only over tasks that write at least one test artifact. Results. Table 1 shows that writing tests is common for most models, but not for gpt-5.2. Some models write tests in almost every task (e.g., minimax-m2 and kimi-k2-thinking). In contrast, gpt-5.2 almost never writes tests (3/500 tasks). Within the same model, resolved and unresolved tasks usually have similar test-writing rates. When tests are written, unresolved tasks often write as many or more distinct test artifacts than resolved tasks. This may reflect that harder tasks trigger more trial-and-error. RQ1.1 Test Writing: Key Pattern Test writing is widespread across models in our high-autonomy issue-resolution setting. Most models write tests in majority of tasks (61.698.6%), and four of six do so in at least 83% of tasks; gpt-5.2 is clear outlier with near-zero test writing (0.6%)."
        },
        {
            "title": "3.2 RQ1.2 Timing: When Are Tests Written During the Run?",
            "content": "Goal and measurements. Beyond whether tests are written (RQ1.1), we examine when test writing happens during task execution. Writing tests in tight window may look like short \"checking phase\", while writing tests throughout the task may look like iterative debugging. This subsection is descriptive and does not claim effectiveness. We analyze only tasks that write at least one test artifact, so the timing metrics are defined. Because gpt-5.2 writes tests in only 3 tasks (RQ1.1), we omit its per-model timing summaries in RQ1.2 to avoid unstable estimates. We also exclude it from later analyses that require tasks with test writing. We use three normalized positions within the task: the first test-writing position, the last test-writing position, and their span: ğ‘¡first = min(ğ‘†write) ğ‘steps , ğ‘¡last = max(ğ‘†write) ğ‘steps ğ‘ write = ğ‘¡last ğ‘¡first = max(ğ‘†write) min(ğ‘†write) ğ‘steps , Vol. 1, No. 1, Article . Publication date: February 2026. Rethinking the Value of Agent-Generated Tests for LLM-Based Software Engineering Agents 7 Here, ğ‘†write is the set of step indices where the agent writes test artifacts, and ğ‘steps is the total number of interaction steps in the task. ğ‘¡first and ğ‘¡last are normalized positions in [0, 1]. Smaller values mean the agent writes tests earlier in the task; larger values mean later. The span ğ‘ write [0, 1] measures how spread out test writing is across the task. Larger values mean more dispersed test writing; smaller values mean more concentrated window. Table 2. Per-model timing of test writing events Resolved Unresolved Model #Tasks w/ tests First test-writing position Last test-writing position Test-writing span #Tasks w/ tests First test-writing position Last test-writing position Test-writing span claude-opus-4-5 gemini-3-pro-preview kimi-k2-thinking minimax-m2 deepseek-v3.2-reasoner All models 314 235 309 302 277 1440 0.34 0.53 0.40 0.35 0.43 0. 0.75 0.67 0.82 0.86 0.80 0.78 0.41 0.14 0.42 0.51 0.37 0.38 101 73 178 191 169 0.30 0.55 0.40 0.29 0.40 0.37 0.78 0.70 0.82 0.85 0.80 0.80 0.48 0.15 0.42 0.56 0.40 0. Notes. Values are macro-over-tasks means. First test-writing position and Last test-writing position are the normalized positions of the first and last test-writing events relative to total_steps. Test-writing span is the normalized distance between the first and last test-writing positions. Results. Table 2 summarizes test-writing positions for tasks that write tests. Across all models, the average first test-writing position is 0.40 for resolved tasks and 0.37 for unresolved tasks. The average last test-writing position is 0.78 (resolved) and 0.80 (unresolved). Models differ in when they start writing tests. For example, gemini-3-pro-preview starts later (0.530.55), while minimax-m2 and claude-opus-4-5 start earlier (0.290.35). Most models finish test writing late in the task (last position around 0.750.86). Models also differ in how spread out test writing is. gemini-3-pro-preview has short span (0.140.15). minimax-m2 has wider span (0.510.56), and claude-opus-4-5 is also relatively wide (0.410.48). kimi-k2-thinking is almost identical between resolved and unresolved tasks (0.400.82; span 0.42). Overall, unresolved tasks have slightly larger average span than resolved tasks (0.43 vs. 0.38). RQ1.2 Test-Writing Timing: Key Pattern Test writing typically finishes late, but its start time and span are mainly model-dependent; unresolved tasks are only slightly more spread out (0.43 vs. 0.38)."
        },
        {
            "title": "3.3 RQ1.3 Execution: How Intensively Are Agent-Written Tests Executed, and With",
            "content": "What Process Outcomes? Goal and measurements. RQ1.3 describes how agents execute tests after they have written them. We measure (i) how often tests are executed, (ii) how often they are rerun relative to the number of written test artifacts, and (iii) how often executions fail at the process level. We treat an execution as failed if it ends with non-zero return code (and successful otherwise). This captures execution friction during interaction with the environment, not patch correctness. For each task ğ‘¡, let ğ¸ğ‘¡ be the number of test executions, ğ´ğ‘¡ the number of agent-written test artifacts, and ğ¹ğ‘¡ the number of executions with non-zero return codes. We report three task-level metrics: ExecCount (ğ¸ğ‘¡ ), test executions per task; ExecPerTest (ğ¸ğ‘¡ /ğ´ğ‘¡ ), executions per written test artifact (rerun intensity); and FailRate (ğ¹ğ‘¡ /ğ¸ğ‘¡ ), the fraction of executions that fail. We report macro-over-tasks means for each metric. , Vol. 1, No. 1, Article . Publication date: February 2026. 8 Zhi Chen, Zhensu Sun, Yuling Shi, Chao Peng, Xiaodong Gu, David Lo, and Lingxiao Jiang Table 3. Task-level execution effort and process-level outcomes of agent-written tests"
        },
        {
            "title": "Model",
            "content": "#Tasks w/ tests"
        },
        {
            "title": "Mean\nExecPerTest",
            "content": "claude-opus-4-5 gemini-3-pro-preview kimi-k2-thinking minimax-m2 deepseek-v3.2-reasoner"
        },
        {
            "title": "All models",
            "content": "314 235 309 302 277 1440 4.87 2.71 5.39 7.19 3.74 4.89 1.50 1.51 1.62 1.55 1.11 1. Mean FailRate (%) 11.97 8.53 24.95 24.11 27.37 19.68 #Tasks w/ tests"
        },
        {
            "title": "Mean\nExecPerTest",
            "content": "101 73 178 191 169 712 6.27 2.79 6.54 9.70 4.66 6.52 1.68 1.40 1.76 2.09 1.32 1. Mean FailRate (%) 11.14 7.08 21.05 24.10 29.55 21.05 Notes. #Tasks: tasks with test writing. Mean ExecCount: test executions per task. Mean ExecPerTest: executions per agent-written test artifact. Mean FailRate: % executions with non-zero return codes (macro-over-tasks). Results. Table 3 summarizes execution effort and process-level outcomes for tasks that write tests. Across all models, unresolved tasks execute tests more often than resolved tasks (Mean ExecCount: 6.52 vs. 4.89). They also rerun tests more per written test artifact (Mean ExecPerTest: 1.70 vs. 1.46). FailRate is slightly higher for unresolved tasks (21.05% vs. 19.68%). Models differ strongly in execution intensity. gemini-3-pro-preview runs tests the least (ExecCount 2.72.8). minimax-m2 runs tests the most, especially for unresolved tasks (ExecCount 9.70; ExecPerTest 2.09). FailRate also varies by model. claude-opus-4-5 and gemini-3-pro-preview have lower FailRate (about 712%), while deepseek-v3.2-reasoner, kimi-k2-thinking, and minimax-m2 are higher (about 2130%). RQ1.3 Test Execution: Key Pattern For tasks that write tests, unresolved tasks run tests more often and rerun them more per written test artifact (ExecCount 6.52 vs. 4.89; ExecPerTest 1.70 vs. 1.46), while process-level execution failures vary mainly by model (FailRate 7%30%)."
        },
        {
            "title": "4 RQ2: What Feedback Signals Do Agent-Written Tests Provide, and What Types of",
            "content": "Assertions Do They Use? Motivation. In our high-autonomy setting where testing is optional, tests may play different roles depending on the feedback they emit when executed. RQ1 treats tests as events in the trajectorywhether agents write them, when they appear, and how often they are run. RQ2 shifts to the content of those tests: the feedback they produce during execution. We capture this feedback , Vol. 1, No. 1, Article . Publication date: February 2026. Rethinking the Value of Agent-Generated Tests for LLM-Based Software Engineering Agents 9 through two common signals in agent-written tests: assertions (which fail when conditions are violated) and value-revealing prints (which expose runtime values). This view clarifies what agents use tests for when resolving GitHub issues. Experiment Design. RQ2 conditions on tasks that write at least one test artifact and reports descriptive summaries of two aspects of test feedback: Signal counts (RQ2.1): how many feedback statements appear in agent-written tests, split into assertions vs. value-revealing prints. Assertion types (RQ2.2): what types of assertions appear in agent-written tests, using four-type categorization."
        },
        {
            "title": "4.1 RQ2.1 Task-level feedback signal amount: How much feedback do agent-written",
            "content": "tests encode? Goal and measurements. Conditioning on tasks that contain agent-written test artifact, we quantify how many feedback statements are encoded in those artifacts. We distinguish two signal types: (i) verification signals (ğ´), i.e., assert statements that specify explicit checks, and (ii) observational signals (ğ‘ƒ), i.e., value-revealing print statements that expose runtime values or computed expressions. To ensure ğ‘ƒ (prints) reflects observational feedback, we exclude pure-literal prints that emit only fixed strings (e.g., print(\"here\")) and count only prints that expose runtime values, expressions, or execution results (e.g., print(obj.attr)). For each task ğ‘¡ with test artifacts Ağ‘¡ , and for signal type ğ‘† {ğ´, ğ‘ƒ }, let ğ‘›ğ‘† ğ‘¡,ğ‘ be the number of signal statements of type ğ‘† in artifact ğ‘ Ağ‘¡ . We define the task-level signal totals: ğ‘ ğ‘† ğ‘¡ = ğ‘›ğ‘† ğ‘¡,ğ‘, ğ‘ total ğ‘¡ = ğ‘ ğ´ ğ‘¡ + ğ‘ ğ‘ƒ ğ‘¡ . ğ‘ Ağ‘¡ We report macro-over-tasks means of ğ‘ ğ´ ğ‘¡ ğ‘ total ğ‘¡ (overall signal count) for each model, separately for resolved and unresolved tasks. (assertion count), ğ‘ ğ‘ƒ ğ‘¡ (value-revealing print count), and Table 4. Task-level feedback signal amount encoded in agent-written tests Resolved Unresolved Model #Tasks w/ tests Assertions per task ( ğ‘ ğ´) Prints per task ( ğ‘ ğ‘ƒ ) Total signals per task ( ğ‘ total) #Tasks w/ tests Assertions per task ( ğ‘ ğ´) Prints per task ( ğ‘ ğ‘ƒ ) Total signals per task ( ğ‘ total) claude-opus-4-5 gemini-3-pro-preview kimi-k2-thinking minimax-m2 deepseek-v3.2-reasoner 314 235 309 302 277 5.16 1.45 2.86 7.37 3.51 25.00 4.34 20.72 34.06 16.43 30.16 5.79 23.57 41.43 19. 101 73 178 191 169 5.36 1.62 3.51 4.66 3.31 25.61 5.04 24.03 43.09 20.95 30.97 6.66 27.54 47.76 24.27 Note. Macro-over-tasks means computed over tasks with tests. Assertions count assert statements; prints count valuerevealing prints. ğ‘ total = ğ‘ ğ´ + ğ‘ ğ‘ƒ . Results. Table 4 shows that, when agent-written tests are present, they can contain substantial number of feedback statements per task (e.g., 19.9447.76 total signals for several models). As shown in Figure 2, feedback is predominantly observational: for every model, value-revealing prints exceed assertions in the macro-average counts per task. Models differ markedly in overall signal volume. For example, minimax-m2 encodes the largest total signal counts (41.4347.76), whereas gemini-3-pro-preview encodes far fewer (5.796.66), with other models in between. Across models, unresolved tasks tend to show slightly higher total signal counts, driven mainly by more value-revealing prints (e.g., deepseek-v3.2-reasoner: +4.33 total; minimax-m2: +6.33 total). Assertion , Vol. 1, No. 1, Article . Publication date: February 2026. 10 Zhi Chen, Zhensu Sun, Yuling Shi, Chao Peng, Xiaodong Gu, David Lo, and Lingxiao Jiang Fig. 2. Composition of feedback signals in agent-written tests across models. Value-revealing prints dominate over assertions for all models. counts are comparatively stable; notably, minimax-m2 has fewer assertions but more prints on unresolved tasks. RQ2.1 Test Signal Amount: Key Takeaway When agents write tests, those tests often emit many runtime feedback statements per task, but the feedback is mostly observational: value-revealing prints consistently outnumber assertions across all models. Signal volume varies widely by model (e.g., minimax-m2 high vs. gemini-3pro-preview low), and unresolved tasks show slightly more total signals mainly due to additional prints, while assertion counts remain relatively stable."
        },
        {
            "title": "4.2 RQ2.2 Assertion categorization: What kinds of verification do assertions encode?",
            "content": "Goal and measurements. RQ2.1 counts how many assert statements appear in agent-written tests, but counts alone do not tell us what those assertions check. Assertions can enforce different kinds of checksfor example, basic preconditions (e.g., non-None or type checks) versus checks against expected values or structures. Models may therefore differ not only in how often they assert, but also in what forms of checks they write. RQ2.2 provides descriptive breakdown of assert statements into four assertion categories: C1 Sanity checks. The assertion only checks existence or type, without constraining the expected behavior. Example: assert is not None. C2 Property checks. The assertion checks property of value or object (e.g., membership or validity) without fixing an exact output. Example: assert hasattr(obj, \"attr\"). C3 Relational checks. The assertion enforces constraint such as range, bound, or relationship between values. Example: assert 0 <= score <= 1. This category also includes checks that expect specific exception, because they constrain the allowed behavior to must fail with an exception of type ğ¸ rather than matching single concrete output. C4 Exact checks. The assertion checks an exact value or deep structural equality. Example: assert output == expected_output. To identify and categorize assertions, we implement rule-based classifier over Python ASTs and map each extracted assertion to exactly one category. The classifier covers both native assert statements (e.g., assert == b) and framework-provided assertion calls (e.g., self.assertEqual(a, b) in unittest). Concretely, for each test artifact, we parse the code into an AST and extract assertion events from: (i) native assert <expr> statements, and (ii) calls to framework assertion APIs. Some assert statements contain multiple checks in one line, combined with boolean operators (e.g., assert > 0 and == 1). In this example, > 0 is constraint check (C3) and == , Vol. 1, No. 1, Article . Publication date: February 2026. Rethinking the Value of Agent-Generated Tests for LLM-Based Software Engineering Agents 11 1 is an exact check (C4). For such compound assert statements, we decompose the expression into atomic checks and assign single category by taking the highest category present under the ordering from C1 to C4, because the most specific check in the statement best reflects what the assertion is trying to enforce. Thus, assert > 0 and == 1 is labeled as C4. Table 5. Assertion category distribution by model. Counts and percentages are computed over all assertion statements written by each model."
        },
        {
            "title": "Model",
            "content": "#Assertions C1 Sanity C2 Property C3 Relational C4 Exact claude-opus-4-5 gemini-3-pro-preview kimi-k2-thinking minimax-m2 deepseek-v3.2-reasoner # 351 76 225 618 285 2160 458 1508 3117 1531 % # % # % # % 807 16.25% 154 16.59% 14.92% 622 19.83% 1291 537 18.62% 93 37.36% 36 33.62% 41.25% 45 41.42% 132 52 35.08% 909 4.31% 192 7.86% 2.98% 616 4.23% 1076 657 3.40% 42.08% 41.92% 40.85% 34.52% 42.91% Note. C1C4 denote four assertion categories defined by the form of the check (sanity, property, relational/approximate, and exact-output). Percentages are computed within each model, relative to the models total assertion count. Results. Table 5 shows that models have broadly similar assertion-category distributions. Across all five models, most assertions fall into C2 Property and C4 Exact, while C3 Relational remains consistently uncommon. For four models (claude-opus-4-5, gemini-3-pro-preview, kimi-k2-thinking, and deepseek-v3.2-reasoner), C4 Exact accounts for roughly 4143% of assertions (40.8542.91%), and C2 Property accounts for roughly 3441% (33.6241.25%). minimax-m2 follows the same overall shape but allocates smaller share to C4 Exact (34.52%) and larger shares to C1 Sanity (19.83%) and C2 Property (41.42%). Across all models, C3 Relational is rare (2.987.86%), with the highest proportion in gemini-3-pro-preview (7.86%). Overall, the consistent scarcity of C3 may have several practical explanations. First, relational or approximate checks can be more delicate to specify and maintain than local property checks or direct equality checks. Second, such checks may be less common in everyday unit-test patterns that models imitate during code generation. Third, for many SWE-bench issues, agents may find it more straightforward to write either local property checks (C2) or exact expected outputs (C4) once they have candidate fix. We treat these distributions as descriptive of which assertion forms appear in agent-written tests, not as evidence of correctness or impact on task resolution. RQ2.2 Assertion Categorization: Key Takeaway Across models, assertions in agent-written tests are dominated by two forms: checks of local properties (e.g., checking that required attribute is present) and checks against exact expected values. Assertions that express relational or range-style constraints are consistently uncommon (only small single-digit fraction of assertions across models). minimax-m2 has lower share of exact-value checks and higher share of sanity-guard and local-property checks than the other models, but the overall pattern is similar across models."
        },
        {
            "title": "4.3 Summary of RQ2\nRQ2 shifts from when agents test (RQ1) to what feedback their agent-written tests produce at\nexecution time. First, when tests are present, they often emit many feedback statements per task,\nbut the feedback is dominated by value-revealing prints rather than assertions: across all models,",
            "content": ", Vol. 1, No. 1, Article . Publication date: February 2026. 12 Zhi Chen, Zhensu Sun, Yuling Shi, Chao Peng, Xiaodong Gu, David Lo, and Lingxiao Jiang print signals consistently outnumber assert checks, and total signal volume varies widely by model (RQ2.1). Second, when agents do write assertions, models show broadly similar mixes of assertion forms: most assertions either check local properties (e.g., the presence or validity of an attribute or field) or check exact expected values, while relational or range-style constraints are rare (RQ2.2). Taken together, RQ2 characterizes agent-written tests primarily as an observational feedback channel: most test feedback comes from value-revealing prints. This naturally raises the next question: do these agent-written tests meaningfully affect task resolution? We address this in RQ3."
        },
        {
            "title": "5 RQ3: Do Agent-Written Tests Truly Affect Task Resolution?",
            "content": "Motivation. In RQ1, we find weak alignment between agent-written tests and the final task success in this high-autonomy setting. For example, gpt-5.2 almost never writes new test artifacts (3/500 tasks, 0.6%), yet it still resolves 71.8% of tasks. In contrast, claude-opus-4.5 writes at least one new test artifact in about 83% of tasks, but its resolution rate is only 2.6 percentage points higher (74.4%). RQ2 further shows that, when tests are written, most feedback comes from value-revealing prints rather than assert-based checks. These findings raise direct question: Do writing tests truly affect task resolution outcomes, and at what cost? Experiment Design. RQ3 answers two questions: RQ3.1 (Outcome impact): If we encourage or discourage agents to write tests, how do task resolution outcomes change? RQ3.2 (Efficiency impact): If we encourage or discourage agents to write tests, how do API calls and token usage change? Model selection. To isolate the effect of agent-written tests, we design two complementary intervention experiments: (i) encouraging agents to write tests, and (ii) discouraging agents from writing new test files. We choose models for each setup based on their baseline test-writing rate observed in RQ1  (Table 1)  , defined as the fraction of tasks where the agent writes test artifacts. For the encourage test writing setup, we focus on low test-writing models and medium testwriting models, so there is meaningful headroom for increasing test creation. Specifically, we include gpt-5.2 (0.6%), an extreme low test-writing model in RQ1 with near-zero test creation. We also include gemini-3-pro-preview (61.1%), medium test-writing model whose baseline test creation is already substantial but still leaves room for further increase. For the discourage test writing setup, we start from high test-writing models that write tests in the vast majority of tasks in RQ1: four models show consistently high test-writing rates (83.0% 98.6%; Table 1). Due to budget constraints, we select two representatives from this group: kimi-k2thinking (97.4%) and deepseek-v3.2-reasoner (89.2%). Concretely, we use two prompt variants: Encourage writing tests: for gpt-5.2 and gemini-3-pro-preview, we append an instruction to write at least one runnable new test file (a file whose name starts with test_ or ends with _test.py), separate from the repositorys existing tests. Discourage writing tests: for kimi-k2-thinking and deepseek-v3.2-reasoner, we (i) remove the sentence Test edge cases to ensure your fix is robust and (ii) append an instruction to not write any new test files or scripts; robustness and edge cases should be handled using reasoning and code inspection only. By comparing the performance of each agent under these revised prompts against their original performance, we isolate the specific impact of test writing. Consequently, the observed changes in behaviors or outcomes can be directly attributed to the agent-written tests. , Vol. 1, No. 1, Article . Publication date: February 2026. Rethinking the Value of Agent-Generated Tests for LLM-Based Software Engineering Agents 13 Fig. 3. Outcome-transition distribution on tasks with an intended test-status change"
        },
        {
            "title": "5.1 RQ3.1 Does encouraging or discouraging test writing change task resolution?",
            "content": "Goal and measurements. To answer whether encouraging or discouraging test writing changes task resolution, for each model, we compare each task under two conditions: the baseline run (under standard mini-SWE-agent prompt) and the intervention run (under our revised prompt). Specifically, we record two features: whether the run creates at least one new test artifact (No test vs. Has test) and whether the patch successfully resolve the issue (Fail vs. Success). Then, we analyze how these two features change from the baseline run to the intervention run. This respectively results in four possible transition groups for both test writing (No testNo test, No testHas test, Has testNo test, Has testHas test) and task outcome (Fail Success, Success Fail, Stable Success, and Stable Fail). To visualize the relationship between these shifts, we represent the results using transition matrix. In this matrix, the rows represent the change in test-writing behavior, while the columns represent the change in task outcomes. This structure allows us to pinpoint the exact impact of our intervention. For instance, the intersection of No test Has test and Fail Success represents the instances where encouraging test-writing directly led to breakthrough in resolving the task. Results. Our prompt interventions substantially change whether models write test artifacts, but these shifts rarely translate into outcome changes. As shown in Table 6, the encourage test writing prompt flips test status for the low test-writing model gpt-5.2 and the medium testwriting model gemini-3-pro-preview: 64.4% and 37.0% of tasks transition from No test to Has test, respectively. Conversely, the discourage test writing prompt removes tests at scale for the high test-writing models kimi-k2-thinking and deepseek-v3.2-reasoner, moving 68.4% and 75.2% of tasks from Has test to No test. Despite these large test-status shifts, resolution outcomes are largely stable. Figure 3 shows that, across models, an average of 83.2% of tasks keep the same final resolution result after the intervention. Table 6 further indicates that even when test status flips, success rates change only slightly. For example, for deepseek-v3.2-reasoner, discouraging test writing removes tests in 376 tasks but yields net decrease of only 20 resolved tasks, small change relative to the behavioral , Vol. 1, No. 1, Article . Publication date: February 2026. 14 Zhi Chen, Zhensu Sun, Yuling Shi, Chao Peng, Xiaodong Gu, David Lo, and Lingxiao Jiang Table 6. Test-writing status flips and outcome transitions Encourage writing tests Outcome transition No test No test No test Has test Has test No test Has test Has test Total Model: gpt-5.2 Fail Success Success Fail Stable Success Stable Fail Net change in #Success Model: gemini-3-pro-preview Fail Success Success Fail Stable Success Stable Fail Net change in #Success Discourage writing tests 9 9 111 46 0 0 1 2 -1 Î” 322 (64.4%) 18 18 218 68 0 Î” 185 (37%) 9 10 123 43 -1 0 0 1 0 0 1 5 0 -1 0 0 2 0 0 8 10 219 -2 27 27 332 114 0 17 22 349 112 -5 Outcome transition No test No test No test Has test Has test No test Has test Has test Total Model: kimi-k2-thinking Fail Success Success Fail Stable Success Stable Fail Net change in #Success Model: deepseek-v3.2-reasoner Fail Success Success Fail Stable Success Stable Fail Net change in #Success 1 1 5 3 0 10 3 19 18 7 0 0 2 1 2 0 1 1 2 Î” 342 (68.4%) 31 42 189 80 11 Î” 376 (75.2%) 29 49 187 111 20 11 13 65 2 7 5 36 22 2 43 56 261 140 13 48 57 243 9 Note. Test status is defined by whether the run writes at least one test artifact (Has test) or writes none (No test). Columns show the baselineintervention test-status transition; rows show the baselineintervention outcome transition (Fail/Success). The highlighted column indicates the intended test-status change (green: No testHas test under Encourage; red: Has testNo test under Discourage); Î” reports the number (and percentage) of tasks in that intended-change column. Net change(#Success) is computed per column as (#FailSuccess) (#SuccessFail). shift. Overall, changing how often model writes test artifacts appears to be weak lever for shifting task outcomes in this setting. RQ3.1: Outcomes vs. Test-Status Changes Discouraging test writing removes agent-written tests at scale (at least 68% of tasks for kimik2-thinking and deepseek-v3.2-reasoner). Yet these large behavioral shifts rarely translate into outcome shifts: across models, 83.2% of tasks keep the same final resolution result after the intervention. Even when tests are induced, success does not increase correspondingly, suggesting that simply writing more test artifacts provides little leverage for fixing the underlying issue. Conversely, suppressing tests for hundreds of tasks does not cause outcomes to collapse; for deepseek-v3.2-reasoner, removing tests in 376 tasks yields only 20 fewer successes. , Vol. 1, No. 1, Article . Publication date: February 2026. Rethinking the Value of Agent-Generated Tests for LLM-Based Software Engineering Agents"
        },
        {
            "title": "5.2 RQ3.2 How do API calls and token usage change?",
            "content": "Goal and measurements. We further analyze the following three metrics based on the trajectories generated in RQ3.1: (i) average API calls per task, (ii) average input tokens per task, and (iii) average output tokens per task. Table 7. API calls and token usage under baseline vs. encourage-tests / discourage-tests conditions. Model gpt-5.2 gemini-3-pro-preview kimi-k2-thinking deepseek-v3.2-reasoner Condition Tasks resolved Avg API Calls Avg Input Tokens Avg Output Tokens Baseline Encourage writing tests Change Baseline Encourage writing tests Change 359 (71.8%) 359 (71.8%) +0 (+0.0%) 371 (74.2%) 366 (73.2%) 5 (1.0%) 19.76 20.84 +1.08 (+5.5%) 40.33 39.21 1.11 (2.8%) 242,855 264,762 +21,907 (+9.0%) 666,096 641,307 24,789 (3.7%) Baseline Discourage writing tests Change 668,449 317 (63.4%) 304 (60.8%) 340,689 13 (2.6%) 16.57 (35.4%) 327,760 (49.0%) 46.82 30.25 Baseline Discourage writing tests Change 637,297 300 (60.0%) 291 (58.2%) 427,780 9 (1.8%) 11.35 (24.5%) 209,518 (32.9%) 46.40 35. 24,550 29,415 +4,866 (+19.8%) 11,114 10,943 171 (1.5%) 14,895 8,468 6,427 (43.1%) 52,120 44,823 7,297 (14.0%) Note. Changes are computed as (Condition Baseline). Encourage writing tests is applied to gpt-5.2 and gemini-3-propreview; Discourage writing tests is applied to kimi-k2-thinking and deepseek-v3.2-reasoner. Results. Table 7 reports task outcomes (Tasks resolved) and efficiency metrics (API calls and tokens) under the baseline and the two intervention conditions. Overall, the interventions have only marginal impact on resolution rates, but they can noticeably reshape efficiencyin ways that depend on each models baseline test-writing propensity. For the encourage test writing setup, the effects differ between the low test-writing model gpt-5.2 and the medium test-writing model gemini-3-pro-preview. For gpt-5.2, encouraging test writing increases overhead: API calls rise by 5.5% and output tokens by 19.8%, while the resolution rate remains unchanged from baseline. In the medium test-writing model gemini-3-pro-preview, we observe very small shifts in overall usage (e.g., 1.5% completion tokens). likely reason is that this model already writes tests in about 61.6% of baseline tasks, leaving limited headroom for the prompt to flip tasks from No test to Has test; accordingly, the net change in testing-related interaction is small on average. Moreover, because the model is nondeterministic, even when task contains tests in both baseline and encouraged runs, the amount of testing (and related interaction) can differ slightly between the paired runs; these small within-task fluctuations average out, keeping aggregate cost differences modest and making small decrease reasonable. The most striking efficiency shifts appear in the discourage test writing setup for the high testwriting models kimi-k2-thinking and deepseek-v3.2-reasoner. Discouraging test writing yields large reductions in resource consumption, with input tokens dropping by 49.0% and 32.9%, respectively. For kimi-k2-thinking, average API calls fall by 35.4%, cutting interaction workload by over third. Notably, these efficiency gains come with only small decreases in success rates (2.6% for kimi-k2thinking and 1.8% for deepseek-v3.2-reasoner), suggesting that substantial portion of the baseline interaction budget spent on test writing for these models has limited marginal benefit for task resolution in this setting. , Vol. 1, No. 1, Article . Publication date: February 2026. 16 Zhi Chen, Zhensu Sun, Yuling Shi, Chao Peng, Xiaodong Gu, David Lo, and Lingxiao Jiang RQ3.2: Cost changes are larger than outcome changes Changing whether agents write tests has little effect on task resolution, but it can noticeably change efficiency. More tests increases overhead for low test-writing model models without improving outcomes: for gpt-5.2, API calls rise by 5.5% and output tokens by 19.8% while the resolution rate stays unchanged. In contrast, suppressing tests yields large efficiency gains for high test-writing models: input tokens drop by 49.0% (kimi-k2-thinking) and 32.9% (deepseekv3.2-reasoner), and kimi-k2-thinking reduces API calls by 35.4%. These savings come with only small success drops (2.6% and 1.8%), implying that most test-writing overhead in these models delivers low marginal utility for producing successful final patch. Summary of RQ3. Overall, more agent-written tests do not mean more solves in this high-autonomy setting. When we push models to write more tests, test-writing status flips at scale (e.g., 64.4% for gpt-5.2), yet task outcomes remain largely unchanged: on average, 83.2% of tasks keep the same success/fail result after the intervention. At the same time, more tests can be expensive: for the low test-writing model gpt-5.2, inducing test writing increases overhead (+5.5% API calls; +19.8% output tokens) without any gain in resolution. Conversely, fewer tests can be much cheaper with only small outcome losses: for the high test-writing models kimi-k2-thinking and deepseek-v3.2-reasoner, suppressing test writing cuts input tokens by 49.0% and 32.9%, respectively, while success drops remain modest (2.6% and 1.8%). Taken together, varying the amount of agent-written tests strongly reshapes resource usage but has limited leverage on whether the final patch resolves the issue, suggesting that much of the test-writing effort provides low marginal utility for task resolution."
        },
        {
            "title": "6.1 Implications\nOur study indicates that the testing behaviors conducted by current LLMs, without clear guidance,\ndo not show significant improvement in agent-based software issue resolution. It suggests two\nmain implications for the research and practice in designing and utilizing LLM software agents.",
            "content": "On the one hand, our results reveal inherent limitations in how existing LLMs leverage testing for issue resolution when explicit guidance is absent. This highlights the importance of instructing agents not only on whether tests should be written, but also on how they should be designed and, more critically, how to effectively interpret and utilize the feedback generated by test execution. On the other hand, given that current LLMs often fail to extract meaningful value from test writing, practitioners may consider adopting more conservative approach to agent-generated tests. Such an approach could involve more sophisticated prompting strategies or guardrail mechanisms that help agents determine when test writing is truly necessary. Additionally, code agents could benefit from incorporating costbenefit monitoring framework that tracks the overhead associated with test-related activities, such as test writing, execution, and failure analysis, to assess their actual contribution to patch refinement."
        },
        {
            "title": "6.2 Future work\nOur findings motivate two future-work topics on agent-written testing in lightly scaffolded, high-\nautonomy setups.\nEvaluating on-the-fly test quality in a non-stationary code state. Traditional test-quality\nmetrics (e.g., coverage, mutation score, fault revelation) assume a fixed snapshot of the system\nunder test and reproducible executions [2, 15, 27, 37, 39, 48, 50]. In agentic development, the code\nstate is non-stationary: tests are written and run against intermediate repository versions that may\nlater be overwritten, and the final patch may not preserve the exact state a test originally validated.",
            "content": ", Vol. 1, No. 1, Article . Publication date: February 2026. Rethinking the Value of Agent-Generated Tests for LLM-Based Software Engineering Agents 17 This complicates attribution (which version/function test exercised), reproducibility, and the use of conventional quality pipelines. Future work should develop instrumentation for execution-time evaluatione.g., capturing runnable snapshots (code, environment, commands) at test timeand define metrics that remain meaningful for transient intermediate artifacts. Self-evolving test-generation strategies without over-constraining exploration. Our results highlight tension between structure and autonomy: fully self-directed testing can increase cost with limited outcome gains, while heavy, fixed workflow can narrow the search space and preclude better strategies. The goal is to produce higher-value tests by deciding when to test, what to verify (oracle design), and how to allocate budget across reasoning, editing, and validation, while preserving task-specific flexibility. promising direction is self-evolving [17, 35, 46, 52] testing policies: instead of static hand-written prompt, allow the agent to revise its own testing prompt/policy from environment feedback and failure modes, adapting to task context and model capabilities [12]. Future work can formalize this as closed-loop optimization under cost and safety constraints, and compare human-specified versus self-adapted strategies under matched budgets and controlled scaffolds."
        },
        {
            "title": "7 Related Work\nEvaluation for LLM-Generated Tests. Prior work evaluates LLM-generated testing artifacts under\npredefined objectives, most commonly unit tests and assertions, via systems and empirical studies\non test-suite quality and model/prompt improvements [21, 24, 38, 49, 51], including targeted oracle\ngeneration such as assertions [53]. Recent surveys further systematize this space, summarizing\nhow requirements artifacts are translated into tests and the quality criteria used to judge generated\ntests [49]. Complementing academic evaluations, industrial studies report closed-loop pipelines that\ncombine LLM-based test generation with mutation-guided feedback to steer or refine generated",
            "content": ", Vol. 1, No. 1, Article . Publication date: February 2026. 18 Zhi Chen, Zhensu Sun, Yuling Shi, Chao Peng, Xiaodong Gu, David Lo, and Lingxiao Jiang tests toward stronger fault-revealing capability [15]. These studies typically score outputs with fixed quality metrics (e.g., coverage, mutation-based adequacy proxies, fault revelation) on fixed target program or code snapshot. Benchmarks similarly cast testing as standalone objective with fixed tasks and protocols (e.g., TestEval [43], SWT-bench [30]). In contrast, our study focuses on agent-written tests that emerge dynamically during high-autonomy, multi-step resolution of real-world GitHub issues, where the codebase and candidate patches evolve over time. We treat test writing and execution as an emergent process behavior, and characterize (i) whether/when/how intensively agents test, (ii) what signals tests encode (assertions vs. observational prints), and (iii) how these behaviors relate to resolution outcomes. Trajectory Analysis of Software Agents. Recent work moves beyond final patch and binary success/failure by analyzing the intermediate reasoning and execution traces of LLM-based agents. Studies have examined actionobservation patterns that distinguish successful from failed runs [3], compared trajectory length and fault-localization accuracy across agents [25], and proposed workflow taxonomies that decompose agent behavior into stages such as localization, patching, and testing-related steps [4]. Others conduct systematic failure analyses that identify root causes such as diagnostic errors and unproductive loops [22], while process-oriented studies further show that agents often hit recurrent execution errors during issue resolution, motivating lightweight checks and recovery components for robustness [7]. Overall, existing trajectory analyses emphasize action sequences, outcome separation, and error categorization, but rarely examine whether and how agents autonomously decide to test. Our work addresses this gap by characterizing the emergent testing behaviors observed in execution trajectories and analyzing what feedback agent-written tests actually provide during issue resolution."
        },
        {
            "title": "8 Conclusion\nThis paper revisited the common intuition that \"testing helps\" for LLM-based software agents in a\nhigh-autonomy setting where writing and running tests is not specified in the prompt. Using SWE-\nbench Verified trajectories produced under a light agent scaffold, we separated (i) whether/when/how\nagents write and execute tests, (ii) what feedback those tests encode, and (iii) whether changing\ntest-writing instructions actually shifts task outcomes and efficiency. RQ1 established a descriptive\nbaseline of emergent testing behaviors. Across models, test writing propensity differs dramatically\n(from near-universal test writing to almost none), while within the same model, resolved and\nunresolved tasks show broadly similar test-writing rates. When tests are present, unresolved tasks\ntend to spread test writing across a slightly larger portion of the trajectory and execute tests more\noften, with higher re-execution intensity; process-level execution failures, however, vary mainly by\nmodel rather than outcome. These results highlight that in a high-autonomy setting, \"testing\" is a\nmodel-specific process behavior and is not tightly coupled to eventual success. RQ2 then showed\nthat the content of agent-written tests is largely observational. Across all models, value-revealing\nprints consistently outnumber assert-based checks, and overall signal volume varies widely by\nmodel. When assertions are used, their forms are broadly similar across models: most are local\nproperty checks or exact-value checks, while relational or range-style constraints are consistently\nrare. Taken together, agent-written tests function primarily as a runtime probing interface rather\nthan a systematic verification mechanism. Finally, RQ3 provided controlled evidence about impact.\nPrompt-only interventions flip test-writing status at scaleâ€”inducing tests for the low test writing\nmodel and suppressing test writing for high test writing modelsâ€”yet task outcomes are mostly\nunchanged (the majority of tasks preserve the same success/fail result). In contrast, efficiency effects\ncan be substantial: for gpt-5.2â€”the near-zero test-writing modelâ€”encouraging test writing increases\noverhead without improving resolution, whereas for test-heavy models (e.g., kimi-k2-thinking and\ndeepseek-v3.2-reasoner) discouraging test writing can sharply reduce API calls and token usage",
            "content": ", Vol. 1, No. 1, Article . Publication date: February 2026. Rethinking the Value of Agent-Generated Tests for LLM-Based Software Engineering Agents 19 with only modest decreases in success. Overall, more agent-written tests do not mean more solves: varying test-writing effort primarily reshapes resource usage and interaction patterns, with limited leverage on whether the final patch resolves the issue. Overall, our findings provide an empirical answer to the question: Help or Habit? In highautonomy software issue resolution, agent-written tests often behave more like reproduced software development lifecycle routine than dependable source of help: models that write many tests are not consistently more successful, and prompt interventions that induce or suppress test writing rarely change whether the issue is resolved. What tests do change is the process footprintAPI calls, token usage, and interaction stepsindicating that test writing frequently reflects how an agent chooses to work, not whether it can reliably validate the patch."
        },
        {
            "title": "9 Data Availability\nTo support transparency and reproducibility, we publicly release a replication package containing\nthe datasets, raw trajectories, and analysis scripts used in this study. The package is available on\nFigshare: https://figshare.com/s/8fa6412b86bf54fcd27e?file=61474897.",
            "content": "References [1] Anthropic. 2025. Introducing Claude Opus 4.5. Anthropic Newsroom. https://www.anthropic.com/news/claude-opus4-5 Model announcement. See also the Claude Opus 4.5 system card page: https://www.anthropic.com/claude-opus-45-system-card. [2] Shreya Bhatia, Tarushi Gandhi, Dhruv Kumar, and Pankaj Jalote. 2024. Unit test generation using generative AI: comparative performance analysis of autogeneration tools. In Proceedings of the 1st International Workshop on Large Language Models for Code. 5461. [3] Islem Bouzenia and Michael Pradel. 2025. Understanding Software Engineering Agents: Study of Thought-ActionResult Trajectories. In Proceedings of the 40th IEEE/ACM International Conference on Automated Software Engineering (ASE). Sacramento, CA, USA. [4] Ira Ceka, Saurabh Pujar, Shyam Ramji, Luca Buratti, Gail Kaiser, and Baishakhi Ray. 2025. Understanding Software Engineering Agents Through the Lens of Traceability: An Empirical Study. arXiv preprint arXiv:2506.08311 (2025). [5] Zhi Chen and Lingxiao Jiang. 2024. Promise and peril of collaborative code generation models: Balancing effectiveness and memorization. In Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering. 493505. [6] Zhi Chen and Lingxiao Jiang. 2025. Evaluating Software Development Agents: Patch Patterns, Code Quality, and Issue Complexity in Real-World GitHub Scenarios. In Proceedings of the 32nd IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER). [7] Zhi Chen, Wei Ma, and Lingxiao Jiang. 2026. Beyond Final Code: Process-Oriented Error Analysis of Software Development Agents in Real-World GitHub Scenarios. In Proceedings of the 47th IEEE/ACM International Conference on Software Engineering (ICSE). Rio de Janeiro, Brazil. [8] Cognition Labs. 2024. Introducing Devin, the First AI Software Engineer. https://cognition.ai/blog/introducing-devin. [9] DeepSeek. 2025. DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models. (2025). arXiv:2512.02556 [cs.CL] https://arxiv.org/abs/2512.02556 [10] DeepSeek. 2025. Reasoning Model (deepseek-reasoner). DeepSeek API Documentation. https://api-docs.deepseek. com/guides/reasoning_model Official API guide for DeepSeek reasoning model endpoint.. [11] Ryan Ehrlich, Bradley Brown, Jordan Juravsky, Ronald Clark, Christopher RÃ©, and Azalia Mirhoseini. 2025. CodeMonkeys: Scaling Test-Time Compute for Software Engineering. arXiv preprint arXiv:2501.14723 (2025). [12] Huan-ang Gao, Jiayi Geng, Wenyue Hua, Mengkang Hu, Xinzhe Juan, Hongzhang Liu, Shilong Liu, Jiahao Qiu, Xuan Qi, Yiran Wu, et al. 2025. survey of self-evolving agents: On path to artificial super intelligence. arXiv preprint arXiv:2507.21046 (2025). [13] Pengfei Gao, Zhao Tian, Xiangxin Meng, Xinchen Wang, Ruida Hu, Yuanan Xiao, Yizhou Liu, Zhao Zhang, Junjie Chen, Cuiyun Gao, Yun Lin, Yingfei Xiong, Chao Peng, and Xia Liu. 2025. Trae Agent: An LLM-based Agent for Software Engineering with Test-time Scaling. arXiv preprint arXiv:2507.23370 (2025). [14] Google Cloud. 2025. Gemini 3 Pro on Vertex AI. Vertex AI Model Documentation. https://docs.cloud.google.com/vertexai/generative-ai/docs/models/gemini/3-pro Official model documentation (includes preview variants).. [15] Mark Harman, Jillian Ritchey, Inna Harper, Shubho Sengupta, Ke Mao, Abhishek Gulati, Christopher Foster, and HervÃ© Robert. 2025. Mutation-guided llm-based test generation at meta. In Proceedings of the 33rd ACM International , Vol. 1, No. 1, Article . Publication date: February 2026. 20 Zhi Chen, Zhensu Sun, Yuling Shi, Chao Peng, Xiaodong Gu, David Lo, and Lingxiao Jiang Conference on the Foundations of Software Engineering. 180191. [16] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Ceyao Zhang, Jinlin Wang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, et al. 2024. MetaGPT: Meta Programming for Multi-Agent Collaborative Framework. In The Twelfth International Conference on Learning Representations. [17] Yue Hu, Yuzhu Cai, Yaxin Du, Xinyu Zhu, Xiangrui Liu, Zijie Yu, Yuchen Hou, Shuo Tang, and Siheng Chen. 2025. SelfEvolving Multi-Agent Collaboration Networks for Software Development. In The Thirteenth International Conference on Learning Representations. https://openreview.net/forum?id=4R71pdPBZp [18] Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. 2024. SWE-bench: Can Language Models Resolve Real-world Github Issues?. In The Twelfth International Conference on Learning Representations. [19] Kabir Khandpur, Kilian Lieret, Carlos E. Jimenez, Ofir Press, and John Yang. 2025. SWE-bench Multilingual. https: //www.swebench.com/multilingual.html. [20] Yubin Kim, Ken Gu, Chanwoo Park, Chunjong Park, Samuel Schmidgall, Ali Heydari, Yao Yan, Zhihan Zhang, Yuchen Zhuang, Mark Malhotra, et al. 2025. Towards science of scaling agent systems. arXiv preprint arXiv:2512.08296 (2025). [21] Yihao Li, Pan Liu, Haiyang Wang, Jie Chu, and Eric Wong. 2025. Evaluating large language models for software testing. Computer Standards & Interfaces 93 (2025), 103942. [22] Simiao Liu, Fang Liu, Liehao Li, Xin Tan, Yinghao Zhu, Xiaoli Lian, and Li Zhang. 2025. An Empirical Study on Failures in Automated Issue Solving. arXiv preprint arXiv:2509.13941 (2025). [23] Yizhou Liu, Pengfei Gao, Xinchen Wang, Jie Liu, Yexuan Shi, Zhao Zhang, and Chao Peng. 2024. MarsCode Agent: AI-native Automated Bug Fixing. arXiv preprint arXiv:2409.00899 (2024). [24] Andrea Lops, Fedelucio Narducci, Azzurra Ragone, Michelantonio Trizio, and Claudio Bartolini. 2025. system for automated unit test generation using large language models and assessment of generated test suites. In 2025 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW). IEEE, 2936. [25] Oorja Majgaonkar, Zhiwei Fei, Xiang Li, Federica Sarro, and He Ye. 2026. Understanding Code Agent Behaviour: An Empirical Study of Success and Failure Trajectories. In Proceedings of the 47th IEEE/ACM International Conference on Software Engineering (ICSE). Rio de Janeiro, Brazil. [26] MiniMax. 2025. MiniMax-M2. MiniMax News. https://www.minimax.io/news/minimax-m2 Official release note / technical overview.. [27] Davide Molinelli, Luca Di Grazia, Alberto Martin-Lopez, Michael D. Ernst, and Mauro PezzÃ¨. 2025. Do LLMs Generate Useful Test Oracles? An Empirical Study with an Unbiased Dataset. In ASE 2025: Proceedings of the 39th Annual International Conference on Automated Software Engineering. Seoul, South Korea. [28] Moonshot AI. 2025. Introducing Kimi K2 Thinking. Project page. https://moonshotai.github.io/Kimi-K2/thinking.html Official page describing Kimi K2 Thinking.. [29] Fangwen Mu, Junjie Wang, Lin Shi, Song Wang, Shoubin Li, and Qing Wang. 2025. EXPEREPAIR: Dual-Memory Enhanced LLM-based Repository-Level Program Repair. arXiv preprint arXiv:2506.10484 (2025). [30] Niels MÃ¼ndler, Mark MÃ¼ller, Jingxuan He, and Martin Vechev. 2024. SWT-bench: Testing and validating real-world bug-fixes with code agents. Advances in Neural Information Processing Systems 37 (2024), 8185781887. [31] OpenAI. 2024. Introducing SWE-bench Verified. https://openai.com/index/introducing-swe-bench-verified/. [32] OpenAI. 2025. Update to GPT-5 System Card: GPT-5.2. Technical Report. OpenAI. https://cdn.openai.com/pdf/3a4153c8c748-4b71-8e31-aecbde944f8d/oai_5_2_system-card.pdf System card (PDF).. [33] Albert Ã–rwall. 2024. Moatless Tools. https://github.com/aorwall/moatless-tools. [34] Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, et al. 2024. ChatDev: Communicative Agents for Software Development. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 1517415186. [35] Maxime Robeyns, Martin Szummer, and Laurence Aitchison. 2025. Self-Improving Coding Agent. In Scaling Self-Improving Foundation Models without Human Supervision. https://openreview.net/forum?id=rShJCyLsOr [36] Haifeng Ruan, Yuntong Zhang, and Abhik Roychoudhury. 2024. Specrover: Code intent extraction via llms. arXiv preprint arXiv:2408.02232 (2024). [37] Gabriel Ryan, Siddhartha Jain, Mingyue Shang, Shiqi Wang, Xiaofei Ma, Murali Krishna Ramanathan, and Baishakhi Ray. 2024. Code-aware prompting: study of coverage-guided test generation in regression setting using llm. Proceedings of the ACM on Software Engineering 1, FSE (2024), 951971. [38] Max SchÃ¤fer, Sarah Nadi, Aryaz Eghbali, and Frank Tip. 2023. An empirical evaluation of using large language models for automated unit test generation. IEEE Transactions on Software Engineering 50, 1 (2023), 85105. [39] Jiho Shin, Sepehr Hashtroudi, Hadi Hemmati, and Song Wang. 2024. Domain adaptation for code model-based unit test case generation. In Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis. 12111222. [40] SWE-agent Team. 2024. mini-SWE-agent. https://github.com/SWE-agent/mini-swe-agent. , Vol. 1, No. 1, Article . Publication date: February 2026. Rethinking the Value of Agent-Generated Tests for LLM-Based Software Engineering Agents [41] SWE-bench Team. 2024. SWE-bench Bash-only Leaderboard. https://www.swebench.com/bash-only.html. [42] Junjie Wang, Yuchao Huang, Chunyang Chen, Zhe Liu, Song Wang, and Qing Wang. 2024. Software testing with large language models: Survey, landscape, and vision. IEEE Transactions on Software Engineering 50, 4 (2024), 911936. [43] Wenhan Wang, Chenyuan Yang, Zhijie Wang, Yuheng Huang, Zhaoyang Chu, Da Song, Lingming Zhang, An Ran Chen, and Lei Ma. 2025. Testeval: Benchmarking large language models for test case generation. In Findings of the Association for Computational Linguistics: NAACL 2025. 35473562. [44] Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, Robert Brennan, Hao Peng, Heng Ji, and Graham Neubig. 2024. OpenHands: An Open Platform for AI Software Developers as Generalist Agents. arXiv preprint arXiv:2407.16741 (2024). [45] Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. 2024. Agentless: Demystifying LLM-based Software Engineering Agents. arXiv preprint arXiv:2407.01489 (2024). [46] Chunqiu Steven Xia, Zhe Wang, Yan Yang, Yuxiang Wei, and Lingming Zhang. 2025. Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly? arXiv preprint arXiv:2511.13646 (2025). [47] John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. 2024. SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering. In Advances in Neural Information Processing Systems, Vol. 37. 5052850652. [48] Lin Yang, Chen Yang, Shutao Gao, Weijing Wang, Bo Wang, Qihao Zhu, Xiao Chu, Jianyi Zhou, Guangtai Liang, Qianxiang Wang, et al. 2024. On the evaluation of large language models in unit test generation. In Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering. 16071619. [49] Zhenzhen Yang, Rubing Huang, Chenhui Cui, Nan Niu, and Dave Towey. 2025. Requirements-based test generation: comprehensive survey. ACM Transactions on Software Engineering and Methodology (2025). [50] Shengcheng Yu, Chunrong Fang, Yuchen Ling, Chentian Wu, and Zhenyu Chen. 2023. Llm for test script generation and migration: Challenges, capabilities, and opportunities. In 2023 IEEE 23rd International Conference on Software Quality, Reliability, and Security (QRS). IEEE, 206217. [51] Zhiqiang Yuan, Mingwei Liu, Shiji Ding, Kaixin Wang, Yixuan Chen, Xin Peng, and Yiling Lou. 2024. Evaluating and improving chatgpt for unit test generation. Proceedings of the ACM on Software Engineering 1, FSE (2024), 17031726. [52] Jenny Zhang, Shengran Hu, Cong Lu, Robert Lange, and Jeff Clune. 2025. Darwin Godel Machine: Open-Ended Evolution of Self-Improving Agents. arXiv:2505.22954 [cs.AI] https://arxiv.org/abs/2505.22954 [53] Quanjun Zhang, Weifeng Sun, Chunrong Fang, Bowen Yu, Hongyan Li, Meng Yan, Jianyi Zhou, and Zhenyu Chen. 2025. Exploring automated assertion generation via large language models. ACM Transactions on Software Engineering and Methodology 34, 3 (2025), 125. [54] Yuntong Zhang, Haifeng Ruan, Zhiyu Fan, and Abhik Roychoudhury. 2024. AutoCodeRover: Autonomous Program Improvement. In Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis. 15921604. , Vol. 1, No. 1, Article . Publication date: February 2026."
        }
    ],
    "affiliations": [
        "ByteDance",
        "Shanghai Jiao Tong University",
        "Singapore Management University"
    ]
}