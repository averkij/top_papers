{
    "paper_title": "Large Language Models Orchestrating Structured Reasoning Achieve Kaggle Grandmaster Level",
    "authors": [
        "Antoine Grosnit",
        "Alexandre Maraval",
        "James Doran",
        "Giuseppe Paolo",
        "Albert Thomas",
        "Refinath Shahul Hameed Nabeezath Beevi",
        "Jonas Gonzalez",
        "Khyati Khandelwal",
        "Ignacio Iacobacci",
        "Abdelhakim Benechehab",
        "Hamza Cherkaoui",
        "Youssef Attia El-Hili",
        "Kun Shao",
        "Jianye Hao",
        "Jun Yao",
        "Balazs Kegl",
        "Haitham Bou-Ammar",
        "Jun Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Agent K v1.0, an end-to-end autonomous data science agent designed to automate, optimise, and generalise across diverse data science tasks. Fully automated, Agent K v1.0 manages the entire data science life cycle by learning from experience. It leverages a highly flexible structured reasoning framework to enable it to dynamically process memory in a nested structure, effectively learning from accumulated experience stored to handle complex reasoning tasks. It optimises long- and short-term memory by selectively storing and retrieving key information, guiding future decisions based on environmental rewards. This iterative approach allows it to refine decisions without fine-tuning or backpropagation, achieving continuous improvement through experiential learning. We evaluate our agent's apabilities using Kaggle competitions as a case study. Following a fully automated protocol, Agent K v1.0 systematically addresses complex and multimodal data science tasks, employing Bayesian optimisation for hyperparameter tuning and feature engineering. Our new evaluation framework rigorously assesses Agent K v1.0's end-to-end capabilities to generate and send submissions starting from a Kaggle competition URL. Results demonstrate that Agent K v1.0 achieves a 92.5\\% success rate across tasks, spanning tabular, computer vision, NLP, and multimodal domains. When benchmarking against 5,856 human Kaggle competitors by calculating Elo-MMR scores for each, Agent K v1.0 ranks in the top 38\\%, demonstrating an overall skill level comparable to Expert-level users. Notably, its Elo-MMR score falls between the first and third quartiles of scores achieved by human Grandmasters. Furthermore, our results indicate that Agent K v1.0 has reached a performance level equivalent to Kaggle Grandmaster, with a record of 6 gold, 3 silver, and 7 bronze medals, as defined by Kaggle's progression system."
        },
        {
            "title": "Start",
            "content": "Agent v1.0 4 2 0 2 5 ] . [ 1 2 6 5 3 0 . 1 1 4 2 : r a"
        },
        {
            "title": "Large Language Models Orchestrating Structured\nReasoning Achieve Kaggle Grandmaster Level",
            "content": "Antoine Grosnit1,3,, Alexandre Maraval1,, James Doran1, Giuseppe Paolo1, Albert Thomas1, Refinath Shahul Hameed Nabeezath Beevi1, Jonas Gonzalez1, Khyati Khandelwal1, Ignacio Iacobacci1, Abdelhakim Benechehab1, Hamza Cherkaoui1, Youssef Attia El-Hili1, Kun Shao1, Jianye Hao1, Jun Yao1, Balazs Kegl1,, Haitham Bou-Ammar1,2,, Jun Wang2, 1 Huawei Noahs Ark 2 AI Centre, Department of Computer Science, UCL 3 Technical University of Darmstadt Corresponding Authors, Equal contributions Abstract: We introduce Agent v1.0, our first version of an end-to-end autonomous data science agent designed to automate, optimise, and generalise across diverse data science tasks. Fully automated, Agent v1.0 manages the entire data science life cycle by learning from experience. Unlike traditional chain-of-thought and reflection methods, which are rigid and limited in incorporating feedback, Agent v1.0 leverages highly flexible structured reasoning framework. This enables it to dynamically process memory in nested structure, effectively learning from accumulated experience stored to handle complex reasoning tasks. Agent v1.0 optimises longand short-term memory by selectively storing and retrieving key information, guiding future decisions based on environmental rewards. This iterative approach allows it to refine decisions without traditional fine-tuning or backpropagation, achieving continuous improvement through experiential learning. We evaluate our agents capabilities using Kaggle competitions as case study. Following fully automated protocol, Agent v1.0 systematically addresses complex data science tasks, employing Bayesian optimisation for hyperparameter tuning and advanced feature engineering. Integrating libraries like Torchvision and HuggingFace, it handles diverse data modalities and, once models are trained, determines optimal submission strategies to the Kaggle leaderboard. This adaptive refinement, based on performance feedback, occurs without human intervention. Our new evaluation framework rigorously assesses Agent v1.0s end-to-end capabilities, from retrieving Kaggle competition URL to submitting results and ranking on the leaderboard. Results demonstrate that Agent v1.0 achieves 92.5% success rate across tasks, spanning tabular, computer vision, NLP, and multimodal domains. When benchmarking against 5,856 human Kaggle competitors by calculating Elo-MMR scores for each, Agent v1.0 ranks in the top 38%, demonstrating an overall skill level comparable to Expert-level users. Notably, its Elo-MMR score falls between the first and third quartiles of scores achieved by human Grandmasters. Furthermore, our results indicate that Agent v1.0 has reached performance level equivalent to Kaggle Grandmaster, with record of 6 gold medals, 3 silver medals, and 7 bronze medals, as defined by Kaggles progression system. 1 Agent v1."
        },
        {
            "title": "1 Introduction",
            "content": "Large language models (LLMs) excel at human interactions through natural language [1], but their ability to engage with the physical world remains in its early stages [6]. Some progress has been made in areas such as robotics [51, 53] through careful skill engineering, as well as in navigating mobile apps [16], and across reinforcement learning (RL) domains [8, 12, 68, 85]. To improve LLM performance in complex real-world domains, various prompting strategies have been proposed to enhance reasoning and planning abilities, such as chains of thought [79, 81], trees of thought [88], and graphs of thought [5], among other methods [80]. These advancements, along with tool integration [25, 66], have motivated the development of generalist AI agents that successfully solve (though still relatively simple and simulated) sequential decision-making problems by relying on LLM outputs as decision policies [14, 24]. Real-world challenges often demand more than isolated solutions, requiring systematic approaches to complex problems. This has driven research into how LLMs can handle agent-based tasks with sequential or parallel modules, enabling dynamic, multi-step problem-solving. In this paper, we adopt first-principle approach, framing the analysis, processing, and prediction of data (i.e., data science) as core and transferable skill for LLMs interacting with real-world environments and external systems. Our motivation is threefold. First, data science is designed to extract insights and knowledge from structured and unstructured data, addressing various complex problems. This necessitates system-level approach, demanding automation and optimisation to adapt to specific task objectives. For example, in Kaggle data science contests (illustrated in Figure 1), data scientists follow structured workflow: collecting, cleaning, preprocessing, and standardising data, creating data loaders for efficient management, generating key evaluation metrics, and developing custom models. These insights and predictions then inform strategic decisions and drive operational optimisation. Thus, resolving these workflows requires data science agents to decompose tasks into subproblems and interact with various subsystems to achieve specified objectives. Second, data enables LLM agents to perceive and comprehend external environments (both physical and virtual). LLM agents can derive meaningful insights and make informed decisions by collecting, cleaning, and analysing this data. This essential connection between data and action bridges computational reasoning with tangible, real-world outcomes. Third, data science is essential for enterprises, driving innovation by transforming raw data into actionable insights that enhance efficiency and competitiveness. As result, global investment in data science is projected to reach around $740 billion by 2031 [61]. data science agent would amplify this impact by generating codes for automating tasks like data cleaning, modelling, and prediction, allowing organisations to scale their data-driven decision-making to maximise returns and profits. An LLM agent for data science faces two challenges, automation and optimisation, due to the complexity and multifaceted nature of data science pipelines, which require technical formulation and domain-specific expertise. First, data science workflows are inherently dynamic, requiring constant monitoring and adaptation to real-time data changes, making 2 Agent v1.0 Figure 1. The above figure illustrates the data science pipeline, showing modular phases, each with its complexities and sequential decision-making nature. After comprehending the data science problem, data scientist performs data engineering and cleaning, followed by data analysis to gather relevant insights. Equipped with those insights, they then build machine-learning models to discover latent data patterns. Finally, feedback from validation and evaluation is used to improve the design choices of each phase before deployment. automation difficult. For example, accurate data cleaning and feature engineering are essential before developing any machine learning model, but their solutions must be tailored to each case. Second, the optimisation process involves numerous steps, including feature selection, model training, hyper-parameter tuning, and evaluation, all of which operate in large search spaces. Moreover, data scientists often face delays in assessing the impact of their choices, as pipelines must process vast amounts of data and execute complex computations before returning performance metrics. Only after receiving this feedback can data scientists identify which parts of the pipeline contributed to errors or suboptimal results, making the credit assignment process both time-consuming and non-trivial. This paper introduces novel approach that utilises LLMs to address the complexities of systematic data science tasks. common technique employed by traditional LLM agents often employs chain-of-thought (CoT) techniques [79, 81], generating answers or actions through sequence of intermediate reasoning steps (as illustrated on the left in Figure 2). More advanced methods, like tree or graph of thought [5, 80, 88], enhance this process by exploring multiple reasoning paths at each step, which increases reliability. However, most prompting techniques, such as CoT or others, are inherently rigid and cannot learn directly from feedback, preventing them from adapting to optimise their reasoning. Typically, overcoming this limitation requires back-propagation [62], fine-tuning of the LLMs [54], or using reward model to guide the search process [20]. Still, those lack real-time flexibility and learning capabilities from feedback signals during task execution. To remedy the above problems, we propose flexible learning-to-reason paradigm that eliminates the need for back-propagation and fine-tuning in LLMs to enable learning and adaptation. We hypothesise that LLMs possess inherent case-based reasoning capabilities [23], allowing them to generalise to new tasks using positive or negative past experiences. By optimising these experiences, learning and adaptation can occur naturally. We formulate this by introducing structured reasoning, which integrates memory module to dynamically leverage past successes and failures for more adaptive learning (as indicated on the right side of Figure 2). This overcomes the limitations of rigid chain-of-thought (CoT) 3 Agent v1.0 Figure 2. An illustration of structured reasoning contrasts with the standard chain-ofthought (CoT) approach. On the left, vanilla CoT reasoning generates intermediate steps sequentially, where each step directly conditions the next one until the final answer is produced. This process is the same as autoregressive token generation when the action space is defined per token. While more robust methods like tree or graph-based thought processes improve reliability by exploring multiple paths simultaneously, CoTs rigidity limits its ability to optimise the reasoning process through feedback. On the right is our proposed structured reasoning approach, which introduces memory module (as form of RAG or long-context) that optimises against external rewards and feedback. This module dynamically revises stored content based on environmental feedback, allowing the agent to learn and adapt without altering the underlying LLM parameters, maximising performance (both effectiveness and automation) based on specific objectives. methods. This memory module allows the agent to store past experiences and adjust its strategies dynamically without retraining. By reflecting on feedback from the environment, the agent continuously refines its stored knowledge, enabling it to optimise its current tasks and improve its approach to future challenges. As shown on the right in Figure 2, structured reasoning enables agents to adapt based on external feedback or rewards by revising the content in memory. This process facilitates learning and adaptability without altering the LLMs core parameters, making the system more efficient at solving complex data science problems by building on past cases and experiences. With this, LLM agents can restructure themselves internally, actively reason through flexible and intrinsic (internal to the agent) self-reflective functions, and adapt proactively before taking extrinsic actions. Our agent, which we call Agent v1.0, is an extension of our structured reasoning framework [14], but specifically designed to address data science challenges. We introduce new classes of intrinsic functions for Agent v1.0 to automatically set up data science tasks before autonomously solving them. Those solutions are optimally constructed by introducing another set of intrinsic functions that can collaborate with external tools like hyperparameter optimisation algorithms, ensembling methods, computer vision, and natural language processing libraries. We assess Agent v1.0 across multimodal Kaggle competitions. Unlike existing methods, we introduce competitive benchmark based on real-world Kaggle competitions. We built on top of Kaggle, which offers 327K diverse data sets, 1.1M notebooks, over 7.7K pretrained machine learning models, and about 27K competitions. Our benchmark enables di4 Agent v1.0 rect comparisons with human participants on transparent leaderboard and encompasses tasks in computer vision, natural language processing, time series, and tabular data, supporting multiple modalities. We have rigorously tested Agent v1.0 for complete automation and performance. On the automation front, we evaluated how effectively our system could set up tasks directly from Kaggle URL. We measured how well our agent stacked against human competitors in terms of performance. Our system achieved 92.5% accuracy in automating tasks across multiple domains, showcasing its ability to fully automate complex setups. In terms of performance, our method became the first to reach Kaggle Grandmaster level 1, achieving performance equivalent to 6 gold, 3 silver, and 7 bronze medals across multimodal challenges. We also measure Agent v1.0s Elo-MMR rating against human participants and demonstrate that it ranks in the top 38% of pool consisting of 5856 competitors. This result showcases that Agent v1.0s skill level lies between the first and third quartiles of scores attained by human Grandmasters."
        },
        {
            "title": "2 Learning to Reason by Experience",
            "content": "Standard protocols for training LLMs rely on fine-tuning [54] and backpropagation [62], which are computationally heavy, memory-intensive and require extensive datasets to achieve meaningful improvements. This process becomes even more cumbersome in data science problem settings due to the slow pace of data collection. Typically, data must be gathered by setting up specific tasks and allowing an agent to model potential solutions. This time-intensive process can take days or weeks to yield single data point, particularly for large-scale data science tasks. Consequently, while theoretically promising, this approach proves challenging to scale for our objectives in this work. Instead, we propose an alternative framework that leverages the agents internal memory alongside an external database, enabling the LLM to adapt dynamically without relying on backpropagation and fine-tuning. To formalise this learning objective, we envision an agent operating within an environment where its goal is to maximise returns through optimal decision-making, best defined as Markov decision process (MDP) [70] = Senv, A, P, R, γ. Here, Senv represents the environments state space, our agents action space, : Senv Senv [0, 1] the environments state transition, : Senv reward function measuring the learners behaviour and γ [0, 1) discount factor used to trade-off immediate versus future returns. The agents actions directly influence the environment, causing state transitions based on its choices. Before deciding on action at step h, the agent has access to three key sources Senv, working memory (its of information: the current state of the environment, internal memory that retains information for immediate and short-term decisions), Mh , and . Contrary to standard sequential decisionthe current status of an external database Dh making, we allow the agent to structure its reasoning process by altering Mh before deciding on an extrinsic action to execute in the environment. Adding this flexibility generalises standard notions from reinforcement learning and enables scalability to high-dimensional (env) 1We added the Grandmaster level to highlight our agents success across the community, playground, and featured competitions. For fair comparison with Kaggle users, we followed Kaggles ranking system, awarding medals to human participants if needed and not already acquired; see Section 5. Agent v1.0 state and action token spaces [14]. Moreover, we allow it to actively modify the external database Dh with information gathered from interacting with the environment. This way, acts like long-term memory database that can be consulted to improve decisions over Dh time. (D) Hence, our agent can perform three actions within this framework. First, it can perform to manage the content of its external database, adding or long-term memory actions removing entries to support learning by allowing past experiences to guide current decithat updates its working memory sions. Second, it executes internal/intrinsic actions to shape its reasoning process. These actions involve generating intermediate thoughts, Mh (D) retrieving specific tool outputs or relevant elements from Dh+1 = (Dh) into memory to assist decision-making. Finally, based on the current state and the updated memory content Mh+1 = that directly interacts with the environment, yielding reward and state transition. Such action can modify the code workspace, generate setup code for data science tasks, or submit results to an external platform like Kaggle. (M) (Mh) , the agent takes an extrinsic action (ext) (M) Upon observing state , our agents action flow is captured by: (cid:16) (ext) , (M) , (D) πLLM(a (ext) (env) , Mh+1)µ(a (M) , (D) (env) , Mh, Dh), (1) (env) (cid:17) with πLLM() being an LLM policy that selects an extrinsic action conditioned on the current (M) environmental state and the content of the updated memory Mh+1 = (Mh). This memory update depends on (potentially nested) intrinsic policyµ(). The role of µ() is twofold: it shapes the agents intrinsic reasoning structure using from Equation (1) while updating the agents long-term memory Dt through to enhance the likelihood of maximising performance. We separate µ() in two types, those that alter Mh and those affecting Dh and write: (D) (M) µ(a (M) , (D) (env) , Mh, Dh) µM(a (M) (env) , Mh, Dh+1)µD(a (D) (env) , Mh, Dh), (D) (Dh) such that with Dh+1 = memory of the agent, i.e., Mh+1 = as intrinsic functions. (D) µD(). Since (M) µM() updates the internal working (M) (Mh), affecting its reasoning structure, we refer to those Using this policy, our agent interacts with the environment for horizon of H, collecting trajectory τ and acquiring returns as dictated by the MDPs reward function R. Its goal is then to find πLLM(),µM() andµD() to maximise the following: max πLLM(),µM(),µD() τp(τ) γhR (cid:16) (env) s , (ext) (cid:35) (cid:17) . (cid:34) (2) In what comes next, we allow LLMs to directly maximise the return above by defining data science-specific intrinsic functions and enabling learning-from-memory to adapt without requiring backpropagation in the parameters of πLLM() or those inµM() andµD(). Before doing so, we note that each trajectory τ in Equation (2) comprises all the information our agent acquired following πLLM(),µM(), and µD(), including the state of the Agent v1.0 environment, the working memory and the external database: (cid:16) τ = D0, M0, (env) , (D) 0 , (M) 0 , (ext) , . . . , DH, MH, (env) , (D) , (M) , (ext) (cid:17) . Given the above, we can factor the trajectory distribution p(τ) using chain rule of probabilities, yielding: p(τ) = νD(D0)νM(M0)νs(s0) H1 h=0 (cid:16) (env) h+1 (env) , (ext) (cid:17) πLLM (cid:16) (ext) s (env) , Mh+1 (cid:17) (3) (M) (env) s , Mh, Dh) µM(a (cid:124) (cid:125) (cid:123)(cid:122) Intrinsic Functions and Long-Term Memory Functions , Mh, Dh+1)µD(a , (D) (env) with νD(), νM(), and νs() being initial distributions for the external database, the working memory, and the initial state. This work assumes that both memories start from scratch or can be pre-populated from other tasks or human intervention, making νD() and νM() Dirac distributions. Learning and Adaptation from External Memory: Rather than relying on traditional backpropagation which requires vast amounts of data, computational resources, and memory we propose novel memory-based approach to updating the policies guiding to solve the problem in Equation (2). The parameters of the LLMs core policy, πLLM(), remain fixed in this approach, but we design specialised intrinsic functions (see Section 3.1 and Section 3.2) that enhance the models internal reasoning processes. These functions allow the LLM to adapt its reasoning patterns based on the unique characteristics of data science tasks, tailoring its reasoning to the problems requirements. Externally, we implement memory management strategy guided by reward-based policyµD(). This policy selectively collects the content of the long-term memory database, ensuring that only data, code snippets, and reasoning structures that demonstrate high rewards are stored over time. When the agent encounters new information or generates new code, the memory policy evaluates its effectiveness in terms of long-term utility. This evaluation is ideally performed by critic assessing the potential success of the code. Our first version executes the code and uses unit tests to evaluate those rewards. If the code passes the test, we consider it to have good value and increase its likelihood of being added to the database."
        },
        {
            "title": "3 Autonomous Data Science Agents",
            "content": "Based on the formulation above, we aim to build highly scalable and autonomous data science agent that can handle and solve multimodal tasks belonging to = {T1, . . . , Tn} where is the total number of available data science problems. In its first version, Agent handles tabular, time series, computer vision, natural language processing, and multimodal tasks. We represent each task as the following tuple Ti = denoting the natural language details of Descriptioni, Raw-Datai with Descriptioni its available datasets. Agent v1.0 commences from set of tasks Ti by automatically fetching and scraping Kaggle using their URLs; see Section 4 for more detailed exposure of the benchmark. Next, we focus on automatically setting up one task and Raw-Datai 7 Agent v1.0 Figure 3. This figure depicts the overall data science pipeline that Agent v1.0 needs to automatically design, code and execute. Our agent starts from Kaggle URL and generates complex codes to perform data cleaning, feature engineering, model creation, and optimised training before automatically generating submission file and deciding to submit it to Kaggle to retrieve its score. (Section 3.1) before presenting the solution generation process in Section 3.2. We then generalise our formulation to multi-task domains in Section 3.3."
        },
        {
            "title": "3.1 Phase I (Automation) - Setting Up Data Science Tasks\nCreating valid code for the setup phase is unsurprisingly tricky. Some consider it the most\ntedious step data scientists must complete due to the vast variability in data science prob-\nlems across domains. Data science tasks rarely follow a clean and standardised format, each\nnecessitating custom and potentially intricate preprocessing. For instance, while many\ntabular tasks on Kaggle are neatly structured and adhere to standard evaluation metrics,\ntasks involving image or textual inputs often have raw data organised in nested subfolders\nwith diverse structures and domain-specific evaluation protocols.",
            "content": "Figure 4. This figure depicts the main steps of the automatic setup stage. After scraping task, this policy operates in two phases. First, it writes series of valid codes to automatically set up train and test data loaders per modality. Each phase step is guided by unit tests that an LLM can use to reflect and generate better codes. Upon completion, we introduce joint set of unit tests that allow our agent to perform cross-step reflection for credit assignment. With those complete, Agent v1.0 executes the second phase of formatted data generation, where submission format and metric function codes are generated. By the end of this phase, the task is set up and ready to be solved, which we check with final unit test. 8 Agent v1.0 Agent v1.0 tackles the automatic setup by generating code and text to restructure raw data into well-formatted dataset and metadata compatible with our solver policy that we describe in Section 3.2. As illustrated in Figure 4, we decompose the setup process into distinct stages. 3.1.1 Setup Pipeline The pipeline begins by extracting raw data and task descriptions, summarising, and planning. The plan depends on what the agent identifies as the nature of the task and as the input and target modalities, which must be inferred from the raw data. In Figure 4, we outline the different stages of the setup pipeline with blue boxes, omitting duplication across different data modalities. For instance, Training Input Map encompasses building maps for tabular, image, and textual inputs, depending on the modalities. The input maps are standardised tables designed to handle the diverse data types and folder structures in Kaggle competitions. The maps act as structured guides, with rows that gather the correct inputs and input types from various files and align them with their corresponding targets. The agent automates this process and aims to pass series of unit tests that we detail in Appendix A.1.3. After creating the plan, Agent v1.0 tackles each relevant stage, whose completion is assessed through stage-specific unit tests, determining the progression to the next stage. Additionally, meta-unit-tests evaluate the consistency of outputs across multiple stages, such as the Data loader Unit Tests. In the figure, we display green boxes for the stagespecific unit tests and the meta-unit tests. If the final unit test passes, we consider the task adequately formatted. This final unit test runs light version of the solution generation process, ensuring that the data loaders prepared by our setup agent are compatible with the following solution generation pipeline. The Setup MDP: We instantiate the MDP described in Section 2 to model the automatic setup process of data science tasks. We define the MDP as follows: Msetup = Ssetup, Asetup, Psetup, Rsetup, γ, where Ssetup is the state space encompassing information about the current stage and the workspace content, Asetup is the action space, Psetup : Ssetup Asetup Ssetup [0, 1] is the transition probability function, Rsetup : Ssetup Asetup Rsetup is reward function taking as input state and an action, γ [0, 1) is discount factor, and is the time horizon or maximum number of steps. The state space is the Cartesian product Usetup of the discrete set of pipeline stages Usetup = {u(1), . . . , u(K)}, and the set of possible workspace contents W. At each step h, given state sh = (uh, wh) Usetup W, the environment expects an action ah whose nature is stage-specific and can correspond to the generation of summary in natural language, the creation of new file, etc. Therefore, the action space Asetup is union of stage-dependent action spaces Asetup(u). Our policy, πsetup, only assigns weights to actions within the current stages action space Asetup(u). If unit tests follow stage, the reward function assigns penalties or bonuses based on the tests outcomes. Failing to set up task leads to an overall negative sum of decayed rewards. Moreover, error logs from unit tests are stored in the workspace, enabling the agent to use this feedback to refine its actions. 9 Agent v1.0 Setup Transition Dynamics: To continue, we formalise the transition dynamics our agent encounters between stages. Image that we have the following sequence of ordered stages Usetup = (u(1), . . . , u(K)) to handle. As the first stages tackle task analysis and raw data summarisation, they arrive with no unit test. As such, for those stages, we have: for 0 < k. At the end of the kth step, the input and output (uh, modalities are determined and induce skipping some setup stage, leaving us with the remaining subset to handle: (v(1), . . . , v(L)) Usetup. , u) = δu=uh+ (ext) Each of those stages is associated with unit tests test(1), . . . , test(L). Moreover, we assume the existence of meta-unit-tests {meta-test(1), . . . , meta-test(M)}. is responsible for checking the consistency of several stages. Each meta-test(m) (v(1), . . . , v(L)) the ordered list of stages We denote by V(meta-test(m)) covered by the meta-test meta-test(m). Finally, we denote by (cid:101)V(v(ℓ)) = meta-test(m) s.t. V(meta-test(m))[1] = v(ℓ)(cid:111) (cid:110) the subset of meta-unit tests that cover v(ℓ) as their last stage. Here, we used the notation z[i] to refer to the ith element of tuple z, and z[i] to its ith element starting from the end. This notation allows us to describe the stage transition dynamics as follows. At stage (ext) v(ℓh) (with ℓh k), if the action leads to failure of test(ℓh), the stage remains the (ext) , v) = δv=v(ℓh). On the other hand, if the unit test test(ℓh) same at the next step: T(v(ℓh), passes, then the meta-unit-tests in (cid:101)V(v(ℓh)) are executed. If all of them pass (or if the set is , v) = δv=v(ℓh+1). Finally, if one of empty), we move on to the next stage, whereby T(v(ℓh), the meta-unit tests, meta-test(m) (cid:101)V(v(ℓh)) fails, then we transition to the earliest stage covered such that T(v(ℓh), , v) = δv=V(meta-test(m))[1] (ext) (ext) . 3.1.2 Action Generation through Structural Reasoning While standard RL techniques could theoretically address the MDP outlined above, the high dimensionality of state and action spaces presents significant challenges to directly applying state-of-the-art RL methods. Specifically, the actions involve generating extensive sequences of tokens to produce code, rewards are exceedingly sparse since it is challenging to attribute unit test failures to specific code segments, and policy adaptation necessitates backpropagation and differentiation through large-scale policy models. To navigate this complex environment, we extend our Pangu-Agent framework [14] to encompass data science scenarios, as in Section 2. This framework empowers agents to structure their internal reasoning processes by incorporating intrinsic functions alongside the standard extrinsic policies that interact with external environments. We generically re- (u) fer toµ setup as the intrinsic function that Agent v1.0 uses to take action when being at stage u. Therefore, we express the automated data science setup policy πsetup as: πsetup(a (ext) , (M) , (D) wh, uh, Mh, Dh) = πLLM(a (ext) wh, uh, Mh+1)µ (uh) setup(a (M) , (D) wh, uh, Mh, Dh) where Mh+1 = (M) (Mh). Moreover, we can adapt Equation (2) to turn task setup automation 10 into the following maximisation problem: Agent v1.0 argmax (cid:110) (u) µ setup (cid:111) uUsetup πLLM, τpsetup(τ) γhRsetup(sh, (cid:35) , (ext) ) (cid:34) h=0 (4) where s0 designates the initial state of task, i.e., its natural language description. Rewards in Equation (4) are obtained by executing the generated code and recording their pass or fail outcomes through unit tests. As shown in Figure 4, we implemented per-stage and crossstage unit tests to assess the success of each step and the overall generated setup code. Importantly, our unit tests are not specific to each task, which would defy the purpose of having an automated setup but cover classes of data science domains, as we elaborate in Appendix A.1.3. To solve the above problem, we follow the same procedure described in Section 2, where we write novel intrinsic functions and allow the LLMs to adapt without backpropagation from memory to maximise setup rewards. 3.1.3 Tackling Credit Assignment with Nested Reasoning Credit assignment (CA) is another crucial component in solving the above problem the process by which πsetup determines which parts of the generated code should be rewritten to maximise the likelihood of passing meta-unit tests. Standard RL performs this analysis by training state critics, which is dataand compute-intensive process. While building those critics is valuable direction for future work, this paper enables credit assignment directly from LLM agents through structured reasoning after encountering meta-unit test failure. Our strategy can be intuitively described as follows: when Agent v1.0 encounters metaunit-test failure, we prompt an LLM to generate thought explaining the potential cause of the error. Armed with this insight, the agent revisits earlier stages of the process and, for each stage, it re-executes the step incorporating the generated thought and the previously written code into its prompt. If the agent suspects the code from that stage contributed to the failure, it will attempt to create new code. If it believes the code is correct, it simply regenerates the same code and moves on to the next potentially faulty stage. This cycle continues, stage by stage until the faulty stage is identified and the meta-unit-test is finally passed. Credit Assignment Intrinsic Functions: Credit assignment occurs after meta-unit test meta-test(m) breaks at step 1, leaving an error message stored in the workspace and . The cause of the error is shared among the actions taken at each stage available in wh of V(meta-test(m)). However, not all actions during these stages are necessarily wrong. Repeating all steps without considering previously successful actions would be inefficient, especially if only single step was the root cause of the failure. Instead, we allow the agent to reflect on which parts to retain and which to correct by analysing the previous steps and the meta-error. As explained when defining the transition dynamics, the stage following the (ext) meta-error, uh = v(ih), corresponds to V(meta-test(m))[1]. To generate the action , for the sake of simplicity: we introduce the following structure for Agent v1.0, omitting Dh Mh,1 = aM h,1(wh, Mh) = Mh {META-ERROR-THOUGHT : F1(LLM(ϕ1(wh, Mh)))} aM with h,1 µ (wh, Mh), (M, uh) 11 Agent v1.0 (M, uh) 1 is deterministic intrinsic policy yielding action aM h,1 where µ LLM to comment on the meta-unit test based on the task descriptions (present in wh prompt ϕ1 , and ii) adds the response parsed with . This action i) queries an ) and Following this, second intrinsic action builds on this reflection by adding another critical layer to the memory: credit-assignment (CA) thought, i.e. an evaluation from the LLM on whether the previous action taken at this stage, stored in the memory as PREVIOUS-CODE, might have contributed to the meta-error: to the memory. Mh,2 = aM h,2(Mh,1) = Mh,1 {CA-THOUGHT : with aM h,2 µ F2(LLM(ϕ2(wh, Mh,1[PREVIOUS-CODE, META-ERROR-THOUGHT])))} (M, uh) 2 (wh, Mh,1), (M, uh) 2 deterministically generates action aM h,2 . This second intrinsic process uses anwhere µ other prompting function ϕ2 taking into account the code and another parsing function F2 previously generated at this stage and stored in the memory, as well as the meta-error message and the previous LLM analysis, Mh,1[META-ERROR-THOUGHT]. Based on this assessment, the LLM then produces code, and Python tool checks its formatting such that: Mh,3 = aM Mh,4 = aM with h,3(Mh,2) = Mh,2 {CODE : LLM(ϕ3(wh, Mh,2[CA-THOUGHT, PREVIOUS-CODE, META-ERROR-THOUGHT]))} h,4(Mh,3) = Mh,3 {CODE-OUTPUT : Python(Mh,3[CODE])} aM h,3 µ (M, uh) 3 (wh, Mh,2), aM h,4 µ (M, uh) 4 (wh, Mh,3) where we denoted by Python function tool parsing the code output by the LLM, executing it, returning whatever is printed in the console, and storing it in the memory. If the code does not trigger further reflections, we can write the intrinsic reasoning that updating the short-term memory to prepare the next external action as: Mh+1 = Mh,4 = aM h,4(Mh,3) = aM = aM = aM h,4(aM h,4(aM h,4(aM h,3(Mh,2)) h,3(aM h,3(aM h,2(Mh,1))) h,2(ah,1(Mh)))) = aM (Mh), where aM µ (M, uh) setup (wh, Mh)."
        },
        {
            "title": "3.2 Phase II (Optimisation) - Solving Data Science Tasks\nOnce πsetup successfully sets up a task t, notably constructing valid train and test data load-\ners for each input and target modality, the role of the policy πsolve is to generate code that\ncan involve feature engineering and creating and training machine learning models to max-\nimise task-specific performance metrics, which have been automatically coded by Agent K\nv1.0 during the task’s setup phase. Additionally, πsolve(·) populates submission files with\ntarget predictions. To get a fair comparison, when running experiments on Kaggle compe-\ntitions, we let our agent upload its submission files to observe the public score and measure\nthe final performance on a leaderboard, as we detail in Section 4.",
            "content": "12 Agent v1.0 This part of the data science pipeline typically involves numerous steps, including generating scripts to preprocess the inputs, create various models, or optimise hyperparameters. Since several attempts can be made, and since each solution is the result of composite pipeline, achieving very competitive performance requires the ability to reason about the strengths and weaknesses of each solution component and make decisions about which part to modify or what to try next. In our case, we leave these decisions to our agent through its extrinsic actions and empower it with several tools tailored to the design of data-driven predictions. Similar to the setup phase, we capture the process of generating an optimal solution as an MDP, Msolve = Ssolve, Asolve, Psolve, Rsolve, γ, H, and we endow πsolve with memory to enable structural reasoning through the use of intrinsic functions and tools specific to this challenging process. The state space is also product of the space of workspace contents and of set of solution generation pipeline stages Usolve. The reward function Rsolve is sparse as it only yields feedback at the end of the episode, corresponding to the score achieved by the final submission on held-out leaderboard. To let Agent v1.0 generate complex solutions based on the setup of t, we notably allow our agent to decide whether to create deep-learning predictive models, to use stateof-the-art hyperparameter optimisation tools for their training, to select which predictions to combine into more robust blended submission, to perform tabular feature engineering, or to adapt the setup task to produce submissions using tabular AutoML framework. Equipped with this set of tools and options, each of our agents choices and code generation can involve intricate and nested decision processes that we express through intrinsic policies , such that: (cid:110) (cid:111) µ (u) solve uUsolve πsolve(a (ext) , (M) , (D) wh, uh, Mh, Dh) = πLLM(a (ext) wh, uh, Mh+1)µ (uh) solve(a (M) , (D) wh, uh, Mh, Dh) with Mh+1 = (M) (Mh), from which we formulate the maximisation problem: argmax (cid:111) (cid:110) (u) µ solve πLLM, uU solve τpsolve(τ) γhRsolve(sh, (cid:35) . (ext) ) (cid:34) h= (5) This time, s0 loaders, metric script, etc. designates the state associated with the setup pipeline output, such as data When solving Equation (5), we again allow LLMs to adapt via long-term memory. Moreover, we introduce tool-based intrinsic functions (e.g., Bayesian optimisation) to aid Agent v1.0 design specific machine learning pipelines in an attempt to maximise rewards. 3.2.1 Solution generation process Modality-based approach: The solution generation process is tailored to the modalities identified during task setup. Not all tools and actions are applicable depending on the nature of the task, as customised approach is required. AutoML tools can offer competitive solutions for fully tabular tasks due to the extensive attention they have received in the field over the past decades. However, they often lack flexibility and require careful data formatting, where Agent v1.0 can excel. To address these challenges, we developed specialised Agent v1.0 tool that enables the agent to autonomously manage tabular submission generation. The agent leverages deep neural networks for competitions involving computer vision, natural language processing, or multimodal data, implemented and trained using PyTorch library [55]. In such cases, we provide our agent with dedicated tools to handle these modalities as we detail next. AutoML Tool for Tabular Submissions: AutoML libraries aim to automatically produce predictions by executing routines that apply standard ML models, hyperparameter tuning, column encoding, imputing, and blending techniques. The main challenge is ensuring that the data is correctly formatted, requiring the creation of specific metadata and providing the appropriate arguments to the AutoML library. Our agent autonomously handles this through specialised tool and reasoning process that bridges the gap between our task setup and the specific input format required by the AutoML library. This bridge notably consists in analysing and providing information regarding column types, determining the positive class of binary classification problem, or wrapping the metric function to make it compatible with the library. Once the data preparation is complete, the agent uses our in-house AutoML library built on RAMP [40], which generates predictions through racebased routine. Late-Fusion Model Generation When our agent adopts deep-NN modelling, the environment guides it in producing late-fusion architecture model. This approach divides the problem into the design of individual modality-specific embedders and separate neural network heads for each target. The prompts invite the agent to integrate and customise pretrained models from the Torchvision and Torchtext libraries, enabling it to use robust pretrained networks. At each code generation stage, Agent v1.0 outlines plan before implementing each model component. Upon completion, our agent comprehensively describes the resulting late-fusion architecture. By design, the generated model is differentiable end-to-end, facilitating optimisation through gradient descent with loss functions selected by the agent based on the competitions evaluation metric. Once training concludes, submission file is generated using the submission formatter created during the setup phase. Bayesian optimisation tool We provide the agent with the HEBO [15] tool to further optimise the models performance. This tool wraps the training code for the late-fusion model and automatically tunes critical hyperparameters, such as the learning rate and optimiser settings, using HEBO, which is state-of-the-art Bayesian optimisation library. The bestperforming model is selected based on validation loss, ensuring optimal performance for the final submission. Blending tool Once several models have been trained, our agent can decide to combine some of their predictions by calling dedicated tool. Based on the performance and description of each model, the agent selects subset of models to blend. This tool gathers predictions (such as logits for classification tasks) from the chosen models and applies small multi-layer perceptron (MLP). The MLP is trained to predict the final targets by leveraging the strengths and diversity of the individual models. This process produces new 14 Agent v1.0 submission file that has chance to improve the overall performance provided that it does not lead to overfitting. Submission After generating the submission files, the agent can make submissions to the Kaggle competition webpage using the dedicated Kaggle API tool. We provide details in Appendix A.4 on how we factor the different submission scores into final performance measure for our agent."
        },
        {
            "title": "3.3 Phase III (Generalisation) - Multi-Task and Active Task Selection\nThe methods detailed above only consider one data science task at a time. However, effec-\ntive agents must be able to tackle multiple tasks with varying domains. This section starts by\nequipping Agent K v1.0 with cross-domain set-up and solving capabilities by generalising\nthe previous formulations to a multi-task setting. Noticing the multi-task solution method-\nology’s infeasibility, we then present an online continual learning extension.",
            "content": "Data Science Agents as Multi-Task Learners: Considering the whole set of multimodal tasks from = {T1, . . . , Tn}, we rewrite the setup optimisation problems as: argmax (u) setup}uUsetup πLLM,{µ 1 j=1 τ(j)p (j) setup(τ) γhRsetup(s (j) , (ext, j) ) (cid:35) (cid:34) h=0 (Multi-Task Setup), with being the total number of tasks, and the superscript (j) is used to denote the jth tasks (j) states and actions. Moreover, setup matches Equation (3) where the initial state distribu- (j) (j) tion ν deterministically yielding the last state of the long-term memory database at the end of the previous episode, D(j1) . Importantly, sharing the long-term memory across tasks enables knowledge transfer between data science domains. is Dirac with weight on the initial state of the jth task, and ν Like the setup phase, we define multi-task version of the solution phase such that: argmax (u) solve}uUsolve πLLM,{µ 1 j=1 τ(j)p (j) solve(τ) (cid:34) h=0 γhRsolve(s (j) , (ext, j) ) (cid:35) (Multi-Task Solution). Again, we leave the intrinsic functions, and the long-term memory shared to allow for knowledge transfer when solving cross-domain tasks. Continual Learning and Active Selection From Long-Term Memory: Data science problems are inherently dynamic and non-stationary, especially as the number and diversity of tasks grow. As such, Agent v1.0 must actively and effectively learn from past experiences stored in the long-term memory to make informed decisions about future tasks. By utilising this memory, our agent can build curriculum that prioritises specific tasks to improve the likelihood of success while acquiring new information to expand its knowledge base. Interestingly, trading off exploration and exploitation in task selection improves the agents learning efficiency and offers computational advantages. By focusing on tasks with higher expected returns, the agent reduces the number of failed attempts and conserves resources. 15 Agent v1. Figure 5. The overall design of the Agent v1.0 agent. Agent v1.0 is multimodal continual data science agent that operates across rounds. Following Kaggles setup, Agent v1.0 only requires natural language descriptions and non-standardised raw data for task to be successful. At each round j, it actively selects data science task to solve based on the history of previously attempted tasks, which it populates in time-varying vector database RAG and pool of remaining tasks. Having selected task, Agent v1.0 then automatically standardises, cleans the tasks data, and prepares training and test metrics using πsetup(), which is also . Once the setup is completed, Agent v1.0 attempts to solve the task conditioned on RAG by accessing tools and refining its solution based on score feedback. Those Python logs update RAG . This selection is executed through the policy π1() conditioned on RAG where the process repeats. j To enable proper utilisation of long-term memory, we frame data science agents as online continual learners with active selection capabilities who improve based on encountered experiences. This online setup allows for scalability as tasks get observed sequentially and eases learnability since agents actively design learning curricula. Thus, we design our agent to operate in discrete rounds, indexed by j. In each round, Agent v1.0 actively selects task based on prior successful and unsuccessful experiences, which are logged in two corretij sponding vector databases RAG+ that are continuously updated as the rounds progress. We let RAG denote the union of the two retrieval databases containing the traces of the successful and failed experiments. The task selection is governed by an LLM policy π1() that we formalise as follows: and RAG tij π (cid:0)RAG j, Pool (cid:1) , with Pool denoting the remaining tasks to solve at round j. Common practice dictates choosing tij . While selecting tij that is most similar to those in RAG+ and dissimilar to RAG based on similarity with the successfully and unsuccessfully processed ones aids in improving sample efficiency (through RAG-based from Pool 16 generation), it does not directly favour ordered curricula with increasing complexity from simple to more complex data science problems, which can lead to suboptimal intra-round knowledge transfer. Agent v1.0 (resp. j Rmjk+ To build effective curricula maximising knowledge transfer between rounds, our agent follows policy π1() that trades off task similarities versus their difficulties. To do so, we build two kinds of matrices, for similarity and difficulty. Imagine that Pool has mj < re- ) successfully (resp. unsuccessfully) processed maining tasks and RAG includes k+ tasks. The similarity matrix, Sim+ ness between task descriptions in Pool ). We compute each element in Sim+ and Sim by embedding task descriptions through an LLM and computing their cosine similarities. Furthermore, for each remaining task ti we can extract positive and negative similarity score by applying an operator opsim to the is row of Sim+ and the alj ready observed tasks. We can choose this operator to return the average of the top-r highest similarity scores, that aggregates the different similarity measures between ti ), measures the close- (resp. in RAG versus those observed in RAG+ Rmjk (resp. Sim and Sim j (cid:16) top (s1, . . . , skj ) (cid:17) = 1 u=1 s[u], with s[1] s[2] s[kj] (6) where the choice trades the number of similar tasks to consider and how similar they are to ti . Applying the operator opsim row-wise, we obtain vector of positive and negative similarity scores. To calculate the difficulty matrices Diff+/ , we first need clear definition of the data science tasks inherent difficulty. Ideally, we would run each task, setting it up and then solving it to assess its difficulty, but this is impractical as it assumes that we could solve the remaining tasks before estimating their difficulty. We instead introduce simple surrogate based on available meta-data to measure data science task difficulties. Our surrogate is motivated by empirical findings that more complex problem structures (e.g., complex organisational trees, data sizes, and the nesting of folders in directories) increase the number of reflection steps state-of-the-art open-source LLMs need to correctly set up and solve problem. We compute the elements in Diff+/ . Specifj ically, we pass natural language description of tasks tree to an LLM and compute its . As we do with the similarity matrix to get similarcosine similarity to those from Pool ity score vector, we also aggregate each row of Diff+/ into single difficulty score per task by applying an operator opdiff , and opdiff to attempt using: chooses task tij similarly to those from Sim+/ . Given Sim+/ , Diff+/ , our agent , opsim j ij argmax i{1,...,mj} (cid:16) opsim Sim+ (RAG+ , Pool (cid:17) j)i + α opdiff (cid:16) Diff+ (RAG+ , Pool (cid:17) j)i (7) (cid:16) opsim Sim (RAG , Pool (cid:17) j)i α opdiff (cid:16) Diff (RAG , Pool (cid:17) j)i βeλi,j, where α R+ denotes parameter that trades off similarity versus difficulty, and the term βeλi,j, with β R+ and λi,j was last chosen, penalises the selection of task that was recently attempted with no success. corresponding to the number of steps since task ti 17 Agent v1. In words, solving the problem in Equation (7) allows our agent to build an increasingly from the remaining pool that is simultaneously simdifficult curriculum by finding task tij ilar (resp. dissimilar) to those in RAG+ (resp. RAG+ ) and not drastically more challenging than the successful cases. Therefore, if we start with reasonably simple task, this policy should provide selection of tasks with gradual difficulty. Note that selected task tij remains in the pool of available tasks Pool after failed experience such that it can still be selected later, but the βeλi,j factor discourages reattempting task if the RAG database does not include new successful cases since the previous trial.. If this task is finally successfully handled, we remove the entries corresponding to the previous attempts from the negative retrieval database, as similar tasks should no longer be penalised by the selection policy. Note that for the initial task, both Sim+/ are empty, and therefore, we prompt an LLM to pick the first task to tackle. In the prompt, we provide the natural language description of each tasks structure tree and let the LLM choose the task that looks the most accessible to solve to initialise the curriculum with simple problem. and Diff+/ 1 1 j"
        },
        {
            "title": "4 A Competitive Data Science Benchmark",
            "content": "As we show in Section 6, current data science agents lack competitive benchmarking, which limits their performance to specialised domains and shows improvements to relatively simple baselines. Our work further contributes by introducing diverse and competitive data science benchmark we built on top of Kaggle competitions as we elaborate next."
        },
        {
            "title": "4.1 Data Science Tasks and Modalities\nWe built our benchmark to cover diverse tasks with varying difficulties from Kaggle so\nthat we can truthfully assess the quality of Agent K v1.0. We chose tasks covering var-\nious modalities while adhering to a high average number of participants; see Figure 6.\nOur tasks included tabular, computer vi-\nsion, natural language processing and mul-\ntimodal ones, as shown in Figure5. We en-\nsured that the tabular tasks were varied to\ninclude numerical, categorical, strings, and\nBoolean input features. In computer vision\ndomains, we dealt with standard image in-\nputs, while in natural language, we consid-\nered tasks that required semantic under-\nstanding. Additionally, we included mul-\ntimodal tasks from both the computer vi-\nsion and natural language processing do-\nmains. These multimodal tasks involve a\nmix of standard input types (e.g., images\nin computer vision and text in natural lan-\nguage processing) and tabular features. In the first version, the supported problems fo-\ncused on classification and regression. We further improved our benchmark’s generality\nand considered data science problems with varying submission types, making it challeng-",
            "content": "Figure 6. This figure depicts the average number of participants per domain we considered when building our benchmark. 18 Agent v1.0 Figure 7. The above figure shows pie chart depicting the distribution of Kaggle tasks in our benchmark. We developed our agent to handle tabular, computer vision, natural language processing, and multimodal tasks. In our current version, tabular tasks constitute 55% of the total. Those include features such as numerical, categorical, strings, and Boolean. Computer vision problems represent 24% of the total, natural language processing 10%, and multimodal tasks 11%. ing for the agent to overfit specific format, e.g., formats involving CSV files with varying IDs and predicted columns. Non-standard evaluation metrics, which typically require human expertise, add to the complexity of real-world data science problems. Being used in real-world scenarios, Kaggle also allows for standard (e.g., mean-squared errors, classification losses) and domainspecific metrics that require substantial reasoning effort by data scientists before successfully solving the specified task. Our benchmark included both and ensured the introduction of Kaggle tasks that required non-standard evaluation metrics, e.g. mean F-scores for plant seedling classification, mean average precision at 5 for whale identification, among others listed on Figure 15a. Given the above, we built automation and performance tests that assessed our Agent v1.0s capabilities as we elaborate next. Automatic Setup Tests: The automatic setup benchmark assesses Agent v1.0s abilities to automatically prepare task tij before solving it; see Section 3.1 for more details. Here, we introduce two performance metrics we evaluate in Section A.1.1. Those include the following: 1) Per-Stage Success rates that measure unit test success per every stage of the automation process (Figure 4), 2) Overall Pass @k rates measuring cross stages unit test success. We adopt standard metrics that have been previously introduced in literature [10], ensuring that we do not wrongly bias our results. When it comes to Per-Stage Success Rates, we consider the available pool of tasks and study if our agent passes each stages unit test given predefined number of seeds. Following [10] and given set of tasks, 19 Agent v1.0 Table 1. table showing our benchmarks supported modalities and task types. Data Modality Task Type Modality Input Target Type Supported Tabular Vision Language Tabular + Vision Tabular + Language Tabular + Vision + Language Regression Classification Time Series Image Generation Text Generation we then compute Overall Pass @k as follows: Overall Pass @k = Etasks 1 (cid:34) (cid:35) , (nc ) (n k) with being the total number of solutions the model generated, being the number of correct solutions, being the number of solutions we are considering for our evaluation, and b) denoting the binomial coefficient defined as (a (a b) = a! Finally, to evaluate the correctness of the detected modalities by the LLM, we first extract the ground truth modalities of the available tasks ground-truth and measure if Agent Ks prediction aligns, i.e., 1 b!(ab)! . . {predicted-modality=ground-truth} Performance Benchmark: We measured our agents performance using standard metrics without introducing unwanted biases. When our agent automatically submits, we evaluate its performance simply by submitting it to Kaggle and acquiring public and private scores. The final submission is based on the best public scores and the final rank correspond to the corresponding private scores, as we detail in Appendix A.4. We measure Agent Ks performance quantiles against participants and follow Kaggles progression system to determine medals; see Table 2. Kaggle grandmaster level is offered to competitors who demonstrated outstanding performance, evaluated by acquiring medals across competitions, datasets, notebooks, or discussions. This paper assesses the level of our agent based on the competition track where the grandmaster title is granted by winning at least win 5 gold medals. While being grandmaster level is an impressive feat for fully autonomous agent, we also wanted to ensure that Agent v1.0 does not overfit to small subset of tasks and achieves impressive performance across the board. Therefore, we also computed its EloMMR rating [19] as detailed in Section 5.1.1. This measure is considered state-of-the-art as it aligns incentives such that rate-maximising player always aims to perform well, and is robust to rating changes for consistent versus volatile players. 20 Agent v1.0 Table 2. This table depicts Kaggles progression system following Kaggles guidelines and style. As noted on the original website, the (Top 10 + 0.2 %) means an extra gold medal will be awarded for every 500 additional teams in the competition. For example, competition with 500 teams will award gold medals to the top 11 teams, and competition with 5000 teams will award gold medals to the top 20 teams. Medal Bronze Silver Gold 0-99 Teams 100-249 Teams 250-999 Teams Top 40% Top 20% Top 10% Top 40% Top 20% Top 10 Top 100 Top 50 1000+ Teams Top 10% Top 5% Top 10 + 0.2%* Top 10 + 0.2%*"
        },
        {
            "title": "5 Experiments",
            "content": "This section provides an in-depth analysis demonstrating the results of our proposed agent. We present results regarding automatic task setup and performance after automatically submitting Agent Ks solution to Kaggle. First, we present performance results showing all the competitions in which our agent participated. Second, we discuss automation results that signify the importance of our proposed experience-based learning from memory when setting up data science tasks."
        },
        {
            "title": "5.1 Kaggle Grandmaster Level Results\nWe tested the performance of Agent K v1.0 across 65 competitions from Kaggle that our\nagent could autonomously setup and for which it could generate at least one submission.\nContrary to the work in [9], our tests followed the standard Kaggle’s competition guidelines,\nin which our agent creates a submission file and uses Kaggle’s API to automatically submit\nits solution. Agent K’s solution is evaluated and ranked on the leaderboard upon submis-\nsion. Its performance is measured in quantiles against the participants. To ensure fairness,\nthose quantiles are based on the private leaderboards when available and only use public\nleaderboard results to decide on the retained submissions, which reflect standard practice\nof data scientists on the Kaggle platform To improve its performance, Agent K uses a val-\nidation loss computed based on an internal split of the training data. This loss and code\nalready available in the agent’s memory help the LLM reflect and produce more successful\ncode, eventually improving its ranking.",
            "content": "In this first version, Agent adapts open-source LLMs to democratise its use. While doing preliminary experiments with various LLMs, we opted for Qwen-2.5 72B [75], the model that demonstrated better code generation in our experiments than others. Regarding tools, we allowed the LLM to select from those described in Section 3.2 based on its current performance, code and workspace status. We ensured that the LLM always selects HEBO at the end of the training stage since hyperparameter optimisation is critical for improving any machine learning models performance. Figure 8 is bar graph summarising those quantile results. The y-axis lists the competitions, and the x-axis reports performance quantiles. Following Kaggles progression system, our results indicate that Agent v1.0 is capable of acquiring an equivalent of 6 gold, 3 silver and 7 bronze medals, which can be regarded as operating at Kaggle grandmaster level. 21 Agent v1.0 Figure 8. bar graph demonstrating Agent v1.0 performance results across various competitions spanning tabular, computer vision, natural language processing and multimodal tasks. On the y-axis, we present the competition ID. On the x-axis, we measure its quantile performance according to Kaggles private leaderboard, i.e., the higher the quantile, the better our agents performance. Our agent acquires 6 gold, 3 silver and 7 bronze medals. We also notice that Agent v1.0 obtains above 80% quantile performance across 22 tasks and 40 above 50% quantile. Agent v1.0 Interestingly, those medals are spread across modalities, signifying the ability of Agent v1.0 to perform well across wide range of data science domains. In terms of gold medals, we acquire: 2 gold in tabular tasks: playground-series-s3e15 with 693 teams and 5,853 submissions, and playground-series-s3e9 that involved 765 teams and 6,998 submissions; 2 gold in computer vision tasks: world-championship-2023-embryo-classification that included 111 teams and 1,372 submissions, and sign-language-imageclassification with 41 teams and 178 submissions; and 2 gold in natural language processing: sentiment-analysis-on-movie-reviews with 860 teams and 6,813 submissions, and sbu-ai-finalproject having 37 teams and 219 submissions. As seen above, our gold medals vary among competitions with many participants (e.g., tabular tasks and sentiment analysis in NLP) and smaller ones. This is not uncommon across Kaggle since computer vision tasks tend to contain large image corpora and thus require heavier computing power that can be hard to acquire for individual competitors. Interestingly, our gold medal in computer vision at the 2023 world championship in embryo classification demonstrates that this success is not solely due to using recent models and libraries. While we employ advanced architectures like CNNs and vision transformers from Torchvision, such performance reflects that Agent v1.0 possesses the ability to go beyond simply leveraging the latest machine learning advancements due to its many tools, including but not limited to feature engineering and state-of-the-art hyperparameter optimisation from HEBO. Figure 8 shows that our agent additionally acquires 3 silver medals. Those are split between tabular task - home-data-for-ml-course with 106,134 submissions - computer vision task - invasive-species-monitoring with 4,239 submissions - and natural language processing competition - nlpsci containing 785 submissions. Finally, in terms of bronze medals, we note that those also ranged across all modalities and included popular tasks, e.g., 10,000 in the dogs versus cats computer vision classification task, and 12,287 in horse health prediction, i.e., the playground-series-s3e22 tabular task. Apart from winning medals, Agent v1.0 also demonstrates solid percentages across the board. It obtains higher than 80% quantile performance across 22 tasks and above 50% quantiles in 62% of competitions we tested on. Of course, there remains set of tasks where we need to improve. Specifically, we achieve lower than 20% performance in 11 tasks. Upon further investigation, we noticed those lapses are attributed to non-converging losses, wrong submission files generated by the LLM during task setup, and class imbalances. Further tool integration that gauges non-convex optimisation and effective and automatic data augmentation can improve our results. Incorporating more tools is an exciting direction for future work and is beyond the scope of the first version of Agent K."
        },
        {
            "title": "5.1.1 Comparison Against Human Participants",
            "content": "Although achieving the performance above places Agent v1.0 ahead of previous work (see Section 6), we aim to further analyse its performance relative to human participants. 23 Agent v1.0 To accomplish this, we identify participants who have competed in at least three of the same competitions, revealing pool of 5,856 competitors. For those participants, we compute their average quantile performance across the relevant tasks and use this as standard for comparison. To assess Agent v1.0s quantile performance, we conduct multiplayer Elo scoring analysis following the approach in [19]. This method is widely adopted in ranking multiplayer competitors and is considered state-of-the-art measure, evidenced by its adoption in large-scale coding platforms like CodeChef. The histogram in Figure 9 summarises those results. The x-axis represents the Elo-MMR scores, while the y-axis shows the number of participants at each score level. The Elo-MMR scores follow an approximately normal distribution, peaking between 1400 and 1500, where most participants are concentrated. yellow dashed line marks our agents Elo-MMR score at 1542, placing it in the 63rd percentile. This means our agent outperforms about 63% of the 5,856 participants in the dataset, ranking within the top 38%. The lower section of Figure 9 presents more detailed view of Elo-MMR distributions for participants grouped by Kaggle levels, from Novice to Grandmaster. At the intersection of the yellow line and these distributions, we see that Agent v1.0s Elo-MMR score falls between the first and third quartiles of the scores achieved by Grandmasters in this cohort. Figure 9. Comparison of Agent v1.0s performance with that of human participants. The graph displays the distribution of Elo-MMR scores among Kaggle users who participated in at least three of the competitions our agent submitted to. Results indicate that Agent v1.0 ranks within the top 38% of this cohort of 5,856 competitors. The bar colours reflect the users official Kaggle levels as of the time of writing (note that 1.8% of users no longer have accounts, so we mark their levels as \"Unknown\"). In the lower portion, the graph shows the Elo-MMR distribution by Kaggle level, illustrating that the score of Agent v1.0 falls between the first and third quartiles for Grandmaster-level users. The previous results demonstrate that Agent v1.0 performs competitively. We aim to assess relative performance improvement against these participants within the same com24 Agent v1.0 petitions. To achieve this, we first identify each participants specific competitions. We then calculate the average rank of Agent in those same competitions, enabling direct comparison. Using this data, we compute percentage improvement relative to each user and summarise our findings in Figure 10. This analysis offers insight into its potential for outperforming human participants across consistent set of tasks. Agent v1.0 demonstrates an improvement over many participants, with notable examples including substantial 200% improvement over users like tracyporter. We also see improvements of over 100% compared to grapestone5321. For Kaggle users like russellgilbert, krbharat and cjd1989, Agent v1.0 also demonstrates significant performance gains of over 50%. For other users including mmellinger66, jimgruman, tcspecialist, gauravduttakiit and arosiel, Agent Ks increases are modest ranging from 5% to 25%. The negative section of Figure 10 shows that while Agent v1.0 performs well in many cases, there are several users against whom it underperforms, suggesting areas for improvement. For instance, compared to user tunguz, Agent v1.0 underperforms by approximately 43%, highlighting significant gap. Similarly, against alexryzhkov, the agents performance falls short by about 30%. This underperformance emphasises the need to improve Agent v1.0s solution strategies, making them more robust and adaptable. In the following releases, we will work on including more effective tools that will allow Agent to further bridge the gap with human data scientists. Figure 10. The above bar graph reports the average percentage improvements that Agent v1.0 achieves compared to human participants. We colour-coded each of the participants levels. Our agent exhibits big improvements to Kaggle users, reaching 200%. Moreover, the left-hand side of the plot shows cases in which Agent v1.0 underperforms compared to human competitors. 25 Agent v1.0 Agent v1.0 vs. Kaggle UsersHead-to-Head Performance: The results above show that our agent achieved Kaggle Grandmaster-level rank, placing it in the top 38% of participants. We now present head-to-head analysis to gain deeper insights into its medalwinning performance compared to human participants. Importantly, since our agent earned medals in community and playground competitions alongside featured ones, we ensured fair comparison by aligning the evaluation criteria with those applied to human participants. Specifically, we focused on the 5,856 human competitors who participated in the same competitions as our agent. We identified the specific tasks in which these participants competed, representing subset of the broader tasks our agent tackled. By isolating our agents performance on this subset, we enabled precise, head-to-head comparison against each human participant. Given that our agent achieved Grandmaster level and won medals in all competition types, we aligned medal criteria for our agent and human participants to compare medal acquisition rates fairly. We applied Kaggles progression system, awarding users Figure 11. Comparison of the percentage of medals obtained by Agent v1.0 versus Kaggle users on shared competitions. The x-axis shows the user count, while the y-axis represents percentage differences. The shaded areas denote gap in medal-winning frequencies between Agent v1.0 and human participants and the colours encode the type of medals considered. This gap can be positive for our agent (on the right) or negative (on the left). The positions in the graph represented by flat line indicate that both Agent v1.0 and Kaggle users acquire the same number of medals. The numbers on the end of the vertical lines give the percentages when our agent acquired specific medals compared to the human competitors. right-facing triangle signifies that our agent has won over humans, while left-directed ones reflect when Agent v1.0 has lost. For example, Agent v1.0 wins humans in gold medals with 14%, losses with 6% and ties with 100% - (14% +6%)=80%. 26 gold, silver, and bronze medals even in competitions that didnt officially offer medals. This mirrors the methodology used for our agent and ensures proper comparison, providing consistent criteria across both groups to ensure the fairness and accuracy of our analysis. Agent v1.0 After the above adjustment, we calculated the percentage of medals our agent and human participants won across overlapping competitions. We determined the rate of competitions where medals were earned for each medal type gold, silver, and bronze. For % of these overlapping competitions and example, if our agent achieved gold medal in p1 - p2 user achieved it in p2 %, this results in percentage difference of (p1 We order those differences and summarise the results against all competitors in Figure 11. In this figure, the x-axis shows the competitors and the y-axis is the percentage of medals acquired by Agent v1.0 versus humans, i.e., Agent v1.0 % - Kaggle User % for winning gold, (at least) silver and (at least) bronze medals among shared competitions. The colour-coded shaded regions highlight the gap for each medal type: yellow for gold, grey for silver, and brown for bronze. For instance, if the golden shaded area is at 100% for particular Kaggle user who participated in shared competitions with our agent, it reflects that Agent v1.0 acquired gold medals while that user won none. On the contrary, if the y-axis was at -100%, user has acquired those medals, while our agent failed to receive any. )%. In addition to plotting those areas, we display the percentages at the ends of the dashed vertical lines on the right (for more medals) and left (for fewer medals) of Figure 11, representing how often our agent wins more or fewer gold, silver, or bronze medals than Kaggle user. Notably, for each type of medal, Agent has higher likelihood of earning strictly more medals in overlapping competitions than earning fewer than its opponents. The difference is particularly significant for bronze medals, where Agent outperforms in 42% of match-ups and underperforms in only 23%. Similarly, for gold medals, the agents winning rate of 14% is over twice its losing rate of 6%. These results emphasise Agent Ks ability to deliver high-performing solutions and secure medals more consistently than most Kaggle users. While this proficiency has propelled Agent to Kaggle Grandmaster level, it has not yet fully translated to an even higher EloMMR score due to occasional underperformance in certain competitions, as previously discussed. In future iterations, we plan to address these areas of underperformance to further boost Elo-MMR scores, fully capitalising on the agents favourable win-to-loss ratio in medal achievements."
        },
        {
            "title": "5.2 Automatic Task Setup\nAs noted in Section 3.1, Agent K v1.0 is an end-to-end agent that produces submissions\nmerely from a natural language description of a data science task from the Kaggle web-\nsite. This section is dedicated to assessing those automation capabilities. We measure its\nsuccess rates to automatically set up varying domain data-science tasks and the effect of\nexperience-based learning in improving efficiency. Before doing so, we briefly note Agent\nK’s automation features compared to the closest state-of-the-art in Table 3.",
            "content": "As we can see, Agent v1.0 is the first to jointly allow automatic data and meta-data download, preprocessing, feature engineering, and modelling. Additionally, it supports Bayesian optimisation to automatically tune the hyperparameters involved in the generated solution. Finally, this work is the first - to the best of our knowledge - to be end-2-end, going from natural-language description of the task to high-quality Kaggle submissions. 27 Agent v1.0 Table 3. table comparing the automation capacity of Agent v1.0 with the closest agents in literature. The legend reads as follows: Download corresponds to the agents abilities in downloading data and meta-data, FE is used for feature engineering, BO denotes whether the agent automatically performs Bayesian Optimisation to optimise its solution hyperparameters. Blending refers to the capacity of the agent to generate multiple solutions and blend them to create new submission later on. Improve and Workflow refer to the agents pipeline and its flexibility to dynamically attempt to improve given solution and modify its plan respectively. Finally, Download and Submit refer to the specific capability of the agent to automatically download data from the Kaggle website and submit solutions for competitions. Agent Download Pre-Process FE Modeling BO Blending Improve Workflow Submit CAAFE [26] MLAgentBench [32] DS-agent [23] DataInt. [27] AutoM3L [47] MLCoPilot [90] AIDE [34] AutoML-Agent [78] AutoKaggle [45] Agent v1.0 (ours) 5.2.1 Setup success rates To evaluate setup performance, we use the tasks from section 4 spanning multiple domains, including tabular data, computer vision, and natural language processing. Success is quantified by the learners ability to pass series of unit tests designed to verify the completeness of each setup stage as described in Section 3.1. Those tests confirm that the essential setup components are functioning as intended. For instance, they validate that the training and testing data mappings are correctly implemented, ensuring that inputs and outputs adhere to the expected formats. Additionally, the tests check for compliance with competition-specific requirements, such as submission formatting and compliance with defined evaluation metrics. The results in Table 4 present detailed analysis of our learners automation performance. We report the \"Pass@k\" scores, specifically Pass@1, Pass@3, and Pass@10, which indicate the percentage of tasks the learner successfully set up after one, three, and ten trials, respectively. Our learner performs well in the tabular tasks, with strong Pass@10 score of 95.7%, indicating that it can complete nearly all tabular setups by the tenth trial. The performance in CV tasks reflects different trend. Pass@1 is relatively lower at 56.7%, indicating that the initial setup attempts have moderate success rate in this domain. However, the learners performance improves with additional attempts, reaching Pass@10 of 85.2%. Finally, Agent v1.0 performs exceptionally in NLP tasks, with perfect Pass@10 score of 100%, indicating correct setup completion after ten trials. When aggregated across all domains, the results show that Agent v1.0 maintains robust overall performance, achieving Pass@10 score of 92.5%. As caveat, it is essential to recognise that the scope of our unit tests fundamentally limits our competition setup pipeline. While setup may be marked as successful when 28 Agent v1.0 Table 4. table showing our setup automation results. Pass@3 and Pass@10 are averaged over tasks. We run 10 seeds, so the Pass@10 counts the number of tasks successfully set after the 10 trials. Tabular tasks CV tasks NLP tasks All tasks Pass@ @1 @3 @10 @1 @3 @10 @1 @3 @10 @1 @3 @10 Agent v1.0 77.9 93.9 95.7 56.7 75.2 85.2 75.4 91.7 100 71.1 87.9 92.5 it passes all unit tests, this success doesnt guarantee optimal performance. In practice, setup can be technically correct but still flawed regarding its impact on model outcomes. For example, consider the script responsible for converting models output into wellformatted submission CSV file. While our unit tests verify that this script produces file structurally identical to the sample submission, they dont assess the accuracy or integrity of the predictions themselves. As result, the script could alter prediction values in ways that harm performance, such as rounding values or transforming outputs without accounting for the models specific characteristics. These unintended modifications could lead to poor competition results despite passing all structural checks. For this reason, Agent v1.0 has performed poorly on some of the tasks in the previous section. This limitation highlights the need for more robust testing in our pipeline. In future work, we plan to extend the validation framework beyond structural correctness to include performance-oriented checks. Figure 12. This figure depicts Agent v1.0s unit test success rates. Those results are averaged over ten seeds across all tasks and domains we considered. In Appendix A.1.3, we present more refined version that depicts those results per domain. 29 Agent v1.0 Unit Tests Success Rates: While successful, as shown in Table 4, we now wish to extend our analysis to understand the exact unit test failures that Agent v1.0 faces when setting up data science tasks. We achieve this by allowing our agent to set up all the data science tasks from the pool and record its success in passing all of our unit tests. We repeat the above for ten seeds across all those domains and report average success rates in Figure 12. The radar chart illustrates the average success rates across various unit tests in setting up data science tasks. We compute the averages across all domains. Each axis represents distinct unit test, with success rates ranging from around 85% to nearly 100%. Notably, unit tests for creating input maps for tabular and natural language data (i.e., Create Map for Inputs Tab. Train and Create Map for Inputs Text Train in the figure) exhibit high rates, nearing 100%. Additionally, tests evaluating metric implementation and task and feature identification (e.g., Implement Metric, Get Tab. Features types) also exhibit high average success rates of 99% and 100%, respectively. Moreover, we observe that unit tests for training and testing deep learning modules (i.e., Unit test for Train DL and Unit rest for Test DL in Figure 12) are highly positive, close to 100%. However, some tests show lower success rates, with the lowest observed around 85.24% for creating submission formats, i.e., Create Conversion to Sub. Format. Creating input training maps for computer vision tasks is another challenging stage our agent faces, with results indicating an average success rate of 86.45 %. Improving those success rates via better LLM prompting and learning mechanisms is an exciting direction we plan to explore in future work. Given unit test success rates, we also measure task success rates across domains and report those results in the bar graph in Figure 13. This visualisation reveals wide range of performance, from nearly 0% for the most challenging tasks to 100% for the most successful ones, e.g., competitions like tabular-playground-series-jan-2021 and tabularplayground-series-feb-2021. Several tasks display much lower setup performance, especially those near the charts top, i.e., scrabble-player-rating, state-farm-distracteddriver-detection, and statoil-iceberg-classifier-challenge. The CV and some NLP tasks, in particular, show more variability, possibly due to the need for more complex setup processes. 5.2.2 Learning to Setup from Experience Our final set of experiments assesses our agents ability to leverage memory to set up tasks effectively. This experiment aims to demonstrate that an agent capable of strategically using its memory exhibits greater adaptability than baseline LLM lacking such memorybased refinement. Following the strategy outlined in Section 2, our agent adjusts its memory contents based on the success of the generated code, thereby iteratively building longterm memory. Initially, this code database is empty and is populated based on tasks Agent v1.0 decides to set up. This long-term database helps the agent selectively retrieve relevant information when setting up new tasks, employing the decision-making framework described in Section 3.3. We evaluate different adaptation strategies to improve the base model when setting up tasks from the pool of 79 data science domains we used in the previous sections. The first is random heuristic, in which the agent randomly selects task to set up. The second is heuristic we designed to help Agent v1.0 by encouraging it not to re-select tasks it preAgent v1.0 Figure 13. This figure depicts Agent v1.0s setup success rate across all tasks and domains. We evaluate each task setup over ten random seeds and report the success rate as percentage. If task setup succeeds at least once, we proceed with πsolve(); otherwise, we exclude the task. This ensures that our agent remains fully end-to-end and autonomous. viously failed to set up. This strategy should perform better than random selection as we increase the potential of our agents success. Here, only the β expλ term from Equation (7) is considered since it avoids selecting tasks that the agent has recently attempted. Fur31 Agent v1.0 Figure 14. This figure depicts learning curve to effectively retrieve from the long-term memory using different heuristics. The dashed black line in the figure corresponds to the base LLM without access to long-term memory database. thermore, we enable active learning from long-term memory to set tasks using the overall approach in Equation (7). We summarise our results in Figure 14, where we show the number of tokens output by the LLM versus the number of tasks that have been successfully set up by Agent v1.0. The \"Base Model (Active Learning from Memory)\" (green line) achieves the highest number of successfully setup tasks, consistently outperforming the other methods, particularly as the number of tokens increases. This result indicates that active learning from memory significantly enhances the models effectiveness in setting up tasks, allowing it to learn from previous interactions and adapt its strategy over time. The \"Base Model (Heuristics)\" (blue line) shows better performance than the random approach but lags behind active learning. The \"Base Model (Random)\" (red line) shows the slowest increase in task setup success, underscoring its inefficiency. Interestingly, heuristics and active learning outperform the base LLM modelthe dashed black line in the figurewithout access to long-term memory database. Here, the base model runs out of the box and attempts to set up data science tasks. After 1M tokens, random selection strategies achieve the same performance as no long-term memory incorporation, signifying the importance of accurate selection strategies. 32 Agent v1."
        },
        {
            "title": "6.1 Traditional automated data science (Auto-DS)\nAchieving performance and automation in data science tasks has been a long-standing\nchallenge. Numerous techniques have been developed to automate the human-intensive\nsteps of the DS pipeline using machine learning. Given that each step in the DS pipeline\nposes challenges, many approaches have focused on specific aspects of the problem, often\nresulting in narrow application ranges or use-case limitations. For example, probabilistic\ninference rules have been used to automate data cleaning [59], while various deep learning\nmethods have been developed to filter, select, or transform features to enhance the accu-\nracy of downstream models [38, 39, 41]. In the modelling domain, a significant research area\nknown as neural architecture search (NAS) optimises the structure of neural networks for\npredictions [46, 82], shifting the manual effort from model design to the design of a model\nsearch space. Beyond neural architecture, many hyperparameters can be automatically\ntuned to achieve better performance using black-box optimisation methods. Techniques\nlike Bayesian optimisation address black-box optimisation in a sample efficient way and are\nsupported by various libraries [2, 4, 15, 18], which users can utilise by correctly identifying\nand providing the hyperparameters of their pipelines. Additionally, meta-learning has been\nintroduced to further improve data efficiency by leveraging past experiments to predict per-\nformance on new tasks based on related tasks solved with similar configurations [29, 49,\n52].",
            "content": "Contrasting with these targeted methods, several libraries have been developed to cover broader parts of the data science pipeline, such as H2O-AutoML [43], AutoKeras [36], Autosklearn [21], and AutoGluon [73]. These tools aim to automate and optimise by making rule-based decisions based on input data modality and combining fixed collection of models that can be refined using preset optimiser tools. However, these approaches have limitations, including restricted scope, flexibility, and usability barriers. Their performance is limited by the set of predefined models and hyperparameters they support, and they require expert knowledge to properly format the problem [11] (e.g., manual specification of column types for AutoKeras). Tools like H2O-AutoML are also limited to tabular data tasks, requiring data to be structured in specific ways."
        },
        {
            "title": "6.2 Data-science Agents\nThe emergence of data-science agents powered by large language models and multi-agent\nsystems represents a shift towards more autonomous and adaptable solutions. Unlike tra-\nditional Auto-DS approaches, these agents are designed to perform end-to-end tasks by\nleveraging advanced reasoning and decision-making capabilities.",
            "content": "6.2.1 Coding Agents Recent advancements in LLMs [1, 33, 50, 60], combined with techniques like chain of thought prompting [79, 92] and ReAct [87] have significantly expanded the scope of AI applications beyond natural language processing. These models are now capable of generating code and interacting with APIs [17, 64, 57], laying the groundwork for their integration into multi-agent systems [24, 28, 74, 76, 83] that can execute complex tasks by thinking, 33 Agent v1.0 planning, calling tools [58], and making decisions [31, 44, 63, 89] across various domains, including robotics [48, 53, 67], medicine [71, 77], and also data science [23, 27, 90]. DS agents exploit the intrinsic capacities of LLM-Agents. They are augmented with specific tools for writing, executing and debugging code, data visualisation tools [86], or access to standard ML libraries APIs [65], offering more integrated approach to data science automation. 6.2.2 Intelligent DS assistants Several agents function as intelligent assistants, enhancing human decision-making by providing automated solutions while keeping users in the loop. For instance, CleanAgent [56] handles data cleaning based on user specifications, and LAMBDA [69] generates data analysis reports based on technical queries. MatPlotAgent [86] pairs coding agent with visual agent to meet the experts needs in terms of data visualisation, while WaitGPT [84] offers smoother monitoring of the code generation by creating graph representations of the generated code and supporting on the fly manual edition or questions. Although these agents enhance user efficiency, they require substantial user input. They are not designed to autonomously achieve high performance in complex, real-world scenarios, such as winning Kaggle competition."
        },
        {
            "title": "6.3 End-to-end DS agents\nEnd-to-end data-science agents aim to manage the entire data science pipeline au-\ntonomously, from data preprocessing to model evaluation and hyperparameter optimi-\nsation, without requiring significant human oversight. While some existing approaches\naddress specific aspects of this workflow, such as CAAFE [26] which allows agents to itera-\ntively generate codes to modify tabular features and reflect based on accuracy feedback, or\nEVAPORATE-CODE+ [3], which focuses on data mining by generating code to extract struc-\ntured data from semi-structured documents, other methods have tackled the entire data\nscience pipeline more holistically.",
            "content": "Recent multi-agent systems like SEED [11], AutoM3L [47], Data Interpreter [27], AutoML-Agent [78], HuggingGPT [65], MLCopilot [90], DS-Agent [23], SELA [13], and AutoKaggle [45] achieve capacities to deal with data-science by empowering their system with numerous tools, using retrieval from expert demonstrations, and supporting complex multiple steps reasoning. These systems employ various strategies for generating automated pipelines for different stages of the data science process such as preprocessing, modelling, and feature selection. While DS-Agent, MLCopilot and AIDE generate and refine the entire solution script as single block, SEED performs skyline-based optimisation to select the modules to apply to solve the task. Alternatively, HuggingGPT delegates the planning to high-level agent, and Data-Interpreter lets this agent decide on hierarchical and dynamic plan. Meanwhile, AutoM3L follows pre-defined modality-specific pipeline to produce final predictions. Our work integrates elements from these various approaches by imposing modality-dependent structure during the task setup phase while allowing agents greater flexibility in selecting the optimal sequence of steps to achieve the best possible outcome. Moreover, several existing systems also incorporate mechanisms for continual learning, enabling them to leverage past experiences to improve performance on new tasks. 34 Agent v1.0 For instance, DS-Agent achieves that by adopting case-based reasoning approach combined with an offline-online setup, retrieving insights during the online phase from pool of scripts that performed well on offline tasks. Similarly, MLCopilot divides its framework into offline and online phases, using an LLM to infer verifiable machine learning strategies from diverse offline experiments, such as determining valid learning rate ranges for specific models. Beyond building knowledge database to retrieve from, Data-Interpreter uses prior experiences to create new tools that the agents can later call. However, these systems theoretically or practically suffer from partial coverage or limited capacities. Indeed, AutoM3L cannot generate solutions involving model ensembling and requires specifically structured input data, while HuggingGPT lacks support for model training and tabular models. Moreover, SEED and DS-Agent do not demonstrate capabilities for handling computer vision tasks, and AIDE, AutoKaggle, and Data Interpreter have only shown strong performance on more straightforward tabular datasets or Kaggle competitions. AutoML-Agent requires comprehensive task-specific prompts which can be advantageous for experts customising its performance but poses challenge for lessexperienced users. Additionally, while some efforts have been made to utilise past experiments for new task resolution, the strategic selection of task order to enable progressive learning remains an underexplored area. We aim to address these gaps by developing system capable of handling various input modalities and achieving consistently high performance in real-world data science competitions. This includes optimising task sequencing to facilitate cumulative learning, advancing beyond the current state of the art in DS agent capabilities."
        },
        {
            "title": "6.4 Data-science benchmarking\nAssessing agents’ performance on data science tasks has been approached from various\nperspectives. While many benchmarks, such as those for coding proficiency [35, 93], focus\non general code generation capabilities, they do not necessarily capture the full range of\nskills required for end-to-end data science. Assessing the performance of agents on data-\nscience tasks has been considered from different scopes.",
            "content": "Some benchmarks are tailored to specific aspects of data science. For example, [42] focuses on code infilling based on data from Stack Overflow and common libraries, targeting localised code generation rather than comprehensive pipeline development. Unlike traditional AutoML benchmarks [22], which typically provide standardised input format to facilitate solver design, data science agent benchmarks evaluate broader capabilities. These include understanding the task requirements and adapting solutions accordingly, offering flexibility in problem formulation and solution strategies. Authors in [32] introduce benchmark using six Kaggle tasks, where success is defined as achieving 10% improvement over baseline provided by starter code. However, this baseline can sometimes represent minimal predictive performance, such as random predictions, rather than robust comparison against established leaderboards. This benchmark also offers predefined scripts and environments for each task, guiding the problemsolving approach. Other benchmarks, such as [7], emphasise the orchestration of workflows and interactions with professional software tools, while [30] provides carefully curated set of questions requiring language models to generate code that answers specific dataset-related queries. This differs from end-to-end problem-solving in competitions like 35 Agent v1.0 Kaggle, whose questions are primarily focused on tabular data. Additionally, [72] centres on understanding machine learning repositories rather than the broader data science process, which limits its applicability to end-to-end DS agent evaluation. [91] uses Kaggle datasets to derive specific questions about dataset characteristics, such as imbalance, missing data, and correlations. Although this approach provides focused evaluation, it does not capture the full scope of capabilities required for complete data science workflows, which is broader objective we aim to achieve. Closer to our work, [37] curates list of 74 Kaggle competitions to assess the data modelling capacity of LLM agents. Nevertheless, the authors pick the benchmark competitions based on the simplicity of their raw structures, featuring only train, test and sample submission file. Moreover, they do not support comparison with the Kaggle leaderboard but split the training set to create custom test set instead. Likewise, MLE-bench [9] includes 75 machine learning competitions from Kaggle to assess agent performance by comparing it against user submissions. However, because this assessment relies on custom split of the training data rather than the competitions actual private test set, it remains uncertain whether an agents high ranking in this context would align with results on the original Kaggle leaderboard. Our work aims to truly assess the end-to-end capacity of agents to achieve high performance in data-science tasks autonomously. We plan to consolidate our task set into proper benchmark."
        },
        {
            "title": "7 Limitations & Future Work",
            "content": "This paper introduced Agent v1.0, the first LLM-based agent to achieve Kaggle Grandmaster level performance across wide range of data science tasks. These tasks spanned multiple domains, including tabular data, time series analysis, computer vision, natural language processing, and multimodal challenges. Our agent operates fully autonomously, seamlessly handling everything from navigating URL to building models, making submissions, and winning competitions. Agent v1.0 advances structured reasoning by introducing novel intrinsic functions for action generation and credit assignment. These functions enable the agent to learn and adapt using memory, eliminating the need for sample-intensive backpropagation methods. We further presented method for scaling our agent based on continual active learning and curriculum building. We demonstrated reduced number of generated tokens to autonomously and correctly set up cross-domain data science tasks from their natural language descriptions. We introduced new techniques that enable Agent v1.0 to interact with external tools such as hyperparameter optimisation frameworks, custom LLM-based feature engineers, RAMP, Torchvision, and Torchtext. This incorporation allows the agent to sequence, update, and generate code in these tools to address data science problems. As result, our agent has achieved remarkable success, earning 6 gold, 3 silver, and 7 bronze medals. We also conducted Elo-MMR ratings to avoid overfitting specific tasks, demonstrating that our agent ranks in the top 38% among 5,856 participants. While our results are successful and present new state-of-the-art performance of data science agents, Agent v1.0 can benefit from further improvements as we detail next. Agent v1.0 In Section 3.1, we introduced two new intrinsic functions that allow Agent v1.0 to automatically set up data science tasks. Currently, those functions only consider feedback from unit and meta-unit tests for credit assignment and adaptation from external memory. While this option is viable, as we demonstrated in Section 5, incorporating feedback from the second phase (Section 3.2) could further enhance the setup process by identifying which parts of the code and data preprocessing techniques contribute to higher performance. Furthermore, Agent v1.0 leveraged various external tools to achieve high performance in Kaggle competitions. Some of these tools were developed in our previous work (e.g., HEBO [15] and RAMP [40]), while others were created by third parties (e.g., Torchvision). New tools, such as feature engineering modules, were also developed specifically for Agent K. Our current mechanism to improve performance based on those tools uses standard intrinsic functions from [14]. In the future, we will investigate performance-guided structures that are more effective for feedback to LLMs. Also, we plan to extend the number of tools we can interact with to introduce new capabilities, such as those to handle video and audio tasks. Additionally, our agent leverages continual learning as critical mechanism to boost scalability and improve generation performance. By dynamically constructing curriculum, the agent can minimise the number of tokens needed for each task, saving computational resources and accelerating learning. When confronted with new task, the agent doesnt just proceed arbitrarilyit actively selects the next task by reviewing its knowledge history and strategically deciding which challenges to address based on prior experience. However, in this initial implementation, the feedback loop guiding the agents choices was limited to task setup considerations. While focusing on setup efficiency is beneficial, it is equally critical to integrate performance feedback into the decision-making process. By doing so, the agent would evaluate how easy it is to set up task and predict how well it can perform, leveraging its accumulated knowledge to maximise results. In the future, we aim to develop more sophisticated intrinsic functions enabling the agent to incorporate performance-based feedback when building its curriculum. Regarding evaluation protocols, our benchmark stands out by going beyond current standards, as it is built from real-world data and competitive environments, offering more meaningful and practical evaluation. While this already provides strong foundation, we are committed to improving it further by expanding the breadth and depth of tasks we evaluate. Specifically, we plan to significantly increase the number of tasks we handle, aiming to scale from the hundreds to thousands. Moreover, we seek to broaden the range of data modalities, incorporating multimodal challenges that include audio, video, and other complex data types. This would diversify the benchmark and push our systems to tackle richer and more varied real-world problems. In addition to expanding the dataset, we are exploring innovative ways to create new tasks by augmenting existing ones. This will allow us to continually generate more data, which can be particularly useful for fine-tuning models and enhancing their overall performance. We are also eager to refine our experiments further, and there are many exciting directions we plan to explore. One key area we must address is adapting to competitions requiring runnable notebooks rather than the typical submission of results file like submission.csv. This will be focus of our future work, as we aim to make Agent v1.0 more versatile and capable of handling broader range of competitive environments. While our agent demonstrates impressive performance across many tasks and modalities, its impor37 Agent v1.0 tant to remember that some of these results measure our rank against submissions made when the competitions were open, when the latest state-of-the-art models our agent now utilises were unavailable. This makes it challenging to assess our agent compared to the best competitors. In the future, we want our agents to participate in live, ongoing competitions. We are already preparing to join several upcoming competitions and look forward to sharing our results soon. Finally, while we refer to our agent as having Kaggle Grandmaster-level performance, we clarify that this does not imply formal Grandmaster title on Kaggle. Our achievements include several gold medals across various competition types community, playground, and featured demonstrating our agents competitive capability among data scientists, but some competitions may be less complex than others, and some are non-medal awarding competitions. To formally pursue the Grandmaster title, we aim to focus on active competitions, scaling and improving our results as we advance to newer versions of Agent K. 38 Agent v1."
        },
        {
            "title": "References",
            "content": "[1] OpenAI Josh Achiam et al. GPT-4 Technical Report. In: 2023. [2] Takuya Akiba et al. Optuna: Next-Generation Hyperparameter Optimization Framework. In: The 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 2019, pp. 26232631. [3] Simran Arora et al. Language Models Enable Simple Systems for Generating Structured Views of Heterogeneous Data Lakes. In: Proc. VLDB Endow. 17.2 (Oct. 2023), pp. 92105. DOI: 10.14778/3626292.3626294. [4] Maximilian Balandat et al. BOTORCH: framework for efficient monte-carlo Bayesian optimization. In: Proceedings of the 34th International Conference on Neural Information Processing Systems. NIPS 20. Vancouver, BC, Canada: Curran Associates Inc., 2020. [5] Maciej Besta et al. Graph of thoughts: Solving elaborate problems with large language models. In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 38. 16. 2024, pp. 1768217690. [6] Anthony Brohan et al. Do as can, not as say: Grounding language in robotic affordances. In: Conference on robot learning. PMLR. 2023, pp. 287318. [7] Ruisheng Cao et al. Spider2-V: How Far Are Multimodal Agents From Automating Data Science and Engineering Workflows? In: CoRR abs/2407.10956 (2024). arXiv: 2407.10956. [8] Yuji Cao et al. Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods. 2024. arXiv: 2404.00282 [cs.LG]. Jun Shern Chan et al. MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering. 2024. arXiv: 2410.07095 [cs.CL]. [9] [10] Mark Chen et al. Evaluating Large Language Models Trained on Code. In: CoRR abs/2107.03374 (2021). arXiv: 2107.03374. [11] Zui Chen et al. SEED: Domain-Specific Data Curation With Large Language Models. In: Arxiv abs/2310.00749 (2023). [12] Maxime Chevalier-Boisvert et al. Babyai: platform to study the sample efficiency of grounded language learning. In: arXiv preprint arXiv:1810.08272 (2018). [13] Yizhou Chi et al. SELA: Tree-Search Enhanced LLM Agents for Automated Machine Learning. 2024. [14] Filippos Christianos et al. Pangu-Agent: Fine-Tunable Generalist Agent with Structured Reasoning. In: ArXiv abs/2312.14878 (2023). [15] Alexander Cowen-Rivers et al. HEBO: Pushing The Limits of Sample-Efficient Hyperparameter Optimisation. In: Journal of Artificial Intelligence Research 74 (July 2022). [16] Shihan Deng et al. Mobile-bench: An evaluation benchmark for llm-based mobile agents. In: arXiv preprint arXiv:2407.00993 (2024). 39 Agent v1.0 [17] Yinlin Deng et al. Large Language Models Are Zero-Shot Fuzzers: Fuzzing DeepLearning Libraries via Large Language Models. In: Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis (2022). [18] Kamil Dreczkowski, Antoine Grosnit, and Haitham Bou Ammar. Framework and Benchmarks for Combinatorial and Mixed-variable Bayesian Optimization. In: Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2023. [19] Aram Ebtekar and Paul Liu. An elo-like system for massive multiplayer competitions. In: arXiv preprint arXiv:2101.00400 (2021). [20] Xidong Feng et al. Alphazero-like tree-search can guide large language model decoding and training. In: arXiv preprint arXiv:2309.17179 (2023). [21] Matthias Feurer et al. Auto-sklearn 2.0: hands-free AutoML via meta-learning. In: J. Mach. Learn. Res. 23.1 (Jan. 2022). [22] P. J. A. Gijsbers et al. An Open Source AutoML Benchmark. In: ArXiv abs/1907.00909 (2019). [23] Siyuan Guo et al. DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based Reasoning. In: Proceedings of the 41st International Conference on Machine Learning. Vol. 235. Proceedings of Machine Learning Research. PMLR, 2024, pp. 1681316848. [24] Taicheng Guo et al. Large Language Model based Multi-Agents: Survey of Progress and Challenges. In: ArXiv abs/2402.01680 (2024). [25] Shibo Hao et al. Toolkengpt: Augmenting frozen language models with massive tools via tool embeddings. In: Advances in neural information processing systems 36 (2024). [26] Noah Hollmann, Samuel Müller, and Frank Hutter. Large Language Models for Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering. In: Thirty-seventh Conference on Neural Information Processing Systems. 2023. [27] Sirui Hong et al. Data Interpreter: An LLM Agent For Data Science. In: ArXiv abs/2402.18679 (2024). [28] Sirui Hong et al. MetaGPT: Meta Programming for Multi-Agent Collaborative Framework. In: The Twelfth International Conference on Learning Representations. 2024. [29] T. Hospedales et al. Meta-Learning in Neural Networks: Survey. In: IEEE Transactions on Pattern Analysis & Machine Intelligence 44.09 (Sept. 2022), pp. 51495169. DOI: 10.1109/TPAMI.2021.3079209. [30] Xueyu Hu et al. InfiAgent-DABench: Evaluating Agents on Data Analysis Tasks. In: [31] ArXiv abs/2401.05507 (2024). Jie Huang and Kevin Chen-Chuan Chang. Towards Reasoning in Large Language Models: Survey. In: ArXiv abs/2212.10403 (2022). 40 Agent v1.0 [32] Qian Huang et al. MLAgentBench: Evaluating Language Agents on Machine Learning Experimentation. In: ArXiv abs/2310.03302 (2023). [33] Albert Qiaochu Jiang et al. Mistral 7B. In: ArXiv abs/2310.06825 (2023). [34] Zhengyao Jiang et al. AIDE: the Machine Learning CodeGen Agent. https : / / github.com/WecoAI/aideml. Accessed: 2024-08-29. 2024. [35] Carlos Jimenez et al. SWE-bench: Can Language Models Resolve Real-world Github Issues? In: The Twelfth International Conference on Learning Representations. 2024. [36] Haifeng Jin et al. AutoKeras: An AutoML Library for Deep Learning. In: Journal of Machine Learning Research 24.6 (2023), pp. 16. [37] Liqiang Jing et al. DSBench: How Far Are Data Science Agents to Becoming Data Science [38] Experts? 2024. James Max Kanter and Kalyan Veeramachaneni. Deep feature synthesis: Towards automating data science endeavors. In: 2015 IEEE International Conference on Data Science and Advanced Analytics (DSAA) (2015), pp. 110. [39] Ambika Kaul, Saket Maheshwary, and Vikram Pudi. AutoLearn Automated Feature Generation and Selection. In: 2017 IEEE International Conference on Data Mining (ICDM) (2017), pp. 217226. [40] Balázs Kégl et al. The RAMP framework: from reproducibility to transparency in the design and optimization of scientific workflows. 2018. [41] Udayan Khurana et al. Cognito: Automated Feature Engineering for Supervised Learning. In: 2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW). 2016, pp. 13041307. DOI: 10.1109/ICDMW.2016.0190. [42] Yuhang Lai et al. DS-1000: Natural and Reliable Benchmark for Data Science Code Generation. In: ArXiv abs/2211.11501 (2022). [43] Erin LeDell and S. Poirier. H2O AutoML: Scalable Automatic Machine Learning. In: 2020. [44] Shuang Li et al. Pre-Trained Language Models for Interactive Decision-Making. In: Advances in Neural Information Processing Systems. Ed. by S. Koyejo et al. Vol. 35. Curran Associates, Inc., 2022, pp. 3119931212. [45] Ziming Li et al. AutoKaggle: Multi-Agent Framework for Autonomous Data Science Competitions. In: 2024. [46] Hanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS: Differentiable Architecture Search. In: International Conference on Learning Representations. 2019. [47] Daqin Luo et al. AutoM3L: An Automated Multimodal Machine Learning Framework with Large Language Models. In: ACM Multimedia 2024. 2024. [48] Zhao Mandi, Shreeya Jain, and Shuran Song. RoCo: Dialectic Multi-Robot Collaboration with Large Language Models. 2023. arXiv: 2307.04738 [cs.RO]. [49] Alexandre Max Maraval et al. End-to-End Meta-Bayesian Optimisation with Transformer Neural Processes. In: Thirty-seventh Conference on Neural Information Processing Systems. 2023. 41 Agent v1.0 [50] Meta Llama team. Introducing Meta Llama 3: The most capable openly available LLM to date. In: Meta AI Blog (2024). [51] Shervin Minaee et al. Large language models: survey. In: arXiv preprint arXiv:2402.06196 (2024). [52] Ambarish Moharil et al. Towards efficient AutoML: pipeline synthesis approach leveraging pre-trained transformers for multimodal data. In: Mach. Learn. 113 (2024), pp. 70117053. [53] Christopher E. Mower et al. ROS-LLM: ROS framework for embodied AI with task feedback and structured reasoning. In: ArXiv abs/2406.19741 (2024). [54] Long Ouyang et al. Training language models to follow instructions with human feedback. In: Advances in neural information processing systems 35 (2022), pp. 2773027744. [55] Adam Paszke et al. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In: Advances in Neural Information Processing Systems 32. Curran Associates, Inc., 2019, pp. 80248035. [56] Danrui Qi and Jiannan Wang. CleanAgent: Automating Data Standardization with LLM-based Agents. In: ArXiv abs/2403.08291 (2024). [57] Yujia Qin et al. ToolLLM: Facilitating Large Language Models to Master 16000+ Realworld APIs. In: ArXiv abs/2307.16789 (2023). [58] Changle Qu et al. Tool Learning with Large Language Models: Survey. In: ArXiv abs/2405.17935 (2024). [59] Theodoros Rekatsinas et al. HoloClean: holistic data repairs with probabilistic inference. In: Proc. VLDB Endow. 10.11 (2017), pp. 11901201. DOI: 10.14778/3137628. 3137631. [60] Xiaozhe Ren et al. PanGu-Σ: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing. In: ArXiv abs/2303.10845 (2023). [61] Grand View Research. Data Science Platform Market Size, Share & Trends Analysis Report By Product (Platform, Services), By Application, By Vertical, By Region, And Segment Forecasts, 2024 - 2030. 2024. [62] David Rumelhart, Geoffrey Hinton, and Ronald Williams. Learning representations by back-propagating errors. In: nature 323.6088 (1986), pp. 533536. [63] Hao Sha et al. LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving. In: ArXiv abs/2310.03026 (2023). [64] Bo Shen et al. PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback. In: ArXiv abs/2307.14936 (2023). [65] Yongliang Shen et al. HuggingGPT: solving AI tasks with chatgpt and its friends in hugging face. In: Proceedings of the 37th International Conference on Neural Information Processing Systems. NIPS 23. New Orleans, LA, USA: Curran Associates Inc., 2024. [66] Zhuocheng Shen. LLM With Tools: Survey. 2024. arXiv: 2409.18807 [cs.AI]. 42 Agent v1.0 [67] Ishika Singh et al. ProgPrompt: Generating Situated Robot Task Plans using Large Language Models. In: 2023 IEEE International Conference on Robotics and Automation (ICRA). 2023, pp. 1152311530. DOI: 10.1109/ICRA48891.2023.10161317. [68] Chuanneng Sun, Songjun Huang, and Dario Pompili. LLM-based Multi-Agent In: arXiv preprint Reinforcement Learning: Current and Future Directions. arXiv:2405.11106 (2024). [69] Maojun Sun et al. LAMBDA: Large Model Based Data Agent. In: arXiv preprint arXiv:2407.17535 (2024). [70] Richard Sutton. Reinforcement learning: An introduction. In: Bradford Book (2018). [71] Xiangru Tang et al. MedAgents: Large Language Models as Collaborators for Zeroshot Medical Reasoning. In: Findings of the Association for Computational Linguistics ACL 2024. Ed. by Lun-Wei Ku, Andre Martins, and Vivek Srikumar. Bangkok, Thailand and virtual meeting: Association for Computational Linguistics, Aug. 2024, pp. 599621. [72] Xiangru Tang et al. ML-Bench: Evaluating Large Language Models and Agents for Machine Learning Tasks on Repository-Level Code. In: (2024). arXiv: 2311.09835 [cs.CL]. [73] Zhiqiang Tang et al. AutoGluon-Multimodal (AutoMM): Supercharging Multimodal AutoML with Foundation Models. In: arXiv preprint arXiv:2404.16233 (2024). [74] Open-interpreter Team. Open-interpreter: natural language interface for computers. https://github.com/OpenInterpreter/open-interpreter. 2013. [75] Qwen Team. Qwen2.5: Party of Foundation Models. Sept. 2024. [76] XAgent Team. XAgent: An Autonomous Agent for Complex Task Solving. 2023. [77] Arun James Thirunavukarasu et al. Large language models in medicine. In: Nature Medicine 29 (2023), pp. 19301940. [78] Patara Trirat, Wonyong Jeong, and Sung Ju Hwang. AutoML-Agent: Multi-Agent LLM Framework for Full-Pipeline AutoML. 2024. [79] Rasul Tutunov et al. Why Can Large Language Models Generate Correct Chain-ofThoughts? In: arXiv preprint arXiv:2310.13571 (2023). [80] Shubham Vatsal and Harsh Dubey. Survey of Prompt Engineering Methods in Large Language Models for Different NLP Tasks. 2024. arXiv: 2407.12994 [cs.CL]. Jason Wei et al. Chain of Thought Prompting Elicits Reasoning in Large Language Models. In: CoRR abs/2201.11903 (2022). arXiv: 2201.11903. [81] [82] Colin White et al. Neural Architecture Search: Insights from 1000 Papers. In: ArXiv abs/2301.08727 (2023). [83] Qingyun Wu et al. AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework. In: COLM. 2024. 43 Agent v1.0 [84] Liwenhan Xie et al. WaitGPT: Monitoring and Steering Conversational LLM Agent in Data Analysis with On-the-Fly Code Visualization. In: Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology. UIST 24. Pittsburgh, PA, USA: Association for Computing Machinery, 2024. DOI: 10 . 1145 / 3654777 . 3676374. [85] Xue Yan et al. Efficient Reinforcement Learning with Large Language Model Priors. 2024. arXiv: 2410.07927 [cs.LG]. [86] Zhiyu Yang et al. MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific Data Visualization. 2024. arXiv: 2402.11453 [cs.CL]. [87] Shunyu Yao et al. ReAct: Synergizing Reasoning and Acting in Language Models. In: ArXiv abs/2210.03629 (2022). [88] Shunyu Yao et al. Tree of Thoughts: Deliberate Problem Solving with Large Language Models. 2023. arXiv: 2305.10601 [cs.CL]. [89] Shunyu Yao et al. Tree of Thoughts: Deliberate Problem Solving with Large Language Models. In: Advances in Neural Information Processing Systems. Ed. by A. Oh et al. Vol. 36. Curran Associates, Inc., 2023, pp. 1180911822. [90] Lei Zhang et al. MLCopilot: Unleashing the Power of Large Language Models in Solving Machine Learning Tasks. In: Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers). Ed. by Yvette Graham and Matthew Purver. St. Julians, Malta: Association for Computational Linguistics, Mar. 2024, pp. 29312959. [91] Yuge Zhang et al. Benchmarking Data Science Agents. In: Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Ed. by Lun-Wei Ku, Andre Martins, and Vivek Srikumar. Bangkok, Thailand: Association for Computational Linguistics, Aug. 2024, pp. 56775700. [92] Qinkai Zheng et al. CodeGeeX: Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X. In: Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (2023). [93] Terry Yue Zhuo et al. BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions. In: arXiv preprint arXiv:2406.15877 (2024). 44 Agent v1.0 Agent v1.0 - Pipeline Details According to our formulation, the design of the agent framework is broken down into two main parts: the setup pipeline and the solution generation pipeline. The setup pipelines goal is to preprocess the raw information and raw data to prepare workspace and DataLoader object directly usable by the solution generation pipeline to solve the task. All the steps related to downloading, summarising, organising and cleaning the data are done in the setup pipeline, whereas steps related to feature engineering, model training, hyperparameter optimisation and submission belong to the prediction pipeline. In this section we give more detail about the pipeline steps, their objective and important aspects of how they work. A.1 Setting up Competitions The goal is to automate the process of setting up task. The pipeline, presented in Figure 4 depicts the sequence of stages that Agent v1.0 tackles in order to setup new Kaggle competition, given its URL. A.1.1 Expected Result - Usable & Interpretable Workspace At the end of successful setup, we expect to have workspace containing useful files, including raw information and raw data from the competition website, as well as new files created by Agent v1.0, such as metadata, different CSV files as well as runnable code files. Our agent creates additional CSV files only if it deems it necessary from the context of the competition: train_tab_input_map.csv - tabular inputs from the train set train_txt_input_map.csv - text inputs from the train set train_img_input_map.csv - paths to the image inputs from the train set train_tab_target_map.csv - tabular targets from the train set train_txt_target_map.csv - text targets from the train set train_img_target_map.csv - paths to the image targets from the train set test_tab_input_map.csv - tabular inputs from the test set test_txt_input_map.csv - text inputs from the test set test_img_input_map.csv - paths to the image inputs from the test set When Agent produces code to generate the CSV files, this code is kept in the workspace, which contributes to the interpretability of the result. Furthermore, our agent also need to implement the useful functions: code_transform_tab_target_train.py - code to transform the targets into format usable for training models and back to the original format 45 Agent v1.0 code_metric.py - the metric function implementation code_submission_format.py - code to transform from the predicted targets format to the exact format accepted by the competition (e.g. from probability of class to the label of that class) code_submission_format_alt.py - an alternative to the previous function, in case it fails. It can also be used as backup for the solution generation pipeline when we format the predictions of the model for submission. A.1.2 Setup Pipeline Steps Fetch & Scrape Raw Data Given the URL of the competition, Agent v1.0 can use the Kaggle API to download its associated data. The agent is also given scraping tool which automatically retrieves information from the competitions web page, such as the raw description of the competition, its data description, metric description, etc. Summarise From the scraped raw descriptions, Agent v1.0 creates digestible summaries that it can later reuse for LLM prompting. The raw text from the competition pages can contain elements that are irrelevant to the task (e.g. emojis, images, urls, . . . ) or worse, information that is misleading. Summarising the raw descriptions can deal with those issues and enables the agent to compress the important information from the task into more useful and shorter text that takes less tokens in the future prompts. Detect Modalities We consider three main modalities: vision data, language data and tabular data, which can be purely tabular or contain time-series. The agent categorises all competitions inputs and targets with respect to these three modalities, based on the summarised descriptions. For instance, competition where participants are asked to create an image classifier would have image inputs and tabular targets (classification). In another case, if competition consists in attributing toxicity score to movie reviews and includes as input the reviews as well as the categories of the movies, then the input modalities would be mixture of text and tabular, while the modality of the target would be tabular (regression). Create Plan & Metadata After the raw data is fetched, the summaries are written and the modalities are detected, Agent v1.0 creates its plan, i.e. its workflow. This plan determines the sequence of steps that should be done in order to complete the setup. This plan is also dynamic in the sense that our agent can decide to prioritise stages over others provided it is permitted by the general sequence order of the environment. Agent v1.0 can come back to previous stages when executing its plan, and it can attempt some stages multiple times if they havent been done properly the first time, as we detail in the paragraph dedicated to unit tests. Create Maps & Transforms The main task in the setup pipeline is for the agent to create maps and transforms, as outlined in Section A.1.1. The idea is to split all raw training data into input and target maps, as well as to split all raw data (train and test) into different Agent v1.0 modalities. task could have multiple types of inputs and targets so we require map for each of them. These maps are then collated together in custom DataLoader object that makes it easy to consume by model in the prediction pipeline. Importantly, the agent is also tasked with creating transform functions in order to treat targets as consumable data for model. For instance if the targets in the training set are text-encoded labels (in classification task) the transform function could map these to one-hot encoding and the inverse transform function would map them back to the text labels. These functions are crucial to enable the prediction pipeline. Create Submission Format & Metric Function Finally, Agent v1.0 generates code to create the submission format and the metric function. Both are generally described in the raw competition description. The submission format is the exact format expected by the competition to read the predictions. It can sometimes be different from the format of the raw train targets so in addition to the transforms, we also need function that maps the predicted targets on the test inputs, in the exact format expected. A.1.3 Unit Tests An important part of the setup pipeline is the set of unit tests used to assert if stage or group of stages has been successfully done by the agent. It is worth noting that these unit tests are manually designed and not generated by the agent itself. However, this does not mean that they are tailored to each task. In fact they are designed to be agnostic to the competition to setup and are rather specific to the framework itself. Knowing the properties of the expected result of particular stage, we can test if the code or the data our agent generate follows an expect format. This way our agent receives insightful feedback through error messages when its code is faulty, which it integrates into its reasoning process to correct itself. For example, suppose our agent generates runnable code to create the training input map of the image modality. During the state transition of the environment, the unit test associated to this stage is ran. The unit test checks that CSV file with specific name exists and is loads it. It then assesses that the table has column named id and that it has at least one row. It also checks the existence of column apart from the id column and that these other columns contain paths to images with known extensions. Moreover, the unit test throws an error if one of the image paths listed in that map is invalid. Finally it can check various trivial things such as non emptiness of columns or absence of duplicated columns. This shows that the unit tests are specific to the setup environment and not specific to the task. If an error is raised in the unit test, the error is saved in memory and the current stage is attempted again. In this new trial, our agent will be able modify and fix its previous code based on the unit test error message that is accessible in its memory. This is repeated until the unit test passes or budget limit is reached (in which case the setup would fail there). Finally we have meta-unit tests which are unit tests that depend on multiple stages. They verify that the result of multiple single stages coordinate well together. If meta-unit test fails, the agent has the opportunity to go back multiple stages and re-attempt the relevant stages. For instance, after all maps and transforms are done for the training data, we test if the DataLoader can be constructed properly and if we can successfully load batch. This 47 Agent v1.0 Table 5. Hyperparmeters used and their values. Name max_epochs max_time batch_size lr optimizer n_trials blend_after Value 30 10h 32 [1e 6, 1e 2] Role Max training epochs allowed Max training time allowed Training & testing batch size BO search space for learning rate {Adam, SGD, AdamW} BO search space for optimizer 20 3 Max BO rounds allowed max nb. of solutions before blending step is crucial as it validates that all the maps generated by the agent up to this point share common id column so the DataLoader can collate them properly. If this test fails, the agent has to go back to the start and figure out which codes to correct given the error raised in the meta-unit test to eventually correct its mistakes. This paragraph highlights the usefulness of the unit tests, showing that they do not only serve as checks for the agent, but that they also provide useful error messages to fuel effective reflection. The meta-unit tests also provide additional verifications that can only be done once group of stages have passed their own individual unit tests. However, there is always limit to what we can test. It is possible that the agent finds way to pass all unit tests but ends up setting up the competition incorrectly. Because we want our unit tests to remain task-agnostic we cannot extend our coverage indefinitely. In the future we would like to extend this work to also use the agent to generate additional unit tests that would be specific to the current competition being setup. In that way we can extend the coverage at inference-time. A.2 Hyperparameters for Generating Solutions When training solution generated by Agent v1.0 we allow for BO over some parameters, i.e. the learning rate and the optimizer choice. The training is run for maximum of n_trials trials, each trial being limited to max_epochs epochs or max_time, whichever is attained first. Finally, blending is used at least after blend_after solutions have been generated. Values are specified in Table 5. A.3 Pseudocode We give in Algorithms 1 and 2 the pseudocode for the setup pipeline and the pseudocode for the solution generation pipeline, respectively. Note that the input of the setup pipeline is the competition URL and the outputs are the setup files and the dataloaders. The solution generation pipeline takes as inputs the outputs of the setup pipeline and carries on generating code to create one or more submission files. The output of the solution generation pipeline is set of solutions along with their performance on the leaderboard. 48 Agent v1. Retry Budget Note that in the pseudocode, there are steps such as while . . . end while. In practice, we do not retry each step indefinitely, there is an upper bound on the number of retries we allow Agent v1.0 to do for each step. As soon as this number is reached we consider the agent to be stuck and we end the episode by raising an error. Algorithm 1 Pseudocode for the Setup Pipeline Inputs: URL Every while loop in the pipeline is subject to retry limit FetchAndScrapeData(URL) SummariseDataAndTaskDetails(D) {X , } DetectModalities(S) CreatePlan({X , }, S) while meta-unit test fails do while unit test fails do"
        },
        {
            "title": "Mtrain\nend while\nwhile unit test fails do",
            "content": "input CreateTrainInputMaps(D, S, {X , }, P) target CreateTrainTargetMaps(D, S, {X , }, P) train target CreateTrainTargetTransforms(D, S, {X , }, P) end while end while while meta-unit test fails do while unit test fails do"
        },
        {
            "title": "Mtest\nend while",
            "content": "input CreateTestInputMaps(D, S, {X , }, P) end while while unit test fails do Fsubmission CreateSubmissionFormat(D, S, {X , }, P) end while while unit test fails do Fmetric CreateMetric(D, S, {X , }, P) end while Outputs: Train and test dataloaders DLtrain and DLtest A.4 Submission Scoring The solution generation pipeline potentially generates multiple solutions and blends of these individual submissions. Because the competitions in our benchmark are already 49 Agent v1. Algorithm 2 Pseudocode for the Solution Generation Pipeline target, Mtest Inputs: Ssetup {D, S, {X , }, P, Mtrain DLtrain and DLtest, Embedders {}, Heads {}, Solutions Sol {} input, Mtrain input, train target, Fsubmission, Fmetric}, if tabular-only task then AdoptClassicalMLFramework() Sol UseRampTool(Ssetup, DLtrain, DLtest) else AdoptDNNFramework() while Agent decides to improve performance do if Ssetup has tabular inputs then Ff.e. TabularFeatureEngineering(Ssetup, DLtrain) Etab TabularEmbedding(Ssetup, DLtrain) {Ff.e., Etab} end if if Ssetup has image inputs then Fimg ImageTransforms(Ssetup, DLtrain) Eimg ImageEmbedding(Ssetup, DLtrain) {Fimg, Eimg} end if if Ssetup has text inputs then Ftxt TextPreprocessing(Ssetup, DLtrain) Etxt TextEmbedding(Ssetup, DLtrain) {Ftxt, Etxt} end if Create Input Encoder(s) Create Target Head(s) Htab TabularHead(Ssetup) {Htab} Generate submission.csv and get validation score sub, val TrainSolutionAndPredict(Ssetup, DLtrain, DLtest, , H) Sol Sol {sub} if enough solutions created then Blend existing solutions sub, val BlendSolutionsAndPredict(Ssetup, DLtrain, DLtest, , H, Sol) Sol Sol {sub} end if end while Send submissions and get private score and rank SendSubmissionandGetLeaderboardRank(sub) end if closed, we get immediate access to both public and private score. However, in order to simulate realistic submission process as if we participated in an ongoing competition, we select our submissions based only on their public score. That is to say, from all solu50 Agent v1. tions our agent submits to Kaggle, we rank them by their public score and select the top-k solutions from the public leaderboard (k 1 is specified by each competition). We record the private scores or the submissions and report the best among these as our final score on that competition, which reproduces the way final ranks were determined on Kaggle for each competition. Note that for few ongoing competitions or competitions only providing public leaderboard, Agent v1.0s final score corresponds to the best public score among its submissions."
        },
        {
            "title": "B Benchmark Details",
            "content": "(a) Different Metric functions with respect to all input modalities. (b) Competition Start Dates with respect to all input modalities. (c) Competition Types with respect to all input modalities. Figure 15. Overview of competition metrics, start dates, and types across different input modalities. This is computed over the 79 benchmark tasks we consider. In this section we provide some insights on the competitions presented in our Benchmark. All competitions are listed on kaggle.com and are accessible through their API provided you create an account. Table 6 lists all competitions included in our benchmark and provides their respective URL. Figure 15 shows different statistics about the selected tasks. Figure 15a shows that there is varied set of metrics. While some of them are standard metrics, such as RMSE, the tail of the distribution includes less common metrics such as the median absolute error, or the quadratic weighted kappa. Metrics relate to the nature of the competitions, whose distribution is presented in Figure 15c. Most competitions are regressions or classifications with few being more complex multi-target tasks. Finally, Figure 15b shows the starting year of the competitions included in our benchmark. Agent v1.0 Table 6. List of Competitions in the Benchmark with their modality and task type. MultiCls is multiclass classification task (single target), MultiTgtCls is multi-target classification task (potentially multiclass as well). BinCls is for binary classification, Reg is for regression and MultiTgtReg is for multi-target regression. ID Competition Modality Task Type Competition # Teams 1 avito-prohibited-content 2 bike-sharing-demand 3 cat-in-the-dat 4 cat-in-the-dat-ii 5 competitive-data-sciencepredict-future-sales 6 diabetic-retinopathydetection 7 digit-recognizer 8 dog-breed-identification 9 dogs-vs-cats-redux-kernelsedition txt, tab tab tab tab tab img img img img 10 dont-overfit-ii 11 forest-cover-type-prediction 12 goodreads-books-reviewstab tab txt, tab 13 h2oai-predict-the-llm 14 histopathologic-cancerdetection 15 home-data-for-ml-course 16 house-prices-advancedregression-techniques 17 human-protein-atlasimage-classification 18 humpback-whaleidentification 19 invasive-species-monitoring 20 jigsaw-toxic-commentclassification-challenge 21 kaggle-pog-series-s01e03 22 leaf-classification 23 mercedes-benz-greenermanufacturing txt img tab tab img img txt BinCls Reg BinCls BinCls Reg featured playground playground playground community 284 3242 1338 1161 16851 MultiCls featured 660 MultiCls MultiCls BinCls BinCls MultiCls MultiCls MultiCls BinCls gettting_started playground playground playground playground community community playground 1395 1280 1314 2315 1692 261 92 1149 Reg Reg community gettting_started 106930 4399 img MultiTgtCls featured 2160 MultiCls featured 2120 BinCls MultiTgtCls playground featured img, tab img, tab tab MultiCls MultiCls Reg community playground featured Continued on next page 52 511 4539 91 1595 3823 ID Competition Modality Task Type Competition # Teams Agent v1.0 img MultiCls community 24 ml-olympiad-landscapeimage-classification 25 nitro-language-processing-2 26 nitro-language-processing-3 27 nlp-getting-started 28 nlp1000-ml-challenge 29 nlpsci 30 noaa-right-whalerecognition 31 otto-group-productclassification-challenge 32 plant-pathology-2020-fgvc7 33 plant-seedlingsclassification 34 planttraits2024 35 playground-series-s3e1 36 playground-series-s3e11 37 playground-series-s3e14 38 playground-series-s3e15 39 playground-series-s3e16 40 playground-series-s3e19 41 playground-series-s3e22 42 playground-series-s3e24 43 playground-series-s3e25 44 playground-series-s3e26 45 playground-series-s3e3 46 playground-series-s3e5 47 playground-series-s3e7 48 playground-series-s3e9 49 playground-series-s4e1 50 playground-series-s4e2 51 playground-series-s4e4 52 playground-series-s4e5 53 playground-series-s4e9 54 porto-seguro-safe-driverprediction 13 46 45 856 16 44 364 1317 833 398 689 952 1875 693 1429 1172 1541 1908 1632 1661 665 901 678 765 3632 3587 2606 2788 3066 txt txt txt, tab txt txt img tab img img MultiCls BinCls BinCls BinCls MultiCls MultiCls community community gettting_started community community research MultiCls featured"
        },
        {
            "title": "MultiCls\nMultiCls",
            "content": "research playground research playground playground playground playground playground playground playground playground playground playground playground playground playground playground playground playground playground playground playground featured img, tab MultiTgtReg tab tab tab tab tab tab tab tab tab tab tab tab tab tab tab tab tab tab tab tab Reg Reg Reg Reg Reg Reg MultiCls BinCls Reg MultiCls BinCls Reg BinCls Reg BinCls MultiCls Reg Reg Reg BinCls Continued on next page 53 Agent v1.0 ID Competition Modality Task Type Competition # Teams 55 restaurant-revenueprediction 56 sberbank-russian-housingmarket 57 sbu-ai-finalproject 58 scrabble-player-rating 59 sentiment-analysis-onmovie-reviews 60 sf-crime 61 sign-language-imageclassification 62 sp-society-camera-modelidentification 63 state-farm-distracteddriver-detection tab tab txt tab txt tab img img img Reg Reg MultiCls Reg MultiCls MultiCls MultiCls featured featured 3264 community playground playground playground community 37 301 860 2331 MultiCls featured 580 MultiCls featured 64 statoil-iceberg-classifierimg, tab BinCls featured 3330 challenge 65 stumbleupon 66 tabular-playground-seriesapr-2021 67 tabular-playground-seriesaug-2021 68 tabular-playground-seriesaug-2022 69 tabular-playground-seriesfeb-2021 70 tabular-playground-seriesjan-2021 71 tabular-playground-seriesjan-2022 72 tabular-playground-seriesmay-2022 73 tabular-playground-seriessep-2022 txt, tab tab BinCls BinCls featured playground 624 1244 tab tab tab tab tab tab tab Reg playground 1753 BinCls playground 1888 Reg Reg Reg playground 1433 playground 1728 playground 1591 BinCls playground 1151 Reg playground 1381 74 tmdb-box-office-prediction 75 vehicle 76 ventilator-pressuretxt, tab img tab Reg MultiCls Reg playground community research 1395 65 2605 prediction Continued on next page 54 Agent v1.0 ID Competition Modality Task Type Competition # Teams 77 whereami 78 world-championship-2023embryo-classification 79 5-flowers-imageclassification img img img MultiCls BinCls community community MultiCls community 211"
        },
        {
            "title": "C Additional results",
            "content": "We show on Figure 16 the success rate of each stage of the setup pipeline for the different type of tasks. As described in Section 3.1, the stages to complete depends on the nature of the task and notably of the modalities of the input data. Nevertheless, the lower success rate is always associated to the creation of the submission format, which is still higher than 80%, irrespective of the input modalities. (a) All tasks (c) Computer vision tasks (b) Tabular tasks (d) Natural Language Processing task Figure 16. Percentage of success for each stage of the setup pipeline per task type. 55 Agent v1."
        },
        {
            "title": "D Examples of Prompts and Responses",
            "content": "Below are some examples of prompts and answers from the LLM that our agent can query. Even though they are presented hereafter in chat format, note that the LLM does not keep memory of the chat as generally thought of in recent chat-based systems. The prompts are long because at each interaction we add all the necessary information about the specific step the agent is performing, knowing that the queried LLM has no context or records of previous interactions. The prompts are therefore shortened for the purpose of display. The paths in the code snippets are taken from our runs so we also anonymised them e.g. /PATH/TO/RAW/DATA/... or /PATH/TO/WORKSPACE/... Note on Prompts The agent is performing complex task broken up into sequence of smaller subtasks. These include summarising information and also past responses. These summaries are the main bulk of the text present in the prompts. They include summarization of the competition description, the raw data and all past steps the agent has done if relevant. The other big part of the prompts are views. These are usually raw text output from terminal command such as $lsor python print() statement, e.g. the view of the raw data directory or head view of some dataframes along with their column types, etc. Finally, some prompts can contain examples of similar situations seen in similar competitions that the agent can pick itself and reuse as examples through RAG. The rest is set of general instructions manually written. D.1 Setup The example below is taken from the setup of the competition leaf-classification which has tabular and image inputs. We show examples of prompts and responses from the LLM in the particular case where the agent creates plan for coding and then fill code template based on the plan. They contain all the necessary information about the task and instructions about the specific step the agent is currently doing for successful answer. In the particular case below, the LLM is prompted to generate plan and then the code to create the training map for the image modality, having already done the tabular modality. Prompt # Data Description Dataset Description The dataset consists approximately 1,584 images of leaf specimens (16 samples each of 99 species) which have been converted to binary black leaves against white backgrounds. Three sets of features are also provided per image: shape contiguous descriptor, an interior texture histogram, and fine-scale margin histogram. For each feature, 64-attribute vector is given per leaf sample. Note that of the original 100 species, we have eliminated one on account of incomplete associated data in the original dataset. File descriptions train.csv - the training set 56 Agent v1.0 test.csv - the test set sample_submission.csv - sample submission file in the correct format images - the image files (each image is named with its corresponding id) Data fields id - an anonymous id unique to an image margin_1, margin_2, margin_3, ..., margin_64 - each of the 64 attribute vectors for the margin feature shape_1, shape_2, shape_3, ..., shape_64 - each of the 64 attribute vectors for the shape feature texture_1, texture_2, texture_3, ..., texture_64 - each of the 64 attribute vectors for the texture feature ### View of the top-level directory, these are the data you can use to solve this task: /PATH/TO/RAW/DATA +-leaf-classification/ +-images/ +-226.jpg +-342.jpg +-1534.jpg +-... <additional files> +-train.csv +-test.csv # Modalities Description For the leaf classification task, the input modalities needed to solve the task are as follows: ### Input Modalities - **Tabular Input Map:** - **YES** because the dataset includes tabular data with pre-extracted features (margin, shape, and texture). - **Related Files:** - train.csv - test.csv - **Image Input Map:** - **YES** as there are images of leaf specimens that can be used for additional feature extraction or as an alternative input. - **Related Files:** - images/ (directory containing the leaf images) 57 Agent v1. - **Text Input Map:** - **NO** because there is no presence of text modality in this task. ### Summary - **Tabular Input Map:** YES -> Related Files: train.csv, test.csv - **Image Input Map:** YES -> Related Files: images/ - **Text Input Map:** NO This structured approach ensures that all relevant input modalities are considered and utilized for the leaf classification task. ### These are some of the tables you have asked information about: #### View of table /PATH/TO/RAW/DATA/leaf-classification/train.csv: /PATH/TO/RAW/DATA/leaf-classification/train.csv ... <Skipped for prompt display purposes in paper appendix> ... [2 rows 194 columns] <class pandas.core.frame.DataFrame> RangeIndex: 990 entries, 0 to 989 Columns: 194 entries, id to texture64 dtypes: float64(192), int64(1), object(1) memory usage: 1.5+ MB - column species contains strings with values in [Acer_Opalus , Pterocarya_Stenoptera, Quercus_Hartwissiana, Tilia_Tomentosa, Quercus_Variabilis, Magnolia_Salicifolia, Quercus_Canariensis, Quercus_Rubra , Quercus_Brantii, Salix_Fragilis, Zelkova_Serrata, Betula_Austrosinensis, Quercus_Pontica, Quercus_Afares, Quercus_Coccifera, Fagus_Sylvatica, Phildelphus, Acer_Palmatum, Quercus_Pubescens, Populus_Adenopoda, Quercus_Trojana, Alnus_Sieboldiana, Quercus_Ilex, Arundinaria_Simonii, Acer_Platanoids, Quercus_Phillyraeoides, Cornus_Chinensis, Liriodendron_Tulipifera, Cytisus_Battandieri, Rhododendron_x_Russellianum, Alnus_Rubra, Eucalyptus_Glaucescens, Cercis_Siliquastrum, Cotinus_Coggygria, Celtis_Koraiensis, Quercus_Crassifolia, Quercus_Kewensis, Cornus_Controversa, Quercus_Pyrenaica, Callicarpa_Bodinieri, Quercus_Alnifolia, Acer_Saccharinum, Prunus_X_Shmittii, Prunus_Avium, Quercus_Greggii, Quercus_Suber, Quercus_Dolicholepis, 58 Agent v1. Ilex_Cornuta, Tilia_Oliveri, Quercus_Semecarpifolia, Quercus_Texana, Ginkgo_Biloba, Liquidambar_Styraciflua, Quercus_Phellos, Quercus_Palustris, Alnus_Maximowiczii , Quercus_Agrifolia, Acer_Pictum, Acer_Rufinerve, Lithocarpus_Cleistocarpus, Viburnum_x_Rhytidophylloides, Ilex_Aquifolium, Acer_Circinatum, Quercus_Coccinea, Quercus_Cerris, Quercus_Chrysolepis, Eucalyptus_Neglecta , Tilia_Platyphyllos, Alnus_Cordata, Populus_Nigra, Acer_Capillipes, Magnolia_Heptapeta, Acer_Mono, Cornus_Macrophylla, Crataegus_Monogyna, Quercus_x_Turneri, Quercus_Castaneifolia, Lithocarpus_Edulis, Populus_Grandidentata, Acer_Rubrum, Quercus_Imbricaria, Eucalyptus_Urnigera, Quercus_Crassipes, Viburnum_Tinus, Morus_Nigra, Quercus_Vulcanica, Alnus_Viridis, Betula_Pendula, Olea_Europaea, Quercus_Ellipsoidalis, Quercus_x_Hispanica, Quercus_Shumardii, Quercus_Rhysophylla, Castanea_Sativa, Ulmus_Bergmanniana , Quercus_Nigra, Salix_Intergra, Quercus_Infectoria_sub, Sorbus_Aria] Your goal is to create map for the training image inputs. The first column is the \"id\" containing the unique ID of each data point, you need to create id so it can be used later to match each data point to their target. For example, the filename name can be use as unique identifier. The subsequent columns are the paths to the training input images for train_img_input_map.csv. Do not include target columns in this table. Now, write **short** (no more than 3 steps) plan of how you would do that. If there is an error in previous attempt, explain in **one** sentence why this error happened and how to fix it. Do not write code yet, simply the plan. Assistant ### Plan to Create the Training Image Input Map 1. **Extract Image IDs and Paths:** - Read the train.csv file to extract the id column, which contains the unique IDs for each image. - Construct the image file paths using the id values and the directory path to the images (e.g., images/226.jpg). 2. **Create the Image Input Map CSV:** - Create new DataFrame with two columns: id and image_path. - Populate the image_path column with the constructed file paths. 59 Agent v1. 3. **Save the Image Input Map CSV:** - Save the DataFrame to CSV file named train_img_input_map.csv. ### Error Handling - If the id values in train.csv do not match the filenames in the images/ directory, ensure that the filenames are correctly formatted and match the id values exactly. Prompt <Skipped first part of prompt repeating the previous prompt for display purposes in paper appendix> ### Create train_img_input_map.csv Your goal is to create map for the training image inputs. The first column of that map must be named \"id\" and must contain the unique ID of each data point. If this column exists already in the data, make sure to rename it to \"id\", otherwise create it. The subsequent columns must be the paths to the training input images. Do not include target columns in the table you are creating, this will be done later in another table. ### This is the code you helped write earlier to create the train tabular inputs: 1 \"\"\" 2 This script creates the data map for the tabular training inputs, i.e. train_tab_input_map.csv. 3 This map is pd.DataFrame object populated with respect to the data description and the data present in the raw data directory. 4 The first column of the created map must be the \"id\" of each test data point. 5 The subsequent columns of train_tab_input_map.csv are the tabular training input features, if any. 6 7 >>> # Example 8 >>> train_tab_input_map = pd.DataFrame(columns=[\"id\", \" tab_input_feature1\", \"tab_input_feature2\", ...]) 9 10 Create train_tab_input_map 11 \"\"\" 12 # <fim_begin> 13 import os 14 import pandas as pd 15 16 import os 17 import pandas as pd 60 Agent v1.0 18 19 # Define the absolute path to the train.csv file 20 train_csv_path = \"/PATH/TO/RAW/DATA/leaf-classification/train. csv\" 21 22 # Read the train.csv file 23 train_df = pd.read_csv(train_csv_path) 24 25 # Select the relevant columns: id and all feature columns, excluding the species column 26 train_tab_input_map = train_df.drop(columns=[species]) 27 28 # Save the new DataFrame to CSV file 29 train_tab_input_map.to_csv(\"./train_tab_input_map.csv\", index= False) 30 print(\"train_tab_input_map.csv created and saved.\") 32 33 # <fim_end> 34 # save 35 train_tab_input_map.to_csv(\"./train_tab_input_map.csv\", index= False) 36 print(\"train_tab_input_map.csv created and saved.\") The script reads the train.csv file, selects the id and feature columns (excluding species), and saves the result to train_tab_input_map.csv. ### Complete the code Complete the following code template in order to create the data map train_img_input_map.csv: 1 \"\"\" 2 This script creates the data map for the image training inputs , i.e. train_img_input_map.csv. 3 This map is pd.DataFrame object populated with respect to the data description and the data present in the raw data directory. 4 The first column of the created map must be the \"id\" of each test data point. 5 The subsequent columns of train_img_input_map.csv are the paths to the training input images, if any. 6 7 >>> # Example 8 >>> train_img_input_map = pd.DataFrame(columns=[\"id\", \" img_input1\", \"img_input2\", ...]) 9 10 Create train_img_input_map 11 \"\"\" 61 Agent v1. 12 # <fim_begin> 13 import os 14 import pandas as pd 15 # <fim_hole> 16 17 # <fim_end> 18 # save 19 train_img_input_map.to_csv(\"./train_img_input_map.csv\", index= False) 20 print(\"train_img_input_map.csv created and saved.\") Pay EXTREME attention to the path where you load from. You should always provide absolute path to avoid FileNotFound error. Do not re-set index of the dataframe to create new id column, instead use the columns present in the dataframe already. Create new index column only when absolutely necessary. Finally, the step to save the final table is already written in the template - do not rewrite this part. The map is saved in the current directory on purpose. Tip: IF you are using the image file name as id, make sure they are converted to strings. ### Plan: ### Plan to Create the Training Image Input Map 1. **Extract Image IDs and Paths:** - Read the train.csv file to extract the id column, which contains the unique IDs for each image. - Construct the image file paths using the id values and the directory path to the images (e.g., images/226.jpg). 2. **Create the Image Input Map CSV:** - Create new DataFrame with two columns: id and image_path. - Populate the image_path column with the constructed file paths. 3. **Save the Image Input Map CSV:** - Save the DataFrame to CSV file named train_img_input_map.csv. ### Error Handling - If the id values in train.csv do not match the filenames in the images/ directory, ensure that the filenames are correctly formatted and match the id values exactly. Your response should be the code to complete the above template, in the format: python code 62 Agent v1. Assistant \"\"\" 1 2 This script creates the data map for the image training inputs , i.e. train_img_input_map.csv. 3 This map is pd.DataFrame object populated with respect to the data description and the data present in the raw data directory. 4 The first column of the created map must be the \"id\" of each test data point. 5 The subsequent columns of train_img_input_map.csv are the paths to the training input images, if any. 6 7 >>> # Example 8 >>> train_img_input_map = pd.DataFrame(columns=[\"id\", \" img_input1\", \"img_input2\", ...]) 9 10 Create train_img_input_map 11 \"\"\" 12 # <fim_begin> 13 import os 14 import pandas as pd 15 16 # Define the absolute path to the train.csv file 17 train_csv_path = \"/PATH/TO/RAW/DATA/leaf-classification/train. csv\" 18 images_dir = \"/PATH/TO/RAW/DATA/leaf-classification/images/\" 19 20 # Read the train.csv file 21 train_df = pd.read_csv(train_csv_path) 22 23 # Ensure the id column is string 24 train_df[id] = train_df[id].astype(str) 25 26 # Construct the image file paths 27 train_df[image_path] = train_df[id].apply(lambda x: os. path.join(images_dir, f\"{x}.jpg\")) 28 29 # Create the image input map DataFrame 30 train_img_input_map = train_df[[id, image_path]] 31 32 # <fim_end> 33 # save 34 train_img_input_map.to_csv(\"./train_img_input_map.csv\", index= False) 35 print(\"train_img_input_map.csv created and saved.\") 63 Agent v1.0 This is during the setup of D.2 Setup with RAG Example In the example below we show how an example of prompt containing an example retrieve looks like. the competition l-olympiad-landscape-image-classification and in particular when generating the code for the training map for the image modality. The previous attempt had en error in the unit test which is shown in the prompt as the agent is asked to retry. The RAG engine retrieves some similar example from competition already setup. In that other competition, namely kaggle-pog-series-s01e03, the agent had similar issue and managed to fix it. These steps are retrieved and displayed in the current prompt so that the agent can take inspiration from that similar error being fixed and try to adapt fix to the current issue it is facing. Prompt ### Data Description Dataset Description The data contains masked images with random patches. These random patches can cover anything between 20% to 60% of the image. Your goal is to correctly predict the classes of the images from the \"test\" directory. All images are 256x256 having 3 channels. Files train.csv - the training set test.csv - the test set sample.csv - sample submission file in the correct format Columns image - Path of the image relative to directory. Eg. In train.csv, mountain/12345.jpg is relative to the \"train\" directory, whereas in test.csv, 12345.jpg is relative to the \"test\" directory. label - True label of the image (only in train.csv) . Submission files should have these column headers, but not the column numbers. Use df.to_csv(filepath, index=False) to save without the column numbers. ### View of the top-level directory, these are the data you can use to solve this task: /PATH/TO/RAW/DATA +-ml-olympiad-landscape-image-classification/ +-train/ +-forest/ +-64935341.jpg +-89262487.jpg +-16354101.jpg 64 Agent v1. +-... <additional files> +-buildings/ +-15220800.jpg +-30615322.jpg +-48061322.jpg +-... <additional files> +-glacier/ +-10983747.jpg +-72741892.jpg +-11747834.jpg +-... <additional files> +-street/ +-16518706.jpg +-88133361.jpg +-17503215.jpg +-... <additional files> +-mountain/ +-11476151.jpg +-14340600.jpg +-72470093.jpg +-... <additional files> +-... <additional folders> +-test/ +-62449337.jpg +-18432742.jpg +-84761017.jpg +-... <additional files> +-test.csv +-train.csv ### Modalities Description ### Input Modalities for the Task - **tabular input map**: NO because there is no presence of tabular data (other than the image paths and labels, which are not used as features in the model). - **image input map**: YES as there are images in this task -> related files: - classification/train/ (training images) - /PATH/TO/RAW/DATA/ml-olympiad-landscape-image-classification/test/ (test images) /PATH/TO/RAW/DATA/ml-olympiad-landscape-image65 Agent v1. - **text input map**: NO because there is no presence of text modality in this task. ### Summary - **Tabular Input**: NO - **Image Input**: YES (related files: train/, test/) - **Text Input**: NO ### These are some tips to avoid common mistakes in the code: - To avoid the error related to non-unique filenames, append the class name to the filenames to ensure they are unique. For example, change 12345678.jpg in the forest directory to forest_12345678.jpg. This can be done by concatenating the class name with the filename, separated by an underscore or another delimiter. - To avoid the File not found error, ensure that all file paths listed in the train.csv and test.csv files are correct and that the corresponding files exist in the dataset. You can write script to verify the existence of all files listed in these CSV files. ### Create train_img_input_map.csv Your goal is to create map for the training image inputs. The first column of that map must be named \"id\" and must contain the unique ID of each data point. If this column exists already in the data, make sure to rename it to \"id\", otherwise create it. The subsequent columns must be the paths to the training input images. Do not include target columns in the table you are creating, this will be done later in another table. ### This was the resulting code after your last attempt: 1 \"\"\" 2 This script creates the data map for the image training inputs , i.e. train_img_input_map.csv. 3 This map is pd.DataFrame object populated with respect to the data description and the data present in the raw data directory. 4 The first column of the created map must be the \"id\" of each test data point. 5 The subsequent columns of train_img_input_map.csv are the paths to the training input images, if any. 6 7 >>> # Example 8 >>> train_img_input_map = pd.DataFrame(columns=[\"id\", \" img_input1\", \"img_input2\", ...]) Agent v1.0 9 10 Create train_img_input_map 11 \"\"\" 12 # <fim_begin> 13 import os 14 import pandas as pd 15 16 \"\"\" 17 This script creates the data map for the image training inputs , i.e. train_img_input_map.csv. 18 This map is pd.DataFrame object populated with respect to the data description and the data present in the raw data directory. 19 The first column of the created map must be the \"id\" of each test data point. 20 The subsequent columns of train_img_input_map.csv are the paths to the training input images, if any. 21 22 >>> # Example 23 >>> train_img_input_map = pd.DataFrame(columns=[\"id\", \" img_input1\", \"img_input2\", ...]) 24 25 Create train_img_input_map 26 \"\"\" 27 # <fim_begin> 28 import os 29 import pandas as pd 30 31 # Load the training data 32 train_df = pd.read_csv(\"/PATH/TO/RAW/DATA/ml-olympiadlandscape-image-classification/train.csv\") 33 34 # Extract the image filenames and use them as unique IDs 35 train_df[id] = train_df.apply(lambda row: f\"{row[label]}_{ row[image].split(/)[-1]}\", axis=1) 36 37 # Ensure that the filenames are unique 38 if train_df[id].nunique() != len(train_df): 39 raise ValueError(\"Filenames are not unique. Please append unique identifier to duplicate filenames.\") 40 41 # Create the input map DataFrame 42 train_img_input_map = train_df[[id, image]].rename(columns ={image: img_input1}) 43 44 # <fim_end> 45 # save 67 Agent v1.0 46 train_img_input_map.to_csv(\"./train_img_input_map.csv\", index= False) 47 print(\"train_img_input_map.csv created and saved.\") 49 50 # <fim_end> 51 # save 52 train_img_input_map.to_csv(\"./train_img_input_map.csv\", index= False) 53 print(\"train_img_input_map.csv created and saved.\") ### This was the output: train_img_input_map.csv created and saved. train_img_input_map.csv created and saved. ### The error in the unit test was: multiprocessing.pool.RemoteTraceback: \"\"\" Traceback (most recent call last): File \"/home/jdoran/uv_python/cpython-3.10.15-linux-x86_64-gnu /lib/python3.10/multiprocessing/pool.py\", line 125, in worker result = (True, func(*args, **kwds)) File \"/home/jdoran/uv_python/cpython-3.10.15-linux-x86_64-gnu /lib/python3.10/multiprocessing/pool.py\", line 48, in mapstar return list(map(*args)) File \"/PATH/TO/WORKSPACE/ml-olympiad-landscape-imageclassification/seed_0/unit_test_map_img_input_train.py\", line 45, in check_file_exists raise FileNotFoundError(f\"File not found: {file_path}\") FileNotFoundError: File not found: glacier/59804579.jpg \"\"\" The above exception was the direct cause of the following exception: Traceback (most recent call last): File \"/PATH/TO/WORKSPACE/ml-olympiad-landscape-imageclassification/seed_0/unit_test_map_img_input_train.py\", line 65, in <module> check_files(files_to_check) File \"/PATH/TO/WORKSPACE/ml-olympiad-landscape-imageclassification/seed_0/unit_test_map_img_input_train.py\", line 58, in check_files 68 Agent v1.0 results = pool.map(check_file_exists, file_list) File \"/home/jdoran/uv_python/cpython-3.10.15-linux-x86_64-gnu /lib/python3.10/multiprocessing/pool.py\", line 367, in map return self._map_async(func, iterable, mapstar, chunksize). get() File \"/home/jdoran/uv_python/cpython-3.10.15-linux-x86_64-gnu /lib/python3.10/multiprocessing/pool.py\", line 774, in get raise self._value FileNotFoundError: File not found: glacier/59804579.jpg You should fix this error next. ### An example of similar error being fixed: Failing code: 1 \"\"\" 2 This script creates the data map for the image training inputs , i.e. train_img_input_map.csv. 3 This map is pd.DataFrame object populated with respect to the data description and the data present in the raw data directory. 4 The first column of the created map must be the \"id\" of each test data point. 5 The subsequent columns of train_img_input_map.csv are the paths to the training input images, if any. 6 7 >>> # Example 8 >>> train_img_input_map = pd.DataFrame(columns=[\"id\", \" img_input1\", \"img_input2\", ...]) 9 10 Create train_img_input_map 11 \"\"\" 12 # <fim_begin> 13 import os 14 import pandas as pd 15 16 \"\"\" 17 This script creates the data map for the image training inputs , i.e. train_img_input_map.csv. 18 This map is pd.DataFrame object populated with respect to the data description and the data present in the raw data directory. 19 The first column of the created map must be the \"id\" of each test data point. 20 The subsequent columns of train_img_input_map.csv are the paths to the training input images, if any. 69 Agent v1.0 22 >>> # Example 23 >>> train_img_input_map = pd.DataFrame(columns=[\"id\", \" img_input1\", \"img_input2\", ...]) 24 25 Create train_img_input_map 26 \"\"\" 27 # <fim_begin> 28 import os 29 import pandas as pd 30 31 # Define the absolute path to the train.csv file 32 train_csv_path = \"/PATH/TO/RAW/DATA/kaggle-pog-series-s01e03/ train.csv\" 33 34 # Load the train.csv file into DataFrame 35 train_df = pd.read_csv(train_csv_path) 36 37 # Generate unique ID by concatenating seed_id and view 38 train_df[id] = train_df[seed_id].astype(str) + _ + train_df[view] 39 40 # Check for duplicates in the generated ID 41 if train_df[id].duplicated().any(): raise ValueError(\"There are duplicate IDs in the generated unique ID column.\") 43 44 # Construct the full path to each image 45 base_image_dir = \"/PATH/TO/RAW/DATA/kaggle-pog-series-s01e03/ train/\" 46 train_df[img_input1] = base_image_dir + train_df[image] 47 48 # Select the relevant columns: id and img_input1 49 train_img_input_map = train_df[[id, img_input1]] 50 51 # <fim_begin> 52 # save 53 train_img_input_map.to_csv(\"./train_img_input_map.csv\", index= False) 54 print(\"train_img_input_map.csv created and saved.\") 55 56 57 # <fim_end> 58 # save 59 train_img_input_map.to_csv(\"./train_img_input_map.csv\", index= False) 60 print(\"train_img_input_map.csv created and saved.\") 70 Agent v1.0 Failing code error: multiprocessing.pool.RemoteTraceback: \"\"\" Traceback (most recent call last): File \"/home/jdoran/uv_python/cpython-3.10.15-linux-x86_64-gnu /lib/python3.10/multiprocessing/pool.py\", line 125, in worker result = (True, func(*args, **kwds)) File \"/home/jdoran/uv_python/cpython-3.10.15-linux-x86_64-gnu /lib/python3.10/multiprocessing/pool.py\", line 48, in mapstar return list(map(*args)) File \"/PATH/TO/WORKSPACE/kaggle-pog-series-s01e03/seed_0/ unit_test_map_img_input_train.py\", line 45, in check_file_exists raise FileNotFoundError(f\"File not found: {file_path}\") FileNotFoundError: File not found: /PATH/TO/RAW/DATA/kaggle-pog -series-s01e03/train/train/03511.png \"\"\" The above exception was the direct cause of the following exception: Traceback (most recent call last): File \"/PATH/TO/WORKSPACE/kaggle-pog-series-s01e03/seed_0/ unit_test_map_img_input_train.py\", line 65, in <module> check_files(files_to_check) File \"/PATH/TO/WORKSPACE/kaggle-pog-series-s01e03/seed_0/ unit_test_map_img_input_train.py\", line 58, in check_files results = pool.map(check_file_exists, file_list) File \"/home/jdoran/uv_python/cpython-3.10.15-linux-x86_64-gnu /lib/python3.10/multiprocessing/pool.py\", line 367, in map return self._map_async(func, iterable, mapstar, chunksize). get() File \"/home/jdoran/uv_python/cpython-3.10.15-linux-x86_64-gnu /lib/python3.10/multiprocessing/pool.py\", line 774, in get raise self._value FileNotFoundError: File not found: /PATH/TO/RAW/DATA/kaggle-pog -series-s01e03/train/train/03511.png Fixed code: 1 \"\"\" 2 This script creates the data map for the image training inputs , i.e. train_img_input_map.csv. 71 Agent v1.0 3 This map is pd.DataFrame object populated with respect to the data description and the data present in the raw data directory. 4 The first column of the created map must be the \"id\" of each test data point. 5 The subsequent columns of train_img_input_map.csv are the paths to the training input images, if any. 6 7 >>> # Example 8 >>> train_img_input_map = pd.DataFrame(columns=[\"id\", \" img_input1\", \"img_input2\", ...]) 9 10 Create train_img_input_map 11 \"\"\" 12 # <fim_begin> 13 import os 14 import pandas as pd 15 16 \"\"\" 17 This script creates the data map for the image training inputs , i.e. train_img_input_map.csv. 18 This map is pd.DataFrame object populated with respect to the data description and the data present in the raw data directory. 19 The first column of the created map must be the \"id\" of each test data point. 20 The subsequent columns of train_img_input_map.csv are the paths to the training input images, if any. 21 22 >>> # Example 23 >>> train_img_input_map = pd.DataFrame(columns=[\"id\", \" img_input1\", \"img_input2\", ...]) 24 25 Create train_img_input_map 26 \"\"\" 27 # <fim_begin> 28 import os 29 import pandas as pd 30 31 # Define the absolute path to the train.csv file 32 train_csv_path = \"/PATH/TO/RAW/DATA/kaggle-pog-series-s01e03/ train.csv\" 33 34 # Load the train.csv file into DataFrame 35 train_df = pd.read_csv(train_csv_path) 36 37 # Generate unique ID by concatenating seed_id and view Agent v1.0 38 train_df[id] = train_df[seed_id].astype(str) + _ + train_df[view] 39 40 # Check for duplicates in the generated ID 41 if train_df[id].duplicated().any(): 42 raise ValueError(\"There are duplicate IDs in the generated unique ID column.\") 43 44 # Construct the full path to each image 45 base_image_dir = \"/PATH/TO/RAW/DATA/kaggle-pog-series-s01e03/ train/\" 46 train_df[img_input1] = base_image_dir + train_df[image]. str.split(/).str[-1] 47 48 # Select the relevant columns: id and img_input1 49 train_img_input_map = train_df[[id, img_input1]] 50 51 # <fim_end> 52 # save 53 train_img_input_map.to_csv(\"./train_img_input_map.csv\", index= False) 54 print(\"train_img_input_map.csv created and saved.\") 55 56 57 # <fim_end> 58 # save 59 train_img_input_map.to_csv(\"./train_img_input_map.csv\", index= False) 60 print(\"train_img_input_map.csv created and saved.\") ### Complete the code Complete the following code template in order to create the data map train_img_input_map.csv: 1 \"\"\" 2 This script creates the data map for the image training inputs , i.e. train_img_input_map.csv. 3 This map is pd.DataFrame object populated with respect to the data description and the data present in the raw data directory. 4 The first column of the created map must be the \"id\" of each test data point. 5 The subsequent columns of train_img_input_map.csv are the paths to the training input images, if any. 6 7 >>> # Example 8 >>> train_img_input_map = pd.DataFrame(columns=[\"id\", \" Agent v1.0 img_input1\", \"img_input2\", ...]) 9 10 Create train_img_input_map 11 \"\"\" 12 # <fim_begin> 13 import os 14 import pandas as pd 15 # <fim_hole> 16 17 # <fim_end> 18 # save 19 train_img_input_map.to_csv(\"./train_img_input_map.csv\", index= False) 20 print(\"train_img_input_map.csv created and saved.\") Pay EXTREME attention to the path where you load from. You should always provide absolute path to avoid FileNotFound error. Do not re-set index of the dataframe to create new id column, instead use the columns present in the dataframe already. Create new index column only when absolutely necessary. Finally, the step to save the final table is already written in the template - do not rewrite this part. The map is saved in the current directory on purpose. Tip: IF you are using the image file name as id, make sure they are converted to strings. ### Plan: 1. **Load the Training Data**: Read the train.csv file to get the image paths and labels. 2. **Create Unique IDs**: Generate unique ID for each image by concatenating the label and the image filename. 3. **Construct the Input Map**: Create DataFrame with the unique IDs and the corresponding image paths, and save it as train_img_input_map.csv. ### Error Explanation: The error occurred because the file path in the train.csv file was incorrect or the file did not exist in the specified directory. To fix it, ensure that the file paths in train.csv are correct and that the files exist in the train/ directory. Your response should be the code to complete the above template, in the format: python code 74 Agent v1. Assistant 1 \"\"\" 2 This script creates the data map for the image training inputs , i.e. train_img_input_map.csv. 3 This map is pd.DataFrame object populated with respect to the data description and the data present in the raw data directory. 4 The first column of the created map must be the \"id\" of each test data point. 5 The subsequent columns of train_img_input_map.csv are the paths to the training input images, if any. 6 7 >>> # Example 8 >>> train_img_input_map = pd.DataFrame(columns=[\"id\", \" img_input1\", \"img_input2\", ...]) 9 10 Create train_img_input_map 11 \"\"\" 12 # <fim_begin> 13 import os 14 import pandas as pd 15 16 # Define the absolute path to the train.csv file 17 train_csv_path = \"/PATH/TO/RAW/DATA/ml-olympiad-landscapeimage-classification/train.csv\" 18 19 # Load the train.csv file into DataFrame 20 train_df = pd.read_csv(train_csv_path) 21 22 # Generate unique ID by concatenating the label and the image filename 23 train_df[id] = train_df.apply(lambda row: f\"{row[label]}_{ row[image].split(/)[-1]}\", axis=1) 24 25 # Check for duplicates in the generated ID 26 if train_df[id].duplicated().any(): 27 raise ValueError(\"There are duplicate IDs in the generated unique ID column.\") 28 29 # Construct the full path to each image 30 base_image_dir = \"/PATH/TO/RAW/DATA/ml-olympiad-landscapeimage-classification/train/\" 31 train_df[img_input1] = base_image_dir + train_df[image] 32 33 # Select the relevant columns: id and img_input1 34 train_img_input_map = train_df[[id, img_input1]] 35 75 Agent v1. 36 # <fim_end> 37 # save 38 train_img_input_map.to_csv(\"./train_img_input_map.csv\", index= False) 39 print(\"train_img_input_map.csv created and saved.\") D.3 Solution Generation The example below is taken from the solution generation pipeline on the comeptition ogs-vs-cats-redux-kernels-edition. In this particular example, the agent has already done one submission which appears in some prompts. The agent is at the start of new submission, generating the plans and codes for preprocessing the images and the image embedding model. Prompt ## Task description ### Summary of the Task Description **Main Goal:** The goal of this competition is to classify images as either dogs or cats. The competition revisits the original \"Dogs vs. Cats\" challenge from 2013, but with modern machine learning techniques and tools, such as Kernels (Kaggles online coding environment) and TensorFlow. **Task Type:** This is binary classification task where the model must predict the probability that given image is dog. The evaluation metric is log loss, which measures the performance of the model by penalizing in correct predictions more severely as the predicted probability diverges from the actual label. **Key Points:** - **Dataset:** The dataset consists of images of dogs and cats. - **Evaluation Metric:** Log loss. - **Submission Format:** CSV file with the format id,label, where id is the image ID and label is the predicted probability that the image is dog. ## Data description ### View of the top-level directory /PATH/TO/RAW/DATA/dogs-vs-catsredux-kernels-edition: after having downloaded and extracted the raw data from the source, these are the data you can use to solve this task: /PATH/TO/RAW/DATA Agent v1.0 +-dogs-vs-cats-redux-kernels-edition/ +-test/ +-8715.jpg +-12271.jpg +-156.jpg +-... <additional files> +-train/ +-cat.8852.jpg +-dog.9444.jpg +-cat.12231.jpg +-... <additional files> ### Summary of Data Organization The dataset is organized as follows: #### Directory Structure /PATH/TO/RAW/DATA/dogs-vs-cats-redux-kernels-edition/ +-test/ +-8715.jpg +-12271.jpg +-156.jpg +-... (additional files) +-train/ +-cat.8852.jpg +-dog.9444.jpg +-cat.12231.jpg +-... (additional files) +-sample_submission.csv #### File Descriptions 1.**/PATH/TO/RAW/DATA/dogs-vs-cats-redux-kernels-edition/train/** - Contains 25,000 images of dogs and cats. - Each image filename includes the label: - cat.<number>.jpg for cat images. - dog.<number>.jpg for dog images. 2.**/PATH/TO/RAW/DATA/dogs-vs-cats-redux-kernels-edition/test/** - Contains 12,500 images. - Each image is named according to numeric ID (e.g., 8715.jpg). Agent v1.0 3.**/PATH/TO/RAW/DATA/dogs-vs-cats-redux-kernelsedition/sample_submission.csv** - sample submission file with the following columns: - id: The numeric ID of the test image. - label: The predicted probability that the image is dog (1 = dog, 0 = cat). #### Feature and Target Names - **Features:** - id: The numeric ID of the test image (used for the test set). - **Target:** - label: The predicted probability that the image is dog (1 = dog, 0 = cat). ### Note - The data is saved under /PATH/TO/RAW/DATA/dogs-vs-cats-redux-kernelsedition. - The directory and file names must remain unchanged. ## Available solutions This is the list of the solutions that have already been generated: * 2024-10-16-07-22-08: The provided code implements comprehensive pipeline for the Dogs vs. Cats classification task. It includes set of torchvision transforms to preprocess images, PyTorch class ImageEmbedder that uses pre-trained ResNet50 model to generate latent representations of images, and TabularHead model to decode these embeddings into classification logits. The pipeline also defines appropriate loss functions for both regression and classification tasks, ensuring robust and flexible approach to the problem. ## Current solution progress You have started creating new solution. Its status is as follows: - [PENDING] Embedder for images - [PENDING] Transform functions for images - [TODO] Embedder for transformed images - [TODO] Logits and regression targets decoder ## What to do next You have to generate code to transform the images using composiYou need to write pytorch torchvition of torchvision transforms. sion.transforms.Compose sequence that transforms the images appropriately with respect to the task. You need to make sure the images are also transformed to torch.Tensor at some point as the images will be fed to vision neural network. You can use any image transforms (Resize, RandomCrop, ToTensor, ...) or even write custom transforms with Lambda() ... 78 Agent v1.0 Output plan presented with maximum 4 bullet points describing the different steps to construct pytorch torchvision transforms composition. The plan should be tailored to the current task as it will be used to guide the implementation of the image transforms. Assistant ### Plan for Constructing PyTorch torchvision Transforms Composition 1. **Resize Images**: Resize all images to standard size (e.g., 224x224 pixels) to ensure they can be fed into pre-trained model like ResNet50, which expects input of this size. 2. **Apply Random Augmentations**: Use random augmentations such as RandomHorizontalFlip, RandomRotation, and ColorJitter to increase the diversity of the training data and improve the models generalization capabilities. 3. **Convert to Tensor**: Use ToTensor to convert the images to PyTorch tensors, which is required format for feeding the images into neural network. 4. **Normalize Images**: Apply normalization using the mean and standard deviation of the ImageNet dataset, as pre-trained models like ResNet50 are typically trained on ImageNet. This ensures the input data is in the expected range and format for the pre-trained model. Prompt <Skipped first part of prompt repeating the previous prompt for display purposes in paper appendix> ## Current solution progress You have started creating new solution. Its status is as follows: - [PENDING] Embedder for images - [PENDING] Transform functions for images - [TODO] Embedder for transformed images - [TODO] Logits and regression targets decoder ## What to do next You have to generate code to transform the images using composition of torchvision transforms. You need to write pytorch torchvision.transforms.Compose sequence that transforms the images appropriately with respect to the task. You need to make sure the images are also transformed to torch.Tensor at some point as the images will be fed to vision neural network. You can use any image transforms (Resize, RandomCrop, ToTensor, ...) Agent v1.0 or even write custom transforms with Lambda() ... For example good default transform is: 1 import numpy as np 2 import pandas as pd 3 import torch 4 from PIL import Image 5 from torch import Tensor 6 from torch.utils.data import Dataset 7 from torchvision import transforms as 8 9 DefaultImageInputTransform: T.Compose = T.Compose( [ 10 11 12 13 14 16 ] 17 18 ) T.Resize((512, 512)), # Resize to fixed size ( should make sense with respect to the task, not always needed) T.Lambda(lambda x: if isinstance(x, torch.Tensor) else T.ToTensor()(x)), if needed T.Lambda( # Convert image to tensor lambda x: x.repeat([3 if (i - x.ndim == -3 and == 1) else 1 for i, in enumerate(x.shape)]) # grayscale to rgb ), T.Lambda(lambda x: x[:3] if x.shape[0] > 3 else x), # take only first 3 channels Note that the last two transforms are using Lambda function. The penultimate one is an important one to include as its goal is to duplicate the number of channels to 3 if there happen to be grayscale images in the data mixed with colored images. The ultimate one is taking on ly the first 3 channels in case the images in the dataset have an inconsistent number of channels. These two last transforms hence force all images to have the same number of channels. If you are not sure all images in the data will have the same number of channels, it is advised to add similar transform. If you find it necessary to normalize the images, do it after taking care fo the number of channels and make sure the vectors you use match the image dimensions. Use your plans to help you implement the image transform: ### Plan for Constructing PyTorch torchvision Transforms Composition 1. **Resize Images**: Resize all images to standard size (e.g., 224x224 pixels) to ensure they can be fed into pre-trained model like ResNet50, which expects input of this size. 80 Agent v1.0 2. **Apply Random Augmentations**: Use random augmentations such as RandomHorizontalFlip, RandomRotation, and ColorJitter to increase the diversity of the training data and improve the models generalization capabilities. 3. **Convert to Tensor**: Use ToTensor to convert the images to PyTorch tensors, which is required format for feeding the images into neural network. 4. **Normalize Images**: Apply normalization using the mean and standard deviation of the ImageNet dataset, as pre-trained models like ResNet50 are typically trained on ImageNet. This ensures the input data is in the expected range and format for the pre-trained model. Write the code to create an image transforms composition. 1. You must fill all the required parameters of the transforms you are using in sensible way with respect to the image shapes of the current task. 2. You must not forget to import the packages when needed. 3. The transforms should be transforming the images to tensor at the appropriate step. 4. Try to use different transforms than this last attempt: The provided code implements comprehensive pipeline for the Dogs vs. Cats classification task. It includes set of torchvision transforms to preprocess images, PyTorch class ImageEmbedder that uses pre-trained ResNet50 model to generate latent repr esentations of images, and TabularHead model to decode these embeddings into classification logits. The pipeline also defines appropriate loss functions or both regression and classification tasks, ensuring robust and flexible approach to the problem.. Do not repeat all the transforms from the attempt. Your task is to fill the blank and implement the image transform in this code: 1 # <fim_begin> 2 import os 3 import pandas as pd 4 import torch 5 from torch import nn 6 from PIL import Image 7 import torchvision.transforms as 8 from torch.utils.data import DataLoader 9 from tqdm import tqdm 10 11 from agent.tools.data_map.map_dataset import MapDataset, map_dataset_collate_function 12 Agent v1.0 13 # --- Create CustomImageInputTransform, composition of torchvision.transforms using torchvision.transforms.Compose 14 # <fim_hole> 15 # --- [End] 16 17 # <fim_end> 18 # Test that the transform works 19 root_path = \"/PATH/TO/RAW/DATA/dogs-vs-cats-redux-kernelsedition/qwen2.5-72b-vp.3.5_seed_2\" 20 tab_target_map_path=os.path.join(root_path, \" train_tab_target_map.csv\") 21 img_target_map_path=os.path.join(root_path, \" train_img_target_map.csv\") 22 train_dataset = MapDataset( 24 25 img_input_map_path=os.path.join(root_path, \" train_img_input_map.csv\"), tab_target_map_path=tab_target_map_path if os.path.exists( tab_target_map_path) else None, img_target_map_path=img_target_map_path if os.path.exists( img_target_map_path) else None, img_input_transform=CustomImageInputTransform, 26 27 ) 28 train_dataloader = DataLoader( 29 train_dataset, batch_size=256, shuffle=True, collate_fn= map_dataset_collate_function, num_workers=20 30 ) 31 shapes = set() 32 for batch in tqdm(train_dataloader): 33 34 35 37 38 39 40 41 43 (indices, (_, img_inputs_batch, _), (tab_targets_batch, _, _)) = batch if img_inputs_batch.shape not in shapes: shapes.add(img_inputs_batch.shape[1:]) # ignore batch size as last batch can have fewer elements if len(shapes) > 1: raise RuntimeError( f\"Multiple shapes detected! After applying CusotmImageTransform, some images have shapes \" f\"{ and .join([str(s) for in shapes])} \" f\"but should have the same shape for all images after the transform. \" f\"(Note that the first dimension is the batch size {train_dataloader.batch_size}).n\" f\"Hint: if needed add lambda transform that ensures the number of channels is, e.g. 3 in this example:n\" f\"T.Lambda(lambda x: x.repeat([3 if (i - x.ndim == -3 and == 1) else 1 for i, in enumerate( 82 Agent v1.0 44 45 x.shape)])),\" f\"nandnT.Lambda(lambda x: x[:3] if x.shape[0] > 3 else x)\" ) 46 47 print(f\"Transformed image tensor shape: {img_inputs_batch. shape}\") Assistant 1 CustomImageInputTransform = T.Compose( T.Resize((256, 256)), T.RandomCrop(224), # Resize to standard size # Randomly crop to the input size expected by the model T.RandomHorizontalFlip(p=0.5), flip with 50% probability # Random horizontal T.ColorJitter(brightness=0.2, contrast=0.2, saturation =0.2, hue=0.1), # Random color jitter T.ToTensor(), # Convert image to tensor T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, # Normalize using ImageNet mean 0.224, 0.225]), and std T.Lambda( lambda x: x.repeat([3 if (i - x.ndim == -3 and == 1) else 1 for i, in enumerate(x.shape)]) # grayscale to rgb ), T.Lambda(lambda x: x[:3] if x.shape[0] > 3 else x), # take only first 3 channels [ 3 4 5 6 7 9 10 11 12 ] 13 14 ) Prompt <Skipped first part of prompt repeating the previous prompt for display purposes in paper appendix> ## Current solution progress You have started creating new solution. Its status is as follows: - [TODO] Embedder for text fields - [TODO] Logits and regression targets decoder ## What to do next You have to generate code to embed the images using pre-trained torchvision model. You need to write pytorch torchvision model, where forward() Agent v1.0 takes the tensor transformed images as input, and that outputs latent representation as ten sor. You can use any pre-trained model from this list: resnet18, resnet50, resnet101, AlexNet, EfficientNetV2, GoogLeNet, MaxVit, MNASNet, RegNet Output plan presented with maximum 4 bullet points describing the different steps to construct pytorch torchvision model to embed images. The plan should be tailored to the current task as it will be used to guide the implementation of the embedder. Assistant ### Plan to Construct PyTorch torchvision Model to Embed Images 1. **Select and Load Pre-trained Model:** - Choose pre-trained model from the provided list (e.g., resnet50). - Load the pre-trained model and remove the final fully connected layer to use it for feature extraction. 2. **Define Image Preprocessing:** - Define set of torchvision transforms to preprocess the images (e.g., resizing, normalization). - Ensure the transforms are consistent with the pre-processing used during the training of the pre-trained model. 3. **Create Image Embedder Class:** - Define PyTorch class ImageEmbedder that takes the pre-processed image tensor as input. - Use the pre-trained model to extract the latent representation (features) from the images. 4. **Implement Forward Method:** - Implement the forward method in the ImageEmbedder class to pass the input image tensor through the pre-trained model. - Return the latent representation tensor as the output of the forward method. This plan will guide the implementation of the embedder, ensuring it is tailored to the current task of classifying dogs and cats. Prompt <Skipped first part of prompt repeating the previous prompt for display purposes in paper appendix> ## Current solution progress You have started creating new solution. Its status is as follows: 84 Agent v1.0 - [TODO] Embedder for text fields - [TODO] Logits and regression targets decoder ## What to do next You have to generate code to embed the images using pre-trained torchvision model. You need to write pytorch torchvision model, where forward() takes the tensor transformed images as input, and that outputs latent representation as ten sor. You can use any pre-trained models like resnet18, resnet50, resnet101, AlexNet, EfficientNetV2, GoogLeNet, MaxVit, MNASNet, RegNet, vit etc... Ensure to unfreeze last few layers of the model based on the model you choose. An example of this model code is: 1 import torch 2 import torchvision.models as models 3 from torch import nn 4 5 class ImageEmbedder(nn.Module): 6 8 9 10 11 12 14 15 16 17 18 20 21 22 23 24 26 27 def __init__(self, pretrained=True): super().__init__() resnet50 = models.resnet50(pretrained=pretrained) self.features = nn.ModuleList(resnet50.children()) [:-1] self.features = nn.Sequential(*self.features) # optionally freeze pre-trained model weights for param in self.features.parameters(): param.requires_grad = False def forward(self, x): output = self.features(x) output = output.view(x.size(0), -1) return output def unfreeze(self, n_last_layers=2): \"\"\"Unfreeze some layers for fine-tuning.\"\"\" layers = list(self.children())[:-1] for layer in layers[-n_last_layers:]: for param in layer.parameters(): param.requires_grad = True Use your plans to help you implement the pytorch embedder model: ### Plan to Construct PyTorch torchvision Model to Embed Images 85 Agent v1. 1. **Select and Load Pre-trained Model:** - Choose pre-trained model from the provided list (e.g., resnet50). - Load the pre-trained model and remove the final fully connected layer to use it for feature extraction. 2. **Define Image Preprocessing:** - Define set of torchvision transforms to preprocess the images (e.g., resizing, normalization). - Ensure the transforms are consistent with the pre-processing used during the training of the pre-trained model. 3. **Create Image Embedder Class:** - Define PyTorch class ImageEmbedder that takes the pre-processed image tensor as input. - Use the pre-trained model to extract the latent representation (features) from the images. 4. **Implement Forward Method:** - Implement the forward method in the ImageEmbedder class to pass the input image tensor through the pre-trained model. - Return the latent representation tensor as the output of the forward method. This plan will guide the implementation of the embedder, ensuring it is tailored to the current task of classifying dogs and cats. Write the code to create model taking tensors of preprocessed features as input, and outputting an embedding tensor. 1. You can assume that the input dimension is already computed in the variable input_dim. 2. You must not forget to import the packages when needed. 3. The model should be able to take as input torch tensor of size (batch, input_dim) and output tensor of shape (batch, latent_dim). 4. Try to use newer models than this last attempt: The provided code implements comprehensive pipeline for the Dogs vs. Cats classification task. It include set of torchvision transforms to preprocess images, PyTorch class ImageEmbedder that uses pre-trained ResNet50 model to generate latent representations of images, and TabularHead model to decode these embeddings into classification logits. The pipeline also defines appropriate loss functions for both regression and classification tasks, ensuring robust and flexible approach to the problem.. Do not repeat these pre-trained models from the attempt. Your task is to fill the blank and implement the image embedder in this code: 86 Agent v1.0 1 # <fim_begin> 2 import os.path 3 4 import torch 5 import pandas as pd 6 import torchvision.models as models 7 import torchvision.transforms as 8 from torch import nn 9 from PIL import Image 10 11 from submissions.new_submission.img_transform import CustomImageInputTransform 12 13 # Load image features 14 root_path = \"/PATH/TO/SETUP\" 15 train_data_path = os.path.join(root_path, \"train_img_input_map .csv\") 16 = pd.read_csv(train_data_path, index_col=\"id\") 17 18 # --- Design torch model using pre-trained torchvision model 19 # Create class ImageEmbedder(nn.Module) and implementing the methods: 20 # - __init__() defines the model architecture, using pretrained model such as resnet50 21 # - forward:(x: torch.Tensor) -> torch.Tensor, embeds batch of inputs and loads the image embeddings. 22 # <fim_hole> 23 # --- [End] 24 25 # Test that the model works 26 path = x.iloc[0, 0] 27 image = Image.open(path) 28 tensor_image = CustomImageInputTransform(image) 29 model = ImageEmbedder() 30 X_embed = model(tensor_image.unsqueeze(0)) 31 32 33 # <fim_end> 34 # @NO_MEMORY_START@ 35 36 def test_unfreeze_functionality() -> None: 38 39 40 41 model = ImageEmbedder() assert hasattr(model, unfreeze), \"The model does not have an unfreeze method.\" try: model.unfreeze() 87 Agent v1.0 except Exception as e: assert False, f\"Unfreeze method raised an exception: { e}\" print(\"Unfreeze method exists and runs successfully.\") 42 43 44 46 47 48 test_unfreeze_functionality() 49 50 print(f\"Embedding size: {X_embed.shape}\") 51 # @NO_MEMORY_END@ Only output the code that should be filled in the blank with no extra text. Dont forget to import any additional packages that you will use in the code. Your response should be in the format: python <code> Assistant 1 class ImageEmbedder(nn.Module): 2 3 4 5 6 8 9 10 11 12 14 15 16 17 18 20 21 22 23 def __init__(self, pretrained=True, latent_dim=2048): super().__init__() # Choose newer model like EfficientNetV2 efficientnet = models.efficientnet_v2_m(pretrained= pretrained) self.features = nn.ModuleList(efficientnet.features. children()) self.features = nn.Sequential(*self.features) # Optionally freeze pre-trained model weights for param in self.features.parameters(): param.requires_grad = False def forward(self, x): output = self.features(x) output = output.mean(dim=[2, 3]) def unfreeze(self, n_last_layers=2): \"\"\"Unfreeze some layers for fine-tuning.\"\"\" layers = list(self.features.children()) for layer in layers[-n_last_layers:]: for param in layer.parameters(): param.requires_grad = True"
        }
    ],
    "affiliations": [
        "Huawei Noahs Ark",
        "AI Centre, Department of Computer Science, UCL",
        "Technical University of Darmstadt"
    ]
}