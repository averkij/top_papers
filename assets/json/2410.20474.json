{
    "paper_title": "GrounDiT: Grounding Diffusion Transformers via Noisy Patch Transplantation",
    "authors": [
        "Phillip Y. Lee",
        "Taehoon Yoon",
        "Minhyuk Sung"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce GrounDiT, a novel training-free spatial grounding technique for text-to-image generation using Diffusion Transformers (DiT). Spatial grounding with bounding boxes has gained attention for its simplicity and versatility, allowing for enhanced user control in image generation. However, prior training-free approaches often rely on updating the noisy image during the reverse diffusion process via backpropagation from custom loss functions, which frequently struggle to provide precise control over individual bounding boxes. In this work, we leverage the flexibility of the Transformer architecture, demonstrating that DiT can generate noisy patches corresponding to each bounding box, fully encoding the target object and allowing for fine-grained control over each region. Our approach builds on an intriguing property of DiT, which we refer to as semantic sharing. Due to semantic sharing, when a smaller patch is jointly denoised alongside a generatable-size image, the two become semantic clones. Each patch is denoised in its own branch of the generation process and then transplanted into the corresponding region of the original noisy image at each timestep, resulting in robust spatial grounding for each bounding box. In our experiments on the HRS and DrawBench benchmarks, we achieve state-of-the-art performance compared to previous training-free approaches."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 1 ] . [ 2 4 7 4 0 2 . 0 1 4 2 : r GrounDiT: Grounding Diffusion Transformers via Noisy Patch Transplantation Phillip Y. Lee Taehoon Yoon Minhyuk Sung KAIST {phillip0701,taehoon,mhsung}@kaist.ac.kr Figure 1: Spatially grounded images generated by our GROUNDIT. Each image is generated based on text prompt along with bounding boxes, which are displayed in the upper right corner of each image. Compared to existing methods that often struggle to accurately place objects within their designated bounding boxes, our GROUNDIT enables more precise spatial control through novel noisy patch transplantation mechanism."
        },
        {
            "title": "Abstract",
            "content": "We introduce GROUNDIT, novel training-free spatial grounding technique for text-to-image generation using Diffusion Transformers (DiT). Spatial grounding with bounding boxes has gained attention for its simplicity and versatility, allowing for enhanced user control in image generation. However, prior training-free approaches often rely on updating the noisy image during the reverse diffusion process via backpropagation from custom loss functions, which frequently struggle to provide precise control over individual bounding boxes. In this work, we leverage the flexibility of the Transformer architecture, demonstrating that DiT can generate noisy patches corresponding to each bounding box, fully encoding the target object and allowing for fine-grained control over each region. Our approach builds on an intriguing property of DiT, which we refer to as semantic sharing. Due to semantic sharing, when smaller patch is jointly denoised alongside generatable-size image, the two become semantic clones. Each patch is denoised in its own branch of the generation process and then transplanted into the corresponding region of the original noisy image at each timestep, resulting in robust spatial grounding for each bounding box. In our experiments on the HRS and DrawBench benchmarks, we achieve state-of-the-art performance compared to previous training-free approaches. Project Page: https://groundit-diffusion.github.io/. *Equal contribution. 38th Conference on Neural Information Processing Systems (NeurIPS 2024)."
        },
        {
            "title": "Introduction",
            "content": "The Transformer architecture [45] has driven breakthroughs across wide range of applications, with diffusion models emerging as significant recent beneficiaries. Despite the success of diffusion models with U-Net [42] as the denoising backbone [22, 43, 41, 39], recent Transformer-based diffusion models, namely Diffusion Transformers (DiT) [37], have marked another leap in performance. This is demonstrated by recent state-of-the-art generative models such as Stable Diffusion 3 [13] and Sora [6]. Open-source models like DiT [37] and its text-guided successor PixArt-α [8] have also achieved superior quality compared to prior U-Net-based diffusion models. Given the scalability of Transformers, Diffusion Transformers are expected to become the new standard for image generation, especially when trained on an Internet-scale dataset. With high quality image generation achieved, the next critical step is to enhance user controllability. Among the various types of user guidance in image generation, one of the most fundamental and significant is spatial grounding. For instance, user may provide not only text prompt describing the image but also set of bounding boxes indicating the desired positions of each object, as shown in Fig. 1. Such spatial constraints can be integrated into text-to-image (T2I) diffusion models by adding extra modules that are designed for spatial grounding and fine-tuning the model. GLIGEN [31] is notable example, which incorporates gated self-attention module [1] into the U-Net layers of Stable Diffusion [41]. Although effective, such fine-tuning-based approaches incur substantial training costs each time new T2I model is introduced. Recent training-free approaches for spatially grounded image generation [9, 47, 11, 36, 38, 48, 12, 26] have led to new advances, removing the high costs for fine-tuning. These methods leverage the fact that cross-attention maps in T2I diffusion models convey rich structural information about where each concept from the text prompt is being generated in the image [7, 19]. Building on this, these approaches aim to align the cross-attention maps of specific objects with the given spatial constraints (e.g. bounding boxes), ensuring that the objects are placed within their designated regions. This alignment is typically achieved by updating the noisy image in the reverse diffusion process using backpropagation from custom loss functions. However, such loss-guided update methods often struggle to provide precise spatial control over individual bounding boxes, leading to missing objects (Fig. 4, Row 9, Col. 5) or discrepancies between objects and their bounding boxes (Fig. 4, Row 4, Col. 4). This highlights the need for finer control over each bounding box during image generation. We aim to provide more precise spatial control over each bounding box, addressing the limitations in previous loss-guided update approaches. well-known technique for manipulating local regions of the noisy image during the reverse diffusion process is to directly replace or merge the pixels (or latents) in those regions. This simple but effective approach has proven effective in various tasks, including compositional generation [17, 50, 44, 32] and high-resolution generation [5, 29, 24, 25]. One could consider defining an additional branch for each bounding box, denoising with the corresponding text prompt, and then copying the noisy image into its designated area in the main image at each timestep. However, key challenge lies in creating noisy image patchat the same noise levelthat reliably contains the desired object while fitting within the specified bounding box. This has been impractical with existing T2I diffusion models, as they are trained on limited set of image resolutions. While recent models such as PixArt-α [8] support wider range of image resolutions, they remain constrained by specific candidate sizes, particularly for smaller image patches. As result, when these models are used to create local image patch, they are often limited to denoising fixed-size image and cropping the region to fit the bounding box. This approach can critically fail to include the desired object within the cropped region. In this work, we show that by exploiting the flexibility of the Transformer architecture, DiT can generate noisy image patches that fit the size of each bounding box, thereby reliably including each desired object. This is made possible through our proposed joint denoising technique. First, we introduce an intriguing property of DiT: when smaller noisy patch is jointly denoised with generatable-size noisy image, the two gradually become semantic clonesa phenomenon we call semantic sharing. Next, building on this observation, we propose training-free framework that involves cultivating noisy patch for each bounding box in separate branch and then transplanting that patch into its corresponding region in the original noisy image. By iteratively transplanting the separately denoised patches into their respective bounding boxes, we achieved fine-grained spatial control over each bounding box region. This approach leads to more robust spatial grounding, particularly in cases where previous methods fail to accurately adhere to spatial constraints. In our experiments on the HRS [3] and DrawBench [43] datasets, we evaluate our framework, GROUNDIT, using PixArt-α [8] as the base text-to-image DiT model. Our approach demonstrates superior performance in spatial grounding compared to previous training-free methods [38, 9, 47, 48], especially outperforming the state-of-the-art approach [47], highlighting its effectiveness in providing fine-grained spatial control."
        },
        {
            "title": "2 Related Work",
            "content": "In this section, we review the two primary approaches for incorporating spatial controls into textto-image (T2I) diffusion models: fine-tuning-based methods (Sec. 2.1) and training-free guidance techniques (Sec. 2.2)."
        },
        {
            "title": "2.1 Spatial Grounding via Fine-Tuning",
            "content": "Fine-tuning with additional modules is powerful approach for enhancing T2I models with spatial grounding capabilities [51, 31, 2, 53, 46, 16, 10, 54]. SpaText [2] introduces spatio-textual representation that combines segmentations and CLIP embeddings [40]. ControlNet [51] incorporates trainable U-Net encoder that processes spatial conditions such as depth maps, sketches, and human keypoints, guiding image generation within the main U-Net branch. GLIGEN [31] enables T2I models to accept bounding boxes by inserting gated attention module into Stable Diffusion [41]. GLIGENs strong spatial accuracy has led to its integration into follow-up spatial grounding methods [48, 38, 30] and applications such as compositional generation [15] and video editing [23]. InstanceDiffusion [46] further incorporates conditioning modules to provide finer spatial control through diverse conditions like boxes, scribbles, and points. While these fine-tuning methods are effective, they require taskspecific datasets and involve substantial costs, as they must be retrained for each new T2I model, underscoring the need for training-free alternatives."
        },
        {
            "title": "2.2 Spatial Grounding via Training-Free Guidance",
            "content": "In response to the inefficiencies of fine-tuning, training-free approaches have been introduced to incorporate spatial grounding into T2I diffusion models. One approach involves region-wise composition of noisy patches, each conditioned on different text input [5, 50, 32]. These patches, extracted using binary masks, are intended to generate the object they are conditioned on within the generated image. However, since existing T2I diffusion models are limited to fixed set of image resolutions, each patch cannot be treated as complete image, making it uncertain whether the extracted patch will contain the desired object. Another approach leverages the distinct roles of attention modules in T2I modelsself-attention captures long-range interactions between image features, while cross-attention links image features with text embeddings. By using spatial constraints such as bounding boxes or segmentation masks, spatial grounding can be achieved either by updating the noisy image using backpropagation based on loss calculated from cross-attention maps [48, 38, 9, 7, 18, 36], or by directly manipulating crossor self-attention maps to follow the given spatial layouts [26, 4, 14]. While the loss-guided methods enable spatial grounding in training-free manner, they still lack precise control over individual bounding boxes, often leading to missing objects or misalignmet between objects and their bounding boxes. In this work, we propose novel training-free framework that offers fine-grained spatial control over each bounding box by harnessing the flexibility of the Transformer architecture in DiT."
        },
        {
            "title": "3 Background: Diffusion Transformers",
            "content": "Diffusion Transformer (DiT) [37] represents new class of diffusion models that utilize the Transformer architecture [45] for their denoising network. Previous diffusion models like Stable Diffusion [41] use the U-Net [42] architecture, of which each layer contains convolutional block and attention modules. In contrast, DiT consists of sequence of DiT blocks, each containing pointwise feedforward network and attention modules, removing convolution operations and instead processing image tokens directly through attention mechanisms. 3 Figure 2: single denoising step in GROUNDIT consists of two stages. The Global Update (Sec. 5.1) established coarse spatial grounding by updating the noisy image with custom loss function. Then, the Local Update (Sec. 5.3) further provides fine-grained spatial control over individual bounding boxes through novel technique called noisy patch transplantation. DiT follows the formulation of diffusion models [22], in which the forward process applies noise to real clean data x0 by xt = αtx0 + 1 αtϵ where ϵ (0, I), αt [0, 1]. (1)"
        },
        {
            "title": "The reverse process denoises the noisy data xt through a Gaussian transition",
            "content": "pθ(xt1xt) = (xt; µθ(xt, t), Σθ(xt, t)) (2) where µθ(xt, t) is calculated by learned neural network trained by minimizing the negative ELBO objective [27]. While Σθ(xt, t) can also be learned, it is usually set as time dependent constants. Positional Embeddings. As DiT is based on the Transformer architecture, it treats the noisy image xt Rhwd as set of image tokens. Specifically, xt is divided into patches, each transformed into an image token via linear embedding. This results in (h/l) (w/l) tokens, where is the patch size. Importantly, before each denoising step, 2D sine-cosine positional embeddings are assigned to each image token to provide spatial information as follows: xt1 Denoise(PE(xt), t, c). (3) Here, PE() applies positional embeddings, Denoise() represents single denoising step in DiT at timestep t, and is the text embedding. This contrasts with U-Net-based diffusion models, which typically do not utilize positional embeddings for the noisy image. Detailed formulations of the positional embeddings are provided in the Appendix (Sec. A)."
        },
        {
            "title": "4 Problem Definition",
            "content": "Let be the input text prompt (i.e., list of words), which we refer to as the global prompt. Let cP be the text embedding of . We define set of grounding conditions = {gi}N 1 i=0 , where each condition specifies the coordinates of bounding box and the desired object to be placed within it. Specifically, each condition gi := (bi, pi, ci) consists of the following: bi R4, the xy-coordinates of the bounding boxs upper-left and lower-right corners, pi , the word in the global prompt describing the desired object within the box, and ci, the text embedding of pi. The objective is to generate an image that aligns with the global prompt while ensuring each specified object is accurately positioned within its corresponding bounding box bi."
        },
        {
            "title": "5 GROUNDIT: Grounding Diffusion Transformers",
            "content": "We propose GROUNDIT, training-free framework based on DiT for generating images spatially grounded on bounding boxes. Each denoising step in GROUNDIT consists of two stages: Global Update and Local Update. Global Update ensures coarse alignment between the noisy image and the bounding boxes through gradient descent update using cross-attention maps (Sec. 5.1). Following this, Local Update further provides fine-grained control over individual bounding boxes via novel noisy patch transplantation technique (Sec. 5.3). This approach leverages our key observation of DiTs semantic sharing property, introduced in Sec. 5.2. An overview of this two-stage denoising step is provided in Fig. 2."
        },
        {
            "title": "5.1 Global Update with Cross-Attention Maps",
            "content": "First, the noisy image xt is updated to spatially align with the bounding box inputs. For this, we leverage the rich structural information encoded in cross-attention maps, as first demonstrated by Chefer et al. [7]. Each cross-attention map shows how region of the noisy image correspond to specific word in the global prompt . Let DiT consist of sequential DiT blocks. As xt passes i,t Rhw1 for object pi is extracted. For each through the m-th block, the cross-attention map am grounding condition gi, the mean cross-attention map Ai,t is obtained by averaging am i,t over all blocks as follows: Ai,t ="
        },
        {
            "title": "1\nM",
            "content": "M 1 (cid:88) m=0 am i,t. For convenience, we denote the operation of extracting Ai,t for every gi as below: {Ai,t}N 1 i=0 ExtractAttention(xt, t, cP , G). (4) (5) Then, following prior works on U-Net-based diffusion models [48, 47, 38, 9, 36, 7], we measure the spatial alignment for each object pi by comparing its mean cross-attention map Ai,t with its corresponding bounding box bi, using predefined grounding loss L(Ai,t, bi) as defined in R&B [47]. The aggregated grounding loss LAGG is then computed by summing the grounding loss across all grounding conditions gi G: LAGG({Ai,t}N 1 i=0 , G) = 1 (cid:88) i=0 L(Ai,t, bi). Based on the backpropagation from LAGG, xt is updated via gradient descent as follows: ˆxt xt ωtxtLAGG (6) (7) where ωt is scalar weight value for gradient descent. We refer to Eq. 7 as the Global Update, as the whole noisy image xt is updated based on an aggregated loss from all grounding conditions in G. The Global Update achieves reasonable accuracy in spatial grounding. However, it often struggles with more complex grounding conditions. For instance, when contains multiple bounding boxes (e.g., six boxes in Fig. 4, Row 9) or small, thin boxes (e.g., Fig. 4, Row 5), the desired objects may be missing or misaligned with the boxes. These examples show that the Global Update lacks fine-grained, box-specific control, underscoring the need for precise controls for individual bounding boxes. In the following sections, we introduce novel method to achieve this fine-grained spatial control."
        },
        {
            "title": "5.2 Semantic Sharing in Diffusion Transformers",
            "content": "In this section, we present our observations on an intriguing property of DiT, semantic sharing, which will serve as key building block for our main method in Sec. 5.3. Joint Denoising. We observed that DiT can jointly denoise two different noisy images together. For example, consider two noisy images, xt and yt, both at timestep in the reverse diffusion process. Position embeddings are applied to the image tokens according to their sizes, resulting in xt = PE(xt) and yt = PE(yt). Notably, the two noisy images can differ in size, providing flexibility in joint denoising. The key part is that the two noisy imagesmore precisely, two sets of image tokensare merged into single set zt. We denote this process as Merge() (see Alg. 1, line 4). zt is passed through the DiT blocks, yielding the output zt1, which is then split into the denoised versions xt1 and yt1 via Split(). Joint denoising is illustrated in Fig. 3-(A), and pseudocode for single step of joint denoising is shown in Alg. 1. Semantic Sharing. Surprisingly, we found that joint denoising of two noisy images generates semantically correlated content in correponding pixels, even when the initial random noise differs. Consider two noisy images, xT Rhxwxd and yT Rhywyd, both initialized from unit Gaussian distribution (0, I). We experiment with reverse diffusion process in which, for the initial 100γ% of the denoising steps (γ [0, 1]), xT and yT undergo joint denoising. For the remaining 5 Figure 3: (A) Joint Denoising. Two different noisy images, xt and yt, are each assigned positional embeddings based on their respective sizes. The two sets of image tokens are then merged and passed through DiT for denoising step. Afterward, the denoised tokens are split back into xt1 and yt1. (B), (C) Semantic Sharing. Denoising two noisy images using joint denoising results in semantically correlated content between the generated images. Here, γ indicates that joint denoising is during the initial 100γ% of the timesteps, after which the images are denoised for the remaining steps. timesteps, they are denoised independently. The same text embedding is used as condition in both cases. Fig. 3 shows the generated images from xT and yT across different γ values. In Fig. 3-(B), xT and yT have the same resolution (hx = hy, wx = wy), while in Fig. 3-(C) their resolutions differ (hx > hy, wx > wy). When γ = 0, the two noisy images are denoised completely independently, resulting in clearly distinct images (leftmost column). We found that DiT models have certain range of resolutions within which they can generate plausible imageswhich we refer to as generatable resolutionsbut face challenges when generating images far outside this range. This is demonstrated in the output of y0 in Fig. 3-(C) with γ = 0. Further discussions and visual analyses are provided in the Appendix (Sec. D). But as γ increases, allowing xT and yT to be jointly denoised in the initial steps, the generated images become progressively more similar. When γ = 1, the images generated from xT and yT appear almost identical. These results demonstrate that, in joint denoising, assigning identical or similar positional embeddings to different image tokens promotes strong interactions between them during the denoising process. This correlated behavior during joint denoising causes the two image tokens to converge toward semantically similar outputsa phenomenon we term semantic sharing. Notably, this pattern holds not only when both noisy images share the same resolution (Fig. 3-(B)), but even when one of the images does not have DiTs generatable resolution (Fig. 3-(C)). While self-attention sharing techniques have been explored in U-Net-based diffusion models to enhance style consistency between images [20, 34], they have been limited to images of equal resolution. By leveraging the flexibility to assign positional embeddings across different resolutions, our joint Algorithm 1: Pseudocode of Joint Denoising (Sec. 5.2). Inputs: xt Rhxwxd, yt Rhy wy d, t, c, l; // Noisy images, timestep, text embedding, patch size. Outputs: xt1, yt1; 3 1 Function JointDenoise(xt, yt, t, c): nx hxwx/l2, ny hywy/l2; 2 xt PE(xt), yt PE(yt); zt Merge(xt, yt); zt1 Denoise(zt, t, c); {xt1, yt1} Split(zt1, {nx, ny}); return xt1, yt1; 5 6 7 4 // Noisy images at timestep 1. // Store the number of image tokens. // Apply positional embeddings. // Merge two sets of image tokens. // Denoising step with DiT. // Split back into two sets. 6 denoising approach extends across heterogeneous resolutions, offering greater versatility. We provide further discussions and analyses on semantic sharing in the Appendix (Sec. D)."
        },
        {
            "title": "5.3 Local Update with Noisy Patch Transplantation",
            "content": "In this section, we introduce our key technique: Local Update via noisy patch cultivation and transplantation. Building on DiTs semantic sharing property from Sec. 5.2, we show how this can be leveraged to provide precise spatial control over each bounding box. Main & Object Branches. We propose parallel denoising approach with multiple branches: one for the main noisy image ˆxt and additional branches for each grounding condition gi. The main branch denoises the main noisy image using the global prompt , while each object branch is designed to denoise local regions within the bounding boxes, enabling fine-grained spatial control over each region. For each i-th object branch, there is distinct noisy object image ui,t, initialized as ui,T (0, I). We predefine the resolution of the noisy object image ui,t by searching in PixArt-αs generatble resolutions that closely match the aspect ratio of the corresponding bounding box bi. With ˆxt obtained from Global Update (Sec. 5.1), each branch performs denoising in parallel. Below we explain the denoising mechanism for each branch. In the main branch at timestep t, the noisy image ˆxt is denoised using Noisy Patch Cultivation. the global prompt as follows: xt1 Denoise(PE(ˆxt), t, cP ), where ˆxt is the output from the Global Update and cP is the text embedding of . For the i-th object branch, there are two inputs: the noisy object image ui,t and subset vi,t of image tokens extracted from ˆxt, corresponding to the bounding box bi. We denote this extraction as vi,t Crop(ˆxt, bi), where vi,t Rhiwid is referred to as noisy local patch. Here, hi and wi corresponds to height and width of bounding box bi, respectively. Joint denoising is then performed on ui,t and vi,t to yield their denoised versions: {ui,t1, vi,t1} JointDenoise(ui,t, vi,t, t, ci), (8) where ci is the text embedding of the object pi. Through semantic sharing with the noisy object image ui,t during joint denoising, the denoised local patch vi,t1 is expected to gain richer semantic features of object pi than it would without joint denoising. Note that even when the noisy local patch vi,t does not meet the typical generatable resolution of DiT (since it often requires cropping small bounding box regions of ˆxt to obtain vi,t), it offers simple and effective way for enriching vi,t of the semantic features of object pi. We refer to this process as noisy patch cultivation. Noisy Patch Transplantation. After cultivating local patches through joint denoising in Eq. 8, each patch is transplanted into xt1, obtained from the main branch. The patches are transplanted in their original bounding box regions specified by bi as follows: xt1 xt1 (1 mi) + Uncrop(vi,t1, bi) mi (9) Here, denotes the Hadamard product, mi is binary mask for the bounding box bi, and Uncrop(vi,t1, bi) applies zero-padding to vi,t1 to align its position with that of bi. This transplantation enables fine-grained local control for the grounding condition gi. After transplanting the outputs from all object branches, we obtain xt1, representing the final output of GROUNDIT denoising step at timestep t. In xt1, the image tokens within the bi region are expected to possess richer semantic information about object pi compared to the initial xt1 from the main branch. This process is referred to as noisy patch transplantation. We provide implementation details and full pseudocode of single GROUNDIT denoising step in the Appendix (Sec. E)."
        },
        {
            "title": "6 Results",
            "content": "In this section, we present the experiment results of our method, GROUNDIT, and provide comparisons with baselines. For the base text-to-image DiT model, we use PixArt-α [8], which builds on the original DiT architecture [37] by incorporating an additional cross-attention module to condition on text prompts. 7 Layout LayoutGuidance [9] AttentionRefocusing [38] BoxDiff [48] R&B [47] PixArt-R&B GROUNDIT dog in the beautiful park. An eagle is flying over tree. duck wearing hat standing near bicycle. plastic bottle and an apple on table. An apple and banana and cup on table. \"A dog wearing sunglasses and red hat and blue tie. chair and table and bed is on the room with photo frame on the wall and ceiling lamp [...] dog and bird sitting on branch while an eagle is flying in the sky. car and dog on the road while horse and chair is on the grass. Figure 4: Qualitative comparisons between our GROUNDIT and baselines. Leftmost column shows the input bounding boxes, and columns 2-6 include the baseline results. The rightmost column includes the results of our GROUNDIT."
        },
        {
            "title": "6.1 Evaluation Settings",
            "content": "Baselines. We compare our method with state-of-the-art training-free approaches for bounding box-based image generation, including R&B [47], BoxDiff [48], Attention-Refocusing [38], and Layout-Guidance [14]. For fair comparison, we also implement R&B using PixArt-α, which we refer to as PixArt-R&B, and treat it as an internal baseline. Note that this is identical to our method without the Local Guidance (Sec. 5.3). Evaluation Metrics and Benchmarks. (Grounding Accuracy) We follow the evaluation protocol of R&B [47] to assess spatial grounding on the HRS [3] and DrawBench [43] datasets, using three criteria: spatial, size, and color. The HRS dataset consists of 1002, 501, and 501 images for each respective criterion, with bounding boxes generated using GPT-4 by Phung et al. [38]. For DrawBench, we use the same 20 positional prompts as in R&B [47]. (Prompt Fidelity) We use the CLIP score [21] to evaluate how well the generated images adhere to the text prompt. Additionally, we assess our method using PickScore [28] and ImageReward [49], which provide human alignment scores based on the consistency between the text prompt and generated images. Method Spatial (%) HRS Size (%) Color (%) DrawBench Spatial (%) Backbone: Stable Diffusion [41] Stable Diffusion [41] PixArt-α [8] Layout-Guidance [9] Attention-Refocusing [38] BoxDiff [48] R&B [47] PixArt-R&B GROUNDIT (Ours) 8.48 17.86 16.47 24.45 16.31 30.14 37.13 45.01 9.18 11.82 12.38 16.97 11.02 26.74 Backbone: PixArt-α [8] 20.76 27.75 12.61 19.10 14.39 23.54 13.23 32.04 29.07 35.67 12.50 20.00 36.50 43.50 30.00 55. 60.00 60.00 Table 1: Quantitative comparisons of grounding accuracy on HRS [3] and DrawBench [43] benchmarks. Bold represents the best, and underline represents the second best method."
        },
        {
            "title": "6.2 Grounding Accuracy",
            "content": "Quantitative Comparisons. Tab. 1 presents quantitative comparison of grounding accuracy between our method, GROUNDIT, and baselines. GROUNDIT outperforms all baselines across different criteria of grounding accuracyspatial, size, and colorincluding the state-of-the-art R&B [47] and our internal baseline PixArt-R&B. Notably, the spatial accuracy on the HRS benchmark [3] (Col. 1) of GROUNDIT is significantly higher, with +14.87% improvement over R&B and +7.88% over PixArt-α. The comparison between PixArt-α [8], PixArt-R&B and GROUNDIT highlights the effectiveness of the two-stage pipeline of GROUNDIT. First, integrating the loss-based Global Update into PixArt-α results in substantial improvement in spatial accuracy (from 17.86% to 37.13%). Then, incorporating our key contribution, the Local Update, further boosts accuracy (from 37.13% to 45.01%). For size accuracy (Col. 2), which evaluates how well the size of each generated object matches its corresponding bounding box, GROUNDIT shows +1.01% improvement over R&B. In terms of color accuracy (Col. 3), our method achieves +6.60% improvement over PixArt-R&B and outperforms R&B by +3.63%. This underscores the effectiveness of our noisy patch transplantation technique in accurately assigning color descriptions to the corresponding objects. As DrawBench [43] only contains images with two bounding boxes, which are relatively easy to generate, employing the Global Update is sufficient for grounding. We present additional quantitative comparisons of grounding accuracy in the Appendix (Sec. B). Qualitative Comparisons. Fig. 4 presents the qualitative comparisons. When the grounding condition involves one or two simple bounding boxes (Rows 1, 2), both our method and the baselines 9 successfully generate objects within the designated regions. However, as the number of bounding boxes increases and the grounding conditions become more challenging, the baselines struggle to correctly place each object inside the bounding box (Rows 4, 8), or even fail to generate the object at all (Rows 5, 7, 9). In contrast, GROUNDIT successfully grounds each object within the boxes, even when the number of boxes is relatively high, such as four boxes (Rows 5, 6, 8), five boxes (Row 7) and six boxes (Row 9). This highlights that our proposed noisy patch transplantation technique provides superior control over each bounding box, addressing the limitations of previous loss-based update methods, as discussed in Sec. 5.1. For more qualitative comparisons, including images generated with various aspect ratios, please refer to the Appendix (Sec. and Fig. 5)."
        },
        {
            "title": "6.3 Prompt Fidelity",
            "content": "Tab. 2 presents quantitative comparison of prompt fidelity between our method and PixArt-R&B. Each metric is measured using the generated images from the HRS dataset [3]. GROUNDIT achieves higher CLIP score [21] than PixArt-R&B (Col. 1), indicating that our noisy patch transplantation improves the text prompt fidelity of the generated images. Additionally, our method achieves higer ImageReward [49] score, which measures human preference by considering both prompt fidelity and overall image quality. While GROUNDIT shows slight underperformance compared to PixArt-R&B in Pickscore [28], it remains comparable overall. We provide further comparisons of prompt fidelity with other baselines in the Appendix (Sec. C). Method PixArt-R&B GROUNDIT (Ours) CLIP score ImageReward PickScore 33.49 33.63 0.28 0.44 0.52 0.48 Table 2: Quantitative comparisons on prompt fidelity on HRS benchmark [3]. Bold represents the best method."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we introduced GROUNDIT, training-free spatial grounding technique for text-to-image generation, leveraging Diffusion Transformers (DiT). To address the limitation of prior approaches, which lacked fine-grained spatial control over individual bounding boxes, we proposed novel approach that transplants noisy patch generated in separate denoising branch into the designated area of the noisy image. By exploiting an intriguing property of DiT, semantic sharing, which arises from the flexibility of the Transformer architecture and the use of positional embeddings, GROUNDIT generates smaller patch by simultaneously denoising two noisy image: one with smaller size and the other with generatable resolution by DiT. Through semantic sharing, these two noisy images become semantic clones, enabling fine-grained spatial control for each bounding box. Our experiments on the HRS and DrawBench benchmarks demonstrated that GROUNDIT achieves state-of-the-art performance compared to previous training-free grounding methods. Limitations and Societal Impacts. limitation of our method is the increased computation time, as it requires separate object branch for each bounding box. We provide further analysis on the computation time in the Appendix (Sec. F). Additionally, like other generative AI techniques, our method is susceptible to misuse, such as creating deepfakes, which can raise significant concerns related to privacy, bias, and fairness. It is crucial to develop safeguards to control and mitigate these risks responsibly."
        },
        {
            "title": "Acknowledgements",
            "content": "We thank Juil Koo and Jaihoon Kim for valuable discussions on Diffusion Transformers. This work was supported by the NRF grant (RS-2023-00209723), IITP grants (RS-2019-II190075, RS2022-II220594, RS-2023-00227592, RS-2024-00399817), and KEIT grant (RS-2024-00423625), all funded by the Korean government (MSIT and MOTIE), as well as grants from the DRB-KAIST SketchTheFuture Research Center, NAVER-Intel Co-Lab, Hyundai NGV, KT, and Samsung Electronics."
        },
        {
            "title": "References",
            "content": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. In NeurIPS, 2022. [2] Omri Avrahami, Thomas Hayes, Oran Gafni, Sonal Gupta, Yaniv Taigman, Devi Parikh, Dani Lischinski, Ohad Fried, and Xi Yin. Spatext: Spatio-textual representation for controllable image generation. In CVPR, 2023. [3] Eslam Mohamed Bakr, Pengzhan Sun, Xiaogian Shen, Faizan Farooq Khan, Li Erran Li, and Mohamed Elhoseiny. Hrs-bench: Holistic, reliable and scalable benchmark for text-to-image models. In ICCV, 2023. [4] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion models with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022. [5] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. MultiDiffusion: Fusing diffusion paths for controlled image generation. In ICML, 2023. [6] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya https://openai.com/research/ Schnurr, Ramesh. Video generation models as world simulators. video-generation-models-as-world-simulators, 2024. [7] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attentionbased semantic guidance for text-to-image diffusion models. ACM Transactions on Graphics, 2023. [8] Junsong Chen, Jincheng YU, Chongjian GE, Lewei Yao, Enze Xie, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. In ICLR, 2024. [9] Minghao Chen, Iro Laina, and Andrea Vedaldi. Training-free layout control with cross-attention guidance. In WACV, 2024. [10] Jiaxin Cheng, Xiao Liang, Xingjian Shi, Tong He, Tianjun Xiao, and Mu Li. Layoutdiffuse: Adapting foundational diffusion models for layout-to-image generation. arXiv preprint arXiv:2302.08908, 2023. [11] Guillaume Couairon, Marlène Careil, Matthieu Cord, Stéphane Lathuilière, and Jakob Verbeek. Zero-shot spatial layout conditioning for text-to-image diffusion models. arXiv preprint arXiv:2403.13589, 2023. [12] Dave Epstein, Allan Jabri, Ben Poole, Alexei Efros, and Aleksander Holynski. Diffusion self-guidance for controllable image generation. In NeurIPS, 2023. [13] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. arXiv preprint arXiv:2403.03206, 2024. [14] Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Akula, Pradyumna Narayana, Sugato Basu, Xin Eric Wang, and William Yang Wang. Training-free structured diffusion guidance for compositional text-to-image synthesis. In ICLR, 2023. [15] Weixi Feng, Wanrong Zhu, Tsu-jui Fu, Varun Jampani, Arjun Akula, Xuehai He, Sugato Basu, Xin Eric Wang, and William Yang Wang. Layoutgpt: Compositional visual planning and generation with large language models. arXiv preprint arXiv:2305.15393, 2023. [16] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene: Scene-based text-to-image generation with human priors. In ECCV, 2022. [17] Songwei Ge, Taesung Park, Jun-Yan Zhu, and Jia-Bin Huang. Expressive text-to-image generation with rich text. In ICCV, 2023. [18] Xiefan Guo, Jinlin Liu, Miaomiao Cui, Jiankai Li, Hongyu Yang, and Di Huang. Initno: Boosting text-to-image diffusion models via initial noise optimization. In CVPR, 2024. [19] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-toprompt image editing with cross attention control. In ICLR, 2023. [20] Amir Hertz, Andrey Voynov, Shlomi Fruchter, and Daniel Cohen-Or. Style aligned image generation via shared attention. In CVPR, 2024. [21] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. CLIPScore: reference-free evaluation metric for image captioning. In EMNLP, 2021. [22] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. [23] Hyeonho Jeong and Jong Chul Ye. Ground-a-video: Zero-shot grounded video editing using text-to-image diffusion models. In ICLR, 2024. [24] Gwanghyun Kim, Hayeon Kim, Hoigi Seo, Dong Un Kang, and Se Young Chun. Beyondscene: Higherresolution human-centric scene generation with pretrained diffusion. arXiv preprint arXiv:2404.04544, 2024. [25] Jaihoon Kim, Juil Koo, Kyeongmin Yeo, and Minhyuk Sung. Synctweedies: general generative framework based on synchronized diffusions. arXiv preprint arXiv:2403.14370, 2024. [26] Yunji Kim, Jiyoung Lee, Jin-Hwa Kim, Jung-Woo Ha, and Jun-Yan Zhu. Dense text-to-image generation with attention modulation. In ICCV, 2023. [27] Diederik Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014. [28] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. In NeurIPS, 2023. [29] Yuseung Lee, Kunho Kim, Hyunjin Kim, and Minhyuk Sung. SyncDiffusion: Coherent montage via synchronized joint diffusions. In NeurIPS, 2023. [30] Yuseung Lee and Minhyuk Sung. Reground: Improving textual and spatial grounding at no cost. arXiv preprint arXiv:2403.13589, 2024. [31] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In CVPR, 2023. [32] Long Lian, Boyi Li, Adam Yala, and Trevor Darrell. Llm-grounded diffusion: Enhancing prompt understanding of text-to-image diffusion models with large language models. TMLR, 2024. [33] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollár. Microsoft coco: Common objects in context, 2015. [34] Yuxin Liu, Minshan Xie, Hanyuan Liu, and Tien-Tsin Wong. Text-guided texturing by synchronized multi-view diffusion. arXiv preprint arXiv:2311.12891, 2023. [35] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. In NeurIPS, 2022. [36] Wan-Duo Kurt Ma, Avisek Lahiri, JP Lewis, Thomas Leung, and Bastiaan Kleijn. Directed diffusion: Direct control of object placement through attention guidance. In AAAI, 2024. [37] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. [38] Quynh Phung, Songwei Ge, and Jia-Bin Huang. Grounded text-to-image synthesis with attention refocusing. arXiv preprint arXiv:2306.05427, 2023. [39] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [40] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. [41] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. [42] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, 2015. [43] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. In NeurIPS, 2022. [44] Takahiro Shirakawa and Seiichi Uchida. Noisecollage: layout-aware text-to-image diffusion model based on noise cropping and merging. In CVPR, 2024. [45] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. [46] Xudong Wang, Trevor Darrell, Sai Saketh Rambhatla, Rohit Girdhar, and Ishan Misra. Instancediffusion: Instance-level control for image generation. In CVPR, 2024. [47] Jiayu Xiao, Liang Li, Henglei Lv, Shuhui Wang, and Qingming Huang. R&b: Region and boundary aware zero-shot grounded text-to-image generation. In ICLR, 2024. [48] Jinheng Xie, Yuexiang Li, Yawen Huang, Haozhe Liu, Wentian Zhang, Yefeng Zheng, and Mike Zheng Shou. Boxdiff: Text-to-image synthesis with training-free box-constrained diffusion. In ICCV, 2023. [49] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. In NeurIPS, 2023. [50] Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, and Bin Cui. Mastering text-to-image diffusion: Recaptioning, planning, and generating with multimodal llms. In ICML, 2024. [51] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV, 2023. [52] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. [53] Guangcong Zheng, Xianpan Zhou, Xuewei Li, Zhongang Qi, Ying Shan, and Xi Li. Layoutdiffusion: Controllable diffusion model for layout-to-image generation. In CVPR, 2023. [54] Dewei Zhou, You Li, Fan Ma, Xiaoting Zhang, and Yi Yang. Migc: Multi-instance generation controller for text-to-image synthesis. In CVPR, 2024. [55] Xingyi Zhou, Vladlen Koltun, and Philipp Krähenbühl. Simple multi-dataset detection. In CVPR, 2022."
        },
        {
            "title": "Appendix",
            "content": "Figure 5: Spatially grounded images generated by our GROUNDIT with varying aspect ratios and sizes. Each image is generated based on text prompt along with bounding boxes, which are displayed next to (or below) each image."
        },
        {
            "title": "A Positional Embeddings in Diffusion Transformers",
            "content": "Diffusion Transformers (DiT) [37] handle noisy images of varying aspect ratios and resolutions by processing them as set of image tokens. For this, the noisy image is first divided into patches, with each patch subsequently converted into an image token of hidden dimension through linear embedding layer. DiT then applies 2D sine-cosine positional embeddings to each image token, based on its coordinates (x, y), defined as follows: px,y := CONCAT [px, py] , where px := [cos(wd x), sin(wd x)]D/4 d=0 py := [cos(wd y), sin(wd y)]D/4 d=0 where wd = 1/10000(4d/D). The positional embedding px,y is then added to each corresponding image token, denoted as PE(). Additional Quantitative Comparisons: Grounding Accuracy In addition to Sec. 6.2, we provide further quantitative comparisons of grounding accuracy between our GROUNDIT and the baselines. Specifically, we generated images based on text prompts and bounding boxes using each method, then calculated the mean Intersection over Union (mIoU) between the detected bounding boxes from an object detection model [55] and the input bounding boxes. Below, we present the quantitative comparisons across three datasets with varying average numbers of bounding boxes: subset of MS-COCO-2014 [33], HRS-Spatial [3], and custom dataset. Dataset Subset of MS-COCO-2014 [33] HRS-Spatial [3] Custom Dataset Avg. # of Bounding Boxes 2.06 3. 4.48 Table 3: Average number of bounding boxes per dataset. Subset of MS-COCO-2014. We filtered the validation set of MS-COCO-2014 [33] to exclude image-caption pairs where the target objects were either not mentioned in the captions or duplicate objects were present. From this filtered set, we randomly selected 500 pairs for evaluation. The results are presented in Tab. 4, column 2. GROUNDIT outperforms R&B by 0.021 (a 5.1% improvement) and PixArt-R&B by 0.014 (a 2.2% improvement). The relatively small margin can be attributed to the simplicity of the task, as this dataset has an average of 2.06 bounding boxes (Tab. 3), making it less challenging even for the baseline methods. HRS-Spatial. Column 3 of Tab. 4 presents the results on the Spatial subset of the HRS dataset [3]. GROUNDIT surpasses R&B [47] by 0.046 (a 14.1% improvement) and PixArt-R&B by 0.038 (an 11.4% improvement). Compared to the results on the MS-COCO-2014 subset, the higher percentage increase in mIoU highlights the robustness of GROUNDIT under more complex grounding conditions. Note that HRS-Spatial has an average of 3.11 bounding boxes (Tab. 3), which is higher than that of the MS-COCO-2014 subset (2.06). Custom Dataset. The custom dataset consists of 500 layout-text pairs, generated using the layout generation pipeline from LayoutGPT [15]. As shown in column 4 of Tab. 4, GROUNDIT outperforms R&B by 0.052 (a 26.3% improvement) and PixArt-R&B by 0.044 (a 21.4% improvement). This dataset has the highest average number of bounding boxes at 4.48 (Tab. 3). These results further emphasize the robustness and effectiveness of our approach in handling more complex grounding conditions with larger number of bounding boxes. 15 Method Subset of MS-COCO-2014 [33] HRS-Spatial [3] Custom Dataset Backbone: Stable Diffusion [41] Stable Diffusion [41] PixArt-α [8] Layout-Guidance [9] Attention-Refocusing [38] BoxDiff [48] R&B [47] 0.176 0.233 0.307 0.254 0.324 0.411 0.068 0.085 0.199 0.145 0.164 0.326 PixArt-R&B GROUNDIT (Ours) 0. 0.432 0.334 0.372 Backbone: PixArt-α [8] 0.030 0.036 0.122 0.078 0.106 0.198 0. 0.250 Table 4: Quantitative comparisons of mIoU () on subset of MS-COCO-2014 [33], HRS-Spatial [3], and our custom dataset. Bold represents the best, and underline represents the second best method. Additional Quantitative Comparisons: Prompt Fidelity In addition to Sec. 6.3, we provide further quantitative comparisons of the prompt fidelity of the generated images between our GROUNDIT and the baselines. We evaluated the generated images from the HRS dataset [3] using three different metrics: CLIP score [21], ImageReward [49], and PickScore [28]. The results are presented in Tab. 5. Since PickScore evaluates preferences between pair of images, we report the difference between our GROUNDIT and each baseline method in column 4. Our GROUNDIT consistently outperforms the baselines in both CLIP score and ImageReward. For PickScore, GROUNDIT outperforms all baselines except PixArt-R&B, while remaining comparable. Method CLIP score ImageReward PickScore (Ours Baseline) Backbone: Stable Diffusion [41] Layout-Guidance [9] Attention-Refocusing [38] BoxDiff [48] R&B [47] 32.48 31.36 32.57 33.16 -0.401 -0.508 -0.199 -0. Backbone: PixArt-α [8] PixArt-R&B GROUNDIT (Ours) 33.49 33.63 0. 0.444 +0.30 +0.22 +0.30 +0.26 -0.04 - Table 5: Quantitative comparisons of prompt fidelity on the HRS dataset [3]. Bold represents the best method."
        },
        {
            "title": "D Additional Analysis on Semantic Sharing",
            "content": "In this section, we provide further analyses on the generatable resolution and the semantic sharing property of DiT, initially introduced in Sec. 5.2. Generatable Resolution of DiT. Although recent DiT models can generate images at various resolutions, they still struggle to produce images at completely arbitrary resolutions. We speculate that this limitation arises not from the model architecture itself, but from the resolution of the training images, which typically falls within specific range [8]. Generating images at resolutions far outside this range often results in implausible outputs, suggesting the existence of an acceptable resolution range for DiT, which we refer to as its generatable resolution. In Fig. 6, we illustrate this phenomenon. When the noisy image size falls within DiTs generatable resolution range, the model produces plausible images (rightmost two images). However, when the image size is significantly outside this range (leftmost two images), DiT fails to generate plausible image. 16 Figure 6: Illustration of the generatable resolution range of DiT. The images are generated using PixArt-α [8] from the text prompt dog, with varying resolutions. Semantic Sharing. Even though DiT models have limited range of generatable resolutions, their Transformer architecture offers flexibility in handling varying lengths of image tokens, making it feasible to merge two sets of image tokens and denoise them through single network evaluation. Leveraging this flexibility of Transformers, we presented our joint denoising technique (Alg. 1). Our main observation was that the joint denoising between two noisy images causes the two generated images to become semantically correlated, as illustrated in Fig. 3-(B) and Fig. 3-(C). In addition to the visualizations in Fig. 3, we further quantify the semantic sharing property by measuring the LPIPS score [52] between two generated images. To explore the effect of joint denoising, we varied the parameter γ [0, 1], which controls the proportion of denoising steps where joint denoising is applied. Specifically, γ = 0 means no joint denoising is applied, and each image is denoised independently, while γ = 1 means full joint denoising across all steps. As shown in Fig. 7, increasing γ (i.e., applying more joint denoising steps) results in decrease in the LPIPS score between the two generated images, indicating that the images become more semantically similar as joint denoising is applied for larger portion of the denoising process. (a) Semantic sharing between equal resolutions (b) Semantic sharing between different resolutions Figure 7: LPIPS score between two generated images with varying γ value. gradual decrease in LPIPS [52] indicates that joint denoising progressively enhances the similarity between the generated images."
        },
        {
            "title": "E Implementation Details",
            "content": "As the base text-to-image DiT model, we used the 512-resolution version of PixArt-α [8]. For sampling we employed the DPM-Solver scheduler [35] with 50 steps. Out of the 50 denoising steps, we applied our GROUNDIT denoising step (Alg. 2) for the initial 25 steps, and applied the vanilla denoising step for the remaining 25 steps. For the grounding loss in Global Update of GROUNDIT, we adopted the definition proposed in R&B [47], and we set the loss scale to 10 and used gradient descent weight of 5 for the gradient descent update in Eq. 7. As discussed in Sec. 5.3, for each i-th object branch we have noisy object image ui,t and noisy local patch vi,t, which is extracted from the noisy image ˆxt in main branch via vi,t Crop(ˆxt, bi). We determine the resolution of the noisy object image ui,t by selecting from PixArt-αs generatable resolutions, choosing one that best aligns with the aspect ratio of the corresponding bounding box bi. 17 All our experiments were conducted an NVIDIA RTX 3090 GPU. In Algorithm 2, we provide the pseudocode of GROUNDIT single denoising step. Algorithm 2: Pseudocode of GROUNDIT denoising step. Parameters : ωt; Inputs: xt, {ui,t}N 1 i=0 , G, cP ; Outputs: xt1, {ui,t1}N 1 i=0 ; // Gradient descent weight. // Noisy images, grounding conditions, text embedding. // Noisy images at timestep 1. 1 Function GlobalUpdate(xt, t, cP , G) : i=0 ExtractAttention(xt, t, cP , G); // bi holds coordinate information of bounding box, (Sec. 4) {Ai,t}N 1 LAGG (cid:80)N 1 ˆxt xt ωtxt LAGG; return ˆxt; // Extract cross-attention maps. // Compute aggregated grounding loss. // Gradient descent (Eq. 7) i=0 L(Ai,t, bi); 3 4 5 6 Function LocalUpdate(ˆxt, {ui,t}N 1 xt1 Denoise(ˆxt, t, cP ); for = 0, . . . , 1 do 7 i=0 , t, cP , G) : // i-th object branch vi,t Crop(ˆxt, bi); {ui,t1, vi,t1} JointDenoise(ui,t, vi,t, t, ci); for = 0, . . . , 1 do // mi is binary mask corresponding to bi xt1 xt1 (1 mi) + Uncrop(vi,t1, bi) mi; 9 11 12 13 xt1 xt1 return xt1, {ui,t1}N 1 i=0 ; 14 15 Function GrounDiTStep(xt, {ui,t}N 1 ˆxt GlobalUpdate(xt, t, cP , G) ; 16 i=0 , t, cP , G): // Main branch // Obtain noisy local patch. // Joint denoising. // Patch Transplantation. // Global update (Sec. 5.1) 17 18 i=0 LocalUpdate(ˆxt, {ui,t}N i=0 , t, cP , G) ; xt1, {ui,t1}N 1 (Sec. 5.3) return xt1, {ui,t1}N 1 i=0 ; // Local update"
        },
        {
            "title": "F Analysis on Computation Time",
            "content": "We present the average inference time based on the number of bounding boxes in Tab. 6. While our method shows slight increase in inference time, the rate of increase remains modest. For three bounding boxes, the inference time is 1.01 times that of R&B and 1.33 times that of PixArt-R&B. Even with six bounding boxes, the inference time is only 1.41 times that of R&B and 1.90 times that of PixArt-R&B. # of bounding boxes R&B [47] PixArt-R&B GROUNDIT (Ours) 3 37.52 28.31 37.71 38.96 28.67 41.10 5 39.03 29.04 47.83 6 39.15 29.15 55.30 Table 6: Comparison of the average inference time based on the number of bounding boxes. Values in the table are given in seconds."
        },
        {
            "title": "G Additional Qualitative Results",
            "content": "We provide more qualitative comparisons in Fig. 8. Our method demonstrates greater robustness against issues such as the missing object problem, attribute leakage, or object interruption problem [47], due to its local update mechanism with semantic sharing. For instance, in Row 1, baseline methods struggle to generate certain objects (i.e. missing object problem). In Row 2, baselines generate banana that retains features of an apple, illustrating attribute leakage. In Row 3, R&B generates bus that interrupts the generation of couch, with part of the bus overlapping with the designate region of the couch. Similarly, in PixArt-R&B, hamburger and donut interrupt the 18 Layout R&B [47] PixArt-R&B GROUNDIT bear sitting between surfboard and chair with bird flying in the sky. banana and an apple and an elephant and backpack in the meadow with bird flying in the sky. realistic photo, hamburger and donut and couch and bus and surfboard in the beach. blue vase and wooden bowl with watermelon sit on table, while bear holding an apple. Figure 8: Additional qualitative comparisons between our GROUNDIT, the previous state-of-the-art, R&B [47], and our internal baseline PixArt-R&B. Leftmost column shows the input bounding boxes, and columns 2-3 include the baseline results. The rightmost column includes the results of our GROUNDIT. generation of surfboard, demonstrating the object interruption problem. In more challenging cases, like Row 4, combinations of these issues appear. By contrast, our method consistently generates each object accurately within specified locations, even under complex bounding box configurations, highlighting its robustness and precision. Additional results are shown in Fig. 9, and examples of various aspect ratio images generated with grounding conditions are provided in Fig. 5. 19 Layout GROUNDIT Layout GROUNDIT boat floating on calm lake. An upright bear riding bicycle. An aurora lights up the sky and house is on the grassy meadow with mountain in the background. person is sitting on chair and bird is sitting on horse while horse is on the top of car. cat sitting on sunny windowsill. bicycle standing near telephone booth in the park. castle stands across the lake and the bird flies in the blue sky. Monet painting of woman standing on flower field holding an umbrella sideways with house in the background. Figure 9: Additional spatially grounded images generated by out GROUNDIT."
        }
    ],
    "affiliations": [
        "KAIST"
    ]
}