{
    "paper_title": "Feedforward 3D Editing via Text-Steerable Image-to-3D",
    "authors": [
        "Ziqi Ma",
        "Hongqiao Chen",
        "Yisong Yue",
        "Georgia Gkioxari"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent progress in image-to-3D has opened up immense possibilities for design, AR/VR, and robotics. However, to use AI-generated 3D assets in real applications, a critical requirement is the capability to edit them easily. We present a feedforward method, Steer3D, to add text steerability to image-to-3D models, which enables editing of generated 3D assets with language. Our approach is inspired by ControlNet, which we adapt to image-to-3D generation to enable text steering directly in a forward pass. We build a scalable data engine for automatic data generation, and develop a two-stage training recipe based on flow-matching training and Direct Preference Optimization (DPO). Compared to competing methods, Steer3D more faithfully follows the language instruction and maintains better consistency with the original 3D asset, while being 2.4x to 28.5x faster. Steer3D demonstrates that it is possible to add a new modality (text) to steer the generation of pretrained image-to-3D generative models with 100k data. Project website: https://glab-caltech.github.io/steer3d/"
        },
        {
            "title": "Start",
            "content": "Feedforward 3D Editing via Text-Steerable Image-to-3D"
        },
        {
            "title": "California Institute of Technology",
            "content": "5 2 0 2 5 1 ] . [ 1 8 7 6 3 1 . 2 1 5 2 : r Figure 1. We present Steer3D, which enables feedforward editing of generated 3D assets by ingesting text steerability to image-to-3D models. Steer3D uses an architecture inspired by ControlNet to leverage image-to-3D pretraining and achieve data efficiency. We build an automated data engine that generates 96k synthetic editing pairs for scalable training. Steer3D shows strong editing capabilities while staying consistent with the original 3D asset, as shown on the right side."
        },
        {
            "title": "Abstract",
            "content": "Recent progress in image-to-3D has opened up immense possibilities for design, AR/VR, and robotics. However, to use AI-generated 3D assets in real applications, critical requirement is the capability to edit them easily. We present feedforward method, Steer3D, to add text steerability to image-to-3D models, which enables editing of generated 3D assets with language. Our approach is inspired by ControlNet, which we adapt to image-to-3D generation to enable text steering directly in forward pass. We build scalable data engine for automatic data generation, and develop two-stage training recipe based on flow-matching training and Direct Preference Optimization (DPO). Compared to competing methods, Steer3D more faithfully follows the language instruction and maintains better consistency with the original 3D asset, while being 2.4 to 28.5 faster. Steer3D demonstrates that it is possible to add new modality (text) to steer the generation of pretrained imageto-3D generative models with 100k data. Project website: https://glab-caltech.github.io/steer3d/ 1. Introduction Generative image-to-3D models [18, 40, 45] enable users to create 3D assets from single image, unlocking new applications in design, AR/VR, and robotic simulation. However, integrating AI generations into real applications requires editing and customizing the generated 3D assets, capability not supported by these models. Existing solutions to 3D editing include 2D-3D pipelines which use image editing to change the object views and then lift to 3D [6, 34, 37]. However, they suffer from inconsistent image edit outputs, which are reflected in the reconstructed 3D shape. These methods are also slow they take several minutes per edit. To address these inherent limitations with pipeline methods, we want to train feedforward model that directly perform editing in 3D. The most intuitive approach is to design generative model which takes the original 3D asset as input, and generates the edited asset conditioned on the edit instruction. However, such models rely on paired 3D editing data with text instructions, which is very difficult 1 to obtain. Due to this challenge, training such model from scratch is impractical. We propose solution from different perspective: instead of training an editing model from scratch, we can turn any image-to-3D model into an editing model by augmenting it with text steerability. Fig. 1 shows an overview of our method, Steer3D. We start from pretrained image-to-3D generator and augment it with ControlNet [43]. ControlNet ingests the steering prompt and guides the base model toward the desired 3D edit. To edit any asset generated by the base model, we can run Steer3D with the input image plus the editing text, and directly obtain an edited asset that follows the instruction and remains consistent with the original asset. By leveraging shape and object priors from pretrained image-to-3D models, this design is more data-efficient than training standalone editing model from scratch. data-efficient architecture makes feedforward 3D editing tractable, yet we still need to collect paired 3D edit data for training. We develop an automated data engine that combines image editing models, large visionlanguage models, and image-to-3D generators to synthesize diverse geometryand texture-edit pairs. In total, we produce 96k training pairs covering wide range of shape and appearance changes. We then train Steer3D with flow matching [25], and apply Direct Preference Optimization (DPO; [35]) to avoid the trivial no-edit solution. Our analysis shows that both the data engine and the training recipe scale effectively. Fig. 1 (right) shows outputs from Steer3D. Our main contributions are as follows: We build feedforward model for 3D editing which demonstrates stronger instruction following and better consistency preservation than existing methods, while being 2.4 to 28.5 faster. We propose novel architecture that adapts ControlNet to image-to-3D generation to enable language steering. We design scalable, automatic data engine that can generate 3D editing paired data. We develop two-stage training recipe that integrates flow-matching and Direct Preference Optimization to add text steering capability to pretrained image-to-3D models. We release EDIT3D-BENCH, 3D editing benchmark with diverse object types and editing instructions. 2. Related Work Image-to-3D Models. Image-to-3D models generate 3D assets based on single image. Recent state-of-art models [38, 40, 45] formulate image-to-3D as conditional generation via rectified flow [25]. These models dont allow direct text-based editing, even though TRELLIS [40] can be combined with inpainting methods [29] to perform limited editing, assuming the additional input of 3D bounding box. We study editing with language guidance and no additional input, and via forward pass. 3D Editing Pipelines Based on 2D Editing. Starting from Instruct-NeRF2NeRF [13], various methods [46, 10, 22, 39] build pipelines with 2D editing models to perform 3D editing. [13, 17] are based on NeRF [30], [13, 17, 37] are based on Gaussian splatting [21], and [34] is based on triplane. While there is active effort on improving multiview editing consistency [4, 20, 37], this issue is still challenging, limiting editing quality. Furthermore, these pipelines are also slow due to rendering, multiple model invocations and NeRF or Gaussian optimization. Test-Time Optimization. Test-time optimization for 3D editing includes score-distillation-based methods and inversion-based methods. Score distillation [33] is adapted for 3D editing in [14, 23, 36, 46]. These methods use the score distillation loss from 2D text-to-image models to guide optimization. 3D-LATTE [31] is an inversion-based method that uses attention injection. These methods are slow due to the optimization required, and oftentimes contain instance-specific hyperparameters. Feedforward 3D Editing. Feedforward 3D editing is very challenging due to the lack of large-scale 3D edit pairs. SHAP-EDITOR [7] trains one model per editing instruction, and only supports 6 editing instructions. Other methods tackle an easier version of 3D editing, assuming the extra input of 3D bounding box, such as MaskedLRM [12] and Instant3DEdit [1]. Another class of feedforward 3D editing models is LLM-based, such as ShapeLLM-Omni [42] which tokenizes 3D assets, or LL3M [28] and BlenderAlchemy [19], which are agentic frameworks that edits blender code of 3D objects. 3. Method We enable editing of 3D assets by augmenting image-to-3D models with text steerability. Given reference image of an object and text instruction, we design feedforward model that generates new asset which follows the text instruction while being faithful to the unsteered generation of the image-to-3D model. To train such model, we introduce three key components: (a) an architecture design that can leverage generative pretraining of image-to-3D models; (b) an automatic data engine that generates creative editing prompts and pre-post editing pairs; (c) two-stage training recipe combining flow-matching training and Direct Preference Optimization (DPO; [35]). 3.1. Architecture Existing image-to-3D generative methods [40, 41, 45] rely on large-scale pretraining. For example, TRELLIS [40] is trained on 500k (image, 3D) pairs. To give them an additional 3D editing capability with natural language instructions is expected to require even more training data, in the Figure 2. Steer3D data engine: we begin with one rendered view for each Objaverese object and query an LLM for diverse editing instructions. Then, 2D editing model is used to generate an edited view, which is subsequently reconstructed into 3D asset. Editing pairs pass through two-stage filter, in which approximately 30% of all generated pairs are kept. radiance field, or mesh. We apply the same architecture to both the geometry and the texture model of TRELLIS. ControlNet for 3D Editing. The base model is composed of 24 stacked transformer blocks. We add ControlNet block corresponding to each layer: we first copy the architecture and weights of the base model transformer block, then add additional cross attention that attends to the editing text, and append projection layer that is initialized at zero. The output of each ControlNet block is added to the output of the corresponding base model block, which is then passed to the next transformer block of the base model. The ControlNet output is also passed to the next ControlNet block. This is illustrated in Fig. 3. The base model is kept frozen, and only ControlNet components are trainable. Since each projection layer is zero-initialized, the initial outputs of all ControlNet blocks are zero, meaning Steer3D predicts the base model output at initialization. This provides us with good optimization starting point. Furthermore, since the ControlNet transformer blocks are initialized with the base model weights, the control branch preserves object and shape priors learned during the base models pretraining. These design choices enable data-efficient, stable training. We deviate from the the original ControlNet paper [43]. ControlNet was originally UNet-based and added control to skip connections. We adapt it for transformer-based flow models and add it directly to each blocks feature. 3.2. Data Engine Feedforward models are data-driven. Yet, relying on humans to generate large-scale 3D editing pair data is infeasible. To tackle this problem, we propose scalable synthetic data pipeline utilizing 2D editing models and image-to-3D generative models. We sample 16k objects from Objaverse [8] and generate 96k high quality 3D editing pairs. Our data engine is illustrated in Fig. 2. For each Objaverse asset, we first rotate the object and render 2D view, so that our data is not constrained to front-side edits. Next, we prompt VLM, GPT-4.1-mini, to propose total of 20 Figure 3. Steer3D architecture: we design ControlNet-based architecture to leverage the shape and geometry prior of pretrained image-to-3D generative models. We add trainable ControlNet block corresponding to each transformer block in the base model. form of (image, instruction, 3D) triplets. Such data is hard to obtain at scale. We propose an alternative that is less data-hungry, inspired by advances in image editing [43]. We build upon existing pretrained 3D models, and learn to steer their representations by designing text-conditioned 3D ControlNet, as shown in Fig. 3. Base Model. Steer3D uses ControlNet architecture [43] adapted to image-to-3D generation. We choose TRELLIS [40], state-of-the-art image-to-3D approach, as our base model. TRELLIS is composed of two rectified flow models: the first flow model generates coarse geometry represented as binary occupancy in 646464 voxel grid. The second flow model takes the generated geometry as input, and generates latent features that can decode into Gaussian splats, 3 creative edits in the following categories: addition, removal, and new texture. Then, we edit the rendered view with state-of-the-art 2D editing model, Step1X-Edit [27], and reconstruct to 3D using an image-to-3D model, Hunyuan-3D 2.1 [45]. Our data engine generates 320k raw 3D pairs using total of 2, 500 H100 hours. To improve data quality, we perform two-stage data filtering to reduce incorrect edits and inconsistent reconstructions. For image editing errors, we first prompt VLM, unaware of the editing instruction, to list the differences between rendered view of the original and edited asset. Then, we query separate LLM (without giving it the images) to determine if the differences listed indicate successful and accurate edit. We separate these two roles to reduce hallucination. For reconstruction inconsistency, we use 2D perceptual similarity [11] thresholding on rendered images of the generated asset and the edited image, and discard pairs with low similarity. This two-stage filter filters out 70% of all generated pairs. We show outputs of our data engine in Fig. 1 and Fig. 13. 3.3. Training Recipe Steer3D uses two-stage training recipe: (1) supervised flow-matching training; (2) Direct Preference Optimization (DPO; [35]) to penalize the no edit behavior, local optimum for our model. Supervised Flow-Matching Training. Similar to our base model, Steer3D is rectified flow model which defines linear forward process between data samples x0 and noise samples ϵ (0,I), x(t) = (1 t)x0 + tϵ. The flowmatching objective aims to align the velocity at each timestep with the overall direction of ϵ x0. We start by fine-tuning the ControlNet weights with the text prompt via the normal flow-matching loss: ϕ = Et,x0,ϵ[vθ,ϕ(xt, t) (ϵ x0)2] LSFT where θ is the frozen base model weights and ϕ is the trainable ControlNet weights, vθ,ϕ is the predicted velocity by the ControlNet-enhanced model. Direct Preference Optimization. Supervised flowmatching training can yield conservative model which sometimes ignores the edit prompt and predicts the unedited asset. We hypothesize that this is because the preand postedit assets are close in the latent space for localized edits, and distribution-matching-type objective might struggle to push the prediction apart from the original output (which is also our initialization based on ControlNet). We want to explicitly discourage the no edit behavior via DPO. For each editing pair, we use the ground truth as positive example, the original generation as negative example, and apply the DPO loss adapted for flow matching [26]: LDPO ϕ = E[log sigmoid( β 2 ((vθ,ϕ(x+, t) (ϵ x+)2 vθ,ϕ(x, t) (ϵ x)2) (vref(x+, t) (ϵ x+)2 vref(x, t) (ϵ x)2))] + αLSFT ϕ where x+ is the edited (positive) asset latent, and the unedited (negative) asset latent, and vref is the checkpoint from the supervised flow-matching training. To stabilize DPO training, we also add the flow-matching loss as regularization. Training Details. Since our base model, TRELLIS, contains two separate flow models for geometry and texture respectively, we perform separate training for each stage, following roughly the same recipe. For both stages, we apply gradient clipping to stabilize training. We sample timesteps with higher standard deviation than the original TRELLIS training, to increase coverage at small timesteps. For the geometry ControlNet, we train separately for addition and removal for the best performance, and do not perform DPO. For texture editing, since geometry should be unchanged, the geometry model is run with no control, yielding the source geometry. We train on 6 A100 GPUs with per-GPU batch size of 2 and gradient accumulation of 2 for the geometry stage, and per-GPU batch size of 8 without gradient accumulation for the texture stage. We note that Steer3D separates geometry and texture because our base model, TRELLIS, separates them into two models. However, our recipe is not limited to two-stage models. Classifier-Free Guidance. Similar to other flow-based models, we apply classifier-free guidance (CFG; [16]) during supervised flow-matching training for the texture ControlNet by dropping the editing text with probability of 0.2. For the geometry model, we train without CFG because it can lead to unstable training. At inference time, we can still optionally perform CFG by using the original base model (which does not take in text) as the unconditional model. 4. EDIT3D-BENCH 3D editing has been largely evaluated qualitatively. Since different methods assume different inputs, such as multiple posed views [4, 13, 37], optimized Gaussian splats [6], or NeRF [17], there is no standard benchmark. Prior methods evaluate editing quality based on metrics such as CLIPScore [15] or PickScore [24], or LLM-based text-to-3D evaluator Eval3D [9]. These metrics, not designed for 3D editing evaluation, have 3 key limitations: (1) they are designed for descriptive, caption-type text, and editing prompts such as add something, remove something, replace something are out of distribution; (2) they only measure align4 LL3M [28]: An agentic framework that performs 3D editing and generation via Blender script. LL3M converts the input images into into Blender geometric primitives and edits the blender script conditioned on the text instruction. We compare to LL3M qualitatively as their API allows for just 5 queries per day. Pipelines using 2D editing: Edit-TRELLIS: Edit-TRELLIS edits the input image with the text instruction using an image editing model, Step1XEdit [27], and subsequently lifts the edited image to 3D using TRELLIS [40], similar to our data engine. Tailor3D [34]: Tailor3D applies image editing on two views of the original 3D asset the input image and back view, then reconstructs the 3D asset. Since it lacks an internal 2D editing module, we employ Step1X-Edit, which is used in our data engine. DGE [6]: DGE takes in the pre-edit 3D Gaussian splats [21], and performs multi-view image editing with InstructPix2Pix [3] to optimize the edited Gaussian splats. Metrics. To thoroughly assess editing quality, we evaluate both geometry and texture. Although there can be multiple correct answers for an editing task, our benchmark is curated to be relatively unambiguous, making alignment with ground truth reasonable proxy for correctness. Geometry: we compare the prediction to ground truth using Chamfer Distance (lower better) and F1 score (higher better) of 10,000 points sampled on the mesh surface. F1 is evaluated with threshold of 0.05. Texture: we render out front, back, left, right, up, and down views of the prediction and ground truth assets, and calculate the mean LPIPS [44] across six views. lower LPIPS means better alignment with ground truth. Our method preserves orientation and scale, whereas EditTRELLIS, LL3M, and ShapeLLM-Omni can return objects with mismatched pose or size. To accommodate these baselines, we align their outputs to the ground truth using Iterative Closest Point [2] before evaluation. 5.1. Qualitative Results Fig. 5 shows qualitative predictions on EDIT3D-BENCH. Steer3D shows consistent advantage compared to baselines in both faithfulness to the text prompt and consistency with the original generation (pre-edit). key aspect of text steerability in 3D is precise 3D localization of the desired changes. Steer3D shows strong localization capability e.g., editing only the lampshade or the chairs hollow backrest while leaving other parts untouched. Feedforward baselines ShapeLLM and LL3M struggle to match both the input image and the edit instruction. ShapeLLM often yields broken geometry (e.g., crab, lamp) or reproduces the input asset while ignoring the edit. LL3M, which operates on simplified Blender primitives, fails on Figure 4. We present EDIT3D-BENCH, which contains diverse edits spanning texture, addition, and removal. It comprises 250 objects and 250 distinct edits, which is 41.6 the size of the existing commonly-evaluated objects from InstructNerf2Nerf. ment to the editing text, but cannot evaluate consistency with the original 3D asset. (3) they rely on 2D renderings and cannot directly evaluate 3D geometry. We propose EDIT3D-BENCH, benchmark composed of (pre-edit 3D, instruction, post-edit 3D) triplets. The ground truth post-edit 3D enables evaluation of both correctness and consistency, via 3D geometry metrics such as Chamfer Distance and F1 score, and 2D perceptual metrics such as LPIPS [44] on rendered views. EDIT3D-BENCH is sourced from our data engine and selected and verified by humans. EDIT3D-BENCH encompasses 250 objects with distinct and detailed edit instructions, including 150 distinct texture edits, 50 addition edits and 50 removal edits. EDIT3D-BENCH is 41.6 the size of the existing commonly-evaluated examples. Examples are shown in Fig. 13. Our instructions are more detailed and diverse than exisitng commonly-evaluated instructions such as add hat or man to clown. We also provide tooling for 3D representation conversion and metric calculation, making the benchmark plug-and-play. 5. Experiments We conduct both qualitative and quantitative evaluations to assess the 3D editing capabilities of Steer3D. We show that Steer3D outperforms competing approaches qualitatively and quantitatively on both geometry and appearance-based edits, while being 2.4 to 28.5 faster. Finally, we include ablation studies and scaling analysis that validate key design and training choices. Baselines. We compare Steer3D with both feedforward methods that directly edit in 3D, and pipeline methods that leverage 2D editing. Feedforward baselines: ShapeLLM-Omni [42]: An LLM that tokenizes 3D meshes and performs text-based editing by generating mesh tokens that decode into meshes. ShapeLLM-Omni supports geometry-only edits (no texture). 5 Figure 5. (a): Qualitative comparison of our method with baselines. Steer3D shows strong advantage both in editing correctness and in consistency with the pre-edit 3D asset. Steer3D demonstrates strong localization capability e.g., editing only the chairs hollow backrest, while baselines like Edit-TRELLIS and LL3M fail by modifying the whole chair. (b): More qualitative examples of Steer3D. (c): Steer3D is able to perform edits that are not visible in the input view, whereas Edit-TRELLIS, which relies on front-view 2D editing, fails. 6 Removal Addition Seen Assets, Unseen Edits Unseen Assets Seen Assets, Unseen Edits Unseen Assets LPIPS () Chamfer () F1 () LPIPS () Chamfer () F1 () LPIPS () Chamfer () F1 () LPIPS () Chamfer () F1 () Tailor3D Edit-TRELLIS DGE ShapeLLM Steer3D (Ours) 0.234 0.192 0.219 0.221 0.168 0.154 0.133 0.235 0.147 0.049 0.117 0.189 0.093 0.181 0.310 0.218 0.169 0.168 0.203 0.146 0.196 0.218 0.262 0.210 0.102 0.138 0.144 0.080 0.128 0. 0.259 0.227 0.229 0.265 0.181 0.160 0.196 0.245 0.162 0.107 0.116 0.179 0.063 0.158 0.285 0.265 0.236 0.227 0.265 0.199 0.152 0.171 0.234 0.180 0.089 0.126 0.183 0.077 0.152 0. Table 1. Quantitative comparison on geometry edits: addition and removal. We evaluate LPIPS, Chamfer Distance, and F1 score both on seen assets (unseen edits) and unseen assets. Steer3D shows strong advantage on all metrics in all settings, with 64% higher F1 score, 63% reduction in Chamfer Distance, and 53% reduction in LPIPS. cases like crab and cup, producing shapes inconsistent with the image. These issues underscore the difficulty of aligning image, text, and 3D in single pass. In contrast, Steer3D delivers high-quality 3D edits faithful to both the the prompt and the unsteered (pre-edit) generation without any intermediate image editing. 2D3D pipelines that rely on image editing often suffer from inconsistencies. These inconsistencies arise because small deviations in the edited images can lead to large variations in the reconstructed 3D assets, even beyond the intended local edit. This is evident in the cup example (last row), where Edit-TRELLIS reconstructs thinner, non-symmetric cup side effect of the inconsistency between the reconstructions of post-edit image vs. the original image. In contrast, Steer3D enables editing by steering its internal diffusion representation; as result, variations across noise samples are minor, and the edited outputs remain consistent with the original 3D asset. Tailor3D exhibits degraded geometry and texture due to mismatches between its edited front and back views. DGE, which performs multiview 2D editing, suffers from analogous inconsistencies, producing floaters and artifacts during Gaussian optimization. Furthermore, due to Steer3Ds 3D nature, it is able to apply edits that are not visible in the front view, as shown in Fig. 5 (c). 2D-editing-based pipelines like Edit-TRELLIS fails because the 2D editing model cannot apply such edits. This results in hallucination (the multiple tires example), or failure to edit (could not add cargo door). Steer3D can also edit 3D assets reconstructed from inthe-wild iPhone or online photos. We provide such examples, as long as more qualitative examples on Steer3D, in the supplementary material. 5.2. Quantitative Results We use separate metrics for texture and geometry for holistic evaluation. We also perform human evaluation in the supplementary material. Texture. We evaluate texture editing on the texture subset of EDIT3D-BENCH in Tab. 2. We measure both appearance metrics, LPIPS, and geometry metrics, Chamfer Distance and F1 score. Since the edits are texture-based, obSeen Assets, Unseen Edits Unseen Assets LPIPS () Chamfer () F1 () LPIPS () Chamfer () F1 () Tailor3D Edit-TRELLIS DGE ShapeLLM Steer3D (Ours) 0.246 0.192 0.265 0.227 0.142 0.134 0.133 0.252 0.141 0.096 0.158 0.189 0.075 0.158 0.266 0.266 0.219 0.233 0.250 0. 0.173 0.158 0.267 0.161 0.071 0.114 0.168 0.064 0.161 0.359 Table 2. Quantitative comparison on texture edits. We evaluate LPIPS for edit correctness, and Chamfer Distance and F1 score for geometry consistency. We evaluate both on seen assets (unseen edits) and unseen assets. Steer3D shows strong advantage on all metrics in all settings, with 113% improvement in F1 score, 55% reduction in Chamfer Distance, and 43% reduction in LPIPS. ject geometry should not be changed in the output, which is captured by our geometry metrics. predicted asset with perfect geometry consistency should yield Chamfer Distance of close to 0 and F1 score of close to 1. To assess generalization, we evaluate both on seen assets with unseen editing instructions, as well as on unseen assets. From Tab. 2 we observe that Steer3D shows 43% reduction in LPIPS compared to the second best method (Edit-TRELLIS), demonstrating better alignment with ground truth in appearance. Our method also obtains 55% lower Chamfer Distance and 113% improvement in F1 score, maintaining better geometry consistency. These results align with the qualitative advantage seen from Fig. 5. Geometry. For geometry edits, we evaluate on removal and addition of object parts, each on both seen assets (unseen edits) and unseen assets. LPIPS alignment with ground truth measures both consistency in texture and correctness in geometry, since it measures the 2D per-view alignment with ground truth. Chamfer Distance and F1 score measure the geometry editing correctness. As seen in Tab. 1, Steer3D shows strong performance advantage compared to all baselines on all metrics. For removal edits, we achieve 63% reduction in Chamfer distance and 64% higher F1 score. For addition, we achieve 41% reduction in Chamfer Distance and 38% higher F1 compared to the second best method. Steer3D also obtains lower LPIPS across all settings, demonstrating better holistic alignment with the ground truth. Inference Time. As seen in Fig. 7, Steer3D is the fastest 7 Steer3D no edit failure (%) w/o DPO w/ DPO 18.67 10.67 (a) LPIPS () Chamfer () F1 () No ControlNet No Data filtering Current 0.238 0.213 0. (b) 0.150 0.114 0.089 0.177 0.244 0.2527 (c) Figure 6. Ablations and scaling analysis of Steer3D. (a): DPO effectively alleviates the failure mode of no edit predictions. (b): Ablations on the ControlNet architecture and data filtering. No ControlNet corresponds to simply adding text as another conditioning signal and finetuning the base model. Both the ControlNet design and data filtering improve all metrics. (c): We see steady improvement on both LPIPS (lower better) and Chamfer Distance as the training data size increases. Reported metrics are evaluated on the unseen addition set. tioning signal, and fine-tuning the full TRELLIS model, instantiated as DiT [32]. As shown in Fig. 6 (b), this alternative results in much worse performance on all metrics, validating the importance of ControlNet. Data Filtering. Data filtering sharpens the distribution around specific edit types. For example, for addition edits, data filtering removes edit pairs with non-obvious geometry change (via voxel mean absolute error thresholding and texture-keyword filtering), only keeping 41% of the original data. Fig. 6(b) shows the effectiveness of filtering on all metrics. Scaling Analysis. Fig. 6(c) shows continued improvement in model performance as we scale up training data, which corroborates the importance of our data engine. Limitations. When the editing instructions are complex, Steer3D exhibits limitations of inconsistency on unedited parts, edit leaking, and partial edits. Fig. 8 shows some examples. 6. Conclusions We present Steer3D, method that enables feedforward 3D editing by adding text steering capability to image-to-3D generative models. Steer3D uses an architecture inspired by ControlNet to enable direct text steering via forward pass. Steer3D works on general objects with complex editing prompts. Compared to existing methods, Steer3D demonstrates better instruction following and improved consistency with original 3D assets, while being 2.4 to 28.5 faster. Beyond unlocking new capability, Steer3D also answers more general research question: given pretrained generative model, is it possible to add steering capability via another modality (e.g., text)? In the case of 3D generation, Steer3D shows that this is possible with just less than 100k data. We hope that by sharing our recipe, we can inspire similar explorations in other domains. Steer3D is general framework that can keep improving with better base models and data generation techniques. We show scalable path towards general 3D editing and cross-modality steering for generative models. Figure 7. Inference time comparison. Steer3D is the fastest 2.4 to 28.5 faster than competing methods. Figure 8. Limitations of Steer3D. For complex edits, Steer3D may yield inconsistency (add potted plant, remove orange petals, edit leaking (change the chair red), or partial edits (remove gold chain). 2.4 to 28.5 faster than competing methods. 2D-editingbased pipelines are slow due to rendering, invocation(s) of 2D editing models, and 3D reconstruction. For the feedforward baselines, LL3M is slow due to its agentic design. ShapeLLM is slow due to mesh decoding and an additional texturing step. The reported 11.8s for Steer3D is the total time of both stages, each running 25 steps for the flow model, with classifier-free guidance. 5.3. Ablations and Analyses DPO. Flow-matching supervised learning yields conservative model that sometimes predicts no edit results. To combat this, we perform second stage of training using DPO [35] with the original 3D assets as negative examples. Fig. 6(a) shows 8% (absolute) reduction of this failure mode. ControlNet Design. The alternative architecture design to our method is directly passing text as an additional condi8 7. Acknowledgments We would like to thank Xiang Li for discussions on DPO. We also thank Bowen Zhang, Bingliang Zhang, Wenda Chu, Sasha Sax, and Jiacheng Liu for general discussions. Ziqi Ma is supported by the Kortschak scholarship. This project is funded in part by NSF #2505096, and CAST."
        },
        {
            "title": "References",
            "content": "[1] Amir Barda, Matheus Gadelha, Vladimir Kim, Noam Aigerman, Amit Bermano, and Thibault Groueix. Instant3dit: Multiview inpainting for fast editing of 3d objects. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1627316282, 2025. 2 [2] Paul Besl and Neil McKay. Method for registration of 3-d shapes. In Sensor fusion IV: control paradigms and data structures, pages 586606. Spie, 1992. 5 [3] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1839218402, 2023. 5 [4] Hansheng Chen, Ruoxi Shi, Yulin Liu, Bokui Shen, Jiayuan Gu, Gordon Wetzstein, Hao Su, and Leonidas Guibas. Generic 3d diffusion adapter using controlled multi-view editing. arXiv preprint arXiv:2403.12032, 2024. 2, 4 [5] Jun-Kun Chen, Samuel Rota Bulo, Norman Muller, Lorenzo Porzi, Peter Kontschieder, and Yu-Xiong Wang. Consistdreamer: 3d-consistent 2d diffusion for high-fidelity scene In Proceedings of the IEEE/CVF Conference on editing. Computer Vision and Pattern Recognition, pages 21071 21080, 2024. [6] Minghao Chen, Iro Laina, and Andrea Vedaldi. Dge: Direct gaussian 3d editing by consistent multi-view editing. In European Conference on Computer Vision, pages 7492. Springer, 2024. 1, 2, 4, [7] Minghao Chen, Junyu Xie, Iro Laina, and Andrea Vedaldi. Shap-editor: Instruction-guided latent 3d editing in seconds. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2645626466, 2024. 2 [8] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: universe of 10m+ 3d objects. Advances in Neural Information Processing Systems, 36:3579935813, 2023. 3, 1 [9] Shivam Duggal, Yushi Hu, Oscar Michel, Aniruddha Kembhavi, William Freeman, Noah Smith, Ranjay Krishna, Antonio Torralba, Ali Farhadi, and Wei-Chiu Ma. Eval3d: Interpretable and fine-grained evaluation for 3d generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1332613336, 2025. 4 [10] Ziya Erkoc, Can Gumeli, Chaoyang Wang, Matthias Nießner, Angela Dai, Peter Wonka, Hsin-Ying Lee, and Peiye Zhuang. Preditor3d: Fast and precise 3d shape editing. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 640649, 2025. 2 [11] Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, and Phillip Isola. Dreamsim: Learning new dimensions of human visual similarity using synthetic data. arXiv preprint arXiv:2306.09344, 2023. 4, 2 [12] Will Gao, Dilin Wang, Yuchen Fan, Aljaz Bozic, Tuur Stuyck, Zhengqin Li, Zhao Dong, Rakesh Ranjan, and Nikolaos Sarafianos. 3d mesh editing using masked lrms. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 71547165, 2025. 2 [13] Ayaan Haque, Matthew Tancik, Alexei Efros, Aleksander Holynski, and Angjoo Kanazawa. Instruct-nerf2nerf: EditIn Proceedings of the ing 3d scenes with instructions. IEEE/CVF international conference on computer vision, pages 1974019750, 2023. 2, [14] Runze He, Shaofei Huang, Xuecheng Nie, Tianrui Hui, Luoqi Liu, Jiao Dai, Jizhong Han, Guanbin Li, and Si Liu. Customize your nerf: Adaptive source driven 3d scene editIn Proceedings of ing via local-global iterative training. the IEEE/CVF conference on computer vision and pattern recognition, pages 69666975, 2024. 2 [15] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. In Proceedings of the 2021 conference on empirical methods in natural language processing, pages 75147528, 2021. 4 [16] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 4, 1 [17] Susung Hong, Johanna Karras, Ricardo Martin-Brualla, and Ira Kemelmacher-Shlizerman. Perturb-and-revise: Flexible 3d editing with generative trajectories. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1629316303, 2025. 2, 4 [18] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. Lrm: Large reconstruction model for single image to 3d. arXiv preprint arXiv:2311.04400, 2023. 1 [19] Ian Huang, Guandao Yang, and Leonidas Guibas. Blenderalchemy: Editing 3d graphics with vision-language models. In European Conference on Computer Vision, pages 297 314. Springer, 2024. 2 [20] Nazmul Karim, Hasan Iqbal, Umar Khalid, Chen Chen, and Jing Hua. Free-editor: zero-shot text-driven 3d scene editing. In European Conference on Computer Vision, pages 436 453. Springer, 2024. [21] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. 2, 5 [22] Umar Khalid, Hasan Iqbal, Nazmul Karim, Muhammad Tayyab, Jing Hua, and Chen Chen. Latenteditor: Text driven local editing of 3d scenes. In European Conference on Computer Vision, pages 364380. Springer, 2024. 2 [23] Hayeon Kim, Ji Ha Jang, and Se Young Chun. Robust 3d-masked part-level editing in 3d gaussian splatting with In Proceedings of regularized score distillation sampling. the IEEE/CVF International Conference on Computer Vision (ICCV), pages 55015510, 2025. 2 9 [38] Guanjun Wu, Jiemin Fang, Chen Yang, Sikuang Li, Taoran Yi, Jia Lu, Zanwei Zhou, Jiazhong Cen, Lingxi Xie, Xiaopeng Zhang, et al. Unilat3d: Geometry-appearance unified latents for single-stage 3d generation. arXiv preprint arXiv:2509.25079, 2025. 2 [39] Jing Wu, Jia-Wang Bian, Xinghui Li, Guangrun Wang, Ian Reid, Philip Torr, and Victor Adrian Prisacariu. Gaussctrl: Multi-view consistent text-driven 3d gaussian splatting editing. In European Conference on Computer Vision, pages 55 71. Springer, 2024. [40] Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3D Latents for Scalable and Versatile 3D Generation, 2024. arXiv:2412.01506 [cs]. 1, 2, 3, 5 [41] Xianghui Yang, Huiwen Shi, Bowen Zhang, Fan Yang, Jiacheng Wang, Hongxu Zhao, Xinhai Liu, Xinzhou Wang, Qingxiang Lin, Jiaao Yu, et al. Hunyuan3d 1.0: unified framework for text-to-3d and image-to-3d generation. arXiv preprint arXiv:2411.02293, 2024. 2 [42] Junliang Ye, Zhengyi Wang, Ruowen Zhao, Shenghao Xie, and Jun Zhu. Shapellm-omni: native multimodal llm for 3d generation and understanding. arXiv preprint arXiv:2506.01853, 2025. 2, 5 [43] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 38363847, 2023. 2, 3 [44] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 5 [45] Zibo Zhao, Zeqiang Lai, Qingxiang Lin, Yunfei Zhao, Haolin Liu, Shuhui Yang, Yifei Feng, Mingxin Yang, Sheng Zhang, Xianghui Yang, et al. Hunyuan3d 2.0: Scaling diffusion models for high resolution textured 3d assets generation. arXiv preprint arXiv:2501.12202, 2025. 1, 2, [46] Jingyu Zhuang, Di Kang, Yan-Pei Cao, Guanbin Li, Liang Lin, and Ying Shan. Tip-editor: An accurate 3d editor following both text-prompts and image-prompts. ACM Transactions on Graphics (TOG), 43(4):112, 2024. 2 [24] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. Advances in neural information processing systems, 36:36652 36663, 2023. 4 [25] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 2 [26] Jie Liu, Gongye Liu, Jiajun Liang, Ziyang Yuan, Xiaokun Liu, Mingwu Zheng, Xiele Wu, Qiulin Wang, Wenyu Qin, Menghan Xia, et al. Improving video generation with human feedback. arXiv preprint arXiv:2501.13918, 2025. 4 [27] Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et al. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025. 4, 5 [28] Sining Lu, Guan Chen, Nam Anh Dinh, Itai Lang, Ari Holtzman, and Rana Hanocka. Ll3m: Large language 3d modelers. arXiv preprint arXiv:2508.08228, 2025. 2, 5 [29] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Repaint Van Gool. Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1146111471, 2023. 2 [30] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. [31] Maria Parelli, Michael Oechsle, Michael Niemeyer, Federico Tombari, and Andreas Geiger. 3d-latte: Latent space 3d editing from textual instructions. arXiv preprint arXiv:2509.00269, 2025. 2 [32] William Peebles and Saining Xie. Scalable Diffusion Models with Transformers. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 41724182, Paris, France, 2023. IEEE. 8 [33] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. 2 [34] Zhangyang Qi, Yunhan Yang, Mengchen Zhang, Long Xing, Xiaoyang Wu, Tong Wu, Dahua Lin, Xihui Liu, Jiaqi Wang, and Hengshuang Zhao. Tailor3d: Customized 3d assets editing and generation with dual-side images. arXiv preprint arXiv:2407.06191, 2024. 1, 2, 5 [35] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. 2, 4, 8 [36] Etai Sella, Gal Fiebelman, Peter Hedman, and Hadar Averbuch-Elor. Vox-e: Text-guided voxel editing of 3d objects. In Proceedings of the IEEE/CVF international conference on computer vision, pages 430440, 2023. [37] Yuxuan Wang, Xuanyu Yi, Zike Wu, Na Zhao, Long Chen, and Hanwang Zhang. View-consistent 3d editing with gaussian splatting. In European conference on computer vision, pages 404420. Springer, 2024. 1, 2, 4 10 Feedforward 3D Editing via Text-Steerable Image-to-3D"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Additional Qualitative Examples of Steer3D We show additional qualitative examples both on our benchmark and in the wild from objects in iPhone or online photos, or AI-generated images. Fig. 10 and Fig. 11 show additional examples on our benchmark in 4 views, and Fig. 12 shows examples on in-the-wild objects based on iPhone or online photos, or an AI-generated image. Inthe-wild evaluation is challenging: Steer3D is only trained on synthetic data based on Objaverse [8] assets, and reconstructed real-world objects are out of distribution. Even in this challenging setting, Steer3D is able to perform edits correctly and preserve consistency with the original asset. B. Additional Examples of EDIT3D-BENCH We provide additional examples of EDIT3D-BENCH in Fig. 13 to show that EDIT3D-BENCH is diverse and representative of edits that normal user would apply to generated 3D assets. C. Human Evaluation We conduct an additional double-blind experiment on EDIT3D-BENCH between Steer3D and the strongest competing method, Edit-TRELLIS, and show results in Tab. 3. Steer3D wins over Edit-TRELLIS with an over 2 : 1 win rate. The experiment UI is shown in Fig. 14. The top row shows the pre-edit 3D asset rendered in 4 views. The editing instruction is shown at the bottom. The middle and bottom rows are two blinded methods in randomized order per example i.e., no method is consistently shown on top or below. The user is instructed to evaluate whether the edited asset not only follows the text instruction but also keeps consistent with the original asset, in shape, appearance (modulo the edit), size, and orientation. The annotators are encouraged to use the tie option sparingly less than 2% from the experiment result. When calculating win rates, we do not include tied comparisons. Three annotators conduct the experiment independently, and we report the average of their preference results. D. Additional Training Details and Analyses Section 5.3 of the main paper provides key ablations and scaling analyses. We provide additional analyses in this section. Additional Scaling Analyses. We further show the scaling behavior on texture in Fig. 9, evaluated by average LPIPS from renderings of the prediction vs. ground truth on unseen assets, as detailed in main paper Section 5. Similar to the scaling trends on the geometry metrics shown in the main paper (Section 5.3), we observe steady decrease of LPIPS as the training data scales up, which further corroborates the importance of our scalable data engine and training recipe. Figure 9. Scaling analysis on texture edits, evaluated on unseen assets. As the training size grows, LPIPS decreases steadily, showing the importance of scaling up. The axis is plotted on log scale. Classifier-Free Guidance. In our experiments, we notice that when the dataset size is small, Classifier-Free Guidance [16] can have negative effect, whereas it helps at large scale. One possible explanation for this observation is that CFG requires strong-enough conditional model, which only emerges when the training scale is large enough. Additional Training Details. Section 3.3 of the main paper briefly discusses training details. We list additional hyperparameters in Tab. 5. We use bfloat16 during training to improve speed, and use gradient checkpointing for stage 2 and DPO training to save memory. Feature normalization is applied to the per-voxel latents during training, as in the original TRELLIS [40] paper. Furthermore, because our data engine leverages Hunyuan [45] whereas our base model uses TRELLIS [40], sometimes the base model prediction might have orientation or size differences from the edited version. Naive ControlNet training puts the additional burden of aligning orientation or size to the control branch, which makes the supervision signal of text-to-3D mapping less clear. To combat this, we first perform supervised finetuning of TRELLIS on Hunyuan outputs for the image-to-3D task, so that the base model prediction and the edited ground truth is generally aligned (in size and orientation), except for the edit. 1 Win Rate (%) Removal Addition Texture Seen Assets, Unseen Edits Unseen Assets Seen Assets, Unseen Edits Unseen Assets Seen Assets, Unseen Edits Unseen Assets Edit-TRELLIS Steer3D 44.0 56.0 24.0 76. 37.3 62.7 48.0 52.0 33.3 66.7 25.3 74.7 Total 32.9 67. Table 3. Human preference results (win rate) of Edit-TRELLIS vs. Steer3D, ranked by 3 independent annotators. Steer3D wins more than 2 : 1 compared to Edit-TRELLIS. correctly, but also keep consistent with the original object (modulo the edit). This means that we do not want big fluctuations in size, shape, geometry, or orientation. To achieve this, we employ 2D perceptual similarity metric, DreamSim [11]. Fig. 16 shows some examples that pass or fail the perceptual check. We note that these metrics cannot be used as the sole determinator of editing consistency no edit would yield perfect consistency distance of 0, despite being incorrect. However, given that the examples already pass through the LLM-based checking described in the previous paragraph, we know the editing has been carried out correctly, and thus perceptual distance can be used here as proxy for consistency. F. Author Contributions ZM conceived the project and developed the model and training recipe. HC implemented and developed the data engine and benchmark. Chamfer Distance () 8k-Scale 13k-Scale without CFG with CFG 0.1206 0.1423 0.1065 0.1021 Table 4. The effect of CFG at small vs. large data regime. We evaluate the effect of CFG with varying sizes of training data. CFG hurts performance when the training set is small (8k-scale), but improves performance when the training set is large (13k-scale). The reported metrics are from the unseen-removal set. Stage Stage 2 DPO (texture model) Effective batch size Gradient accumulation step Learning rate (AdamW) Training precision Timestep sampling (LogitNormal, mean;std) Gradient checkpointing CFG-puncond DPO-β DPO-supervised loss weight (α) 12 2 2e-5 bfloat16 48 1 5e-5 bfloat16 1.0;1.8 No 0.0 - - 1.0;1.0 Yes 0.2 - - 12 2 1e-6 bfloat16 1.0;1.0 Yes 0.0 0.2 1.0 Table 5. Additional training hyperparameters. E. Additional Data Engine Details We provide additional details about the data engine. As mentioned in Section 3.2, we use two-stage filter to improve data quality. LLM-Based Filtering of Editing Correctness. We use LLM-based filtering to ensure that the editing is performed correctly. Directly asking VLM whether the edit is carried out correctly results in hallucinations. We hypothesis that this is because distinguishing localized edits requires finegrained visual perception capabilities, which current VLMs struggle with. To address this challenge, we use two LLMs collaboratively. As shown in Fig. 15, one LLM is instructed to list the differences between two renderings of the object from the same camera angle, without knowing the editing instruction. Another LLM, without seeing the images, compares whether the difference description generated by the first LLM aligns exactly with the editing instruction. This method effectively reduces hallucination and improves filtering effectiveness, thus ensuring better data quality. 2D Perceptual Similarity for Consistency. We want the edited object to not only follow the text instruction 2 Figure 10. Additional qualitative examples on EDIT3D-BENCH. Pre-Edit (3D) shows the base image-to-3D model generation that we want to steer. Steer3D output is shown in 4 views. 3 Figure 11. Additional qualitative examples on EDIT3D-BENCH. Pre-Edit (3D) shows the base image-to-3D model generation that we want to steer. Steer3D output is shown in 4 views. 4 Figure 12. Additional qualitative examples on challenging in-the-wild objects, captured by iPhone photos, online photo, or AI-generated image. Image-to-3D shows the original base model output based on the input image. Steer3D output is shown in 4 views. We show best-of-3 output for in-the-wild evaluation. 5 Figure 13. We provide additional examples from EDIT3D-BENCH, which shows that EDIT3D-BENCH is diverse. 6 Figure 14. User interface for human evaluation. The top row shows the pre-edit 3D asset rendered in 4 views. The middle and bottom rows are two blinded methods in randomized order per example. The user presses for method A, for method B, or for tie. 7 Figure 15. LLM-based filter to filter out image editing errors. One LLM, without knowing the editing instruction, is asked to list the differences between two renderings of the object from the same camera angle. Another LLM compares whether the difference matches exactly with the edit instruction. Objects that fail this check are rejected. 8 Figure 16. 2D perceptual similarity (DreamSim) score to filter out reconstruction inconsistency. Consistent objects yield low distance, and inconsistent objects yield high distance."
        }
    ],
    "affiliations": [
        "Caltech"
    ]
}