{
    "paper_title": "MonoPlace3D: Learning 3D-Aware Object Placement for 3D Monocular Detection",
    "authors": [
        "Rishubh Parihar",
        "Srinjay Sarkar",
        "Sarthak Vora",
        "Jogendra Kundu",
        "R. Venkatesh Babu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Current monocular 3D detectors are held back by the limited diversity and scale of real-world datasets. While data augmentation certainly helps, it's particularly difficult to generate realistic scene-aware augmented data for outdoor settings. Most current approaches to synthetic data generation focus on realistic object appearance through improved rendering techniques. However, we show that where and how objects are positioned is just as crucial for training effective 3D monocular detectors. The key obstacle lies in automatically determining realistic object placement parameters - including position, dimensions, and directional alignment when introducing synthetic objects into actual scenes. To address this, we introduce MonoPlace3D, a novel system that considers the 3D scene content to create realistic augmentations. Specifically, given a background scene, MonoPlace3D learns a distribution over plausible 3D bounding boxes. Subsequently, we render realistic objects and place them according to the locations sampled from the learned distribution. Our comprehensive evaluation on two standard datasets KITTI and NuScenes, demonstrates that MonoPlace3D significantly improves the accuracy of multiple existing monocular 3D detectors while being highly data efficient."
        },
        {
            "title": "Start",
            "content": "MonoPlace3D: Learning 3D-Aware Object Placement for 3D Monocular Detection Rishubh Parihar* Srinjay Sarkar Sarthak Vora Jogendra Nath Kundu R. Venkatesh Babu"
        },
        {
            "title": "IISc Bangalore",
            "content": "5 2 0 2 0 1 ] . [ 2 1 0 8 6 0 . 4 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Current monocular 3D detectors are held back by the limited diversity and scale of real-world datasets. While data augmentation certainly helps, its particularly difficult to generate realistic scene-aware augmented data for outdoor settings. Most current approaches to synthetic data generation focus on realistic object appearance through improved rendering techniques. However, we show that where and how objects are positioned is just as crucial for training effective 3D monocular detectors. The key obstacle lies in automatically determining realistic object placement parameters - including position, dimensions, and directional alignment when introducing synthetic objects into actual scenes. To address this, we introduce MonoPlace3D, novel system that considers the 3D scene content to create realistic augmentations. Specifically, given background scene, MonoPlace3D learns distribution over plausible 3D bounding boxes. Subsequently, we render realistic objects and place them according to the locations sampled from the learned distribution. Our comprehensive evaluation on two standard datasets KITTI and NuScenes, demonstrates that MonoPlace3D significantly improves the accuracy of multiple existing monocular 3D detectors while being highly data efficient. project page 1. Introduction Monocular 3D object detection has rapidly progressed recently, enabling its use in autonomous navigation and robotics [18, 32]. However, the performance of 3D detectors relies heavily on the quantity and quality of the training dataset. Given the considerable effort and time required to curate extensive, real-world 3D-annotated datasets, specialized data augmentation for 3D object detection has emerged as promising direction. However designing realistic augmentations for 3D tasks, is non-trivial, as the generated augmentations must adhere to the physical constraints of the real world, such as maintaining 3D geometric consistency and handling collisions. *equal contribution Figure 1. a) We compare augmentations from our learned placement with heuristic-based placements from Lift3D [22]. In our augmentations, vehicles follow the lane orientations and are placed appropriately. b) These realistic augmentations significantly improve the 3D detection performance (KITTI [6] val set, (easy)). Notably, we achieve detection performance comparable to that of the fully labeled dataset using only 50% of the dataset. Existing techniques [14, 25] for 3D augmentation use relatively simple heuristics for placing synthetic objects in an input scene. For instance, in the context of road scenes, recent approach [22] generates realistic cars and places them on the segmented road region. However, such heuristics result in highly unnatural scene augmentations  (Fig. 1)  , resulting in marginal improvement in 3D detection performance. In this work, we ask the following two crucial questions: (1) What key factors are essential for generating realistic augmentations to improve monocular 3D object detection?, and (2) How can these factors be integrated to generate effective scene-aware augmentations? For the first question, we discover two critical factors responsible for generating effective 3D augmentations: 1. Object Placement: Plausible placement of augmented objects, with appropriate object placement (location, scale, and orientation), is essential for rendering realistic scene augmentations. For instance, in road scenes, car should be placed on the road, be of appropriate size based on the distance from the camera, and follow the lane orientation. Augmentations that respect such physical constraints generalize better to real scenes by faithfully modelling the true distribution of the vehicles in the real world. To give an example of how such an augmentation looks, we compare our proposed augmentation approach against heuristicbased placement from Lift3D [22] in Fig. 1. Given the same rendering, our generation looks much more plausible regarding car placement and orientation compared to the baseline approach. Notably, when used for object detection training, our approach leads to significantly greater performance improvement, making the detector not only performant, but also highly data efficient (refer Fig. 1c) 2. Object Appearance: For 3D augmentation, it is desired that the generated objects exhibit realism and seamlessly integrate with the background to preserve visual consistency. This, in turn, minimizes the domain disparity between real and augmented data. Existing augmentation methods for 3D detection [14, 22, 25] primarily focus on the object appearance. This limits their ability to exploit the full potential of the data augmentations for 3D detection. To address both these factors, we propose MonoPlace3D, novel scene-aware augmentation method that generates effective 3D augmentations, as shown in Fig. 1. For plausible object placement, we train 3D Scene-Aware Placement Network (SA-PlaceNet), which maps given scene image to distribution of plausible 3D bounding boxes. It learns realistic object placements that adhere to the physical rules of road scenes, facilitating sampling of diverse and plausible 3D bounding boxes (see Fig. 1a). For training this network, we consider existing 3D detection datasets, which typically contain only limited number of objects per scene, resulting in sparse training signal. Therefore, to enable dense placement prediction, we introduce novel modules based on (1) geometric augmentations of 3D boxes, along with (2) modeling of continuous distribution of 3D boxes. For realistic object appearance, we propose rendering pipeline that leverages synthetic 3D assets and an imageto-image translation model. We translate the synthetic renderings into realistic version using ControlNet [53](see Fig. 1b) and blend them with the background to get final augmentations. This allows us to utilize amateur-quality 3D assets and transform them into diverse, highly realistic car renderings that resemble real-world scenes. Our two-stage augmentation approach is highly effective and modular, allowing seamless integration with advancements in placement and rendering for enhancing 3D object detection datasets. Using our augmentation method on popular 3D detection datasets led to significant improvements over the prior baselines and set new state-of-the-art monocular detection benchmark. Notably, as shown in Figure 1, using only 40% of the real training data and our 3D augmentations outperforms model that is trained on the complete data without any 3D augmentations. Through extensive ablation studies, we thoroughly analyze the role of different components and their effect on detection performance. We summarize our contributions below: 1. We identify the critical role of 3D-aware object placement and realistic appearance for generating effective scene augmentations for 3D object detection. 2. We propose MonoPlace3D, novel approach to generate plausible 3D augmentations for road scenes by realistically placing objects following scene grammar. 3. We demonstrate the effectiveness of the proposed augmentations on multiple 3D detection datasets and detector architectures with significant gains in performance as well as data efficiency. 2. Related Work Object Placement. There are numerous works [1, 35, 49, 54, 60] which aim to predict object placement by learning transformation or the bounding box parameters directly for given background image. set of works [35, 49] learns the distribution of indoor synthetic objects. Another set of works [20, 21, 34, 44, 54] learns the plausible locations for humans and other outdoor objects in 2D manner. Few works aim to learn the arrangement conditioned on the scene-graph [19, 31, 52]. Similarly, another set of works [21, 44, 54] train deep network adversarially in order to learn plausible 2D bounding box locations. Similarly, ST-GAN [26] learns to predict the geometric transformation of bounding box in the given scene using adversarial [24] uses variational autoencoder to predict training. plausible location heatmap over the scene but is limited to placement in restricted indoor environments. Monocular Object Detection. The current monocular 3D detection methods can be grouped as image-based or pseudo-lidar-based. Image-based detectors [2, 27, 28, 33, 37, 41, 43, 47, 56] estimate the 3D bounding box information for an object from single RGB image. Due to the lack of depth information, these methods rely on geometric consistency in order to predict the class and the location of the object. Some works [23, 28, 32] use the prediction of key points of 3D bounding boxes as an intermediate task in order to improve its performance on 3D monocular detection. In this work, we aim to improve the performance of imagebased monocular detection models since RGB images are the most commonly used modality and easy to acquire with low acquisition costs, unlike LIDAR and depth sensors. Scene Data Augmentation. Multiple works use 2D data augmentation techniques to improve the performance of perception tasks [40]. However, these augmentations cannot be lifted directly to 3D without violating the geometric constraints. To alleviate this problem, recent method augments the training dataset for the task of 3D monocular detection [9, 12, 22, 25, 45]. One approach is to copypaste cars from an archived dataset by considering the effect of 3D scene geometry, such as the scale and pose of the car [25]. Another approach is to model synthetic urban scene from real-world datasets [9]. On the contrary, Lift3D [22] learns an object-centric neural radiance field to generate realistic 3D cars with GAN-augmented views and [45] learns radiance field for the full 3D scene. Another set of approaches fully generates realistic multiview scenes with diffusion models for generating realistic scenes [11, 13, 48, 50]. All these methods use heuristics such as lane segments to place cars; however, we aim to learn the distribution over car locations, scale, and orientation from the real-world object detection dataset. 3. Method In this section, we first explain why its important to have specialized methods for creating realistic scene-based augmentations for 3D detection. Then, we delve into the details of our unique approach to 3D augmentation. Insight-1: Unlike the object-based augmentations suitable for broad image classification tasks, enhancing structured tasks as 3D object detection requires careful consideration of object-background and object-object interactions for generation of plausible scene-based augmentations. Remarks: Synthetic object-based augmentation for image classification typically involves placing objects on any suitable background. This method may not always respect the interaction between the object and the background, its impact on the classification task remains minimal. In contrast, for scene-based augmentation, which is crucial in tasks like 3D detection, the interactions between objects and backgrounds, as well as between objects, becomes pivotal. For example, implausible placements such as car in sky background, two cars occluding each others 3D volume, or car-oriented perpendicular to lanes on the road, need to be avoided. While one might argue that random placement could aid in 3D object detection task by helping the model distinguish objects from the background, empirical evidence suggests otherwise. Hence, its crucial to devise placement-based augmentation method that respects the scene-prior, thereby instilling this understanding into the detector model during training. Insight-2: The distribution of augmented samples for given real sample xr, denoted as q(xaugxr), can be enhanced by better scene-prior modeling; this leads to augmented scenes that closely align with the real distribution, fostering robust model that is resilient to failures and can achieve superior performance with fewer real samples. the distribution q(xaugxr) = Remarks: equation The q(xaugz, xr)q(zxr) of represents augmented samples for given real sample xr. Here, q(xz, xr) represents pipeline that generates the augmented scene image upon applying an effective placementbased augmentation. Here, q(zxr) denotes the scene-prior related latent factor given the real image. This factor can model the distribution of plausible location, orientation, and scale to place objects given the scene layout. Improved modeling of the scene prior ensures that the augmented scene closely matches the real distribution. Training with such augmentations imbues the model with strong understanding of the scene prior, enhancing its robustness and reliability. We demonstrate that this strategy enables efficient training, yielding superior performance with fewer real samples compared to the baseline. Approach overview. Our method for 3D augmentation consists of two stages. First, we train the placement model that maps monocular RGB image to distribution over plausible 3D bounding boxes (Sec. 3.1). Subsequently, we sample set of 3D bounding boxes from this distribution to place cars. In the second stage, we render realistic cars following the sampled 3D bounding box and blend them with the background road scene. (Sec. 3.2). 3.1. Scene-aware Plausible 3D Placement Realistic 3D placement in road scenes is extremely challenging due to the high diversity in the scene layouts and underlying grammatical rules of the road scenes (Sec.1). Existing methods use simple heuristic placement [22] based on the road segmentation unable to model these complexities and hence result in unnatural augmentations  (Fig. 1)  . We propose data-driven approach to learn the real-world placement distribution by training Scene-Aware Placement Network (SA-PlaceNet), that maps given image to the distribution of plausible 3D bounding boxes. Learning such distribution requires dense supervision about object location, scale, and orientation for each 3D point in space. Having such dense annotated real dataset is impractical and can only be generated in controlled synthetic setting that does not generalize to the real world. Hence, we take an alternate approach to learn the 3D bounding box distribution from an existing 3D object detection dataset. Object detection datasets only provide information on where cars are located but not where they could be. To mitigate this, we inpaint the vehicles from the scene to generate paired image dataset with/without the vehicles. However, detection datasets have only few vehicles in each scene, which provides only sparse signals for plausible 3D bounding boxes. Directly training with such dataset will lead to overfitting and the model learns the sparse point estimate of locations as each scene has only few car locations in the ground truth. To truly learn the underlying Figure 2. a) SA-PlaceNet Architecture: Given an input background image and corresponding depth to predict the means of multidimensional Gaussian distribution over 3D bounding boxes. 3D bounding boxes are sampled from each of these Gaussian to compute the training loss. b) Geometry-aware augmentation in BEV (Birds Eye View). For given source car location (bloc), we first find nearest neighbors with the same orientation and augment the location to bloc by interpolating with neighboring locations nloc (Alg.1) distribution of 3D bounding boxes, we propose two novel modules during training of placement network. Geometry aware augmentation and predicting distribution over 3D bounding box instead of single estimate. The proposed modules enable diverse placements for given scene that follow the underlying rules of the road scene. The complete architecture for placement is shown in Fig. 2a. We build SA-PlaceNet using the backbone of MonoDTR [18]. MonoDTR is designed to perform monocular 3D object detection and is trained with auxiliary depth supervision. However, depth is not required during inference. We adapt the architecture of MonoDTR to learn the mapping from background road images to set of 3D bounding boxes B. Following [18], we define bounding box as 8 dimensional vector = [bx, by, bz, bh, bw, bl, bθ, bα], where (bx, by, bz) are 3D locations, (bh, bw, bl) are height, width, and length of the box, and bθ and bα are orientation angles. Note that bα can be computed deterministically from bθ and hence we have only 7 variables defining given bounding box. As convention, we consider the xz plane as the road plane. Dataset preparation. There is no existing real-world dataset that provides plausible placement annotations for given road scene. Instead, we take advantage of the KITTI [15] dataset with 3D object detection annotations. We preprocess the dataset by inpainting the foreground cars in the scene using off-the-shelf inpainting [38]. Through this process, we obtain an image dataset (I) with no cars on the road and set of corresponding 3D bounding boxes (B). Next, we obtain depth images Id for the inpainted images using [36]. The obtained paired dataset, = {I, Id, B}, is used to train the SA-PlaceNet. 3.1.1. Geometry aware augmentation Training SA-PlaceNet directly with the paired dataset could easily learn mapping to sparse 3D locations where Algorithm 1 Geometry-aware augmentation procedure 1. Input: query box: = [bx, by, bz, bh, bw, bl, bθ, bα] where bloc = (bx, by, bz) number of neighbors: radius of interpolation: amount of jitter: dj orientation threshold: ϵθ 2. Sample neighbors {ni}K 1 B, s.t. loc bloc2 < & ni ni θ bθ < ϵθ 3. If there are no neighbours i.e = 0, then do (1) (2) bx bx + dx bz bz + dz where dz > 2dx and dx, dz (0, dj) end If 4. Else do Generate the augmented location bloc = (bx, by, bz) using Eq. 7 end Else 5. Output : Augmented bounding box parameters : [bx, by, bz, bh, bw, bl, bθ, bα] real cars were present before inpainting. Additionally, the model can cheat by using the inpainting artifacts to predict cars at the source location. To overcome these limitations, we propose geometry-aware augmentation in the 3D bounding box space. We build on the intuition that the regions neighboring ground truth car locations are also plausible for placement. The augmentation transforms the ground truth bounding box of car, located at bloc = (bx, by, bz) into plausible neighboring box = G(b) located at bloc = (bx, by, bz) shown in Fig. 2b. The detailed algorithm for geometry-aware augmentation is given in detail in Alg.1. Specifically, we first find set of neighboring car boxes {ni}i=K i=1 to the given car b. We consider ni as the neighbor of if ni loc bloc2 < and ni θ bθ < ϵθ, for given threshold and ϵθ. We assume the selected nearest cars will be in the same lane and Figure 3. Rendering pipeline: Given 3D asset, we first render an image and shadow from fixed light source according to the 3D box parameters. Next, we used edge-conditioned ControlNet [53] to generate realistic car version that follows the same orientation and scale as the rendered image. Finally, we use the obtained shadow, rendered car, and 3D location to place the car and render augmented images. follow similar orientations. To augment the location bloc, we take convex combination of neighboring locations ni and bloc and obtain location bloc. (cid:88) loc bloc = λ0 bloc + λi ni loc (3) i=1 where (cid:80) λi = 1, λi 0 are hyperparameters randomly sampled for each ground truth box b. This transformation enables us to span large region of plausible locations during training, hence enabling diverse placement locations during inference for each scene. If car doesnt have any neighboring cars, we apply uniform jitter along the length and smaller jitter along the width of the car bounding box. 3.1.2. Distribution over 3D bounding boxes Geometry-aware augmentation enables the generation of diverse placement locations, but it learns direct mapping from the input image to point estimate of bounding boxes. To learn continuous representation in the output space, we map the input image to the distribution of 3D boxes. This improves the coverage of plausible locations and enables diverse bounding box sampling from predicted set of mean boxes. Specifically, we approximate each predicted bounding box as multi-dimensional Gaussian distribution with mean µb and fixed covariance matrix as αI, where α is used to control the spread as shown in Fig. 2a. We empirically observed that having fixed covariance improves training stability. Having higher α value results in strong augmentations, where the sampled car is far away from the mean location, resulting in weaker training signal. During the forward pass, the SA-PlaceNet predicts mean bounding box parameters µb. To sample box ˆb, we first sample ϵ (0, I) and use the reparametrization trick as follows: ˆb = µb + ϵ αI (4) 3.1.3. SA-PlaceNet Training We train SA-PlaceNet with the acquired paired dataset = {I, Id, B}, consisting of inpainted background image (I), inpainted depth image (Id) and the ground truth 3D bounding boxes (B). Following [18], we train the model with Lcls for objectness and class scores, Ldep for depth supervision, and Lreg for bounding box regression.The proposed modules for geometry-aware augmentation and learning distribution over 3D bounding boxes can be easily integrated into modified version of the regression loss Lm reg as discussed below. The total loss is then defined as: = Lcls + Lm reg + Ldep (5) For given ground-truth bounding box parameter b, we first augment it using geometry-aware augmentation following Eq. (7) to obtain modified bounding box parameters = G(b). To capture the distribution of 3D boxes, we predict mean bounding box parameter µb instead of point estimate of the box parameters and randomly sample new bounding box ˆb using the reparameterization trick outlined in Eq. (4). Subsequently, we compute the modified regression loss between the model prediction µb and the ground truth box as follows: reg(µb, b) = Lreg(ˆb, b) Lm (6) 3.2. What to place? Rendering cars We generate realistic scenes by selecting cars and rendering them within the projected 3D coordinates of the predicted location, as shown in Fig. 3. To accurately render car based on 3D bounding box parameters, we utilize 3D car assets from ShapeNet [5] that can be adjusted through orientation and scale transformations. Upon acquiring the 3D bounding box predictions, our rendering step entails sampling cars from the ShapeNet. Subsequently, the car model undergoes rotation according to the 3D observation angle of the object before positioning it within the designated scene. We separately render car shadows with predefined lighting in the rendering environment, following [7]. The rendered ShapeNet car images, although following the 3D bounding boxes, look unrealistic when pasted into the scene (Fig. 6, Figure 4. Given an input source image, we plot the heatmaps of the mean objectness score at each pixel location. The generated heatmaps span large region on the road with plausible locations of objects. Next, we show samples of bounding boxes and realistic renderings of cars in the scene. row-2). To resolve this, we leverage the advances in conditional generation using text-to-image models. For the generated synthetic car images, we apply an edge detector to obtain an edge map. The edge map preserves the cars structure and still follows the same orientation and scale as the original car. Next, we use edgeconditioned text-to-image diffusion model ControlNet [53] to render realistic car using the prompt realistic car on the street. We further finetune the backbone diffusion model in ControlNet using LoRA [17] on subset of car images from the KITTI dataset. This enables us to generate natural-looking versions of cars that blend well with the background scene  (Fig. 6)  . As ControlNet enables diverse generations from the same edge image, we can generate multiple renderings of cars from the edge map of single ShapeNet car. This enables the generation of many diverse cars from small, fixed set of 3D assets. The generated renderings look realistic and substantially boost object detection performance, as shown in Tab. We believe, the proposed approach of using few 3D assets with conditional text-to-image models is promising and can be applied to generate diverse 3D augmentations for other tasks as well. Apart from the proposed rendering technique, we also experiment directly placing ShapeNet [5] and renderings from Lift3D [22], which is generative radiance field approach that generates realistic 3D car assets. 4. Experiments In this section, we present results for 3D-aware placement (Sec. 4.1) and car renderings (Sec. 4.2). Next, we present results for 3D detection trained with our generated augmentations (Sec. 4.3). We show additional results for monocular 3D detection on indoor SUNRGBD [58] dataset, 2D detection on KITTI, additional ablations, and quantitative analysis of SA-PlaceNet in the suppl. Figure 5. a) Ablation for object placement - For background road scene, we visualize the heatmaps of aggregated objectness scores at each pixel location. Geometric augmentation and variational inference help to generate diverse and plausible object placements. b) Histogram of the distribution of orientations of the ground truth bounding boxes and the generated bounding boxes. Dataset. We use the KITTI [15] and NuScenes [3] datasets for our experiments. KITTI consists of total of 7481 realworld images captured from camera mounted on car. Following [6, 22, 46], we split the data into 3712 train and 3679 validation samples. For NuScenes, we use the official split with 700 train scenes containing 28134 samples and 150 validation scenes containing 6019 samples. 4.1. Evaluation of Placement Model The placement network is trained with RGB images from the train split. We prepare the training data by inpainting the moving objects using [38] and obtain paired dataset = {I, Id, B} as detailed in Sec. 3.1. To visualize the performance of the placement, we generate heatmaps over the center of the bottom face of the bounding box in Fig. 4. For visualization, we use the mean objectness score of the anchor boxes corresponding to each grid cell. Geometryaware augmentation enables learning of large region for placing cars even though trained with input scenes with only few cars. This allows for the sampling of diverse physically plausible placement locations for given input scene shown as set of 3D bounding boxes. We sample two sets of boxes from the predicted distribution. The sampled boxes have appropriate locations, scales, and orientations based on the background road. We present detailed quantitative analysis of our method in the suppl. document. Figure 6. Ablation over rendering methods: Given the source image and predicted 3D bounding boxes, we sample and render synthetic ShapeNet [5] car; Lift3D [22] rendered method; and our realistic rendering. We show smaller domain gap between the rendered cars and the original samples. Analysis. We analyze the impact of each component on placement performance in Fig. 5a). The naive baseline of directly training object placement without geometric augmentation and variational modeling only learns point estimate and results in few concentrated spots for placement location. Adding the variational head for learning distribution of boxes instead expands the space of plausible locations but is still segregated in small regions. For the variational head, we have fixed the alpha as 0.1. This highlights the sparse training signals for placement using ground truth boxes. However, when coupled with the geometryaware augmentation, the predicted distribution covers large driveable area on the road. To further analyze the orientations, we plot histogram of predicted and the ground truth orientations in Fig. 5b), where the predictions closely follow the ground truth. 4.2. Evaluation of object renderings We augment the road scenes by placing synthetic cars rendered by several approaches in Fig. 6. We compare the rendering quality of the proposed method with 1) ShapeNet - 3D car assets renderings sampling from ShapeNet [5], 2) Lift3D [22] - generalized NeRF method for generating 3D car models. ShapeNet renderings result in unnatural augmentations due to synthetic car appearance and domain gaps from real scenes. On the other hand, Lift3D renderings, although realistic, lack diversity and suffer from artifacts. Our rendering method leverages conditional textto-image diffusion models and generates extremely realistic cars that blend well with the background and are of high fidelity. Additionally, as our rendering starts from an underlying 3D asset, we use it to render shadows in synthetic environment and copy the same shadow to the generated Figure 7. Qualitative comparison of the generated augmentations with all the baseline methods. Our augmentations are highly realistic, place cars following plausible placement properties, and have minimal domain gap from the training dist. realistic renderings. The proposed rendering pipeline effectively generates realistic augmentations and results in superior object detection performance (Tab. 1). Further, we report FID of the generated augmentations with the real training set to evaluate the realism. 4.3. Improving 3D Object Detection We evaluate the effectiveness of our augmentations for monocular 3D object detection. We augment the training set with the same number of images to prepare an augmented version of the dataset. We compare our proposed augmentation method with the following augmentation approaches: Geometric Copy-paste (Geo-CP) [25]. We use instancelevel augmentation from [25], where cars from the training images are archived along with the corresponding 3D bounding boxes to create dataset. For augmenting scene, car, and its 3D box parameters are sampled from the dataset, and the car is pasted in the background. Lift-3D [22] proposed generative radiance field network to synthetize realistic 3D cars. The generated cars are then placed on the road using heuristic-based placement. Specifically, placement location is sampled on the segmented road, and other 3D bounding box parameters are sampled from predefined parameter distribution. CARLA [10]. To compare the augmentations generated by simulated road scene environments, we use state-of-theart CARLA simulator engine for rendering realistic scenes with multiple cars. It can generate diverse traffic scenarios that are implemented programmatically. However, its extremely challenging for simulators to capture the true diversity from real-world road scenes and they often suffer a) MonoDLE[32] Table 1. Monocular 3D detection performance on KITTI dataset 3D@IOU=0.7 Easy Mod. Hard 11.69 13.66 17.45 12.57 14.60 17.52 12.17 14.30 17.98 12.48 14.65 17.19 11.29 14.32 20.50 12.89 15.44 22. 3D@IOU=0.5 Easy Mod. Hard 37.81 43.42 55.41 38.66 44.23 58.95 38.81 44.41 58.33 39.13 44.21 56.81 38.55 43.69 60.30 40.35 45.59 63.59 w/o 3D Augmentation Geo-CP CARLA Lift3D RBP Ours b) GUPNet[30] w/o 3D Augmentation Geo-CP CARLA Lift3D RBP Ours 3D@IOU=0.7 Easy Mod. Hard 13.27 16.46 22.76 13.24 15.65 21.81 13.61 16.17 22.50 12.64 14.84 19.05 11.23 14.56 21.67 14.71 17.28 23.94 3D@IOU=0.5 Easy Mod. Hard 37.59 42.33 57.62 39.16 44.03 59.12 38.22 43.52 59.89 39.22 43.81 57.50 36.95 43.25 60.40 41.48 47.18 61. from large sim2real gap. Rule Based Placement (RBP). We create strong rulebased baseline to show the effectiveness of our learningbased placement. Specifically, we first segment out the road region with [16] and sample placement locations in this region. To get plausible orientation, we copy the orientation of the closest car in the scene, assuming neighboring cars follow the same orientations. We used our our rendering pipeline to generate realistic augmentations. Qualitative comparison of generated augmentations are shown in Fig. 7. Lift3D augmentations have cars placed in incorrect orientation as the orientation is sampled from general predefined distribution. RBP and Geo-CP augmentations are relatively better in terms of orientation but fail to place cars in the correct lanes. The proposed augmentation method follows the underlying grammar of the road well and generates realistic scene augmentations. 4.3.1. Realistic augmentations improves 3D detection We evaluate our augmentation technique on two state-ofthe-art monocular 3D detection networks - MonoDLE [32] and GUPNet [30] in Tab. 1 on KITTI [15] dataset. We generate one augmentation per real image for all the baselines. All the augmentation techniques improve over the baseline for MonoDLE. However, gains from Lift3D, CARLA, and Geo-CP are marginal. RBP performs better than other baselines primarily due to our realistic renderings. For GUPNet, none of the baselines can improve the detection performance overall. Our method significantly improves the score detection scores for both networks. This indicates strong generalization of our augmentations on various 3D object detection models. We also show results on the current stateof-the-art MonoDETR [55] in the suppl. document. Table 2. Rendering ablation with fixed placement 3D@IOU=0.5 3D@IOU=0.7 Rendering Easy Mod. Hard Easy Mod. Hard 37.81 43.42 55.41 11.69 13.66 17.45 37.64 43.48 59.54 12.28 14.17 20.91 37.53 42.65 60.38 11.65 14.25 21.35 38.28 43.27 61.23 11.73 14.21 21.45 40.35 45.59 63.59 12.89 15.44 22.49 w/o 3D Augmentation ShapeNet Lift3D Ours (w/o shadow) Ours 4.3.2. Impact of object rendering on 3D detection Table 2 presents an ablation study of various rendering approaches for augmentation in 3D detection. All renderings, when used with our learned placement, significantly outperform the baselines, demonstrating their compatibility with any rendering method. ShapeNet shows the lowest performance due to limited synthetic car diversity and substantial sim2real gap. Lift3D rendering performs better than ShapeNet but exhibits noticeable artifacts when cars are close to the camera  (Fig. 6)  . Our rendering approach, which uses generative text-to-image model, outperforms all baselines but also enhances and achieves state-of-the-art performance when combined with shadows. 4.3.3. Augmenting other object categories Though the car is the major category in the road 3D detection benchmarks, we also perform augmentation for two additional categories of cyclists and pedestrians, given they occur at 3.79% and 11.39% in the KITTI training set. For simplicity, we integrate our placement method with copypaste rendering as described in the suppl. document (similar to Geo-CP [25]). Note that we trained another placement model to predict the placement of all the classes together. We use the augmented dataset with renderings of cyclists and pedestrians to train MonoDLE [32] object detector. The results are shown in Tab. 3; our augmentation significantly improves the detection performance of both categories over the baselines. We show qualitative results for these classes with copy-paste in the suppl. document. Table 3. Augmenting multiple categories for 3D detection 3D@IOU=0.50 3D@IOU=0.25 Cyclist w/o 3D Augmentation Ours Easy Mod Hard Easy Mod 10.82 1.85 4.92 11.23 3.37 6.75 18.41 21.59 2.03 3.41 Hard 9.52 9.90 Pedestrian 3D@IOU=0. 3D@IOU=0.25 w/o 3D Augmentation Ours Easy Mod Hard Easy Mod 18.38 2.99 4.60 20.81 3.34 4.98 22.98 26.28 3.81 3.89 Hard 15.12 16. 4.4. Experiments on large datasets of Table 4. Detection on NuScenes FCOS3D [3] w/o 3D Augmentation Lift3D Ours We validate the genour eralization training method by on SA-PlaceNet large driving dataset - NuScenes [3]. Our approach produces plausible realistic augmentations for the given scene (suppl.) and we show improved performance on the NuScenes dataset with the FCOS3D [3] monocular detection network in Tab. 4. MAP 0.3430 0.3211 0. NDS 0.415 0.371 0.440 4.5. Computational Cost of MonoPlace3D Training SA-PlaceNet for generating augmentation takes fraction of the time of the overall detection training. Specifically, on the KITTI dataset, SA-PlaceNet takes 12 hours vs 20 hours for 3D detector (GUPNet) on single A5000 GPU. Similarly, on larger nuScenes dataset, SA-PlaceNet takes 32 hours vs 5 days for 3D detector (FCOS3D). We have provided additional details for computational requirements across configurations in suppl. document. 5. Conclusion This work proposes novel scene-aware augmentation technique to improve outdoor monocular 3D detectors. The core of our method is an object placement network, that learns the distribution of physically plausible object placement for background road scenes from single image. We utilize this information to generate realistic augmentations by placing cars on the road scenes with geometric consistency. Our results with scene-aware augmentation on monocular 3D object detectors suggest that realistic placement is the key to substantially improving the augmentation quality and data efficiency of the detector. The primary limitation of our approach is the dependency on the off-the-shelf inpainting method for data preparation for the training of the placement network. Also, our current framework does not consider more nuanced appearance factors in augmentations such as the lighting of the scene. In conclusion, we provide important insights for designing effective scene-based augmentations to improve monocular 3D object detection. Acknowledgements. We thank Tejan Karmali for their helpful comments and discussions, and Abhijnya Bhat for reviewing the draft. Rishubh Parihar is supported by PMRF from the Government of India."
        },
        {
            "title": "References",
            "content": "[1] Diego Martin Arroyo, Janis Postels, and Federico Tombari. Variational transformer networks for layout generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1364213652, 2021. 2 [2] Garrick Brazil and Xiaoming Liu. M3D-RPN: monocular 3d region proposal network for object detection. CoRR, abs/1907.06038, 2019. 2 [3] Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu nuscenes: Pan, Giancarlo Baldan, and Oscar Beijbom. CoRR, multimodal dataset for autonomous driving. abs/1903.11027, 2019. 6, 8, 12, 14 [4] John Canny. computational approach to edge detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, PAMI-8(6):679698, 1986. 17 [5] Angel Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015. 5, 6, 7, 16, 17 [6] Xiaozhi Chen, Kaustav Kundu, Yukun Zhu, Andrew Berneshawi, Huimin Ma, Sanja Fidler, and Raquel Urtasun. 3d object proposals for accurate object class detection. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, page 424432, Cambridge, MA, USA, 2015. MIT Press. 1, 6, [7] Yun Chen, Frieda Rong, Shivam Duggal, Shenlong Wang, Xinchen Yan, Sivabalan Manivasagam, Shangjie Xue, Ersin Yumer, and Raquel Urtasun. Geosim: Realistic video simulation via geometry-aware composition for self-driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 72307240, 2021. 5 [8] Blender Online Community. Blender - 3D modelling and rendering package. Blender Foundation, Stichting Blender Foundation, Amsterdam, 2018. 12, 16, 17 [9] Shubham Dokania, Anbumani Subramanian, Manmohan Chandraker, and CV Jawahar. Trove: Transforming road scene datasets into photorealistic virtual environments. In European Conference on Computer Vision, pages 592608. Springer, 2022. 3 [10] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. Carla: An open urban driving simulator. In Conference on robot learning, pages 116. PMLR, 2017. 7, 15 [11] Ruiyuan Gao, Kai Chen, Enze Xie, HONG Lanqing, Zhenguo Li, Dit-Yan Yeung, and Qiang Xu. Magicdrive: Street In The view generation with diverse 3d geometry control. Twelfth International Conference on Learning Representations. 3 [12] Ruiyuan Gao, Kai Chen, Enze Xie, Lanqing Hong, Zhenguo Li, Dit-Yan Yeung, and Qiang Xu. Magicdrive: Street view generation with diverse 3d geometry control. arXiv preprint arXiv:2310.02601, 2023. [13] Ruiyuan Gao, Kai Chen, Zhihao Li, Lanqing Hong, Zhenguo Li, and Qiang Xu. Magicdrive3d: Controllable 3d generation for any-view rendering in street scenes. arXiv preprint arXiv:2405.14475, 2024. 3 [14] Yunhao Ge, Hong-Xing Yu, Cheng Zhao, Yuliang Guo, Xinyu Huang, Liu Ren, Laurent Itti, and Jiajun Wu. 3d copypaste: Physically plausible object insertion for monocular 3d detection. Advances in Neural Information Processing Systems, 36, 2024. 1, 2 [15] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. The International Journal of Robotics Research, 32(11):12311237, 2013. 4, 6, 8, 15 [16] Cheng Han, Qichao Zhao, Shuyi Zhang, Yinzi Chen, Zhenlin Zhang, and Jinwei Yuan. Yolopv2: Better, faster, arXiv preprint stronger for panoptic driving perception. arXiv:2208.11434, 2022. 8, 12, 15 [17] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. 6, 17 [18] Kuan-Chih Huang, Tsung-Han Wu, Hung-Ting Su, and Winston Hsu. Monodtr: Monocular 3d object detection with depth-aware transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 40124021, 2022. 1, 4, [19] Akash Abdu Jyothi, Thibaut Durand, Jiawei He, Leonid Sigal, and Greg Mori. Layoutvae: Stochastic scene layout genIn Proceedings of the IEEE/CVF eration from label set. International Conference on Computer Vision, pages 9895 9904, 2019. 2 [20] Sumith Kulal, Tim Brooks, Alex Aiken, Jiajun Wu, Jimei Yang, Jingwan Lu, Alexei Efros, and Krishna Kumar Singh. Putting people in their place: Affordance-aware human insertion into scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1708917099, 2023. 2 [21] Donghoon Lee, Sifei Liu, Jinwei Gu, Ming-Yu Liu, MingHsuan Yang, and Jan Kautz. Context-aware synthesis and placement of object instances. Advances in neural information processing systems, 31, 2018. 2 [22] Leheng Li, Qing Lian, Luozhou Wang, Ningning Ma, and Ying-Cong Chen. Lift3d: Synthesize 3d training data by lifting 2d gan to 3d generative radiance field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 332341, 2023. 1, 2, 3, 6, 7, 15, 16 [23] Peixuan Li, Huaici Zhao, Pengfei Liu, and Feidao Cao. RTM3D: real-time monocular 3d detection from object keypoints for autonomous driving. CoRR, abs/2001.03343, 2020. 2 [24] Xueting Li, Sifei Liu, Kihwan Kim, Xiaolong Wang, MingHsuan Yang, and Jan Kautz. Putting humans in scene: Learning affordance in 3d indoor environments. CoRR, abs/1903.05690, 2019. 2 [25] Qing Lian, Botao Ye, Ruijia Xu, Weilong Yao, and Tong Zhang. Exploring geometric consistency for monocular 3d object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16851694, 2022. 1, 2, 3, 7, [26] Chen-Hsuan Lin, Ersin Yumer, Oliver Wang, Eli Shechtman, and Simon Lucey. ST-GAN: spatial transformer generative adversarial networks for image compositing. CoRR, abs/1803.01837, 2018. 2 [27] Yuxuan Liu, Yixuan Yuan, and Ming Liu. Ground-aware monocular 3d object detection for autonomous driving. CoRR, abs/2102.00690, 2021. 2 [28] Zechen Liu, Zizhang Wu, and Roland Toth. SMOKE: singlestage monocular 3d object detection via keypoint estimation. CoRR, abs/2002.10111, 2020. 2 [29] Hannan Lu, Xiaohe Wu, Shudong Wang, Xiameng Qin, Junyu Han, Wangmeng Zuo, and Ji Seeing beyond views: Multi-view driving scene arXiv preprint Xinyu Zhang, Tao. video generation with holistic attention. arXiv:2412.03520, 2024. 13 [30] Yan Lu, Xinzhu Ma, Lei Yang, Tianzhu Zhang, Yating Liu, Qi Chu, Junjie Yan, and Wanli Ouyang. Geometry uncertainty projection network for monocular 3d object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 31113121, 2021. 8, 14 [31] Andrew Luo, Zhoutong Zhang, Jiajun Wu, and Joshua In Tenenbaum. End-to-end optimization of scene layout. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 37543763, 2020. [32] Xinzhu Ma, Yinmin Zhang, Dan Xu, Dongzhan Zhou, Shuai Yi, Haojie Li, and Wanli Ouyang. Delving into localization errors for monocular 3d object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 47214730, 2021. 1, 2, 8, 14, 15 [33] Arsalan Mousavian, Dragomir Anguelov, John Flynn, and Jana Kosecka. 3d bounding box estimation using deep learning and geometry. CoRR, abs/1612.00496, 2016. 2 [34] Rishubh Parihar, Harsh Gupta, Sachidanand VS, and Text2place: Affordance-aware text Venkatesh Babu. guided human placement. In European Conference on Computer Vision, pages 5777. Springer, 2024. 2 [35] Despoina Paschalidou, Amlan Kar, Maria Shugrina, Karsten Kreis, Andreas Geiger, and Sanja Fidler. Atiss: Autoregressive transformers for indoor scene synthesis. Advances in Neural Information Processing Systems, 34:1201312026, 2021. 2 [36] Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. ViIn Proceedings of sion transformers for dense prediction. the IEEE/CVF international conference on computer vision, pages 1217912188, 2021. 4 [37] Thomas Roddick, Alex Kendall, and Roberto Cipolla. Orthographic feature transform for monocular 3d object detection. CoRR, abs/1811.08188, 2018. 2 [38] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1068410695, 2022. 4, 6, 15 [39] Danila Rukhovich, Anna Vorontsova, and Anton Konushin. Imvoxelnet: Image to voxels projection for monocular and multi-view general-purpose 3d object detection. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 23972406, 2022. [40] Connor Shorten and Taghi M. Khoshgoftaar. survey on image data augmentation for deep learning. Journal of Big Data, 6:148, 2019. 2 [41] Andrea Simonelli, Samuel Rota Bul`o, Lorenzo Porzi, Manuel Lopez-Antequera, and Peter Kontschieder. DisCoRR, entangling monocular 3d object detection. abs/1905.12365, 2019. 2 [42] Andrea Simonelli, Samuel Rota Bul`o, Lorenzo Porzi, Manuel Lopez-Antequera, and Peter Kontschieder. DisCoRR, entangling monocular 3d object detection. abs/1905.12365, 2019. 13 [43] Andrea Simonelli, Samuel Rota Bul`o, Lorenzo Porzi, Elisa Single-stage monocuCoRR, Ricci, and Peter Kontschieder. lar 3d object detection with virtual cameras. abs/1912.08035, 2019. 2 [44] Jin Sun, Hadar Averbuch-Elor, Qianqian Wang, and Noah Snavely. Hidden footprints: Learning contextual walkability from 3d human trails. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XVIII 16, pages 192207. Springer, 2020. [45] Wenwen Tong, Jiangwei Xie, Tianyu Li, Hanming Deng, Xiangwei Geng, Ruoyi Zhou, Dingchen Yang, Bo Dai, Lewei Lu, and Hongyang Li. 3d data augmentation for driving scenes on camera, 2023. 3 recognition using places database. Advances in neural information processing systems, 27, 2014. 6, 13 [59] Xingyi Zhou, Dequan Wang, and Philipp Krahenbuhl. Objects as points. CoRR, abs/1904.07850, 2019. 13 [60] Sijie Zhu, Zhe Lin, Scott Cohen, Jason Kuen, Zhifei Zhang, and Chen Chen. Topnet: Transformer-based object placement network for image compositing, 2023. [46] Wenwen Tong, Jiangwei Xie, Tianyu Li, Hanming Deng, Xiangwei Geng, Ruoyi Zhou, Dingchen Yang, Bo Dai, Lewei Lu, and Hongyang Li. 3d data augmentation for driving scenes on camera. arXiv preprint arXiv:2303.10340, 2023. 6 [47] Tai Wang, Xinge Zhu, Jiangmiao Pang, and Dahua Lin. Probabilistic and geometric depth: Detecting objects in perspective. CoRR, abs/2107.14160, 2021. 2 [48] Xiaofeng Wang, Zheng Zhu, Guan Huang, Xinze Chen, Jiagang Zhu, and Jiwen Lu. Drivedreamer: Towards real-worldIn European drive world models for autonomous driving. Conference on Computer Vision, pages 5572. Springer, 2024. 3 [49] Qiuhong Anna Wei, Sijie Ding, Jeong Joon Park, Rahul Sajnani, Adrien Poulenard, Srinath Sridhar, and Leonidas Guibas. Lego-net: Learning regular rearrangements of objects in rooms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19037 19047, 2023. 2 [50] Yuqing Wen, Yucheng Zhao, Yingfei Liu, Fan Jia, Yanhui Wang, Chong Luo, Chi Zhang, Tiancai Wang, Xiaoyan Sun, and Xiangyu Zhang. Panacea: Panoramic and controllable video generation for autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 69026912, 2024. 3 [51] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2. https://github. com/facebookresearch/detectron2, 2019. 15 [52] Cheng-Fu Yang, Wan-Cyuan Fan, Fu-En Yang, and YuChiang Frank Wang. Layouttransformer: Scene layout generation with conceptual and spatial diversity. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 37323741, 2021. [53] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023. 2, 5, 6, 16, 17 [54] Lingzhi Zhang, Tarmily Wen, Jie Min, Jiancong Wang, David Han, and Jianbo Shi. Learning Object Placement by Inpainting for Compositional Data Augmentation, pages 566581. 2020. 2 [55] Renrui Zhang, Han Qiu, Tai Wang, Xuanzhuo Xu, Ziyu Guo, Yu Qiao, Peng Gao, and Hongsheng Li. Monodetr: Depthguided transformer for monocular 3d object detection. ICCV 2023, 2022. 8, 12, 13, 14 [56] Yunpeng Zhang, Jiwen Lu, and Jie Zhou. Objects are different: Flexible monocular 3d object detection. CoRR, abs/2104.02323, 2021. 2 [57] Hanqing Zhao, Dianmo Sheng, Jianmin Bao, Dongdong Chen, Dong Chen, Fang Wen, Lu Yuan, Ce Liu, Wenbo Zhou, Qi Chu, Weiming Zhang, and Nenghai Yu. X-paste: Revisiting scalable copy-paste for instance segmentation using clip and stablediffusion. In International Conference on Machine Learning, 2023. 14 [58] Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva. Learning deep features for scene MonoPlace3D: Learning 3D-Aware Object Placement for 3D Monocular Detection"
        },
        {
            "title": "Supplementary Material",
            "content": "Table 5. Ablation over SA-PlaceNet components"
        },
        {
            "title": "Contents",
            "content": "A. Additional placement results . . A.1. Quantitative evaluation . A.2. Placement on nuScenes [3] dataset . . A.3. Controlling traffic density in scenes . . A.4. Placing other categories . . . . . . . . . . . . . . . . . . . . . . . . . . . B. Additional object detection results B.1. Monocular 3D detection in indoor scenes . . B.2. Improving 2D object detection . . . . . B.3. 3D object detection on BEV based detector . B.4. 3D object detection on MonoDETR [55] . . . . . . B.5. Effect of Poisson Blending . . . . . . . . . C. Computational cost of MonoPlace3D . C.1. Data Efficiency on KITTI . C.2. Scalability of generated augmentations . C.3. Rendering Ablation on NuScenes . . . . . . . . . . . . . . . . D. Data Augmentation for Corner Cases E. Implementations details E.1. Placement data Preprocessing . . E.2. Baseline methods . . . . . . . . . . . . . . . . . . . . . F. Rendering details . . . . . . F.1. Copy-Paste . . . . . . . . F.2. ShapeNet F.3. Reaslistic rendering with Text-to-image . . . . . . . . . . . . F.4. Rendering shadows in Blender [8] . models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Method Overlap θKL Random w/o var & geo w/o geo w/o var Ours 0.36 0. 0.35 0.32 0.20 1.37 0.17 1.18 0.15 0.66 A.2. Placement on nuScenes [3] dataset We validate the generalization of our method by training SA-PlaceNet on subset of recent driving dataset - NuScenes [3] in Fig. 8. We visualize predicted 3D bounding boxes and realistic renderings from our method. Our approach produces plausible placements and authentic augmentations for the given scene. Figure 8. Placement on nuScenes [3] dataset. 12 12 12 12 12 13 13 13 13 13 14 14 14 14 14 14 15 15 15 15 16 16 17 A. Additional placement results A.1. Quantitative evaluation To quantify the performance of placement, we compute the following three metrics on the training set of KITTI: 1) Overlap: As road regions can cover most of the plausible locations for cars, we evaluate the predicted location by checking whether the center of the base of the 3D bounding box is on the road. Specifically, we compute the fraction of boxes that overlap with the road segmentation obtained using [16]. 2) θKL: We evaluate the KL-divergence between the distribution of orientation of the predicted 3D bounding box and the ground truth boxes. We present quantitative results in Tab. 5, where our method achieves superior overlap scores, suggesting the superiority of placement. A.3. Controlling traffic density in scenes Our augmentation method enables us to control the traffic density of vehicles in the input scenes by controlling the number of bounding boxes to be sampled. We present results for generating low-density (13 cars added) and highdensity (3 5 cars added) traffic scenes in Fig. 9. A.4. Placing other categories Our method enables us to learn placement for other categories from KITTI datasets. Specifically, we trained joint placement model to learn the distribution of 3D bounding boxes for cars, pedestrians, and cyclists. To render the pedestrians and cyclists, we leverage simple copy-paste rendering as discussed in Sec. F.1. We present placement results in additional categories in Fig. 10. The prodetection performance, as shown in Tab. 6. This indicates the superior generalization of our method for diverse environments. We believe detailed exploration of our work for indoor environments is promising future direction. B.2. Improving 2D object detection 2D Detection PerforTable 7. mance on Car category with CenterNet [59] As our approach provides consistent 3D augmentations, it also enables to improve the performance of 2D object detectors. Specifically, our placement model also predicts the 2D bounding box along with the 3D bounding box (followed in most of the 3D detection works). We use these predicted 2D bounding box annotations to obtain labeled 2D detection dataset. We evaluate the gains from our augmentations on 2D object detection on off-the-shelf 2D detector CenterNet [59] in Tab. 7. Following [42], we use standardized approach to report AP40 metric instead of the AP11 for evaluation. Notably, our proposed augmentation method, though designed for 3D detection, can also improve the performance of 2D object detection, proving the task generalization of the proposed approach. AP2D@IOU=0.5 Easy Mod. Hard 65.08 73.74 86.03 72.28 76.79 89.56 w/o 3D Aug. Ours config. B.3. 3D object detection on BEV based detector Table 8. Detection on BEV based 3D detector DeTR3D Our method generalizes to BEV-based detection, as our placement model predicts 3D bounding boxes in the world coordinate space. We train BEV-based DeTR3D on multinuScenes, view augmenting individual camera by placing our 2D car renderings in non-overlapping image regions. Since overlapping regions are mostly confined to the peripheries of adjacent camera views [29], our augmentations effectively improve detection performance (Tab. 8). For overlapping image regions, possible solution is to use 3D cars and render consistent multi-views for placement. NDS mAP Detr3D - w/o 3D augm. 0.434 0.349 0.451 0.381 Detr3D - ours views config. B.4. 3D object detection on MonoDETR [55] To validate the generalizability of our approach, we evaluate proposed 3D augmentation on recent 3D monocular detection model MonoDETR [55] on the KITTI dataset in Tab. 9. We report the baseline results without our augmentation from the original paper. Our method consistently outperforms the baseline in all three settings. The comprehensive evaluation across several detectors (also in the main paper) evidently shows the generalization of our proposed Figure 9. Augmented training dataset for 3D object detection: Given sparse scene with few cars, we place cars at the predicted 3D bounding box locations using our rendering algorithm. We present two sets of results, one with low density (1 3 cars added) and another with high density (4 5 cars added) for each scene. posed method predicts plausible locations, orientation, and shape of the object, enabling rich scene augmentations. Using these augmentations for training leads to significant improvement in performance for less frequent cyclist and pedestrian categories (Tab.3 main paper). Figure 10. Placement results for pedestrian and cycle categories on KITTI dataset. Note that we applied copy-paste in the predicted 3D object box locations to generate the augmentations. Though copy-pasting causes image artifacts, these augmentations still improve 3D detection performance, as shown in the main paper. B. Additional object detection results B.1. Monocular 3D detection in indoor scenes Our proposed method is generalizable for 3D detection in indoor environments. To demonstrate this, we performed preliminary experiment involving monocular 3D detection on SunRGBD [58] dataset. We adapt our placement network building on an indoor detection network - [39]. ImVoxelNet We copyused paste along with the predicted object locations to generate data augmentations. The generated augmentations are highly effective and improve upon the monocular 3D config. ImVoxelNet - w/o 3D augm. ImVoxelNet - ours mAP@0.25 0.410 0.430 Table 6. Indoor 3D detection 3D augmentation method. Table 9. 3D Detection Performance on Car with MonoDETR [55] MonoDETR w/o 3D Augmentation Geo-CP Lift3D RBP Ours 3D@IOU=0.7 Easy Mod. Hard 16.38 20.61 28.84 14.58 16.41 23.26 14.59 16.61 22.00 15.90 17.75 24.92 16.85 21.91 29.90 3D@IOU=0.5 Easy Mod. Hard 43.57 48.92 68.86 37.71 43.93 60.65 38.57 47.34 63.45 38.04 44.02 61.99 43.63 49.10 69.63 B.5. Effect of Poisson Blending We use Poisson blending to enhance the quality of the composition of synthetic cars with the background scene. We observe slight dip in the detection performance using the obtained augmentations as reported in Tab. 10. similar observation was made in [57], where improved blending does not positively affect the detection performance. Table 10. Monocular 3D detection performance of Poisson Blending on our Rendering on KITTI [6] validation set. Table 12. Data efficiency of SA-PlaceNet on KITTI dataset MonoDLE % Real Data % Aug. Data 10 25 50 75 100 100 10 25 50 75 100 0 3D@IOU=0.7 Easy Mod. Hard 3.26 3.90 4.94 8.23 9.78 13.38 11.71 13.70 20.46 12.38 14.95 21.53 12.89 15.44 22.49 11.69 13.66 17. 3D@IOU=0.5 Easy Mod. Hard 18.06 21.03 27.21 30.83 36.99 48.28 37.87 43.83 58.04 39.99 45.19 60.94 40.35 45.59 63.59 37.81 43.42 55.41 reduce the dependence on real data when training monocular detection networks. Specifically, augmenting just 50 % of the real data can achieve better performance than training with 100 % of the original training data. C.2. Scalability of generated augmentations To evaluate the effectiveness of the scale of our augmentations, we perform scalability experiment on large nuScenes dataset consisting of 35K images. We use different fractions of real and augmented data to train monocular 3D detector and achieve consistent gains across the amount of data. (a) MonoDLE[32] on Car with and without Poisson Blending Table 13. Scaling on NuScenes dataset Rendering w/o 3D Aug. Ours Ours (+Poisson) 3D@IOU=0.7 Easy Mod. Hard 11.69 13.66 17.45 12.89 15.44 22.49 12.81 14.44 21.34 3D@IOU=0.5 Easy Mod 43.42 55.41 45.59 63.59 44.11 59.60 (b) GUPNet[30] on Car with and without Poisson Blending Rendering w/o 3D Aug. Ours Ours (+Poisson) 3D@IOU=0.7 Easy Mod. Hard 13.27 16.46 22.76 14.71 17.28 23.94 14.55 17.03 22.43 3D@IOU=0.5 Easy Mod 42.33 57.62 47.18 61.01 45.28 60.00 Hard 37.81 40.35 38.15 Hard 37.59 41.48 39.60 Table 11. Analysis of Training Time Model SA-PlaceNet SA-PlaceNet GUPNet GUPNet FCOS3D FCOS3D Dataset KITTI NuScenes Original KITTI Augmented KITTI Original NuScenes Augmented NuScenes Training Time 12h 32h 20h 22h 5d18h 6d #GPUs GPU Model 1 1 1 1 2 2 A5000 A5000 A5000 A5000 A5000 C. Computational cost of MonoPlace3D Training of SA-PlaceNet takes fraction of the time of the detection training. The relative training time is significantly reduced for large datasets such as NuScenes. We present the computational requirements of our augmentation in comparison to the training time in Table 11. We train GUPNet and MonoDLE for an additional 10 epochs and FCOS3D for an additional 5 epochs when training with our augmented data. C.1. Data Efficiency on KITTI In this section, we demonstrate the data efficiency of our method. As observed in Tab.12 our method can significantly % Data mAP (w/o aug) mAP (ours) NDS (w/o aug) NDS (ours) 15 30 50 0.131 0.231 0.310 0.343 0.151 0.253 0.342 0.371 0.223 0.311 0.392 0.415 0.239 0.339 0.411 0.440 C.3. Rendering Ablation on NuScenes We also present an ablation study of various rendering approaches for augmentation in 3D detection for NuScenes. All renderings, when used with our learned placement, outperform the baseline, demonstrating the compatibility of our placement with different rendering methods. Table 14. Ablation on NuScenes FCOS3D [3] w/o 3D Augmentation ShapeNet Lift3D Ours MAP 0.3430 0.3441 0.3460 0.3704 NDS 0.415 0.414 0.416 0.440 D. Data Augmentation for Corner Cases We aim to approximate the training data distribution p(x), with learned distribution qθ(x), which can be sampled (x qθ(x)) to generate augmentations. In principle, our approach can also model abnormal cases by learning distribution to approximate the conditional distribution p(xstate =abnormal). During inference from SA-PlaceNet we sample the least likely positions from the learned distribution to simulate corners cases for autonomous driving . We augment the training data with these corner cases and train MonoDLE [32] . In Fig 11 we show qualitatively how training with our data can improve the model performance on corner cases . Figure 11. Detection improvement in corner cases. E. Implementations details E.1. Placement data Preprocessing Image-to-Image Inpainting We use the state-of-the-art method [38] to remove vehicles and objects from the KITTI dataset [15]. The input prompt inpaint is passed to the inpainting pipeline. few outputs from this method can be seen in Fig. 12 Figure 12. Outputs generated from Stable Diffusion Inpainting pipeline [38]. These inpainted images are used for training our placement model. E.2. Baseline methods Geometric Copy-paste (Geo-CP). To augment given scene, car is randomly sampled from the database, and its 3D parameters are altered before placement. Specifically, the depth of the box (z coordinate) is randomly sampled, and corresponding and are transformed using geometric operations. Other parameters, such as bounding box size and orientation, are kept unchanged. The sampled car is then pasted using simple blending on the background scene. CARLA [10]. To compare the augmentations generated by simulated road scene environments, we use state-of-theart CARLA simulator engine for rendering realistic scenes It can generate diverse traffic scenarwith multiple cars. ios that are implemented programmatically. However, its extremely challenging for simulators to capture the true diversity from real-world road scenes and they often suffer from large sim2real gap. Rule Based Placement (RBP). We create strong rulebased baseline to show the effectiveness of our learningbased placement. Specifically, we first segment out the road region with [16] and sample placement locations in this region. To get plausible orientation, we copy the orientation of the closest car in the scene, assuming neighboring cars follow the same orientations. We used our proposed rendering pipeline to generate realistic augmentations. Lift-3D [22] proposed generative radiance field network to synthetize realistic 3D cars. Lift3D trains conditional NeRF on multi-view car images generated by StyleGANs. However, the car shape is changed following the 3D bounding box dimensions. The generated cars are then placed on the road using heuristic based on road segmentation. We used single generated 3D car provided in the official code to augment the dataset as the training code is unavailable. Specifically, road region is segmented using off-the-shelf drivable area segmentor [16]. Next, the 3D bounding box of cars is sampled from predefined distribution of box parameters as given in Tab.15, and the ones outside the drivable area are filtered out. For sampled 3D bounding box parameters b=[bx, by, bz, bw, bh, bl, bθ], we render the car at adjusted orientation angle θ using Eq. 7. We place the camera at the fixed height of 1.6m, with an elevation angle of 0. Also, we used (bw, bh, bl) to render the car of particular shape. We render the car image for 512x512 resolution using volume rendering and the defined camera parameters. Along with the RGB image, Lift3D also outputs the segmentation mask for the car which is used to blend it with the background. Fig. 13 shows some sample renderings from Lift3D. Figure 13. Sampled views rendered from Lift3D [22]. F. Rendering details F.1. Copy-Paste In simple copy-paste rendering, the cars from the training corpus are added to the predicted 3D bounding boxes. We extract cars of various orientations from the training set images through instance segmentation using Detectron2 [51]. Table 15. Preset distribution of bounding boxes. Lift3D [22] samples bounding boxes from the predefined parameter distribution. Pose Distribution l θ"
        },
        {
            "title": "Uniform\nGaussian\nUniform\nGaussian\nGaussian\nGaussian\nGaussian",
            "content": "Parameters {[20m, 20m]} µ = height, σ = 0.2 {[5m, 45m]} µ = lmean , σ = 0.5 µ = wmean , σ = 0.5 µ = hmean , σ = 0.5 µ = π/2, σ = π/2 Figure 14. Sample cars from the Copy-Paste Database These cars are archived in database with their corresponding 3D orientation and binary segmentation mask data. During inference, given 3D bounding box, we query and search for cars whose orientation closely aligns with the given 3D box orientation. certain degree of randomness is introduced in selecting the nearest-matching car, contributing to increased diversity and seamless integration with the input scene. Next, we compose the retrieved car image onto the background scene using the 2D-coordinated obtained from the 3D bounding box and the binary mask. This simple rendering essentially captures the diverse cars present in the training dataset and helps in generating scenes that are close to training distribution. However, such rendering has problem with shadows as the composition is not 3D-aware, given the placed cars are stored as images. F.2. ShapeNet Figure 15. Sample of ShapeNet [5] cars rendered at different views. the dataset. We exclude any car model with dimensions exceeding 50% of the computed average, and we repeat this random sampling procedure until the specified conditions are satisfied. Following that, we align and render the car by 3D rotation angle. Specifically, as the orientation angle θ is defined in 3D, using it directly to render the image does not take care of perspective projection. Eg. all the cars following lane will have similar orientation angles (close to zero) but look visually different when projected on the image as shown in Fig. 16. Both the rendered cars have 0 orientation angle in 3D but when projected onto the image planes, the rendered orientation changes with the location. To this end, we adjust the car orientation by correction factor to incorporate the perspective view, as described in equation (7), θ = θ + tan1( ) (7) where and are the respective 3D coordinates of the bounding box. We use the final corrected θ value for rendering the ShapeNet car. We render car images at 512x512, with white background, which can be later used as segmentation mask to blend the rendered image. few examples of the ShapeNet cars rendered with different orientations are visualized in Fig. 15. ShapeNet [5] is large-scale synthetic dataset that provides 3D models for various object categories, including cars. The ShapeNet Cars dataset focuses specifically on providing 3D models of different car models from various viewpoints. We leverage the high diversity of cars (nearly 7500 models) in the dataset and render the cars at the predicted box locations with 3D bounding box parameters using Blender [8] software. We employ random sampling technique to select 3D car model from this extensive dataset, which is then loaded in the Blender [8] environment. To ensure consistency in the car shapes, we initially calculated the average dimensions of the cars within Figure 16. Perspective and Absolute projection of cars with the same 3D orientation. F.3. Reaslistic rendering with Text-to-image models. We leverage state-of-the-art image-to-image translation method based on the powerful StableDiffusion model [53] to convert the synthetic ShapeNet renderings into realistic cars. We use edge-conditioned ControlNet [53], which takes an edge image and text prompt to generate images following the edge map and the prompt. Specifically, we utilize canny edge detector to create edge maps for synthetic car images rendered using ShapeNet [5], preservable orientation, we render the entire scene while setting both the car and the 2D plane as transparent. This method enables us to create collection of shadow renderings with transparent background for each car in the placement setting. ing the cars structure while maintaining its original orientation and scale. These edge maps, generated through the Canny Edge Detection algorithm [4], serve as input for the edge-conditioned ControlNet [53], enabling the rendering of realistic cars using the prompt realistic car on the street. Furthermore, given an edge map and hence ShapeNet-rendered car, we can obtain various realistic renderings at each iteration, facilitating diverse scene generations  (Fig. 17)  . We further enhance ControlNets backbone diffusion model using LoRA [17] on subset of car images from the KITTI dataset. This process enables the generation of natural-looking car versions that seamlessly blend with the background scene. Finally, we integrate the ControlNet-rendered car and its shadow base into the predicted location within the scene to achieve realistic rendering. Figure 17. a) Diverse renderings generated with edge-conditioned ControlNet. B) Shadows are generated by rendering 3D assets with point light source in the blender [8] environment F.4. Rendering shadows in Blender [8] To generate realistic composition of the augmented cars, we generate realistic shadows for cars using the ShapeNet [5] dataset and rendered with Blender. We modify the rendering method to generate shadows by introducing 2D mesh plane beneath the car base and adding uniform Sun Light source along the z-axis of the blender environment, placed at the top on the z-axis of the car  (Fig. 17)  . Additionally, we introduce slight variations across all axes for the light source position. Once the cars are positioned within the Blender [8] environment with suit-"
        }
    ],
    "affiliations": []
}