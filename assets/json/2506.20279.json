{
    "paper_title": "From Ideal to Real: Unified and Data-Efficient Dense Prediction for Real-World Scenarios",
    "authors": [
        "Changliang Xia",
        "Chengyou Jia",
        "Zhuohang Dang",
        "Minnan Luo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Dense prediction tasks hold significant importance of computer vision, aiming to learn pixel-wise annotated label for an input image. Despite advances in this field, existing methods primarily focus on idealized conditions, with limited generalization to real-world scenarios and facing the challenging scarcity of real-world data. To systematically study this problem, we first introduce DenseWorld, a benchmark spanning a broad set of 25 dense prediction tasks that correspond to urgent real-world applications, featuring unified evaluation across tasks. Then, we propose DenseDiT, which maximally exploits generative models' visual priors to perform diverse real-world dense prediction tasks through a unified strategy. DenseDiT combines a parameter-reuse mechanism and two lightweight branches that adaptively integrate multi-scale context, working with less than 0.1% additional parameters. Evaluations on DenseWorld reveal significant performance drops in existing general and specialized baselines, highlighting their limited real-world generalization. In contrast, DenseDiT achieves superior results using less than 0.01% training data of baselines, underscoring its practical value for real-world deployment. Our data, and checkpoints and codes are available at https://xcltql666.github.io/DenseDiTProj"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 9 7 2 0 2 . 6 0 5 2 : r From Ideal to Real: Unified and Data-Efficient Dense Prediction for Real-World Scenarios Changliang Xia Chengyou Jia Zhuohang Dang Minnan Luo Xian Jiaotong University {202066, cp3jia, dangzhuohang}@stu.xjtu.edu.cn, minnluo@xjtu.edu.cn Figure 1: Comparison of idealized vs. real-world dense prediction. Left: Idealized tasks under controlled conditions. Right: Real-world tasks with complexity and noise, posing greater challenges."
        },
        {
            "title": "Abstract",
            "content": "Dense prediction tasks hold significant importance of computer vision, aiming to learn pixel-wise annotated label for an input image. Despite advances in this field, existing methods primarily focus on idealized conditions, with limited generalization to real-world scenarios and facing the challenging scarcity of real-world data. To systematically study this problem, we first introduce DenseWorld, benchmark spanning broad set of 25 dense prediction tasks that correspond to urgent real-world applications, featuring unified evaluation across tasks. Then, we propose DenseDiT, which maximally exploits generative models visual priors to perform diverse real-world dense prediction tasks through unified strategy. DenseDiT combines parameter-reuse mechanism and two lightweight branches that adaptively integrate multi-scale context, working with less than 0.1% additional parameters. Evaluations on DenseWorld reveal significant performance drops in existing general and specialized baselines, highlighting their limited Equal Contribution. Corresponding Author. Preprint. Under review. real-world generalization. In contrast, DenseDiT achieves superior results using less than 0.01% training data of baselines, underscoring its practical value for real-world deployment. Our data, and checkpoints and codes are available at https://xcltql666.github.io/DenseDiTProj"
        },
        {
            "title": "Introduction",
            "content": "Dense prediction tasks [1, 2, 3], such as segmentation and depth estimation, represent class of fundamental computer vision problems that aim to learn mappings from input images to pixelwise annotated labels. These tasks are critical for numerous applications, such as autonomous driving [4], medical imaging [5], remote sensing [6], and others. While recent advances [7, 8, 9] have achieved strong performance through carefully designed strategies, they are mostly developed under idealized conditions [3, 10, 11] featuring uniform lighting and minimal occlusion, resulting in limited generalization to practical dense prediction tasks [12, 13, 14, 15]. In contrast to idealized conditions, dense prediction in real-world scenarios offers more practical value. These scenarios cover broad applications such as depth estimation in adverse weather to enhance autonomous driving safety, crack detection on infrastructure surfaces to enable proactive maintenance, and underwater image segmentation to advance deep-sea exploration. As illustrated in Figure 1, real-world scenarios differ from idealized conditions in two aspects: inherent complexity and data scarcity. This motivates our core question: Can we develop data-efficient dense prediction model that generalizes effectively across diverse real-world scenarios? To this end, we first introduce DenseWorld, benchmark to advance dense prediction in real-world scenarios. The core principle of DenseWorld is to cover broad spectrum of real-world tasks that are both practical and challenging, while being difficult to collect and annotate at scale. We carefully curate 25 diverse dense prediction tasks, each corresponding to distinct real-world scenario. These tasks span wide range of applications, including ecology [16, 17], healthcare [14, 18], infrastructure [13, 19], public safety [20, 21], and industrial operations [22]. In this sense, DenseWorld provides unified platform to evaluate dense prediction methods under real-world complexity. We further propose DenseDiT, dense prediction framework built upon generative models. While prior works [23, 24] concatenate query and noisy tokens along the channel dimension, they inevitably modify the base model architecture, which in turn compromises its pretrained visual priors. Moreover, such rigid concatenation restricts flexible interactions between query and noisy tokens. Instead, DenseDiT adopts parameter-reuse mechanism to process query tokens and noisy tokens independently within the intact latent space where the visual priors reside. This independence enables more expressive and flexible interactions via the Multi-Modal Attention (MMA) module [25] of the generative model. To further enhance generalization across diverse real-world scenarios, we introduce two lightweight branches: prompt branch, which reuses the text encoder to inject task-specific semantic cues, and demonstration branch, which reuses the latent encoder to align visual priors with complex scene distributions. We conduct comprehensive experiments on the proposed DenseWorld benchmark to evaluate various approaches. Extensive quantitative and qualitative results demonstrate that DenseDiT significantly outperforms both general-purpose and task-specific models across all tasks, showcasing strong capabilities for real-world dense prediction. Compared to existing generative-based methods [23, 24, 26], DenseDiT achieves strong performance in more challenging real-world scenarios with limited supervision, demonstrating superior utilization of generative visual priors. Moreover, the effectiveness of DenseDiTs branches reveals the strong flexible contextual modeling capacity of generative models, highlighting promising directions for future research. In summary, our contributions are as follows: We introduce DenseWorld, benchmark for unified evaluation of dense prediction across real-world scenarios, encompassing 25 practically relevant applications. We propose DenseDiT, generative-based framework that fully exploits visual priors for data-efficient dense prediction in diverse real-world scenarios. Extensive experiments on DenseWorld demonstrate the superiority of DenseDiT over general-purpose and task-specific baselines, revealing their limited generalization to realworld scenarios and highlighting DenseDiTs potential for dense prediction in the wild. 2 Figure 2: Overview of the DenseWorld benchmark. Upper left: the construction pipeline. Center left: examples of representative tasks across five real-world categories. Lower left: unified evaluation. Right: full taxonomy of 25 dense prediction tasks, each aligned with practical application scenario."
        },
        {
            "title": "2.1 Dense Prediction",
            "content": "Dense prediction tasks [1, 2, 3] such as segmentation and depth estimation have achieved substantial progress through carefully designed architectures and training pipelines [7, 8, 9]. However, most existing approaches are developed under idealized conditions [3, 10, 11], including bright and minimally occluded indoor scenarios [10] or outdoor scenarios captured under favorable weather conditions [11], as shown in the left part of Figure 1. This limits their practical utility in diverse and complex real-world scenarios. To mitigate these limitations, task-specific models [27, 28] have been proposed, but they lack generalizability across diverse dense prediction tasks. Corresponding datasets are also tailored to specific domains, such as OmniCrack30K [12] for road crack segmentation and the Global-Scale [29] for satellite image road extraction. In this work, we tackle these limitations with unified and data-efficient framework tailored for real-world dense prediction."
        },
        {
            "title": "2.2 Diffusion Models For Dense Prediction",
            "content": "Prior work [30] has demonstrated that pretrained generative models possess rich visual priors. These priors have been successfully exploited for other tasks, such as representation learning [31]. By redefining dense prediction as an image-to-image task, recent work [23, 32, 33, 34] has explored diffusion models [35, 36, 37] for dense prediction, such as depth estimation and segmentation. In depth estimation, DepthGen [24] performs monocular depth estimation using diffusion models, addressing noisy and incomplete depth data through step-unrolled denoising and infilling strategies. Marigold [23] fine-tunes diffusion model on clean synthetic data to elegantly exploit its visual priors for depth estimation. In segmentation, RefLDM-Seg [38] formulates in-context segmentation as latent mask generation without auxiliary decoders. However, these methods typically require modifying the backbone architecture, which compromises the visual priors. Moreover, they are mostly trained under idealized conditions, lacking generalization to real-world scenarios. In contrast, our work enables diffusion models to generalize across diverse real-world dense prediction tasks with minimal adaptation effort."
        },
        {
            "title": "3 DenseWorld Benchmark",
            "content": "We introduce the DenseWorld benchmark to cover broad spectrum of practical real-world dense prediction tasks. As illustrated in the right portion of Figure 2, the core idea behind DenseWorld is to cover diverse tasks that offer significant real-world utility yet suffer from limited data."
        },
        {
            "title": "3.1 Benchmark Construction",
            "content": "An overview of the benchmark construction pipeline is shown in the upper left portion of Figure 2, which consists of three main stages: multi-source dataset collection, reasonable task filtering, and necessary data pruning. To build DenseWorld, we collect data from diverse sources, including open platforms (e.g., Kaggle [39]), academic repositories (e.g., Papers With Code [40]), and domainspecific portals (e.g., NASA EarthData [41]). Unlike previous benchmarks that emphasize scale, we prioritize diversity and utility of real-world scenarios. This design encourages data-efficient learning practices. Built upon curated collection of cleaned source datasets, DenseWorld consists of 25 dense prediction tasks across five major categories, each aligned with critical area of practical relevance."
        },
        {
            "title": "3.2 Unified Evaluation Metrics",
            "content": "To enable consistent and comprehensive evaluation across dense prediction tasks, we propose D/SScore, unified metric that consolidates multiple heterogeneous indicators into single summary score, facilitating fair and balanced comparisons. For regression tasks (i.e., depth estimation in DenseWorld), we define D-Score, which combines four error-based metrics (AbsRel, RMSE, SqRel, RMSE-log) and one accuracy-based metric (Threshold Accuracy). All metrics follow the protocols in [42, 43] and are normalized to [0, 1]. The final D-Score is computed by averaging the normalized values. For classification tasks (i.e., segmentation in DenseWorld), we define S-Score as the average of three metrics [44, 45, 46]IoU, PA, and Dice."
        },
        {
            "title": "3.3 Benchmark Analysis",
            "content": "From the perspective of real-world diversity and utility, DenseWorld offers significant advantages over existing benchmarks, as shown in Table 1. Each task in DenseWorld is directly aligned with pressing real-world application, spanning diverse and practically meaningful scenarios. Moreover, by adopting data-efficient paradigm, DenseWorld encourages the development of automated methods itself for real-world dense prediction under limited supervision. The full taxonomy of DenseWorld is illustrated in the right panel of Figure 2, with detailed statistics and full name of tasks in Appendix A."
        },
        {
            "title": "4 Method",
            "content": "Table 1: Comparison of dense prediction benchmarks. Dataset Taskonomy[3] COCO[47] NYUv2[10] KITTI[11] Omnicrack30k[12] Global-Scale[29] DenseWorld #Practical Tasks #Training Samples DataEfficiency? Real-World Utility? Unified Eval? 1 2 1 2 1 1 25 4M 118K 795 7K 22K 2K 15 (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:34) (cid:37) (cid:37) (cid:37) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:34) Our goal is to reformulate generative models into data-efficient dense predictors capable of generalizing across diverse real-world scenarios. We begin by introducing the generative model with strong visual priors and our model overview in Section 4.1. Next, we formalize the dense prediction tasks in Section 4.2. Finally, we present the details of our proposed method, DenseDiT, in Section 4.3."
        },
        {
            "title": "4.1 Preliminary and Model Overview",
            "content": "Our approach builds upon the DiT architecture [48], which has been adopted in several recent stateof-the-art generative models [37, 49, 50]. DiT uses transformer-based denoising network to refine noisy latent iteratively. Each DiT block includes Multi-Modal Attention (MMA) module [25], which enables effective interaction between noisy latent and conditioning inputs. 4 Figure 3: Overview of the DenseDiT architecture for data-efficient real-world dense prediction. DiT models learn velocity field [51] to map noisy samples back to clean ones, optimized by minimizing the discrepancy between predicted and ground-truth velocities: Loss = Ez0,t,c vθ(zt, t, c) u(zt)2 where is the conditioning information, z0 is the clean latent, and zt is the noisy latent at timestep t. (1) 2 Figure 3 illustrates our DenseDiT, an efficient framework for dense prediction. It takes query image, textual prompt, and an optional demonstration pair as inputs. parameter-reused VAE encoder projects the query images into the latent space. Two lightweight auxiliary branches, the prompt branch and the demonstration branch, provide semantic and visual contextual cues, with their activation controlled by task attrbute called DAI. The input representations are jointly processed through MMA [25] modules for dense prediction. Details are provided in Section 4.3."
        },
        {
            "title": "4.2 Standardizing Task Representation",
            "content": "Dense prediction tasks involve diverse data formats, posing challenges for unified processing. Inspired by [33], we standardize task representations into RGB format, as most mainstream vision models [52, 53, 35], including DiT, are trained on RGB data, making this alignment critical for leveraging visual priors. We first align channel dimensions: single-channel data are duplicated to fit the RGB format, ensuring compatibility with RGB-based visual encoders. Next, we normalize pixel values for pixel-level regression tasks, where value scales vary across datasets. Specifically, we apply: rnorm = (cid:18) rmin rmax rmin (cid:19) 0. 2 (2) where rmax and rmin represent the maximum and minimum values, respectively."
        },
        {
            "title": "4.3 DenseDiT",
            "content": "Building upon the DiT architecture, we aim to perform diverse real-world dense prediction in data-efficient manner. As illustrated in Figure 3, we convert the generative backbone into real-world dense predictor with minimal architectural changes, preserving its pretrained strengths."
        },
        {
            "title": "4.3.1 Parameter-Reuse Mechanism",
            "content": "Visual priors embedded in DiT serve as key source of DenseDiTs effectiveness. To preserve these priors, we perform all the processing within the intact latent space, where the visual priors are encoded. Specifically, DenseDiT reuses the pretrained VAE from DiT to encode the query image into the same latent space as noisy latent, making it directly compatible with the existing DiT blocks. DenseDiT then employs shared DiT blocks for processing. This parameter-reuse mechanism ensures 5 architectural simplicity and low parameter complexity, while maintaining strong compatibility with the pretrained generative backbone. Adaptation to the new input (i.e., the query image) is achieved solely through lightweight LoRA [54] modules, which introduce only 0.1% additional parameters."
        },
        {
            "title": "4.3.2 Branch-Enhanced Task Understanding",
            "content": "The tasks in DenseWorld span diverse real-world scenarios, with inputs ranging from natural to nonnatural image types (e.g., medical images). Although DiT is pretrained on large-scale datasets like LAION-5B [55], adapting its priors to specific tasks under limited supervision remains challenge. To enhance task understanding with minimal cost, DenseDiT introduces two lightweight branches. The first is the prompt branch, built directly on DiTs native text encoder. This is natural design choice, as DiT is pretrained on image-text pairs, and its text encoder can be reused without further adaptation. For each task, we provide simple prompt using the template: [output format] of [real-world scene], injecting contextual cues to improve alignment during generation. The second is the demonstration branch, designed to mitigate domain gaps for tasks with more complex inputs. We introduce another parameter-reused VAE to encode demonstration example [IQ; IT ], where IQ is query and IT is its dense label. The latent token produced by this VAE interacts with the query latent and prompt token via MMA [25]. We control demonstration branch activation via binary attribute called the Distribution Alignment Indicator (DAI), which flags whether tasks data distribution aligns with DiTs pretraining. When DAI = No , the demonstration branch is enabled. DAI labels are obtained using GPT [56]-based classification, where the model evaluates each task description and determines whether its imagery aligns with DiTs pretraining. This branch-enhanced design strengthens DenseDiTs task comprehension and enables it to generalize to challenging tasks under imited supervision."
        },
        {
            "title": "4.3.3 Input Representation and Training Objective",
            "content": "Our training objective is aligned with the DiT framework. Specifically, we optimize the model to predict the velocity field vθ, minimizing the discrepancy between the predicted and true velocity: Loss = Ez0,t vθ(zt, z, t, Cd, Cp) u(zt)2 where zt is the noisy latent at timestep t, is the latent of the query image, and Cd, Cp are the latent tokens from the demonstration and prompt branches, respectively. (3)"
        },
        {
            "title": "5 Experiments",
            "content": "5."
        },
        {
            "title": "Implementation Details",
            "content": "DenseDiT is built upon FLUX.1-dev [37]. By default, we train DenseDiT model for each task individually. As an additional observation, we also train DenseDiT model on mixture of data from all tasks to further evaluate its generalization ability in joint training setting. All training is performed using LoRA [54] fine-tuning. We adopt the Prodigy optimizer [57] with safeguard warmup and bias correction enabled, and set the weight decay to 0.01. Training is conducted on NVIDIA L40S GPUs with batch size of 1 and gradient accumulation over 8 steps."
        },
        {
            "title": "5.2.1 Comparison with general-purpose models",
            "content": "To evaluate the generalization ability of DenseDiT across diverse real-world scenarios, we compare its performance on the DenseWorld benchmark with several strong general-purpose models. For pixellevel regression tasks, we select five stable baselines: Marigold [23], EcoDepth [58], SQLdepth [59], ZeoDepth [60]+PF [61], and Depth-Anything [62]+PF [61]. For pixel-level classification tasks, we include three widely used general-purpose baselines: SAM [63], CLIPSeg [64], and GroundedSAM [65]. Table 2 presents the quantitative comparison results. Overall, DenseDiT outperforms all competing models on all tasks in DenseWorld, despite training under limited supervision. DenseDiT achieves an average D-Score of 0.944 and an average S-Score of 0.744, surpassing the second-best 6 models by 4.8% and 45.3%, respectively. These results underscore DenseDiTs strong generalization ability across diverse real-world scenarios and its practicality for efficient dense prediction under limited supervision. Detailed results for non-unified metrics are reported in the Appendix B.1. We further evaluate DenseDiT on additional real-world dense prediction benchmarks, and the promising results are provided in Appendix C. Model Table 2: Comparison with general-purpose models. Training Samples Medical Assist. (S-Score) Adverse Env. (D-Score) Ecological Mon. (S-Score) Smart City. (S-Score) SAM [63] CLIPSeg [64] Grounded-SAM [65] Marigold [23] EcoDepth [58] SQLdepth [59] ZeoDepth [60]+PF [61] Depth-Anything [62]+PF [61] DenseDiT DenseDiT w/ Mixed Training 1.1B 345K 1.1B 74K 48K 24K 19K 19K 15 1525 - - - 0.901 0.044 0.352 0.577 0.771 0.944 0. 0.401 0.562 0.423 - - - - - 0.734 0.624 0.596 0.484 0.447 - - - - - 0.825 0.683 0.479 0.498 0.403 - - - - - 0.749 0.606 Safety Ctrl. (S-Score) Avg. (D/S-Score) 0.572 0.499 0.478 - - - - - 0.669 0. - / 0.512 - / 0.511 - / 0.438 0.901 / - 0.044 / - 0.352 / - 0.577 / - 0.771 / - 0.944 / 0.744 0.904 / 0.634 We also train single DenseDiT model on data mixed from all tasks. Although its performance is lower than training separately on each task, it substantially outperforms all baselines across all tasks, further confirming DenseDiTs strong generalization ability under limited supervision."
        },
        {
            "title": "5.2.2 Comparison with Task-Specific Models",
            "content": "We observe that among the tasks in DenseWorld, two have widely studied task-specific models: Pavement Crack Detect. and Urban Layout Anal.. These tasks correspond to longstanding research problems with curated datasets. Such models often exhibit strong performance within their domains due to architectural customization and large-scale training data, but they typically lack generalizability across broader real-world scenarios. To examine this, we conduct comparisons between DenseDiT and state-of-the-art task-specific models on these two tasks. To ensure detailed evaluation, we adopt metrics that are commonly used in each specific domain. As shown in Table 3, DenseDiT outperforms task-specific models across nearly all metrics in both tasks, despite not relying on any task-engineered architecture or large-scale training data. These results highlight the significant performance advantage and efficient generalization capability of DenseDiT in important real-world applications. Table 3: Comparison with task-specific models. (a) Pavement Crack Detect. (b) Urban Layout Anal."
        },
        {
            "title": "Model",
            "content": "IoU PA Dice cIoU"
        },
        {
            "title": "Model",
            "content": "Recall Precision IoU F1 CrackFormer [66] 0.512 0.729 0.585 0.198 0.569 0.594 0.625 0.489 TOPO [67] nnUet [12] 0.732 0.791 0.811 0.818 CT-CrackSeg [27] 0.677 0.770 0.758 0.603 D-LinkNet [68] NL-LinkNet [69] RCFSNet [70] sam-road [71]"
        },
        {
            "title": "DenseDiT",
            "content": "0.774 0.863 0.855 0."
        },
        {
            "title": "DenseDiT",
            "content": "0.337 0.435 0.813 0.343 0.619 0.509 0.567 0.569 0.715 0.237 0.371 0.317 0.469 0.501 0.665 0.296 0.437 0.746 0.512 0."
        },
        {
            "title": "5.2.3 Visual Comparisons",
            "content": "We present qualitative comparisons in Figure 4 and Figure 5. As shown in Figure 4, prior methods often struggle with occlusions and low-visibility conditions. In contrast, DenseDiT demonstrates strong global understanding of complex scene structures, producing coherent depth maps even in challenging conditions like fog, rain, and overcast lighting. This improvement stems from its ability to incorporate additional semantic and visual cues via the prompt and demonstration branches. Similarly, Figure 5 illustrates the advantages of DenseDiT in pixel-level classification tasks. Notably, in the fourth row, DenseDiT preserves structured urban layouts in satellite imagery where other models struggle. While SAM is typically strong segmentation baseline, its design prioritizes object-centric cues, leading to suboptimal results in densely structured or abstract scenes. More visual comparisons are provided in Appendix B.2. 7 Figure 4: Qualitative comparisons on pixel-level regression tasks. In the first and second column, DenseDiT successfully predicts occluded structures in fog or shadow, highlighting its capability for scene-level reasoning. The third column showcases DenseDiTs ability to capture fine-grained details such as distant lampposts and layered foliage. The forth column emphasizes its sensitivity to abrupt depth transitions, producing sharper and more consistent boundaries than competing models. Figure 5: Qualitative comparisons on pixel-level classification tasks. The first row shows strong resilience to cluttered backgrounds in complex scenes. In the second row, DenseDiT perceptively captures abstract concepts such as fire regions with minimal ambiguity. The third and fourth rows demonstrate its ability to localize fine structures, including spinal contours in medical images and dense urban building layouts in satellite views. 8 Table 4: Effectiveness of the demonstration branch."
        },
        {
            "title": "Method",
            "content": "Instant Fire Alert Spinal Morphology Assessment IoU PA Dice D/S-Score IoU PA Dice D/S-Score SAM [63] CLIPSeg [64] Grounded-SAM [65] w/o demonstration branch w/ demonstration branch 0.528 0.642 0.469 0.704 0.822 0.591 0.753 0.530 0.777 0.926 0.574 0.710 0.516 0.751 0.880 0.564 0.702 0.505 0.744 0.876 0.555 0.519 0.464 0.589 0. 0.611 0.569 0.481 0.604 0.971 0.612 0.551 0.481 0.646 0.964 0.593 0.546 0.475 0.613 0.956 Figure 6: Effect of loss functions. Figure 7: Effect of inference steps."
        },
        {
            "title": "5.3.1 Effectiveness of the Demonstration Branch",
            "content": "As introduced in Section 4.3.2, DenseDiT employs demonstration branch to enhance task understanding for some complex scenarios, activated when DAI=No. To validate its effectiveness, we conduct ablation studies on two representative tasks: Spinal Morphology Assessment, which involves complex medical structures, and Instant Fire Alert, which requires detecting dynamic and irregular visual patterns under extreme conditions. These tasks serve as representative examples of highly challenging real-world scenarios where additional contextual cues are crucial. Table 4 presents the detailed comparison results with and without the demonstration branch. In both tasks, we observe substantial performance improvements, confirming that the demonstration branch strengthens DenseDiTs ability to generalize to complex real-world scenarios."
        },
        {
            "title": "5.3.2 Training Loss",
            "content": "DenseDiT is generative-based framework for real-world dense prediction tasks. Prior generativebased dense prediction methods [24] typically employ L1 loss, motivated by its robustness to noise in earlier benchmarks [11, 10]. However, in DenseDiT, we find that L2 loss, defined in Eq.3, yields smoother convergence and more stable plateau, as shown in Figure6. We attribute this to two factors: (1) the high quality of DenseWorld reduces the need for noise-tolerant objectives, and (2) L2 loss provides stronger gradient signals on hard samples, promoting more effective optimization."
        },
        {
            "title": "5.3.3 The Function of Prompt Branch",
            "content": "As introduced in Section 4.3.2, DenseDiT incorporates lightweight prompt branch based on the native text encoder to deliver task definitions using unified text template. Prompt quality is known to be crucial for guiding generative models [72, 73], motivating us to assess its impact in DenseDiT. Figure 8 compares three settings: with prompt, without prompt, and random prompt (#$%ˆ&*@). We report results on two representative tasks under both DAI=No and DAI=Yes conditions. Similar trends are observed across other tasks in DenseWorld. Prompts consistently improve D/S-Score across two conditions. In DAI=No tasks, removing or randomizing the prompt causes degradation. The effect is more pronounced in DAI=Yes tasks, as these rely solely on the prompt to convey task semantics in the absence of 9 Figure 8: Effect of the prompt branch. demonstration-based visual cues. These results highlight the importance of task-aligned prompts in enabling robust and generalizable dense prediction."
        },
        {
            "title": "5.3.4 Inference steps",
            "content": "The number of inference steps is key factor in generative model performance. More steps improve quality at higher computational cost. As shown in Figure 7, we evaluate this effect on two representative tasks under DAI=No and DAI=Yes conditions. Similar trends are observed across other tasks in DenseWorld. DenseDiT demonstrates stable performance across wide range of inference steps, with noticeable improvement up to around 20 steps. This indicates that approximately 20 steps provide favorable balance between accuracy and efficiency. Notably, for DAI=Yes tasks, performance slightly declines beyond 30 steps, likely due to over-sampling effects degrading fine-grained details [74]."
        },
        {
            "title": "6 Conclusion",
            "content": "Our work aims to enable robust and efficient dense prediction across diverse real-world scenarios under limited supervision. To achieve this goal, we introduce DenseWorld, benchmark spanning broad set of practical, challenging tasks. We further propose DenseDiT, data-efficient framework based on generative models, featuring parameter-reuse architecture and lightweight task-enhancing branches. Extensive experiments demonstrate that DenseDiT achieves strong performance and generalization across complex real-world tasks with minimal training data. We believe this work takes meaningful step toward practical and scalable dense prediction in the wild."
        },
        {
            "title": "References",
            "content": "[1] Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong Tang, Zheng Zhu, Guan Huang, Jie Zhou, and Jiwen Lu. Denseclip: Language-guided dense prediction with context-aware In Proceedings of the IEEE/CVF conference on computer vision and pattern prompting. recognition, pages 1808218091, 2022. [2] Chunlong Xia, Xinliang Wang, Feng Lv, Xin Hao, and Yifeng Shi. Vit-comer: Vision transformer with convolutional multi-scale feature interaction for dense predictions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 54935502, 2024. [3] Amir Zamir, Alexander Sax, William Shen, Leonidas Guibas, Jitendra Malik, and Silvio In Proceedings of the IEEE Savarese. Taskonomy: Disentangling task transfer learning. conference on computer vision and pattern recognition, pages 37123722, 2018. [4] Holger Caesar, Varun Bankiti, Alex Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: multimodal dataset for autonomous driving. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1162111631, 2020. [5] Ali Hatamizadeh, Yucheng Tang, Vishwesh Nath, Dong Yang, Andriy Myronenko, Bennett Landman, Holger Roth, and Daguang Xu. Unetr: Transformers for 3d medical image segmentation. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 574584, 2022. [6] Sijie Zhao, Hao Chen, Xueliang Zhang, Pengfeng Xiao, Lei Bai, and Wanli Ouyang. Rs-mamba for large remote sensing image dense prediction. IEEE Transactions on Geoscience and Remote Sensing, 2024. [7] Lintao Dong, Wei Zhai, and Zheng-Jun Zha. Unidense: Unleashing diffusion models with metarouters for universal few-shot dense prediction. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 1052510534, 2024. [8] Donggyun Kim, Seongwoong Cho, Semin Kim, Chong Luo, and Seunghoon Hong. Chameleon: data-efficient generalist for dense visual prediction in the wild. In European Conference on Computer Vision, pages 422441. Springer, 2024. 10 [9] Donggyun Kim, Jinwoo Kim, Seongwoong Cho, Chong Luo, and Seunghoon Hong. Universal few-shot learning of dense prediction tasks with visual token matching. arXiv preprint arXiv:2303.14969, 2023. [10] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support inference from rgbd images. In Computer VisionECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part 12, pages 746760. Springer, 2012. [11] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. The international journal of robotics research, 32(11):12311237, 2013. [12] Christian Benz and Volker Rodehorst. Omnicrack30k: benchmark for crack segmentation and the reasonable effectiveness of transfer learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 38763886, 2024. [13] Volodymyr Mnih. Machine learning for aerial image labeling. University of Toronto (Canada), 2013. [14] Kai Jin, Xingru Huang, Jingxing Zhou, Yunxiang Li, Yan Yan, Yibao Sun, Qianni Zhang, Yaqi Wang, and Juan Ye. Fives: fundus image dataset for artificial intelligence based vessel segmentation. Scientific data, 9(1):475, 2022. [15] Rui Fan, Umar Ozgunalp, Brett Hosking, Ming Liu, and Ioannis Pitas. Pothole detection based on disparity transformation and road surface modeling. IEEE Transactions on Image Processing, 29:897908, 2019. [16] Sebastian Haug and Jörn Ostermann. crop/weed field image dataset for the evaluation of computer vision based precision agriculture tasks. In Computer Vision - ECCV 2014 Workshops, pages 105116, 2015. [17] Earthshot Labs. Tree binary segmentation. https://www.kaggle.com/datasets/ earthshot/tree-binary-segmentation, 2022. [18] Chengwen Chu, DL Belavy, Armbrecht, Bansmann, Felsenberg, and Zheng. Annotated t2-weighted mr images of the lower spine. Zenodo, 2015. [19] Lei Zhang, Fan Yang, Yimin Daniel Zhang, and Ying Julie Zhu. Road crack detection using deep convolutional neural network. In 2016 IEEE international conference on image processing (ICIP), pages 37083712. IEEE, 2016. [20] Daniel YT Chino, Letricia PS Avalhais, Jose Rodrigues, and Agma JM Traina. Bowfire: detection of fire in still images by integrating pixel color and texture analysis. In 2015 28th SIBGRAPI conference on graphics, patterns and images, pages 95102. IEEE, 2015. [21] Liming Wang, Jianbo Shi, Gang Song, and I-fan Shen. Object detection combining recognition and segmentation. In Asian conference on computer vision, pages 189199. Springer, 2007. [22] Xuebin Qin, Hang Dai, Xiaobin Hu, Deng-Ping Fan, Ling Shao, and Luc Van Gool. Highly accurate dichotomous image segmentation. In European Conference on Computer Vision, pages 3856. Springer, 2022. [23] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 94929502, 2024. [24] Saurabh Saxena, Abhishek Kar, Mohammad Norouzi, and David Fleet. Monocular depth estimation using diffusion models. arXiv preprint arXiv:2302.14816, 2023. [25] Zexu Pan, Zhaojie Luo, Jichen Yang, and Haizhou Li. Multi-modal attention for speech emotion recognition. arXiv preprint arXiv:2009.04107, 2020. 11 [26] Minh-Quan Le, Tam Nguyen, Trung-Nghia Le, Thanh-Toan Do, Minh Do, and Minh-Triet Tran. Maskdiff: Modeling mask distribution with diffusion probabilistic model for few-shot In Proceedings of the AAAI Conference on Artificial Intelligence, instance segmentation. volume 38, pages 28742881, 2024. [27] Huaqi Tao, Bingxi Liu, Jinqiang Cui, and Hong Zhang. convolutional-transformer network for crack segmentation with boundary awareness. In 2023 IEEE International Conference on Image Processing (ICIP), pages 8690. IEEE, 2023. [28] Stefano Gasperini, Nils Morbitzer, HyunJun Jung, Nassir Navab, and Federico Tombari. Robust monocular depth estimation under challenging conditions. In Proceedings of the IEEE/CVF international conference on computer vision, pages 81778186, 2023. [29] Pan Yin, Kaiyu Li, Xiangyong Cao, Jing Yao, Lei Liu, Xueru Bai, Feng Zhou, and Deyu Meng. Towards satellite image road graph extraction: global-scale dataset and novel method. arXiv preprint arXiv:2411.16733, 2024. [30] Anand Bhattad, Daniel McKee, Derek Hoiem, and David Forsyth. Stylegan knows normal, depth, albedo, and more. Advances in Neural Information Processing Systems, 36:7308273103, 2023. [31] Jeff Donahue and Karen Simonyan. Large scale adversarial representation learning. Advances in neural information processing systems, 32, 2019. [32] Zigang Geng, Binxin Yang, Tiankai Hang, Chen Li, Shuyang Gu, Ting Zhang, Jianmin Bao, Zheng Zhang, Houqiang Li, Han Hu, et al. Instructdiffusion: generalist modeling interface for vision tasks. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, pages 1270912720, 2024. [33] Canyu Zhao, Mingyu Liu, Huanyi Zheng, Muzhi Zhu, Zhiyue Zhao, Hao Chen, Tong He, and Chunhua Shen. Diception: generalist diffusion model for visual perceptual tasks. arXiv preprint arXiv:2502.17157, 2025. [34] Hsin-Ying Lee, Hung-Yu Tseng, and Ming-Hsuan Yang. Exploiting diffusion prior for generalizable dense prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 78617871, 2024. [35] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [36] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. [37] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. [38] Chaoyang Wang, Xiangtai Li, Henghui Ding, Lu Qi, Jiangning Zhang, Yunhai Tong, Chen Change Loy, and Shuicheng Yan. Explore in-context segmentation via latent diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 75457553, 2025. [39] Kaggle. Kaggle: Your machine learning and data science community. https://www.kaggle. com, 2010. [40] Papers with Code. Papers with code: State-of-the-art research and benchmarks. https: //paperswithcode.com, 2019. [41] NASA. Nasa earthdata: Open access earth observation data. https://earthdata.nasa.gov, 1994. [42] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from single image using multi-scale deep network. Advances in neural information processing systems, 27, 2014. 12 [43] René Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE transactions on pattern analysis and machine intelligence, 44(3):16231637, 2020. [44] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. International journal of computer vision, 88:303338, 2010. [45] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 32133223, 2016. [46] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-net: Fully convolutional neural networks for volumetric medical image segmentation. In 2016 fourth international conference on 3D vision (3DV), pages 565571. Ieee, 2016. [47] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer visionECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part 13, pages 740755. Springer, 2014. [48] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [49] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis, 2024. URL https://arxiv. org/abs/2403.03206, 2. [50] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-{alpha}: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. [51] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in neural information processing systems, 35:2656526577, 2022. [52] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. [53] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. [54] Shilpa Devalal and Karthikeyan. Lora technology-an overview. In 2018 second international conference on electronics, communication and aerospace technology (ICECA), pages 284290. IEEE, 2018. [55] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion5b: An open large-scale dataset for training next generation image-text models. Advances in neural information processing systems, 35:2527825294, 2022. [56] OpenAI. Chatgpt, 2022. Large language model. [57] Konstantin Mishchenko and Aaron Defazio. Prodigy: An expeditiously adaptive parameter-free learner. arXiv preprint arXiv:2306.06101, 2023. [58] Suraj Patni, Aradhye Agarwal, and Chetan Arora. Ecodepth: Effective conditioning of diffusion In Proceedings of the IEEE/CVF Conference on models for monocular depth estimation. Computer Vision and Pattern Recognition, pages 2828528295, 2024. [59] Youhong Wang, Yunji Liang, Hao Xu, Shaohui Jiao, and Hongkai Yu. Sqldepth: Generalizable self-supervised fine-structured monocular depth estimation. In Proceedings of the AAAI conference on artificial intelligence, volume 38, pages 57135721, 2024. [60] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, and Matthias Müller. Zoedepth: Zero-shot transfer by combining relative and metric depth. arXiv preprint arXiv:2302.12288, 2023. [61] Zhenyu Li, Shariq Farooq Bhat, and Peter Wonka. Patchfusion: An end-to-end tile-based In Proceedings of the framework for high-resolution monocular metric depth estimation. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1001610025, 2024. [62] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1037110381, 2024. [63] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laurens Gustafson, Tete Xiao, Samuel Whitehead, Benjamin Caine, Roozbeh Mottaghi, et al. Segment anything. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2129321312, 2023. [64] Theresa Lüddecke and Alexander Ecker. Image segmentation using text and image prompts. arXiv preprint arXiv:2112.10003, 2022. [65] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, et al. Grounded sam: Assembling open-world models for diverse visual tasks. arXiv preprint arXiv:2401.14159, 2024. [66] Huajun Liu, Xiangyu Miao, Christoph Mertz, Chengzhong Xu, and Hui Kong. Crackformer: Transformer network for fine-grained crack detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 37833792, 2021. [67] Bryan Pantoja-Rosero, Oner, Mateusz Kozinski, Radhakrishna Achanta, Pascal Fua, Fernando Pérez-Cruz, and Katrin Beyer. Topo-loss for continuity-preserving crack detection using deep learning. Construction and Building Materials, 344:128264, 2022. [68] Lichen Zhou, Chuang Zhang, and Ming Wu. D-linknet: Linknet with pretrained encoder and dilated convolution for high resolution satellite imagery road extraction. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pages 182186, 2018. [69] Yooseung Wang, Junghoon Seo, and Taegyun Jeon. Nl-linknet: Toward lighter but more accurate road extraction with nonlocal operations. IEEE Geoscience and Remote Sensing Letters, 19:15, 2021. [70] Zhigang Yang, Daoxiang Zhou, Ying Yang, Jiapeng Zhang, and Zehua Chen. Road extraction from satellite imagery by road context and full-stage feature. IEEE Geoscience and Remote Sensing Letters, 20:15, 2022. [71] Congrui Hetang, Haoru Xue, Cindy Le, Tianwei Yue, Wenping Wang, and Yihui He. Segment anything model for road network graph extraction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 25562566, 2024. [72] Tingfeng Cao, Chengyu Wang, Bingyan Liu, Ziheng Wu, Jinhui Zhu, and Jun Huang. Beautifulprompt: Towards automatic prompt engineering for text-to-image synthesis. arXiv preprint arXiv:2311.06752, 2023. [73] Chengyou Jia, Changliang Xia, Zhuohang Dang, Weijia Wu, Hangwei Qian, and Minnan Luo. Chatgen: Automatic text-to-image generation from freestyle chatting. arXiv preprint arXiv:2411.17176, 2024. [74] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International conference on machine learning, pages 81628171. PMLR, 2021. 14 [75] Immersive Limit. Cigarette butts dataset. https://www.immersivelimit.com/datasets/ cigarette-butts, 2024. Accessed: 2025-04-30. [76] killa92. Cardiac semantic segmentation dataset. https://www.kaggle.com/datasets/ killa92/cardiac-semantic-segmentation-dataset, 2020. Accessed: 2025-04-30. [77] Gaelle Letort, Adrien Eichmuller, Christelle Da Silva, Elvira Nikalayevich, Flora Crozet, Jeremy Salle, Nicolas Minc, Elsa Labrune, Jean-Philippe Wolf, Marie-Emilie Terret, et al. An interpretable and versatile machine learning approach for oocyte phenotyping. Journal of Cell Science, 135(13):jcs260281, 2022. [78] Sadjad Rezvani, Mansoor Fateh, and Hossein Khosravi. Abanet: Attention boundary-aware network for image segmentation. Expert Systems, 41(9):e13625, 2024. [79] Rui Fan, Xiao Ai, and Naim Dahnoun. Road surface 3d reconstruction based on dense subpixel disparity map estimation. IEEE Transactions on Image Processing, 27(6):30253035, 2018. [80] Rui Fan and Ming Liu. Road damage detection based on unsupervised disparity map segmentation. IEEE Transactions on Intelligent Transportation Systems, 21(11):49064911, 2019. [81] Chinese Academy of Cultural Heritage. 2020 sar water body sample and annotation dataset. https://noda.ac.cn/datasharing/datasetDetails/650427f8671ff33bd4ffedb6, 2022. Data source for the National Key R&D Program Project \"Assessment and Emergency Response for Natural Disaster Risks of Immovable Cultural Heritage\" (2019YFC1520800). [82] Qiqi Zhu, Yanan Zhang, Ziqi Li, Xiaorui Yan, Qingfeng Guan, Yanfei Zhong, Liangpei Zhang, and Deren Li. Oil spill contextual and boundary-supervised detection network based on marine sar images. IEEE Transactions on Geoscience and Remote Sensing, 60:110, 2021. [83] Peter Naylor, Marick Laé, Fabien Reyal, and Thomas Walter. Segmentation of nuclei in histopathology images by deep regression of the distance map. IEEE transactions on medical imaging, 38(2):448459, 2018. [84] Deng-Ping Fan, Ge-Peng Ji, Guolei Sun, Ming-Ming Cheng, Jianbing Shen, and Ling Shao. Camouflaged object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 27772787, 2020. [85] Yohann Cabon, Naila Murray, and Martin Humenberger. Virtual kitti 2. arXiv preprint arXiv:2001.10773, 2020. [86] Tianwei Yin, Michaël Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 66136623, 2024."
        },
        {
            "title": "A Details of DenseWorld",
            "content": "Table 5 presents detailed statistics for all 25 tasks in DenseWorld. Beyond the diversity of task categories, the dataset exhibits considerable variation in visual characteristics, covering natural images and specialized domains like medical imaging and remote sensing, fully reflecting the complexity and diversity of real-world scenarios. Notably, the data size per task is highly imbalanced, with some tasks containing only tens of images and others comprising several thousand. This imbalance introduces difficulties for adopting unified dense prediction strategy across tasks and highlights the importance of data-efficient frameworks that can generalize well without relying on large-scale data. Furthermore, most of these real-world tasks have lacked standardized benchmarks in prior literature, and DenseWorld addresses this gap by offering comprehensive platform for fair, consistent, and systematic evaluation. 15 Table 5: Detailed statistics of all 25 real-world dense prediction tasks in DenseWorld. Dataset Task (Abbreviation) Image Sizes Tota Samples Primary Reference DAI CIGARETTE-BUTT-DATASET CRACK500 CWFID DIS5K BoWFire Fives CSSD BILL (human) BILL (seg urchin) Massachusetts Road Massachusetts Building MFSD Pothole-600 PennFudanPed SAR SOS Lumbar-Spine-Segmentation TNBC Tree Binary Segmentation COD10K VKITTI2 (Fog) VKITTI2 (Rain) VKITTI2 (Overcast) VKITTI2 (Sunrise) VKITTI2 (Sunset) Intelligent Cigarette Butt Detection (Cigarette Detect.) Intelligent Crack Detection for Pavements (Pavement Crack Detect.) Weed Distribution Mapping (Weed Distrib. Map.) Close-Range Scene Segmentation (Close-Range Segment.) Instant Fire Alert (Fire Alert Sys.) Retinal Vascular Abnormality Analysis (Retinal Vessel Anal.) Cardiac Structure Screening (Cardiac Screen.) Oocyte Recognition (Oocyte Detect.) Fresh Food Cell Inspection (Food Cell Inspect.) High-Precision Road Network Modeling (Precise Road Model.) Urban Building Layout Analysis (Urban Layout Anal.) Real-Time Mask-Wearing Monitoring (Mask-Wear Monitor.) Automated Pothole Identification (Auto. Pothole Detect.) Pedestrian Crossing Safety Monitoring (Pedestrian Safety Monitor.) Water Body Delineation (Water Body Map.) Marine Oil Spill Surveillance (Oil Spill Track.) Spinal Morphology Assessment (Spine Morph. Assess.) Nucleus Localization (Nucleus Loc.) Automated Tree Tagging (Auto. Tree Tag.) Camouflaged Animal Detection (Camouflage Detect.) Depth Estimation in Foggy Scenes (Fog Depth Est.) Depth Estimation in Rainy Scenes (Rain Depth Est.) Depth Estimation in Overcast Scenes (Overcast Depth Est.) Depth Estimation at Sunrise (Sunrise Depth Est.) Depth Estimation at Sunset (Sunset Depth Est.) 512 25601440 1024768 1024768 640480 960960 110 60 515 118 215 512 1717 512512 623 512512 960960 64 960960 151 480640 9383 400 512512 420 170 256256 1000 512 512512 512512 791 420 50 512 2718 640480 515 1232368 2041 1232 1232368 1232368 1232368 515 515 515 [75] [19] [16] [22] [20] [14] [76] [77] [77] [13] [13] [78] Yes Yes Yes Yes No Yes No No No Yes Yes Yes [79, 15, 80] No [21] [81] [82] [18] [83] [17] [84] [85] [85] [85] [85] [85] Yes No No No No Yes Yes Yes Yes Yes Yes Yes To automatically assign DAI labels, we use GPT [56]-based classification process. Specifically, GPT evaluates each task description and determines whether the corresponding imagery aligns with the visual distribution typically seen in DiT model pretraining. The prompt used for this classification is provided below."
        },
        {
            "title": "Prompt for obtaining DAI labels",
            "content": "You are given description and demo image of dense prediction task. Please determine whether the images involved in this task are well-aligned with the training distribution of DiT (e.g., images from LAION-5B), or if they represent significant distribution shift (e.g., medical scans, satellite imagery). Please respond with Yes if the task aligns with DiTs training distribution, and No otherwise."
        },
        {
            "title": "B More detailed and visual results",
            "content": "B.1 Detailed results for non-unified metrics We report the detailed results for each of the 25 tasks in DenseWorld using standard non-unified metrics, as shown in Table 6 to Table 30. Consistent with the main results, DenseDiT trained separately on each task achieves the best performance in most cases, demonstrating its strong effectiveness and generalization capability across diverse real-world dense prediction tasks. Interestingly, as an additional observation, the DenseDiT model trained on mixed-task data outperforms the individually trained models on some tasks, such as Fog Depth Est.  (Table 6)  , Oocyte Detection  (Table 19)  and Cigarette Detection  (Table 27)  . This is notable because mixed-task training under limited supervision typically increases optimization difficulty and cross-task interference. The improvement reflects DenseDiTs strong real-world generalization capability and its ability to leverage shared structural priors across tasks, enabling beneficial cross-task knowledge transfer. Table 6: Fog Depth Est."
        },
        {
            "title": "Model",
            "content": "Marigold [23] ECoDepth [58] SQLdepth [59] ZoeDepth [60]+PF [61] Depth-Anything [62]+PF [61] DenseDiT DenseDiT w/ Mixed Training δ1 0.762 0.481 0.344 0.575 0.588 0.845 0.873 δ2 0.938 0.776 0.670 0.898 0.932 0.969 0.976 δ3 0.978 0.915 0.889 0.971 0.984 0.991 0.991 REL Sq-rel RMS RMS log 0.163 0.368 0.408 0.239 0.223 0.139 0.120 1.187 3.579 3.945 1.960 1.702 0.599 0.517 6.114 9.657 10.563 7.888 7.804 3.794 3. 0.101 0.160 0.189 0.133 0.110 0.077 0."
        },
        {
            "title": "Model",
            "content": "Marigold [23] ECoDepth [58] SQLdepth [59] ZoeDepth [60]+PF [61] Depth-Anything [62]+PF [61] DenseDiT DenseDiT w/ Mixed Training"
        },
        {
            "title": "Model",
            "content": "Marigold [23] ECoDepth [58] SQLdepth [59] ZoeDepth [60]+PF [61] Depth-Anything [62]+PF [61] DenseDiT DenseDiT w/ Mixed Training REL Table 7: Sunrise Depth Est. δ1 0.846 0.459 0.538 0.615 0.694 0.763 0.720 δ2 0.958 0.760 0.857 0.891 0.943 0.948 0.941 δ3 0.983 0.897 0.958 0.975 0.983 0.982 0.983 0.134 0.386 0.275 0.233 0.189 0.183 0. REL δ2 δ3 Table 8: Overcast Depth Est. δ1 0.840 0.516 0.546 0.620 0.713 0.810 0.758 0.956 0.802 0.881 0.913 0.953 0.963 0.952 0.982 0.936 0.966 0.976 0.983 0.987 0. 0.137 0.337 0.261 0.225 0.183 0.152 0.172 Sq-rel RMS RMS log 1.114 3.969 2.119 1.935 1.317 1.005 1.115 5.533 9.978 7.782 7.468 6.345 4.847 5.367 0.085 0.166 0.136 0.127 0.107 0.095 0.099 Sq-rel RMS RMS log 1.081 3.283 2.055 1.826 1.222 0.749 0.954 5.526 9.331 7.941 7.317 6.118 4.189 5.035 0.088 0.150 0.132 0.127 0.106 0.083 0.092 17 Table 9: Rain Depth Est."
        },
        {
            "title": "Model",
            "content": "Marigold [23] ECoDepth [58] SQLdepth [59] ZoeDepth [60]+PF [61] Depth-Anything [62]+PF [61] DenseDiT DenseDiT w/ Mixed Training δ1 0.790 0.488 0.553 0.627 0.719 0.846 0.832 δ2 0.943 0.784 0.860 0.916 0.957 0.970 0.970 δ3 0.978 0.920 0.963 0.975 0.989 0.989 0.989 REL Sq-rel RMS RMS log 0.155 0.359 0.265 0.219 0.176 0.135 0.140 1.195 3.602 2.188 1.684 1.182 0.649 0.729 6.007 9.683 8.371 7.261 6.138 4.002 4. 0.099 0.157 0.137 0.125 0.093 0.075 0."
        },
        {
            "title": "Model",
            "content": "Marigold [23] ECoDepth [58] SQLdepth [59] ZoeDepth [60]+PF [61] Depth-Anything [62]+PF [61] DenseDiT DenseDiT w/ Mixed Training REL Table 10: Sunset Depth Est. δ1 0.836 0.446 0.561 0.593 0.670 0.724 0.702 δ2 0.955 0.748 0.866 0.861 0.923 0.929 0.923 δ3 0.984 0.893 0.966 0.967 0.978 0.979 0.979 0.138 0.397 0.263 0.248 0.200 0.201 0. Sq-rel RMS RMS log 1.094 4.050 2.039 2.027 1.391 1.125 1.246 5.561 10.113 7.751 7.589 6.416 5.107 5.667 0.087 0.169 0.133 0.133 0.110 0.102 0.104 Table 11: Pavement Crack Detect. Table 12: Auto. Pothole Detect."
        },
        {
            "title": "Model",
            "content": "IoU PA DiCE"
        },
        {
            "title": "Model",
            "content": "IoU PA DiCE 0.351 0.453 0.400 SAM [63] 0.487 0.499 0.493 CLIPSeg [64] 0.393 0.429 0.428 Grounded-SAM [65] 0.774 0.863 0.855 DenseDiT DenseDiT w/ Mixed Training 0.637 0.694 0.683 0.162 0.415 0.186 SAM [63] 0.597 0.819 0.668 CLIPSeg [64] 0.429 0.469 0.460 Grounded-SAM [65] 0.637 0.774 0.671 DenseDiT DenseDiT w/ Mixed Training 0.677 0.766 0.714 Table 13: Pedestrian Safety Monitor. Table 14: Precise Road Model."
        },
        {
            "title": "Model",
            "content": "IoU PA DiCE"
        },
        {
            "title": "Model",
            "content": "IoU PA DiCE 0.417 0.495 0.478 SAM [63] 0.616 0.717 0.687 CLIPSeg [64] 0.423 0.505 0.478 Grounded-SAM [65] 0.577 0.650 0.614 DenseDiT DenseDiT w/ Mixed Training 0.705 0.769 0.753 0.434 0.465 0.464 SAM [63] 0.439 0.562 0.505 CLIPSeg [64] 0.278 0.472 0.357 Grounded-SAM [65] 0.741 0.806 0.829 DenseDiT DenseDiT w/ Mixed Training 0.489 0.516 0.505 Table 15: Urban Layout Anal. Table 16: Cardiac Screen."
        },
        {
            "title": "Model",
            "content": "IoU PA DiCE"
        },
        {
            "title": "Model",
            "content": "IoU PA DiCE 0.386 0.475 0.437 SAM [63] 0.315 0.616 0.421 CLIPSeg [64] 0.284 0.555 0.390 Grounded-SAM [65] 0.619 0.789 0.757 DenseDiT DenseDiT w/ Mixed Training 0.437 0.524 0.478 0.535 0.732 0.652 SAM [63] 0.439 0.515 0.481 CLIPSeg [64] 0.402 0.478 0.449 Grounded-SAM [65] 0.550 0.667 0.617 DenseDiT DenseDiT w/ Mixed Training 0.693 0.800 0.792 Table 17: Retinal Vessel Anal. Table 18: Spine Morph. Assess."
        },
        {
            "title": "Model",
            "content": "IoU PA DiCE"
        },
        {
            "title": "Model",
            "content": "IoU PA DiCE 0.273 0.594 0.370 SAM [63] 0.472 0.510 0.499 CLIPSeg [64] 0.344 0.376 0.409 Grounded-SAM [65] 0.858 0.906 0.913 DenseDiT DenseDiT w/ Mixed Training 0.638 0.682 0.675 0.555 0.611 0.612 SAM [63] 0.519 0.569 0.551 CLIPSeg [64] 0.464 0.481 0.481 Grounded-SAM [65] 0.934 0.971 0.964 DenseDiT DenseDiT w/ Mixed Training 0.497 0.517 0.508 18 Table 19: Oocyte Detect. Table 20: Nucleus Loc."
        },
        {
            "title": "Model",
            "content": "IoU PA DiCE"
        },
        {
            "title": "Model",
            "content": "IoU PA DiCE 0.952 0.974 0.969 SAM [63] 0.366 0.500 0.422 CLIPSeg [64] 0.390 0.518 0.460 Grounded-SAM [65] DenseDiT 0.930 0.950 0.942 DenseDiT w/ Mixed Training 0.947 0.970 0.968 0.277 0.498 0.336 SAM [63] 0.452 0.511 0.488 CLIPSeg [64] 0.447 0.506 0.496 Grounded-SAM [65] 0.688 0.735 0.745 DenseDiT DenseDiT w/ Mixed Training 0.489 0.545 0.525 Table 21: Water Body Map. Table 22: Auto. Tree Tag."
        },
        {
            "title": "Model",
            "content": "IoU PA DiCE"
        },
        {
            "title": "Model",
            "content": "IoU PA DiCE 0.299 0.415 0.348 SAM [63] 0.172 0.504 0.235 CLIPSeg [64] 0.332 0.435 0.406 Grounded-SAM [65] 0.546 0.634 0.581 DenseDiT DenseDiT w/ Mixed Training 0.437 0.522 0.464 0.765 0.840 0.809 SAM [63] 0.463 0.722 0.579 CLIPSeg [64] 0.391 0.529 0.457 Grounded-SAM [65] 0.858 0.935 0.898 DenseDiT DenseDiT w/ Mixed Training 0.731 0.816 0.771 Table 23: Camouflage Detect. Table 24: Oil Spill Track."
        },
        {
            "title": "Model",
            "content": "IoU PA DiCE"
        },
        {
            "title": "Model",
            "content": "IoU PA DiCE 0.548 0.612 0.601 SAM [63] 0.569 0.753 0.667 CLIPSeg [64] 0.454 0.504 0.497 Grounded-SAM [65] DenseDiT 0.555 0.618 0.610 DenseDiT w/ Mixed Training 0.474 0.523 0.499 0.133 0.437 0.187 SAM [63] 0.427 0.618 0.511 CLIPSeg [64] 0.376 0.490 0.463 Grounded-SAM [65] 0.667 0.770 0.740 DenseDiT DenseDiT w/ Mixed Training 0.408 0.533 0.492 Table 25: Weed Distrib. Map. Table 26: Mask-Wear Monitor."
        },
        {
            "title": "Model",
            "content": "IoU PA DiCE"
        },
        {
            "title": "Model",
            "content": "IoU PA DiCE 0.364 0.399 0.398 SAM [63] 0.609 0.858 0.725 CLIPSeg [64] 0.126 0.420 0.173 Grounded-SAM [65] 0.927 0.945 0.947 DenseDiT DenseDiT w/ Mixed Training 0.786 0.814 0.815 0.540 0.614 0.584 SAM [63] 0.377 0.513 0.469 CLIPSeg [64] 0.337 0.498 0.405 Grounded-SAM [65] DenseDiT 0.334 0.500 0.399 DenseDiT w/ Mixed Training 0.333 0.500 0.398 Table 27: Cigarette Detect. Table 28: Fire Alert Sys."
        },
        {
            "title": "Model",
            "content": "IoU PA DiCE"
        },
        {
            "title": "Model",
            "content": "IoU PA DiCE SAM [63] 0.304 0.308 0.324 CLIPSeg [64] 0.495 0.500 0.497 Grounded-SAM [65] 0.479 0.483 0.486 0.669 0.681 0.683 DenseDiT DenseDiT w/ Mixed Training 0.717 0.975 0.731 SAM [63] 0.528 0.591 0.574 CLIPSeg [64] 0.642 0.753 0.710 Grounded-SAM [65] 0.469 0.530 0.516 0.822 0.926 0.880 DenseDiT DenseDiT w/ Mixed Training 0.491 0.544 0.517 Table 29: Food Cell Inspect. Table 30: Close-Range Segment."
        },
        {
            "title": "Model",
            "content": "IoU PA DiCE"
        },
        {
            "title": "Model",
            "content": "IoU PA DiCE 0.936 0.961 0.956 SAM [63] 0.291 0.500 0.367 CLIPSeg [64] 0.402 0.561 0.487 Grounded-SAM [65] DenseDiT 0.880 0.930 0.929 DenseDiT w/ Mixed Training 0.860 0.916 0.919 0.412 0.478 0.474 SAM [63] 0.429 0.498 0.459 CLIPSeg [64] 0.436 0.566 0.516 Grounded-SAM [65] DenseDiT 0.427 0.507 0.460 DenseDiT w/ Mixed Training 0.448 0.527 0.482 19 B.2 More visualization results We provide additional qualitative results in Figure 9 to Figure 13, covering diverse real-world dense prediction tasks. It can be observed that DenseDiT generalizes well across complex and varied scenarios, consistently producing reliable and structured predictions under challenging conditions. The results highlight DenseDiTs strong practical applicability and robustness in real-world scenarios. Figure 9: Adverse Env. Perception Figure 10: Adverse Env. Perception 20 Figure 11: Adverse Env. Perception Figure 12: Adverse Env. Perception Figure 13: Adverse Env. Perception"
        },
        {
            "title": "C Additional Evaluation Results",
            "content": "To further assess the generalization ability of DenseDiT, we provide additional evaluation results on two real-world dense prediction benchmarks: Global-Scale [29] and Omnicrack30k [12]. These results complement the main experiments and demonstrate the robustness of DenseDiT across diverse, large-scale scenarios. Following the same protocol as in the main experiments, DenseDiT is still trained with only 15 labeled samples per task. Quantitative results are shown in Table 31 and Table 32. Table 31: Quantitative results on the Global-Scale benchmark. Method Precision Recall IoU F1 D-LinkNet [68] NL-LinkNet [69] RCFSNet [70] sam-road [71] DenseDiT 0.099 0.080 0.013 0.031 0.179 0.139 0.151 0.140 0.141 0.234 0.052 0.047 0.011 0.023 0.111 0.095 0.086 0.020 0.043 0.194 Table 32: Quantitative results on the Omnicrack30k benchmark."
        },
        {
            "title": "Method",
            "content": "mIoU mPA mDice cIoU CrackFormer [66] TOPO [67] nnUet [12] CT-CrackSeg [27] DenseDiT 0.503 0.531 0.683 0.531 0.608 0.715 0.649 0.756 0.649 0.635 0.560 0.572 0.732 0.572 0. 0.153 0.158 0.479 0.158 0."
        },
        {
            "title": "D Broader Impacts",
            "content": "This work aims to promote practical, data-efficient dense prediction across diverse real-world scenarios. By addressing challenges in safety, healthcare, environmental monitoring, and other critical applications, it holds potential for positive societal impact. We believe this research provides useful insights for advancing reliable and scalable dense prediction systems in real-world deployments."
        },
        {
            "title": "E Limitations",
            "content": "Task Scope. It is impractical for any benchmark to comprehensively cover all possible dense prediction tasks in the wild, and our DenseWorld is no exception. DenseWorld currently comprises 25 clearly distinguished tasks spanning ecology, healthcare, infrastructure, and other fields, far exceeding the coverage of existing dense prediction datasets, with each task corresponding to range of potential real-world applications. While the scope of real-world tasks is far broader, we prioritize widely encountered and practically important scenarios frequently appearing in public datasets and online repositories, ensuring meaningful coverage. To complement this, our DenseDiT is developed as general-purpose framework without task-specific customization, offering strong extensibility to support future tasks beyond the current benchmark. Computational Efficiency. While DenseDiT maintains lightweight design with less than 0.1% additional parameters, it operates within generative modeling framework, which typically requires more inference steps [23, 33] than specialized discriminative models. Although our experiments suggest that approximately 20 steps achieve favorable trade-off between performance and efficiency, further optimizations could be explored in future work. Recent methods such as Distribution Matching Distillation (DMD) [86] have explored one-step diffusion approaches, yet they still face limitations in balancing performance and speed. Future work could investigate integrating such techniques into dense prediction pipelines built upon generative models."
        }
    ],
    "affiliations": [
        "Xian Jiaotong University"
    ]
}