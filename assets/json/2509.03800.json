{
    "paper_title": "MedVista3D: Vision-Language Modeling for Reducing Diagnostic Errors in 3D CT Disease Detection, Understanding and Reporting",
    "authors": [
        "Yuheng Li",
        "Yenho Chen",
        "Yuxiang Lai",
        "Jike Zhong",
        "Vanessa Wildman",
        "Xiaofeng Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Radiologic diagnostic errors-under-reading errors, inattentional blindness, and communication failures-remain prevalent in clinical practice. These issues often stem from missed localized abnormalities, limited global context, and variability in report language. These challenges are amplified in 3D imaging, where clinicians must examine hundreds of slices per scan. Addressing them requires systems with precise localized detection, global volume-level reasoning, and semantically consistent natural language reporting. However, existing 3D vision-language models are unable to meet all three needs jointly, lacking local-global understanding for spatial reasoning and struggling with the variability and noise of uncurated radiology reports. We present MedVista3D, a multi-scale semantic-enriched vision-language pretraining framework for 3D CT analysis. To enable joint disease detection and holistic interpretation, MedVista3D performs local and global image-text alignment for fine-grained representation learning within full-volume context. To address report variability, we apply language model rewrites and introduce a Radiology Semantic Matching Bank for semantics-aware alignment. MedVista3D achieves state-of-the-art performance on zero-shot disease classification, report retrieval, and medical visual question answering, while transferring well to organ segmentation and prognosis prediction. Code and datasets will be released."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 0 0 8 3 0 . 9 0 5 2 : r MedVista3D: Vision-Language Modeling for Reducing Diagnostic Errors in 3D CT Disease Detection, Understanding and Reporting Yuheng Li Department of Biomedical Engineering Georgia Institute of Technology Atlanta, GA Yenho Chen Department of Machine Learning Georgia Institute of Technology Atlanta, GA Yuxiang Lai Department of Radiation Oncology Emory University School of Medicine, Atlanta, USA Atlanta, GA Jike Zhong Department of Computer Science University of Southern California Los Angeles, CA Vanessa Wildman Department of Radiation Oncology Emory University School of Medicine, Atlanta, USA Atlanta, GA Xiaofeng Yang Department of Radiation Oncology Emory University School of Medicine, Atlanta, USA Atlanta, GA xiaofeng.yang@emory.edu"
        },
        {
            "title": "Abstract",
            "content": "Radiologic diagnostic errorsunder-reading errors, inattentional blindness, and communication failuresremain prevalent in clinical practice. These issues often stem from missed localized abnormalities, limited global context, and variability in report language. These challenges are amplified in 3D imaging, where clinicians must examine hundreds of slices per scan. Addressing them requires systems with precise localized detection, global volume-level reasoning, and semantically consistent natural language reporting. However, existing 3D vision-language models are unable to meet all three needs jointlylacking local-global understanding for spatial reasoning and struggling with the variability and noise of uncurated radiology reports. We present MedVista3D, multi-scale semantic-enriched vision-language pretraining framework for 3D CT analysis. To enable joint disease detection and holistic interpretation, MedVista3D performs local and global image-text alignment for fine-grained representation learning within full-volume context. To address report variability, we apply language model rewrites and introduce Radiology Semantic Matching Bank for semantics-aware alignment. MedVista3D achieves state-of-the-art performance on zero-shot disease classification, report retrieval, and medical visual question answering, while transferring well to organ segmentation and prognosis prediction. Code and datasets will be released. Preprint. Under review."
        },
        {
            "title": "Introduction",
            "content": "Despite decades of clinical experience, radiologic diagnostic errors remain common and pose persistent source of patient harm [4]. In large-scale study [17], three categories of errors were found to be prevalent. Under-reading errors occur when abnormalities are simply missed even within the field of view, often due to insufficient attention to localized findings. Inattentional blindness arise due to tunnel vision or limited global context, missing lesions outside the area of focus or in underexamined slices. Communication failures occur when correctly identified findings are ineffectively conveyed, often due to ambiguous phrasing or inconsistent terminology in the radiology report [35]. Addressing these errors requires systems capable of precise local detection, comprehensive image understanding, and clear, consistent communication of findings. The development of such systems is particularly crucial for 3D medical images, where physicians must examine hundreds of cross-sectional slices which remains both time-consuming and expertisedriven [31]. Fundamentally, radiologic image interpretation spans three related tasks: (1) localized detection of anomalies like tumors or opacities; (2) global understanding of disease patterns across whole volume, which informs tasks like disease classification and report retrieval; and (3) reporting, which involves accurately describing findings and answering clinical questions in natural language. Recent advances in medical vision-language models (VLMs) have shown promise in automating these componentsenabling localized disease identification, global image-report retrieval, zero-shot classification, and report generation or visual question answering [32, 21, 34, 42]. However, current medical VLMs cannot concurrently address these three diagnostic challenges, due to limitations in their training objectives and supervision data. First, existing models lack the capability to jointly perform local detection and global understanding, demonstrating under-reading and inattentional blindness for disease diagnosis. We analyze two state-of-the-art 3D CT VLMs (CTCLIP [11] and fVLM [32]) under local and global settings for disease query. CT-CLIP is trained with global-only objective, aligning entire volumes with full reports. As result, it struggles to identify small and localized abnormalities due to insufficient local alignment. As shown in Figure 1 (top row, second column), its gradient activations focus on irrelevant regions when queried about gallbladder carcinoma, paralleling the under-reading error. Conversely, fVLM aligns organ-level features with their corresponding text descriptions, but lacks mechanism for global understanding. As shown in Figure 1 (bottom row, third column), its activations neglect relevant organs under global query, analogous to inattentional blindness where context beyond the region of focus is neglected. Second, the variability in real-world radiology reports could hinder learning consistent disease representations for effective reporting. Figure 2 (right) shows text examples from large-scale public dataset (e.g. CT-RATE). Empirically, we find that unstructured reports often contain inconsistent interpretations, repetitive phrasing, and vague expressions that fail to clearly convey clinically significant findings, such as lymphadenopathy. These issues degrade the quality of learned representations and introduce ambiguity in downstream tasks such as report generation and visual question answering (VQA). Addressing them requires medical VLMs to combine multi-scale visual grounding with semantically enriched alignment signals. We propose MedVista3D, 3D VLM to enhance the detection, understanding, and reporting of 3D CT image analysis. Our approach learns local-global representations while enhancing the diseasesemantics understanding of the model. First, we derive multi-scale loss that simultaneously aligns CT volumes and organ-level features with their corresponding text descriptions. This maximizes the mutual information shared between CT images and corresponding text descriptions, enabling both local detection and global understanding of the model. We theoretically demonstrate that this multi-scale loss captures more mutual information between global and local images and texts than single-scale alignment loss. dual-pathway vision encoder is used to jointly process global 3D CT volumes and local segmented organs. Second, we improve semantic supervision through multi-scale semantic alignment. We enhance radiology reports via Large Language Model (LLM) rewrites to emphasize the presence or absence of each disease to ensure consistency. We then propose the Radiology Semantic Matching Bank (RSMB) for additional semantic alignment at global and local scales. RSMB retrieves semantically matched disease descriptions via nearest-neighbor search, providing robust text supervision. As shown in Figure 2 (left), our MedVista3D considerably outperforms existing medical vision-language models on global disease zero-shot classification. We summarize the following contributions: Figure 1: We visualize gradient activation maps for both global and local queries on global model (CT-CLIP) and local model (fVLM). Each row shows model attention for either local (top) or global (bottom) query, with ground truth (GT) segmentations color-coded by sentence. The global model fails to detect tumor given local query. The local model does not capture relevant anatomical regions given global query. Our model effectively attends to relevant regions in both cases, demonstrating superior multi-scale understanding. Figure 2: Left: Global zero-shot performance of MedVista3D-ViT on CT-RATE. AUC scores are reported per disease, reflecting the models generalization across diverse pathologies. Right: LLMbased refinement of radiology reports. To address ambiguity and inconsistency in uncurated CT-RATE reports, we apply large language models (e.g., GPT-4o, Qwen2.5) to rewrite them with improved clarity and clinical coherence. We identify the limitations of single-scale training objectives in existing 3D medical VLMs and derive multi-scale alignment loss. We theoretically demonstrate that our loss can capture more shared cross-modal information than single-scale losses. Then, we present our unified objective and the corresponding architecture to jointly learn multi-scale representations from 3D CT volumes. To address the variations in unstructured reports, we introduce multi-scale semantic supervision using LLM-rewritten reports and the Radiology Semantic Matching Bank, which retrieves semantically similar disease texts to enhance contrastive training across scales. We validate MedVista3D through comprehensive experiments across diverse medical tasks (e.g. disease zero-shot detection, report retrieval, medical VQA, organ segmentation, disease classification), achieving state-of-the-art performance through unified alignment."
        },
        {
            "title": "2 Related work",
            "content": "Vision language models for medical imaging. Previous medical VLMs predominantly employ global alignment, contrasting entire images and reports [5, 44, 23, 33, 1, 11]. More recent works introduced local region-text alignment [22, 32] or local token-wise [36, 13] alignment to learn finegrained visual features. However, our investigation reveals large gap between representations learned from global model and local model (Figure 1). We motivate our approach by building multi-scale pretraining method to combine the strengths of each alignment. Multi-scale alignment for VLM. There remains limited research in multi-scale alignment for VLMs. Existing methods [14, 9] focus on multiscale radiography-report alignment but lack the use of region masks or bounding boxes for fine-grained detection. Other approaches [6, 41] combine image-text and region-text alignments using coarse bounding boxes, which are less effective for precise organ localization. In contrast, our work utilizes segmentation masks to extract fine-grained organ features, enabling more accurate local alignment. Improving medical VLMs using synthetic data. Given the scarcity of annotated data and privacy concerns in medical imaging, synthetic data has been widely explored to augment images [18, 27, 7]. few studies explored generating synthetic image or text data to support VLM pretraining [38, 24, 2]. MedKlip [38] extracts named entities from reports and supervises using these disease-specific queries. However, this approach overlooks the context and completeness of query sentence. The closest to our work are local VLMs [32, 22] that learn fine-grained representation using LLMs to decompose long reports into specific regions. However, these works do not learn multi-scale representations for local-global disease understanding, nor do they address the text variations in radiology reports. We tackle this by enhancing disease semantics in reports via LLMs and performing semantic alignment using nearest-neighbor search in the text embedding space."
        },
        {
            "title": "3 Method",
            "content": "Overview. Our approach performs alignment at four levels (Figure 3): (1) global volume with report, (2) local region with text, (3) global volume with semantically enriched report, and (4) local region with semantically enriched sentence. The multi-scale alignment strategy is detailed in Section 3.1, while semantic alignment using LLM-based rewrites and the Radiology Semantic Matching Bank is presented in Section 3.2. The global and local vision-language models used in our framework are described in Appendix C. 3.1 Multi-scale Global and Local Alignment Connection to mutual information maximization (MI). Contrastive vision-language pretraining can be viewed as maximizing the mutual information between the image and text. MI quantifies the shared information between two random variables by measuring how much knowing one variable reduces the uncertainty about the other. Given CT volume as XG and paired report as YG, global alignment maximizes mutual information between full-volume and full-report pairs, IG(XG; YG), where (xg, yg) are volume-report pairs. Similarly, given CT region as XL and region text as YL, local alignment maximizes mutual between organ regions and their descriptions, I(XL; YL), where (xr, yr) are region-text pairs and is the total number of regions defined. From [28], contrastive loss (InfoNCE) estimates lower bound for MI. Extending this theorem to both global and local alignment, we have: I(XG; YG) LGlobal + log(NG), (1) I(XL; YL) LLocal + log(NL), (2) where NG and NL are the number of negative samples in global and local alignment, respectively. LGlobal and LLocal are defined in equation 12 and equation 14 respectively. multi-scale local and global objective. We observe that equations 1 and 2 can only capture partial structure as they focus exclusively on either local or global views separately. Our core insight is that capturing both the holistic contexts and fine-grained details requires maximizing unified mutual information between the full set of global and local CT images = (XL, XG) and text reports = (YL, YG), defined as, IUnified(X, ) = I(XG, XL; YL, YG). (3) 4 Figure 3: a). MedVista3D encodes 3D CT volumes at both global and local scales. For local alignment, visual organ embeddings are paired with organ and semantic-enriched phrases. For global alignment, global volume embedding is matched with the report embedding and its semantic-enriched versions augmented by LLMs. b). radiology semantic matching bank maintains queue of text embeddings from diverse radiology descriptions. For each query, top-k similarity search retrieves semantically matching texts, filtering out less relevant ones. By the chain rule for mutual information, we have IUnified(X, ) max {I(XL; YL), I(XG; YG)}, indicating that the unified objective can capture more shared information between the modalities than considering either global or local inputs alone. This makes it better suited for learning representations that encode both global semantics and local alignment. However, directly optimizing IUnified(X, ) is computationally intractable. Instead, we propose multi-scale contrastive loss that linearly integrates global and local alignment: This objective is part of valid lower bound, LMulti-scale = 1 [LGlobal + LLocal] . IUnified(X, ) LMulti-scale + 1 2 [log(NL) + log(NG)] , (4) (5) and explicitly encourages the learned representation to jointly capture information from both global and local views of the input data. Architecture details. Our MedVista3D is designed with 2 pathways (Figure 3a). For global pathway, given CT volume xi, the model first extracts patch embeddings pi Rcdhw using 3D convolutional layer. Transformer blocks then generate latent image embedding vL , which the final transformer block refines into the global image embedding vG . The radiology report is encoded by text encoder into global text embedding tG . For local pathway, given anatomical region and {0, 1}dhw, its segmentation map matching the patch grid resolution. Active region tokens (threshold 0.5 in i ) are selected from pi element-wise. These are processed by the last transformer block to produce the local image embedding vr . The corresponding region-specific text phrase is encoded into local text embedding tr . This preserves the spatial relationships between global and local embeddings. {0, 1}DHW , mask pooling downsamples it to 3.2 Radiology Semantic Enrichment and Alignment Semantic enrichment of radiology reports. While multi-scale alignment yields richer representations, communication errors could still be caused by directly training on unstructured reports. Free-form radiology reports often suffer from length and inconsistent terminologies (e.g., nodular (6) (7) (8) (9) opacities vs. lesions). To address this, we prompt the LLMs to identify all possible abnormalities from the report and rewrite the findings as discrete, presence-or-absence statements. This process, applied to both global reports and local region phrases, yields standardized, succinct text descriptions where each sentence details at least one abnormality (prompts in Appendix D). Radiology semantics matching bank. Building on these enriched texts, the RSMB provides robust supervision by retrieving semantically similar embeddings, addressing minor wording variations. We observe that enriched texts often describe the same findings with only minor variations in wording (e.g., mild pleural thickening vs. slight pleural thickening). RSMB is 64k-sized first-in-first-out queue storing previously encoded enriched global and local text features. For new enriched query text, its top-1 nearest neighbor (via cosine similarity) is retrieved from RSMB. The corresponding image embedding is then aligned with this retrieved text, ensuring robustness to text variations while maintaining consistent disease semantics. Multi-scale semantic alignment. Using RSMB and the enriched texts, we establish semantic alignment at two levels. For global-level, semantically-enriched text embedding ˆtG queries RSMB to retrieve its nearest neighbor ˆtGN . This embedding is aligned with global image embedding vG using contrastive loss: LGlobal Semantic = LGN IT + LGN I , (cid:16) , ˆtGN sim(vG (cid:16) exp (cid:80)N j=1 exp sim(vG , ˆtGN j (cid:17) )/τ (cid:17) . )/τ LGN IT = 1 (cid:88) i=1 log Similarly for local-level, semantically-enriched region embedding ˆtr ˆtrN , which is aligned with the region image embedding vr : IT + LLN I , (cid:16) LLocal Semantic = LLN LLN IT = 1 RN (cid:88) (cid:88) r=1 i=1 log exp sim(cid:0)vr (cid:16) , ˆtrN (cid:17) )(cid:1)/τ (cid:80)N j=1 exp sim(cid:0)vr , ˆtrN (cid:17) (cid:1)/τ queries RSMB for its neighbor We combine global and local alignment through, LMulti-scale Semantic = LGlobal Semantic + LLocal Semantic. Combining both multi-scale and semantic alignment, our final pretraining objective is defined as, (10) LMedVista3D = LMulti-scale + LMulti-scale Semantic. (11)"
        },
        {
            "title": "4 Experiments",
            "content": "Pretraining dataset and implementation. We pretrain MedVista3D on the CT-RATE dataset [11] using the training split (24,128 volumes) and perform testing on the internal test split (1,564 volumes). Local alignment uses Radgenome masks and region texts [43]. The vision encoder is either ViT [40] or UniMISS [39], and the text encoder is pretrained BERT [3]. See Appendix for details. 4.1 Reducing under-reading and inattentional blindness via MedVista3D We evaluate how MedVista3D reduces under-reading and inattentional blindness by assessing both global understanding and local disease detection from CT volumes. We compare with global VLMstrained on the entire CT volume and corresponding text (e.g., CLIP [29], CT-CLIP [11], Merlin [1])and local VLMs aligning region features with region text (e.g., fVLM [32]). Local Task: Assess under-reading errors by evaluating localized disease detection within anatomical regions (lungs, heart, aorta, and esophagus). 1. Disease zero-shot classification: Given text prompts and segmentation masks, identify the presence of diseases. For global models we crop the CT volume to the segmentation mask and perform padding. Metrics include area under the ROC curve (AUC), balanced accuracy (ACC), precision, and weighted F1-score (F1). Table 1: Performance comparison of VLMs at global tasks and local task on CT-RATE. Blue for global models and green for local model. BOLD means best result and underline second best. : our implementation. : using official checkpoint. Method CLIP [29] Merlin [1] CT-CLIP [11] fVLM [32] fVLM [32] MedVista3D-ViT (ours) MedVista3D-UniMISS (ours) Global Local Disease zero-shot Report retrieval Disease zero-shot Precision ACC AUC Recall 5 Recall 10 Precision ACC AUC 0.334 0.229 0.306 0.293 0.248 0.379 0.385 0.726 0.612 0. 0.684 0.684 0.760 0.770 0.691 0.558 0.651 0.641 0.600 0.737 0.745 0.703 0.578 0. 0.644 0.591 0.778 0.782 2.67% 1.11% 2.34% 1.82% 0.32% 5.00% 2.02% 3.95% 3.06% 1.09% 6.64% 10.68% 8.65% 5.01% 0.306 0.199 0.297 0.372 0.379 0.377 0.372 0.696 0.479 0.678 0.752 0. 0.765 0.754 0.657 0.433 0.636 0.722 0.718 0.742 0.726 0.659 0.538 0.645 0.759 0. 0.780 0.753 Table 2: Generalization of VLMs at global tasks and local task on Rad-ChestCT. Blue for global models and green for local model. BOLD means best result and underline second best. : our implementation. : using official checkpoint. Method CLIP [29] Merlin [1] CT-CLIP [11] fVLM [32] fVLM [32] MedVista3D-ViT (ours) MedVista3D-UniMISS (ours) Global Local Disease zero-shot Disease zero-shot Precision ACC AUC Precision F1 ACC AUC 0.352 0.339 0.339 0.314 0.332 0.426 0.393 0.637 0.605 0.648 0.587 0.561 0.693 0. 0.617 0.581 0.599 0.562 0.535 0.684 0.646 0.609 0.596 0.632 0.518 0.544 0.702 0. 0.321 0.210 0.334 0.315 0.374 0.402 0.378 0.593 0.562 0.608 0.596 0.688 0.681 0. 0.569 0.513 0.584 0.571 0.647 0.668 0.628 0.559 0.552 0.689 0.524 0.680 0.710 0. Global Tasks: Address inattentional blindness by evaluating the models ability to detect findings outside expected regions and retrieve reports correctly describing relevant findings. 1. Disease zero-shot classification: Given text prompts, identify the presence of diseases in the CT volume without segmentation masks. We report the same metrics as in the local task. 2. Report retrieval: Given CT volume, retrieve the corresponding radiology report from the entire dataset. We measure recall at top-5 and top-10. Results for local task. On localized zero-shot detection, both MedVista3D backbones match or surpass fVLM. Importantly, fVLM suffers from poor generalization to global tasks (AUC drops from 0.759 to 0.644), whereas MedVista3D maintains superior performance across both tasks. This demonstrates our models ability to reduce under-reading errors. Results for global tasks. Both MedVista3D-ViT and MedVista3D-UniMISS outperform all global models in disease zero-shot and report retrieval  (Table 1)  . For global disease zero-shot, MedVista3DUniMISS achieves the highest AUC (0.782) and F1 (0.770), outperforming CT-CLIP by 7.4 points in AUC and 6.9 points in F1. For report retrieval, MedVista3D-ViT surpasses CT-CLIP by 4.3% and 6.7% in top-5 and top-10 recall. These results validate our models ability to jointly reduce inattentional blindness and under-reading errors. External validation. To assess generalization, we perform external validation on the full RadChestCT dataset [8] (3626 volumes), following CT-CLIP and fVLM. We evaluate on global and local zero-shot disease detection. Segmentation masks are obtained using TotalSegmentator model [37]. As shown in Table 2, MedVista3D-ViT consistently outperforms existing global models (CLIP, Merlin, CT-CLIP) across all global metrics, achieving an AUC of 0.702. MedVista3D-UniMISS achieves the highest global AUC of 0.713. For local tasks, MedVista3D-ViT also surpasses the fVLM with an AUC of 0.710, demonstrating robust generalization capabilities. Qualitative results. Figure 4 illustrates how region masking modulates the attention map of [CLS]- to-patch tokens in CLIP, fVLM, and our model. CLIP shows broad attention without masking, but it fails to localize the correct region with mask, reflecting under-reading and explaining its poor 7 Table 3: Comparison of various LLaVA architectures on medical VQA (long answer, short answer, report generation, and multiple choice) on CT-RATE. BOLD means best result and underline second best. Blue for 2D MLLMs and green for 3D MLLMs. Method Long Answer Short Answer BLEU_1 METEOR ROUGE_L CIDER BLEU_1 METEOR ROUGE_L CIDER CXR-LLaVA LLaVA-Med CT-CHAT MedVista3D-LLaVA (ours) 0.203 0.137 0.480 0.516 0.140 0.156 0.294 0. 0.231 0.202 0.512 0.546 0.577 0.315 3.100 3.395 0.016 0.014 0.280 0.299 0.000 0.051 0.160 0.178 0.021 0.025 0.598 0.602 0.040 0.007 1.821 1. Method Report Generation Multiple Choice BLEU_1 METEOR ROUGE_L CIDER BLEU_1 METEOR ROUGE_L CIDER CXR-LLaVA LLaVA-Med CT-CHAT MedVista3D-LLaVA (ours) 0.050 0.002 0.381 0. 0.000 0.024 0.217 0.252 0.020 0.056 0.334 0.386 0.049 0.000 0.221 0.349 0.057 0.085 0.838 0.936 0.009 0.175 0.578 0.668 0.063 0.135 0.895 0. 0.065 0.151 7.850 8.210 local detection performance. fVLM attends correctly with mask but fixates on irrelevant, tiny background areas without it, indicating inattentional blindness and poor global understanding. In contrast, MedVista3D demonstrates both fine-grained attention with mask and global attention on the anatomy without mask, effectively mitigating both error types. 4.2 Mitigating communication errors with MedVista3D-LLaVA To evaluate how our model mitigates communication errors in CT reporting, we train MedVista3DLLaVA, multimodal large language model (MLLM), on the CT-RATE VQA dataset. The dataset includes long-answer questions, short-answer questions, multiple-choice questions, and report generation tasks. Following our pretraining setup, we train on the CT-RATE training split and validate on its internal validation split. Evaluation follows CT-CHAT [11], using BLEU, METEOR, ROUGE_L, and CIDER scores. As shown in Table 3, our method consistently outperforms CT-CHAT as well as 2D multimodal assistants (LLaVA-Med [20], CXR-LLaVA [19]) by considerable margins. It achieves the best performance on multiple-choice questions (BLEU_1: 0.936, METEOR: 0.668, ROUGE_L: 0.927, CIDER: 8.21), and surpasses CT-CHAT by 3.6% and 1.9% BLEU_1 on long and short answer tasks, respectively. On the accuracy of multiple choice, our method achieves 91.5%. For report generation, our model shows 9.3-point BLEU_1 improvement. These gains demonstrate the effectiveness of our multi-scale alignment and semantic enrichment of radiology reports, which mitigate potential communication errors in diagnostic workflows. Table 4: Ablation study on multi-scale and semantic image-text alignment. Pretraining Strategy Global Alignment + Local Alignment + Mask Pooling + Global Semantic Alignment + Local Semantic Alignment Region phrase grounding Report retrieval Global disease zero-shot Top 10 0.04% 0.19% 0.48% 0.38% 0.83% Top 50 0.36% 0.76% 2.42% 1.99% 3.46% Top 5 Top 10 Precision ACC AUC 4.53% 7.88% 4.98% 8.32% 4.53% 7.88% 4.98% 8.25% 6.64% 10.68% 0.293 0.281 0.279 0.398 0.379 0.689 0.676 0.674 0.789 0.760 0.633 0.634 0.631 0.758 0.737 0.675 0.664 0.609 0.807 0. 4.3 Ablation Study Ablation on multi-scale and semantic alignment. We conduct detailed analysis on our proposed objective loss on both global and local tasks using CT-RATE. For local task, we evaluate region phrase grounding, where the model retrieves the correct region description given an image and segmentation mask. As summarized in Table 4, we begin with single global-level loss, with moderate global disease zero-shot and report retrieval performance, but lacking local capabilities. We further add local alignment which enables region grounding by explicitly learning organ-level embeddings. Next, we add mask pooling to allow more focused vision features on the segmentation mask, which further improve the region grounding but slightly compromising global zero-shot performance. Adding our proposed semantic alignment at global level considerably boosts global 8 Figure 4: Impact of region masking on attention for CLIP, fVLM and MedVista3D (on CT-RATE). We visualize the attention maps of [CLS] token with other patch tokens given CT volume with (top) and without (bottom) region mask. MedVista3D remains focused on important organs regardless of masking. With mask CLIP shows diffuse attention; fVLM struggles without the mask. disease zero-shot and maintaining decent region retrieval. Finally, adding local semantic alignment yields the best overall balance between global and local tasks. Ablation on mask pooling. We conduct study on the mask pooling mechanism of our method on local disease zero-shot. Specifically, we choose various layers of vision transformer blocks to perform organ mask pooling. As shown in Table 5, we find that applying mask pooling before the last transformer block yields the best performance. Applying mask pooling either before the first block or before the second block yields suboptimal performance. Table 5: Ablation study on transformer block for mask pooling. Table 6: Organ segmentation on TotalSegmentator and prognosis prediction on STOIC. Local disease zero-shot Method TotalSeg (DSC) STOIC (AUC) Layer 1st block 2nd block 12th block Precision F1 ACC AUC 0.336 0.342 0.377 0.709 0.716 0. 0.656 0.666 0.742 0.703 0.732 0.780 nnUNet CT-CLIP Merlin M3D RadFM Ours 0.852 0.805 0.860 0.597 - 0.872 - 0.631 0.782 0.627 0.649 0.807 4.4 Additional Applications Organ Segmentation. MedVista3D learns transferable representations for organ segmentation. We finetune our model using Totalsegmentator [37] which contains 1204 patients and 104 organs, covering wide range of anatomical structures. We attach U-Net decoder to our MedVista3DUniMISS backbone for adaptation to segmentation task. We evaluate the segmentation performance using dice coefficient (DSC). We use nnUNets default 5-fold cross-validation split for training and testing. MedVista3D-UniMISS achieves DSC of 0.872 on TotalSegmentator, outperforming the state-of-the-art nnUNet by 2 points in DSC and Merlin by 1.2 point  (Table 6)  . Prognosis prediction. MedVista3D also enables accurate COVID prognosis prediction. We finetune using STOIC 2021 [30] dataset for pneumonia severity prediction. We randomly select 80% for training, 10% for validation and 10% for testing. linear head is attached for classifying severe or non-severe (defined as death or need for intubation). AUC is used to evaluate the performance. MedVista3D-UniMISS achieves 0.807 AUC outperforming all comparable methods. These results show the adaptability of our method beyond multi-modal tasks."
        },
        {
            "title": "5 Conclusion",
            "content": "We present MedVista3D, 3D VLM using multi-scale semantic alignment to address three major diagnostic errors in radiology: under-reading, inattentional blindness, and communication failures. To jointly support local detection and global understanding from 3D CT volumes, we propose multi-scale alignment loss based on mutual information maximization. To mitigate variability in report language, we leverage LLM-based rewrites and introduce Radiology Semantic Matching Bank for robust semantic alignment. MedVista3D consistently outperforms existing 3D VLMs across multiple downstream tasks, including zero-shot disease classification, report retrieval, and VQA. It also demonstrates strong transferability to organ segmentation and prognosis prediction, highlighting its potential as general-purpose foundation model for 3D medical imaging. Limitation and Future Work. Our current pretraining is limited to chest CT, primarily due to the lack of large-scale, publicly available 3D image-report datasets for other anatomical regions. In future work, we aim to expand MedVista3D to include additional anatomical sites such as the brain, head-and-neck region, and pelvis. Moreover, we plan to extend our framework to other imaging modalities, such as MRI and PET, to further enhance its generalizability across clinical contexts."
        },
        {
            "title": "References",
            "content": "[1] Louis Blankemeier, Joseph Paul Cohen, Ashwin Kumar, Dave Van Veen, Syed Jamal Safdar Gardezi, Magdalini Paschali, Zhihong Chen, Jean-Benoit Delbrouck, Eduardo Reis, Cesar Truyts, et al. Merlin: vision language foundation model for 3d computed tomography. Research Square, pages rs3, 2024. [2] Christian Bluethgen, Pierre Chambon, Jean-Benoit Delbrouck, Rogier van der Sluijs, Małgorzata Połacin, Juan Manuel Zambrano Chaves, Tanishq Mathew Abraham, Shivanshu Purohit, Curtis Langlotz, and Akshay Chaudhari. visionlanguage foundation model for the generation of realistic chest x-ray images. Nature Biomedical Engineering, pages 113, 2024. [3] Benedikt Boecking, Naoto Usuyama, Shruthi Bannur, Daniel Castro, Anton Schwaighofer, Stephanie Hyland, Maria Wetscherek, Tristan Naumann, Aditya Nori, Javier Alvarez-Valle, et al. Making the most of text semantics to improve biomedical visionlanguage processing. In European conference on computer vision, pages 121. Springer, 2022. [4] Michael Bruno, Eric Walker, and Hani Abujudeh. Understanding and confronting our mistakes: the epidemiology of error in radiology and strategies for error reduction. Radiographics, 35(6):16681676, 2015. [5] Geeticka Chauhan, Ruizhi Liao, William Wells, Jacob Andreas, Xin Wang, Seth Berkowitz, Steven Horng, Peter Szolovits, and Polina Golland. Joint modeling of chest radiographs and radiology reports for pulmonary edema assessment. In Medical Image Computing and Computer Assisted InterventionMICCAI 2020: 23rd International Conference, Lima, Peru, October 48, 2020, Proceedings, Part II 23, pages 529539. Springer, 2020. [6] Hong-You Chen, Zhengfeng Lai, Haotian Zhang, Xinze Wang, Marcin Eichner, Keen You, Meng Cao, Bowen Zhang, Yinfei Yang, and Zhe Gan. Contrastive localized language-image pre-training. arXiv preprint arXiv:2410.02746, 2024. [7] Phillip Chlap, Hang Min, Nym Vandenberg, Jason Dowling, Lois Holloway, and Annette Haworth. review of medical image data augmentation techniques for deep learning applications. Journal of Medical Imaging and Radiation Oncology, 65(5):545563, 2021. [8] Rachel Lea Draelos, David Dov, Maciej Mazurowski, Joseph Lo, Ricardo Henao, Geoffrey Rubin, and Lawrence Carin. Machine-learning-based multiple abnormality prediction with large-scale chest computed tomography volumes. Medical image analysis, 67:101857, 2021. [9] Yuexi Du, John Onofrey, and Nicha Dvornek. Multi-view and multi-scale alignment for contrastive language-image pre-training in mammography. arXiv preprint arXiv:2409.18119, 2024. 10 [10] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [11] Ibrahim Ethem Hamamci, Sezgin Er, Chenyu Wang, Furkan Almas, Ayse Gulnihan Simsek, Sevval Nil Esirgun, Irem Doga, Omer Faruk Durugol, Weicheng Dai, Murong Xu, et al. Developing generalist foundation models from multimodal dataset for 3d computed tomography. arXiv preprint arXiv:2403.17834, 2024. [12] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. [13] Shih-Cheng Huang, Liyue Shen, Matthew Lungren, and Serena Yeung. Gloria: multimodal global-local representation learning framework for label-efficient medical image recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 39423951, 2021. [14] Weijian Huang, Cheng Li, Hong-Yu Zhou, Jiarun Liu, Hao Yang, Yong Liang, Guangming Shi, Hairong Zheng, and Shanshan Wang. Enhancing representation in medical vision-language foundation models via multi-scale information extraction techniques. In 2024 IEEE International Symposium on Biomedical Imaging (ISBI), pages 15. IEEE, 2024. [15] Ziyan Huang, Haoyu Wang, Zhongying Deng, Jin Ye, Yanzhou Su, Hui Sun, Junjun He, Yun Gu, Lixu Gu, Shaoting Zhang, et al. Stu-net: Scalable and transferable medical image segmentation models empowered by large-scale supervised pre-training. arXiv preprint arXiv:2304.06716, 2023. [16] Fabian Isensee, Paul Jaeger, Simon AA Kohl, Jens Petersen, and Klaus Maier-Hein. nnu-net: self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2):203211, 2021. [17] Young Kim and Liem Mansfield. Fool me twice: delayed diagnoses in radiology with emphasis on perpetuated errors. American journal of roentgenology, 202(3):465470, 2014. [18] Lennart Koetzier, Jie Wu, Domenico Mastrodicasa, Aline Lutz, Matthew Chung, Adam Koszek, Jayanth Pratap, Akshay Chaudhari, Pranav Rajpurkar, Matthew Lungren, et al. Generating synthetic data for medical imaging. Radiology, 312(3):e232471, 2024. [19] Seowoo Lee, Jiwon Youn, Hyungjin Kim, Mansu Kim, and Soon Ho Yoon. Cxr-llava: multimodal large language model for interpreting chest x-ray images. European Radiology, pages 113, 2025. [20] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training large language-and-vision assistant for biomedicine in one day. Advances in Neural Information Processing Systems, 36, 2024. [21] Xiang Li, Lin Zhao, Lu Zhang, Zihao Wu, Zhengliang Liu, Hanqi Jiang, Chao Cao, Shaochen Xu, Yiwei Li, Haixing Dai, et al. Artificial general intelligence for medical imaging analysis. IEEE Reviews in Biomedical Engineering, 2024. [22] Jingyang Lin, Yingda Xia, Jianpeng Zhang, Ke Yan, Le Lu, Jiebo Luo, and Ling Zhang. Ct-glip: 3d grounded language-image pretraining with ct scans and radiology reports for full-body scenarios. arXiv preprint arXiv:2404.15272, 2024. [23] Weixiong Lin, Ziheng Zhao, Xiaoman Zhang, Chaoyi Wu, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-clip: Contrastive language-image pre-training using biomedical documents. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 525536. Springer, 2023. [24] Che Liu, Zhongwei Wan, Haozhe Wang, Yinda Chen, Talha Qaiser, Chen Jin, Fariba Yousefi, Nikolay Burlutskiy, and Rossella Arcucci. Can medical vision-language pre-training succeed with purely synthetic data? arXiv preprint arXiv:2410.13523, 2024. 11 [25] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. [26] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. [27] Muzaffer Özbey, Onat Dalmaz, Salman UH Dar, Hasan Bedel, Saban Özturk, Alper Güngör, and Tolga Çukur. Unsupervised medical image translation with adversarial diffusion models. IEEE Transactions on Medical Imaging, 2023. [28] Ben Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker. On variational In International Conference on Machine Learning, pages bounds of mutual information. 51715180. PMLR, 2019. [29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. [30] Marie-Pierre Revel, Samia Boussouar, Constance de Margerie-Mellon, Inès Saab, Thibaut Lapotre, Dominique Mompoint, Guillaume Chassagnon, Audrey Milon, Mathieu Lederlin, the stoic project. Radiology, Souhail Bennani, et al. Study of thoracic ct in covid-19: 301(1):E361E370, 2021. [31] Dinggang Shen, Guorong Wu, and Heung-Il Suk. Deep learning in medical image analysis. Annual review of biomedical engineering, 19(1):221248, 2017. [32] Zhongyi Shui, Jianpeng Zhang, Weiwei Cao, Sinuo Wang, Ruizhe Guo, Le Lu, Lin Yang, Xianghua Ye, Tingbo Liang, Qi Zhang, and Ling Zhang. Large-scale and fine-grained visionlanguage pre-training for enhanced ct image understanding. In The Thirteenth International Conference on Learning Representations, 2025. [33] Samuel Stevens, Jiaman Wu, Matthew Thompson, Elizabeth Campolongo, Chan Hee Song, David Edward Carlyn, Li Dong, Wasila Dahdul, Charles Stewart, Tanya Berger-Wolf, et al. Bioclip: vision foundation model for the tree of life. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1941219424, 2024. [34] Omkar Thawkar, Abdelrahman Shaker, Sahal Shaji Mullappilly, Hisham Cholakkal, Rao Muhammad Anwer, Salman Khan, Jorma Laaksonen, and Fahad Shahbaz Khan. Xraygpt: Chest radiographs summarization using medical vision-language models. arXiv preprint arXiv:2306.07971, 2023. [35] Stephen Waite, Jinel Moore Scott, Ian Drexler, Jennifer Martino, Alan Legasto, Brian Gale, and Srinivas Kolla. Communication errors in radiologypitfalls and how to avoid them. Clinical imaging, 51:266272, 2018. [36] Fuying Wang, Yuyin Zhou, Shujun Wang, Varut Vardhanabhuti, and Lequan Yu. Multigranularity cross-modal alignment for generalized medical visual representation learning. Advances in Neural Information Processing Systems, 35:3353633549, 2022. [37] Jakob Wasserthal, Hanns-Christian Breit, Manfred Meyer, Maurice Pradella, Daniel Hinck, Alexander Sauter, Tobias Heye, Daniel Boll, Joshy Cyriac, Shan Yang, et al. Totalsegmentator: robust segmentation of 104 anatomic structures in ct images. Radiology: Artificial Intelligence, 5(5), 2023. [38] Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Medklip: Medical knowledge enhanced language-image pre-training for x-ray diagnosis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2137221383, 2023. [39] Yutong Xie, Jianpeng Zhang, Yong Xia, and Qi Wu. Unimiss: Universal medical self-supervised learning via breaking dimensionality barrier. In European Conference on Computer Vision, pages 558575. Springer, 2022. 12 [40] Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, ShangWen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph Feichtenhofer. Demystifying clip data. arXiv preprint arXiv:2309.16671, 2023. [41] Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun Chen, Liunian Harold Li, Xiyang Dai, Lijuan Wang, Lu Yuan, Jenq-Neng Hwang, and Jianfeng Gao. Glipv2: unifying localization and vl understanding. In Proceedings of the 36th International Conference on Neural Information Processing Systems, pages 3606736080, 2022. [42] Kai Zhang, Jun Yu, Eashan Adhikarla, Rong Zhou, Zhiling Yan, Yixin Liu, Zhengliang Liu, Lifang He, Brian Davison, Xiang Li, et al. Biomedgpt: unified and generalist biomedical generative pre-trained transformer for vision, language, and multimodal tasks. arXiv e-prints, pages arXiv2305, 2023. [43] Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Jiayu Lei, Ya Zhang, Yanfeng Wang, and Weidi Xie. Radgenome-chest ct: grounded vision-language dataset for chest ct analysis. arXiv preprint arXiv:2404.16754, 2024. [44] Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya Zhang, Yanfeng Wang, and Weidi Xie. Development of large-scale medical visual question-answering dataset. Communications Medicine, 4(1):277, 2024."
        },
        {
            "title": "NeurIPS Paper Checklist",
            "content": "1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the papers contributions and scope? Answer: [Yes] Justification: The claims made in the abstract and introduction are aligned with the actual contributions and findings presented in the paper. Each of these claims is supported by theoretical analysis, ablation studies, and experimental results. Guidelines: The answer NA means that the abstract and introduction do not include the claims made in the paper. The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. No or NA answer to this question will not be perceived well by the reviewers. The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: See conclusion section. Guidelines: The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. The authors are encouraged to create separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on few datasets or with few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach. For example, facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, worse outcome might be that reviewers discover limitations that arent acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3. Theory assumptions and proofs Question: For each theoretical result, does the paper provide the full set of assumptions and complete (and correct) proof? Answer: [Yes] Justification: See section 3.1 and 3.2. Guidelines: The answer NA means that the paper does not include theoretical results. All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. All assumptions should be clearly stated or referenced in the statement of any theorems. The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide short proof sketch to provide intuition. Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. Theorems and Lemmas that the proof relies upon should be properly referenced. 4. Experimental result reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: See Appendix for detailed descriptions of model architecture, training settings, hyperparameter. Guidelines: The answer NA means that the paper does not include experiments. If the paper includes experiments, No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. If the contribution is dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is novel architecture, describing the architecture fully might suffice, or if the contribution is specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to hosted model (e.g., in the case of large language model), releasing of model checkpoint, or other means that are appropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is new model (e.g., large language model), then there should either be way to access this model for reproducing the results or way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. 5. Open access to data and code 15 Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We will publicly release the codebase and model checkpoints upon acceptance for publication, along with detailed instructions for reproducing the main experimental results. Guidelines: The answer NA means that paper does not include experiments requiring code. Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. While we encourage the release of code and data, we understand that this might not be possible, so No is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for new open-source benchmark). The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only subset of experiments are reproducible, they should state which ones are omitted from the script and why. At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. 6. Experimental setting/details Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Hyperparameter settings are reported for each experiment, and additional implementation details are provided in the Appendix B. Guidelines: The answer NA means that the paper does not include experiments. The experimental setting should be presented in the core of the paper to level of detail that is necessary to appreciate the results and make sense of them. The full details can be provided either with the code, in appendix, or as supplemental material. 7. Experiment statistical significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: We do not perform statistical significance testing in this work due to (1) the high computational cost of repeated runs and (2) the robustness of our dataset, which includes 24,000 samples. Guidelines: The answer NA means that the paper does not include experiments. The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. 16 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). The method for calculating the error bars should be explained (closed form formula, call to library function, bootstrap, etc.) The assumptions made should be given (e.g., Normally distributed errors). It should be clear whether the error bar is the standard deviation or the standard error of the mean. It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report 2-sigma error bar than state that they have 96% CI, if the hypothesis of Normality of errors is not verified. For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 8. Experiments compute resources Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: See Appendix B. Guidelines: The answer NA means that the paper does not include experiments. The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didnt make it into the paper). 9. Code of ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have reviewed the NeurIPS Code of Ethics and confirm that our research conforms to its principles. All data used in this study are from publicly available, deidentified medical datasets, and no personally identifiable information (PII) was accessed or used. Guidelines: The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. If the authors answer No, they should explain the special circumstances that require deviation from the Code of Ethics. The authors should make sure to preserve anonymity (e.g., if there is special consideration due to laws or regulations in their jurisdiction). 10. Broader impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: See Appendix A. Guidelines: The answer NA means that there is no societal impact of the work performed. If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how system learns from feedback over time, improving the efficiency and accessibility of ML). 11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [Yes] Justification: We use only publicly available and de-identified medical datasets to ensure compliance with privacy regulations. For model release, we will provide pretrained weights under research license that includes responsible use guidelines, including prohibitions on clinical deployment without regulatory approval. Guidelines: The answer NA means that the paper poses no such risks. Released models that have high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make best faith effort. 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We use publicly available datasets and models whose licenses and sources are properly cited in the paper. Guidelines: The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include URL. 18 The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from particular source (e.g., website), the copyright and terms of service of that source should be provided. If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of dataset. For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. If this information is not available online, the authors are encouraged to reach out to the assets creators. 13. New assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We introduce several new assets as part of this work, including pretrained MedVista3D model checkpoints, LLM-rewritten radiology reports for CT-RATE. These assets will be released with accompanying documentation upon paper acceptance. Guidelines: The answer NA means that the paper does not release new assets. Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. The paper should discuss whether and how consent was obtained from people whose asset is used. At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. 14. Crowdsourcing and research with human subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing or research with human subjects. All data used are from publicly available, de-identified medical datasets with appropriate licenses and do not involve any direct interaction with individuals. Guidelines: The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional review board (IRB) approvals or equivalent for research with human subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This study does not involve human subjects or direct human participation. Therefore, IRB approval was not required. Guidelines: The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. 16. Declaration of LLM usage Question: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research? Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientific rigorousness, or originality of the research, declaration is not required. Answer: [Yes] Justification: Large language models such as GPT-4o and Qwen2.5, were used to rewrite radiology reports for improving semantic clarity during pretraining. See method section and Figure 2. Guidelines: The answer NA means that the core method development in this research does not involve LLMs as any important, original, or non-standard components. Please refer to our LLM policy (https://neurips.cc/Conferences/2025/LLM) for what should or should not be described."
        },
        {
            "title": "A Broader Impact",
            "content": "Reproducibility statement. We are committed to efficient and reproducible research. Our code and datasets will be publicly released. Potential benefits. MedVista3D could support radiologists by improving diagnostic accuracy, automating report generation, and medical image segmentation. Potential risks. Using large language models for rewriting medical reports may inadvertently introduce hallucinated content if not properly validated. While our approach is intended for training supervision, not clinical deployment, future work should explore robust validation pipelines to detect hallucinations, ensure factual consistency, and maintain clinical reliability. We emphasize that such systems should only be used in practice with rigorous testing, explainability safeguards, and alignment with domain expertise."
        },
        {
            "title": "B Implementation details",
            "content": "Pretraining on CT-RATE: For dataset preprocessing, we use volumes resampled to 3.0 mm 1.0 mm 1.0 mm from Radgenome dataset [43]. We also use its segmentation masks and region sentences for regional image-text alignment. For intensity normalization, we follow CT-CLIP [11] preprocessing. We uniformly resize all volumes to 96 320 320 using padding or center cropping. For text encoder, we use BiomedVLP-CXR-BERT-specialized [3]. For vision encoder, we use 1). ViT-B, with embedding dimension as 768 and depth as 12; and 2). UniMISS-Small [39]. We train MedVista3D-ViT using batch size of 32 and MedVista3D-UniMISS using batch size of 20 for total of 16 epochs on our proposed loss. The optimizer is AdamW with the weight decay of 1e-5. 20 We use linear warmup with cosine decay scheduler for 200 steps and learning rate of 5e-5. All experiments were conducted using NVIDIA A100 GPUs (80GB) on an internal cluster. Visual question answering using MedVista3D-LLaVA: We use Llama-3.1-7B [10] as the language decoder and the pretrained MedVista3D-ViT as the vision encoder. For multi-modal projector, we use two-layer MLP-GELU following LLaVA-1.5 [25]. We follow the two-stage training strategy same as LLaVA: 1). First, we perform contrastive alignment using CT-RATEs volume-report pairs to tune the multi-modal projector; 2). Second, we perform supervised finetuning using LoRA [12] with rank set to 128, scaling factor α set to 256, and learning rate of 2e-5. We train total of 10 epochs following CT-CHAT. Segmentation on TotalSegmentator: We adopt the pretrained MedVista3D-UniMISS as the segmentation encoder. We attach STU-Net-Bs decoder [15] to our encoder. We use nn-UNet [16] to preprocess the TotalSegmentator dataset and train within their framework. We use learning rate of 5e-5 and batch size of 2. Input volumes are uniformly cropped to 128 128 128. We train for total of 1000 epochs following the default setting. Classification on STOIC 2021: MedVista3D-UniMISS is initialized with pretrained CT-RATE weights. We resample CT volumes to 3.0 mm 1.0 mm 1.0 mm and crop/pad to 96 320 320. From the full 2000 volumes, we randomly select 80% for training, 10% for validation and 10% for testing. We use batch size of 96, learning rate of 1e-4 and finetune for 10 epochs."
        },
        {
            "title": "C Global vs Local Alignment",
            "content": "Global alignment. Contrastive VLM aims to learn positive-negative image-text embeddings by jointly training an image encoder fI() and text encoder fT(). One common approach is global image-text alignment, such as CLIP [29]. Given dataset of pairs of CT image volumes and their corresponding radiology reports, = {x1, . . . , xP } and = {y1, . . . , yP }, the global embeddings for the ith volume-report pair can be obtained as vG = fT(yi), where xi R1DHW and yi Rl represent the dimensions of the input CT volume and radiology report, respectively. To align image and text representations, contrastive objective pushes the embeddings of matched volume-report pairs together while pushing those of unmatched pairs apart. Using InfoNCE loss [26], the global alignment objective becomes, (cid:3) , = fI(xi) and tG (cid:2)LG (12) IT + LG The first term consists of the global image-to-text loss, LG LGlobal = I 1 2 LG IT = 1 (cid:88) i=1 log IT , and is defined as, )/τ (cid:1) , tG exp (cid:0)sim(vG , tG j=1 exp (cid:0)sim(vG )/τ (cid:1) , (cid:80)N (13) where is the batch size and sim(, ) is the similarity function and τ is learnable logit. We omit LG since it is symmetric. Local alignment. However, global approach can overlook fine-grained similarities or differences among various organs. Alternatively, local image-text alignment identifies all possible regions in the CT image and extracts region-specific features [32, 22]. Assuming the CT image can be divided into image regions x1 , and radiology reports can also be decomposed into fine-grained captions , . . . , yr y1 , yr )}. For region r, fI extracts local image embedding vr . The local alignment loss can be defined as: ), . . . , (xr and fT extracts local text embeddings tL describing each organ, region-text pairs can be formed as {(x , . . . , xr , y1 LLocal = 1 2 (cid:2)LL IT + LL I (cid:3) , The local image-to-text loss can be written as: LL IT = 1 RN (cid:88) (cid:88) r=1 i=1 log (cid:16) exp (cid:17) sim(cid:0)vr (cid:16) , tr sim(cid:0)vr )(cid:1)/τ , tr (cid:1)/τ (cid:80)N j=1 exp (14) (15) (cid:17) , where is the total number of regions. However, local alignment methods often lack broader contextual information and requires anatomical priors (i.e. segmentation masks), which may not always be feasible in clinical settings."
        },
        {
            "title": "D Prompting LLMs for improving disease semantics",
            "content": "Figure 5: Prompts for report-level rewrites to emphasize disease presences. Figure 6: Prompts for region-level rewrite with few-shot prompting."
        },
        {
            "title": "E Training algorithm",
            "content": "The training pseudo code of MedVista3D is shown below: )B )B , yG Algorithm 1 MedVista3D Require: (xG , ˆyr , yr i=1, (xr 1: function COMPUTE_MEDVISTA3D_LOSS(fI , fT ) ), fT (yG ) 2: ), fT (yr ) 3: ), fT (ˆyG ) 4: ), fT (ˆyr ) 5: fI (xG fI (xr fI (xG fI (xr vG , tG , tr vr , ˆtG vG , ˆtr vr i=1, (xG i=1, (xr , ˆyG )B )B i=1, fI , fT , RSMB Global features. Local features. Global semantic features. Local semantic features. 6: 7: 8: 9: 10: 11: ˆtGN Top-1 nearest neighbor of ˆtG ˆtrN Top-1 nearest neighbor of ˆtr from RSMB Global semantic query with RSMB. Local semantic query with RSMB. from RSMB Compute LGlobal from (vG , ˆtrN , ˆtGN Compute LGlobal Semantic from (vG Compute LMedVista3D from LGlobal, LLocal, LGlobal Semantic and LLocal Semantic. , tr ) ) and LLocal Semantic from (vr ) and LLocal from (vr , tG ) Backward LMedVista3D and update fI , fT Calculate the losses. Update the network. RSMB Queue_Update(RSMB, ˆtG ) RSMB Queue_Update(RSMB, ˆtr ) batch size of ˆti ptr next free position in RSMB length of RSMB if ptr + then 12: 13: 14: end function 15: 16: function QUEUE_UPDATE(RSMB, ˆti) 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: end function RSM B[:, ptr : ptr + B] ˆti ptr ptr + end if return RSMB else RSM B[:, ptr : S] ˆti[:, 0 : (S ptr)] ptr 0 Update RSMB. Queue size is exceeded. Fill remaining slots. Reset pointer to the start. Push embeddings into the queue. Advance pointer by the batch size. Return updated RSMB."
        }
    ],
    "affiliations": [
        "Department of Biomedical Engineering, Georgia Institute of Technology, Atlanta, GA",
        "Department of Computer Science, University of Southern California, Los Angeles, CA",
        "Department of Machine Learning, Georgia Institute of Technology, Atlanta, GA",
        "Department of Radiation Oncology, Emory University School of Medicine, Atlanta, USA"
    ]
}