{
    "paper_title": "VADER: Towards Causal Video Anomaly Understanding with Relation-Aware Large Language Models",
    "authors": [
        "Ying Cheng",
        "Yu-Ho Lin",
        "Min-Hung Chen",
        "Fu-En Yang",
        "Shang-Hong Lai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video anomaly understanding (VAU) aims to provide detailed interpretation and semantic comprehension of anomalous events within videos, addressing limitations of traditional methods that focus solely on detecting and localizing anomalies. However, existing approaches often neglect the deeper causal relationships and interactions between objects, which are critical for understanding anomalous behaviors. In this paper, we propose VADER, an LLM-driven framework for Video Anomaly unDErstanding, which integrates keyframe object Relation features with visual cues to enhance anomaly comprehension from video. Specifically, VADER first applies an Anomaly Scorer to assign per-frame anomaly scores, followed by a Context-AwarE Sampling (CAES) strategy to capture the causal context of each anomalous event. A Relation Feature Extractor and a COntrastive Relation Encoder (CORE) jointly model dynamic object interactions, producing compact relational representations for downstream reasoning. These visual and relational cues are integrated with LLMs to generate detailed, causally grounded descriptions and support robust anomaly-related question answering. Experiments on multiple real-world VAU benchmarks demonstrate that VADER achieves strong results across anomaly description, explanation, and causal reasoning tasks, advancing the frontier of explainable video anomaly analysis."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 9 9 2 7 0 . 1 1 5 2 : r VADER: Towards Causal Video Anomaly Understanding with Relation-Aware Large Language Models Ying Cheng1, Yu-Ho Lin1, Min-Hung Chen2, Fu-En Yang2, Shang-Hong Lai1* 1National Tsing Hua University 2NVIDIA"
        },
        {
            "title": "Abstract",
            "content": "tions, causal reasoning, and context-aware explanations. Video anomaly understanding (VAU) aims to provide detailed interpretation and semantic comprehension of anomalous events within videos, addressing limitations of traditional methods that focus solely on detecting and localizing anomalies. However, existing approaches often neglect the deeper causal relationships and interactions between objects, which are critical for understanding anomalous behaviors. In this paper, we propose VADER, an LLM-driven framework for Video Anomaly unDErstanding, which integrates keyframe object Relation features with visual cues to enhance anomaly comprehension from video. Specifically, VADER first applies an Anomaly Scorer to assign per-frame anomaly scores, followed by ContextAwarE Sampling (CAES) strategy to capture the causal context of each anomalous event. Relation Feature Extractor and COntrastive Relation Encoder (CORE) jointly model dynamic object interactions, producing compact relational representations for downstream reasoning. These visual and relational cues are integrated with LLMs to generate detailed, causally grounded descriptions and support robust anomaly-related question answering. Experiments on multiple real-world VAU benchmarks demonstrate that VADER achieves strong results across anomaly description, explanation, and causal reasoning tasks, advancing the frontier of explainable video anomaly analysis. 1. Introduction Video anomaly detection (VAD) plays critical role in surveillance, traffic monitoring, and public safety. Conventional methods primarily focus on temporal localization and classification [13, 28, 29, 40, 46, 53, 54, 71], but often lack semantic interpretability, limiting their utility in decisionmaking. To address this, recent research has shifted from video anomaly detection to video anomaly understanding (VAU) [34, 47, 50, 57, 67], emphasizing detailed descrip- *Corresponding author Figure 1. Concept of VADER. VADER enables detailed anomaly understanding by extracting key visual and relational cues from selected key frames. Recent advances in Large Language Models (LLMs) [3, 16, 21, 30, 31, 36, 61] have enabled natural language understanding of video content, paving the way for explainable VAU. Several works have leveraged LLMs for openworld VAU, including causation-focused pipeline [11], interaction-aware framework [47], and temporal segment selection [67]. However, these approaches often neglect deeper causal relationships and dynamic object interactions, which are critical for understanding unusual behaviors. In this work, we propose VADER, an LLM-driven framework for Video Anomaly unDErstanding, integrating keyframe-level object Relation features and visual cues to enhance anomaly comprehension in video, as illustrated in Figure 1. VADER first applies an Anomaly Scorer to assign per-frame anomaly scores, followed by ContextAwarE Sampling (CAES) strategy that adaptively selects keyframes to capture the full causal context of each event. For each sampled keyframe, Relation Feature Extractor provides rich object and relational representations, which are further aggregated by COntrastive Relation Encoder (CORE) into compact relational tokens that encode salient and temporally resolved interaction dynamics. By integrating both visual and relational cues into the LLM, VADER enables detailed, causally consistent narrative generation and robust anomaly-related question answering. We extensively evaluate VADER on three recent and challenging VAU benchmarks, covering tasks such as anomaly description, question answering, and causal reasoning. Our method outperforms strong baselines and recent state-of-the-art systems across multiple metrics. Ablation analyses further highlight the impact of each major component. In summary, our main contributions are as follows: We introduce VADER, an integrated framework that combines context-aware event sampling with dynamic relational modeling to support causally grounded video anomaly understanding with LLMs. We propose Context-AwarE Sampling strategy (CAES) based on anomaly scores and temporal gradients, enabling story-driven keyframe selection. We develop weakly supervised COntrastive Relation Encoder (CORE) that produces dynamic tokens for evolving object interactions, enhancing both interpretability and reasoning depth. We validate VADER on multiple challenging benchmarks, achieving state-of-the-art or highly competitive performance in video anomaly description, explanation, and reasoning tasks. 2. Related Work 2.1. Video Anomaly Detection and Understanding Video Anomaly Detection (VAD) aims to identify events that deviate from typical patterns in videos. Early approaches [23, 37, 42, 63] relied on handcrafted features and traditional algorithms to capture motion or appearance anomalies. With the advent of deep learning, CNNs and RNNs have been widely adopted to model spatiotemporal dynamics [10, 27, 48, 71] and enhance detection performance. Recent methods [14, 35, 38, 72, 72, 73] often utilize reconstruction-based or weakly supervised approaches to address these tasks, aiming to learn normal patterns and detect deviations as anomalies. To facilitate progress in this field, several datasets [2, 7, 27, 33, 46, 52, 59] have been introduced, covering diverse video content and annotation granularity. Beyond VAD, Video Anomaly Understanding (VAU) involves deeper analysis, such as describing anomaly contents or investigating their contexts. With recent advancements in text generation and multimodal learning, VAU research has seen significant progress. Several benchmarks such as [11, 47, 62, 67] have been developed. Building on these resources, VAU emerged as new task, and traditional VAD was further extended with MLLMbased approaches; we review these in the following section. 2.2. Video Anomaly Analysis with MLLMs Recent advances in Multimodal Large Language Models (MLLMs) [3, 21, 26, 74] have greatly expanded the scope of video anomaly analysis. By jointly modeling visual content and natural language, MLLMs enable capabilities such as open-vocabulary anomaly detection and reasoning [5, 56]. Video-capable MLLMs [16, 20, 30, 31, 36] further enhance these abilities by reasoning over video data. Related works on VAD include LAVAD [65] and SUVAD [12] for training-free anomaly detection; AnomalyRuler [57] with rule-based reasoning; VAD-LLaMA [34] and VERA [60] as an explainable detector. Works on VAU include Holmes-VAU [67] with an anomaly-focused sampling; CUVA [11] with causation-focused reasoning; HAWK [47] with interaction-aware modeling; and AssistPDA [58] for real-time analysis. Despite these advances, existing approaches still overlook the deeper causal relationships and interactions between objects, which are critical for understanding anomalous behaviors. 2.3. Modeling Relationships with Scene Graphs Understanding object relationships is vital for high-level video analysis. Many anomalies arise from unusual interactions or collective behaviors, such as person running against crowd, which cannot be fully captured by appearance features alone. Scene graphs provide structured representation of these interactions by modeling objects as nodes and relationships as edges, enabling models to move beyond low-level features toward relational understanding. EGTR [17] extracts relational information from DETR [6] for efficient graph generation, while DecoAD [8] integrates scene and action features via knowledge graphs to better detect human-centric anomalies. Lohner et al. [32] further showed that adding encoded scene graph features improves detection performance, highlighting the value of explicit relational modeling. Building on these advances, we leverage [17] to extract object-level relations and integrate them into our framework, enabling more interpretable and comprehensive anomaly understanding. 3. Method In this section, we present VADER, novel framework for deep, causal understanding of anomalous events in complex videos. Unlike traditional anomaly detection approaches, VADER is designed to interpret and explain both the occurrence and underlying causes of anomalous events. Our framework consists of two main components: (1) keyframe detection and selection (Sec 3.1) for narrative-driven context sampling, and (2) relation extraction and integration (Sec 3.2) for fine-grained object interaction modeling. An overview of our pipeline is shown in Figure 2. Figure 2. Overview of VADER framework. Given an input video, the Anomaly Scorer and Context-AwarE Sampling (CAES) identify keyframes for narrative-driven anomaly analysis. Visual and relational features are extracted and encoded, with dynamic relational patterns distilled by the COntrastive Relation Encoder (CORE). All cues are fused by pretrained LLM for comprehensive video anomaly understanding. The right panel illustrates the relational branch, including temporal association, volatility mining, and contrastive token learning. 3.1. Keyframe Detection and Selection To focus computational resources on the most informative video content, VADER first assigns an anomaly score to each frame using an Anomaly Scorer and then applies keyframe selection strategy to filter and refine the candidate frames for downstream analysis. 3.1.1. Anomaly Scoring To build robust and widely applicable system, we leverage an Anomaly Scorer inspired by the CLIP-based framework [64]. Rather than training specialized model for each target dataset [72, 73], our scorer is pre-trained once on large, aggregated dataset encompassing diverse video scenarios. Following [64], we compute normality prototype by averaging features extracted by the CLIP [41] image encoder EI on all normal training video frames Inorm : = 1 Inorm (cid:88) xInorm EI (x), (1) Both visual and text features are then re-centered by subtracting m, aligning the origin with normal behavior, where distances indicate abnormality and directions encode semantics. The re-centered frame features are then projected onto class-specific semantic directions dc and passed through softmax to produce conditional class distribution pcA(Ii), representing the probability of each anomaly class given that an anomaly occurs. The per-frame anomaly probability pA(Ii), estimated by temporal module that captures shortand long-term dependencies across frame sequences, is combined with the conditional class distribution pcA(Ii) to form joint probability. The frame-level anomaly score is computed as: Si = max (cid:0)pA(Ii) pcA(Ii)(cid:1), (2) The resulting anomaly scores are used to guide keyframe selection, while the predicted anomaly classes provide semantic cues for downstream reasoning by LLMs. 3.1.2. Context-AwarE Sampling (CAES) To capture the narrative structure of anomalous events, we introduce CAES, Context-AwarE keyframe Sampling strategy that preserves both causal context and event diversity. Formally, = CAES(S), (3) where is the set of selected keyframes and is the anomaly score sequence. As illustrated in Figure 3, CAES first detects all anomalous intervals using percentile-based adaptive threshold for each video, then automatically expands each event into pre-event and post-event context segments by applying rise and calm thresholds to the slope of anomaly scores. Keyframes are then uniformly sampled from pre-event, onevent, and post-event segments to cover the causal lead-up, climax, and aftermath of each anomaly, ensuring comprehensive yet compact representation. If multiple anomalies are detected or the total exceeds the frame budget (e.g., 64), frames with the highest anomaly scores are prioritized. Any remaining slots are filled by Anomaly Score On-Event Pre-Event Rise threshold Post-Event Rising Slope (pre-event) Peak (on-event) Falling Slope (post-event) Frame Calm threshold Figure 3. Illustration of CAES keyframe selection strategy. The anomaly score curve is segmented into pre-event (blue), on-event (yellow), and post-event (green) intervals. Blue dots are sampled keyframes, and red dots indicate rise and calm thresholds. sampling from background segments to ensure temporal coherence. This approach ensures comprehensive yet compact representation for downstream narrative reasoning. 3.2. Relation Extraction and Integration While CAES identifies what and when to analyze, the core of our VADER framework lies in its ability to understand how the relationships between objects dynamically evolve. Moving beyond static scene description, VADER first extracts rich object and relational features from each selected frame, then automatically discovers temporal patterns of interaction using only video-level labels. Finally, it distills these patterns into compact dynamic representations and integrates them into the main LLM for deep, causallygrounded reasoning about complex events. 3.2.1. Relational Feature Extraction For each keyframe selected by CAES, we utilize pretrained DETR-based scene graph generation model [17] as feature extractor to extract appearance embeddings for detected objects and relational tensor. This tensor contains the intermediate features computed for each object pair before the relationship classification head, serving as comprehensive snapshot of the scenes relational dynamics. To maintain consistent object identities across frames, we perform object association using combination of IoU overlap and cosine similarity of appearance embeddings. To capture temporal changes in these interactions, we compute relational volatility curve for each video by tracking objects and, for each pair of adjacent frames, taking the maximum L2 norm of the difference between relational features for all co-tracked object pairs: Volatility(t) = max (i,j) rij(t) rij(t 1)2 , (4) This sequence of volatility scores, together with the associated before/after relational vectors, is cached for efficient downstream processing. 3.2.2. COntrative Relation Encoder (CORE) Based on the relational volatility curves, we automatically mine and encode salient patterns of object interaction change in weakly-supervised manner. To ensure robust peak detection, each volatility curve is first smoothed with Gaussian filter. For abnormal videos, relational-change pairs, i.e., [rij(t 1); rij(t)], corresponding to the top k% of these smoothed peaks are designated as positive samples, while pairs from volatility valleys and from normal videos serve as negatives."
        },
        {
            "title": "To distill",
            "content": "these transitions into compact representations, we train lightweight COntrastive Relation Encoder (CORE) that maps each relational-change pair to compact relation tokens. To train CORE, we employ triplet margin loss, formulated as Ltriplet = max (0, d(fa, fp) d(fa, fn) + α) , (5) where fa, fp, and fn denote the anchor, positive, and negative samples encoded by CORE, d(, ) is the L2 distance, and α is the margin. We further adopt semi-hard negative mining [43] to improve discriminative power. 3.3. LLM Integration and Fine-tuning For each video, keyframes selected by CAES are encoded into visual tokens and passed through the Relation Feature Extractor to obtain raw relational tensors, from which relation tokens are generated by CORE. Task instructions are converted into text tokens. All tokens are concatenated and jointly fed into the Multimodal LLM, enabling the model to reason over both visual content and the evolving object relationships that underlie complex events. During training, we fine-tune only the multimodal projectors and LoRA [15] adapters, keeping the backbone frozen. This setup allows VADER to align both static and dynamic video cues with causal, semantically rich natural language output. As result, the model can generate detailed, context-aware anomaly descriptions and perform robust anomaly-related reasoning. Implementation Details. Including hyperparameters, threshold values, and training configurations, are provided in the Supplementary Material. 4. Experiments In this section, we present the experimental results of applying the proposed VADER model to three latest comprehensive benchmark datasets commonly used for evaluating VAU methods. We also compare the experimental results with the SOTA methods and discuss some ablation studies on the proposed method. 4.1. Benchmark Datasets and Evaluation Metrics To evaluate the validity of VADER, we use three latest comprehensive VAU benchmarks: HIVAU-70k [67], HAWK Benchmark BLEU ROUGE CIDEr METEOR MoverScore GPT Score MMEval Method Metric Description Causes Effect Lexical-level Semantic-level Judge-based HIVAU-70k HAWK CUVA mPLUG-owl [61] Table 1. Evaluation metrics for each benchmark. Metric types are grouped into lexical-level (blue), semantic-level (yellow), and judge-based (green). Video-LLAMA [66] BLEU ROUGE BLEURT MoverScore UniEval MMEval BLEU ROUGE BLEURT MoverScore UniEval MMEval BLEU ROUGE BLEURT MoverScore UniEval MMEval BLEU ROUGE BLEURT MoverScore UniEval MMEval PandaGPT [45] Otter [20] Video-ChatGPT [36] BLEU CUVA [11] VADER (Ours) ROUGE BLEURT MoverScore UniEval MMEval BLEU ROUGE BLEURT MoverScore UniEval MMEval BLEU ROUGE BLEURT MoverScore UniEval MMEval 0.55 12.58 40.66 51.97 67.46 73. 0.60 13.15 40.55 51.32 52.28 65.65 0.66 13.33 38.23 51.73 57.05 74.19 1.07 15.19 29.92 53.54 45.14 76.30 0.30 9.75 46.83 50.73 70.82 78.55 0.55 14.35 47.10 52.25 68.18 79.65 1.09 12.85 32.34 52.65 85.00 78. 0.65 13.54 43.28 52.71 62.29 17.15 0.53 12.36 43.02 51.25 47.29 16.24 0.51 14.09 43.95 51.54 54.88 22.47 1.09 15.87 32.52 54.25 49.05 3.53 0.29 9.08 49.52 50.70 70.77 44.57 0.51 9.08 48.13 52.28 63.41 58. 1.19 16.84 37.48 52.78 79.37 66.30 0.47 8.83 37.95 50.06 59.07 44.31 0.35 8.02 39.68 49.48 43.03 32.84 0.30 8.79 39.95 49.62 50.84 69.45 1.11 11.40 28.94 51.91 47.51 39.21 0.41 8.23 37.24 49.83 54.35 46. 0.38 8.23 48.28 49.95 51.87 50.64 1.11 17.38 37.60 53.21 82.06 63.26 Table 2. Evaluation on the CUVA benchmark. VADER shows competitive results across anomaly description and causation. Bold indicate the best performance, and underlined indicate the second best. Models marked with are finetuned on CUVA; those marked with employ prompt or adapter tuning; all others are evaluated without domain-specific finetuning. MMEval, indicating its strength in generating coherent and accurate causal explanations beyond lexical overlap. Performance on HIVAU-70k. As shown in Table 3, VADER achieves state-of-the-art results across all metrics and evaluation levels (clip, event, and video). The per- [47], and CUVA[11]. HIVAU-70k [67] is large-scale dataset comprising videos from UCF-Crime [55] and XD-Violence [46], providing hierarchical annotations at clip, event, and video levels for both perception and reasoning tasks. HAWK [47] focuses on open-world video anomaly understanding, from seven diverse datasets [2, 7, 27, 33, 46, 52, 59], and supports both event description and question answering tasks. collecting videos CUVA [11] targets causation understanding in realworld anomaly videos, offering detailed annotations that explain what happened, why, and how for each event. (1) Lexical-level metrics, We adopt the evaluation metrics following the official protocols of each benchmark. These can be categorized into three groups: including BLEU [39], ROUGE [25], METEOR [4], and CIDEr [49], measure n-gram overlap, precision, recall, and synonym matching with reference texts. (2) Semantic-level metrics, such as BLEURT [44] and MoverScore [70], use pretrained language models to capture meaning and contextual similarity beyond exact word matches. (3) Judge-based evaluative metrics, including GPT score [1] and MMEval [11], directly use LLMs or VLMs as evaluators to judge plausibility, informativeness, and consistency of the generated explanations with video content, simulating human judgment in open-ended settings. All metrics are reported such that higher scores indicate better performance. Table 1 summarizes the metrics used for each benchmark. 4.2. Experimental Results We present quantitative evaluations of VADER across three challenging benchmarks, highlighting its robust and superior performance. Performance on CUVA. Table 2 demonstrates VADERs competitive performance across anomaly description and causation tasks. VADER achieves the highest MMEval score for Causes task with an improvement of 7.38 points and maintains comparable score on the other tasks. While VADERs BLEURT scores are slightly lower than some baselines, this is due to BLEURTs focus on surface-level wording similarity. In contrast, VADER excels on human-aligned metrics such as UniEval and Method BLEU CIDEr METEOR ROUGE V V V V Video-ChatGPT [36] Video-LLAMA [66] Video-LLAVA [24] LLAVA-Next-Video [69] QwenVL2 [51] InternVL2 [9] NVILA [20] Holmes-VAU [67] VADER (Ours) VADER (Ours) 0.152 0.151 0.164 0.435 0.312 0.331 0.610 0.913 1.035 1.266 0.068 0.079 0.046 0.091 0.082 0.101 0.340 0.804 1.163 1. 0.066 0.104 0.055 0.120 0.155 0.145 0.283 0.566 1.068 1.268 0.033 0.024 0.032 0.102 0.044 0.052 0.261 0.467 0.612 1.040 0.011 0.014 0.009 0.015 0.020 0.022 0.154 1.519 1.650 1.763 0.013 0.017 0.013 0.031 0.044 0.035 0.098 1.437 1.403 1.812 0.102 0.112 0.097 0.117 0.133 0.141 0.157 0.190 0.215 0.247 0.069 0.076 0.022 0.085 0.092 0.095 0.096 0.165 0.183 0. 0.044 0.057 0.014 0.096 0.112 0.101 0.072 0.121 0.137 0.164 0.153 0.156 0.132 0.198 0.163 0.182 0.273 0.329 0.376 0.429 0.048 0.067 0.023 0.080 0.081 0.102 0.218 0.370 0.444 0.463 0.079 0.090 0.045 0.106 0.137 0.122 0.198 0.355 0.408 0.446 Table 3. Evaluation on the HIVAU-70k benchmark. The dataset supports hierarchical evaluation at the clip (C), event (E), and video (V) levels. VADER achieves consistently strong results across metrics and levels. Models marked with are finetuned on HIVAU-70k; those marked with are finetuned and use the same backbone size as baseline [67]; all others are evaluated without domain-specific finetuning. Anomaly Video Description Generation Anomaly Video Question-Answering Text-Level GPT-Guided Text-Level GPT-Guided Method BLEU-1 BLEU-2 BLEU-3 BLEU-4 Reason. Detail Consist. BLEU-1 BLEU-2 BLEU-3 BLEU-4 Reason. Detail Consist. Video-ChatGPT [36] VideoChat [22] Video-LLAMA [66] LLAMA-Adapter [68] Video-LLAVA [24] NVILA [20] HAWK [47] VADER (Ours) 0.107 0.053 0.062 0.132 0.071 0.132 0.270 0.324 0.046 0.023 0.025 0.052 0.030 0.058 0.139 0.196 0.017 0.008 0.009 0.018 0.012 0.016 0.074 0.127 0.008 0.003 0.004 0.008 0.005 0.003 0.043 0. 0.084 0.107 0.120 0.060 0.077 0.355 0.283 0.428 0.108 0.205 0.217 0.091 0.115 0.527 0.320 0.442 0.055 0.054 0.066 0.038 0.038 0.299 0.218 0.357 0.177 0.261 0.156 0.199 0.094 0.003 0.319 0.484 0.096 0.133 0.081 0.109 0.054 0.001 0.179 0.311 0.058 0.074 0.045 0.067 0.034 0.001 0.112 0. 0.038 0.043 0.027 0.043 0.023 0.001 0.073 0.150 0.508 0.699 0.586 0.646 0.393 0.428 0.840 0.828 0.430 0.631 0.485 0.559 0.274 0.304 0.794 0.825 0.421 0.598 0.497 0.549 0.316 0.362 0.753 0.794 Table 4. Evaluation on the HAWK benchmark. VADER achieves leading results in both anomaly description generation and questionanswering tasks across text-level and GPT-guided metrics. Bold indicate the best performance, and underlined indicate the second best. Models marked with are finetuned on HAWK; all others are evaluated without domain-specific finetuning. formance gains are particularly significant at the event and video levels, highlighting VADERs ability to reason over complex, temporally extended anomalous events and generate coherent, causally-aware narratives. Performance on HAWK. Results presented in Table 4 illustrate VADERs strong performance in anomaly description and question-answering tasks. For description generation, VADER surpasses previous best models by 0.045 in BLEU-1 and 0.026 in BLEU-4. For the question-answering task, VADER demonstrates even greater improvements, outperforming prior methods by 0.164 in BLEU-1 and 0.077 in BLEU-4. Additionally, it achieves leading GPTguided scores, notably in Detail (description task) and Consistency (QA task), underscoring its ability to generate detailed, plausible, and contextually consistent narratives. These results underscore VADERs versatility and high performance across diverse evaluation scenarios. 4.3. Ablation Study We conduct ablation studies on the HAWK [47] description task to systematically assess the contribution of each component in VADER. Impact of Key Components. As shown in Table 5, removing the relation reasoning branch, CORE, leads to consistent drop in both text-level and GPT-guided metrics, confirming the importance of modeling object interactions. Disabling the proposed context-aware sampling strategy, CAES, also degrades performance across all metrics, further validating its effectiveness. Without fine-tuning, performance drops significantly, though the GPT-guided detail score is anomalously high, likely because the model generates lengthy but less focused responses due to the lack of domain adaptation. Effect of Sampling Strategy. Table 6 compares our CAES with alternative keyframe selection methods. Uniform sampling results in lower BLEU and ROUGE scores, as it may overlook key anomaly frames. Top-K focuses on frames with the highest anomaly scores, often overconcentrating on anomalous segments and neglecting essential context. The ATS method [67] adaptively samples frames based on anomaly scores using density-aware approach; however, in our setting, it shows only marginal Text-Level GPT-Guided Text-Level GPT-Guided Method BLEU ROUGE Reasonability Detail Consistency Method BLEU ROUGE Reasonability Detail Consistency VADER w/o CORE w/o CAES w/o Fine-Tuning 0.718 0.668 0.594 0. 0.283 0.274 0.244 0.143 0.428 0.419 0.387 0.355 0.442 0.477 0.435 0.527 0.357 0.343 0.331 0.299 Uniform Top-K ATS [67] CAES (Ours) 0.594 0.663 0.641 0. 0.244 0.273 0.273 0.274 0.387 0.381 0.383 0.419 0.435 0.411 0.423 0.477 0.331 0.327 0.335 0.343 Table 5. Ablation study on key components of VADER. Removing either CORE, CAES, or fine-tuning leads to performance drops in both text-level and GPT-guided metrics, underscoring the importance of each module to the overall performance. Table 6. Comparison of keyframe selection strategies. Our Context-AwarE Sampling (CAES) outperforms uniform, Top-K, and ATS approaches across text-level and GPT-guided metrics. Text-Level GPT-Guided Text-Level GPT-Guided Context Sampling Method BLEU ROUGE Reasonability Detail Consistency Method BLEU ROUGE Reasonability Detail Consistency Fixed window Exponential interval Dynamic window 0.647 0.665 0.668 0.273 0.278 0.274 0.419 0.392 0.419 0.477 0.421 0. 0.343 0.342 0.343 Relational visual cue Scene graph text Relation token 0.670 0.686 0.718 0.276 0.277 0.283 0.385 0.391 0.428 0.430 0.434 0. 0.326 0.335 0.357 Table 7. Ablation study on context sampling strategies in CAES. Dynamic window sampling achieves the best overall performance across both text-level and GPT-guided metrics. Table 8. Comparison of relation representations. Using relation tokens as relation representations outperforms both relational visual cue and scene graph text approaches across all text-level and GPT-guided metrics. text frames in CAES. Fixed Window samples consecutive frames immediately before the anomaly, which may lead to high redundancy and limited temporal scope. Exponential Interval samples at increasing intervals to provide multiscale context, but may still miss critical transitions. Our Dynamic Window adaptively determines context boundaries based on anomaly score gradients, enabling event-specific, informative context coverage. The results show that Dynamic Window achieves strong performance across metrics, highlighting the value of adaptive, narrative-aware context selection. Relation Representation. Table 8 compares different strategies for encoding relational information. Using only relational visual cues encodes the detected objects static visual features and locations, but fails to capture interactions or evolving relationships between objects. Scene graph text descriptions leverage the relationship labels predicted by the pretrained EGTR [17] on Visual Genome [18], but these generic labels are not tailored for the nuanced, context-specific interactions common in anomaly scenarios. In contrast, our relation token, generated by CORE, provides continuous and fine-grained temporal cues, yielding consistently better results across all metrics. 4.4. Qualitative Results Figure 4 illustrates qualitative example that highlights VADERs strengths in causal reasoning. Given video of nighttime break-in, VADER offers detailed description, root cause, and outcome summary of the anomalous event. Figure 4. Qualitative results of VADERs causal reasoning capabilities. Given video depicting nighttime break-in, VADER generates detailed, context-aware answers for sequence of causal reasoning queries, capturing both the key actions and underlying cause-effect chains within the event. gains over Top-K on text-level metrics, likely due to limited context diversity in complex scenarios. Our CAES achieves the best trade-off across both text-level and GPTguided metrics, effectively balancing anomaly saliency and context diversity to produce coherent event narratives. Effect of Context Sampling Strategies. Table 7 compares different approaches for selecting pre/post-event conFigure 5. Three examples of the task of describing anonymous videos are depicted here. The descriptions generated by Otter [20] and Video-ChatGPT [36] contain hallucination or incorrect analysis. In contrast, VADER produces concise, contextually grounded, and causally coherent descriptions that accurately reflect the events and their underlying dynamics across various challenging cases. Its responses capture not only key visual cues such as the broken window and scattered belongings, but also the underlying object-level interactions, such as the intruders actions in the environment, demonstrating the benefit of our keyframe selection and relational modeling. Figure 5 further compares VADER with existing MLLMs [20, 36] on diverse real-world anomalous videos. Across various scenarios, physical altercations, road accidents, and complex crowd behaviors, VADER produces more accurate, context-aware, and semantically coherent descriptions than recent baselines. These results illustrate VADERs ability to mitigate hallucinations, capture causal structure, and provide comprehensive explanations, thereby advancing explainable video anomaly understanding. 5. Limitation While VADER advances video anomaly understanding, several limitations remain: Dependency on Upstream Models. VADER relies on upstream modules such as the Anomaly Scorer and Relation Feature Extractor, where errors in detection or association can propagate and affect the final reasoning results. Inherent Bias towards High-Motion Events. The reliance on relational volatility as the core signal introduces bias toward anomalies with strong motion, making the framework less sensitive to subtle or low-motion anomalies. Limitation to Object-Centric View. VADERs current design is object-centric, which limits its ability to capture scene-level or group-level anomalies such as environmental changes or collective crowd behaviors. Detailed potential mitigation strategies are discussed in Supplementary Material Sec. D. 6. Conclusion In this work, we presented VADER, novel framework for video anomaly understanding that leverages Context-AwarE Sampling (CAES) and dynamic relational modeling to deliver detailed, causally grounded interpretations of anomalous events. Extensive experiments on multiple challenging benchmarks demonstrate that VADER achieves state-of-the-art or highly competitive performance across anomaly description, causal explanation, and question answering tasks. Systematic ablation studies further highlight the critical roles of adaptive context sampling and relational dynamics encoding."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 5 [2] Andra Acsintoae, Andrei Florescu, Mariana-Iuliana Georgescu, Tudor Mare, Paul Sumedrea, Radu Tudor Ionescu, Fahad Shahbaz Khan, and Mubarak Shah. Ubnormal: New benchmark for supervised open-set video the IEEE/CVF anomaly detection. conference on computer vision and pattern recognition, pages 2014320153, 2022. 2, 5 In Proceedings of [3] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. 1, 2 [4] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pages 6572, 2005. 5 [5] Yunkang Cao, Xiaohao Xu, Yuqi Cheng, Chen Sun, Zongwei Du, Liang Gao, and Weiming Shen. Personalizing visionlanguage models with hybrid prompts for zero-shot anomaly detection. IEEE Transactions on Cybernetics, 2025. 2 [6] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-toend object detection with transformers. In European conference on computer vision, pages 213229. Springer, 2020. 2 [7] Antoni Chan and Nuno Vasconcelos. Modeling, clustering, and segmenting video with mixtures of dynamic texIEEE transactions on pattern analysis and machine tures. intelligence, 30(5):909926, 2008. 2, [8] Chenglizhao Chen, Xinyu Liu, Mengke Song, Luming Li, Shaojiang Yuan, Xu Yu, and Shanchen Pang. Unveiling context-related anomalies: Knowledge graph empowered decoupling of scene and action for human-related video anomaly detection. IEEE Transactions on Circuits and Systems for Video Technology, 2025. 2 [9] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. Science China Information Sciences, 67(12):220101, 2024. 6 [10] Yong Shean Chong and Yong Haur Tay. Abnormal event detection in videos using spatiotemporal autoencoder. In International symposium on neural networks, pages 189196. Springer, 2017. 2 [11] Hang Du, Sicheng Zhang, Binzhu Xie, Guoshun Nan, Jiayang Zhang, Junrui Xu, Hangyu Liu, Sicong Leng, Jiangming Liu, Hehe Fan, Dajiu Huang, Jing Feng, Linli Chen, Can Zhang, Xuhuan Li, Hao Zhang, Jianhang Chen, Qimei Cui, and Xiaofeng Tao. Uncovering what, why and how: comprehensive benchmark for causation understanding of In 2024 IEEE/CVF Conference on Comvideo anomaly. puter Vision and Pattern Recognition (CVPR), pages 18793 18803, 2024. 1, 2, 5, 12, 13 [12] Shibo Gao, Peipei Yang, and Linlin Huang. Suvad: Semantic understanding based video anomaly detection using mllm. In ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2025. [13] Mariana-Iuliana Georgescu, Antonio Barbalau, Radu Tudor Ionescu, Fahad Shahbaz Khan, Marius Popescu, and Mubarak Shah. Anomaly detection in video via selfIn Proceedings of supervised and multi-task learning. the IEEE/CVF conference on computer vision and pattern recognition, pages 1274212752, 2021. 1 [14] Mahmudul Hasan, Jonghyun Choi, jan Neumann, Amit Roy-Chowdhury, and Larry Davis. Learning temporal regularity in video sequences. In Proceedings of IEEE Computer Vision and Pattern Recognition, 2016. 2 [15] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. 4, 15 [16] Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et al. Minicpm: Unveiling the potential of small arXiv language models with scalable training strategies. preprint arXiv:2404.06395, 2024. 1, 2 [17] Jinbae Im, JeongYeon Nam, Nokyung Park, Hyungmin Lee, and Seunghyun Park. Egtr: Extracting graph from transIn Proceedings of the former for scene graph generation. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2422924238, 2024. 2, 4, 7 [18] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123(1):3273, 2017. [19] Harold Kuhn. The hungarian method for the assignment problem. Naval research logistics quarterly, 2(1-2):8397, 1955. 15 [20] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Joshua Adrian Cahyono, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Otter: multi-modal model with in-context IEEE Transactions on Pattern Analysis instruction tuning. and Machine Intelligence, 2025. 2, 5, 6, 8, 15 [21] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with In Infrozen image encoders and large language models. ternational conference on machine learning, pages 19730 19742. PMLR, 2023. 1, 2 [22] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. 6 [23] Weixin Li, Vijay Mahadevan, and Nuno Vasconcelos. Anomaly detection and localization in crowded scenes. IEEE transactions on pattern analysis and machine intelligence, 36(1):1832, 2013. [24] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual represenarXiv preprint tation by alignment before projection. arXiv:2311.10122, 2023. 6 [25] Chin-Yew Lin. Rouge: package for automatic evaluation In Text summarization branches out, pages of summaries. 7481, 2004. 5 [26] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. 2 [27] Wen Liu, Weixin Luo, Dongze Lian, and Shenghua Gao. Future frame prediction for anomaly detectiona new baseline. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 65366545, 2018. 2, 5 [28] Wenrui Liu, Hong Chang, Bingpeng Ma, Shiguang Shan, and Xilin Chen. Diversity-measurable anomaly detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1214712156, 2023. 1 [29] Zhian Liu, Yongwei Nie, Chengjiang Long, Qing Zhang, and Guiqing Li. hybrid video anomaly detection framework via memory-augmented flow reconstruction and flow-guided frame prediction. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1358813597, 2021. [30] Zuyan Liu, Yuhao Dong, Ziwei Liu, Winston Hu, Jiwen Lu, and Yongming Rao. Oryx mllm: On-demand spatial-temporal understanding at arbitrary resolution. arXiv preprint arXiv:2409.12961, 2024. 1, 2 [31] Zhijian Liu, Ligeng Zhu, Baifeng Shi, Zhuoyang Zhang, Yuming Lou, Shang Yang, Haocheng Xi, Shiyi Cao, Yuxian Gu, Dacheng Li, et al. Nvila: Efficient frontier visual language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 41224134, 2025. 1, 2, 14 [32] Aaron Lohner, Francesco Compagno, Jonathan Francis, and Alessandro Oltramari. Enhancing vision-language models with scene graphs for traffic accident understanding. In 2024 IEEE International Automated Vehicle Validation Conference (IAVVC), pages 17. IEEE, 2024. 2 [33] Cewu Lu, Jianping Shi, and Jiaya Jia. Abnormal event detection at 150 fps in matlab. In Proceedings of the IEEE international conference on computer vision, pages 27202727, 2013. 2, 5 [34] Hui Lv and Qianru Sun. Video anomaly detection and arXiv preprint explanation via large language models. arXiv:2401.05702, 2024. 1, [35] Hui Lv, Zhongqi Yue, Qianru Sun, Bin Luo, Zhen Cui, and Hanwang Zhang. Unbiased multiple instance learning for weakly supervised video anomaly detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 80228031, 2023. 2 [36] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL 2024), 2024. 1, 2, 5, 6, 8 [37] Ramin Mehran, Alexis Oyama, and Mubarak Shah. Abnormal crowd behavior detection using social force model. In 2009 IEEE conference on computer vision and pattern recognition, pages 935942. IEEE, 2009. 2 [38] Rashmiranjan Nayak, Umesh Chandra Pati, and Santos Kumar Das. comprehensive review on deep learning-based Image and Vision methods for video anomaly detection. Computing, 106:104078, 2021. 2 [39] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311318, 2002. 5 [40] Hyunjong Park, Jongyoun Noh, and Bumsub Ham. Learning In Promemory-guided normality for anomaly detection. ceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1437214381, 2020. 1 [41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [42] Venkatesh Saligrama and Zhu Chen. Video anomaly detection based on local statistical aggregates. In 2012 IEEE Conference on computer vision and pattern recognition, pages 21122119. IEEE, 2012. 2 [43] Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: unified embedding for face recognition and clustering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 815823, 2015. 4, 15 [44] Thibault Sellam, Dipanjan Das, and Ankur Parikh. Bleurt: Learning robust metrics for text generation. arXiv preprint arXiv:2004.04696, 2020. 5 [45] Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. Pandagpt: One model to instruction-follow them all. arXiv preprint arXiv:2305.16355, 2023. 5 [46] Waqas Sultani, Chen Chen, and Mubarak Shah. Real-world anomaly detection in surveillance videos. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 64796488, 2018. 1, 2, 5 [47] Jiaqi Tang, Hao Lu, Ruizheng Wu, Xiaogang Xu, Ke Ma, Cheng Fang, Bin Guo, Jiangbo Lu, Qifeng Chen, and YingCong Chen. Hawk: Learning to understand open-world video anomalies. In Neural Information Processing Systems (NeurIPS), 2024. 1, 2, 5, 6, 14 [48] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spatiotemporal features with 3d convolutional networks. In Proceedings of the IEEE international conference on computer vision, pages 44894497, 2015. [49] Ramakrishna Vedantam, Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluaIn Proceedings of the IEEE conference on computer tion. vision and pattern recognition, pages 45664575, 2015. 5 [50] Hao Wang, Jiayou Qin, Ashish Bastola, Xiwen Chen, John Suchanek, Zihao Gong, and Abolfazl Razi. Visiongpt: Llmassisted real-time anomaly detection for safe visual navigation. arXiv preprint arXiv:2403.12415, 2024. 1 [51] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 6 [52] Shu Wang and Zhenjiang Miao. Anomaly detection in crowd scene. In IEEE 10th International Conference on Signal Processing Proceedings, pages 12201223. IEEE, 2010. 2, 5 [53] Xuanzhao Wang, Zhengping Che, Bo Jiang, Ning Xiao, Ke Yang, Jian Tang, Jieping Ye, Jingyu Wang, and Qi Qi. Robust unsupervised video anomaly detection by multipath frame prediction. IEEE transactions on neural networks and learning systems, 33(6):23012312, 2021. [54] Jhih-Ciang Wu, He-Yen Hsieh, Ding-Jie Chen, Chiou-Shann Fuh, and Tyng-Luh Liu. Self-supervised sparse representation for video anomaly detection. In European Conference on Computer Vision, pages 729745. Springer, 2022. 1 [55] Peng Wu, Jing Liu, Yujia Shi, Yujia Sun, Fangtao Shao, Zhaoyang Wu, and Zhiwei Yang. Not only look, but also listen: Learning multimodal violence detection under weak In European conference on computer vision, supervision. pages 322339. Springer, 2020. 5 [56] Ruiyao Xu and Kaize Ding. Large language models for anomaly and out-of-distribution detection: survey. arXiv preprint arXiv:2409.01980, 2024. 2 [57] Yuchen Yang, Kwonjoon Lee, Behzad Dariush, Yinzhi Cao, and Shao-Yuan Lo. Follow the rules: Reasoning for video In Proanomaly detection with large language models. ceedings of the European Conference on Computer Vision (ECCV), 2024. 1, 2 [58] Zhiwei Yang, Chen Gao, Jing Liu, Peng Wu, Guansong Pang, and Mike Zheng Shou. Assistpda: An online video surveillance assistant for video anomaly prediction, detection, and analysis. arXiv preprint arXiv:2503.21904, 2025. 2 [59] Yu Yao, Xizi Wang, Mingze Xu, Zelin Pu, Yuchen Wang, Ella Atkins, and David Crandall. Dota: Unsupervised detection of traffic anomaly in driving videos. IEEE transactions on pattern analysis and machine intelligence, 45(1): 444459, 2022. 2, 5 [60] Muchao Ye, Weiyang Liu, and Pan He. Vera: Explainable video anomaly detection via verbalized learning of visionlanguage models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 86798688, 2025. 2 [61] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023. 1, [62] Tongtong Yuan, Xuange Zhang, Kun Liu, Bo Liu, Chen Chen, Jian Jin, and Zhenzhen Jiao. Towards surveillance video-and-language understanding: New dataset, baselines, and challenges, 2023. 2 [63] Andrei Zaharescu and Richard Wildes. Anomalous behaviour detection using spatiotemporal oriented energies, subset inclusion histogram comparison and event-driven processing. In European Conference on Computer Vision, pages 563576. Springer, 2010. 2 [64] Luca Zanella, Benedetta Liberatori, Willi Menapace, Fabio Poiesi, Yiming Wang, and Elisa Ricci. Delving into clip latent space for video anomaly recognition. Computer Vision and Image Understanding, 249:104163, 2024. 3 [65] Luca Zanella, Willi Menapace, Massimiliano Mancini, Yiming Wang, and Elisa Ricci. Harnessing large language models for training-free video anomaly detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1852718536, 2024. 2 [66] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. 5, 6 [67] Huaxin Zhang, Xiaohao Xu, Xiang Wang, Jialong Zuo, Xiaonan Huang, Changxin Gao, Shanjun Zhang, Li Yu, and Nong Sang. Holmes-vau: Towards long-term video anomaly In Proceedings of the understanding at any granularity. Computer Vision and Pattern Recognition Conference, pages 1384313853, 2025. 1, 2, 4, 5, 6, 7, 12, [68] Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, and Yu Qiao. Llama-adapter: Efficient fine-tuning of arXiv preprint language models with zero-init attention. arXiv:2303.16199, 2023. 6 [69] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data, 2024. 6 [70] Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian Meyer, and Steffen Eger. Moverscore: Text generation evaluating with contextualized embeddings and earth mover distance. arXiv preprint arXiv:1909.02622, 2019. 5 [71] Yiru Zhao, Bing Deng, Chen Shen, Yao Liu, Hongtao Lu, and Xian-Sheng Hua. Spatio-temporal autoencoder for video anomaly detection. In Proceedings of the 25th ACM international conference on Multimedia, pages 19331941, 2017. 1, 2 [72] Hang Zhou, Junqing Yu, and Wei Yang. Dual memory units with uncertainty regulation for weakly supervised video anomaly detection. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 37693777, 2023. 2, 3 [73] Yixuan Zhou, Yi Qu, Xing Xu, Fumin Shen, Jingkuan Song, and Heng Tao Shen. Batchnorm-based weakly supervised video anomaly detection. IEEE Transactions on Circuits and Systems for Video Technology, 2024. 2, 3 [74] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023."
        },
        {
            "title": "Supplementary Material",
            "content": "A. Illustrative Example of Motivation B. Ablation Study on Sample Mining Hyperparameters C. Failure Cases 12 12 D. Potential Mitigations for Limitations 13 D.1. Mitigating Dependency on Upstream Modules 13 13 D.2. Reducing Bias Toward High-Motion Events . 13 D.3. Moving Beyond Object-Centric Reasoning . E. Computational Efficiency F. Intermediate Visualization of Module Contributions G. Implementation Details 14 14 15 A. Illustrative Example of Motivation To better illustrate the motivation behind our work, Figure 6 presents real-world anomalous event where dog suddenly attacks boy walking on the roadside. This example demonstrates the challenges in understanding not only what happens in the scene, but also how it occurs, which requires modeling both causal relationships and dynamic object interactions. In this case, different methods generate diverse outputs when asked to describe the anomalous segment: CUVA [11] focuses on basic visual elements but produces description that does not align with the actual event. While certain textual metrics such as ROUGE are relatively high, the description lacks factual accuracy and fails to capture the core abnormal interaction. Holmes-VAU [67] includes some relevant context, such as the presence of danger and dog, but its narrative remains ambiguous. The description does not clearly reflect the cause-effect chain or the temporal progression of the event, limiting interpretability. VADER (Ours) explicitly models the interactions and causal dynamics between objects, resulting in coherent description that clearly states who was involved and how the anomaly unfolded. This produces outputs that align well with human judgments, as shown by higher semantic-level and human-evaluation metrics (e.g., BLEURT, UniEval, and mmEval). The quantitative metrics and qualitative feedback below each output further highlight the importance of deeper reasoning. While lexical overlap metrics alone may not fully capture factual correctness, human-aligned evaluations reflect the clarity and interpretability of the descriptions. VADER achieves the highest ratings by providing precise and logically structured explanation of the anomalous event. B. Ablation Study on Sample Mining Hyperparameters We perform an ablation study on the Gaussian smoothing parameter (σ) and the peak selection threshold (top-k percentile) in our weakly-supervised sample mining pipeline. As shown in Table 9, we report AUC scores on the test set for different combinations of σ and top-k. Our results reveal two main findings. First, with σ = 2.0, top-k percentile of 5% yields the best AUC (72.12). Increasing the threshold to 7% slightly reduces performance, likely due to the inclusion of less informative (noisy) samples. Conversely, stricter threshold of 3% also degrades performance, suggesting that overly selective sampling yields too few positives for effective training. Second, across all thresholds, σ = 2.0 consistently outperforms both smaller (σ = 1.0) and larger (σ = 3.0) values. We attribute this to improved noise filtering at σ = 2.0 without excessive smoothing that would obscure salient events. In summary, σ = 2.0 and top-k = 5% strike the best balance between sample quality and quantity, and are used in all subsequent experiments. Smooth Sigma (σ) Top-k Percentile (%) 1.0 2.0 3.0 3.0% 5.0% 7.0% 69.53 70.85 68.17 70.18 72.12 69.25 68.91 71.64 68. Table 9. Ablation study on sample mining hyperparameters. Test set AUC (%) for different combinations of Gaussian smoothing parameter (σ) and top-k percentile thresholds used in peak selection. σ = 2.0 and top-k = 5% achieve the best performance, highlighting the importance of balancing sample quality and quantity in our weakly-supervised mining strategy. C. Failure Cases Figure 7 presents three examples highlighting VADERs tendency to favor visually dynamic events. While VADER accurately captures prominent high-motion activities, it often overlooks the underlying causes of anomalies or neglects subtle contextual cues, such as unattended objects or gradual environmental changes. For instance, in the middle example, VADER correctly identifies the car collision but misses the key causal factor, the vehicle ignoring the trafFigure 6. Illustrative example showing the need for causal and relational modeling. Given the same video of dog attacking boy, CUVA [11] and Holmes-VAU [67] produce incorrect or incomplete descriptions, while VADER captures key interactions and event progression, resulting in accurate and coherent descriptions with higher human-aligned evaluation scores. fic signal, resulting in an incomplete explanation of why the anomaly occurred. Figure 8 presents three examples highlighting VADERs limitation to object-centric reasoning. While VADER accurately captures localized actions and pairwise object interactions, it often fails to represent the broader scene context or collective behaviors. For instance, in the left example, VADER correctly describes individual pedestrians entering and exiting the subway but misses the group of people gathered to watch street performance, resulting in an incomplete understanding of the true anomaly. D. Potential Mitigations for Limitations To address the three primary limitations discussed in Sec. 5, we outline potential mitigation strategies. These strategies aim to enhance VADERs robustness, reduce bias, and broaden its capacity to handle diverse anomaly scenarios. D.1. Mitigating Dependency on Upstream Modules VADER relies on the performance of upstream modules, which may cause errors to propagate and impact the final reasoning results. To reduce this dependency and improve the reliability of the pipeline, we consider the following approaches to strengthen upstream components and improve their alignment with downstream tasks: End-to-End Joint Training. Transform the current pipeline into jointly trainable framework, allowing gradients from the LLM to update the upstream modules so they can better align with the final reasoning objectives. Module Robustness Enhancement. Before end-to-end integration, each module can be strengthened individually by training on more diverse data and employing more advanced tracking algorithms, thereby reducing upstream errors and improving overall robustness D.2. Reducing Bias Toward High-Motion Events VADERs reliance on relational volatility can lead to bias toward anomalies involving strong motion while overlooking subtle anomalies. To address this imbalance, additional cues can be incorporated to complement volatility: Object State Modeling. Track and interpret object states, such as transitions from carried to stationary without an owner, to capture static anomalies like abandoned objects. Global Scene Context. Model typical activity flow within the scene and identify deviations, enabling the detection of subtle anomalies such as loitering or suspicious inactivity. D.3. Moving Beyond Object-Centric Reasoning The current design of VADER focuses on pairwise object interactions, limiting its ability to capture scene-level or group-level anomalies. To broaden the scope of anomaly understanding, the following extensions can be considered: Global Scene Modeling. Add global feature stream that directly encodes entire frames to detect anomalies not tied to specific objects, such as lighting changes or smoke. Group-Level Reasoning. Identify and model groups of Figure 7. Examples illustrating VADERs high-motion bias. VADER tends to focus on visually prominent or dynamic actions, overlooking subtle or context-dependent cues. In the left example, it emphasizes pouring water while ignoring the public smoking. In the middle example, it identifies the car collision but misses the underlying cause, which is the vehicle running the traffic signal. In the right example, it overemphasizes the physical confrontation while neglecting the initial theft that triggered the anomaly. Figure 8. Examples illustrating VADERs limitation to object-centric reasoning. VADER focuses on localized pairwise interactions, overlooking group behaviors or scene-level environmental factors. In the left example, it describes individual pedestrian movements but misses the crowd gathered to watch performance. In the middle example, it identifies the ships movement but overlooks the collective panic it causes. In the right example, it focuses on the police officer and the crowd but ignores the environmental cause behind the stranded people. objects to capture collective behaviors, allowing detection of emergent events like crowd panic or mass movements. E. Computational Efficiency We provide comparison of inference speed between VADER and NVILA [31] on the HAWK benchmark in Table 10. The table reports both the total inference time (in minutes) and frames per second (fps). Although VADER introduces additional modules for relational reasoning and context-aware sampling, it still operates at practical speed for real-world applications. F. Intermediate Visualization of Module Contributions Figure 9 illustrates how each module incrementally improves the generated descriptions. The example shows Method Total Time (min) FPS NVILA [31] VADER 49.85 87.25 33.63 19.22 Table 10. NVILA [31] and VADER on the HAWK [47] benchmark. Comparison of inference time and fps between bustling scene at train station. The Base Model generates generic narrative without focusing on key details. CAES helps the model attend to important frames, capturing activities like walking and standing in line. CORE further models object interactions, enabling it to distinguish directional actions and explain relationships between people and their surroundings. This step-by-step progression demonstrates how each module contributes to improving both descriptive accuracy and interpretability. Figure 9. Intermediate visualization showing how CAES and CORE improve descriptions. CAES helps focus on key frames, while CORE models object interactions for more interpretable outputs. G. Implementation Details Anomaly intervals are detected using an adaptive threshold with the 97th percentile computed per video. For each interval, preand post-event contexts are determined by the 95th and 85th percentiles of the score slope over 5-frame window, with maximum context window of 30 frames. We uniformly sample 4 frames from each context, 8 from the event, and fill to 64 frames with background frames. For relational analysis, object association combines cosine similarity of appearance embeddings (weight 0.8) and IoU, with matching solved by the Hungarian algorithm [19] and maximum track age of 15 frames. Relational volatility at each timestep is measured as the maximum L2 distance between all co-tracked relation pairs in adjacent frames, followed by Gaussian smoothing with standard deviation of 2.0. The top 5% of volatility peaks are used as positives. The Relational Dynamic Encoder is two-layer MLP trained with triplet margin loss [43] with margin 0.5 and semi-hard negative mining [43] with pool size 30. The encoder training uses Adam (learning rate 1 104) with StepLR for 50 epochs. For LLM fine-tuning, we adopt NVILA [20] as the backbone, updating only the projector and LoRA [15] adapters while freezing all other parameters. The learning rate is set to 2 105 with cosine schedule and warm-up ratio of 0.03. Fine-tuning is performed for 3 epochs on single NVIDIA RTX 4090 GPU."
        }
    ],
    "affiliations": [
        "NVIDIA",
        "National Tsing Hua University"
    ]
}