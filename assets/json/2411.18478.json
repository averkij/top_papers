{
    "paper_title": "Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS",
    "authors": [
        "Jinyang Wu",
        "Mingkuan Feng",
        "Shuai Zhang",
        "Feihu Che",
        "Zengqi Wen",
        "Jianhua Tao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In-context Learning (ICL) enables large language models (LLMs) to tackle downstream tasks through sophisticated prompting and high-quality demonstrations. However, this traditional ICL paradigm shows limitations when facing complex mathematical reasoning tasks, primarily due to its heavy dependence on example quality and the necessity for human intervention in challenging scenarios. To address these limitations, this paper presents HiAR-ICL, a \\textbf{Hi}gh-level \\textbf{A}utomated \\textbf{R}easoning paradigm in \\textbf{ICL} that shifts focus from specific examples to abstract thinking patterns, extending the conventional concept of context in ICL. HiAR-ICL introduces five atomic reasoning actions as fundamental components for constructing chain-structured patterns. Using Monte Carlo Tree Search, we explore reasoning paths and construct thought cards to guide subsequent inference. We then develop a cognitive complexity framework that dynamically matches problems with appropriate thought cards. Experimental results demonstrate HiAR-ICL's effectiveness, achieving state-of-the-art accuracy (79.6$\\%$) on the MATH benchmark with Qwen2.5-7B-Instruct, surpassing GPT-4o (76.6$\\%$) and Claude 3.5 (71.1$\\%$)."
        },
        {
            "title": "Start",
            "content": "Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS Jinyang Wu * 1 Mingkuan Feng * 1 Shuai Zhang 1 Feihu Che 2 Zengqi Wen 2 Jianhua Tao 1 2 4 2 0 2 7 2 ] . [ 1 8 7 4 8 1 . 1 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "In-context Learning (ICL) enables large language models (LLMs) to tackle downstream tasks through sophisticated prompting and high-quality demonstrations. However, this traditional ICL paradigm shows limitations when facing complex mathematical reasoning tasks, primarily due to its heavy dependence on example quality and the necessity for human intervention in challenging scenarios. To address these limitations, this paper presents HiAR-ICL, High-level Automated Reasoning paradigm in ICL that shifts focus from specific examples to abstract thinking patterns, extending the conventional concept of context in ICL. HiAR-ICL introduces five atomic reasoning actions as fundamental components for constructing chain-structured patterns. Using Monte Carlo Tree Search, we explore reasoning paths and construct thought cards to guide subsequent inference. We then develop cognitive complexity framework that dynamically matches problems with appropriate thought cards. Experimental results demonstrate HiAR-ICLs effectiveness, achieving state-of-the-art accuracy (79.6%) on the MATH benchmark with Qwen2.5-7B-Instruct, surpassing GPT-4o (76.6%) and Claude 3.5 (71.1%). 1. Introduction Give man fish and you feed him for day. Teach man to fish and you feed him for lifetime. An old proverb Large language models (LLMs) have demonstrated remarkable capabilities across diverse tasks and domains (Zhao et al., 2023; OpenAI, 2023; Yang et al., 2024a; Dubey et al., *Equal contribution 1Department of Automation, Tsinghua University, Beijing, China 2Beijing National Research Center for Information Science and Technology, Beijing, China. Correspondence to: Shuai Zhang <zhang shuai@mail.tsinghua.edu.cn>, Jianhua Tao <jhtaoo@tsinghua.edu.cn>. Preprint 1 2024). Among these capabilities, complex reasoning proficiency, particularly in mathematical tasks, has emerged as critical benchmark for evaluating these models fundamental cognitive abilities (Hao et al., 2023; Xi et al., 2024). This aptitude highlights their logical reasoning skills and reflects their ability to solve structured problems effectively (Fu et al., 2023; Plaat et al., 2024). The mastery of multi-step reasoning often demands rigorous adherence to intricate rules, precise execution, and application of various problem-solving strategies, which poses unique challenges for existing LLMs (Ahn et al., 2024). Due to its simplicity and parameter-free nature (zero training cost), in-context learning (ICL), also known as few-shot prompting, has garnered increasing attention and emerged as promising approach for eliciting the reasoning potential of LLMs (Zhou et al., 2024c; Zhao et al., 2024). Originally introduced by (Brown et al., 2020), the key idea of ICL is analogy-based learning (Dong et al., 2024). This approach expects LLMs to discern hidden patterns from carefully curated demonstration examples and subsequently generate appropriate reasoning steps for test problems. Extensive research has focused on enhancing ICL performance through improved prompt engineering, encompassing both instruction optimization (Wang et al., 2023c) and demonstration selection (Luo et al., 2024). pivotal advancement in this domain is Chain-of-thought (CoT) reasoning (Wei et al., 2022; Kojima et al., 2022). By incorporating the prompt Lets think step by step alongside step-by-step reasoning examples, this approach enables models to emulate human-like reasoning processes, achieving notable success in complex problem-solving, especially in mathematical reasoning tasks (Sprague et al., 2024). Despite these advances, current ICL paradigms face several limitations. First, ICL-based reasoning performance is highly contingent upon the provided demonstrations. Empirical studies (Wang et al., 2023a; Cui et al., 2024; Wang et al., 2024c) have revealed that LLMs exhibit high sensitivity to task-specific characteristics and multiple facets of ICL examples, including demonstration quantity, ordering, and label distributions. Consequently, suboptimal demonstrations may fail to elicit the best model performance and even hinder reasoning capabilities. Second, crafting high-quality Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS demonstrations often requires substantial human expertise, which can be time-consuming and labor-intensive for complex reasoning domains such as mathematical problemsolving. Third, ICLs generalization ability remains limited. When encountering reasoning tasks that share similar logical structures but differ in presentation format, reconstructing corresponding demonstration examples is often necessary. To address these challenges, this paper proposes HiARICL, High-level Automated Reasoning paradigm in ICL through Monte Carlo Tree Search (MCTS). We aim to expand the conventional conecept of ICL by redefining context beyond mere demonstration examples to encompass higher-order cognitive reasoning patterns. This paradigm embodies the principle of teaching people how to think, rather than merely what to think, aligning with the proverb, Give man fish and you feed him for day. Teach man to fish and you feed him for lifetime. Furthermore, while traditional ICL approaches (e.g., CoT) typically follow linear left-to-right reasoning trajectory (Chen & Li, 2024), we employ tree search to expand search spaces for reasoning patterns ( Swiechowski et al., 2023; Koh et al., 2024). Specifically, our method comprises four key steps: (1) define atom reasoning actions, (2) construct thought cards via MCTS, (3) select reasoning patterns, and (4) solve and verify. First, we define five atomic reasoning actions that collectively form the building blocks of chain-structured reasoning patterns (termed thought card in this paper). These actions are designed to simulate human-like cognitive behaviors, such as problem decomposition and reasoning step reflection. Second, using small set of randomly sampled seed data, we leverage MCTS to derive reference reasoning patterns through its standard four steps: selection, expansion, simulation, and backpropagation, ultimately constructing multiple thought cards. Third, we introduce cognitive complexity metric comprising three indicators: subquestion count, problem condition complexity, and semantic similarity. Based on this metric, we match and select the optimal three thought cards that best align with the target problems cognitive complexity. Finally, guided by the selected thought cards, we execute the reasoning process and validate the final solution through multiple verification mechanisms: output reward model (ORM), process reward model (PRM), and consistency checks, ensuring high-quality results. Empirical experiments across multiple complex reasoning datasets demonstrate that HiAR-ICL not only outperforms state-of-the-art (SOTA) methods but also achieves reduced time complexity. The core contributions are summarized as follows: Automated Reasoning Paradigm: We propose fully automated reasoning paradigm through MCTS, which eliminates human intervention in demonstration design, and aligns with LLMs intrinsic reasoning capabilities. Human-Like Reasoning Behavior: We introduce five atomic reasoning actions that emulate human cognitive processes, enabling more effective problem-solving. Superior Performance: HiAR-ICL significantly outperforms existing methods on complex reasoning benchmarks, achieving 79.6% accuracy on MATH with Qwen2.5-7B-Instruct, substantially outperforming GPT-4o (76.6%). 2. Preliminary In this section, we begin with the problem statement in Section 2.1, and then illustrate our motivation from two aspects (Section 2.2 and Section 2.3). 2.1. Problem Statement For pre-trained policy LLM πθ parameterized by θ, complex problem-solving can be framed as multi-step reasoning process. Specifically, given problem and predefined instructions Φ like lets think step by step (Kojima et al., 2022; Wei et al., 2022), we guide the model to generate sequence of reasoning steps that lead to final answer πθ(Φ(x)). In detail, this involves intermediate reasoning steps s0...T = [s0, s1, s2, ..., sT ], where s0 := and sT := y. At each time step t, the model receives state St1, which consists of the original input and preceding reasoning steps (s1, s2, ..., st1). The policy model πθ then generates the current action at = πθ(Φ(St1)), which is used to prompt the LLM to produce the next reasoning step st. The entire process from the initial step s1 to the final output sT naturally forms complete chain of thought. When direct mapping from input to output (e.g., in mathematical reasoning) is complex (Figure 1(a)), few-shot Chain-of-Thought (CoT) prompting (Wei et al., 2022) serves as an effective approach to guide reasoning. This approach requires high-quality demonstrations to facilitate analogical learning for test problems. However, LLMs often exhibit sensitivity to exemplar selection, necessitating human intervention in choosing appropriate samples (Sclar et al., 2024). Moreover, pure analogical learning may lead to rigid thinking patterns, hindering its ability to develop genuine problem-solving capabilities (illustrated in Figure 1(b)). Novel ICL Framework: We extend the traditional concept of context from specific examples to higherlevel cognitive reasoning patterns, advancing the frontier of ICL research. This paper aims to teach LLMs high-level thinking patterns to reason about such problems rather than mere example imitation. We enhance in-context learning to (1) eliminate dependency on carefully curated examples; (2) foster ef2 Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS Figure 1. Schematic comparison between HiAR-ICL and traditional zero-shot and few-shot in-context learning methods. We illustrate the learning process through teacher-student paradigm. (a) Direct Prompting (Zero-shot CoT) provides only generic Lets think step by step instruction, proving insufficient for executing step-by-step reasoning; (b) In-Context Learning (Few-shot CoT) offers carefully selected examples but fails to generalize when encountering dissimilar problems; (c) Our Method (HiAR-ICL) teaches high-level thought patterns, enabling robust performance on various problems. fective reasoning strategies; and (3) transcend linear CoT limitations by adopting more precise reasoning paradigm. 2.2. Motivation 1: Traditional ICL suffers from example bias, human costs, and limited generalization Our primary motivation stems from the limitations of InContext Learning (ICL), which can be intuitively illustrated through the teacher-student analogy. Here, the teacher typically represents human, while the student refers to model designed for problem-solving. Figure 1 provides visual comparison of traditional ICL and our method. Scenario (a) The teacher merely provides step-by-step instructions without explaining the underlying reasoning process or the appropriate thinking pattern for each step. Consequently, the student (particularly smaller models below 10B parameters) struggles to comprehend the teachers intent and internalize the problem-solving approach. Scenario (b) The teacher must carefully construct highquality examples for the student to imitate. These examples are crucial since slight variations may influence the models understanding of the problem (Sclar et al., 2024). While this approach helps the student learn to solve similar problems, the student often struggles with novel problems that deviate from the original format. This necessitates additional teacher intervention to provide new, relevant examples, resulting in time-consuming and labor-intensive process. Scenario (c) In contrast, more experienced teacher might focus on fostering generalized thinking patterns rather than solving individual problems. This approach equips students with robust problem-solving strategies, enabling them to effectively tackle novel and complex challenges, even in unfamiliar scenarios. Such long-term perspective empowers students to independently adapt and solve similar types of problems efficiently. 2.3. Motivation 2: Precise reasoning paradigms unleash model potential The recently introduced OpenAI o1 model1 has demonstrated outstanding performance in solving complex reasoning problems, which highlights that precise reasoning paradigms (i.e., Test-time Compute methods) can significantly unleash LLM potential and enhance reasoning capabilities (Wu et al., 2024; Qin et al., 2024). Given that CoT follows linear left-to-right structure, numerous works have extended reasoning paradigms to tree structures to expand potential search spaces and achieve more precise reasoning, such as ToT (Yao et al., 2023), rStar (Qi et al., 2024), and ReST-MCTS* (Zhang et al., 2024c). Motivated by this, we also leverage tree structures (MCTS) to explore more comprehensive reasoning paths. By incorporating prior cognitive patterns into the ICL-based reasoning process, we significantly reduce the computational complexity of infinite search spaces compared to existing approaches while maintaining performance. Overall, this paper constructs precise reasoning paradigm that effectively balances efficiency and accuracy. 1https://openai.com/o1/ 3 Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS Figure 2. Flowchart of our proposed method HiAR-ICL. This framework consists of four main parts: (1) Define Atom Reasoning Actions; (2) Construct Thought Cards via MCTS; (3) Select Reasoning Patterns; (4) Solve and Verify. 3. Methodology each chain-structured thought card. Overview of HiAR-ICL In this section, we introduce HiARICL (High-level Automated Reasoning Paradigm in ICL via MCTS) in detail, and illustrate the whole process in Figure 2. Our method consists of four main components: Define Atom Reasoning Actions: Define five humanlike fundamental thought actions as building blocks for Construct Thought Cards via MCTS: Leverage MCTS (Selection, Expansion, Simulation, Backpropagation) to construct thought cards comprehensively. Select Reasoning Patterns: Identify the optimal three reasoning patterns based on the problems cognitive complexity level. 4 Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS Solve and Verify: Perform reasoning process under the selected patterns, and validate candidate solutions using PRM, ORM, or consistency-based verification. 3.1. Define Atom Reasoning Actions Understanding how humans engage in complex reasoning is essential for modeling human cognition (Jaffe et al., 2023). Existing studies often distinguish between two categories of reasoning ormore broadlycognitive processes: System 1 and System 2 (Kahneman, 2011; Da Silva, 2023; Hagendorff et al., 2023). System 1 refers to fast, intuitive, yet error-prone thinking, while System 2 involves slow, deliberative thinking with superior reasoning performance. Given OpenAI o1 models impressive complex reasoning abilities, developing efficient System 2 approaches has gained increasing attention among researchers aiming to emulate human cognitive processes (Qin et al., 2024; Sun et al., 2024). Inspired by this, we introduce five atomic human-like reasoning actions to bridge the gap between model reasoning and human cognition as follows: (a1) System Analysis (SA): Analyzing the overall structure of the problem and identifying the constraints and conditions before addressing it, thereby clarifying task requirements effectively. (a2) One-Step Thought (OST): Generating the next one-step thought based on the given question and the preceding reasoning steps. (a3) Chain-of-Thought (CoT): Facilitating step-by-step reasoning by constructing logical sequence of intermediate thoughts, where each step incrementally builds on the previous ones. (a4) Divide and Conquer (DC): Breaking down complex reasoning problem into several smaller subproblems and progressively solving them to achieve the overall solution. (a5) Self-Reflection and Refinement (SRR): Engaging in timely reflection of prior solutions and implementing necessary refinement during the reasoning process to ensure accuracy and reliability. 3.2. Construct Thought Cards via MCTS Following the action definition, we establish prior reasoning patterns. For clarity of presentation, we introduce the concept of thought cards, which serve as reasoning templates in subsequent ICL-based reasoning processes. Specifically, using small seed dataset of several hundred samples, we aim to derive their optimal reasoning paths and distill them into multiple thought cards. These thought cards then serve 5 as prior knowledge when encountering test cases, facilitating efficient reasoning by providing reference insights. Step 1: Acquiring Reasoning Paths for Seed dataset As shown in Figure 2, for reasoning path acquisition, we employ Monte Carlo Tree Search (MCTS) to iteratively evaluate and optimize the solution search process and obtain high-quality reaonsing paths of given seed dataset. This design leverages both the iterative nature of MCTS and the inherent reasoning capabilities of LLMs, thereby leading to enhanced search outcomes (Ye et al., 2021; Zhou et al., 2024a). In our framework, following (Wang et al., 2024a; Qi et al., 2024), we formulate each complex reasoning question as tree search problem, where represents the root node, and subsequent tree nodes denote reasoning steps (comprising actions and corresponding outcomes) generated by an LLM policy πθ. We define the state St1 as an (incomplete) trajectory q, s1, ..., st1, where S0 = q. The next step in the sequence can then be sampled as st πθ(St1). Another critical component of MCTS is the reward function, which guides tree expansion by evaluating action values. We define Q(s) as the reward value for node s. Initially, all unexplored nodes are assigned Q(si, ai) = 0. They will be updated as weighted average of its current value and the Q-value of its child node: Qnew(p) = (1 α)Qold(p) + αQ(s) (1) where α serves as discount factor, reflecting the importance of future rewards. For terminal nodes, following (Qi et al., 2024), we adopt the likelihood (confidence) of selfconsistency majority voting as the reward value. This design eliminates the need for external supervision, thereby enhancing the generalization of our approach. Specifically, this step undergones four MCTS phases: selection, expansion, simulation, and backpropagation. (1) Selection. This phase identifies the most suitable node for subsequent expansion. Starting from the root node, child node is chosen at each tree level until reaching leaf node, which we define as either achieving the maximum tree depth or arriving at an answer here. To balance the exploration and exploitation, we use the well-known Upper Confidence Bounds applied to Trees (UCT) (Kocsis & Szepesvari, 2006) for node selection: CT (s) = Q(s, a) (s) + (cid:115) lnN (p) (s) (2) where Q(s) is the reward value for node s, (s) is the number of visits to s, is the parent node of s, and is the exploration weight. The node with the highest UCT value is selected for subsequent phases. (2) Expansion. During the expansion phase, the selected node is expanded by sampling actions from πθ and then Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS generating corresponding reasoning outcomes, as described above. This results in child nodes, which are added to the tree and stored in an external long-term memory structure. (3) Simulation. In MCTS, complete simulation process involves iteratively selecting and expanding nodes from the initially selected node until reaching terminal node. Here, we define terminal node as either the maximum tree depth (d=5 in our implementation) or an answer node, indicated by specific answer marker in the output, such as the answer is in our implementation. At each tree level, nodes are sampled and expanded following consistent procedure. To enhance efficiency and avoid unnecessary expansion, we implement an early termination strategy. Inspired by (Fluri et al., 2023; Zhou et al., 2024a), we incorporate heuristic based on self-consistency (Wang et al., 2023b). This strategy leverages the observation that actions sampled multiple times at the same state are more likely to be accurate, indicating that the model has likely completed the task successfully. Specifically, if the models consistency score exceeds predefined threshold c, i.e., SC(s) > c, the simulation can be terminated early. (4) Backpropagation. When the end of rollout is reached, backpropagation is carried out. During this phase, we update node information, including the number of visits and node values based on the outcome of trajectory. Specifically, the number of visits of each node si along the simulation path s0, ...sd will be updated as Nnew(s) = Nold(s) + 1. The value Q(s) of newly visited node is propagated backward to its parent node p, where the reward value of is adjusted as Eq. 1. Note that, these updated values are used in the UCT formula (Eq. 2) to guide the selection of the next node. Step 2: Distilling Paths into Thought Cards After executing the MCTS procedure, we obtain tree structure for each question in the seed dataset, yielding multiple valid reasoning trajectories. To identify the optimal reasoning path per question, we draw inspiration from the concept of Value of Computation (VOC) (Russell & Wefald, 1991). This principle posits that intelligent systems should optimize the trade-off between computational benefits and costs, which leads us to propose novel VOC-inspired selection metric as follows: score(x, pa, y) = Reward(pax)(1k)C(pa) (3) where represents the task input, denotes the target answer, pa is candidate reasoning path, is balance factor between computational benefits and costs. Reward(pax) represents the final reward score of path pa in the generated tree, and C(pa) measures the path length of the reasoning sequence. Here, for simplicity, we define Reward(pax)as the Q-value of the leaf node, and C(pa) as the total number of utilized actions in the reasoning sequence. More complex settings are left for future research. Then, for each question in the seed dataset, we select the path pa with the highest score(x, pa, y) value to construct the Question-path repository, establishing one-to-one mappings between questions and their optimal reasoning paths. Drawing inspiration from metareasoning approaches (Russell & Wefald, 1991; De Sabbata et al., 2024), which advocate for adaptive reasoning strategies based on problem characteristics, we perform distillation process to these fundamental question-path pairs into more abstract thought cards for subsequent test-time reference. This transformation is guided by novel cognitive complexity framework designed for complex reasoning tasks (Lee & Heyworth, 2000; Embretson & Daniel, 2008). The framework encompasses three key dimensions: (1) Subquestion Count (SC): Quantifying the number of subproblems; (2) Problem Condition Complexity (PCC): Measuring the number of known conditions; (3) Semantic Similarity (SS): Assessing semantic distance between the target problem and the seed dataset. Here, using the first two metrics, we refine the Question-path Repository to generate multiple high-level thought cards, as shown in Figure 2. The third metric is reserved for the testing phase. The next section describes how we utilized these thought cards in the testing phase. 3.3. Select Reasoning Patterns During the evaluation phase, we apply our cognitive complexity framework to each test question. We instruct Llama38B-instruct to compute its SC, PCC, and SS, with the latter metric being calculated based on few dozen examples from the seed dataset. Using these metrics, we perform matching process within our thought cards, selecting two to three cards that exhibit the closest alignment in scores for each metric. These selected cards are then utilized as reference templates in the final solution generation and verification process. 3.4. Solve and Verify Solve leveraging two to three selected thought cards, we generates candidate solutions for given test problem. Extending the tree-structured reasoning framework introduced in Section 3.2, our method incorporates prior reasoning patterns, eliminating the need to expand multiple child nodes. Instead, we follow the action sequences specified on each thought card at each hierarchical level, as depicted in Figure 2. In this way, for test problem qtest, we can obtain several candidate solutions. Verify Identifying the most accurate reasoning trajectory among multiple candidate solutions represents critical challenge (Uesato et al., 2022; Qi et al., 2024; Zhang et al., 2024c). We investigate three verification methodologies: process-supervision, outcome-supervision, and consistencybased approaches. Our empirical results demonstrate that 6 Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS Table 1. Evaluation of HiAR-ICLs reasoning capabilities against ICL methods across four reasoning benchmarks. The best results in each box are highlighted in bold. Our method, HiAR-ICL consistently achieves the best performance across models and datasets. MODEL SETTING MATHEMATICS ARITHMETIC COMMONSENSE AVERAGE MATH GSM8K SVAMP StrategyQA Qwen2.5-14B-instruct Qwen2.5-7B-instruct Qwen2-7B-instruct Yi-1.5-6B-Chat Llama-3-8B-Instruct Llama-3.1-8B-Instruct Zero-shot CoT Few-shot CoT CoT+SC@4 Ours Zero-shot CoT Few-shot CoT CoT+SC@4 Ours Zero-shot CoT Few-shot CoT CoT+SC@4 Ours Zero-shot CoT Few-shot CoT CoT+SC@4 Ours Zero-shot CoT Few-shot CoT CoT+SC@4 Ours Zero-shot CoT Few-shot CoT CoT+SC@4 Ours 69.8 80.0 76.2 80. 64.8 75.5 76.4 79.6 36.9 52.9 55.6 63.8 30.4 40.5 42.2 54.0 5.8 17.8 28.8 43.2 18.0 47.2 44.2 55.0 92.4 94.8 94.0 95. 86.2 91.6 92.0 92.8 76.6 85.7 87.7 90.6 76.4 78.9 79.4 81.4 68.3 74.5 80.6 89.6 61.5 76.6 80.5 90.7 91.6 91.3 91.0 93. 91.3 92.3 92.3 93.0 85.2 87.3 90.3 92.7 64.4 81.3 87.6 90.0 70.9 81.0 88.0 92.7 69.3 82.0 85.6 93.0 62.8 53.1 69.7 77. 52.8 67.6 73.2 76.0 55.3 62.3 65.5 72.0 46.2 61.1 65.2 70.3 57.2 68.4 66.8 73.0 52.4 63.6 69.8 73.2 79.1 79.8 82.7 86. 73.7 81.7 83.4 85.4 63.5 72.0 74.8 79.8 54.3 65.4 68.6 74.0 50.5 60.4 66.0 74.6 50.3 67.3 70.0 78.0 even the simple consistency-based method can effectively select the most precise reasoning chains, achieving robust reasoning performance. In summary, our approach can be seen as an optimized variant of tree search algorithms. Unlike traditional MCTS methods that require extensive explorationoften involving exhaustive search to identify effective paths, which is computationally expensiveour method reallocates computational complexity. By strategically incorporating prior knowledge, we significantly enhance search efficiency. As illustrated in the fourth section of Figure 2, our approach effectively traverses the solution tree by directly targeting promising trajectories. Therefore, this approach maintains high-performance standards while simultaneously reducing computational time complexity, representing notable advancement in efficient reasoning strategies. 4. Experiments 4.1. Setups Datasets To evaluate the effectiveness of our method, HiARICL, we conduct comprehensive experiments across diverse datasets and reasoning tasks. We sample several hundred instances from the training sets as seed data for thought card construction (described in Section 3.2). The models performance was then evaluated on corresponding test sets. Our evaluation benchmarks encompass: (1) arithmetic reasoning: GSM8K1319 (Cobbe et al., 2021) and SVAMP300 (Patel et al., 2021); (2) complex mathematical reasoning: MATH500 (Hendrycks et al., 2021); (3) multi-hop commonsense reasoning: StrategyQA687 (Geva et al., 2021). Models HiAR-ICL is general approach applicable to various LLMs. In our experiments, we evaluate its effectiveness using state-of-the-art (SOTA) models: Llama3-8BInstruct (Dubey et al., 2024), Llama-3.1-8B-Instruct (Meta AI, 2024), Yi-1.5-6B-Chat (Young et al., 2024), Qwen2-7BInstruct (Yang et al., 2024a), and Qwen2.5-7B/14B-Instruct (Qwen Team, 2024). By focusing on LLMs with parameter counts generally under 10B, we aim to demonstrate the robustness and efficiency of our method. We expect that applying HiAR-ICL to small language models will achieve results comparable to or exceeding closed-source LLMs. Baselines We evaluate HiAR-ICL against three strong baseline categories: (1) traditional example-based ICL methods, including zero-shot CoT (Kojima et al., 2022), few-shot CoT (Wei et al., 2022), and SC+CoT (Wang et al., 2023b); (2) 7 Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS Table 2. Comparison with leading closed-source LLMs. The best results in each box are highlighted in bold. Results for closedsource models are sourced from corresponding official websites. CS and OS represent closed-source and open-source LLMs, respectively. Notably, the 7B model, Qwen2.5-7b-instruct, surpasses all closed-source models, achieving SOTA performance. MODEL SETTING MATH GSM8K Claude-3-Opus CS Claude-3.5-Sonnet CS GPT-3.5 CS GPT-4 CS GPT-4o CS GPT-4o mini CS Gemini-1.5-Pro CS Llama-3.1-405B-Instruct OS 405B Llama-3.1-70B-Instruct OS 70B Llama-3-70B-Instruct OS 70B Nemotron4-340B-Instruct OS 340B OS 123B Mixtral-large2-Instruct OS 141B Mixtral-8x22B-Instruct OS 72B NuminaMath-72B CoT OS 72B Qwen2-72B-Instruct OS 34B Yi-1.5-34B-Chat Qwen2.5-14B-instruct Qwen2.5-7B-instruct Qwen2-7B-instruct Yi-1.5-6B-Chat Llama-3-8B-Instruct Llama-3.1-8B-Instruct Ours Ours Ours Ours Ours Ours 60.1 71.1 43.1 64.5 76.6 70.2 67.7 73.8 68.0 50.4 41.1 69.9 54.1 66.7 69.0 50.1 80.2 79.6 63.8 54.0 43.2 55.0 95.0 96.4 81.6 94.2 96.1 93.2 90.8 96.8 95.1 93.0 92.3 92.7 88.2 90.8 93.2 90. 95.3 92.8 90.6 81.4 89.6 90.7 tree-based methods, including ToT (Yao et al., 2023), RAP (Hao et al., 2023), ReST-MCTS (Zhang et al., 2024c), LiteSearch (Wang et al., 2024a), MCTSr (Zhang et al., 2024a), BEATS (Sun et al., 2024), LLaMA-Berry (Zhang et al., 2024b), and rStar (Qi et al., 2024). (3) powerful closedsource LLMs, including GPT-4 (OpenAI, 2023), GPT-4o (OpenAI, 2024), Claude-3.5 (Anthropic, 2024) and Gemini1.5-pro (Google DeepMind, 2024). Evaluation Metrics We evaluate our approach using two key metrics. First, we report accuracy as our primary evaluation metric, where correctness is determined by comparing the models final answer with the ground truth. To ensure consistent answer extraction, we require the LLM to explicitly state its solution following predefined format (e.g., The answer is). Additionally, we measure the average reasoning time to analyze our methods computational complexity compared to existing search-based approaches. Implementation Details We utilize the vLLM framework2 with the following parameters: temperature set to 0.8, top set to 0.9, and max tokens set to 1024. All experiments were conducted on machine running Ubuntu 22.04, equipped with NVIDIA A100-80GB GPUs. 2https://github.com/vllm-project/vllm 4.2. Results on diverse reasoning benchmarks As shown in Table 1, we evaluate the effectiveness of HiARICL across four mainstream reasoning benchmarks. We provide comprehensive comparisons between HiAR-ICL and SOTA closed-source LLMs performances (sourced from official websites and technical reports). We have two key findings: HiAR-ICL consistently performs better than traditional ICL methods across all tasks. For example, Llama3-8Bs accuracy on the MATH benchmark improved from 17.8% (few-shot CoT) to 43.2% with HiAR-ICL, representing substantial performance enhancement of 2.4 times. Our method exhibits the most substantial performance improvements on relatively small language models. For example, Qwen2-7B-Instruct improved from 52.9% to 63.8%, Yi-1.5-6B-Chat from 40.5% to 54.0%, and Llama3-8BInstruct from 17.8% to 43.2%. These results underscore our approachs potential to efficiently guide smaller language models in generating and selecting optimal solutions. 4.3. Comparison with powerful Closed-source LLMs To comprehensively demonstrate the effectiveness of our method, we compare our approach with current state-of-theart closed-source models and prominent open-source models wwith several hundred billion parameters. As shown in Table 2, HiAR-ICL-empowered LLMs achieved competitive performance comparable to models with several hundred billion parameters, even surpassing powerful closed-source models. Notably, the 7B Qwen2.5 model achieved 79.6% accuracy on the challenging MATH benchmark, exceeding GPT-4os performance. Similarly, Qwen2-7B-Instruct, utilizing the HiAR-ICL reasoning paradigm, outperformed Llama3-70B-Instruct. 4.4. Comparison with tree-based methods We compare our approach with other tree-based reasoning methods on GSM8K and MATH. As shown in Table 3, our method demonstrates superior performance and notable generalizability across various models and datasets. Notably, as benchmark complexity increases, existing methods such as ToT and RAP face significant challenges. While methods like BEATs show comparable performance to ours on Llama3-8B-Instruct, performance disparities persist across other models like Yi-1.5-6B-Chat and Qwen2-7B-Instruct. We also compare with rStar (Qi et al., 2024) across four datasets. As recently proposed powerful method, rStar significantly expands the solution search space and achieves state-of-the-art performance in tree-based reasoning. Our analysis in Figure 3 systematically assesses performance and computational efficiency. Notably, our approach achieves Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS Table 3. Evaluation of HiAR-ICLs reasoning capabilities against state-of-the-art tree-based methods. Our results are marked in blue , with baseline results sourced from the original paper when accessible. The best results are highlighted in bold. MODEL METHOD GSM8K MATH Yi-1.5-6B-Chat Qwen2-7B-instruct Llama-3-8B-Instruct Llama-3.1-8B-Instruct BEATS Ours BEATS Ours ToT RAP ReST-MCTS* LiteSearch LLaMA-Berry BEATS Ours LLaMA-Berry Ours 76.1 81.4 83.0 90. 69.0 80.5 - 82.3 88.1 88.4 89.6 89.8 90.7 51.2 54.0 61.5 63.8 13.6 18.8 34.2 - 39.6 42.9 43.2 54.8 55. Table 4. The effect of verification method for HiAR-ICL. PRM, ORM, SC denote process-reward model, output-reward model, and self-consistency, respectively. MODEL SETTING GSM8K MATH Llama3-8B-Instruct Qwen2-7B-instruct ORM PRM (min) PRM (product) SC ORM PRM (min) PRM (product) SC 86.4 89.6 87.9 89.0 88.7 90.6 90.4 90.4 38.6 42.8 41.4 43.2 56.6 62.2 61.4 63. competitive performance with rStar while substantially reducing time complexity. For example, our method demonstrates significant time reduction on relatively straightforward datasets such as GSM8K and StrategyQA, with reductions of 27.6X, and 47.3X, respectively. This indicates that for simpler problems, our approach can efficiently complete reasoning without excessive exploration by adaptively selecting simple reasoning patterns. Similarly, we achieve tenfold reduction in computational time on the more challenging MATH dataset. Consequently, our method presents clear performance-efficiency trade-off. 4.5. The effect of verification method Figure 3. Comparison with State-of-the-Art Tree-Based Method, rStar. (a) Performance Comparison (accuracy). (b) Time Cost Per Sample. The results demonstrate that our method achieves remarkable performance while significantly reducing computational time, with an overall 28-fold time reduction and an impressive 47-fold time reduction on the StrategyQA dataset. Table 5. Performance variations of Qwen2.5-7B-Instruct across different difficulty levels on MATH. We list the result of Zero-shot CoT, fewshot CoT+SC, and our method. METHOD 2 3 4 5 AVERAGE CoT CoT+SC HiAR-ICL 83.7 95.3 97.7 83.3 90.0 94.5 82.8 91.4 92.3 59.4 73.4 80.5 37.3 52.2 53.0 64.8 76.4 79. methods: Process Reward Model (PRM), Output Reward Model (ORM), and Self-Consistency (SC). The PRM utilized math-shepherd-mistral-7b-prm3 and ORM employed Llama3.1-8B-ORM-Mistral-Data4. For PRM, two scoring strategies (Lightman et al., 2024; Wang et al., 2024b) were investigated: product (final solution score computed by multiplying individual step scores) and min (final solution score determined by the minimum step score). As illustrated in Table 4, the results reveal that even naive consistencybased method demonstrates promising performance. More sophisticated verification methods are reserved for future exploration. 4.6. The effect of complexity levels As shown in Table 5, we present the performance of Zeroshot CoT, Few-shot CoT+SC, and our method HiAR-ICL on the challenging MATH dataset at different difficulty levels. Compared to the first two methods, our approach improves performance across all levels, with an average accuracy boost of 2.6% for the easier levels 1-3. Notably, for the more difficult level 4, the improvement reaches +7.1%. This indicates that our method has the potential to solve more challenging problems and enhance reasoning performance. This may be due to the introduction of thinking patterns, We conduct experiments to assess the verification parts of HiAR-ICL. As illustrated in Section 3.4, we adopt three 3huggingface.co/peiyi9979/math-shepherd-mistral-7b-prm 4huggingface.co/RLHFlow/Llama3.1-8B-ORM-Mistral-Data Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS which help LLMs find clearer solution more quickly. 5. Related Work In-context learning via examples In-context learning enables LLMs to learn from few demonstrations without finetuning (Zhou et al., 2024c; Dong et al., 2024). For example, Chain-of-Thought (CoT) (Kojima et al., 2022; Wei et al., 2022) prompts LLMs with simple instruction like Lets think step by step to guide reasoning with few examples. Self-Consistency (Wang et al., 2023b) improves performance by generating multiple reasoning paths and selecting the most consistent answer. However, this traditional paradigm primarily focuses on example-level analogical learning, with performance constrained by demonstration selection and often requiring human expert intervention for complex reasoning tasks (Wang et al., 2023a; Yang et al., 2024b; Zhao et al., 2024). In contrast, we expand the context concept by shifting focus from specific examples to highlevel reasoning patterns. Therefore, our approach offers greater generalizability, enabling fully automated efficient inference without human intervention, even for small models under 10B parameters. Tree-based search LLMs have demonstrated remarkable capabilities but struggle with complex multi-step reasoning tasks (Zhao et al., 2023). Tree search algorithms, particularly Monte Carlo Tree Search (MCTS) (Chaslot et al., 2008), have emerged to expand search spaces and enhance reasoning capabilities (Koh et al., 2024; Zhang et al., 2024a; Zhou et al., 2024b). Recent approaches like Tree of Thought (ToT) (Yao et al., 2023) and GOT (Besta et al., 2024) extend reasoning by using multiple LLM queries to explore non-linear paths. AlphaMath (Chen et al., 2024) enhances LLM mathematical reasoning via MCTS and value model, without human-annotated supervision. Similarly, rStar (Qi et al., 2024) leverages the models inherent capabilities for iterative exploration. However, these methods are often time-intensive. In contrast, our approach introduces novel paradigm by frontloading computational resources and incorporating prior reasoning patterns, achieving competitive performance with reduced computational complexity. 6. Conclusion In this work, we propose HiAR-ICL, novel automated reasoning paradigm that extends the concept of context in ICL and enables LLMs to perform adaptive and efficient reasoning for challenging problems. By incorporating abstract thinking patterns rather than relying on human intervention and example quality, HiAR-ICL empowers LLMs to develop genuine reasoning capabilities instead of merely imitating demonstration examples. Extensive experimental results demonstrate that HiAR-ICL significantly outperforms existing methods, including traditional example-based ICL and test-time OpenAI o1-like search methods. The success of HiAR-ICL in mathematical reasoning tasks establishes solid foundation for advancing complex reasoning in LLMs. This work represents promising starting point, paving the way for future research to explore more sophisticated highlevel automated reasoning paradigms and their applicability across broader spectrum of complex domains."
        },
        {
            "title": "References",
            "content": "Ahn, J., Verma, R., Lou, R., Liu, D., Zhang, R., and Yin, W. Large language models for mathematical reasoning: In Falk, N., Papi, S., and Progresses and challenges. Zhang, M. (eds.), Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop, pp. 225237, St. Julians, Malta, March 2024. Association for Computational Linguistics. Anthropic. URL claude-3-5-sonnet. Introducing claude 3.5 sonnet, 2024. https://www.anthropic.com/news/ Besta, M., Blach, N., Kubicek, A., Gerstenberger, R., Podstawski, M., Gianinazzi, L., Gajda, J., Lehmann, T., Niewiadomski, H., Nyczyk, P., et al. Graph of thoughts: Solving elaborate problems with large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 1768217690, 2024. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 18771901. Curran Associates, Inc., 2020. Chaslot, G., Bakkes, S., Szita, I., and Spronck, P. MonteIn carlo tree search: new framework for game ai. Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, volume 4, pp. 216217, 2008. Chen, G., Liao, M., Li, C., and Fan, K. Alphamath almost zero: Process supervision without process. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. Chen, S. and Li, B. Toward adaptive reasoning in large language models with thought rollback. In Forty-first International Conference on Machine Learning, 2024. 10 Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Cui, Y., He, P., Tang, X., He, Q., Luo, C., Tang, J., and Xing, Y. theoretical understanding of chain-of-thought: Coherent reasoning and error-aware demonstration. arXiv preprint arXiv:2410.16540, 2024. Da Silva, S. System 1 vs. system 2 thinking. Psych, 5(4): 10571076, 2023. De Sabbata, C. N., Sumers, T. R., and Griffiths, T. L. Rational metareasoning for large language models. arXiv preprint arXiv:2410.05563, 2024. Dong, Q., Li, L., Dai, D., Zheng, C., Ma, J., Li, R., Xia, H., Xu, J., Wu, Z., Chang, B., Sun, X., Li, L., and Sui, Z. survey on in-context learning. In Al-Onaizan, Y., Bansal, M., and Chen, Y.-N. (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 11071128, Miami, Florida, USA, November 2024. Association for Computational Linguistics. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Embretson, S. E. and Daniel, R. C. Understanding and quantifying cognitive complexity level in mathematical problem solving items. Psychology Science, 50(3):328, 2008. Fluri, L., Paleka, D., and Tram`er, F. Evaluating superhuman models with consistency checks. In Socially Responsible Language Modelling Research, 2023. Fu, Y., Peng, H., Ou, L., Sabharwal, A., and Khot, T. Specializing smaller language models towards multi-step reasoning. In International Conference on Machine Learning, pp. 1042110430. PMLR, 2023. Geva, M., Khashabi, D., Segal, E., Khot, T., Roth, D., and Berant, J. Did aristotle use laptop? question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9: 346361, 2021. Google DeepMind. Gemini models, May 2024. URL https://deepmind.google/technologies/ gemini/. Hagendorff, T., Fabi, S., and Kosinski, M. Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in chatgpt. Nature Computational Science, 3(10):833838, 2023. Hao, S., Gu, Y., Ma, H., Hong, J., Wang, Z., Wang, D., and Hu, Z. Reasoning with language model is planning with world model. In Bouamor, H., Pino, J., and Bali, K. (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 81548173, Singapore, December 2023. Association for Computational Linguistics. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the MATH dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. Jaffe, P. I., Poldrack, R. A., Schafer, R. J., and et al. Modelling human behaviour in cognitive tasks with latent dynamical systems. Nature Human Behaviour, 7:9861000, 2023. Kahneman, D. Thinking, Fast and Slow. Farrar, Straus and Giroux, New York, NY, 2011. ISBN 978-0374275631. Kocsis, L. and Szepesvari, C. Bandit based monte-carlo planning. In Furnkranz, J., Scheffer, T., and Spiliopoulou, M. (eds.), Machine Learning: ECML 2006, pp. 282293, Berlin, Heidelberg, 2006. Springer Berlin Heidelberg. ISBN 978-3-540-46056-5. Koh, J. Y., McAleer, S., Fried, D., and Salakhutdinov, R. Tree search for language model agents. arXiv preprint arXiv:2407.01476, 2024. Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35: 2219922213, 2022. Langley, P. Crafting papers on machine learning. In Langley, P. (ed.), Proceedings of the 17th International Conference on Machine Learning (ICML 2000), pp. 12071216, Stanford, CA, 2000. Morgan Kaufmann. Lee, F.-L. and Heyworth, R. Problem complexity: measure of problem difficulty in algebra by using computer. EDUCATION JOURNAL-HONG KONG-CHINESE UNIVERSITY OF HONG KONG-, 28(1):85108, 2000. Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and In The Twelfth Cobbe, K. Lets verify step by step. International Conference on Learning Representations, 2024. Luo, M., Xu, X., Liu, Y., Pasupat, P., and Kazemi, M. Incontext learning with retrieved demonstrations for language models: survey. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. Survey Certification. 11 Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS Meta AI. Introducing llama 3.1, July 2024. URL https: //ai.meta.com/blog/meta-llama-3-1/. OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. OpenAI. Hello gpt-4o, May 2024. URL https:// openai.com/index/hello-gpt-4o/. Patel, A., Bhattamishra, S., and Goyal, N. Are nlp models really able to solve simple math word problems? In North American Chapter of the Association for Computational Linguistics, 2021. Plaat, A., Wong, A., Verberne, S., Broekens, J., van Stein, N., and Back, T. Reasoning with large language models, survey. arXiv preprint arXiv:2407.11511, 2024. Qi, Z., Ma, M., Xu, J., Zhang, L. L., Yang, F., and Yang, M. Mutual reasoning makes smaller llms stronger problemsolvers. arXiv preprint arXiv:2408.06195, 2024. Qin, Y., Li, X., Zou, H., Liu, Y., Xia, S., Huang, Z., Ye, Y., Yuan, W., Liu, H., Li, Y., et al. O1 replication journey: strategic progress reportpart 1. arXiv preprint arXiv:2410.18982, 2024. Qwen Team. Qwen2.5: party of foundation models, September 2024. URL https://qwenlm.github. io/blog/qwen2.5/. Russell, S. and Wefald, E. Principles of metareasoning. Artificial Intelligence, 49(1):361395, 1991. ISSN 00043702. Sclar, M., Choi, Y., Tsvetkov, Y., and Suhr, A. Quantifying language models sensitivity to spurious features in prompt design or: How learned to start worrying about prompt formatting. In ICLR, 2024. Sprague, Z., Yin, F., Rodriguez, J. D., Jiang, D., Wadhwa, M., Singhal, P., Zhao, X., Ye, X., Mahowald, K., and Durrett, G. To cot or not to cot? chain-of-thought helps mainly on math and symbolic reasoning. arXiv preprint arXiv:2409.12183, 2024. Uesato, J., Kushman, N., Kumar, R., Song, F., Siegel, N., Wang, L., Creswell, A., Irving, G., and Higgins, I. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022. Wang, A., Song, L., Tian, Y., Peng, B., Yu, D., Mi, H., Su, J., and Yu, D. Litesearch: Efficacious tree search for llm. arXiv preprint arXiv:2407.00320, 2024a. Wang, L., Li, L., Dai, D., Chen, D., Zhou, H., Meng, F., Zhou, J., and Sun, X. Label words are anchors: An information flow perspective for understanding in-context learning. In Bouamor, H., Pino, J., and Bali, K. (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 98409855, Singapore, December 2023a. Association for Computational Linguistics. Wang, P., Li, L., Shao, Z., Xu, R., Dai, D., Li, Y., Chen, D., Wu, Y., and Sui, Z. Math-shepherd: Verify and reinforce LLMs step-by-step without human annotations. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 94269439, Bangkok, Thailand, August 2024b. Association for Computational Linguistics. Wang, S., Chen, Z., Shi, C., Shen, C., and Li, J. Mixture of demonstrations for in-context learning. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024c. Wang, X., Wei, J., Schuurmans, D., Le, Q. V., Chi, E. H., Narang, S., Chowdhery, A., and Zhou, D. Selfconsistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023b. Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H. Self-instruct: Aligning language models with self-generated instructions. In Rogers, A., Boyd-Graber, J., and Okazaki, N. (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1348413508, Toronto, Canada, July 2023c. Association for Computational Linguistics. Sun, L., Liang, H., Wei, J., Yu, B., He, C., Zhou, Z., and Zhang, W. Beats: Optimizing llm mathematical capabilities with backverify and adaptive disambiguate based efficient tree search. arXiv preprint arXiv:2409.17972, 2024. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Swiechowski, M., Godlewski, K., Sawicki, B., and Mandziuk, J. Monte carlo tree search: review of recent modifications and applications. Artificial Intelligence Review, 56(3):24972562, 2023. Wu, S., Peng, Z., Du, X., Zheng, T., Liu, M., Wu, J., Ma, J., Li, Y., Yang, J., Zhou, W., et al. comparative study on reasoning patterns of openais o1 model. arXiv preprint arXiv:2410.13639, 2024. 12 Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS 1237512400, Miami, Florida, USA, November 2024. Association for Computational Linguistics. Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., et al. survey of large language models. arXiv preprint arXiv:2303.18223, 2023. Zhou, A., Yan, K., Shlapentokh-Rothman, M., Wang, H., and Wang, Y.-X. Language agent tree search unifies reasoning, acting, and planning in language models. In Forty-first International Conference on Machine Learning, 2024a. Zhou, A., Yan, K., Shlapentokh-Rothman, M., Wang, H., and Wang, Y.-X. Language agent tree search unifies reasoning, acting, and planning in language models. In Forty-first International Conference on Machine Learning, 2024b. Zhou, Y., Li, J., Xiang, Y., Yan, H., Gui, L., and He, Y. The mystery of in-context learning: comprehensive survey on interpretation and analysis. In Al-Onaizan, Y., Bansal, M., and Chen, Y.-N. (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 1436514378, Miami, Florida, USA, November 2024c. Association for Computational Linguistics. Xi, Z., Chen, W., Hong, B., Jin, S., Zheng, R., He, W., Ding, Y., Liu, S., Guo, X., Wang, J., Guo, H., Shen, W., Fan, X., Zhou, Y., Dou, S., Wang, X., Zhang, X., peng sun, Gui, T., Zhang, Q., and Huang, X. Training large language models for reasoning through reverse curriculum reinforcement learning. In Forty-first International Conference on Machine Learning, 2024. Yang, A., Yang, B., Hui, B., Zheng, B., Yu, B., Zhou, C., Li, C., Li, C., Liu, D., Huang, F., et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024a. Yang, L., Yu, Z., Zhang, T., Cao, S., Xu, M., Zhang, W., Gonzalez, J. E., and Cui, B. Buffer of thoughts: Thoughtaugmented reasoning with large language models. Advances in Neural Information Processing Systems, 2024b. Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T., Cao, Y., and Narasimhan, K. Tree of thoughts: Deliberate problem solving with large language models. In Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 1180911822. Curran Associates, Inc., 2023. Ye, W., Liu, S., Kurutach, T., Abbeel, P., and Gao, Y. Mastering atari games with limited data. In Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 2547625488. Curran Associates, Inc., 2021. Young, A., Chen, B., Li, C., Huang, C., Zhang, G., Zhang, G., Li, H., Zhu, J., Chen, J., Chang, J., et al. Yi: arXiv preprint Open foundation models by 01. ai. arXiv:2403.04652, 2024. Zhang, D., Li, J., Huang, X., Zhou, D., Li, Y., and Ouyang, W. Accessing gpt-4 level mathematical olympiad solutions via monte carlo tree self-refine with llama-3 8b. arXiv preprint arXiv:2406.07394, 2024a. Zhang, D., Wu, J., Lei, J., Che, T., Li, J., Xie, T., Huang, X., Zhang, S., Pavone, M., Li, Y., et al. Llama-berry: Pairwise optimization for o1-like olympiad-level mathematical reasoning. arXiv preprint arXiv:2410.02884, 2024b. Zhang, D., Zhoubian, S., Hu, Z., Yue, Y., Dong, Y., and Tang, J. Rest-mcts*: Llm self-training via process reward guided tree search. Advances in Neural Information Processing Systems, 2024c. Zhao, A., Ye, F., Fu, J., and Shen, X. Unveiling in-context learning: coordinate system to understand its working mechanism. In Al-Onaizan, Y., Bansal, M., and Chen, Y.-N. (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp."
        }
    ],
    "affiliations": ["Department of Automation, Tsinghua University", "Beijing National Research Center for Information Science and Technology"]
}