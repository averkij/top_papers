{
    "paper_title": "Linguistic Generalizability of Test-Time Scaling in Mathematical Reasoning",
    "authors": [
        "Guijin Son",
        "Jiwoo Hong",
        "Hyunwoo Ko",
        "James Thorne"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Scaling pre-training compute has proven effective for achieving mulitlinguality, but does the same hold for test-time scaling? In this work, we introduce MCLM, a multilingual math benchmark featuring competition-level problems in 55 languages. We test three test-time scaling methods-Outcome Reward Modeling (ORM), Process Reward Modeling (ORM), and Budget Forcing (BF)-on both Qwen2.5-1.5B Math and MR1-1.5B, a multilingual LLM we trained for extended reasoning. Our experiments show that using Qwen2.5-1.5B Math with ORM achieves a score of 35.8 on MCLM, while BF on MR1-1.5B attains 35.2. Although \"thinking LLMs\" have recently garnered significant attention, we find that their performance is comparable to traditional scaling methods like best-of-N once constrained to similar levels of inference FLOPs. Moreover, while BF yields a 20-point improvement on English AIME, it provides only a 1.94-point average gain across other languages-a pattern consistent across the other test-time scaling methods we studied-higlighting that test-time scaling may not generalize as effectively to multilingual tasks. To foster further research, we release MCLM, MR1-1.5B, and evaluation results."
        },
        {
            "title": "Start",
            "content": "Linguistic Generalizability of Test-Time Scaling in Mathematical Reasoning Guijin Son1,2 Jiwoo Hong3 Hyunwoo Ko2 James Thorne3 Yonsei University1 OneLineAI2 KAIST AI3 spthsrbwls123@yonsei.ac.kr 5 2 0 F 4 2 ] . [ 1 7 0 4 7 1 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Scaling pre-training compute has proven effective for achieving multilinguality, but does the same hold for test-time scaling? In this work, we introduce MCLM, multilingual math benchmark featuring competition-level problems in 55 languages. We test three test-time scaling methodsOutcome Reward Modeling (ORM), Process Reward Modeling (PRM), and Budget Forcing (BF)on both Qwen2.5-1.5B Math and MR1-1.5B, multilingual LLM we trained for extended reasoning. Our experiments show that using Qwen2.5-1.5B Math with ORM achieves score of 35.8 on MCLM, while BF on MR1-1.5B attains 35.2. Although thinking LLMs have recently garnered significant attention, we find that their performance is comparable to traditional scaling methods like best-of-N once constrained to similar levels of inference FLOPs. Moreover, while BF yields 20-point improvement on English AIME, it provides only 1.94-point average gain across other languagesa pattern consistent across the other test-time scaling methods we studiedhighlighting that test-time scaling may not generalize as effectively to multilingual tasks. To foster further research, we release MCLM, MR1-1.5B, and evaluation results."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have achieved impressive gains across wide range of tasks by scaling compute during pre-training (Thoppilan et al., 2022; Smith et al., 2022). Contrary to early concerns about so-called curse of multilinguality, (Conneau et al., 2020; Pfeiffer et al., 2022) which suggested that training in diverse languages would degrade overall performance, sufficiently large decoder-only architectures have demonstrated strong multilingual capabilities (Dubey et al., 2024; Aryabumi et al., 2024b). 1https://github.com/gauss5930/MCLM 1 Figure 1: Performance of Qwen2.5-1.5B-Math with different test-time scaling strategies.Once configured to use comparable inference FLOPs, all three methods (Outcome Reward Modeling, Process Reward Modeling, and Budget Forcing) achieve similar performance. Yet as further scaling becomes increasingly difficultdue to data scarcity (Longpre et al., 2024), diminishing returns, or prohibitive costs (Achiam et al., 2023)researchers have begun exploring test-time scaling methods that expand models reasoning or generation capacity at test time. An intriguing question arises: Does test-time scaling confer the same cross-lingual benefits we see at train-time scaling during pre-training? Early studies demonstrated that chain-of-thought prompting (Wei et al., 2022) and scratchpads (Nye et al., 2021) can significantly boost model performanceparticularly in mathematics (Lewkowycz et al., 2022; Azerbayev et al., 2023) and code (Le et al., 2022; Chae et al., 2024). Building on this, recent work proposes test-time scaling, which further lengthens the chain-of-thought (Snell et al., 2025; Muennighoff et al., 2025). While such methods have proven effective for puzzles like Sudoku (Jellyfish042, 2024) and Hex (Jones, 2021), where action spaces are limited, mathematical reasoning remains relatively unexplored, largely due to its exponentially larger search space. To address this challenge, researchers have investigated external verifierssuch as best-of-N selection (Wang et al., 2022), Monte Carlo Tree Search (Guan et al., 2025; Tian et al., 2024; Feng et al., 2023), and process/outcome reward modeling (Zhang et al., 2025; Liu et al., 2025). Meanwhile, state-of-theart LLMs (OpenAI, 2024, 2025) are capable of self-correctionoften referred to as system 2 reasoning (Xiang et al., 2025)without explicit external verification. While longer chains of reasoning provide more room for in-depth thinking, they may also amplify the risk of error propagation (Bengio et al., 2015; Arora et al., 2022; Holtzman et al., 2019), making them more susceptible to out-of-domain disturbances such as language variation (Zhao et al., 2023; Chen et al., 2024b). From this vein, it remains unclear whether these strategies robustly generalize to new questions (SRI_Lab, 2025), languages, or domains. In this work, we investigate the linguistic generalizability of test-time scaling methods by proposing fine-grained multilingual complex reasoning benchmark, showing that test-time scaling alone does not yield robust multilingual performance. We build MCLM (Multilingual Competition Level Math), math reasoning dataset composed of four subsets varying source covering 55 languages. We analyze three test-time scaling methods, outcome reward modeling (Wang et al., 2022, ORM), process reward modeling (Zhang et al., 2025, PRM), and budget forcing (Muennighoff et al., 2025, BF). We examine (1) accuracy to determine whether models retain overall performance across languages and (2) consistency to observe whether models can solve the same questions in different languages. While ORM and PRM provide clear gains on relatively easy datasets, the improvements are marginal for challenging tasks and inconsistent across the languages. Meantime, BF delivers noticeable gains only in English for tougher questions, with minimal impact on other languages. These findings underscore that while test-time scaling can enhance accuracy under certain conditions, it does not guarantee robust or consistent performance across multiple languages. Finally, we introduce MR1-1.5B, an open multilingual thinking LLM trained on Deepseek-R11.5B using 100k R1-distilled instances translated by GPT-4o. Despite having only 1.5B parameters, MR1 achieves performance on par with GPT-4oMini in multilingual mathematical reasoning."
        },
        {
            "title": "2 Multilingual Competition Level Math",
            "content": "In this section, we introduce Multilingual Competition Level Math (MCLM), multilingual math reaModels MGSM Gemma2-9B Qwen2.5-14B-Instruct Qwen2.5-72B-Instruct Mistral-Large GPT-4o-mini o3-mini 78.37 82. 88.16 89.01 87.36 89.30 Table 1: MGSM performance of different models. The 2025-01-31 version is used for o3-mini, remaining scores were sourced from the Yang et al. (2024b). soning benchmark with challenging competitionlevel questions in 55 languages. Going beyond math word problems translated version of GSM8K (Cobbe et al., 2021), MGSM (Shi et al., 2022), has been widely used to assess the mathematical reasoning skills of multilingual LLMs (Anil et al., 2023; Shao et al., 2024; Aryabumi et al., 2024a). However, in Table 1, we observe that recent LLMs saturate MGSM. This implies the limitations of simple math word problems in accurately assessing the math reasoning capabilities of LLMs and necessitates higher degree of complexity in reasoning benchmarks. Assessing complex reasoning capabilities In this vein, recent studies have evaluated the reasoning capabilities of LLMs using competition-level math questions (MAA, 2024; Gao et al., 2024). While these benchmarks address limitations in simple math word problems, they are largely restricted to English and Chinese, limiting their ability to study multilingualism at scale."
        },
        {
            "title": "2.1 Curating the MCLM benchmark",
            "content": "Machine-translated reasoning We select AIME and MATH-500 (Lightman et al., 2023), two widely used mathematical benchmarks, as the main source of complex math questions. For 100 questions randomly sampled from MATH-500 and full AIME datasets, we translate both benchmarks with GPT4o (OpenAI et al., 2024), as shown to be proficient in translating mathematical contexts (Chen et al., 2024a; Lai et al., 2023). We then verified that the answers and equations remained unchanged after translation, removing one sample from MATH500 due to translation inconsistencies. Both subsets consist of questions with numerical answers only. For further details on the machine translation and sampling process, see Appendix A.2. 2 Subset Source Benchmark Languages Sample Size per Language Evaluation Method MT-MATH100 Math-500 MT-AIME2024 AIME 2024 M-IMO M-MO IMO (2006, 2024) Domestic/Regional Olympiads 55 55 38 11 100 30 2227 2831 Rule-based verifier Rule-based verifier LLM-as-a-Judge LLM-as-a-Judge Table 2: Overview of benchmark subsets: source benchmarks, language coverage (full lists in the appendix), sample sizes, and evaluation methods. Please see Appendix A.1 for the full list of languages. Human-annotated reasoning To mitigate the potential biases in machine-translated data from translation artifacts (Plaza et al., 2024; Son et al., 2024), we also include human-translated or originally written questions. First, we manually review 114 International Mathematical Olympiad (IMO, 2024, IMO) questions from 2006 to 2024 in English, excluding proof-based and image-heavy problems, resulting in final set of 27. We then collect their official translations in 38 languages. Where official translations are unavailable, we do not substitute machinegenerated versions, leaving those entries missing. Second, we gather problems from various domestic and regional mathematical Olympiads worldwide. These contests originate in multiple languages, providing valuable data for multilingual mathematical reasoning. For English, Chinese, and Koreanwhere competition-level benchmarks already exist (He et al., 2024; Ko et al., 2025)we incorporate existing datasets rather than recollecting data. While we exclude proof-based questions for simplicity, the final dataset still features diverse range of answer formats (e.g., numerical, Boolean, descriptive) and spans 11 languages. We use GPT4o-mini1 for evaluation. An overview is provided in Table 2, additional details are in Appendix A.3."
        },
        {
            "title": "3 Experimental Settings",
            "content": "In this section, we provide an overview of the test-time scaling methods evaluated (Section 3.1), compare the inference budgets in terms of FLOPs across different scaling techniques (Section 3.2), and describe the evaluation metrics used to assess their performance (Section 3.3)."
        },
        {
            "title": "3.1 Baselines: Test-Time Scaling Strategies",
            "content": "In this work, we evaluate three test-time scaling strategies using Qwen2.5-1.5B and 7B instruct models (Yang et al., 2024a) as baselines (Figure 2). We selected these model sizes because they offer 12024-07-18 version (OpenAI et al., 2024) Figure 2: Comparison of different inference-time scaling strategies. Blue boxes represent selected outputs, while red boxes indicate rejected ones. balanced trade-off between reasoning capacity and computational efficiency. Models smaller than 1.5B lack the capacity to solve complex problems, while larger models can be prohibitively expensive to scale (Biderman et al., 2023). Outcome Reward Modeling We generate responses per instance and use Qwen2.5-Math-72BRM (Yang et al., 2024b) to evaluate them, selecting the highest-scoring answer as the final output. Process Reward Modeling In contrast to outcome reward modeling, this strategy integrates the reward model during inference to guide the generation process. We employ Qwen2.5-Math-72BPRM (Zhang et al., 2025); the model generates candidate continuations at each step and selects the best one. For both ORM and PRM, the generator and reward model are served on separate servers, thereby avoiding the overhead of repeatedly onand off-loading model weights. Budget Forcing Recent LLMs, such as R1 (Guo et al., 2025) and O1 (OpenAI, 2024), are designed to generate longer chain-of-thoughts with incontext exploration and correction, allowing them to naturally scale during inference. However, this approach lacks controllability. To mitigate this, we adopt the budget-forcing method proposed by Muennighoff et al. (2025). In budget forcing, these thinking models are truncated and required to output an answer if they exceed predefined budget. Conversely, if they fall short of the budget, they are prompted to generate additional reasoning steps, 3 2 4 8 (S, c) BF (3, 3) 2048 tokens (4, 5) 4096 tokens (5, 8) 8192 tokens Table 3: Selected configurations for PRM and BF. Each S, c, and BF is set so that the inference FLOPs match ORM. Assuming NG = 1.5 109 and NV = 72 109, this configuration results in an estimated cost of approximately 6.10 1012 FLOPs per instance. Process Reward Modeling In PRM, at each generation step, the model produces candidates, with each candidate generating tokens (we fix = 128 in our experiments). The total inference cost over steps is given by PRM FLOPs = (cid:16) 2NG + 4NV (cid:17) . (3) For PRM configurations, and c, our preliminary experiments indicated that scaling one parameter in isolation produced suboptimal performance: high with low failed to explore sufficient alternatives, while an excessively low prevented the generation process from completing. Therefore, we opted to proportionally scale both and to achieve balanced search during generation. Budget Forcing In contrast, BF relies solely on the generator, so its inference cost is given by SC FLOPs = 2NG BF, (4) where BF denotes the effective number of tokens that may be generated during the inference with budget forcing. In Table 3, we select the PRM parameters and and adjust the BF token budget BF so that the overall inference cost of each method matches that of ORM for = 2, 4, and 8."
        },
        {
            "title": "3.3 Evaluation Metrics",
            "content": "We evaluate our models using two primary metrics that capture performance at multiple levels: (1) accuracy and (2) cross-lingual consistency. Accuracy We measure accuracy at surface level to determine whether single model achieves comparable performance across different languages. Cross-Lingual Consistency To examine whether the model tends to solve (or fail) the same questions across languages, we compute Fleiss kappa (Fleiss, 1971), which is originally designed to measure agreement among multiple annotators. In our Figure 3: # of generated tokens for 1.5B and 7B models in greedy setting, divided by correctness. Languages are represented as scatter plots, overlaid on box plots. encouraging further exploration and correction."
        },
        {
            "title": "3.2 Calculating Inference FLOPs",
            "content": "For our experiments, we first establish unified inference budget based on two key estimates: the generators cost is approximated as 2NGD (Kaplan et al., 2020), where NG is the number of parameters in the generation model and is the total number of tokens generated per instance. The verifiers (or reward models) cost is estimated as 4NV , where NV is the number of parameters in the verifier. Here, the multiplier of 4 for the reward models cost reflects base cost of 2NV that is doubled to account for the additional overhead incurred when invoking the reward model during inference (Snell et al., 2025). In the ORM, we generate responses per instance, leading to total inference cost of (cid:16) 2NGD + 4NV (cid:17) . (1) In Figure 3, under greedy generation, we observe that models tend to generate longer responses once they produce an error, particularly on harder benchmarks. Additionally, the 7B model generally produces longer outputs than the 1.5B model. However, no systematic trends emerge across the 55 languagesthere is no clear pattern, such as longer outputs for low-resource languages. Although token counts vary with configuration, these differences are negligible compared to the dominant effect of model size on inference FLOPs. Consequently, we use an average of 921 tokens per question to estimate cost. In ORM with = Outcome Reward Modeling 2 responses, the inference cost is approximated as ORM FLOPs 2 (cid:16) 2NG 921 + 4NV (cid:17) . (2) 4 Figure 4: Gains of ORM compared to greedy-decoding baseline. The semi-transparent cloud indicates the 2D data distribution via KDE density plot, and the overlaid lines are third-order polynomial regressions modeling how each ORM setting scales with the baseline score. setup, however, we treat each language as an annotator: for each problem, each annotator (i.e., each language version of the model) provides either correct or incorrect label. We then define consistency through Fleiss kappa as: κ = Pe 1 Pe (cid:88) j=1 ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 1 n(n 1) nij(nij 1) (5) (6) Pe = (cid:88) j=1 p2 , pj ="
        },
        {
            "title": "1\nN n",
            "content": "N (cid:88) i=1 nij, (7) where is the number of problems, is the number of languages, and nij is the count of how many times language gives particular label (correct or incorrect) for problem i. In this formulation, is the observed agreement (i.e., the proportion of problems for which all languages concur on correctness or incorrectness), and Pe is the expected agreement by chance. high Fleiss kappa indicates that the model responds consistently across languages (solving the same problems), not merely achieving similar overall accuracy by chance."
        },
        {
            "title": "4 Result 1: ORM and PRM",
            "content": "In this section, we assess the multilingual robustness of both ORM and PRM. We find that, while each approach can boost performance, these gains do not consistently generalize across different languages and levels of difficulty. Figure 5: PRM inference FLOPs as function of generation steps and candidates per step c. The left panel uses verifier size of 72B, while the right panel uses 7B RM, displaying adjusted configurations to yield similar costs. and then apply Qwen2.5-Math-72B to score each sample, selecting the one with the highest score. Limited gains at scale in non-English settings In Figure 4, we plot each models baseline performance (averaged across 55 languages) on the x-axis versus the relative gain of each ORM setting (with {2, 4, 8}) on the y-axis. On the MT-MATH100 dataset, both the 1.5B and 7B models show consistent improvement as increases. However, on the more challenging MT-AIME2024 dataset, the gains for different values are largely indistinguishable and, in some cases, even negative. This trend is comparable to English, which shows steady improvements also on MT-AIME2024for instance, the 1.5B model rises from 16.67 to 26.67 to 36.67 as increases, while the 7B model goes from 20.00 to 26.67 to 36.67. Overall, while ORM is viable scaling strategy in English, it yields limited returns in many other languagespossibly due to the models difficulty in generating high-quality candidates. With few plausible options available, the reward model cannot effectively identify an improved solution."
        },
        {
            "title": "4.2 Process Reward Modeling",
            "content": "Along with the configurations mentioned in Table 3, we experiment with additional setups to study how PRMs scale. Details are provided in Figure 5. In general, we evaluate two approaches: fixing while increasing (pink) and fixing while scaling (green). Additionally, we compare the efficacy of 7B verifier against the original 72B. Due to cost constraints, these configurations are tested only on 14 languages of MT-MATH100 using Qwen2.5-Math-1.5B-Instruct."
        },
        {
            "title": "4.1 Outcome Reward Modeling",
            "content": "For ORM, we use Qwen2.5-Math-1.5B and 7BInstruct models to generate samples per query No scalable gains for variance or consistency Figure 6 shows that, in PRM, the average performance of Qwen2.5-Math-1.5B-Instruct increases 5 Figure 6: Inference FLOPs versus PRM performance and consistency. (Left) Second-degree polynomial regressions for average performance on 14 languages, comparing the 7B (blue) and 72B (green) reward models. (Right) Fleiss kappa (top) and standard deviation (bottom) plotted against the same FLOPs budget; the fitted curves reveal no clear monotonic trend. steadily with the inference budget. Even though the 7B reward model provides larger search space, the 72B reward model achieves better outcomes under comparable compute. From hardware standpoint, it can be more effective to run fewer steps and rely on larger verifier. However, no clear pattern emerges for Fleiss kappa or the standard deviation of individual scores, suggesting that adding more budget does not necessarily improve model consistency or reduce variance. In practical terms, while accuracy may scale with compute for PRM, crosslingual consistency does not appear to follow, even for relatively easier dataset like MT-MATH100."
        },
        {
            "title": "4.3 ORM over PRM",
            "content": "As discussed earlier, both ORM and PRM exhibit unstable multilingual performance growth, with greater variance and lower Fleiss Kappa scores at higher inference FLOPs. However, despite this instability, as shown in Figure 7, ORM consistently outperforms PRM in average accuracy, suggesting that, in general, it may be the more reliable choice. This is especially true given that, despite being assigned the same inference FLOPs, PRM invokes the reward model more frequently, requiring iterative back-and-forth interactions between the generator and evaluator, leading to higher latency. Figure 7: Comparison of PRM vs. ORM performance on MATH (solid lines) and AIME (dashed lines). 1.5B models are shown with plus markers, 7B models with stars. Blue lines represent PRM, green lines represent ORM. White box annotations indicate the performance difference (ORM PRM) at the highest compute setting for each line. refining responses within single inference (without external verifiers), these models seek to dramatically expand the inference budget for improved performance. In this section, we examine their effectiveness as test-time scaling strategy. Because proprietary solutions remain largely opaque, we train our own LLMs with system 2 thinking. Here, we describe our training methods (Section 5.1), report performance on MCLM (Section 5.2) and scaling affects from budget forcing(Section 5.3)."
        },
        {
            "title": "5 Result 2: Budget Forcing",
            "content": "5.1 Inducing System 2 Thinking LLMs with system 2 reasoning (Xiang et al., 2025)such as OpenAIs newest O-series models (OpenAI, 2024), Googles Gemini Thinking1, and Deepseek R1 (Guo et al., 2025)are emerging as test-time scaling approach. By generating and 1https://deepmind.google/technologies/gemini/flashthinking/ number of concurrent works propose diverse strategies for developing LLMs with long thinking. Broadly, these approaches fall into two main categories: (1) online reinforcement learning with verifiable outputs (Luo et al., 2025; Ye et al., 2025a), and (2) supervised fine-tuning on the \"thinking trajectories\" of proprietary LLMs (Muennighoff et al., 6 Models MT-MATH100 MT-AIME2024 M-IMO M-MO Average Qwen2.5-Math-1.5B-Instruct Deepseek-R1-1.5B GPT-4o-Mini o3-Mini 42.32 8.61 49.40 8.84 70.30 3.68 84.89 2.80 Qwen2.5-Math-1.5B + SFT Qwen2.5-Math-1.5B + MT-SFT Deepseek-R1-1.5B + MT-SFT 37.47 7.56 42.02 7.46 55.61 10.93 16.36 6.89 17.21 6.69 20.18 6.83 45.33 5.35 14.85 6.69 16.67 7.31 19.94 8. 12.23 6.02 21.94 6.75 13.33 5.36 29.75 6.86 10.50 5.16 10.52 4.63 19.20 6.24 25.00 19.10 26.77 19.83 30.81 15.80 51.42 16.94 18.40 14.92 19.92 12.68 28.97 16.64 23.98 28.83 33.66 52.85 20.30 22.28 30. Table 4: Model performance across MCLM. Best model highlighted in bold for each panel. For results per language see Appendix C. 2025; Ye et al., 2025b). However, Luo et al. (2025) reports requiring 3,800 A100 GPU hours to induce such behavior, making it prohibitively expensive for our setting. Instead, we opt for supervised finetuning using thinking trajectories distilled from R1. Below are three training configurations. Qwen2.5-Math-1.5B + SFT Following Muennighoff et al. (2025) and Huang et al. (2024), we randomly sample 50K thinking trajectories generated by R1 from the OpenR1-220K 1 dataset and fine-tune our model for three epochs. Qwen2.5-Math-1.5B + SFT with Translated Data While training exclusively on English math problems has been shown to be effective and can generalize to new languages to some extent (Liu et al., 2024)likely due to the universal nature of mathematical logicwe explore whether translating the data helps training. Following Ko et al. (2025); Zhang et al. (2023), we translate the problem and solution components into 14 languages2 using GPT-4o, while keeping the reasoning process in English to leverage the models strong proficiency in English-based logical reasoning. For further details on the training dataset, see Appendix B. Deepseek-R1-1.5B + SFT with Translated Data Finally, we try initializing the translated SFT from Deepseek-R1-1.5B (hereafter MR1-1.5B). Since the model is already proficient in generating extended reasoning, we observe that longer training leads to performance degradation. To mitigate this, we terminate training at 0.5 epochs."
        },
        {
            "title": "5.2 Performance of trained models",
            "content": "Translated data improves cross-lingual performance. Table 4 compares Qwen2.5-Math-1.5B and Deepseek-R1-1.5B under various fine-tuning Figure 8: Performance of Qwen2.5-Math-1.5B +SFT and + MT-SFT at each training checkpoint. Average score and error bars for each checkpoint are displayed. The shaded region is the mean standard deviation for MT-SFT. regimes, revealing two key trends. First, incorporating translated data into Qwen2.5-Math-1.5B delivers modest +1.98% improvement over an English-only setup, indicating that relying exclusively on English data is insufficient for robust cross-lingual performance (Liu et al., 2024). As shown in Figure 8, models trained on multilingual inputs begin with lower accuracylikely due to increased entropybut soon surpass their Englishonly counterparts and maintain lower variance across languages. Second, initiating fine-tuning from Deepseek-R1-1.5B, already adept at extended chain-of-thought reasoning, yields even greater gains on MT-AIME2024, M-IMO, and M-MO, performing on par with GPT-4o-Mini. Notably, MR11.5B reaches an average score of 30.93 (+2.1% over its original baseline) with just 0.5 epochs of training, underscoring how self-correcting model more readily benefits from multilingual data. Collectively, these results suggest that while incorporating translated data benefits monolingual base, leveraging model with established self-correction capabilities can amplify these gains in multilingual math reasoning."
        },
        {
            "title": "5.3 Budget-Constrained Scaling",
            "content": "1https://huggingface.co/datasets/open-r1 2See Appendix A.1 for the complete list of languages. To better compare self-correction with other scaling methods (e.g., ORM and PRM), we examine 7 Intuitively, the reasoning capacity of an LLM is limited by the number of tokens it can generate; hence, more challenging questions may require longer chain of thought (Wu et al., 2025). An early example is self-consistency CoT (Wang et al., 2022), which generates multiple responses and selects the best via voting. This idea has since been developed into more cost-effective strategies for searching broader solution spaces (e.g., tree-ofthought methods (Yao et al., 2024), Monte Carlo Tree Search (Guan et al., 2025), and process supervision (Zhang et al., 2025; Luo et al., 2024)). Recently, models trained with online reinforcement learning (Shao et al., 2024) appear to exhibit an aha moment, (Guo et al., 2025) wherein they dynamically decide to generate longer sequences to iteratively explore, solve, and self-correct. Mathematical Reasoning in Non-English Early attempts at multilingual math reasoning involved supervised fine-tuning on translated datasets (Chen et al., 2023; Lai and Nissim, 2024), but performance often deteriorated when models shifted away from their original language embeddings (Hong et al., 2024). To minimize such degradation, more recent work has increasingly relied on English as pivot language. This approach can be implemented in various ways: either internally, by mapping multilingual inputs into an English-centric latent space (Yoon et al., 2024; Fan et al., 2025; Zhu et al., 2024; She et al., 2024), or externally, by translating non-English tasks into English and then back to the target language (Zhang et al., 2023; Ko et al., 2025). Although this strategy has reduced the performance gap between English and other languages, the stability of transfer under different training conditions remains underexplored. Moreover, many studies rely on the MGSM benchmark (Shi et al., 2022), which appears too easy for large-scale models or those enhanced by advanced reasoning techniques such as test-time scaling."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we examine the linguistic generalizability of three test-time scaling methodologiesOutcome Reward Modeling (ORM), Process Reward Modeling (PRM), and Budget Forcing (BF)under budget-constrained settings. Using Qwen2.5-1.5B Math as generator, ORM achieves 35.84 score on our newly introduced multilingual math benchmark, MCLM, which spans 55 Figure 9: Performance of MR1 on MT-AIME2024 at BF = {2048, 4096, 8192}. Grey dots represent individual languages. Solid lines indicate average performance, while dashed lines highlight reference performances for selected languages. its performance under fixed inference budgets. We apply the budget forcing approach introduced by Muennighoff et al. (2025) to constrain the generation budget of MR1-1.5B. Following the budget settings in Table 3, during inference, if the model reaches 90% of its allocated budget, we truncate the output and append \"The final answer is\" to prompt concise answer. Conversely, if the model completes generation before reaching the limit, we truncate at the last line break, append \"Wait...\", and prompt the model to continue generating. R1-like LLMs offer no clear edge over ORM Contrary to the recent surge of interest in system 2 LLMs, that scale test-time compute by generating long reasoning traces, constraining these models to the same inference budgets reveals no clear advantage over test-time scaling methods such as ORM or PRM (see Figure 1). As shown in Figure 9, budget forcing yields nearly linear performance gains for English but provides limited benefits for most other languages. The distribution remains largely unchanged, achieving an overall average increase of only 1.9% as BF scales from 2048 to 8096. In some cases, such as Latvian and Romanian, performance even declines. Implying that there is scant evidence that the variance in performance diminishes or that coverage expands uniformly."
        },
        {
            "title": "6 Related Works",
            "content": "Test-Time Scaling As concerns grow that the benefits of scaling pre-training compute may be saturating (Longpre et al., 2024), research has shifted toward test-time scaling, which expands the notion of chain-of-thought reasoning (Wei et al., 2022). 8 languages. With BF, MR1-1.5Bour multilingual LLM demonstrating extended reasoningattains 35.23. Notably, once constrained to similar inference budgets, all three scaling methods exhibit comparable levels of improvement. Additionally, although these approaches appear promising in English (e.g., 20-point improvement on AIME for the 1.5B model), we find that such gains do not consistently extend to other languages, where improvements average only 1.94 pointsa pattern observed across all three methods. Moreover, increasing test-time compute often amplifies performance variance and reduces cross-linguistic consistency. To enable further study of these issues, we release both MCLM and MR1-1.5B."
        },
        {
            "title": "Limitations",
            "content": "Although this work focuses solely on mathematical tasks, the lack of multilingual generalization we observe could be even more pronounced in areas requiring extensive cultural or domain-specific understanding; we leave this for future works. Additionally, due to budget constraints, this work primarily focuses on smaller-scale experiments (e.g., Qwen2.5-Math-1.5B-Instruct and occasionally Qwen2.5-Math-7B-Instruct). Although these parameter ranges are commonly used in both industry and academia, the observed lack of multilingual generalization for test-time scaling may not necessarily extend to significantly larger models (70B or more). Moreover, our test-time scaling experiments also remain on the smaller side; for instance, El-Kishky et al. (2025) scale to as many as 1162 candidates in their best-of-n setting. (It should still be noted that even experiments at this scale required over 2500 A100 GPU hours) Given that the so-called curse of multilinguality (Conneau et al., 2020) naturally disappears as pre-training compute grows by several orders of magnitude (Aryabumi et al., 2024b), it is plausible that larger models may behave differently. Nevertheless, our findings at smaller scaleswhere test-time scaling shows no indication of fostering robust multilingualismremain valuable, as they reveal potential boundaries for less resource-rich setups and highlight directions for future research."
        },
        {
            "title": "References",
            "content": "Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy GurAri, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. 2023. Palm 2 technical report. Preprint, arXiv:2305.10403. Kushal Arora, Layla El Asri, Hareesh Bahuleyan, and Jackie Cheung. 2022. Why exposure bias matters: An imitation learning perspective of error accumulation in language generation. In Findings of the Association for Computational Linguistics: ACL 2022, pages 700710, Dublin, Ireland. Association for Computational Linguistics. Viraat Aryabumi, John Dang, Dwarak Talupuru, Saurabh Dash, David Cairuz, Hangyu Lin, Bharat Venkitesh, Madeline Smith, Jon Ander Campos, Yi Chern Tan, Kelly Marchisio, Max Bartolo, Sebastian Ruder, Acyr Locatelli, Julia Kreutzer, Nick Frosst, Aidan Gomez, Phil Blunsom, Marzieh Fadaee, Ahmet Üstün, and Sara Hooker. 2024a. Aya 23: Open weight releases to further multilingual progress. Preprint, arXiv:2405.15032. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Viraat Aryabumi, John Dang, Dwarak Talupuru, Saurabh Dash, David Cairuz, Hangyu Lin, Bharat Venkitesh, Madeline Smith, Jon Ander Campos, 9 Yi Chern Tan, et al. 2024b. Aya 23: Open weight releases to further multilingual progress. arXiv preprint arXiv:2405.15032. Axolotl AI. 2025. Axolotl: Scalable fine-tuning framehttps://axolotl-ai-cloud.github.io/ work for llms. axolotl/. Github. Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Jiang, Jia Deng, Stella Biderman, and Sean Welleck. 2023. Llemma: An open language model for mathematics. arXiv preprint arXiv:2310.10631. Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. 2015. Scheduled sampling for sequence prediction with recurrent neural networks. Advances in neural information processing systems, 28. Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. 2023. Pythia: suite for analyzing large language modIn International els across training and scaling. Conference on Machine Learning, pages 23972430. PMLR. Hyungjoo Chae, Yeonghyeon Kim, Seungone Kim, Kai Tzu-iunn Ong, Beong-woo Kwak, Moohyeon Kim, Seonghwan Kim, Taeyoon Kwon, Jiwan Chung, Youngjae Yu, et al. 2024. Language models as compilers: Simulating pseudocode execution improves algorithmic reasoning in language models. arXiv preprint arXiv:2404.02575. Nuo Chen, Zinan Zheng, Ning Wu, Ming Gong, Dongmei Zhang, and Jia Li. 2024a. Breaking language barriers in multilingual mathematical reasoning: Insights and observations. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 70017016, Miami, Florida, USA. Association for Computational Linguistics. In cross-lingual representation learning at scale. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 84408451, Online. Association for Computational Linguistics. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Ahmed El-Kishky, Alexander Wei, Andre Saraiva, Borys Minaev, Daniel Selsam, David Dohan, Francis Song, Hunter Lightman, Ignasi Clavera, Jakub Pachocki, et al. 2025. Competitive programming with large reasoning models. arXiv preprint arXiv:2502.06807. Yuchun Fan, Yongyu Mu, YiLin Wang, Lei Huang, Junhao Ruan, Bei Li, Tong Xiao, Shujian Huang, Xiaocheng Feng, and Jingbo Zhu. 2025. Slam: Towards efficient multilingual reasoning via selective language alignment. In Proceedings of the 31st International Conference on Computational Linguistics, pages 94999515. Association for Computational Linguistics. Xidong Feng, Ziyu Wan, Muning Wen, Stephen Marcus McAleer, Ying Wen, Weinan Zhang, and Jun Wang. 2023. Alphazero-like tree-search can guide large language model decoding and training. arXiv preprint arXiv:2309.17179. Joseph Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological bulletin, 76(5):378. Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, et al. 2024. Omni-math: universal olympiad level mathematic benchmark for large language models. arXiv preprint arXiv:2410.07985. Nuo Chen, Zinan Zheng, Ning Wu, Linjun Shou, Ming Gong, Yangqiu Song, Dongmei Zhang, and Jia Li. 2023. Breaking language barriers in multilingual mathematical reasoning: Insights and observations. arXiv preprint arXiv:2310.20246. Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang. 2025. rstar-math: Small llms can master math reasoning with self-evolved deep thinking. arXiv preprint arXiv:2501.04519. Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, et al. 2024b. Do not think that much for 2+ 3=? on the overthinking of o1-like llms. arXiv preprint arXiv:2412.21187. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. 2024. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751. 10 Jiwoo Hong, Noah Lee, Rodrigo Martínez-Castaño, César Rodríguez, and James Thorne. 2024. Crosslingual transfer of reward models in multilingual alignment. arXiv preprint arXiv:2410.18027. Pin-Lun Hsu, Yun Dai, Vignesh Kothapalli, Qingquan Song, Shao Tang, Siyu Zhu, Steven Shimizu, Shivam Sahni, Haowen Ning, and Yanning Chen. 2024. Liger kernel: Efficient triton kernels for llm training. arXiv preprint arXiv:2410.10989. Zhen Huang, Haoyang Zou, Xuefeng Li, Yixiu Liu, Yuxiang Zheng, Ethan Chern, Shijie Xia, Yiwei Qin, Weizhe Yuan, and Pengfei Liu. 2024. O1 replication journeypart 2: Surpassing o1-preview through simple distillation, big progress or bitter lesson? arXiv preprint arXiv:2411.16489. IMO. 2024. International Mathematical Olympiad Website. Accessed: 2024-01-31. Jellyfish042. 2024. Sudoku-rwkv: specialized rwkv model for solving sudoku puzzles. https://github. com/Jellyfish042/Sudoku-RWKV. Accessed: 202502-11. Andy L. Jones. 2021. Scaling scaling laws with board games. arXiv preprint arXiv:2104.03113. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361. Hyunwoo Ko, Guijin Son, and Dasol Choi. 2025. Understand, solve and translate: Bridging the multilingual mathematical reasoning gap. arXiv preprint arXiv:2501.02448. Bespoke Labs. 2025. Bespoke-stratos-17k dataset. Accessed: February 1, 2025. Huiyuan Lai and Malvina Nissim. 2024. mcot: Multilingual instruction tuning for reasoning consistency in language models. arXiv preprint arXiv:2406.02301. Viet Dac Lai, Chien Van Nguyen, Nghia Trung Ngo, Thuat Nguyen, Franck Dernoncourt, Ryan Rossi, and Thien Huu Nguyen. 2023. Okapi: Instructiontuned large language models in multiple languages with reinforcement learning from human feedback. arXiv preprint arXiv:2307.16039. Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Chu Hong Hoi. 2022. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. Advances in Neural Information Processing Systems, 35:21314 21328. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. 2022. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:3843 3857. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Lets verify step by step. arXiv preprint arXiv:2305.20050. Pengfei Liu, Yiming Wang, Tianyu Gao, et al. 2025. through implicit rewards. Process reinforcement arXiv preprint arXiv:2502.01456. Zihan Liu, Yang Chen, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. 2024. Acemath: Advancing frontier math reasoning with post-training and reward modeling. arXiv preprint arXiv:2412.15084. Shayne Longpre, Robert Mahari, Ariel Lee, Campbell Lund, Hamidah Oderinwale, William Brannon, Nayan Saxena, Naana Obeng-Marnu, Tobin South, Cole Hunter, et al. 2024. Consent in crisis: The rapid decline of the ai data commons. In NEURIPS. Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, et al. 2024. Improve mathematical reasoning in language models by automated process supervision. arXiv preprint arXiv:2406.06592. Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Tianjun Zhang, Erran Li, Raluca Ada Popa, and Ion Stoica. 2025. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl. https://pretty-radio-b75.notion.site/DeepScaleRSurpassing-O1-Preview-with-a-1-5B-Model-byScaling-RL-19681902c1468005bed8ca303013a4e2. Notion Blog. MAA. 2024. American invitational mathematics examination - aime. In American Invitational Mathematics Examination - AIME 2024. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and s1: Simple test-time Tatsunori Hashimoto. 2025. scaling. Preprint, arXiv:2501.19393. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114. Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, et al. 2024. 2 olmo 2 furious. arXiv preprint arXiv:2501.00656. OpenAI, :, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander adry, Alex Baker-Whitcomb, 11 Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex Nichol, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexander Kirillov, Alexi Christakis, Alexis Conneau, Ali Kamali, Allan Jabri, Allison Moyer, Allison Tam, Amadou Crookes, Amin Tootoochian, Amin Tootoonchian, Ananya Kumar, Andrea Vallone, Andrej Karpathy, Andrew Braunstein, Andrew Cann, Andrew Codispoti, Andrew Galu, Andrew Kondrich, Andrew Tulloch, Andrey Mishchenko, Angela Baek, Angela Jiang, Antoine Pelisse, Antonia Woodford, Anuj Gosalia, Arka Dhar, Ashley Pantuliano, Avi Nayak, Avital Oliver, Barret Zoph, Behrooz Ghorbani, Ben Leimberger, Ben Rossen, Ben Sokolowsky, Ben Wang, Benjamin Zweig, Beth Hoover, Blake Samic, Bob McGrew, Bobby Spero, Bogo Giertler, Bowen Cheng, Brad Lightcap, Brandon Walkin, Brendan Quinn, Brian Guarraci, Brian Hsu, Bright Kellogg, Brydon Eastman, Camillo Lugaresi, Carroll Wainwright, Cary Bassin, Cary Hudson, Casey Chu, Chad Nelson, Chak Li, Chan Jun Shern, Channing Conger, Charlotte Barette, Chelsea Voss, Chen Ding, Cheng Lu, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christina Kim, Christine Choi, Christine McLeavey, Christopher Hesse, Claudia Fischer, Clemens Winter, Coley Czarnecki, Colin Jarvis, Colin Wei, Constantin Koumouzelis, Dane Sherburn, Daniel Kappler, Daniel Levin, Daniel Levy, David Carr, David Farhi, David Mely, David Robinson, David Sasaki, Denny Jin, Dev Valladares, Dimitris Tsipras, Doug Li, Duc Phong Nguyen, Duncan Findlay, Edede Oiwoh, Edmund Wong, Ehsan Asdar, Elizabeth Proehl, Elizabeth Yang, Eric Antonow, Eric Kramer, Eric Peterson, Eric Sigler, Eric Wallace, Eugene Brevdo, Evan Mays, Farzad Khorasani, Felipe Petroski Such, Filippo Raso, Francis Zhang, Fred von Lohmann, Freddie Sulit, Gabriel Goh, Gene Oden, Geoff Salmon, Giulio Starace, Greg Brockman, Hadi Salman, Haiming Bao, Haitang Hu, Hannah Wong, Haoyu Wang, Heather Schmidt, Heather Whitney, Heewoo Jun, Hendrik Kirchner, Henrique Ponde de Oliveira Pinto, Hongyu Ren, Huiwen Chang, Hyung Won Chung, Ian Kivlichan, Ian OConnell, Ian OConnell, Ian Osband, Ian Silber, Ian Sohl, Ibrahim Okuyucu, Ikai Lan, Ilya Kostrikov, Ilya Sutskever, Ingmar Kanitscheider, Ishaan Gulrajani, Jacob Coxon, Jacob Menick, Jakub Pachocki, James Aung, James Betker, James Crooks, James Lennon, Jamie Kiros, Jan Leike, Jane Park, Jason Kwon, Jason Phang, Jason Teplitz, Jason Wei, Jason Wolfe, Jay Chen, Jeff Harris, Jenia Varavva, Jessica Gan Lee, Jessica Shieh, Ji Lin, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joanne Jang, Joaquin Quinonero Candela, Joe Beutler, Joe Landers, Joel Parish, Johannes Heidecke, John Schulman, Jonathan Lachman, Jonathan McKay, Jonathan Uesato, Jonathan Ward, Jong Wook Kim, Joost Huizinga, Jordan Sitkin, Jos Kraaijeveld, Josh Gross, Josh Kaplan, Josh Snyder, Joshua Achiam, Joy Jiao, Joyce Lee, Juntang Zhuang, Justyn Harriman, Kai Fricke, Kai Hayashi, Karan Singhal, Katy Shi, Kavin Karthik, Kayla Wood, Kendra Rimbach, Kenny Hsu, Kenny Nguyen, Keren Gu-Lemberg, Kevin Button, Kevin Liu, Kiel Howe, Krithika Muthukumar, Kyle Luther, Lama Ahmad, Larry Kai, Lauren Itow, Lauren Workman, Leher Pathak, Leo Chen, Li Jing, Lia Guy, Liam Fedus, Liang Zhou, Lien Mamitsuka, Lilian Weng, Lindsay McCallum, Lindsey Held, Long Ouyang, Louis Feuvrier, Lu Zhang, Lukas Kondraciuk, Lukasz Kaiser, Luke Hewitt, Luke Metz, Lyric Doshi, Mada Aflak, Maddie Simens, Madelaine Boyd, Madeleine Thompson, Marat Dukhan, Mark Chen, Mark Gray, Mark Hudnall, Marvin Zhang, Marwan Aljubeh, Mateusz Litwin, Matthew Zeng, Max Johnson, Maya Shetty, Mayank Gupta, Meghan Shah, Mehmet Yatbaz, Meng Jia Yang, Mengchao Zhong, Mia Glaese, Mianna Chen, Michael Janner, Michael Lampe, Michael Petrov, Michael Wu, Michele Wang, Michelle Fradin, Michelle Pokrass, Miguel Castro, Miguel Oom Temudo de Castro, Mikhail Pavlov, Miles Brundage, Miles Wang, Minal Khan, Mira Murati, Mo Bavarian, Molly Lin, Murat Yesildal, Nacho Soto, Natalia Gimelshein, Natalie Cone, Natalie Staudacher, Natalie Summers, Natan LaFontaine, Neil Chowdhury, Nick Ryder, Nick Stathas, Nick Turley, Nik Tezak, Niko Felix, Nithanth Kudige, Nitish Keskar, Noah Deutsch, Noel Bundick, Nora Puckett, Ofir Nachum, Ola Okelola, Oleg Boiko, Oleg Murk, Oliver Jaffe, Olivia Watkins, Olivier Godement, Owen Campbell-Moore, Patrick Chao, Paul McMillan, Pavel Belov, Peng Su, Peter Bak, Peter Bakkum, Peter Deng, Peter Dolan, Peter Hoeschele, Peter Welinder, Phil Tillet, Philip Pronin, Philippe Tillet, Prafulla Dhariwal, Qiming Yuan, Rachel Dias, Rachel Lim, Rahul Arora, Rajan Troll, Randall Lin, Rapha Gontijo Lopes, Raul Puri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza Zamani, Ricky Wang, Rob Donnelly, Rob Honsby, Rocky Smith, Rohan Sahai, Rohit Ramchandani, Romain Huet, Rory Carmichael, Rowan Zellers, Roy Chen, Ruby Chen, Ruslan Nigmatullin, Ryan Cheu, Saachi Jain, Sam Altman, Sam Schoenholz, Sam Toizer, Samuel Miserendino, Sandhini Agarwal, Sara Culver, Scott Ethersmith, Scott Gray, Sean Grove, Sean Metzger, Shamez Hermani, Shantanu Jain, Shengjia Zhao, Sherwin Wu, Shino Jomoto, Shirong Wu, Shuaiqi, Xia, Sonia Phene, Spencer Papay, Srinivas Narayanan, Steve Coffey, Steve Lee, Stewart Hall, Suchir Balaji, Tal Broda, Tal Stramer, Tao Xu, Tarun Gogineni, Taya Christianson, Ted Sanders, Tejal Patwardhan, Thomas Cunninghman, Thomas Degry, Thomas Dimson, Thomas Raoux, Thomas Shadwell, Tianhao Zheng, Todd Underwood, Todor Markov, Toki Sherbakov, Tom Rubin, Tom Stasi, Tomer Kaftan, Tristan Heywood, Troy Peterson, Tyce Walters, Tyna Eloundou, Valerie Qi, Veit Moeller, Vinnie Monaco, Vishal Kuo, Vlad Fomenko, Wayne Chang, Weiyi Zheng, Wenda Zhou, Wesam Manassra, Will Sheu, Wojciech Zaremba, Yash Patil, Yilei Qian, Yongjik Kim, Youlong Cheng, Yu Zhang, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, and Yury Malkov. 2024. Gpt-4o system card. Preprint, arXiv:2410.21276. OpenAI. 2024. Openai o1 system card. arXiv preprint arXiv:2412.16720. OpenAI. 2025. Openai o3-mini system card. 12 Jonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li, James Cross, Sebastian Riedel, and Mikel Artetxe. 2022. Lifting the curse of multilinguality by pre-training modular transformers. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 34793495, Seattle, United States. Association for Computational Linguistics. Irene Plaza, Nina Melero, Cristina del Pozo, Javier Conde, Pedro Reviriego, Marina Mayor-Rocher, and María Grandury. 2024. Spanish and llm benchmarks: is mmlu lost in translation? arXiv preprint arXiv:2406.17789. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. Zero: Memory optimizations toward training trillion parameter modIn SC20: International Conference for High els. Performance Computing, Networking, Storage and Analysis, pages 116. IEEE. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. Preprint, arXiv:2402.03300. Shuaijie She, Wei Zou, Shujian Huang, Wenhao Zhu, Xiang Liu, Xiang Geng, and Jiajun Chen. 2024. MAPO: Advancing multilingual reasoning through multilingual-alignment-as-preference optimization. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1001510027. Association for Computational Linguistics. Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, et al. 2022. Language models are multilingual chain-of-thought reasoners. arXiv preprint arXiv:2210.03057. Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, and Bryan Catanzaro. 2022. Using deepspeed and megatron to train megatron-turing nlg 530b, large-scale generative language model. arXiv preprint arXiv:2201.11990. Charlie Victor Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. 2025. Scaling test-time compute optimally can be more effective than scaling LLM parameters. In The Thirteenth International Conference on Learning Representations. Guijin Son, Hanwool Lee, Sungdong Kim, Seungone Kim, Niklas Muennighoff, Taekyoon Choi, Cheonbok Park, Kang Min Yoo, and Stella Biderman. 2024. Kmmlu: Measuring massive multitask language understanding in korean. arXiv preprint arXiv:2402.11548. SRI_Lab. 2025. eth-sri/matharena. Original-date: 202502-12T19:06:14Z. Qwen Team. 2024. Qwq: Reflect deeply on the boundaries of the unknown. Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Pranesh Srinivasan, Laichee Man, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise AgueraArcas, Claire Cui, Marian Croak, Ed Chi, and Quoc Le. 2022. LaMDA: Language models for dialog applications. arXiv preprint arXiv:2201.08239. Open Thoughts. 2025. Openthoughts-114k dataset. Accessed: February 1, 2025. Ye Tian, Baolin Peng, Linfeng Song, Lifeng Jin, Dian Yu, Haitao Mi, and Dong Yu. 2024. Toward selfimprovement of llms via imagination, searching, and criticizing. arXiv preprint arXiv:2404.12253. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837. Yuyang Wu, Yifei Wang, Tianqi Du, Stefanie Jegelka, and Yisen Wang. 2025. When more is less: Understanding chain-of-thought length in llms. arXiv preprint arXiv:2502.07266. Violet Xiang, Charlie Snell, Kanishk Gandhi, Alon Albalak, Anikait Singh, Chase Blagden, Duy Phung, Rafael Rafailov, Nathan Lile, Dakota Mahan, et al. 2025. Towards system 2 reasoning in llms: Learning how to think with meta chain-of-though. arXiv preprint arXiv:2501.04682. Cheng Xu, Shuhao Guan, Derek Greene, Kechadi, et al. 2024. Benchmark data contamination of 13 large language models: survey. arXiv preprint arXiv:2406.04244. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. 2024a. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. 2024b. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2024. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36. Guanghao Ye, Khiem Duc Pham, Xinzhi Zhang, Sivakanth Gopi, Baolin Peng, Beibin Li, Janardhan Kulkarni, and Huseyin Inan. 2025a. On the emergence of thinking in llms i: Searching for the right intuition. arXiv preprint arXiv:2502.06773. Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. 2025b. Limo: Less is more for reasoning. arXiv preprint arXiv:2502.03387. Dongkeun Yoon, Joel Jang, Sungdong Kim, Seungone Kim, Sheikh Shafayat, and Minjoon Seo. 2024. Langbridge: Multilingual reasoning without multilingual supervision. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 75027522. Association for Computational Linguistics. Qian Zhang, Wei Li, Hao Chen, Jun Wang, and Yang Liu. 2025. The lessons of developing process reward models in mathematical reasoning. arXiv preprint arXiv:2501.07301. Zhihan Zhang, Dong-Ho Lee, Yuwei Fang, Wenhao Yu, Mengzhao Jia, Meng Jiang, and Francesco Barbieri. 2023. Plug: Leveraging pivot language in cross-lingual instruction tuning. arXiv preprint arXiv:2311.08711. Zirui Zhao, Wee Sun Lee, and David Hsu. 2023. Large language models as commonsense knowledge for large-scale task planning. Advances in Neural Information Processing Systems, 36:3196731987. Wenhao Zhu, Shujian Huang, Fei Yuan, Shuaijie She, Jiajun Chen, and Alexandra Birch. 2024. Question translation training for better multilingual reasoning. CoRR, abs/2401.07817."
        },
        {
            "title": "A Additional details on MCLM",
            "content": "In this section, we provide additional details on the MCLM benchmark, including the languages covered by each subset (Section A.1), the sampling process for MT-MATH100 (Section A.2), the sources for the M-IMO, and M-MO subset (Section A.3), the prompts used for GPT-4o and -mini (Section A.4), and contamination considerations (Section A.5). A.1 Details in Language Coverage We examine four groups of languages in this paper: (A) the 55 languages into which MATH500 and AIME2024 have been translated, (B) the 14 languages frequently sampled to reduce evaluation costs, (C) the languages covered in M-IMO, and (D) those in M-MO. The complete list for each group is provided in Table 5. Lang. Group Languages (ISO Codes, Sorted Alphabetically) # Lang. (A) (B) (C) (D) af, ar, bg, bn, ca, cs, cy, da, de, el, en, es, et, fa, fi, fr, gu, he, hi, hr, hu, id, it, ja, kn, ko, lt, lv, mk, ml, mr, ne, nl, no, pa, pl, pt, ro, ru, sk, sl, so, sq, sv, sw, ta, te, th, tl, tr, uk, ur, vi, zh-cn, zh-tw af, ar, de, en, es, fr, he, id, it, ja, ko, tr, vi, zh-cn af, ar, bg, cs, da, de, el, en, et, es, fi, fr, he, hr, hu, id, it, ja, ko, lt, lv, mk, nl, no, pl, pt, ro, ru, sk, sl, sq, sv, th, tr, uk, vi, zh-cn, zh-tw cs, de, en, fr, ja, ko, nl, pl, ru, sk, zh-cn 55 14 38 Table 5: Full language lists for each dataset subset. MT-MATH100, MT-AIME2024, M-IMO, and M-MO cover 55, 38, and 11 ISO codes respectively. Running evaluations on all 55 languages can be computationally intensive, particularly when testing for test-time scaling. To address this, we have created downsampled group (B) consisting of 14 languages for our experiments. These languages were chosen to represent broad spectrum of linguistic families and writing systems. In terms of language families, the selection includes representatives from Afro-Asiatic (Arabic, Hebrew), Austronesian (Indonesian), Japonic (Japanese), Koreanic (Korean), Turkic (Turkish), Austroasiatic (Vietnamese), and Sino-Tibetan (Chinese). Additionally, the chosen languages encompass diverse scripts: several, including Afrikaans, German, English, Spanish, French, and Italian, use the Latin alphabet; others use distinct writing systemsArabic and Hebrew employ abjads (consonant-based scripts); Japanese combines logographic characters (Kanji) with syllabic scripts (Hiragana and Katakana); Korean is written in Hangul; Turkish uses modified Latin alphabet; Vietnamese utilizes Latin-based alphabet with diacritics; and Chinese is written using Chinese characters. A.2 Sampling MATH100 Once creating MGSM, Shi et al. (2022) opted to randomly sample 250 questions from the GSM8K dataset for computational efficiency. Similarly, we sample 100 questions from MATH500 to keep evaluation costs manageable across multiple languages. Initially, we conduct random sampling. Before extending this approach to all 55 languages, we first apply it to language group (B). For language group (B), we create both the MT-MATH100 and MT-MATH500 versions, where entire subsets are translated for the later. We then evaluate 10 modelseach trained using different methods to enhance mathematical reasoningto determine whether the sampled MATH100 subset reliably represents the full dataset. In Table 6, we report the performance of the evaluated models. The score differences are relatively small, and even after accounting for minor variations, the ranking of the 10 models remains largely consistentwith only few instances of rank switching. We conclude that the sampled version serves as an acceptable proxy for the full dataset and proceed accordingly."
        },
        {
            "title": "Model",
            "content": "MATH-500 MATH-100 Score Diff. Rank Diff. 1 2 3 4 5 6 7 8 9 10 o3-mini Eurus-2-7B-PRIME Qwen2.5-Math-7B-Instruct DeepSeek-R1-Distill-Qwen-32B DeepSeek-R1-Distill-Qwen-7B AceMath-7B-Instruct AceMath-1.5B-Instruct DeepSeek-R1-Distill-Qwen-1.5B Qwen2.5-Math-1.5B-Instruct Qwen2.5-Math-1.5B-OREO 85.00 73.76 73.70 72.73 67.25 65.90 65.60 53.74 51.80 39.92 85.93 76.63 75.98 75.98 68.69 70.06 68.19 56.78 51.30 38. 0.93 2.86 2.27 3.24 1.44 4.16 2.58 3.05 0.51 1.47 - - - - 1 1 - - - - Table 6: Model rankings and score comparison between MATH-500 and MATH-100. The score difference was computed as the absolute difference between the MATH-500 and MATH-100 scores. The rank difference indicates the change in ranking on MATH-100 relative to the performance on MATH-500. A.3 Sourcing M-IMO and M-MO Relying solely on machine-translated benchmarks (MT-MATH100 and MT-AIME2024) carries inherent risks. To mitigate this, we supplement our dataset with questions from the International Mathematical Olympiad (IMO) and various regional math olympiads. Figure 10 provides an overview of the IMO questions included from 2004 to 2024, while Table 7 lists the sources for regional olympiads. For English, Chinese, and Korean, we utilize existing datasets rather than recollecting questions (He et al., 2024; Ko et al., 2025)."
        },
        {
            "title": "Russian",
            "content": "https://euler.ac-versailles.fr/spip.php?rubrique207 DeMO https://www.imojp.org/domestic/jmo_overview.html#Problems https://prime.ugent.be/activiteiten/puma/ https://wiskundeolympiade.nl/wedstrijdarchief/1e-ronde https://www.matematickaolympiada.cz/mo-pro-ss/rocnik https://iksko.org/problems.php https://om.sem.edu.pl/problems/ https://skmo.sk/dokumenty.php?rocnik=74 https://riesky.sk/archiv/ https://mmo.mccme.ru// Table 7: Link to mathematical competition links that has been included in M-MO subset. A.4 Prompts The prompts used for question translation (Figure 13), solution translation (Figure 14), and model judgment (Figure 15) are detailed accordingly. A.5 Benchmark Contamination As LLMs are trained on ever-growing datasets, concerns about benchmark contamination have emerged (Xu et al., 2024). Because most training data and procedures remain proprietary, it is virtually impossible to guarantee that benchmark is entirely free of contamination. Existing detection methods remain unstable, and our MCLM dataset, particularly its M-IMO and M-MO subsets collected from the Internet, may be prone to exposure in various model-training corpora. For instance, we observe that Llama-3.1-3B-Instruct can produce correct answers without any visible reasoning, suggesting prior 16 familiarity with certain questions. However, lacking robust decontamination method, we have chosen not to remove potentially contaminated samples. Despite this possibility, the overall low performance of most models on these subsets suggests that MCLM remains challenging and valuable resource for evaluating multilingual, competition-level math. Furthermore, our primary focus is on assessing multilingual consistency. If model correctly guesses single language version of question (possibly due to prior exposure), it does not necessarily indicate strong multilingual reasoning. Conversely, if the model can solve all language variations after seeing only few, it may demonstrate degree of cross-lingual robustness. A.6 License The machine-translated subsets of MCLM are released under the MIT License. The remaining subsets are provided under CC BY-NC-ND license, although we may transition to more permissive license pending further review of the original competition data policies. Additional details in training LLMs with system 2 thinking In this section, we provide additional details on the dataset used to train self-correcting LLMs (Section B.1), ablation studies on MR1 training (Section B.2), and the training configurations. B.1 Additional details on the training dataset In training MR1 (i.e., Deepseek-R1-1.5B + SFT with translated data), we first collect thinking trajectories generated by R1 (Guo et al., 2025) from the Numina Math dataset in Be-Spoke Stratos (Labs, 2025) and OpenThoughts (Thoughts, 2025). To ensure high-quality reasoning supervision, we exclude any data distilled from smaller thinking-model variants (e.g., QwQ-32B-Preview (Team, 2024) or the R1-Distil series). We then employ GPT-4o as an LLM-based judge to filter out instances with incorrect answers. Next, the question and solution for each remaining instance are translated into one of the 14 languages in Group (B). We opt for only 14 languagesas opposed to all 55because our ablation studies indicate that oversampling languages can negatively impact overall performance. rule-based parser validates that the question and answer content remains unchanged post-translation. Beyond this verification, no additional quality checks are performed. However, when we encounter elevated loss spikes during training, we backtrack through the dataset to identify and remove problematic instances. Following this process, we retain approximately 120K instances. Proxying problem difficulty Table 8 compares EULER-INSTRUCT with existing multilingual math datasets: MGSM8KInstruct (Chen et al., 2023) and mCoT-MATH (Lai and Nissim, 2024). MGSM8KInstruct extends GSM8K (Lightman et al., 2023) by translating it into 10 languages, yielding parallel dataset of approximately 8,000 questions per language. In contrast, mCoT-MATH sources 560,000 seed questions from MetaMathQA and MathInstruct and translates them into 10 languages. Dataset # Lang. # Inst. Diff. MGSM8KInstruct mCoT-MATH Euler-Instruct (Ours) 10 10 55 73.6k 6.3M 250K G.S G.S C.L Table 8: Comparison of Multilingual Mathematical Reasoning Datasets. The Diff. column indicates difficulty level, where G.S represents grade school level and C.L represents competition level. To estimate the difficulty level of each dataset, we randomly sample 1,000 questions and measure the solve rate of LLMs. Since MGSM8KInstruct and mCoT-MATH were published before EULER-INSTRUCT, they may have been included in the training of existing LLMs, potentially making direct comparison unfair. To address this, we first evaluate using OLMo2-{7, 13}B (OLMo et al., 2024); we use the base model instead of the instruct model since GSM8K is used during the instruction tuning phase. Since OLMo2-7B-base is not instruction-tuned, we provide three-shot examples and prompt it to solve the questions in structured format: \"The answer is X.\" Additionally, we evaluate with Qwen2.5-{7, 32, 72}B-Instruct (Yang et al., 2024b) in zero-shot setting, using parser to extract answers. Figure 11 illustrates that, for both mCoT-MATH and MGSM8KInstruct, over half of the problems are solved by Qwen2.5-7B-Instruct, and Qwen2.5-72B-Instruct solves more than 70%. In contrast, the solve rate for EULER-INSTRUCT remains low across all models, with the 7B variants solving less than 20% of the problems and even the 72B variants achieving below 40%. Notably, the Verify subset exhibits lower solve rate than previous datasets, indicating that its difficulty remains high even when restricted to numerical answers. B.2 Training Ablations Before training MR1 we meet the question: How little additional data is required to incorporate new language into self-correcting model? To address this, we train four models under fixed total budget of 24,000 training instances. Table 9 details the language composition and the per-language instance allocation for each model. Languages # Lang. # Instances ko af, fr, ko af, ar, fr, he, id, ko, tr all 14 in EULER-INSTRUCT 1 3 7 14 24k 8k 3.5k 1.7k Table 9: Details on trained models. All models are trained with total of 24,000 instances. # Instances denote the number of instances used per language. Figure 12 presents the performance gains over the base model (DeepSeek-R1-1.5B) as function of the number of training instances. On MT-MATH100, performance rises sharply once the per-language budget reaches approximately 3.5k instances. By contrast, MT-AIME2024 shows more gradual improvements, suggesting that transferring self-correction capabilities to challenging set of new-language questions requires larger data allocation. Based on these findings, we use 14 languages within total budget of 120K, ensuring that each language includes at least 8k instances. B.3 Training Configurations and Logs Axolotl (Axolotl AI, 2025) is used for the SFT training in Section 5. We train Qwen2.5-Math-1.5B with DeepSpeed-Zero1 (Rajbhandari et al., 2020) on 4 A100 80GB GPUs for 8 hours per run. Hsu et al. (2024) is used for optimization. Category Section 5 Sequence Length Learning Rate Global Batch (Effective) Learning Rate Scheduler Cosine Decay 16,384 2 105 Warmup Ratio Training Epochs 0.05 3 Table 10: SFT configuration details for Section 5."
        },
        {
            "title": "C Additional Results",
            "content": "From Tables 11 to 26, we report detailed evaluation results for 55 languages with varying models and test-time scaling methods. 18 Language MT-MATH100 MT-AIME2024 M-IMO M-MO Afrikaans Albanian Arabic Bengali Bulgarian Catalan Chinese (Simplified) Chinese (Traditional) Croatian Czech Danish Dutch Estonian Finnish French German Greek Gujarati Hebrew Hindi Hungarian Indonesian Italian Japanese Kannada Korean Latvian Lithuanian Macedonian Malayalam Marathi Nepali Norwegian Persian Polish Portuguese Punjabi Romanian Russian Slovak Slovenian Somali Spanish Swahili Swedish Tagalog Tamil Telugu Thai Turkish Ukrainian Urdu Vietnamese Welsh English Average Standard Deviation Fleiss Kappa 47.47 45.45 38.38 37.37 39.39 50.51 63.64 61.62 49.49 44.44 53.54 50.51 39.39 41.41 62.63 47.47 33.33 39.39 38.38 35.35 51.52 56.57 51.52 56.57 37.37 44.44 40.40 45.45 43.43 43.43 34.34 36.36 53.54 38.38 54.55 55.56 37.37 49.49 59.60 48.48 49.49 42.42 55.56 34.34 58.59 46.46 38.38 39.39 39.39 43.43 38.38 35.35 44.44 39.39 67.68 46.01 8.61 0.56 20.00 10.00 30.00 3.33 13.33 23.33 26.67 20.00 20.00 13.33 16.67 36.67 10.00 16.67 30.00 26.67 13.33 10.00 13.33 6.67 10.00 16.67 20.00 16.67 10.00 13.33 10.00 6.67 10.00 23.33 13.33 6.67 23.33 10.00 26.67 10.00 16.67 13.33 20.00 20.00 10.00 23.33 20.00 16.67 20.00 16.67 10.00 6.67 23.33 13.33 13.33 20.00 13.33 16.67 20.00 16.36 6.89 0.68 11.11 4.00 11.11 7. 18.52 18.52 7.41 14.81 22.22 11.11 4.00 8.00 18.52 11.11 5.26 3.70 8.00 14.29 20.00 8.00 3.70 12.00 18.52 11.11 11.11 14.81 24. 25.93 16.00 11.11 14.81 18.52 8.00 3.70 7.41 11.11 7.41 18. 12.23 6.02 0.24 40.00 6.67 20.00 51.61 10.00 0. 36.67 26.67 20.00 6.67 56.67 25.00 19.10 Table 11: Evaluation results of Qwen2.5-Math-1.5B-Instruct with greedy decoding on MCLM. 19 Language Afrikaans Albanian Arabic Bengali Bulgarian Catalan Chinese (Simplified) Chinese (Traditional) Croatian Czech Danish Dutch Estonian Finnish French German Greek Gujarati Hebrew Hindi Hungarian Indonesian Italian Japanese Kannada Korean Latvian Lithuanian Macedonian Malayalam Marathi Nepali Norwegian Persian Polish Portuguese Punjabi Romanian Russian Slovak Slovenian Somali Spanish Swahili Swedish Tagalog Tamil Telugu Thai Turkish Ukrainian Urdu Vietnamese Welsh English Average Standard Deviation Fleiss Kappa ORM (K=2) ORM (K=4) ORM (K=8) MT-MATH100 MT-AIME2024 MT-MATH100 MT-AIME2024 MT-MATH100 MT-AIME2024 53.54 52.53 43.43 41.41 45.45 59.60 69.70 68.69 51.52 49.49 53.54 51.52 46.46 41.41 64.65 54.55 39.39 44.44 44.44 40.40 53.54 58.59 57.58 59.60 45.45 53.54 45.45 48.48 50.51 47.47 39.39 38.38 59.60 40.40 54.55 58.59 41.41 51.52 60.61 52.53 47.47 44.44 58.59 37.37 57.58 50.51 41.41 42.42 44.44 50.51 44.44 38.38 49.49 38.38 71.72 50.01 8.47 0.57 23.33 10.00 20.00 10.00 26.67 33.33 36.67 13.33 16.67 13.33 23.33 30.00 13.33 13.33 40.00 23.33 13.33 10.00 16.67 10.00 10.00 20.00 26.67 16.67 10.00 16.67 10.00 10.00 13.33 20.00 13.33 6.67 26.67 13.33 16.67 13.33 16.67 23.33 20.00 10.00 16.67 16.67 23.33 13.33 20.00 16.67 16.67 13.33 10.00 16.67 23.33 16.67 23.33 16.67 16.67 17.64 7.05 0. 56.57 50.51 46.46 40.40 46.46 63.64 76.77 70.71 59.60 56.57 56.57 57.58 48.48 48.48 68.69 63.64 44.44 43.43 46.46 45.45 57.58 56.57 60.61 66.67 47.47 56.57 51.52 52.53 51.52 52.53 43.43 46.46 61.62 41.41 57.58 60.61 43.43 54.55 65.66 54.55 51.52 46.46 65.66 41.41 59.60 55.56 44.44 46.46 49.49 46.46 51.52 41.41 50.51 44.44 73.74 53.50 8.83 0.60 16.67 10.00 13.33 10.00 20.00 33.33 30.00 20.00 23.33 10.00 20.00 26.67 13.33 20.00 33.33 23.33 10.00 16.67 13.33 13.33 10.00 20.00 26.67 23.33 16.67 23.33 20.00 10.00 13.33 20.00 23.33 3.33 16.67 13.33 16.67 13.33 20.00 23.33 23.33 20.00 20.00 16.67 26.67 20.00 23.33 20.00 16.67 20.00 20.00 13.33 16.67 16.67 30.00 16.67 26.67 18.85 6.23 0.64 60.61 47.47 51.52 41.41 51.52 61.62 78.79 74.75 58.59 59.60 59.60 63.64 50.51 53.54 73.74 64.65 47.47 47.47 49.49 47.47 63.64 59.60 69.70 70.71 52.53 57.58 54.55 57.58 50.51 56.57 43.43 46.46 65.66 39.39 64.65 62.63 42.42 56.57 68.69 55.56 54.55 46.46 68.69 45.45 60.61 57.58 47.47 48.48 57.58 54.55 52.53 44.44 52.53 44.44 76.77 56.25 9.50 0. 23.33 13.33 16.67 13.33 16.67 26.67 26.67 26.67 30.00 16.67 26.67 23.33 13.33 20.00 30.00 30.00 10.00 13.33 10.00 16.67 16.67 16.67 16.67 26.67 13.33 13.33 16.67 13.33 10.00 23.33 20.00 6.67 23.33 16.67 16.67 26.67 16.67 20.00 23.33 33.33 30.00 10.00 30.00 13.33 20.00 23.33 16.67 20.00 13.33 20.00 26.67 20.00 33.33 20.00 36.67 20.12 6.97 0.63 Table 12: Evaluation results of Qwen2.5-Math-1.5B-Instruct with Best-of-N (K = 2, 4, 8) using Qwen2.5-Math-RM-72B as ORM on MT-MATH100 and MT-AIME2024. 20 Language Afrikaans Albanian Arabic Bengali Bulgarian Catalan Chinese (Simplified) Chinese (Traditional) Croatian Czech Danish Dutch Estonian Finnish French German Greek Gujarati Hebrew Hindi Hungarian Indonesian Italian Japanese Kannada Korean Latvian Lithuanian Macedonian Malayalam Marathi Nepali Norwegian Persian Polish Portuguese Punjabi Romanian Russian Slovak Slovenian Somali Spanish Swahili Swedish Tagalog Tamil Telugu Thai Turkish Ukrainian Urdu Vietnamese Welsh English Average Standard Deviation Fleiss Kappa PRM (S=3, c=3) PRM (S=4, c=5) PRM (S=5, c=8) MT-MATH100 MT-AIME2024 MT-MATH100 MT-AIME2024 MT-MATH MT-AIME2024 M-IMO M-MO 52.53 44.44 41.41 40.40 42.42 55.56 64.65 63. 50.51 50.51 57.58 56.57 47.47 41.41 62.63 54.55 42.42 42.42 46.46 39.39 57.58 56.57 61.62 64. 44.44 46.46 47.47 42.42 41.41 38.38 39.39 41.41 59.60 37.37 49.49 58.59 39.39 57.58 53.54 51. 44.44 43.43 60.61 38.38 55.56 47.47 41.41 42.42 39.39 45.45 39.39 39.39 47.47 43.43 73. 48.87 8.76 0.57 6.67 13.33 13.33 13.33 20.00 10.00 13.33 26.67 13.33 10.00 10. 20.00 13.33 10.00 13.33 40.00 13.33 6.67 6.67 10.00 26.67 16.67 13.33 20.00 23.33 10. 6.67 10.00 13.33 16.67 10.00 16.67 23.33 20.00 23.33 20.00 20.00 16.67 23.33 10.00 23.33 6. 16.67 13.33 13.33 20.00 10.00 6.67 6.67 13.33 6.67 20.00 26.67 10.00 26.67 15. 6.93 0.78 57.58 52.53 52.53 44.44 42.42 66.67 75.76 73.74 51.52 52.53 60.61 56.57 51. 43.43 65.66 62.63 39.39 39.39 42.42 46.46 61.62 57.58 61.62 66.67 42.42 45.45 50.51 49.49 47. 42.42 43.43 41.41 65.66 43.43 58.59 57.58 40.40 55.56 65.66 52.53 47.47 42.42 65.66 41.41 57. 51.52 45.45 45.45 47.47 50.51 45.45 40.40 53.54 48.48 79.80 52.54 9.98 0. 20.00 10.00 13.33 13.33 10.00 26.67 16.67 16.67 20.00 16.67 30.00 26.67 3.33 6.67 30.00 30. 6.67 13.33 23.33 20.00 10.00 13.33 20.00 26.67 13.33 13.33 16.67 6.67 16.67 16.67 10.00 26. 30.00 13.33 23.33 16.67 13.33 13.33 23.33 13.33 16.67 23.33 26.67 13.33 13.33 10.00 16.67 13. 6.67 23.33 23.33 16.67 20.00 6.67 23.33 17.15 6.95 0.61 64.65 45.45 45. 41.41 55.56 61.62 71.72 72.73 54.55 58.59 60.61 59.60 49.49 49.49 70.71 58.59 44.44 41.41 47. 47.47 57.58 64.65 67.68 66.67 47.47 50.51 51.52 45.45 48.48 43.43 36.36 42.42 59.60 39. 62.63 61.62 49.49 57.58 64.65 53.54 45.45 40.40 72.73 41.41 57.58 55.56 45.45 48.48 50.51 45. 51.52 42.42 51.52 51.52 72.73 53.54 9.71 0.60 10.00 16.67 10.00 16.67 10. 26.67 33.33 26.67 23.33 20.00 20.00 20.00 10.00 10.00 20.00 23.33 20.00 13.33 6.67 10.00 3. 13.33 23.33 16.67 13.33 13.33 10.00 16.67 23.33 13.33 13.33 10.00 26.67 13.33 20.00 30.00 6. 10.00 20.00 20.00 26.67 3.33 30.00 10.00 20.00 10.00 16.67 16.67 10.00 10.00 6.67 13.33 13. 6.67 23.33 16.00 7.15 0.62 53.33 10.00 20. 51.61 16.67 7.14 26.67 36.67 23.33 22.73 11.54 7. 11.11 25.93 29.63 14.81 14.81 22.22 7.41 11.54 15.38 18.52 22.22 4. 7.41 19.23 20.83 23.08 15.38 14.81 15.38 14.81 11.11 18. 25.93 19.23 22.22 15.38 14.81 11.11 29.63 15.38 14.81 11. 18.52 29.63 29.63 60.00 17.31 6.44 0. 30.54 18.88 Table 13: Evaluation results of Qwen2.5-Math-1.5B-Instruct using Qwen2.5-Math-PRM-72B as PRM on MCLM."
        },
        {
            "title": "Language",
            "content": "Afrikaans Arabic Chinese (Simplified) French German Hebrew Indonesian Italian Japanese Korean Spanish Turkish Vietnamese English Average Standard Deviation Fleiss Kappa MT-MATH100 PRM (S=7, c=5) PRM (S=7, c=7) PRM (S=7, c=11) 55.56 44.44 71.72 64.65 57.58 46.46 59.60 60.61 67.68 48.48 64.65 50.51 51.52 75.76 58.51 9.62 0.56 51.52 42.42 74.75 72.73 58.59 39.39 62.63 60.61 67.68 45.45 67.68 53.54 49.49 79.80 59.02 12.57 0.57 58.59 44.44 76.77 69.70 58.59 44.44 61.62 58.59 63.64 50.51 68.69 48.48 51.52 74.75 59.31 10.60 0. Table 14: Evaluation results of Qwen2.5-Math-1.5B-Instruct using Qwen2.5-Math-PRM-72B as PRM with steps fixed at (S = 7) on MT-MATH100."
        },
        {
            "title": "Language",
            "content": "Afrikaans Arabic Chinese (Simplified) French German Hebrew Indonesian Italian Japanese Korean Spanish Turkish Vietnamese English Average Standard Deviation Fleiss Kappa MT-MATH100 PRM (S=3, c=8) PRM (S=6, c=8) PRM (S=9, c=8) 54.55 41.41 71.72 67.68 56.57 42.42 60.61 56.57 63.64 47.47 65.66 53.54 57.58 75.76 58.23 10.22 0.56 55.56 44.44 71.72 64.65 57.58 46.46 59.60 60.61 67.68 48.48 64.65 50.51 51.52 75.76 58.51 9.62 0.58 60.61 52.53 70.71 67.68 66.67 45.45 62.63 61.62 62.63 48.48 72.73 49.49 57.58 77.78 61.18 9.65 0. Table 15: Evaluation results of Qwen2.5-Math-1.5B-Instruct using Qwen2.5-Math-PRM-72B as PRM with the number of candidates fixed at 8, on MT-MATH100."
        },
        {
            "title": "Language",
            "content": "Afrikaans Arabic Chinese (Simplified) French German Hebrew Indonesian Italian Japanese Korean Spanish Turkish Vietnamese English Average Standard Deviation Fleiss Kappa MT-MATH100 PRM (S=7, c=7) PRM (S=7, c=11) PRM (S=7, c=18) 51.52 42.42 74.75 72.73 58.59 39.39 62.63 60.61 67.68 45.45 67.68 53.54 49.49 79.80 59.02 12.57 0.52 58.59 44.44 76.77 69.70 58.59 44.44 61.62 58.59 63.64 50.51 68.69 48.48 51.52 74.75 59.31 10.60 0.55 58.59 52.53 76.77 71.72 60.61 41.41 62.63 64.65 61.62 50.51 68.69 52.53 51.52 70.71 60.32 9.84 0. Table 16: Evaluation results of Qwen2.5-Math-1.5B-Instruct using Qwen2.5-Math-PRM-7B as PRM with the number of candidates fixed at 7, on MT-MATH100."
        },
        {
            "title": "Language",
            "content": "Afrikaans Arabic Chinese (Simplified) French German Hebrew Indonesian Italian Japanese Korean Spanish Turkish Vietnamese English Average Standard Deviation Fleiss Kappa MT-MATH100 PRM (S=3, c=13) PRM (S=6, c=13) PRM (S=9, c=13) 55.56 44.44 75.76 64.65 55.56 46.46 56.57 62.63 58.59 49.49 60.61 49.49 52.53 71.72 57.43 9.10 0.54 59.60 45.45 70.71 71.72 63.64 43.43 58.59 60.61 67.68 48.48 73.74 50.51 48.48 73.74 59.74 10.90 0.55 54.55 44.44 79.80 73.74 61.62 47.47 61.62 61.62 59.60 51.52 64.65 49.49 45.45 77.78 59.52 11.59 0. Table 17: Evaluation results of Qwen2.5-Math-1.5B-Instruct using Qwen2.5-Math-PRM-7B as PRM with the number of candidates fixed at 13, on MT-MATH100. 23 Language MT-MATH100 MT-AIME2024 M-IMO M-MO Afrikaans Albanian Arabic Bengali Bulgarian Catalan Chinese (Simplified) Chinese (Traditional) Croatian Czech Danish Dutch Estonian Finnish French German Greek Gujarati Hebrew Hindi Hungarian Indonesian Italian Japanese Kannada Korean Latvian Lithuanian Macedonian Malayalam Marathi Nepali Norwegian Persian Polish Portuguese Punjabi Romanian Russian Slovak Slovenian Somali Spanish Swahili Swedish Tagalog Tamil Telugu Thai Turkish Ukrainian Urdu Vietnamese Welsh English Average Standard Deviation Fleiss Kappa 47.47 31.31 36.36 33.33 41.41 47.47 57.58 43.43 38.38 33.33 41.41 45.45 38.38 30.30 39.39 45.45 30.30 27.27 36.36 36.36 39.39 37.37 41.41 45.45 32.32 39.39 30.30 31.31 31.31 27.27 33.33 35.35 37.37 29.29 38.38 47.47 29.29 41.41 46.46 35.35 35.35 26.26 46.46 36.36 39.39 35.35 33.33 34.34 30.30 42.42 35.35 28.28 31.31 30.30 65.66 37.47 7.56 0.41 36.67 13.33 23.33 10.00 10.00 16.67 23.33 16.67 16.67 30.00 23.33 16.67 10.00 23.33 6.67 23.33 16.67 6.67 16.67 10.00 16.67 13.33 13.33 20.00 10.00 16.67 6.67 6.67 0.00 13.33 13.33 13.33 16.67 20.00 6.67 20.00 16.67 10.00 16.67 16.67 23.33 16.67 16.67 6.67 13.33 13.33 10.00 13.33 10.00 6.67 3.33 13.33 10.00 23.33 20.00 14.85 6.69 0.13 5.56 8.00 7.41 11. 18.52 22.22 7.41 3.70 7.41 7.41 12.00 12.00 7.41 18.52 0.00 7.41 8.00 4.76 12.00 12.00 14.81 4.00 14.81 7.41 11.11 11.11 8. 18.52 12.00 11.11 11.11 11.11 8.00 7.41 11.11 11.11 7.41 25. 10.50 5.16 0.19 23.33 3.33 16.67 35.48 6.67 3. 16.67 13.33 20.00 10.00 53.33 18.40 14.92 Table 18: Evaluation results of Qwen2.5-Math-1.5B-Instruct + SFT on MCLM. 24 Language MT-MATH100 MT-AIME2024 M-IMO M-MO Afrikaans Albanian Arabic Bengali Bulgarian Catalan Chinese (Simplified) Chinese (Traditional) Croatian Czech Danish Dutch Estonian Finnish French German Greek Gujarati Hebrew Hindi Hungarian Indonesian Italian Japanese Kannada Korean Latvian Lithuanian Macedonian Malayalam Marathi Nepali Norwegian Persian Polish Portuguese Punjabi Romanian Russian Slovak Slovenian Somali Spanish Swahili Swedish Tagalog Tamil Telugu Thai Turkish Ukrainian Urdu Vietnamese Welsh English Average Standard Deviation Fleiss Kappa 39.39 39.39 41.41 39.39 42.42 51.52 50.51 52.53 38.38 51.52 40.40 48.48 37.37 40.40 46.46 49.49 28.28 42.42 39.39 45.45 43.43 51.52 48.48 50.51 32.32 55.56 42.42 36.36 39.39 34.34 37.37 42.42 42.42 47.47 38.38 50.51 29.29 45.45 57.58 47.47 39.39 22.22 44.44 34.34 42.42 35.35 36.36 36.36 34.34 39.39 49.49 32.32 47.47 28.28 51. 42.02 7.46 0.40 10.00 16.67 16.67 30.00 10.00 26.67 23.33 20.00 13.33 23.33 6.67 20.00 23.33 20.00 10.00 10.00 20.00 13.33 13.33 13.33 40.00 16.67 13.33 6.67 10.00 10.00 10.00 13.33 13.33 26.67 23.33 16.67 10.00 10.00 10.00 26.67 16.67 6.67 13.33 20.00 23.33 26.67 16.67 6.67 10.00 6.67 23.33 13.33 26.67 23.33 10.00 20.00 10.00 20.00 26.67 16.67 7.31 0.13 13.64 7.69 14.81 11.11 7.41 11.11 11.11 11.11 3.70 11.11 15.38 7.69 7.41 7.41 17. 3.70 11.54 16.67 11.54 11.54 11.11 15.38 7.41 18.52 3.70 14.81 11.54 11.11 7.69 7.41 18. 0.00 3.85 14.81 7.41 7.41 18.52 7.41 10.52 4.63 0. 13.33 10.00 20.00 32.26 3.33 3.57 26. 20.00 36.67 40.00 20.58 13.17 Table 19: Evaluation results of Qwen2.5-Math-1.5B-Instruct + MT-SFT on MCLM. Language MT-MATH100 MT-AIME2024 M-IMO M-MO Afrikaans Albanian Arabic Bengali Bulgarian Catalan Chinese (Simplified) Chinese (Traditional) Croatian Czech Danish Dutch Estonian Finnish French German Greek Gujarati Hebrew Hindi Hungarian Indonesian Italian Japanese Kannada Korean Latvian Lithuanian Macedonian Malayalam Marathi Nepali Norwegian Persian Polish Portuguese Punjabi Romanian Russian Slovak Slovenian Somali Spanish Swahili Swedish Tagalog Tamil Telugu Thai Turkish Ukrainian Urdu Vietnamese Welsh English Average Standard Deviation Fleiss Kappa 58.59 46.46 51.52 56.57 57.58 64.65 69.70 67.68 59.60 57.58 56.57 64.65 39.39 52.53 63.64 63.64 38.38 47.47 61.62 61.62 55.56 69.70 69.70 62.63 42.42 61.62 49.49 40.40 59.60 41.41 39.39 50.51 67.68 61.62 62.63 75.76 42.42 58.59 68.69 58.59 56.57 30.30 69.70 42.42 54.55 47.47 40.40 36.36 59.60 61.62 67.68 50.51 61.62 34.34 67.68 55.61 10.93 0. 20.00 30.00 20.00 10.00 16.67 30.00 16.67 20.00 36.67 33.33 16.67 30.00 6.67 16.67 26.67 16.67 13.33 3.33 23.33 23.33 26.67 13.33 36.67 16.67 16.67 20.00 6.67 23.33 23.33 3.33 23.33 10.00 13.33 13.33 16.67 23.33 13.33 26.67 33.33 13.33 30.00 20.00 30.00 20.00 13.33 23.33 16.67 23.33 13.33 36.67 16.67 20.00 13.33 16.67 20.00 19.94 8.10 0.30 11.11 16.00 18.52 11.11 25.93 18.52 18.52 18.52 14.81 22.22 12.00 20.00 29.63 25.93 10.53 7. 24.00 23.81 28.00 12.00 11.11 20.00 14.81 25.93 18.52 22.22 16.00 22.22 20.00 11.11 14.81 25. 20.00 29.63 22.22 18.52 33.33 14.81 19.20 6.24 0.19 33. 16.67 23.33 48.39 26.67 3.57 30.00 23. 26.67 20.00 66.67 28.97 16.64 Table 20: Evaluation results of DeepSeek-R1-1.5B + MT-SFT on MCLM. 26 Language Afrikaans Albanian Arabic Bengali Bulgarian Catalan Chinese (Simplified) Chinese (Traditional) Croatian Czech Danish Dutch Estonian Finnish French German Greek Gujarati Hebrew Hindi Hungarian Indonesian Italian Japanese Kannada Korean Latvian Lithuanian Macedonian Malayalam Marathi Nepali Norwegian Persian Polish Portuguese Punjabi Romanian Russian Slovak Slovenian Somali Spanish Swahili Swedish Tagalog Tamil Telugu Thai Turkish Ukrainian Urdu Vietnamese Welsh English Average Standard Deviation Fleiss Kappa BF (N=2048) BF (N=4096) BF (N=8192) MT-AIME2024 MT-AIME2024 MT-MATH100 MT-AIME2024 M-IMO M-MO 23.33 23.33 16.67 33.33 33.33 20.00 20.00 26.67 30.00 40.00 30.00 10.00 23.33 20.00 16.67 26.67 6.67 16.67 33.33 26.67 30.00 10.00 20.00 20.00 10.00 16.67 30.00 10.00 20.00 10.00 20.00 30.00 26.67 26.67 23.33 20.00 23.33 30.00 36.67 40.00 20.00 20.00 30.00 13.33 13.33 10.00 26.67 13.33 26.67 20.00 30.00 23.33 20.00 20.00 20.00 22.48 7.94 0.33 23.33 26.67 23.33 30.00 33.33 43.33 16.67 26.67 30.00 20.00 33.33 23.33 16.67 33.33 23.33 20.00 13.33 16.67 23.33 10.00 26.67 30.00 26.67 16.67 13.33 23.33 20.00 6.67 20.00 13.33 26.67 13.33 26.67 23.33 20.00 26.67 26.67 23.33 30.00 23.33 20.00 16.67 30.00 13.33 16.67 20.00 20.00 16.67 13.33 16.67 26.67 20.00 26.67 16.67 26.67 22.24 6.85 0.37 59.60 48.48 60.61 54.55 61.62 64.65 69.70 70.71 60.61 62.63 61.62 70.71 40.40 51.52 72.73 75.76 42.42 51.52 60.61 61.62 58.59 73.74 74.75 63.64 49.49 64.65 52.53 46.46 63.64 51.52 51.52 54.55 65.66 62.63 66.67 79.80 51.52 60.61 72.73 66.67 60.61 35.35 71.72 41.41 62.63 52.53 44.44 44.44 64.65 61.62 73.74 46.46 62.63 42.42 71.72 59.45 10.52 0. 30.00 26.67 26.67 23.33 26.67 43.33 16.67 36.67 30.00 20.00 30.00 36.67 20.00 20.00 16.67 26.67 16.67 16.67 16.67 20.00 23.33 30.00 36.67 36.67 10.00 20.00 10.00 26.67 23.33 13.33 23.33 20.00 20.00 36.67 16.67 20.00 20.00 10.00 30.00 30.00 33.33 16.67 40.00 30.00 23.33 23.33 23.33 20.00 23.33 16.67 23.33 20.00 40.00 13.33 40.00 24.42 8.32 0.32 40.00 33.33 20.00 51.61 30. 7.14 40.00 23.33 30.00 9.09 7.69 14.81 22. 22.22 18.52 37.04 29.63 22.22 25.93 15.38 30.77 25.93 25.93 21.74 14.81 26.92 25 23.08 23.08 11.11 23.08 18.52 25.93 18.52 14.81 15. 22.22 23.08 25.93 25.93 18.52 19.23 11.11 33.33 22.22 25.93 22. 76.67 21.55 6.44 0.19 35.21 19.01 Table 21: Evaluation results of Qwen2.5-Math-1.5B-Instruct with Budget Forcing (BF = 2048, 4096, 8192). 27 Language MT-MATH100 MT-AIME2024 M-IMO M-MO Afrikaans Albanian Arabic Bengali Bulgarian Catalan Chinese (Simplified) Chinese (Traditional) Croatian Czech Danish Dutch Estonian Finnish French German Greek Gujarati Hebrew Hindi Hungarian Indonesian Italian Japanese Kannada Korean Latvian Lithuanian Macedonian Malayalam Marathi Nepali Norwegian Persian Polish Portuguese Punjabi Romanian Russian Slovak Slovenian Somali Spanish Swahili Swedish Tagalog Tamil Telugu Thai Turkish Ukrainian Urdu Vietnamese Welsh English Average Standard Deviation Fleiss Kappa 72.73 60.61 76.77 72.73 72.73 73.74 77.78 73.74 73.74 75.76 72.73 77.78 57.58 70.71 77.78 76.77 64.65 55.56 71.72 70.71 71.72 69.70 78.79 76.77 57.58 77.78 59.60 61.62 77.78 56.57 63.64 67.68 73.74 74.75 71.72 78.79 58.59 76.77 77.78 74.75 71.72 38.38 75.76 46.46 76.77 60.61 54.55 60.61 73.74 70.71 76.77 63.64 76.77 50.51 83.84 69.33 9.42 0.61 13.33 16.67 13.33 16.67 16.67 20.00 20.00 23.33 30.00 20.00 23.33 16.67 13.33 20.00 20.00 23.33 13.33 16.67 20.00 30.00 26.67 20.00 23.33 23.33 20.00 20.00 13.33 16.67 16.67 10.00 16.67 20.00 23.33 30.00 16.67 26.67 16.67 23.33 20.00 23.33 23.33 6.67 30.00 13.33 16.67 16.67 10.00 16.67 20.00 20.00 23.33 50.00 26.67 20.00 20. 20.12 6.57 0.51 27.78 20 14.81 7.41 11.11 22.22 11.11 18.52 18.52 20 16 25.93 25.93 10.53 7.41 20 19.05 12 16 14.81 20 25.93 22. 22.22 22.22 20 14.81 20 18.52 14.81 14.81 24 14.81 7.41 14. 14.81 22.22 17.64 5.38 0.38 56.67 16.67 23. 48.39 26.67 3.57 40 26.67 43.33 23.33 46.67 32.30 15.92 15. Table 22: Evaluation results of Qwen2.5-Math-7B-Instruct with greedy decoding on MCLM. 28 Language Afrikaans Albanian Arabic Bengali Bulgarian Catalan Chinese_(Simplified) Chinese_(Traditional) Croatian Czech Danish Dutch Estonian Finnish French German Greek Gujarati Hebrew Hindi Hungarian Indonesian Italian Japanese Kannada Korean Latvian Lithuanian Macedonian Malayalam Marathi Nepali Norwegian Persian Polish Portuguese Punjabi Romanian Russian Slovak Slovenian Somali Spanish Swahili Swedish Tagalog Tamil Telugu Thai Turkish Ukrainian Urdu Vietnamese Welsh English Average Standard Deviation Fleiss Kappa ORM (K=2) ORM (K=4) ORM (K=8) MT-MATH100 MT-AIME2024 MT-MATH100 MT-AIME2024 MT-MATH100 MT-AIME2024 74.75 68.69 76.77 69.70 73.74 75.76 77.78 77.78 75.76 75.76 73.74 76.77 62.63 73.74 81.82 78.79 65.66 58.59 73.74 70.71 73.74 75.76 79.80 78.79 55.56 79.80 61.62 63.64 76.77 59.60 65.66 64.65 72.73 76.77 77.78 81.82 58.59 79.80 78.79 77.78 73.74 38.38 75.76 48.48 77.78 58.59 59.60 61.62 76.77 76.77 77.78 66.67 73.74 51.52 84.85 70.98 9.46 0.62 16.67 20.00 13.33 16.67 16.67 26.67 20.00 23.33 30.00 20.00 26.67 20.00 16.67 23.33 23.33 33.33 20.00 13.33 13.33 26.67 26.67 30.00 26.67 23.33 13.33 16.67 16.67 20.00 16.67 10.00 26.67 13.33 26.67 23.33 10.00 26.67 20.00 23.33 26.67 30.00 13.33 6.67 26.67 13.33 30.00 13.33 16.67 20.00 16.67 26.67 23.33 33.33 33.33 20.00 26. 21.21 6.52 0.55 73.74 65.66 82.83 75.76 77.78 77.78 81.82 81.82 78.79 81.82 72.73 78.79 64.65 77.78 81.82 81.82 67.68 59.60 75.76 75.76 76.77 76.77 79.80 79.80 57.58 76.77 65.66 68.69 80.81 62.63 68.69 69.70 74.75 75.76 78.79 80.81 59.60 81.82 82.83 79.80 78.79 42.42 78.79 49.49 76.77 65.66 65.66 63.64 79.80 79.80 78.79 67.68 76.77 53.54 84.85 73.35 9.20 0.65 26.67 26.67 23.33 16.67 20.00 20.00 26.67 23.33 33.33 23.33 43.33 26.67 23.33 33.33 20.00 40.00 23.33 20.00 20.00 26.67 20.00 33.33 26.67 30.00 13.33 23.33 10.00 30.00 20.00 16.67 20.00 16.67 30.00 23.33 10.00 36.67 16.67 26.67 20.00 33.33 20.00 13.33 26.67 20.00 30.00 10.00 10.00 23.33 23.33 26.67 23.33 30.00 33.33 16.67 30.00 23.82 7.41 0.57 76.77 68.69 83.84 74.75 79.80 76.77 82.83 81.82 78.79 81.82 74.75 81.82 65.66 75.76 81.82 83.84 70.71 64.65 76.77 75.76 76.77 77.78 82.83 80.81 59.60 77.78 66.67 69.70 79.80 68.69 69.70 68.69 76.77 76.77 78.79 83.84 62.63 79.80 86.87 81.82 78.79 44.44 81.82 51.52 77.78 66.67 62.63 62.63 77.78 79.80 79.80 72.73 80.81 56.57 86. 74.62 8.86 0.67 33.33 26.67 20.00 16.67 16.67 30.00 26.67 23.33 33.33 23.33 43.33 40.00 30.00 33.33 26.67 40.00 16.67 16.67 30.00 36.67 23.33 43.33 33.33 23.33 20.00 26.67 10.00 20.00 23.33 23.33 16.67 16.67 33.33 16.67 16.67 40.00 26.67 30.00 26.67 30.00 23.33 20.00 30.00 23.33 30.00 16.67 10.00 16.67 30.00 26.67 26.67 30.00 36.67 6.67 26.67 25.76 8.37 0.57 Table 23: Evaluation results of Qwen2.5-Math-7B-Instruct with Best-of-N (K = 2, 4, 8) using Qwen2.5-Math-RM-72B as ORM on MT-MATH100 and MT-AIME2024. 29 Language Afrikaans Albanian Arabic Bengali Bulgarian Catalan Chinese (Simplified) Chinese (Traditional) Croatian Czech Danish Dutch Estonian Finnish French German Greek Gujarati Hebrew Hindi Hungarian Indonesian Italian Japanese Kannada Korean Latvian Lithuanian Macedonian Malayalam Marathi Nepali Norwegian Persian Polish Portuguese Punjabi Romanian Russian Slovak Slovenian Somali Spanish Swahili Swedish Tagalog Tamil Telugu Thai Turkish Ukrainian Urdu Vietnamese Welsh English Average Standard Deviation Fleiss Kappa PRM (S=3, c=3) PRM (S=4, c=5) PRM (S=5, c=8) MT-MATH100 MT-AIME2024 MT-MATH100 MT-AIME2024 MT-MATH100 MT-AIME 70.71 60.61 65.66 67.68 69.70 72.73 72.73 71.72 69.70 69.70 63.64 71.72 46.46 64.65 73.74 73.74 63.64 56.57 66.67 58.59 68.69 69.70 71.72 71.72 46.46 69.70 59.60 55.56 69.70 49.49 56.57 51.52 69.70 71.72 61.62 72.73 46.46 66.67 75.76 70.71 70.71 40.40 71.72 48.48 70.71 55.56 50.51 53.54 67.68 63.64 75.76 57.58 72.73 50.51 73.74 64.17 9.25 0.56 20.00 16.67 26.67 16.67 20.00 16.67 16.67 16.67 20.00 16.67 23.33 6.67 20.00 16.67 20.00 10.00 16.67 13.33 10.00 16.67 16.67 26.67 16.67 23.33 16.67 16.67 10.00 20.00 16.67 20.00 20.00 16.67 20.00 26.67 13.33 10.00 13.33 13.33 16.67 23.33 23.33 3.33 13.33 6.67 16.67 23.33 10.00 13.33 10.00 20.00 20.00 26.67 23.33 20.00 23.33 17.27 5.33 0.56 70.71 62.63 78.79 70.71 74.75 70.71 73.74 76.77 72.73 77.78 69.70 72.73 51.52 66.67 72.73 68.69 64.65 56.57 68.69 63.64 69.70 68.69 77.78 75.76 53.54 72.73 63.64 62.63 75.76 57.58 55.56 61.62 67.68 71.72 67.68 71.72 45.45 70.71 76.77 75.76 72.73 42.42 77.78 42.42 76.77 59.60 55.56 58.59 71.72 71.72 77.78 62.63 73.74 43.43 75.76 67.09 9.65 0. 16.67 33.33 26.67 10.00 10.00 20.00 33.33 20.00 16.67 10.00 33.33 26.67 13.33 13.33 16.67 10.00 13.33 26.67 20.00 20.00 30.00 20.00 30.00 16.67 10.00 13.33 13.33 13.33 16.67 23.33 23.33 20.00 20.00 16.67 13.33 26.67 10.00 33.33 16.67 26.67 30.00 10.00 20.00 10.00 36.67 16.67 3.33 20.00 16.67 20.00 26.67 20.00 13.33 13.33 20.00 19.27 7.61 0.57 70.71 61.62 82.83 68.69 75.76 71.72 78.79 77.78 70.71 73.74 66.67 75.76 59.60 72.73 76.77 76.77 67.68 55.56 75.76 72.73 72.73 72.73 73.74 76.77 54.55 74.75 63.64 65.66 75.76 52.53 57.58 52.53 69.70 72.73 76.77 79.80 52.53 77.78 76.77 70.71 74.75 42.42 80.81 44.44 71.72 58.59 57.58 54.55 71.72 64.65 79.80 66.67 73.74 45.45 75.76 68.45 10.02 0.56 20.00 26.67 30.00 23.33 30.00 16.67 30.00 23.33 33.33 30.00 30.00 26.67 20.00 33.33 26.67 26.67 13.33 13.33 26.67 13.33 20.00 10.00 23.33 13.33 16.67 16.67 16.67 16.67 23.33 20.00 23.33 23.33 26.67 23.33 10.00 26.67 20.00 30.00 33.33 13.33 20.00 6.67 23.33 16.67 26.67 13.33 20.00 20.00 26.67 16.67 20.00 23.33 33.33 20.00 23.33 22.00 6.56 0. Table 24: Evaluation results of Qwen2.5-Math-7B-Instruct using Qwen2.5-Math-PRM-72B as PRM on MT-MATH100 and MT-AIME2024. 30 Language MT-MATH100 MT-AIME2024 M-IMO M-MO Afrikaans Albanian Arabic Bengali Bulgarian Catalan Chinese (Simplified) Chinese (Traditional) Croatian Czech Danish Dutch Estonian Finnish French German Greek Gujarati Hebrew Hindi Hungarian Indonesian Italian Japanese Kannada Korean Latvian Lithuanian Macedonian Malayalam Marathi Nepali Norwegian Persian Polish Portuguese Punjabi Romanian Russian Slovak Slovenian Somali Spanish Swahili Swedish Tagalog Tamil Telugu Thai Turkish Ukrainian Urdu Vietnamese Welsh English Average Standard Deviation Fleiss Kappa 73.74 66.67 71.72 64.65 72.73 70.71 70.71 69.70 72.73 71.72 71.72 69.70 76.77 72.73 70.71 73.74 71.72 67.68 71.72 70.71 73.74 68.69 72.73 70.71 61.62 72.73 69.70 68.69 71.72 62.63 63.64 67.68 75.76 66.67 72.73 70.71 69.70 73.74 73.74 72.73 72.73 57.58 71.72 65.66 72.73 71.72 67.68 66.67 70.71 71.72 73.74 68.69 71.72 65.66 75.76 70.30 3.68 0.71 23.33 20.00 16.67 3.33 20.00 26.67 23.33 23.33 16.67 33.33 23.33 20.00 16.67 6.67 23.33 20.00 10.00 13.33 10.00 6.67 26.67 13.33 23.33 30.00 23.33 26.67 20.00 16.67 20.00 23.33 20.00 10.00 30.00 26.67 13.33 26.67 16.67 26.67 23.33 20.00 16.67 20.00 26.67 23.33 23.33 20.00 20.00 16.67 26.67 10.00 23.33 23.33 6.67 26.67 33.33 20.18 6.83 0.33 9.09 15.38 3.70 18. 14.81 11.11 18.52 11.11 22.22 3.70 15.38 15.38 14.81 18.52 13.04 7.41 11.54 16.67 11.54 7.69 22.22 7.69 7.41 22.22 11.11 22.22 7. 11.11 15.38 18.52 7.41 14.81 23.08 7.41 11.11 14.81 14.81 7. 13.33 5.36 0.25 26.67 36.67 3.33 48.39 26.67 7. 36.67 26.67 50.00 50.00 30.81 15.80 Table 25: Evaluation results of GPT-4O-MINI with greedy decoding on MCLM. 31 Language MT-MATH100 MT-AIME2024 M-IMO M-MO Afrikaans Albanian Arabic Bengali Bulgarian Catalan Chinese (Simplified) Chinese (Traditional) Croatian Czech Danish Dutch Estonian Finnish French German Greek Gujarati Hebrew Hindi Hungarian Indonesian Italian Japanese Kannada Korean Latvian Lithuanian Macedonian Malayalam Marathi Nepali Norwegian Persian Polish Portuguese Punjabi Romanian Russian Slovak Slovenian Somali Spanish Swahili Swedish Tagalog Tamil Telugu Thai Turkish Ukrainian Urdu Vietnamese Welsh English Average Standard Deviation Fleiss Kappa 85.86 86.87 86.87 86.87 87.88 87.88 85.86 84.85 84.85 84.85 85.86 86.87 83.84 84.85 86.87 86.87 87.88 83.84 81.82 83.84 86.87 84.85 82.83 86.87 86.87 77.78 87.88 85.86 83.84 85.86 83.84 79.8 82.83 87.88 81.82 82.83 87.88 81.82 85.86 87.88 85.86 87.88 72.73 86.87 79.8 85.86 84.85 82.83 84.85 84.85 84.85 84.85 85.86 85.86 83. 84.89 2.80 0.88 46.67 53.33 43.33 43.33 46.67 53.33 50 40 46.67 36.67 40 50 50 40 43.33 43.33 56.67 46.67 40 43.33 53.33 43.33 50 50 43.33 46.67 46.67 46.67 43.33 46.67 36.67 46.67 53.33 53.33 43.33 36.67 43.33 40 56.67 46.67 46.67 50 50 43.33 43.33 46.67 43.33 33.33 40 40 50 36.67 46.67 46.67 36.67 45.33 5.35 0.73 33.33 28.00 22.22 40.74 25.93 29.63 33.33 29.63 40.74 33.33 28.00 28.00 29.63 33.33 21. 7.41 28.00 33.33 36.00 16.00 25.93 32.00 33.33 33.33 22.22 37.04 36.00 40.74 20.00 33.33 29. 29.63 28.00 22.22 33.33 29.63 37.04 29.63 29.75 6.86 0. 66.67 53.33 40.00 67.74 43.33 17.86 60. 40.00 50.00 46.67 80.00 51.42 16.94 Table 26: Evaluation results of O3-MINI with greedy decoding on MCLM. Figure 10: Heatmap representation of IMO problems from 2006 to 2024. Each row corresponds to competition year, and each column represents problem (Q1Q6). Green cells indicate questions that have been included in the M-IMO subset, while gray cells represent problems that were not selected. 33 Figure 11: Solve rates (%) of different multilingual math datasets evaluated. For the OLMo2 series, we use the base models, while for the Qwen2.5 series, the instruct-tuned variants are used. EULER-INSTRUCT presents significantly lower solve rate, indicating its greater difficulty. Figure 12: Model Results from Table 9. Left shows accuracy on MT-MATH500 (entire translated subset for language group (B)), and right shows average performance of MT-AIME2024. 34 You will be given an English question in the following format. [Question] <question> {..question...} </question> Your job is to return translated version of the question. * Translate to <language>. * The translation must be fluent, easy to read by native speakers. * Do not solve the prompt translate it. * You must preserve all details including math notations (latex) and code. * The math notations and code must not be translated, keep it as is. * Return you translation in the following format. [Translation] <translation> {..translated question...} </translation> -------------------------------------------------- The following is the math problem for you task: [Question] <question> <source_question> </question> Figure 13: Question Translation Template You will be given an English solution in the following format. [Solution] <solution> {..solution in English...} </solution> Your job is to rewrite the English solution to <language>. * The solution must preserve the original structure and details. * You must preserve all details including math notations (latex) and code. * The math notations and code must not be translated, keep it as is. * The solution must be natural, easy and polite for native speaker to read. [Translation] <translation> {..translated solution...} </translation> -------------------------------------------------- The following is the math problem and solution for your task: [Solution] <solution> <source_solution> </solution> Figure 14: Solution Translation Template 35 You will be given math problem, the correct answer, and solution generated by language model. Your task is to determine whether the solution generated by the model is correct. [Question] <question> {..math question...} </question> [Correct Answer] <answer> {..correct answer...} </answer> [Model Solution] <solution> {..model-generated solution...} </solution> Instructions: * Compare the model's solution with the correct answer. * If the model's solution is correct, output [[TRUE]]. * If the model's solution is incorrect, output [[FALSE]]. * You do not have to judge the solution process; there are numerous possible 'Gold' solutions, and the model solution does not have to be identical with the one provided. As long as the model reaches the correct answer, it is correct. * Do not provide any explanations -- only return your judgment ONLY. -------------------------------------------------- The following is the math problem and solution for your task: [Question] <question> <math_question> </question> [Correct Answer] <answer> <correct_answer> </answer> [Model Solution] <solution> <model_solution> </solution> Figure 15: Judge Template"
        }
    ],
    "affiliations": [
        "KAIST AI",
        "OneLineAI",
        "Yonsei University"
    ]
}