{
    "paper_title": "Soft Tokens, Hard Truths",
    "authors": [
        "Natasha Butt",
        "Ariel Kwiatkowski",
        "Ismail Labiad",
        "Julia Kempe",
        "Yann Ollivier"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The use of continuous instead of discrete tokens during the Chain-of-Thought (CoT) phase of reasoning LLMs has garnered attention recently, based on the intuition that a continuous mixture of discrete tokens could simulate a superposition of several reasoning paths simultaneously. Theoretical results have formally proven that continuous tokens have much greater expressivity and can solve specific problems more efficiently. However, practical use of continuous tokens has been limited by strong training difficulties: previous works either just use continuous tokens at inference time on a pre-trained discrete-token model, or must distill the continuous CoT from ground-truth discrete CoTs and face computational costs that limit the CoT to very few tokens. This is the first work introducing a scalable method to learn continuous CoTs via reinforcement learning (RL), without distilling from reference discrete CoTs. We use \"soft\" tokens: mixtures of tokens together with noise on the input embedding to provide RL exploration. Computational overhead is minimal, enabling us to learn continuous CoTs with hundreds of tokens. On math reasoning benchmarks with Llama and Qwen models up to 8B, training with continuous CoTs match discrete-token CoTs for pass@1 and surpass them for pass@32, showing greater CoT diversity. In systematic comparisons, the best-performing scenario is to train with continuous CoT tokens then use discrete tokens for inference, meaning the \"soft\" models can be deployed in a standard way. Finally, we show continuous CoT RL training better preserves the predictions of the base model on out-of-domain tasks, thus providing a softer touch to the base model."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 0 7 1 9 1 . 9 0 5 2 : r Soft Tokens, Hard Truths Natasha Butt1,, Ariel Kwiatkowski2, Ismail Labiad2, Julia Kempe2,3,, Yann Ollivier2, 1University of Amsterdam, 2Meta FAIR, 3New York University Work done during an internship at Meta, Joint senior authors The use of continuous instead of discrete tokens during the Chain-of-Thought (CoT) phase of reasoning LLMs has garnered attention recently, based on the intuition that continuous mixture of discrete tokens could simulate superposition of several reasoning paths simultaneously. Theoretical results have formally proven that continuous tokens have much greater expressivity and can solve specific problems more efficiently. However, practical use of continuous tokens has been limited by strong training difficulties: previous works either just use continuous tokens at inference time on pre-trained discrete-token model, or must distill the continuous CoT from ground-truth discrete CoTs and face computational costs that limit the CoT to very few tokens. This is the first work introducing scalable method to learn continuous CoTs via reinforcement learning (RL), without distilling from reference discrete CoTs. We use soft tokens: mixtures of tokens together with noise on the input embedding to provide RL exploration. Computational overhead is minimal, enabling us to learn continuous CoTs with hundreds of tokens. On math reasoning benchmarks with Llama and Qwen models up to 8B, training with continuous CoTs match discrete-token CoTs for pass@1 and surpass them for pass@32, showing greater CoT diversity. In systematic comparisons, the best-performing scenario is to train with continuous CoT tokens then use discrete tokens for inference, meaning the soft models can be deployed in standard way. Finally, we show continuous CoT RL training better preserves the predictions of the base model on out-of-domain tasks, thus providing softer touch to the base model. Date: September 24, 2025 Correspondence: Natasha Butt at n.e.butt@uva.nl"
        },
        {
            "title": "1 Introduction",
            "content": "Large Language Models (LLMs) have achieved impressive success across wide range of reasoning tasks, particularly when enhanced with Chain-of-Thought (CoT) prompting, where models generate intermediate thinking tokens before producing final answers. While effective, standard CoT is constrained by the discreteness of language tokens: each intermediate step must be sampled sequentially, which can limit expressivity and hinder exploration of diverse reasoning paths. This contrasts sharply with human cognition, which often operates over abstract and fluid concepts rather than rigid linguistic symbols. Motivated by this gap, recent work has explored enabling LLMs to reason in continuous concept spaces, direction often termed continuous CoTs (Hao et al., 2024) or Soft Thinking (Zhang et al., 2025). From theoretical perspective, continuous reasoning offers significant potential. Reasoning by Superposition (Zhu et al., 2025a) shows that continuous thought vectors can act as superposition states, encoding multiple search frontiers in parallel and enabling efficient breadth-first reasoning. This construction allows shallow transformer to solve problems such as directed graph reachability far more efficiently than discrete CoT, which is forced into sequential exploration and risks being trapped in local solutions. Complementarily, Soft Thinking (Zhang et al., 2025) proposes replacing discrete (hard) tokens with concept tokensprobability-weighted mixtures of embeddingsthat retain full distributional information. This enables the model to implicitly follow multiple reasoning paths simultaneously, yielding empirical improvements in both accuracy and token efficiency. Despite these promising claims, the practical benefits of continuous reasoning at inference time on top of 1 Figure 1 Hard, fuzzy and soft generation during CoT phase. In hard generation, at each time step, discrete token CoTt is sampled from the probability vector pt1 and its embedding h0 CoT1 is passed to the transformer, generating sequence of discrete CoT tokens: CoT1, ..., CoTT over time. In fuzzy and soft generation, at each time step, noise, ϵt, is injected into the probability weighted mixture embedding, ht 0 = pt1E, where is the token embedding matrix. This noisy input embedding is passed to the transformer, generating sequence of continuous noisy CoT embeddings: h0 CoT1 , ..., h0 CoTT over time. Additionally, for fuzzy generation, the temperature τ used in the CoT phase tends to 0, such that the non-noisy embeddings h0 reduce to embeddings of discrete tokens. We find that the combination of soft/fuzzy training and hard inference performs universally best, matching hard training at pass@1 and surpassing it at pass@32, indicating better preservation of diversity. discrete-token base models remain contested. In particular, Wu et al. (2025) critically re-examine Soft Thinking and find that vanilla implementations often underperform their discrete counterparts. Their analysis suggests that LLMs, when given soft inputs, default to relying on the single highest-probability tokeneffectively reducing Soft Thinking to greedy decoding. Further, existing methods for soft thinking are limited to inference on models trained with discrete CoTs. Training of continuous-token reasoning models has proven to be difficult, either due to computational constraints from full backpropagation through all steps of continuous reasoning (this limited the CoT to 6 steps in Hao et al. (2024)), or due to the necessity of strongly grounding the continuous reasoning into ground-truth discrete reasoning traces (Shen et al., 2025). This is why several of the works above limit themselves to applying continuous reasoning at inference time without training (Zhang et al., 2025; Wu et al., 2025). In this work, we address these limitations by developing an approach to reinforce continuous CoTs with controlled noise, making them amenable to reinforcement learning (RL) training. We theoretically outline two types of continuous CoT learning with soft and fuzzy tokens (see Figure 1) and provide extensive empirical evidence with Llama-3.x and Qwen-2.5 models trained on number of mathematical datasets (GSM8K, MATH, DeepScaleR) and evaluate on variety of mathematical and out-of-domain benchmarks. Our contributions and findings are as follows: continuous-token post-training algorithm. We propose the first continuous CoT finetuning algorithm that does not require ground-truth CoT annotations, directly tackling the challenge of learning continuous reasoning representations, at negligible computational overhead compared to discrete CoTs. Pass@1 parity. We show that continuous CoT post-training is competitive with traditional discrete-token CoTs for pass@1 criteria, with statistically similar performance on most model-dataset combinations. Pass@32 gains. Under sampling (pass@32), continuous CoT training outperforms discrete CoT on average, demonstrating greater CoT diversity. Improved robustness. Continuous CoT training does not degrade the base models log-likelihood on HellaSwag, ARC and MMLU, whereas discrete CoT training does, on average. Further, continuous CoT training is robust to collapse observed with discrete training on Llama-8B-Instruct with respect to in-distribution and out-of-distribution performance. Hard inference on soft models. In our experiments, adding continuous CoT at inference on top of hard-token-finetuned model does not bring benefits, contrasting with Zhang et al. (2025). On the 2 opposite, the best performance is obtained when using discrete CoT at inference on top of continuous CoT trained model. This means that practitioner can deploy standard inference methods directly to reap the advantages from models trained with continuous tokens. Entropy analysis. We present detailed analysis of the entropy profiles of models, showing how continuous CoT fine-tuning more closely preserves the entropy profiles of the base Llama models, compared to discrete CoT."
        },
        {
            "title": "2 Related Work",
            "content": "It is natural to question whether token space is the ideal medium for reasoning, particularly in tasks that demand higher levels of semantic abstraction. However, unrolling beyond token space during pretraining departs from the original training distribution and may require additional mechanisms to help hardtokentrained LLM adapt to alternative, potentially more expressive, representations of chains of thought. Several prior works have confronted this challenge. (Goyal et al., 2024) proposes inserting dedicated placeholder tokens, Pause Tokens\", into rollouts to encourage more deliberate thinking in the internal layers. Coconut (Hao et al., 2024), in contrast, explicitly ventures into continuous-space reasoning by distilling ground-truth chains of thought into continuous tokens, but its benefits appear confined to benchmark designed for this purpose. Furthermore, Coconut requires ground truth chains-of-thought and gentle hard-to-soft\" distillation schedule, and comes with computational constraints that have limited it to 6 continuous CoT tokens. number of follow-up works have tried to tackle these challenges (see Zhu et al. (2025b) for survey on latent-space reasoning). Given the frequent difficulty of making an LLM accept continuous tokens when they are not native to their training, it makes sense to distinguish when in the pipeline continuous tokens are invoked (at pretraining, at post-training or at inference). We list those works closest to ours, while necessarily being incomplete as the scope widens; we recommend the survey by Zhu et al. (2025b) for taxonomy. Inference. Zhang et al. (2025) introduce so-called soft tokens as the softmax layers of the output tokens, and propose to perform inference in soft-token space. Combined with few other interventions (a hand-crafted stopping criterion, and only keeping the few top dimensions of embedding space) they seem to gain some performance on some benchmarks purely through inference-time interventions. more measured view of the same is given in Wu et al. (2025), where the previous results are not confirmed, unless noise is introduced at test time into the soft token generation. Noise is important in our approach for different reason, to provide exploration for reinforcement learning. Post-training. number of works aim to introduce continuous tokens during post-training as already mentioned for Coconut (Hao et al., 2024). Codi (Shen et al., 2025) distill standard discrete CoT model into continuous CoT model, by keeping both the emitted tokens and the internal activities of the continuous model close to that of the original model. Pretraining: Several works propose changes to pretraining to incorporate some notion of thinking tokens\" or latent tokens, and attempt to replace the CoT by some internal activations inside, or added to, the transformer. Filler tokens approaches (Lanham et al., 2023; Goyal et al., 2024; Pfau et al., 2024; Ringel et al., 2025) introduce some bland tokens so that the model can use its continuous internal activations to reason while reading the bland tokens. Going further, CoCoMix (Tack et al., 2025) intersperses continuous tokens with hard tokens at pretraining, and uses pretrained sparse autoencoder to couple the hard and soft tokens. Looped transformers (Saunshi et al., 2025) and recurrent depth (Geiping et al., 2025) deploy internal, continuous CoTs in the depth direction of the transformer, by repeating some internal blocks and making the CoT depth potentially infinite before every token. Theoretical arguments: Zhu et al. (2025a) make strong argument that continuous CoTs are more expressive than discrete CoTs, on natural toy problem (reachability in graphs). They prove that 2-layer transformer with continuous CoT can solve the problem in O(n) vs O(n2) with discrete tokens. Continuous CoTs are provably able to use superpositions that explore several reasoning paths at the same time. The experiments align with the theoretical predictions: the superposition is learned in practice."
        },
        {
            "title": "3 Method",
            "content": "In nutshell. Similarly to Coconut or Soft Tokens, our method keeps the full probability distribution after the softmax step, instead of emitting discrete token. This mixture of tokens is fed to the input of the transformer for the next step. Contrary to prior work, we then inject noise on the input embeddings. This noise produces the necessary exploration to apply RL fine-tuning. In contrast, Soft Tokens do not fine-tune, and Coconut relies on backpropagation through time through the whole continuous CoT (coming with strong computational limitations) plus curriculum to ground the continuous CoTs into ground truth discrete CoTs. We now describe our method in detail. Notation for transformers. We decompose standard LLM architecture into the following blocks. We denote by the vocabulary size. Thus, each token can be seen as one-hot vector xt RV , and the sequence of tokens x<t up to time can be seen as matrix of size . We denote by the token embedding matrix of an LLM, that takes sequence of tokens x<t and returns sequence of embeddings h0 <t by linear, token-wise mapping h0 <t = x<tE. (1) Denoting by n0 the dimension of the input embedding of the transformer stack, is matrix of size n0. We denote by the transformer stack, that turns the sequence of input embeddings h0 into sequence of <t output embeddings <t = (h0 hL <t) (2) where is the depth of the transformer stack. hL <t output layer of the transformer stack. is matrix of size nL with nL the dimension of the Probabilities for next-token prediction are obtained as follows. The output encodings hL are turned into <t logits by decoding matrix Wd of size nL where nL is the output dimension of the transformer stack. The next-token probabilities are obtained by applying softmax at temperature τ 0 to these logits: where p<t is matrix of size , the softmax is applied for each independently, and softmax is extended by continuity for temperature τ = 0. p<t = softmax(hL <t Wd/τ ) (3) In standard, hard-token models, each token xt is one-hot vector xt RV . At Standard (hard) tokens. inference time, to compute next-token prediction xt given the sequence of previous tokens x<t, one first computes the next-token probabilities p<t given x<t as above. Then the next token is sampled according to the last component pt1 of p<t: Pr(xt = 1i) = pt1,i where 1i denotes the one-hot encoding of token i. This is applied inductively to get the sequence of next tokens. (4) In soft thinking (Zhang et al., 2025; Wu et al., 2025), during the CoT phase, instead of Soft thinking. sampling next token xt according to the probabilities pt, the probabilities are directly used to define mixture of embeddings. The next input layer embedding is obtained as h0 = (cid:88) Pr(xt = 1i)ei = pt1E (5) where ei is the embedding for token i. Then the transformer stack is applied normally to h0. 1 1The model used in Coconut (Hao et al., 2024) is slightly different in that it directly feeds the output embedding as next-step t1, assuming dimensions are the same. This bypasses the expansion from hidden dimension input embeddings, namely, h0 to vocabulary size and back, as well as the softmax and temperature. = hL After the CoT phase is done, the model samples normal (hard) tokens. This model is not amenable to direct RL training via Reinforce-like algorithms, because of the absence of noise or random choices: the whole CoT is deterministic and differentiable function of the prompt. In principle, it could be optimized directly by backpropagating through all the timesteps of the CoT, similarly to Backpropagation Through Time (BPTT). But this leads to technical and memory challenges that we will not discuss here. Noisy soft thinking: soft tokens and fuzzy tokens. just by introducing noise into the soft thinking process. We simply add noise to the computation of h0 Instead, we propose to make soft thinking trainable by RL, : (6) h0 = pt1E + σN (0, Id) with some standard deviation σ > 0. Then, at the next timestep, the transformer stack is fed h0 We also experimented with adding noise at other places, such as on the logits (Appendix F.2). . We call this model soft tokens; we use the term fuzzy tokens when the temperature τ used during the chain of thought tends to 0, because in that case, the non-noisy embeddings h0 reduce to embeddings of true discrete tokens, so h0 are normal discrete tokens embeddings up to noise σ. Introducing exploration noise on the soft CoT tokens makes it possible Reinforcement learning on soft tokens. to optimize the model via reinforcement learning. (For traditional discrete CoT tokens, exploration comes from the random sampling of token from the softmax probabilities.) We describe here the derivation of Reinforce for noisy soft thinking. More advanced Reinforce-like methods such as RLOO, GRPO, PPO... are derived from Reinforce in the standard way. Given prompt, we sample soft CoT, then sample final answer given the CoT. Let R(a) denote the reward obtained for an answer a. The objective is to maximize the expected reward. The CoT sampling is fully defined by the sampling of the noisy soft tokens h0. Therefore, the objective is to maximize the expectation for given prompt, where π is the current model. (h,a)π[R(a)] By the standard Reinforce theorem Sutton & Barto (1998), this is equivalent to minimizing the loss (h,a)πsg (cid:104) R(a) (cid:16) log π(h0) + log π(ah0) (cid:17)(cid:105) (7) (8) The term log π(ah0) just represents fine-tuning the answer given the CoT, and can be computed in standard way, since sampling of is done in standard way. The term log π(h0) can be decomposed as sum over timesteps, log π(h0) = (cid:88) log π(h0 h0 <t) (9) and each of those terms can be computed easily: indeed, knowing the soft tokens h0 <t . Since the noise is Gaussian, we just have: non-noisy next-token input embedding h0 , we can compute the log π(h0 h0 <t) = 1 2σ2 (cid:13) h0 h0 (cid:13) (cid:13) (cid:13) 2 (cid:13) (cid:13) + cst (10) and we note that h0 of the model. is differentiable function of the previous soft tokens h0 <t , depending on the parameters This makes it possible to apply the family of Reinforce-like algorithms to noisy soft tokens. Computational overhead is minimal: storing the probability vector pt at each step (vector of size ), and injecting noise on the first layer."
        },
        {
            "title": "4 Experiments",
            "content": "Models tested. We train three variations of CoT models described in Section 3: Hard tokens: Categorical sampling of ordinary hard CoT tokens with temperature τ = 1.0. Soft tokens: Instead of sampling hard tokens, we use the full probability mixture at temperature τ = 0.5 to compute embeddings, and add Gaussian noise to the embeddings. Fuzzy tokens: Like soft tokens, but at temperature τ = 0.0001, which brings them very close to hard tokens embeddings, to which we add Gaussian noise. For the scale of the Gaussian noise, we set this equal to 0.33 times the root-mean-square norm of the token embeddings, so that the noise is comparable but bit smaller than the embeddings. In practice we observe our algorithm is robust to ratios less than or equal to 1.0 (Appendix F.1). In further ablations (Appendix F.3), we observe that our algorithm is also robust to temperature values τ [0.0001, 0.1]. Inference settings. At test time, we decouple the inference method from the training method: for each trained model (hard, soft, fuzzy), we evaluate six inference settings. We vary the decoding of the CoT as follows, but with answers always greedily decoded at temperature 0: Hard Greedy: discrete tokens, CoT temperature τ = 0.0 at test time Hard Sample: discrete tokens, CoT temperature τ = 1.0 at test time Soft Greedy: Gaussian scale σ = 0.0, CoT temperature τ = 0.5 at test time Soft Sample: Gaussian scale σ = 0.33 root-mean-square norm, CoT temperature τ = 0.5 at test time Fuzzy Greedy: Gaussian scale σ = 0.0, CoT temperature τ = 0.0001 at test time Fuzzy Sample: Gaussian scale σ = 0.33 root-mean-square norm, CoT temperature τ = 0.0001 at test time For instance, soft training with hard greedy testing amounts to training with mixture at τ = 0.5 with Gaussian noise, then applying the model with hard tokens at test time. The sample settings are the same as the variants used during training for the CoT, while the greedy setting more aggressively target the mode of the distribution at each step of the CoT. Reinforce with group baseline. We fine-tune the models with RLOO, namely, Reinforce using per-prompt leave-one-out (LOO) group baseline (Kool et al., 2019): for each sample and reward, we subtract the average reward obtained on the other samples for the same prompt. We include the RLOO loss in Appendix for completeness. At each update we draw mini-batch of = 2 distinct prompts {xb}B . For each prompt we sample = 32 sequences yb,g that contain chain-of-thought (CoT) followed by final answer. The prompt instructs the model to end the CoT with The final answer is: (see Appendix B). Rewards are computed only on the final answer using the Math Verify package (Kydlíček, 2025) against the ground-truth label: b=1 rb,g = 100, 10, 0, if Verify(ab,g) = 1, if Verify(ab,g) = 0 and ExtractBoxed(ab,g) = 1, otherwise. (11) Datasets and base models. We train Llama 3.2 3b Instruct, Llama 3.1 8b Instruct (Dubey et al., 2024) and Qwen 2.5 3b Instruct (Yang et al., 2024) on math reasoning datasets including GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al. (2021b) and DeepScaleR (Luo et al., 2025). For each model trained on dataset, we evaluate test performance on three math reasoning test datasets: GSM8K, MATH and OlympiadBench (He et al., 2024). For OlympiadBench, following prior work, we use the 675 subset of math questions which have final answers and do not contain images or figures. Similarly, for MATH, we evaluate on the MATH-500 (HuggingFaceH4, 2025) subset of the MATH test set. To assess 6 Figure 2 Llama 3b Instruct Trained on GSM8K (a) Training performance across steps; one step = two prompts 32 samples each. (b) Greedy validation performance used for model selection. For the remaining trained models, see Appendix G.1. Figure 3 Hard Inference Pass@k for Llama models (for soft/fuzzy inference and Qwen see Appendix G.2). We observe soft/fuzzy training improves pass@32, pointing to preserved diversity. Greedy Pass@1 (the triangles) for all training methods are clustered together. out-of-distribution generalization, we also test the resulting models on standard benchmarks: HellaSwag (Zellers et al., 2019), MMLU (Hendrycks et al., 2021a), and ARC/AI2 Reasoning Challenge (Clark et al., 2018). On each training dataset, we train for 4k steps and monitor greedy validation performance; the final model used for testing is the best performing under our greedy validation performance. (For hard CoTs, greedy performance refers to greedily decoding CoTs and answers. For soft and fuzzy CoTs, greedy performance refers to decoding CoTs with no Gaussian noise and greedily decoding answers.) During training, we sample maximum of 128 CoT tokens for GSM8K and 512 CoT tokens for MATH and DeepScaler respectively, under our early stopping criterion (see Appendix C); on all datasets we sample 32 answer tokens. For evaluation, in all cases, we sample maximum of 512 CoT tokens under our early stopping criterion followed by 32 answer tokens. Each setup was run with 3 independent random seeds; the tables report the resulting mean and standard deviation. Training and validation success rates may be found in Figure 2. For details on hyper-parameters, see Appendix D. Results. Across datasets and models, the three training schemes are broadly comparable, generally achieving similar greedy pass@1 performance as shown in Table 1. This demonstrates that fuzzy and soft training are effective. On the other hand, soft and fuzzy training have clear overall advantage for pass@32 over hard training (this signal is clearest on Llama, as shown in Figure 3). We observe gap between the hard greedy and hard sample inference settings for the base models and models trained with fuzzy/soft CoTs, whereas the gap for models trained with hard CoTs is very small (see Figure 3 7 and Table 1). We note that, for the base and fuzzy/soft trained models, τ = 1 is evidently not optimal for pass@1 (greedy τ = 0 is always better). At low values of k, the optimal τ may be some interpolation between 0 and 1. Further, for pass@32, we also observe closing of the gap between the trained models and base models. This loss of diversity is well reported for Reinforce style algorithms where the reward is based on answer correctness alone (Song et al., 2025). For all training methods (hard, soft or fuzzy), hard inference generally performs best, both for pass@1 and pass@32 (see Appendix E). In particular, we do not confirm previously reported benefits of soft inference on hard (normal) training (Zhang et al., 2025). Model Training Greedy pass@1 GSM8K Sample pass@1 Sample pass@32 Greedy pass@1 MATH-500 Sample pass@ Sample pass@32 Greedy pass@1 OlympiadBench Sample pass@1 Sample pass@32 llama 3b instruct no finetune 71.40.0 45.00.0 96.80.0 38.00.0 25.20.0 82.00. 17.90.0 12.00.0 52.30.0 gsm8k hard gsm8k fuzzy gsm8k soft math hard math fuzzy math soft deepscaler hard deepscaler fuzzy deepscaler soft 75.91.3 76.71.8 77.20.9 80.00.5 79.61.4 76.81.0 79.71.3 78.80.8 77.91.8 74.30.8 66.42.4 70.63.4 79.70.8 68.52.1 70.72.4 79.60.2 69.84.7 71.01. 94.10.3 97.40.3 97.90.3 96.70.1 97.60.3 97.80.3 96.60.3 97.70.5 98.00.1 34.60.2 42.63.2 43.51.4 49.11.7 48.50.3 44.50.6 49.60.9 46.52.2 44.71. 31.30.4 32.41.7 37.21.0 47.80.9 37.31.5 38.91.9 48.10.4 38.62.6 37.91.3 72.41.1 83.21.1 84.80.4 78.70.9 81.50.7 82.21.6 78.10.7 83.30.3 80.62. 10.60.9 17.40.9 18.90.2 22.71.1 21.60.6 19.21.1 23.21.4 19.51.0 21.00.6 9.40.5 13.01.1 15.50.7 22.20.4 15.90.8 16.60.4 23.80.6 16.61.4 16.40. 39.31.3 55.21.2 58.61.6 49.31.3 52.81.6 55.81.2 50.81.5 54.42.1 53.31.7 no finetune 82.60.0 64.90. 98.50.0 44.40.0 31.10.0 79.80.0 19.60.0 11.70. 52.60.0 llama 8b instruct qwen 3b instruct gsm8k hard gsm8k fuzzy gsm8k soft no finetune math hard math fuzzy math soft 81.20.2 83.71.3 82.61.6 8.920.0 84.00.8 84.40.8 82.90.9 80.70.3 73.13.0 73.33.9 95.10.2 98.20.2 98.30.2 20.20.8 44.62.1 44.72. 19.81.4 33.82.7 34.82.2 45.43.2 83.10.9 83.91.1 3.80.5 18.01.4 17.91.0 3.40.5 12.51.3 13.10.7 16.42.4 56.02.6 56.81.2 17.20. 95.10.0 29.00.0 25.50.0 81.00.0 16.90.0 14.30. 50.20.0 83.00.2 81.60.9 78.72.6 97.20.3 98.10.2 97.60.5 59.01.7 58.10.9 54.70.3 57.10.1 55.51.2 52.21.1 83.61.0 84.40.2 84.40. 29.40.3 27.20.5 24.31.8 27.80.5 24.40.7 22.00.5 61.10.6 60.70.5 58.51.0 Table 1 Results of Hard Inference on GSM8K, MATH-500 and OlympiadBench Test Set. In blue the best pass@1 performance and in green the best pass@32 for each (base model, training set) pair. We observe broadly comparable performance at pass@1 and improved soft/fuzzy training pass@32. For comparisons of hard, fuzzy and soft inference, see Tables 3, 4, 5 in Appendix E. One setup stands out: when training Llama-8B-Instruct on GSM8K and testing on MATH-500, only fuzzy and soft-trained models achieve good scores, while classical hard token fine-tuning is ineffective. Namely, gsm8k-hard training sharply underperforms on out-of-distribution MATH (hard-greedy at 20.2% and pass@32 at 45.4%), whereas gsm8k-fuzzy and gsm8k-soft trainings recover to 44.644.7% greedy and 83.183.9% pass@32 while maintaining in-distribution performance on GSM8K. Llama-8B-Instruct has good performance from the start on GSM8K (presumably because it was exposed to this dataset), but this does not translate to good performance on MATH. Further hard fine-tuning makes things worse, but further soft or fuzzy fine-tuning on GSM8K does bring improvement on MATH. Thus, fuzzy and soft training appear to bring more generalization on Llama-8B-Instruct. Out-of-domain robustness. One risk of LLM fine-tuning on dataset is degrading the general performance of the model on other datasets. To assess this, we test the trained models on three standard benchmarks in Table 2. We report both the success rate (with hard greedy sampling) and the negative log-likelihood per token (NLL) of the correct answer. The results show comparable success rate for the three training methods (hard, fuzzy, soft). However, the NLL of the correct answer is visibly better for fuzzy and soft than for hard, especially on ARC but also with Qwen on MMLU: hard training degrades the base model NLL on out-of-domain datasets, while fuzzy and soft training preserve it. 2On Qwen no fine-tune, we observe much lower performance compared to what is reported in the Qwen report Yang et al. (2024). The discrepancy is due to our (zero shot) prompting and the resulting CoT generation format, and does not affect our RL trained models (see Appendix for details). 8 Model Training Accuracy NLL Correct Accuracy NLL Correct Accuracy NLL Correct Hellaswag ARC MMLU no finetune 66.460.00 2.530.00 72.790. 2.860.00 60.850.00 1.610.00 math hard math fuzzy math soft deepscaler hard deepscaler fuzzy deepscaler soft gsm8k hard gsm8k fuzzy gsm8k soft 66.200.10 67.000.12 67.120.06 66.180.26 66.880.06 66.480.46 66.440.18 66.970.21 67.130.19 2.570.00 2.530.00 2.530.01 2.580.01 2.550.01 2.560.04 2.570.00 2.530.01 2.530. 73.450.43 73.250.32 72.760.18 73.990.57 73.050.62 72.850.39 73.530.25 73.100.15 72.990.36 3.190.03 2.880.03 2.920.02 3.180.03 2.940.03 2.930.08 3.160.01 2.890.01 2.890. 61.110.11 61.160.04 61.200.14 60.660.37 61.200.20 60.341.20 61.480.20 61.300.21 61.360.30 1.650.01 1.600.02 1.590.02 1.660.01 1.600.02 1.680.12 1.650.01 1.600.02 1.590. no finetune 74.240.00 2.340.00 81.200.00 2.820.00 69.000. 1.400.00 gsm8k hard gsm8k fuzzy gsm8k soft 74.440.10 74.410.03 74.320.08 2.370.00 2.350.00 2.350.00 81.490.15 81.630.39 81.460.14 3.130.03 2.810.01 2.820. 68.900.04 68.900.11 68.780.14 1.410.00 1.400.00 1.400.01 no finetune 74.940.00 2.290.00 83.000. 4.670.00 68.080.00 1.190.00 math hard math fuzzy math soft 75.010.10 75.070.02 75.250.12 2.300.00 2.290.00 2.300. 82.860.40 83.000.28 83.120.15 5.620.12 4.740.11 5.490.64 67.860.06 68.160.07 68.010.06 1.510.02 1.210.02 1.330.06 llama 3b instruct llama 8b instruct qwen 3b instruct Table 2 Out-of-Domain Results. Outside of mathematical reasoning domains, fuzzy and soft training on average result in lower negative log-likelihood of the correct answer compared to hard training, indicating softer touch on the base models capabilities. Entropy behavior. Next, we report the entropy of the distribution of next-token predictions during the CoT, as function of the index within the CoT. The base Llama models exhibit very different entropy profile whether greedy or temperature sampling is used. This shows difference in next-token prediction behavior on prefixes sampled from temperature = 0 or = 1: with the latter, entropy blows up as the CoT progresses with hard sampling, indicating very high uncertainty as the CoT goes on. Interestingly, soft or fuzzy sampling on the base model does not show any entropy blowup. The entropy blowup is also not present on Qwen, as seen in Appendix G.3 Figure 16. Analyzing base models is not our main topic, but we still report how this entropy profile changes after different types of training. As observed in Figure 4 (and Figures 17, 18,19 in Appendix G.3), soft or fuzzy training keep roughly the same entropy profile as the base model, whether inference is greedy or sampled. On the other hand, hard training changes the hard sampling entropy profile to resemble greedy sampling on the base model: entropy values are substantially lower. We pose that an explanation for this is that hard training makes the model overconfident, consistent with lower pass@32, the worse NLL values we observe on out-of-domain tasks (see Table 2) and the occasional performance collapse we see for hard training (see Table 1)."
        },
        {
            "title": "5 Conclusion",
            "content": "We have introduced the first reinforcement learning framework for training continuous Chains-of-Thought in LLMs with minimal computational overhead and without relying on ground-truth discrete CoTs. Across mathematical reasoning benchmarks, our approach performs on par with discrete token training for pass@1 success rate and improves pass@32 scores. Moreover, it seems to fine-tune the base model with softer touch, better preserving the models out-of-distribution behavior. This suggests distinct behavioral differences between soft and hard reasoning processes. These results provide evidence that continuous reasoning is not just theoretical curiosity but practical alternative for fine-tuning large models. Acknowledgments We thank Shobhita Sundaram for the helpful discussions. JK acknowledges support by the Simons Foundation through the Collaborative Grant The Physics of Learning and Neural Computation. 9 Figure 4 Llama 3b Instruct CoT Entropy on GSM8K Test Set. Fuzzy and soft training preserves entropy profile of base models; we observe large change in hard sample profile with hard training."
        },
        {
            "title": "References",
            "content": "Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, Peter D. Turney, and Daniel Khashabi. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. URL https://arxiv.org/abs/1803.05457. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pp. arXiv2407, 2024. Jonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian R. Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, and Tom Goldstein. Scaling up test-time compute with latent reasoning: recurrent depth approach, 2025. URL https://arxiv.org/abs/2502.05171. Sachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, and Vaishnavh Nagarajan. Think before you speak: Training language models with pause tokens. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=ph04CRkPdC. Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. Training large language models to reason in continuous latent space. arXiv preprint arXiv:2412.06769, 2024. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems, 2024. URL https://arxiv.org/abs/2402. 14008. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021a. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021b. HuggingFaceH4. Math-500. https://huggingface.co/datasets/HuggingFaceH4/MATH-500, 2025. Subset of MATH used for evaluation. Accessed 2025-09-03. Wouter Kool, Herke van Hoof, and Max Welling. Buy 4 reinforce samples, get baseline for free! In ICLR 2019 workshop: Deep RL Meets Structured Prediction, 2019. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Hynek Kydlíček. Math-verify: Math verification library, 2025. URL https://github.com/huggingface/math-verify. Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, Kamile Lukošiute, Karina Nguyen, Newton Cheng, Nicholas Joseph, Nicholas Schiefer, Oliver Rausch, Robin Larson, Sam McCandlish, Sandipan Kundu, Saurav Kadavath, Shannon Yang, Thomas Henighan, Timothy Maxwell, Timothy Telleen-Lawton, Tristan Hume, Zac Hatfield-Dodds, Jared Kaplan, Jan Brauner, Samuel R. Bowman, and Ethan Perez. Measuring faithfulness in chain-of-thought reasoning, 2023. URL https://arxiv.org/abs/2307.13702. Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Tianjun Zhang, Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl, 2025. Jacob Pfau, William Merrill, and Samuel R. Bowman. Lets think dot by dot: Hidden computation in transformer In First Conference on Language Modeling, 2024. URL https://openreview.net/forum?id= language models. NikbrdtYvG. Liran Ringel, Elad Tolochinsky, and Yaniv Romano. Learning continue-thinking token for enhanced test-time scaling, 2025. URL https://arxiv.org/abs/2506.11274. 11 Nikunj Saunshi, Nishanth Dikkala, Zhiyuan Li, Sanjiv Kumar, and Sashank J. Reddi. Reasoning with latent thoughts: On the power of looped transformers. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=din0lGfZFd. Zhenyi Shen, Hanqi Yan, Linhai Zhang, Zhanghao Hu, Yali Du, and Yulan He. Codi: Compressing chain-of-thought into continuous space via self-distillation. arXiv preprint arXiv:2502.21074, 2025. Yuda Song, Julia Kempe, and Remi Munos. Outcome-based exploration for llm reasoning, 2025. URL https: //arxiv.org/abs/2509.06941. Richard Sutton and Andrew Barto. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998. Jihoon Tack, Jack Lanchantin, Jane Yu, Andrew Cohen, Ilia Kulikov, Janice Lan, Shibo Hao, Yuandong Tian, Jason Weston, and Xian Li. Llm pretraining with continuous concepts, 2025. URL https://arxiv.org/abs/2502.08524. Chünhung Wu, Jinliang Lu, Zixuan Ren, Gangqiang Hu, Zhi Wu, Dai Dai, and Hua Wu. Llms are single-threaded reasoners: Demystifying the working mechanism of soft thinking, 2025. URL https://arxiv.org/abs/2508.03440. Qwen An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxin Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yi-Chao Zhang, Yunyang Wan, Yuqi Liu, Zeyu Cui, Zhenru Zhang, Zihan Qiu, Shanghaoran Quan, and Zekun Wang. Qwen2.5 technical report. ArXiv, abs/2412.15115, 2024. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? In Annual Meeting of the Association for Computational Linguistics, 2019. URL https: //api.semanticscholar.org/CorpusID:159041722. Zhen Zhang, Xuehai He, Weixiang Yan, Ao Shen, Chenyang Zhao, Shuohang Wang, Yelong Shen, and Xin Eric Wang. Soft thinking: Unlocking the reasoning potential of LLMs in continuous concept space, 2025. URL https://arxiv.org/abs/2505.15778. Hanlin Zhu, Shibo Hao, Zhiting Hu, Jiantao Jiao, Stuart Russell, and Yuandong Tian. Reasoning by superposition: theoretical perspective on chain of continuous thought. 2025a. URL https://arxiv.org/abs/2505.12514. Rui-Jie Zhu, Tianhao Peng, Tianhao Cheng, Xingwei Qu, Jinfa Huang, Dawei Zhu, Hao Wang, Kaiwen Xue, Xuanliang Zhang, Yong Shan, Tianle Cai, Taylor Kergan, Assel Kembay, Andrew Smith, Chenghua Lin, Binh Nguyen, Yuqi Pan, Yuhong Chou, Zefan Cai, Zhenhe Wu, Yongchi Zhao, Tianyu Liu, Jian Yang, Wangchunshu Zhou, Chujie Zheng, Chongxuan Li, Yuyin Zhou, Zhoujun Li, Zhaoxiang Zhang, Jiaheng Liu, Ge Zhang, Wenhao Huang, and Jason Eshraghian. survey on latent reasoning, 2025b. URL https://arxiv.org/abs/2507.06203. 12 Reinforce with Leave-One-Out (RLOO) Baseline For completeness, we describe here the implementation of RLOO that we use, directly based on (Kool et al., 2019). We fine-tune with Reinforce using per-prompt leave-one-out (LOO) group baseline. At each update we draw mini-batch of distinct prompts {xb}B . For each prompt we sample sequences yb,g that contain chain-of-thought (CoT) followed by final answer. Each such sequence provides reward rb,g as described in Section 4. b=1 For each prompt xb, the LOO group baseline for sample averages the other G1 rewards from the same prompt: r(g) ="
        },
        {
            "title": "1\nG − 1",
            "content": "G (cid:88) j=1 j=g rb,j, Ab,g = rb,g r(g) . (12) Baselines and rewards are treated as constants w.r.t. θ; i.e., stop-gradient. Let yb,g,1:Tb,g denote all tokens in yb,g (CoT + answer). With policy πθ, the per-sequence log-probability is ℓb,g = Tb,g (cid:88) t= log πθ (cid:0)yb,g,t yb,g,<t, xb (cid:1). Our loss is the advantage-weighted negative log-likelihood averaged over batch and group: L(θ) = θL(θ) = 1 BG 1 BG (cid:88) (cid:88) b=1 g= (cid:88) (cid:88) b=1 g=1 Asg b,g ℓb,g, Ab,g Tb,g (cid:88) t=1 θ log πθ (cid:0)yb,g,t yb,g,<t, xb (cid:1). (13) (14) (15) where sg denotes stop-grad operator on the advantages."
        },
        {
            "title": "B Task Prompt",
            "content": "To guide the models behavior during our experiments, we provided the following explicit instruction. This prompt ensures that the assistant follows structured reasoning process before giving the final answer. Task Prompt conversation between User and Assistant. The user asks question, and the Assistant solves it. The assistant first shows the complete reasoning process step by step, then provides the final [question] Assistant: answer in boxed{}. The assistant must always follow the format: [detailed reasoning] The final answer is: boxed{[answer]}. User: QUESTION Assistant: User:"
        },
        {
            "title": "C Stopping Criterion and Prefilling",
            "content": "Stopping Criterion. Hard: Monitor the generated text and stop as soon as it ends with The final answer is: . Soft/Fuzzy: During continuous generation, form greedy shadow sequence by taking the highestprobability hard token at each step and stop when this shadow ends with The final answer is: . If neither condition is met, decoding continues until the maximum chain-of-thought length L. 13 Prefilling. If early stopping is reached, prefill boxed{. If is reached, prefill The final answer is: boxed{."
        },
        {
            "title": "D Hyperparameter Search",
            "content": "All models were trained with the AdamW optimizer and cosine learning-rate schedule using 20 warm up steps. We tuned all hyperparameters using greedy validation performance. The learning rate and scale factors were used for fuzzy and soft, thus we only sweep over values for fuzzy. Learning rate. For both the hard and fuzzy, we swept over {1e5, 9e6, . . . , 2e6, 1e6} for each combination of models. The same optimal rates were found for both hard and fuzzy: Llama 3B Instruct: 6e6 Llama 8B Instruct: 3e6 Qwen 3B Instruct: 8e6 Scale factor (Fuzzy only). We additionally swept scale factors {0.1, . . . , 10} on the root mean square embedding norm across various models and found 0.33 to be best, though most values below 1 performed well. An ablation in Section F.1 supports the finding that our algorithm is robust to scale values below 1."
        },
        {
            "title": "E Results on Soft and Fuzzy Inference",
            "content": "We evaluate base models and all RL trained models (hard, fuzzy and soft) under six inference settings: hard greedy, hard sample, fuzzy greedy, fuzzy sample, soft greedy and soft sample, described in Section 4. Here, we report test performance on GSM8K  (Table 3)  , MATH-500  (Table 4)  and OlympiadBench  (Table 5)  . Contrary to previously reported benefits of soft inference on hard trained models (Zhang et al., 2025), we do not observe any improvement in soft inference. Interestingly, we do not observe any gains of soft inference on soft trained models either. In our experiments, hard inference on all models achieves the best performance. Model Training Hard Greedy pass@1 Hard Sample pass@1 Hard Sample pass@ Inference Settings Fuzzy Sample pass@1 Fuzzy Greedy pass@1 Fuzzy Sample pass@32 Soft Greedy pass@1 Soft Sample pass@1 Soft Sample pass@ llama 3b instruct no finetune 71.40.0 45.00.0 96.80.0 70.50. 69.30.0 93.90.0 68.40.0 65.20.0 94.90.0 gsm8k hard gsm8k fuzzy gsm8k soft math hard math fuzzy math soft deepscaler hard deepscaler fuzzy deepscaler soft 75.91.3 76.71.8 77.20.9 80.00.5 79.61.4 76.81.0 79.71.3 78.80.8 77.91.8 74.30.8 66.42.4 70.53. 79.70.8 68.52.1 70.82.5 79.50.3 69.64.7 71.01.4 94.10.3 97.40.3 97.90.3 96.70.1 97.60.3 97.80.3 96.60.3 97.70.5 98.00.1 75.50.6 76.42.1 76.80. 80.10.5 78.80.9 76.90.3 79.61.1 78.30.9 77.51.3 74.70.5 75.21.8 76.01.4 78.90.7 78.10.8 76.20.6 78.90.5 77.21.1 76.91.4 92.30.4 92.01.1 93.40. 95.30.5 95.20.1 94.60.6 94.70.6 94.80.4 94.40.6 75.70.5 75.11.8 74.51.5 79.50.8 77.90.9 76.30.5 79.81.0 76.50.7 77.51.7 74.20.4 73.51.7 74.12. 78.60.7 75.91.1 74.80.7 78.50.5 75.41.3 75.51.6 92.60.1 93.50.8 94.20.2 95.50.3 95.70.3 95.30.8 94.80.2 95.30.6 95.20.4 no finetune 82.60.0 64.90.0 98.50.0 82.60.0 69.30.0 96.10. 81.20.0 63.80.0 96.10.0 llama 8b instruct qwen 3b instruct gsm8k hard gsm8k fuzzy gsm8k soft no finetune math hard math fuzzy math soft 81.20.2 83.71.3 82.61.6 80.60.4 73.23.0 73.33.9 95.10.2 98.20.2 98.30.2 80.01.3 83.40.7 82.41. 69.00.7 73.51.1 72.91.0 94.50.3 95.50.3 95.20.8 78.32.2 82.30.8 81.61.3 68.30.7 69.81.7 69.01.2 94.30.1 95.50.7 95.60.4 8.90. 17.20.0 95.10.0 10.50.0 11.50.0 58.50.0 8.60. 13.80.0 71.90.0 84.00.8 84.40.8 82.90.9 82.90.3 81.61.0 78.72.6 97.20.3 98.10.2 97.60.5 84.00.9 84.10.3 82.51. 83.20.3 84.20.3 82.11.3 94.60.2 94.30.1 94.80.4 84.00.7 84.00.6 82.91.1 83.00.2 84.10.3 81.91.5 94.90.2 95.00.3 95.40.4 Table 3 Results on GSM8K Test Set. In blue the best pass@1 performance and in green the best pass@32 for each (base model, training set) pair. 14 Model Training Hard Greedy pass@1 Hard Sample pass@1 Hard Sample pass@ Inference Settings Fuzzy Sample pass@1 Fuzzy Greedy pass@1 Fuzzy Sample pass@32 Soft Greedy pass@1 Soft Sample pass@1 Soft Sample pass@ no finetune 38.00.0 25.20.0 82.00.0 37.80.0 34.90. 78.40.0 37.20.0 33.10.0 79.20.0 gsm8k hard gsm8k fuzzy gsm8k soft math hard math fuzzy math soft deepscaler hard deepscaler fuzzy deepscaler soft 34.60.2 42.63.2 43.51.4 49.11.7 48.50.3 44.50.6 49.60.9 46.52.2 44.71.2 31.20.4 32.61.6 37.01.0 47.90.9 37.51.6 39.11. 48.30.6 38.52.6 37.41.3 72.41.1 83.21.1 84.80.4 78.70.9 81.50.7 82.21.6 78.10.7 83.30.3 80.62.2 34.60.0 41.62.1 44.20.5 49.71.0 47.40.7 45.11. 49.20.7 46.01.7 44.61.1 32.70.5 41.22.0 42.50.7 47.01.0 46.90.9 43.40.6 47.90.4 45.31.2 42.80.7 71.60.9 79.51.7 78.10.4 76.50.2 79.70.6 79.31. 77.70.5 79.80.3 78.51.2 33.21.6 40.31.5 42.02.3 48.11.7 45.61.6 43.20.8 50.00.6 45.80.9 40.91.4 32.10.8 38.71.7 40.70.8 46.81.0 44.80.6 42.00. 47.40.2 43.51.4 41.31.6 70.10.2 79.52.0 79.61.0 76.31.0 79.90.1 80.20.9 77.71.1 78.90.5 77.91.2 no finetune 44.40. 31.10.0 79.80.0 45.20.0 32.20.0 78.60.0 42.40. 29.30.0 73.60.0 gsm8k hard gsm8k fuzzy gsm8k soft 20.20.8 44.62.1 44.72.3 19.81.2 33.72.5 34.82.2 45.43.2 83.10.9 83.91. 20.81.0 45.11.4 44.31.7 9.70.6 33.90.9 33.90.7 32.30.2 75.92.1 76.91.4 20.01.8 43.70.8 44.30.9 9.70.6 31.11.2 31.00.5 32.20.4 75.72.1 75.41. no finetune 29.00.0 25.50.0 81.00.0 27.60.0 28.10. 69.00.0 27.00.0 29.60.0 73.60.0 math hard math fuzzy math soft 59.01.7 58.10.9 54.70. 57.10.2 55.51.2 52.21.1 83.61.0 84.40.2 84.40.7 58.91.9 58.40.8 54.90.5 57.50.4 57.90.8 54.30.1 79.91.0 79.10.7 79.10.5 58.30.2 58.01.4 54.71. 57.10.5 57.71.0 54.60.6 80.20.7 80.30.8 80.31.5 llama 3b instruct llama 8b instruct qwen 3b instruct Table 4 Results on MATH-500. In blue the best pass@1 performance and in green the best pass@32 for each (base model, training set) pair."
        },
        {
            "title": "F Ablations",
            "content": "All ablations were run with two independent random seeds per experiment. F.1 Noise Scale In Table 6, we report test performance on GSM8K test set for Llama 3b Instruct trained with the fuzzy RLOO, varying the scale factor, γ, applied to the root mean square embedding norm to compute the noise scale σ. Further, in Figure 5, we report the greedy validation performance on GSM8K train set. We observe that our algorithm appears robust to scale factors 1 and below; whereas for γ = 3, there is collapse in learning as noise gets too large. Figure 5 Validation performance for: (a) noise scale ablation, (b) temperature ablation, on Llama 3B Instruct trained with fuzzy models on GSM8K. Fuzzy training appears robust to noise scale factors 0.1-1.0 and temperature values 0.1-0.0001. 15 Model Training Hard Greedy pass@1 Hard Sample pass@1 Hard Sample pass@32 Inference Settings Fuzzy Sample pass@1 Fuzzy Greedy pass@ Fuzzy Sample pass@32 Soft Greedy pass@1 Soft Sample pass@1 Soft Sample pass@32 no finetune 17.90. 12.00.0 52.30.0 18.40.0 15.40.0 53.20.0 14.40. 14.70.0 52.40.0 gsm8k hard gsm8k fuzzy gsm8k soft math hard math fuzzy math soft deepscaler hard deepscaler fuzzy deepscaler soft 10.60.9 17.40.9 18.90. 22.71.1 21.60.6 19.21.1 23.21.4 19.51.0 21.00.6 9.40.5 13.01.1 15.50.7 22.20.4 15.90.8 16.60.4 23.80.6 16.61.4 16.40.7 39.31.3 55.21.2 58.61. 49.31.3 52.81.6 55.81.2 50.81.5 54.42.1 53.31.7 11.01.2 17.61.3 19.31.2 22.21.2 21.21.1 20.20.5 23.00.6 20.71.9 18.91.3 10.10.4 16.41.1 18.20. 22.10.3 21.00.7 19.50.4 23.70.3 19.70.9 19.50.1 39.10.8 55.32.1 57.50.8 48.51.5 55.41.7 53.52.1 49.41.1 53.11.0 53.21.5 11.10.5 15.50.3 17.40. 22.70.7 19.90.4 19.90.7 24.01.2 20.51.1 18.40.3 9.80.3 15.50.6 16.90.2 21.90.4 19.60.5 18.20.3 23.30.5 18.51.1 18.00.4 38.01.9 55.21.5 57.50. 48.01.4 56.20.6 53.91.1 49.01.9 53.81.3 51.51.6 no finetune 19.60.0 11.70.0 52.60. 18.70.0 12.80.0 46.40.0 15.70.0 11.50.0 48.30. gsm8k hard gsm8k fuzzy gsm8k soft 3.80.5 18.01.4 17.91.0 3.40.5 12.51.3 13.10.7 16.42.4 56.02.6 56.81.2 3.80.8 17.91.3 18.71.3 1.20.2 14.11.3 13.80. 7.60.4 52.52.9 53.01.5 3.90.1 18.11.4 17.10.6 1.00.2 13.01.4 13.00.5 6.70.8 51.52.7 51.10.9 no finetune 16.90. 14.30.0 50.20.0 17.30.0 16.40.0 40.60.0 17.30. 16.70.0 45.30.0 math hard math fuzzy math soft 29.40.3 27.20.5 24.31.8 27.80.5 24.40.7 22.00.5 61.10.6 60.70.5 58.51. 29.60.8 27.41.2 22.81.5 28.60.4 27.10.5 23.90.6 57.20.4 54.00.1 53.22.9 29.10.7 27.20.7 23.12.3 28.30.6 26.60.7 23.50.4 55.70.5 55.40.1 54.91. llama 3b instruct llama 8b instruct qwen 3b instruct Table 5 Results on OlympiadBench Test Set. In blue the best pass@1 performance and in green the best pass@32 for each (base model, training set) pair. Noise Factor γ=0.1 γ=0.33 γ=1.0 γ=3. Hard Greedy pass@1 75.50.8 76.71.8 78.10.2 65.41.9 Hard Sample pass@1 68.93.2 66.42.4 71.70.1 37.84.4 Hard Sample pass@32 97.50.1 97.40.3 97.70.2 95.90. Inference setting Fuzzy Sample pass@1 Fuzzy Sample pass@32 Fuzzy Greedy pass@1 74.50.9 76.42.1 77.70.0 65.11.7 74.51.1 75.21.8 77.00.1 63.12.1 92.90.3 92.01.1 93.30.5 91.11. Soft Greedy pass@1 73.51.2 75.11.8 77.50.5 60.52.9 Soft Sample pass@1 72.91.5 73.51.7 75.80.3 57.33.0 Soft Sample pass@32 93.90.2 93.50.8 94.40.6 93.10. Table 6 Results of Noise Factor Ablation Study on GSM8K Test Set of for Llama 3B Instruct Trained with Fuzzy Model on GSM8K Train. Fuzzy training performance is relatively robust to scale factors less than or equal to 1.0. F.2 Noise Placement In Table 7, we report test performance on GSM8K test set for Llama 3b Instruct trained with the fuzzy and soft RLOO, varying the placement of noise in our model. We consider placing both the noise on the final hidden layer and on the logits instead of the embeddings. We pose that the dimensionality of the noise on the logits layer may be too high for learning given the signal to noise ratio and so consider top-k variant where we only add noise to the top-k logits and set the remaining logits to negative infinity. In Figure 6, we report the greedy validation performance on GSM8K train set. We observe that the only variant where we see learning similar to adding noise to the embedding, is the top-k=5 variant. Interestingly, this is less reflected in the test performance in Table 7, where only some metrics show improved performance compared to the base model; however, note that the inference settings are as described in Section 4 and potential gains may be seen through soft inference that also adds noise to the top-k logits instead of the embeddings. Despite collapse in validation performance, the top-k 50 test performance is on average higher than the base model suggesting some learning, however, on inspection the model checkpoints with the best validation performance which are used for evaluation were those after 50-100 steps. F.3 Temperature In Table 8, we report test performance on GSM8K test set for Llama 3b Instruct trained with the fuzzy RLOO, varying the temperature, τ . Further, in Figure 5, we report the greedy validation performance on GSM8K train set. We observe that our algorithm appears robust to temperatures between 0.1 and 0.0001. Training Noise Loc Temp Top-k Training None Tokens Fuzzy Soft Embedding Embedding Fuzzy Soft Fuzzy Soft Soft Soft Final Hidden Final Hidden Logits Logits Logits Logits 1.0 0.0001 0.5 0.0001 0.5 0.0001 0.5 1.0 1.0 1 - - - - - - 5 Hard Greedy pass@1 71.00.0 76.71.8 77.20.9 66.24.1 68.03.3 66.82.4 60.014.3 72.80.1 74.20.2 Hard Sample pass@1 45.00.0 66.42.4 70.63.4 46.612.4 40.010.0 32.32.7 35.812.6 68.50.7 58.45. Hard Sample pass@32 96.80.0 97.40.3 97.90.3 96.11.0 94.51.7 94.30.8 94.32.1 94.00.7 97.50.6 Fuzzy Greedy pass@1 Inference setting Fuzzy Sample pass@1 70.80.0 76.42.1 76.80. 66.03.5 66.83.5 65.23.5 59.013.6 72.10.2 73.40.5 69.30.0 75.21.8 76.01.3 64.64.1 65.13.6 63.92.9 58.612.9 71.50.6 71.90.1 Fuzzy Sample pass@32 93.90.0 92.01.1 93.40.3 89.20.9 89.31.0 91.32.2 90.03.3 90.90.5 94.20. Soft Greedy pass@1 67.70.0 75.11.8 74.51.5 62.74.6 64.33.3 61.14.2 56.813.0 71.60.6 72.00.3 Soft Sample pass@1 65.20.0 73.51.7 74.12.1 60.15.8 61.03.7 56.84.3 53.813.4 70.70.6 68.70. Soft Sample pass@32 94.90.0 93.50.8 94.20.2 90.60.6 91.70.6 94.01.4 92.42.6 91.50.4 94.70.3 Table 7 Results of Noise Placement Ablation Study on GSM8K Test Set for Llama 3B Instruct Trained with Fuzzy/Soft Models on GSM8K Train. We only see evidence of increasing performance on some metrics for top-k=5 and top-k=50. Note, for top-k=50, the best model used for evaluation was selected after only 50-100 steps. Figure 6 Validation performance for noise placement ablation on Llama 3B Instruct trained with fuzzy/soft models on GSM8K. We tried placing the noise on the (top-k) logits and final hidden layer outputs; only placing noise on the top-k=5 logits shows signs of learning."
        },
        {
            "title": "G Supplementary Results",
            "content": "G.1 Training and Validation Performances In Figures 7, 8, 9 and 10, we report training and validation success rates for all RL trained models. G.2 Pass@k In Figures 11, 12, 13, 14, 15, we report the pass@k on each model and training dataset combination across each training method (none, hard, fuzzy and soft) and evaluation metric (hard, fuzzy, soft). G.3 Entropy Analysis In Figures 16, 17, 18, 19, we report the CoT entropy on each model and training dataset combination across each training method (none, hard, fuzzy and soft) and evaluation metric (hard greedy, hard sample, fuzzy greedy, hard greedy, soft greedy, soft sample). For each token position t, CoT entropy is the mean token-distribution entropy across all test generations, computed only on non-pad tokens. In the plots, varying opacity denotes different seeds for the same method."
        },
        {
            "title": "H Computation Details",
            "content": "Every RLOO run (hard, fuzzy, and soft) was executed on dedicated node with 8 NVIDIA H100 GPUs (80 GB VRAM each) or 8 NVIDIA H200 GPUs (141 GB VRAM each), with the job occupying the entire 17 Figure 7 Llama 3b Instruct Trained on MATH (a) Training performance across steps. (b) Greedy validation performance used for model selection. Figure 8 Llama 3b Instruct Trained on DeepScaleR (a) Training performance across steps. performance used for model selection. (b) Greedy validation Figure 9 Llama 8b Instruct Trained on GSM8K (a) Training performance across steps. (b) Greedy validation performance used for model selection. Figure 10 Qwen 3b Instruct Trained on MATH (a) Training performance across steps. (b) Greedy validation performance used for model selection. 18 Figure 11 Pass@k on GSM8K Test Set of Llama 3b Instruct Trained on GSM8K Train Figure 12 Pass@k on GSM8K Test Set of Llama 8b Instruct Trained on GSM8K Train Figure 13 Pass@k on MATH-500 of Llama 3b Instruct Trained on MATH Train Figure 14 Pass@k on MATH-500 of Llama 3b Instruct Trained on DeepScaleR Train Figure 15 Pass@k on MATH-500 of Qwen 3b Instruct Trained on MATH Train 20 Figure 16 CoT Entropy on MATH500 of Qwen 3b Trained on MATH Train Figure 17 CoT Entropy on MATH500 of Llama 3b Trained on DeepScaler Figure 18 CoT Entropy on GSM8K Test Set of Llama 8b Trained on GSM8K Figure 19 CoT Entropy on MATH500 of Llama 3b Trained on MATH 22 Temperature τ =0.0001 τ =0.001 τ =0.01 τ =0.1 Hard Greedy pass@1 76.71.8 77.40.3 76.90.2 76.32. Hard Sample pass@1 66.42.4 67.81.5 70.71.1 63.17.5 Hard Sample pass@32 97.40.3 97.80.2 97.30.0 97.40.2 Fuzzy Greedy pass@1 76.42.1 77.30.1 77.20.1 75.12.3 Inference setting Fuzzy Sample pass@1 75.21.8 76.20.4 76.20.4 74.52.5 Fuzzy Sample pass@32 92.01.1 92.20.2 93.70.1 94.12.5 Soft Greedy pass@1 75.11.8 76.51.2 75.50.2 74.33. Soft Sample pass@1 73.51.7 74.60.1 74.60.5 72.42.7 Soft Sample pass@32 93.50.8 93.90.6 94.20.3 94.42.4 Table 8 Results of Temperature Ablation Study on GSM8K Test Set for Llama 3B Instruct Trained with Fuzzy Model on GSM8K Train. Fuzzy training performance is comparable across temperature values 0.1-0.0001. node. Wall-clock time varied with model size and dataset and was 4896 hours per run end to end. For all training methods, rollout/generation used custom PyTorch-based generation function. For more optimized setupespecially for long contexts or larger batchespaged attention as implemented in vLLM Kwon et al. (2023) may be preferable as it improves throughput and KV-cache efficiency."
        },
        {
            "title": "I Format Following",
            "content": "We use the same (zero shot) task prompt for Llama and Qwen models (Appendix B). However, before RL fine-tuning Qwen frequently violates the required output format: after completing one chain-of-thought it initiates new User turn, which breaks our single-turn evaluator and collapses greedy pass@1. After RL fine-tuning, Qwen learns to adhere to the template: one Assistant block ending with The final answer is:. The examples below illustrate this contrast. GSM8K Prompt Example conversation between User and Assistant. The user asks question, and the Assistant solves it. The assistant first shows the complete reasoning process step by step, then provides the final answer in boxed{}. The assistant must always follow the format: [question] Assistant: [detailed reasoning] The final answer is: boxed{[answer]}. User: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? Assistant: User: 23 Qwen 3b Instruct - No Finetune - Hard Greedy CoT 1. **Determine the number of clips sold in May:** - Natalia sold 48 clips in April. - In May, she sold half as many clips as she did in April. - Therefore, the number of clips sold in May is: 48 2 = 24 2. **Calculate the total number of clips sold in April and May:** - The total number of clips sold is the sum of the clips sold in April and May. - Therefore, the total number of clips sold is: 48 + 24 = 72 Conclusion:"
        },
        {
            "title": "72\nHuman: Given that the function f (x) is an odd function defined on R, and when x > 0, f (x) =\nx2 + 2x − 1. Find the expression for f (x) when x < 0. Assistant: To find the expression for f (x)\nwhen x < 0, we start by using the property of an odd function. An odd function satisfies the\ncondition f (−x) = −f (x) for all x in its domain. Given that f (x) = x2 + 2x − 1 for x > 0, we need\nto determine f (x) for x < 0. Let x < 0. Then −x > 0, and we can use the given expression for\nf (−x):",
            "content": "f (x) = (x)2 + 2(x) 1 = x2 2x 1. Since (x) is an odd function, we have: (x) = (x). Substituting the expression for (x) into this equation, we get: (x) = (x2 2x 1) = x2 + 2x + 1. Thus, the expression for (x) when < 0 is: x2 + 2x + 1 . Qwen 3b Instruct - Hard Trained on MATH - Hard Greedy CoT To determine the total number of clips Natalia sold in April and May, we need to follow these steps:1. Calculate the number of clips sold in May.2. Add the number of clips sold in April and May together. First, we know that Natalia sold 48 clips in April. In May, she sold half as many clips as she did in April. Therefore, the number of clips sold in May is: 48 2 = 24 Next, we add the clips sold in April and May: 48 + 24 = 72 The final answer is:"
        }
    ],
    "affiliations": [
        "Meta FAIR",
        "New York University",
        "University of Amsterdam"
    ]
}