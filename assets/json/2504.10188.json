{
    "paper_title": "Efficient Generative Model Training via Embedded Representation Warmup",
    "authors": [
        "Deyuan Liu",
        "Peng Sun",
        "Xufeng Li",
        "Tao Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion models excel at generating high-dimensional data but fall short in training efficiency and representation quality compared to self-supervised methods. We identify a key bottleneck: the underutilization of high-quality, semantically rich representations during training notably slows down convergence. Our systematic analysis reveals a critical representation processing region -- primarily in the early layers -- where semantic and structural pattern learning takes place before generation can occur. To address this, we propose Embedded Representation Warmup (ERW), a plug-and-play framework where in the first stage we get the ERW module serves as a warmup that initializes the early layers of the diffusion model with high-quality, pretrained representations. This warmup minimizes the burden of learning representations from scratch, thereby accelerating convergence and boosting performance. Our theoretical analysis demonstrates that ERW's efficacy depends on its precise integration into specific neural network layers -- termed the representation processing region -- where the model primarily processes and transforms feature representations for later generation. We further establish that ERW not only accelerates training convergence but also enhances representation quality: empirically, our method achieves a 40$\\times$ acceleration in training speed compared to REPA, the current state-of-the-art methods. Code is available at https://github.com/LINs-lab/ERW."
        },
        {
            "title": "Start",
            "content": "Deyuan Liu1* Peng Sun2,1* Xufeng Li1,3 1Westlake University 2Zhejiang University Tao Lin1 3Nanjing University 5 2 0 2 4 ] . [ 1 8 8 1 0 1 . 4 0 5 2 : r Figure 1. Embedded Representation Warmup (ERW). Throughout the training process, we demonstrate that incorporating representations at the early stages is highly beneficial. To this end, we propose representation warmup stage that employs representation alignment loss to integrate representations from models such as Dinov2 [38] into the ERW. This initialized representation region is subsequently embedded into the diffusion model pipeline, providing strong starting point for training. Our ERW method thus enhances both efficiency and effectiveness, leading to faster convergence and superior performance compared to the REPA [54] method, thereby establishing new state-of-the-art."
        },
        {
            "title": "Abstract",
            "content": "Diffusion models excel at generating high-dimensional data but fall short in training efficiency and representation quality compared to self-supervised methods. We identify key bottleneck: the underutilization of high-quality, semantically rich representations during training notably slows down convergence. Our systematic analysis reveals critical representation processing regionprimarily in the early layers where semantic and structural pattern learning takes place before generation can occur. To address this, we propose Embedded Representation Warmup (ERW), plug-and-play framework where in the first stage we get the ERW module serves as warmup that initializes the early layers of the diffusion model with high-quality, pretrained representations. This warmup minimizes the burden of learning representations from scratch, thereby accelerating convergence and boosting performance. Our theoretical analysis demonstrates that ERWs efficacy depends on its precise integration into specific neural network layerstermed the representation processing regionwhere the model primarily processes and transforms feature representations for later generation. We further establish that ERW not only accelerates training convergence but also enhances representation quality: empirically, our method achieves 40 acceleration in training speed compared to REPA [54], the current state-of-the-art methods. Code is available at https://github.com/LINs-lab/ERW . 1. Introduction All roads lead to Rome, but it is not as good as being born in Rome. Deep generative models, particularly diffusion models [16, 49], have made remarkable strides in producing highfidelity, realistic images. These models have demonstrated impressive performance in tasks such as unconditional image generation [8] and text-to-image synthesis [43, 45], largely due to their capacity to model complex data distributions through iterative denoising processes. By refining noisy inputs over multiple stages, diffusion models effectively capture intricate features that yield visually coherent outputs. Despite these achievements, diffusion models often lag behind self-supervised learning methods in both training efficiency and the quality of learned representations. This gap is notable because efficient training and robust representation learning are pivotal for scaling generative models to real-world applications. While diffusion models implicitly learn semantic features during the denoising process [52, 53], these representations typically lack the depth and versatility seen in self-supervised approaches [3, 38]. Recent studies [29, 54] have investigated techniques to enhance internal representations in diffusion models, aiming to improve both the expressiveness of the generated data and the training efficiency. Moreover, Kadkhodaie et al. [20] highlight critical bottleneck: the tension between memorizing semantic information and generalizing to realistic data distributions. These observations lead to pivotal question: Q: Can we first learn high-quality external representations from well-established representation model and then integrate their representation into diffusion models, rather than training both representation and generation from scratch? Self-supervised learning approaches, such as contrastive learning [4] and masked autoencoders [14], as well as more recent methods like Dinov2 [38], have shown strong capabilities in extracting rich semantic representations from unlabeled data. However, adapting these external representations to diffusion models poses inherent challenges. Primarily, diffusion models operate on progressively denoised inputs, whereas self-supervised encoders are trained on clean data. In addition, the architectures of self-supervised encoders may differ significantly from typical diffusion model backbones, further complicating direct integration. Our approach. We observe that diffusion model training can be effectively conceptualized as two-phase process: first learning semantic representations, and then leveraging those representations for generative purposes. Motivated by this viewpoint, we propose Embedded Representation Warmup (ERW), which explicitly separates training into two phases. In the warmup phase, we initialize the earlylayer parameters of the diffusion model with weights from pretrained self-supervised backbone (e.g., Dinov2). Since early layers predominantly capture semantic features, this partial initialization effectively injects strong semantic priors into the diffusion model. Our ablation studies confirm that restricting representation learning to early layers, rather than extending it to the entire architecture, leads to both better performance and higher training efficiency. Furthermore, we decompose the diffusion models workflow into two distinct circuits: ① the Latent-toRepresentation (L2R) circuit, mapping raw inputs into semantic embedding space, and ② the Representation-toGeneration (R2G) circuit, synthesizing final outputs from these embeddings. Building on this insight, ERW uniformly initializes the models early layers to embed robust semantic features, forming solid foundation for subsequent generative training. Validation. Extensive experiments confirm that providing high-quality representations from the outset via ERW substantially accelerates convergence. We demonstrate significant training speedup (up to 40 compared to REPA) while maintaining state-of-the-art generative quality. Notably, ERW enables SiT-XL to reach an FID of 6.4 on ImageNet-1k without classifier-free guidance in under 4 hours on 8H800 GPUs. Since the warmup phase accounts for only fraction of the overall training cost, reusing warmup-optimized checkpoints further enhances training efficiency across multiple tasks. Contributions: (a) We identify and demonstrate that diffusion model training can be effectively decomposed into two distinct phases: representation learning phase and generation phase, with the representation processing mainly occurring in early layers (see Section 3 ). (b) We introduce Embedded Representation Warmup, twophase training framework that first initializes the representation layers using pretrained self-supervised models before proceeding to the main generation-focused training (see Section 4 ). (c) We empirically validate our approach on and demonstrating significant improvements in training efficiency and generation quality (see Section 5 ). 2. Related Work Our work builds upon and extends several areas of research in diffusion models, representation learning, and the integration of external representations for generative modeling. Integrating pretrained representations into diffusion models. Recent work exploits pretrained encoders to enhance diffusion-based generation. Würstchen [41] adopts two-stage design where an initial network learns semantic map, guiding second diffusion model for high-resolution output. RCG [32] further underscores the utility of compact latent codes, generated by first diffusion model and consumed by second, boosting generative performance. REPA [54] applies regularization throughout the diffusion training process to align model representations with pretrained visual encoders. In contrast, our approach builds on these ideas by decoupling representation learning from the denoising process. We explicitly separates the training into two phases first learning representation then learning generation. This design not only alleviates the optimization challenges associated with joint learning but also leads to significant acceleration in training convergence. 2 work for more effective and interpretable generative modeling. Our guiding hypothesis is that conventional diffusion methods entangle representation learning and generative decoding within the denoising process, burdening the network with two inherently distinct tasks. By explicitly isolating the semantic processing (L2R) from the generative inversion (R2G), our approach promotes modularity and simplifies the overall optimization landscape. This theoretical perspective also clarifies how different regions of the diffusion backbone specialize in semantic vs. generative aspects, offering fresh viewpoint on the internal mechanics of diffusion models. 3.1. Preliminaries Latent diffusion models. While classic diffusion models such as DDPM [16] adopt discrete-time denoising process, flow-based methods [1, 33, 48] explore diffusion in continuous-time setting. In particular, Scalable Interpolant Transformers (SiT) [10, 33, 34] offer unifying framework for training diffusion models on continuous-time stochastic interpolant. Below, we describe how SiT can be leveraged to learn powerful latent diffusion models. Forward process via stochastic interpolants. Consider data sample p(x) (e.g., an image) and let the encoder Hθ(x) map it to its latent representation denoted as Z. Given standard Gaussian noise ϵ (0, I), SiT defines forward process in the latent space, parameterized by continuous time [0, 1]: zt = αt + σt ϵ , (1) where αt and σt are deterministic, differentiable functions satisfying the boundary conditions: (α0, σ0) = (1, 0) and (α1, σ1) = (0, 1) . (2) This construction implies that z0 = (the clean latent representation) and z1 = ϵ (pure noise). Under mild conditions [1], the sequence {zt} forms stochastic interpolant that smoothly transitions between data and noise in the latent space. Velocity-based learning. To train diffusion model in this continuous-time framework, SiT employs velocity formulation. Differentiating zt with respect to yields: zt = αt + σt ϵ . (3) Conditioning on zt, we can rewrite the derivative as velocity field: zt = v(zt, t) , (4) where v(zt, t) is defined as the conditional expectation of zt given zt. neural network vθ(z, t) is then trained to Figure 2. Illustration of the Circuits in Hypothesis. From left to right, we first apply VAE encoder (the Pixel-to-Latent or (P2L) stage) to map high-dimensional inputs to compressed latent space. We then perform latent diffusion on these codes, dividing the backbone into two subregions: the Latent-to-Representation (L2R) region that capture and refine semantic features, and the Representation-to-Generation (R2G) region that decode the learned representation into final outputs. Leveraging self-supervised learning for generative modeling. Self-supervised encoders pretrained via contrastive learning [4] or masked reconstruction [14] have made significant inroads in representation quality. Recent developments integrate such encoders into generative pipelines, for instance using them as discriminators in GAN-based frameworks [26, 47] or to guide diffusion distillation [30]. Enhancing representation learning within diffusion models. Numerous studies highlight the discriminative potential of diffusion representations [52, 53], revealing that intermediate layers encode semantic features. Efforts like Repfusion [53] and DreamTeacher [30] distill these features to smaller networks for downstream tasks. Inspired by these findings, we identify distinct representation processing region where crucial representation learning occurs before generation can take place. By initializing this region with pretrained weights during dedicated warmup phase, we circumvent the costly need to learn complex features from scratch in the main training phase, leading to faster convergence and improved fidelity. Towards unified and efficient diffusion training. Beyond two-stage pipelines [32, 41], alternative methods fuse self-supervision with the diffusion objective, such as MaskDiT [55] or MAGE [31], yet often require careful objective balancing. Our plug-and-play method instead introduces clear two-phase approach: representation-focused warmup phase that initializes early diffusion layers with mature pretrained weights, followed by standard diffusion training phase that benefits from this strong representational foundation. 3. Hypothesis of Latent Diffusion Models In this section, we propose three-stage diffusion circuit Pixel-to-Latent (P2L), Latent-to-Representation (L2R), and Representation-to-Generation (R2G)as structured frame3 approximate v(z, t) by minimizing: Lvelocity(θ) = Ez,ϵ,t (cid:20)(cid:13) (cid:13) (cid:13)vθ(zt, t) (cid:16) αt + σt ϵ 2(cid:21) (cid:17)(cid:13) (cid:13) (cid:13) . (5) Successfully learning vθ(z, t) enables one to integrate the reverse-time ordinary differential equation (ODE) [49], thereby mapping noise samples back to coherent latent representations. 3.2. Three-Stage Diffusion Circuit Recent studies indicate that diffusion models jointly perform both representation learning and generative decoding during the denoising procedure [52, 54]. Notably, every layer in the network contributes to feature extraction and generative tasks to varying degrees. To make this dual functionality clearer, we propose decomposing the diffusion process into three distinct stages: Pixel-to-Latent (P2L), Latent-to-Representation (L2R), and Representation-to-Generation (R2G), as illustrated in Figure 2 . Formally, we posit that the diffusion sampling procedure can be written as: = Hθ(x) , = RθL2R(z) , = GθR2G(r) , (Pixel to Latent (P2L)) (Latent to Representation (L2R)) (Representation to Generation (R2G)) the Variational Autoencoder (VAE) encoder Hθ Here, compresses high-dimensional pixel data into lowerdimensional latent z. The diffusion model components RθL2R and GθR2G share parameters in diffusion model backbone but serve distinct functions: RθL2R extracts semantic representations from noisy latent codes (L2R), while GθR2G transforms these representations into refined latent outputs (R2G). During inference, VAE decoder Dθ would convert these refined latents back to pixel space. This decomposition into distinct processing stages provides more transparent view of how latent diffusion models function and support the idea from latent space compression in Saharia et al. [45] and the diffusion model training via represention alignment in REPA Yu et al. [54]. Loss Function Decomposition. The diffusion models objective is to learn the score function zt log p(zt), which represents the gradient of the log probability density of the data distribution. Our key theoretical insight is that this score function can be orthogonally decomposed into representation and generation components (see details in Appendix ): zt log p(zt) = zt log p(rt) (cid:125) (cid:123)(cid:122) (cid:124) Representation + zt log p(zt rt) (cid:123)(cid:122) (cid:125) Generation (cid:124) . (6) This decomposition permits us to partition the overall terms, diffusion objective into two nearly orthogonal focusing respectively on (i) extracting meaningful features from corrupted inputs and (ii) translating those features into coherent outputs. Such structure naturally aligns with the three-stage diffusion circuit we will soon discuss, clarifying the distinct roles played by different network regions. Stage I: Pixel-to-Latent (P2L). Before performing the denoising process in the high-dimensional pixel domain where noise may obscure semantic cuesmany contemporary methods [8, 16, 45] compress images into more tractable latent space: = Hθ(x) , (7) where (Hθ, Dθ) typically refers to variational autoencoder or related autoencoding architecture. This P2L stage reduces computational complexity and filters out low-level details, thus preserving more essential semantic information. From the perspective of the decomposed loss, P2L transforms the high-dimensional denoising problem into lowerdimensional one where representation components (capturing semantic concepts) and reconstruction components (handling fine details) become more clearly separable, facilitating favorable conditions for separating the training stages. Stage II: Latent-to-Representation (L2R). Given noisy latent zt from Eq. (7), the model initially extracts semantic representation rt using the mapping RθL2R. rt = RθL2R(zt) . (8) This step corresponds to estimating log p(rt) within the velocity loss decomposition (where we separate representation learning from generation, as detailed in Appendix ), sometimes termed the Representation Score [49]. Intuitively, the model should discern salient patterns (e.g., object shapes, style characteristics, or conditioning signals) before denoising. Under the sufficient statistic assumption (see Assumption 1 ), the representation rt effectively captures the essential information from the latent zt, allowing us to formulate the representation learning process as an independent objective: LL2R = Et,zt (cid:2) RθL2R(zt)zt log p(rt) 2(cid:3) , (9) where RθL2R represents the parameterized representation extraction module of the network. By explicitly decoupling the objective for semantic feature extraction from that of generative refinement, the model is guided to learn robust high-level representations and ensures that the early layers focus on capturing essential semantic features. Stage III: Representation-to-Generation (R2G). In the final phase of each reverse diffusion update in (3), known as the R2G stage, the extracted semantic representation is transformed into an updated latent with reduced noise: ztt = GθR2G(rt, t) . (10) 4 Figure 3. Selected Samples on ImageNet 256 256. Images generated by the SiT-XL/2 + REPA + ERW model using classifier-free guidance (CFG) with scale of = 2.2 under 40 epochs. This output serves the same purpose as the term introduced in our frameworks third Eq. (3), but is specifically defined for the discrete time step in the continuous-time diffusion process. In the decomposed view, this step aligns with the Generation Score term, zt log p(zt rt), detailed in Appendix : LR2G = Et,zt (cid:104) GθR2G(rt, t)zt log p(zt rt)2 (cid:105) , (11) where GθR2G represents the generation modules score estimator, transforming representations back into the latent space for denoising. Injecting the semantic representation rt into cleaner latent zttwhich is significantly less noisy than ztensures that abstract semantic features are effectively transformed into the precise latent elements required for content generation. Meanwhile, the cross term between L2R and R2G (also elaborated in Appendix ) remains small under the assumption that these two gradients are sufficiently orthogonal, mitigating destructive interference. Two-Stage Sampling Process: Representation Extraction Precedes Generation. Within the continuous-time framework, after mapping pixel data to the latent space through P2L, each infinitesimally small time step during the reverse SDE update can be interpreted as two-stage process: rt = RθL2R(zt) ztt = GθR2G(rt) (12) Hence, every time step naturally splits into (i) L2R for refining the representation and (ii) R2G for synthesizing an updated latent. This loop neatly implements the principle of first representation, then generation. Empirically, prior work [52, 54] confirms that early layers of the diffusion model predominantly focus on representation extraction, whereas later layers emphasize generative refinement. Consequently, the staged design mirrors the reverse-time diffusion trajectory, concluding in final latent z0 that is decoded via Dθ to yield the synthesized output x0. 4. Embedded Representation Warmup: Two-"
        },
        {
            "title": "Phase Training Framework",
            "content": "Diffusion models generate high-fidelity images [8, 16], yet their training is impeded by the simultaneous challenges of learning robust semantic features and performing generative denoising [49]. This coupling creates an inherently complex optimization landscape that demands prolonged training periods. In this section, we present Embedded Representation Warmup (ERW), framework that strategically decouples these challenges into two distinct phases. In Phase 1, we initialize the early layers of the diffusion model with highquality semantic features from pretrained models; in Phase 2, we transition to standard diffusion training with gradually diminishing representation alignment term, allowing the model to increasingly focus on generation. This separation addresses the inefficiency of joint optimization in conventional diffusion models and significantly enhances training efficiency. 4.1. Phase 1: Representation Warmup Stage To alleviate the burden of learning semantic features from scratch, we begin with dedicated warmup stage. During this phase, the models L2R circuit is initialized to align with semantically rich features extracted from pretrained representation model (e.g., DINOv2, MAE, or CLIP). Let Hθ(x) denote an encoder that maps an image to its latent representation Z, and let frep : be high-quality pretrained representation model. Our objective is to learn mapping RθL2R such that its output closely aligns with frep(x). Specifically, we define the alignment loss as: Lalign = Expdata(x) (cid:104)(cid:13) (cid:13)RθL2R(Hθ(x)) frep(x)(cid:13) 2 (cid:13) 2 (cid:105) (13) 5 By minimizing Eq. (13), we ensure that RθL2R produces features well-aligned with the pretrained representation model. This alignment mitigates the need for the diffusion model to learn complex semantic features purely from noisy inputs; instead, subsequent layers can operate on features that are already semantically meaningful. 4.2. Phase 2: Generative Training with Decaying"
        },
        {
            "title": "Representation Guidance",
            "content": "After the warmup stage has effectively initialized the diffusion model with semantically rich features, we proceed with joint objective that combines the standard diffusion loss with gradually diminishing representation alignment term. Formally, the overall training loss is given by: Ltotal = Ldiffusion + λ(t) Lalign (14) Here, Ldiffusion denotes the conventional diffusion loss (e.g., the velocity prediction loss as defined in Eq. (5)), Lalign is the representation alignment loss, and λ(t) is time-dependent weight that modulates the impact of alignment during training. In this phase, the alignment loss Lalign encourages the diffusion models hidden representations, after being projected by trainable function Tθ, to maintain alignment with the high-quality semantic features of the external encoder: Lalign = Et,x (cid:34) (cid:13) (cid:13) (cid:13)Tθ (cid:16) RθL2R(Hθ(x)) (cid:17) frep(x) (cid:13) 2 (cid:13) (cid:13) (cid:35) (15) The time-dependent alignment weight follows an exponential decay schedule: refining its generative performance rather than correcting representation discrepancies. To formalize this intuition, consider the gradient of the total loss with respect to the model parameters θ: θLtotal = θLdiffusion + λ(t) θLalign (17) Since the alignment loss has been minimized during warmup, its gradient θLalign is small. Moreover, the alignment weight λ(t) decays over time (as specified in Eq. (16)), further reducing the impact of the second term. Consequently, parameter updates are predominantly driven by the diffusion loss: θLtotal θLdiffusion (18) This separation means the model no longer needs to allocate capacity to learn semantic features from scratch during the main training phase. Instead, it can concentrate on perfecting its generative capabilities, leading to substantially faster convergence and improved sample quality. 5. Experiments In this section, we provide comprehensive evaluation of our proposed ERW approach. We begin by outlining experimental setups ( Section 5.1 ), including dataset and implementation details. Next, we present comparisons with state-ofthe-art baselines to demonstrate the benefits of ERW in both FID and training speed ( Section 5.2 ). We then analyze the role of our warmup procedure in boosting training efficiency ( Section 5.3 ). Finally, we conduct ablation studies to examine the effects of various alignment strategies, architecture depths, and target representation models ( Section 5.4 ). (cid:18) λ(t) = c0 exp (cid:19) τ (16) 5.1. Setup where c0 is the initial weight of the alignment loss and τ is hyperparameter controlling the decay rate. This schedule provides strong semantic guidance during early trainingwhen representation learning is most criticaland gradually reduces its influence, allowing the network to increasingly focus on optimizing generative capabilities. 4.3. Theoretical Analysis: Warmup Leads to Faster"
        },
        {
            "title": "Convergence",
            "content": "Our warmup phase minimizes the misalignment between the models internal representation RθL2R(Hθ(x)) and the target semantic features frep(x). After warmup, this misalignment becomes negligible for almost all data samples x. With the residual error minimized, the overall training objective during the diffusion stage is effectively dominated by the generative loss Ldiffusion. The alignment loss Lalign, scaled by the diminishing weight λ(t), contributes minimally to the total loss. This enables the model to focus on Implementation Details. We adhere closely to the experimental setups described in DiT [40] and SiT [36], unless otherwise noted. Specifically, we utilize the ImageNet dataset [7], preprocessing each image to resolution of 256 256 pixels. Following the protocols of ADM [8], each image is encoded into compressed latent vector R32324 using the Stable Diffusion VAE [44]. For our model configurations, we employ the B/2, L/2, and XL/2 architectures as introduced in the SiT papers, which process inputs with patch size of 2. To ensure fair comparison with SiT models, we maintain consistent batch size of 256 throughout training. Further experimental details, including hyperparameter settings and computational resources, are provided in Appendix . Evaluation. We report Fréchet inception distance (FID; Heusel et al. 15), sFID [37], inception score (IS; Salimans et al. 46), precision (Pre.) and recall (Rec.) [27] using 50K samples. We also include CKNNA [19] as discussed in 6 Table 1. System-level comparison on ImageNet 256256 with CFG. and indicate whether lower or higher values are better, respectively. Results that include additional CFG scheduling are marked with an asterisk (*), where the guidance interval from [28] is applied for ERW. Model Epochs FID sFID IS Pre. Rec. Pixel diffusion ADM-U VDM++ Simple diffusion CDM Latent diffusion, U-Net 400 560 800 2160 3.94 2.40 2.77 4.88 6.14 - - - 186.7 225.3 211.8 158. 0.82 - - - 0.52 - - - LDM-4 200 3.60 - 247.7 0.87 0.48 Latent diffusion, Transformer + U-Net hybrid U-ViT-H/2 DiffiT* MDTv2-XL/2* 240 - Latent diffusion, Transformer 1600 480 MaskDiT SD-DiT DiT-XL/2 SiT-XL/2 + REPA + ERW (ours) 1400 1400 200 2.29 1.73 1.58 2.28 3.23 2.27 2.06 1.96 1.94 5.68 - 4.52 5.67 - 4.60 4.50 4.49 7.91 263.9 276.5 314.7 276.6 - 278.2 270.3 264.0 300. 0.82 0.80 0.79 0.80 - 0.83 0.82 0.82 0.75 0.57 0.62 0.65 0.61 - 0.57 0.59 0.60 0.63 ablation studies. Detailed setups for evaluation metrics are provided in Appendix . Sampler and Alignment objective. Following SiT [36], we always use the SDE Euler-Maruyama sampler (for SDE with wt = σt) and set the number of function evaluations (NFE) as 250 by default. We use Normalized Temperaturescaled Cross Entropy (NT-Xent) training objective for alignment. Baselines. We use several recent diffusion-based generation methods as baselines, each employing different inputs and network architectures. Specifically, we consider the following four types of approaches: (a) Pixel diffusion: ADM [8], VDM++ [23], Simple diffusion [18], CDM [17], (b) Latent diffusion with U-Net: LDM [44], (c) Latent diffusion with transformer+U-Net hybrid models: U-ViT-H/2 [2], DiffiT [12], and MDTv2-XL/2 [11], and (d) Latent diffusion with transformers: MaskDiT [55], SD-DiT [56], DiT [40], and SiT [36]. Here, we refer to Transformer+U-Net hybrid models that contain skip connections, which are not originally used in pure transformer architecture. Detailed descriptions of each baseline method are provided in Appendix . 5.2. Comparison to Prior Methods Table 2. FID comparisons with SiT-XL/2. In this table, we report the FID of ERW with SiT-XL/2 on ImageNet 256 256 at various Training iterations. Here is only full training without warmup, because we load well trained warmuped checkpoint. For comparison, we also present the performance of the state-of-theart baseline REPA at similar iterations or comparable FID values. Note that indicates that lower values are preferred and all results reported are without classifier-free guidance. Model #Params Iter. FID sFID IS Prec. Rec. SiT-XL/2 +REPA +ERW (ours) +REPA +ERW (ours) 8.3 7M 675M 52.3 50K 675M 8.5 675M 50K 675M 100K 19.4 6.0 675M 100K 6.32 31.24 9.59 6.06 6.37 131.7 24.3 154.7 67.4 207. 0.68 0.45 0.72 0.64 0.67 0.67 0.53 0.51 0.61 0.63 Figure 4. Comparison of Training Efficiency and Cost Analysis with Warmup and Full Training Stages. Left: Scatter plot depicting the relationship between the total training cost (in TFLOPs) and the FID score for various training strategies. Each point is annotated with simplified label (e.g., 10K+90K) representing the warmup and full training split, and the marker sizes are scaled based on combination of the FID and FLOPs values to highlight the relative differences. Right: Bar chart comparing the computational costs of the warmup and full training stages for different strategies (all evaluated over 100K iterations). The chart shows the warmup cost, full training cost, and their corresponding total cost. ative training, thereby reducing the need for extended epochs. Figure 3 illustrates generated samples on ImageNet, further confirming the high-quality outputs achieved by ERW. 5.3. Effect of ERW: Quantitative Gains and Training Efficiency Performance without Classifier-Free Guidance. We begin by examining how ERW influences SiT-XL/2s Frechet Inception Distance (FID) when no classifier-free guidance (CFG) is used. Table 1 summarizes our results on ImageNet 256 256 under classifier-free guidance (CFG). Our ERW significantly boosts the convergence speed of SiT-XL/2, enabling strong FID scores at just 40 epochs. Importantly, we observe that ERW provides an alternative path to large-scale diffusion modeling by separating representational learning from gener- (a) Efficient FID Improvements. In Table 2 , ERW consistently achieves competitive or superior FID values compared to baselines. For instance, ERW reaches an FID of 6.0 at 100k iterations, markedly outperforming the REPA method [54] within the same iteration budget. (b) Leveraging Pretrained Features. This gain highlights 7 Table 3. Analysis of ERW on ImageNet 256256. All models are SiT-B/2 trained for 50K iterations. All metrics except FID without classifier-free guidance. For Acc., we report linear probing results on the ImageNet validation set using the latent features aligned with the target representation. We fix λ = 0.5 here. and indicate whether lower or higher values are better, respectively. Target Repr. Depth Objective FID sFID IS Prec. Rec. MoCov3-B MoCov3-L CLIP-L DINOv2-B DINOv2-L DINOv2-g 8 8 8 8 8 NT-Xent NT-Xent NT-Xent NT-Xent NT-Xent NT-Xent 61.1 73.0 58.9 55.6 55.5 59.4 7.6 8.0 7.7 7.8 7.8 7. 22.38 17.96 23.68 25.45 25.45 25.53 0.42 0.38 0.44 0.44 0.44 0.44 0.58 0.52 0.54 0.56 0.56 0. Table 4. Analysis of ERW places influences in SiT-B/2. All models are based on SiT-B/2 and trained for 50K iterations under the batch size of 256 without using classifier-free guidance on ImageNet 256 256. indicates lower values are better. Results empirically validate our hypothesis that placing ERW at the forefront of the architecture yields optimal performance. Target Repr. Depth Objective FID sFID IS Prec. Rec. SiT-B/2 + REPA [54] DINOv2-B DINOv2-B DINOv2-B DINOv2-B DINOv2-B 0-8 1-9 2-10 3-11 4NT-Xent NT-Xent NT-Xent NT-Xent NT-Xent 78.2 54.2 69.1 67.7 67.5 67.8 11.71 8.12 13.0 13.4 11.8 13.1 17. 27.2 18.7 19.0 19.5 19.0 0.33 0.45 0.37 0.38 0.38 0.38 0.48 0.59 0.51 0.52 0.52 0.52 the advantage of injecting pretrained semantic priors via warmup, thereby accelerating convergence in the subsequent full diffusion training. Summary. By incorporating short warmup step, ERW enables the model to focus its capacity on generative decoding rather than learning semantic features from scratch, resulting in improved FID at fewer training steps. Warmup Versus Full Training. Next, we analyze how splitting the total training budget between warmup and full diffusion training impacts both generation quality and computational overhead ( Figure 4 ). (a) Training Splits. We experiment with various allocations: 10k warmup + 90k full training, 20k warmup + 80k full training, and so on, always keeping the total at 100k iterations. (b) Accelerated Convergence. As shown in Figure 4 , 10k warmup already yields notable FID improvement (down to 6.4) with only slight increase in total FLOPs ( Figure 4 ). This underscores that early alignment with strong pretrained encoder can significantly hasten the later diffusion training. Summary. dedicated, relatively brief warmup phase materializes substantial downstream speedups and quality gains, confirming that investing in warmup alignment is costeffective strategy. Table 5. Analysis of ERW depth, projection depth, and different dynamic or consistent projection loss λ influences in SiT-XL/2. All models are based on SiT-XL/2 and trained for 50K iterations under batch size of 256 without using classifier-free guidance on ImageNet 256 256. The target representation model is DINOv2B, and the objective is NT-Xent. indicates lower values are better. The results show that projection depth of 14 and projection loss λ of 4.0 yield substantial improvements in both FID and sFID, indicating an optimal configuration for model performance. ERW Depth Proj. Depth λ FID sFID SiT-XL/2 + REPA [54] 3 4 5 6 8 12 4 4 4 4 4 4 4 4 4 4 4 8 8 8 8 8 8 8 10 12 14 16 14 14 14 14 14 52.3 13.3 11.7 13.1 13.7 15.6 19.2 13.7 12.8 12.3 12.4 12.6 21.6 12.4 10.5 9.0 8.6 8.8 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0. 0.1 0.5 1.0 2.0 4.0 6.0 31.24 13.65 17.93 15.25 16.62 16.42 16.91 17.93 17.25 17.03 16.97 16.79 19.58 16.97 15.07 11.17 9.84 9.46 IS 24.3 113.2 114.5 117.0 115.7 104.0 87.8 114.5 121.2 123.7 124.4 124.2 74.9 124.3 139.9 151.2 153.6 151.1 Prec. Rec. 0.45 0.69 0.68 0.69 0.68 0.67 0.65 0.68 0.68 0.69 0.68 0.67 0.62 0.68 0.70 0.72 0.72 0.71 0.53 0.48 0.48 0.48 0.49 0.46 0. 0.48 0.48 0.48 0.48 0.49 0.49 0.48 0.48 0.51 0.51 0.52 5.4. Ablation Studies We further dissect the effectiveness of ERW by conducting ablation studies on various design choices and parameter settings. Target Representation. We first compare alignment with multiple self-supervised encoders: MoCov3, CLIP, and DINOv2, as summarized in Table 3 . (a) Universality of Pretrained Encoders. All encoders tested offer improvements over baselines, indicating that ERW can broadly benefit from variety of strong representation models. (b) Marginal Differences among DINOv2 Variants. DINOv2-B, DINOv2-L, and DINOv2-g yield comparable gains, suggesting that ERW does not require the largest possible teacher encoder for effective representation transfer. Summary. Regardless of which high-quality self-supervised model is used, early-stage alignment provides consistent boost in sample fidelity and training efficiency. Placement of ERW Depth. We hypothesize that early layers in the diffusion backbone primarily learn semantic features (the L2R circuit), whereas deeper layers specialize in generative decoding. (a) Empirical Validation. In Table 4 , initializing the earliest layers (08) notably outperforms re-initializing middle or late sections (FID 54.2 vs. > 67). 8 (b) Consistent with Circuit Perspective. This corroborates our three-stage diffusion circuit ( Section 3 ), underscoring that aligning deeper layers for representation can be suboptimal since those layers focus more on generative refinement. Summary. Targeting the front portion of the network for warmup is crucial, reinforcing our theoretical claim that representation learning predominantly resides in early layers. Projection Depth and Alignment Weight. We also investigate how the final projection head depth and the alignmentloss coefficient λ affect training ( Table 5 ). (a) Optimal Hyperparameters. Using 4 warmup layers, projection head at depth 14, and λ = 4.0 achieves an FID of 8.6 at 50k iterationsa substantial gain over baselines. (b) Trade-off in λ. Larger λ offers stronger representation alignment initially but may disrupt convergence if pushed to extremes, highlighting the need for moderate scheduling. Summary. Properly balancing projection depth and alignment strength is key to maximizing ERW benefits in practice. Representation Dynamics. We examine the temporal progression of representation alignment in Figure 5 . (a) Initial Dip, Subsequent Recovery. Alignment falls early on as the pretrained features adjust to the diffusion objectives, but it then recovers and improves. (b) Role of Decaying Guidance. decaying weight in the alignment term ( Section 4.2 ) fosters stable synergy between semantic alignment and generative refinement. Summary. Representation alignment follows U-shaped trajectory, revealing the models gradual reconfiguration of pretrained features for denoising tasks before distilling them into robust, generative-friendly embeddings. CKNNA Analysis. Finally, we measure layer-wise representation quality using Class-conditional k-Nearest Neighbor Accuracy (CKNNA) [3], which indicates how well the hidden features capture class discriminability. (a) Improved Semantic Alignment. Compared to random initialization, ERW yields systematically higher CKNNA scores, confirming stronger semantic preservation. (b) Evolving Layer-wise Semantics. The alignment initially drops then recovers, mirroring the trends seen in Figure 5 and supporting the conclusion that pretrained features are effectively integrated rather than merely overwritten. Summary. CKNNA measurements reinforce our findings that ERW preserves and refines semantic information across layers, enabling more discriminative feature representations to emerge during diffusion training. 5.5. Hyperparameter Tuning We adopt bisection-style search to determine the key hyperparameters for ERW, specifically the ERW Depth (i.e., 9 (a) Alignment with ERW (b) Training Dynamic for Alignment Figure 5. Scalability of ERW. (a) Alignment with ERW plot showing that the representation models alignment with the ERW model exhibits high degree of alignment across all ERW depths. (b) Training dynamics for alignment indicate that within the 500K training steps for SiT-XL/2, the alignment between DINOv2-g and the diffusion model first decreases and then increases. which early layers to initialize), the Projection Depth, and the initial value of λ in Eq. (14). To keep the search computationally manageable, we do the following for each candidate hyperparameter setting: We run short warmup stage for 10k iterations, followed by 20k iterations of main diffusion training. To evaluate performance quickly, we reduce the sampling steps from the usual 250 to 50 and generate only 10k samples (instead of 50k) to compute preliminary FID score. This procedure substantially reduces the search cost while retaining sufficient fidelity to guide hyperparameter choices. In practice, around three to five such tests suffice to converge upon near-optimal settings for ERW Depth, Projection Depth, and λ, enabling both efficient training and highquality generation. 6. Conclusion and Future Work In this work, we introduced Embedded Representation Initialization (ERW), novel method that significantly enhances the training efficiency and representation quality of diffusion models by embedding high-quality representations from pretrained self-supervised encoders into the models representation processing region. Through both theoretical analysis and empirical validation, we demonstrated that ERW accelerates training convergence by up to 40 and improves the fidelity of generated data, establishing new stateof-the-art performance based on REPA [54]. Our findings underscore the importance of make representation embedded in initialization in optimizing diffusion model training."
        },
        {
            "title": "References",
            "content": "[1] Michael S. Albergo, Nicholas M. Boffi, and Eric VandenEijnden. Stochastic interpolants: unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023. 3 [2] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: ViT backbone for diffusion models. In IEEE Conference on Computer Vision and Pattern Recognition, 2023. 7, 17 [3] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In IEEE International Conference on Computer Vision, 2021. 2, 9 [4] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning In International Conference on of visual representations. Machine Learning, 2020. 2, 3 [5] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020. 14 [6] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. In IEEE International Conference on Computer Vision, 2021. 14 [7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: large-scale hierarchical image database. In IEEE Conference on Computer Vision and Pattern Recognition, 2009. 6, 14 [8] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat GANs on image synthesis. In Advances in Neural Information Processing Systems, 2021. 1, 4, 5, 6, 7, 16 [9] Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoidweighted linear units for neural network function approximation in reinforcement learning. Neural networks, 107: 311, 2018. [10] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In International Conference on Machine Learning, 2024. 3 [11] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. MDTv2: Masked diffusion transformer is strong image synthesizer. arXiv preprint arXiv:2303.14389, 2023. 7, 17 [12] Ali Hatamizadeh, Jiaming Song, Guilin Liu, Jan Kautz, and Arash Vahdat. DiffiT: Diffusion vision transformers for image generation. In European Conference on Computer Vision, 2024. 7, 17 [13] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In IEEE Conference on Computer Vision and Pattern Recognition, 2020. 14 [14] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. In IEEE Conference on Computer Vision and Pattern Recognition, 2022. 2, 3, 17 [15] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by two time-scale update rule converge to local nash equilibrium. In Advances in Neural Information Processing Systems, 2017. 6, 16 [16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 1, 3, 4, 5 [17] Jonathan Ho, Chitwan Saharia, William Chan, David Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. Journal of Machine Learning Research, 23(47):133, 2022. 7, 16 [18] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. Simple diffusion: End-to-end diffusion for high resolution images. In International Conference on Machine Learning, 2023. 7, 16 [19] Minyoung Huh, Brian Cheung, Tongzhou Wang, and Phillip Isola. The platonic representation hypothesis. In International Conference on Machine Learning, 2024. 6, 14 [20] Zahra Kadkhodaie, Florentin Guth, Eero Simoncelli, and Stéphane Mallat. Generalization in diffusion models arises from geometry-adaptive harmonic representations. In The Twelfth International Conference on Learning Representations, 2024. [21] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for improved quality, stability, and variation. In International Conference on Learning Representations, 2018. 16 [22] Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine. Analyzing and improving the training dynamics of diffusion models. In IEEE Conference on Computer Vision and Pattern Recognition, 2024. 15 [23] Diederik Kingma and Ruiqi Gao. Understanding diffusion objectives as the ELBO with simple data augmentation. Advances in Neural Information Processing Systems, 2024. 7, 16 [24] Diederik Kingma. Adam: method for stochastic optimization. In International Conference on Learning Representations, 2015. 15 [25] Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural network representations revisited. In International Conference on Machine Learning, 2019. 14 [26] Nupur Kumari, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Ensembling off-the-shelf models for GAN training. In IEEE Conference on Computer Vision and Pattern Recognition, 2022. 3 [27] Tuomas Kynkäänniemi, Tero Karras, Samuli Laine, Jaakko Improved precision and recall Lehtinen, and Timo Aila. metric for assessing generative models. In Advances in Neural Information Processing Systems, 2019. 6, [28] Tuomas Kynkäänniemi, Miika Aittala, Tero Karras, Samuli Laine, Timo Aila, and Jaakko Lehtinen. Applying guidance in limited interval improves sample and distribution quality in diffusion models. arXiv preprint arXiv:2404.07724, 2024. 7 [29] Alexander Li, Mihir Prabhudesai, Shivam Duggal, Ellis Brown, and Deepak Pathak. Your diffusion model is secretly zero-shot classifier. In IEEE International Conference on Computer Vision, 2023. 2 10 [30] Daiqing Li, Huan Ling, Amlan Kar, David Acuna, Seung Wook Kim, Karsten Kreis, Antonio Torralba, and Sanja Fidler. DreamTeacher: Pretraining image backbones with deep generative models. In IEEE International Conference on Computer Vision, 2023. 3 [31] Tianhong Li, Huiwen Chang, Shlok Mishra, Han Zhang, Dina Katabi, and Dilip Krishnan. MAGE: Masked generative encoder to unify representation learning and image synthesis. In IEEE Conference on Computer Vision and Pattern Recognition, 2023. 3 [32] Li, Katabi, and He. Return of unconditional generation: self-supervised representation generation method. In Advances in Neural Information Processing Systems, 2024. 2, 3, [33] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 3 [34] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In International Conference on Learning Representations, 2023. 3 [35] Loshchilov. Decoupled weight decay regularization. In International Conference on Learning Representations, 2017. 15 [36] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In European Conference on Computer Vision (ECCV), 2024. 6, 7, 15, 17 [37] Charlie Nash, Jacob Menick, Sander Dieleman, and Peter Battaglia. Generating images with sparse representations. In International Conference on Machine Learning, 2021. 6, 16 [38] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 1, 2 [39] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, Mido Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision. Transactions on Machine Learning Research, 2024. [40] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. 6, 7, 15, 17 [41] Pablo Pernias, Dominic Rampas, Mats Leon Richter, Christopher Pal, and Marc Aubreville. Würstchen: An efficient architecture for large-scale text-to-image diffusion models. In International Conference on Learning Representations, 2024. 2, 3 [42] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, 2021. 14 [43] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2): 3, 2022. 1 [44] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In IEEE Conference on Computer Vision and Pattern Recognition, 2022. 6, 7, 15, 16 [45] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. In Advances in Neural Information Processing Systems, 2022. 1, [46] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training GANs. In Advances in Neural Information Processing Systems, 2016. 6, 16 [47] Axel Sauer, Kashyap Chitta, Jens Müller, and Andreas Geiger. Projected GANs converge faster. Advances in Neural Information Processing Systems, 2021. 3 [48] Yuyang Shi, Valentin De Bortoli, Andrew Campbell, and Arnaud Doucet. Diffusion schrödinger bridge matching. Advances in Neural Information Processing Systems, 36, 2024. 3 [49] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 1, 4, 5 [50] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 15, 16 [51] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the Inception architecture for computer vision. In IEEE Conference on Computer Vision and Pattern Recognition, 2016. [52] Weilai Xiang, Hongyu Yang, Di Huang, and Yunhong Wang. Denoising diffusion autoencoders are unified self-supervised In IEEE International Conference on Computer learners. Vision, 2023. 2, 3, 4, 5 [53] Xingyi Yang and Xinchao Wang. Diffusion model as repIn IEEE International Conference on resentation learner. Computer Vision, 2023. 2, 3 [54] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. arXiv preprint arXiv:2410.06940, 2024. 1, 2, 4, 5, 7, 8, 9, 16 [55] Hongkai Zheng, Weili Nie, Arash Vahdat, and Anima Anandkumar. Fast training of diffusion models with masked transformers. Transactions on Machine Learning Research, 2024. 3, 7, 17 [56] Rui Zhu, Yingwei Pan, Yehao Li, Ting Yao, Zhenglong Sun, Tao Mei, and Chang Wen Chen. SD-DiT: Unleashing the power of self-supervised discrimination in diffusion transformer. In IEEE Conference on Computer Vision and Pattern Recognition, 2024. 7, 17 12 A. Loss Function Decomposition Proof This section presents theoretical decomposition of the diffusion models loss function into two orthogonal learning mechanisms: representation learning through latent-to-representation (L2R) mapping and generation inversion through representationto-generation (R2G) dynamics. The decomposition reveals fundamental statistical independence between representation quality and generation fidelity in diffusion processes. Let the diffusion models score matching objective be defined as: Ldiff = Et,zt (cid:2)λ(t)Fθ(zt, t) zt log p(zt)2(cid:3) , (19) where Fθ(zt, t) parameterizes the estimated score function, and λ(t) is time-dependent weighting function. Our analysis hinges on the existence of sufficient representation rt = R(zt) that statistically decouples the latent space geometry from generation dynamics. Formally, we assume: Assumption 1 (Sufficient Representation) . The representation rt = R(zt) is sufficient statistic for zt with respect to the data distribution p(zt), implying that rt captures all necessary information about zt for generation. This sufficiency leads to the decomposition of the Fisher score: zt log p(zt) = zt log p(zt rt) + zt log p(rt), (20) where the generation score zt log p(zt rt) and the representation score zt log p(rt) are orthogonal under the FisherRao metric: Eztrt [zt log p(zt rt), zt log p(rt)] = 0. (21) This orthogonality reflects the statistical independence between the generation mechanism and the representation learning process, ensuring that the two components can be optimized independently. Under this assumption, the ground truth score decomposes into generation and representation components: zt log p(zt) = zt log p(zt rt) (cid:125) (cid:124) (cid:123)(cid:122) Generation Score + zt log p(rt) (cid:125) (cid:123)(cid:122) (cid:124) Representation Score . (22) Parametrizing the models score function as Fθ(zt, t) = Gθ(rt)+Rθ(zt), where Gθ and Rθ independently model generation and representation scores, we expand the loss function: Ldiff = Et,zt = Et,zt (cid:2)λ(t)(Gθ(rt) + Rθ(zt)) ( log p(zt rt) + log p(rt))2(cid:3) (cid:2)λ(t)Rθ(zt) log p(rt)2(cid:3) (cid:2)λ(t)Gθ(rt) log p(zt rt)2(cid:3) + Et,zt + 2Et,zt [λ(t)Rθ(zt) log p(rt), Gθ(rt) log p(zt rt)] . The cross-term vanishes due to the orthogonality condition in Assumption 1, as: This yields the decoupled learning objectives: Eztrt [ log p(zt rt), log p(rt)] = 0. Ldiff = Et,zt (cid:124) (cid:2)λ(t)Rθ(zt) log p(rt)2(cid:3) (cid:125) (cid:123)(cid:122) LL2R + Et,zt (cid:124) (cid:2)λ(t)Gθ(rt) log p(zt rt)2(cid:3) (cid:125) (cid:123)(cid:122) LR2G . (23) (24) (25) (26) Through the above decomposition, we effectively split the optimization objective of the diffusion model into two independent parts: Representation Learning Rθ(zt) and Generation Inversion Gθ(rt). 13 B. Analysis Details B.1. Evaluation details CKNNA (Centered Kernel Nearest-Neighbor Alignment) is relaxed version of the popular Centered Kernel Alignment (CKA; Kornblith et al. 25) that mitigates the strict definition of alignment. We generally follow the notations in the original paper for an explanation [19]. First, CKA have measured global similarities of the models by considering all possible data pairs: CKA(K, L) = HSIC(K, L) (cid:112)HSIC(K, K)HSIC(L, L) , (27) where and are two kernel matrices computed from the dataset using two different networks. Specifically, it is defined as Kij = κ(ϕi, ϕj) and Lij = κ(ψi, ψj) where ϕi, ϕj and ψi, ψj are representations computed from each network at the corresponding data xi, xj (respectively). By letting κ as inner product kernel, HSIC is defined as HSIC(K, L) = 1 (n 1) (cid:16) (cid:88) (cid:88) (cid:0)ϕi, ϕj El[ϕi, ϕl](cid:1)(cid:0)ψi, ψj El[ψi, ψl](cid:1)(cid:17) . (28) CKNNA considers relaxed version of Eq. (27) by replacing HSIC(K, L) into Align(K, L), where Align(K, L) computes Eq. (28) only using k-nearest neighborhood embedding in the datasets: Align(K, L) = 1 (n 1)2 (cid:16) (cid:88) (cid:88) α(i, j)(cid:0)ϕi, ϕj El[ϕi, ϕl](cid:1)(cid:0)ψi, ψj El[ψi, ψl](cid:1)(cid:17) , where α(i, j) is defined as α(i, j; k) = [i = and ϕj knn(ϕi; k) and ψj knn(ψi; k)], (29) (30) so this term only considers k-nearest neighbors at each i. In this paper, we randomly sample 10,000 images in the validation set in ImageNet [7] and report CKNNA with = 10 based on observation in Huh et al. [19] that smaller shows better better alignment. B.2. Description of pretrained visual encoders MoCov3 [6] studies empirical study to train MoCo [5, 13] on vision transformer and how they can be scaled up. CLIP [42] proposes contrastive learning scheme on large image-text pairs. DINOv2 [39] proposes self-supervised learning method that combines pixel-level and patch-level discriminative objectives by leveraging advanced self-supervised techniques and large pre-training dataset. C. Hyperparameter and More Implementation Details Table 6. Hyperparameter setup. Figure 1,2,3 Table 3,4 (SiT-B) Table 1,2,5 (SiT-XL) 32324 28 1,152 32324 12 768 12 NT-Xent DINOv2-B NT-Xent DINOv2-B 256 AdamW 0.0001 (0.9, 0.999) 256 AdamW 0.0001 (0.9, 0.999) 32324 24 1,152 NT-Xent DINOv2-B 256 AdamW 0.0001 (0.9, 0.999) 1 σt v-prediction Euler-Maruyama 250 - 1 σt v-prediction Euler-Maruyama 250 - 1 σt v-prediction Euler-Maruyama 250 - Architecture Input dim. Num. layers Hidden dim. Num. heads ERW sim(, ) Encoder (x) Optimization Batch size Optimizer lr (β1, β2) Interpolants αt σt wt Training objective Sampler Sampling steps Guidance Further implementation details. We implement our model based on the original SiT implementation [36]. Throughout the experiments, we use the exact same structure as DiT [40] and SiT [36]. We use AdamW [24, 35] with constant learning rate of 1e-4, (β1, β2) = (0.9, 0.999) without weight decay. To speed up training, we use mixed-precision (fp16) with gradient clipping. We also pre-compute compressed latent vectors from raw pixels via stable diffusion VAE [44] and use these latent vectors. Because of this, we do not apply any data augmentation, but we find this does not lead to big difference, as similarly observed in EDM2 [22]. We also use stabilityai/sd-vae-ft-ema decoder for decoding latent vectors to images. For MLP used for projection, we use three-layer MLP with SiLU activations [9]. We provide detailed hyperparameter setup in Table 6 . Pretrained encoders. For MoCov3-B and -L models, we use the checkpoint in the implementation of RCG [32];1 for other checkpoints, we use their official checkpoints released in their official implementations. To adjust different number of patches between the diffusion transformer and the pretrained encoder, we interpolate positional embeddings of pretrained encoders. Sampler. For sampling, we use the Euler-Maruyama sampler with the SDE with diffusion coefficient wt = σt. We use the last step of the SDE sampler as 0.04, and it gives significant improvement, similar to the original SiT paper [36]. Training Tricks. We explore the influence of various training techniques on ERWs performance. Notably, we observe performance improvements when incorporating Rotary Positional Embeddings [50]. Discussion of Quantifiable Metrics. rigorous quantitative analysis: Iteration Time (titer): The average time per training iteration (in seconds), which directly impacts Cw. Warmup Iteration Count (Nw): The precise number of iterations employed during the warmup phase. In addition to the cost breakdown above, the following metrics are instrumental for 1https://github.com/LTH14/rcg 15 Table 7. Impact of Training Tricks in ERW. Using the SD-VAE [44], ERW achieves an FID of 55.6 at 50K training steps on ImageNet class-conditional generation. This table illustrates how each training trick incrementally improves the FID, demonstrating that advanced design techniques enhance the original DiT performance."
        },
        {
            "title": "Training Trick",
            "content": "Training Step FID-50k"
        },
        {
            "title": "Representation Alignment Loss",
            "content": "+ REPA [54] 50K"
        },
        {
            "title": "Architecture Improvements",
            "content": "+ Rotary Pos Embed [50] 50K"
        },
        {
            "title": "Initialization",
            "content": "+ ERW (Ours) 50K 78.2 73.6 51.7 Main Training Iteration Count (Nm): The total number of iterations in the main training phase. Hyperparameter Search Steps (bi): The number of binary search steps needed for each hyperparameter. Evaluation Efficiency: The use of reduced evaluation protocol (e.g., FID measurement on 10K images using 50 sampling steps) accelerates the hyperparameter search process. Overall Speedup Factor: The ratio Cm/Cw is approximately 10, underscoring that the main training phase is the dominant cost. D. Evaluation Details We strictly follow the setup and use the same reference batches of ADM [8] for evaluation, following their official implementation.2 We use 8NVIDIA H800 80GB GPUs or for evaluation and enable tf32 precision for faster generation, and we find the performance difference is negligible to the original fp32 precision. In what follows, we explain the main concept of metrics that we used for the evaluation. FID [15] measures the feature distance between the distributions of real and generated images. It uses the Inception-v3 network [51] and computes distance based on an assumption that both feature distributions are multivariate gaussian distributions. sFID [37] proposes to compute FID with intermediate spatial features of the Inception-v3 network to capture the generated images spatial distribution. IS [46] also uses the Inception-v3 network but use logit for evaluation of the metric. Specifically, it measures KLdivergence between the original label distribution and the distribution of logits after the softmax normalization. Precision and recall [27] are based on their classic definitions: the fraction of realistic images and the fraction of training data manifold covered by generated data. E. Baselines In what follows, we explain the main idea of baseline methods that we used for the evaluation. ADM [8] improves U-Net-based architectures for diffusion models and proposes classifier-guided sampling to balance the quality and diversity tradeoff. VDM++ [23] proposes simple adaptive noise schedule for diffusion models to improve training efficiency. Simple diffusion [18] proposes diffusion model for high-resolution image generation by exploring various techniques to simplify noise schedule and architectures. CDM [17] introduces cascaded diffusion models: similar to progressiveGAN [21], it trains multiple diffusion models starting from the lowest resolution and applying one or more super-resolution diffusion models for generating high-fidelity images. LDM [44] proposes latent diffusion models by modeling image distribution in compressed latent space to improve the training efficiency without sacrificing the generation performance. 2https://github.com/openai/guided-diffusion/tree/main/evaluations U-ViT [2] proposes ViT-based latent diffusion model that incorporates U-Net-like long skip connections. DiffiT [12] proposes time-dependent multi-head self-attention mechanism for enhancing the efficiency of transformerbased image diffusion models. MDTv2 [11] proposes an asymmetric encoder-decoder scheme for efficient training of diffusion-based transformer. They also apply U-Net-like long-shortcuts in the encoder and dense input-shortcuts in the decoder. MaskDiT [55] proposes an asymmetric encoder-decoder scheme for efficient training of diffusion transformers, where they train the model with an auxiliary mask reconstruction task similar to MAE [14]. SD-DiT [56] extends MaskdiT architecture but incorporates self-supervised discrimination objective using momentum encoder. DiT [40] proposes pure transformer backbone for training diffusion models based on proposing AdaIN-zero modules. SiT [36] extensively analyzes how DiT training can be efficient by moving from discrete diffusion to continuous flow-based modeling."
        }
    ],
    "affiliations": [
        "Nanjing University",
        "Westlake University",
        "Zhejiang University"
    ]
}