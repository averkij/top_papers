{
    "paper_title": "Fantastic Pretraining Optimizers and Where to Find Them",
    "authors": [
        "Kaiyue Wen",
        "David Hall",
        "Tengyu Ma",
        "Percy Liang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "AdamW has long been the dominant optimizer in language model pretraining, despite numerous claims that alternative optimizers offer 1.4 to 2x speedup. We posit that two methodological shortcomings have obscured fair comparisons and hindered practical adoption: (i) unequal hyperparameter tuning and (ii) limited or misleading evaluation setups. To address these two issues, we conduct a systematic study of ten deep learning optimizers across four model scales (0.1B-1.2B parameters) and data-to-model ratios (1-8x the Chinchilla optimum). We find that fair and informative comparisons require rigorous hyperparameter tuning and evaluations across a range of model scales and data-to-model ratios, performed at the end of training. First, optimal hyperparameters for one optimizer may be suboptimal for another, making blind hyperparameter transfer unfair. Second, the actual speedup of many proposed optimizers over well-tuned baselines is lower than claimed and decreases with model size to only 1.1x for 1.2B parameter models. Thirdly, comparing intermediate checkpoints before reaching the target training budgets can be misleading, as rankings between two optimizers can flip during training due to learning rate decay. Through our thorough investigation, we find that all the fastest optimizers such as Muon and Soap, use matrices as preconditioners -- multiplying gradients with matrices rather than entry-wise scalars. However, the speedup of matrix-based optimizers is inversely proportional to model scale, decreasing from 1.4x over AdamW for 0.1B parameter models to merely 1.1x for 1.2B parameter models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 6 4 0 2 0 . 9 0 5 2 : r a"
        },
        {
            "title": "Fantastic Pretraining Optimizers and\nWhere to Find Them",
            "content": "Kaiyue Wen Stanford University kaiyuew@stanford.edu David Hall Stanford University dlwh@cs.stanford.edu Tengyu Ma Stanford University tengyuma@stanford.edu Percy Liang Stanford University pliang@cs.stanford.edu September 3, 2025 Abstract AdamW has long been the dominant optimizer in language model pretraining, despite numerous claims that alternative optimizers offer 1.4 to 2 speedup. We posit that two methodological shortcomings have obscured fair comparisons and hindered practical adoption: (i) unequal hyperparameter tuning and (ii) limited or misleading evaluation setups. To address these two issues, we conduct systematic study of ten deep learning optimizers across four model scales (0.1B-1.2B parameters) and data-to-model ratios (18 the Chinchilla optimum). We find that fair and informative comparisons require rigorous hyperparameter tuning and evaluations across range of model scales and data-to-model ratios, performed at the end of training. First, optimal hyperparameters for one optimizer may be suboptimal for another, making blind hyperparameter transfer unfair. Second, the actual speedup of many proposed optimizers over well-tuned baselines is lower than claimed and decreases with model size to only 1.1 for 1.2B parameter models. Thirdly, comparing intermediate checkpoints before reaching the target training budgets can be misleading, as rankings between two optimizers can flip during training due to learning rate decay. Through our thorough investigation, we find that all the fastest optimizers such as Muon and Soap, use matrices as preconditioners multiplying gradients with matrices rather than entry-wise scalars. However, the speedup of matrix-based optimizers is inversely proportional to model scale, decreasing from 1.4 over AdamW for 0.1B parameter models to merely 1.1 for 1.2B parameter models."
        },
        {
            "title": "1 Introduction",
            "content": "Pretraining has been the most computationally expensive component in the training pipeline for large language models, accounting for over 95% of the cost in DeepSeek V3 DeepSeek-AI et al. [2025b], and the additional RL training cost in DeepSeek R1 DeepSeek-AI et al. [2025a] is also comparatively much smaller. Until recently, AdamW has been the standard optimizer. Recent studies have introduced novel optimizers that claim to accelerate pretraining by 1.4 to 2 compared to AdamW Liu et al. [2024a], Vyas et al. [2025], Liu et al. [2025a], Yuan et al. [2025], Liang et al. [2025], Wang et al. [2025], Liu et al. [2025c], Pethick et al. [2025], Ma et al. [2025], yet these optimizers have not yet been widely adopted in real-world pretraining DeepSeek-AI et al. [2024], Yang et al. [2025], Grattafiori et al. [2024], with the notable exception of Kimi K2 Team et al. [2025], which uses the Muon-clip optimizer (a variant of Muon Jordan et al. [2024]). We pinpoint two problems in optimizer evaluation in the evaluation process of these optimizers that both undermine confidence in new methods and limit their practical adoption. First, some baselines suffer from improperly tuned hyperparameters. Second, many experiments are confined to smaller-scale settings, leaving unanswered questions about how these optimizers perform in broader, more realistic scenarios. 1 Figure 1: Top Left: The commonly used AdamW baseline for optimizer design is under-tuned. Up to 2 speedup is achievable by tuning single hyperparameter (learning rate) in the GPT-3 recipe Brown et al. [2020] for 100M model (adopted in Liu et al. [2024a], Wen et al. [2024], Yuan et al. [2025], Liang et al. [2025], Wang et al. [2025]), highlighting the importance of proper hyperparameter optimization. Top Right: Fixing hyperparameters across optimizers does not guarantee fair comparison. Shared hyperparameters such as learning rate and weight decay are commonly set to constant in previous studies. However, even conceptually similar optimizers may correspond to very different optimal hyperparameters. Bottom Left: Speedup decays with model size. While some optimizers show high (1.3-1.4) speedup over AdamW on models under 1B parameters, the speedup decays with model size to only 1.1 for 1.2B parameters. Bottom Right: Matrix-based optimizers consistently outperform scalar-based optimizers. The loss curves for three scalar-based optimizers (AdamW, Nesterov AdamW, Mars) and three matrix-based optimizers (Kron, Soap, Muon) trained with different Chinchilla ratios of data are shown. Matrix-based optimizers achieve consistent speedup over scalar-based optimizers. Furthermore, the three matrix-based optimizers converge to similar loss in an overtrained setting. To address these issues, we benchmark eleven optimizers, including AdamW, in rigorously controlled setup, focusing on two key questions: 1. How to ensure hyperparameter optimality? Previous works typically rely on manual hyperparameter selection and keep common hyperparameters such as learning rate and weight decay fixed across optimizers (e.g. Liu et al. [2025a]). This tuning process may result in weak baseline, as the hyperparameters chosen may favor the proposed optimizers rather than for AdamW. We validate this concern by showing that tuning just one hyperparameter in the widely adopted GPT-3 recipe (introduced in Brown et al. [2020] and used in Liu et al. [2024a], Wen et al. [2024], Yuan et al. [2025], Liu et al. [2025d,c], Liang et al. [2025], Wang et al. [2025]) can yield 2 speedup for pretraining (Figure 1, top left). To address, this we perform coordinate descent in the hyperparameter space on hyperparameters for all eleven optimizers, iterating until convergence across for each of six different settings for models with up to 0.5B parameters to ensure that the hyperparameters are near-optimal for each setting. 2 2. How do speedups differ across different scaling regimes? Large language models are trained in different regimes: depending on the model size, the data-to-model ratio (the number of tokens over the number of parameters) can range from 1 to more than 50 times the Chinchilla optimal (about 20 based on Hoffmann et al. [2022]). Typical optimizer experiments in 1 Chinchilla optimal regimes raise concerns about new optimizers effectiveness in high data-to-model ratio. To address this, we benchmark the optimizers in four distinct data-to-model ratios (1, 2, 4 and 8 the Chinchilla optimal regime) and scale up to 1.2B parameter models (following Everett et al. [2024], Zhang et al. [2025a]). Concretely, we employ the Llama 2 architecture Touvron et al. [2023b], Grattafiori et al. [2024] (ranging from 0.1B to 1.2B parameters) and data mixture similar to OLMo 2s OLMo et al. [2025]. We focus on the final validation loss on the C4-EN mixture as known proxy for downstream performance Bhagia et al. [2024], while also tracking exact downstream performance on various benchmarks. As previous works Vyas et al. [2025], Liu et al. [2025a] show that the step-wise computation overhead of matrix-based optimizers can be reduced to under 10% through proper implementation, we primarily compare algorithms by the number of tokens needed to reach given loss. Our empirical results show the necessity of careful hyperparameter tuning and end-of-training evaluations across range of model scales and data-to-model ratios: wo 1. Hyperparameter transfer between optimizers is non-trivial. Even similar optimizers may need very different hyperparameters (e.g., Lions optimal weight decay 0.6 vs. AdamWs 0.1, Figure 1, top right), so fixing hyperparameters across optimizers can lead to unfair comparisons. 2. The speedup of new optimizers is lower than claimed and diminishes with model size. Many reported speedups of 2 simply reflect weak baseline. Against our well-tuned AdamW baseline, the speedup of alternative optimizers does not exceed 1.4(Figure 3). Furthermore, while new optimizers such as Muon and Soap show 1.3 speedups for small models (0.1B), the speedups diminish to around 1.1 for 1.2B parameter models at 8 Chinchilla Ratio (Figure 1, bottom left), regime that is not tested in previous works studying the scaling law of these optimizers 1. 3. Early-stage loss curves can mislead significantly. During learning rate decay, loss curves of different optimizers may cross multiple times (Figure 5), so judging optimizers using intermediate checkpoints may result in different ranking than comparing models at the target training budget. Our benchmarking also reveals new insights about optimizer design: 1. Matrix-based optimizers consistently outperform scalar-based optimizers for small models. Scalarbased optimizers (e.g., AdamW, Lion, Mars, etc.) update each parameter individually using scalar operations. After proper tuning, all scalar-based optimizers achieve similar optimization speeds to AdamW, with an average speedup ratio of less than 1.2. Matrix-based optimizers (e.g., Kron, Muon, Soap, etc.) leverage the inherent matrix structure of neural network parameters and precondition gradients using matrix multiplication. Despite their diverse update rules, matrix-based optimizers all deliver approximately 1.3 speedup over AdamW (Figure 1, bottom right) for models under 520M parameters. 2. Optimal choice of optimizer shifts depends on data-to-model ratios. winner in the 1 Chinchilla regime may be suboptimal when data-to-model ratio increases. For example, while Muon is consistently the best optimizer in smaller Chinchilla ratio regimes, it is outperformed by Kron and Soap when the data-to-model ratio increases to 8x or larger (Figures 3 and 4)."
        },
        {
            "title": "2 Related Works",
            "content": "Optimizers for Deep Learning. long line of work has studied optimization for deep learning, incorporating insights from classical optimization literature Robbins and Monro [1951], Nesterov [1983], Duchi et al. [2011] and domain knowledge about deep neural networks Sutskever et al. [2013]. (i) Early insights motivated the 1The original Soap paper Vyas et al. [2025] investigate model sizes up to 0.6B parameters. and Kimis paper on Muon Liu et al. [2025a] only considers 1 Chinchilla regime. 3 rise of optimizers that use adaptive learning rates based on second-order momentum Tieleman [2012], Zeiler [2012], Kingma and Ba [2017]. Adam Kingma and Ba [2017] later became the default baseline for optimizers with adaptive learning rates. Since then, improvements over Adam and SGD have been proposed, with notable examples including Nesterov Adam Dozat [2016] and AdamW with decoupled weight decay Loshchilov and Hutter [2019]. Other improvements include addressing the convergence of Adam on convex loss Reddi et al. [2018], Zaheer et al. [2018], Taniguchi et al. [2024], considering interpolation between Adam and SGD to improve generalization Luo et al. [2019], Xie et al. [2022], performing further variance reductions on optimizer updates Liu et al. [2021], Zhang et al. [2019], Yuan et al. [2025], Xie et al. [2024], Pagliardini et al. [2024], Zhuang et al. [2020], incorporating momentum on weights Ivgi et al. [2023], Defazio et al. [2024b], allowing easier hyperparameter tuning Defazio and Mishchenko [2023], Mishchenko and Defazio [2024], Defazio et al. [2024a], reducing memory usage by incorporating the structure of neural networks Shazeer and Stern [2018], Zhang et al. [2025b], Zhu et al. [2025], Luo et al. [2023], Modoranu et al. [2024], Zhao et al. [2024], and modifying the algorithm to allow larger batch sizes You et al. [2017, 2020]. (ii) Starting from Preconditioned SGD Li [2018a] and Shampoo Gupta et al. [2018], another line of optimizer design Morwani et al. [2024], Eschenhagen et al. [2023], Martens and Grosse [2020], Li [2018b], Eschenhagen et al. [2025] began to incorporate matrix preconditioners instead of simple scalar preconditioners. Techniques including learning rate grafting Agarwal et al. [2020], blocking, and distributed methodology Anil et al. [2021] have since been proposed. These matrix-based approaches later led to the theory of modular duality in deep learning optimization Bernstein and Newhouse [2024a,b], Large et al. [2024] and new optimizers including Muon Jordan et al. [2024] and Scion Pethick et al. [2025]. (iii) Motivated by Newtons algorithm, there has been line of work that tries to incorporate Hessian information Becker and Cun [1989], Yao et al. [2021], Schaul et al. [2013], Yao et al. [2021]. (iv) Symbolic discovery of optimizers Chen et al. [2023] has discovered memory-efficient SignGD Bernstein et al. [2018] variant called Lion, which claims to outperform Adam on wide range of tasks. Optimization for Pretraining. Since Brown et al. [2020], the cost of pretraining has increased dramatically. One of the challenges is how to choose hyperparameters with minimal cost. pivotal line of work in this direction is the tensor program series that allows for extrapolating some hyperparameters across scales Yang [2020b,c,a, 2021], Yang and Littwin [2023], Yang et al. [2022, 2024]. Empirical results suggest that fitting power law to scale hyperparameters is now common practice for large models Li et al. [2025a], Everett et al. [2024], Liu et al. [2025a], DeepSeek-AI et al. [2024], Zhang et al. [2025a]. We have also incorporated this approach in our paper. Another popular line of research is designing better optimizers specifically for pretraining. These optimizers are our main objects of study. An (incomplete) list of optimizers and their claimed speedups over AdamW includes Sophia Liu et al. [2024a] (2), Soap Vyas et al. [2025] (1.4), Muon Jordan et al. [2024], Liu et al. [2025a] (2), MARS Yuan et al. [2025] (2), Cautious AdamW Liang et al. [2025] (2), Block-wise Learning Rate Adam Wang et al. [2025] (2), FOCUS Liu et al. [2025c] (2), SWAN Ma et al. [2025] (2), DION Ahn et al. [2025] (3), and SPlus Frans et al. [2025] (2). We present comparison of setups with these prior works in Appendix G. Re-evaluation Methodology. Our work is rigorous evaluation of optimizers for pretraining. Rigorous evaluation has been an important part of deep learning research to clarify the current status of research and move the community forward. Jiang et al. [2019] critically examines metrics for predicting LLMs generalization capability and has facilitated research on understanding loss landscape sharpness and generalization and new optimizers such as SAM Foret et al. [2021]. Schmidt et al. [2021] re-evaluated optimizers at that time and showed that (i) which optimizer is optimal is problem-specific, and (ii) rigorous hyperparameter tuning is required and unequal tuning can account for most of the claimed speedup. Unlike Schmidt et al. [2021], we evaluate the LLM pretraining task and include modern optimizers they did not test. However, we arrive at similar conclusion: rigorous and fair hyperparameter tuning is still not the norm, but rather the exception in optimizer design research. Zhao et al. [2025] also examines how different optimizers perform on the pretraining tasks. However, the focus of Zhao et al. [2025] is on understanding loss structure, and the optimizers tested in the paper are shown to have slower convergence compared to AdamW, whereas the optimizers tested in this paper show small but significant improvements in convergence speed. The Algoperf competition Kasimbeg et al. [2025] evaluates different optimizers across different settings and arrives at similar conclusion that (i) matrix-based optimizers and (ii) denoising methods such as Nesterov momentum lead to speedup over AdamW. Our works focus on the pretraining setting and investigates the effect of scaling dataset and model sizes."
        },
        {
            "title": "3 Methodology",
            "content": "In this section, we detail the experimental design and evaluation protocol that underpin our empirical investigation. In Section 3.1, we specify the general setup for all subsequent studies. We then describe our three-phase hyperparameter-tuning framework: Phase (in Section 3.2) performs fine-grained coordinatedescent sweeps across multiple model sizes and data-to-model ratios to identify scaling-sensitive parameters; Phase II (in Section 3.3) refines these sensitive parameters on mid-scale settings and selects the most promising optimizers; and Phase III (in Section 3.4) extrapolates hyperparameter scaling laws to the 1.2 billion-parameter regime. Together, these protocols ensure principled, fair, and reproducible comparisons across different optimizers. We present the optimal configurations found and how loss changes with respect to each hyperparameter in Appendix and hope that this can facilitate future research. We also open-source the code (https://github.com/marin-community/marin/tree/kaiyue/optimizers) and the corresponding WandB runs (https://wandb.ai/marin-community/optimizer-scaling)."
        },
        {
            "title": "3.1 General Experimental Setup",
            "content": "Following OLMo et al. [2025], we conduct all our experiments on large-scale pretraining corpus composed of three publicly available datasets, tokenized with the Llama3 tokenizer: DCLM-baseline (3.8 trillion tokens, Li et al. [2025b]), StarCoder V2 Data (0.25 trillion tokens, Lozhkov et al. [2024]), and ProofPile 2 (55 billion tokens, Azerbayev et al. [2024]). Optimizer References Algorithm Baseline AdamW Kingma and Ba [2017], Loshchilov and Hutter [2019] Algorithm 1 Variance-reduced AdamW Variants NadamW Mars Cautious Dozat [2016] Yuan et al. [2025] Liang et al. [2025], Wang et al. [2024] Memory-efficient Optimizers Lion Adam-mini Chen et al. [2023] Zhang et al. [2025b] Matrix-based Optimizers Jordan et al. [2024] Pethick et al. [2025] Muon Scion Kron (PSGD) Li [2018a, 2022] Vyas et al. [2025] Soap Hessian-Approximation Optimizers Sophia Liu et al. [2024a] Algorithm 2 Algorithm 5 Algorithm 7 Algorithm 3 Algorithm 6 Algorithm 8 Algorithm 9 Algorithm 10 Algorithm 11 Algorithm 4 Table 1: Optimizers under study Our benchmarks cover four model sizes derived from the architecture Touvron et al. [2023a,b], Grattafiori et al. [2024], with approximately 130M, 300M, 520M, and 1.2B parameters. Each variant uses fixed sequence length of 4,096 and 32 transformer layers (following Liu et al. [2024b]), differing only in hidden dimension, intermediate dimension, and number of attention heads. Detailed hyperparameters are summarized in Table 2. Training is implemented in JAX and executed on TPU v5 hardware. We employ mixed-precision scheme (parameters in fp32 and activations in bf16). For each model, we use 20 times its non-embedding parameter count to compute the Chinchilla optimal data-to-model ratio based on Hoffmann et al. [2022]. We will use Chinchilla to represent training the models for times the Chinchilla optimal number of tokens. 5 Model Params Seq Len Hidden Dim Inter Dim # Layers # Heads Llama-130M Llama-300M Llama-520M Llama-1.2B 130M 300M 520M 1.2B 4096 4096 4096 4096 512 768 1024 1536 2048 3072 4096 6144 32 32 32 8 12 16 24 Table 2: Detailed architecture hyperparameters for each model size we studied. Our primary evaluation metric for the model is the language modeling loss on the English split of the C4 dataset Raffel et al. [2023], which has been shown to be strong proxy for downstream performance Bhagia et al. [2024]. We also track downstream accuracy and bits-per-byte on the following suite of benchmarks: ARC (Easy and Challenge) Clark et al. [2018], BoolQ Clark et al. [2019], COPA Gordon et al. [2012], CommonsenseQA Talmor et al. [2018], HellaSwag Zellers et al. [2019], LAMBADA Paperno et al. [2016], OpenBookQA Mihaylov et al. [2018], PIQA Bisk et al. [2020], WSC273 Levesque et al. [2012], and Winogrande Sakaguchi et al. [2020]. Our study includes wide range of eleven optimizers listed in Table 1. Due to the page limit, we defer the exact algorithm descriptions of these optimizers to Appendix A. We selected these eleven optimizers according to three guiding principles: (i) include widely adopted baselines such as AdamW and Lion; (ii) cover recently proposed optimizers; and (iii) when multiple methods share similar update rule, choose few representative algorithms. These choices ensure both breadth and depth in our comparison. We group the optimizers under study into five general classes: 1. Our baseline algorithm is AdamW, with mt and vt being firstand second-order momentum of gradient. vt+ϵ η λ wt. AdamWs update rule is wt+1 = wt η mt 2. Reducing the variance of updates is shared motivation behind many optimizers. For example, Nesterov AdamW incorporates the Nesterov lookahead technique to estimate gradients more accurately, with the following update rule: wt+1 = wt η β1mt+(1β1)gt η λ wt. vt+ϵ 3. Optimizers such as Lion aim to reduce the memory required by AdamW by only keeping the first-order momentum. Lion is observed to perform better than AdamW at small scale Liang et al. [2025]: wt+1 = wt η sign(β2mt + (1 β2)gt), where mt is the first-order momentum. 4. Other optimizers like Muon leverage the matrix structure of neural networks and perform preconditioning of gradients through matrix multiplication instead of scalar multiplication. For Muon, the key operation is called Newton-Schulz: NS(M ) = (aM + bM + c(M pM )2). With appropriate a, b, c, one can prove that NS(5)(M ) arg maxOop=1 Tr(OM ) when op < 1. Muon has the following update rule: wt+1 = wt η NS(5)(β2 mt + (1 β2)gt). This is done for all the matrices except the token classification head and the embedding in the network. 5. Optimizers including Sophia are motivated by the famous Newtons method and use Hessian-vector product to approximate the diagonalized version of the Hessian matrix empirically."
        },
        {
            "title": "3.2 Phase I: Fine-grained Hyperparameter Coordinate Descent",
            "content": "Fixed or lightly tuned baselines can severely understate an optimizers capability and lead to overstated speedup claims. For instance, single tweak to the learning-rate schedule with peak learning rate 6e-4 in the GPT-3 recipe Brown et al. [2020] can produce nearly 2 speedup (Figure 1, left). To avoid such artifacts, we perform an exhaustive, one-at-a-time sweep over each hyperparameter, identify the set of near-best configurations in each regime, and then determine which knobs truly require re-tuning as scale changes. For each optimizer, we define discrete grid for every hyperparameter (e.g., for AdamW, our swept hyperparameters include learning rate, weight decay, warmup steps, β1, β2, ϵ, gradient-norm clipping, and batch size). Starting from initial hyperparameter configuration similar to the hyperparameter configuration 2We defer the result of Sophia to Appendix B.2. 6 provided in the original paper proposing the optimizer, in each iteration, we hold all but one hyperparameter fixed at the current best values, then search the whole grid for that parameter, and accept the new value if the validation loss improves by more than 1 = 3 103. We repeat passes until no parameter update yields further significant gain. We perform this sweeping for 6 different settings, namely 130M, 300M, 500M at 1 Chinchilla and 130M at 2, 4, 8 Chinchilla. One exemplary hyperparameter optimization procedure for AdamW on model with 300M parameters and 1 Chinchilla is shown in Table 3. Result of Phase I. By the end of Phase I, we have, for each optimizer and each of the six regimes (130M, 300M, 500M at 1; 130M at 2, 4, 8 Chinchilla), identified coordinate-wise local optimum of hyperparametersthat is, the single value which, when all other knobs are held fixed, minimizes validation loss."
        },
        {
            "title": "3.3 Phase II: Coordinate Descent on Scaling-Sensitive Hyperparameters",
            "content": "While the extensive coordinate descent guarantees coordinate-wise optimality, it is too costly to perform on larger-scale experiments. We empirically observe two crucial properties of the sweeping that allow us to simplify the descent procedure: 1. Losses are sensitive to only subset of hyperparameters; many have little effect on performance when perturbed from their optimal values. 2. Among the sensitive hyperparameters, the optimal settings for most remain stable across scales, so tuning is needed only at smaller scales. Building on these observations, we can simplify our hyperparameter search by identifying scaling-sensitive parameters, which (i) crucially influence the final performance, and (ii) change with respect to the model scale. We define the approximate-optimal configuration for given regime as the set of all hyperparameter tuples whose final loss lies within 2 =6.4e-3 of the regimes best-observed loss r, where denotes regime/setting, that is, is pair of choice of model size and data budget. Concretely, let ch denotes hyperparameter (where is the index) and be the tuple of all hyperparameters. 1. For each regime r, let Cr = { : L(c) + 2} be all the hyperparameter configurations in our coordinate descent procedure that yield approximately optimal loss. 2. hyperparameter ch is called scaling-insensitive if there exists single value vh such that there exists Cr such that ch = vh for every regime in Phase I. Otherwise, ch is scaling-sensitive, meaning its optimal value shifts depending on model size or data budget. We carry forward only the scaling-sensitive hyperparameters (shown in Table 4) into Phase II, thereby focusing our next round of coordinate-descent sweeps on the hyperparameters that truly require re-tuning across scaling regimes. We then perform sweeping for another 6 different settings, namely 300M, 500M at 2, 4, 8 Chinchilla. Stage LR WD min lr ratio Warmup Max Grad Norm Batch Val. Loss Init Round 1 Round 2 Best 0.008 0.008 0.008 0.008 0.1 0.1 0.1 0.1 0 0 0 0 1000 2000 2000 1 1 1 2 256 256 128 128 3.298 3.282 3.263 3.263 Table 3: Illustrative coordinate-descent steps for AdamW on the 130M 1 Chinchilla regime. Changed hyperparameter values are highlighted in red; We omitted some unchanged hyperparameters (β1 = 0.9, β2 = 0.98, ϵ = 1010). 7 Optimizer Scaling-Sensitive Hyperparameters learning rate, warmup, weight decay, batch size AdamW Nesterov AdamW learning rate, warmup learning rate, beta2 Lion learning rate, weight decay, warmup Adam-mini learning rate, batch size Cautious learning rate, warmup, beta1 Mars learning rate, beta1, # decay steps in WSD Hu et al. [2024] Scion learning rate Muon learning rate, warmup, block size Soap learning rate Kron Table 4: Scaling-sensitive hyperparameters identified in Phase (Section 3.2). To reduce the memory required by Soap, we apply the parameter blocking as in Anil et al. [2021] Result of Phase II. Combined with the results in Phase I, we obtain set of near-optimal hyperparameters and their corresponding losses for 12 different settings (130M, 300M, 500M and 1, 2, 4, 8 Chinchilla). To quantify the speedup of different optimizers over the baseline AdamW, we fit how AdamWs loss scales with data budget for each model size with the following functional form: ˆLN (D) = αN DBN + βN . Suppose an optimizer achieves loss Loptimizer at data budget DOptimizer. We calculate the corresponding data budget needed for AdamW to achieve this loss, denoted by DAdamW by finding the solution to the equation ˆLN (DAdamW) = Loptimizer. We then use DAdamW/DOptimizer as the estimated speedup ratio. Through this set of experiments, we observed two phenomena: (i) matrix-based optimizers consistently outperform scalar-based optimizers, but all optimizers speedup ratios over AdamW do not exceed 1.5; and (ii) within matrix-based optimizers, Muon performs the best at 1-4 Chinchilla ratio but is overtaken by Soap and Kron when the Chinchilla ratio increases."
        },
        {
            "title": "3.4 Phase III: Hyperparameter Scaling Law for Further Extrapolation",
            "content": "Having obtained optimized hyperparameter settings from Phase II (Section 3.3), we now fit smooth scaling law that predicts the optimal value of each scaling-sensitive hyperparameter as function of model size and data budget D. Concretely, we model the optimal value for each scaling-sensitive hyperparameter as: h(N, D) = α DB + β, where A, B, α, and β are learned coefficients. We estimate these parameters via non-linear least-squares on the 12 observed (N, D, h) triples for each optimizer, minimizing the squared error between predicted and actual optimal hyperparameter values. To test the quality of our prediction, we ran full Phase sweep at = 1.2B and Chinchilla = 1 for AdamW. Comparing the identified optimum against our fitted hyperparameters, we observe that our predicted hyperparameters yield final loss within 3e-3 of the optimal configuration, showing that our hyperparameter scaling law can effectively predict the optimal hyperparameters. We then performed two case studies to further extrapolate our benchmarking: 1. To test the effect of scaling up model sizes, we train 1.2B models using AdamW, Nesterov AdamW, and Muon on 1 to 8 Chinchilla ratio. 2. To further test the effect of different optimizers when data-to-model ratios are high, we train 130M and 300M models using AdamW, Nesterov AdamW, Muon, and Soap on 16x Chinchilla ratio. Results of Phase III. We demonstrate two potential shortcomings of Muon optimizers, which is the best optimizers in Phase and Phase II, through experiments in this phase: (i) while Muons speedup persists for models up to 1.2B parameters, the speedup decreases to under 1.2; (ii) With 16 Chinchilla ratio, NAdamW and Soap outperform Muon on the 130M model, and Soap also surpasses Muon on the 300M model. 8 Figure 2: Main Results For Phase & II. Top: We plot the validation loss on C4/EN for the experiments in Phase and Phase II. Every point corresponds to the optimal loss achieved at the corresponding Chinchilla ratio for each optimizer. Bottom: we plot the HellaSwag performance corresponding to the selected run for subset of optimizers: the AdamW baseline, the top 2 most performant scalar-based optimizers, and the top 3 most performant matrix-based optimizers. Analysis is deferred to Section 4.1. Figure 3: Speedup of different optimizers across scale. We estimate the speedup of different optimizers by fitting scaling law for AdamW and then map the loss of different optimizers to the corresponding equivalent data budget. We observe that (i) The highest speedup is capped at 1.4; (ii) matrix-based optimizers consistently outperform scalar-based optimizers and show an increasing speedup with data budget."
        },
        {
            "title": "4.1 Main Results",
            "content": "Results on 0.1B0.5B-parameter models. Figure 2 shows the validation loss curves for the 130M, 300M, and 520M models with varying Chinchilla ratios (1 to 8) in our benchmark. We further show that HellaSwag accuracy improvements closely mirror validation-loss gains. This is consistent with prior works that show lower losses translates to better downstream accuracy Bhagia et al. [2024], Liu et al. [2025b]. Across all model scales and compute budgets, both the variance-reduced Adam variants (NAdamW, Mars, Cautious) and the matrix-based optimizers deliver speedups over the AdamW baseline. However, no method achieves 2 step-wise acceleration claimed in previous literature. We note that Soap Vyas et al. [2025] is one of the few works that conduct independent hyperparameter sweeping for the baseline, and it indeed reports speedup closest to the actual observed improvement. Following the methodology defined in Section 3.3, we calculate the estimated speedup ratio of different optimizers in Figure 3 and the highest speedup ratio is 1.4. From the measured speedups, three patterns stand out in this computation regime: 9 Figure 4: Case Studies. Left: Validation loss scaling on 1.2B model for AdamW, NAdamW, Muon and Soap. Muon and Soap still offer significant speedup over AdamW but no longer significantly speed up over NAdamW. Mid: Estimated speedup ratio with the same methodology Figure 3, we observe that Muon and Soaps speedup decays with model size to only 1.1. Last: Experiment with 300M 16 Chinchilla setting, Soap outperforms Muon when data-to-model ratio further increases. 1. Matrix-based methods outperform scalar-based methods. The speedup ratio increases with data budget yet decreases with model size. For every model size, matrix-based optimizers (Soap, Muon, Kron, Scionsolid curves) consistently drive validation loss below that of their scalar-based counterparts (dashed curves). In the base (1 Chinchilla) compute regime, Muon performs best, but at 8 Chinchilla compute, the advantage shifts to Soap and Kron. As shown by the super-linear trend in Figure 3, the speedup ratios of these three optimizers grow with increasing data budget. To the best of authors knowledge, this dependency on data budget is not noted in prior works, which typically only experiment on one data-to-model ratio. However, the greatest gains occur on the 130M model, after which the speedup decreases to roughly 1.3 for larger model sizes. We will affirm this observation further in the experiments of 1.2B models, showing that the speedup ratios of matrix-based optimizers decrease to 1.1 for the 1.2B model. 2. Variance-reduction techniques provide small but clear lift. Within the scalar-based family, all variancereduced Adam variants (NAdamW, Mars, Cautious) consistently surpass vanilla AdamWexcept for small lag at the smallest experiment. Notably, Muon combines matrix-based updates with Nesterov momentum, illustrating how variance reduction compounds with matrix adaptation can yield greater efficiency. This result is vastly different from the 2 speedup reported in some of the works and we attribute this disparity to the better-tuned baseline. 3. Memory-efficient variants of AdamW closely track the performance of AdamW. The two memory-efficient AdamW variants (Lion, Adam-mini)despite their reduced auxiliary stateclosely track the performance of AdamW, with slowdown of at most 5% and sometimes even perform better than AdamW. Interestingly, Lion and Adam-mini show different scaling trends regarding model size: the disadvantage of Lion relative to AdamW widens while the disadvantage of Adam-mini over AdamW narrows. Results on 1.2B-parameters Models. Using the hyperparameter scaling law we fit (Section 3.3), we scale up the model size to 1.2B to examine how the speedup of optimizers scales with model size. We observe that NAdamW, Muon, and Soap still deliver speedup over AdamW, but the speedup deminishes to 1.1 for all these optimizers (Figure 4, Left and Mid) and no longer leads to downstream improvements  (Table 5)  . Based on this observation, we fit scaling laws for both AdamW and Muon based on the loss of 16 runs, and our scaling law predicts that Muon will result in slightly higher loss than AdamW in the 7B and 1 Chinchilla regime. We defer the fitting procedure to Appendix B.1. High data-to-model Ratio. In our previous experiments, Muon is outperformed by Soap in the 8 Chinchilla regime for the 130M and 520M models. To further test this, we train three 300M models to 16 Chinchilla and verify that Muon is no longer the optimal optimizer when the data-to-model ratio increases (Figure 4, right). We conjecture that the second-order momentum maintained by Soap and Kron becomes more effective when the data-to-model ratio increases. In the long run, adaptivity to heterogeneity in parameter directions may lead to larger speedup. We also perform similar experiments on 130M models and reach the same results (deferred to Appendix B.3). 10 Table 5: Benchmark performance of 1.2B models with different optimizers and Chinchilla scaling. Optimizer LAMBADA OpenBook Wino PIQA BoolQ WSC273 Hella ARC-C. ARC-E COPA Avg AdamW 67.16 NAdamW 67.84 67.53 Muon 41.40 40.20 39.80 64.96 76.12 68.59 64.80 77.15 68.10 67.09 77.09 68.81 82.78 83.52 80.95 67.56 67.40 67.67 43.43 43.34 43.34 74.49 73.61 73. 85.00 67.15 81.00 66.70 84.00 66.98 8x Chinchilla (193B)"
        },
        {
            "title": "4.2 Necessity of Rigorous Benchmarking",
            "content": "Our systematic sweeps uncover both universal optimization principles and surprising optimization-specific nuances, which call for rigorous study when designing future optimizers. First, we find that even superior optimizer can underperform less advanced method when its hyperparameters are not precisely tuned. In our exhaustive grid searches, slight deviations from each optimizers ideal learning rate or other critical hyperparameter often lead to degradation in validation loss that is large enough to flip the ordering (Figure 5, Left). This hyperparameter sensitivity means manual selection without systematic sweeps will likely produce arbitrary ranking of the optimizers. To further demonstrate this, we plot how validation losses vary when only one of the hyperparameters deviates from optimal value for Muon, Soap, Mars on 520M model in 1 Chinchilla regime (Figure 5, Mid). The ordering of the optimizers can easily flip if one chooses sub-optimal hyperparameter. Second, using the same hyperparameters for different optimizers do not guarantee fair comparison between optimizers. For example, while weight decay is essential across all optimizers for optimal performance, the optimal decay strength varies markedly between optimizers. As shown in (Figure 1, top right), optimal weight decay coefficients differ between the optimizers studied. We also note the optimal weight decay for Kron is larger than the conventional 0.1 and is approximately 0.5, which is crucial for Kron to outperform AdamW. Third, early training behavior can be highly misleading. Validation-loss curves during this initial phase tend to exaggerate performance gaps (Figure 1, bottom right) and, in some cases, even reverse the eventual ranking, (Figure 5, right). Many optimizers exhibit rapid early descent followed by plateauing, meaning that assessments based solely on losses before the end of the training trajectories fail to predict final outcomes. To avoid such pitfalls, we recommend evaluating optimizers only on the final checkpoints rather than relying on intermediate checkpoints (e.g. in Liu et al. [2025a]). We also note that the evaluaton in Sophia Liu et al. [2024a] largely follows the correct procedure of the comparisons above the peak learning rate was tuned to be optimal for the baseline. However, as the data loader in the codebase used did not fully randomize the order of the data, the optimal peak learning rate in that code base for AdamW is significantly smaller than the optimal peak learning rate for fully randomized data loader setting. It turns out that Sophia doesnt offer significant speedup over AdamW for models under 0.5B in our setting (Figure 7)."
        },
        {
            "title": "4.3 Common Phenomena Across Optimizers",
            "content": "Through logging the evolution of weight and gradient norms, we discovered some shared optimization phenomena across optimizers. Parameter norms typically track learning rate decay when there is weight decay We observe that the parameter norms of all optimizers show similar pattern of increase and decrease, closely aligning with the increase and decrease of the learning rate if there is decay phase. However, the absolute values of parameter norms are very different across optimizers (Figure 6, Middle Left). Gradient norm increases during learning rate decay. Across all the optimizers, the gradient norms increase during the training run. However, this increase does not lead to loss increase. Similar to the parameter norms, the absolute values of gradient norms are not consistent across optimizers. These two phenomena are also reported in the previous work Defazio [2025], where the author also provides theoretical explanation for both phenomena. We provide additional evidence that these two phenomena are not artifacts of the chosen optimizer AdamW but rather common phenomena across optimizers (Figure 6, Middle Right). Different optimizers have similar generalization behavior. For architecture design, it has been observed that different architectures can have vastly different generalization behaviors Lu et al. [2025]. However, this is 11 Figure 5: Necessity of Careful Tuning. Left: 2 the optimal learning rate diminishes Soaps loss improvement over Mars on 520M 8x experiment; Mid: Variation of loss when only one hyperparameter differs from optimal learning rate and runs converge to within 0.02 of optimal. The order of optimizers may flip arbitrarily if rigorous tuning is missing. Right: Changing single hyperparameter like weight decay may lead to misleading faster loss improvement but plateaus later. Figure 6: Common Phenomena Across Optimizers. Left: Learning rate used for different optimizers. Middle Left: Parameter norm of all optimizers shows similar trend of increment and decrease, closely aligning the increasing and decaying of learning rate schedule. Middle Right: Gradient norm increases during learning rate decay. However, this increase does not lead to loss increase. Right: The training loss and evaluation loss follows the same trend for all optimizers. not the case for optimizers, and the evaluation losses and training losses follow roughly the same trend for all optimizers at the regimes we experimented on (Figure 6, Right)."
        },
        {
            "title": "5 Conclusion and Limitations",
            "content": "We benchmarked 11 deep learning optimizers in pretraining and found that their true gains over AdamW are much smaller than previously reported. Our results highlight three key lessons: (i) many claimed speedups 12 stem from insufficient hyperparameter tuning, as fair sweeps eliminate most of the apparent advantages; (ii) comparisons based on early or inconsistent evaluation can be misleading, since optimizer rankings often change over the full training trajectory; and (iii) even the best-performing alternatives provide only modest speedups, which further diminish with model scale, dropping to 1.1 at 1.2B parameters. This benchmarking study has the limitation that it does we havent scaled to models larger than 1.2B parameters. However, we believe that evaluating optimizers on models of comparable size to prior studies is still valuable, as it reveals that insufficient tuning is major cause of subpar speedups. Promising future work includes extending our benchmarking to larger models beyond 1.2B parameters to test whether the diminishing speedup trend persists at frontier scales. Another direction is to design optimizers whose efficiency remains stable under scaling laws, ensuring consistent benefits as model size and data budget grow."
        },
        {
            "title": "Acknowledgement",
            "content": "This work was supported by the Google TPU Research Cloud (TRC), the Stanford HAIGoogle Cloud Credits Program, and NSF IIS 2211780, and is part of the Marin Project. The authors thank Evan Walters, Omead Pooladzandi, Jiacheng You, and Zhiyuan Li for discussions and help during our research."
        },
        {
            "title": "References",
            "content": "N. Agarwal, R. Anil, E. Hazan, T. Koren, and C. Zhang. Disentangling adaptive gradient methods from learning rates, 2020. URL https://arxiv.org/abs/2002.11803. K. Ahn, B. Xu, N. Abreu, and J. Langford. Dion: Distributed orthonormalized updates, 2025. URL https://arxiv.org/abs/2504.05295. E. AI, :, I. Shah, A. M. Polloreno, K. Stratos, P. Monk, A. Chaluvaraju, A. Hojel, A. Ma, A. Thomas, A. Tanwer, D. J. Shah, K. Nguyen, K. Smith, M. Callahan, M. Pust, M. Parmar, P. Rushton, P. Mazarakis, R. Kapila, S. Srivastava, S. Singla, T. Romanski, Y. Vanjani, and A. Vaswani. Practical efficiency of muon for pretraining, 2025. URL https://arxiv.org/abs/2505.02222. R. Anil, V. Gupta, T. Koren, K. Regan, and Y. Singer. Scalable second order optimization for deep learning, 2021. URL https://arxiv.org/abs/2002.09018. Z. Azerbayev, H. Schoelkopf, K. Paster, M. D. Santos, S. McAleer, A. Q. Jiang, J. Deng, S. Biderman, and S. Welleck. Llemma: An open language model for mathematics, 2024. URL https://arxiv.org/abs/ 2310.10631. S. Becker and Y. L. Cun. Improving the convergence of back-propagation learning with second order methods. In D. S. Touretzky, G. E. Hinton, and T. J. Sejnowski, editors, Proceedings of the 1988 Connectionist Models Summer School, pages 2937. San Francisco, CA: Morgan Kaufmann, 1989. J. Bernstein and L. Newhouse. Modular duality in deep learning, 2024a. URL https://arxiv.org/abs/ 2410.21265. J. Bernstein and L. Newhouse. Old optimizer, new norm: An anthology, 2024b. URL https://arxiv.org/ abs/2409.20325. J. Bernstein, Y.-X. Wang, K. Azizzadenesheli, and A. Anandkumar. signSGD: Compressed optimisation for non-convex problems. In J. Dy and A. Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 560569. PMLR, 1015 Jul 2018. URL https://proceedings.mlr.press/v80/bernstein18a.html. A. Bhagia, J. Liu, A. Wettig, D. Heineman, O. Tafjord, A. H. Jha, L. Soldaini, N. A. Smith, D. Groeneveld, P. W. Koh, J. Dodge, and H. Hajishirzi. Establishing task scaling laws via compute-efficient model ladders, 2024. URL https://arxiv.org/abs/2412.04403. Y. Bisk, R. Zellers, R. Le Bras, J. Gao, and Y. Choi. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the 34th AAAI Conference on Artificial Intelligence (AAAI), pages 74327439. AAAI Press, 2020. doi: 10.1609/aaai.v34i05.6239. URL https://doi.org/10.1609/aaai.v34i05. 6239. T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners, 2020. URL https://arxiv.org/abs/2005.14165. X. Chen, C. Liang, D. Huang, E. Real, K. Wang, Y. Liu, H. Pham, X. Dong, T. Luong, C.-J. Hsieh, Y. Lu, and Q. V. Le. Symbolic discovery of optimization algorithms, 2023. URL https://arxiv.org/abs/2302.06675. C. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins, and K. Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. In J. Burstein, C. Doran, and T. Solorio, editors, Proceedings of NAACL-HLT 2019, pages 29242936, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1300. URL https://aclanthology.org/N19-1300/. 14 P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. CoRR, abs/1803.05457, 2018. URL http://arxiv.org/abs/1803.05457. DeepSeek-AI, :, X. Bi, D. Chen, G. Chen, S. Chen, D. Dai, C. Deng, H. Ding, K. Dong, Q. Du, Z. Fu, H. Gao, K. Gao, W. Gao, R. Ge, K. Guan, D. Guo, J. Guo, G. Hao, Z. Hao, Y. He, W. Hu, P. Huang, E. Li, G. Li, J. Li, Y. Li, Y. K. Li, W. Liang, F. Lin, A. X. Liu, B. Liu, W. Liu, X. Liu, X. Liu, Y. Liu, H. Lu, S. Lu, F. Luo, S. Ma, X. Nie, T. Pei, Y. Piao, J. Qiu, H. Qu, T. Ren, Z. Ren, C. Ruan, Z. Sha, Z. Shao, J. Song, X. Su, J. Sun, Y. Sun, M. Tang, B. Wang, P. Wang, S. Wang, Y. Wang, Y. Wang, T. Wu, Y. Wu, X. Xie, Z. Xie, Z. Xie, Y. Xiong, H. Xu, R. X. Xu, Y. Xu, D. Yang, Y. You, S. Yu, X. Yu, B. Zhang, H. Zhang, L. Zhang, L. Zhang, M. Zhang, M. Zhang, W. Zhang, Y. Zhang, C. Zhao, Y. Zhao, S. Zhou, S. Zhou, Q. Zhu, and Y. Zou. Deepseek llm: Scaling open-source language models with longtermism, 2024. URL https://arxiv.org/abs/2401.02954. DeepSeek-AI, D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, X. Zhang, X. Yu, Y. Wu, Z. F. Wu, Z. Gou, Z. Shao, Z. Li, Z. Gao, A. Liu, B. Xue, B. Wang, B. Wu, B. Feng, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan, D. Dai, D. Chen, D. Ji, E. Li, F. Lin, F. Dai, F. Luo, G. Hao, G. Chen, G. Li, H. Zhang, H. Bao, H. Xu, H. Wang, H. Ding, H. Xin, H. Gao, H. Qu, H. Li, J. Guo, J. Li, J. Wang, J. Chen, J. Yuan, J. Qiu, J. Li, J. L. Cai, J. Ni, J. Liang, J. Chen, K. Dong, K. Hu, K. Gao, K. Guan, K. Huang, K. Yu, L. Wang, L. Zhang, L. Zhao, L. Wang, L. Zhang, L. Xu, L. Xia, M. Zhang, M. Zhang, M. Tang, M. Li, M. Wang, M. Li, N. Tian, P. Huang, P. Zhang, Q. Wang, Q. Chen, Q. Du, R. Ge, R. Zhang, R. Pan, R. Wang, R. J. Chen, R. L. Jin, R. Chen, S. Lu, S. Zhou, S. Chen, S. Ye, S. Wang, S. Yu, S. Zhou, S. Pan, S. S. Li, S. Zhou, S. Wu, S. Ye, T. Yun, T. Pei, T. Sun, T. Wang, W. Zeng, W. Zhao, W. Liu, W. Liang, W. Gao, W. Yu, W. Zhang, W. L. Xiao, W. An, X. Liu, X. Wang, X. Chen, X. Nie, X. Cheng, X. Liu, X. Xie, X. Liu, X. Yang, X. Li, X. Su, X. Lin, X. Q. Li, X. Jin, X. Shen, X. Chen, X. Sun, X. Wang, X. Song, X. Zhou, X. Wang, X. Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. Zhang, Y. Xu, Y. Li, Y. Zhao, Y. Sun, Y. Wang, Y. Yu, Y. Zhang, Y. Shi, Y. Xiong, Y. He, Y. Piao, Y. Wang, Y. Tan, Y. Ma, Y. Liu, Y. Guo, Y. Ou, Y. Wang, Y. Gong, Y. Zou, Y. He, Y. Xiong, Y. Luo, Y. You, Y. Liu, Y. Zhou, Y. X. Zhu, Y. Xu, Y. Huang, Y. Li, Y. Zheng, Y. Zhu, Y. Ma, Y. Tang, Y. Zha, Y. Yan, Z. Z. Ren, Z. Ren, Z. Sha, Z. Fu, Z. Xu, Z. Xie, Z. Zhang, Z. Hao, Z. Ma, Z. Yan, Z. Wu, Z. Gu, Z. Zhu, Z. Liu, Z. Li, Z. Xie, Z. Song, Z. Pan, Z. Huang, Z. Xu, Z. Zhang, and Z. Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025a. URL https://arxiv.org/abs/2501.12948. DeepSeek-AI, A. Liu, B. Feng, B. Xue, B. Wang, B. Wu, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan, D. Dai, D. Guo, D. Yang, D. Chen, D. Ji, E. Li, F. Lin, F. Dai, F. Luo, G. Hao, G. Chen, G. Li, H. Zhang, H. Bao, H. Xu, H. Wang, H. Zhang, H. Ding, H. Xin, H. Gao, H. Li, H. Qu, J. L. Cai, J. Liang, J. Guo, J. Ni, J. Li, J. Wang, J. Chen, J. Chen, J. Yuan, J. Qiu, J. Li, J. Song, K. Dong, K. Hu, K. Gao, K. Guan, K. Huang, K. Yu, L. Wang, L. Zhang, L. Xu, L. Xia, L. Zhao, L. Wang, L. Zhang, M. Li, M. Wang, M. Zhang, M. Zhang, M. Tang, M. Li, N. Tian, P. Huang, P. Wang, P. Zhang, Q. Wang, Q. Zhu, Q. Chen, Q. Du, R. J. Chen, R. L. Jin, R. Ge, R. Zhang, R. Pan, R. Wang, R. Xu, R. Zhang, R. Chen, S. S. Li, S. Lu, S. Zhou, S. Chen, S. Wu, S. Ye, S. Ye, S. Ma, S. Wang, S. Zhou, S. Yu, S. Zhou, S. Pan, T. Wang, T. Yun, T. Pei, T. Sun, W. L. Xiao, W. Zeng, W. Zhao, W. An, W. Liu, W. Liang, W. Gao, W. Yu, W. Zhang, X. Q. Li, X. Jin, X. Wang, X. Bi, X. Liu, X. Wang, X. Shen, X. Chen, X. Zhang, X. Chen, X. Nie, X. Sun, X. Wang, X. Cheng, X. Liu, X. Xie, X. Liu, X. Yu, X. Song, X. Shan, X. Zhou, X. Yang, X. Li, X. Su, X. Lin, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. X. Zhu, Y. Zhang, Y. Xu, Y. Xu, Y. Huang, Y. Li, Y. Zhao, Y. Sun, Y. Li, Y. Wang, Y. Yu, Y. Zheng, Y. Zhang, Y. Shi, Y. Xiong, Y. He, Y. Tang, Y. Piao, Y. Wang, Y. Tan, Y. Ma, Y. Liu, Y. Guo, Y. Wu, Y. Ou, Y. Zhu, Y. Wang, Y. Gong, Y. Zou, Y. He, Y. Zha, Y. Xiong, Y. Ma, Y. Yan, Y. Luo, Y. You, Y. Liu, Y. Zhou, Z. F. Wu, Z. Z. Ren, Z. Ren, Z. Sha, Z. Fu, Z. Xu, Z. Huang, Z. Zhang, Z. Xie, Z. Zhang, Z. Hao, Z. Gou, Z. Ma, Z. Yan, Z. Shao, Z. Xu, Z. Wu, Z. Zhang, Z. Li, Z. Gu, Z. Zhu, Z. Liu, Z. Li, Z. Xie, Z. Song, Z. Gao, and Z. Pan. Deepseek-v3 technical report, 2025b. URL https://arxiv.org/abs/2412.19437. A. Defazio. Why gradients rapidly increase near the end of training, 2025. URL https://arxiv.org/abs/ 2506.02285. A. Defazio and K. Mishchenko. Learning-rate-free learning by d-adaptation. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, editors, Proceedings of the 40th International Conference 15 on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 74497479. PMLR, 2329 Jul 2023. URL https://proceedings.mlr.press/v202/defazio23a.html. A. Defazio, A. Cutkosky, H. Mehta, and K. Mishchenko. Optimal linear decay learning rate schedules and further refinements, 2024a. URL https://arxiv.org/abs/2310.07831. A. Defazio, X. A. Yang, H. Mehta, K. Mishchenko, A. Khaled, and A. Cutkosky. The road less scheduled, 2024b. URL https://arxiv.org/abs/2405.15682. T. Dozat. Incorporating nesterov momentum into adam. 2016. J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of machine learning research, 12(7), 2011. R. Eschenhagen, A. Immer, R. Turner, F. Schneider, and P. Hennig. Kronecker-factored approximate curvature for modern neural network architectures. Advances in Neural Information Processing Systems, 36: 3362433655, 2023. R. Eschenhagen, A. Defazio, T.-H. Lee, R. E. Turner, and H.-J. M. Shi. Purifying shampoo: Investigating shampoos heuristics by decomposing its preconditioner, 2025. URL https://arxiv.org/abs/2506. 03595. K. Everett, L. Xiao, M. Wortsman, A. A. Alemi, R. Novak, P. J. Liu, I. Gur, J. Sohl-Dickstein, L. P. Kaelbling, J. Lee, and J. Pennington. Scaling exponents across parameterizations and optimizers, 2024. URL https://arxiv.org/abs/2407.05872. P. Foret, A. Kleiner, H. Mobahi, and B. Neyshabur. Sharpness-aware minimization for efficiently improving generalization, 2021. URL https://arxiv.org/abs/2010.01412. K. Frans, S. Levine, and P. Abbeel. stable whitening optimizer for efficient neural network training, 2025. URL https://arxiv.org/abs/2506.07254. A. Gordon, Z. Kozareva, and M. Roemmele. Semeval-2012 task 7: Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In E. Agirre, J. Bos, M. Diab, S. Manandhar, Y. Marton, and D. Yuret, editors, *SEM 2012: The First Joint Conference on Lexical and Computational Semantics (SemEval 2012), pages 394398, Montréal, Canada, June 78 2012. Association for Computational Linguistics. URL https://aclanthology.org/S12-1052/. A. Grattafiori, A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Vaughan, A. Yang, A. Fan, A. Goyal, A. Hartshorn, A. Yang, A. Mitra, A. Sravankumar, A. Korenev, A. Hinsvark, A. Rao, A. Zhang, A. Rodriguez, A. Gregerson, A. Spataru, B. Roziere, B. Biron, B. Tang, B. Chern, C. Caucheteux, C. Nayak, C. Bi, C. Marra, C. McConnell, C. Keller, C. Touret, C. Wu, C. Wong, C. C. Ferrer, C. Nikolaidis, D. Allonsius, D. Song, D. Pintz, D. Livshits, D. Wyatt, D. Esiobu, D. Choudhary, D. Mahajan, D. Garcia-Olano, D. Perino, D. Hupkes, E. Lakomkin, E. AlBadawy, E. Lobanova, E. Dinan, E. M. Smith, F. Radenovic, F. Guzmán, F. Zhang, G. Synnaeve, G. Lee, G. L. Anderson, G. Thattai, G. Nail, G. Mialon, G. Pang, G. Cucurell, H. Nguyen, H. Korevaar, H. Xu, H. Touvron, I. Zarov, I. A. Ibarra, I. Kloumann, I. Misra, I. Evtimov, J. Zhang, J. Copet, J. Lee, J. Geffert, J. Vranes, J. Park, J. Mahadeokar, J. Shah, J. van der Linde, J. Billock, J. Hong, J. Lee, J. Fu, J. Chi, J. Huang, J. Liu, J. Wang, J. Yu, J. Bitton, J. Spisak, J. Park, J. Rocca, J. Johnstun, J. Saxe, J. Jia, K. V. Alwala, K. Prasad, K. Upasani, K. Plawiak, K. Li, K. Heafield, K. Stone, K. El-Arini, K. Iyer, K. Malik, K. Chiu, K. Bhalla, K. Lakhotia, L. Rantala-Yeary, L. van der Maaten, L. Chen, L. Tan, L. Jenkins, L. Martin, L. Madaan, L. Malo, L. Blecher, L. Landzaat, L. de Oliveira, M. Muzzi, M. Pasupuleti, M. Singh, M. Paluri, M. Kardas, M. Tsimpoukelli, M. Oldham, M. Rita, M. Pavlova, M. Kambadur, M. Lewis, M. Si, M. K. Singh, M. Hassan, N. Goyal, N. Torabi, N. Bashlykov, N. Bogoychev, N. Chatterji, N. Zhang, O. Duchenne, O. Çelebi, P. Alrassy, P. Zhang, P. Li, P. Vasic, P. Weng, P. Bhargava, P. Dubal, P. Krishnan, P. S. Koura, P. Xu, Q. He, Q. Dong, R. Srinivasan, R. Ganapathy, R. Calderer, R. S. Cabral, R. Stojnic, R. Raileanu, R. Maheswari, R. Girdhar, R. Patel, R. Sauvestre, R. Polidoro, R. Sumbaly, R. Taylor, R. Silva, R. Hou, R. Wang, S. Hosseini, S. Chennabasappa, S. Singh, S. Bell, S. S. Kim, S. Edunov, S. Nie, S. Narang, S. Raparthy, S. Shen, S. Wan, S. Bhosale, S. Zhang, 16 S. Vandenhende, S. Batra, S. Whitman, S. Sootla, S. Collot, S. Gururangan, S. Borodinsky, T. Herman, T. Fowler, T. Sheasha, T. Georgiou, T. Scialom, T. Speckbacher, T. Mihaylov, T. Xiao, U. Karn, V. Goswami, V. Gupta, V. Ramanathan, V. Kerkez, V. Gonguet, V. Do, V. Vogeti, V. Albiero, V. Petrovic, W. Chu, W. Xiong, W. Fu, W. Meers, X. Martinet, X. Wang, X. Wang, X. E. Tan, X. Xia, X. Xie, X. Jia, X. Wang, Y. Goldschlag, Y. Gaur, Y. Babaei, Y. Wen, Y. Song, Y. Zhang, Y. Li, Y. Mao, Z. D. Coudert, Z. Yan, Z. Chen, Z. Papakipos, A. Singh, A. Srivastava, A. Jain, A. Kelsey, A. Shajnfeld, A. Gangidi, A. Victoria, A. Goldstand, A. Menon, A. Sharma, A. Boesenberg, A. Baevski, A. Feinstein, A. Kallet, A. Sangani, A. Teo, A. Yunus, A. Lupu, A. Alvarado, A. Caples, A. Gu, A. Ho, A. Poulton, A. Ryan, A. Ramchandani, A. Dong, A. Franco, A. Goyal, A. Saraf, A. Chowdhury, A. Gabriel, A. Bharambe, A. Eisenman, A. Yazdan, B. James, B. Maurer, B. Leonhardi, B. Huang, B. Loyd, B. D. Paola, B. Paranjape, B. Liu, B. Wu, B. Ni, B. Hancock, B. Wasti, B. Spence, B. Stojkovic, B. Gamido, B. Montalvo, C. Parker, C. Burton, C. Mejia, C. Liu, C. Wang, C. Kim, C. Zhou, C. Hu, C.-H. Chu, C. Cai, C. Tindal, C. Feichtenhofer, C. Gao, D. Civin, D. Beaty, D. Kreymer, D. Li, D. Adkins, D. Xu, D. Testuggine, D. David, D. Parikh, D. Liskovich, D. Foss, D. Wang, D. Le, D. Holland, E. Dowling, E. Jamil, E. Montgomery, E. Presani, E. Hahn, E. Wood, E.-T. Le, E. Brinkman, E. Arcaute, E. Dunbar, E. Smothers, F. Sun, F. Kreuk, F. Tian, F. Kokkinos, F. Ozgenel, F. Caggioni, F. Kanayet, F. Seide, G. M. Florez, G. Schwarz, G. Badeer, G. Swee, G. Halpern, G. Herman, G. Sizov, Guangyi, Zhang, G. Lakshminarayanan, H. Inan, H. Shojanazeri, H. Zou, H. Wang, H. Zha, H. Habeeb, H. Rudolph, H. Suk, H. Aspegren, H. Goldman, H. Zhan, I. Damlaj, I. Molybog, I. Tufanov, I. Leontiadis, I.-E. Veliche, I. Gat, J. Weissman, J. Geboski, J. Kohli, J. Lam, J. Asher, J.-B. Gaya, J. Marcus, J. Tang, J. Chan, J. Zhen, J. Reizenstein, J. Teboul, J. Zhong, J. Jin, J. Yang, J. Cummings, J. Carvill, J. Shepard, J. McPhie, J. Torres, J. Ginsburg, J. Wang, K. Wu, K. H. U, K. Saxena, K. Khandelwal, K. Zand, K. Matosich, K. Veeraraghavan, K. Michelena, K. Li, K. Jagadeesh, K. Huang, K. Chawla, K. Huang, L. Chen, L. Garg, L. A, L. Silva, L. Bell, L. Zhang, L. Guo, L. Yu, L. Moshkovich, L. Wehrstedt, M. Khabsa, M. Avalani, M. Bhatt, M. Mankus, M. Hasson, M. Lennie, M. Reso, M. Groshev, M. Naumov, M. Lathi, M. Keneally, M. Liu, M. L. Seltzer, M. Valko, M. Restrepo, M. Patel, M. Vyatskov, M. Samvelyan, M. Clark, M. Macey, M. Wang, M. J. Hermoso, M. Metanat, M. Rastegari, M. Bansal, N. Santhanam, N. Parks, N. White, N. Bawa, N. Singhal, N. Egebo, N. Usunier, N. Mehta, N. P. Laptev, N. Dong, N. Cheng, O. Chernoguz, O. Hart, O. Salpekar, O. Kalinli, P. Kent, P. Parekh, P. Saab, P. Balaji, P. Rittner, P. Bontrager, P. Roux, P. Dollar, P. Zvyagina, P. Ratanchandani, P. Yuvraj, Q. Liang, R. Alao, R. Rodriguez, R. Ayub, R. Murthy, R. Nayani, R. Mitra, R. Parthasarathy, R. Li, R. Hogan, R. Battey, R. Wang, R. Howes, R. Rinott, S. Mehta, S. Siby, S. J. Bondu, S. Datta, S. Chugh, S. Hunt, S. Dhillon, S. Sidorov, S. Pan, S. Mahajan, S. Verma, S. Yamamoto, S. Ramaswamy, S. Lindsay, S. Lindsay, S. Feng, S. Lin, S. C. Zha, S. Patil, S. Shankar, S. Zhang, S. Zhang, S. Wang, S. Agarwal, S. Sajuyigbe, S. Chintala, S. Max, S. Chen, S. Kehoe, S. Satterfield, S. Govindaprasad, S. Gupta, S. Deng, S. Cho, S. Virk, S. Subramanian, S. Choudhury, S. Goldman, T. Remez, T. Glaser, T. Best, T. Koehler, T. Robinson, T. Li, T. Zhang, T. Matthews, T. Chou, T. Shaked, V. Vontimitta, V. Ajayi, V. Montanez, V. Mohan, V. S. Kumar, V. Mangla, V. Ionescu, V. Poenaru, V. T. Mihailescu, V. Ivanov, W. Li, W. Wang, W. Jiang, W. Bouaziz, W. Constable, X. Tang, X. Wu, X. Wang, X. Wu, X. Gao, Y. Kleinman, Y. Chen, Y. Hu, Y. Jia, Y. Qi, Y. Li, Y. Zhang, Y. Zhang, Y. Adi, Y. Nam, Yu, Wang, Y. Zhao, Y. Hao, Y. Qian, Y. Li, Y. He, Z. Rait, Z. DeVito, Z. Rosnbrick, Z. Wen, Z. Yang, Z. Zhao, and Z. Ma. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. V. Gupta, T. Koren, and Y. Singer. Sahampoo: Preconditioned stochastic tensor optimization, 2018. URL https://arxiv.org/abs/1802.09568. J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las Casas, L. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den Driessche, B. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, J. W. Rae, O. Vinyals, and L. Sifre. Training compute-optimal large language models, 2022. URL https://arxiv.org/abs/2203.15556. S. Hu, Y. Tu, X. Han, C. He, G. Cui, X. Long, Z. Zheng, Y. Fang, Y. Huang, W. Zhao, X. Zhang, Z. L. Thai, K. Zhang, C. Wang, Y. Yao, C. Zhao, J. Zhou, J. Cai, Z. Zhai, N. Ding, C. Jia, G. Zeng, D. Li, Z. Liu, and M. Sun. Minicpm: Unveiling the potential of small language models with scalable training strategies, 2024. URL https://arxiv.org/abs/2404.06395. M. Ivgi, O. Hinder, and Y. Carmon. Dog is sgds best friend: parameter-free dynamic step size schedule, 2023. URL https://arxiv.org/abs/2302.12022. Y. Jiang, B. Neyshabur, H. Mobahi, D. Krishnan, and S. Bengio. Fantastic generalization measures and where to find them, 2019. URL https://arxiv.org/abs/1912.02178. K. Jordan, Y. Jin, V. Boza, J. You, F. Cesista, L. Newhouse, and J. Bernstein. Muon: An optimizer for hidden layers in neural networks, 2024. URL https://kellerjordan.github.io/posts/muon/. P. Kasimbeg, F. Schneider, R. Eschenhagen, J. Bae, C. S. Sastry, M. Saroufim, B. Feng, L. Wright, E. Z. Yang, Z. Nado, S. Medapati, P. Hennig, M. Rabbat, and G. E. Dahl. Accelerating neural network training: An analysis of the algoperf competition, 2025. URL https://arxiv.org/abs/2502.15015. D. P. Kingma and J. Ba. Adam: method for stochastic optimization, 2017. URL https://arxiv.org/ abs/1412.6980. T. Large, Y. Liu, M. Huh, H. Bahng, P. Isola, and J. Bernstein. Scalable optimization in the modular norm, 2024. URL https://arxiv.org/abs/2405.14813. H. J. Levesque, E. Davis, and L. Morgenstern. The winograd schema challenge. KR, 2012:13th, 2012. H. Li, W. Zheng, J. Hu, Q. Wang, H. Zhang, Z. Wang, S. Xuyang, Y. Fan, S. Zhou, X. Zhang, and D. Jiang. Predictable scale: Part optimal hyperparameter scaling law in large language model pretraining, 2025a. URL https://arxiv.org/abs/2503.04715. J. Li, A. Fang, G. Smyrnis, M. Ivgi, M. Jordan, S. Gadre, H. Bansal, E. Guha, S. Keh, K. Arora, S. Garg, R. Xin, N. Muennighoff, R. Heckel, J. Mercat, M. Chen, S. Gururangan, M. Wortsman, A. Albalak, Y. Bitton, M. Nezhurina, A. Abbas, C.-Y. Hsieh, D. Ghosh, J. Gardner, M. Kilian, H. Zhang, R. Shao, S. Pratt, S. Sanyal, G. Ilharco, G. Daras, K. Marathe, A. Gokaslan, J. Zhang, K. Chandu, T. Nguyen, I. Vasiljevic, S. Kakade, S. Song, S. Sanghavi, F. Faghri, S. Oh, L. Zettlemoyer, K. Lo, A. El-Nouby, H. Pouransari, A. Toshev, S. Wang, D. Groeneveld, L. Soldaini, P. W. Koh, J. Jitsev, T. Kollar, A. G. Dimakis, Y. Carmon, A. Dave, L. Schmidt, and V. Shankar. Datacomp-lm: In search of the next generation of training sets for language models, 2025b. URL https://arxiv.org/abs/2406.11794. X. Li. Black box lie group preconditioners for sgd, 2022. URL https://arxiv.org/abs/2211.04422. X.-L. Li. Preconditioned stochastic gradient descent. IEEE Transactions on Neural Networks and Learning ISSN 2162-2388. doi: 10.1109/tnnls.2017.2672978. URL Systems, 29(5):14541466, May 2018a. http://dx.doi.org/10.1109/TNNLS.2017.2672978. X.-L. Li. Preconditioner on matrix lie group for sgd, 2018b. URL https://arxiv.org/abs/1809.10232. K. Liang, L. Chen, B. Liu, and Q. Liu. Cautious optimizers: Improving training with one line of code, 2025. URL https://arxiv.org/abs/2411.16085. H. Liu, Z. Li, D. Hall, P. Liang, and T. Ma. Sophia: scalable stochastic second-order optimizer for language model pre-training, 2024a. URL https://arxiv.org/abs/2305.14342. J. Liu, J. Su, X. Yao, Z. Jiang, G. Lai, Y. Du, Y. Qin, W. Xu, E. Lu, J. Yan, Y. Chen, H. Zheng, Y. Liu, S. Liu, B. Yin, W. He, H. Zhu, Y. Wang, J. Wang, M. Dong, Z. Zhang, Y. Kang, H. Zhang, X. Xu, Y. Zhang, Y. Wu, X. Zhou, and Z. Yang. Muon is scalable for llm training, 2025a. URL https://arxiv.org/abs/2502.16982. L. Liu, H. Jiang, P. He, W. Chen, X. Liu, J. Gao, and J. Han. On the variance of the adaptive learning rate and beyond, 2021. URL https://arxiv.org/abs/1908.03265. Q. Liu, X. Zheng, N. Muennighoff, G. Zeng, L. Dou, T. Pang, J. Jiang, and M. Lin. Regmix: Data mixture as regression for language model pre-training, 2025b. URL https://arxiv.org/abs/2407.01492. Y. Liu, Z. Liu, and J. Gore. Focus: First order concentrated updating scheme, 2025c. URL https: //arxiv.org/abs/2501.12243. Z. Liu, C. Zhao, F. Iandola, C. Lai, Y. Tian, I. Fedorov, Y. Xiong, E. Chang, Y. Shi, R. Krishnamoorthi, L. Lai, and V. Chandra. Mobilellm: Optimizing sub-billion parameter language models for on-device use cases, 2024b. URL https://arxiv.org/abs/2402.14905. 18 Z. Liu, Y. Liu, E. J. Michaud, J. Gore, and M. Tegmark. Physics of skill learning, 2025d. URL https: //arxiv.org/abs/2501.12391. I. Loshchilov and F. Hutter. Decoupled weight decay regularization, 2019. URL https://arxiv.org/abs/ 1711.05101. A. Lozhkov, R. Li, L. B. Allal, F. Cassano, J. Lamy-Poirier, N. Tazi, A. Tang, D. Pykhtar, J. Liu, Y. Wei, T. Liu, M. Tian, D. Kocetkov, A. Zucker, Y. Belkada, Z. Wang, Q. Liu, D. Abulkhanov, I. Paul, Z. Li, W.-D. Li, M. Risdal, J. Li, J. Zhu, T. Y. Zhuo, E. Zheltonozhskii, N. O. O. Dade, W. Yu, L. Krauß, N. Jain, Y. Su, X. He, M. Dey, E. Abati, Y. Chai, N. Muennighoff, X. Tang, M. Oblokulov, C. Akiki, M. Marone, C. Mou, M. Mishra, A. Gu, B. Hui, T. Dao, A. Zebaze, O. Dehaene, N. Patry, C. Xu, J. McAuley, H. Hu, T. Scholak, S. Paquet, J. Robinson, C. J. Anderson, N. Chapados, M. Patwary, N. Tajbakhsh, Y. Jernite, C. M. Ferrandis, L. Zhang, S. Hughes, T. Wolf, A. Guha, L. von Werra, and H. de Vries. Starcoder 2 and the stack v2: The next generation, 2024. URL https://arxiv.org/abs/2402.19173. X. Lu, Y. Zhao, S. Wei, S. Wang, B. Qin, and T. Liu. How does sequence modeling architecture influence base capabilities of pre-trained language models? exploring key architecture design principles to avoid base capabilities degradation, 2025. URL https://arxiv.org/abs/2505.18522. L. Luo, Y. Xiong, Y. Liu, and X. Sun. Adaptive gradient methods with dynamic bound of learning rate, 2019. URL https://arxiv.org/abs/1902.09843. Y. Luo, X. Ren, Z. Zheng, Z. Jiang, X. Jiang, and Y. You. Came: Confidence-guided adaptive memory efficient optimization, 2023. URL https://arxiv.org/abs/2307.02047. C. Ma, W. Gong, M. Scetbon, and E. Meeds. Swan: Sgd with normalization and whitening enables stateless llm training, 2025. URL https://arxiv.org/abs/2412.13148. J. Martens and R. Grosse. Optimizing neural networks with kronecker-factored approximate curvature, 2020. URL https://arxiv.org/abs/1503.05671. T. Mihaylov, P. Clark, T. Khot, and A. Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. In E. Riloff, D. Chiang, J. Hockenmaier, and J. Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 23812391, Brussels, Belgium, OctNov 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1260. URL https://aclanthology.org/D18-1260/. K. Mishchenko and A. Defazio. Prodigy: An expeditiously adaptive parameter-free learner, 2024. URL https://arxiv.org/abs/2306.06101. I.-V. Modoranu, M. Safaryan, G. Malinovsky, E. Kurtić, T. Robert, P. Richtárik, and D. Alistarh. Microadam: Accurate adaptive optimization with low space overhead and provable convergence. Advances in Neural Information Processing Systems, 37:143, 2024. D. Morwani, I. Shapira, N. Vyas, E. Malach, S. Kakade, and L. Janson. new perspective on shampoos preconditioner, 2024. URL https://arxiv.org/abs/2406.17748. Y. Nesterov. method for solving the convex programming problem with convergence rate (1/k2). In Dokl akad nauk Sssr, volume 269, page 543, 1983. T. OLMo, P. Walsh, L. Soldaini, D. Groeneveld, K. Lo, S. Arora, A. Bhagia, Y. Gu, S. Huang, M. Jordan, N. Lambert, D. Schwenk, O. Tafjord, T. Anderson, D. Atkinson, F. Brahman, C. Clark, P. Dasigi, N. Dziri, M. Guerquin, H. Ivison, P. W. Koh, J. Liu, S. Malik, W. Merrill, L. J. V. Miranda, J. Morrison, T. Murray, C. Nam, V. Pyatkin, A. Rangapur, M. Schmitz, S. Skjonsberg, D. Wadden, C. Wilhelm, M. Wilson, L. Zettlemoyer, A. Farhadi, N. A. Smith, and H. Hajishirzi. 2 olmo 2 furious, 2025. URL https://arxiv.org/abs/2501.00656. M. Pagliardini, P. Ablin, and D. Grangier. The ademamix optimizer: Better, faster, older, 2024. URL https://arxiv.org/abs/2409.03137. 19 D. Paperno, G. Kruszewski, A. Lazaridou, N. Q. Pham, R. Bernardi, S. Pezzelle, M. Baroni, G. Boleda, and R. Fernández. The LAMBADA dataset: Word prediction requiring broad discourse context. In K. Erk and N. A. Smith, editors, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15251534, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1144. URL https://aclanthology.org/P16-1144/. T. Pethick, W. Xie, K. Antonakopoulos, Z. Zhu, A. Silveti-Falls, and V. Cevher. Training deep learning models with norm-constrained lmos, 2025. URL https://arxiv.org/abs/2502.07529. C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer, 2023. URL https://arxiv.org/abs/ 1910.10683. S. J. Reddi, S. Kale, and S. Kumar. On the convergence of adam and beyond. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=ryQu7f-RZ. H. Robbins and S. Monro. stochastic approximation method. The annals of mathematical statistics, pages 400407, 1951. K. Sakaguchi, R. Le Bras, C. Bhagavatula, and Y. Choi. Winogrande: An adversarial winograd schema challenge at scale. In Proceedings of the 34th AAAI Conference on Artificial Intelligence (AAAI), pages 87328740. AAAI Press, 2020. doi: 10.1609/aaai.v34i05.6399. URL https://doi.org/10.1609/aaai. v34i05.6399. T. Schaul, S. Zhang, and Y. LeCun. No more pesky learning rates, 2013. URL https://arxiv.org/abs/ 1206.1106. R. M. Schmidt, F. Schneider, and P. Hennig. Descending through crowded valley - benchmarking deep learning optimizers, 2021. URL https://arxiv.org/abs/2007.01547. N. Shazeer and M. Stern. Adafactor: Adaptive learning rates with sublinear memory cost, 2018. URL https://arxiv.org/abs/1804.04235. I. Sutskever, J. Martens, G. Dahl, and G. Hinton. On the importance of initialization and momentum in deep learning. In S. Dasgupta and D. McAllester, editors, Proceedings of the 30th International Conference on Machine Learning, volume 28 of Proceedings of Machine Learning Research, pages 11391147, Atlanta, Georgia, USA, 1719 Jun 2013. PMLR. URL https://proceedings.mlr.press/v28/sutskever13. html. A. Talmor, J. Herzig, N. Lourie, and J. Berant. Commonsenseqa: question answering challenge targeting commonsense knowledge. CoRR, abs/1811.00937, 2018. URL http://arxiv.org/abs/1811.00937. S. Taniguchi, K. Harada, G. Minegishi, Y. Oshima, S. C. Jeong, G. Nagahara, T. Iiyama, M. Suzuki, Y. Iwasawa, and Y. Matsuo. Adopt: Modified adam can converge with any β2 with the optimal rate, 2024. URL https://arxiv.org/abs/2411.02853. K. Team, Y. Bai, Y. Bao, G. Chen, J. Chen, N. Chen, R. Chen, Y. Chen, Y. Chen, Y. Chen, Z. Chen, J. Cui, H. Ding, M. Dong, A. Du, C. Du, D. Du, Y. Du, Y. Fan, Y. Feng, K. Fu, B. Gao, H. Gao, P. Gao, T. Gao, X. Gu, L. Guan, H. Guo, J. Guo, H. Hu, X. Hao, T. He, W. He, W. He, C. Hong, Y. Hu, Z. Hu, W. Huang, Z. Huang, Z. Huang, T. Jiang, Z. Jiang, X. Jin, Y. Kang, G. Lai, C. Li, F. Li, H. Li, M. Li, W. Li, Y. Li, Y. Li, Z. Li, Z. Li, H. Lin, X. Lin, Z. Lin, C. Liu, C. Liu, H. Liu, J. Liu, J. Liu, L. Liu, S. Liu, T. Y. Liu, T. Liu, W. Liu, Y. Liu, Y. Liu, Y. Liu, Y. Liu, Z. Liu, E. Lu, L. Lu, S. Ma, X. Ma, Y. Ma, S. Mao, J. Mei, X. Men, Y. Miao, S. Pan, Y. Peng, R. Qin, B. Qu, Z. Shang, L. Shi, S. Shi, F. Song, J. Su, Z. Su, X. Sun, F. Sung, H. Tang, J. Tao, Q. Teng, C. Wang, D. Wang, F. Wang, H. Wang, J. Wang, J. Wang, J. Wang, S. Wang, S. Wang, Y. Wang, Y. Wang, Y. Wang, Y. Wang, Y. Wang, Z. Wang, Z. Wang, Z. Wang, C. Wei, Q. Wei, W. Wu, X. Wu, Y. Wu, C. Xiao, X. Xie, W. Xiong, B. Xu, J. Xu, J. Xu, L. H. Xu, L. Xu, S. Xu, W. Xu, X. Xu, Y. Xu, Z. Xu, J. Yan, Y. Yan, X. Yang, Y. Yang, Z. Yang, Z. Yang, Z. Yang, H. Yao, X. Yao, W. Ye, Z. Ye, B. Yin, L. Yu, E. Yuan, H. Yuan, M. Yuan, H. Zhan, D. Zhang, H. Zhang, W. Zhang, X. Zhang, Y. Zhang, 20 Y. Zhang, Y. Zhang, Y. Zhang, Y. Zhang, Y. Zhang, Z. Zhang, H. Zhao, Y. Zhao, H. Zheng, S. Zheng, J. Zhou, X. Zhou, Z. Zhou, Z. Zhu, W. Zhuang, and X. Zu. Kimi k2: Open agentic intelligence, 2025. URL https://arxiv.org/abs/2507.20534. T. Tieleman. Lecture 6.5-rmsprop: Divide the gradient by running average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26, 2012. H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. Llama: Open and efficient foundation language models, 2023a. URL https://arxiv.org/abs/2302.13971. H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023b. URL https://arxiv.org/abs/2307.09288. N. Vyas, D. Morwani, R. Zhao, M. Kwun, I. Shapira, D. Brandfonbrener, L. Janson, and S. Kakade. Soap: Improving and stabilizing shampoo using adam, 2025. URL https://arxiv.org/abs/2409.11321. J. Wang, M. Wang, Z. Zhou, J. Yan, W. E, and L. Wu. The sharpness disparity principle in transformers for accelerating language model pre-training, 2025. URL https://arxiv.org/abs/2502.19002. S. Wang, A. Liu, J. Xiao, H. Liu, Y. Yang, C. Xu, Q. Pu, S. Zheng, W. Zhang, and J. Li. Cadam: Confidence-based optimization for online learning, 2024. URL https://arxiv.org/abs/2411.19647. K. Wen, Z. Li, J. Wang, D. Hall, P. Liang, and T. Ma. Understanding warmup-stable-decay learning rates: river valley loss landscape perspective, 2024. URL https://arxiv.org/abs/2410.05192. X. Xie, P. Zhou, H. Li, Z. Lin, and S. Yan. Adan: Adaptive nesterov momentum algorithm for faster optimizing deep models, 2024. URL https://arxiv.org/abs/2208.06677. Z. Xie, X. Wang, H. Zhang, I. Sato, and M. Sugiyama. Adaptive inertia: Disentangling the effects of adaptive learning rate and momentum, 2022. URL https://arxiv.org/abs/2006.15815. A. Yang, A. Li, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Gao, C. Huang, C. Lv, C. Zheng, D. Liu, F. Zhou, F. Huang, F. Hu, H. Ge, H. Wei, H. Lin, J. Tang, J. Yang, J. Tu, J. Zhang, J. Yang, J. Yang, J. Zhou, J. Zhou, J. Lin, K. Dang, K. Bao, K. Yang, L. Yu, L. Deng, M. Li, M. Xue, M. Li, P. Zhang, P. Wang, Q. Zhu, R. Men, R. Gao, S. Liu, S. Luo, T. Li, T. Tang, W. Yin, X. Ren, X. Wang, X. Zhang, X. Ren, Y. Fan, Y. Su, Y. Zhang, Y. Zhang, Y. Wan, Y. Liu, Z. Wang, Z. Cui, Z. Zhang, Z. Zhou, and Z. Qiu. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. G. Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior, gradient independence, and neural tangent kernel derivation, 2020a. URL https://arxiv.org/abs/1902.04760. G. Yang. Tensor programs iii: Neural matrix laws. arXiv preprint arXiv:2009.10685, 2020b. G. Yang. Tensor programs ii: Neural tangent kernel for any architecture. arXiv preprint arXiv:2006.14548, 2020c. G. Yang. Tensor programs i: Wide feedforward or recurrent neural networks of any architecture are gaussian processes, 2021. URL https://arxiv.org/abs/1910.12478. G. Yang and E. Littwin. Tensor programs ivb: Adaptive optimization in the infinite-width limit, 2023. URL https://arxiv.org/abs/2308.01814. 21 G. Yang, E. J. Hu, I. Babuschkin, S. Sidor, X. Liu, D. Farhi, N. Ryder, J. Pachocki, W. Chen, and J. Gao. Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer, 2022. URL https://arxiv.org/abs/2203.03466. G. Yang, J. B. Simon, and J. Bernstein. spectral condition for feature learning, 2024. URL https: //arxiv.org/abs/2310.17813. Z. Yao, A. Gholami, S. Shen, M. Mustafa, K. Keutzer, and M. Mahoney. Adahessian: An adaptive second order optimizer for machine learning. Proceedings of the AAAI Conference on Artificial Intelligence, 35(12): 1066510673, May 2021. doi: 10.1609/aaai.v35i12.17275. URL https://ojs.aaai.org/index.php/ AAAI/article/view/17275. Y. You, I. Gitman, and B. Ginsburg. Large batch training of convolutional networks, 2017. URL https: //arxiv.org/abs/1708.03888. Y. You, J. Li, S. Reddi, J. Hseu, S. Kumar, S. Bhojanapalli, X. Song, J. Demmel, K. Keutzer, and C.-J. Hsieh. Large batch optimization for deep learning: Training bert in 76 minutes, 2020. URL https://arxiv.org/abs/1904.00962. H. Yuan, Y. Liu, S. Wu, X. Zhou, and Q. Gu. Mars: Unleashing the power of variance reduction for training large models, 2025. URL https://arxiv.org/abs/2411.10438. M. Zaheer, S. Reddi, D. Sachan, S. Kale, and S. Kumar. Adaptive methods for nonconvex optimization. Advances in neural information processing systems, 31, 2018. M. D. Zeiler. Adadelta: An adaptive learning rate method, 2012. URL https://arxiv.org/abs/1212.5701. R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. Hellaswag: Can machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL), pages 47914800, 2019. doi: 10.18653/V1/P19-1472. URL https://doi.org/10.18653/V1/P19-1472. H. Zhang, D. Morwani, N. Vyas, J. Wu, D. Zou, U. Ghai, D. Foster, and S. Kakade. How does critical batch size scale in pre-training?, 2025a. URL https://arxiv.org/abs/2410.21676. M. R. Zhang, J. Lucas, G. Hinton, and J. Ba. Lookahead optimizer: steps forward, 1 step back, 2019. URL https://arxiv.org/abs/1907.08610. Y. Zhang, C. Chen, Z. Li, T. Ding, C. Wu, D. P. Kingma, Y. Ye, Z.-Q. Luo, and R. Sun. Adam-mini: Use fewer learning rates to gain more, 2025b. URL https://arxiv.org/abs/2406.16793. J. Zhao, Z. Zhang, B. Chen, Z. Wang, A. Anandkumar, and Y. Tian. Galore: Memory-efficient llm training by gradient low-rank projection, 2024. URL https://arxiv.org/abs/2403.03507. R. Zhao, D. Morwani, D. Brandfonbrener, N. Vyas, and S. Kakade. Deconstructing what makes good optimizer for language models, 2025. URL https://arxiv.org/abs/2407.07972. H. Zhu, Z. Zhang, W. Cong, X. Liu, S. Park, V. Chandra, B. Long, D. Z. Pan, Z. Wang, and J. Lee. Apollo: Sgd-like memory, adamw-level performance, 2025. URL https://arxiv.org/abs/2412.05270. J. Zhuang, T. Tang, Y. Ding, S. Tatikonda, N. Dvornek, X. Papademetris, and J. S. Duncan. Adabelief optimizer: Adapting stepsizes by the belief in observed gradients, 2020. URL https://arxiv.org/abs/2010.07468."
        },
        {
            "title": "A Optimizer Definitions",
            "content": "In this section, we present the algorithm for each optimizer we evaluated. Throughout this section, we use the following notation: wt for model parameters, gt for gradients at step t, η for learning rate, λ for weight decay, β1, β2 for moment decay rates, ϵ for numerical stability, gnorm for gradient norm, and m, for first and second moments. All operations are element-wise unless specified. Algorithm 1 AdamW Hyperparameters: β1, β2, ϵ, η, λ, gnorm State: m, Update Rule: ˆgt = gt max{1, gnorm gt } mt = β1 mt1 + (1 β1) ˆgt, vt = β2 vt1 + (1 β2) ˆg2 , vt 1 βt 2 mt 1 βt 1 ˆmt = ˆvt = , , wt+1 = wt η ˆmt ˆvt + ϵ η λ wt. Algorithm 2 Nesterov AdamW Hyperparameters: β1, β2, ϵ, η, λ, gnorm State: m, Update Rule: ˆgt = gt max{1, gnorm gt2 } mt = β1 mt1 + (1 β1) ˆgt, vt = β2 vt1 + (1 β2) ˆg2 , mt = β1 mt + (1 β1) ˆgt, vt 1 βt 2 mt 1 βt ˆmt = ˆvt = , , wt+1 = wt η ˆmt ˆvt + ϵ η λ wt. 23 Algorithm 3 Lion Hyperparameters: β1, β2, η, ϵ, λ, gnorm State: Update Rule: ˆgt = gt max{1, gnorm gt2 } ˆmt = β1 mt1 + (1 β1) ˆgt, mt+1 = β2 mt1 + (1 β2) ˆgt, wt+1 = wt η sign(cid:0) ˆmt (cid:1) η λ wt. Algorithm 4 Sophia-H Hyperparameters: {ηt}T State: m0 = 0, h1k = 0, θ1 For = 1, . . . , : t=1, λ, k, β1, β2, γ, ϵ gt = θLt(θt) mt = β1 mt1 + (1 β1) gt If mod = 1: {1}d, ˆh = = gt r, = θv, ht = β2 htk + (1 β2) ˆh Else: ht = ht1 θt = θt ηt λ θt (cid:16) θt+1 = θt ηt clip (cid:17) mt max(γ ht,ϵ) , 1 Algorithm 5 MARS Hyperparameters: β1, β2, γ, ϵ, η, λ, gnorm State: m, v, gt1 Update Rule: ct = gt + γ β1 1 β1 (gt gt1), ˆct = ct max{1, gnorm ct2 }, mt = β1 mt1 + (1 β1) ˆct, vt = β2 vt1 + (1 β2) ˆc2 , vt 1 βt 2 mt 1 βt 1 ˆmt = ˆvt = , , wt+1 = wt η ˆmt ˆvt + ϵ η λ wt. Algorithm 6 Adam-mini Hyperparameters: β1, β2, ϵ, η, λ, gnorm State: (with the same shape as w), (one scalar for each block) Setup: 1. Partition all parameters into param_blocks: please refer to Zhang et al. [2025b] for the exact partition scheme. 2. We will use wt,b and mt,b to denote the parameters in block at step Update ˆgt = gt max{1, gnorm gt2 } mt = β1 mt1 + (1 β1) ˆgt, ˆmt = mt 1 βt , for each block param_blocks vt,b = β2 vt1,b + (1 β2)mean( g2 t,b) ˆvt,b = vt,b 1 βt . wt,b = wt1,b η ˆmt,b/( vt,b + ϵ) η λ wt1,b. 25 Algorithm 7 Cautious Hyperparameters: β1, β2, ϵ, η, λ, gnorm State: m, Update Rule: ˆgt = gt max{1, gnorm gt2 } mt = β1 mt1 + (1 β1) ˆgt, vt = β2 vt1 + (1 β2) ˆg2 , vt 1 βt 2 ˆmt = ˆvt = , , mt 1 βt 1 ˆmt ˆvt + ϵ ut st mean(st) , ut = ˆut = st = I(cid:0)ut ˆgt > 0(cid:1), , wt+1 = wt η ˆut η λ wt. Algorithm 8 Muon Hyperparameters: β, η, ϵ, β1, β2, ϵAdam, ηAdam, λ, gnorm State: Update Rule For Weights in LM Head, Embedding, and LayerNorm: Same as AdamW Update Rule For Matrices in Transformer Layer : ˆgt = gt max{1, gnorm gt2 } mt = β mt1 + ˆgt, = β mt + ˆgt, = NewtonSchulz(u, steps = 5), (cid:113) = max(cid:0)1, rows(w) cols(w) (cid:1), = u, wt+1 = wt η η λ wt. NewtonSchulz Orthogonalization (Operating on Matrices): , transpose (rows(X) > cols(X)), + ϵ if transpose: , for = 1 . . . 5 : = , = 3.4445 4.7750 A2 + 2.0315 A3, 3.4445 + X, if transpose: , return X. 26 Algorithm 9 Scion Hyperparameters: β, η, ϵ, β1, β2, ηSignGD, gnorm State: Update Rule For Matrices in LM Head and Embedding: ˆgt = gt max{1, gnorm gt2 } mt = β1 mt1 + (1 β1) ˆgt, (cid:1). wt+1 = wt ηSignGD sign(cid:0) ˆmt Update Rule For Matrices in Transformer Layer : Same as Muon 27 Algorithm 10 PSGD Kron ϵ, β1, η, λ, gnorm, normalize_grads, partition_grads_into_blocks, Hyperparameters: merge_small_dims, block_size, target_merged_dim_size, pupd(t), Setup: If merge_small_dims is True, then try merging small dimensions into single dimension with size target_merged_dim_size greedily. If partition_grads_into_blocks is True, then partition all parameters into block_size block_size blocks. Define unfoldi as the function that unfolds all dimensions except the i-th dimension into single dimension and foldi as the inverse function. State (per block ℓ): Denote w(ℓ) as the parameters in block ℓ and assume it has shape d1 d2 dn. µ(ℓ), with the same shape as w(ℓ), Q(ℓ) (i = 1, . . . , n), lower-triangular matrix with shape di di if gnorm > 0 : ˆgt = gt max{1, }, else ˆgt = gt gnorm gt2 ˆgt ˆgt2 + ϵ if normalize_grads : ˆgt = , else ˆgt = ˆgtµt β1 µt1 + (1 β1) ˆgt, ˆµt µt 1 β 1 . Balance check: bal_ctr bal_ctr + 1.If bal_ctr 100, then Q(ℓ) Balance function: balance(Q(ℓ) ) performs the following steps: balance(Q(ℓ) ), bal_ctr 0. 1. Compute the maximum absolute value for each row/column of Q(ℓ) : normsi = maxj,k Q(ℓ) [j, k] 2. Calculate the geometric mean: gmeani = (Πn j=1normsi[j]) 1 di 3. Scale each element: Q(ℓ) Q(ℓ) gmeani normsi 4. Return the balanced Q(ℓ) Preconditioner update: with probability pupd(t), 1. Random probe: (ℓ) tree_random_like(g(ℓ) ). 2. Dampen: ˆµ(ℓ) ˆµ(ℓ) + ϵ mean(ˆµ(ℓ) ) (ℓ) 3. Conjugate sketch (B): (0) = (ℓ), (i) = foldi (cid:18)(cid:16) (cid:17)T Q(ℓ) unfoldi(X (i1)) (cid:19) , = (n). 4. Pre-sketch (A): (0) = ˆµ(ℓ), (i) = foldi (cid:16) Q(ℓ) (cid:17) unfoldi(Y (i1)) , = (n). 5. For each in 1, . . . , n, Mi = unfoldi(A), Ci = unfoldi(B), T1 = MiM , T2 = CiC , = max u,v (T1 + T2)u,v, Q(ℓ) Q(ℓ) α T1 T2 Q(ℓ) . G(0) = ˆµ(ℓ), G(i) = foldi (cid:0)Q(ℓ) unfoldi(G(i1))(cid:1), g(ℓ) = G(d). (Weight-decay & update) g(ℓ) g(ℓ) + λ w(ℓ) w(ℓ) t1 η g(ℓ) . 28 t1, w(ℓ) Algorithm 11 SOAP Hyperparameters: β1, β2, µ, k, ϵ, block_size, gnorm State: m, v, GA, GB, QA, QB Partition all parameters into block_size block_size block Update Rule For Each Block: ˆgt = gt max{1, gnorm gt2 } ˆgt = QA ˆgt QB, mt = β1 mt1 + (1 β1) ˆgt, ˆmt = mt 1 βt , ˆvt = (cid:16) wt+1 = wt ηt vt 1 βt 2 ˆmt (cid:17) B, vt = β2 vt1 + (1 β2) ˆg2 , , ˆvt + ϵ GA = µ GA + (1 µ) ˆgt ˆg + ϵI, GB = µ GB + (1 µ) ˆg if mod = 0 : QA = QR(GA QA), QB = QR(GB QB). ˆgt + ϵI,"
        },
        {
            "title": "B Omitted Experiments",
            "content": "B.1 Scaling Law Based on Appendices and D, we fitted our scaling law for the 1.2B run of Muon, NAdamW and AdamW, we round the fitted value to our grid of hyperparameter and used hyperparameters are shown in Appendix E. After training the models, we fitted scaling law for Muon and AdamW of the following form: L(N, D) = αN + βDB + γ (1) The fitted values are For AdamW, α = 21.4289, = 0.1555, β = 276.4235, = 0.2804, γ = 1.7324, with RMS error of 3 103. For Muon α = 32.7458, = 0.1864, β = 59.0221, = 0.2074, γ = 1.8063 , with RMS error of 5 103. This scaling law predicts that when the parameter scale reaches 7B, Muon will actually result in higher loss compared to AdamW in 1 Chinchilla regime. B.2 Sophia Experiments We performed Phase experiment with Sophia, with detailed hyperparameter settings shown in Appendix C. We found that Sophia tends to underperform AdamW in smaller compute regimes and eventually slightly outperforms AdamW when either the model size or the data size increases. 29 Figure 7: Sophia Experiments. Left: Loss curve of Sophia and AdamW in 1 Chinchilla setting. Right: Loss curve of Sophia and AdamW for 130M model size. B.3 High Data-to-model Ratio Figure 8: More Case Studies. Experiment with 130M 16 Chinchilla setting, SOAP outperforms Muon in the overtraining setting. B.4 Evaluation Performance Table 6: Evaluation Performance for Mars, Model Size = 130m DATA SIZE PERFORMANCE METRIC 2B 5B 10B 21B LAMBADA OPENAI OPENBOOKQA WINOGRANDE PIQA BOOLQ WSC273 HELLASWAG 0SHOT ARC CHALLENGE ARC EASY COPA FINAL C4 LOSS 0.327 0.377 0.402 0.451 0.274 0.304 0.298 0.302 0.521 0.512 0.526 0.522 0.625 0.652 0.662 0.678 0.616 0.504 0.569 0.557 0.557 0.560 0.579 0.590 0.308 0.343 0.365 0.399 0.225 0.258 0.272 0.247 0.462 0.517 0.532 0.544 0.650 0.690 0.660 0.710 3.537 3.396 3.323 3.247 30 Table 7: Evaluation Performance for Mars, Model Size = 300m DATA SIZE PERFORMANCE METRIC 6B 12B 24B 48B LAMBADA OPENAI OPENBOOKQA WINOGRANDE PIQA BOOLQ WSC273 HELLASWAG 0SHOT ARC CHALLENGE ARC EASY COPA FINAL C4 LOSS 0.450 0.493 0.514 0.538 0.300 0.312 0.328 0.328 0.534 0.557 0.549 0.561 0.680 0.686 0.695 0.720 0.480 0.533 0.612 0.608 0.586 0.608 0.667 0.667 0.395 0.437 0.471 0.502 0.253 0.269 0.298 0.307 0.547 0.580 0.608 0.616 0.700 0.740 0.720 0.730 3.249 3.158 3.097 3.040 Table 8: Evaluation Performance for Mars, Model Size = 520m DATA SIZE PERFORMANCE METRIC 10B 21B 42B 85B LAMBADA OPENAI OPENBOOKQA WINOGRANDE PIQA BOOLQ WSC273 HELLASWAG 0SHOT ARC CHALLENGE ARC EASY COPA FINAL C4 LOSS 0.513 0.548 0.576 0.601 0.330 0.336 0.366 0.378 0.548 0.559 0.586 0.601 0.704 0.717 0.725 0.735 0.559 0.598 0.625 0.631 0.601 0.696 0.736 0.780 0.462 0.513 0.546 0.579 0.294 0.317 0.333 0.351 0.611 0.648 0.665 0.676 0.730 0.760 0.750 0.750 3.101 3.015 2.955 2.906 Table 9: Evaluation Performance for Muon, Model Size = 130m DATA SIZE PERFORMANCE METRIC 2B 5B 10B 21B LAMBADA OPENAI OPENBOOKQA WINOGRANDE PIQA BOOLQ WSC273 HELLASWAG 0SHOT ARC CHALLENGE ARC EASY COPA FINAL C4 LOSS 0.358 0.396 0.431 0.456 0.286 0.288 0.318 0.300 0.515 0.518 0.530 0.541 0.640 0.660 0.670 0.678 0.571 0.504 0.583 0.452 0.553 0.564 0.579 0.582 0.330 0.354 0.381 0.407 0.231 0.244 0.269 0.276 0.493 0.511 0.543 0.559 0.650 0.720 0.730 0.660 3.464 3.369 3.296 3.240 31 Table 10: Evaluation Performance for Muon, Model Size = 300m DATA SIZE PERFORMANCE METRIC 6B 12B 24B 48B LAMBADA OPENAI OPENBOOKQA WINOGRANDE PIQA BOOLQ WSC273 HELLASWAG 0SHOT ARC CHALLENGE ARC EASY COPA FINAL C4 LOSS 0.459 0.492 0.529 0.550 0.302 0.310 0.330 0.346 0.504 0.534 0.548 0.570 0.679 0.689 0.695 0.714 0.482 0.555 0.599 0.570 0.615 0.641 0.659 0.656 0.406 0.447 0.481 0.509 0.270 0.289 0.290 0.316 0.564 0.591 0.620 0.638 0.670 0.730 0.700 0.700 3.224 3.143 3.079 3.032 Table 11: Evaluation Performance for Muon, Model Size = 520m DATA SIZE PERFORMANCE METRIC 10B 21B 42B 85B LAMBADA OPENAI OPENBOOKQA WINOGRANDE PIQA BOOLQ WSC273 HELLASWAG 0SHOT ARC CHALLENGE ARC EASY COPA FINAL C4 LOSS 0.520 0.559 0.597 0.611 0.328 0.340 0.342 0.368 0.560 0.567 0.594 0.590 0.713 0.718 0.730 0.736 0.590 0.555 0.633 0.613 0.659 0.667 0.762 0.736 0.482 0.525 0.554 0.587 0.300 0.318 0.351 0.348 0.623 0.640 0.671 0.674 0.730 0.760 0.730 0.780 3.073 3.002 2.945 2.900 Table 12: Evaluation Performance for Lion, Model Size = 130m DATA SIZE PERFORMANCE METRIC 2B 5B 10B 21B LAMBADA OPENAI OPENBOOKQA WINOGRANDE PIQA BOOLQ WSC273 HELLASWAG 0SHOT ARC CHALLENGE ARC EASY COPA FINAL C4 LOSS 0.316 0.380 0.417 0.450 0.280 0.296 0.312 0.292 0.503 0.518 0.519 0.521 0.629 0.649 0.663 0.680 0.603 0.507 0.419 0.489 0.564 0.538 0.590 0.623 0.304 0.343 0.366 0.402 0.229 0.272 0.262 0.272 0.464 0.500 0.536 0.539 0.700 0.670 0.730 0.730 3.552 3.409 3.331 3.252 32 Table 13: Evaluation Performance for Lion, Model Size = 300m DATA SIZE PERFORMANCE METRIC 6B 12B 24B 48B LAMBADA OPENAI OPENBOOKQA WINOGRANDE PIQA BOOLQ WSC273 HELLASWAG 0SHOT ARC CHALLENGE ARC EASY COPA FINAL C4 LOSS 0.428 0.471 0.509 0.535 0.306 0.320 0.326 0.346 0.506 0.530 0.532 0.564 0.676 0.686 0.699 0.713 0.486 0.490 0.609 0.593 0.579 0.615 0.608 0.667 0.386 0.428 0.466 0.502 0.267 0.296 0.294 0.322 0.535 0.586 0.600 0.618 0.690 0.730 0.710 0.760 3.268 3.170 3.100 3.046 Table 14: Evaluation Performance for Lion, Model Size = 520m DATA SIZE PERFORMANCE METRIC 10B 21B 42B 85B LAMBADA OPENAI OPENBOOKQA WINOGRANDE PIQA BOOLQ WSC273 HELLASWAG 0SHOT ARC CHALLENGE ARC EASY COPA FINAL C4 LOSS 0.507 0.543 0.569 0.597 0.336 0.334 0.356 0.358 0.560 0.548 0.581 0.590 0.699 0.713 0.726 0.740 0.600 0.546 0.612 0.631 0.608 0.667 0.685 0.700 0.463 0.509 0.545 0.571 0.303 0.320 0.340 0.371 0.603 0.638 0.664 0.673 0.710 0.750 0.740 0.770 3.108 3.029 2.965 2.915 Table 15: Evaluation Performance for NAdamW, Model Size = 130m DATA SIZE PERFORMANCE METRIC 2B 5B 10B 21B LAMBADA OPENAI OPENBOOKQA WINOGRANDE PIQA BOOLQ WSC273 HELLASWAG 0SHOT ARC CHALLENGE ARC EASY COPA FINAL C4 LOSS 0.325 0.383 0.413 0.446 0.284 0.286 0.308 0.322 0.515 0.507 0.546 0.507 0.633 0.652 0.667 0.682 0.591 0.569 0.459 0.557 0.535 0.560 0.612 0.597 0.312 0.348 0.368 0.399 0.239 0.245 0.242 0.270 0.465 0.503 0.518 0.547 0.680 0.700 0.650 0.720 3.531 3.394 3.319 3.251 33 Table 16: Evaluation Performance for NAdamW, Model Size = 300m DATA SIZE PERFORMANCE METRIC 6B 12B 24B 48B LAMBADA OPENAI OPENBOOKQA WINOGRANDE PIQA BOOLQ WSC273 HELLASWAG 0SHOT ARC CHALLENGE ARC EASY COPA FINAL C4 LOSS 0.437 0.477 0.514 0.543 0.310 0.332 0.316 0.348 0.520 0.536 0.549 0.564 0.681 0.693 0.708 0.712 0.539 0.564 0.520 0.612 0.582 0.637 0.645 0.700 0.397 0.434 0.475 0.505 0.264 0.275 0.292 0.307 0.553 0.589 0.605 0.632 0.710 0.730 0.700 0.750 3.248 3.160 3.090 3.039 Table 17: Evaluation Performance for NAdamW, Model Size = 520m DATA SIZE PERFORMANCE METRIC 10B 21B 42B 85B LAMBADA OPENAI OPENBOOKQA WINOGRANDE PIQA BOOLQ WSC273 HELLASWAG 0SHOT ARC CHALLENGE ARC EASY COPA FINAL C4 LOSS 0.500 0.551 0.577 0.599 0.332 0.350 0.358 0.342 0.553 0.571 0.594 0.597 0.695 0.719 0.731 0.738 0.613 0.606 0.639 0.618 0.637 0.689 0.718 0.714 0.469 0.513 0.550 0.580 0.294 0.322 0.342 0.354 0.605 0.646 0.657 0.674 0.700 0.720 0.760 0.780 3.100 3.013 2.955 2. Table 18: Evaluation Performance for Kron, Model Size = 130m DATA SIZE PERFORMANCE METRIC 2B 5B 10B 21B LAMBADA OPENAI OPENBOOKQA WINOGRANDE PIQA BOOLQ WSC273 HELLASWAG 0SHOT ARC CHALLENGE ARC EASY COPA FINAL C4 LOSS 0.341 0.387 0.426 0.451 0.268 0.278 0.296 0.314 0.520 0.522 0.515 0.543 0.636 0.662 0.662 0.683 0.533 0.530 0.575 0.557 0.564 0.564 0.553 0.604 0.323 0.347 0.374 0.397 0.235 0.259 0.256 0.281 0.487 0.519 0.543 0.572 0.700 0.690 0.710 0.710 3.492 3.389 3.307 3. 34 Table 19: Evaluation Performance for Kron, Model Size = 300m DATA SIZE PERFORMANCE METRIC 6B 12B 24B 48B LAMBADA OPENAI OPENBOOKQA WINOGRANDE PIQA BOOLQ WSC273 HELLASWAG 0SHOT ARC CHALLENGE ARC EASY COPA FINAL C4 LOSS 0.429 0.485 0.516 0.548 0.310 0.314 0.314 0.326 0.527 0.534 0.542 0.571 0.675 0.690 0.706 0.709 0.487 0.475 0.610 0.611 0.546 0.619 0.659 0.718 0.396 0.438 0.479 0.501 0.262 0.285 0.298 0.323 0.551 0.586 0.618 0.632 0.670 0.720 0.720 0.750 3.244 3.151 3.083 3. Table 20: Evaluation Performance for Kron, Model Size = 520m DATA SIZE PERFORMANCE METRIC 10B 21B 42B 85B LAMBADA OPENAI OPENBOOKQA WINOGRANDE PIQA BOOLQ WSC273 HELLASWAG 0SHOT ARC CHALLENGE ARC EASY COPA FINAL C4 LOSS 0.512 0.549 0.569 0.603 0.338 0.332 0.350 0.352 0.536 0.567 0.575 0.601 0.721 0.713 0.724 0.739 0.529 0.560 0.537 0.559 0.648 0.663 0.696 0.762 0.474 0.516 0.556 0.582 0.287 0.312 0.344 0.353 0.616 0.648 0.662 0.681 0.710 0.720 0.790 0.750 3.084 3.009 2.946 2.900 Table 21: Evaluation Performance for Scion, Model Size = 130m DATA SIZE PERFORMANCE METRIC 2B 5B 10B 21B LAMBADA OPENAI OPENBOOKQA WINOGRANDE PIQA BOOLQ WSC273 HELLASWAG 0SHOT ARC CHALLENGE ARC EASY COPA FINAL C4 LOSS 0.355 0.382 0.427 0.446 0.282 0.300 0.288 0.314 0.515 0.502 0.502 0.535 0.641 0.655 0.675 0.684 0.574 0.518 0.532 0.474 0.505 0.531 0.553 0.608 0.323 0.354 0.374 0.401 0.247 0.241 0.276 0.264 0.501 0.513 0.541 0.557 0.650 0.660 0.660 0.680 3.477 3.379 3.302 3.246 Table 22: Evaluation Performance for Scion, Model Size = 300m DATA SIZE PERFORMANCE METRIC 6B 12B 24B 48B LAMBADA OPENAI OPENBOOKQA WINOGRANDE PIQA BOOLQ WSC273 HELLASWAG 0SHOT ARC CHALLENGE ARC EASY COPA FINAL C4 LOSS 0.445 0.489 0.511 0.540 0.306 0.308 0.326 0.358 0.531 0.550 0.555 0.566 0.684 0.689 0.701 0.712 0.576 0.610 0.574 0.585 0.597 0.612 0.685 0.696 0.406 0.442 0.478 0.506 0.272 0.285 0.298 0.316 0.564 0.592 0.625 0.633 0.680 0.730 0.700 0.740 3.232 3.152 3.086 3.039 Table 23: Evaluation Performance for Scion, Model Size = 520m DATA SIZE PERFORMANCE METRIC 10B 21B 42B 85B LAMBADA OPENAI OPENBOOKQA WINOGRANDE PIQA BOOLQ WSC273 HELLASWAG 0SHOT ARC CHALLENGE ARC EASY COPA FINAL C4 LOSS 0.521 0.552 0.575 0.601 0.338 0.362 0.366 0.374 0.569 0.551 0.605 0.586 0.704 0.717 0.736 0.743 0.575 0.615 0.620 0.641 0.674 0.692 0.736 0.736 0.481 0.520 0.549 0.581 0.289 0.314 0.346 0.354 0.625 0.647 0.667 0.686 0.730 0.730 0.750 0.770 3.080 3.007 2.952 2.904 Table 24: Evaluation Performance for Cautious, Model Size = 130m DATA SIZE PERFORMANCE METRIC 2B 5B 10B 21B LAMBADA OPENAI OPENBOOKQA WINOGRANDE PIQA BOOLQ WSC273 HELLASWAG 0SHOT ARC CHALLENGE ARC EASY COPA FINAL C4 LOSS 0.314 0.385 0.420 0.458 0.298 0.280 0.308 0.318 0.515 0.520 0.519 0.508 0.638 0.652 0.664 0.669 0.580 0.512 0.512 0.550 0.560 0.527 0.593 0.604 0.314 0.347 0.371 0.401 0.239 0.244 0.261 0.270 0.476 0.508 0.532 0.544 0.690 0.720 0.710 0.690 3.535 3.403 3.334 3.253 36 Table 25: Evaluation Performance for Cautious, Model Size = 300m DATA SIZE PERFORMANCE METRIC 6B 12B 24B 48B LAMBADA OPENAI OPENBOOKQA WINOGRANDE PIQA BOOLQ WSC273 HELLASWAG 0SHOT ARC CHALLENGE ARC EASY COPA FINAL C4 LOSS 0.437 0.479 0.510 0.544 0.316 0.314 0.326 0.328 0.515 0.533 0.530 0.576 0.677 0.699 0.707 0.711 0.547 0.492 0.584 0.574 0.568 0.608 0.634 0.667 0.397 0.432 0.477 0.505 0.267 0.280 0.292 0.308 0.541 0.569 0.606 0.618 0.670 0.730 0.760 0.720 3.260 3.165 3.094 3.043 Table 26: Evaluation Performance for Cautious, Model Size = 520m DATA SIZE PERFORMANCE METRIC 10B 21B 42B 85B LAMBADA OPENAI OPENBOOKQA WINOGRANDE PIQA BOOLQ WSC273 HELLASWAG 0SHOT ARC CHALLENGE ARC EASY COPA FINAL C4 LOSS 0.509 0.549 0.578 0.604 0.318 0.338 0.342 0.364 0.548 0.575 0.571 0.605 0.705 0.715 0.733 0.739 0.601 0.611 0.582 0.606 0.619 0.718 0.659 0.736 0.467 0.514 0.550 0.583 0.305 0.315 0.344 0.358 0.609 0.649 0.676 0.689 0.730 0.760 0.770 0.770 3.100 3.017 2.956 2.910 Table 27: Evaluation Performance for SOAP, Model Size = 130m DATA SIZE PERFORMANCE METRIC 2B 5B 10B 21B LAMBADA OPENAI OPENBOOKQA WINOGRANDE PIQA BOOLQ WSC273 HELLASWAG 0SHOT ARC CHALLENGE ARC EASY COPA FINAL C4 LOSS 0.344 0.404 0.434 0.458 0.280 0.270 0.300 0.318 0.511 0.510 0.523 0.517 0.643 0.652 0.666 0.676 0.569 0.491 0.519 0.559 0.513 0.568 0.597 0.612 0.321 0.351 0.376 0.401 0.247 0.255 0.270 0.270 0.475 0.519 0.537 0.571 0.690 0.710 0.720 0.710 3.487 3.376 3.295 3.240 37 Table 28: Evaluation Performance for SOAP, Model Size = 300m DATA SIZE PERFORMANCE METRIC 6B 12B 24B 48B LAMBADA OPENAI OPENBOOKQA WINOGRANDE PIQA BOOLQ WSC273 HELLASWAG 0SHOT ARC CHALLENGE ARC EASY COPA FINAL C4 LOSS 0.001 0.492 0.518 0.541 0.256 0.324 0.324 0.334 0.506 0.534 0.546 0.555 0.546 0.689 0.706 0.711 0.402 0.593 0.565 0.593 0.498 0.619 0.703 0.670 0.257 0.443 0.478 0.506 0.219 0.281 0.305 0.324 0.309 0.590 0.612 0.632 0.590 0.740 0.720 0.700 5.437 3.147 3.082 3.030 Table 29: Evaluation Performance for SOAP, Model Size = 520m DATA SIZE PERFORMANCE METRIC 10B 21B 42B 85B LAMBADA OPENAI OPENBOOKQA WINOGRANDE PIQA BOOLQ WSC273 HELLASWAG 0SHOT ARC CHALLENGE ARC EASY COPA FINAL C4 LOSS 0.513 0.555 0.582 0.606 0.308 0.312 0.368 0.350 0.548 0.562 0.594 0.583 0.695 0.714 0.729 0.739 0.563 0.623 0.596 0.643 0.648 0.703 0.722 0.755 0.478 0.523 0.553 0.586 0.292 0.345 0.334 0.356 0.610 0.656 0.655 0.678 0.740 0.760 0.740 0.770 3.079 3.004 2.957 2.899 Table 30: Evaluation Performance for AdamW, Model Size = 130m DATA SIZE PERFORMANCE METRIC 2B 5B 10B 21B LAMBADA OPENAI OPENBOOKQA WINOGRANDE PIQA BOOLQ WSC273 HELLASWAG 0SHOT ARC CHALLENGE ARC EASY COPA FINAL C4 LOSS 0.322 0.380 0.410 0.442 0.306 0.290 0.298 0.292 0.506 0.519 0.515 0.530 0.632 0.663 0.665 0.674 0.568 0.415 0.553 0.576 0.582 0.553 0.564 0.623 0.312 0.340 0.365 0.391 0.229 0.255 0.262 0.270 0.463 0.497 0.523 0.544 0.660 0.650 0.720 0.730 3.529 3.409 3.322 3.262 38 Table 31: Evaluation Performance for AdamW, Model Size = 300m DATA SIZE PERFORMANCE METRIC 6B 12B 24B 48B LAMBADA OPENAI OPENBOOKQA WINOGRANDE PIQA BOOLQ WSC273 HELLASWAG 0SHOT ARC CHALLENGE ARC EASY COPA FINAL C4 LOSS 0.435 0.484 0.500 0.537 0.300 0.312 0.328 0.318 0.519 0.530 0.542 0.552 0.673 0.691 0.697 0.713 0.454 0.496 0.508 0.602 0.586 0.626 0.667 0.692 0.386 0.432 0.471 0.501 0.269 0.295 0.304 0.310 0.535 0.581 0.612 0.616 0.700 0.700 0.690 0.710 3.264 3.166 3.094 3.043 Table 32: Evaluation Performance for AdamW, Model Size = 520m DATA SIZE PERFORMANCE METRIC 10B 21B 42B 85B LAMBADA OPENAI OPENBOOKQA WINOGRANDE PIQA BOOLQ WSC273 HELLASWAG 0SHOT ARC CHALLENGE ARC EASY COPA FINAL C4 LOSS 0.500 0.540 0.570 0.591 0.320 0.336 0.342 0.356 0.539 0.586 0.583 0.601 0.705 0.719 0.732 0.739 0.555 0.615 0.596 0.609 0.604 0.656 0.703 0.729 0.456 0.507 0.543 0.578 0.299 0.311 0.323 0.349 0.613 0.639 0.669 0.688 0.740 0.680 0.710 0.780 3.110 3.023 2.958 2.913 Table 33: Evaluation Performance for Adam-Mini, Model Size = 130m DATA SIZE PERFORMANCE METRIC 2B 5B 10B 21B LAMBADA OPENAI OPENBOOKQA WINOGRANDE PIQA BOOLQ WSC273 HELLASWAG 0SHOT ARC CHALLENGE ARC EASY COPA FINAL C4 LOSS 0.317 0.369 0.412 0.444 0.286 0.282 0.282 0.302 0.525 0.500 0.515 0.528 0.640 0.655 0.662 0.670 0.506 0.496 0.445 0.583 0.571 0.557 0.546 0.634 0.310 0.337 0.364 0.390 0.241 0.247 0.255 0.264 0.465 0.495 0.525 0.555 0.690 0.710 0.730 0.700 3.542 3.416 3.328 3.266 39 Table 34: Evaluation Performance for Adam-Mini, Model Size = 300m DATA SIZE PERFORMANCE METRIC 6B 12B 24B 48B LAMBADA OPENAI OPENBOOKQA WINOGRANDE PIQA BOOLQ WSC273 HELLASWAG 0SHOT ARC CHALLENGE ARC EASY COPA FINAL C4 LOSS 0.429 0.468 0.509 0.535 0.300 0.318 0.318 0.350 0.502 0.528 0.545 0.586 0.688 0.693 0.683 0.701 0.480 0.546 0.609 0.573 0.586 0.546 0.593 0.689 0.381 0.424 0.464 0.492 0.270 0.292 0.294 0.300 0.545 0.579 0.607 0.625 0.700 0.720 0.740 0.730 3.272 3.178 3.103 3.049 Table 35: Evaluation Performance for Adam-Mini, Model Size = 520m DATA SIZE PERFORMANCE METRIC 10B 21B 42B 85B LAMBADA OPENAI OPENBOOKQA WINOGRANDE PIQA BOOLQ WSC273 HELLASWAG 0SHOT ARC CHALLENGE ARC EASY COPA FINAL C4 LOSS 0.500 0.538 0.576 0.604 0.306 0.326 0.330 0.368 0.534 0.561 0.571 0.594 0.707 0.721 0.723 0.733 0.543 0.626 0.619 0.536 0.619 0.700 0.733 0.744 0.459 0.505 0.541 0.576 0.288 0.327 0.332 0.352 0.613 0.638 0.654 0.671 0.710 0.720 0.760 0.760 3.112 3.027 2.966 2."
        },
        {
            "title": "C Hyperparameter Ablation in Phase I",
            "content": "We reported the results for the optimizers we swept in Phase I. The result is formulated as follows: the first row shows the approximately best configuration found and the following rows show the results for the 1-dimensional ablations centered around the found configuration. The loss presented here is the final loss on the C4/EN validation set. 40 C.1 Sweeping Results for AdamW Table 36: Hyperparameter ablation for AdamW on 130m on 1x Chinchilla Data β1 β ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.9 0.98 1e-20 0.008 1 0.95 0.98 0.9 0.95 1e-25 1e-15 1e-10 0.004 0.016 0.032 0 2.0 0 128 256 2000 0.1 3.529 500 1000 4000 3.539 3.882 3.545 3.535 3.529 3.531 3.531 3.550 3.538 7.781 3.534 3.534 3.611 7.452 3.532 3.575 0 3.545 0.2 3. 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 Table 37: Hyperparameter ablation for AdamW on 130m on 2x Chinchilla Data β1 β2 ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.9 0.98 1e-20 0.008 1 0.95 0.98 0.9 0.95 1e-25 1e-15 1e-10 0.004 0.016 0.032 0 2.0 128 256 512 0.1 3.409 500 1000 4000 3.417 7.557 3.423 3.413 3.409 3.409 3.410 3.420 3.419 7.840 3.410 3.408 3.437 3.527 7.277 3.413 3.415 0 3.436 0.2 3.415 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 41 Table 38: Hyperparameter ablation for AdamW on 130m on 4x Chinchilla Data β1 β2 ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.9 0.98 1e-20 0.008 1 0.95 0.98 0.9 0.95 1e-25 1e-15 1e-10 0.004 0.016 0.032 0 2.0 0 2000 0.1 3.322 256 512 1024 500 1000 4000 3.330 3.416 3.338 3.329 3.322 3.323 3.324 3.329 3.337 7.562 3.327 3.324 3.331 3.373 3.480 7.262 3.327 3.325 0 3.359 0.2 3.335 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Table 39: Hyperparameter ablation for AdamW on 130m on 8x Chinchilla Data β1 β2 ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.9 0.98 1e-20 0.008 1 0.95 0.98 0.9 0.95 1e-25 1e-15 1e-10 0.004 0.016 0.032 0 2.0 256 1000 0.1 3.262 128 512 1024 500 2000 4000 3.273 3.430 3.272 3.266 3.262 3.263 3.261 3.270 7.435 7.658 3.263 3.264 3.264 3.286 3.328 3.278 3.263 3.262 0 3.310 0.2 3.269 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 0 42 Table 40: Hyperparameter ablation for AdamW on 300m on 1x Chinchilla Data β1 β2 ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.9 0.98 1e-10 0.008 1 0.95 0.98 0.9 0.95 1e-25 1e-20 1e-15 0.004 0.016 0.032 0 2.0 0 128 256 512 2000 0.1 3.264 500 1000 4000 3.271 7.351 3.280 3.269 3.265 3.265 3.263 3.272 7.760 7.784 3.263 3.263 3.282 3.367 7.704 7.759 3.270 0 3.303 0.2 3.275 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 Table 41: Hyperparameter ablation for AdamW on 520m on 1x Chinchilla Data β1 β2 ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.9 0.98 1e-10 0.004 1 0.95 0.98 0.9 0.95 1e-25 1e-20 1e-15 0.008 0.016 0.032 0 2.0 256 1000 0.2 3.110 128 512 1024 500 2000 4000 3.112 3.229 3.116 3.111 3.116 3.116 3.115 7.837 7.756 7.680 3.114 3.118 7.630 3.169 3.302 3.165 3.113 3.126 0 7.270 0.1 3.135 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 0 C.2 Sweeping Results for Cautious Table 42: Hyperparameter ablation for Cautious on 130m on 1x Chinchilla Data β1 β2 ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.95 0.98 1e-15 0.008 1 0.8 0.9 0.98 0.9 0.95 1e-25 1e-20 1e-10 0.016 0.032 0 2.0 0 128 256 2000 0.1 3.535 500 1000 4000 6.698 3.549 3.534 3.551 3.543 3.537 3.537 3.536 3.539 7.802 3.534 3.532 3.610 3.572 3.535 3.582 0 3.552 0.2 3.537 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 Table 43: Hyperparameter ablation for Cautious on 130m on 2x Chinchilla Data β1 β2 ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.98 0.98 1e-15 0.008 2 0.8 0.9 0.95 0.9 0.95 1e-25 1e-20 1e-10 0.016 0.032 0 1.0 128 256 512 2000 0.1 3.403 500 1000 4000 7.417 3.427 3.413 >10 3.408 3.404 3.404 3.404 3.416 3.513 3.415 3.401 3.422 3.499 3.453 3.412 3.410 0 3.436 0.2 3.405 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 0 Table 44: Hyperparameter ablation for Cautious on 130m on 8x Chinchilla Data β1 β2 ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.98 0.98 1e-15 0.008 2 0.8 0.9 0.95 0.9 0.95 1e-25 1e-20 1e-10 0.016 0.032 0 1.0 0 256 2000 0.1 3. 128 512 1024 500 1000 4000 7.395 3.271 3.259 >10 3.253 3.251 3.251 3.250 3.257 3.383 3.254 3.251 3.265 3.261 3.294 3.276 3.253 3.255 0 3.291 0.2 3.253 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Table 45: Hyperparameter ablation for Cautious on 300m on 1x Chinchilla Data β1 β2 ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.98 0.98 1e-25 0. 0.9 0.95 0.9 0.95 1e-25 1e-15 2 0 1 128 256 2000 0.1 3.260 1000 4000 3.286 3.271 >10 >10 3.260 3.260 3.274 3.259 3.270 7.352 3.264 0.0 3.310 0.2 3. 0 1 2 3 4 5 6 7 8 9 10 11 12 13 0 45 Table 46: Hyperparameter ablation for Cautious on 520m on 1x Chinchilla Data β1 β2 ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.98 0.98 1e-25 0. 1 0.8 0.9 0.95 0.9 0.95 1e-25 1e-20 1e-15 1e-10 0.016 0.032 0 2.0 0 256 128 512 2000 0.1 3. 500 1000 4000 >10 7.264 3.108 >10 3.105 3.100 3.100 3.101 3.101 3.125 7.662 3.123 3.102 >10 3.123 >10 3.119 3.107 0 3.131 0.2 3.105 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 46 C.3 Sweeping Results for Lion Table 47: Hyperparameter ablation for Lion on 130m on 1x Chinchilla Data β1 β2 η gnorm ηmin BSZ warmup λ Loss Link 0.9 0.95 0. 0.8 0.95 0.98 0.9 0.98 0.0005 0.001 0.004 0.008 1 0 2.0 128 256 2000 0.7 3.552 500 1000 4000 3.575 3.557 7.644 3.550 3.630 3.579 3.549 7.806 7.828 3.559 3.557 3.643 7.840 7.739 3.601 0 3.570 0.1 3.562 0.2 3.557 0.3 3.555 0.4 3.553 0.5 3.551 0.6 3.549 0.8 3.554 0.9 3.556 1 3.557 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 47 Table 48: Hyperparameter ablation for Lion on 130m on 2x Chinchilla Data β β2 η gnorm ηmin BSZ warmup λ Loss Link 0.9 0.95 0.001 0.8 0.95 0.98 0.9 0.98 0.0005 0.002 0.004 0.008 1 0 2.0 0 256 512 2000 0.7 3.414 500 1000 4000 3.436 3.418 7.313 3.433 3.409 3.435 3.414 3.489 7.826 3.414 3.413 3.447 3.540 3.435 3.415 3.423 0.4 3.419 0.5 3.418 0.6 3.414 0.8 3.414 0.9 3.413 1 3.413 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 48 Table 49: Hyperparameter ablation for Lion on 130m on 4x Chinchilla Data β1 β2 η gnorm ηmin BSZ warmup λ Loss Link 0.9 0.95 0.001 0.8 0.95 0.98 0.9 0.98 0.0005 0.002 0.004 0.008 0 2.0 0 128 2000 0.7 3. 256 512 1024 500 1000 4000 3.345 3.335 7.330 3.345 3.340 3.338 3.342 7.406 >10 3.333 3.334 3.346 3.386 3.492 3.364 3.336 3.333 0.4 3.335 0.5 3.333 0.6 3.329 0.8 3.332 0.9 3.332 1 3.335 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 Table 50: Hyperparameter ablation for Lion on 130m on 8x Chinchilla Data β1 β2 η gnorm ηmin BSZ warmup λ Loss Link 0.9 0.98 0. 0.8 0.95 0.98 0.9 0.95 0.0005 0.002 0.004 0.008 1 0 2.0 128 2000 0.7 3.252 256 512 1024 500 1000 4000 3.287 3.264 3.286 3.277 3.263 3.254 3.310 7.829 NaN 3.329 3.265 3.260 3.287 3.342 3.336 3.273 3.258 0.4 3.256 0.5 3.251 0.6 3.252 0.8 3.258 0.9 3.259 1 3.261 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 50 Table 51: Hyperparameter ablation for Lion on 300m on 1x Chinchilla Data β β2 η gnorm ηmin BSZ warmup λ Loss Link 0.9 0.95 0.001 0.8 0.95 0.98 0.9 0.98 0.0005 0.002 0.004 0.008 1 0 2.0 0 256 512 2000 0.6 3.268 500 1000 4000 3.283 3.271 7.690 3.283 3.271 3.286 3.283 7.817 7.863 3.274 3.269 3.295 3.367 7.607 3.278 3.277 0.4 3.272 0.5 3.271 0.7 3.268 0.8 3.268 0.9 3.269 1 3.269 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 51 Table 52: Hyperparameter ablation for Lion on 520m on 1x Chinchilla Data β1 β2 η gnorm ηmin BSZ warmup λ Loss Link 0.9 0.95 0.001 0.8 0.95 0.98 0.9 0.98 0.0005 0.002 0.004 0.008 0 2.0 0 128 2000 0.7 3. 256 512 1024 500 1000 4000 3.117 3.121 7.577 3.119 3.208 3.113 7.734 8.046 NaN 3.128 3.109 3.117 3.151 3.252 7.403 7.119 3.115 0.4 3.109 0.5 3.108 0.6 3.108 0.8 3.109 0.9 3.112 1 3.115 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 C.4 Sweeping Results for Mars Table 53: Hyperparameter ablation for Mars on 130m on 1x Chinchilla Data β1 β2 ϵ γ η gnorm ηmin BSZ warmup λ Loss Link 0.9 0.95 1e-25 0.025 0.016 0.8 0.95 0.98 0.9 0.98 0.99 1e-30 1e-25 1e-20 1e-15 1e-10 0.0125 0.05 0.1 0.008 0.032 1 0 128 256 2000 0.1 3.537 500 1000 4000 3.568 3.548 3.586 3.548 3.536 3.562 3.537 3.537 3.537 3.537 3.537 3.540 3.542 3.553 3.538 7.574 3.616 >10 >10 3.596 0 3.552 0.2 3. 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 53 Table 54: Hyperparameter ablation for Mars on 130m on 2x Chinchilla Data β1 β ϵ γ η gnorm ηmin BSZ warmup λ Loss Link 0.95 0.98 1e-25 0.025 0.008 0.8 0.9 0.98 0.9 0.95 0.99 1e-30 1e-20 1e-15 1e-10 0.0125 0.05 0.1 0.016 0.032 1 0 128 256 512 2000 0.1 3. 500 1000 4000 3.421 3.402 3.401 3.409 3.400 3.398 3.396 3.396 3.398 3.397 3.398 3.404 3.415 3.402 3.441 3.427 3.524 3.400 3.395 3.409 0 3.430 0.2 3.404 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 54 Table 55: Hyperparameter ablation for Mars on 130m on 4x Chinchilla Data β1 β2 ϵ γ η gnorm ηmin BSZ warmup λ Loss Link 0.9 0.98 1e-25 0.025 0.008 0.8 0.95 0.98 0.9 0.95 0.99 1e-30 1e-25 1e-20 1e-15 1e-10 0.0125 0.05 0.1 0.016 0.032 1 0 128 0.1 3.323 256 512 1024 500 1000 4000 3.336 3.326 3.350 3.337 3.330 3.324 3.323 3.323 3.323 3.321 3.322 3.323 3.324 3.330 3.337 8.642 3.336 3.384 3.525 3.328 3.321 3.327 0 3.359 0.2 3.333 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 55 Table 56: Hyperparameter ablation for Mars on 130m on 8x Chinchilla Data β1 β2 ϵ γ η gnorm ηmin BSZ warmup λ Loss Link 0.98 0.99 1e-25 0.025 0.008 0.8 0.9 0.95 0.9 0.95 0.98 1e-30 1e-25 1e-20 1e-15 1e-10 0.0125 0.05 0.1 0.016 0.032 1 0 128 2000 0.1 3.247 256 512 1024 500 1000 4000 3.272 3.252 3.256 NaN 3.257 3.249 3.247 3.247 3.247 3.247 3.248 3.255 3.252 3.410 3.262 3.322 3.250 3.277 3.339 3.250 3.248 3.251 0 3.292 0.2 3. 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 56 Table 57: Hyperparameter ablation for Mars on 300m on 1x Chinchilla Data β1 β ϵ γ η gnorm ηmin BSZ warmup λ Loss Link 0.98 0.98 1e-25 0. 0.008 0.8 0.9 0.95 0.9 0.95 0.99 1e-30 1e-20 1e-15 1e-10 0.0125 0.025 0.1 0.016 0.032 1 0 128 0.1 3.249 256 512 1024 500 2000 4000 7.899 3.259 3.247 NaN 3.256 3.247 3.249 3.250 3.250 3.252 3.259 3.249 3.280 3.265 3.410 3.288 3.389 3.624 3.254 3.254 3.269 0 3.312 0.2 3.262 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 57 Table 58: Hyperparameter ablation for Mars on 520m on 1x Chinchilla Data β1 β2 ϵ γ η gnorm ηmin BSZ warmup λ Loss Link 0.95 0.95 1e-25 0.025 0.008 0.8 0.9 0.98 0.9 0.98 0.99 1e-30 1e-20 1e-15 1e-10 0.0125 0.05 0.1 0.016 0.032 1 0 256 2000 0.1 3.101 0 128 512 1024 500 1000 4000 1 >10 2 3.108 3 3.107 4 3.108 5 3.099 6 3.102 7 3.101 8 3.101 9 3.101 10 3.101 11 3.100 12 3.103 13 3.113 3.111 14 NaN 15 16 3.102 17 3.130 18 3.224 19 3.128 20 3.105 21 3.112 22 0 3.131 23 0.2 3.103 58 C.5 Sweeping Results for NAdamW Table 59: Hyperparameter ablation for NAdamW on 130m on 1x Chinchilla Data β1 β ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.95 0.98 1e-25 0.008 1 0.8 0.9 0.98 0.9 0.95 1e-20 1e-15 1e-10 0.016 0.032 0 2.0 0 128 256 2000 0.1 3.531 500 1000 4000 4.764 3.552 3.585 3.552 3.535 3.531 3.533 3.531 3.545 >10 3.537 3.539 3.624 3.646 3.545 3.577 0 3.547 0.2 3. 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 Table 60: Hyperparameter ablation for NAdamW on 130m on 2x Chinchilla Data β1 β2 ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.95 0.98 1e-10 0.008 1 0.8 0.9 0.98 0.9 0.95 1e-25 1e-20 1e-15 0.016 0.032 0 2.0 128 256 512 0.1 3.394 500 1000 4000 3.452 3.408 3.402 3.403 3.399 3.394 3.394 3.393 3.406 7.675 3.400 3.396 3.423 3.520 3.424 3.400 3.405 0 3.421 0.2 3.398 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 59 Table 61: Hyperparameter ablation for NAdamW on 130m on 4x Chinchilla Data β1 β2 ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.95 0.98 1e-10 0.008 1 0.8 0.9 0.98 0.9 0.95 1e-25 1e-20 1e-15 0.016 0.032 0 2.0 0 2000 0.1 3.319 256 512 1024 500 1000 4000 7.085 3.331 3.349 3.332 3.327 3.321 3.321 3.323 3.343 7.733 3.324 3.323 3.332 3.372 3.496 6.917 3.328 3.321 0 3.359 0.2 3.330 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Table 62: Hyperparameter ablation for NAdamW on 130m on 8x Chinchilla Data β1 β2 ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.95 0.98 1e-10 0.008 1 0.8 0.9 0.98 0.9 0.95 1e-25 1e-20 1e-15 0.016 0.032 0 2.0 128 2000 0.1 3.251 256 512 1024 500 1000 4000 3.282 3.257 3.253 3.260 3.255 3.251 3.251 3.250 3.274 7.670 3.253 3.250 3.249 3.270 3.321 3.274 3.255 3.252 0 3.286 0.2 3.265 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 0 60 Table 63: Hyperparameter ablation for NAdamW on 300m on 1x Chinchilla Data β1 β2 ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.95 0.98 1e-10 0.008 1 0.8 0.9 0.98 0.9 0.95 1e-25 1e-20 1e-15 0.016 0.032 0 2.0 0 128 256 512 2000 0.1 3.248 500 1000 4000 7.542 7.054 3.254 3.263 3.256 3.250 3.250 3.250 7.538 7.695 3.260 3.250 3.272 3.344 7.744 7.421 3.256 0 3.283 0.2 3.257 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 C.6 Sweeping Results for Adam-Mini Table 64: Hyperparameter ablation for Adam-Mini on 130m on 1x Chinchilla Data β1 β2 ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.9 0.98 1e-15 0.008 1 0.95 0.98 0.9 0.95 1e-25 1e-20 1e-10 0.004 0.016 0.032 0 2.0 128 256 2000 0.1 3. 500 1000 4000 3.560 7.733 3.554 3.545 3.546 3.546 3.548 3.558 7.800 7.825 3.542 3.542 3.785 7.725 7.729 3.587 0 3.566 0.2 3.589 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 0 61 Table 65: Hyperparameter ablation for Adam-Mini on 130m on 2x Chinchilla Data β1 β2 ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.9 0.98 1e-20 0.008 2 0.95 0.98 0.9 0.95 1e-25 1e-15 1e-10 0.004 0.016 0.032 0 1.0 0 128 256 512 2000 0.1 3.416 500 1000 4000 3.429 7.520 3.422 3.419 3.416 3.416 3.415 3.425 7.796 7.721 3.416 3.416 3.487 3.758 7.617 7.445 3.424 0 3.447 0.2 3.426 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 Table 66: Hyperparameter ablation for Adam-Mini on 130m on 4x Chinchilla Data β1 β2 ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.9 0.98 1e-10 0.008 1 0.95 0.98 0.9 0.95 1e-25 1e-20 1e-15 0.004 0.016 0.032 0 2.0 128 2000 0.1 3.328 256 512 1024 500 1000 4000 3.360 7.771 3.337 3.331 3.331 3.331 3.334 3.334 7.717 7.652 3.329 3.329 3.363 3.447 3.784 7.855 7.411 3.331 0 3.364 0.2 3.365 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 0 Table 67: Hyperparameter ablation for Adam-Mini on 130m on 8x Chinchilla Data β1 β2 ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.9 0.98 1e-10 0.008 1 0.95 0.98 0.9 0.95 1e-25 1e-20 1e-15 0.004 0.016 0.032 0 2.0 0 128 2000 0.1 3. 256 512 1024 500 1000 4000 3.290 7.552 3.281 3.267 3.267 3.267 3.266 3.264 7.614 7.773 3.268 3.270 3.281 3.324 3.426 7.868 7.022 3.266 0 3.304 0.2 3.291 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Table 68: Hyperparameter ablation for Adam-Mini on 300m on 1x Chinchilla Data β1 β2 ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.9 0.98 1e-25 0. 2 0.95 0.98 0.9 0.95 1e-25 1e-20 1e-15 1e-10 0.008 0.016 0.032 0 1.0 128 256 512 2000 0.2 3.272 500 1000 4000 3.276 8.024 3.279 3.277 3.272 3.272 3.273 3.273 7.859 7.990 8.581 3.272 3.272 5.691 3.629 7.020 7.042 3.280 0 3.336 0.1 3. 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 0 63 Table 69: Hyperparameter ablation for Adam-Mini on 520m on 1x Chinchilla Data β1 β2 ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.9 0.98 1e-10 0. 0 0.95 0.98 0.9 0.95 1e-25 1e-20 1e-15 0.008 0.016 0.032 1.0 2.0 0 128 256 512 4000 0.1 3. 500 1000 2000 3.113 7.459 3.120 3.115 3.115 3.115 3.112 7.771 7.746 7.778 3.112 3.112 3.133 3.200 7.457 7.274 7.289 0 7.792 0.2 3.115 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 64 C.7 Sweeping Results for Kron Table 70: Hyperparameter ablation for Kron on 130m on 1x Chinchilla Data β1 blocksize η gnorm ηmin NormGrad Blocking Initpc ηpc ppc BSZ Steppc warmup λ Loss Link 0.95 0.9 0.98 256 128 512 0.002 0.0005 0.001 0.004 0.008 1 0.0 2.0 0 True False True 1 0.2 0.05 128 2000 1000 0.7 3. 0.1 0.1 256 512 500 1000 2000 4000 3.500 3.497 3.492 3.494 3.528 3.501 3.514 7.838 3.492 3.492 3.493 3.498 3.493 3.525 3.632 3.503 3.498 3.507 3.583 0.0 3.519 0.5 3.491 0.9 3. 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 65 Table 71: Hyperparameter ablation for Kron on 130m on 2x Chinchilla Data β1 blocksize η gnorm ηmin NormGrad Blocking Initpc ηpc ppc BSZ Steppc warmup λ Loss Link 0.95 0.9 0.98 256 512 0. 0.0005 0.001 0.004 0.008 1 0.0 2.0 0 True False True 1 0.2 0.05 2000 1000 0.5 3.389 0.1 0.1 256 512 1024 500 1000 2000 4000 3.393 3.391 3.390 3.409 3.391 3.410 7.458 3.389 3.389 3.419 3.390 3.388 3.402 3.438 3.532 3.393 3.391 3.392 3.407 0.0 3.420 0.7 3.392 0.9 3.401 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 Table 72: Hyperparameter ablation for Kron on 130m on 4x Chinchilla Data β1 blocksize η gnorm ηmin NormGrad Blocking Initpc ηpc ppc BSZ Steppc warmup λ Loss Link 0.95 0.9 0.98 256 128 512 0.002 0.0005 0.001 0.004 0.008 1 0.0 2.0 0 True False True 1 0.2 0.05 128 2000 1000 0.5 3. 0.1 0.1 256 512 1024 500 1000 2000 4000 3.316 3.310 3.308 3.313 3.316 3.303 6.703 7.492 3.307 3.307 5.676 3.311 3.304 3.307 3.327 3.370 3.311 3.310 3.310 3.317 0.0 3.341 0.7 3.326 0.9 3. 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 67 Table 73: Hyperparameter ablation for Kron on 130m on 8x Chinchilla Data β1 blocksize η gnorm ηmin NormGrad Blocking Initpc ηpc ppc BSZ Steppc warmup λ Loss Link 0.95 0.9 0.98 256 512 0. 0.0005 0.002 0.004 0.008 1 0.0 2.0 0 True False True False 1 0.2 0. 128 2000 1000 0.5 3.239 0.1 0.05 256 512 1024 500 1000 2000 4000 3.243 3.239 3.239 3.245 3.252 6.279 7.504 3.239 3.239 3.247 3.240 3.242 3.240 3.247 3.264 3.294 3.240 3.239 3.239 3.240 0.0 3.282 0.7 3.241 0.9 3.246 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 68 Table 74: Hyperparameter ablation for Kron on 300m on 1x Chinchilla Data β1 blocksize η gnorm ηmin NormGrad Blocking Initpc ηpc ppc BSZ Steppc warmup λ Loss Link 0. 0.9 0.98 256 512 0.001 0.0005 0.002 0.004 0.008 0.0 2.0 0 True False True 1 0.2 0.1 128 1000 0.5 3.244 0.1 0.05 256 512 1024 500 1000 2000 4000 3.248 3.244 3.243 3.263 3.245 7.636 6.923 3.244 3.244 3.253 3.247 3.245 3.268 3.309 3.393 3.248 3.245 3.249 3.255 0.0 3.280 0.7 3.241 0.9 3.243 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 69 Table 75: Hyperparameter ablation for Kron on 520m on 1x Chinchilla Data β1 blocksize η gnorm ηmin NormGrad Blocking Initpc ηpc ppc BSZ Steppc warmup λ Loss Link 0.95 0.9 0.98 512 0.001 0.0005 0.002 0.004 0.008 1 0.0 2.0 True False True 0.2 0.1 128 2000 1000 0.5 3. 0.1 0.05 256 512 500 1000 2000 4000 3.088 6.663 3.087 3.095 6.412 6.927 7.018 3.084 3.084 5.487 3.088 3.093 3.099 3.126 3.087 3.085 3.090 3.094 0.0 3.127 0.7 3.088 0.9 3. 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 70 C.8 Sweeping Results for Soap Table 76: Hyperparameter ablation for Soap on 130m on 1x Chinchilla Data β β2 blocksize ϵ η gnorm ηmin Blocking fpc βshampoo BSZ warmup λ Loss Link 0.95 0.98 0.8 0.9 0.9 0.95 0.99 256 128 512 1e-15 0.016 1e-20 1e-10 0.004 0.008 1 0 True 1 5 0.95 0.9 0.98 0.99 128 256 512 1000 0.1 3.483 500 2000 4.868 4.547 3.496 3.491 3.482 3.489 3.487 3.513 3.483 3.509 3.491 3.488 3.488 3.489 3.491 3.523 3.611 3.483 3.507 0 3.508 0.2 3.501 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 71 Table 77: Hyperparameter ablation for Soap on 130m on 2x Chinchilla Data β1 β2 blocksize ϵ η gnorm ηmin Blocking fpc βshampoo BSZ warmup λ Loss Link 0.95 0.99 0.8 0.9 0.9 0.95 0.98 256 128 512 1e-15 0.016 1e-20 1e-10 0.004 0.008 1 False 1 10 0. 0.9 0.95 0.99 128 500 0.1 3.376 256 512 1024 1000 2000 5.163 3.397 3.393 3.384 3.380 3.376 3.376 3.384 3.375 3.388 3.374 3.381 3.378 3.376 3.379 3.385 3.414 3.479 3.379 3.385 0 3.437 0.2 3.395 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 72 Table 78: Hyperparameter ablation for Soap on 130m on 4x Chinchilla Data β β2 blocksize ϵ η gnorm ηmin Blocking fpc βshampoo BSZ warmup λ Loss Link 0.95 0.99 0.8 0.9 0.9 0.95 0.98 256 128 512 1e-15 0.008 1e-20 1e-10 0.004 0.016 1 0 False 1 10 0.98 0.9 0.95 0.99 128 0.1 3.295 256 512 1024 1000 2000 3.328 3.302 3.312 3.303 3.298 3.295 3.295 3.297 3.295 3.303 3.303 3.299 3.296 3.294 3.297 3.305 3.325 3.358 3.298 3.300 0 3.370 0.2 3.304 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 73 Table 79: Hyperparameter ablation for Soap on 130m on 8x Chinchilla Data β1 β2 blocksize ϵ η gnorm ηmin Blocking fpc βshampoo BSZ warmup λ Loss Link 0.95 0.99 0.8 0.9 0.9 0.95 0.98 512 128 256 1e-15 0.008 1e-20 1e-10 0.004 0.016 1 True 10 0. 0.9 0.95 0.99 256 1000 0.1 3.239 128 512 1024 500 2000 3.298 3.250 3.251 3.244 3.241 3.240 3.242 3.239 3.238 3.248 3.249 3.239 3.239 3.241 3.242 3.250 3.276 3.240 3.240 0 3.308 0.2 3.243 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 Table 80: Hyperparameter ablation for Soap on 300m on 1x Chinchilla Data β1 β2 blocksize ϵ η gnorm ηmin Blocking fpc βshampoo BSZ warmup λ Loss Link 0.95 0.99 0.8 0.9 0.9 0.95 0.98 512 128 256 1e-10 0.008 1e-20 1e-15 0.004 0.016 0 0.9 0.95 0.98 0.99 256 512 1000 0.1 3.231 500 2000 4.544 3.251 3.247 3.240 3.234 3.239 3.235 3.233 3.233 3.239 5.559 3.233 3.235 3.237 3.248 3.283 3.231 3.235 0 3.268 0.2 3.238 0.3 3.249 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 True 10 Table 81: Hyperparameter ablation for Soap on 520m on 1x Chinchilla Data β1 β2 blocksize ϵ η gnorm ηmin Blocking fpc βshampoo BSZ warmup λ Loss Link 0.95 0.99 0.8 0.9 0.9 0.95 0.98 512 128 256 1e-10 0. 1e-20 1e-15 0.004 0.016 1 0 True 10 0.95 0.9 0.98 0.99 128 256 512 1000 0.1 3.079 500 2000 4.630 4.316 3.097 3.090 3.085 5.395 4.392 3.082 3.081 3.079 5.762 3.080 3.082 3.083 3.085 4.215 4.163 3.081 0 6.106 0.2 3.527 0.3 3. 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 75 C.9 Sweeping Results for Muon ηadam β1 Table 82: Hyperparameter ablation for Muon on 130m on 1x Chinchilla Data β2 Decay(WSD) η ϵ Schedule gnorm ηmin βmuon ϵmuon BSZ warmup λ Loss Link 1 0 2.0 0.95 1e-05 128 0.8 0.9 0.98 1e-25 1e-20 1e-15 1e-10 256 512 1024 0 0.1 3.464 3.474 3.463 3.468 3.467 3.470 3.481 3.469 3.516 3.482 3.468 3.467 3.465 3.465 3.465 3.491 3.484 3.535 3.688 3.464 3.464 3.503 3.473 3.474 3.465 3.465 3.465 3.465 3.504 3.601 3.811 0 3.532 0.2 3.483 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 0.0032 0.8 0.98 0.0016 0.0048 0.9 0.95 0.98 0.9 0.95 0.8 0.2 0.4 0.6 1.0 1e-15 0.016 linear 1e-25 1e-20 1e-10 0.008 0.032 0.064 0.128 ηadam β1 Table 83: Hyperparameter ablation for Muon on 130m on 2x Chinchilla Data β2 Decay(WSD) η ϵ Schedule gnorm ηmin βmuon ϵmuon BSZ warmup λ Loss Link 0 2.0 0 0.98 1e-05 128 0.8 0.9 0.95 1e-25 1e-20 1e-15 1e-10 256 512 1024 0 0.1 3.369 3.385 3.373 3.372 3.373 3.375 3.390 3.377 3.404 3.380 3.371 3.374 3.370 3.370 3.369 3.373 3.406 3.549 5.932 3.371 3.370 3.417 3.385 3.371 3.370 3.370 3.370 3.370 3.400 3.465 3.608 0 3.431 0.2 3. 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 0.0024 0.8 0.98 0.0008 0.0016 0.9 0.95 0.98 0.9 0.95 0.8 0.2 0.4 0.6 1.0 1e-15 0.008 linear 1e-25 1e-20 1e-10 0.016 0.032 0.064 0.128 77 ηadam β1 Table 84: Hyperparameter ablation for Muon on 130m on 4x Chinchilla Data β2 Decay(WSD) η ϵ Schedule gnorm ηmin βmuon ϵmuon BSZ warmup λ Loss Link 1 0 2.0 0 0. 1e-05 128 0.8 0.9 0.98 1e-25 1e-20 1e-15 1e-10 256 512 1024 0 0.1 3.296 3.309 3.300 3.297 3.299 3.299 3.317 3.305 3.334 3.310 3.301 3.297 3.297 3.296 3.297 3.306 3.347 3.431 4.807 3.296 3.296 3.327 3.307 3.295 3.296 3.296 3.296 3.297 3.308 3.346 3.420 0 3.352 0.2 3.316 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 0.0024 0.8 0.98 0.0008 0.0016 0.9 0.95 0.98 0.9 0.95 0.8 0.2 0.4 0.6 1.0 1e-15 0.008 linear 1e-25 1e-20 1e-10 0.016 0.032 0.064 0.128 78 ηadam β1 Table 85: Hyperparameter ablation for Muon on 130m on 8x Chinchilla Data β2 Decay(WSD) η ϵ Schedule gnorm ηmin βmuon ϵmuon BSZ warmup λ Loss Link 1 0 2.0 0.98 1e-05 128 0.8 0.9 0.95 1e-25 1e-20 1e-15 1e-10 256 512 1024 0 0.1 3.240 3.249 3.243 3.240 3.240 3.241 3.260 3.247 3.280 3.257 3.248 3.242 3.238 3.238 3.241 3.253 3.298 3.407 5.568 3.240 3.239 3.265 3.250 3.243 3.239 3.239 3.239 3.239 3.240 3.257 3.302 0 3.310 0.2 3.256 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 0.0024 0.8 0.98 0.0008 0.0016 0.9 0.95 0.98 0.9 0.95 1 0.2 0.4 0.6 0.8 1e-15 0.008 linear 1e-25 1e-20 1e-10 0.016 0.032 0.064 0.128 ηadam β1 Table 86: Hyperparameter ablation for Muon on 300m on 1x Chinchilla Data β2 Decay(WSD) η ϵ Schedule gnorm ηmin βmuon ϵmuon BSZ warmup λ Loss Link 0 2.0 0 0.98 1e-05 128 0.8 0.9 0.95 1e-25 1e-20 1e-15 1e-10 256 512 1024 0 0.1 3.224 3.232 3.225 3.226 3.226 3.227 3.241 3.231 3.271 3.240 3.229 3.225 3.224 3.224 3.225 3.236 3.281 4.695 5.976 3.224 3.225 3.257 3.235 3.224 3.226 3.226 3.226 3.226 3.244 3.301 3.419 0 3.285 0.2 3. 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 0.0024 0.8 0.98 0.0008 0.0016 0.9 0.95 0.98 0.9 0.95 0.8 0.2 0.4 0.6 1.0 1e-15 0.008 linear 1e-25 1e-20 1e-10 0.016 0.032 0.064 0.128 80 ηadam β1 Table 87: Hyperparameter ablation for Muon on 520m on 1x Chinchilla Data β2 Decay(WSD) η ϵ Schedule gnorm ηmin βmuon ϵmuon BSZ warmup λ Loss Link 1 0 2.0 0 0. 1e-05 128 0.8 0.9 0.95 1e-25 1e-20 1e-15 1e-10 256 512 1024 0 0.1 3.073 3.076 3.072 3.073 3.074 3.073 3.090 3.080 3.134 3.101 3.086 3.076 3.074 3.073 3.073 3.095 3.147 7.886 7.900 3.072 3.071 3.098 3.080 3.073 3.072 3.072 3.072 3.072 3.080 3.115 3.188 0 3.134 0.2 3.091 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 0.0024 0.8 0.98 0.0008 0.0016 0.9 0.95 0.98 0.9 0.95 1 0.2 0.4 0.6 0.8 1e-25 0.008 linear 1e-20 1e-15 1e-10 0.016 0.032 0.064 0.128 81 C.10 Sweeping Results for Scion Table 88: Hyperparameter ablation for Scion on 130m on 1x Chinchilla Data ηadam β1 Decay(WSD) η Schedule gnorm ηmin βmuon ϵscion BSZ warmup λ Loss Link 0.0032 0.95 0.0016 0.0048 0.0064 0.008 0.0096 0.8 0.9 0.98 0.8 0.0 0.2 0.4 0.6 1.0 0.016 linear 0.008 0.032 0.064 0.128 1 0 2.0 0 0.95 1e-15 0.8 0.9 0.98 1e-20 1e-10 1e-05 256 512 1024 0 0.1 3. 3.477 3.481 3.485 3.488 3.490 3.493 3.479 3.479 3.779 3.525 3.493 3.480 3.477 3.490 3.509 5.195 6.308 3.478 3.478 3.480 3.474 3.496 3.477 3.477 3.477 3.505 3.583 3.761 0 3.537 0.2 3.495 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 82 Table 89: Hyperparameter ablation for Scion on 130m on 2x Chinchilla Data ηadam β1 Decay(WSD) η Schedule gnorm ηmin βmuon ϵscion BSZ warmup λ Loss Link 0.0032 0.95 0.0016 0.0048 0.0064 0.008 0.0096 0.8 0.9 0.98 0.0 0.2 0.4 0.6 0.8 0.016 linear 0.008 0.032 0.064 0.128 0 2.0 0 0.9 1e-15 128 0.8 0.95 0.98 1e-20 1e-10 1e-05 256 512 1024 0 0.1 3.379 3.378 3.382 3.384 3.386 3.388 3.400 3.383 3.379 3.738 3.437 3.405 3.390 3.383 3.390 3.411 3.513 6.193 3.379 3.378 3.385 3.380 3.388 3.379 3.380 3.380 3.391 3.436 3.532 0 3.443 0.2 3. 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 83 Table 90: Hyperparameter ablation for Scion on 130m on 4x Chinchilla Data ηadam β1 Decay(WSD) η Schedule gnorm ηmin βmuon ϵscion BSZ warmup λ Loss Link 0.0016 0.98 0.0008 0.0024 0.0032 0.004 0.0048 0.8 0.9 0.95 1 0.0 0.2 0.4 0.6 0.8 0.008 linear 0.016 0.032 0.064 0.128 2 0 1.0 0 0.9 1e-15 128 0.8 0.95 0.98 1e-20 1e-10 1e-05 256 512 1024 0 0.1 3.302 3.302 3.306 3.307 3.306 3.307 3.331 3.314 3.306 3.584 3.340 3.317 3.307 3.303 3.310 3.361 5.152 5.894 3.303 3.303 3.307 3.302 3.303 3.302 3.303 3.302 3.323 3.365 3.447 0 3.375 0.2 3.309 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 84 Table 91: Hyperparameter ablation for Scion on 130m on 8x Chinchilla Data ηadam β1 Decay(WSD) η Schedule gnorm ηmin βmuon ϵscion BSZ warmup λ Loss Link 0.0016 0.98 0.0008 0.0024 0.0032 0.004 0.0048 0.8 0.9 0.95 1 0.0 0.2 0.4 0.6 0.8 0. linear 0.016 0.032 0.064 0.128 2 0 1.0 0.9 1e-05 128 0.8 0.95 0.98 1e-20 1e-15 1e-10 256 512 1024 0 0.1 3.246 3.245 3.250 3.249 3.251 3.252 3.275 3.257 3.249 3.558 3.288 3.266 3.255 3.249 3.265 3.325 5.264 6.174 3.247 3.247 3.249 3.245 3.247 3.246 3.246 3.247 3.250 3.272 3.319 0 3.308 0.2 3.261 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 85 Table 92: Hyperparameter ablation for Scion on 300m on 1x Chinchilla Data ηadam β1 Decay(WSD) η Schedule gnorm ηmin βmuon ϵscion BSZ warmup λ Loss Link 0.0008 0. 0.0016 0.0024 0.0032 0.004 0.0048 0.8 0.9 0.95 0.8 0.0 0.2 0.4 0.6 1.0 0.008 linear 0.016 0.032 0.064 0.128 2 0 1.0 0 0.9 1e-05 128 0.8 0.95 0.98 1e-20 1e-15 1e-10 256 512 1024 0.1 3.232 3.237 3.242 3.242 3.241 3.243 3.256 3.240 3.236 3.497 3.271 3.245 3.234 3.237 3.242 3.301 3.400 5.627 3.233 3.233 3.240 3.231 3.236 3.232 3.232 3.233 3.259 3.319 3.432 0 3.303 0.2 3.241 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 Table 93: Hyperparameter ablation for Scion on 520m on 1x Chinchilla Data ηadam β1 Decay(WSD) η Schedule gnorm ηmin βmuon ϵscion BSZ warmup λ Loss Link 0.0008 0.98 0.0016 0.0024 0.0032 0.004 0.0048 0.8 0.9 0.95 1 0.0 0.2 0.4 0.6 0.8 0.008 linear 0.016 0.032 0.064 0.128 2 0 1.0 0 0. 1e-05 128 0.8 0.95 0.98 1e-20 1e-15 1e-10 256 512 1024 0 0.1 3.080 3.089 3.090 3.090 3.090 3.091 3.104 3.090 3.081 3.400 3.136 3.106 3.091 3.083 3.105 3.173 4.420 7.335 3.082 3.081 3.084 3.079 3.086 3.081 3.081 3.080 3.093 3.134 3.215 0 3.150 0.2 3.097 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 87 C.11 Sweeping Results for Sophia Table 94: Hyperparameter ablation for Sophia on 130m on 1x Chinchilla Data β1 β2 ϵ γ η BSZ warmup λ Loss Link 0.95 0.9 1e-07 0.0125 0.004 128 0.8 0.9 0.98 0.95 0.98 0.99 0.995 1e-17 1e-12 0.00625 0.025 0.05 0.002 0.008 0.016 0.032 4000 500 1000 2000 0 3.544 3.581 3.546 3.624 3.546 3.553 3.563 3.573 3.559 3.559 3.546 3.546 3.554 3.551 3.576 7.769 7.745 7.754 7.824 3.576 0.1 3.553 0.2 3.566 0.3 3.579 0 1 2 3 1 2 3 4 1 2 1 2 3 1 2 3 4 1 2 3 1 2 3 88 Table 95: Hyperparameter ablation for Sophia on 130m on 2x Chinchilla Data β1 β2 ϵ γ η BSZ warmup λ Loss Link 0.95 0.9 1e-07 0.0125 0.004 128 4000 0.1 3.414 0.8 0.9 0.98 0.95 0.98 0.99 0.995 1e-17 1e-12 0.00625 0.025 0.05 0.002 0.008 0.016 0.032 256 500 1000 2000 3.439 3.416 3.469 3.414 3.418 3.421 3.428 3.426 3.424 3.416 3.415 3.423 3.417 3.438 7.282 6.938 3.446 7.811 7.549 3.436 0 3.431 0.2 3.415 0.3 3.419 0 1 2 3 1 2 3 4 1 2 1 2 3 1 2 3 4 1 1 2 3 1 2 3 89 Table 96: Hyperparameter ablation for Sophia on 130m on 4x Chinchilla Data β β2 ϵ γ η BSZ warmup λ Loss Link 0.95 0.99 1e-07 0.0125 0.004 4000 0.2 3.330 0.8 0.9 0.98 0.9 0.95 0.98 1e-17 1e-12 0.00625 0.025 0.002 0.016 0.032 256 512 500 1000 2000 3.351 3.332 3.371 3.328 3.328 3.328 3.337 3.338 3.330 3.330 3.329 7.059 6.664 3.340 3.390 7.345 7.022 3.349 0 3.367 0.1 3.330 0.3 3.332 0 1 2 3 1 2 3 1 2 1 2 1 3 4 1 2 1 2 3 1 2 Table 97: Hyperparameter ablation for Sophia on 130m on 8x Chinchilla Data β1 β2 ϵ γ η BSZ warmup λ Loss Link 0.95 0.95 1e-07 0.0125 0.002 128 4000 0.2 3.259 0.8 0.9 0.9 0.98 0.99 1e-17 1e-12 0.004 0.008 256 1000 2000 3.291 3.265 3.259 3.260 3.260 3.266 3.265 3.265 3.308 3.265 3.277 3.260 0 3. 0 1 2 1 2 3 1 2 1 2 1 1 2 1 90 Table 98: Hyperparameter ablation for Sophia on 300m on 1x Chinchilla Data β1 0. 0.8 0.95 0.98 β2 ϵ γ η BSZ warmup λ Loss Link 0.9 1e-07 0.0125 0.004 128 4000 0.1 3.267 0.95 0.98 0.99 0.995 1e-17 1e-12 0.00625 0.025 0.05 0.002 0.008 0.016 0.032 256 500 1000 2000 3.288 7.390 7.525 3.270 3.275 3.280 3.280 3.289 3.289 3.273 3.271 3.274 3.276 6.966 7.142 6.811 3.298 7.149 7.093 7.382 0 3.288 0.2 3.274 0.3 3.281 1 2 3 1 2 3 4 1 2 1 2 3 1 2 3 4 1 1 2 3 1 2 3 Table 99: Hyperparameter ablation for Sophia on 520m on 1x Chinchilla Data β1 β2 ϵ γ η BSZ warmup λ Loss Link 0.95 0.9 1e-07 0.0125 0.002 128 4000 0.3 3.106 0.8 0.9 0.98 0.95 0.98 0.99 1e-17 1e-12 0.00625 0.025 0.004 1000 2000 3.125 3.109 3.133 3.111 3.111 6.571 3.116 3.116 3.107 3.107 6.940 6.908 6.823 0 3.148 0.1 3.113 0.2 3.105 0 1 2 3 1 2 3 1 2 1 2 1 1 2 1 2"
        },
        {
            "title": "D Hyperparameter Ablation in Phase II",
            "content": "We reported the results for the optimizers we swept in Phase II. The result is formulated in the same way as in Phase I. D.1 Sweeping Results for AdamW Table 100: Hyperparameter ablation for AdamW on 300m on 2x Chinchilla Data β1 β2 ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.9 0.98 1e-15 0.008 0.004 1 0 256 2000 0.1 3.166 3.167 3.170 0.2 3.183 1 2 3 Table 101: Hyperparameter ablation for AdamW on 300m on 4x Chinchilla Data β1 β2 ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.9 0.98 1e-10 0.008 0.004 2 0 256 128 2000 0.1 3.094 3.101 3.103 0.2 3.103 0 1 2 Table 102: Hyperparameter ablation for AdamW on 300m on 8x Chinchilla Data β1 β2 ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.9 0.98 1e-10 0.008 0.004 0 256 128 0.1 3.043 3.042 3.057 0.2 3.059 0 1 2 3 Table 103: Hyperparameter ablation for AdamW on 520m on 2x Chinchilla Data β1 β2 ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.9 0.98 1e-10 0. 1 0 256 128 1000 0.2 3. 6.654 0.1 3.025 0 1 2 Table 104: Hyperparameter ablation for AdamW on 520m on 4x Chinchilla Data β β2 ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.9 0.98 1e-10 0.004 0.008 1 128 1000 0.1 2.958 7.075 7.139 0.2 2.962 1 2 3 0 92 Table 105: Hyperparameter ablation for AdamW on 520m on 8x Chinchilla Data β β2 ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.9 0.98 1e-10 0.004 0.008 1 256 128 1000 0.1 2.913 7.183 6.932 0.2 2.921 0 1 2 3 D.2 Sweeping Results for Cautious Table 106: Hyperparameter ablation for Cautious on 300m on 2x Chinchilla Data β β2 ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.98 0.98 1e-25 0.008 0.004 1 256 128 2000 0.1 3.165 3.175 3.171 0 1 2 Table 107: Hyperparameter ablation for Cautious on 300m on 4x Chinchilla Data β1 β ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.98 0.98 1e-25 0.008 0.004 1 0 256 128 2000 0.1 3.094 3.098 3. 0 1 2 Table 108: Hyperparameter ablation for Cautious on 300m on 8x Chinchilla Data β1 β2 ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.98 0.98 1e-25 0.008 0.004 1 0 128 2000 0.1 3.043 3.041 3.071 1 2 Table 109: Hyperparameter ablation for Cautious on 520m on 2x Chinchilla Data β1 β2 ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.98 0.98 1e-25 0.008 0.004 1 0 256 2000 0.1 3.017 3.019 >10 0 1 Table 110: Hyperparameter ablation for Cautious on 520m on 4x Chinchilla Data β1 β2 ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.98 0.98 1e-25 0.004 0.008 256 128 2000 0.1 2.956 2.959 2.971 0 1 2 0 Table 111: Hyperparameter ablation for Cautious on 520m on 8x Chinchilla Data β1 β2 ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.98 0.98 1e-25 0.004 0.008 0 256 128 0.1 2.910 2.919 2.931 0 1 2 D.3 Sweeping Results for Lion Table 112: Hyperparameter ablation for Lion on 300m on 2x Chinchilla Data β1 β2 η gnorm ηmin BSZ warmup λ Loss Link 0.9 0.98 0. 0.9 0.95 0.0005 1 128 256 2000 0.7 3.170 3.189 3.175 3.172 3.183 0 1 2 3 4 Table 113: Hyperparameter ablation for Lion on 300m on 4x Chinchilla Data β1 β η gnorm ηmin BSZ warmup λ Loss Link 0.9 0.98 0.001 0.9 0.95 0.0005 1 0 256 2000 0.7 3.100 3.114 3.103 3.105 3.104 0 1 2 3 Table 114: Hyperparameter ablation for Lion on 300m on 8x Chinchilla Data β1 β2 η gnorm ηmin BSZ warmup λ Loss Link 0.9 0.98 0. 0.9 0.95 0.0005 1 256 128 2000 0.7 3.046 3.058 3.050 3.043 3.061 0 1 2 3 4 Table 115: Hyperparameter ablation for Lion on 520m on 2x Chinchilla Data β1 β η gnorm ηmin BSZ warmup λ Loss Link 0.9 0.95 0.001 0.9 0.98 0.0005 1 0 128 2000 0.6 3.029 3.045 7.779 3.028 3.030 0 1 2 3 94 Table 116: Hyperparameter ablation for Lion on 520m on 4x Chinchilla Data β1 β2 η gnorm ηmin BSZ warmup λ Loss Link 0.9 0.95 0.001 0.9 0.98 0.0005 1 0 256 128 2000 0.6 2. 2.971 2.965 2.966 2.975 0 1 2 3 4 Table 117: Hyperparameter ablation for Lion on 520m on 8x Chinchilla Data β β2 η gnorm ηmin BSZ warmup λ Loss Link 0.9 0.95 0.0005 0.9 0.98 0.001 1 0 128 2000 0.6 2.915 2.922 2.915 2.920 2.922 1 2 3 4 D.4 Sweeping Results for Mars Table 118: Hyperparameter ablation for Mars on 300m on 2x Chinchilla Data gnorm ηmin BSZ warmup λ Loss Link β1 β2 γ η ϵ 0.98 0.99 1e-25 0.05 0.008 0.9 0.95 0.004 1 0 256 1000 0.1 3.158 2000 4000 3.174 3.156 3.168 3.171 3.159 3.167 1 2 3 4 5 6 Table 119: Hyperparameter ablation for Mars on 300m on 4x Chinchilla Data gnorm ηmin BSZ warmup λ Loss Link β1 β2 γ η ϵ 0.98 0.99 1e-25 0.05 0.008 0.9 0.95 0.004 1 0 128 256 1000 0.1 3.097 2000 4000 3.111 3.096 3.095 3.098 3.097 3.101 0 1 2 3 4 5 95 Table 120: Hyperparameter ablation for Mars on 300m on 8x Chinchilla Data gnorm ηmin BSZ warmup λ Loss Link β1 β2 γ η ϵ 0.98 0.99 1e-25 0.05 0.008 0.9 0.95 0.004 1 0 256 128 1000 0.1 3.040 2000 4000 3.049 3.038 3.046 3.050 3.049 3.043 0 1 2 3 4 5 Table 121: Hyperparameter ablation for Mars on 520m on 2x Chinchilla Data gnorm ηmin BSZ warmup λ Loss Link β1 β2 γ η ϵ 0.95 0.98 1e-25 0.025 0. 0.9 0.98 0.004 0 256 128 0.1 3.015 1000 4000 3.019 3.014 3.019 3.025 3.023 3.019 0 1 2 3 4 5 6 Table 122: Hyperparameter ablation for Mars on 520m on 4x Chinchilla Data gnorm ηmin BSZ warmup λ Loss Link β β2 γ η ϵ 0.95 0.98 1e-25 0.025 0.008 0.9 0.98 0.004 1 0 256 128 2000 0.1 2. 1000 4000 2.960 2.953 2.953 2.974 2.964 2.956 0 1 2 3 4 5 6 Table 123: Hyperparameter ablation for Mars on 520m on 8x Chinchilla Data gnorm ηmin BSZ warmup λ Loss Link β1 β γ η ϵ 0.95 0.98 1e-25 0.025 0.004 0.9 0.98 0.008 1 256 128 2000 0.1 2.906 1000 2.908 2.906 2.917 2.916 2.906 2.907 0 1 2 3 4 5 6 96 D.5 Sweeping Results for NAdamW Table 124: Hyperparameter ablation for NAdamW on 300m on 2x Chinchilla Data β1 β2 ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.95 0.98 1e-10 0. 0.004 1 0 128 256 2000 0.1 3. 3.160 3.165 0 1 2 Table 125: Hyperparameter ablation for NAdamW on 300m on 4x Chinchilla Data β β2 ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.95 0.98 1e-10 0.008 0.004 1 256 128 2000 0.1 3.090 3.097 3.098 0 1 2 Table 126: Hyperparameter ablation for NAdamW on 300m on 8x Chinchilla Data β1 β ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.95 0.98 1e-10 0.008 0.004 1 0 256 128 2000 0.1 3.039 3.039 3. 0 1 2 Table 127: Hyperparameter ablation for NAdamW on 520m on 2x Chinchilla Data β1 β2 ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.98 0.98 1e-10 0.004 0.008 1 0 256 4000 0.1 3.013 3.023 3.020 1 2 Table 128: Hyperparameter ablation for NAdamW on 520m on 4x Chinchilla Data β1 β2 ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.98 0.98 1e-10 0.004 0.008 1 0 128 4000 0.1 2.955 2.971 2.954 0 1 Table 129: Hyperparameter ablation for NAdamW on 520m on 8x Chinchilla Data β1 β2 ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.98 0.98 1e-10 0.004 0.008 256 128 4000 0.1 2.907 2.910 2.913 0 1 2 0 D.6 Sweeping Results for Adam-Mini Table 130: Hyperparameter ablation for Adam-Mini on 300m on 2x Chinchilla Data β1 β2 ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.9 0.98 1e-25 0.004 0.002 2 0 128 256 2000 0.2 3.178 4000 3.180 6.960 3.183 0.1 3.179 0 1 2 3 Table 131: Hyperparameter ablation for Adam-Mini on 300m on 4x Chinchilla Data β1 β2 ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.9 0.98 1e-25 0.004 0.002 0 128 256 0.1 3.103 4000 3.111 3.109 3.104 0.2 3.111 0 1 2 3 4 Table 132: Hyperparameter ablation for Adam-Mini on 300m on 8x Chinchilla Data β1 β2 ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.9 0.98 1e-25 0. 0.004 2 0 128 256 2000 0.2 3. 4000 3.064 3.052 3.050 0.1 3.051 0 1 2 3 4 Table 133: Hyperparameter ablation for Adam-Mini on 520m on 2x Chinchilla Data β β2 ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.9 0.98 1e-10 0.004 0.002 1 128 256 4000 0.1 3.027 2000 3.031 3.032 7.359 0.2 3.037 0 1 2 3 4 98 Table 134: Hyperparameter ablation for Adam-Mini on 520m on 4x Chinchilla Data β β2 ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.9 0.98 1e-10 0.004 0.002 1 128 256 4000 0.1 2.966 2000 2.963 2.963 7.529 0.2 2.981 0 1 2 3 4 Table 135: Hyperparameter ablation for Adam-Mini on 520m on 8x Chinchilla Data β1 β ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.9 0.98 1e-10 0.004 0.002 1 0 256 128 4000 0.1 2.912 2000 2.918 2.921 7.449 0.2 3. 0 1 2 3 4 D.7 Sweeping Results for Muon ηadam β1 Table 136: Hyperparameter ablation for Muon on 300m on 2x Chinchilla Data β2 Decay(WSD) Schedule gnorm ηmin βmuon ϵmuon BSZ warmup λ Loss Link η ϵ 0.0024 0.8 0.98 0.8 1e-15 0.008 linear 0.004 1 0 0. 1e-05 128 256 0 0.1 3.143 3.144 3.145 0 1 2 ηadam β1 Table 137: Hyperparameter ablation for Muon on 300m on 4x Chinchilla Data β2 Decay(WSD) Schedule gnorm ηmin βmuon ϵmuon BSZ warmup λ Loss Link η ϵ 0.0012 0.8 0.98 0.8 1e-15 0.004 linear 0.008 1 0 0.98 1e-05 128 256 0.1 3.079 3.088 3.083 0 1 2 ηadam β Table 138: Hyperparameter ablation for Muon on 300m on 8x Chinchilla Data β2 Decay(WSD) Schedule gnorm ηmin βmuon ϵmuon BSZ warmup λ Loss Link η ϵ 0.0024 0.8 0.98 0.8 1e-15 0.008 linear 0.004 1 0.98 1e-05 256 0 0.1 3.032 3.029 3.049 0 1 99 ηadam β1 Table 139: Hyperparameter ablation for Muon on 520m on 2x Chinchilla Data β2 Decay(WSD) Schedule gnorm ηmin βmuon ϵmuon BSZ warmup λ Loss Link η ϵ 0.0012 0.8 0.98 1 1e-25 0.004 linear 0.008 0 0.98 1e-05 128 256 0 0.1 3.002 3.008 3. 0 1 2 ηadam β1 Table 140: Hyperparameter ablation for Muon on 520m on 4x Chinchilla Data β2 Decay(WSD) Schedule gnorm ηmin βmuon ϵmuon BSZ warmup λ Loss Link η ϵ 0.0024 0.8 0.98 1e-25 0.008 linear 0.004 2 0 0.98 1e-05 128 0 0.1 2. 2.944 2.963 0 1 2 ηadam β1 Table 141: Hyperparameter ablation for Muon on 520m on 8x Chinchilla Data β2 Decay(WSD) Schedule gnorm ηmin βmuon ϵmuon BSZ warmup λ Loss Link η ϵ 0.0024 0.8 0.98 1 1e-25 0.008 linear 0.004 2 0 0. 1e-05 256 128 0 0.1 2.906 2.900 2.930 0 1 2 D.8 Sweeping Results for Scion Table 142: Hyperparameter ablation for Scion on 300m on 2x Chinchilla Data ηadam β1 Decay(WSD) η Schedule gnorm ηmin βmuon ϵscion BSZ warmup λ Loss Link 0.0008 0.98 0.8 0.008 linear 0.004 2 0 0.95 1e-05 256 0 0.1 3. 3.153 3.154 0 1 2 Table 143: Hyperparameter ablation for Scion on 300m on 4x Chinchilla Data ηadam β1 Decay(WSD) η Schedule gnorm ηmin βmuon ϵscion BSZ warmup λ Loss Link 0.0008 0.98 1 0.008 linear 0.004 2 0 0.95 1e-05 256 128 0 0.1 3.086 3.099 3.090 1 2 Table 144: Hyperparameter ablation for Scion on 300m on 8x Chinchilla Data ηadam β1 Decay(WSD) η Schedule gnorm ηmin βmuon ϵscion BSZ warmup λ Loss Link 0.0004 0. 0.8 0.004 linear 0.008 2 0 0.95 1e-05 128 256 0.1 3.039 3.057 3.037 0 1 2 Table 145: Hyperparameter ablation for Scion on 520m on 2x Chinchilla Data ηadam β1 Decay(WSD) η Schedule gnorm ηmin βmuon ϵscion BSZ warmup λ Loss Link 0.0004 0.98 1 0.004 linear 0.008 2 0 0. 1e-05 128 256 0 0.1 3.007 3.015 3.020 0 1 2 Table 146: Hyperparameter ablation for Scion on 520m on 4x Chinchilla Data ηadam β1 Decay(WSD) η Schedule gnorm ηmin βmuon ϵscion BSZ warmup λ Loss Link 0.0008 0.98 0.008 linear 0.004 0 0.95 1e-05 256 128 0 0.1 2.952 2.952 2. 0 1 2 Table 147: Hyperparameter ablation for Scion on 520m on 8x Chinchilla Data ηadam β1 Decay(WSD) η Schedule gnorm ηmin βmuon ϵscion BSZ warmup λ Loss Link 0.0004 0.98 1 0. linear 0.008 2 0.95 1e-05 256 0 0.1 2.904 2.913 2.913 0 1 D.9 Sweeping Results for Kron Table 148: Hyperparameter ablation for Kron on 300m on 2x Chinchilla Data β1 blocksize η gnorm ηmin NormGrad Blocking Initpc ηpc ppc BSZ Steppc warmup λ Loss Link 0. 256 0.001 0.0005 0 True True 1 0.2 0.1 128 2000 0.7 3.151 3.157 0 1 Table 149: Hyperparameter ablation for Kron on 300m on 4x Chinchilla Data β1 blocksize η gnorm ηmin NormGrad Blocking Initpc ηpc ppc BSZ Steppc warmup λ Loss Link 0.95 256 0.0005 0.001 1 0 True True 1 0.2 0.1 2000 1000 0.7 3.083 3.090 0 1 Table 150: Hyperparameter ablation for Kron on 300m on 8x Chinchilla Data β1 blocksize η gnorm ηmin NormGrad Blocking Initpc ηpc ppc BSZ Steppc warmup λ Loss Link 0.95 256 0. 0.001 1 True True 0.2 0.1 128 2000 1000 0.7 3.031 3.074 1 101 Table 151: Hyperparameter ablation for Kron on 520m on 2x Chinchilla Data β1 blocksize η gnorm ηmin NormGrad Blocking Initpc ηpc ppc BSZ Steppc warmup λ Loss Link 0.95 256 0.0005 0. 1 0 True True 1 0.2 0.1 128 1000 0.5 3.009 3.009 0 1 Table 152: Hyperparameter ablation for Kron on 520m on 4x Chinchilla Data β1 blocksize η gnorm ηmin NormGrad Blocking Initpc ηpc ppc BSZ Steppc warmup λ Loss Link 0.95 256 0.0005 0.001 1 0 True True 1 0.2 0.1 128 2000 1000 0.5 2.946 2.950 0 Table 153: Hyperparameter ablation for Kron on 520m on 8x Chinchilla Data β1 blocksize η gnorm ηmin NormGrad Blocking Initpc ηpc ppc BSZ Steppc warmup λ Loss Link 0.95 0.0005 0.001 1 0 True True 1 0.2 0.1 128 2000 1000 0.5 2. 256 2.909 2. 0 1 2 D.10 Sweeping Results for Soap Table 154: Hyperparameter ablation for Soap on 300m on 2x Chinchilla Data β1 β2 blocksize ϵ η gnorm ηmin Blocking fpc βshampoo BSZ warmup λ Loss Link 0.95 0.99 512 128 256 1e-10 0.008 0.004 0 True 10 0. 128 256 0.1 3.147 3.154 3.150 3.147 3.153 0 1 2 3 4 Table 155: Hyperparameter ablation for Soap on 300m on 4x Chinchilla Data β1 β2 blocksize ϵ η gnorm ηmin Blocking fpc βshampoo BSZ warmup λ Loss Link 0.95 0. 512 128 256 1e-10 0.008 0.004 1 0 True 10 0.9 128 1000 0.1 3.084 3.086 3.084 3.086 3.091 1 2 3 4 102 Table 156: Hyperparameter ablation for Soap on 300m on 8x Chinchilla Data β1 β2 blocksize ϵ η gnorm ηmin Blocking fpc βshampoo BSZ warmup λ Loss Link 0.95 0.99 128 256 1e-10 0.008 0.004 1 0 True 10 0.9 256 128 1000 0.1 3. 3.034 3.032 3.031 3.043 0 1 2 3 4 Table 157: Hyperparameter ablation for Soap on 520m on 2x Chinchilla Data β β2 blocksize ϵ η gnorm ηmin Blocking fpc βshampoo BSZ warmup λ Loss Link 0.95 0.99 512 128 256 1e-10 0.008 0.004 1 0 True 0.95 256 1000 0.1 3.004 3.013 3.010 3.008 3.011 0 1 2 3 Table 158: Hyperparameter ablation for Soap on 520m on 4x Chinchilla Data β1 β2 blocksize ϵ η gnorm ηmin Blocking fpc βshampoo BSZ warmup λ Loss Link 0.95 0.99 512 128 256 1e-10 0. 0.008 1 0 True 10 0.95 256 128 1000 0.1 2.944 2.948 2.945 2.949 2. 0 1 2 3 4 Table 159: Hyperparameter ablation for Soap on 520m on 8x Chinchilla Data β1 β2 blocksize ϵ η gnorm ηmin Blocking fpc βshampoo BSZ warmup λ Loss Link 0.95 0.99 512 1e-10 0.004 0.008 1 0 True 10 0.95 256 1000 0.1 2.899 2.906 0 1 Hyperparameter Ablation in Phase III 1.2B experiments We reported the results for the optimizers we swept in Phase III. The result is formulated in the same way as in Phase I. 103 E.1 Sweeping Results for AdamW Table 160: Hyperparameter ablation for AdamW on 1.2b on 1x Chinchilla Data β1 β2 ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.9 0.98 1e-10 0.002 2 0.95 0.98 0.9 0.95 1e-25 1e-20 1e-15 0.004 0.008 0 1.0 0 2000 0.2 2.905 128 512 1024 500 1000 4000 2.905 2.909 2.914 2.909 2.907 2.907 2.907 2.916 7.347 2.909 2.908 2.904 2.928 2.985 2.917 2.910 2.912 0 2.946 0.1 2.916 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 Table 161: Hyperparameter ablation for AdamW on 1.2b on 2x Chinchilla Data β1 β2 ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.9 0.98 1e-10 0.002 1 0 256 0.2 2.836 0 Table 162: Hyperparameter ablation for AdamW on 1.2b on 4x Chinchilla Data β1 β2 ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.9 0.98 1e-10 0.002 1 0 1000 0.2 2.787 0 Table 163: Hyperparameter ablation for AdamW on 1.2b on 8x Chinchilla Data β1 β ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.9 0.98 1e-10 0.002 1 256 1000 0.2 2.752 0 104 E.2 Sweeping Results for NAdamW Table 164: Hyperparameter ablation for NAdamW on 1.2b on 1x Chinchilla Data β1 β2 ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.98 0.98 1e-10 0.004 1 0 256 4000 0.1 2. 0 Table 165: Hyperparameter ablation for NAdamW on 1.2b on 2x Chinchilla Data β1 β2 ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.98 0.98 1e-10 0.004 1 0 256 0.1 2.833 0 Table 166: Hyperparameter ablation for NAdamW on 1.2b on 4x Chinchilla Data β1 β2 ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.98 0.98 1e-10 0.004 1 0 4000 0.1 2.785 0 Table 167: Hyperparameter ablation for NAdamW on 1.2b on 8x Chinchilla Data β1 β ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.98 0.98 1e-10 0.004 1 256 4000 0.1 2.749 0 E.3 Sweeping Results for Soap Table 168: Hyperparameter ablation for Soap on 1.2b on 1x Chinchilla Data β1 β2 blocksize ϵ η gnorm ηmin Blocking fpc βshampoo BSZ warmup λ Loss Link 0.95 0. 512 1e-10 0.004 1 0 True 0.9 256 1000 0.1 2.940 0 Table 169: Hyperparameter ablation for Soap on 1.2b on 2x Chinchilla Data β1 β2 blocksize ϵ η gnorm ηmin Blocking fpc βshampoo BSZ warmup λ Loss Link 0.95 0. 512 1e-10 0.004 1 0 True 0.9 256 1000 0.1 2.829 0 Table 170: Hyperparameter ablation for Soap on 1.2b on 4x Chinchilla Data β1 β2 blocksize ϵ η gnorm ηmin Blocking fpc βshampoo BSZ warmup λ Loss Link 0.95 0. 512 1e-10 0.004 1 0 True 0.9 256 1000 0.1 2.783 0 Table 171: Hyperparameter ablation for Soap on 1.2b on 8x Chinchilla Data β1 β2 blocksize ϵ η gnorm ηmin Blocking fpc βshampoo BSZ warmup λ Loss Link 0.95 0.99 512 1e-10 0.004 1 0 True 10 0.9 256 1000 0.1 2.749 E.4 Sweeping Results for Muon ηadam β1 Table 172: Hyperparameter ablation for Muon on 1.2b on 1x Chinchilla Data β2 Decay(WSD) η ϵ Schedule gnorm ηmin βmuon ϵmuon BSZ warmup λ Loss Link 0.0012 0.8 0.98 1 1e-15 0.004 linear 0.008 0 0.98 1e-05 256 0 0.1 2.891 2. 0 1 ηadam β1 Table 173: Hyperparameter ablation for Muon on 1.2b on 2x Chinchilla Data β2 Decay(WSD) η ϵ Schedule gnorm ηmin βmuon ϵmuon BSZ warmup λ Loss Link 0.0012 0.8 0.98 1e-15 0.004 linear 0.008 2 0 0.98 1e-05 0 0.1 2. 2.833 0 1 ηadam β1 Table 174: Hyperparameter ablation for Muon on 1.2b on 4x Chinchilla Data β2 Decay(WSD) η ϵ Schedule gnorm ηmin βmuon ϵmuon BSZ warmup λ Loss Link 0.0012 0.8 0.98 1 1e-15 0.004 linear 0. 2 0 0. 1e-05 256 0 0.1 2.780 2.793 0 1 ηadam β1 Table 175: Hyperparameter ablation for Muon on 1.2b on 8x Chinchilla Data β2 Decay(WSD) η ϵ Schedule gnorm ηmin βmuon ϵmuon BSZ warmup λ Loss Link 0.0012 0.8 0.98 1 1e-15 0. linear 2 0 0.98 1e-05 256 0.1 2.748 0 Hyperparameter Ablation in Phase III 16x Chinchilla experiments F.1 Sweeping Results for AdamW Table 176: Hyperparameter ablation for AdamW on 130m on 16x Chinchilla Data β β2 ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.9 0.98 1e-10 0.008 0 256 1000 0.1 3.207 0 Table 177: Hyperparameter ablation for AdamW on 300m on 16x Chinchilla Data β1 β2 ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.9 0.98 1e-10 0.004 2 0 256 2000 0.1 3. 0 F.2 Sweeping Results for NAdamW Table 178: Hyperparameter ablation for NAdamW on 130m on 16x Chinchilla Data β1 β2 ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.95 0.98 1e-10 0.008 1 0 2000 0.1 3.200 0 Table 179: Hyperparameter ablation for NAdamW on 300m on 16x Chinchilla Data β1 β ϵ η gnorm ηmin BSZ warmup λ Loss Link 0.95 0.98 1e-10 0.004 1 256 2000 0.1 2.998 0 F.3 Sweeping Results for Soap Table 180: Hyperparameter ablation for Soap on 130m on 16x Chinchilla Data β1 β2 blocksize ϵ η gnorm ηmin Blocking fpc βshampoo BSZ warmup λ Loss Link 0.95 0. 512 1e-10 0.008 1 0 True 0.98 256 1000 0.1 3.191 0 Table 181: Hyperparameter ablation for Soap on 300m on 16x Chinchilla Data β1 β2 blocksize ϵ η gnorm ηmin Blocking fpc βshampoo BSZ warmup λ Loss Link 0.95 0. 512 1e-10 0.004 1 0 True 0.9 256 1000 0.1 2.990 0 F.4 Sweeping Results for Muon ηadam β1 Table 182: Hyperparameter ablation for Muon on 130m on 16x Chinchilla Data β2 Decay(WSD) Schedule gnorm ηmin βmuon ϵmuon BSZ warmup λ Loss Link η ϵ 0.0012 0.8 0. 1 1e-25 0. linear 0.008 1 0 0.98 1e-05 128 0 0.1 3.192 3.202 1 ηadam β1 Table 183: Hyperparameter ablation for Muon on 300m on 16x Chinchilla Data β2 Decay(WSD) Schedule gnorm ηmin βmuon ϵmuon BSZ warmup λ Loss Link η ϵ 0.0012 0.8 0.98 0.8 1e-15 0.004 linear 1 0.98 1e-05 256 0 0.1 2.991"
        },
        {
            "title": "G Comparison with Prior Work",
            "content": "This paper benchmarks the performance of 11 proposed optimizers and show vastly different speed-up ratio than prior works reported. In this section, we will compare the setup of our experiments with prior works with the hope of understanding the difference. 1. Sophia Liu et al. [2024a] (2) This work utilizes small peak learning rate of learning rate smaller than 6e-4 (similar to the one shown in Figure 1 Top Left). The reason for the small peak learning rate is likely 2-fold: (i) the authors are training on pretraining dataset PILE with lower quality compared to the current pretraining dataset and (ii) in the implementation of Levanter that the authors used, the data shuffling is not completely random and instead is correlated on every compute node. Upon reproducing the results, we note that this difference can significantly impact the stability of the training process and complete random shuffling is crucial for the usage of large learning rate. 2. MARS Yuan et al. [2025] (2) This papers considers similar setup as Sophia and uses similar AdamW baseline. We note that in the first version of the paper, the authors also reported that increasing the learning rate of AdamW to 3e-3 can significantly improve the performance of AdamW (see Figure 6 of Yuan et al. [2025] arxiv version 1). 3. Soap Vyas et al. [2025] (1.4) The actual speedup of Soap on 300M and 520M models are 1.2 to 1.3, which is only slightly lower than the claimed 1.4 speedup. We note that our implementation of Soap is slightly different from the one used in the paper that we performs blocking of weight in order to reduce the memory footprint and further uses bfloat16 for the momentum in the 1.2B experiments. Both modifications may lead to slightly lower step-wise performance. 4. Muon Jordan et al. [2024], Liu et al. [2025a] (2) The speedup of Muon reported in different works are vastly different. In the original Nanogpt speedrun, Muon achieves 1.3 speedup over AdamW. Later the reproduction of Kimi reported much higher speedup of 2. We note that the Kimi version utilizes notably low learning rate for AdamW (8e-4 to 9e-4 for model between 400M to 1.5B) in the scaling experiments. We also note that the smaller learning rate is important for hyperparameter transfer from AdamW when roughly matching the update norm and AdamW can perform better higher learning rate in our experiments. Further, their comparison of Muon and AdamW on the MoE experiments compare two models with not fully decayed learning rate and this may significantly favors Muon, as shown in Figure 5. It is later shown independently in the work of Essential AI AI et al. [2025] that Muons token efficiency compared to AdamW is only 1.1 to 1.2. 5. Cautious Liang et al. [2025], Block-wise Learning Rate Adam Wang et al. [2025], FOCUS Liu et al. [2025c] report 2 speedup over AdamW. These papers use similar baseline as Sophia and MARS. 6. SWAN Ma et al. [2025] and DION Ahn et al. [2025] report 2-3 speedup over AdamW. The comparison between these two optimizers and AdamW is carried out on smaller than 1 Chinchilla regime, where the speed-up of matrix-based optimizer may be larger. We also note that we didnt consider the communication cost, which is the main focus of DION. 7. SPlus Frans et al. [2025] reports 2 speedup over AdamW. This work considers an atypical setup where the model is trained with constant learning rate."
        }
    ],
    "affiliations": [
        "Stanford University"
    ]
}