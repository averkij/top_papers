{
    "paper_title": "The Principles of Diffusion Models",
    "authors": [
        "Chieh-Hsin Lai",
        "Yang Song",
        "Dongjun Kim",
        "Yuki Mitsufuji",
        "Stefano Ermon"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This monograph presents the core principles that have guided the development of diffusion models, tracing their origins and showing how diverse formulations arise from shared mathematical ideas. Diffusion modeling starts by defining a forward process that gradually corrupts data into noise, linking the data distribution to a simple prior through a continuum of intermediate distributions. The goal is to learn a reverse process that transforms noise back into data while recovering the same intermediates. We describe three complementary views. The variational view, inspired by variational autoencoders, sees diffusion as learning to remove noise step by step. The score-based view, rooted in energy-based modeling, learns the gradient of the evolving data distribution, indicating how to nudge samples toward more likely regions. The flow-based view, related to normalizing flows, treats generation as following a smooth path that moves samples from noise to data under a learned velocity field. These perspectives share a common backbone: a time-dependent velocity field whose flow transports a simple prior to the data. Sampling then amounts to solving a differential equation that evolves noise into data along a continuous trajectory. On this foundation, the monograph discusses guidance for controllable generation, efficient numerical solvers, and diffusion-motivated flow-map models that learn direct mappings between arbitrary times. It provides a conceptual and mathematically grounded understanding of diffusion models for readers with basic deep-learning knowledge."
        },
        {
            "title": "Start",
            "content": "Chieh-Hsin Lai Sony AI"
        },
        {
            "title": "Dongjun Kim\nStanford University",
            "content": "Yuki Mitsufuji Sony Corporation, Sony AI"
        },
        {
            "title": "Stefano Ermon\nStanford University",
            "content": "5 2 0 2 4 2 ] . [ 1 0 9 8 1 2 . 0 1 5 2 : r a"
        },
        {
            "title": "Contents",
            "content": "Acknowledgements Introduction to Deep Generative Modeling 1 Deep Generative Modeling 1.1 What is Deep Generative Modeling? . . . . . . . . . . . . . . . 1.2 Prominent Deep Generative Models . . . . . . . . . . . . . . . 1.3 Taxonomy of Modelings . . . . . . . . . . . . . . . . . . . . . . 1.4 Closing Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . Origins and Foundations of Diffusion Models 2 Variational Perspective: From VAEs to DDPMs 2.1 Variational Autoencoder . . . . . . . . . . . . . . . . . . . . . . 2.2 Variational Perspective: DDPM . . . . . . . . . . . . . . . . . . 2.3 Closing Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Score-Based Perspective: From EBMs to NCSN 3.1 Energy-Based Models . . . . . . . . . . . . . . . . . . . . . . . 3.2 From Energy-Based to Score-Based Generative Models . . . . . 3.3 Denoising Score Matching . . . . . . . . . . . . . . . . . . . . . 3.4 Multi-Noise Levels of Denoising Score Matching (NCSN) . . . . 3.5 Summary: Comparative View of NCSN and DDPM . . . . . . 3 14 15 16 22 26 30 32 33 43 55 56 57 64 68 79 84 3.6 Closing Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . 85 4 Diffusion Models Today: Score SDE Framework 4.1 Score SDE: Its Principles . . . . . . . . . . . . . . . . . . . . . 4.2 Score SDE: Its Training and Sampling . . . . . . . . . . . . . . Instantiations of SDEs . . . . . . . . . . . . . . . . . . . . . . . 4.3 4.4 (Optional) Rethinking Forward Kernels in Score-Based and Variational Diffusion Models . . . . . . . . . . . . . . . . . . . . . . 4.5 (Optional) FokkerPlanck Equation and Reverse-Time SDEs 86 87 105 110 115 via Marginalization and Bayes Rule . . . . . . . . . . . . . . . 4.6 Closing Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . 121 126 5 Flow-Based Perspective: From NFs to Flow Matching 127 129 5.1 Flow-Based Models: Normalizing Flows and Neural ODEs . . . . 5.2 Flow Matching Framework . . . . . . . . . . . . . . . . . . . . 136 5.3 Constructing Probability Paths and Velocities Between Distributions148 159 5.4 (Optional) Properties of the Canonical Affine Flow . . . . . . . 165 5.5 Closing Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . 6 Unified and Systematic Lens on Diffusion Models 6.1 Conditional Tricks: The Secret Sauce of Diffusion Models . . . . 6.2 Roadmap for Elucidating Training Losses in Diffusion Models 6.3 Equivalence in Diffusion Models . . . . . . . . . . . . . . . . . 6.4 Beneath It All: The FokkerPlanck Equation . . . . . . . . . . . 6.5 Closing Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . 7 (Optional) Diffusion Models and Optimal Transport 7.1 Prologue of Distribution-to-Distribution Translation . . . . . . . 7.2 Taxonomy of the Problem Setups . . . . . . . . . . . . . . . . . 7.3 Relationship of Variant Optimal Transport Formulations . . . . . . . Is Diffusion Models SDE Optimal Solution to SB Problem? 7.4 Is Diffusion Models ODE an Optimal Map to OT Problem? . . 7.5 Sampling of Diffusion Models 8 Guidance and Controllable Generation 8.1 Prologue . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8.2 Classifier Guidance . . . . . . . . . . . . . . . . . . . . . . . . . 8.3 Classifier-Free Guidance . . . . . . . . . . . . . . . . . . . . . . 166 168 170 175 186 190 191 192 194 206 212 224 226 227 232 235 8.4 (Optional) Training-Free Guidance . . . . . . . . . . . . . . . . 8.5 From Reinforcement Learning to Direct Preference Optimization . . . . . . . . . . . . . . . . . . . . . . . 8.6 Closing Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . for Model Alignment 9 Sophisticated Solvers for Fast Sampling 9.1 Prologue . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9.2 DDIM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9.3 DEIS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9.4 DPM-Solver . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9.5 DPM-Solver++ . . . . . . . . . . . . . . . . . . . . . . . . . . . 9.6 PF-ODE Solver Families and Their Numerical Analogues . . . . 9.7 (Optional) DPM-Solver-v3 . . . . . . . . . . . . . . . . . . . . 9.8 (Optional) ParaDiGMs . . . . . . . . . . . . . . . . . . . . . . 9.9 Closing Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . Toward Learning Fast Diffusion-Based Generators 10 Distillation-Based Methods for Fast Sampling 10.1 Prologue . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10.2 Distribution-Based Distillation . . . . . . . . . . . . . . . . . . 10.3 Progressive Distillation . . . . . . . . . . . . . . . . . . . . . . 10.4 Closing Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . 11 Learning Fast Generators from Scratch 11.1 Prologue . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11.2 Special Flow Map: Consistency Model in Discrete Time . . . . . 11.3 Special Flow Map: Consistency Model in Continuous Time . . . . . . . . . . . 11.4 General Flow Map: Consistency Trajectory Model 11.5 General Flow Map: Mean Flow . . . . . . . . . . . . . . . . . . 11.6 Closing Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . Appendices Crash Course on Differential Equations A.1 Foundation of Ordinary Differential Equations . . . . . . . . . . A.2 Foundation of Stochastic Differential Equations . . . . . . . . . 238 243 253 254 255 263 275 282 295 301 304 315 321 323 324 329 334 340 341 343 348 356 365 375 380 381 382 383 394 Density Evolution: From Change of Variable to FokkerPlanck B.1 Change-of-Variable Formula: From Deterministic Maps to Stochastic Flows . . . . . . . . . . B.2 Intuition of the Continuity Equation . . . . . . . . . . . . . . . 399 409 Behind the Scenes of Diffusion Models: Itôs Calculus and Girsanovs Theorem C.1 Itôs Formula: The Chain Rule for Random Processes . . . . . . C.2 Change-of-Variable For Measures: Girsanovs Theorem in Diffusion Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Supplementary Materials and Proofs . . . . . . . . . . . . . . . . . . . . . . D.1 Variational Perspective D.2 Score-Based Perspective . . . . . . . . . . . . . . . . . . . . . . D.3 Flow-Based Perspective . . . . . . . . . . . . . . . . . . . . . . D.4 Theoretical Supplement: Unified and Systematic View on Diffu- . . . . . . . . . . . . . . . . . . . . . . . . . . . . 445 D.5 Theoretical Supplement: Learning Fast Diffusion-Based Generators 446 450 D.6 (Optional) Elucidating Diffusion Model (EDM) . . . . . . . . . sion Models 412 413 422 426 426 430 References 454 The Principles of Diffusion Models Chieh-Hsin Lai1, Yang Song2, Dongjun Kim3, Yuki Mitsufuji4 and Stefano Ermon5 1Sony AI; chieh-hsin.lai@sony.com / chiehhsinlai@gmail.com 2OpenAI; thusongyang@gmail.com 3Stanford University; dongjun@stanford.edu 4Sony Corporation, Sony AI; yuhki.mitsufuji@sony.com 5Stanford University; ermon@cs.stanford.edu ABSTRACT This monograph focuses on the principles that have shaped the development of diffusion models, tracing their origins and showing how different formulations arise from common mathematical ideas. Diffusion modeling begins by specifying forward corruption process that gradually turns data into noise. This forward process links the data distribution to simple noise distribution by defining continuous family of intermediate distributions. The core objective of diffusion model is to construct another process that runs in the opposite direction, transforming noise into data while recovering the same intermediate distributions defined by the forward corruption process. We describe three complementary ways to formalize this idea. The variational view, inspired by variational autoencoders, sees diffusion as learning to remove noise step by step, solving small denoising objectives that together teach the model to turn noise back into data. The score-based view, rooted in energy-based modeling, learns the gradient of the evolving data distribution, which indicates how to nudge samples toward more likely regions. The flow-based view, related to normalizing flows, treats generation as following smooth path that moves samples from noise to data under learned velocity field. These perspectives share common backbone: learned timedependent velocity field whose flow transports simple prior to the Affiliation reflects the institution at the time of the work. 2 data. With this in hand, sampling amounts to solving differential equation that evolves noise into data along continuous generative trajectory. On this foundation, the monograph discusses guidance for controllable generation, advanced numerical solvers for efficient sampling, and diffusion-motivated flow-map models that learn direct mappings between arbitrary times along this trajectory. This monograph is written for readers with basic deep learning background who seek clear, conceptual, and mathematically grounded understanding of diffusion models. It clarifies the theoretical foundations, explains the reasoning behind their diverse formulations, and provides stable footing for further study and research in this rapidly evolving field. It serves both as principled reference for researchers and as an accessible entry point for learners."
        },
        {
            "title": "Acknowledgements",
            "content": "The authors are deeply grateful to Professor Dohyun Kwon from the University of Seoul and KIAS for his generous time and effort in engaging with this work. He carefully reviewed parts of Chapter 7, helping to ensure the correctness of statements and proofs, and he contributed to several valuable discussions that clarified the presentation. Beyond technical suggestions, his thoughtful feedback and willingness to share perspectives have been source of encouragement throughout the writing of this monograph. We sincerely appreciate his support and collegial spirit, which have enriched the final version."
        },
        {
            "title": "Preface and Roadmap",
            "content": "Diffusion models have rapidly become central paradigm in generative modeling, with vast body of work spanning machine learning, computer vision, natural language processing, and beyond. This literature is dispersed across communities and highlights different dimensions of progress, including theoretical foundations that concern modeling principles, training objectives, sampler design, and the mathematical ideas behind them; implementation advances that cover engineering practices and architectural choices; practical applications that adapt the models to specific domains or tasks; and system level optimizations that improve efficiency in computation, memory, and deployment. This monograph sets out to provide principled foundation of diffusion models, focusing on the following central themes: We present the essential concepts and formulations that anchor diffusion model research, giving readers the core understanding needed to navigate the broader literature. We do not survey all variants or domain specific applications; instead we establish stable conceptual foundation from which such developments can be understood. Unlike classical generative models that learn direct mapping from noise to data, diffusion models view generation as gradual transformation over time, refining coarse structures into fine details. This central idea has been developed through three main perspectives, i.e., variational, score-based, and flow-based methods, which offer complementary ways to understand and implement diffusion modeling. We focus on the core principles and foundations of these formulations, aiming to trace the Preface 5 origins of their key ideas, clarify the relations among different formulations, and develop coherent understanding that connects intuitive insight with rigorous mathematical formulation. Building on these foundations, we examine how diffusion models can be further developed to generate samples more efficiently, provide greater control over the generative process, and inspire standalone forms of generative modeling grounded in the principles of diffusion. This monograph is intended for researchers, graduate students, and practitioners who have basic understanding of deep learning (for example, what neural network is and how training works), or more specifically, deep generative modeling, and who wish to deepen their grasp of diffusion models beyond surface-level familiarity. By the end, readers will have principled understanding of the foundations of diffusion modeling, the ability to interpret different formulations within coherent framework, and the background needed to both apply existing models with confidence and pursue new research directions."
        },
        {
            "title": "Roadmap of This Monograph",
            "content": "This monograph systematically introduces the foundations of diffusion models, tracing them back to their core underlying principles. Suggested Reading Path. We recommend reading this monograph in the presented order to build comprehensive understanding. Sections marked as Optional can be skipped by readers already familiar with the fundamentals. For instance, those comfortable with deep generative models (DGM) may bypass the overview in Chapter 1. Similarly, prior knowledge of Variational Autoencoders (Section 2.1), Energy-Based Models (Section 3.1), or Normalizing Flows (Section 5.1) allows skipping these introductory sections. Other optional parts provide deeper insights into advanced or specialized topics and can be consulted as needed. The monograph is organized into four main parts. Parts & B: Foundations of Diffusion Models. This section traces the origins of diffusion models by reviewing three foundational perspectives that have shaped the field. Figure 2 provides an overview of this part. 6 Preface Part A: Introduction to Deep Generative Modeling (DGM). We begin in Chapter 1 with review of the fundamental goals of deep generative modeling. Starting from collection of data examples, the aim is to build model that can produce new examples that appear to come from the same underlying, and generally unknown, data distribution. Many approaches achieve this by learning how the data are distributed, either explicitly through probability model or implicitly through learned transformation. We then explain how such models represent the data distribution with neural networks, how they learn from examples, and how they generate new samples. The chapter concludes with taxonomy of major generative frameworks, highlighting their central ideas and key distinctions. 1 0 / 5 8 9 1 2 1 / 3 1 0 2 2 1 / 4 1 0 2 5 0 / 5 1 0 2 6 0 / 8 1 0 7 0 / 9 1 0 2 6 0 / 0 2 0 2 EBM VAE NF DPM NODE NCSN DDPM 1 1 / 0 2 0 2 SDE Score 0 1 / 2 2 0 2 FM Figure 1: Timeline of diffusion model perspectives. Each group shares the same color. In Chapter 2, Variational Autoencoder (VAE) (Kingma and Welling, 2013) Diffusion Probabilistic Models (DPM) (Sohl-Dickstein et al., 2015) DDPM (Ho et al., 2020). In Chapters 3 and 4, Energy-Based Model (EBM) (Ackley et al., 1985) Noise Conditional Score Network (NCSN) (Song and Ermon, 2019) Score SDE (Song et al., 2020c). In Chapter 5, Normalizing Flow (NF) (Rezende and Mohamed, 2015) Neural ODE (NODE) (Chen et al., 2018) Flow Matching (FM) (Lipman et al., 2022). Part B: Core Perspectives on Diffusion Models. Having outlined the general goals and mechanisms of deep generative modeling, we now turn to diffusion models, class of methods that realize generation as gradual transformation from noise to data. We examine three interconnected frameworks, each characterized by forward process that gradually adds noise and reverse-time process approximated by sequence of models performing gradual denoising: Variational View (Chapter 2): Originating from Variational Autoencoders (VAEs) (Kingma and Welling, 2013), it frames diffusion as learning denoising process through variational objective, giving rise to Denoising Diffusion Probabilistic Models (DDPMs) (Sohl-Dickstein et al., 2015; Ho et al., 2020). Score-Based View (Chapter 3): Rooted in Energy-Based Models (EBMs) (Ackley et al., 1985) and developed into Noise Conditional Score Networks Preface 7 (NCSN) (Song and Ermon, 2019). It learns the score function, the gradient of the log data density, which guides how to gradually remove noise from samples. In continuous time, Chapter 4 introduces the Score SDE framework, which describes this denoising process as Stochastic Differential Equation (SDE) and its deterministic counterpart as an Ordinary Differential Equation (ODE). This view connects diffusion modeling with classical differential equation theory, providing clear mathematical basis for analysis and algorithm design. Flow-Based View (Chapter 5): Building on Normalizing Flows (Rezende and Mohamed, 2015) and generalized by Flow Matching (Lipman et al., 2022), this view models generation as continuous transformation that transports samples from simple prior toward the data distribution. The evolution is governed by velocity field through an ODE, which explicitly defines how probability mass moves over time. This flow-based formulation naturally extends beyond prior-to-data generation to more general distribution-to-distribution translation problems, where one seeks to learn flow connecting any pair of source and target distributions. Although these perspectives may seem different at first, Chapter 6 shows that they are deeply connected. Each uses conditioning strategy that turns the learning objective into tractable regression problem. At deeper level, they all describe the same temporal evolution of probability distributions, from the prior toward the data. This evolution is governed by the FokkerPlanck equation, which can be viewed as the continuous-time change of variables for densities, ensuring consistency between the stochastic and deterministic formulations. Since diffusion models can be viewed as approaches for transporting one distribution to another, Chapter 7 develops their connections to classical optimal transport and the Schrödinger bridge, interpreted as optimal transport with entropy regularization. We review both the static and dynamic formulations and explain their relations to the continuity equation and the FokkerPlanck perspective. This chapter is optional for readers focused on practical aspects, but it provides rigorous mathematical background and pointers to the classical literature for those who wish to study these links in depth. Part & D: Controlling and Accelerating the Diffusion Sampling. With the foundational principles unified, we now turn to practical aspects of utilizing diffusion models for efficient generation. Sampling from diffusion model corresponds to solving differential equation. However, this procedure is 8 Preface Figure 2: Part B. Unifying and Principled Perspectives on Diffusion Models. This diagram visually connects classical generative modeling approachesVariational Autoencoders, Energy-Based Models, and Normalizing Flowswith their corresponding diffusion model formulations. Each vertical path illustrates conceptual lineage, culminating in the continuoustime framework. The three views (Variational, Score-Based, and Flow-Based) offer distinct yet mathematically equivalent interpretations. Chapter 1 Overview of Deep Generative Modeling i p P i e n u ff i Variational View Score-Based View Flow-Based View Variational Autoencoder t C Energy-Based Model Normalizing Flows 3 p r a Denoising Diffusion Probabilistic Model (DDPM) Noise Conditional Score Network (NCSN) Gaussian Flow Matching Continuous-Time Formulation (e.g., Score SDE) Chapter 4 Unifying Principles Chapter 6 Conditional Strategy Fokker-Planck Equation Preface 9 typically computationally expensive. Parts and focus on improving generation quality, controllability, and efficiency through enhanced sampling and learned acceleration techniques. Part C: Sampling from Diffusion Models. The generation process of diffusion models exhibits distinctive coarse-to-fine refinement: noise is removed step by step, yielding samples with increasingly coherent structure and detail. This property comes with trade-offs. On the positive side, it affords fine-grained control; by adding guidance term to the learned, time-dependent velocity field, we can steer the ODE flow to reflect user intent and make sampling controllable. On the negative side, the required iterative integration makes sampling slow compared with single-shot generators. This part focuses on improving the generative process at inference time, without retraining. Steering Generation (Chapter 8): Techniques such as classifier guidance and classifier-free guidance make it possible to condition the generation process on user-defined objectives or attributes. Building on this, we next discuss how the use of preference dataset can further align diffusion models with such preferences. Fast Generation with Numerical Solvers (Chapter 9): Sampling can be significantly accelerated using advanced numerical solvers that approximate the reverse process in fewer steps, reducing cost while preserving quality. Part D: Learning Fast Generative Models. Beyond improving existing sampling algorithms, we investigate how to directly learn fast generators that approximate the diffusion process. Distillation-Based Methods (Chapter 10): This approach focuses on training student model to imitate the behavior of pre-trained, slow diffusion model (the teacher). Instead of reducing the teachers size, the goal is to reproduce its sampling trajectory or output distribution with far fewer integration steps, often only few or even one. Learning from Scratch (Chapter 11): Since sampling in diffusion models can be seen as solving an ODE, this approach learns the solution map (i.e., the flow map) directly from scratch, without relying on teacher model. The learned map can take noise directly to data, or more generally perform anytime-to-anytime jumps along the solution trajectory. 10 Preface Appendices. To ensure our journey is accessible to all, the appendices provide background for foundational concepts. Chapter offers crash course on the differential equations that have become the language of diffusion models. The core insight behind diffusion models, despite their varied perspectives and origins, lies in the change-of-variables formula. This foundation naturally extends to deeper concepts such as the FokkerPlanck equation and the continuity equation, which describe how probability densities transform and evolve under mappings defined by functions (discrete time) or differential equations (continuous time). Chapter offers gentle introduction that bridges these foundational ideas to more advanced concepts. In Chapter C, we present two powerful but often overlooked tools underlying diffusion models: Itôs formula and Girsanovs theorem, which provide rigorous support for the FokkerPlanck equation and the reverse-time sampling process. Finally, Chapter gathers proofs of selected propositions and theorems discussed in the main chapters. What This Monograph Covers and What It Does Not. We aim for durability. From top-down viewpoint, this monograph begins with single principle: construct continuous-time dynamics that transport simple prior to the data distribution while ensuring that the marginal distribution at each time matches the marginal induced by prescribed forward process from data to noise. From this principle, we develop the stochastic and deterministic flows that enable sampling, show how to steer the trajectory (guidance), and explain how to accelerate it (numerical solvers). We then study diffusion-motivated fast generators, including distillation methods and flow-map models. With these tools, readers can place new papers within common template, understand why methods work, and design improved models. We do not attempt to provide an exhaustive survey of the diffusion model literature, nor do we catalog architectures, training practices, hyperparameters, compare empirical results across methods, cover datasets and leaderboards, describe domainor modality-specific applications, address system-level deployment, provide recipes for large-scale training, or discuss hardware engineering. These topics evolve rapidly and are better covered by focused surveys, open repositories, and implementation guides."
        },
        {
            "title": "Notations",
            "content": "a A Tr(A) ID diag(a) ϕ, θ ϕ, θ ϕ, θ Numbers and Arrays scalar. column vector (e.g., RD). matrix (e.g., Rmn). Transpose of A. Trace of A. Identity matrix of size D. Identity matrix; dimension implied by context. Diagonal matrix with diagonal entries given by a. Learnable neural network parameters. Parameters after training (fixed during inference). Optimal parameters of an optimization problem. Calculus 11 12 Notations or Dy(x) or xF x dy dx xy x 2 xf (x) or H(f )(x) (x) dx Partial derivatives of w.r.t. (componentwise). Total (Fréchet) derivative of w.r.t. x. Gradient of scalar : RD R; column in RD. Jacobian of : Rn Rm; shape n. Divergence of vector field : RD RD; scalar. Hessian of : RD R; shape D. Integral of over the domain of x. p(a) pdata pprior psrc ptgt (cid:2)f (x)(cid:3) Exp E(cid:2)f (x)z(cid:3), or Exp(z) Var(cid:0)f (x)(cid:1) Cov(cid:0)f (x), g(x)(cid:1) (cid:2)f (x)(cid:3) DKL (pq) ϵ (0, I) (x; µ, Σ) Probability and Information Theory Density/distribution over continuous vector a. Data distribution. Prior distribution (e.g., standard normal). Source distribution. Target distribution. Random vector is distributed as p. Expectation of (x) under p(x). Conditional expectation of (x) given z, with distributed as p(z). Variance under p(x). Covariance under p(x). KullbackLeibler divergence from to p. Standard normal sample. Gaussian over with mean µ and covariance Σ. Clarification. We use the same symbol for random vector and its realized value. This convention, common in deep learning and generative modeling, keeps notation compact and uncluttered. The intended meaning is determined by context. For example, in expressions such as p(x), the symbol serves as dummy variable, and the expression denotes the distribution or density as function Notations of its input. Thus p(x) refers to the functional form rather than evaluation at particular sample. When evaluation at given point is intended, we state it explicitly (for instance, evaluate at the given point x). Conditional expressions are read by context. For p(xy), fixing makes it density in x; fixing makes it function of y. For conditional expectations, E[f (x)z] denotes function of z, giving the expected value of (x) conditional on z. When conditioning on specific realized value, we write E[f (x)Z = z]. Equivalently, this can be written as an integral with respect to the conditional distribution, Exp(z)[f (x)] = f (x) p(xz) dx. This distinction clarifies whether is treated as variable defining function, 7 E[f (x)z], or as fixed value at which that function is evaluated."
        },
        {
            "title": "Introduction to Deep\nGenerative Modeling",
            "content": ""
        },
        {
            "title": "Deep Generative Modeling",
            "content": "What cannot create, do not understand. Richard P. Feynman Deep generative models (DGMs) are neural networks that learn probability distribution over high-dimensional data (e.g., images, text, audio) so they can generate new examples that resemble the dataset. We denote the model distribution by pϕ and the data distribution by pdata. Given finite dataset, we fit ϕ by minimizing loss that measures how far pϕ is from pdata. After training, generation amounts to running the models sampling procedure to draw pϕ (the density pϕ(x) may or may not be directly computable, depending on the model class). Model quality is judged by how well generated samples and their summary statistics match those of pdata, together with task-specific or perceptual metrics. This chapter builds the mathematical and conceptual foundations behind these ideas. We formalize the problem in Section 1.1, present representative model classes in Section 1.2, and summarize practical taxonomy in Section 1.3. 15 Deep Generative Modeling 1.1 What is Deep Generative Modeling? DGMs take as input large collection of real-world examples (e.g., images, text) drawn from an unknown and complex distribution pdata and output trained neural network that parameterizes an approximate distribution pϕ. Their goals are twofold: 1. Realistic Generation: To generate novel, realistic samples indistinguishable from real data. 2. Controllable Generation: To enable fine-grained and interpretable control over the generative process. This section presents the fundamental concepts and motivations behind DGMs, preparing for detailed exploration of their mathematical framework and practical applications. 1.1.1 Mathematical Setup We assume access to finite set of samples drawn independently and identically distributed (i.i.d.) from an underlying, complex data distribution pdata(x)1. Goal of DGM. The primary goal of DGM is to learn tractable probability distribution from finite dataset. These data points are observations assumed to be sampled from an unknown and complex true distribution pdata(x). Since the form of pdata(x) is unknown, we cannot draw new samples from it directly. The core challenge is therefore to create model that approximates this distribution well enough to enable the generation of new, realistic samples. To this end, DGM uses deep neural network to parameterize model distribution pϕ(x), where ϕ represents the networks trainable parameters. The training objective is to find the optimal parameters ϕ that minimize the divergence between the model distribution pϕ(x) and the true data distribution pdata(x). Conceptually, pϕ(x) pdata(x). When the statistical model pϕ(x) closely approximates the data distribution pdata(x), it can serve as proxy for generating new samples and evaluating probability values. This model pϕ(x) is commonly referred to as generative model. 1This is common assumption in machine learning. For simplicity, we use the symbol to represent either probability distribution or its probability density/mass function, depending on the context. 1.1. What is Deep Generative Modeling? 17 Figure 1.1: Illustration of the target in DGM. Training DGM is essentially minimizing the discrepancy between the model distribution pϕ and the unknown data distribution pdata. Since pdata is not directly accessible, this discrepancy must be estimated efficiently using finite set of independent and identically distributed (i.i.d.) samples, xi, drawn from it. Capability of DGM. Once proxy of the data distribution, pϕ(x), is available, we can generate an arbitrary number of new data points using sampling methods such as Monte Carlo sampling from pϕ(x). Additionally, we can compute the probability (or likelihood) of any given data sample by evaluating pϕ(x). Training of DGM. We learn parameters ϕ of model family {pϕ} by minimizing discrepancy D(pdata, pϕ): ϕ arg min ϕ D(pdata, pϕ). (1.1.1) Because pdata is unknown, practical choice of must admit efficient estimation from i.i.d. samples from pdata. With sufficient capacity, pϕ can closely approximate pdata. Forward KL and Maximum Likelihood Estimation (MLE). standard choice is the (forward) KullbackLeibler divergence2 DKL (cid:0)pdatapϕ (cid:1) := pdata(x) log pdata(x) pϕ(x) dx =Expdata (cid:2) log pdata(x) log pϕ(x)(cid:3). which is asymmetric, i.e., DKL(pdatapϕ) = DKL(pϕpdata). 2All integrals are in the Lebesgue sense and reduce to sums under counting measures. 18 Deep Generative Modeling Importantly, minimizing DKL(pdatapϕ) encourages mode covering: if there exists set of positive measure with pdata(A) > 0 but pϕ(x) = 0 for A, then the integrand contains log (cid:0)pdata(x)/0(cid:1) = + on A, so DKL = +. Thus minimizing forward KL forces the model to assign probability wherever the data has support. Although the data density pdata(x) cannot be evaluated explicitly, the forward KL divergence can be decomposed as DKL (cid:0)pdatapϕ (cid:1) = Expdata \" log # pdata(x) pϕ(x) = Expdata where H(cid:0)pdata (cid:2) log pdata(x)(cid:3) is the entropy of the data distribution, which is constant with respect to ϕ. This observation implies the following equivalence: (cid:1) := Expdata (cid:2) log pϕ(x)(cid:3) + H(cid:0)pdata (cid:1), Lemma 1.1.1: Minimizing KL MLE min ϕ DKL (cid:0)pdata pϕ (cid:1) max ϕ Expdata (cid:2) log pϕ(x)(cid:3). (1.1.2) In other words, minimizing the forward KL divergence is equivalent to performing MLE. In practice we replace the population expectation by its Monte Carlo i=1 pdata, yielding the empirical MLE estimate from i.i.d. samples {x(i)}N objective ˆLMLE(ϕ) := 1 i=1 log pϕ (cid:0)x(i)(cid:1), optimized via stochastic gradients over minibatches; no evaluation of pdata(x) is required. Fisher Divergence. The Fisher divergence is another important concept for (score-based) diffusion modeling (see Chapter 3). For two distributions and q, it is defined as DF(pq) := Exp log p(x) log q(x)2 2 . (1.1.3) It measures the discrepancy between the score functions log p(x) and log q(x), which are vector fields pointing toward regions of higher probability. In short, DF(pq) 0 with equality if and only if = almost everywhere. 1.1. What is Deep Generative Modeling? 19 It is invariant to normalization constants, since scores depend only on gradients of log-densities, and it forms the basis of score matching (Equations (3.1.3) and (3.2.1)): method that learns the gradient of the log-density for generation (score-based models). In this setting, the data distribution = pdata serves as the target, while the model = pϕ is trained to align its score field with that of the data. Beyond KL. Although the KL divergence is the most widely used measure of difference between probability distributions, it is not the only one. Different divergences capture different geometric or statistical notions of discrepancy, which in turn affect the optimization dynamics of learning algorithms. broad family is the -divergences (Csiszár, 1963): Df (pq) = q(x)f (cid:19) (cid:18) p(x) q(x) dx, (1) = 0, (1.1.4) where : R+ is convex function. By changing , we obtain many well-known divergences: (u) = log Df = DKL(pq) (forward KL), (u) = 1 2 (u) = 1 2 1 log (u + 1) log 1+u 2 Df = DJS(pq) (JensenShannon), Df = DTV(p, q) (total variation). For clarity, the explicit forms are DJS(pq) = 1 2 DKL (cid:0)p 1 2 (p + q)(cid:1) + 1 2 DKL (cid:0)q 1 2 (p + q)(cid:1), and DTV(p, q) = 1 2 RD q dx = sup ARD p(A) q(A). Intuitively, the JS divergence provides smooth and symmetric measure that balances both distributions and avoids the unbounded penalties of KL (we will later see that it helps interpret the Generative Adversarial Network (GAN) framework), while the total variation distance captures the largest possible probability difference between the two. different viewpoint comes from optimal transport (see Chapter 7), whose representative is the Wasserstein distance (see . It measures the minimal cost of moving probability mass from one distribution to another. Unlike -divergences, which compare density ratios, Wasserstein distances depend on the geometry of the sample space and remain meaningful even when the supports of and do not overlap. 20 Deep Generative Modeling Each divergence embodies different notion of closeness between distributions and thus induces distinct learning behavior. We will revisit these divergences when they arise naturally in the context of generative modeling throughout this monograph. 1.1.2 Challenges in Modeling Distributions To model complex data distribution, we can parameterize the probability density function pdata using neural network with parameters ϕ, creating model we denote as pϕ. For pϕ to be valid probability density function, it must satisfy two fundamental properties: (i) Non-Negativity: pϕ(x) 0 for all in the domain. (ii) Normalization: The integral over the entire domain must equal one, i.e., pϕ(x) dx = 1. network can naturally produce real scalar Eϕ(x) for input x. To interpret this output as valid density, it must be transformed to satisfy conditions (i) and (ii). practical alternative is to view Eϕ : RD as defining an unnormalized density and then enforce these properties explicitly. Step 1: Ensuring Non-Negativity. We can guarantee that our models output is always non-negative by applying positive function to the raw output of the neural network Eϕ(x), such as Eϕ(x), E2 ϕ(x). standard and convenient choice is the exponential function. This gives us an unnormalized density, pϕ(x), that is guaranteed to be positive: pϕ(x) = exp(Eϕ(x)). Step 2: Enforcing Normalization. The function pϕ(x) is positive but does not integrate to one. To create valid probability density, we must divide it by its integral over the entire space. This leads to the final form of our model: pϕ(x) = pϕ(x) pϕ(x) dx = exp(Eϕ(x)) exp(Eϕ(x)) dx . The denominator in this expression is known as the normalizing constant or partition function, denoted by Z(ϕ): Z(ϕ) := exp(Eϕ(x)) dx. 1.1. What is Deep Generative Modeling? 21 While this procedure provides valid construction for pϕ(x), it introduces major computational challenge. For most high-dimensional problems, the integral required to compute the normalizing constant Z(ϕ) is intractable. This intractability is central problem that motivates the development of many different families of deep generative models. In the following sections, we introduce several prominent approaches of DGM. Each is designed to circumvent or reduce the computational cost of evaluating this normalizing constant. 22 Deep Generative Modeling 1.2 Prominent Deep Generative Models central challenge in generative modeling is to learn expressive probabilistic models that can capture the rich and complex structure of high-dimensional data. Over the years, various modeling strategies have been developed, each making different trade-offs between tractability, expressiveness, and training efficiency. In this section, we explore some of the most influential strategies that have shaped the field, accompanied by comparison of their computation graphs in Figure 1.2. Energy-Based Models (EBMs). EBMs (Ackley et al., 1985; LeCun et al., 2006) define probability distribution through an energy function Eϕ(x) that assigns lower energy to more probable data points. The probability of data point is defined as: where pϕ(x) := 1 Z(ϕ) exp(Eϕ(x)), Z(ϕ) = exp(Eϕ(x)) dx is the partition function. Training EBMs typically involves maximizing the log-likelihood of the data. However, this requires techniques to address the computational challenges arising from the intractability of the partition function. In the following chapter, we will explore how Diffusion Models offer an alternative by generating data from the gradient of the log density, which does not depend on the normalizing constant, thereby circumventing the need for partition function computation. Autoregressive Models. Deep autoregressive (AR) models (Frey et al., 1995; Larochelle and Murray, 2011; Uria et al., 2016) factorize the joint data distribution pdata into product of conditional probabilities using the chain rule of probability: pdata(x) = i=1 pϕ(xix<i), where = (x1, . . . , xD) and x<i = (x1, . . . , xi1). Each conditional pϕ(xix<i) is parameterized by neural network, such as Transformer, allowing flexible modeling of complex dependencies. Because each term is normalized by design (e.g., via softmax for discrete or parameterized Gaussian for continuous variables), global normalization is trivial. 1.2. Prominent Deep Generative Models 23 Training proceeds by maximizing the exact likelihood, or equivalently minimizing the negative log-likelihood, While AR models achieve strong density estimation and exact likelihoods, their sequential nature limits sampling speed and may restrict flexibility due to fixed ordering. Nevertheless, they remain foundational class of likelihoodbased generative models and key approaches in modern research. Variational Autoencoders (VAEs). VAEs (Kingma and Welling, 2013) extend classical autoencoders by introducing latent variables that capture hidden structure in the data x. Instead of directly learning mapping between and z, VAEs adopt probabilistic view: they learn both an encoder, qθ(zx), which approximates the unknown distribution of latent variables given the data, and decoder, pϕ(xz), which reconstructs data from these latent variables. To make training feasible, VAEs maximize tractable surrogate to the true log-likelihood, called the Evidence Lower Bound (ELBO): LELBO(θ, ϕ; x) = Eqθ(zx) [log pϕ(xz)] DKL (qθ(zx) pprior(z)) . Here, the first term encourages accurate reconstruction of the data, while the second regularizes the latent variables by keeping them close to simple prior distribution pprior(z) (often Gaussian). VAEs provide principled way to combine neural networks with latentvariable models and remain one of the most widely used likelihood-based approaches. However, they also face practical challenges, such as limited sample sharpness and training pathologies (e.g., the tendency of the encoder to ignore latent variables). Despite these limitations, VAEs laid important foundations for later advances, including diffusion models. Normalizing Flows. Classic flow-based models, such as Normalizing Flows (NFs) (Rezende and Mohamed, 2015) and Neural Ordinary Differential Equations (NODEs) (Chen et al., 2018), aim to learn bijective mapping fϕ between simple latent distribution and complex data distribution via an invertible operator. This is achieved either through sequence of bijective transformations (in NFs) or by modeling the transformation as an Ordinary Differential Equation (in NODEs). These models leverage the change-of-variable formula for densities, enabling MLE training: log pϕ(x) = log p(z) + log (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) det 1 ϕ (x) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) , 24 Deep Generative Modeling where fϕ represents the invertible transformation mapping to x. NFs explicitly model normalized densities using invertible transformations with tractable Jacobian determinants. The normalization constant is absorbed analytically via the change-of-variables formula, making likelihood computation exact and tractable. Despite their conceptual elegance, classic flow-based models often face practical limitations. For instance, NFs typically impose restrictive architectural constraints to ensure bijectivity, while NODEs may encounter training inefficiencies due to the computational overhead of solving ODEs. Both approaches face challenges when scaling to high-dimensional data. In later chapters, we will explore how Diffusion Models relate to and build upon these classic flow-based methods. Generative Adversarial Networks (GANs). GANs (Goodfellow et al., 2014) consist of two neural networks, generator Gϕ and discriminator Dζ, that compete against each other. The generator aims to create realistic samples Gϕ(z) from random noise pprior, while the discriminator attempts to distinguish between real samples and generated samples Gϕ(z). The objective function for GANs can be formulated as: min Gϕ max Dζ Expdata(x)[log Dζ(x)] {z } real + Ezpprior(z) [log(1 Dζ (Gϕ(z)))] {z } fake . GANs do not define an explicit density function and therefore bypass likelihood estimation entirely. Instead of computing normalization constant, they focus on generating samples that closely mimic the data distribution. From divergence perspective, the discriminator implicitly measures the discrepancy between the true data distribution pdata and the generator distribution pGϕ, where pGϕ denotes the distribution of generated samples Gϕ(z) obtained from noise pprior. With an optimal discriminator for fixed generator Gϕ computed as pdata(x) pdata(x) + pGϕ(x) , the generators minimization reduces to min Gϕ 2 DJS (cid:16) pdata pGϕ (cid:17) log 4. Here, DJS denotes the JensenShannon divergence, defined as (cid:17) (cid:16) (cid:17) (cid:16) (cid:13) (cid:13) (cid:13) p+q 2 + 2 DKL (cid:13) (cid:13) (cid:13) p+q 2 . DJS(p q) := 2 DKL 1.2. Prominent Deep Generative Models 25 This shows that GANs implicitly minimize DJS(pdata pGϕ). More broadly, extensions such as -GANs (Nowozin et al., 2016) generalize this view by demonstrating that adversarial training can minimize family of -divergences, placing GANs within the same divergence-minimization framework as other generative models. Although GANs are capable of generating high-quality data, their min-max training process is notoriously unstable, often requiring carefully designed architectures and engineering techniques to achieve satisfactory performance. However, GANs have since been revived as an auxiliary component to enhance other generative models, particularly Diffusion Models. Deep Generative Modeling 1.3 Taxonomy of Modelings As we have seen, DGMs span wide spectrum of modeling strategies. fundamental distinction lies in how these models parameterize the underlying data distribution, that is, whether they specify pϕ(x) explicitly or only implicitly, irrespective of the training objective. Explicit Models: These models directly parameterize probability distribution pϕ(x) via tractable or approximately tractable density or mass function. Examples include ARs, NFs, VAEs, and DMs, all of which define pϕ(x) either exactly or through tractable bound. Implicit Models: These models specify distribution only through sampling procedure, typically of the form = Gϕ(z) for some noise variable pprior. In this case, pϕ(x) is not available in closed form and may not be defined at all. The table in Table 1.1 offers concise summary of these contrasting approaches. Table 1.1: Comparison of Explicit and Implicit Generative Models Explicit Implicit Exact Likelihood Approx. Likelihood Likelihood Tractable Bound/Approx. Objective Examples MLE NFs, ARs ELBO VAEs, DMs Not Directly Modeled/ Intractable Adversarial GANs Connection to Diffusion Models. Taken together, these classical families of DGMs illustrate complementary strategies for modeling complex distributions. Beyond their standalone importance, they also provide guiding principles for understanding diffusion models. Diffusion methods inherit ideas from several of these perspectives: they connect to VAEs through variational training objectives, to EBMs through score-matching approaches that learn gradients of the log-density (closely tied to energy functions), and to NFs through continuous-time transformations. To lay the groundwork for the diffusion methods discussed in later chapters, we will focus on three central paradigms: VAEs (Section 2.1), EBMs 1.3. Taxonomy of Modelings 27 (Section 3.1), and NFs (Section 5.1). This exploration provides foundation for the core principles that underlie modern diffusion-based generative modeling, which will be developed further in the chapters that follow. Deep Generative Modeling 1.4 Closing Remarks This chapter has established the foundational concepts of deep generative modeling. We begin by defining the primary objective: to learn tractable model distribution pmodel (parametrized by ϕ) that approximates an unknown, complex data distribution pdata. central challenge is the computational intractability of the normalizing constant, or partition function Z(ϕ), which is required to define valid probability density. To circumvent this problem, various families of deep generative models have been developed, each employing distinct strategy. We surveyed several prominent approaches, including Energy-Based Models (EBMs), Autoregressive Models (ARs), Variational Autoencoders (VAEs), Normalizing Flows (NFs), and Generative Adversarial Networks (GANs). These models can be broadly categorized into explicit models, which define tractable density, and implicit models, which define distribution only through sampling procedure. While each of these classical frameworks is significant, three in particular serve as the conceptual origins for the diffusion models that are the focus of this monograph: VAEs, EBMs, and NFs. In the chapters that follow, we will trace the evolution of diffusion models from these three foundational paradigms: 1. Part will begin by exploring the variational perspective (Chapter 2), showing how (the hierarchical latent variable structure of) VAEs leads naturally to the formulation of Denoising Diffusion Probabilistic Models (DDPMs). 2. Next, we will examine the score-based perspective (Chapter 3), which originates from EBMs and score matching, and develops into Noise Conditional Score Networks (NCSN) and the more general Score SDE framework (Chapter 4). 3. Finally, we will investigate the flow-based perspective (Chapter 5), which builds upon the principles of Normalizing Flows to frame generation as continuous transformation, generalized by the concept of Flow Matching. By understanding these origins, we will build coherent framework for interpreting the diverse formulations of diffusion models and uncovering the deep principles that unify them. 1.4. Closing Remarks 29 EBM AR Energy Eϕ(x) value x1 x2 xL1 xL VAE Encoder qθ(zx) NF Forward fϕ(x) Decoder pϕ(xz) Inverse 1 ϕ (z) x Discriminator Dζ 0/1 z Generator Gϕ(z) GAN DM x0 x1 x2 xL1 xL Figure 1.2: Computation graphs of prominent deep generative models. Top to bottom: EBM maps an input to scalar energy; AR generates sequence {xℓ} left to right with causal dependencies; VAE encodes to latent and decodes to reconstruction x; NF applies an invertible map fϕ between and and uses 1 ϕ to produce x; GAN transforms noise to sample that is judged against real by discriminator Dζ; DM iteratively refines noisy sample through multi-step denoising chain {xℓ}. Boxes denote variables, trapezoids are learnable networks, ovals are scalars; arrows indicate computation flow."
        },
        {
            "title": "Origins and Foundations of\nDiffusion Models",
            "content": "31 t s i r e n u ff Chapter 1 Overview of Deep Generative Modeling Variational View Score-Based View Flow-Based View Variational Autoencoder 2 p Energy-Based Model Normalizing Flows 3 p 5 p Denoising Diffusion Probabilistic Model (DDPM) Noise Conditional Score Network (NCSN) Gaussian Flow Matching Continuous-Time Formulation (e.g., Score SDE) Chapter 4 Unifying Principles Chapter 6 Conditional Strategy Fokker-Planck Equation 2 Variational Perspective: From VAEs to DDPMs In this chapter we view diffusion models through variational lens. We begin with the Variational Autoencoders (VAEs), which represents data with latent variables and is trained by maximizing tractable lower bound on the log likelihood. In this setting learned encoder maps observations to latents, and learned decoder maps latents back to observations, closing the modeling loop. Building on this pattern, hierarchical variants (Hierarchical VAEs) stack several latent layers to capture structure at multiple scales. With this setup, Denoising Diffusion Probabilistic Models (DDPM) follow the same template: instead of jointly training both the encoder and decoder, the encoder is fixed as forward noising process that gradually maps data to noise, and training learns decoder that reverses this path in successive denoising steps. In this view, VAEs, hierarchical VAEs, and diffusion models all optimize likelihood surrogate defined by variational bound, providing common foundation for the methods introduced here. 32 2.1. Variational Autoencoder 33 2.1 Variational Autoencoder How can neural network learn to generate realistic data? natural starting point is the autoencoder, which consists of two networks: deterministic encoder that compresses an input to low-dimensional latent code, and deterministic decoder that reconstructs the input from this code. Training minimizes the reconstruction error between the original input and its reconstruction. While this setup enables accurate reconstruction, the latent space is unstructured: randomly sampling latent codes usually produces meaningless outputs, limiting the models use for generation. The Variational Autoencoder (VAE) (Kingma and Welling, 2013) solves this by imposing probabilistic structure on the latent space. This transforms the model from simple reconstruction tool into true generative model, capable of producing novel and realistic data. 2.1.1 Probabilistic Encoder and Decoder Encoder qθ(zx) Decoder pϕ(xz) Figure 2.1: Illustration of VAE. It consists of stochastic encoder qθ(zx) that maps data to latent variable z, and decoder pϕ(xz) that reconstructs data from the latent. Construction of Decoder (Generator). In VAEs, we distinguish between two types of variables: observed variables x, which correspond to the data we see (e.g., an image), and latent variables z, which capture the hidden factors of variation (e.g., object shape, color, or style). The model assumes that each observation is generated from latent variable sampled from simple prior distribution, typically standard Gaussian, pprior := (0, I). To map back to data space, we define decoder (generator) distribution pϕ(xz). In practice, this decoder is kept simple, often factorized Gaussian (see Section 2.1.3) or similar distribution, so that learning focuses on extracting useful latent features rather than memorizing data. Intuitively, directly generating pixels one by one is extremely hard; instead, the latent variable provides compact representation, from which decoding the exact pixel arrangement becomes much easier. New samples are drawn by first sampling pprior and then decoding via pϕ(xz). 34 Variational Perspective: From VAEs to DDPMs The VAE thereby defines latent-variable generative model through the marginal likelihood: pϕ(x) = pϕ(xz)p(z) dz. Ideally, the decoder parameters ϕ are learned by maximizing this marginal likelihood, as in maximum likelihood estimation (see Equation (1.1.2)). However, because the integral over is intractable for expressive, non-linear decoders, direct MLE is computationally infeasible, motivating the variational approach used in VAEs. Construction of Encoder (Inference Network). To connect our intractable generator to real data, consider the reverse question: given an observation x, what latent codes could have produced it? By Bayes rule, the posterior distribution is pϕ(zx) = pϕ(xz)p(z) pϕ(x) . The difficulty is that the denominator involves the marginal likelihood pϕ(x), which requires integrating over all latent variables and is intractable for nonlinear decoders. Thus, exact inference of from is computationally prohibitive. The variational step in VAEs addresses this by replacing the intractable posterior with tractable approximation. We introduce an encoder (or inference network) qθ(zx), parameterized by neural network, whose role is to serve as learnable proxy: qθ(zx) pϕ(zx). In practice, the encoder maps each observed data point to distribution over latent codes, providing feasible and trainable pathway from back to that enables learning. 2.1.2 Training via the Evidence Lower Bound (ELBO) We now define computable training objective. While we cannot directly optimize log pϕ(x), we can maximize lower bound on itthe Evidence Lower Bound (ELBO): 2.1. Variational Autoencoder 35 Theorem 2.1.1: Evidence Lower Bound (ELBO) For any data point x, the log-likelihood satisfies: log pϕ(x) LELBO(θ, ϕ; x), where the ELBO is given by: LELBO = Ezqθ(zx) [log pϕ(xz)] } {z Reconstruction Term DKL (qθ(zx)p(z)) } {z Latent Regularization . (2.1.1) Proof for Theorem. The ELBO arises from Jensens inequality: log pϕ(x) = log pϕ(x, z)dz = log qθ(zx) = log Ezqθ(zx) (cid:21) (cid:20) pϕ(x, z) qθ(zx) Ezqθ(zx) dz pϕ(x, z) qθ(zx) (cid:20) log pϕ(x, z) qθ(zx) (cid:21) . The ELBO objective naturally decomposes into two parts: Reconstruction: Encourages accurate recovery of from its latent code z. With Gaussian encoder and decoder assumptions, this term reduces exactly to the familiar reconstruction loss of an autoencoder (cf. Section 2.1.3). However, as in autoencoders, optimizing this term alone risks memorizing the training data, motivating an additional regularization. Latent KL: Encourages the encoder distribution qθ(zx) to stay close to simple Gaussian prior pprior(z). This regularization shapes the latent space into smooth and continuous structure, enabling meaningful generation by ensuring that samples drawn from the prior can be reliably decoded. This trade-off ensures both faithful reconstructions and coherent sampling. Information-Theoretic View: ELBO as Divergence Bound. The ELBO objective has natural information-theoretic interpretation. Recall that maximum likelihood training amounts to minimizing the KL divergence DKL(pdata(x)pϕ(x)), 36 Variational Perspective: From VAEs to DDPMs which measures how well the model distribution approximates the data distribution. Since this term is intractable in general, the variational framework introduces joint comparison. Specifically, consider two joint distributions: The generative joint, pϕ(x, z) = p(z)pϕ(xz), which describes how the model generates data; The inference joint, qθ(x, z) = pdata(x)qθ(zx), which couples real data with its inferred latent. Comparing these distributions yields the inequality DKL(pdata(x)pϕ(x)) DKL(qθ(x, z)pϕ(x, z)), (2.1.2) sometimes referred to as the chain rule for KL divergence. Intuitively, comparing only marginals (x) can hide mismatches that are revealed when the full latentdata joint is considered. Formally, one can expand the joint KL as DKL(qθ(x, z)pϕ(x, z)) } {z Total Error Bound \" =Eqθ(x,z) log # pdata(x)qθ(zx) pϕ(x)pϕ(zx) \" =Epdata(x) log pdata(x) pϕ(x) # + DKL (qθ(zx)pϕ(zx)) = DKL(pdatapϕ) } {z True Modeling Error + Epdata(x) (cid:2)DKL(qθ(zx)pϕ(zx))(cid:3) , } {z Inference Error where the first term is the true modeling error and the second is the inference error, i.e., the gap between the approximate and true posteriors. The latter is always non-negative, which explains Equation (2.1.2). Finally, note that log pϕ(x) LELBO(θ, ϕ; x) = DKL (qθ(zx)pϕ(zx)) . Thus the inference error is exactly the gap between the log-likelihood and the ELBO. Maximizing the ELBO therefore corresponds to directly reducing inference error, ensuring that training minimizes meaningful part of the overall bound. 2.1. Variational Autoencoder 2.1.3 Gaussian VAE standard formulation of the VAE employs Gaussian distributions for both the encoder and decoder. Encoder Part. The encoder qθ(zx) is typically modeled as Gaussian distribution as: qθ(zx) := (cid:16) z; µθ(x), diag(σ (cid:17) θ(x)) , where µθ : RD Rd and σθ : RD Rd encoder network. + are deterministic outputs of the Decoder Part. The decoder is typically modeled as Gaussian distribution with fixed variance: pϕ(xz) := (cid:0)x; µϕ(z), σ2I(cid:1), where µϕ : Rd RD is neural network, and σ > 0 is small constant controlling the variance. Under this assumption, the reconstruction term in the ELBO simplifies as Eqθ(zx) [log pϕ(xz)] = 1 2σ2 Eqθ(zx) µϕ(z)2i + C, where is constant independent of θ and ϕ. The ELBO objective thus reduces to: min θ,ϕ Eqθ(zx) (cid:20) 1 2σ2 µϕ(z)2 (cid:21) + DKL (cid:0)qθ(zx)pprior(z)(cid:1), where the KL term admits closed-form solution due to the Gaussian assumption. Training the VAE therefore reduces to minimizing regularized reconstruction loss. 2.1.4 Drawbacks of Standard VAE Despite the theoretical appeal of the VAE framework, it suffers from critical drawback: it often produces blurry outputs. Blurry Generations in VAEs. To understand this phenomenon, consider fixed Gaussian encoder qenc(zx), and decoder of the form pdec(xz) = (x; µ(z), σ2I), 38 Variational Perspective: From VAEs to DDPMs where µ(z) denotes the decoder network. With an arbitrary encoder, optimizing the ELBO reduces (up to an additive constant) to minimizing the expected reconstruction error: arg min µ Epdata(x)qenc(zx) x µ(z)2i . This is standard least squares problem in µ(z), and its solution is given in closed form by the conditional mean: µ(z) = Eqenc(xz)[x], where qenc(xz) is the encoder-induced posterior on inputs given latents, defined via Bayes rule: qenc(xz) = qenc(zx)pdata(x) pprior(z) . An equivalent form of the optimal generator via Bayes rule is: µ(z) = Epdata(x)[qenc(zx) x] Epdata(x)[qenc(zx)] . Now suppose that two distinct inputs = are mapped to overlapping regions in latent space, i.e., the supports of qenc(x) and qenc(x) intersect. That is, µ(z) averages over multiple, potentially unrelated inputs, which leads to blurry, non-distinct outputs. This averaging effect over conflicting modes is fundamental reason for the characteristic blurriness in VAE-generated samples. 2.1. Variational Autoencoder 39 2.1.5 (Optional) From Standard VAE to Hierarchical VAEs To model complex data, Hierarchical Variational Autoencoders (HVAEs) (Vahdat and Kautz, 2020) enhance VAEs by introducing hierarchy of latent variables. This deep, layered structure allows the model to capture data features at multiple levels of abstraction, significantly boosting expressive power and mirroring the compositional nature of real-world data. qθ(z1x) qθ(z2z1) qθ(zLzL1) z2 zL pϕ(xz1) pϕ(z1z2) pϕ(zL1zL) Figure 2.2: Computation graph of the HVAE. It has hierarchical structure with stacked, trainable encoders and decoders across multiple latent layers. HVAEs Modeling. Unlike standard VAEs that use single latent code z, hierarchical VAEs (HVAEs) introduce multiple layers of latent variables arranged in top-down hierarchy. Each latent layer conditions the one below it, forming chain of conditional priors that captures structure at progressively finer levels of abstraction. This leads to the following top-down factorization of the joint distribution: pϕ(x, z1:L) = pϕ(xz1) i=2 pϕ(zi1zi)p(zL). This structure defines the marginal data distribution, pHVAE(x) := pϕ(x, z1:L) dz1:L. Generation proceeds progressively: starting from the top latent variable zL, each latent is decoded sequentially down to z1, followed by generating the final observation x. For encoding part, HVAEs utilize structured, learnable variational encoder qθ(z1:Lx) that mirrors the generative hierarchy. common choice is bottom-up Markov factorization: qθ(z1:Lx) = qθ(z1x) i=2 qθ(zizi1). 40 Variational Perspective: From VAEs to DDPMs HVAEs ELBO. Similar to Equation (2.1.1), ELBO is derived via Jensens inequality: log pHVAE(x) = log pϕ(x, z1:L) dz1:L = log pϕ(x, z1:L) qθ(z1:Lx) qθ(z1:Lx) dz1:L = log Eqθ(z1:Lx) Eqθ(z1:Lx) (cid:20) log (cid:20) pϕ(x, z1:L) qθ(z1:Lx) pϕ(x, z1:L) qθ(z1:Lx) (cid:21) (cid:21) (2.1.3) Substituting the factorized forms yields: =: LELBO(ϕ). LELBO = Eqθ(z1:Lx) log \" p(zL) QL i=2 pϕ(zi1zi)pϕ(xz1) i=2 qθ(zizi1) qθ(z1x) QL # . This hierarchical ELBO decomposes into interpretable terms, including reconstruction term and KL divergences between each generative conditional and its corresponding variational approximation. The leap from shallow to deep networks revolutionized machine learning, and similar idea transformed generative models. HVAEs showed the power of using deep, stacked layers to build data. This concept of layered hierarchy is cornerstone of modern generative modeling, appearing again in score-based methods (Section 3.4) and normalizing flows (Section 5.1). The core insight is simple yet powerful: Observation 2.1.1: Stacking layers allows the model to generate data progressively, starting with coarse details and adding finer ones at each step. This process makes it far easier to capture the complex structure of high-dimensional data. Why Deeper Networks in Flat VAE are Not Enough. There are two fundamental limitations of standard flat VAE that are not resolved by simply making the encoder and decoder deeper. The first limitation is the variational family. In standard VAE, qθ(zx) = (cid:0)z; µθ(x), diag(σ2 θ (x))(cid:1), 2.1. Variational Autoencoder 41 so for each fixed the encoder posterior is single Gaussian with diagonal covariance. Greater network depth improves the accuracy of µθ and σθ but does not expand the family; even full covariance remains one unimodal ellipsoid. When pϕ(zx) is multi-peaked, this family cannot match it, which loosens the ELBO and weakens inference. Addressing this requires richer posterior class, not merely deeper networks. Second, if the decoder is too expressive, the model may suffer from posterior collapse. To see why, let us recall that the objective of the VAE is Epdata(x)[LELBO(x)] (cid:0)qθ(zx)p(z)(cid:1)(cid:3) = Epdata(x)qθ(zx)[log pϕ(xz)] Epdata(x) = Epdata(x)qθ(zx)[log pϕ(xz)] Iq(x; z) DKL(qθ(z)p(z)), (cid:2)DKL where Iq(x; z) is the mutual information defined by Iq(x; z) = Eq(x,z) log qθ(zx) q(z) = Epdata(x) (cid:2)DKL(qθ(zx)q(z))(cid:3), and the aggregated posterior is qθ(z) = pdata(x)qθ(zx) dx. If the decoder class can model the data well without using (i.e., it contains some pϕ(xz) = r(x) close to pdata), then maximizer of the ELBO sets qθ(zx) = p(z), so Iq(x; z) = 0 and qθ(z) = p(z). This ignore solution does not disappear by making the networks deeper: (1) the learned code becomes independent of (so it carries no data-dependent structure useful for downstream tasks), and (2) conditioning or moving in has no effect on generated samples, so controllable generation fails. What Hierarchy Changes? An HVAE introduces multiple latent levels, pϕ(x, z1:L) = pϕ(xz1) i=2 pϕ(zi1zi)p(zL), with ELBO LELBO(x) = Eq[log pϕ(xz1)] Eq DKL(qθ(z1x)pϕ(z1z2)) L1 DKL(qθ(zizi1)pϕ(zizi+1)) Eq i=2 Eq DKL(qθ(zLzL1)p(zL)) . Here, we denote Eq := Epdata(x)qθ(z1:Lx). Each inference conditional is aligned with its top-down generative counterpart: qθ(z1x) with pϕ(z1z2), intermediate 42 Variational Perspective: From VAEs to DDPMs layers with pϕ(zizi+1), and the top with the prior p(zL). This distributes the information penalty across levels and localizes learning signals through these adjacent KL terms. These properties stem from the hierarchical latent graph, not from simply deepening networks in flat VAE. What Will be Ahead? While HVAEs extend the VAE framework with multiple latent layers for expressiveness, their training poses unique challenges. Because the encoder and decoder must be optimized jointly, learning becomes unstable: lower layers and the decoder can already reconstruct x, leaving higher-level latents with little effective signal. Moreover, gradient information reaching deeper variables is often indirect and weak, making it difficult for them to contribute meaningfully. An additional difficulty lies in balancing model capacity, since overly expressive conditionals can dominate the reconstruction task and suppress the utility of higher latents. Interestingly, the core idea of deep, layered hierarchy finds more powerful incarnation in variational diffusion models, topic we explore in Section 2.2. Diffusion models inherit the progressive structure of HVAEs but elegantly sidestep their central weakness. By fixing the encoding process and focusing solely on learning the generative reversal, they unlock newfound stability and modeling flexibility, leading to significant leap in the quality of generated outputs. For notational simplicity, we deviate from the common VAE convention that uses for the encoder and for the generator. To avoid ambiguity, we denote distributions as and will always specify their roles through appropriate subscripts or superscripts, clarifying them in context. 2.2. Variational Perspective: DDPM 43 2.2 Variational Perspective: DDPM Denoising Diffusion Probabilistic Models (DDPMs) (Sohl-Dickstein et al., 2015; Ho et al., 2020) represent cornerstone of diffusion modeling. Conceptually, they operate within variational framework, much like VAEs and HVAEs. However, DDPMs introduce clever twist that tackles some of the challenges faced by their predecessors. At their core, DDPMs involve two distinct stochastic processes: The Forward Pass (Fixed Encoder): This process gradually corrupts data by injecting Gaussian noise over multiple steps via transition kernel p(xixi1). The data evolves into an isotropic Gaussian distribution, effectively becoming pure noise. This means the encoder is fixed and not learned. The Reverse Denoising Process (Learnable Decoder): Here, neural network learns to reverse the noise corruption through parameterized distribution pϕ(xi1xi). Starting from pure noise, this process iteratively denoises to generate realistic samples. Crucially, each individual denoising step is more manageable task than generating complete sample from scratch, as VAEs often attempt to do. By fixing the encoder and concentrating learning on the gradual generative trajectory, DDPMs achieve remarkable stability and expressive power. p(x1x0) p(x2x1) p(xLxL1) x0 x1 x2 xL pϕ(x0x1) pϕ(x1x2) pϕ(xL1xL) Figure 2.3: Illustration of DDPM. It consists of fixed forward process (in gray) that gradually adds Gaussian noise to the data, and learned reverse process that denoises step-by-step to generate new samples. In this section, we focus on DDPMs, postponing the broader discussion to Section 4.4, where we present more general and flexible framework. 2.2.1 Forward Process (Fixed Encoder) In DDPMs, the forward process is fixed, non-trainable operation that serves as an encoder. It progressively corrupts the original data by adding noise over multiple steps, eventually transforming it into simple prior distribution 44 Variational Perspective: From VAEs to DDPMs pprior := (0, I). This transformation is depicted as the forward chain in Figure 2.3 or illustration in Figure 2.4. Figure 2.4: Illustration of the DDPM forward process, wherein Gaussian noise is incrementally added to corrupt data sample into pure noise. Let us formalize this step-by-step degradation: Fixed Gaussian Transitions. Each step in the forward process is governed by fixed Gaussian transition kernel1: p(xixi1) := (xi; 1 β2 xi1, β2 I). Here, the process begins with x0, representing sample drawn from the real data distribution pdata. The sequence {βi}L i=1 denotes pre-determined, monotonically increasing noise schedule, where each βi (0, 1) controls the variance of the Gaussian noise injected at step i. For convenience, we define αi := . This mathematical definition is precisely equivalent to the following intuitive iterative update: 1 β2 xi = αixi1 + βiϵi, where ϵi (0, I) are independently and identically distributed. This means at each step i, we scale down the previous state xi1 by αi and add controlled amount of Gaussian noise scaled by βi. Perturbation Kernel and Prior Distribution. By recursively applying the transition kernels, we obtain closed-form expression for the distribution of noisy samples at step given the original data x0: where pi(xix0) = (cid:16) xi; αix0, (1 α2 (cid:17) )I , αi := k=1 1 β2 = k=1 αk. 1This formulation, while potentially appearing different, is mathematically equivalent to the original DDPM transition kernel. 2.2. Variational Perspective: DDPM 45 This means we can sample xi directly from as2 xi = αix0 + 1 α2 ϵ, ϵ (0, I). (2.2.1) Let the noise schedule {βi}L distribution of the forward process converges as i=1 be an increasing sequence, then the marginal pL(xLx0) (0, I) as , which motivates the choice of the prior distribution as with no reliance on data x0. pprior := (0, I) 2.2.2 Reverse Denoising Process (Learnable Decoder) At its core, the essence of DDPMs lies in their ability to reverse the controlled degradation imposed by the forward diffusion process. Starting from pure, unstructured noise, xL pprior, the objective is to progressively denoise this randomness, step by step, until coherent and meaningful data sample emerges. This reverse generation proceeds through Markov chain, illustrated by Figure 2.5. The fundamental challenge, and the central question guiding DDPM development, then becomes: Question 2.2.1 Can we precisely compute, or at least effectively approximate, these reverse transition kernels p(xi1xi), especially when considering the complex distribution of xi pi(xi)? 2For fixed index t, we will often use, both here and later on, the Gaussian perturbation form pt(xtx0) = (cid:0)xt; αtx0, σ2 which we equivalently write as the identity in distribution I(cid:1), xt = αtx0 + σtϵ, that is Law(xt) = Law(αtx0 + σtϵ), where x0 pdata, ϵ (0, I) is independent of x0, and αt, σt are deterministic scalars. = means the two random variables have the same probability density (i.e., law), Equality hence the same expectations for any test function ϕ: E[ϕ(xt)] = E[ϕ(αtx0 + σtϵ)]. For brevity, we will write xt = αtx0 + σtϵ, understood either as an equality in distribution or, by context, as sample realization; this shorthand will be used throughout. 46 Variational Perspective: From VAEs to DDPMs Figure 2.5: Illustration of DDPM reverse (denoising) process. Starting from noise xL pprior, the model sequentially samples xi1 p(xi1xi) for = L, . . . , 1 to obtain newly generated data x. The oracle transition p(xi1xi) is unknown; thus, we aim to approximate it. Rather than diving immediately into the mathematically intricate derivation of the Evidence Lower Bound (ELBO), as the original DDPM paper does (for which detailed discussion awaits in Section 2.2.5), we will instead approach the training objective from more intuitive perspective: by leveraging conditional probabilities to achieve tractable formulation. Overview: Modeling and Training Objective. To enable the generative process, our goal is to approximate the unknown true reverse transition kernel, p(xi1xi). We achieve this by introducing learnable parametric model, pϕ(xi1xi), and training it to minimize the expected KL divergence: Epi(xi) (cid:2)DKL (cid:0)p(xi1xi)pϕ(xi1xi)(cid:1)(cid:3) . (2.2.2) However, direct computation of the target distribution p(xi1xi) is challenging. By Bayes theorem, we would need to evaluate: p(xi1xi) = p(xixi1) pi1(xi1) pi(xi) {z intractable } . The marginals pi(xi) and pi1(xi1) are expectations over the unknown data distribution pdata, given by: pi(xi) = pi(xix0)pdata(x0)dx0, and analogously for pi1(xi1). Since pdata is unknown, these integrals have no closed-form evaluation; at best they can be approximated from samples, so the exact densities are not available in practice. Overcoming Intractability with Conditioning. central insight in DDPMs resolves this intractability: we condition the reverse transition on clean data 2.2. Variational Perspective: DDPM 47 sample x. This subtle yet powerful step transforms the intractable kernel into one that is mathematically tractable: p(xi1xi, x) = p(xixi1) p(xi1x) p(xix) . This tractability arises from two key properties of the forward process: its Markov property, meaning p(xixi1, x) = p(xixi1), and the Gaussian nature of all involved distributions. As result, p(xi1xi, x) itself is Gaussian and admits closed-form expression (which we saw in Equation (2.2.4)). Crucially, this elegant conditioning strategy allows us to derive tractable objective that is functionally equivalent to the seemingly intractable marginal KL divergence in Equation (2.2.2). Theorem 2.2.1: Equivalence Between Marginal and Conditional KL Minimization The following equality holds: (cid:2)DKL Epi(xi) (cid:0)p(xi1xi)pϕ(xi1xi)(cid:1)(cid:3) = Epdata(x)Ep(xix) (cid:2)DKL (cid:0)p(xi1xi, x)pϕ(xi1xi)(cid:1)(cid:3) + C, (2.2.3) where is constant independent of ϕ. Moreover, the minimizer of Equation (2.2.3) satisfies p(xi1xi) = Ep(xxi) (cid:2)p(xi1xi, x)(cid:3) = p(xi1xi), xi pi. Proof for Theorem. The proof rewrites KL-divergence expectation by expanding definitions, applying the chain rule of probability, and using logarithmic identity to decompose it into the sum of an expected conditional KL divergence and marginal KL divergence. complete derivation is in Section D.1.1. This alternative viewpoint: conditioning to obtain tractable objective, forms the foundation of DDPMs and reveals profound commonality with other influential diffusion models, as we will explore in Chapter 3 and Chapter 5. It reveals powerful equivalence: minimizing the KL divergence between marginal distributions is mathematically identical to minimizing the KL divergence between specific conditional distributions. This latter formulation is exceptionally useful because the crucial conditional distribution, p(xi1xi, x), possesses convenient closed-form expression: Variational Perspective: From VAEs to DDPMs Lemma 2.2.2: Reverse Conditional Transition Kernel p(xi1xi, x) is Gaussian with the closed-form expression: p(xi1xi, x) = (cid:16) (cid:17) xi1; µ (xi, x, i) , σ2(i)I , where µ (xi, x, i) := αi1β2 1 α2 + (1 α i1)αi 1 α2 xi, σ2(i) := 1 α2 i1 1 α2 β2 . (2.2.4) Later in Lemma 4.4.2, we present more general formula that extends beyond the DDPM noising process described in Equation (2.2.1). 2.2.3 Modeling of Reverse Transition Kernel pϕ(xi1xi) Leveraging the gradient-level equivalence as in Theorem 2.2.1 and the Gaussian form of the reverse conditional p(xi1xi, x) as in Lemma 2.2.2, DDPM assumes that each reverse transition pϕ(xi1xi) is Gaussian, parameterized as pϕ(xi1xi) := (cid:0)xi1; µϕ(xi, i), σ2(i)I(cid:1), (2.2.5) where µϕ(, i) : RD RD is learnable mean function, and σ2(i) > 0 is fixed as defined in Equation (2.2.4). We denote the KL divergence, averaged over time steps and conditioned on data x0 pdata, to match all layers of distributions as: Ldiffusion(x0; ϕ) := i=1 Ep(xix0) (cid:2)DKL (cid:0)p(xi1xi, x0)pϕ(xi1xi)(cid:1)(cid:3) . (2.2.6) Thanks to the Gaussian forms of both distributions and the parameterization defined in Equation (2.2.5), the objective admits closed-form expression and can be simplified as: Ldiffusion(x0; ϕ) = i=1 1 2σ2(i) µϕ(xi, i) µ(xi, x0, i)2 2 + C, (2.2.7) where is constant independent of ϕ. Averaging over the data distribution and omitting the constant (which does not affect the optimization), the final DDPM training objective is LDDPM(ϕ) := i=1 1 2σ2(i) where x0 pdata. Ex Ep(xix0) µϕ(xi, i) µ(xi, x0, i)2 2 , (2.2.8) 2.2. Variational Perspective: DDPM 49 2.2.4 Practical Choices of Predictions and Loss In typical DDPM implementations, training is not conducted ϵ-Prediction. directly using the original loss based on the mean prediction parameterization from Equation (2.2.8). Instead, an equivalent reparameterization, known as the ϵ-prediction (noise prediction) formulation, is commonly adopted. Recall that in the DDPM forward process, noisy sample xi p(xix) at noise level is generated by xi = αix0 + 1 α2 ϵ, x0 pdata, ϵ (0, I). (2.2.9) Using this expression, the reverse mean µ(xi, x0, i) from Equation (2.2.4) can be rewritten as: µ(xi, x0, i) = xi 1 αi . ϵ 1 α2 1 α2 This motivates parameterization of the model mean µϕ using neural network ϵϕ(xi, i) that directly predicts the noise: µϕ(xi, i) = 1 αi xi 1 α2 1 α2 ϵϕ(xi, i) } {z ϵ-prediction . Substituting this into the original loss leads to squared ℓ2 error between predicted and true noise: µϕ(xi, i) µ(xi, x0, i)2 2 ϵϕ(xi, i) ϵ2 2 , up to weighting factor depending on i. Intuitively, the model acts as noise detective, estimating the random noise added at each step of the forward process. Subtracting this estimate from the corrupted sample moves it closer to the clean original, and repeating this step-by-step reconstructs the data from pure noise. Simplified Loss with ϵ-Prediction. In practice, this expression is further simplified by omitting the weighting term, yielding the widely used DDPM training loss: Lsimple(ϕ) := EiExpdata(x)EϵN (0,I) ϵϕ(xi, i) ϵ2 2 , (2.2.10) where xi = αix0 + ϵ with x0 pdata. Since the target noise has unit variance at every timestep t, the ℓ2 loss in Equation (2.2.10) maintains 1 α2 50 Variational Perspective: From VAEs to DDPMs consistent scale across all t. This prevents vanishing or exploding targets and eliminates the need for explicit loss weighting. Importantly, both LDDPM and Lsimple share the same optimal solution ϵ, this is because Equation (2.2.10) essentially reduces to least-squares problem (as shown similarly in Proposition 4.2.1 or Proposition 6.3.1): ϵ(xi, i) = [ϵxi] , xi pi. Intuitively, the ϵ-prediction network ϵϕ(xi, i) estimates the noise added by the forward process to produce xi. At optimality, this estimate coincides with the conditional expectation of the true noise, even though xi does not uniquely determine the original clean sample. Another Equivalent Parametrization: x-Prediction. Equation (2.2.4) motivates an alternative yet equivalent parameterization, known as x-prediction (clean prediction), in which neural network xϕ(xi, i) is trained to predict clean (denoised) sample from given noisy input xi pi(xi) at noise level i. Replacing the ground-truth clean sample in the reverse mean expression with xϕ(xi, i) leads to the following model: µϕ(xi, i) = αi1β2 1 α2 xϕ(xi, i) + (1 α i1)αi 1 α2 xi. Analogous to the ϵ-prediction formulation, the training objective can be expressed as µϕ(xi, i) µ(xi, x0, i)2 2 xϕ(xi, i) x0 2 , x0 pdata, where the model is trained to predict the original data sample from its noisy version xi. This equivalence reduces the mean-matching loss in Equation (2.2.8) to EiEx0pdata EϵN (0,I) ωi xϕ(xi, i) x02 2 i , for some weighting function ωi. Since this is least-squares problem, the optimal solution is given by (see Proposition 4.2.1 or Proposition 6.3.1) x(xi, i) = [x0xi] , xi pi, that is, the model should predict the expected clean data given noisy observation xi at timestep i. (2.2.11) The x-prediction and ϵ-prediction parameterizations are mathematically equivalent and connected via the forward process: xi = αixϕ(xi, i) + 1 α2 ϵϕ(xi, i). (2.2.12) That is, one may either predict the clean sample xϕ(xi, i) or the noise ϵϕ(xi, i), such that their combination reproduces xi under the forward noising process. 2.2. Variational Perspective: DDPM 51 2.2.5 DDPMs ELBO With the reverse transitions defined as in Equation (2.2.5), this leads to the definition of the joint generative distribution in DDPM as: pϕ(x0, x1:L) := pϕ(x0x1)pϕ(x1x2) pϕ(xL1xL)pprior(xL), and the marginal generative model for data is given by: pϕ(x0) := pϕ(x0, x1:L) dx1:L. Indeed, DDPM training via Equation (2.2.6) can be rigorously grounded in maximum likelihood estimation (Equation (1.1.2)). Specifically, its objective forms an ELBO, similar to those in VAEs and HVAEs from Section 2.1, which serves as lower bound on the log-density: Theorem 2.2.3: DDPMs ELBO log pϕ(x0) LELBO(x0; ϕ) := Lprior(x0) + Lrecon.(x0; ϕ) + Ldiffusion(x0; ϕ) Here, each component of losses are defined as: (2.2.13) (cid:16) (cid:17) Lprior(x0) := DKL p(xLx0)pprior(xL) Lrecon.(x0; ϕ) := Ep(x1x0) [ log pϕ(x0x1)] Ldiffusion(x0; ϕ) = i=1 Ep(xix0) (cid:16) DKL p(xi1xi, x0)pϕ(xi1xi) (cid:17)i . Proof for Theorem. The derivation applies Jensens inequality, as in the HVAE/VAE ELBO (Equation (2.1.3)), with further simplifications. The detailed proof is deferred to Section D.1.2. The ELBO LELBO consists of three terms: Lprior can be made negligible by choosing the noise schedule {βi} such that p(x0) pprior(). For Lrecon., this can be approximated and optimized using Monte Carlo estimate; see (Ho et al., 2020; Kingma et al., 2021) for practical implementations. 52 Variational Perspective: From VAEs to DDPMs Ldiffusion (cf. Equation (2.2.6)) matches the reverse conditionals pϕ(xi1xi) to p(xi1xi) at all steps i. The ELBO objective LELBO can also be interpreted through the lens of the Data Processing Inequality with latents = x1:L, as illustrated in Equation (2.1.2): DKL(pdata(x0)pϕ(x0)) DKL (p(x0, x1:L)pϕ(x0, x1:L)) , where p(x0, x1:L) := pdata(x0)p(x1x0)p(x2x1) p(xLxL1) denotes the joint distribution along the forward process. Remark. Diffusions variational view fits the HVAE template: the encoder is the fixed forward noising chain, and the latents x1:T share the data dimensionality. Training maximizes the same ELBO. There is no learned encoder and no per-level KL terms; instead, the objective decomposes into wellconditioned denoising subproblems from large to small noise (coarse to fine), yielding stable optimization, and high sample quality while preserving coarse-to-fine hierarchy over time/noise. 2.2.6 Sampling After training the ϵ-prediction model, ϵϕ(xi, i)3, sampling is performed sequentially as illustrated in Figure 2.5, using the parametrized transition pϕ(xi1xi) instead. More specifically, starting from random seed xL pprior = (0, I), we recursively sample from pϕ(xi1xi) following the update rule below for = L, 1, . . . , 1: xi1 xi 1 αi 1 α2 1 α2 {z µϕ (xi,i) } ϵϕ(xi, i) +σ(i)ϵi, ϵi (0, I). (2.2.14) This denoising process continues until x0 is obtained as the final clean generated sample. Another Interpretation of DDPMs Sampling. From Equation (2.2.12), the clean sample prediction corresponding to noise estimate ϵϕ(xi, i) can be 3We use the symbol to indicate that the model has been trained and is now frozen. 2.2. Variational Perspective: DDPM expressed as xϕ(xi, i) = xi 1 α2 ϵϕ(xi, i) αi . Plugging this into the DDPM sampling rule in Equation (2.2.14) yields the equivalent update: xi1 (interpolation between xi and clean prediction xϕ) + σ(i)ϵi indicating that each step is centered around the predicted clean sample, with added Gaussian noise scaled by σ(i). This reveals that DDPM sampling can be viewed as an iterative denoising process that alternates between: 1. Estimating the clean data xϕ(xi, i) from the current noisy input xi, 2. Sampling less noisy latent xi1 via the update rule using this clean estimate. Figure 2.6: Illustration of DDPM sampling with clean prediction: estimate xϕ (xi, i) from xi, then update to xi1. Variational Perspective: From VAEs to DDPMs However, even if xϕ is trained as the optimal denoiser (i.e., the conditional expectation minimizer; see Equation (2.2.11)), it can only predict the average clean sample given xi. This limitation leads to blurry predictions, particularly at high noise levels, where recovering detailed structure from severely corrupted inputs becomes difficult. From this viewpoint, diffusion sampling typically moves from high to low noise and progressively refines an estimate of the clean signal. Early steps set the global structure, later steps add fine detail, and the sample becomes more realistic as the noise is removed. Slow Sampling Speed of DDPM. DDPM (a.k.a., diffusion model) sampling is inherently slow4 due to the sequential nature of its reverse process, constrained by the following factors. Theorem 2.2.1 shows that an expressive pϕ(xi1xi) can theoretically match the true reverse distribution p(xi1xi). However, in practice, pϕ(xi1xi) is typically modeled as Gaussian to approximate p(xi1xi), limiting its expressiveness. For small forward noise scales βi, the true reverse distribution is approximately Gaussian, enabling accurate approximation. Conversely, large βi induce multimodality or strong non-Gaussianity that single Gaussian cannot capture. To maintain accuracy, DDPM employs many small βi steps, forming sequential chain where each step depends on the previous and requires neural network evaluation ϵϕ(xi, i). This results in O(L) sequential passes, preventing parallelization and slowing generation. Later in Chapter 4 we show more principled interpretation of this inherent sampling bottleneck as differential-equation problem, which motivates continuous-time numerical strategies for accelerating generation. 4DDPM typically needs 1,000 denoising steps. 2.3. Closing Remarks 2.3 Closing Remarks 55 In this chapter, we have traced the origins of diffusion models through the variational lens. We began with the Variational Autoencoder (VAE), foundational generative model that learns probabilistic mapping between data and structured latent space via the Evidence Lower Bound (ELBO). We saw how Hierarchical VAEs (HVAEs) extended this idea by stacking latent layers, introducing the powerful concept of progressive, coarse-to-fine generation. However, these models face challenges with training stability and sample quality. We then framed Denoising Diffusion Probabilistic Models (DDPMs) as pivotal evolution within this variational framework. By fixing the encoder to gradual noising process and learning only the reverse denoising steps, DDPMs elegantly sidestep the training instabilities of HVAEs. Crucially, we demonstrated that DDPMs are also trained by maximizing variational bound on the log-likelihood , with training objective that decomposes into series of simple denoising tasks. This tractability is enabled by powerful conditioning strategy that transforms an intractable marginal objective into tractable conditional one, recurring theme in diffusion models. While this variational framework provides complete and powerful foundation for DDPMs, it is not the only way to understand them. An alternative and equally fundamental perspective emerges from the principles of energy-based modeling. In the next chapter, we will explore this score-based perspective: 1. We will shift our focus from learning the denoising transition probabilities pϕ(xi1xi) to directly learning the gradient of the datas log-density, i.e., the score function. 2. We will see how this approach, originating from EBMs, gives rise to Noise Conditional Score Networks (NCSN) and reveals deep, mathematical equivalence between the noise prediction (ϵ-prediction) learned in DDPMs and the score function itself. This alternative viewpoint will not only offer new insights but also serve as another cornerstone for the unified, continuous-time framework of diffusion models to be developed later. Score-Based Perspective: From EBMs to NCSN In the previous chapters we traced diffusion models to their variational roots and showed how they arise within the framework of VAEs. We now turn to second, equally fundamental viewpoint: Energy Based Models (EBMs) (Ackley et al., 1985; LeCun et al., 2006). An EBM represents distribution by an energy landscape that is low on data and high elsewhere. Sampling typically relies on Langevin dynamics, which moves samples toward high density regions by following the gradient of this landscape. This gradient field, known as the score, points toward directions of higher probability. The central observation is that knowing the score is enough for generation: it moves samples toward likely regions without computing the intractable normalization constant. Score-based diffusion models build directly on this idea. Instead of focusing only on the clean data distribution, they consider sequence of Gaussian noiseperturbed distributions whose scores are easier to approximate. Learning these scores yields family of vector fields that guide noisy samples step by step back to data, turning generation into progressive denoising. 56 3.1. Energy-Based Models 3.1 Energy-Based Models For readers already familiar with EBMs, this section is meant as concise refresher and bridge to the score-based view of diffusion. 3.1.1 Modeling Probability Distributions Using Energy Functions Let RD denote data point. EBMs define probability density via an energy function Eϕ(x), parameterized by ϕ, which assigns lower energy to more likely configurations. The resulting distribution is given by pϕ(x) := exp(Eϕ(x)) Zϕ , Zϕ := RD exp(Eϕ(x)) dx, where Zϕ is called the partition function ensuring normalization: RD pϕ(x) dx = 1. Figure 3.1: Illustration of EBM training. The model lowers density (raises energy) at bad data points (red arrows), and raises density (lowers energy) at good data points (green arrows). In this view, points with lower energy correspond to higher probability, much like ball rolling down into valley. The partition function Zϕ ensures that all probabilities add up to one, and as result only the relative values of energy matter. For instance, adding constant to all energies multiplies both numerator and denominator by the same factor, leaving the distribution unchanged. Moreover, because the partition function Zϕ enforces that probabilities sum to one, it follows mathematically that decreasing the energy within region increases its probability, while the probability of its complement Score-Based Perspective: From EBMs to NCSN decreases accordingly. Thus, EBMs obey strict global trade-off: making one valley deeper inevitably makes others shallower, and probability mass is redistributed across the entire space rather than assigned independently to each region. In principle, EBMs Challenges of Maximum Likelihood Training in EBMs. can be trained by maximum likelihood, which naturally balances fitting the data with global regularization (see Equation (1.1.2)): LMLE(ϕ) = Epdata(x) log \" # exp(Eϕ(x)) Zϕ (3.1.1) = Epdata[Eϕ(x)] } {z lowers energy of data log exp(Eϕ(x)) dx } {z global regularization , with Zϕ = exp(Eϕ(x)) dx. The first term lowers the energy of real data, while the second enforces normalization via the partition function. However, in high dimensions computing log Zϕ and its gradient is intractable, as it requires expectations under the model distribution. This motivates alternative objectives that either approximate the term, such as contrastive divergence (Hinton, 2002), or avoid it altogether through score matching. In what follows, we first introduce the notion of the score function in Section 3.1.2 and present score matching as tractable training objective that bypasses the partition function in Section 3.1.3, and then discuss Langevin dynamics as practical sampling method with score functions in Section 3.1.4. 3.1. Energy-Based Models 59 3.1.2 Motivation: What Is the Score? For density p(x) on RD, the score function is the gradient of the log-density: s(x) := log p(x), : RD RD. Intuitively, the score forms vector field that points toward regions of higher probability, providing local guide to where the data is most likely to occur (see Figure 3.2). Figure 3.2: Illustration of score vector fields. Score vector fields log p(x) indicate directions of increasing density. Why Model Scores Instead of Densities? Modeling the score offers both theoretical and practical benefits: 1. Freedom from Normalization Constants. Many distributions are defined only up to an unnormalized density p(x), e.g., exp(Eϕ(x)) in EBMs: p(x) While computing is intractable, the score depends only on p: p(x) dx. p(x) = = , x log p(x) = log p(x) log } {z =0 = log p(x), (3.1.2) since is constant in x. This bypasses the partition function entirely. 60 Score-Based Perspective: From EBMs to NCSN 2. Complete Representation. The score function fully characterizes the underlying distribution. Since it is the gradient of the log-density, the density can be recovered (up to constant) via log p(x) = log p(x0) + 0 s(x0 + t(x x0))(x x0) dt, where x0 is reference point and log p(x0) is fixed by normalization. Thus, modeling the score is as expressive as modeling p(x) itself, while often more tractable for generative modeling. 3.1.3 Training EBMs via Score Matching In EBMs, the density is defined as pϕ(x) = exp(Eϕ(x)) . Maximum likelihood training requires computing Zϕ, which is generally intractable. key observation is that the model score pϕ simplifies to: xEϕ(x), independent of Zϕ (see Equation (3.1.2)). Zϕ Score matching (Hyvärinen and Dayan, 2005) leverages the fact that scores depend only on the energy function. Instead of fitting normalized probabilities, it trains EBMs by aligning the model score with the (unknown) data score: LSM(ϕ) = 1 2 Epdata(x) (cid:13)x log pϕ(x) log pdata(x)(cid:13) (cid:13) 2 2. (cid:13) (3.1.3) Although the data score is inaccessible, integration by parts yields an equivalent expression involving only the energy and its derivatives (see Proposition 3.2.1 for more details): LSM(ϕ) = Epdata(x) Tr (cid:0)2 xEϕ(x)(cid:1) + 1 2 xEϕ(x)2 + C, where 2 xEϕ(x) is the Hessian of Eϕ and is constant independent of ϕ. This formulation is attractive because it eliminates the partition function and avoids sampling from the model during training. Its main drawback is the need for second-order derivatives, which can be computationally prohibitive in high dimensions. We will revisit approaches to addressing this limitation later in the chapter. 3.1.4 Langevin Sampling with Score Functions Sampling from EBMs, defined by the energy function Eϕ(x), can be performed using Langevin dynamics. We first present the discrete-time Langevin update and then its continuous-time limit as stochastic differential equation (SDE). Finally, we discuss the physical intuition behind how Langevin dynamics enables efficient exploration of complex energy landscapes. 3.1. Energy-Based Models 61 Figure 3.3: Illustration of Langevin sampling. Langevin sampling using the score function log pϕ(x) to guide trajectories toward high-density regions via the update in Equation (3.1.5) (indicating by arrows). Discrete-Time Langevin Dynamics. The discrete-time Langevin update is xn+1 = xn ηxEϕ(xn) + p2ηϵn, = 0, 1, 2, . . . , (3.1.4) where x0 is initialized from some distribution (often Gaussian), η > 0 is the step size, and ϵn (0, I) is Gaussian noise. The noise enables exploration beyond local minima by adding stochasticity. Since the score function can be computed as log pϕ(x) = xEϕ(x). the update can equivalently be written as xn+1 = xn + ηx log pϕ(xn) + p2ηϵn, (3.1.5) where the score function guides the samples toward high-density regions. This formulation is central to diffusion models, as will be detailed later. Continuous-Time Langevin Dynamics. As the step size η approaches zero, the discrete Langevin updates naturally converge to continuous-time process described by the Langevin Stochastic Differential Equation (SDE)1 : dx(t) = log pϕ(x(t)) dt + 2 dw(t), (3.1.6) 1With the factor 2, the Langevin dynamics leave pϕ unchanged in time. Namely, pϕ is stationary: if x(0) pϕ then x(t) pϕ for all 0. Equivalently, pϕ is the stationary 62 Score-Based Perspective: From EBMs to NCSN where w(t) denotes standard Brownian motion (also known as Wiener process2). It is important to understand that the discrete update rule in Equation (3.1.4) serves as the EulerMaruyama discretization of this continuous SDE. Under standard regularity assumptions (e.g., pϕ eEϕ with confining, sufficiently smooth Eϕ), the distribution of x(t) converges (exponentially fast) to pϕ as ; thus we can sample by simulating (solving) the SDE Equation (3.1.6). Why Langevin Sampling? natural way to understand Langevin sampling is through the lens of physics, where the energy function Eϕ(x) defines potential landscape that shapes the behavior of particles. According to Newtonian dynamics, the motion of particle under the force field derived from this energy is described by the ordinary differential equation (ODE) dx(t) = xEϕ (cid:0)x(t)(cid:1) dt, which deterministically drives the particle downhill toward local minimum of the energy function. However, such deterministic dynamics can become trapped in local minima, preventing exploration of the full data distribution. To overcome this limitation, Langevin dynamics introduces stochastic perturbations, resulting in the SDE dx(t) = xEϕ (cid:0)x(t)(cid:1) dt + 2 dw(t) } {z injected noise , where w(t) is standard Brownian motion. The noise term allows the particle to escape local minima by crossing energy barriers, making the trajectory stochastic process whose stationary distribution converges to the Boltzmann distribution pϕ(x) eEϕ(x). From this perspective, EBMs can be viewed as learning force field that pushes samples toward regions of high probability. Langevin sampling is solution of the FokkerPlanck equation (see Chapter B): tρ = (ρ log pϕ) + σ2 2 ρ. Setting ρ = pϕ gives ( σ2 2 1)pϕ = 0, which holds only if σ = 2Brownian increments satisfy w(t + η) w(t) (0, ηI). EulerMaruyama therefore η uses step noise factor.; this is the source of the square-root scaling. For detailed introduction to Brownian motion and SDEs, please refer to Chapter A. 2ηϵn with ϵn (0, I), which explains the 2[w(t + η) w(t)] = 2. 3.1. Energy-Based Models 63 particularly useful for EBMs because it provides practical method to generate samples from the model distribution pϕ(x) without explicitly computing the partition function. By iteratively applying the Langevin update, one obtains samples that approximate the target distribution. Inherent Challenges of Langevin Sampling. Langevin dynamics, widely used MCMC-based sampler, faces serious limitations in high-dimensional spaces. Its efficiency is highly sensitive to the choice of step size η, noise scale, and the number of iterations required to approximate the target distribution accurately. At the heart of this inefficiency lies the issue of poor mixing time: In complex data distributions with many isolated modes, Langevin sampling often requires an extremely long time to transition between regions of high probability. This problem becomes significantly worse as dimensionality increases, leading to prohibitively slow convergence. One can think of sampling as exploring vast and rugged landscape with many distant valleys, each corresponding to different data mode. Langevin dynamics, relying on local stochastic updates, struggles to traverse between these valleys efficiently. As result, it often fails to capture the full diversity of the distribution. This inefficiency hints the need for more structured and guided sampling methods that can navigate complex data manifolds more effectively than purely random exploration. 64 Score-Based Perspective: From EBMs to NCSN 3.2 From Energy-Based to Score-Based Generative Models EBMs show that generation depends only on the score, which points toward regions of higher probability, rather than on the full normalized density. While score matching avoids the partition function, training through the energy still requires expensive second derivatives. The key idea is that since sampling with Langevin dynamics needs only the score, we can learn it directly with neural network. This shift, from modeling energies to modeling scores, forms the foundation of score-based generative models. Figure 3.4: Illustration of Score Matching. The neural network score sϕ(x) is trained to match the ground truth score s(x) using MSE loss. Both are represented as vector fields. 3.2.1 Training with Score Matching Score Matching. To approximate the score function s(x) = log pdata(x) from samples of pdata, we approximate it directly as vector field parameterized by neural network sϕ(x) (see Figure 3.4): sϕ(x) s(x). Score matching fits this vector field by minimizing the mean squared error (MSE) between the true and estimated scores: 3.2. From Energy-Based to Score-Based Generative Models 65 LSM(ϕ) := Expdata 1 2 sϕ(x) s(x)2 2 . (3.2.1) Tractable Score Matching. At first glance, this objective seems infeasible because the true score s(x), which serves as the regression target, is unknown. Fortunately, Hyvärinen and Dayan (2005) showed that integration by parts yields an equivalent objective that depends only on the model sϕ and the data samples, without requiring access to the true score. We state this key result in the following proposition: Proposition 3.2.1: Hyvärinens Tractable Form of SM We can express the following equation as: LSM(ϕ) = eLSM(ϕ) + C. where eLSM(ϕ) := Expdata(x) (cid:20) Tr (xsϕ(x)) + sϕ(x)2 2 (cid:21) , 1 (3.2.2) and is constant that does not depend on ϕ. The minimizer is obtained as: s() = log p(). Proof for Proposition. The result follows by expanding the MSE in LSM and applying integration by parts. The proof is given in Section D.2.1. Using the equivalent objective in Equation (3.2.2), we train the score model sϕ(x) solely from observed samples of pdata, eliminating the need for the true score function. Intuition of Equation (3.2.2). The alternative score matching objective eLSM(ϕ) can be understood directly from its two terms. The norm term 1 2 sϕ(x)2 suppresses the score in regions where pdata is large, making them stationary. The divergence term Tr(xsϕ(x)) favors negative values, so these stationary points act as attractive sinks. Together, the loss shapes high-density regions into stable and contracting points of the score field. We explain this in detail below. 66 Score-Based Perspective: From EBMs to NCSN Stationarity from the Magnitude Term. Since the expectation in eLSM(ϕ) is taken under pdata, so regions where pdata(x) is large (high data density) contribute most to the loss. The magnitude term 1 2 sϕ(x)2 therefore drives sϕ(x) 0 precisely in those high-probability areas, i.e., those locations become stationary. Concavity When the Field is (Approximately) Gradient. The divergence term Tr(xsϕ(x)) encourages the vector field to have negative divergence in regions of high data density. Negative divergence means that nearby vectors converge rather than spread out, so stationary point in such region acts as sink: nearby trajectories are pulled inward. To make this precise, assume sϕ = xu for scalar function : RD R, as is natural when matching log density. Then xsϕ = 2 xu(x)) (the divergence). xu (the Hessian) and sϕ(x) = Tr(2 At stationary point x, where sϕ(x) = xu(x) = 0, second order Taylor expansion gives u(x) = u(x) + 1 2 (x x)2 xu(x)(x x) + o(x x2). If the Hessian 2 xu(x) is negative definite, then is locally concave at and the log density attains strict local maximum3 there. Because all eigenvalues of the Hessian are negative, the trace is also negative: Tr(2 xu(x)) < 0. Thus the learned vector field has negative divergence and the stationary point is sink: small perturbations are contracted back toward x. 3.2.2 Sampling with Langevin Dynamics Once trained by minimizing Equation (3.2.2), the score model sϕ(x) can replace the oracle score in Langevin dynamics for sampling: xn+1 = xn + ηsϕ(xn) + p2ηϵn, ϵn (0, I), (3.2.3) for = 0, 1, 2, . . . , initialized at x0. As in the EBM case Equation (3.1.6), this recursion is precisely the EulerMaruyama discretization of the continuoustime Langevin SDE: dx(t) = sϕ(x(t)) dt + 2 dw(t), 3We remark that strict concavity (and thus strict local maximum of the log density) requires the entire Hessian 2 xu to be negative definite, not merely to have negative trace. negative trace guarantees that the sum of eigenvalues is negative, but some eigenvalues could still be positive, leading to saddle point rather than maximum. 3.2. From Energy-Based to Score-Based Generative Models 67 with initialization x(0). Hence, in the limit of small step size, the discrete and continuous formulations coincide. In practice, one can either run the discrete sampler or directly simulate the SDE. 3.2.3 Prologue: Score-Based Generative Models In the remainder of this chapter, we examine the foundational role of the score function in modern diffusion models. Initially introduced to enable efficient training of EBMs, the score function has evolved into central component of new generation of generative models. Building on this foundation, we explore how the score function informs the theoretical formulation and practical implementation of score-based diffusion models, offering principled framework for data generation via stochastic processes. 68 Score-Based Perspective: From EBMs to NCSN 3.3 Denoising Score Matching 3.3.1 Motivation Although the alternative objective in Equation (3.2.2) eLSM(ϕ) = Expdata (cid:20) Tr(cid:0)xsϕ(x)(cid:1) + (cid:21) sϕ(x)2 2 1 is more tractable, it still requires computing the trace of the Jacobian, Tr(xsϕ(x)), which has worst-case complexity O(D2). Such complexity limits scalability to high-dimensional data. To address this, sliced score matching (Song et al., 2020b) replaces the trace term with stochastic estimate based on random projections. We briefly outline the idea below. Sliced Score Matching and Hutchinsons Estimator. Sliced score matching replaces the trace in score matching by averaging directional derivatives along random slices. Let RD be an isotropic random vector (e.g., Rademacher or standard Gaussian) with E[u] = 0 and E[uu] = I. By Hutchinsons identity Tr(A) = Eu[uAu], and Eu[(usϕ(x))2] = sϕ(x)2 2, we obtain the exact form eLSM(ϕ) = Ex,u u(cid:0)xsϕ(x)(cid:1)u + 1 2 (usϕ(x))2i . This objective can be evaluated efficiently with automatic differentiation, using Jacobianand vector-Jacobian-product operations (JVP/VJP) instead of explicitly computing large Jacobian or Hessian matrices. Averaging over random probes yields an unbiased estimator with variance O(1/K), and the directional term u(xsϕ)u can be computed efficiently using JVP/VJP routines without explicit Jacobians. Intuitively, this means we only check the models behavior along random directions: the projected score is nudged to align with regions of higher data density, so data points become stationary in expectation. From Sliced to Denoising Score Matching. Sliced score matching sidesteps Jacobians but still relies on the raw data distribution. This makes it fragile: for image data lying on low-dimensional manifolds, the score log pdata(x) may be undefined or unstable, and the method only constrains the vector field 3.3. Denoising Score Matching 69 at observed points, providing weak control in their neighborhoods. It further suffers from probe-induced variance and repeated JVP/VJP costs. more robust alternative, which we focus on here, is Denoising Score Matching (DSM) (Vincent, 2011), which offers principled and scalable solution. 3.3.2 Training Let us revisit the SM loss in Equation (3.2.1): LSM(ϕ) = Expdata(x) 1 2 sϕ(x) log pdata(x)2 2 , where the issue arises from the intractable term log pdata(x). Vincent (2011)s Solution by Conditioning. To overcome the intractability of log pdata(x), Vincent (2011) proposed injecting noise into the data pdata via known conditional distribution pσ(xx) with scale σ. The neural network sϕ(x; σ) is trained to approximate the score of the marginal perturbed distribution pσ(x) = pσ(xx)pdata(x) dx by minimizing the loss LSM(ϕ; σ) := Expσ 1 2 sϕ(x; σ) log pσ(x)2 . (3.3.1) Even though log pσ(x) is generally intractable, Vincent (2011) showed that conditioning on pdata yields an equivalent, tractable objectivethe Denoising Score Matching (DSM) loss: LDSM(ϕ; σ) := 1 Expdata,xpσ(x) sϕ(x; σ) log pσ(xx)2 2 (3.3.2) . The optimal minimizer of Equation (3.3.2) satisfies s(x; σ) = log pσ(x), which is also optimal for Equation (3.3.1). For example, when pσ(xx) is Gaussian noise with variance σ2, pσ(xx) = (x; x, σ2I), 70 Score-Based Perspective: From EBMs to NCSN the gradient log pσ(xx) has closed form (see Equation (3.3.4)), making the regression target explicit and computationally tractable. Moreover, as σ 0, pσ(x) pdata(x) and s(x; σ) = log pσ(x) log pdata(x), indicating the learned score approximates the original data score, enabling its use in generation. We formalize this discussion on the gradient equivalence between LSM and LDSM in the following theorem: Theorem 3.3.1: Equivalence of LSM and LDSM For any fixed noise scale σ > 0, the following holds: LSM(ϕ; σ) = LDSM(ϕ; σ) + C, (3.3.3) where is constant independent of the parameter ϕ. Furthermore, the minimizer s(; σ) of both losses satisfies s(x; σ) = log pσ(x), for almost every x. Proof for Theorem. The equivalence follows from direct computation: by expanding the MSE in LSM and LDSM, all ϕ-dependent terms cancel, leaving only constant difference independent of ϕ. The derivation of the minimizer follows the same argument as in Proposition 4.2.1. This theorem, like Theorem 2.2.1 in DDPM, illustrates key shared principle: Insight 3.3.1: Conditioning Technique The conditioning technique also appears in the variational view of diffusion models in DDPM (see Theorem 2.2.1), where conditioning on data point turns an intractable loss into tractable one for Monte Carlo estimation. similar idea arises in the flow-based perspective (e.g., Flow Matching (Lipman et al., 2022)), as we will see in Section 5.2. Special Case: Additive Gaussian Noise. We now consider the common case where Gaussian noise (0, σ2I) with variance σ2 is added to each data point 3.3. Denoising Score Matching 71 Figure 3.5: Illustration of DSM via the conditioning technique. By perturbing the data distribution pdata with small additive Gaussian noise (0, σ2I), the resulting conditional distribution pσ(xx) = (x; x, σ2I) admits closed-form score function. pdata: so that the corrupted data follows = + σϵ, ϵ (0, I), pσ(xx) = (x; x, σ2I). In this setting, the conditional score is analytically given by log pσ(xx) = σ . Hence, the DSM loss simplifies to: LDSM(ϕ; σ) = = 1 2 1 Ex,xx \"(cid:13) (cid:13) sϕ(x; σ) (cid:13) (cid:13) σ2 Ex,ϵ \"(cid:13) (cid:13) sϕ(x + σϵ; σ) + (cid:13) (cid:13) ϵ σ (cid:13) 2 (cid:13) (cid:13) (cid:13) 2 # (cid:13) 2 (cid:13) (cid:13) (cid:13) 2 # (3.3.4) , where ϵ (0, I). This objective forms the core of the (score-based) Diffusion Model. When the noise level σ is small, the Gaussian smoothed marginal pσ = pdata (0, σ2I), so their high density regions and scores nearly coincide: log pσ(x) log pdata(x). Consequently, taking small step along the noisy score direction log pσ moves noisy sample toward essentially the same high likelihood regions of the clean distribution, which is similar to the intuition behind score matching summarized in Section 3.2.1. By contrast, 72 Score-Based Perspective: From EBMs to NCSN when σ is large, the smoothing over simplifies the landscape: pσ washes out local modes and its score mostly pulls toward global mass (think shrinkage toward the mean), yielding coarse denoising that can over smooth. In practice, however, DSM typically assumes that the injected noise is small and mild. To better see why the objective naturally corresponds to denoising process, we expand on the discussion in Sections 3.3.4 and 3.3.5. 3.3.3 Sampling Once we have trained score model sϕ(x; σ) at noise level σ, we generate samples using Langevin dynamics by replacing the true score with the learned model. The update rule is: xn+1 = xn + η sϕ(xn; σ) } {z log pσ(xn) +p2ηϵn, ϵn (0, I), (3.3.5) for = 0, 1, 2, . . . , starting from an initial value x0. If σ is sufficiently small, then after enough iterations, xn approximates samples from pdata. Advantages of Noise Injection. We additionally remark that, compared to vanilla score matching in Equation (3.2.1), injecting Gaussian noise to form pσ (e.g., Equation (3.3.4)) provides two key advantages (Song and Ermon, 2019): Well-Defined Gradients. The noise perturbs data away from its lowerdimensional manifold, resulting in distribution pσ with full support in RD. Consequently, the score function log pσ(x) is well-defined everywhere. Improved Coverage. The noise smooths out sparse regions between modes, enhancing training signal quality and facilitating Langevin dynamics to traverse low-density regions more effectively. 3.3.4 Why DSM is Denoising: Tweedies Formula We begin with Tweedies formula (Efron, 2011), which provides principled basis for denoising from noisy observations alone. Concretely, it states that: given single Gaussiancorrupted observation ( ; αx, σ2I) from an unknown pdata, denoised estimate (the average over all plausible clean signals given x) is obtained by nudging step of size σ2 in the direction of the score log pσ(x) of its noisy marginal defined as: pσ(x) := (x; αx0, σ2I)pdata(x) dx. 3.3. Denoising Score Matching 73 We present the proposition formally below. Lemma 3.3.2: Tweedies Formula Assume pdata and, conditionally on x, ( ; αx, σ2I) with α = 0. Then Tweedies formula states αExp(xx) (cid:2)x(cid:12) (cid:12)x(cid:3) = + σ2x log pσ(x), (3.3.6) where the expectation is taken over the posterior distribution p(xx) of given x. Proof for Lemma. The proof proceeds by computing the score of the marginal p(x) = p(xx)pdata(x) dx. Differentiating under the integral and using the Gaussian form of the conditional density leads directly to an expression that rearranges into the desired identity linking the score with the posterior mean. See Section D.2.3 for details. Tweedies formula plays central role in diffusion models, where multiple layers of noise are introduced as in DDPM. It enables the estimation of clean samples from noisy observations via the score function, thereby establishing fundamental link between score prediction and denoiser: [xx] {z } denoiser estimated from = 1 α (cid:16) (cid:17) + σ2x log pσ(x) . Especially, single gradient-ascent step on the noisy log-likelihood with the particular step size σ2 is the denoised estimate (the conditional average clean signal). This makes DSM training and denoising tightly related: if sϕ(x) log pσ(x) trained from DSM, then (cid:16) (cid:17) + σ2sϕ(x) 1 α is the denoiser. (Optional) Higher Order Tweedies Formula. The classical Tweedies formula expresses the posterior mean E[x0x] through the gradient log p(x). Higher order extensions (Meng et al., 2021a) express the posterior covariance and higher cumulants through higher derivatives of log p(x). 74 Score-Based Perspective: From EBMs to NCSN Exponential Family Setup with the Log-Normalizer λ(x). Assume the conditional law of given latent natural parameter η RD belongs to natural exponential family written as qσ(xη) = exp(cid:0)η ψ(η)(cid:1) q0(x). Here q0(x) is the base measure, namely the part that does not depend on η; for additive Gaussian noise with variance σ2I it equals (2πσ2)D/2 exp(x2/2σ2). Let p(η) be the pre-defined distribution of the latent natural parameter, which can be viewed as the reparameterized clean-data distribution (for Gaussian location, η = x/σ2). The observed noisy marginal is pσ(x) = qσ(xη) p(η) dη. Define the log-partition (log-normalizer) in by λ(x) := log pσ(x) log q0(x). Then the posterior of η given is p(ηx) exp(cid:0)η ψ(η) λ(x)(cid:1) p(η), which shows that, as function of x, the posterior has exponential-family form with natural parameter x, sufficient statistic η, and log-partition λ(x). Derivatives of λ Produce Posterior Cumulants. Two simple rules are at play. First, normalization: for every x, exp(cid:0)η ψ(η) λ(x)(cid:1) p(η) dη = 1. Differentiating this identity with respect to brings down powers of η from the exponential and derivatives of λ(x); setting the result to zero yields equalities between derivatives of λ and posterior moments of η. Second, standard property of exponential families: the log-partition is the cumulant generating function of the sufficient statistic. Therefore xλ(x) = E[ηx], 2 xλ(x) = Cov[ηx], (k) where κk are the conditional cumulants of order of the random vector η given x, obtained via the standard momentcumulant relations. λ(x) = κk(ηx) (k 3), These are the higher order Tweedies formulas. Specializing to the Gaussian location model with η = x/σ2 yields the familiar forms in terms of derivatives of log pσ(x): E[xx] = + σ2x log pσ(x), Cov[xx] = σ2I + σ42 log pσ(x), 3.3. Denoising Score Matching 75 and higher cumulants scale with higher derivatives of log pσ(x). Several studies have explored training neural networks to estimate higher order scores (Meng et al., 2021a; Lu et al., 2022a; Lai et al., 2023a). In contrast, our aim is to clarify their relationship with statistical quantities, and we refer the reader to these works for methodological details. 3.3.5 (Optional) Why DSM is Denoising: SURE SURE (Steins Unbiased Risk Estimator). At high level, Steins Unbiased Risk Estimator (SURE) is technique that allows one to estimate the mean squared error (MSE) of denoiser without knowing the clean signal. In other words, SURE provides way to select or train denoisers when only noisy data are available. For clarity, consider the additive Gaussian noise setting: = + σϵ, ϵ (0, I), where RD is the unknown clean signal and is the observed noisy version. denoiser is any (weakly differentiable) mapping : RD RD that produces an estimate D(x) of x. The natural quality measure is the conditional MSE R(D; x) := Exx D(x) x2 2 (cid:12) (cid:12) . This quantity depends on the unknown ground truth x, and therefore cannot be computed directly. Steins identity (see Section D.2.4), however, yields the following observable surrogate: SURE(D; x) = D(x) 2 + 2σ2 D(x) Dσ2, (3.3.7) where D(x) denotes the divergence of D. We emphasize that SURE(D; x) requires only the noisy observation x, not the clean x. Intuitively, SURE consists of two parts that complement each other. The term D(x) x2 measures how far the denoisers output is from the noisy input; by itself this underestimates the true error since is already corrupted. The divergence term acts as correction: it captures how sensitive the denoiser is to small perturbations in its input, effectively accounting for the variance introduced by the noise. Importantly, for any fixed but unknown x, Exx (cid:2)SURE(D; + σϵ) (cid:12) (cid:12) x(cid:3) = R(D; x), where the expectation is over the Gaussian noise ϵ (0, I). Thus, minimizing SURE (in expectation or empirically) is equivalent to minimizing the true 76 Score-Based Perspective: From EBMs to NCSN MSE, while relying only on noisy data. In practice, averaging SURE over both pdata and the corruption noise ϵ yields an unbiased estimate of the global MSE risk. Link to Tweedies Formula and Bayes Optimality. Let pσ(x) = (cid:0)pdata (0, σ2I)(cid:1)(x) denote the noisy marginal considered in this section. SURE is an unbiased estimator of the mean squared error with respect to the noise, conditional on x: Exx (cid:2)SURE(D; x)(cid:3) = Exx (cid:2)D(x) x2(cid:3). (cid:2)D(x) x2(cid:3) = Ex Hence minimizing the expected SURE equals minimizing the Bayes risk (cid:2)D(x) x2(cid:3)(cid:3) by the law of total expectation E(x,x) (tower property). This decomposition yields pointwise optimization: for almost every x, (cid:2)Exx D(x) = arg min Exx (cid:2)z x2(cid:3) = E[xx]. Therefore the SURE-optimal denoiser coincides with the Bayes estimator in Section 3.3.4, and by Tweedies identity: D(x) = E[xx] = + σ2x log pσ(x). (3.3.8) Relationship of SURE and Score Matching. The identity in Equation (3.3.8) motivates parameterizing the denoiser via score field: D(x) = + σ2sϕ(x; σ), with sϕ(; σ) meant to approximate the noisy score log pσ(). Plugging D(x) = + σ2sϕ(x; σ) in Equation (3.3.7) gives 2σ4 SURE(D; x) = Tr(cid:0)xsϕ(x; σ)(cid:1) + 1 2 sϕ(x; σ)2 2 + const(σ). 1 Therefore, taking expectation with respect to pσ, minimizing SURE is equivalent (up to an additive constant) to minimizing Hyvärinens alternative score matching objective at noise level σ, with the expectation taken under pσ (see Equation (3.2.2)). Consequently, both objectives share the same minimizer, namely the denoiser in Equation (3.3.8). 3.3. (Optional) Generalized Score Matching Motivation. Classical score matching, denoising score matching, and higher order variants all target Lp(x) p(x) , for some density 3.3. Denoising Score Matching 77 with linear operator acting on the density. In the classical case = x, this gives log p(x) = xp(x) p(x) The Lp structure allows integration by parts to remove normalizing constants, yielding tractable objective that depends only on samples from and the learned field sϕ. This viewpoint motivates the generalized score matching framework. Generalized Fisher Divergence. Let be the data distribution and any model distribution. For linear operator on scalar functions of x, define the generalized Fisher divergence DL(p q) := p(x) (cid:13) (cid:13) (cid:13) (cid:13) Lp(x) p(x) Lq(x) q(x) 2 (cid:13) (cid:13) (cid:13) (cid:13) 2 dx. If is complete, i.e., Lp1 p1 = Lp2 p2 a.e. implies p1 = p2 a.e., then DL(p q) = 0 identifies = p. For = this recovers the classical Fisher divergence (see Equation (1.1.3)). Score Parameterization. In practice we do not model normalized density q. Instead, we directly parameterize vector field sϕ(x) to approximate the generalized score Lp(x) p(x) . Consider DL(p sϕ) := Exp \"(cid:13) (cid:13) (cid:13) (cid:13) sϕ(x) Lp(x) p(x) # . 2 (cid:13) (cid:13) (cid:13) (cid:13) 2 Although Lp(x) p(x) on sϕ. Let be the adjoint of L, defined by is unknown, integration by parts makes the loss depend only (cid:0)Lf (cid:1)g = f (Lg) for all test functions f, g, which formally moves across the integral when boundary terms vanish. Expanding the square and applying this identity yields the tractable objective LGSM(ϕ) = Exp sϕ(x)2 2 (cid:0)Lsϕ (cid:21) (cid:1)(x) + const, (cid:20) 1 2 78 Score-Based Perspective: From EBMs to NCSN where the constant does not depend on ϕ. We use only through expectations, so the generalized score matching loss admits an empirical estimator from training data, exactly as in classical score matching. For = we have = , which recovers Hyvärinens score matching (cid:3) in Equation (3.2.2). objective Ep 2 + sϕ (cid:2) 1 2 sϕ2 Examples of Operators. Classical Score Matching. Consider = x. Then the generalized score reduces to the classical score function Lp(x) p(x) = log p(x). Denoising Score Matching. For additive Gaussian noise, define the operator Then (Lf )(x) = (x) + σ2xf (x). Lpσ(x) pσ(x) = + σ2x log pσ(x) = E[x0x], with pσ(x) := (x; αx0, σ2I)pdata(x) dx and = + σϵ. This is exactly the Tweedies identity. Minimizing LGSM with this operator trains sϕ to approximate the denoiser, recovering the denoising score matching objective. Higher Order Targets. Stacking derivatives inside exposes 2 log and higher derivatives, which align with posterior covariance and higher order cumulants. Extensions and Use Cases. Generalized score matching extends beyond continuous variables to discrete settings, including language modeling (Meng et al., 2022; Lou et al., 2024). It also motivates score inspired training that yields denoising style objectives. This operator view unifies range of objectives, admits empirical estimation from data, and offers general principle for designing loss functions through suitable choices of L. 3.4. Multi-Noise Levels of Denoising Score Matching (NCSN) 79 3.4 Multi-Noise Levels of Denoising Score Matching (NCSN) 3.4.1 Motivation Adding Gaussian noise with single fixed variance to the data distribution smooths it to certain extent, but training score-based model at only one noise level introduces key limitations. At low levels of injected noise, Langevin dynamics struggles to traverse modes in multi-modal distributions due to vanishing gradients in low-density regions. In contrast, at high noise levels, sampling becomes easier, but the model captures only coarse structures, resulting in blurry samples that lack fine detail. Furthermore, Langevin dynamics can be slow to converge or even fail in high-dimensional spaces. Since it depends on the gradient of the log-density for guidance, poor initialization, particularly in plateau regions or near saddle points, can impede exploration or cause the sampler to get trapped in single mode. Figure 3.6: Illustration of SM inaccuracy (revisiting Figure 3.4). the red region indicates low-density areas with potentially inaccurate score estimates due to limited sample coverage, while high-density regions tend to yield more accurate estimates. To address these challenges, Song and Ermon (2019) propose injecting Gaussian noise at multiple levels into the data distribution and jointly training noise-conditional score network (NCSN) to estimate score functions across range of noise scales. During generation, Langevin dynamics is applied in noise-annealed fashion: beginning with high-noise levels to enable coarse exploration, and gradually refining toward low-noise levels to recover fine 80 details. Score-Based Perspective: From EBMs to NCSN Figure 3.7: Illustration of NCSN. The forward process perturbs the data with multiple levels of additive Gaussian noise pσ(xσx). Generation proceeds via Langevin sampling at each noise level, using the result from the current level to initialize sampling at the next lower variance. 3.4.2 Training To overcome the limitations of score-based models trained at single noise level, Song and Ermon (2019) propose adding Gaussian noise at multiple levels to the data distribution. Specifically, sequence of noise levels {σi}L i=1 is chosen such that 0 < σ1 < σ2 < < σL, where σ1 is small enough to preserve most of the datas fine details, and σL is large enough to sufficiently smooth the distribution, facilitating easier training. Each noisy sample is constructed by perturbing clean data point pdata as xσ = + σϵ with ϵ (0, I). This defines the Perturbation Kernel: pσ(xσx) := (xσ; x, σ2I), which induces the Marginal Distribution: pσ(xσ) = pσ(xσx)pdata(x) dx, at each noise level σ. It presents the Gaussian smoothed data distribution. 3.4. Multi-Noise Levels of Denoising Score Matching (NCSN) 81 Training Objective of NCSN. The goal is to train noise-conditional score network sϕ(x, σ) to estimate the score function log pσ(x) for all σ {σi}L i=1. This is achieved by minimizing the DSM objective across all noise levels: LNCSN(ϕ) := i= λ(σi)LDSM(ϕ; σi), (3.4.1) where LDSM(ϕ; σ) = 1 2 Expdata(x),xpσ(xx) \"(cid:13) (cid:13) sϕ(x, σ) (cid:13) (cid:13) (cid:18) σ2 (cid:19)(cid:13) 2 (cid:13) (cid:13) (cid:13) 2 # , and λ(σi) > 0 is weighting function for each scale. Minimizing this objective yields the score model s(x, σ) that recovers the true score at each noise level: s(, σ) = log pσ(), for all σ {σi}L i=1, as it is essentially DSM minimization (see Theorem 3.3.1). Relationship with DDPM Loss. Let xσ = + σϵ with ϵ (0, I) and let pσ denote the marginal distribution. By Tweedies formula, xσ log pσ(xσ) = 1 σ [ϵxσ] . Thus the NCSN optimum is the true score s(xσ, σ) = xσ log pσ(xσ), while the Bayes optimal noise predictor under the DDPM loss Equation (2.2.10) is ϵ(xσ, σ) = E[ϵxσ]. They are exactly equivalent via s(xσ, σ) = 1 σ ϵ(xσ, σ), ϵ(xσ, σ) = σs(xσ, σ). In the DDPMs perturbation Equation (2.2.9) with discrete index i, the same relation gives xi = αix0 + 1 α2 s(xi, i) = 1 σi [ϵxi] , so minimizing Equation (2.2.10) learns the conditional denoiser for ϵ, which is scaled reparameterization of the true score at noise level i. We will systematically compare and summarize this equivalence of parameterizations in Chapter 6. 82 Score-Based Perspective: From EBMs to NCSN Algorithm 1 Annealed Langevin Dynamics Input: Trained score sϕ(, σℓ), step sizes ηℓ, and Langevin iteration budgets Nℓ for each noise level ℓ = L, . . . , 1: xσL (0, I) 2: for ℓ = L, . . . , 2 do x0 xσℓ 3: Initialize Langevin from previous noise levels output 4: 5: 6: 7: 8: for = 0 to Nℓ 1 do ϵn (0, I) xn+1 xn + ηℓsϕ(xn, σℓ) + 2ηℓϵn end for xσℓ1 xNℓ Output used as initialization for next noise level 9: end for Output: xσ1 3.4.3 Sampling With trained score networks available at multiple noise levels sϕ(, σ1), sϕ(, σ2), , sϕ(, σL1), sϕ(, σL), the sampling procedure known as annealed Langevin dynamics (Song and Ermon, 2019) generates data by progressively denoising from high noise level σL down to low noise level σ1 0. Starting from Gaussian noise xσL (0, I), the algorithm applies Langevin dynamics at each noise level σℓ to approximately sample from the perturbed distribution pσℓ(x). The output at level σℓ is used to provide better initialization at the next lower noise level σℓ1. At each level, Langevin dynamics iteratively updates: xn+1 = xn + ηℓsϕ(xn, σℓ) + p2ηℓϵn, ϵn (0, I), starting from x0 := xσℓ. The step size is typically scaled by the noise level: ηℓ = δ σ2 ℓ σ2 1 , for some fixed δ > 0. This noise-annealed refinement proceeds down to the lowest noise level σ1, where the final sample xσ1 is obtained. By progressively using the output of the previous level as better initialization for the next, this strategy enables more effective exploration and improved coverage of complex data distributions. Algorithm 1 summarizes the procedure. 3.4. Multi-Noise Levels of Denoising Score Matching (NCSN) 83 Slow Sampling Speed of NCSN. NCSN generates samples using annealed MCMC (commonly Langevin dynamics) across noise scales {σi}L i=1. For each scale σi, it performs iterative updates of the form update xn using the score sϕ(xn, σi) plus small random perturbation, each requiring forward pass through the score network. Two factors necessitate large K: (i) Local Accuracy and Stability: the learned score is reliable only for small perturbations, requiring small step sizes and many iterations per noise level to avoid bias or instability; (ii) Slow Mixing in High Dimensions: local MCMC moves explore multimodal, high-dimensional targets inefficiently, demanding many iterations to reach typical data regions. Because updates are strictly sequential (each iteration depends on the previous one) and each requires an expensive network evaluation, the overall cost is O(LK) sequential network passes, making sampling computationally slow. 84 Score-Based Perspective: From EBMs to NCSN 3.5 Summary: Comparative View of NCSN and DDPM Comparison. We begin by comparing the graphical models of NCSN and DDPM in Figure 3.7, with key differences and similarities summarized in Table 3.1. Table 3.1: Comparisons of NCSN and DDPM xi+1xi xix pprior Loss NCSN Derive as xi+1 = xi + i+1 σ2 σ2 ϵ Define as xi = + σ2 ϵ (0, σ2 (cid:20)(cid:13) (cid:13)sϕ(xi, σi) + ϵ (cid:13) LI) σi (cid:13) 2 (cid:13) (cid:13) 2 EiEpdata(x)EϵN (0,I) DDPM Define as xi+1 = 1 βixi + βiϵ Derive as xi = αix + p1 α2 ϵ (0, I) (cid:21) EiEpdata(x)EϵN (0,I) ϵϕ(xi, i) ϵ2 2 Sampling Apply Langevin per layer; use output to initialize the next Traversing the Markovian chain with pϕ (xi1xi) Shared Bottleneck. Despite their different formulations, both NCSN and DDPM rely on dense time discretization. This leads to critical limitation: sampling often requires hundreds or even thousands of iterations, making generation slow and computationally intensive. Question 3.5.1 How can we accelerate sampling in diffusion models? We will revisit this challenge in Chapter 9 and Chapter 10. 3.6. Closing Remarks 3.6 Closing Remarks This chapter has charted second major path to diffusion models, beginning from the score-based perspective rooted in Energy-Based Models (EBMs). We started by identifying the core challenge of EBMsthe intractable partition functionand introduced the score function, log p(x), as powerful tool that circumvents this issue entirely. Our journey led us from classic score matching to its more scalable and robust variant, Denoising Score Matching (DSM). Through DSM, we saw how perturbing data with noise enables tractable training objective, once again leveraging conditioning strategy to create simple regression target. Furthermore, we established profound connection between score estimation and the act of denoising via Tweedies formula, which showed that the score provides the precise direction needed to estimate clean signal from its noisy observation. This principle was then extended from single noise level to continuum with Noise Conditional Score Networks (NCSN), which learn single score model conditioned on multiple noise scales and generate samples via annealed Langevin dynamics. By the end of our exploration, we found that NCSN and the DDPM from the variational view, despite their different origins, share strikingly similar structure and common bottleneck: slow, sequential sampling. This convergence is no coincidence; it hints at deeper, unified mathematical structure. The limitations of these discrete-time models motivate the need for more general framework. In the next chapter, we will take this crucial step: 1. We will move into continuous-time perspective, showing that both DDPMs and NCSNs can be elegantly unified as different discretizations of single, powerful process described by Stochastic Differential Equation (SDE). 2. This Score SDE framework will formally connect the variational and score-based views, recasting the problem of generation as one of solving differential equation. This unifying lens will not only provide profound theoretical clarity but also unlock new class of advanced numerical methods designed to tackle the fundamental challenge of slow sampling. 4 Diffusion Models Today: Score SDE Framework There is only one precise way of presenting the laws, and that is by means of differential equations. They have the advantage of being fundamental and, so far as we know, precise. Richard P. Feynman So far, we have studied diffusion models from two perspectives: the variational view and the score-based view, the latter naturally emerging from the EBM formulation. We now take the next step and move to the continuous-time framework. At its core lies the Score SDE, the continuous limit that unifies DDPM and NCSN into single formulation. This perspective is powerful because it extends discrete updates with clean, principled description grounded in differential equations (DE). In this view, generation reduces to solving DE over time. This lets us directly apply tools from numerical analysis: for example, the basic Euler method can simulate the dynamics, while more advanced solvers improve stability and efficiency. By working in continuous time, we also gain richer mathematical structure and unified foundation for understanding, analyzing, and improving diffusion models. This perspective will be developed further in this monograph. 86 4.1. Score SDE: Its Principles 87 Figure 4.1: Illustration of the discrete-time noise-adding step. It adds noise from to + with mean drift (xt, t) and diffusion coefficient g(t). 4.1 Score SDE: Its Principles The use of multiple noise scales has been crucial ingredient in the success of NCSN and DDPM frameworks. In this section, we introduce the foundation of the Score SDE (Song et al., 2020c), which elevates this idea by considering continuum of noise levels. continuous-time limit of forward and reverse diffusion processes had already been noted by Sohl-Dickstein et al. (2015), but Song et al. (2020c) make this perspective central by formulating the data evolution as stochastic/ordinary differential equation, where the noise level increases smoothly over time. This continuous-time formulation not only unifies prior discrete-time models but also provides principled and flexible foundation for generative modeling by casting it as the problem of solving differential equations. 4.1.1 Motivation: From Discrete to Continuous-Time Processes We revisit the forward noise injection schemes of NCSN and DDPM. NCSN uses sequence of increasing noise levels {σi}L i=1. Each clean sample pdata is perturbed as xσi = + σiϵi, ϵi (0, I). DDPM instead injects noise incrementally with variance schedule {βi}L i=1: xi = 1 β2 xi1 + βiϵi, ϵi (0, I). We view them together on discrete time grid, where the sequential update 88 Diffusion Models Today: Score SDE Framework from xt to xt+t takes the form1: NCSN: xt+t = xt + t+t σ2 σ ϵt xt + dσ2 dt tϵt DDPM: xt+t = p1 βtxt + pβtϵt xt βtxtt + pβttϵt, 1 2 where ϵt (0, I). Interestingly, both noise injection processes follow common structural pattern: xt+t xt + (xt, t)t + g(t) tϵt, (4.1.1) with : RD RD and : given by: NCSN: (x, t) = 0, g(t) = dσ2(t) dt DDPM: (x, t) = β(t)x, g(t) = β(t). 1 2 This formulation corresponds to the following Gaussian transition: p(xt+txt) := (cid:16) xt+t; xt + (xt, t)t, g2(t)tI (cid:17) , (4.1.2) where, by slight abuse of notation, we treat xt as fixed sample and xt+t as random variable. As 0 (which can be conceptually understood as preparing infinitely many layers of noises), the discrete time process converges to continuous time SDE evolving forward in time2: dx(t) = (x(t), t) dt + g(t) dw(t), where w(t) is standard Wiener process (or Brownian motion). Remark. While full formal definition is not necessary here, Wiener process is continuous-time stochastic process w(t) that starts at zero, has independent increments, and satisfies that for any < t, the increment w(t) w(s) is normally distributed with mean zero and variance s. It represents the 1For convenience, we use x(t) and xt interchangeably (and similarly for other timedependent variables) to denote samples at time t. 2The forward kernel in Equation (4.1.2) converges, as 0, to the solution of the corresponding Itô SDE. fully rigorous proof relies on advanced results which we defer to the literature. 4.1. Score SDE: Its Principles 89 accumulation of independent Gaussian fluctuations over time, and although it is almost surely continuous, it is nowhere differentiable. Over an infinitesimal time interval [t, + dt], the increment of Wiener process is defined as dw(t) := w(t + dt) w(t), which is modeled as Gaussian random variable with zero mean and variance dt: dw(t) (0, dtI). brief introduction to the foundations of SDEs is provided in Section A.2, with more advanced discussion in Chapter C. However, we can conceptually understand the connection between the discrete and continuous formulations as follows: x(t + t) x(t) dx(t), dt, tϵt (0, tI) dw(t). Once the drift (x, t) and diffusion g(t) are specified, the forward time SDE automatically induces reverse time SDE that transports the terminal noise distribution back to the data distribution. The reverse dynamics involve only single unknown term, surprisingly the score function at each continuous-time level. This identifies score matching as the training objective; once the score is learned, sampling amounts to numerically integrating the reverse time SDE with the learned score. While Section 4.2 presents practical implementations, we first examine the theoretical foundations of the forward and reverse processes in Section 4.1.2 and Section 4.1.3. 4.1.2 Forward-Time SDEs: From Data to Noise With this formulation, earlier methods based on discrete time, such as NCSN (Song and Ermon, 2019) and DDPM (Sohl-Dickstein et al., 2015; Ho et al., 2020), can be unified under the continuous-time framework through stochastic process x(t) governed by forward SDE defined on the interval [0, ]: 90 Diffusion Models Today: Score SDE Framework Figure 4.2: (1D) Visualization of the forward process in diffusion model. The process starts from initial points sampled (denoted as ) from complex bimodal data distribution (p0 = pdata) and evolves toward simple, unimodal Gaussian prior (pT pprior). The background heatmap illustrates the evolving marginal probability density, pt, which smooths over time. Sample trajectories are shown evolving from = 0 to = , comparing the stochastic forward SDE process (blue paths) with its deterministic counterpart, the PF-ODE (white paths). Note that the PF-ODE is deterministic transport map for densities, not generally the mean of sample paths started from single point. dx(t) = (x(t), t) dt + g(t) dw(t), x(0) pdata. (4.1.3) Here, (, t) : RD RD is the drift, g(t) is the scalar diffusion coefficient, and w(t) denotes standard Wiener process. We refer to this as the forward SDE, which describes how clean data is gradually perturbed into noise over time. Once the drift and diffusion coefficient are specified, the forward process is fully determined, describing how the data variable is progressively corrupted through the injection of Gaussian noise. In particular, two families of time-dependent densities are induced: Perturbation Kernels. The conditional law pt(xtx0) describes how clean data sample x0 pdata evolves into its noisy counterpart xt at time t. In general, the drift term (x, t) in Equation (4.1.3) can be an 4.1. Score SDE: Its Principles 91 arbitrary function of x, but common and analytically convenient choice is to assume it is affine: (x, t) = (t)x, (4.1.4) where (t) is scalar function of t, typically taken to be non-positive. Under this structure, the process remains Gaussian at every time, and the conditional distribution admits closed-form solution obtained by solving the associated meanvariance ODEs (Särkkä and Solin, 2019) (see also Section 4.3.3). In particular, pt(xtx0) = (cid:0)xt; m(t), (t)ID (cid:1), with m(t) = exp (cid:16) 0 (cid:17) (u) du x0, (t) = 0 (cid:16) exp f (u) du (cid:17) g2(s) ds, and initial conditions m(0) = x0, (0) = 0. This explicit form allows one to sample xt given x0 directly, without numerically integrating the SDE, hence the term simulation-free. Both NCSN and DDPM fall into this affine-drift setting. In the remainder, we develop the general theory for arbitrary drifts (x, t), but will return to the affine drift when closed-form analysis is useful. Marginal Densities. The time-marginal density pt(xt) is obtained by integrating over the perturbation kernel: pt(xt) := pt(xtx0)pdata(x0) dx0, with p0 = pdata. (4.1.5) By choosing the coefficients (t) and g(t) appropriately, the forward process gradually adds noise until the influence of the initial state is effectively forgotten. As becomes large, the conditional distribution pT (xT x0) no longer depends on x0, because its mean evolves as m(T ) = exp (cid:16) f (u) du (cid:17) x0 0, as , provided (u) is non-positive so that the exponential factor decays. At the same time, the variance grows and stabilizes to match chosen prior distribution. Consequently, the marginal pT (xT ) = pT (xT x0)pdata(x0) dx0, which initially represents complicated mixture over data samples, converges to simple prior pprior, typically Gaussian. In this limit, pT (xT ) pprior(xT ) and pT (xT x0) pprior(xT ), 92 Diffusion Models Today: Score SDE Framework so the forward process maps any data distribution into tractable prior, providing convenient starting point for reversal and generation. 4.1.3 Reverse-Time Stochastic Process for Generation Figure 4.3: Visualization of the reverse-time stochastic process for data generation. It begins from samples drawn from simple prior distribution (pprior) at = (denoted as ), which are evolved backward in time using reverse-SDE. The resulting trajectories terminate at = 0 and collectively form the target bimodal data distribution (p0 = pdata). The background heatmap illustrates how the probability density is gradually transformed from simple Gaussian into the complex target distribution. Intuitively, data generation from noise can be achieved by reversing the forward process: starting from random point sampled from the prior distribution and evolving it backward in time to obtain generated sample. For deterministic systems (that is, ODEs), this idea works naturally. Since no randomness is involved, reversing time simply means tracing the trajectory of point in the opposite direction along the same path as in the forward process3. In contrast, SDEs incorporate stochasticity at every time step, meaning that single point can evolve along many plausible random trajectories. As result, reversing such processes is more subtle4. While individual stochastic trajectories are not reversible, the remarkable insight is that the distribution over these trajectories can be reversed. This is formalized by foundational result from Anderson (1982), which shows that the 3Technically, this corresponds to solving the ODE with time-flipping substitution t. 4Naively flipping time does not yield the correct reverse process. 4.1. Score SDE: Its Principles 93 5 of the forward process in Equation (4.1.3) time-reversed process {x(t)}t[0,T ] is itself governed by well-defined SDE. This reverse-time process evolves from to 0, and its dynamics are given by: dx(t) = f (x(t), t)g2(t)x log pt(x(t)) dt + g(t) w(t), (4.1.6) x(T ) pprior pT . Here, w(t) denotes standard Wiener process in reverse time, defined as w(t) := w(T t) w(T ). To build intuition for Equation (4.1.6), we present concrete example in Section 4.1.6 with Gaussian data distribution and linearGaussian dynamics. This setting is analytically tractable: one can derive the time-reversal formula directly using basic calculus and linear algebra, without invoking the full general theory of Anderson (1982). Note that the presence of stochasticity (g = 0) introduces an additional correction term, g2(t)x log pt(x(t)), which accounts for the effect of diffusion and ensures that the reversed dynamics correctly reproduce the evolution of marginal distributions induced by the forward SDE (see Section 4.1.5). Conceptually, Why Does the Reverse Process Work? Section 4.5.2 presents an intuitive derivation of the reverse-time SDE by connecting it to the DDPM variational framework (optional but insightful). Here, we provide complementary intuition for how the reverse-time dynamics recover structured data from noise. At first glance, the presence of Brownian noise in the reverse time process may seem paradoxical. If the forward diffusion spreads data into increasingly noisy configurations, it is unclear how reversing this process, particularly one that introduces additional randomness through w(t), can produce clean, structured samples concentrated near the data manifold. The key point is that the reverse time SDE does not inject arbitrary randomness. The diffusion term g(t) w(t) is always coupled with the scoredriven drift g2(t)x log pt(x(t)). Together, these terms balance one another: the score guides trajectories toward regions of higher density, while the noise introduces controlled stochasticity that allows exploration without overwhelming the dynamics. 5We use the bar notation to distinguish the reverse process {x(t)}t[0,T ] from the forward process {x(t)}t[0,T ], defined by the forward-time SDE. 94 Diffusion Models Today: Score SDE Framework To see this more clearly, return to the Langevin intuition in Equation (3.1.6). When (t) 0, Equation (4.1.6) reads dx(t) = g2(t)x log pt (cid:0)x(t)(cid:1) dt + g(t) w(t). Reparameterize time forward via := (so dt = ds), and rename the Brownian motion in law so that w(t) = dws. Writing xs := x(T s) and πs := pT then gives dxs = g2(T s) log πs (cid:0)xs = 2τ (s) log πs (cid:0)xs (cid:1) ds + (cid:1) ds + g(T s) dws 2τ (s) dws, τ (s) := 1 2 g2(T s). This has the Langevin form with time-varying temperature τ (s), targeting the evolving density πs. By Tweedies formula (Equation (3.3.6)), the score direction log πs points toward the conditional clean signal at each time slice, so the drift continually pulls back denoised structure. Crucially, g(t) is annealing along the reverse trajectory. Early on (s 0, i.e., ), g(T s) is typically larger, so the injected noise is stronger and the process explores broadly. As increases, g(T s) decreases, the stochastic term weakens, and the score term dominates, pulling samples into high-density regions of πs; by = (i.e., = 0), trajectories concentrate near the data manifold. Overview of Reverse-Time SDE Capabilities. time-dependent score function It is fascinating how the s(x, t) := log pt(x) naturally appears in Equation (4.1.6). Once the forward coefficients (t) and g(t) are specified, the score is the only remaining unknown in the reverse dynamics. This highlights its central role: with the score in hand, the reverse process is determined, and sampling amounts to numerically integrating Equation (4.1.6) with the learned score. Since the oracle score generally lacks closed-form expression, we adopt the approach of Chapter 3 and train neural network sϕ(x, t) to approximate it via score matching; see Section 4.2.1 for details. Substituting s(x, t) with sϕ(x, t) in Equation (4.1.6) then specifies the reverse dynamics completely. Generation corresponds to solving the reverse-time SDE reversely from = , starting with xT pprior, to = 0. Importantly, Anderson (1982) proves that the marginal densities of the forward and reverse processes coincide, ensuring that samples at = 0 approximately follow pdata when pprior pT . We will explore this further in Section 4.2.2. 4.1. Score SDE: Its Principles 95 4.1.4 Deterministic Process (Probability Flow ODE) for Generation Although the SDE in Equation (4.1.6) introduces stochasticity and potentially increases the diversity of generated samples, question arises: Question 4.1.1 Is it necessary to sample using the SDE in Equation (4.1.6)? Inspired by Maoutsa et al. (2020), Song et al. (2020c) also introduced deterministic process, an ODE, that evolves samples with the same marginal 6, called the Probdistributions as the forward SDE. This process {x(t)}t[0,T ] ability Flow ODE (PF-ODE), is given by: dt x(t) = (x(t), t) 1 2 g2(t)x log pt(x(t)). (4.1.7) Analogous to the SDE case, one can replace the true score with learned approximation and integrate the reverse-time ODE from = to = 0 to generate samples. Concretely, the generated sample (solution of PF-ODE at time = 0) takes the form x(T ) + 0 (x(τ ), τ ) 1 2 g2(τ )x log pτ (x(τ )) dτ, where the initial condition x(T ) pprior. Since this integral is intractable in closed form, practical generation relies on numerical solvers (e.g., Euler method, see Equation (4.2.4)). Compared to the reverse-time SDE, the PF-ODE offers two key advantages: The ODE can be integrated in either direction, from = 0 to = or from = to = 0, using the same formulation of equation, provided the corresponding initial condition is specified at the chosen endpoint. This bidirectionality contrasts with SDEs, which generally admit only forward time integration. It benefits from wide range of well-established, off-the-shelf numerical solvers developed for ODEs. We emphasize that the PF-ODE is not obtained by simply removing the 2 in its drift term diffusion term in Equation (4.1.6); notably, the factor of 1 6We use tilde to distinguish processes associated with the forward and reverse-time SDEs. Going forward, we omit this notational distinction for simplicity. 96 Diffusion Models Today: Score SDE Framework has principled origin. At high level, Equation (4.1.7) arises by choosing the drift of an ODE such that its evolution preserves the same marginal densities as the forward SDE in Equation (4.1.3). The underlying principle (i.e., Fokker-Planck Equation (Øksendal, 2003)) ensuring this alignment of marginals will be detailed in the next section. 4.1.5 Matching Marginal Distributions in Forward/Reverse-Time SDEs and PF-ODE Figure 4.4: (2D) Temporal evolution of the marginal density pt. The forward SDE has 0 2t on [0, ]. It starts with p0 = pdata two-mode Gaussian mixture and ends and g(t) = at pT pprior := (0, 2I). The temporal-spatial evolution of pt follows the FokkerPlanck equation. 4.1. Score SDE: Its Principles 97 Fokker-Planck Equation to Ensure Alignment of Marginal Densities. central concept in diffusion models is that different processes can lead to the same sequence of marginal distributions (as we will illustrate later in this subsection). The objective is to construct process that transforms pprior into pdata by aligning the marginals across time, and in particular at = 0. The exact form of the process is secondary, provided it is tractable and admits efficient sampling. This naturally leads to fundamental question: Question 4.1.2 How can we ensure that different processes yield identical marginal distributions? Returning to our setup, once the forward SDE is specified, it defines the evolution of marginal densities from pdata to pprior. The reverse-time SDE and PF-ODE are then constructed so that their trajectories yield marginal distributions that exactly match those of the forward process. The key to this correspondence lies in the FokkerPlanck equation, which governs how marginal densities evolve under diffusion processes. The following theorem (Anderson, 1982; Song et al., 2020c) establishes the foundation for this connection: Diffusion Models Today: Score SDE Framework Theorem 4.1.1: FokkerPlanck Equation Ensures Marginals Alignment Let {x(t)}t[0,T ] evolves with the forward SDE dx(t) = (x(t), t) dt + g(t) dw(t), with initial condition x(0) p0 = pdata. Then its marginal densities pt satisfy the FokkerPlanck equation tpt(x) = (cid:2)f (x, t)pt(x)(cid:3) + 1 2 g2(t)xpt(x) = (cid:2)f (x, t)pt(x)(cid:3), (4.1.8) where denotes the Laplacian operator, and v(x, t) = (x, t) 2 g2(t)x log pt(x). Then, both the PF-ODE and the reverse-time SDE yield the same family {pt}t[0,T ], with the latter evolving in reverse time: (i) The PF-ODE {x(t)}t[0,T ] dx(t) dt = v(x(t), t), if started with x(0) p0 and run forward in t, or equivalently started with x(T ) pT and run backward in t, has marginals x(t) pt for all [0, ]. (ii) The reverse-time SDE {x(t)}t[0,T ] dx(t) = (cid:2)f (x(t), t) g2(t)x log pt(x(t))(cid:3) dt + g(t) w(t), with x(0) pT and w(t) standard Wiener process in reverse time, has marginals x(t) pT t. Proof for Theorem. The proof is provided in Section D.2.5, while Section 4.5.1 offers further intuition behind the FokkerPlanck equation using the marginalization technique of probability. Multiple Conditional Distributions for Fixed Marginal. To understand how the PF-ODE transports pdata forward in time (or equivalently pprior in reverse), consider the flow map Ψst : RD RD, where Ψst(xs) denotes 4.1. Score SDE: Its Principles 99 the PF-ODE solution at time initialized from xs at time s, for any time s, [0, ]. In other words, this map takes an initial state xs and directly jumps to its state at t: Ψst(xs) := xs + s v(xτ , τ ), dτ, (4.1.9) with velocity field v(x, τ ) := (x, τ ) 1 2 g2(τ )x log pτ (x). Here, the integral captures the net displacement accumulated along the PFODE trajectory xτ . Under mild smoothness assumptions on v, the flow map Ψst : RD RD is smooth bijection7. For any [0, ], the pushforward density is defined as pfwd (xt) := δ(xt Ψt0(x0))pdata(x0) dx0, denoted Ψt0#pdata, representing the distribution at time under Ψt0. Theorem 4.1.1 ensures pfwd = pt, where pt is the marginal density of the forward SDE, equating the deterministic PF-ODE and stochastic kernel: pt(xt) = pt(xtx0)pdata(x0) dx0 = δ(xt Ψt0(x0))pdata(x0) dx0. This implies infinitely many conditionals Qt(xtx0) yield the same pt(xt), for instance: Stochastic (Simulation-Free): Qt(xtx0) = pt(xtx0), Deterministic (Requires ODE Solving): Qt(xtx0) = δ(cid:0)xt Ψt0(x0)(cid:1), Mixture: Qt(xtx0) = λpt(xtx0) + (1 λ)δ(xt Ψt0(x0)), λ [0, 1]. This nonuniqueness of Qt(xtx0) arises from the fact that the marginal constraint does not uniquely determine the conditional distribution. This concept reappears in Section 5.2.2 and Section 9.2.3. In particular, there exists an entire family of reverse-time SDEs that are consistent with the same marginal pt. 7Spoiler: the PF-ODE flow map Ψst is exactly the Normalizing Flow (NF) bijection carrying ps to pt (to be detailed in Section 5.1). The difference is that PF-ODE fixes the unique vector field dictated by the SDEs FokkerPlanck dynamics, whereas NF (or continuous-time NF) parameterizes this field but relies on the same change-of-variables principle. 100 Diffusion Models Today: Score SDE Framework Observation 4.1.1: Matching Prescribed Marginal Densities Multiple processes can give rise to the same sequence of marginal densities; what truly matters is satisfying the FokkerPlanck equation. This fundamental insight affords us remarkable flexibility in designing generative processes that transition from pprior to pdata, or vice versa. The FokkerPlanck equation lies at the heart of diffusion models and is rooted in the fundamental change-of-variable formula for probability densities (see Chapter for systematic treatment). Far from being minor technical detail, this principle recurs throughout our development, most notably in Section 5.2. 4.1.6 Computable Example: Evolutions of Gaussian Dynamics When pdata is normal distribution (or mixture of Gaussians), the score function admits closed-form expression. This makes it an ideal setting for building intuition about diffusion processes: we can explicitly derive the reverse-time SDE and the PF-ODE using only basic calculus, without resorting to advanced mathematical tools. In this subsection, we illustrate how these equations behave in such tractable case. Exact Computation of the Reverse-Time SDE with Gaussian. When pdata is Gaussian, the formula in Equation (4.1.6) can be derived directly, without relying on the general theory and proofs of Anderson (1982). To illustrate the core idea, we consider the one-dimensional case; the extension to higher dimensions follows in the same way. Start from the forward SDE dx(t) = (t)x(t) dt + g(t) dwt, and take one small Euler step of size > 0: where := 1 + (t)t, := g(t) forward one-step transition kernel is Gaussian: t, and ϵ (0, 1). Equivalently, the xt+t = axt + rϵ, xt+txt N(cid:0)axt, r2(cid:1). Since pdata is assumed to be Gaussian, the current marginal at time is also Gaussian, which takes the following form: xt (mt, t ), 4.1. Score SDE: Its Principles 101 for some scalar mt and st. So conditioning will amount to multiplying two Gaussians and renormalizing. This keeps the algebra elementary. By Bayes rule the conditional density is, up to constant, the product of the prior and the transition kernel: p(xtxt+t) p(xt+txt)pt(xt) exp (cid:16) (xt mt)2 2s2 (cid:17) (cid:16) exp (xt+t axt)2 2r2 (cid:17) . The exponent is quadratic in xt. Expanding both squares and grouping terms shows exactly which coefficients matter: 2 log p(xtxt+t) = Ax2 2Bxt + const, with + := 1 s2 a2 r2 , := Here is the sum of precisions (prior precision plus the transition-kernel precision transported through a), while is the corresponding precisionweighted sum of targets. With these in hand, completing the square gives the posterior in one line: axt+t r2 mt s2 + . Ax2 (cid:16) 2Bxt = xt (cid:17)2 B2 , so the conditional distribution is Gaussian with variance 1/A and mean B/A: Var(xtxt+t) = 1 + a2 r2 , 1 s2 E[xtxt+t] = mt s2 + axt+t r2 + a2 r2 1 s2 . These closed forms already describe the reverse transition for any small t. To read off reverse-time SDE, we now expand them for small t. Use = 1 + (t)t and r2 = g2(t)t. As 0, the contribution a2 r2 1 g2(t)t dominates the precision, so the variance becomes Var(xtxt+t) = ! 1 s2 + a2 r2 = g2(t)t + O(t2), which tells us the reverse step has the same diffusion scale g(t) as the forward step. For the mean, expand the ratio B/A to first order: E[xtxt+t] = xt+t + (t) + \" ! g2(t) s2 xt+t + # mt + O(t2). g2(t) s2 102 Diffusion Models Today: Score SDE Framework Putting the mean and variance together yields the one-step reverse transition kernel xtxt+t xt+t + + \" ! g2 s2 xt+t + g2 s2 # ! mt , g2t + O(t2). This is recognized as the EulerMaruyama update, run backward from + to t: \" xt xt+t = + ! g2 s2 xt+t + # mt + g2 s2 tϵ + O(t2). Letting 0 gives the SDE on the original clock (time decreasing along the path) dx(t) = (cid:16) (t) + (cid:17) g2(t) s2 x(t) + g2(t) s2 mt dt + g(t) wt. This drift can be written with the score because for Gaussian marginal pt = (mt, s2 ), (cid:16) = mt s2 log pt(x) = g2 s2 To express the conventional forward-in-t reverse-time parametrization, define the reversed process x(t) := x(T t) (so that we now evolve forward in t). The time flip turns the drift into mt = + g2x log pt(x). g2 s2 + + (cid:17) dx(t) = (t)x(t) g2(t)x log pt(x(t)) dt + g(t) wt, where x(T ) pprior pT . This is exactly the conventional reverse-time SDE. In vector form this matches the general Equation (4.1.6) with log pt in place of the 1D derivative. Exact Computation of PFODE with Gaussian. When the data distribution is assumed to be Gaussian, we can also directly derive the PF-ODE formula, avoiding heavy machinery such as the FokkerPlanck equation. In the end, we will see that the marginal densities of the PF-ODE coincide with those of both the forward SDE and the reverse-time SDE, providing constructive verification of the FokkerPlanck theory to be discussed in Section 4.1.5. Assume xt (mt, s2 can be written as smooth map ) at time t. small deterministic step of size xt+t = Φt,t(xt) = xt + tvt(xt) + O(t2), 4.1. Score SDE: Its Principles 103 which is simply the firstorder Taylor expansion in t. Our goal is to see what form vt must take so that, whenever the input is Gaussian, the output remains Gaussian. To this end, expand vt around the current mean mt: vt(x) = vt(mt) + 2 t(mt)(x mt) + 1 Now set := xt mt, so that (0, s2 ). Next, center the output by subtracting its mean (to first order in t): (mt)(x mt)2 + . (cid:16) := xt+t E[xt+t] = + t(mt)y + 1 2 (mt)(y2 s2 ) (cid:17) + O(t2). At this point, recall that Gaussian has zero skewness; in other words, its third centered moment is zero. Therefore, computing E[z3] to first order and , E[y3] = 0, E[y4] = 3s4 using E[y] = 0, E[y2] = s2 , we obtain E[z3] = 3t 1 2 t (mt)(cid:0)E[y4] s2 E[y2](cid:1) + O(t2) = 3tv (mt)s4 + O(t2). For the output to stay Gaussian for all small t, this quantity must vanish at order t, which forces (mt) = 0. Repeating the same argument for higher derivatives rules out higher powers as well. Consequently, vt must be linear plus shift: Plugging this back into the step gives vt(x) = atx + bt. xt+t = (1 + αtt)xt + βtt + O(t2), αt := at, βt := bt. We now push xt (mt, s2 ) through this map and track mean and variance to first order: E[xt+t] = mt + t(αtmt + βt) + O(t2), Var(xt+t) = s2 + t(2αts2 ) + O(t2). On the other hand, the forward SDE dx = (t)x dt + g(t) dwt has the elementary moment formulas (see Equation (4.3.3)): = (t)mt, (s2 ) = 2f (t)s2 + g2(t). Matching the coefficients of gives αt = (t) + g2(t) 2s2 , βt = g2(t) 2s2 mt. With these choices, the step becomes xt+t = xt + \" (cid:16) (t) + (cid:17) g2(t) 2s2 xt g2(t) 2s2 # mt + O(t2). 104 Diffusion Models Today: Score SDE Framework Since for Gaussian pt = (mt, s2 can rewrite the bracket as (t)xt 1 ) we have log pt(x) = (x mt)/s2 , we 2 g2(t)x log pt(xt). Therefore, xt+t = xt + f (t)xt 1 2 g2(t)x log pt(xt) + O(t2). Finally, dividing by and letting 0 yields the PF-ODE x(t) = (t)x(t) 1 2 g2(t)x log pt (cid:0)x(t)(cid:1). To see why this ODE has the same marginals as the forward SDE (and the reversetime SDE), observe that the drift above is linear plus shift. Thus x(t) depends affinely on x(0), and affine maps send Gaussians to Gaussians. Moreover, the mean mt and variance s2 along this ODE satisfy exactly the same two scalar ODEs as the forward SDE (by our matching), with the same initial values. Hence pt = (mt, s2 ) is identical for both evolutions at every time t. 4.2. Score SDE: Its Training and Sampling 4.2 Score SDE: Its Training and Sampling 4.2.1 Training Building on the philosophy as in Chapter 3, we approximate the oracle score log pt(x) using time-conditioned neural network sϕ = sϕ(x, t) across all [0, ], by minimizing score-matching objective as in Equation (3.2.1): LSM(ϕ; ω()) := Etptime Extpt 1 2 ω(t) sϕ(xt, t) log pt(xt)2 2 , where ptime is some time distribution (e.g., uniform on [0, ]), ω() is timeweighting function. To avoid relying on the intractable oracle score log pt(x), the DSM loss in Equation (3.3.2) is employed.Conditioned on data point x0, this approach allows the use of the analytically tractable score xt log pt(xtx0) via Equation (D.2.4), with concrete examples given in Section 4.3. Specifically, we exploit the following loss function: LDSM(ϕ; ω()) := 1 2 Ept(xtx0) EtEx ω(t) sϕ(xt, t) xt log pt(xtx0)2 2 , (4.2.1) where x0 pdata. Equation (4.2.1) can be interpreted as the continuous-time counterpart of Equation (3.4.1), with the summation in the discrete case replaced by integration. Similar to the result in Theorem 3.3.1, the minimizer of Equation (4.2.1) is uniquely determined as follows: Proposition 4.2.1: Minimizer of DSM The minimizer satisfies s(xt, t) = Ex0p(x0xt) (cid:2)xt log pt(xtx0)(cid:3) = xt log pt(xt), (4.2.2) for almost every xt pt and [0, ]. Proof for Proposition. 106 Diffusion Models Today: Score SDE Framework DSM objective can be understood as least-squares error problem. Specifically, at each time t, the optimal score function is given by the conditional expectation of the gradient of the log conditional density, which, under Bayes rule, is equivalent to the gradient of the log marginal density. For detailed proof, see Appendix D.2.6. 4.2.2 Sampling and Inference Figure 4.5: (2D) Illustration of sampling from the Score SDE. Sampling is by solving the reverse-time SDE (blue; via Equation (4.2.4)) and the PF-ODE (red; via Equation (4.2.6)) for the same forward SDE setup as in Figure 4.4. Starting from random point xT pprior (dark ), both trajectories terminate near the support of pdata at = 0. After learning sϕ := sϕ(x, t) log pt(x), we replace the intractable oracle score log pt(x) in the reverse-time SDE (Equation (4.1.6)) and PF-ODE (Equation (4.1.7)) with the learned proxy sϕ(x, t). This substitution enables tractable inference via either the SDE or the ODE. For clarity, we distinguish the resulting processes as xSDE ϕ (t) and ϕ (t), respectively, but will omit this distinction in later sections.8 xODE Empirical Reverse-Time SDE. By substituting the trained score model sϕ for the true score in Equation (4.1.6), we obtain the parameterized reverse-time 8This is to simplify notation after this subsection. 4.2. Score SDE: Its Training and Sampling 107 SDE used for generation: dxSDE ϕ (t) = (cid:16) (cid:17) xSDE ϕ (t), g2(t)sϕ (cid:16) xSDE ϕ (t), (cid:17)i dt + g(t) w(t). (4.2.3) To generate sample, we first draw an initial value xT from the prior distribution pprior and then numerically solve Equation (4.2.3) backward in time from = to = 0. standard numerical solver for this is the EulerMaruyama method, which provides the discrete update rule: xtt xt (xt, t) g2(t)sϕ(xt, t) + g(t) ϵ, (4.2.4) where ϵ (0, I) and > 0 is the step size. Iterating this update rule yields final sample xSDE is accurate, the distribution of these generated samples, denoted pSDE provides close approximation to the true data distribution9: ϕ (0). If the score model ϕ (; 0), pSDE ϕ (; 0) pdata(). Indeed, the DDPM sampling scheme presented in Equation (2.2.14) is special case of this EulerMaruyama discretization applied to specific choices of and (see Section 4.3). Empirical PF-ODE. The PF-ODE defines continuous flow connecting pprior and pdata, enabling sampling, encoding, and exact likelihood evaluation. The following section provides further details on each of these operations. I. Sampling with PF-ODE. Replacing the oracle score in Equation (4.1.7) with sϕ yields the empirical PF-ODE: xODE ϕ (t) = (cid:16) (cid:17) xODE ϕ (t), dt g2(t)sϕ (cid:16) (cid:17) xODE ϕ (t), . 1 2 (4.2.5) To generate samples, we begin by drawing an initial sample xT from the prior distribution, pprior. We then numerically solve the PF-ODE from Equation (4.2.5) backward in time from = to = 0. This process is equivalent to approximating the integral: xODE ϕ (0) = xT + 0 (cid:16) (cid:20) xODE ϕ (τ ), τ (cid:17) 1 2 g2(τ )sϕ (cid:16) xODE ϕ (τ ), τ (cid:17)(cid:21) dτ. 9Theoretically, estimation accuracy depends on the discrepancy between pT and pprior (typically negligible), model training error, and numerical discretization error (De Bortoli, 2022). We do not pursue formal bounds here. 108 Diffusion Models Today: Score SDE Framework Solving this integral yields final sample, xODE generated via this deterministic process, denoted pODE approximation to the data distribution, such that pODE ϕ (0). The distribution of samples ϕ (; 0), provides an ϕ (; 0) pdata. Let > 0 denote discretization step size. standard numerical integration approach is the Euler method, which estimates (xτ , τ ) 1 g2(τ )sϕ(xτ , τ ) (xt, t) 1 2 g2(t)sϕ(xt, t), τ [t t, t], leading to the following update rule: xtt xt (cid:20) (xt, t) 1 2 (cid:21) g2(t)sϕ(xt, t) t. (4.2.6) This connection allows us to reframe the process of generation with the following core insight: Insight 4.2.1: Generation ODE/SDE Solving Sampling from diffusion models is fundamentally equivalent to solving corresponding probability flow ODE or reverse-time SDE. This equivalence provides clear explanation for the slow sampling speeds of diffusion models, as raised in Question 3.5.1. Generation is computationally intensive because numerical solvers for these differential equations are inherently iterative, often requiring many steps to accurately approximate solution trajectory10. However, the PF-ODE formulation is also advantageous, as it allows us to leverage the extensive literature on accelerated numerical solvers. Exploring these techniques to speed up diffusion model sampling is the primary focus of Chapter 9. II. Inversion with PF-ODE. As discussed, unlike in the case of SDEs, we can solve the same Equation (4.2.5) both forward (from 0 to ) and reverse (from to 0) in time. When solving it forward, the ODE flow maps data to its (noisy) latent representations across all [0, ], which plays role of an encoder. This concept enables powerful applications for controllable generation, such as image translation and editing and beyond (Mokady et al., 2023; Su et al., 2022). 10For example, DDPM and Score SDE typically use 1, 000 function evaluations for generation. 4.2. Score SDE: Its Training and Sampling 109 III. Exact Log-Likelihood Computation via PF-ODE. We reinterpret the dynamics in Equation (4.2.5) as Neural ODE (Chen et al., 2018) variant (introduced in Section 5.1.2) that parameterizes only the score function, rather than the full velocity field. This PF-ODE formulation enables exact log-likelihood computation via the change-of-variables formula. Applying the identity from Equation (5.1.9) to the PF-ODE in Equation (4.2.5), we define the velocity field as vϕ(x, t) := (x, t) 1 2 g2(t)sϕ(x, t), with the learned score sϕ. The time evolution of the log-density pODE along the PF-ODE trajectory {xODE ϕ (t)}t[0,T ] satisfies ϕ (; t) dt log pODE ϕ (cid:0)xODE ϕ (t), t(cid:1) = vϕ (cid:16) (cid:17) xODE ϕ (t), , where denotes the divergence in x. To evaluate the likelihood of data point x0 pdata, we integrate the following augmented ODE system from = 0 to = : \" # x(t) δ(t) dt = \" vϕ(x(t), t) vϕ(x(t), t) # , \" # x(0) δ(0) \" # , x0 0 = (4.2.7) where δ(t) accumulates the log-density change over time. Upon solving the system up to = , we obtain the terminal state: \" # x(T ) δ(T ) . The log-likelihood of the original sample x0 under the model can then be evaluated as log pODE ϕ (x0; 0) = log pprior (x(T )) + δ(T ), where pprior (x(T )) denotes the closed-form prior density evaluated at x(T ). 110 Diffusion Models Today: Score SDE Framework 4.3 Instantiations of SDEs Song et al. (2020c) categorize the drift term (x, t) and the diffusion term g(t) in the forward SDE into three types based on the behavior of the variance during evolution. Here, we focus on two commonly used types: the Variance Explosion (VE) SDE and Variance Preserving (VP) SDE. While it is possible to design custom noise schedulers, their design can substantially influence empirical performance. Table 4.1 summarizes these two SDE instantiations. Table 4.1: Summary of the forward SDEs (x, t) g(t) SDE VE SDE 0 dσ2(t) dt dx(t) = g(t) dw(t) pt(xtx0) (cid:0)xt; x0, (cid:0)σ2(t) σ2(0)(cid:1) I(cid:1) pprior (0, σ2(T )I) VP SDE 1 2 β(t)x pβ(t) dx(t) = 1 (cid:18) xt; x0e 1 2 β(t)x(t) dt + pβ(t) dw(t) β(τ ) dτ , Ie t (0, I) β(τ ) dτ 2 0 (cid:19) 4.3.1 VE SDE VE SDE has the following components: Drift Term: zero drift term = 0. Diffusion Term: g(t) = dσ2(t) dt for some function σ(t). The forward SDE then takes the following form: dx(t) = dσ2(t) dt dw(t). (4.3.1) Similarly, the results from Section 4.3.3 imply the perturbation kernel for the VE SDE and suggest selecting an appropriate prior distribution: Perturbation Kernel: pt(xtx0) = (cid:16) xt; x0, (cid:16) σ2(t) σ2(0) (cid:17) (cid:17) Prior Distribution: Assume that σ(t) is an increasing function for [0, ] and that σ2(T ) σ2(0). The prior distribution is given by: pprior := (0, σ2(T )I). 4.3. Instantiations of SDEs 111 typical instance of VE SDE is NCSN with the following design: σ(t) := σmin (cid:19)t (cid:18) σmax σmin , for (0, 1], where σmin and σmax are pre-specified constants. Namely, the sequence of variances is designed as geometric sequence. With this, NCSN is viewed as discretized version of VE SDE, as discussed in Section 4.1.1. 4.3.2 VP SDE Let β : [0, ] R0 be non-negative function of t. VP SDE is defined with the following components: Drift Term: linear drift given by (x, t) = 1 2 β(t)x. Diffusion Term: g(t) = pβ(t). Thus, the forward SDE is expressed as: dx(t) = 1 2 β(t)x(t) dt + β(t) dw(t). (4.3.2) Using the results from Section 4.3.3, we can derive the perturbation kernel for the VP SDE and select an appropriate prior distribution: Perturbation Kernel: pt(xtx0) = (cid:18) xt; x0e 1 0 β(τ ) dτ , Ie 0 β(τ ) dτ (cid:19) . Prior Distribution: pprior := (0, I). We remark that since the perturbation kernel is Gaussian with known mean and covariance, we can apply Equation (D.2.5) to compute its score function. classic example of VP SDE is the DDPM, where the noise schedule β(t) is defined as: β(t) := βmin + t(βmax βmin), for all [0, 1]. Here, βmin and βmax are pre-defined constants. With this, DDPM can be interpreted as discretization of the VP SDE, as discussed in Section 4.1.1. 112 Diffusion Models Today: Score SDE Framework 4.3.3 (Optional) How Is the Perturbation Kernel pt(xtx0) Derived? If the drift term in the forward SDE Equation (4.1.3) is linear in x, taking the form (x, t) = (t)x, for some scalar-valued, time-dependent function (t) R, then Equation (4.1.3) becomes linear SDE: dx(t) = (t)x(t) dt + g(t) dw(t). Even if the initial distribution pdata is non-Gaussian, the linearity of the drift ensures that the conditional process remains Gaussian. In particular, for > 0, the transition kernel admits the form: pt(xtx0) = (xt; m(t), (t)ID) , where x0 pdata, and m(t) RD, (t) R0 denote the conditional mean and (scalar) variance given x0, defined as: m(t) = [xtx(0) = x0] , (t)ID = Cov [xtx(0) = x0] . These first and second moments evolve according to the following ODEs (Särkkä and Solin, 2019): dm(t) dt dP (t) dt = (t)m(t), = 2f (t)P (t) + g2(t), (4.3.3) provided that the initial mean m(0) and variance (0) are finite. Since both ODEs are linear, they admit closed-form solutions via the integrating factor method. Given the initial condition x0, the mean and variance evolve as m(t) = E(0 t)x0, (t) = 0 2(s t)g(s)2 ds, (4.3.4) with m(0) = x0 and (0) = 0. Here E(s t) denotes the exponential integrating factor E(s t) := exp (u) du , (cid:18)Z (cid:19) which captures the accumulated effect of the drift from time to t. Consequently, the transition kernel pt(xtx0) also admits closed-form expression. 4.3. Instantiations of SDEs 113 We defer the justification that the conditional covariance of pt(xtx0) is isotropic, that is Cov[xtx0] = (t)ID under D-dimensional Wiener process with independent coordinates and diffusion g(t)ID, as well as the derivation of Equation (4.3.3), to Section C.1.5, which relies on Itô calculus. Example: VE SDEs Transition Kernel dσ2(t) In the special case of VE SDE: 0 and g(t) = dt covariance of the solution to the SDE evolve as follows. , the mean and Mean. Variance. dm(t) dt = 0, with m(0) = x0 = m(t) = x0. dP (t) dt = dσ2(t) dt , with (0) = 0 = (t) = σ2(t) σ2(0). Therefore pt(xtx0) = (cid:16) xt; x0, (cid:0)σ2(t) σ2(0)(cid:1)ID (cid:17) . Example: VP SDEs Transition Kernel In the VP SDE case with drift (x, t) = 1 pβ(t): 2 β(t)x and diffusion g(t) = Mean m(t). dm dt = 1 2 β(t)m(t), B(t) := 0 β(s) ds, m(t) = 1 2 B(t)x0. Variance (t). The variance satisfies dP dt Applying the integrating factor eB(t) with B(t) = = β(t)P (t) + β(t). 0 β(s) ds, we obtain (t)eB(t)i = β(t)eB(t). dt Integrating both sides gives (t) = 1 eB(t). 114 Diffusion Models Today: Score SDE Framework Hence the covariance is isotropic with P(t) = (t)ID = (cid:0)1 eB(t)(cid:1)ID. Final Closed-Form Transition Kernel. pt(xt x0) = xt; 1 , (cid:0)1 eB(t)(cid:1)ID 2 B(t)x0 } {z } {z m(t) (t)ID , B(t) = 0 β(s) ds. (Optional) Rethinking Forward Kernels in Score-Based and Variational 4.4. Diffusion Models 115 4.4 (Optional) Rethinking Forward Kernels in Score-Based and Variational Diffusion Models DDPM and Score SDE are typically introduced via the forward transition kernel p(xtxtt), discretely defined in DDPM and as continuous-time SDE in Score SDE. However, what is most relevant in practice, especially in their loss functions (Equations (2.2.8) and (4.2.1)), is the accumulated transition kernel from the data, pt(xtx0). Both frameworks ultimately rely on this kernel, either through recursive computation (DDPM) or by solving an ODE, as detailed in Section 4.3.3 (Score SDE). In this section, we start by defining pt(xtx0) (in continuous time), which provides neater and more direct perspective. Overall, while p(xtxtt) and pt(xtx0) are theoretically equivalent, defining the latter often results in cleaner and more interpretable formulation. In particular, pt(xtx0) offers direct insight into the prior as , and aligns naturally with the practical loss design. Figure 4.6: Illustration of Lemma 4.4.1. Incremental noise injection via continuous-time SDE (t 0) and direct perturbation of Equation (4.4.1) are mathematically equivalent. 4.4.1 General Affine Forward Process pt(xtx0) We begin with defining general forward perturbation kernel: pt(xtx0) := (cid:0)xt; αtx0, σ2 I(cid:1), (4.4.1) where x0 pdata, and αt, σt are nonnegative scalar functions of [0, ] satisfying: (i) αt > 0 and σt > 0 for all (0, 1] (allowing σ0 = 0), and (ii) Typically, α0 = 1 and σ0 = 0. That is, xt pt(xtx0) can be sampled as xt = αtx0 + σtϵ, ϵ (0, I). Diffusion Models Today: Score SDE Framework This framework subsumes several well-known instances, including the VE (e.g., NCSN), the VP (e.g., DDPM), and the Flow Matching (FM) forward kernel (Lipman et al., 2022; Liu, 2022), which linearly interpolates between x0 and ϵ (see later in Section 5.2). VE (NCSN) Kernel: αt 1, σT 1; VP (DDPM) Kernel: αt := 1 σ , so that α2 + σ2 = 1; FM Kernel: αt = 1 t, σt = t. 4.4.2 Connection to Score SDE For Score SDE, specifying pt(xtx0) in linear form naturally induces an SDE with affine coefficients, providing more intuitive alternative to starting from drift and diffusion terms and solving ODEs for the moments (see Section 4.3.3). Given the forward perturbation kernel in Equation (4.4.1), the corresponding forward SDE takes the linear-in-x form as in Equation (4.3.2): dx(t) = (t)x(t) } {z (x(t),t) dt + g(t) dw(t), where f, : [0, ] are real-valued functions of time. The coefficients (t) and g(t) can be expressed analytically in terms of αt and σt, as summarized in the following lemma. (Optional) Rethinking Forward Kernels in Score-Based and Variational 4.4. Diffusion Models 117 Lemma 4.4.1: Forward Perturbation Kernel Linear SDE Define λt := log αt σt for (0, ]. Given the forward perturbation kernel xt = αtx0 + σtϵ, ϵ (0, I), the linear SDE with coefficients dx(t) = (t)x(t) dt + g(t) dw(t), log αt, (t) = g2(t) = dt dσ2 dt 2 dt log αtσ2 = 2σ2 d dt λt, (4.4.2) has the conditional transition pt(xtx0) = (cid:0)xt; αtx0, σ2 I(cid:1) for all (0, ]. Conversely, if linear SDE has conditional transitions (xt; αtx0, σ2 I) so that αt > 0 and σt > 0 for all (0, ], then its coefficients satisfy Equation (4.4.2) for (0, ]. Proof for Lemma. From Section 4.3.3, the proof matches the mean and covariance ODEs m(t) = (t)m(t), P(t) = 2f (t)P(t) + g2(t)I with m(t) = αtx0 and P(t) = σ on (0, ]. Remark. To exactly match Gaussian prior at the terminal time, the process must completely forget x0 and attain the target variance; this requires αT = 0 and σ2 equal to the prior variance. In the SDE formulation, one has αt = exp (cid:16) 0 (u) du (cid:17) . Thus, enforcing αT = 0 at finite forces 0 (u) du = , meaning the drift must contract infinitely fast as . At the same time, the diffusion must diverge in order to maintain the prescribed variance, 118 Diffusion Models Today: Score SDE Framework which is reflected by g2(t) = σ2 2 α αt σ2 as T. If and remain bounded on [0, ], then necessarily αT > 0 and residual dependence on x0 remains. In that case, the Gaussian prior is attained only asymptotically: either in the limit (without exact attainment) or exactly on an infinite horizon after an appropriate time reparameterization as . From the above lemma, specifying the incremental noise injection via linear SDE with coefficients (t) and g(t) is mathematically equivalent to defining the perturbation kernel with parameters αt and σt. In the diffusion model literature, these two viewpoints are used interchangeably. Therefore, we conclude: Observation 4.4.1: Defining pt(xtx0) is equivalent to specifying the linear SDE coefficients (t) and g(t). 4.4.3 Connection to Variational-Based Diffusion Model We revisit core identity from DDPM, derived via Bayes rule: p(xttxt, x) = p(xtxtt) ptt(xttx) pt(xtx) , (4.4.3) for any (usually pdata). This reverse conditional p(xttxt, x) is central to modeling, enabling both tractable training targets and efficient sampling. Although DDPM typically defines the incremental kernel p(xtxtt) first, the accumulated transition pt(xtx0) often provides more interpretable and practical formulation, especially for the prior and loss design. Deriving Transition Kernels. We now extend this to the continuous-time setting. Let 0 < be two (continuous) time points. Given the perturbation kernel pt(xtx0), we can compute the reverse conditional p(xtxs, x) for any by applying Equation (4.4.3)11, using the forward kernel p(xsxt) as an intermediate. The following lemma summarizes this derivation, extending Lemma 2.2.2 without assuming α2 + σ2 = 1. 11This identity extends naturally to continuous time by treating as general earlier time. (Optional) Rethinking Forward Kernels in Score-Based and Variational 4.4. Diffusion Models 119 Lemma 4.4.2: Reverse Conditional Transition Kernels Let 0 < . The reverse conditional transition kernel is: p(xtxs, x) = (cid:16) (cid:17) xt; µ(xs, x; s, t), σ2(s, t)I , where µ(xs, x; s, t) := αstσ2 σ2 xs + αtσ2 st σ2 x, σ2(s, t) := σ2 st σ2 σ2 . (4.4.4) Here, αst and σst are defined as: αst := αs αt , st := σ2 σ2 α2 stσ2 . Proof for Lemma. We first compute the forward transition kernel: p(xsxt) = (cid:16) xs; αstxt, σ2 stI (cid:17) . (4.4.5) The reverse kernel then follows from Bayes rule, and since all involved distributions are Gaussian, the result can be derived by direct computation. For further details, see Appendix of (Kingma et al., 2021). Although p(xt+txt) and pt(xtx0) are theoretically equivalent, pt(xtx0) often takes more central role. The step-wise transition in Equation (4.4.5) mainly serves to obtain closed-form reverse kernel. Recent works (Kingma et al., 2021) thus favor directly specifying pt(xtx0) for its clarity and interpretability. Reverse Process Modeling, Training, and Sampling. The training objective (ELBO in Equation (2.2.13)) and the modeling framework introduced in Section 2.2 remain applicable under our generalized setting. For clarity, we adopt the x-prediction formulation, denoted by xϕ(xs, s), following Kingma et al. (2021). However, the equivalent ϵ-prediction perspective, represented by ϵϕ(xs, s), is also valid due to the relationship (as in Equation (2.2.12)) xs = αsxϕ(xs, s) + σsϵϕ(xs, s), for any given xs qs. Modeling and Diffusion Loss Ldiffusion. Similar to DDPM, the conditional distribution p(xtxs, x) in Equation (4.4.4) motivates replacing the clean signal 120 Diffusion Models Today: Score SDE Framework with learnable predictor xϕ(xs, s), yielding parameterized reverse model of the form: pϕ(xtxs) := (cid:16) (cid:17) xt; µϕ(xs, s, t), σ2(s, t)I , (4.4.6) with the mean parametrized as: µϕ(xs, s, t) = αstσ2 σ2 xs + αtσ2 st σ2 xϕ(xs, s). Given the forward kernel in Equation (4.4.1), the KL divergence in Ldiffusion(x; ϕ) reduces to weighted regression loss: DKL (cid:0)p(xtxs, x0)pϕ(xtxs)(cid:1) = = µ(xs, x0; s, t) µϕ(xs, s, t)2 2 1 2σ2(s, t) 1 (cid:0)SNR(t) SNR(s)(cid:1) x0 xϕ(xs, s)2 2 , 2 (4.4.7) where xs = αsx0 + σsϵ, with x0 pdata, ϵ (0, I), and SNR(s) := α2 denotes the signal-to-noise ratio at time s. s/σ2 Remark. In (Kingma et al., 2021), the authors study the continuous-time limit of Equation (4.4.7) as s, yielding: VDM(x0) = 1 2 Es,ϵN (0,I)SNR(s)(cid:13) (cid:13)x0 xϕ(xs, s)(cid:13) 2 2. (cid:13) This setup also introduces learnable noise schedule, and while it generalizes beyond continuous data, such extensions fall outside the scope of our current discussion. Sampling. Sampling proceeds similarly to DDPM using the parameterized kernel from Equation (4.4.6): xt = αstσ2 σ2 xs + αtσ2 st σ2 {z µϕ (xs,t,s) } xϕ(xs, s) +σst σt σs ϵs, ϵs (0, I). (4.4.8) (Optional) FokkerPlanck Equation and Reverse-Time SDEs 4.5. via Marginalization and Bayes Rule 4.5 (Optional) FokkerPlanck Equation and Reverse-Time SDEs via Marginalization and Bayes Rule In this section, we offer probabilistic perspective on the structure of the FokkerPlanck equation and the reverse-time SDE. By leveraging fundamental tools such as the marginalization trick and Bayes rule, we illuminate the connection between the statistical formulation of stochastic processes and their corresponding differential equations. We emphasize that the derivation presented here is not mathematically rigorous proofs, but rather heuristic arguments intended to convey the underlying connections. 4.5.1 Fokker-Planck Equation from the Marginalization of Transition Kernels Given the forward transition probability as in Equation (4.1.2) (cid:17) (cid:16) p(xt+txt) = xt+t; xt + (xt, t)t, g2(t)tI , and the marginal distributions pt(xt), pt+t(xt+t), we aim to derive the Fokker-Planck equation that governs the time evolution of the marginal distribution pt. Change of Variables. By the Markov property, the marginal distribution at time + can be expressed as an integral over the previous state xt (i.e., Chapman-Kolmogorov equation): pt+t(x) = (cid:0)x; + (y, t)t, g2(t)tI(cid:1)pt(y) dy. We introduce new variable := + (y, t)t, so the Gaussian is centered at u. For small t, this map is invertible with = (u, t)t + O(t2), (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) det u (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) = 1 (u )(u, t)t + O(t2). Hence, change-of-variable formula leads us to: pt+t(x) = (cid:16) x; u, g2(t)tI (cid:17) pt(u) tf (u, t) upt(u) t(u )(u, t)pt(u) du + O(t2), 122 Diffusion Models Today: Score SDE Framework Taylor Expansion. For any smooth function ϕ : RD and scale σ > 0, if (0, I), the following approximation holds (known as the TaylorGaussian smoothing formula): N (x; u, σ2I)ϕ(u) du = [ϕ(x + σz)] = ϕ(x) + σ2 2 xϕ(x) + O(σ4). This is because Taylor expansion for: ϕ(x + σz) = ϕ(x) + σxϕ(x) + σ2 z2 xϕ(x)z + O(σ3) and E[z] = 0, E[zz] = I. Apply this with ϕ = pt, ϕ = upt, and ϕ = (u )pt, and use σ2 = g2(t)t, we can obtain pt+t(x) pt(x) = tf (x, t) xpt(x) t(x )(x, t)pt(x) + g2(t) 2 txpt(x) + O(t2) = tx (cid:0)f (x, t)pt(x)(cid:1) + g2(t) 2 txpt(x) + O(t2). Divide by and let 0 to obtain the FokkerPlanck equation. In Section C.1.4, we present the Itôbased derivation to complement the discretetime view above. 4.5.2 Why Does Reverse-Time SDE Take The Form? The rigorous derivation of the reverse-time SDE is technical and requires delving into the properties of the Fokker-Planck equation. However, the form of the reverse-time SDE can be understood intuitively through Bayes theorem. Here, we present heuristic derivation to provide insight into why Equation (4.1.6) takes its form, with the appearance of score functions12. Using Bayes Rule for Inversion. Our goal is to determine the reverse-time transition kernel by first considering the discrete-time case: p(xtxt+t), 12This derivation is inspired by the approach in this post. (Optional) FokkerPlanck Equation and Reverse-Time SDEs 4.5. via Marginalization and Bayes Rule 123 and then taking 0 to obtain the continuous-time formulation. Using Bayes theorem, we express: p(xtxt+t) = p(xt+txt) pt(xt) pt+t(xt+t) = p(xt+txt) exp (log pt(xt) log pt+t(xt+t)) . (4.5.1) The forward transition kernel is assumed to be as in Equation (4.1.2): p(xt+txt) = (cid:16) xt+t; xt + (xt, t)t, g2(t)tI (cid:17) Taylor Expansion. To handle the exponential term, we apply first-order Taylor expansion. The key insight is to expand around the point (xt, t) in both space and time: log pt+t(xt+t) = log pt(xt) + log pt(xt) (xt+t xt) + log pt(xt) t + O(h2 2) where := (xt+t xt, t). Therefore: log pt(xt) log pt+t(xt+t) = log pt(xt) (xt+t xt) log pt(xt) t + O(h2 2) (4.5.2) For the forward process with finite drift and diffusion, we have E[xt+t 2] = O(t), which ensures that the remainder term is O((t)2) in expecxt2 tation. Substituting into the Reverse Transition. Substituting equations Equation (4.1.2) and Equation (4.5.2) into Equation (4.5.1): p(xtxt+t) = 1 (2πg2(t)t)D/2 exp xt+t xt (xt, t)t2 2 2g2(t)t ! (cid:18) exp log pt(xt) (xt+t xt) log pt(xt) t + O((t)2) (cid:19) . Algebraic Manipulation. The key step is to complete the square in the exponent. We have: = xt+t xt (xt, t)t2 2 2g2(t)t (cid:2)xt+t xt (xt, t)t2 log pt(xt) (xt+t xt) 2 + 2g2(t)tx log pt(xt) (xt+t xt)(cid:3) 2g2(t)t 124 Diffusion Models Today: Score SDE Framework Let δ := xt+t xt and µ := (xt, t)t. Then: δ µ2 2 + 2g2(t)tx log pt(xt) δ 2 + 2g2(t)tx log pt(xt) δ =δ2 =δ2 =δ [µ g2(t)tx log pt(xt)]2 2 2δ µ + µ2 2 2δ [µ g2(t)tx log pt(xt)] + µ2 2 2 g2(t)tx log pt(xt)2 2 Substituting back: δ [f (xt, t)t g2(t)tx log pt(xt)]2 2 =xt+t xt [f (xt, t) g2(t)x log pt(xt)]t2 2. Therefore, p(xtxt+t) = 1 (2πg2(t)t)D/2 exp xt+t xt [f (xt, t) g2(t)x log pt(xt)]t2 2 2g2(t)t ! exp(O(t)) (cid:16) = xt; xt+t [f (xt, t) g2(t)x log pt(xt)]t, g2(t)tI (cid:17) (1 + O(t)). The additional term g2(t)tx log pt(xt)2 2 from completing the square is O((t)2) and can be absorbed into the error term. Similarly, the time derivative term log pt(xt) t is O(t) and will vanish in the continuous limit. Taking 0 Limit. As 0, under smoothness assumptions, the following approximations hold: (xt, t) (xt+t, + t), g(t) g(t + t), log pt(xt) log pt+t(xt+t) = s(xt+t, + t). (Optional) FokkerPlanck Equation and Reverse-Time SDEs 4.5. via Marginalization and Bayes Rule 125 Using these approximations and some rearrangements, we obtain: p(xtxt+t) 1 (2πg2(t)t)D/2 exp (cid:13) (cid:13) (cid:13)xt (cid:16) xt+t (cid:2)f (xt+t, + t) g2(t + t)s(xt+t, + t)(cid:3)t 2 (cid:17)(cid:13) (cid:13) (cid:13) 2 2g2(t + t)t ! . This implies that p(xtxt+t) is roughly normal distribution with: Mean:xt+t (cid:2)f (xt+t, + t) g2(t + t)s(xt+t, + t)(cid:3)t, Covariance:g2(t + t)tI. Taking the limit as 0, we derive the reverse-time continuous SDE given in Equation (4.1.6). 126 Diffusion Models Today: Score SDE Framework 4.6 Closing Remarks This chapter marked pivotal moment in our journey, unifying the discretetime diffusion processes from the variational and score-based perspectives into single, elegant continuous-time framework. We demonstrated that both DDPM and NCSN can be understood as discretizations of Stochastic Differential Equations (SDEs) with different drift/volatility coefficients. The cornerstone of this framework is the existence of corresponding reverse-time SDE, which formally defines generative process that reverses the noise corruption. Crucially, the drift of this reverse process depends on single unknown quantity: the score function, log pt(x), of the marginal data distributions at every point in time. This insight solidifies the score functions central role in generative modeling. Furthermore, we introduced purely deterministic counterpart, the Probability Flow Ordinary Differential Equation (PF-ODE), whose solution trajectories evolve along the same marginal densities {pt} as the SDEs. This remarkable consistency is guaranteed by the underlying Fokker-Planck equation. The profound implication is that the complex task of generation is fundamentally equivalent to solving differential equation. Training reduces to learning the score function that defines the equations vector field, while sampling becomes problem of numerical integration. The introduction of the PF-ODE, purely deterministic flow, provides powerful bridge to the third and final perspective on diffusion models. This concept of learning deterministic transformation governed by velocity field is the central principle of recent major family of generative models. In the next chapter, we will: 1. Explore this flow-based perspective, starting from its origins in Normalizing Flows and Neural ODEs. 2. Show how this viewpoint leads to the modern framework of Flow Matching, which directly learns velocity field to transport samples between distributions. Ultimately, we will see how the deterministic PF-ODE, which we derived from stochastic principles, can be constructed and generalized from this entirely different, flow-based origin, completing our unified picture of diffusion modeling. 5 Flow-Based Perspective: From NFs to Flow Matching Everything flows. Heraclitus The change-of-variables formula, cornerstone of probability theory (Tabak and Vanden-Eijnden, 2010; Turner, 2013), takes on new life in modern generative modeling. While Score SDEs offer differential equation framework to bridge data and prior distributions via the FokkerPlanck equation (Section 4.1.5), this continuous evolution is, at its core, dynamic form of the same fundamental principle. Change-of-Variables Formula of Densities. Given an invertible transformation , the density of = (z) where pprior is: p(x) = pprior(z) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) det 1(x) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) , where = 1(x). (5.0.1) This deceptively simple formula unlocks exact, bidirectional transport of densities and samples when is tractable, forming the very foundation of Normalizing Flows that we will introduce in Section 5.1. But what if we rethink this idea through the lens of continuous-time transformations? In this chapter, we build on this core principle to explore fresh view on diffusion models: Flow Matching (in Section 5.2). Emerging naturally from 127 Flow-Based Perspective: From NFs to Flow Matching (Continuous) Normalizing Flows, Flow Matching deepens our understanding of diffusion as powerful density transport process. To support solid understanding of this chapter, we provide in Chapter an intuitive, self-contained overview of the different variants of the change-ofvariables formula, progressing step by step from the basic case to the continuity equation and finally to the FokkerPlanck equation. 5.1. Flow-Based Models: Normalizing Flows and Neural ODEs 129 5.1 Flow-Based Models: Normalizing Flows and Neural ODEs In this section, we will introduce Flow-Based Models, including Normalizing Flows (NFs) (Rezende and Mohamed, 2015) and Neural Ordinary Differential Equations (NODEs) (Chen et al., 2018). NFs enable flexible and tractable probability density estimation by applying series of invertible transformations to simple base distribution. NODEs extend this framework to continuous time, where the transformation is governed by an ODE. By treating the transformations as continuous-time dynamics, NODEs provide smooth, scalable extension to the NF paradigm. Figure 5.1: Illustration of sample movement of NF under an invertible map. It consists of sequence of invertible functions : 7 that transform latent variable into data x, together with the inverse mapping 1 : 7 that reconstructs the data. An NF resembles an encoderdecoder structure, but with the encoder realized as smooth invertible map and the decoder given exactly by its inverse. The corresponding change in density can be computed via the change-of-variables formula, as given in Equation (5.0.1). 5.1.1 Normalizing Flows NFs (Rezende and Mohamed, 2015) model complex data distribution pdata(x) by transforming simple prior pprior(z) (e.g., standard Gaussian (0, I)) via an invertible mapping fϕ : RD RD, 130 Flow-Based Perspective: From NFs to Flow Matching with = fϕ(z) and pprior. Here, and share the same dimension. Using the change-of-variables formula in Equation (5.0.1), the model likelihood is1 log pϕ(x) = log pprior(z) + log (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) det 1 ϕ (x) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) . (5.1.1) Training Objective. Parameters ϕ are learned by maximizing the likelihood over data: LNF(ϕ) = Expdata [log pϕ(x)] . (5.1.2) Computing the Jacobian determinant in Equation (5.1.1) can be costly, scaling as O(D3) in general. Constructing Invertible Transformations. single complex invertible network can be expensive due to its Jacobian determinant. Conversely, simple transforms (e.g., linear) are efficient but lack expressivity. To balance this, NFs employ sequence of trainable invertible mappings k=0 , each with efficiently computable Jacobians: {fk}L1 fϕ = fL1 fL2 f0. Each fk is parameterized by neural network, though we omit the explicit dependence on ϕ for notational simplicity. Samples transform via xk+1 = fk(xk), = 0, . . . , 1, (5.1.3) with = x0 pprior and = xL, corresponding to data. The resulting (log-)density is derived as pϕ(x) = pprior(x0) L1 k=0 (cid:12) (cid:12) (cid:12) (cid:12) det fk xk (cid:12) (cid:12) (cid:12) (cid:12) , or equivalently, log pϕ(x) = log pprior(x0) + L1 k= log (cid:12) (cid:12) (cid:12) (cid:12) det 1 . fk xk (cid:12) (cid:12) (cid:12) (cid:12) (5.1.4) 1If the map is further constrained to be the gradient of convex potential, fϕ = ψϕ with ψϕ convex, then Equation (5.1.1) reduces to the MongeAmpère relation in Equation (7.2.4). This PDE characterizes the optimal transformation of one distribution into another under the quadratic cost. See Chapter 7 and (Huang et al., 2021) for further details. 5.1. Flow-Based Models: Normalizing Flows and Neural ODEs 131 xL fL1 1 L1 x2 f1 1 x1 f0 1 0 x0 Figure 5.2: Illustration of NF. NF is consisting of stack of invertible maps fϕ = fL1 fL2 f0. The transformation maps latent samples x0 pprior to data samples xL pdata. Examples of Invertible Flows. Extensive literature had focused on designing single-layer flow constructions that enable efficient computation of the Jacobian. Below, we introduce two representative types: Planar Flows (Rezende and Mohamed, 2015) and Residual Flows (Chen et al., 2019; Behrmann et al., 2019), with the latter motivating the developments in Section 5.1.2. Planar Flows: It applies simple transformation (z) = + uh(wz + b), where u, RD, R, and h() is an activation. The Jacobian determinant is (cid:12) (cid:12)1 + uh(wz + b)w (cid:12) (cid:12) (cid:12) (cid:12) . Residual Flows: Define the transform as (z) = + v(z), (5.1.5) with contractive (Lipschitz constant < 1). This ensures invertibility via the Banach fixed-point theorem. The log-determinant of the Jacobian reduces to trace expansion: log (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) det (z) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:18) = log det (cid:18) log = Tr (cid:19) (z) (cid:18) (z) (cid:19)(cid:19) (cid:18) (cid:18) log + = Tr = k=1 (1)k+1 (cid:19)(cid:19) v(z) (cid:18) v(z) Tr (cid:19)k! , (5.1.6) making evaluation efficient via trace estimators (Hutchinson, 1989). Flow-Based Perspective: From NFs to Flow Matching Sampling and Inference. Sampling from NFs is straightforward: draw x0 pprior and compute = fϕ(x0). Exact likelihoods are obtained from Equation (5.1.4). 5.1.2 Neural ODEs Figure 5.3: Discretevs. continuous-time normalizing flows. (Left) discrete NF transports samples by finite sequence of invertible maps xk+1 = fk(xk), yielding stepwise, non-crossing trajectories (dots with arrows). (Right) continuous NF (Neural ODE) evolves states along integral curves of dx(t) dt = vϕ(x(t), t), where black paths with tangent arrows are shown over the gray vector field. From Discrete-Time NFs to Continuous-Time NFs (Neural ODEs). NFs are typically formulated as sequence of discrete, invertible transformations. Viewed through the lens of Equation (5.1.3) and the Residual Flow formulation in Equation (5.1.5), each layer can be written as the following: xk+1 = fk(xk) := xk + vϕk (xk, k), where vϕk (, k) is layer-dependent velocity field parameterized by neural networks. Intuitively, this velocity field is learned vector-valued function that pushes the data points in the input space in small, smooth steps. Each transformation moves points along the directions suggested by this velocity, gradually morphing the simple prior distribution into the complex target distribution. This formulation, indeed, corresponds to the Euler discretization of the continuous-time ODE with learnable parameter ϕ: dx(t) dt = vϕ(x(t), t). 5.1. Flow-Based Models: Normalizing Flows and Neural ODEs 133 In the limit of infinite layers and vanishing step size (t 0), the discrete NFs converges to continuous model, yielding the framework of Neural ODEs (NODEs) (Chen et al., 2018), also known as Continuous Normalizing Flows (CNFs). Formal Setup of Neural ODEs. Neural ODE defines continuous transformation through: dx(t) dt = vϕ(x(t), t), [0, ] (5.1.7) where: x(t) RD is the state at time t; we sometimes write xt for brevity; vϕ(x(t), t) is neural network parameterized by ϕ. Goal of NODE. Starting from the initial condition x(0) pprior, the ODE evolves the state continuously over time, inducing family of marginal distributions pϕ(xt, t) (similar to PF-ODEs!)2. The goal is to learn the neural vector field vϕ, which intuitively represents velocity that transports points along continuous trajectories in data space. By learning this velocity, the terminal distribution at = 0 matches the target distribution pdata(). This continuous transformation unifies discrete normalizing flows and neural ODEs within single framework. Continuous-Time Change-of-Variables Formula. Analogous to Equation (5.0.1) or Equation (5.1.4), Chen et al. (2018) derived continuous-time analog of the change-of-variables formula. For the time-dependent density pϕ(x(t), t) of process x(t) evolving under Equation (5.1.7), the so-called Instantaneous Change-of-Variables Formula is: dt log pϕ(x(t), t) = vϕ(x(t), t). Thus, with the given prior pprior(x(T ), ), the log-density of the terminal state x(T ) induced by the neural ODE is given by log pϕ(x(T ), ) = log pprior(x(0), 0) 0 vϕ(x(t), t)dt. (5.1.8) 2We adopt flipped time convention, with = 0 denoting the prior (source) and = 1 the data (target) distribution. The prior is interchangeably written as pϕ(x(0), 0), pprior(x(0), 0), or simply pprior(z). 134 Flow-Based Perspective: From NFs to Flow Matching This expression enables exact likelihood evaluation by numerically solving the ODE, which in turn allows for maximum likelihood training of the model. We will return to this in detail later. Although it may appear unfamiliar at first, this instantaneous change of variable formula is special case of the FokkerPlanck equation, specifically its deterministic form known as the Continuity Equation (see Chapter B). It can also be interpreted as the continuous time limit of Equation (5.1.4). We summarize this result and its derivation in the following lemma: Lemma 5.1.1: Instantaneous Change of Variables Let z(t) be continuous random process with time-dependent density p(z(t), t), and suppose it evolves according to the ODE dz(t) dt = F(z(t), t). Assuming is uniformly Lipschitz in and continuous in t, the time derivative of the log-density satisfies: log p(z(t), t) = F(z(t), t). (5.1.9) Proof for Lemma. We present two alternative derivations in Section D.3. Connection to Discrete-Time Formula. The NODE likelihood in Equation (5.1.8), log pϕ(x(T ), ) = log pprior(x(0), 0) x vϕ(x(t), t) dt, can be seen as the continuous-time analogue of the discrete normalizing flow formulation in Equation (5.1.4): log pϕ(xL) = log pprior(x0) L1 k=0 log (cid:12) (cid:12) det (cid:12) (cid:12) fk xk (cid:12) (cid:12) (cid:12) (cid:12) . The integral mirrors the summation, and the trace operator replaces the log-determinant, as discussed in Equation (5.1.6). These parallels are further explored in the proof of the lemma. 5.1. Flow-Based Models: Normalizing Flows and Neural ODEs 135 Training NODEs. Based on Equation (5.1.8), NODEs learn parameterized velocity field vϕ such that the terminal distribution pϕ(, ) pdata, where trajectories evolve from latent variables x(0) pprior via the ODE flow. Training follows the MLE framework from Equation (1.1.2): LNODE(ϕ) := Expdata (cid:2) log pϕ(x, )(cid:3). Exact Log-Likelihood Computation. To compute log pϕ(x, ) for data point x, we integrate the change-of-variables formula equation 5.1.8: log pϕ(x, ) = log pprior(z(0)) 0 vϕ(z(t), t) dt. (5.1.10) Here, z(t) solves the ODE reversely from = to = 0: dz dt = vϕ(z(t), t) with z(T ) = x. The prior term log pprior(z(0)) is tractable for standard distributions. This enables exact likelihood-based training and evaluation in neural ODEs. Gradient-Based Optimization. Maximizing LNODE requires backpropagation through the ODE solver. The adjoint sensitivity method (Pontryagin, 2018; Chen et al., 2018) computes gradients via an auxiliary ODE with O(1) memory complexity, but NODE training remains expensive due to numerical integration at each step. Inference with NODEs. Sampling with trained model vϕ proceeds by drawing x(0) pprior and integrating forward (by numerical solvers): x(T ) = x(0) + 0 vϕ(x(t), t) dt. The terminal state x(T ) approximates sample from pdata. Moreover, we note that for any vector field F, the following identity holds: Tr (cid:19) (cid:18) z(t) = F. Hence, the divergence can be efficiently estimated using stochastic trace estimators, such as Hutchinsons estimator (Hutchinson, 1989), which makes exact likelihood computation more tractable in high-dimensional settings. 136 Flow-Based Perspective: From NFs to Flow Matching 5.2 Flow Matching Framework Score SDEs (Chapter 4) and NODEs (Section 5.1) offer an alternative perspective on generative modeling: learning continuous-time flow, either stochastic or deterministic, that transports simple Gaussian prior sample ϵ pprior to data-like sample from pdata. The Flow Matching (FM) framework (Lipman et al., 2022; Lipman et al., 2024; Tong et al., 2024) builds on this idea, but generalizes it to learn flow between two arbitrary fixed endpoint distributions: source distribution psrc and target distribution ptgt, both assumed to be easy to sample from. In this broader setup, the generation task becomes special case where psrc is Gaussian prior and ptgt is the data distribution. In this section, we adopt the FM viewpoint3, emphasizing its core principle: learning time-dependent vector field vt(xt) whose associated ODE flow matches predefined probability path {pt}t[0,1] subject to the boundary conditions p0 = psrc, p1 = ptgt. When psrc is Gaussian, we refer to this setting as Gaussian Flow Matching. Compared to classical diffusion models, FM enables efficient, simulation-free training for broad class of transport problems using only samples from the endpoints. 5.2.1 Lesson from Score-Based Methods We revisit the Score SDE framework (Chapter 4) using slightly different but equivalent formulation to extract key insights that motivate the FM approach. This analysis reveals how diffusion models implicitly learn probability flows and motivates more direct formulation. Step 1: Defining Conditional Path and Its Marginal Densities. diffusion model specifies continuous-time family of densities {pt}t[0,1] that transports simple prior pprior (e.g., Gaussian) at = 1, used as the source, to target data distribution pdata at = 0: p1(x1) = pprior(x1), p0(x0) = pdata(x0). 3Several related approaches share the core idea of transporting between endpoint distributions using continuous-time flow, though with slightly different formulations. These include Flow Matching (FM) (Lipman et al., 2022; Neklyudov et al., 2023), Rectified Flow (RF) (Liu, 2022; Heitz et al., 2023), and Stochastic Interpolants (Albergo et al., 2023; Albergo and Vanden-Eijnden, 2023; Ma et al., 2024). Here, we use the FM terminology as unifying representation. 5.2. Flow Matching Framework 137 Figure 5.4: (xt; αtx0, σ2 x0 pdata (left) towards the Gaussian prior pprior (right). transition distribution. pt(xtx0) = Illustration of I), defines (Gaussian) conditional probability path from data sample the conditional This path is implicitly defined via the forward conditional distribution pt(xtx0) = (xt; αtx0, σ2 I), x0 pdata (5.2.1) which induces the marginal density pt(xt) := pt(xtx0)pdata(x0)dx0. The increasing variance σ2 pt toward the Gaussian prior. of the conditional Gaussian drives the evolution of Step 2: Velocity Field. The time evolution of the marginal density pt is governed by velocity field vt : RD RD, derived from the FokkerPlanck equation: vt(x) := (t)x 1 2 g2(t)x log pt(x), (5.2.2) which defines deterministic particle flow through the PF-ODE: dx(t) dt = (t)x(t) 1 2 g2(t)x log pt {z vt(x(t)) . (cid:0)x(t)(cid:1) } This ODE transports an initial random variable x(0) pdata forward in time or x(1) pprior backward in time, such that the evolving marginal density of x(t) matches pt at every [0, 1] (see Underlying Rule below). 138 Flow-Based Perspective: From NFs to Flow Matching The scalar functions (t) and g(t) are determined by the coefficients of the associated forward SDE, or equivalently the Gaussian kernel parameters αt and σt defined in the conditional path (see Lemma 4.4.1). Step 3: Learning via the Conditional Strategy. The goal is to approximate the oracle velocity field vt(xt) using neural network sϕ(xt, t) trained via the expected squared error: LSM(ϕ) = EtU[0,1],xtpt sϕ(xt, t) xt log pt(xt)2i . Since the marginal score xt log pt(xt) is inaccessible, we exploit the tractable conditional distribution to define the conditional velocity: vt(xtx0) := (t)xt 1 2 g2(t)xt log pt(xtx0). By the law of total expectation, the marginal score is recovered as xt log pt(xt) = Ex0p(xt) [xt log pt(xtx0)] . (5.2.3) This justifies the surrogate training objective: LSM(ϕ) = Et,x0pdata,xtpt(x0) sϕ(xt, t) xt log pt(xtx0)2i } {z LDSM(ϕ) +C, where is constant independent of ϕ. The minimizer s(xt, t) satisfies s(xt, t) = Ex0p(xt) [xt log pt(xtx0)] = xt log pt(xt), where the second equality follows from Equation (5.2.3), thereby validating the conditional training objective. Underlying Rule: The FokkerPlanck Equation. The marginal density pt evolves according to the FokkerPlanck equation: pt(x) + (cid:18) (t)x 1 2 g2(t)x log pt(x) (cid:19) ! pt(x) = 0. {z vt(x) } This PDE ensures that the density given by the PF-ODE matches the marginal distribution of the forward SDE. To see this, recall the flow map Ψst(xs) of the PF-ODE as defined in Equation (4.1.9), which carries an initial state 5.2. Flow Matching Framework 139 Figure 5.5: Illustration of conditional versus marginal perspectives in diffusion. (This figure is motivated by Lipman et al. (2024).) (1) Conditional Gaussian path pt(x0), showing expanding densities from fixed x0 toward the prior. (2) Conditional velocity 2 g2(t)xt log pt(xtx0). (3) Marginal density pt, transporting the vt(xtx0) = (t)xt 1 data distribution pdata (orange) into the prior pprior (gray). (4) Marginal velocity vt(xt) = 2 g2(t)xt log pt(xt), obtained by averaging conditional directions from xt to multiple (t)xt 1 plausible origins (dashed), yielding the red arrow. In the FM framework with one-sided conditioning = x0, the same illustration applies to vt(xtx0) and vt(xt), without requiring them to be written explicitly in terms of the scores xt log pt(xtx0) or xt log pt(xt). xs at time directly to its state at t. Running the PF-ODE backward from = 1 to = 0, starting with x1 pprior, we obtain time-dependent densities through the pushforward formula: prev (x) = δ (x Ψ1t(x1)) pprior(x1)dx1. (5.2.4) The FokkerPlanck equation ensures that the induced density path coincides with the same evolving density: prev = pt. (5.2.5) In particular, this implies prev 0 = p0 = pdata, thereby recovering the data distribution at time = 0. Since the ODE solution map is bidirectional, we can similarly consider initializing at x0 pdata and solving the ODE forward in time, enabling parallel analysis. 5.2.2 Flow Matching Framework The analysis in Section 5.2.1 reveals that diffusion models succeed by learning velocity field, specifically, the score, that transports between distributions while satisfying boundary conditions. The design of the Gaussian conditional path in Equation (5.2.1), with increasing variance σ2 , implicitly anchors one endpoint to Gaussian prior while allowing the conditional density to be defined over the entire space, enabling score-based gradient computation. In this subsection, we introduce the FM framework, which builds on this insight (the same illustration in Figure 5.5 also applies to the FM framework) and extends it to learning continuous flows that transport samples between two arbitrary distributions, psrc and ptgt. 140 Flow-Based Perspective: From NFs to Flow Matching Step 1: Defining Conditional Path and Its Marginal Densities. Consider arbitrary source and target probability distributions psrc and ptgt on RD. We set4 p0(x) = psrc(x), p1(x) = ptgt(x). (5.2.6) FM implicitly defines continuous family of intermediate densities {pt}t[0,1] interpolating between these endpoints. Each marginal pt is expressed via latent variable drawn from known distribution π(z) and conditional distribution pt(xtz): pt(xt) = pt(xtz)π(z) dz, (5.2.7) with (π(z), {pt(z)}) chosen to satisfy the boundary conditions in Equation (5.2.6). We remark that, in general, the marginal densities pt are not tractable, since they require integrating over π(z), and both π(z) and the conditional distributions pt(xtz) can be complex. Nonetheless, conditioning on the latent grants FM the flexibility to model broad class of interpolation paths beyond those discussed in Section 5.2.1. Common choices for include: Two-sided conditioning: = (x0, x1) psrc(x0)ptgt(x1), where π couples source and target distributions. This allows FM to define transport between arbitrary distributions. One-sided conditioning: = x0 or = x1. It especially recovers diffusionlike setups when the source distribution is chosen to be Gaussian. In all cases, the conditional distributions pt(xtz) should admit tractable closed-form expressions. We make this assumption throughout and present specific constructions in Section 5.3.2 with illustrations in Figure 5.6. Step 2: Velocity Field. In standard diffusion models or Gaussian FM, the intermediate densities {pt}t[0,1] are constructed with one endpoint set to standard Gaussian. In this setting, the velocity field vt is uniquely defined and admits closed-form expression related to scores (see Equation (5.2.2)). In contrast, general FM interpolates between general source and target distributions psrc and ptgt, where the velocity field is no longer uniquely determined (as explained later). 4To align with the standard notation in FM literature, we reverse the time axis compared to earlier sections: = 0 corresponds to the source distribution and = 1 to the target. 5.2. Flow Matching Framework 141 The goal is to find velocity field vt(x) such that the induced ODE, which enables sample-wise transformation, dx(t) dt = vt(x(t)), [0, 1], produces marginal distributions of x(t) that match with pt at each time t, whether integrating forward from x(0) psrc or backward from x(1) ptgt (see Section 5.2.4 for more formal discussion). This requirement is captured by the continuity equation5: pt(x) + (cid:0)vt(x)pt(x)(cid:1) = 0. (5.2.8) Any velocity field vt that satisfies Equation (5.2.8) ensures that the ODE flow transports samples in way that exactly follows the prescribed pt (see Section 5.2.4 for details). Thus, solving the ODE enables transport from psrc to ptgt while matching all intermediate distributions. Intuitively, many different flows can induce the same marginal evolution. This is because Equation (5.2.8) is scalar equation, while vt is vector field in RD, so the equation admits infinitely many solutions. For example, if vt solves the equation, then so does vt + 1 pt vt, for any divergence-free vector field vt (i.e., vt = 0). FM therefore seeks particular velocity field vt that satisfies Equation (5.2.8), enabling continuous transport of samples along the path {pt}. For arbitrary distributions, however, pt and vt are generally not available in closed form. As concrete illustration, in Section 5.3.1 we consider the Gaussian-to-Gaussian bridge, where both quantities can be computed explicitly. Step 3: Learning via the Conditional Strategy. The goal of FM training is to approximate the oracle velocity field vt using neural network vϕ, by minimizing the expected squared error: LFM(ϕ) = Et,xtpt vϕ(xt, t) vt(xt)2i . We refer to this neural network parameterization as v-prediction (velocity prediction), which aims to learn the ODE drift term directly. 5The deterministic analogue of the FokkerPlanck equation, without the diffusion term. 142 Flow-Based Perspective: From NFs to Flow Matching As in Section 5.2.1, the oracle velocity vt(x) is generally intractable. To address this, we introduce latent variable π(z) and define conditional velocity field vt(xz) by construction. This allows us to rewrite the loss via the law of total expectation6: LFM(ϕ) = Et,zπ(z),xtpt(z) vϕ(xt, t) vt(xtz)2i {z } LCFM(ϕ) +C, (5.2.9) where is constant independent of ϕ. The main term LCFM is referred to as conditional flow matching. That is, minimizing LFM(ϕ) is equivalent to minimizing LCFM(ϕ), with the latter offering more tractable formulation. For LCFM(ϕ) to enable tractable, simulation-free training, two requirements must be met: (i) Sampling from the conditional probability path pt(xtz) should be straightforward (simulation-free). (ii) The conditional velocity vt(xtz), used as the regression target, must admit simple closed-form expression. We will provide explicit constructions that satisfy these conditions in Section 5.3.2. This conditional view makes training feasible: instead of learning the intractable unconditional velocity field vt(), the model learns the tractable conditional field vt(z): in direct analogy to denoising score matching. Even though there are infinitely many possible unconditional velocity fields consistent with given pt, one such field can be recovered by marginalizing the conditional velocity fields: vt(xt) := Ezp(xt) [vt(xtz)] , (5.2.10) where the expectation is taken over p(zxt). We can show that the minimizer of the conditional flow matching objective in Equation (5.2.9) recovers this marginal velocity: v(xt, t) = vt(xt). (5.2.11) Thus, learning to match the conditional velocity field vt(z) suffices to recover valid unconditional velocity field. 6This follows standard integration-by-parts argument, as in the derivation of Equation (3.3.3). Likewise, Equation (5.2.11) is derived using similar approach within the score matching framework. 5.2. Flow Matching Framework 143 We summarize the above discussion as follows: Theorem 5.2.1: Equivalence of LFM and LCFM The following holds: LFM(ϕ) = LCFM(ϕ) + C, where is constant independent of the parameter ϕ. Furthermore, the minimizer of both losses satisfies v(xt, t) = vt(xt), for almost every xt pt, where vt(xt) is defined in Equation (5.2.10). Proof for Theorem. The argument and derivation of the minimizer follows exactly the same reasoning as in the score matching case of Proposition 4.2.1. This marks the third instance where the conditioning trick yields tractable training objective. Notably, the variational, score based, and flow based approaches all reflect the same underlying principle. Remark. Taking π = pdata, we can apply Bayes rule: p(x0xt) = pt(xtx0)pdata(x0) pt(xt) , similar decomposition of Equation (5.2.10) appears in score-based models: xt log pt(xt) = Ex0p(xt) [xt log pt(xtx0)] = Ex0pdata (cid:20) xt log pt(xtx0) pt(xtx0) pt(xt) (cid:21) , which mirrors the marginalization strategy in Equation (5.2.10). As in Section 5.2.1, where the conditional density pt(xtz) and conditional velocity field vt(xtz) must be explicitly specified, with pt(xtx0) = (xt; αtx0, σ2 2 g2(t)xt log pt(xtx0), the general conditional flow matching framework also requires these two components. However, we have not yet construct the conditional density pt(xtz) or the conditional velocity field vt(xtz) in this general case. In the next section, we I) and vt(xtx0) = (t)xt 144 Flow-Based Perspective: From NFs to Flow Matching introduce several common instantiations of these components. 5.2.3 Comparison of Diffusion Models, General Flow Matching, and NODEs Comparison of Diffusion Models and General Flow Matching. The insight from Section 5.2.1 leads to an extended FM framework that retains the same underlying principles. To highlight their similarities, we summarize them in Table 5.1. Table 5.1: Comparison between diffusion models (or Gaussian FM) and the general FM framework. Here, the general FM framework refers to the setting with two-sided conditioning, where x0 psrc and x1 ptgt are sampled independently. Aspect Diffusion Model General FM Source dist. psrc Target dist. ptgt Latent dist. π(z) Cond. dist. pt(xtz) Marginal dist. pt(xt) Cond. velocity vt(xz) Marginal velocity vt(x) Learning objective Underlying Rule Gaussian prior Data distribution pdata (xt; αtx0, σ2 pt(xtx0)pdata(x0) dx I) (t)x 1 (t)x 1 2 g2(t) log pt(xx0) 2 g2(t) log pt(x) LSM = LDSM + Any Any See Section 5.3.2 See Section 5.3.2 pt(xtz)π(z) dz See Section 5.3.2 See Equation (5.2.10) LFM = LCFM + Fokker-Planck / Continuity Equation We remark that since Gaussian FM is essentially equivalent to the standard diffusion model (see more in Chapter 6), we will not differentiate between them unless explicitly stated. Connection to NODEs. FM can be viewed as simulation free alternative to NODEs, introduced in Section 5.1.2. While CNFs require solving ODEs during maximum likelihood training, which is computationally intensive, FM bypasses this by directly regressing prescribed velocity field through simple regression loss. The key insight is that when the marginal density path connecting the source and target distributions is fixed, exact simulation during training becomes unnecessary. 5.2.4 (Optional) Underlying Rules Continuity Equation: Mass Conservation Criterion. Similar to the PF-ODE and FokkerPlanck analysis in Section 5.2.1, we now present criterion for 5.2. Flow Matching Framework 145 verifying whether the density path induced by an ODE flow aligns with prescribed path {pt}t[0,1]. Consider the ODE describing the flow of particles under time-dependent velocity field vt: dx(t) dt = vt (x(t)) . As in Equation (5.2.4), this ODE defines flow map Ψst(x0) for any s, [0, 1], which in particular transports an initial point x0 psrc at time 0 to its state at time t. The induced distribution at time is given by the pushforward pfwd (x) = δ (x Ψ0t(x0)) psrc(x0)dx0 =: Ψ0t#psrc, (5.2.12) so that Ψ0t(x0) pfwd backward from x1 ptgt to psrc via Ψ10(x1). whenever x0 psrc. Similarly, one can transport Suppose we are given prescribed density path {pt}t[0,1], and we construct velocity field {vt}t[0,1] to define particle flow. This naturally raises the question: Question 5.2.1 Under what conditions does the flow-induced density pfwd the target density pt for all [0, 1]? exactly match Once the two density evolutions align, we can leverage the ODE flow to flexibly transport samples between psrc and ptgt by solving the ODE. As in Equation (5.2.5), principled way to verify this alignment is via the continuity equation, which captures the conservation of mass in time-evolving densities: Theorem 5.2.2: Mass Conservation Criterion The flow-induced density pfwd [0, 1]; i.e., equals the prescribed path pt for all pfwd = pt, for all [0, 1], if and only if the pair (pt, vt) satisfies the continuity equation: tpt(x) + (pt(x)vt(x)) = 0, for all [0, 1] and x. Proof for Theorem. 146 Flow-Based Perspective: From NFs to Flow Matching conceptual derivation is provided in Section D.3.2, while more rigorous treatment can be found in (Villani et al., 2008) (see Mass Conservation Formula). From Conditional to Marginal Paths. As seen in Section 5.2.2, we begin by defining conditional probability path pt(z) and corresponding conditional velocity field vt(z). We then construct the marginal velocity field via: vt(x) = vt(xz) pt(xz)π(z) pt(x) dz, as in Equation (5.2.10). However, we still need to ensure that the resulting marginal velocity vt induces an ODE flow whose density path aligns with the prescribed pt. Fortunately, this verification can be done entirely at the conditional level: if each conditional velocity field vt(z) induces the conditional density path pt(z), then the resulting marginal velocity vt also induces the correct marginal path. Formally, this is stated as follows: Proposition 5.2.3: Marginal VF Generates Given Marginal Density If the conditional velocity fields vt(z) induce conditional density paths that match pt(z) (starting from p0(z)), then the marginal velocity field vt() defined in Equation (5.2.10) induces marginal density path that aligns with pt(), starting from p0(). Proof for Proposition. This result follows by verifying that the pair (pt, vt) satisfies the Continuity Equation. We present the argument in converse manner to provide intuition for why the marginalized velocity field takes the form in Equation (5.2.10). Since the conditional velocity fields vt(z) induce density paths matching the conditional densities pt(z) for π, the continuity equation holds for each conditional pair: dt pt(xz) = (cid:0)vt(xz)pt(xz)(cid:1). (5.2.13) We aim to find velocity field vt() whose induced densities align with the marginal density pt, i.e., satisfy dt pt(x) = (cid:0)vt(x)pt(x)(cid:1). (5.2.14) 5.2. Flow Matching Framework 147 Starting from the definition of pt in Equation (5.2.7), dt pt(x) = dt = pt(xtz)π(z) dz (cid:16) (cid:16) (cid:17) vt(xz)pt(xz) π(z) dz vt(xz)pt(xz)π(z) dz (cid:17) , = where the second equality follows by applying Equation (5.2.13). Comparing this with the right-hand side of Equation (5.2.14) shows that, up to divergence-free term, vt(x)pt(x) = vt(xz)pt(xz)π(z) dz. Therefore, we can define vt(x) := vt(xz) pt(xz) pt(x) π(z) dz, which is precisely the form in Equation (5.2.10). The proof of this theorem essentially follows the reverse of this argument. This connection allows us to reduce the construction of the potentially intractable marginal velocity field to defining simpler conditional fields vt(z), which are easier to work with by construction. 148 Flow-Based Perspective: From NFs to Flow Matching 5.3 Constructing Probability Paths and Velocities Between Distributions The essence of flow matching lies in the gradual transformation of source distribution into target. To direct this transformation, two key elements are needed: the probability path pt, which provides snapshot of the evolving distribution at each time t, and the velocity field vt, which describes how individual particles move along the path. These two objects are not independent; they are linked through the continuity equation, which ensures that particle dynamics are consistent with the evolution of the distribution. Thus, the learning task reduces to finding velocity field vt that faithfully drives the process. The difficulty, however, is that for general and complex distributions, the true marginal velocity vt is unknown, leaving us with an intractable target that cannot be accessed directly. The core idea of Conditional Flow Matching is to address the intractability of the true marginal velocity by constructing an artificial but tractable process. To do this, we introduce conditioning variable and design either conditional velocity vt(xtz) and/or conditional path pt(xtz), which are deliberately chosen to be simple. Because these conditional objects are known in closed form, they serve as surrogate targets that the model can regress against. This leads to valid training loss LCFM, provided two practical requirements are met: (i) we can sample efficiently from pt(z), and (ii) the corresponding velocity vt(z) admits closed-form expression. How should we design well behaved conditional process? For inspiration, we turn to the one case that is fully understood: the Gaussian to Gaussian bridge (Section 5.3.1). This example highlights two natural design strategies: adopt Gaussian probability path at each time t, or prescribe an affine velocity field, both of which are analytically tractable. Guided by this insight, we extend to general endpoint distributions with two complementary views (see also Section B.1.2) for constructing conditional paths and velocities: Conditional Probability Path First (Eulerian View). It begins with conditional probability path pt(z) and derives the corresponding conditional velocity field. Conditional Flow First (Lagrangian View). It starts from conditional flow Ψ0t(z), typically affine, and derives the conditional velocity field by differentiating with respect to time along trajectories. 5.3. Constructing Probability Paths and Velocities Between Distributions 149 In Section 5.3.2, we detail the first approach, which shows its close analogy to diffusion model construction discussed in Section 5.2.1, while in Section 5.3.3 we present the second. Together, these perspectives provide practical framework for defining pt(xtz) and vt(xtz), enabling simulation-free training and the construction of flows between arbitrary source and target distributions. 5.3.1 Key Special Case: Marginal pt(xt) and Velocity vt(xt) in the Gaussian-to-Gaussian Bridge We begin with the Gaussianendpoint case, where we can compute the marginal density pt(xt) and velocity field vt(xt) analytically. This serves as template for the general construction of the conditional density pt(xtzt) and velocity field vt(xtzt). When the source and target distributions, psrc and ptgt, are both Gaussian, the velocity field vt() admits closed-form expression. We consider the interpolated marginal density path: pt(xt) = (cid:16) (cid:17) xt; µ(t), σ2(t)I , (5.3.1) with time-varying mean µ(t) and variance σ2(t) > 0. The two endpoints are given by psrc = p0 = (cid:16) (cid:17) x; µ(0), σ2(0)I , ptgt = p1 = (cid:16) (cid:17) x; µ(1), σ2(1)I , so that the path {pt}t[0,1] connects these distributions. With the given path {pt}t[0,1], there are indeed many velocity fields that induce an ODE flow Ψ0t(x) such that p0 implies Ψ0t(x) pt. For this Gaussian path, particularly simple realization is given by7: Ψ0t(x) := µ(t) + σ(t) (cid:18) µ(0) σ(0) (cid:19) . (5.3.2) For the defined Gaussian path pt (Gaussian for all t), the velocity field vt() inducing the ODE flow in Equation (5.3.2) is uniquely and analytically characterized as follows (Lipman et al., 2022): 7In (Lipman et al., 2022), the authors consider Ψ0t(x) = µ(t) + σ(t)x, which requires constraints on µ(t) and σ(t) to ensure boundary conditions. We adopt an equivalent normalized formulation that avoids such constraints. 150 Flow-Based Perspective: From NFs to Flow Matching Proposition 5.3.1: Closed-Form Velocity Field for Gaussian Density Path Let pt be the Gaussian path in Equation (5.3.1). Then the velocity field vt() that generates the ODE flow Equation (5.3.2) is unique for the defined Ψ0t and has the closed-form expression: vt(x) = σ(t) σ(t) (x µ(t)) + µ(t). Proof for Proposition. Consider the ODE with initial condition y: dt Ψ0t(y) = vt(Ψ0t(y)). Since Ψ0t is invertible (as σ(t) > 0), we may set = Ψ0t(y) and = Ψ1 0t(x) = Ψt0(x) to obtain (cid:0)Ψ1 Ψ 0t 0t(x)(cid:1) = vt(x). Differentiating Equation (5.3.2) with respect to gives Ψ 0t(x) = µ(t) + σ(t) (cid:18) µ(0) σ(0) (cid:19) . Solving for = Ψ1 0t(x) yields = µ(0) + σ(0) (cid:18) µ(t) σ(t) (cid:19) . Substituting this into Ψ 0t(x) gives σ(t) σ(t) vt(x) = (x µ(t)) + µ(t), as claimed. We note that for fixed flow map Ψ0t (flow-first view), the velocity is uniquely determined by vt = tΨ0t Ψ1 0t. Under this construction, the pair (pt, vt) automatically satisfies the continuity equation. By contrast, for given density path 7 pt without fixing Ψ0t (probability-path-first view), the velocity field is not unique. 5.3. Constructing Probability Paths and Velocities Between Distributions 151 This distinction precisely characterizes the difference between the flow-first and probability-path-first perspectives. This closed-form characterization remains valid when conditioning on latent variable z. In the following, we extend this insight to construct conditional Gaussian path pt(z) and derive the corresponding conditional velocity field vt(z) for the general marginal setting. 5.3.2 Conditional Probability-Path-First Construction of vt(z) and pt(z) Figure 5.6: Illustrations of two common types of conditioning probability paths. It includes: (1) two-sided, conditioned on x0 ptgt and x1 psrc with general endpoint distributions; (2) one-sided, conditioned at either x0 ptgt or x1 psrc. We aim to construct conditional density path pt(z) first and then derive its corresponding conditional velocity field vt(z) (via Proposition 5.3.1), under conditioning with respect to π(z). Depending on how is chosen, there are two natural scenarios: (i) two-sided conditioning with = (x0, x1), or (ii) one-sided conditioning with = x0 or x1. In either case, the construction must match the boundary distributions: psrc(x0) = p0(x0z)π(z) dz, ptgt(x1) = p1(x1z)π(z) dz. Since verifying these constraints is straightforward once concrete construction is specified, we do not emphasize the verification step here. I. Two-Sided = (x0, x1) Beam-Like Path. Choice of π(z). Consider general distributions psrc and ptgt over RD. Let = (x0, x1) with x0 psrc and x1 ptgt independently, i.e., π(z) = psrc(x0)ptgt(x1). Choice of Conditional Path pt(z). Define the conditional path by linear interpolation with fixed variance σ > 0: pt(xtz = (x0, x1)) = (xt; atx0 + btx1, σ2I), 152 Flow-Based Perspective: From NFs to Flow Matching where at and bt are time-dependent functions satisfying a0 = 1, b0 = 0 and a1 = 0, b1 = 1. choice suggested by (Lipman et al., 2022; Liu, 2022) is at = 1 t, bt = t. In the deterministic case σ = 0, we obtain pt(xtz) = δ(cid:0)xt [atx0 + btx1](cid:1), which describes deterministic interpolating path from x0 to x1. Derived Conditional Velocity vt(z). By Proposition 5.3.1, the conditional velocity is vt(xz) = tx0 + tx1. CFM Loss. When σ = 0 so that xt = atx0 + btx1, the CFM loss reduces to (cid:1)(cid:13) 2 . (cid:13) From Equations (5.2.10) and (5.2.11), the optimal velocity field is LCFM = Et,x0psrc,x1ptgt (cid:13) (cid:13)vϕ(xt, t) (cid:0)a tx0 + tx1 v(xt, t) = (cid:2)x txt (cid:3) = (cid:2)a tx0 + tx1xt (cid:3) . Here, the expectation is taken over p(x0, x1xt), the conditional distribution over source-target pairs (x0, x1) that could have produced the observed interpolation xt = atx0 + btx1 at time t. II. One-Sided = x0 or x1 Spotlight-Like Path. We illustrate the conditional probabilitypathfirst construction in the one-sided setting, considering the standard generative setup with psrc = (0, I) and ptgt = pdata. Crucially, this Gaussian source is not an additional assumption but direct consequence of the conditional path defined below. more general treatment of arbitrary endpoints will be given in Section 5.3.3. Choice of π(z). We take = x1 with π(z) = pdata(x1) (the case = x0 pprior follows analogously). Choice of Conditional Path pt(z). For fixed x1 pdata, define pt(xtz = x1) = (cid:0)xt; btx1, a2 with a0 = 1, b0 = 0, a1 = 0, b1 = 1 (usually interpreted as the limit). At the boundaries, I(cid:1), p0(z = x1) = (; 0, I), p1(z = x1) = δ( x1). Marginalizing over x1 yields {pt}t[0,1] with p0 = (0, I) (independent of pdata) and p1 = pdata. 5.3. Constructing Probability Paths and Velocities Between Distributions Derived Conditional Velocity vt(z). For (0, 1) with bt > 0, applying Proposition 5.3.1 to the conditional Gaussian path gives vt(xx1) = tx1 + at (cid:0)x btx (cid:1). One-Sided CFM Objective. With U(0, 1) (or any fixed sampling distribution) and x1 pdata, the CFM loss becomes LCFM = Et,x1 Extpt(x1) (cid:13) (cid:13) (cid:13)vϕ(xt, t) tx1 + b at (cid:0)xt btx1 2 (cid:1)i(cid:13) (cid:13) (cid:13) 2 . (5.3.3) By MSE optimality, the unique minimizer is the marginal velocity field v(x, t) = [vt(xx1)xt = x] = (cid:2)a tx0 + tx1 (cid:12) (cid:12)xt = x(cid:3) . Equivalence to Two-Sided Target. For paired samples (x0, x1) with xt = atx0 + btx1, vt(xtx1) = tx1 + at (cid:0)xt btx (cid:1) = tx0 + tx1. Thus the one-sided loss regresses to the conditional expectation of the two-sided target given xt: v(x, t) = E(cid:2)a tx0 + tx1xt = x(cid:3), so the one-sided and two-sided CFM objectives share the same minimizer. Gaussian FM = Diffusion Model. We use the FM convention where = 0 denotes the source/prior and = 1 denotes the target/data: psrc = pprior, ptgt = pdata. By contrast, diffusion models typically index time from data to noise (i.e., = 0 is data and = 1 is prior). Here, we consistently adopt the FM convention indexing to avoid confusion. If further psrc = (0, I), then for fixed condition x1 pdata, the conditional path pt(x1) is naturally chosen to be Gaussian, while the target distribution ptgt itself need not be Gaussian. Some literature usually refer to this setting as Gaussian FM. Choosing at = 1 and bt = (equivalently, αt = and σt = 1 under the relabeling at := σt, bt := αt in diffusion model) recovers the familiar FM/RF schedule (Lipman et al., 2022; Liu, 2022). In the Gaussian FM setting, both the beam-like and spotlight-like conditional paths lead to training objectives that are similar to the standard diffusion losses. As we will elaborate in Chapter 6, Gaussian FM can in fact 154 Flow-Based Perspective: From NFs to Flow Matching Table 5.2: Summary of Different Interpolants written in FM convention xt = atx0 + btx1, where x0 psrc = pprior, x1 ptgt = pdata. VE/VP are converted from their diffusion convention (data noise) via at := σt, bt := αt. FM/RF Trig. (Albergo et al., 2023) at (prior coeff.) bt (data coeff.) a0 b0 a1 b1 pprior VE at 1 0 1 a1 VP p1 b2 bt 0 1 1 0 1 1 0 0 1 (0, 1I) (0, I) (0, I) cos(cid:0) π sin(cid:0) π 2 t(cid:1) 2 t(cid:1) 1 0 0 1 (0, I) be equivalently interpreted as diffusion model trained to predict the velocity, under the linear schedule at = 1 and bt = t. This perspective highlights that flow matching and diffusion are not fundamentally different, but rather two equivalent formulations that can be transformed into one another. The Gaussian FM objective is particularly appealing in practice: its loss function (Et,xt ) is simple, and it has been shown to achieve competitive performance at scale (Esser et al., 2024). vϕ(xt, t) (x1 x0)2 Remark. It is worth emphasizing that some prior works (Liu, 2022; Lipman et al., 2022) suggest that adopting the canonical affine flow, at = 1 and bt = t, yields straight-line ODE trajectory enabling faster sampling. However, this claim does not hold in general. The velocity field in this formulation is given by the conditional expectation v(x, t) = E[x1 x0Xt = x], which depends on and thus does not always align with the naive direction x1 x0. In practice, the choice of time-weighting functions and parameterizations strongly influences training dynamics and can improve empirical performance, but such improvements cannot be attributed to the claimed straightness of the scheduler (at = 1 t, bt = t). 5.3. Constructing Probability Paths and Velocities Between Distributions 5.3.3 Conditional Flow-First Construction of vt(z) and pt(z) We treat the general case where the endpoints psrc (at = 0) and ptgt (at = 1) are arbitrary. Our goal is to design, directly in trajectory space, conditional flow that transports samples from psrc to ptgt and yields closed-form vt(xtz) usable as regression target. Motivation. Instead of first designing conditional density path, we may directly specify conditional flow map Ψ0t(; z) that moves samples along trajectories. This has two practical advantages: (i) it immediately yields regression target for training via time derivative along trajectories; (ii) on geometry-structured spaces (Riemannian manifolds, Lie groups, or constrained submanifolds), it is often natural to construct the conditional flow map Ψ0t directly from the geometry (e.g., geodesics, exponential maps, or premetrics) (Lipman et al., 2024) which yields analytic, simulation-free target velocities for training. Conditional Affine Flow (Link to Proposition 5.3.1). We fix conditioning variable π (e.g., = x1 ptgt in one-sided spotlight training) and push forward x0 psrc through the time-varying conditional affine flow Ψ0t(x0; z) := µt(z) + At(z)x0, [0, 1], where µt(z) RD and At(z) RDD is invertible for (0, 1). The boundary A0(z) = I, µ0(z) = 0 recovers psrc at = 0. It is standard to interpret boundary when 1 as limit (the terminal map may concentrate mass on lower-dimensional set or point)8. Induced Conditional Path pt(z). The construction defines pt(z) = (cid:0)Ψ0t(; z)(cid:1) #psrc, pt() = pt(z)π(z) dz. What ultimately matters in LCFM is how to sample from it: first draw π, then draw x0 psrc, and finally set xt = µt(z) + At(z)x0. We remark that when Ψ0t is affine in x0, then pt(z) is Gaussian if and only if psrc is Gaussian. In particular, for arbitrary (non-Gaussian) psrc, an affine flow yields generally non-Gaussian pt(z). 8Allowing A1(z) to be singular (e.g., 0) is compatible with invertibility on (0, 1) and causes the path to contract onto the prescribed endpoint at = 1. 156 Flow-Based Perspective: From NFs to Flow Matching Derived Conditional Velocity vt(z). The conditional velocity vt(z) is obtained by t-differentiating the conditional flow map Ψ0t. Following the derivation in Proposition 5.3.1, consider the conditional ODE defined by the flow map Ψ0t(y; z) with initial condition y, where the goal is to identify the corresponding conditional velocity field vt(z): dt Ψ0t(y; z) = vt (cid:16) Ψ0t(y; z) {z } (cid:12) (cid:17) (cid:12) (cid:12)z . Since Ψ0t(; z) is invertible for (0, 1), we may express in terms of the current state := Ψ0t(y; z) as = Ψ1 0t(x; z) = Ψt0(x; z). Substituting this into the ODE yields the following construction of the conditional velocity field: vt(xz) := Ψ0t (cid:0)Ψt0(x; z); z(cid:1), dt which makes explicit that the derivative must be taken along the trajectory that reaches the spatial point at time t. Since xt = µt(z) + At(z)x0 and At(z) is invertible on (0, 1), we have x0 = At(z)1(cid:0)x µt(z)(cid:1), giving vt(xz) = µ t(z) + t(z)At(z)1(cid:0)x µt(z)(cid:1). One-Sided Conditioning (z = x1). Choosing µt(z) = btz and At(z) = atI with a0 = 1, a1 = 0 and b0 = 0, b1 = 1 (with at > 0 for (0, 1)) yields xt = atx0 + btx1, vt(xx1) = tx1 + at (cid:0)x btx1 (cid:1). On paired samples (x0, x1) (with xt = atx0 + btx1), this simplifies to the usual CFM target: vt(xtx1) = tx0 + tx1. Two-Sided Conditioning (z = (x0, x1)). The same template with µt(x0, x1) = btx1 and At(x0, x1) = atI makes the conditional path deterministic: xt = atx0 + btx1, pt(x0, x1) = δ ( (atx0 + btx1)) , and the conditional velocity is vt(xtx0, x1) = tx0 + tx1, i.e., the standard two-sided CFM target. 5.3. Constructing Probability Paths and Velocities Between Distributions Unconditional Gaussian Path as Special Case. of (denoted µ(t)) and At = σ(t) σ(0) I, then If µt is independent Ψ0t(x0) = µ(t) + σ(t) x0 µ(0) σ(0) , vt(x) = µ(t) + σ(t) σ(t) (cid:0)x µ(t)(cid:1), which recovers the Gaussian density path and the closed-form velocity in Proposition 5.3.1. 5.3.4 Probability-Path-First vs. Flow-First Construction Both constructions aim to connect source distribution psrc and target distribution ptgt through conditional dynamics. The probability-path-first (Eulerian) view begins by positing conditional density path pt(z), often chosen from Gaussian or affine families so that the associated velocity vt(z) can be solved analytically. The flow-first (Lagrangian) view instead specifies conditional flow map Ψ0t(z) and obtains the velocity directly by differentiation along particle trajectories. While both yield equivalent transport under regularity, they differ in identifiability, ease of computation, and how endpoint constraints are enforced. The following table summarizes these contrasts. The takeaway: path-first is natural when conditional paths admit closed-form velocities; flow-first is natural when you have strong structural priors on trajectories. 158 Axis Given Flow-Based Perspective: From NFs to Flow Matching Conditional Probability-Path-First Conditional Flow-First Conditional density path pt(z). Conditional flow map Ψ0t(z) (trajectories, for each fixed z). Get Velocity For each z, find vt(z) s.t. Along paths (for each z): tpt(z) + (pt(z)vt(z)) = 0; vt (Ψ0t(z)z) = dt Ψ0t(z). Non-unique: if (ptwt) = 0 then vt + wt yields the same pt. When Ψ0t is invertible, one can solve vt(xz) = dt Ψ0t(Ψ 0t(x)z). Closed Form of vt(z) Uniqueness of vt(z) Convenient when pt(z) is Gaussian / exponential-family; otherwise obtaining vt(z) is nontrivial. For each z, vt(z) is underdetermined unless selection rule (e.g., potential flow / min. kinetic energy) is imposed. Convenient when Ψ0t(z) has structure (affine/low-rank); avoids density evaluation. both Given Ψ0t(z), = (Ψ0t(z))#p0 and vt(z) are determined; non-invertible maps still define vt(z) along trajectories, while invertible ones make it unique. pt(z) Realizability Must verify the constructed vt(z) solving the continuity equation on the intended support. Holds by construction: pt(z) = (Ψ0t(z))# p0(z). Match (psrc, ptgt) Mix conditionals: psrc = p0(z)π(z) dz, ptgt = p1(z)π(z) dz. Under Gaussianaffine conditional paths with z-independent coefficients, psrc can be forced to be Gaussian. For general fixed endpoint psrc (possibly non-Gaussian), the choice of pt(z) does not generally pin psrc. Preferred Scenarios Diffusion-style constructions; analytic targets via conditional Gaussians pt(z). Set Ψ00 = Id and choose boundary condition to hit any ptgt. Strong structural priors via maps Ψ0t(z); easy boundary control; accommodates singular/low-dimensional endpoints; natural for map-based regularization/transport costs. 5.4. (Optional) Properties of the Canonical Affine Flow 159 5.4 (Optional) Properties of the Canonical Affine Flow Given two endpoint distributions p0 = psrc and p1 = ptgt, natural and widely used choice for defining the conditional path in flow matching (FM) (Lipman et al., 2022) and rectified flow (RF) (Liu, Gong, et al., 2022) is the linear interpolation which yields the interpolant at = 1 t, bt = t, xt = (1 t)x0 + tx1, x0 psrc, x1 ptgt. Under this choice, the training objective simplifies to EtU[0,1]Ex0,x1 h(cid:13) (cid:13)vϕ(xt, t) (x1 x0)(cid:13) 2 (cid:13) 2 . This linear flow enjoys several appealing properties. In particular, it admits an iterative refinement scheme, known as Reflow, which progressively straightens the path between distributions while preserving the marginals. 5.4.1 Rectifying Flows: From Noisy Guesses to Structured Pairings From Noise to Data via Coherent Paths. Take the generation task where psrc is the prior and ptgt is the real data. We want continuous path that transports noise to data. naive tactic samples z0 psrc and x1 ptgt independently, interpolates (e.g. xt = (1 t)x0 + tx1), and fits velocity field to that line. This creates incoherent pairings: endpoints are unrelated across iterations, so trajectories fluctuate, variance explodes, convergence slows, and sample quality suffers. Why Independent Couplings Fall Short. Conditional flow matching with independent draws uses π(z) = psrc(x0)ptgt(x1), or one-sided variants. Such couplings are sampling-friendly but induce jagged, high-variance paths that velocity field struggles to model. Rectify the Flow via Dependent Coupling. Rather than relying on arbitrary pairings, we use pre-trained diffusion model vϕ(, t) as the drift in PF-ODE to deterministically transport each source point. Starting from z(0) = z0 psrc, we integrate dz(t) dt = vϕ(z(t), t), [0, 1], 160 Flow-Based Perspective: From NFs to Flow Matching to obtain ˆz1 := z(1) positioned near the data space learned from the pretrained model. The resulting pair (z0, ˆz1) forms dependent coupling: it follows structured, model-guided path rather than an arbitrary interpolation. This idea extends naturally to affine reference paths of the form xt = atx0 + btx1, where x0 psrc and x1 ptgt. Algorithm 2 Rectify Operation Input: Reference path {xt}t[0,1] (e.g. xt = atx0 + btx1) 1: Pre-Train Diffusion. Fit vϕ on the chosen path by minimizing ϕ arg min ϕ Et,x0,x1 (cid:20)(cid:13) (cid:13) (cid:13)vϕ(xt, t) (cid:21) . dxt dt 2 (cid:13) (cid:13) (cid:13) 2 2: Rectify. Sample z0 psrc and integrate dz(t) dt = vϕ(z(t), t), z(0) = z0, [0, 1], to obtain ˆz1 = z(1) and the trajectory {z(t)}t[0,1]. Output: Dependent (coherent) pair (z0, ˆz1) or the full trajectory. Why It Works: Marginal-Preserving Structure. Let Φ0t denote the flow map generated by the above ODE defined by the pre-trained diffusion vϕ; then z(t) = Φ0t(z0) and ˆz1 = Φ01(z0). The Rectify procedure pairs each source point with its flow endpoint, giving the deterministic joint πRectify(z0, z1) = psrc(z0)δ(cid:0)z1 Φ01(z0)(cid:1). We have two immediate consequences: Source Marginal is Preserved: πRectify(z0, z1) dz1 = psrc(z0). Pushforward Along the Flow: (Φ0t)#psrc = Law(z(t)), i.e., the timet distribution is the pushforward of psrc by Φ0t. If vϕ matches the oracle drift of given reference path xt, then all intermediate marginals coincide: Law(z(t)) = Law(xt), for all [0, 1], and (Φ01)#psrc = ptgt. Summary. Rectification replaces noisy independent pairings with smooth teacher-guided trajectories, lowering variance, easing optimization, and improving samples. The idea covers canonical linear paths xt = (1 t)x0 + tx1 and general affine forms xt = atx0 + btx1. 5.4. (Optional) Properties of the Canonical Affine Flow For the canonical path, repeatedly applying Rectify (Reflow ) further straightens trajectories without increasing transport cost, making training still easier. 5.4.2 Reflow: Iteratively Straightening Flows Why Reflow? Independent pairings often induce irregular and meandering ODE trajectories between psrc and ptgt, which increase discretization error and variance during simulation. This raises natural question: Question 5.4.1 Can we learn couplings that induce transport paths that are closer to straight lines between the two distributions, while still preserving the correct marginals? This motivates Reflow : repeatedly apply Rectify to update the coupling so that successive flows become easier to integrate. Core Idea: Recursive Straightening via Rectify. Start from the canonical interpolation on the product coupling π(0) := psrc(x0)ptgt(x1), xt = tx0 + (1 t)x1. Applying Rectify replaces the independent pairing with dependent one (z0, ˆz1), which empirically induces lower-curvature trajectories under the learned field. Iterating this update progressively reduces path curvature (never forcing literal straight lines), improving numerical stability and alignment. The Reflow Procedure. Each iteration performs two steps: Re-Fit Flow: Train new velocity field from samples of the current coupling: ϕ (cid:16) ϕk+1 = arg min (cid:12)π(k)(cid:17) with zt = tz(k) 0 ,ˆz(k) 0 + (1 t)ˆz(k) 1 . := t,(z(k) ϕ (cid:12) (cid:12) (cid:16) ϕ (cid:12) (cid:12) , where (cid:12)π(k)(cid:17) (cid:20)(cid:13) (cid:13)vϕ(zt, t) (ˆz(k) (cid:13) 1 )π(k) 2(cid:21) (cid:13) 1 z(k) (cid:13) 0 ) (cid:13) (5.4.1) Generate New Coupling: Solve the learned ODE starting from new source samples z(k+1) 0 psrc: 1 z(k+1) ˆz(k+1) 0 + 1 0 vϕk+1(z(t), t)dt, Flow-Based Perspective: From NFs to Flow Matching and define the updated coupling: π(k+1)(z0, z1) := psrc(z0)δ (cid:16) z1 ˆz(k+1) 1 (cid:17) . In other words, Reflow can be viewed as repeatedly applying the Rectify operator, producing sequence of progressively refined couplings: π(k+1) = Rectify (cid:16) π(k)(cid:17) (5.4.2) so that both the flow and the coupling evolve together, yielding progressively more stable transport paths. 5.4.3 Properties of Reflow Two key theoretical properties drive the usefulness of Reflow: it reduces transport cost and it straightens the trajectories. I. Reflow Never Increases Transport Cost. Let c(y) be convex cost function (e.g., yp 2 with 1). Each Rectify step forms new coupling (z0, ˆz1) whose cost is no worse than the original: Proposition 5.4.1: Rectify May Reduce Transport Costs Assuming an ideal velocity field = vϕ, we have: [c (ˆz1 z0)] [c (x1 x0)] . Proof for Proposition. Follows from Jensens inequality. See Liu, Gong, et al. (2022) for full derivation. Applying this result recursively shows that the Reflow process does not increase the transport cost. II. Reflow Straightens the Path. The longer we iterate Reflow, the straighter the learned trajectories may become. To measure this, define the straightness functional of path = {yt}t[0,1] as S(Y) := 1 0 \"(cid:13) (cid:13) y1 y0 (cid:13) (cid:13) # dyt dt (cid:13) 2 (cid:13) (cid:13) (cid:13) 2 dt. If S(Y) = 0, then is exactly straight line. 5.4. (Optional) Properties of the Canonical Affine Flow 163 (a) 1-rectified flow (b) 2-rectified flow (c) 3-rectified flow Figure 5.7: Illustration of Reflow from Liu, Gong, et al. (2022). Paths become progressively straighter with Rectify procedure. Proposition 5.4.2: Reflow Straightens the Stochastic Path For rectified paths Z(k), we have: min k{0,...,K} S(Z(k)) (cid:2)x1 x02(cid:3) . Proof for Proposition. See Theorem 3.7 of Liu (2022). The FM or RF formulation with linear interpolation kernels, together with the Reflow procedure, provides simpler training objective and practical method for refining stochastic couplings. For theoretical details, we refer readers to (Liu, Gong, et al., 2022; Liu, 2022). III. Connection to Optimal Transport. Lastly, we note that straight-line couplings are not necessarily optimal in the sense of optimal transport (OT). This involves some terminology that will be introduced in Section 7.2; we therefore refer readers who are not familiar with OT to that section. hallmark of quadratic-cost optimal transport is that particles travel along straight lines: particle at x0 moves to T(x0) via xt = (1t)x0 +tT(x0), where is the optimal transport map. However, not every map generating such straight-line paths, i.e., xt = (1 t)x0 + tS(x0), is optimal. The map yields the optimal flow only if it minimizes the Monge cost E[x0 S(x0)2]. Thus, while straight-line paths are necessary, they are not sufficient; optimality also depends on the correct endpoint map T. Example: Straight Couplings Need Not Be Optimal 164 Flow-Based Perspective: From NFs to Flow Matching Let psrc = ptgt = (0, I). For the cost c(x, y) = yp with > 0, the c-optimal coupling is the identity coupling π, where π is the law of (x, x) with psrc. Now consider the coupling πA defined as the law of (x, Ax), where psrc and is rotation matrix satisfying AA = I, det(A) = 1, = I, and 1 is not an eigenvalue. Then πA is valid coupling of psrc and ptgt, and corresponds to straight-line paths between and Ax, but it is not c-optimal for any twice-differentiable strictly convex cost with invertible Hessian. The suboptimality arises from the rotational transformation. As discussed in Equation (7.5.2), even removing the rotation may not lead to an optimal coupling. We will continue exploring the connection to OT in Section 7.5.2. 5.5. Closing Remarks 165 5.5 Closing Remarks This chapter has illuminated the third and final foundational perspective on diffusion models, one rooted in the principles of deterministic flows. Our exploration began with Normalizing Flows (NFs), which leverage the change-ofvariables formula to learn an exact, invertible mapping between simple prior and the data distribution. We then saw this concept evolve into continuoustime process with Neural ODEs, where learned velocity field governs the transformation. However, this approach comes with the significant drawback of requiring costly ODE simulations within the training loop. The modern framework of Flow Matching (FM) was presented as an elegant and efficient solution to this challenge. By pre-defining probability path {pt}t and corresponding velocity field that satisfies the continuity equation, FM establishes clear target for the ODE flow. Crucially, just as we saw in the variational and score-based views, FM employs powerful conditioning trick. This transforms the intractable problem of matching the marginal velocity field into simple and tractable regression against known conditional velocity, making training entirely simulation-free. This perspective recasts diffusion models themselves as special case of learning deterministic flow to transport Gaussian prior to the data distribution. With the introduction of the flow-based view, our survey of the three conceptual pillars of diffusion modeling is now complete. Throughout this journey, remarkable pattern has emerged: each framework, despite its unique origins in VAEs, EBMs, or NFs, has converged on continuous-time generative process and has relied on conditioning strategy to enable tractable learning. In the next chapter, we will finally synthesize these parallel threads into single, unified framework. We will: 1. Formally demonstrate that the variational, score-based, and flow-based perspectives are not merely analogous but are mathematically equivalent at fundamental level. 2. Show how the Fokker-Planck equation serves as the universal law governing density evolution across all three views, revealing that they are simply different lenses for describing the same core generative principle. This unified lens will provide complete and systematic understanding of the modern diffusion paradigm."
        },
        {
            "title": "A Unified and Systematic Lens on Diffusion Models",
            "content": "Mathematics is the art of giving the same name to different things. Henri Poincaré This chapter presents systematic viewpoint that connects the variational, score based, and flow based perspectives within coherent picture. While motivated by different intuitions, these approaches converge on the same core mechanism underlying modern diffusion methods. Building on Chapters 2 to 5, we observe common recipe: define forward corruption process that traces path of marginals, then learn time varying vector field that transports simple prior to the data distribution along this path. key ingredient across all perspectives is the conditioning trick introduced in Section 6.1, which transforms an intractable marginal objective into tractable conditional one, leading to stable and efficient training. In Section 6.2 we analyze the training objective in systematic way, identifying its essential components and clarifying how loss functions are formulated in the variational, score-based, and flow-based viewpoints. Section 6.3 shows that any affine forward noise injection of the form xt = αtx0 + σtϵ can be equivalently transformed into the standard linear schedule xt = (1t)x0+tϵ. Moreover, common parameterizations such as noise prediction, clean data prediction, score prediction, and velocity prediction are interchangeable at the level of gradients. Thus, the choices of noise schedulers and parameterizations both adhere to the same modeling principle. 166 167 Finally, Section 6.4 brings the discussion together and identifies the governing rule: the FokkerPlanck equation. Whether viewed as variational scheme (discrete time denoising), score-based method (SDE formulation), or flow-based method (ODE formulation), each constructs generator whose marginals follow the same density evolution. The FokkerPlanck equation thus serves as the universal constraint respected by all three viewpoints, with differences arising only in parameterization and training objectives. 168 Unified and Systematic Lens on Diffusion Models 6.1 Conditional Tricks: The Secret Sauce of Diffusion Models Until now, we have explored diffusion models from three seemingly distinct origins: variational, score-based, and flow based perspectives. Each was originally motivated by different goals and led to its own training objectives (with fixed t): Variational View: Learn parametrized density pϕ(xttxt) to approximate the oracle reverse transition p(xttxt) by minimizing: JKL(ϕ) := Ept(xt) (cid:2)DKL (cid:0)p(xttxt)pϕ(xttxt)(cid:1)(cid:3) ; Score-Based View: Learn score model sϕ(xt, t) to approximate the marginal score log pt(xt) via: JSM(ϕ) := Ept(xt) sϕ(xt, t) log pt(xt)2 2 ; Flow-Based View: Learn velocity model vϕ(xt, t) to match the oracle velocity vt(xt) (e.g., defined by Equation (5.2.10)) by minimizing: JFM(ϕ) := Ept(xt) vϕ(xt, t) vt(xt)2 2 . At first glance, these objectives seem hopelessly intractable, since they all require access to oracle quantities that are fundamentally unknowable in general. But here comes the exciting twist: each method independently arrives at the same elegant solution to this problem: conditioning on the data x0. This technique transforms each intractable training target into tractable one. This elegant conditioning technique rewrites the objectives as expectations over the known Gaussian conditionals pt(xtx0), yielding gradientequivalent closed-form regression targets and tractable training objectives: Variational View (Equation (2.2.3)): JKL(ϕ) = Ex0 Ept(xtx0) (cid:2)DKL (cid:0)p(xttxt, x0)pϕ(xttxt)(cid:1)(cid:3) } {z JCKL(ϕ) +C; Score-Based View (Equation (3.3.3)): JSM(ϕ) = Ex Ept(xtx0) sϕ(xt, t) xt log pt(xtx0)2 2 {z JDSM(ϕ) +C; } 6.1. Conditional Tricks: The Secret Sauce of Diffusion Models 169 Flow-Based View (Equation (5.2.9)): JFM(ϕ) = Ex Ept(xtx0) vϕ(xt, t) vt(xtx0)2i } {z JCFM(ϕ) +C. To build unified view, we next revisit the conditional KL, score, and velocity objectives in systematic manner. Crucially, these objectives are not only tractable but also equivalent to their original forms up to constant vertical shift. The conditional versions (JCKL, JDSM, JCFM) differ from the originals (JKL, JSM, JFM) only by this shift, which leaves the gradients unchanged and thus preserves the optimization landscape. As result, the minimizers remain uniquely identified with the true oracle targets, since each reduces to least-squares regression problem whose solution recovers the corresponding conditional expectation: p(xttxt) = Ex0p(xt) s(xt, t) = Ex0p(xt) v(xt, t) = Ex0p(xt) (cid:2)p(xttxt, x0)(cid:3) (cid:2)xt log pt(xtx0)(cid:3) = xt log pt(xt), (cid:2)vt(xtx0)(cid:3) = p(xttxt), = vt(xt). (6.1.1) This is no coincidence: by making training tractable, these conditional forms reveal profound unification. Variational diffusion, score-based SDEs, and flow matching are simply different facets of the same principle. Three perspectives, one insight, elegantly connected. We will continue to explore their equivalence throughout the rest of this chapter. 170 Unified and Systematic Lens on Diffusion Models 6.2 Roadmap for Elucidating Training Losses in Diffusion Models This section builds systematic view of training losses in diffusion models. In Section 6.2.1, we extend the standard three objectives to broader set of four parameterizations, showing how they arise from different modeling perspectives. In Section 6.2.2, we then distill these results into general framework that disentangles the structure of diffusion objectives, laying the groundwork for the equivalence results in Section 6.3. 6.2.1 Four Common Parameterizations in Diffusion Models Throughout this section, we consider the forward perturbation kernel pt(xtx0) = (cid:16) xt; αtx0, σ2 (cid:17) , where x0 pdata, as defined in Equation (4.4.1), unless stated otherwise. Let ω : [0, ] R>0 denote positive time-weighting function. The four standard parameterizations (noise ϵϕ, clean xϕ, score sϕ, and velocity vϕ), together with their respective minimizers ϵ, x, s, and v, are summarized below for clarity and to facilitate further discussion. Variational View. Based on the KL divergence in DDPMs (see Sections 2.2.4 and 4.4.3), this approach reduces to predicting either the expected noise that produces xt or the expected clean signal that xt was perturbed from. 1. ϵ-Prediction (Noise Prediction) (Ho et al., 2020): ϵϕ(xt, t) E[ϵxt] = ϵ(xt, t) (6.2.1) with training objective Lnoise(ϕ) := Et ω(t)Ex0,ϵ ϵϕ(xt, t) ϵ2 2 . Here, ϵ means the average noise that was injected to obtain the given xt. 2. x-Prediction (Clean Prediction) (Kingma et al., 2021; Karras et al., 2022; Song et al., 2023): xϕ(xt, t) E[x0xt] = x(xt, t) (6.2.2) with training objective Lclean(ϕ) := Et ω(t)Ex0,ϵ xϕ(xt, t) x02 2 . Here, means the average of all plausible clean guesses, given the noisy observation xt. 6.2. Roadmap for Elucidating Training Losses in Diffusion Models 171 Score-Based View. Predicts the score function at noise level t, which points in the average direction to denoise xt back toward all possible clean samples that could have generated it: 3. Score Prediction (Song and Ermon, 2019; Song et al., 2020c): sϕ(xt, t) xt log pt(xt) = [xt log pt(xtx0)xt] = s(xt, t) (6.2.3) with training objective ω(t)Ex0,ϵ sϕ(xt, t) xt log pt(xtx0)2 2 Lscore(ϕ) := Et , where the conditional score satisfies xt log pt(xtx0) = 1 σt ϵ. Flow-Based View. Predicts the instantaneous average velocity of the data as it evolves through xt: 4. v-Prediction (Velocity Prediction) (Lipman et al., 2022; Liu, 2022; Salimans and Ho, 2021; Albergo et al., 2023): vϕ(xt, t) (cid:20) dxt dt (cid:12) (cid:12) (cid:12) (cid:12) (cid:21) xt = v(xt, t) (6.2.4) with training objective Lvelocity(ϕ) := Et ω(t)Ex0,ϵ vϕ(xt, t) vt(xtx0, ϵ)2 2 , where the conditional velocity is vt(xtx0, ϵ) = α Here, indicates the average velocity vector passing through the observation point xt. tx0 + σ tϵ. Building on the insight from Equation (6.1.1), all four prediction types ultimately aim to approximate conditional expectation in the form of the average noise, clean data, score, or velocity given an observed xt. 6.2.2 Disentangling the Training Objective of Diffusion Models As shown in Section 6.2.1, the objective functions for the four prediction types commonly share the following template form for diffusion model training: L(ϕ) := Ex0,ϵ Eptime(t) } {z time distribution ω(t) {z } time weighting (cid:13) (cid:13) (cid:13) NNϕ ( xt , t) (Atx0 + Btϵ) {z MSE part (cid:13) 2 (cid:13) (cid:13) 2 } . (6.2.5) 172 Unified and Systematic Lens on Diffusion Models Here, to enhance training efficiency and optimize the diffusion model learning pipeline, several key design choices are crucial (Karras et al., 2022; Lu and Song, 2024): (A) Noise schedule in the forward process of xt via αt and σt; (B) Prediction types of NNϕ and their associated regression targets (Atx0 + Btϵ) ; (C) Time-weighting function ω() : [0, ] R0; (D) Time distribution ptime . We elaborate on these four components here to serve as roadmap for the discussions in the following sections. (A) Noise Schedule αt and σt. Users have the flexibility to choose schedules tailored to their applications, with common examples summarized in Table 5.2. Importantly, as we will demonstrate in Equations (6.3.3) and (6.3.5), all affine flows of the form xt = αtx0 + σtϵ are mathematically equivalent. Specifically, any such interpolation can be converted to the canonical linear schedule (αt = 1 t, σt = t) or to trigonometric schedule (αt = cos t, σt = sin t) by appropriate time reparametrization and spatial rescaling. (B) Parameterization NNϕ and Training Target Atx0 + Btϵ. Users can flexibly choose the models prediction target: the clean signal, noise, score, or velocity prediction. As detailed in Section 6.2.1, all these prediction types share common regression target of the form Regression Target = Atx0 + Btϵ, where the coefficients At and Bt depend on both the chosen prediction type and the schedule (αt, σt). These relationships are summarized in Table 6.1. Although these four parameterizations appear distinct, we will demonstrate in Equation (6.3.1) that they can be transformed into one another through simple algebraic manipulations. Furthermore, we will also show in Equation (6.3.6) that the squared-ℓ2 loss term in Equation (6.2.5) remains gradient-equivalent across all prediction types, differing only by time-weighting factor (beyond ω(t)ptime(t)) that depends solely on the noise schedule (αt, σt). 6.2. Roadmap for Elucidating Training Losses in Diffusion Models 173 Table 6.1: Summary of the Relationships Between Different Parameterizations. All four parameterizations are mathematically equivalent and can be converted into one another through straightforward algebraic transformations. Regression Target = Atx0 + Btϵ At Clean Noise Conditional Score 1 0 0 Conditional Velocity α Bt 0 1 - 1 σt σ (C) Time Distribution ptime(t). Since the training loss is an expectation over t, sampling times from ptime(t) is mathematically equivalent to weighting the per-t MSE by ptime(t); this factor can be absorbed into the existing time weighting ω(t)1. However, empirical evidence2 indicates that different choices of ptime(t) can affect performance. Therefore, we discuss the time distribution ptime(t) and the time weighting function ω(t) separately. common choice for the time distribution is the uniform distribution over [0, ] (Ho et al., 2020; Song et al., 2020c; Lipman et al., 2022; Liu, 2022). Alternative options include the log-normal distribution (Karras et al., 2022) and adaptive importance sampling methods (Song et al., 2021; Kingma et al., 2021). (D) Time-Weighting Function ω(t). common choice for the weighting function is the constant weighting ω 1 (Ho et al., 2020; Karras et al., 2022; Lipman et al., 2022; Liu, 2022), although adaptive weighting schemes have also been proposed (Karras et al., 2023). Certain choices of ω(t) transform 1Our target population objective over time is an integral of the form = ω(t)mse(t) dt, 0 where mse(t) denotes the per-t MSE-like term. If we draw ptime(t) during training, an unbiased Monte Carlo estimator of is obtained by bL = Etptime ω(t) ptime(t) mse(t) , i.e., sampling and weighting are interchangeable via importance weighting. 2In practice, though, we approximate the training objective using minibatch SGD on discrete set of times. Under this approximation, different choices of ptime(t) change both the variance of the gradients and the effective weight placed on each time step. For this reason we discuss ptime(t) (sampling) and ω(t) (weighting) separately. Unified and Systematic Lens on Diffusion Models Equation (6.2.5) into tighter upper bound on the negative log-likelihood, effectively reformulating the objective as maximum likelihood training. Notable weighting schemes for ω(t) include setting ω(t) = g2(t) (Song et al., 2021), where is the diffusion coefficient from the forward SDE in Equation (4.1.3). Other approaches use signal-to-noise ratio (SNR) weighting (Kingma et al., 2021) or monotonic weighting functions (Kingma and Gao, 2023), where ω(t) is monotone function of time. Overall, regardless of the choice of noise scheduler, prediction type, or time sampling distribution, these factors theoretically converge to influencing the time-weighting in the objective functions. This time-weighting can impact the practical training landscape and, consequently, the models performance. 6.3. Equivalence in Diffusion Models 175 6.3 Equivalence in Diffusion Models The four prediction types introduced in Section 6.2.1 will later be shown (Section 6.3.1) to be equivalent under gradient minimization. We then broaden this view in Section 6.3.3, showing that different forward noise schedules are connected by simple time and space rescalings. 6.3.1 Four Prediction Types Are Equivalent We begin by analyzing the design choices for component (B) in Equation (6.2.5). We have seen that the four prediction types are not independent choices but different views of the same underlying quantity. For example, noise and clean predictions are directly related (Section 2.2.4), as are score and noise predictions (Section 3.4). This recurring pattern points to deeper principle: all four parameterizations are algebraically equivalent and can be converted into one another through simple transformations. To make this connection precise, we state the following proposition, illustrated in Figure 6.1, following (Kingma et al., 2021). Proposition 6.3.1: Equivalence of Parametrizations Let the optimal predictions minimizing their respective objectives be ϵ(xt, t), x(xt, t), s(xt, t), v(xt, t), corresponding to noise, clean, score, and velocity parameterizations. These satisfy the following equivalences: x(xt, t) = ϵ(xt, t) = σts(xt, t), σ2 1 αt αt tϵ = (t)xt tx + σ v(xt, t) = α s(xt, t), xt + (6.3.1) 1 2 g2(t)s(xt, t). Here, (t) and g(t) are related to αt and σt via Lemma 4.4.1. Moreover, these minimizers satisfy the identities given in Equations (6.2.1) to (6.2.4). Proof for Proposition. The proof is similar to that of Theorem 4.2.1, which analyzes the global optimum of various matching losses under the DSM objective. See Section D. 176 Unified and Systematic Lens on Diffusion Models for details. ϵ = σts Score Noise = 1 αt xt + σ2 αt v = (t)xt 1 2 g2(t)s = αtx + σtϵ() Clean () Velocity Figure 6.1: Equivalent relations among four parameterizations. v-prediction is given by = αtx + σtϵ, where clean and ϵ-predictions are interchangeable via xt = αtx + σtϵ. Equation (6.3.1) induces one-to-one conversion (at each t, given the forward noising coefficients) between the four parameterizations ϵϕ(xt, t), xϕ(xt, t), sϕ(xt, t), vϕ(xt, t). In practice, we train single network in one parameterization (e.g., ϵϕ). The other quantities are then defined post hoc by the conversions in Equation (6.3.1). 6.3.2 PF-ODE in Different Parameterizations The PF-ODE admits several equivalent parameterizations (score, noise, denoised, and velocity). Although interchangeable in principle, the choice has practical consequences: it changes the stiffness of the vector field, the behavior of discretization error, and the ease of optimization. For fast sampling with advanced ODE solvers (see Chapter 9), practitioners often work with ϵ or prediction because they align well with solver inputs and reduce error accumulation. For training generators that use only few function evaluations (see Chapter 11), or prediction often yields smoother objectives and better step to step consistency. We write the PF-ODE under each parameterization and make the conversions explicit using Equation (6.3.1). The results are collected in the following proposition. 6.3. Equivalence in Diffusion Models 177 Proposition 6.3.2: PF-ODE in Different Parameterizations Let αt and σt be the forward perturbation schedules, and denote time := dσt derivatives by α dt . Then the empirical PF-ODE admits the equivalent forms dt and σ := dαt dx(t) dt = = x(t) σt x(t) + αt (cid:18) α α αt αt (cid:18) α σ αt σt (cid:18) α α αt αt tx(x(t), t) + σ = α = v(x(t), t). x(t) + σ2 = (cid:19) (cid:19) σ σt σ σt σ σt tϵ(x(t), t) (cid:19) ϵ(x(t), t) x(x(t), t) s(x(t), t) (6.3.2) To see the Score SDE notation, we recall Lemma 4.4.1. If we set (t) = α αt , g2(t) = dt (cid:0)σ2 (cid:1) 2 α αt = 2σtσ σ 2 α αt σ2 , then the PF-ODE can be written in the familiar Score SDE form: dx(t) dt = (t)x(t) g2(t)s(x(t), t). 1 2 To give concrete sense of how the PF-ODE is discretized for sampling, we will present in Section 9.2 the update rule of widely used diffusion-based ODE sampler, the DDIM scheme. This example will show how an Euler discretization naturally connects with the PF-ODE. 6.3.3 All Affine Flows Are Equivalent We next analyze the design choices for component (A) in Equation (6.2.5). State-Level Equivalence. convenient canonical interpolation used in FM (Lipman et al., 2022) and RF (Liu, 2022) is xFM = (1 t)x0 + tϵ = x0 + t(ϵ x0), whose velocity is the constant vector ϵ x0. The key point of this subsection is that the apparent simplicity of this choice is not essential: any affine interpolation xt = αtx0 + σtϵ 178 Unified and Systematic Lens on Diffusion Models can be written as timereparameterized and rescaled version of the canonical path. Define c(t) := αt + σt, τ (t) := σt αt + σt (cid:0)c(t) = 0(cid:1). direct algebraic rewrite yields xt = αtx0 + σtϵ = (cid:0)αt + σt (cid:1) (cid:18) αt αt + σt x0 + σt αt + σt (cid:19) ϵ = c(t) (cid:0)(cid:0)1 τ (t)(cid:1)x0 + τ (t)ϵ(cid:1) = c(t)xFM τ (t). Hence every affine path is the image of the canonical FM path under the change of variables 7 τ (t) and the spatial rescaling 7 c(t)x. The equality holds pointwise and therefore also in distribution. For the associated velocities, apply the chain rule to xt = c(t)xFM τ (t): v(xt, t) := = dt dt (αtx0 + σtϵ) (cid:0)c(t)xFM τ (t) (cid:1) = c(t)xFM = c(t)xFM ds xFM τ (t) + c(t)τ (t) τ (t) + c(t)τ (t)vFM (cid:16) (cid:12) (cid:12) (cid:12) (cid:12)s=τ (t) (cid:17) xFM τ (t), τ (t) , since vFM(xFM τ , τ ) = x0 + ϵ along the canonical path. We summarize the above derivation as formal statement in the following proposition. Proposition 6.3.3: Equivalence of Affine Flows = (1 t)x0 + tϵ and xt = αtx0 + σtϵ with c(t) := αt + σt = 0 Let xFM and τ (t) := σt/(αt + σt). Then xt = c(t)xFM τ (t), τ (t) + c(t)τ (t)vFM (cid:16) v(xt, t) = c(t)xFM (cid:17) xFM τ (t), τ (t) . (6.3.3) In particular, all affine interpolations are equivalent up to time reparameterization and spatial rescaling. 6.3. Equivalence in Diffusion Models 179 Equivalence with Trigonometric Flow. Another widely used affine flow is the trigonometric interpolation (Salimans and Ho, 2021; Albergo et al., 2023; Lu and Song, 2024). As concrete example, we also show that any affine flow can be expressed in this form. The trigonometric path is defined by xTrig := cos(u)x0 + sin(u)ϵ. (6.3.4) Let Rt := + σ2 α2 and assume Rt > 0. Choose an angle τt so that cos τt = αt Rt , sin τt = σt Rt . Then every affine interpolation xt = αtx0 + σtϵ is rescaled and re timed trigonometric path: (cid:19) xt = αtx0 + σtϵ = Rt (cid:18) αt Rt The pair (αt, σt) is point in the plane. Normalizing by Rt places it on the unit circle, which fixes the angle τt and hence the state xTrig ; the radius Rt gives the overall scale. Differentiating xTrig with respect to gives its velocity, = RtxTrig σt Rt x0 + (6.3.5) ϵ τt τt . vTrig = sin(u)x0 + cos(u)ϵ. Through the same change of variables as in equation 6.3.5, this relation provides closed-form conversions for the velocity (and analogously for other parameterizations). Summarizing the above discussion, we arrive at the following conclusion: Conclusion 6.3.1: Regardless of the schedule (αt, σt), including VE, VP (such as trigonometric), FM, or RF, affine interpolations are mutually convertible by suitable change of time variable and scalar rescaling. Training Objectives of Four Parameterizations. Let xt = αtx0 + σtϵ with σt > 0 and differentiable (αt, σt) such that α = 0. Consider the oracle targets tσt αtσ ϵ(xt, t) = E[ϵxt], 0(xt, t) = E[x0xt], v(xt, t) = E[α tx0 + σ tϵxt]. From Proposition 6.3.1, they satisfy xt log pt(xt) = 1 σt ϵ(xt, t) = (cid:18) αt σ2 x 0(xt, t) (cid:19) xt αt , = α tx 0 + σ tϵ. 180 Unified and Systematic Lens on Diffusion Models Under the head conversions sϕ 1 σt ϵϕ (cid:18) αt σ2 xϕ (cid:19) , xt αt and the velocity-to-score conversion is sϕ = αt σt(α tσt αtσ t) vϕ α tσt αtσ t) σt(α xt, the persample squared losses match up to timedependent weights: (cid:13)sϕ xt log pt(xt)(cid:13) (cid:13) 2 2 = (cid:13) = = 1 σ2 α2 σ4 (cid:18) (cid:13)ϵϕ ϵ(cid:13) (cid:13) 2 (cid:13) 2 (cid:13) (cid:13)xϕ (cid:13) 2 (cid:13) 2 αt (cid:19)2 σt(α tσt αtσ t) (6.3.6) (cid:13)vϕ v(cid:13) (cid:13) 2 2. (cid:13) By Proposition 6.3.3, any affine flow xt = αtx0 + σtϵ is transferable to the τ (t) with c(t) = αt+σt and τ (t) = σt/(αt+σt). canonical FM path via xt = c(t)xFM Differentiating gives vϕ(xt, t) = c(t)xFM τ (t) + c(t)τ (t)vFM ϕ (cid:16) (cid:17) xFM τ (t), τ (t) , xFM τ (t) = xt c(t) , and the same relation holds for v. Hence the velocity loss transforms by (cid:13)vϕ(xt, t) v(xt, t)(cid:13) (cid:13) 2 (cid:13) 2 (cid:18) xt =(cid:0)c(t)τ (t)(cid:1)2(cid:13) (cid:13)vFM (cid:13) c(t) ϕ , τ (t) (cid:19) (cid:0)vFM(cid:1) (cid:18) xt c(t) , τ (t) (cid:19) (cid:13) 2 (cid:13) (cid:13) 2 . With the above observation, we arrive at the following conclusion: Conclusion 6.3.2: Score, noise, clean, and velocity training objectives are theoretically equivalent up to timedependent weights (and, for velocity, an affine head conversion involving xt) determined by (αt, σt). 6.3. (Optional) Conceptual Analysis of Parametrizations and the Canonical Flow Even though we have shown in the previous sections that all four parameterizations are mathematically equivalent and can be transformed into one 6.3. Equivalence in Diffusion Models 181 another, and that the forward affine noise-injection flow is equivalent to the canonical form xFM = (1 t)x0 + tϵ, in this subsection we provide further intuition and analyze the potential advantages of using the v-prediction parameterization together with this canonical affine flow. This subsection asks simple question: how do different parameterizations and schedules shape what the model learns and how we sample? We proceed in three steps: Regression Targets and Schedules. We focus on why combining vprediction with the canonical linear schedule (αt, σt) = (1 t, t) is natural: it maintains stable target scale over time and eliminates curvature effects in the dynamics. Solver Implications. We examine how this parameterization conceptually interacts with numerical integration schemes while deferring concrete examples such as the Euler solver and Heuns method to Sections 9.2.2 and 9.4.5. Before proceeding, we distinguish between two types of velocity fields to avoid ambiguity. The conditional velocity, which serves as tractable training target, is defined as vt(xtz) = t = α tx0 + σ tϵ, where = (x0, ϵ), while the oracle (marginalized) velocity, used to move samples during inference of PF-ODE solving, is given by v(x, t) = E(cid:2)vt(z)(cid:12) (cid:12)xt = x(cid:3). Perspective 1: Why (αt, σt) = (1 t, t) is Natural Schedule. Writing σt := ρ(t) and αt := 1 ρ(t) for time-varying ρ(t), the conditional velocity becomes vt(xtz) = ρ(t)(ϵ x0), where = (x0, ϵ). Unit-Scale Regression Targets. For the canonical schedule ρ(t) = t, the conditional velocity vt(z) satisfies vt(z)2 = Eϵ ϵ2 2 + Ex0 x02 2 = + Tr Cov[x0] } {z total variance + Ex02 2 {z } mean . (6.3.7) 182 Unified and Systematic Lens on Diffusion Models Thus the expected target magnitude is constant in t. After standardizing the data to zero mean and identity covariance (i.e., Cov[x0] = I), the two components α tϵ contribute comparably for all t, avoiding gradient explosion/vanishing near the endpoints. To see this, we consider the diffusions training objective: tx0 and σ Lvelocity(ϕ) = EtEx0,ϵ h(cid:13) (cid:13)vϕ(xt, t) vt(xtz)(cid:13) 2 (cid:13) 2 . By applying the chain rule, the gradient of this loss with respect to the model parameters ϕ is ϕLvelocity(ϕ) = 2EtEx0,ϵ ϕvϕ(xt, t) (vϕ(xt, t) vt(xtz)) . Thus the scale of the target vt(xtz)2 influences gradient stability: if it collapses to 0 (or blows up) at some t, gradients tend to vanish (or explode), all else equal. With the canonical choice ρ(t) = t, Equation (6.3.7) gives tindependent target magnitude, so there is no endpoint (t = 0 or = 1) collapse or blow-up arising from the regression signal (assuming Eϕvϕ(xt, t)2 and any time-weights are controlled). Interplay of the Canonical Schedule and v-Prediction. Under the affine path xt = αtx0 + σtϵ, the oracle velocity decomposes as v(x, t) = α tx(x, t) + σ tϵ(x, t), with = E[x0xt = x] and ϵ = E[ϵxt = x]. Differentiating at fixed gives tv = α ϵ + σ } {z schedule curvature +α ttx + σ ttϵ. = σ = 0), so the time-variation of With the linear schedule αt = 1 t, σt = t, the curvature terms vanish (α primarily reflects the posterior evolution (tx, tϵ) rather than the schedule. This effect is especially clean for v-prediction: the coefficients α are constants (1 and +1), avoiding extra tdependent rescaling in the drift. By contrast, score-, x0, or ϵ-parameterizations often introduce ratios such as σ t/αt that can vary sharply near the endpoints, even under linear schedule. Hence, while not exclusive in principle, the linear (1 t, t) schedule combined with v-prediction offers particularly stable and transparent time dependence for the oracle velocity. t/σt or α t, σ Minimizing the Conditional Energy. We next adopt more theoretical perspective of optimal transport (see Chapter 7). Here, the conditional kinetic 6.3. Equivalence in Diffusion Models 183 energy quantifies the total expected motion of the conditional velocity along the forward path, that is, the amount of instantaneous movement (or kinetic effort) required to traverse from x0 to ϵ: K[ρ] := 1 0 Ex0,ϵ (cid:2)vt(z)2 2 (cid:3) dt = (cid:16) + Tr Cov[x0] + Ex02 2 (cid:17) 1 0 (cid:0)ρ(t)(cid:1)2 dt. Minimizing K[ρ] therefore corresponds to finding the smoothest, leastenergy path in expectation. With the boundary conditions ρ(0) = 0 and ρ(1) = 1, the EulerLagrange equation ρ(t) = 0 gives the minimizer ρ(t) = t, corresponding to straight conditional path. This means that, among all smooth interpolations connecting x0 and ϵ, the canonical flow ρ(t) = is the most energy-efficient way to move between them. We will revisit this point in Proposition 7.5.1 for more detailed treatment. Remark on the Oracle Velocity. If instead we evaluate the energy defined by marginal velocities 1 0 Ext (cid:2)v(xt, t)2(cid:3) dt, then with = (x0, ϵ) and vt(xtz) = ρ(t)(ϵ x0), v(x, t) = E[vt(z)xt = x] = ρ(t)E[ϵ x0xt = x]; and hence, the energy of the marginal velocity becomes 1 0 Extpt (cid:2)v(xt, t)2 2 1 (cid:3) dt = Ext (cid:2)ρ(t)E[ϵ x0xt]2 2 (cid:3) dt = 1 0 (cid:0)ρ(t)(cid:1)2κ(t) dt, where κ(t) := Extpt 0 h(cid:13) (cid:13)E[ϵ x0xt](cid:13) 2 (cid:13) 2 . Consequently, the marginal-optimal schedule ρ(t) need not be linear. It is linear iff κ(t) is constant; in general, the EulerLagrange condition (κ(t)ρ(t)) = 0 ρ(t) 1 κ(t) implies that the oracle-optimal schedule re-parameterizes time adaptively. Intuitively, κ(t) quantifies how much of the label (ϵ x0) is predictable from xt pt: the oracle flow slows down where κ(t) is large, reflecting regions where the oracle velocity has high expected magnitude, and speeds up where κ(t) is small. Hence, even though the conditional flow uses the linear schedule (1t, t), the corresponding marginalized (oracle) dynamics are generally nonlinear. Perspective 2: Why Velocity Prediction Can Be Considered Natural for Sampling. 184 Unified and Systematic Lens on Diffusion Models Semilinear Form of the PFODE under x-, ϵ-, and s-Predictions. Under the clean, noise, and score parameterizations, the drift takes semilinear form (see the first three identities in Equation (6.3.2)): dx(t) dt = L(t)x(t) } {z linear part + Nϕ(x(t), t) } {z nonlinear part , Nϕ {xϕ, ϵϕ, sϕ}. When the linear drift L(t)x(t) drives changes in x(t) at very different rates in some directions compared with the nonlinear part, the system is stiff, meaning that the Jacobian (in x) of the drift J(x, t) := L(t) + xNϕ(x, t) has eigenvalues whose real parts differ by orders of magnitude (a larger magnitude corresponds to faster direction)3. For instance, the dynamics may involve fast linear change alongside slow nonlinear one in x(t). In such cases, explicit solvers must take very small time steps to remain numerically stable. To address this imbalance, higher-order stable solvers often apply an integrating factor that treats the linear term L(t)x analytically and discretizes only the slower nonlinear remainder, albeit at the cost of additional algebraic and implementation complexity. Chapter 9 is dedicated to detailed discussion of this topic. PFODE under v-Prediction. With v-prediction, the model directly learns the velocity field and integrates dx(t) dt = vϕ(x(t), t) v(x(t), t). In this formulation, the explicit linear term is absorbed into single learned field, so the dynamics no longer split into separate parts. The step size is 3Let the PFODE drift be F(x, t) = L(t)x + Nϕ(x, t) and assume Nϕ is (locally) (t). For nearby states x, y, Lipschitz in with constant LipNϕ (x, t) (y, t) (cid:0)L(t) + LipNϕ {z =: C(t) y. (t)(cid:1) } Equivalently, the Jacobian (w.r.t. x) J(x, t) = L(t) + xNϕ(x, t) satisfies J(x, t)op C(t) ( i.e., the operator norm induced by the Euclidean norm on RD). Hence, the real parts of all eigenvalues of are bounded in magnitude by C(t). Thus large C(t) means fast local rates, so explicit solvers need small steps (h 1/C(t)). 6.3. Equivalence in Diffusion Models 185 thus governed by how smoothly the learned field vϕ(x, t) varies with and t, rather than by the magnitude of prescribed scalar coefficient L(t). In other words, the potentially rapid linear drift is folded into one coherent velocity field, reducing time-scale disparity and simplifying numerical integration. Later in Section 9.2.2, we will illustrate, with simple example, the structural simplicity of v-prediction in the sampling process. To obtain the same discretization update of the PF-ODE as DDIM (Song et al., 2020a) (one of the most widely used fast samplers in diffusion modeling), plain Euler step under the ϵ-, x-, or s-parameterizations only approximates the linear term rather than computing it exactly (see Equation (9.1.8)). Consequently, these parameterizations require more advanced approach, the exponential integrator, to isolate and compute the linear term exactly. In contrast, with v-prediction, there is no separate linear term to isolate in the PF-ODE drift, so the plain Euler update naturally coincides with the DDIM formulation. closely related analogy appears in Section 9.4.5: the second-order DPMSolver (Lu et al., 2022b) coincides with the classical Heun method: for vprediction this is plain Heun, whereas for ϵ-, x-, or s-prediction it is the exponential Heun. We leave the detailed discussion to their respective sections. We remark that any improvements in generation (such as achieving higher sample quality with fewer model evaluations in PF-ODE solving) depend on both how accurately vϕ approximates the oracle velocity and how effectively the sampling algorithm (including the numerical integrator, discretization schedule, and step-size control) interacts with it. Thus, adopting the v-parameterization does not by itself guarantee better sampling performance. Conclusion. While v-prediction combined with the canonical linear schedule offers certain theoretical advantages, such as constant target magnitude and the absence of schedule curvature, these properties do not necessarily make it universally superior. In practice, model performance depends on range of interacting factors, including network architecture, normalization schemes, loss weighting over time, the choice of sampler and discretization steps, guidance strength, regularization strategy, data scaling, and overall training budget. Different datasets and objectives may favor alternative parameterizations or schedules, and the optimal configuration is ultimately an empirical question that should be resolved through validation and ablation studies. 186 Unified and Systematic Lens on Diffusion Models 6.4 Beneath It All: The FokkerPlanck Equation Figure 6.2: Unified perspective connecting variational, SDE, and ODE formulations through the continuity equation, where all pt(x) evolve under shared dynamic. The velocity field v(x, t) = (t)x 1 2 g2(t)s(x, t) is governed by the score function s(x, t) := log pt(x). Coefficients (t), g(t), σt, and αt are pre-defined time-dependent functions, and γ(t) is tunable time-varying hyperparameter."
        },
        {
            "title": "Forward Process",
            "content": "Variational Approach: Defining p(xtxtt), or pt(xtx0) xt = αtx0 + σtϵ Lemma 4.4.1 Forward SDE: dx(t) = (t)x(t) dt + g(t) dw(t) Forward from pdata at = 0 Continuity Equation/Fokker-Planck Equation for pt(x)"
        },
        {
            "title": "Reverse Process",
            "content": "Variational Approach: p(xttxt) from Bayes rule 0 discretization dx = (cid:2)v 1 Reverse SDE: 2 γ2(t)s(cid:3) dt + γ(t) w(t) γ γ 0 PF-ODE: dx(t) = v(x(t), t) dt Reverse from pprior at = In this section, we show that the three main perspectives of diffusion modelsvariational, score-based, and flow-basedare not separate constructions but arise from single unifying principle: the continuity (FokkerPlanck) equation that governs density evolution under chosen forward process. First, we recall that the analysis in Section 4.5 unifies the variational perspective, based on discrete kernels and Bayes rule, with the score-based SDE perspective of continuous dynamics. We establish this connection by showing that variational models act as consistent discretizations of the underlying forward and reverse SDEs. Specifically, the marginal densities calculated 6.4. Beneath It All: The FokkerPlanck Equation 187 step-by-step via the discrete kernels evolve in manner consistent with the FokkerPlanck equation that governs the continuous-time dynamics. This confirms that the two perspectives are fundamentally equivalent. We then connect the flow-based and score-based views. In Section 6.4.1, we show that an ODE flow determines density path whose marginals can always be realized by family of stochastic processes. This places deterministic flows and stochastic SDEs within the same family. Together, these results unify the three perspectives under one framework (see Figure 6.2). At last, we conclude this chapter in Section 6.5. 6.4.1 Connection of Flow-Based Approach and Score SDE remarkable aspect of diffusion model lies in how different dynamic systems, deterministic or stochastic, can trace out the same evolution of probability distributions. In this section, we reveal natural and elegant connection between ODE-based flows of Section 5.2 and Score SDEs. Specifically, we show that the velocity field defining generative ODE can be transformed into stochastic counterpart that follows the same FokkerPlanck dynamics, providing principled bridge between deterministic interpolation and stochastic sampling. This offers us continuous family of models, ranging from ODEs to SDEs, that generate the same data distribution path. We consider the continuous-time setup where the perturbation kernel is given by pt(xtx0) = (xt; αtx0, σ2 I), where x0 pdata. This conditional distribution induces marginal density path pt(xt) = Ex0pdata[p(xtx0)] as usual, with pT pprior. To match this density path, consider the ODE dx(t) dt tx0 + σ = vt(x(t)), [0, ], (6.4.1) where vt(x) = [α tϵx] is the oracle velocity as shown in Equation (5.2.10) (noting that time is flipped to follow the diffusion model convention). Integrating equation 6.4.1 backward from x(T ) pprior yields samples from p0. Although this ODE suffices for generating high-quality samples, incorporating stochasticity may improve sample diversity. This motivates the following question: Question 6.4.1 Is there an SDE whose dynamics, starting from pprior, yield the same 188 Unified and Systematic Lens on Diffusion Models marginal densities as the ODE in Equation (6.4.1)? This statement affirms that there exists family of reverse-time SDEs that induce the same marginal density path as the corresponding PF-ODE. The densities induced by these SDEs satisfy the same FokkerPlanck equation for this path, and therefore their one time marginals coincide with the prescribed interpolation {pt}t[0,T ] 4. Proposition 6.4.1: Reverse-Time SDEs Generate the Same Marginals as Interpolations Let γ(t) 0 be an arbitrary time-dependent coefficient. Consider the reverse-time SDE dx(t) = v(x(t), t) 1 2 γ2(t)s(x(t), t) dt + γ(t) w(t), (6.4.2) evolving backward from x(T ) pT down to = 0. Then this process {x(t)}t[0,T ] matches the prescribed marginals {pt}t[0,T ] induced by the ODEs density path. Here, s(x, t) := log pt(x) is the score function, and it is related to the velocity field v(x, t) by v(x, t) = (t)x 1 2 g2(t)s(x, t), s(x, t) = 1 σt Proof for Proposition. tx αtv(x, t) α tσt αtσ α (6.4.3) . The reverse-time FokkerPlanck equation corresponding to Equation (6.4.2) is tp = (cid:16)(cid:2)v 1 2 γ2s(cid:3)p (cid:17) + 1 2 γ2p. Using the identity (sp) = (since = log p), the second-order terms cancel, yielding tp = (vp), i.e., the first-order (drift-only) FokkerPlanck equation associated with the PF-ODE. Hence the reverse-time SDE and the ODE induce the same marginal density path {pt}. See Appendix A.2A.3 of Ma et al. (2024). 4For completeness, forward SDE representation (not needed here) is with (t) and g(t) related to (αt, σt) via Equation (4.4.2). dx(t) = (t)x(t) dt + g(t) dw(t), 6.4. Beneath It All: The FokkerPlanck Equation The hyperparameter γ(t) can be chosen arbitrarily, independent of αt and σt, even after training, as it does not affect the velocity v(x, t) or the score s(x, t). Below are some examples: Setting γ(t) = 0 recovers the ODE in Equation (6.4.1). When γ(t) = g(t), Equation (6.4.2) becomes the reverse-time SDE in Equation (4.1.6), since the oracle velocity v(x, t) satisfies (See Proposition 6.3.1): v(x, t) = (t)x g2(t)s(x, t). 1 Other choices for γ(t) have been explored; e.g., Ma et al. (2024) select γ(t) to minimize the KL gap between pdata and the = 0 density obtained by solving Equation (6.4.2) from = . Following Score SDE, the trained velocity field vϕ(x, t) can be converted into parameterized score function sϕ(x, t) via Equation (6.4.3). Plugging this into Equation (6.4.2) defines an empirical reverse-time SDE, which can be sampled by numerically integrating from = with x(T ) pprior. This proposition highlights remarkable flexibility of diffusion models: once marginal density path {pt}t[0,T ] is fixed, an entire family of dynamics can reproduce it, including both the PF-ODE and the reverse-time SDEs dx(t) = (cid:2)v(x, t) 1 2 γ2(t)s(x, t)(cid:3) dt + γ(t) w(t), γ(t) 0. All such dynamics satisfy the same reversetime FokkerPlanck equation and hence yield the same marginal evolution. The function γ(t) continuously modulates the level of stochasticity without affecting the one-time distributions, revealing deep connection between the deterministic flow-based ODE and its stochastic SDE counterpart, as illustrated in Figure 6.2. 190 Unified and Systematic Lens on Diffusion Models 6.5 Closing Remarks This chapter has served as the keystone of our theoretical exploration, synthesizing the variational, score-based, and flow-based perspectives into single, cohesive framework. We have shown that these three seemingly distinct approaches are not merely parallel but are deeply and fundamentally interconnected. Our unification rests on two core insights. First, we identified the secret sauce common to all frameworks: conditioning trick that transforms an intractable marginal training objective into tractable conditional one, enabling stable and efficient learning. Second, we established that the Fokker-Planck equation is the universal law governing the evolution of probability densities. All three perspectives, in their own way, construct generative process that respects this fundamental dynamic. Furthermore, we demonstrated that the various model parameterizations, i.e., noise, clean data, score, or velocity prediction, are all interchangeable. This reveals that the choice of prediction target is matter of implementation and stability rather than fundamental modeling difference. The ultimate takeaway is that modern diffusion methods, despite their diverse origins, all instantiate the same core principle: they learn time-dependent vector field to transport simple prior to the data distribution. With this unified and principled foundation firmly established, we are now equipped to move from the foundational theory to the practical application and acceleration of diffusion models. The central insight that generation is equivalent to solving differential equation provides powerful platform for control and optimization. The subsequent parts of this monograph will leverage this unified understanding to address key practical challenges: 1. Part will focus on improving the sampling process at inference time. We will explore how to steer the generative trajectory for controllable generation (Chapter 8) and investigate advanced numerical solvers to dramatically accelerate the slow, iterative sampling process (Chapter 9). 2. Part will then look beyond iterative solvers to learn fast generators directly. We will examine methods that can produce high-quality samples in just one or few steps, either through distillation from teacher model (Chapter 10) or by training from scratch (Chapter 11). Having unified the what and why of diffusion models, we now turn our attention to the exciting and practical frontiers of how. 7 (Optional) Diffusion Models and Optimal Transport Mapping one distribution to another (with generation as special case) is central challenge. Flow matching addresses this by learning time dependent velocity field that transports mass from source to target. This naturally connects to transport theory: classical optimal transport seeks the minimal cost path between distributions, while its entropy regularized form, the Schrödinger bridge, selects the most likely controlled diffusion relative to reference such as Brownian motion. In this chapter we review the foundations of optimal transport, entropic optimal transport, and Schrödinger bridges as formulations of the distributiontodistribution problem. This leads to central question: to what extent do diffusion models realize such optimal transports? They admit two views: as stochastic processes, defined through forward and reverse SDEs, and as deterministic processes, given by PF-ODEs. The stochastic view aligns directly with entropic optimal transport, while the PF-ODE does not generally correspond to any known transport objective. This gap leaves an open question: under what conditions can diffusion models be regarded as solving an optimal transport problem? 191 (Optional) Diffusion Models and Optimal Transport 7.1 Prologue of Distribution-to-Distribution Translation Diffusion models fix the terminal distribution to standard Gaussian, pprior. However, many applications require distribution-to-distribution translation: transforming source distribution psrc into different target ptgt. Examples include converting sketches to photorealistic images or translating between artistic styles. Modern diffusion methods provide practical ways to achieve this. Oneendpoint methods such as SDEdit (Meng et al., 2021b) begin with source image at = 0, diffuse it to an intermediate step t, and then use pre-trained diffusion model for the target domain to reverse the process. This produces an output that matches the style and content of the target distribution. Two-endpoint methods, like Dual Diffusion Bridge (Su et al., 2022), instead connect the two domains through shared latent distribution, typically Gaussian at = 1. forward probabilityflow ODE transports samples from psrc into this latent space, while reverse ODE trained on the target domain maps them back to ptgt. Beyond such sampling-time approaches, the Flow Matching framework described in Section 5.2 offers training-based alternative: it directly learns an ODE flow that continuously moves mass from psrc to ptgt. Crucially, transforming between distributions requires more than two separately trained models. It demands principled mapping that aligns the dynamics at both endpoints and does so in the cheapest (cost-efficient) way. In this section, rather than surveying the many diffusion-based translation applications, we shift our focus to the mathematical foundations of this classic distribution-to-distribution problem. In particular, we highlight optimal transport (OT) and its entropic variant, the Schrödinger bridge (SB), which have long been studied in the theoretical community as canonical formulations of cost-efficient (in mathematical sense) distributional transformation. At its core, the fundamental question is: Question 7.1.1 Given two probability distributions, what is the most efficient way to transform one into the other while minimizing the total cost? Here, the cost, c(x, y), is non-negative function that assigns penalty for moving unit of mass from point to point y. common choice is the squared distance, c(x, y) = y2. This section provides brief overview to clarify how diffusion-based approaches, including flow matching, connect to classical and regularized optimal transport. The central question we aim to explore is: 7.1. Prologue of Distribution-to-Distribution Translation Question 7.1.2 Is diffusion model form of optimal transport connecting pdata and pprior, and in what sense? To address this question, we first clarify what optimality means in Section 7.2. We review classical optimal transport (OT) in the static Monge Kantorovich form (Equation (7.2.1)) and its dynamic BenamouBrenier formulation (Equation (7.2.3); minimizing kinetic energy subject to the continuity equation), as well as the entropy regularized variant (entropic OT ) in Equation (7.2.5), which is equivalent to the Schrödinger Bridge Problem (Equation (7.2.8)). In the dynamic view, OT induces deterministic flow satisfying the continuity equation, whereas SB induces controlled diffusion whose marginals evolve by the FokkerPlanck equation. We provide high level map between these formulations in Section 7.3. We then split the discussion into two parts. First, in Section 7.4, we explain that the fixed forward noising SDE used in standard diffusion models is not, by itself, Schrödinger bridge between arbitrary psrc and ptgt: the forward process is chosen reference diffusion and both forward-in-time or reverse-time SDE do not, in general, enforce exact endpoint matching to prescribed target. Hence it is not entropic OT optimal unless one explicitly solves the SB problem with those endpoints; while it is an optimal solution to the half-bridge problem as it is anchored with one starting point. Second, in Section 7.5, we return to the generative setting with psrc = pprior (Gaussian) and ptgt = pdata. The PF-ODE defines deterministic map that transports pprior to pdata by construction. However, this flow is generally not an OT map for prescribed transport cost (e.g., quadratic W2): it realizes one admissible deterministic coupling among many and does not minimize the BenamouBrenier action. What follows is we discuss if the rectify flow procedure (Section 5.4.1) can lead to OT map; however, in general, there is no such theoretical guarantee. Therefore, the exact characterization between diffusion models PF-ODE map and OT is remaining challenging and unsolved problem. (Optional) Diffusion Models and Optimal Transport 7.2 Taxonomy of the Problem Setups In this section, we introduce notions of what constitutes the most efficient or optimal way to transport mass from psrc to ptgt. These include classical optimal transport (OT) and its entropy-regularized variant, which admits an equivalent formulation known as the Schrödinger Bridge. This taxonomy provides background that will later clarify connections with diffusion models. 7.2.1 Optimal Transport (OT) MongeKantorovich (Static) Formulation of OT Problem. We fix cost function : RD RD that specifies the expense of sending probability mass from to y. The goal is to transform the source distribution psrc into the target distribution ptgt as cheaply as possible. To even define cost, we must know which pairs (x, y) are matched. This role is played by coupling: joint distribution γ on RD RD whose marginals are psrc and ptgt. In other words, sampling (x, y) γ means we match from the source with from the target. If γ admits density γ(x, y) with respect to Lebesgue measure, the marginal constraints read RD γ(x, y) dy = psrc(x), RD γ(x, y) dx = ptgt(y). That is, integrating out recovers the source density in x, while integrating out recovers the target density in y. We give two standard examples for illustration: 1. Discrete Supports. If psrc and ptgt are supported on finitely many points, coupling is represented by nonnegative matrix (γij) whose row sums equal psrc(i) and column sums equal ptgt(j). Each entry γij is the amount of mass sent from to j. 2. Deterministic Map. If there exists measurable map with T#psrc = ptgt, then γ = (I, T)#psrc is deterministic coupling that moves each point directly to T(x). Once coupling γ is fixed, the transport cost is simply the average unit cost under this plan: c(x, y) dγ(x, y) = E(x,y)γ (cid:2)c(x, y)(cid:3). In the discrete case, this reduces to i,j cijγij, whereas in the continuous setting it becomes double integral. In what follows, we will focus only on the continuous case. 7.2. Taxonomy of the Problem Setups 195 The optimal transport problem is then to choose, among all admissible couplings, the one that minimizes this expected cost. OT(cid:0)psrc, ptgt (cid:1) := inf γΓ(psrc,ptgt) c(x, y) dγ(x, y), (7.2.1) where the feasible set simply enforces the marginal, or mass-conservation, constraints: Γ(psrc, ptgt) = γ P(RD RD) : γ(x, y) dy = psrc(x), γ(x, y) dx = ptgt(y) , where P(RD RD) denotes the set of all probability measures on RD RD. Special Case: Wasserstein-2 Distance. The Wasserstein-2 distance is special case of the MongeKantorovich problem with the quadratic cost c(x, y) = y2. It measures the distance between two probability distributions as: y2i 2 (psrc, ptgt) := E(x,y)γ . inf γΓ(psrc,ptgt) Under suitable assumptions on psrc and ptgt, Breniers theorem (see Theorem 7.1)1 guarantees that the optimal coupling γ for the quadratic cost is concentrated on the graph of deterministic map : RD RD. Consequently, the Wasserstein-2 distance can be equivalently expressed as2: T(x) x2i (7.2.2) 2 2 (psrc, ptgt) = Expsrc . inf T:RDRD, s.t. T#psrc=ptgt Here, T#psrc = ptgt means that pushes psrc forward to ptgt, i.e., T(x) ptgt for psrc. Thus, the Wasserstein-2 distance represents the minimal expected squared transport cost among all couplings or transport maps that match the given marginals. The optimal transport map denoted by T(x), known as the Monge map, yields the most efficient way to transform psrc into ptgt. 1Breniers theorem is about the existence and structure of the optimal transport map for quadratic cost. In particular, if psrc does not give mass to sets of dimension at most 1, then an optimal transport map uniquely exists. 2There are three commonly used formulations of the W2 distance: the Monge formulation (based on an optimal transport map), the Kantorovich formulation (based on couplings), and the BenamouBrenier dynamic formulation (see Equation (7.2.3)). These are equivalent under appropriate regularity conditions. 196 (Optional) Diffusion Models and Optimal Transport Figure 7.1: Illustration of dynamic view of OT. The interpolation pOT evolves continuously in time, providing the least-cost transport plan that deterministically maps psrc to ptgt (the McCanns displacement interpolation). BenamouBrenier (Dynamic) Formulation of OT. Instead of mapping distributions directly in static manner, as in the MongeKantorovich formulation, transport can also be modeled as continuous-time flow: p0 := psrc pt p1 := ptgt, [0, 1]. This dynamic formulation of optimal transport, introduced by Benamou and Brenier (2000), seeks smooth velocity field vt(x) that describes how mass in pt(x) evolves over time. The BenamouBrenier formulation3 shows that, for the quadratic cost c(x, y) = y2 2 (i.e., the W2 distance), the optimal value of the static OT problem in Equation (7.2.1) is equal to the optimal value of the kinetic energy minimization problem: 3BenamouBrenier formulation describes how to compute the W2 distance by minimizing kinetic energy over continuous paths of measures and velocities. 7.2. Taxonomy of the Problem Setups 197 2 2 (psrc, ptgt) = min (pt,vt) s.t. tpt+(ptvt)=0, p0=psrc, p1=ptgt 1 0 RD vt(x)2pt(x) dx dt (7.2.3) where pt is probablity distribution on RD for each [0, 1]. In particular, The optimal transport flow pt(x) follows McCanns displacement interpolation: t (x) = (1 t)x + tT(x), where T(x) is the OT map that transports psrc to ptgt. This linear interpolation moves mass along straight lines with constant velocity: pt = #psrc for each [0, 1]. The optimal transport map satisfies the MongeAmpère equation: ptgt (ψ(x)) det (cid:16) (cid:17) 2ψ(x) = psrc(x), (7.2.4) where T(x) = ψ(x) for some convex function ψ by Breniers theorem. However, this nonlinear PDE is typically intractable for explicit solutions. Note that this is precisely the change-of-variables relation used by normalizing flows (c.f., Equation (5.0.1)): flows parametrize an invertible transport map with tractable Jacobian determinant, but do not in general impose the gradient-of-potential structure = ψ; consequently, trained flow can differ substantially from the Brenier/OT map. 7.2.2 Entropy-Regularized Optimal Transport (EOT) To motivate EOT concretely, consider empirical distributions built from i=1 RD with weights samples. Suppose psrc is supported on points {x(i)}n j=1 RD with weights bj. coupling is then an ai, and ptgt on {y(j)}n nonnegative matrix γ = (γij) whose row sums match and column sums match b. Each entry γij represents the amount of mass transported from x(i) to y(j)4 . Why Regularize OT? Classical OT in this discrete setting (obtained by taking counting measures in the continuous formulation Equation (7.2.1)) 4Empirical (discrete) measures provide principled proxy for continuous distributions. When the ground cost is c(x, y) = d(x, y)p (so the OT value equals ) and the measures have finite pth moments, the empirical measures converge to the population in Wp with quantitative rates; see Fournier and Guillin (2015) and the overview in Peyré, Cuturi, et al. (2019). 198 (Optional) Diffusion Models and Optimal Transport reduces to minimizing min γ=(γij ) i,j Cij γij, over all feasible couplings γ = (γij), where Cij = c(x(i), y(j)) is the cost of moving one unit of mass from source point x(i) to target point y(j), for prescribed ground cost : RD RD R0 (e.g., c(x, y) = y2 2). Two main issues arise: 1. Non-Uniqueness and Instability: The minimizer γ need not be unique. For example, if two transport plans achieve the same minimum cost, the solver may select either one. Consequently, small changes in the inputs (a, b, C) (such as moving sample, adjusting weights, or slightly perturbing costs) can cause abrupt jumps in the solution. 2. High Computational Cost: The problem is linear program with n2 variables and 2n constraints. Practical solvers (e.g., Hungarian algorithm, network simplex (Peyré, Cuturi, et al., 2019)) typically scale as O(n3), which is infeasible for large n. To overcome these bottlenecks, EOT objective function introduces regularization term to the classical OT problem, controlled by parameter ε > 0: EOTε(psrc, ptgt) := min γΓ(psrc,ptgt) c(x, y) dγ(x, y) + εDKL (γM ) . (7.2.5) The reference measure is typically chosen as the product of the marginals, psrc ptgt. The KL divergence term is directly related to the Shannon entropy of the transport plan γ: DKL(γ psrc ptgt) = H(γ) + Constant, where H(γ) := γ(x, y) log γ(x, y) dx dy. The addition of this regularization term yields several theoretical and practical advantages, which we briefly outline below: Why Entropy Regularizer Helps? 7.2. Taxonomy of the Problem Setups 199 1. Mass Spreading. Since 7 log is convex and grows rapidly for large t, minimizing γ log γ penalizes peaky couplings (some γ(x, y) very large, most near zero). For fixed total mass γ = 1, it favors plans where γ(x, y) is more evenly distributed over (x, y) RD RD. Equivalently, maximizing Shannon entropy promotes higher uncertainty (diffuseness). 2. Strict Convexity and Uniqueness. Because is strictly concave, the objective in Equation (7.2.5) is strictly convex in γ, yielding unique minimizer γ ε that depends continuously on (psrc, ptgt, c). 3. Sinkhorn Form and Positivity. Under mild conditions5, the optimizer has the Schrödinger/Sinkhorn form ε (x, y) = u(x) exp(cid:0) c(x,y) γ ε (cid:1) v(y)psrc(x)ptgt(y), for positive scaling functions u, (unique up to global factor). In practice, the continuous formulation is approximated with finite samples, reducing EOT to finite (sampled) Sinkhorn iteration. The entropic objective is strictly convex, and the scaling (Sinkhorn/IPFP) algorithm solves it efficiently (Sinkhorn, 1964; Cuturi, 2013). For dense problem with support points per marginal (an kernel), each Sinkhorn iteration costs O(n2) time and O(n2) memory, making the method more scalable and practical (Altschuler et al., 2017). 4. Limits in ε. As ε 0, the optimal plan γ ε becomes increasingly concentrated, approaching (possibly singular) classical OT coupling (we will revisit this connection in Section 7.3.2). As ε increases, γ ε gradually spreads out and approaches the independent coupling psrc ptgt. 7.2.3 Schrödinger Bridge (SB) KL Formulation of SB. The Schrödinger Bridge (SB) problem, introduced by Erwin Schrödinger in the 1930s, asks the following question. Suppose particles move according to some simple reference dynamics, such as Brownian motion. Now imagine that we observe the particles at two times: at = 0 their distribution is psrc, and at = 1 it is ptgt. Among all possible stochastic processes that connect these two distributions, which one deviates the least from 5We assume that < holds psrc ptgt-almost everywhere, and that the marginal ε , psrc, kernel integrals are finite and positive. For simplicity, we focus on the case where γ and ptgt admit densities with respect to the Lebesgue measure. (Optional) Diffusion Models and Optimal Transport the reference dynamics? Here deviation is measured by the KL divergence, so the solution to the SB problem is the most likely way to deform Brownian motion into process that satisfies the prescribed boundary conditions. To make this precise, let x0:T := {xt}t[0,T ] denote complete trajectory of the process. We write for the law of trajectories, that is, the probability distribution over entire sample paths. The time-t marginal of is denoted by pt (or Pt), which describes the distribution of the state xt at single time. Formally, for measurable set RD, pt(A) = (xt A). In other words, pt can be viewed as the empirical distribution obtained by sampling many full trajectories from and then collecting the states at time tfor instance, as histogram if the state is one-dimensional. Consider reference diffusion {xt}t[0,T ] governed by the SDE dxt = (xt, t) dt + g(t) dwt, (7.2.6) where : RD [0, ] RD, : [0, ] R, and {wt}t[0,T ] is standard Brownian motion. Let denote the path law (joint distribution) of the full trajectory x0:T := {xt}t[0,T ]; this will serve as the reference trajectory distribution. With this notation, the Schrödinger Bridge (SB) problem seeks trajectory law that is closest to in KL divergence while matching the prescribed endpoint marginals: SB(psrc, ptgt) := min DKL(P R) s.t. P0 = psrc, PT = ptgt. (7.2.7) The optimizer depends on the chosen reference process R. Stochastic control view of SB. Rather than optimizing over arbitrary path distributions in Equation (7.2.7), more tractable approach is to take the reference dynamics as an anchor and allow it to drift. This is done by introducing time-dependent drift vt(xt), which perturbs the reference process and generates family of candidate trajectory distributions. The resulting dynamics take the form of controlled diffusion: dxt = (cid:2)f (xt, t) + vt(xt)(cid:3) dt + g(t) dwt, where vt : RD RD is the drift to be optimized later (Equation (7.2.8)). Under standard integrability conditions (e.g., Novikovs condition) and by Girsanovs 7.2. Taxonomy of the Problem Setups 201 Figure 7.2: Illustration of stochastic control view of SB. The bridge seeks the stochastic path that deviates least from the reference while connecting psrc and ptgt. theorem (see Section C.2.1), the KL divergence between the controlled law and the reference admits the dynamic (kinetic) form DKL(P R) = EP \" 1 2 0 vt(xt)2 g2(t) # dt = 1 2 RD vt(x)2 g2(t) pt(x) dx dt, where pt is the timet marginal of xt under the controlled process. The second equality follows from the law of total expectation. Hence, the SB problem can be reformulated as minimizing the expected control energy over all admissible drifts vt that steer the process from psrc at = 0 to ptgt at = (Dai Pra, 1991; Pra and Pavon, 1990; Pavon and Wakolbinger, 1991; Chen et al., 2016). This leads to the stochastic control formulation: (Optional) Diffusion Models and Optimal Transport SBε(psrc, ptgt) = min vt s.t. dxt=[f (xt,t)+vt(xt)] dt+g(t) dwt, x0psrc, xT ptgt 1 2 Z 0 RD vt(x)2 g2(t) pt(x) dx dt, (7.2.8) Importantly, the endpoint distributions psrc and ptgt are arbitrary; the control vt is chosen precisely to bridge the reference dynamics between these marginals while staying as close as possible (in KL divergence) to the reference process R. Special Brownian Reference. Equation (7.2.8) resembles the BenamouBrenier formulation of OT in Equation (7.2.3), especially when the reference process Rε (with ε > 0) is chosen to be Brownian motion: so that 0 and g(t) ε. dxt = ε dwt, In this setting, the SB problem seeks path distribution that stays closest (in KL divergence) to the Brownian reference Rε, while matching the endpoint marginals: SBε(psrc, ptgt) := min DKL(P Rε) s.t. P0 = psrc, PT = ptgt. (7.2.9) The equivalent stochastic control formulation then becomes SBε(psrc, ptgt) = min vt s.t. dxt= ε dwt, x0psrc, xT ptgt 1 2ε RD vt(x)2 pt(x) dx dt. (7.2.10) Why We Need to Specify Reference Distribution? Unlike in classical OT, the SB problem requires reference distribution due to its stochastic nature. In OT, the cost function (e.g., c(x, y) y2) implicitly defines unique, deterministic geodesic path, making reference unnecessary. In contrast, the SB setting admits infinitely many stochastic processes connecting the marginals, with no intrinsic notion of natural path. The reference measure encodes the systems underlying physics or geometric structure (e.g., Brownian motion) and defines the KL-based optimization objective DKL(P R), without which the notion of optimality is undefined. 7.2. Taxonomy of the Problem Setups Coupled PDE Characterization. convenient way to describe the SB solution is through two spacetime potentials Ψ(x, t) and bΨ(x, t). Let pSB denote the marginal at time [0, ] of the optimal trajectory law in Equation (7.2.7). Then one has the symmetric factorization (Dai Pra, 1991) pSB (x) = Ψ(x, t) bΨ(x, t), (7.2.11) where Ψ and bΨ solve the (linear) Schrödinger system (Caluya and Halder, 2021; Chen et al., 2021; Chen et al., 2022): Ψ bΨ g2(t) 2 g2(t) (x, t) = xΨ(x, t) (x, t) xΨ(x, t), (x, t) = (cid:0) bΨ(x, t) (x, t)(cid:1) + bΨ(x, t) (7.2.12) subject to Ψ(x, 0) bΨ(x, 0) = psrc(x), Ψ(x, ) bΨ(x, ) = ptgt(x). Forward-Time Schrödinger Bridge SDE. Once Ψ is known, the optimal dynamics is the reference diffusion tilted by the spacetime factor Ψ: dxt = (cid:2)f (xt, t) + g2(t) log Ψ(xt, t)(cid:3) dt + g(t) dwt, x0 psrc. (7.2.13) Let denote the trajectory law of Equation (7.2.13) (so Q0 = psrc and QT = ptgt by Equations (7.2.11) and (7.2.12)). Then = and the minimizer to Equation (7.2.8) is (see (Chen et al., 2021)s Section 4.6): (x) = g2(t)x log Ψ(x, t). That is, drift correction g2x log Ψ is precisely the minimal KL perturbation of the reference needed to match the endpoint marginals. Reverse-Time Schrödinger Bridge SDE. The same optimal path law can be generated reverse in time. convenient way to see the form of the reverse-time drift is to conceptually use the standard time-reversal identity for diffusions: b(x, t) = b+(x, t) g2(t)x log pSB (x), where b+ = + g2 log Ψ and pt = Ψ bΨ. This gives b(x, t) = (x, t) g2(t)x log bΨ(x, t). 204 (Optional) Diffusion Models and Optimal Transport Thus the reverse-time SDE reads dxt = f (xt, t) g2(t)x log bΨ(xt, t) dt + g(t) wt, xT ptgt. (7.2.14) Equivalently, reparametrizing time by yτ := xT τ so that τ increases from 0 to . Then yτ evolves forward in τ from y0 ptgt as dyτ = (cid:2) (yτ , τ ) + g2(T τ )y log bΨ(yτ ,T τ )(cid:3) dτ + g(T τ ) dwτ . (7.2.15) In the reverse-time stochastic control formulation of Equation (7.2.8)(same quadratic energy with the reversed clock): min uτ s.t. dyτ =[f (yτ ,T τ )+uτ (yτ )] dτ +g(T τ ) dwτ , y0ptgt, yT psrc 1 2 Z 0 RD uτ (y)2 g2(T τ ) pT τ (y) dy dτ. (7.2.16) the optimal control is (x) = g2(t)x log bΨ(x, t). Both the forward and reverse descriptions yield the same optimal path law which are linked by log pSB = log Ψ + log bΨ, = b+ g2 log pSB , so their marginals coincide with pSB at every time. The additional drift terms g2 log Ψ (forward) and g2 log bΨ (reverse-time) act as control forces that steer the reference diffusion to match the endpoint marginals while staying closest to the reference in relative entropy. Practical Obstacles to the Coupled PDE Approach. To construct the generative process based on Equation (7.2.14), one must solve the coupled PDEs in Equation (7.2.12) to obtain the backward Schrödinger potential bΨ. However, these PDEs are notoriously difficult to solve, even in low-dimensional settings, which makes their direct application in generative modeling challenging. To circumvent this, several works have proposed alternative strategies: leveraging Score SDE techniques to iteratively solve each half-bridge problem (ptgt psrc and ptgt psrc) (De Bortoli et al., 2021); optimizing surrogate likelihood bounds (Chen et al., 2022; Liu et al., 2023); or designing simulationfree training based on an analytical solution of the posterior xtx0, xT for sample pairs (x0, xT ) psrc ptgt (Liu et al., 2023). We do not delve into the technical details here but briefly discuss the connection between diffusion models and SB in Section 7.4. 7.2. Taxonomy of the Problem Setups 7.2.4 Global Pushforwards and Local Dynamics: An OT Analogy for DGMs From the optimal-transport viewpoint (in Equation (7.2.1)), one can leverage deep generative models to learn transport (pushforward) map from simple prior to the data, i.e., Gϕ#pprior pdata. Although Gϕ generally does not coincide with the optimal transport map (except in works (Genevay et al., 2018; Onken et al., 2021) that impose an OT objective under suitable conditions), the BenamouBrenier formulation (in Equation (7.2.3)) provides complementary, dynamic perspective. Rather than directly learning single global map, it describes transport as continuous flow generated by time-dependent local vector field, tracing smooth path between pprior and pdata. This dynamic formulation parallels the relationship between the static Schrödinger Bridge problem (in Equation (7.2.7)) and its stochastic-control counterpart (in Equation (7.2.8)), where the optimal coupling is realized as controlled diffusion process. similar analogy emerges in generative modeling: standard DGMs such as GANs or VAEs learn global pushforward map, whereas diffusion models learn time-dependent local vector field that drives the generative dynamics. 206 (Optional) Diffusion Models and Optimal Transport 7.3 Relationship of Variant Optimal Transport Formulations Figure 7.3: Relationship between variants of optimal transport with c(x, y) = y2 2 and Reference Rε in SB. We summarize the equivalences: (i) SBε (stochastic control) SBε (Static formulation), where pt is the timet marginal of the path measure ; (ii) SBε (Static formulation) EOTε (see Section 7.3.1); (iii) EOTε (Static formulation) OTε (static) (see Section 7.3.2); (iv) SBε (stochastic control) OT (dynamic) (see Section 7.3.3). SBε (Stochastic Control) See Equation (7.2.10). pt = (xt ) (i) SBε (Static Formulation) DKL(P Rε) min : P0=psrc PT =ptgt Endpoint projection γ = (x0, xT )#P (ii) ε 0 (iv) OT (Dynamic Formulation) See Equation (7.2.3). pt = Ψt#γ where Ψt(x, y) = (1 t)x + ty EOTε min γΓ(psrc,ptgt) y2 2 dγ + εDKL(γpsrc ptgt) ε 0 (iii) OT (Static Formulation) min γΓ(psrc,ptgt) y2 2 dγ Before delving into the technical details, it is helpful to clarify how the different formulations of optimal transport and its entropic regularizations are connected. At high level, these problems can be viewed as related (see Figure 7.3 for their diagram for connection): (i) SB problem SBε with the specific reference Rε given by Brownian motion dxt = ε dwt is equivalent to its static formulation: the evolving marginals pt are precisely the timet slices of the optimal path measure (see Section 7.2.3); (ii) Static formulation of SBε connects directly to the entropic OT problem, EOTε (see Section 7.3.1); (iii) EOTε, in turn, can be related back to the static formulation of entropic OT, OTε (see Section 7.3.2); (iv) Stochastic control perspective of SBε can also be linked to the dynamic formulation of classical OT (see Section 7.3.3). Together, these non-trivial relationships provide compact view across stochastic control, entropy-regularized, and classical OT frameworks. 7.3. Relationship of Variant Optimal Transport Formulations 207 7.3.1 SB and EOT are (Dual) Equivalent In this section, we present two complementary perspectives showing that SB are essentially equivalent to EOT. Unlike classical optimal transport, which produces single deterministic map, SB yields stochastic flow of particles: mass is transported probabilistically, with marginals evolving under diffusion-like dynamics. From the static viewpoint, SB coincides with EOT, where the goal is to find coupling between the two endpoint distributions that balances transport cost with entropy. From the dynamic viewpoint, SB describes controlled diffusion process that remains as close as possible to simple reference (such as Brownian motion) while still matching the desired endpoints. Each perspective independently establishes the equivalence, providing two consistent ways to understand SB/EOT as canonical formulations of distribution-to-distribution transformation. Static Schrödinger Bridge. Let eRε(x, y) := 1 Zε ec(x,y)/ε psrc(x) ptgt(y), with normalizing constant: ZZ Zε := ec(x,y)/εpsrc(x)ptgt(y)dxdy. Then the entropic OT objective min γΓ(psrc,ptgt) cdγ + εDKL (cid:0)γpsrc ptgt (cid:1)o = ε min γΓ(psrc,ptgt) DKL (cid:0)γ eRε(cid:1) ε log Zε, (7.3.1) so it is equivalent (up to an additive constant) to the static Schrödinger Bridge (in Equation (7.2.9)): DKL(γ eRε). min γΓ Dynamic Equivalence (Brownian Reference). We can also view the equivalence from the dynamic equivalence which is the classical result (Mikami and Thieullen, 2006) says that entropic OT with quadratic cost c(x, y) = x2 2T 208 (Optional) Diffusion Models and Optimal Transport is affinely equivalent to the SB problem where the reference path law Rε is Brownian motion on [0, ], dxt = ε dwt. Here, affinely equivalent means the optimal values differ by positive scaling and an additive constant (independent of the decision variable), so the minimizers coincide. In particular, let be the optimal path distribution for SB and let γ be the optimal transport plan for EOT. Then if x[0:T ] , the pair of endpoints (x0, xT ) has distribution γ: solves SB γ solves EOT and (x0, xT ) γ. In words: the optimal process from the dynamic (SB) problem induces the optimal coupling for the static (EOT) problem. Conversely, (under mild conditions on the heat kernel,) any optimal static coupling can be realized as the endpoints of some optimal SB process. The key idea to derive this fact is that the KL divergence over paths can be broken down according to the endpoints, which means the Schrödinger bridge problem reduces to KL divergence just over the joint distribution of (x0, xT ). For Brownian motion the transition density between and has Gaussian form, so its negative log is quadratic: x2 2T ε log pT (y x) = + const. This shows that the endpoint KL is exactly the same as the entropic OT objective with quadratic cost, up to an irrelevant constant. SB with General Reference Determines the EOT Cost. As we discussed in Equation (7.2.7), the SB problem is not restricted to Brownian motion; it can be defined with any (well-posed) reference process. This choice uniquely determines the cost function in the corresponding EOT problem. The key connection is that the SB reference dynamics induce the EOT cost function. Let the reference process be governed by an SDE over [0, ], yielding transition density pT (yx), the likelihood of reaching at time from at time 0. Then, the EOT cost function is given (up to scaling constant) by c(x, y) log pT (yx). With this cost, solving the SB problem becomes equivalent to solving an EOT problem. In short, choosing the reference dynamics in SB is mathematically equivalent to specifying the transport cost in EOT. By Equation (7.3.1), the entropic OT objective differs from the static SB objective; hence the two problems are equivalent and have the same minimizer. 7.3. Relationship of Variant Optimal Transport Formulations 209 7.3.2 EOTε is Reduced to OT as ε 0 ε denote the optimal plan for the EOTε, and let γ be an optimal Let γ plan for the unregularized OT problem in Equation (7.2.1). The following result (Mikami and Thieullen, 2008; Peyré, Cuturi, et al., 2019) shows that as ε 0, the entropic optimal plan γ ε converges (in suitable sense) to the OT plan γ, and the EOT cost converges to the OT cost. This convergence result is both fundamental and practically important. One of the reasons is that the entropy-regularized OT problem EOTε admits efficient numerical solutions via algorithms such as Sinkhorn. Thus, the result provides theoretical justification for using EOTε with small ε as computationally tractable proxy for the classical OT problem in Equation (7.2.1), even when the cost function c(x, y) is more general than the quadratic case. Theorem 7.3.1: (Informal) EOTε Converges to OT. As ε 0, the optimal values converge: lim ε0 EOTε(psrc, ptgt) = OT(psrc, ptgt). Moreover, the optimal plans γ ε converge weakly to γ. That is, E(x,y)γ ε [g(x, y)] E(x,y)γ[g(x, y)], for all bounded continuous (test) functions : RD RD R. Proof for Theorem. For rigorous proof, we refer to the literature (Mikami and Thieullen, 2008; Peyré, Cuturi, et al., 2019). Below we provide heuristic derivation of the value convergence. Let us denote the corresponding optimal values by Vε := EOTε(psrc, ptgt), V0 := OT(psrc, ptgt) for notational simplicity. Upper Bound. By optimality of γ using the plan γ: ε , its value Vε is bounded by the cost of Vε dγ + εDKL(γpsrc ptgt). Assuming the KL term is finite constant K, we get Vε V0 + εK. Taking the limit superior yields lim supε0 Vε V0. 210 (Optional) Diffusion Models and Optimal Transport Lower Bound. Since the KL-divergence is non-negative, Vε dγ ε . By definition of V0 as the minimal transport cost, any plans cost is at least V0, so dγ ε V0. This implies Vε V0 for all ε > 0, and thus lim inf ε0 Vε V0. Combining the upper and lower bounds shows the convergence of the optimal value, limε0 Vε = V0. The convergence of the optimal plan itself, γ ε γ in the weak sense, is more advanced result from Γ-convergence theory that we omit. 7.3.3 SBε is Reduced to OT as ε 0 be minimizer of the SB problem as in Equation (7.2.10), be the marginal distribution of the controlled SDE xt induced by vε . satisfies the associated FokkerPlanck equation. In contrast, denote by ) minimizer of the BenamouBrenier formulation of optimal transport For each ε > 0, let vε and let pε Then pε (p0 , v0 (see Equation (7.2.3)). The following theorem6 states that as ε 0, the SB problem converges to the OT problem. This result is practically important for reasons similar to those in Theorem 7.3.1. The objective SBε can be efficiently solved using Sinkhorn type algorithms, yielding numerically tractable and differentiable proxy for optimal transport. This is especially valuable in high dimensional or large scale settings, where direct solvers (e.g., based on the BenamouBrenier formulation) become computationally expensive. Theorem 7.3.2: (Informal) SBε Converges to OT. As ε 0, we have: lim ε0 SBε(psrc, ptgt) = OT(psrc, ptgt), where OT is of the BenamouBrenier formulation as in Equation (7.2.3). Moreover, pε in the appropriate function spaces. converges weakly to t converges weakly to p0 , and vε Proof for Theorem. 6We remark that the convergence of the optimal values in the theorem is in the sense of Γ-convergence, rather than classical pointwise limit. Although this requires more technical background, we omit the details here and state only the conceptual result. 7.3. Relationship of Variant Optimal Transport Formulations full rigorous proof of the convergence result is beyond our scope; we refer the reader to Léonard (2012) and Léonard (2014) for detailed derivations. Nevertheless, we can heuristically understand why this convergence may hold. In the stochastic control formulation of the SB problem Equation (7.2.10), the controlled SDE are given by: dxt = vε (xt)dt + 2εdwt. As ε 0, the noise term vanishes, and the SDE formally approaches deterministic ODE: dxt = v0 (xt)dt. This suggests that the optimal value of the SB problem converges to that of the optimal transport problem: lim ε SBε(psrc, ptgt) = OT(psrc, ptgt). In parallel, the marginal density pε satisfies the FokkerPlanck equation: tpε + (pε vε ) = εpε . Again, as ε 0, the diffusion term vanishes, and the equation formally reduces to the continuity equation: tp0 + (cid:17) (cid:16) v0 p0 = 0. Until now, we have presented the fundamental equivalences (under their respective assumptions) between EOT and SB, as well as their important connection to OT through limiting process, illustrated in Figure 7.3. Next, we will explore how diffusion models connect to these concepts. 212 (Optional) Diffusion Models and Optimal Transport 7. Is Diffusion Models SDE Optimal Solution to SB Problem? 7.4.1 Diffusion models as Special Case of Schrödinger Bridges SB framework extends (score-based) diffusion models by enabling nonlinear interpolation between arbitrary source and target distributions. It achieves this by adding control drift terms derived from scalar potentials Ψ(x, t) and bΨ(x, t), which guide reference diffusion process to match prescribed endpoint marginals (see Equation (7.2.12)) and follow the decomposition: log Ψ(x, t) + log ˆΨ(x, t) = log pSB (x). This generalization allows the model to move beyond standard Gaussian priors and generate samples from broader distributions. Connection to Diffusion Models. Diffusion models arise as special case of the SB framework. Suppose the potential is constant, Ψ(x, t) 1. Under this assumption, the second PDE in Equation (7.2.12) reduces to the standard FokkerPlanck equation, whose solution is the marginal density of the reference process: bΨ(x, t) = pSB (x). (7.4.1) The corresponding SB forward SDE thus becomes the uncontrolled reference process: dxt = (xt, t) dt + g(t) dwt, and the SB backward SDE simplifies to: (xt, t) g2(t) log pSB dxt = (xt) dt + g(t) wt, which matches Andersons reverse-time SDE used in diffusion models. This correspondence shows that diffusion models can be interpreted as the zerocontrol limit of SB, where no additional drift is introduced by the potentials. Boundary Conditions and Generality. The above reduction is purely formal unless the boundary constraints are compatible. For arbitrary source/target (psrc, ptgt), the PDE boundary conditions in Equation (7.2.12) are generally not satisfied by the choice Ψ 1. Full SB resolves this by learning nontrivial potentials that induce nonlinear control drift, bending the reference dynamics to match any prescribed endpoints. By contrast, diffusion models fix one endpoint to simple prior (typically Gaussian) and learn only the reversetime score to reach the data. With this perspective, SB is the more flexible 7.4. Is Diffusion Models SDE Optimal Solution to SB Problem? 213 umbrella: with nontrivial potentials it bridges arbitrary endpoints; with Ψ 1 it collapses to the diffusion-model case above. We additionally remark that in the standard linear diffusion model, pT pprior holds only as , so the match to the prior is merely approximate. 7.4.2 Diffusion Models as Schrödinger Half-Bridges In this section, we explain why diffusion models are not full Schrödinger bridges, but can instead be understood through the relaxed notion of Schrödinger half-bridges. half-bridge enforces only one endpoint constraint (either pprior or pdata) rather than both, making it one-sided variant of the full bridge. Before formalizing this connection, we introduce the definition of Schrödinger half-bridges, building on the general formulation in Equation (7.2.7) with arbitrary psrc and ptgt. We will then return to diffusion models and show how the half-bridge viewpoint naturally applies when the endpoints are given by pprior and pdata. Schrödinger Half-Bridges The SB problem asks for stochastic process whose law is closest (in KL divergence) to simple reference process, while matching two endpoint distributions psrc and ptgt. Solving the full bridge requires enforcing both boundary conditions, which is often computationally difficult. useful relaxation is the half-bridge problem: instead of matching both endpoints, we match only one of them. Formally, let be the reference path distribution. The forward half-bridge seeks path distribution minimizing min :P0=psrc DKL(P R), subject to the single constraint P0 = psrc. Similarly, the backward half-bridge constrains only the terminal distribution, min :PT =ptgt DKL(P R). In words, the forward half-bridge asks: among all processes starting from the desired initial distribution, which one looks most like the reference? The backward half-bridge asks the same question for processes ending at the desired terminal distribution. By combining these two relaxations iteratively, one can approximate the full SB. Diffusion Models Miss Exact Endpoint Matching. key difference between diffusion models and the SB framework lies in the treatment of the terminal 214 (Optional) Diffusion Models and Optimal Transport distribution pT . In standard diffusion models, the forward SDE is typically linear in xt (see Equation (4.3.2)) and designed so that pT approximates the prior only as : pT pprior. At finite time, however, pT is Gaussian whose parameters depend on pdata (see Section C.1.5). As result, it generally does not match the desired prior without careful tuning. In contrast, the SB framework enforces exact marginal matching at finite time by introducing an additional control drift of the form g2(t)x log Ψ(x, t). This ensures that the terminal distribution precisely satisfies pT = pprior, regardless of the initial data distribution p0 = pdata. In summary: Diffusion Models: pT pprior, asymptotically as , Schrödinger Bridge: pT = pprior exactly at finite , enabled by solving for the control potentials Ψ and bΨ. Diffusion Schrödinger Bridge. Standard diffusion models do not enforce PT = pprior, and thus only solve Schrödinger half-bridge from pdata to pprior. To address this, the Diffusion Schrödinger Bridge (DSB) (De Bortoli et al., 2021) alternates between matching both endpoint marginals by following the idea of the Iterative Proportional Fitting (IPF) algorithm, an alternating projection method. This extends diffusion models to solve the full SB problem as follows7: Step 0: Reference Process. Initialize with (0) := Rfwd, the reference forward SDE: This ensures (0) dxt = (xt, t)dt + g(t)dwt, x0 pdata. 0 = pdata, but typically (0) = pprior. Step 1: Backward Pass. Compute the process (1) that matches pprior at time while staying close to (0): (1) = arg min :PT =pprior DKL(P (0)). This is achieved via approximating the oracle score function with neural network sϕ, which results in the reverse-time SDE: dxt = (xt, t) g2(t)sϕ(xt, t) dt + g(t)d wt, 7Although this description uses pdata and pprior, the DSB framework applies to any pair of endpoint distributions. 7.4. Is Diffusion Models SDE Optimal Solution to SB Problem? 215 simulated backward from xT pprior. Iteration. The process (1) satisfies (1) = pprior, but its initial marginal (1) typically deviates from pdata. IPF addresses this by learning 0 forward SDE to adjust (1) back to pdata, followed by another backward pass to enforce pprior. This alternation continues, refining the process until convergence to the optimal bridge , which satisfies both 0 = pdata = pprior. De Bortoli et al. (2021) prove convergence under mild and conditions. 0 216 (Optional) Diffusion Models and Optimal Transport 7.5 Is Diffusion Models ODE an Optimal Map to OT Problem? In this section, we focus on quadratic-cost optimal transport problem. 7.5.1 PF-ODE Flow Is Generally Not Optimal Transport This section presents the result of Lavenant and Santambrogio (2022), which demonstrates that the solution map of the PF-ODE does not generally yield the optimal transport map under quadratic cost. Setup. We consider VP SDE, specifically the OrnsteinUhlenbeck process, which evolves smooth initial density p0 toward the standard Gaussian (0, I): 2 dw(t), x(0) p0. dx(t) = x(t) dt + The associated PF-ODE is given by dSt(x) dt = St(x) log pt(St(x)), S0(x) = x. Here, St denotes the flow map pushing forward p0 to the marginal pt: (St) #p0 = pt, that is, pt(y) = RD δ(y St(x))p0(x) dx. These densities pt evolves via the Fokker-Planck equation: pt = (xpt) + pt. This is equivalent to continuity equation with velocity field: vt(x) = log pt(x), whose flow is given by St(x). In other words, the PF-ODE can be written as: dSt(x) dt = vt (St(x)) . As , the map transports the initial distribution to the prior: S#p0 = (0, I) =: pprior. Objective of Lavenant and Santambrogio (2022)s Argument. Lavenant and Santambrogio (2022) do not directly assess whether the terminal map from p0 to the Gaussian is optimal. Instead, they construct specific 7.5. Is Diffusion Models ODE an Optimal Map to OT Problem? 217 initial distribution p0 and examine the entire PF-ODE trajectory. Their key observation is that optimality may fail at some point along the flow. They consider the intermediate marginal pt = St#p0 and define the residual transport map from pt0 to the Gaussian as Tt := S1 , for all 0. The core of their argument shows that, for carefully chosen p0, there exists time t0 0 such that Tt0 is not the quadratic-cost optimal transport map from the new starting distribution pt0 to (0, I). This result demonstrates that PF-ODE flows do not, in general, yield optimal transport maps, and that the property of optimality can break down for certain initial distributions. Some Tools. The argument of Lavenant and Santambrogio (2022) crucially relies on the following result, known as Breniers theorem: Theorem 7.1 (Informal Breniers Theorem). Let ν1, ν2 be two probability distributions on RD with smooth densities. smooth map : RD RD is the optimal transport from ν1 to ν2 (under quadratic cost) if and only if = for some convex function u. In this case, DT is symmetric and positive semi-definite, and satisfies the MongeAmpère equation: det D2u(x) = ν1(x) ν2(u(x)) . The proof also implicitly uses the following fact, which we will not repeat each time: map is the optimal transport between two distributions if and only if its inverse is the optimal transport in the reverse direction. Proof Sketch: PF-ODE Is Not an OT Map in General. Lavenant and Santambrogio (2022) employ proof by contradiction: they assume that for every 0, the map Tt = St S1 is the quadratic-cost optimal transport map from (0, I) to pt. (Optional) Diffusion Models and Optimal Transport Step 1: Breniers Theorem. By Breniers Theorem, the Jacobian of any optimal transport map from Gaussian must be symmetric and positive semi-definite. Thus, DTt(x) = DSt(S1 (x))D(S1 )(x) must be symmetric for all and x. Here, DTt(x) denotes the total differentiation with respect to x. Step 2: Time-Differentiating the Symmetry Condition. Differentiating in time: DTt(x) = (cid:18) (cid:19) DSt (S1 (x))D(S1 )(x). Given that the symmetry holds for all t, it follows that this derivative remains symmetric. Using the flow ODE (differentiating in x), we obtain: (DSt) = Dvt(St) DSt = (cid:16) (cid:17) D2 log pt(St) DSt. Combining the above, we see that (cid:16) (cid:17) D2 log pt(St) DSt D(S1 ) is symmetric for all 0. At = 0, we have S0 = and DS0 = I, yielding: (cid:16) D2 log p0(S1 (cid:17) (x)) D(S1 )(x) is symmetric. Step 3: The Commutation Condition. Since T0 = is assumed to be optimal, its Jacobian DT0 = D(S1 ) is symmetric. Moreover, the Hessian D2 log p0 is symmetric. Recall that two symmetric matrices multiply to symmetric matrix if and only if they commute. Hence, for all RD, D2 log p0 (cid:0)S1 (x)(cid:1) must commute with D(S1 )(x). (x) gives the equivalent condition: for all RD, Setting = S1 D2 log p0(y) must commute with DS(y). Now, we transform this condition into more computable form. Since is optimal between p0 and (0, I), Breniers theorem guarantees that = for some convex function u. From the MongeAmpère equation, it follows that: 1 2 The condition becomes (with DS = D2u): log p0(y) = log det(D2u(y)) u(y)2 + Constant. 7.5. Is Diffusion Models ODE an Optimal Map to OT Problem? 219 (cid:18) D2 log det D2u (cid:19) 1 2 must commute with D2u. (7.5.1) This yields necessary condition for Tt to be optimal. Step 4: Constructing the Counterexample. Let us show how to leverage this necessary condition to derive contradiction. Assume we can construct convex function such that (cid:18) log det D2u(x) D2 (cid:19) u(x) 1 2 does not commute with D2u(x) for some RD. Defining p0 = (u)1#N (0, I), Breniers theorem implies that is the optimal transport from p0 to (0, I). However, the condition in Equation (7.5.1) fails, leading to contradiction. Thus, our goal is to construct such function. Consider u(x) = 1 2 x2 + εϕ(x), for small ε. Then D2u(0) = + εD2ϕ(0), and the commutation condition at = 0 requires D2ϕ(0) to commute with D2(ϕ)(0). For example, in R2, choosing ϕ(x1, x2) = x1x2 + x4 1 provides counterexample where the Hessian D2 log p0 and the Jacobian D2u do not commute. This contradiction shows that Tt cannot be optimal for all 0. Therefore, there exists some t0 0 such that the map Tt0 is not optimal. (Optional) Diffusion Models and Optimal Transport 7.5.2 Can Canonical Linear Flow and Reflow Leads to an OT Map? We have seen that the PF-ODE (especially in VP type forward kernel) is generally not an OT map. One natural question now is: Question 7.5.1 Does the linear interpolation flow (1 t)x0 + tx1 with x0 psrc and x1 ptgt, when applied to the independent coupling π(x0, x1) = psrc(x0)ptgt(x1), recover the OT map? The answer to the question is no. Nevertheless, combining linear path with given coupling offers practical upper bound on the true OT cost. Among all possible paths, linear interpolation provides the tightest such upper bound, as we will see in the following discussion. Canonical Linear Flow and Optimal Transport. Focusing on optimal transport with quadratic cost, we consider the equivalent form of Equation (7.2.1), the BenamouBrenier formulation in Equation (7.2.3): (psrc, ptgt) := min (pt,vt) s.t. tpt+(ptvt)=0, p0=psrc, p1=ptgt 1 0 RD vt(x)2pt(x) dx dt. However, solving this minimization problem directly is typically intractable, as it requires solving highly nonlinear partial differential equation, namely the MongeAmpère equation. While solving the Benamou-Brenier formulation is generally intractable, Liu (2022) and Lipman et al. (2024) reveal that its kinetic energy admits practical upper bound. This is achieved by restricting the search to simpler family of conditional flows, where each path is defined by its fixed endpoints (x0, x1) drawn from coupling π0,1 of the source and target distributions. Within this conditional flow family, the canonical linear interpolation emerges as the optimal choice, as formalized below. 7.5. Is Diffusion Models ODE an Optimal Map to OT Problem? 221 Proposition 7.5.1: An Upper Bound on OT Kinetic Energy via Conditional Flows Let π0,1 be any coupling between psrc and ptgt. (1) The kinetic energy is bounded above by the expected path energy of any conditional flow Ψt(x0, x1) that connects the endpoints: (psrc, ptgt) E(x0,x1)π0,1 (cid:20)Z 1 0 Ψ t(x0, x1)2 dt (cid:21) . (2) The unique conditional flow Ψ that minimizes the upper bound on the right-hand side is the linear interpolation path: Ψ (x0, x1) = (1 t)x0 + tx1. Substituting this optimal path yields the tightest version of the bound: (psrc, ptgt) E(x0,x1)π0,1x1 x02. Proof for Proposition. The proof relies on straightforward application of Jensens inequality and the tower property of conditional expectations, before solving the simplified variational problem with the Euler-Lagrange equation; we refer to (Lipman et al., 2024)s Section 4.7 for the complete argument. In other words, the linear interpolation Ψ (i.e., the forward kernel used by Flow Matching and Rectified Flow) minimizes an upper bound on the true kinetic energy for any chosen coupling π0,1. We emphasize that optimality within this class of conditional flows does not guarantee global optimality on the marginal distributions. Reflow and Optimal Transport. The most naive transport plan between two distributions is to connect their samples with straight lines using simple independent coupling. However, this approach is demonstrably not optimal, as the failure lies not in the straight-line paths themselves, but in the inefficient initial pairing of points. The Reflow procedure may offer constructive response. It is an iterative algorithm designed specifically to correct this pairing, and crucially, each step is guaranteed to be cost-non-increasing (Liu, Gong, et al., 2022). This property suggests Reflow systematically pushes the transport plan towards 222 (Optional) Diffusion Models and Optimal Transport more optimal configuration, which naturally motivates the central question of its convergence. Question 7.5.2 What happens if we apply the Rectify operator iteratively? Can the resulting sequence of transport plans converge to the optimal one, or does the fixed point of the Reflow process yield the OT map? The short answer is no in general. Below, we explain what may go wrong. To recall, the Reflow procedure iteratively refines the coupling between psrc and ptgt via the update: π(k+1) = Rectify(π(k)), initialized with the product coupling π(0) := psrc(x0)ptgt(x1). More precisely, Rectify output the updated coupling π(k+1) via the following: At each iteration = 0, 1, 2, . . ., velocity field v(k) is learned via: v(k) arg min ut L(ut (cid:12) (cid:12)π(k)), (cid:12) (cid:12)π(k)) is the loss (e.g., RF or FM loss) defined in Equation (5.4.1). where L(ut Here, for notational simplicity, we adopt non-parametric formulation for the velocity field, rather than parameterized form ϕ employed in other contexts. The updated coupling is then given by: π(k+1)(x0, x1) := psrc(x0) δ(cid:0)x1 Ψ(k) 1 (x0)(cid:1), where Ψ(k) v(k) 1 denotes the solution map at time = 1 obtained by integrating from initial condition x0. It has been observed in (Liu, Gong, et al., 2022) that for coupling π between psrc and ptgt, the existence of velocity field vt that minimizes the Reflow loss, that is, satisfies L(vtπ) = 0, does not necessarily imply that the transport is optimal. Motivated by the BenamouBrenier framework, where the optimal transport velocity is known to be the gradient of potential function, Liu (2022) proposed an additional constraint: the velocity field vt should be potential field. Accordingly, the objective in Equation (5.4.1) is modified to restrict vt to the space of gradient vector fields, also known as potential flows: w(k) arg min ut: ut=φ for some φ : RDR L(ut (cid:12) (cid:12)π(k)), (7.5.2) 7.5. Is Diffusion Models ODE an Optimal Map to OT Problem? 223 with the rest of the procedure remaining the same as in Rectify. We denote this associated operator as Rectify, emphasizing the projection onto irrotational vector fields. Let π be coupling between psrc and ptgt. Liu, Gong, et al. (2022) conjecture the following equivalence characterizing optimality: (i) π is an optimal transport coupling. (ii) π is fixed point of the potential rectification operator: π = Rectify(π). (iii) There exists gradient velocity field vt = φt such that the rectify loss vanishes: L(vtπ) = 0. However, Hertrich et al. (2025) exhibit two types of counterexamples: 1. When the intermediate distributions pt have disconnected support, one can find fixed points of Rectify with zero Reflow loss and gradient velocity fields that nonetheless fail to produce the optimal coupling. 2. Even when both endpoint distributions are Gaussian, there exist couplings whose loss is arbitrarily small but whose deviation from the optimal coupling is arbitrarily large. Therefore, while rectified flows may yield strong generative models, their reliability as optimal transport solvers remains limited. This highlights an important gap between generative modeling and principled optimal transport theory, inviting further research at their intersection. Finally, we note that transport cost does not always correlate with downstream performance; as such, computing the exact optimal transport map may not necessarily lead to better practical outcomes. Nonetheless, variants of optimal transport remain fundamental to many problems in science and engineering. Diffusion models offer powerful framework for exploring these challenges."
        },
        {
            "title": "Sampling of Diffusion Models",
            "content": "225 Chapter 4 Generation with Diffusion Model v(x, t) Solve the ODE backward from to 0 with x(T ) pprior (more generally, from to with > t): dx(t) dt = v(x(t), t) x(0) = x(T ) + 0 v(x(t), t) dt Steering Generation x(0) = x(T ) + [v(x(t), t) 0 +Guidance] dt Fast Generation with Numerical Solvers Learning Fast Diffusion-Based Generator x(0) = x(T ) + 0 v(x(t), t) dt x(0) = x(T ) + v(x(t), t) dt Estimating the Integration Learning the Integration Chapter 8 Chapter 9 Chapter 10 and Chapter"
        },
        {
            "title": "Guidance and Controllable Generation",
            "content": "Diffusion models are powerful generative frameworks. In the unconditional setting, the goal is to learn pdata(x) and generate samples without external input. Many applications, however, require conditional generation, where outputs satisfy user-specified criteria. This can be achieved by steering an unconditional model or directly learning the conditional distribution p0(xc), with condition (e.g., label, text description, or sketch) guiding the process. This chapter builds on principled view of the conditional score, which decomposes into an unconditional direction and guidance direction that nudges samples toward the condition while preserving realism. We explain why guidance is essential, show how the conditional score serves as unifying interface for control, and survey ways to approximate the guidance term. We then distinguish control (meeting the condition) from alignment (meeting human preference under the condition), and describe how preferences can be incorporated into the same framework. Finally, we discuss direct optimization of preference without additional reward models (i.e., learned scorer that assigns higher values to outputs better aligned with human preference). 226 8.1. Prologue 8.1 Prologue 227 Figure 8.1: Illustration of steered diffusion sampling. Reverse-time PF-ODE sampling begins from pure noise at the right (t = ) and gradually evolves toward clean sample at the left (t = 0). During this process, guidance directions xt log pt(cxt), weighted by wt, modify the velocity field according to xt log pt(xt) + wt xt log pt(cxt). These additional directions steer the trajectory toward the desired attribute (Japanese painting style) while the sample is progressively refined from coarse to fine detail. The generation process of diffusion models proceeds in coarse-to-fine manner, providing flexible framework for controllable generation. At each step, small amount of noise is removed and the sample becomes clearer, gradually revealing more structure and detail. This property enables control over the generation process: by adding guidance term to the learned, timedependent velocity field, we can steer the generative trajectory to reflect user intent. principled foundation for guidance-based sampling in diffusion models is the Bayesian decomposition of the conditional score. For each noise level t, xt log pt(xtc) = xt log pt(xt) } {z unconditional direction + xt log pt(cxt) } {z guidance direction . (8.1.1) This identity shows that conditional sampling can be implemented by adding guidance term xt log pt(cxt) on top of the unconditional score. wide 228 Guidance and Controllable Generation range of controllable generation methods (e.g., classifier guidance (Dhariwal and Nichol, 2021), general training-free guidance (Ye et al., 2024)) can be interpreted as different approximations of this guidance term, since pt(cxt) is generally intractable due to marginalization over x0. Once such an approximation is available, sampling simply replaces the unconditional score with its conditional counterpart. Using Equation (8.1.1), the PF-ODE becomes dx(t) dt = (t)x(t) = (t)x(t) 1 2 1 2 g2(t) xt log pt(x(t)c) } {z conditional score g2(t) xt log pt(x(t)) + xt log pt(cx(t)) . (8.1.2) We highlight that steering these time-dependent vector fields fundamentally relies on their linearity, so the discussion below, formulated in score prediction, naturally extends to x-, ϵ-, and v-prediction through their linear relationships as in Equation (6.3.1). Instantiations of the Guidance Direction. 1. Classifier Guidance (CG). In Section 8.2, classifier guidance (CG) trains time-conditional classifier pψ(cxt, t) on noised data xt (obtained by corrupting clean labeled samples at level t). At sampling time, its input gradient provides the guidance term: xt log pψ(cxt, t) xt log pt(cxt), which is then added to the unconditional score (Dhariwal and Nichol, 2021). 2. Classifier-Free Guidance (CFG). In Section 8.3, CFG directly trains single conditional model sϕ(xt, t, c) xt log pt(xtc), where the unconditional model is learned jointly by randomly replacing the condition with special null token for fraction of the training steps. 3. Training-Free (Surrogate) Guidance. The conditional pt(cxt) is generally intractable because it requires marginalizing over the clean latent x0: pt(cxt) = p(cx0)p(x0xt)dx0, and, in typical applications, at least one of these factors is unknown, making the integral intractable. 8.1. Prologue 229 In Section 8.4.1, training-free (loss-based) guidance avoids evaluating the conditional likelihood pt(cxt) directly. Instead, it introduces an off-the-shelf loss ℓ(xt, c; t) and defines surrogate conditional distribution ept(cxt) as, ept(cxt) exp(cid:0) τ ℓ(xt, c; t)(cid:1), τ > 0, which acts as pseudo-likelihood. This formulation sidesteps the intractability of computing the true conditional likelihood while still enabling guidance through gradients of the chosen loss. Its conditional score is computed solely by the gradient of the loss with τ : xt log ept(cxt) = τ xtℓ(xt, c; t). This term is added to the unconditional score with guidance weight wt: xt log pt(xt) + wt (cid:2)τ xtℓ(xt, c; t)(cid:3). which is exactly the score of the tilted density eptilt (xtc) defined as: (xtc) pt(xt)ept(cxt)wt pt(xt) exp(cid:0) wtτ ℓ(xt, c; t)(cid:1). eptilt In practice, we replace the conditional score of sampling in Equation (8.1.2) with this tilted score, and solving the resulting ODE to draw samples. In view of this, classifier guidance is simply surrogate guidance with learned classifier ept(cxt) := pψ(cxt, t) via: ℓ(xt, c; t) = log pψ(cxt, t), τ = 1. The effect of guidance on the sampling trajectory is illustrated in Figure 8.1. All of these techniques can likewise be applied on top of conditional model, allowing extra control signals to be injected during generation. Remark. Guided PF-ODE does not sample from the tilted family (in general). Even with exact scores and exact ODE integration, replacing the score by the tilted score does not make the timet marginals equal to {eptilt (c)}t[0,1], nor the terminal law equal to eptilt 0 (c). Define vorig = 1 2 g2(t) log pt, ht(x) = ewtτ ℓ(x,c;t), = ptht eptilt Zt . 230 Guidance and Controllable Generation The guided PF-ODE uses vtilt = 1 2 g2(t) log eptilt = vorig 1 2 g2(t) log ht. If eptilt were the true marginals, they would satisfy eptilt + (eptilt vtilt ) = 0. But direct calculation gives the residual vtilt + (eptilt eptilt t log ht + vorig =eptilt ) log ht 1 2 g2(t)(cid:0) log ht + log ht2(cid:1) Zt . Thus eptilt bracket vanishes for all x, i.e. coincides with the PF-ODE marginals if and only if this log ht + vorig log ht = 1 2 g2(t)(cid:0) log ht + log ht2(cid:1) + Zt . This condition holds trivially when ωt 0 (unconditional generation), but almost never for ht(x) = ewtτ ℓ(x,c;t), except in very special cases of wt or ℓ. } are not the PF-ODE marginals, and terminal Therefore, in general, {eptilt samples are not distributed as eptilt 0 (x0c). From Control to Better Alignment with Direct Preference Optimization. Strong control can be on-condition but off-preference: sample may satisfy the conditioning signal (e.g., the prompt) yet deviate from what humans actually prefer. We formalize this by tilting the conditional target by preference rating1: 0 (x0c) p0(x0c) exp (cid:0)βr(x0, c)(cid:1), eptilt where r(x0, c) is scalar alignment rating (reward) for clean sample x0 and condition (larger indicates better alignment). In practice, may be (i) the logit or log-probability of an external reward/classifier, (ii) similarity measure (e.g., CLIP/perceptual (Radford et al., 2021)), or (iii) learned preference model. Existing methods for achieving such steerability typically collect human labels of the relative quality of model generations and fine-tune the conditional diffusion model to align with these preferences, often through reinforcement learning from human feedback (RLHF). However, RLHF is complex and often 1We remark that the training-free guidance can also be viewed in the same framework of finding tilted distribution with guidance of loss ℓ(xt, c, t) 8.1. Prologue 231 unstable procedure: it first fits reward model to capture human preferences, and then fine-tunes the conditional diffusion model with reinforcement learning to maximize this estimated reward while constraining policy drift from the original model. This naturally raises the question: can we remove the reward model training stage altogether? We address this with Diffusion-DPO (Wallace et al., 2024), an adaptation of Direct Preference Optimization (Rafailov et al., 2023) originally developed for large language models. As described in Section 8.5, DiffusionDPO learns the preference tilt directly from pairwise choices, so the conditional diffusion model is fine-tuned to align to preferences without separate reward model. 232 Guidance and Controllable Generation 8.2 Classifier Guidance 8.2.1 Foundation of Classifier Guidance Let denote conditioning variable drawn from distribution p(c), such as class label, caption, or other auxiliary information. Our goal is to draw samples from p0(xc). In diffusion-based conditional generation, we realize this goal by running the reverse-time dynamics whose time marginals are pt(c). The drift of these dynamics depends on the conditional score xt log pt(xtc), [0, ]. Hence standard and effective route2 is to estimate this quantity. fundamental insight, based on Bayes rule, is that the conditional score can be decomposed as: xt log pt(xtc) = xt log (cid:18) pt(xt)pt(cxt) p(c) (cid:19) = xt log pt(xt) + xt log pt(cxt) xt log p(c) = xt log pt(xt) } {z unconditional score , + xt log pt(cxt) } {z classifier gradient (8.2.1) where pt(cxt) indicates probability of conditioned on xt which predicts the condition from the noisy input xt at time t. This decomposition3 motivates the Classifier Guidance (CG) approach proposed by Dhariwal and Nichol (2021), which leverages pre-trained timedependent classifier pt(cxt) to steer the generation process. Specifically, we define one-parameter family of guided densities (tilted conditionals) with guidance scale ω 0: pt(xtc, ω) pt(xt)pt(cxt)ω, (8.2.2) which yields the score function: xt log pt(xtc, ω) = xt log pt(xt) + ωxt log pt(cxt). (8.2.3) Geometrically, this tilts the unconditional flow in the direction that increases the class likelihood. When ω = 1, pt(xtc, ω) coincides with the true conditional 2One could in principle obtain p0(xc) from an unconditional generator via rejection or importance sampling if p(cx) were available and well calibrated. This is rarely practical for high-dimensional or rare conditions. 3In the last identity, since xt log p(c) does not depend on xt, it vanishes under differentiation. 8.2. Classifier Guidance 233 pt(xtc); for ω = 1, it is guided (tempered) reweighting rather than the literal conditional. The scalar ω 0 modulates the influence of the classifier: ω = 1: recovers the true conditional score xt log pt(xtc). ω > 1: amplifies the classifier signal, typically increasing conditional fidelity (often at the expense of diversity). 0 ω < 1: down-weights the classifier signal, typically increasing sample diversity while weakening conditioning. Practical Approximation in CG. In practice, CG is training-free method (w.r.t. the diffusion model) for steering pre-trained unconditional diffusion model, sϕ(xt, t) xt log pt(xt). CG is applied only at sampling time, without modifying the diffusion model itself. To enable this, time-dependent classifier pψ(cxt, t) is trained separately to predict the condition from noisy inputs xt at different noise levels t. The classifier is trained in standard way by minimizing the cross-entropy loss: EtU[0,T ],(x,c)pdata,ϵN (0,I) , log pψ(cxt, t) (8.2.4) where (x, c) pdata denotes paired labeled data, and xt = αtx + σtϵ is the noisy input at time t. The classifier must be explicitly conditioned on (e.g., via time embeddings), since it is expected to operate reliably across all noise levels. After training, the classifier provides scores that serves as surrogate for the true likelihood gradient: xt log pψ(cxt, t) xt log pt(cxt). 8.2.2 Inference with CG At inference time, the classifier gradient xt log pψ(cxt, t) is added to the unconditional score function and scaled by guidance weight ω, yielding an approximation to the guided score xt log pt(xtc, ω) from Equation (8.2.3): sCG(xt, t, c; ω) := sϕ(xt, t) } {z uncond. direction + ω xt log pψ(cxt, t) } {z guidance direction xt log pt(xtc, ω). 234 Guidance and Controllable Generation Accordingly, one simply replaces the unconditional score function sϕ(xt, t) in the reverse-time SDE or PF-ODE with the guided score sCG(xt, t, c; ω) for specified ω as in Equation (8.1.2), thereby steering the generative trajectory toward samples that align with the condition c. 8.2.3 Advantages and Limitations CG provides simple and flexible mechanism for conditional generation, allowing for explicit control over the strength of conditioning via ω. It can be used with any pre-trained unconditional diffusion model, requiring only an additional classifier for conditioning. However, the approach has notable limitations: Training Cost: The classifier must be trained to operate across all noise levels, which is computationally expensive. Robustness: Classifiers must generalize well to severely corrupted inputs xt, especially for large t, which can be challenging. Separate Training: Since the classifier is trained independently of the diffusion model, it may not align perfectly with the learned data distribution. 8.3. Classifier-Free Guidance 235 8.3 Classifier-Free Guidance 8.3.1 Foundation of Classifier-Free Guidance Figure 8.2: Illustration of CFG. The adjusted score xt log pt(xtc, ω) is obtained as linear interpolation between the unconditional score xt log pt(xt) and the conditional score xt log pt(xtc), weighted by ω. The resulting direction steers samples from the prior toward modes of the data distribution consistent with the target condition. Classifier-free guidance (CFG) (Ho and Salimans, 2021) is simplified approach to classifier-based guidance that eliminates the need for separate classifier. The key idea is to modify the gradient of the score function in way that allows for effective conditioning without explicit classifiers. Specifically, the gradient of the log-probability of the conditional distribution is adjusted as follows: xt log pt(cxt) = xt log pt(xtc) xt log pt(xt). (8.3.1) Substituting this expression into Equation (8.2.3) yields the following formulation for the conditioned score: xt log pt(xtc, ω) = xt log pt(xt) + ω (xt log pt(xtc) xt log pt(xt)) = ω xt log pt(xtc) } {z conditional score +(1 ω) xt log pt(xt) } {z unconditional score . (8.3.2) The hyperparameter ω again plays critical role in controlling the influence of the conditioning information (we take ω 0): At ω = 0, the model behaves as an unconditional diffusion model, completely ignoring the conditioning. At ω = 1, the model uses the conditional score without additional guidance. 236 Guidance and Controllable Generation For ω > 1, the model places more emphasis on the conditional score and less on the unconditional score, strengthening alignment with but typically reducing diversity. 8.3.2 Training and Sampling of CFG Joint Training of Unconditional and Conditional Diffusion Models via CFG. Unlike CG, CFG requires retraining diffusion model that explicitly accounts for the conditioning variable c. Training two separate models for the conditional and unconditional score functions, however, is often computationally prohibitive. To address this, CFG adopts single model sϕ(xt, t; c) that learns both score functions within single model by treating as an additional input. The training procedure is defined as follows: For unconditional training, null token is passed in place of the conditioning input, yielding sϕ(xt, t, ). For conditional training, the true conditioning variable is provided as input, resulting in sϕ(xt, t, c). These two training regimes are unified by randomly replacing with the null input with probability puncond (a user-defined hyperparameter typically set to 0.1). This joint training strategy enables the model to simultaneously learn both conditional and unconditional score functions. The full training algorithm is presented in Algorithm 4, alongside comparison to standard unconditional training shown in Algorithm 3. We remark that during training, the CFG weight ω is not utilized. Algorithm 3 Uncond. DM 1: Repeat 2: 3: 4: 5: 6: pdata(x) U[0, ] ϵ (0, I) xt = αtx + σtϵ Take gradient step on: ϕ sϕ(xt, t) s2 7: until converged Algorithm 4 CFG for Cond. DM Input: puncond: prob. of unconditional dropout 1: Repeat 2: 3: 4: 5: 6: 7: (x, c) pdata(x, c) with prob. puncond U[0, ] ϵ (0, I) xt = αtx + σtϵ Take gradient step on: ϕ sϕ(xt, t, c) s2 8: until converged 8.3. Classifier-Free Guidance 237 Conditioned Sampling with CFG. Once the model sϕ(xt, t, c) is trained using Algorithm 4, the CFG can be applied during sampling. The gradient of the log-probability is given by: xt log pt(xtc, ω) = ωxt log pt(xtc) + (1 ω)xt log pt(xt) +(1 ω) ω sϕ(xt, t, c) } {z conditional score ϕ (xt, t, c; ω). =: sCFG sϕ(xt, t, ) } {z unconditional score (8.3.3) During sampling, fixed (or optionally time-dependent) classifier-free guidance weight ω is applied. The unconditional score xt log pt(xt) in the reverse-time SDE (Equation (4.1.6)) or PF-ODE (Equation (4.1.8)) is then replaced by the guided score sCFG ϕ (xt, t, c; ω) as in Equation (8.1.2), which combines the conditional and unconditional scores in weighted manner. This formulation enables controllable generation by adjusting ω, allowing samples to be guided toward the conditioning signal while retaining diversity. CFG thus offers an effective and computationally efficient way to achieve precise conditional generation, as it requires training only single diffusion model. 238 Guidance and Controllable Generation 8.4 (Optional) Training-Free Guidance In this section, we present the high-level philosophy underlying wide range of training-free guidance methods (Chung et al., 2023; Ye et al., 2024; He et al., 2024; Bansal et al., 2023). Despite variations in implementation and application, these methods are unified by the central principle expressed in Equation (8.1.1). We first introduce the high-level approach of training-free guidance in Section 8.4.1 and then extend this idea to training-free inverse problem solving, with brief overview provided in Section 8.4.2. Setup and Notations. Let denote conditioning variable. We assume access to pre-trained diffusion model sϕ(xt, t) expressed in score prediction4. In addition, suppose we are given non-negative function ℓ(, c) : RD R0 that quantifies how well sample RD aligns with the condition c, where smaller values of ℓ(x, c) indicate stronger alignment. Concrete examples of such function include: (i) is reference image, and ℓ(, c) is similarity score measuring perceptual closeness; (ii) ℓ(, c) is feature-based similarity score computed via pre-trained model such as CLIP (Radford et al., 2021). Consider the standard linearGaussian forward noising kernel pt(x0) := I(cid:1). We recall the DDIM update in Equation (9.2.3) and take it (cid:0); αtx0, σ2 as an example: xtt1 = αt1 ˆx0(xt) {z } in data space σt1σt ˆs(xt) {z } in noise space , (8.4.1) where ˆx0(xt) := xϕ(xt, t) is the (clean) x-prediction, and ˆs(xt) := sϕ(xt, t) as the score-prediction from xt at time level t. 8.4.1 Conceptual Framework for Training-Free Guidance Most training-free guidance methods (Ye et al., 2024) introduce corrections either in the data space or the noise space to steer the DDIM update in Equation (8.4.2) toward satisfying the condition c: xtt1 = αt1 (cid:16) ˆx0(xt) + ηdata {z A. data space G0 (cid:17) } σt1σt (cid:16) ˆs(xt) + ηlatent {z B. noise space Gt (cid:17) , } (8.4.2) 4Here, we adopt the score and x-prediction parameterization for simplicity of mathematical expression; other parameterizations (e.g., ϵ-prediction) can be handled analogously. 8.4. (Optional) Training-Free Guidance 239 where ηdata , ηlatent correction terms defined below. 0 are time-dependent guidance strengths, and G0, Gt are A. Guidance in Data Space. By descending along the negative gradient direction the modified clean estimate in data space, G0 := x0ℓ(x0, c), ˆx0(xt) + ηdata G0, can be gradually steered toward samples that better satisfy the condition c. This gradient-descent scheme can be applied iteratively to progressively improve alignment. Representative examples include MGPD (He et al., 2023) and UGD (Bansal et al., 2023). B. Guidance in Noise Space. As discussed in Section 8.1, the conditional score xt log pt(cxt) is generally intractable. practical approximation is to introduce surrogate likelihood ept(cxt): ept(cxt) exp (cid:0) ηℓ (ˆx0(xt), c) (cid:1) with re-scaling constant η > 0 so that xt log ept(cxt) = ηxtℓ (ˆx0(xt), c) =: Gt, where ˆx0(xt) is obtained via the diffusion models prediction. Plugging this into the Bayes rule for conditional scores yields the proxy: xt log pt(xtc) xt log pt(xt) } {z unconditional ˆs(xt) + ηlatent + xt log ept(cxt) {z } guidance Gt, which serves as the correction with the guidance in the noise spaces. However, we note that evaluating Gt requires backpropagation through the x-prediction, i.e., xt ˆx0(xt) x0 log ℓc(x0)x0=ˆx0(xt) , which may result in substantial computational cost in practice. Representative examples include (Yu et al., 2023; Chung et al., 2022; Bansal et al., 2023). 240 Guidance and Controllable Generation 8.4.2 Examples of Training-Free Approaches to Inverse Problems The principle introduced in Section 8.4.1 has important applications in inverse problems. We begin with an overview of the background and then provide several concrete examples illustrating how to leverage pre-trained diffusion models for inference-time inverse problem solving. Background on Inverse Problems. Let be corruption operator (which may be linear or nonlinear, known or unknown), such as blurring kernel or inpainting, and let be an observation generated by the following corruption model: = A(x0) + σyz, (0, I). (8.4.3) The objective of inverse problems is to sample from the posterior distribution p0(x0y), where there may exist infinitely many possible reconstructions x0 corresponding to the given observation y. The goal is to recover an x0 that removes the corruptions in while preserving its faithful and semantic features. Traditional approaches to solving inverse problems typically follow supervised framework, which requires collecting paired data of corrupted and restored samples (y, x) and relies on optimization methods or supervised training of neural networks. Such approaches can be costly in terms of data preparation and may lack generalization to unseen data. Pre-Trained Diffusion Models as Inverse Problems Solvers. As previously shown, the conditional score can be decomposed via Bayes rule: xt log pt(xty) = xt log pt(xt) } {z data score + xt log pt(yxt) } {z measurement alignment . (8.4.4) This decomposition separates the data score and measurement alignment term with specific to the inverse problem. It enables solving Equation (8.4.3) in an unsupervised manner by modeling the clean data distribution pdata and applying it during inversion. More specifically: Data score xt log pt(xt): Approximated using pre-trained diffusion model sϕ(xt, t) trained on clean data. Measurement alignment xt log pt(yxt): Intractable in closed form, as it involves marginalizing over latent variables. 8.4. (Optional) Training-Free Guidance 241 Consequently, most training-free approaches using pre-trained diffusion models focus on approximating xt log pt(yxt). We adopt common metaform summarized in (Daras et al., 2024): xt log pt(yxt) Pt Mt γt . Here: Mt: error vector quantifying the mismatch between the observation and the estimated signal, Pt: mapping that projects Mt back to the ambient space of xt, γt: scalar controlling the guidance strength. Representative methods instantiate Mt, Pt, and γt differently, as highlighted below with color-coded components. Instantiations of Diffusion-Based Inverse Problem Solvers. We present representative methods that leverage pre-trained diffusion model to provide unsupervised approaches (requiring no paired data) that can be flexibly applied to various inverse problems using the same learned proxy for pdata. Score SDE (Song et al., 2020c). One of the earliest works on diffusionbased inverse problem solvers. It considers known linear corruption model and focuses on the noiseless setting with σy = 0. Since is linear, one can form noise-levelmatched observation yt := αty + σtϵ, and use the residual yt Axt (note: yt = Axt in general) to drive likelihoodstyle correction. common approximation (dropping the multiplicative constant) is xt log pt(yxt) ( yt Axt ). Iterative Latent Variable Refinement (ILVR) (Choi et al., 2021). Using the same setup as ScoreSDEs case, ILVR estimates: xt log pt(yxt) A(yt Axt) = (AA)1A ( yt Axt ), where is the MoorePenrose pseudoinverse, and yt = αty + σtϵt. 242 Guidance and Controllable Generation Diffusion Posterior Sampling (DPS) (Chung et al., 2022). widely used method for inverse problems with known nonlinear forward operator and additive Gaussian noise level σy 0 is Denoising Posterior Score (DPS), which approximates xt log pt(yxt) xt log pt (cid:0)yX0 = ˆx0(xt)(cid:1), (8.4.5) where ˆx0(xt) := E[x0xt] denotes the conditional mean of the clean sample given the noisy observation xt at time t, and is typically estimated using Tweedies formula (Equation (3.3.6)) from pre-trained diffusion model. This one-point approximation assumes that the conditional distribution p(x0xt) is sharply concentrated, and follows from: Z pt(yxt) = = pt(yxt, x0)p(x0xt)dx0 pt(yx0)p(x0xt)dx0 pt (cid:0)yX0 = ˆx0(xt)(cid:1), where we have used that depends only on x0 (not on xt) given x0, and the approximation holds under the assumption that the posterior p(x0xt) is tightly peaked around its mean. Since we compute pt (cid:0)yX0 = ˆx0(xt)(cid:1) = (cid:0)y; A(ˆx0(xt)), σ2 yI(cid:1), xt log pt(yxt) xt log (cid:0)y; A(ˆx0), σ2 (cid:13)y A(ˆx0)(cid:13) (cid:13) 2 (cid:13) = xt yI(cid:1) 1 2σ2 1 (cid:2)JA σ2 = (cid:0)ˆx0(xt)(cid:1) xt ˆx0(xt)(cid:3) (cid:0)y A(cid:0)ˆx0(xt)(cid:1)(cid:1), (cid:0)ˆx0(xt)(cid:1) := x0A(x)(cid:12) where JA (cid:12)x=ˆx0(xt) denotes the Jacobian of the forward operator with respect to its input. This formula propagates the gradient through the score approximation pipeline, reflecting how the measurement likelihood changes with respect to perturbations in the noisy sample xt. For linear inverse problems, this further simplifies to: xt log pt(yxt) 1 σ2 [A xt ˆx0(xt)] (cid:16) A(ˆx0(xt)) (cid:17) . large body of work explores diffusion-based inverse problem solvers by proposing various approximations for xt log pt(yxt). For comprehensive overview, we refer readers to the survey by Daras et al. (2024). 8.5. From Reinforcement Learning to Direct Preference Optimization for Model 243 Alignment 8.5 From Reinforcement Learning to Direct Preference Optimization for Model Alignment In the pursuit of aligning generative models with human intent, the prevailing paradigm has been Reinforcement Learning from Human Feedback (RLHF). While effective, RLHF is complex, multi-stage process that can be unstable. This section introduces Direct Preference Optimization (DPO) (Rafailov et al., 2023), more streamlined and stable method that reaches the same goal without explicit reward modeling or reinforcement learning. We then outline its extension to diffusion models via Diffusion-DPO (Wallace et al., 2024). 8.5.1 The Motivation: Circumventing the Pitfalls of RLHF The goal of alignment is to steer base, pre-trained model (e.g., an SFT model) toward outputs that humans prefer. RLHF proceeds in three stages. First, supervised fine-tuning (SFT) trains base model on promptresponse pairs. Second, reward modeling (RM) fits model on preference data consisting of prompts and paired responses (a preferred winner xw and dispreferred loser xl), learning scalar r(c, x) with r(c, xw) > r(c, xl). Third, RL fine-tuning optimizes the SFT model (policy π5) with an algorithm such as PPO (Schulman et al., 2017), maximizing expected reward from while regularizing by KL penalty that keeps π close to the reference/SFT distribution. Despite its impact, this pipeline suffers from drawbacks: the RL stage is unstable and computationally expensive because it is on-policyeach update requires freshly generated samples from the current model; it also requires training and hosting multiple large models (SFT, reward, and sometimes value model); and it optimizes only proxy for human preferences, so flaws in the reward model can be exploited. This motivates central question: Question 8.5.1 Can we eliminate explicit reward modeling and the unstable RL step, directly optimizing the model on preference data? Direct Preference Optimization (DPO) streamlines alignment by replacing the multi-stage RLHF pipeline with single, supervised-style step. Instead of training separate reward model and running unstable RL algorithms like PPO, DPO directly fits the policy to preference pairs using simple logistic loss, while staying close to fixed reference model. The key insight is that the 5A policy maps prompt/history (state) to distribution over responses/actions. 244 Guidance and Controllable Generation KL-regularized RLHF objective can be rewritten so that the log-likelihood ratio between the policy and the reference acts as an implicit reward. This preserves the same regularization toward the reference policy but avoids costly rollouts and explicit reward modeling. In Section 8.5.2, we briefly review the RLHF pipeline and its reliance on large reward models and RL fine-tuning. In Section 8.5.3, we present DPO, originally proposed for language models, which circumvents reward model training and simplifies alignment fine-tuning. Finally, in Section 8.5.4, we extend this idea to diffusion models, introducing Diffusion-DPO as practical and stable alignment method in the generative modeling setting. 8.5.2 RLHF: BradleyTerry View Short Introduction to RLHF. RLHF begins with learned judge: reward model rψ that assigns scalar preference score to candidate responses for the same prompt c. The dataset consists of pairs (x, x) annotated with label indicating whether is preferred over x. The label can be binary {0, 1} or soft value [0, 1] obtained by aggregating multiple raters. The training objective is simple logistic loss LRM(ψ) = E(c,x,x,y)D y log σ(cid:0)rψ(c, x) rψ(c, x)(cid:1) + (1 y) log (cid:0)1 σ(cid:0)rψ(c, x) rψ(c, x)(cid:1)(cid:1)i , (8.5.1) where σ(u) = 1/(1+eu). In practice, preference pairs in may originate from various sources: curated responses, model snapshots at different checkpoints, or generations from pre-trained conditional diffusion model. standard convention is to store them in an ordered format (winner, loser). Under this convention we simply set = 1, and Equation (8.5.1) reduces to the special case (with = xw and = xl): LRM(ψ) = E(c,xw,xl)D log σ(cid:0)rψ(c, xw) rψ(c, xl)(cid:1)i . (8.5.2) BradleyTerry View and KL Connection. It is standard to interpret prψ (x xc) := σ(cid:0)rψ(c, x) rψ(c, x)(cid:1) through the BradleyTerry (BT) model (Bradley and Terry, 1952), which converts two scalar scores into win probability. This formulation highlights two key properties: (i) only the difference of scores matters (so rψ(c, ) is 8.5. From Reinforcement Learning to Direct Preference Optimization for Model 245 Alignment shift-invariant), and (ii) the loss pushes the predicted winners score above the losers score. To see (ii) intuitively, consider one pair with label {0, 1} and define := rψ(c, x) rψ(c, x), := σ(r), σ(u) = 1 1+eu . The per-example logistic loss is ℓ = (cid:2)y log + (1 y) log(1 p)(cid:3). Then ℓ = σ(r) y. Under gradient descent with step size η > 0, the score gap updates as r η(cid:0)σ(r) y(cid:1). Hence, if = 1 (x wins), then σ(r) 1 0, so increases (winner up, loser down); if = 0, decreases. Each per-example term in Equation (8.5.1) can be viewed as the crossentropy between the observed Bernoulli label and the models predicted win probability: (cid:2)y log prψ + (1 y) log(1 prψ )(cid:3) = DKL (cid:0)Bern(y)(cid:13) (cid:13)Bern(prψ )(cid:1) + H(cid:0)Bern(y)(cid:1), where is the entropy of the target Bernoulli distribution. Averaging over the dataset gives LRM(ψ) = ED DKL (cid:16) Bern(y)(cid:13) (cid:13)Bern(prψ ) (cid:17) + ED (cid:2)H(Bern(y))(cid:3) } {z independent of ψ . (8.5.3) Thus, minimizing the logistic loss is equivalent to minimizing the KL divergence between the empirical Bernoulli distribution of human labels and the models predicted Bernoulli distribution. In the binary case (y {0, 1}), this equivalence is exact; for soft labels (y [0, 1]), the result holds up to an entropy constant offset. Intuitively, the reward model is trained to adjust its win probabilities until they align with the empirical human win rates observed in the dataset. From this point onward, we adopt the most common convention where stores pairs in an ordered format: (xw, xl, c) D. Under this convention, the label is always = 1, and the loss simplifies to the ordered form given in Equation (8.5.2), which we will use in the following discussion. 246 Guidance and Controllable Generation KL Regularized Policy Optimization (with Fixed Reward). With the fitted reward := rψ trained via Equation (8.5.2), and conditional pre-trained diffusion model pϕ(xc), RLHF then adjusts learnable policy πθ(xc), usually fine-tuned on top of pϕ(xc), toward higher-reward responses. At the same time, the policy is regularized to stay close to reference model, taken as the pre-trained diffusion model πref (xc) := pϕ(xc), using DKL penalty: Ecp(c) hExπθ(c) (cid:2)rψ(c, x)(cid:3) βDKL (cid:0)πθ(c)(cid:13) (cid:13)πref (c)(cid:1)i , (8.5.4) max θ which makes the two forces explicit: seek samples the judge prefers, but stay close to the pre-trained reference. We remark that the reward objective in Equation (8.5.2) uses only labeled pairs and does not require that be generated by the reference model (i,e., the pre-trained conditional diffusion model). While not required, collecting pairs from models close to the intended policy can reduce distribution shift and make the learned reward more reliable in the region where it will be used. In summary, RLHF proceeds in two stages: first fit the reward by minimizing the loss in Equation (8.5.2) (equivalently, the expected binary DKL in Equation (8.5.3)); then optimize the policy π by solving Equation (8.5.4). 8.5. From Reinforcement Learning to Direct Preference Optimization for Model 247 Alignment 8.5.3 DPO Framework The Bridge from RLHF. The KL-regularized policy objective in Equation (8.5.4) has simple closed-form solution for each prompt c, given the fitted reward := rψ, expressed in the following energy-based form (Peters et al., 2010): π(xc) = 1 Z(c) πref (xc) exp(r(c, x)/β), (8.5.5) where πref (xc) := pϕ(xc), and Z(c) is the partition function ensuring π(xc) dx = 1. For smaller β, exp(r/β) becomes sharper, so π concentrates on high reward regions: reward dominates, the policy moves farther from πref , diversity decreases, and training may become unstable or prone to reward hacking. For larger β, exp(r/β) flattens, keeping π closer to πref : the KL term dominates, updates are conservative, diversity follows the reference, but reward gains are limited. Since our aim is to fine-tune the policy directly (without training separate reward model), Equation (8.5.5) lets us define an implicit reward from any policy. We introduce below: Defining an Implicit Reward Motivated by Inverting Equation (8.5.5). Equation (8.5.5) suggests an immediate inversion: for any policy π (with support contained in πref ), define rπ(c, x) = β log π(xc) πref (xc) + β log Z(c). (8.5.6) Then Equation (8.5.5) holds with π in place of π, i.e., π would be the optimizer of Equation (8.5.4) for the reward function rπ. In this sense, rπ is an implicit (policy-induced) reward: it is identified up to the prompt-dependent constant β log Z(c), which vanishes in any pairwise comparison such as in the BT model: rπ(c, xw) rπ(c, xl) = β (cid:16) log π(xwc) πref (xwc) log π(xlc) πref (xlc) (cid:17) . This cancellation is exactly what makes the constant irrelevant for preference learning and leads directly to the DPO loss on log-probability differences. DPOs Training Loss. Plug the implicit reward Equation (8.5.6) into the BT model of Equation (8.5.2) for labeled pair (xw, xl) under the same prompt Guidance and Controllable Generation c. The constants log Z(c) cancel between winner and loser, yielding single logistic-loss objective on log-probability differences: LDPO(θ; πref ) = E(c,xw,xl)D (cid:20) log σ (cid:16) β(cid:0) log πθ(xwc) πref (xwc) log πθ(xlc) πref (xlc) (cid:1)(cid:17)(cid:21) . In words: DPO pushes up the (temperature-scaled) advantage of the winner over the loser, measured as the difference of log-likelihood improvements over the reference: (cid:16) log σ β(cid:2)log-ratio difference of πθ πref at xw vs. xl (cid:3)(cid:17) . This achieves the goal of RLHF in single, stable maximum-likelihoodstyle stage, without training an explicit reward model. 8.5.4 Diffusion-DPO Why Naive DPO Fails for Diffusion Models? Evaluating the sample likelihood πθ(xc) in diffusion models requires the instantaneous change-of-variables formula (divergence of the drift) of ODE solving (see Equation (4.2.7))6, which is computationally intensive. Moreover, differentiating through the entire sampling trajectory can suffer from vanishing or exploding gradients. To avoid these issues, Diffusion-DPO works at the path level. We take the discrete-time diffusion model (e.g., DDPM) as an illustrative example; the continuous-time diffusion model is analogous. Defining Pathwise Implicit Rewards. Let trajectory be x0:T := (xT , . . . , x0) under the reverse-time Markov chain with conditionals π(xt1xt, c). Here, xT denotes sample from the prior (highest noise), and x0 is the clean output in data space. Since generation in diffusion models proceeds along full denoising path, it is natural to extend preferences from final outputs to the entire trajectory. We therefore assign each trajectory reward R(c, x0:T ), which reduces to an endpoint reward if it depends only on x0, but can also capture cumulative effects along the path. We replace the sample-level KL in Equation (8.5.4) by pathwise KL as: max θ Ecp(c) Ex0:T πθ(c)[R(c, x0:T )] βDKL (cid:0)πθ(c)(cid:13) (cid:13)πref (c)(cid:1)i , {z reward over paths } 6In discrete-time diffusion models (e.g., DDPM), evaluating πθ(x0c) requires marginalizing over the latent reverse trajectory x1:T . 8.5. From Reinforcement Learning to Direct Preference Optimization for Model 249 Alignment where πθ(c) and πref (c) are the path distributions. It aims to maximize the reward for reverse process πθ(c), while matching the distribution of the original reference reverse process πref (c). For each prompt c, the optimizer has the simple energy-based form π(x0:T c) = 1 Z(c) πref (x0:T c) exp (cid:0)R(c, x0:T )/β(cid:1), (8.5.7) with Z(c) normalizer. Inverting Equation (8.5.7) motivates the definition of an implicit path reward for any policy π: Rπ(c, x0:T ) := β log π(x0:T c) πref (x0:T c) + β log Z(c), whose constant β log Z(c) is irrelevant for pairwise comparisons. From Pathwise Implicit Rewards to DPO. Apply the BradleyTerry model to paths for labeled pair (xw 0) under the same prompt c, and use the standard logistic log-loss: 0 , xl LDiff-DPO(θ; πref ) := (c,xw 0 ,xl R(c; θ) := Exw 1:T πθ(xw 0)D (cid:2)log σ(cid:0)R(c; θ)(cid:1)(cid:3) , where Rπθ {z winner path expectation (cid:0)c, (xw 0 , xw 0 ,c) 1:T )(cid:1)i } 1:T )(cid:1)i } 0, xl . 1:T πθ(xl xl (cid:0)c, (xl 0,c) Rπθ {z loser path expectation (8.5.8) Here, the expectation Ex1:T πθ(x0,c)[] means: given fixed endpoint x0 (e.g., the winner xw 0 ) from the dataset, we take an expectation over latent denoising trajectories x1:T under the model-induced conditional path distribution (the posterior over reverse-time trajectories) that, with kernels πθ(xt1xt, c), could produce x0. Since these intermediate states are unobserved, we average the path reward over all such trajectories. However, Equation (8.5.8) is impractical for three practical reasons: 1. Endpoint Conditioning Induces an Intractable Path Posterior. The term Eπθ(x1:T x0,c)[] averages over reverse paths constrained to hit x0, whereas the sampler runs xT x0 without this constraint. Conditioning on the endpoint creates diffusion-bridge posterior with generally no closed form and costly sampling. 2. Nested, θ-Coupled Expectations. The loss log σ(R(c; θ)) with = Epathsxw 0 ,c[Rπθ ] pathsxl 0,c[Rπθ ] 250 Guidance and Controllable Generation has both the path joint distribution and the integrand Rπθ depending on θ. Thus θ must differentiate through the sampling distribution, leading to REINFORCE/pathwise couplings and high-variance gradients. 3. Long Chains, Large Sums, and Expensive Backpropagation. In Rπθ (c, x0:T ), computing β [log πθ(x0:T c) log πref (x0:T c)] requires O(T ) per-step log-densities with 102103, for both policy πθ and reference πref , and for both winner/loser paths. Backpropagating through these stochastic chains (or bridge samplers) is memory and compute heavy and can be unstable; repeating this over many samples per pair and across all triplets pushes training beyond practical budgets. Toward Tractable Surrogate for Equation (8.5.8). To make this computable, we apply key mathematical insight. By leveraging properties of diffusion models and applying Jensens inequality, we can optimize tractable upper bound on this loss. This transforms the problem from evaluating an entire paths likelihood to evaluating an expectation over the individual, single-step transitions within the path: Because log σ() is convex, Jensens inequality yields an upper bound by moving the inner expectations outside the log: LDiff-DPO(θ; πref ) (c,xw 0 ,xl 0)D Exw 1:T πθ(xw 1:T πθ(xl xl 0 ,c) 0,c) log σ (cid:16) β(cid:0)R(c, xw 0:T ) R(c, xl 0:T )(cid:1)(cid:17)i . Using the implicit-reward identity Rπθ = β log πθ πref of the constant between winner and loser, the bound becomes +β log Z(c) and cancellation LDiff-DPO(θ; πref ) \" log σ (cid:16) β(cid:0) log πθ(xw πref (xw 0:T c) 0:T c) log πθ(xl πref (xl 0:T c) 0:T c) # (cid:1)(cid:17) . (8.5.9) Tractable Surrogate (Stepwise Form). We now exploit the Markov property of the reverse process to decompose the upper bound of LDiff-DPO. This allows us to express the path-level preference as sum of per-step contributions, converting the intractable pathwise loss into tractable singlestep estimator. The resulting form reduces to DSM-style MSE difference. 8.5. From Reinforcement Learning to Direct Preference Optimization for Model 251 Alignment Concretely, for the reverse chain, πθ(x0:T c) = πθ(xT c) πθ(xt1xt, c), πref (x0:T c) = πref (xT c) t=1 πθ(x0:T c) πref (x0:T c) = πθ(xT c) πref (xT c) t=1 t= πref (xt1xt, c). πθ(xt1xt, c) πref (xt1xt, c) . Hence If the prior at time is the same for both models, πθ(xT c) = πref (xT c), then the first factor equals 1, and taking logs yields log πθ(x0:T c) πref (x0:T c) = t=1 log πθ(xt1xt, c) πref (xt1xt, c) . It follows that the bound in Equation (8.5.9) can be written as LDiff-DPO(θ; πref ) log σ \" # (cid:17) , (cid:16) β t=1 where each per-step contribution is = log πθ(xw πref (xw t1xw t1xw , c) , c) log πθ(xl πref (xl t1xl t1xl t, c) t, c) . To obtain tractable estimator, we apply single step Jensen upper bound: sample U{1, . . . , } (one timestep per training pair) and rescale by . This yields log σ (cid:16) β (cid:17) X t=1 Et (cid:2) log σ(cid:0)βT (cid:1)(cid:3) . Thus the final objective is an expected per-step surrogate, LDiff-DPO(θ; πref ) (cid:2)log σ(cid:0)βT (cid:1)(cid:3) , (c,xw 0 ,xl 0)D tU{1,...,T } which reduces the original pathwise loss to tractable single step upper-bound estimator. For Gaussian reverse conditionals used in diffusion models (take ϵ-prediction as an example), log πθ(xt1xt, c) πref (xt1xt, c) = const λt (cid:16) (cid:13) (cid:13)ˆϵθ(xt, t, c) ϵt {z policy (cid:13) 2 (cid:13) } (cid:13) (cid:13)ˆϵref (xt, t, c) ϵt {z reference (cid:13) 2 (cid:13) } (cid:17) , 252 Guidance and Controllable Generation where λt > 0 absorbs noise schedule factors. Thus each per-time contribution is proportional to an MSE difference (policy vs. reference) at slice t. For notation simplicity, define for any xt: MSE(xt) := (cid:13) 2 (cid:13) (cid:13)ˆϵθ(xt, t, c) ϵ(cid:13) (cid:13) (cid:13)ˆϵref (xt, t, c) ϵ(cid:13) 2. (cid:13) This motivates the following practical surrogate for LDiff-DPO(θ; πref ): LDiff-DPO(θ; πref ) := (c,xw 0 ,xl tU{1,...,T },ϵN (0,I) 0)D w(t)(cid:0)MSE(xw ) MSE(xl t)(cid:1)i , = αtxw where xw reduction, and w(t) > 0 collects the time weighting (e.g., w(t) λt). 0 +σtϵ share the same noise ϵ for variance 0 +σtϵ and xl = αtxl Intuitively, minimizing LDiff-DPO increases the models prediction accuracy on the winner relative to the reference and decreases it on the loser. Because improvements are always measured relative to πref at the same time step, the policy is nudged toward winner-like denoising trajectories and away from loser-like ones, while remaining anchored to the reference. 8.6. Closing Remarks 253 8.6 Closing Remarks This chapter has shifted our focus from foundational principles to the practical challenge of controllable generation. We established unified framework for guidance based on the Bayesian decomposition of the conditional score, which elegantly separates the generative process into an unconditional direction and steering term. We saw this principle manifest in several powerful techniques. We covered methods that require dedicated training, such as Classifier Guidance (CG), which uses an external classifier , and the more efficient Classifier-Free Guidance (CFG), which learns conditional and unconditional scores within single model. We also explored flexible training-free guidance methods, which can steer pre-trained model at inference time by defining surrogate likelihood from an arbitrary loss function, enabling applications from artistic control to solving inverse problems without any retraining. Beyond simple conditioning, we delved into the nuanced task of aligning model outputs with human preferences. After reviewing the standard but complex RLHF pipeline, we introduced Direct Preference Optimization (DPO) and its novel adaptation, Diffusion-DPO, as more direct and stable alternative. This approach elegantly bypasses the need for an explicit reward model and reinforcement learning by deriving loss directly from preference data. Through these techniques, we have assembled powerful toolkit for steering the generative process. However, major practical hurdle remains untouched: the significant computational cost and latency of the iterative sampling process itself. Having addressed what to generate, we now turn to the equally important question of how fast we can generate it. The next chapter will tackle this challenge directly: 1. We will leverage the insight that sampling is equivalent to solving an ODE to explore sophisticated numerical solvers designed to drastically reduce the number of required steps. 2. We will investigate sequential of influential methods, including DDIM, DEIS, and the DPM-Solver family, which have made diffusion models far more practical by accelerating sampling speed by orders of magnitude."
        },
        {
            "title": "Sophisticated Solvers for Fast Sampling",
            "content": "The generation process of diffusion model, which maps noise to data samples, is mathematically equivalent to solving either an SDE or its associated ODE. This procedure is inherently slow, since it relies on numerical solvers that approximate solution trajectories with many small integration steps (see Chapter for brief introduction). Accelerating inference has therefore become central research objective. Broadly, existing approaches fall into two categories: Training-Free Approaches: The focus of this chapter. These methods develop advanced numerical solvers to improve the efficiency of diffusion sampling without additional training. Training-Based Approaches: Covered in Chapters 10 and 11. These techniques either distill pre-trained diffusion model into fast generator, or directly learn the ODE flow map (solution) so that only few sampling steps are required. SDE-based samplers (e.g., EulerMaruyama) may yield more diverse samples due to stochasticity but typically require more steps (Xu et al., 2023). Here we focus on ODE-based generation, whose principles extend naturally to the SDE setting. 254 9.1. Prologue 9.1 Prologue 255 9.1.1 Advanced Solvers for Diffusion Models The Score SDE framework (Song et al., 2020c) established key foundation by rigorously linking the discrete-time diffusion and ELBO formulations (SohlDickstein et al., 2015; Ho et al., 2020) with the continuous-time SDE/ODE perspective of generative modeling. This unification not only provides theoretical clarity but also enables principled development of efficient sampling algorithms based on numerical integration. Concretely, suppose we have pre-trained diffusion model sϕ(x, t) log pt(x) (which admits the other three equivalent expressions as in Section 6.3). In this case, the sampling procedure can be viewed as solving the PF-ODE with initial condition x(T ) pprior, integrated backward in time from = down to = 0: dx(t) dt = (x(t), t) 1 2 g2(t) log pt(x(t)) } {z sϕ (x(t),t) . This ODE is directly associated with the forward stochastic process dx(t) = (x(t), t) dt + g(t) dw(t), showing the continuous-time connection between the generative (reverse-time) and noising (forward-time) dynamics. The exact solution of the PF-ODE can be written equivalently in integral form: ΨT 0 (x(T )) = x(T ) + x(T ) + 0 0 (τ )x(τ ) (τ )x(τ ) 1 2 1 2 g2(τ )x log pτ (x(τ )) dτ g2(τ )sϕ (cid:0)x(τ ), τ (cid:1)i dτ (9.1.1) =: eΨT 0 (x(T )) . Here, Ψst(x) denotes the flow map of the oracle PF-ODE, mapping state at time to its evolved state at time (see Equation (4.1.9)). In contrast, eΨst(x) denotes the flow map of the empirical PF-ODE, obtained by replacing the true diffusion model log pt(x) with its learned approximation sϕ(x, t). Thus, eΨst Ψst. Since the integral form of eΨst cannot be evaluated in closed form, sampling must rely on numerical solvers. These methods approximate the solution by discretizing time and replacing the continuous integral with 256 Sophisticated Solvers for Fast Sampling finite sum of local drift evaluations, thereby tracing an approximate trajectory. Such solver-based integral approximations are referred to as training-free algorithms for fast diffusion sampling, since they aim to approximate the PFODE solution directly from the frozen pre-trained score model sϕ without requiring any additional learning. Below we first detail the common concept of numerical solvers and introduce the notations used later. Discretized Approximation of Continuous Trajectories. Let xT denote the initial state at time , and consider decreasing partition = t0 > t1 > > tM = 0. (9.1.2) Starting from xt0 = xT pprior, the solver produces sequence {xti}M i=0 that ideally approximates the empirical PF-ODE flow eΨT ti(xT ), itself proxy for the oracle map ΨT ti(xT ). Each numerical step advances the state via this empirical velocity field, and the final iterate xtM serves as an estimate of the clean sample x0 at = 0. 9.1.2 Common Framework for Designing Solvers in Literature Zhang and Chen (2022) highlighted three practical principles for designing numerical solvers for the PF-ODE associated with diffusion models. I. Semilinear Structure. Although Song et al. (2020c) establish the foundation for general drift (x(t), t), in most scheduler formulations the drift is instantiated in linear form (x, t) := (t) x, : R, which induces the PF-ODE in semilinear structure: dx(t) dt = (t)x(t) } {z linear part 2 g2(t)sϕ(x(t), t) 1 {z } nonlinear part . (9.1.3) This linearnonlinear split in is advantageous for accuracy and stability and motivates specialized integrators (see discussion near Equation (9.1.6) below) (Hochbruck and Ostermann, 2005; Hochbruck and Ostermann, 2010). II. Parameterizations beyond the Score. As 0, the true score log pt() can change very rapidly (for example, when pdata is concentrated near lowdimensional manifold) (Kim et al., 2022). This makes it difficult for neural 9.1. Prologue 257 network sϕ, which is trained to approximate the score directly, to remain accurate. To see why, recall the oracle relation (see Equation (6.3.1)) ϵ(xt, t) = σtx log pt(xt), where ϵ(xt, t) = E[ϵxt] is the oracle noise, and (αt, σt) are the mean and standard deviation of the perturbation kernel xtx0 (αtx0, σ2 I), connected to (t), g(t) via Equation (4.4.2). From the orthogonality property in L2, ϵ2 2 = ϵ2 2 + ϵ ϵ2 2 ϵ 2 ϵ2 2 = D. Hence the oracle noise predictor is always bounded, but the score grows like s(xt, t)2 2 = σ2 E ϵ(xt, t)2 2 σ2 . Thus, as 0, the score can blow up at the rate 1/σ2 , while the noise predictor stays bounded. Because neural networks can only approximate smoothly growing functions, score prediction tends to be numerically unstable and less accurate, which in turn can harm numerical PF-ODE solvers when relying on pre-trained model as drift. For this reason, widely used alternative is to predict the noise ϵϕ (or its variants such as xor v-prediction), which is stably bounded and admits simple closed-form relation to the score: sϕ(x, t) = 1 σt ϵϕ(x, t). Substituting this relation into the PF-ODE (cf. Equation (6.3.2)) gives dx(t) dt = (t)x(t) } {z linear part + 1 2 g2(t) σt ϵϕ(x(t), t) } {z nonlinear part . (9.1.4) This parameterization is commonly adopted by modern PF-ODE solvers. III. Exponential Integrators for semilinear PF-ODEs. For the semilinear structure in Equation (9.1.4), the exponential integrator formula in Equation (9.1.6) provides an exact alternative representation of the solution. To see this, let xs denote the state at start time s, and let [0, s] be the terminal time1. 1Here, is the start time and the terminal time, so sampling integrates backward with > t. 258 Sophisticated Solvers for Fast Sampling For clarity, write the nonlinear part of Equation (9.1.4) as N(x(t), t) := 1 2 g2(t) σt ϵϕ(x(t), t). The ODE can then be written as dx(t) dt (t)x(t) } {z linear part = N(x(t), t) } {z nonlinear part . (9.1.5) To isolate the linear term, we introduce the exponential integrator (cid:16)Z t) := exp (u) du (cid:17) , E(s (cid:1) and multiply both sides of the ODE by its inverse E(t rule, (cid:18) dx(t) dt Hence the equation becomes 1(s (cid:1) t) (t)x(t) (cid:19) = dt 1(s s). By the product t)x(t) . (cid:1) (cid:1) dt 1(s (cid:1) t)x(t) = 1(s t) N(x(t), t). (cid:1) Integrating from to and then multiplying back by E(s solution: (cid:1) t) gives the eΨst(xs) = E(s t)xs } (cid:1) {z linear part + 1 s g2(τ ) στ E(τ (cid:1) t)ϵϕ(xτ , τ ) dτ. (9.1.6) We refer the reader to Section A.1.3 for the full details of the derivation. To explain why the exponentialintegration form in Equation (9.1.6) is preferable to Equation (9.1.4) for few-step sampling (large s), we compare s) = ef (s)s their onestep updates. Using variation of constants, E(s and freezing N(x(τ ), τ ) N(xs, s) for τ [s s, s], the exponentialEuler update of Equation (9.1.6) is (cid:1) xExp-Euler ss = ef (s)sxs } {z linear part + ef (s)s 1 (s) , N(xs, s) (9.1.7) {z nonlinear part } with the natural limit (cid:0)ef 1(cid:1)/f as 0. Here the linear factor ef (s)s is exactly computed (no approximation). 9.1. Prologue 259 In contrast, approximating (τ )xτ N(xτ , τ ) (s)xs N(xs, s) for τ [s s, s] yields the plainEuler step for Equation (9.1.4): xEuler ss = xs [ (s) xs + N(xs, s) ] = (1 (s)s) xs } {z linear part N(xs, s) } {z nonlinear part . (9.1.8) The linear factor in Equation (9.1.8) is the firstorder Taylor approximation of the exponential in Equation (9.1.7): ea = 1 + + a2 2 + 6 + , := (s)s, so the gap is ea (1 + a) = a2 2 + O(a3). As soon as (s)s is not tiny (i.e., the step size is not sufficiently small), Eulers linear update (1 + a)xs mis-scales the true factor eaxs by relative error of order a/2. This is purely linear distortion from the discretization. The exponentialEuler step avoids it by applying the exact linear multiplier, which is especially important when taking large steps. 9.1.3 Approaches of PF-ODE Numerical Solvers Numerical solvers for diffusion models can be broadly grouped into two categories. Time Stepping Methods. This class of methods discretizes the time interval [0, ] and approximates the PF-ODE using various numerical integration schemes designed for efficiency. We present the most fundamental, principled, and widely adopted approaches as representative examples: Denoising Diffusion Implicit Model (DDIM). DDIM, introduced in Section 9.2 (with its update form already appearing in Section 4.1.4), is one of the earliest fast samplers for diffusion models. Originally proposed from variational perspective, it introduces non-Markovian forward family whose marginals match those of the original diffusion, thereby enabling deterministic reverse process and flexible step skipping. From the ODE viewpoint, however, DDIM can be understood more directly: it corresponds to applying single exponential-Euler step, i.e., approximating the diffusion model term inside the integral as constant, to the exponential-integration formula Equation (9.1.6), which yields the update in Equation (9.1.7). 260 Sophisticated Solvers for Fast Sampling Diffusion Exponential Integrator Sampler (DEIS). DEIS (Zhang and Chen, 2022), introduced in Section 9.3, was the first to exploit the semilinear structure of the PF-ODE by applying exponential integrators. The key idea is to treat the linear part exactly via an integrating factor and approximate only the nonlinear integral term. Unlike the Euler method, which assumes constant integrand inside the exponential integrator formula, DEIS reuses the history of previously estimated points along the trajectory. Specifically, it fits higher-order interpolation (a Lagrange polynomial) to the past evaluations and uses it to approximate the integral at the next step. Geometrically, this polynomial interpolation captures the curvature of the trajectory much more accurately than constant approximation, enabling higher-order accuracy and improved stability for large step sizes. This reuse of past evaluations to anchor the next update (so that each step requires only one new model call) is referred to as multistep method. In contrast, singlestep method (e.g., DDIM) relies only on the most recent state for the next update. Such methods are simpler but typically more costly to achieve high accuracy, since they require more function evaluations (or more steps) overall. The Diffusion Probabilistic Model (DPM)-Solver Family. The DPMSolver family, including DPM-Solver (Lu et al., 2022b) (Section 9.4), DPMSolver++ (Lu et al., 2022c) (Section 9.5), and DPM-Solver-v3 (Zheng et al., 2023) (Section 9.7), builds on the semilinear structure of the PF-ODE with crucial time reparameterization, the half-log signal-to-noise ratio (SNR): λt := 1 2 log α2 σ2 = log αt σt . This change of variables transforms the nonlinear term into an exponentially weighted integral λt λs eλ ˆϵϕ(ˆxλ, λ) dλ, where ˆϵϕ denotes the model expressed in the reparameterized time λ (details in Equation (9.4.4)). This representation makes higher-order approximations of the integral both more accurate. DPM-Solver introduced higher-order solvers by using Taylor expansions in λ, tailored to the half-log SNR reparameterization, showing that few NFEs suffice for high-quality samples. DPM-Solver++ adapted the method to classifierfree guidance with x-prediction for greater stability. DPM-Solver-v3 further 9.1. Prologue 261 automated the choice of parameterization by casting it as an optimization problem that minimizes local error in principled way. (Optional) Time Parallel Methods. complementary strategy accelerates sampling by parallelizing computations across different time intervals, rather than processing them strictly in sequence. ParaDiGMs. Introduced in Section 9.8, this method (Shih et al., 2023) reformulates the ODE solution as fixed-point problem. This perspective allows integral terms to be evaluated in parallel, alleviating the sequential bottleneck of standard time-stepping solvers. Importantly, this approach is not limited to the exponential-integrator form; it applies equally to general PF-ODEs with nonlinear drift (x, t). Moreover, it is solver-agnostic: the fixedpoint formulation wraps any time-stepping rule by replacing the integral with weighted sum of model evaluations at selected times, so Euler-, DEIS-, or DPM-Solverstyle updates can be used while their evaluations are performed in parallel. In practice, the wallclock cost is domiTrue Computational Cost (NFEs). nated not by the number of discretization steps, but by how many times we must call the model network. We refer to this count as the number of function evaluations (NFE). If sampler performs evaluations per step over steps, the cost scales as NFE = N. For example, firstorder Euler or exponentialEuler schemes have = 1, while singlestep kthorder methods typically require (e.g., kth order of DPM-Solver). Multistep methods (e.g., DEIS, multistep version of DPMSolver++) reuse past evaluations so that after short warm-up phase the average is close to 1. Classifier-free guidance effectively doubles the number of calls at each step. Thus, in practice, faster sampling means achieving lower NFE, not simply taking fewer steps. In the discussion Remark on Using the Equivalent Form of the PF-ODE. below, we will use the results in Section 6.3, which support the interchangeable use of the equivalent parameterizations (f (t), g(t)) and (αt, σt) of the perturbation kernel with xtx0 (; αtx0, σ2 I), related via (t) = α αt , g2(t) = dt (cid:0)σ2 (cid:1) 2 α αt = 2σtσ σ2 2 α αt σ2 . 262 Sophisticated Solvers for Fast Sampling Under these relations, the PF-ODE can be written in several equivalent forms (cf. Equation (6.3.2)). 9.2. DDIM 9.2 DDIM In this section, we introduce one of the pioneering approaches for accelerating sampling in diffusion models: Denoising Diffusion Implicit Models (DDIM), which is also among the most widely used ODE-based solvers. Although its name suggests variational origin, as demonstrated in Section 6.3.2 for (x, ϵ)- prediction, we will show that its practical update rule can also be interpreted as straightforward application of the Euler method to approximate the integral in Equation (9.1.6). This ODE perspective not only provides principled reinterpretation of DDIM, but also lays foundation for designing more flexible and efficient fast samplers. The original variational derivation of DDIM will be revisited in Section 9.2.3. In Section 9.2.4, we establish clear correspondence between the DDIM update rule and conditional flow matching, showing that the DDIM dynamics can be interpreted as the flow learned by CFM. 9.2.1 Interpreting DDIM as an ODE Solver Let > denote two discrete time steps, with being the starting time and the target time for the update. To approximate the integral in Equation (9.1.6), natural choice is to fix the integrand at (the start of the step), assuming that ϵϕ(xτ , τ ) ϵϕ(xs, s), for all τ [t, s]. This assumption leads to an Euler update approximation (see also Equation (9.1.7)), which gives rise to the following update rule: xt = E(s t)xs + (cid:1) 1 s g2(τ ) στ E(τ (cid:1) ! t) dτ ϵϕ(xs, s), (9.2.1) for an initial point xs. Here, the integral becomes analytically tractable, resulting in the following practical and efficient DDIM update formula: Proposition 9.2.1: DDIM = Euler Method (Exponential Euler) The update rule in Equation (9.2.1), derived by applying the Euler method to the exponential integrator form in Equation (9.1.6), yields the following DDIM update: xt = αt αs xs αt (cid:18) σs αs (cid:19) σt αt ϵϕ(xs, s). (9.2.2) 264 Sophisticated Solvers for Fast Sampling Proof for Proposition. We use Equation (4.4.2) that (t) = α αt , g2(t) = dt (cid:0)σ2 (cid:1) 2 α αt = 2σtσ σ2 2 α αt σ2 . With this, we obtain E(s (cid:1) t) = f (u) du = elog αuu=t u=s = αt αs . So s g2(τ ) 2στ τ (u) dudτ = s dτ Z g2(τ ) 2στ αt ατ 1 2στ ατ (cid:16) στ ατ dτ σt (cid:16) σs αt αs (cid:16) dσ2 τ dτ (cid:17) dτ = αt = αt = αt 2 log ατ dτ (cid:17) dτ σ2 τ (cid:17) . This correspondence reveals that DDIM can be interpreted as first-order Euler method applied to the exponential-integrator transformed semilinear PF-ODE. 9.2.2 Intuition Behind DDIM with Different Parameterizations DDIM is one of the most widely used methods for accelerating diffusion sampling and usually may take in different parametrizations (see Equation (6.3.1)) other than ϵ-prediction. In this subsection, we present reformulation under different parameterizations, with later on provide more intuitive intepretation of DDIM. In practice, one uses pre-trained DDIM in Different Parameterizations. diffusion model expressed in one of the standard parameterizations and substitutes the corresponding predictor for the oracle target in the DDIM discretization of the PF-ODE. For clarity, we state the oracle version below; the implementable version follows by the replacements ϵϕ ϵ, xϕ x, sϕ s, vϕ v. 9.2. DDIM 265 Corollary 9.2.1: DDIM in Different Parametrizations Let > t. Starting from xs ps and ending at time t, the DDIM update in different parametrizations are as: xt = = = αt αs σt σs αt αs xs + αt xs + αs xs + σ2 (cid:18) σt αt (cid:18) αt αs (cid:18) αt αs (cid:19) (cid:19) (cid:19) σs αs σt σs σt σs ϵ(xs, s) x(xs, s) s(xs, s) = αt x(xs, s) } {z xϕ estimated clean + σt ϵ(xs, s) } {z ϵϕ estimated noise . (9.2.3) The last identity in Equation (9.2.3) gives clear view of DDIM: starting from xs ps, the (estimated) clean part x(xs, s) and (estimated) noise part ϵ(xs, s) act as interpolation endpoints that reconstruct xt pt with coefficients (αt, σt). Indeed, DDIM can be viewed as an direct Euler discretization of the v-parametrized PF-ODE without applying exponential integrators. From Proposition 6.3.2, the PF-ODE also takes the following form of v-prediction: dx(τ ) dτ = α τ x(x(τ ), τ ) + σ τ ϵ(x(τ ), τ ), τ [t, s]. Starting at xs and integrating over [t, s], Eulers method freezes the predictors at the right endpoint: x(x(τ ), τ ) x(xs, s), ϵ(x(τ ), τ ) ϵ(xs, s), for all τ [t, s]. This gives xt = xs + (cid:0)α τ + σ τ ϵ(cid:1) dτ xs + (αt αs)x(xs, s) + (σt σs)ϵ(xs, s) = αtx(xs, s) + σtϵ(xs, s), where the last identity follows directly from Equation (6.3.1). The derived formula above exactly matches the final identity in the DDIM update (Equation (9.2.3)). See Equation (9.2.3) for illustration. With velocity prediction, the linear term (t)x in the PF-ODE is absorbed tϵ. By the Fundamental Theorem of into the target v(x(t), t) = α tx0 + σ 266 Sophisticated Solvers for Fast Sampling Figure 9.1: Illustration of DDIM as an Euler discretization of the PF-ODE. Starting from state xs at time s, the oracle PF-ODE trajectory (gray curve) deterministically evolves to Ψst(xs) at time t. In contrast, the DDIM update (orange) directly maps xs to αtx(xs, s) + σtϵ(xs, s). The discrepancy between this Euler step and the true PF-ODE trajectory introduces discretization error, shown in blue. If is far from s, the discrepancy can become large, leading to degraded generation quality. Calculus, the integrals so single Euler step already yields the closed-form DDIM update: τ dτ simplify to (αt αs) and (σt σs), τ dτ and α σ xt = αt x(xs, s) + σt ϵ(xs, s). That is, with v-prediction, there is no separate linear term to isolate in the PF-ODE drift, so the plain Euler update naturally coincides with the DDIM formulation. In contrast, under the ϵ-, x-, or s-prediction parameterizations, the PF-ODE drift can be decomposed into semilinear form consisting of linear term and nonlinear correction, which fits the general template given in Equation (9.1.5). naïve Euler step then only approximates the linear term instead of computing it exactly (see the argument in Equation (9.1.8)). DDIM, on the other hand, corresponds to an exponentialEuler (integrating-factor) step that handles this linear component analytically. Therefore, v-prediction leads to the simplest and most direct Euler integration, whereas the other parameterizations require the exponentialEuler form to achieve the same DDIM behavior. The above discussion also echoes the arguments presented in Section 6.3. and leads to the following conclusion: 9.2. DDIM 267 Observation 9.2.1: (Exponential) Euler and DDIM Updates Given the same schedulers (αt, σt), v-prediction: Euler = DDIM, ϵ-, x-, or s-prediction: expEuler = DDIM = plain Euler, where, in the ϵ-, x-, or s-prediction cases, the plain Euler step is not equivalent to DDIM, since the linear term is only approximated and may lead to reduced stability. Illustrative Example of DDIM Under Different Parameterizations. We illustrate with simple example using oracle replacements (ϵ, x, log pt, and v), based on Equation (9.2.3). Assume the forward kernel αt = 1 and σt = (Karras et al., 2022). The DDIM (expEuler) update xt = αt αs xs αt (cid:18) σs αs (cid:19) σt αt ϵ(xs, s) reduces to xt = xs (s t) ϵ(xs, s). Conceptually, subtracting the time gap (s t) multiplied by the oracle noise estimate ϵ(xs, s) pushes the current sample xs toward cleaner estimate. Using the x-prediction oracle x, which is related to the noise oracle by ϵ(xs, s) = xs x(xs, s) , we obtain xt = xs (cid:0)xs x(xs, s)(cid:1) = xs + (cid:16) 1 (cid:17) x(xs, s). (9.2.4) Thus, xt is convex combination of the current sample xs and the x-prediction x(xs, s), which serves as the oracle estimate of the clean data. Moreover, we can rewrite this as xt = (cid:0)xs x(cid:1), < s, which shows that the denoising residual contracts by the factor t/s (0, 1) at each step (so no overshoot occurs when < s). Using the score oracle, related to the noise oracle by ϵ(xs, s) = σsx log ps(xs), 268 Sophisticated Solvers for Fast Sampling the DDIM (expEuler) update becomes xt = xs + (s t) log ps(xs). This moves xs uphill along the score field (toward higher likelihood regions), with step size proportional to the time gap (s t) and the noise scale s. Finally, using the velocity oracle with v(xs, s) = ϵ(xs, s), the DDIM update can be written as xt = xs + (t s) v(xs, s), so the secant slope satisfies the finite-difference identity xt xs = v(xs, s). Intuitively, this means the update is straight-line step following the local ODE drift. Challenge of DDIM. However, the first-order Euler discretization has global error O(h), so accuracy degrades as the maximum step size := maxi ti ti1 grows. To improve accuracy, the literature develops higher-order schemes that raise the global order to O(hk) (k 2) through richer local approximations. With suitable timestep allocation, these methods may achieve target quality in fewer steps. It is important to note, however, that higher order alone does not guarantee fewer steps or lower wall-clock cost, since each step may require multiple model evaluations. In practice, the true measure of efficiency is the number of function evaluations, NFE = , and faster means reaching the desired quality with smaller NFE, not merely fewer steps. 9.2.3 (Optional) Variational Perspective on DDIM Indeed, the motivation for DDIM comes from revisiting DDPM through its variational perspective. In DDPM, the reverse process is tied to particular Markovian forward transition kernel p(xtxtt), which enforces small step sizes in order to approximate the multi-step posterior correctly. DDIM departs from this restriction by observing that the training objective depends only on the marginal perturbations pt(xtx0), not on the specific forward transition. This insight allows one to construct reverse dynamics directly from the marginals, so intermediate steps can be skipped while marginal consistency is preserved. Because the transition is defined to map ps(xsx0) to pt(xtx0) for any < s, we may use coarse time grid with far fewer updates, which reduces the number of model evaluations and yields fast few-step sampling. 9.2. DDIM 269 Revisiting DDPMs Variational View. In DDPM, training fixes family of marginal perturbation kernels pt(xtx0) and optimizes surrogate objective that depends only on these marginals. At sampling time, however, the reverse conditional is the Bayesian posterior under the one-step forward kernel: p(xttxt, x0) = p(xtxtt) ptt(xttx0) pt(xtx0) . This ties the reverse update to the particular forward transition p(xtxtt). If one tries to skip steps by enlarging while reusing the same one-step kernel, this no longer matches the true multi-step posterior and typically degrades the marginals. Original DDIM Motivation. DDIM observes that the training objective constrains only the marginals pt(xtx0), not the intermediate reverse transitions. Hence, one may specify family of reverse conditionals π(xtxs, x0) for any < that are one-step marginally consistent2: 2If we choose the user-defined reverse transition kernel π in Equation (9.2.5) to be exactly the same as the true conditional distribution: π(xtxs, x0) = p(xtxs, x0), then the marginal consistency condition π(xtxs, x0) ps(xsx0) dxs = pt(xtx0) is simply the consequence of law of total probability (also known as the tower property) for the conditional joint distribution: Z pt(xtx0) = p(xt, xsx0) dxs = p(xtxs, x0) ps(xsx0) dxs. Or equivalently, by explicitly expressing the Bayesian posterior as p(xtxs, x0) = p(xsxt, x0) pt(xtx0) ps(xsx0) , then multiplying by ps(xsx0) and marginalizing over xs, we recover p(xtxs, x0) ps(xsx0) dxs = pt(xtx0), which is exactly the same marginal-consistency condition. In the Markov forward case, one further has p(xtxs, x0) = p(xtxs), reducing the following expression: pt(xtx0) = p(xtxs) ps(xsx0) dxs. Sophisticated Solvers for Fast Sampling π(xtxs, x0) ps(xsx0) dxs = pt(xtx0). (9.2.5) This construction removes any dependence on the forward one-step kernel p(xtxtt) and legitimizes coarse (skipped) time steps. Derivation of Discrete-Time DDIM. Consider the general forward perturbation: pt(xtx0) := (cid:0)xt; αtx0, σ2 I(cid:1), where x0 pdata. DDIM does not require the reverse update to coincide with the Bayesian posterior tied to the one-step forward kernel. It suffices to choose reverse conditional that preserves the marginals. Concretely, for any < we posit the Gaussian family π(xtxs, x0) = (cid:0)xt; at,s x0 + bt,s xs, c2 t,s I(cid:1), (9.2.6) with coefficients (at,s, bt,s, ct,s) to be determined by the marginal-consistency constraint Equation (9.2.5). Since all involved kernels are Gaussian, sampling xsx0 = αsx0 + σsϵ and then xtxs, x0 from Equation (9.2.6) yields xt = at,s x0 + bt,s xs + ct,s ϵ = at,s x0 + bt,s = (at,s + bt,sαs) x0 + (cid:0)αsx0 + σsϵ(cid:1) + ct,s ϵ + c2 b2 t,sσ t,s ϵ, (9.2.7) where ϵ, ϵ, ϵ (0, I) are independent (Gaussian-sum property). On the other hand, xt pt(xtx0) = (cid:0)xt; αtx0, σ2 I(cid:1). Equating means and variances between this target and Equation (9.2.7) gives αt = at,s + bt,sαs, = b2 σ2 t,sσ2 + c2 t,s. This system is underdetermined, so we treat ct,s as free parameter with the natural constraint 0 ct,s σt, and solve for at,s, bt,s: bt,s = c2 σ2 t,s σs , at,s = αt αs bt,s. (9.2.8) Here, we take the nonnegative root for bt,s without loss of generality. 9.2. DDIM 271 Substituting Equation (9.2.8) into Equation (9.2.6) yields π(xtxs, x0) = (cid:16) xt; αtx0 + c2 σ2 t,s σs {z mean (xs αsx0) , c2 (cid:17) t,sI . (9.2.9) } Equivalently, the mean in Equation (9.2.9) expands to (cid:16) αt αs c2 σ2 t,s σs (cid:17) x0 + (cid:16) c2 σ2 t,s σs (cid:17) xs. Lemma 9.2.2: DDIM Coefficients Let π(xtxs, x0) be given by Equation (9.2.6). If the marginalconsistency condition Equation (9.2.5) holds, then the coefficients are exactly those in Equation (9.2.8), with 0 ct,s σt. Remark. 1. In DDIM we choose the reverse kernel π(xtxs, x0) to satisfy the marginalconsistency constraint, and in general π(xtxs, x0) = p(xtxs, x0), where p(xtxs, x0) is the Bayesian posterior associated with particular forward one-step kernel. By Bayes rule, p(xtxs, x0) p(xsxt) pt(xtx0), and this posterior is not required for specifying π or for training. 2. Only in the special case where the variance parameter is chosen to match the DDPM posterior variance (the η = 1 setting in Equation (9.2.10)) do we have π(xtxs, x0) = p(xtxs, x0); otherwise π(xtxs, x0) = p(xtxs, x0). 3. Without imposing Markov constraint, in general p(xsxt, x0) = p(xsxt). The equality p(xsxt, x0) = p(xsxt) is tied to particular Markov forward model, which DDIM does not assume for its reverse construction. The forward marginals {pt(xtx0)}t do not uniquely determine the reverse conditional transitions. There exist infinitely many kernels π(xtxs, x0) that 272 Sophisticated Solvers for Fast Sampling satisfy Equation (9.2.5), any of which can be freely specified. The parameter ct,s indexes this family and controls the amount of noise injected at each reverse step t. Below, we introduce this family of DDIM solvers. DDIM Sampler (Step t). The DDIM sampler follows from the chosen reverse kernel π(xtxs, x0) in Equation (9.2.9) by replacing x0 with predictor from pre-trained model. Using the ϵ-prediction network ϵϕ (plug-and-play, no retraining), we set xϕ(xs, s) := xs σs ϵϕ(xs, s) αs , pϕ(xtxs) := π(cid:0)xt (cid:12) (cid:12) xs, xϕ(xs, s)(cid:1). Substituting xϕ into Equation (9.2.9) yields the update xt = αt αs (cid:16)q xs + c2 σ2 t,s (cid:17) σs αt αs ϵϕ(xs, s) + ct,s ϵt, ϵt (0, I), where ct,s [0, σt] controls stochasticity. For notational convenience define the forward factors αts := αt , αs so that p(xtxs) = (αtsxs, σ ts := σ2 σ2 α2 ts σ2 , tsI). Then the sampler can be written as xt = αts xs + (cid:0)q t,s αtsσs By varying ct,s, one obtains family of samplers that share the same (cid:1) ϵϕ(xs, s) + ct,s ϵt. c2 σ2 pre-trained diffusion model and do not require retraining: DDPM Step (Posterior Variance): ct,s = σs σt σts makes π(xtxs, x0) equal to the Bayesian posterior p(xtxs, x0) induced by the one-step forward kernel. Replacing x0 with its predictor yields the standard DDPM reverse update pϕ(xtxs), i.e., the Markov DDPM step with + σ2 α2 = 1 (Equation (2.2.14)). Deterministic DDIM (η = 0): ct,s = 0 gives xt = αtsxs + (cid:0)σt αtsσs which matches the ODE-view DDIM jump. (cid:1)ϵϕ(xs, s), Interpolation: Define ct,s = η σs σt σts, η [0, 1], (9.2.10) so that η smoothly interpolates between the stochastic DDPM update (η = 1) and the deterministic DDIM update (η = 0). 9.2. DDIM 273 9.2.4 DDIM as Conditional Flow Matching In this subsection, we will see that deterministic DDIM can be understood as searching for conditional flow map that pushes ps(x0) forward to pt(x0). The tangent of this conditional flow coincides with the conditional velocity used in conditional flow matching (CFM). Marginalizing this conditional velocity yields the PFODE drift, whose plain Euler discretization recovers the marginal DDIM update in v-prediction. We revisit the DDIM onestep conditional marginalconsistency identity (Equation (9.2.5)) π(xtxs, x0)ps(xsx0) dxs = pt(xtx0), < s, i.e., if xs ps(x0) then pushing xs forward by the chosen reverse kernel reproduces pt(x0). When the reverse kernel is deterministic, it amounts to finding conditional map Ψst(x0) that pushes ps(x0) forward to pt(x0): π(xtxs, x0) = δ(cid:0)xt Ψst(xsx0)(cid:1), (cid:0)Ψst(x0)(cid:1) #ps(x0) = pt(x0). Under the linearGaussian path xτ = ατ x0 + στ ϵ, similar arguments as in Equations (9.2.6) and (9.2.7) lead to the conditional map Ψst(xsx0) = σt σs (cid:16) xs + αt αs (cid:17) x0, σt σs whose instantaneous conditional velocity is (xx0) = (cid:12) (cid:12)h=0Ψtt+h(xx0) = σ σt (cid:16) + α αt (cid:17) x0. σ σt We refer to Ψst(x0) as the DDIM conditional map. With pt(xx0), conditional flow matching fits the timedependent field to this target velocity, LCFM(ϕ) = Et,x0,xtpt(x0) (cid:13) (cid:13)vϕ(xt, t) (xtx0)(cid:13) 2, (cid:13) so the CFM regression target equals the conditional velocity of the DDIM conditional map. Observation 9.2.2: Conditional Level Along the conditional Gaussian path, the DDIM conditional map and the CFM target generate the same conditional flow Ψst(x0). Sophisticated Solvers for Fast Sampling Averaging the conditional velocity over the posterior of x0 given xt = yields the marginal PFODE drift, v(x, t) = [v (xx0)xt = x] , which, under the linearGaussian scheduler, takes the separable predictor form v(x, t) = α x(x, t) + σ ϵ(x, t), = αt x(x, t) + σt ϵ(x, t). We have seen that the plain Euler step of the PF-ODE with this marginalized vprediction is exactly the DDIM update (the last identity in Equation (9.2.3)). In short, DDIM is (i) deterministic conditional transport whose tangent equals the CFM target, and (ii) after marginalizing that tangent, Euler step of the PFODE whose step coincides with the DDIM update. 9.3. DEIS 9.3 DEIS 275 In the exponentialintegrator formula (Equation (9.1.6)), g2(τ ) 2 στ E(τ t) ϵϕ(xτ , τ ) dτ, the only unknown is the model output ϵϕ(xτ , τ ); the schedule terms and the weight E(τ t) are known once (α, σ, g) are fixed. DDIM (Eulers method) approximates this integral by holding the model output constant: ϵϕ(xτ , τ ) ϵϕ(xs, s), τ [t, s]. However, this is only firstorder accurate and can fail when the model output changes quickly in time. natural question then arises: can we make better use of the model evaluations already computed? As in classical multistep solvers, instead of treating ϵϕ(xτ , τ ) as constant (Euler), we can reuse previous outputs (anchors) to fit simple curve in time. Because the weight g2(τ ) E(τ t) is known, 2στ the integral can then be evaluated exactly for this fitted curve. In effect, the hard integral of an unknown function is replaced by the exact integral of an approximating curve defined by past model calls. This is precisely the principle behind Diffusion Exponential Integrator Sampler (DEIS) (Zhang and Chen, 2022). For readers familiar with classical ODE solvers, DEIS can be viewed as an AdamsBashforth scheme (Iserles, 2009) applied in the framework of exponential integrators for the semilinear PF-ODE (Equation (9.1.6)): the linear drift is treated exactly via the integrating factor, while the remaining nonlinear term is advanced using multistep polynomial extrapolation. We begin in Section 9.3.1 by introducing how to construct smooth curve that passes through set of anchors. In Section 9.3.2, we then apply this interpolation technique to approximate the PF-ODE integral, leading to the DEIS algorithm. Finally, in Section 9.3.3, we show that DDIM arises as the special case of DEIS with constant polynomial. 9.3.1 Polynomial Extrapolation Anchor Interpolation for Simple Curves. Assume we know the value of some timevarying quantity at few recent times (τ0, Y0), (τ1, Y1), . . . , (τn, Yn), τ0 < τ1 < < τn, Sophisticated Solvers for Fast Sampling where each Yj may be vector-valued. The most natural way to get simple curve that exactly matches these anchors is to use the lowest-degree polynomial that passes through them. The easiest way to enforce that is to multiply factors that vanish at the other nodes and then normalize so that the value at τj becomes 1. Small cases are intuitive: Example: = 0 (Constant): use the last value, Y(τ ) Yn. = 1 (Line): draw the straight line through the last two anchors, Y(τ ) = τ τn τn1τn Yn1 + τ τn1 τnτn1 Yn. = 2 (Quadratic; Parabola): pass quadratic curve through the last three anchors. For example, if the anchors are (τn2, Yn2), (τn1, Yn1), (τn, Yn), the quadratic interpolant is Y(τ ) = Yn2 ℓn2(τ ) + Yn1 ℓn1(τ ) + Yn ℓn(τ ), where the Lagrange basis functions are ℓn2(τ ) = ℓn1(τ ) = (τ τn1)(τ τn) (τn2τn1)(τn2τn) , (τ τn2)(τ τn) (τn1τn2)(τn1τn) , (τnτn2)(τnτn1) . ℓn(τ ) = (τ τn2)(τ τn1) These satisfy the interpolation conditions ℓj(τk) = δjk, for j, {n 2, 1, n} and ℓn2(τ ) + ℓn1(τ ) + ℓn(τ ) = 1 for all τ . This curve not only matches all three anchors but also bends to reflect the local curvature. These cases are all part of single recipe, known as the Lagrange polynomial. The idea is simple: we form the curve as linear blend of the anchors with timedependent weights, Y(τ ) = j=0 ℓj(τ ) Yj, ℓj(τk) = δjk, j=0 ℓj(τ ) = 1. 9.3. DEIS 277 Each ℓj(τ ) acts like spotlight, taking value 1 at its own anchor (ℓj(τj) = 1) and 0 at the others (ℓj(τk) = 0, = j). In this sense, the Lagrange interpolant is just linear combination of the anchors with basis functions ℓj(τ ). 9.3.2 DEIS: Lagrange Polynomial Approximation of the PF-ODE Integral Let 0 be the chosen polynomial degree. At step i, we approximate the unknown map τ 7 ϵϕ(xτ , τ ) over [ti1, ti] by degree-n polynomial interpolant built from past model outputs, and substitute this approximation into the exponentialintegrator update (Equation (9.1.6)) to obtain xti. By fitting polynomial that bends to capture shortterm trends of the trajectory, the update intuitively follows the curved behavior of the true ODE solution more closely, especially for larger step sizes. Figure 9.2: Illustration of DEIS as multistep method. With three past anchors at t2, t3, t4, DEIS builds quadratic curve through the model outputs and analytically integrates it to step from t4 to t5 (extrapolation). This higher-order update reduces discretization error compared to first-order methods like DDIM, which only use the value at t4 (constant approximation of the integral). degree-n update needs n+1 anchors. When they are available (sufficient history, n+1), we use the full degree-n scheme. In the early steps (insufficient history, n), we apply the same construction at the highest feasible degree, i1, and increase the degree as more anchors accumulate. Below we treat these two scenarios in turn. 278 Sophisticated Solvers for Fast Sampling Case I: = + 1, . . . , (Sufficient History). Instead of relying solely on the most recent estimate ϵϕ(xti1, ti1), DEIS reuses the last n+1 model evaluations as anchors, (τj, Yj) := (cid:0)ti1j, ϵϕ(xti1j , ti1j)(cid:1), = 0, . . . , n. as anchors. Viewing τ 7 ϵϕ(xτ , τ ) as smooth function of time along the trajectory, we construct the degree-n polynomial (Lagrange interpolant) Pn(τ ) = h j=0 k=0 k=j τ ti1k ti1j ti1k {z =: ℓ(i) (τ ) } ϵϕ (cid:0)xti1j , ti1j (cid:1) which by construction satisfies Pn(τj) = Yj for each anchor: Pn (cid:0)τj (cid:1) = Yj = ϵϕ (cid:0)xti1j , ti1j (cid:1), = 0, . . . , n. Each ℓ(i) satisfies ℓ(i) (cid:0)ti1m (cid:1) = 1, = j, 0, = j. The Lagrange polynomial provides smooth extrapolation over the new step: ϵϕ(xτ , τ ) Pn(τ ) = j=0 ℓ(i) (τ )ϵϕ (cid:0)xti1j , ti1j (cid:1), τ [ti1, ti]. We then substitute Pr(τ ) for ϵϕ(xτ , τ ) in the exponentialintegrator formula (Equation (9.1.6)): ti ti1 g2(τ ) 2στ E(τ ti)ϵϕ(xτ , τ ) dτ Z ti ti1 j=0 g2(τ ) 2στ E(τ ti)ℓ(i) (τ ) dτ ϵϕ(xti1j , ti1j). {z =: Ci,j } The weights Ci,j are given by Ci,j := 1 2 ti ti1 g2(τ ) στ E(τ ti)ℓ(i) (τ ) dτ, depending only on the schedule (ατ , στ ) and the grid {ti}. Hence, they can be precomputed exactly in closed form once the steps are fixed. 9.3. DEIS 279 Integrating the linear part exactly with E(ti1 ti), this leads to the AB-DEIS-r update rule3, xti = E(ti1 ti)xti1 + j=0 Ci,jϵϕ(xti1j , ti1j). It yields local truncation error of order r+1 under standard smoothness assumptions. Case II: = 1, . . . , (Insufficient History). For the initial steps, only past points are available. We therefore set the degree to i1 and define Pi1(τ ) = i1 j=0 ℓ(i) (τ )ϵϕ (cid:0)xti1j , ti1j (cid:1), where ℓ(i) is the Lagrange basis of degree i1 built on the nodes at time {ti1, ti2, . . . , t0}. This matches all available anchors and seamlessly transitions into the full-history formula once n+1. This is standard warm start in multistep solvers. When history is short, we fit the richest polynomial the data allow: with one anchor (i=1), use degree 0 (constant); with two anchors (i=2), use degree 1 (linear); with three anchors (i=3), use degree 2 (quadratic); and so on, until we reach the target degree n. In effect, we gradually ramp up from one-step forecast to true (n+1)-step forecast as more history becomes available. Example: Special Cases of Lagrange Polynomials When = 0 (one anchor): P0(τ ) = ϵϕ(xti1, ti1). This uses only the most recent value, so the approximation is flat in τ . It corresponds to left-endpoint of the integrand. When = 1 (two anchors): the Lagrange polynomial is linear map passing through the two pre-specified anchors. P1(τ ) = τ ti2 ti1 ti2 } {z ℓi1(τ ) ϵϕ(xti1, ti1) + τ ti1 ti2 ti1 {z } ℓi2(τ ) ϵϕ(xti2, ti2). Here ℓi1(τ ) and ℓi2(τ ) are the Lagrange basis weights. They satisfy the 3AB refers to the classical AdamsBashforth family and exponential timedifferencing multistep methods (Hochbruck and Ostermann, 2010). 280 Sophisticated Solvers for Fast Sampling interpolation (nodal) conditions P1(ti1) = ϵϕ(xti1, ti1) and P1(ti2) = ϵϕ(xti2, ti2), and with ℓi1(τ ) + ℓi2(τ ) = 1. Summary of AB-DEIS-n Update. Combining the two cases, sufficient history and warm start (insufficient history), yields the AB-DEIS-n update4 where is the polynomial degree (using up to n+1 past evaluations) as follows: xti = E(ti1 ti) xti1 + min{n, i1} j=0 Ci,j ϵϕ(xti1j , ti1j), with coefficients Ci,j := 1 2 ti ti1 g2(τ ) στ E(τ ti) \" min{n, i1} k=0 k=j τ ti1k ti1j ti1k # dτ. When n+1 (sufficient history), min{n, 1} = and the step attains local truncation error O(h n+1) under standard smoothness assumptions. During warm start (i n), min{n, 1} = 1 and the per-step order is O(h min{n, i1}+1), ramping up until full order is reached. However, very large often degrades performance due to interpolation ill-conditioning, noise amplification, and tighter stability constraints; small degrees (e.g., {1, 2, 3}) usually provide the best accuracystability tradeoff. As we will see in the following subsection, the special case n=0 reduces to exponential Euler/DDIM. 9.3.3 DDIM = AB-DEIS-0 We observe that when = 0 (i.e., constant polynomial), the coefficient simplifies to: g2(τ ) στ Substituting into the update formula yields the zeroth-order AB-DEIS scheme: Ci0 = ti)dτ. E(τ ti 1 2 ti (cid:1) xti = E(ti1 ti)xti1 + Ci0ϵϕ(xti1, ti1) ti ti1 = (cid:1) (u) du xti1 + (cid:16) ti ti1 g2(τ ) 2στ ti τ (u) dudτ (cid:17) ϵϕ(xti1, ti1). (9.3.1) 4AB refers to the AdamsBashforth family of exponential timedifferencing multistep methods (Hochbruck and Ostermann, 2010). 9.3. DEIS This is exactly the exponentialEuler step (constant-in-time ϵϕ over [ti1, ti]), which coincides with the deterministic DDIM update. We state this correspondence formally below. Proposition 9.3.1: DDIM = AB-DEIS-0 Equation (9.3.1) is identical to the DDIM update in Equation (9.2.2). 282 Sophisticated Solvers for Fast Sampling 9.4 DPM-Solver The DPM-Solver family, including DPM-Solver (Lu et al., 2022b), DPMSolver++ (Lu et al., 2022c), and DPM-Solver-v3 (Zheng et al., 2023), represents major advance in solvers for the PF-ODE. The goal is simple: achieve similar sample quality with far fewer steps. In practice, these methods reduce the steps required by DDIM from more than 50 to about 10-15, which makes generation much more efficient. In addition, DPM-Solver++ and DPM-Solverv3 are designed to handle classifier free guidance (CFG) (see Section 8.3) for conditional generation. In this section, we first explain the core DPMSolver (Lu et al., 2022b); its extensions appear in Section 9.5 and Section 9.7. High-Level Idea of DPM-Solver. Like DEIS, DPM-Solver starts from the semilinear form of the PF-ODE and works in the ϵ-prediction parameterization, using the exponential integrator (variation of constants) representation in Equation (A.1.2): dxt dt = α αt xt σt (cid:18) α αt (cid:19) σ σt ϵϕ(xt, t). (9.4.1) The key idea is to reparameterize time by the half-log signal-to-noise ratio, so that the nonlinear term in the exponential integrator formula becomes an exponentially weighted integral. This representation admits low-cost Taylor expansions in λ, which naturally yield higher-order update rules. We will shortly provide an intuitive explanation for why this reparameterization is effective. 9.4.1 DPM-Solvers Insight: Time Reparameterization via Log-SNR On top of the semilinear structure, key insight from of DPM-solver is that the standard time parameterization is suboptimal for numerical integration in diffusion models. They instead propose reparameterizing time using the half-log signal-to-noise ratio (half-log SNR) λt := 1 2 log α2 σ2 = log αt σt , (9.4.2) following the log-SNR parameterization of VDM (Kingma et al., 2021). This change-of-variables simplifies the nonlinear integrand, thereby enabling more tractable and accurate higher-order model estimation. 9.4. DPM-Solver 283 Change-of-Variable to Log-SNR in PF-ODE. We now reparametrize time using the halflog SNR, λt := log(αt/σt). For common noise schedules, λt is strictly decreasing in t. Under this assumption, it has an inverse function tλ() that maps λ to t, satisfying = tλ(λ(t)). We then change the subscripts of and ϵϕ from to λ. hat ( ˆ ) indicates that the quantity is expressed in λ. More precisely, we define: ˆxλ := xtλ(λ), ˆϵϕ(ˆxλ, λ) := ϵϕ(xtλ(λ), tλ(λ)). (9.4.3) With this change of variables from to λt, the exact solution eΨst of the PF-ODE in Equation (9.4.1) becomes: Proposition 9.4.1: Exponentially Weighted Exact Solution Given an initial value xs at time > 0, the exact solution eΨst(xs) at time [0, s] of the PF-ODE can be re-expressed as: eΨst(xs) = αt αs xs αt λt λs eλ ˆϵϕ(ˆxλ, λ) dλ. (9.4.4) Proof for Proposition. While one may directly apply the change of variables to Equation (9.4.1) to obtain the result, we provide an alternative derivation below for clarity dλt and completeness. Using the relation g2(t) = 2σ2 dt , Equation (9.4.1) can be rewritten as: dxt dt = log αt dt xt σt dλt dt ϵϕ(xt, t). Applying the chain rule: dˆxλ dλ dxt dt dλt dt the ODE in is transformed into an ODE in λ as follows: log αλ dλ log αt dt dλt dt and = = , (cid:17)1 dxt dt (cid:17)1h log αt dˆxλ dλ = = = = (cid:16) dλt dt (cid:16) dλt dt (cid:16) dλt dt log αλ dλ xt σt dλt dt ϵϕ(xt, t) (cid:17)1h log αλ dλt dt ˆxλ σλ dλt dt ˆϵϕ(ˆxλ, λ) dt dλ ˆxλ σλ ˆϵϕ(ˆxλ, λ). 284 Sophisticated Solvers for Fast Sampling Thus, the transformed ODE becomes Equation (9.4.5). We can then apply the same Exponential Integrator (EI) technique to Equation (9.4.5) to derive Equation (9.4.4). In λtime, the model appears inside an exponentially weighted integral, λt λs eλ ˆϵϕ(ˆxλ, λ) dλ, where the eλ factor produces closed-form coefficients and smooths the integrand, exactly what high-order local approximations require. Equivalently, changing variables from to λ transforms the PF-ODE into the differential form below (see the derivation in the previous proposition): dˆxλ dλ = α λ αλ ˆxλ σλ ˆϵϕ(ˆxλ, λ). (9.4.5) Intuition of Why Reparameterize Time? For strictly monotone λ(t), the firstorder change of variables gives λ λ(t) . Thus, for fixed λ, the induced is smaller where λ(t) is larger (i.e. where λ changes rapidly with t), and larger where λ(t) is smaller. This reparameterization does not alter the PFODE solution path, only the speed: dˆxλ dλ = 1 λ(t) dxt dt . Consequently, in regions with large λ(t), the λdomain derivative is scaled by 1/λ(t), often making the integrand smoother to approximate on uniform λ grid. (The precise location of large λ(t) depends on the chosen schedule.) Conceptually, we may want to allocate more timesteps when the process gets closer to the complicated (data) distribution. Below are two simple schedules that illustrate this effect: (αt, σt) = (1 t, t): This corresponds to the FM scheduler. Then λ(t) = log 1 , λ(t) = 1 t(1 t) , λt(1 t). Hence steps are tiny near both ends (t 0, 1) and largest around mid-time. 9.4. DPM-Solver (αt, σt) = (1, t): This is the EDM scheduler (Karras et al., 2022), introduced in Section D.6. If we take the independent variable directly as the noise level = σt, then λ(t) = log 1 , λ(t) = 1 , λt. Uniform spacing in λ is geometric in t, or equivalently in the variance (many small steps at small t/high SNR, coarser at large t). 9.4.2 Estimating the Integral with Taylor Expansion DEIS fits the integrand by Lagrange interpolation across past evaluations. DPM-Solver instead uses local Taylor expansion in λ: it is cheaper to evaluate, aligns with the smoothness induced by the λ-parametrization, and yields closed-form step coefficients. We present the details below. From Equation (9.4.4), starting with the previous point xs at time s, the solution xt at time is given by xt = αt αs xs αt λt λs eλ ˆϵϕ(ˆxλ, λ) dλ. (9.4.6) Therefore, we are led to approximate integrals of the form: λt λs eλ ˆϵϕ(ˆxλ, λ) dλ. On the interval λ [λs, λti], we approximate the integrand ˆϵϕ(ˆxλ, λ) in Equation (9.4.6) by Taylor expansion with respect to λ. For 1, the (n1)-th order Taylor expansion about λs is given by ˆϵϕ(ˆxλ, λ) = n1 k=0 (λ λs)k k! ˆϵ(k) ϕ(ˆxλs, λs) + O((λ λs)n), where the k-th total derivative with respect to λ is denoted by ˆϵ(k) ϕ(ˆxλ, λ) := dk dλk ˆϵϕ(ˆxλ, λ). Substituting this expansion into the integral in Equation (9.4.6) yields closed-form approximation, which defines the n-th order solver, referred to as DPM-Solver-n. 286 Sophisticated Solvers for Fast Sampling Starting from the previous step estimation xs, xt = αt αs xs αt n1 k=0 ˆϵ(k) ϕ(ˆxλs, λs) Ck + O(hn+1), (9.4.7) Here, we denote := λt λs, and define: Ck := λt λs eλ (λ λs)k k! dλ. Ck can be precomputed analytically by applying integration by parts times. We note that the change of variables 7 λ is used to smooth the integrand and derive coefficients, whereas the solver returns estimates xt on the t-grid. Below, we illustrate DPM-Solver-1 as an example. Example: DPM-Solver-1 Consider = 1 (first order) for demonstration . Starting from the previous estimated point xs, Equation (9.4.7) simplifies to: xt = = αt αs αt αs xs αtϵϕ(xs, s) λt λs eλ dλ + O(h2) xs σt(eh 1)ϵϕ(xs, s) + O(h2). (9.4.8) The above formula is exactly the DDIM update; we prove the equivalence in Proposition 9.4.2. DPM-Solver-n with 2 requires evaluating the kth-derivative ˆϵ(k) ϕ(ˆxλ, λ) for 1. However, directly computing higher-order derivatives is computationally expensive in practice. Lu et al. (2022b) also propose efficient approximation methods for these derivatives, which will be detailed in the next subsection. 9.4.3 Implementation of DPM-Solver-n DPM-Solver-n with 2. Solver-n entails the following: In practice, implementing higher-order DPMPrecomputing the coefficients Ck; 9.4. DPM-Solver 287 Approximating the kth derivative ˆϵ(k) ϕ(ˆxλ, λ) for 1 to circumvent the costly computation of exact higher-order derivativesa challenge well-studied in the ODE literature (Hochbruck and Ostermann, 2005; Luan, 2021). One common strategy is finite difference approximation. We now elaborate on the first two points. Precomputing Ck. Let and denote the start and end times, respectively, and define := λt λs. Starting from xs, the analytical expansion of the exact solution to Equation (9.1.6) reads: xt = αt αs xs σt n1 k=0 hk+1φk+1(h)ˆϵ(k) ϕ(ˆxλs, λs) + O(hn+1), (9.4.9) where each φk+1() admits closed-form. For = 0, 1, 2, they are: φ1(h) = eh 1 , φ2(h) = eh 1 h2 , φ3(h) = eh h2 2 1 h3 . Example: DPM-Solver-2/3 with Exact Derivatives For = 3 and discrete time steps with := λt λs, the expansion becomes: xt = αt αs xs σt (cid:16) eh 1 (cid:17) ˆϵϕ(ˆxλs, λs) (cid:16) (cid:17) eh 1 σt h2 2 σt eh + O(h4). ˆϵ(1) ϕ(ˆxλs, λs) ! ˆϵ(2) ϕ(ˆxλs, λs) (9.4.10) Approximating ˆϵ(k) ϕ (ˆxλ, λ) for 1. For 2, following the standard approach of single-step ODE solvers (Atkinson et al., 2009), Lu et al. (2022b) introduce an intermediate timestep smid between and to approximate higher-order derivatives using function evaluations at and smid. We illustrate this with the case of = 2. Let γ (0, 1] be hyperparameter specifying an interpolation point within the log-SNR interval [λs, λt]. Given an estimate xs at s, define smid = tλ (λs + γh) , where := λt λs, 288 Sophisticated Solvers for Fast Sampling The intermediate estimate is given by: xmid = αsmid αs This yields the following second-order approximation: (cid:17) xs σsmid eγh 1 (cid:17) (cid:16) (cid:16) ϵϕ(xs, s). xt = αt αs xs σt σt γh eh ϵϕ(xs, s) (cid:17) (cid:16) (cid:16) eh 1 (cid:17) ϵϕ(xmid, smid) ϵϕ(xs, s) + O(h3). (9.4.11) With γ = 1 2 , the two-stage update in Algorithm 5 is equivalent to Equation (9.4.11) up to O(h3) (local truncation error). Algorithm 5 DPM-Solver-2 (with γ = 1 2 ). Input: initial value xT , time steps {ti}M i=0, model ϵϕ 1: xt0 xT 2: for 1 to do hi λti λti1 3: smid tλ α (cid:16) λti1 +λti 4: (cid:17) (cid:19) xti1 σsmid (cid:18) hi 2 1 (cid:17) (cid:16) xti1 σti ehi 1 ϵϕ(xmid , smid ) ϵϕ(xti1, ti1) 5: 6: smid αti1 xmid xti αti αti 7: end for 8: return xtM Remark. In Equation (9.4.11), the difference quotient ˆϵ(1) ϕ(ˆxλs, λs) ϵϕ(xmid, smid) ϵϕ(xs, s) γh approximates the total λderivative of the model along the trajectory. This approximation is accurate up to O(h), and in Equation (9.4.11) it is multiplied by the exact φ2 coefficient eh 1 = O(h2). Hence, the resulting contribution is only O(h3), so the overall scheme achieves secondorder accuracy for any γ (0, 1]. Each step requires exactly two model evaluations: one at (xs, s) and one at the predicted midpoint (xmid, smid). The interpolation parameter γ does not affect the order of accuracy, but it changes the error constant: 9.4. DPM-Solver 289 setting γ = 1 which is why the midpoint version is preferred in practice. 2 symmetrizes the stencil and typically minimizes the constant, For higher-order DPM-Solver-n with 3, similar approach is employed, utilizing intermediate timesteps to approximate higher-order derivatives in finite difference manner. The detailed methodology is deferred to the original DPM paper. For readers familiar with numerical ODE solvers, DPM-Solver can be viewed as one-step exponential integrator for the semilinear PF-ODE, combined with change of time variable to the (half-)logSNR. Its secondand third-order variants are exponential RungeKuttatype schemes that use few staged model evaluations within each step. Implementation Detail: Selection of Sampling Timesteps. To perform i=0. Lu et al. sampling, solvers must first predefine sequence of timesteps {ti}M (2022b) propose selecting these steps based on uniform spacing in logSNR time λt, where λti = λT + (λ0 λT ), = 0, . . . , M. This differs from earlier approaches (Ho et al., 2020; Song et al., 2020c) that use uniform spacing directly in the physical time variable t. Empirically, DPMSolver achieves high-quality samples even with very few steps when using uniform λ spacing5. Conceptually, this can be understood geometrically: the accuracy of the local Taylor approximation depends on how smoothly the dynamics evolve in λ. Uniform spacing in λ therefore yields approximately uniform local error across the trajectory, resulting in finer (denser) steps in where the signal dominates (high SNR), and coarser (sparser) steps in the noise-dominated regime. Although the derivation operates in λspace and the PFODE is formulated in convenient semilinear form in that domain, the pre-trained model and noise schedules (αt, σt) are usually defined with respect to the original time variable t. During sampling, the solver selects nodes that are uniformly spaced in λ for numerical stability, but all update equations are expressed in t. Whenever it needs to evaluate the model or retrieve schedule values, the chosen λ node is mapped back to the corresponding time variable, such as the 5Alternatively, adaptive step-size strategies dynamically adjust the timesteps by combining solvers of different orders; see Appendix of Lu et al. (2022b). Sophisticated Solvers for Fast Sampling physical time = tλ(λ) or the variance parameter σt, depending on how the model is parameterized (see, for instance, Algorithm 5). 9.4.4 DDIM = DPM-Solver-1 For fixed schedule (αt, σt), the DPM-Solver-1 step coincides with the deterministic DDIM (η = 0) update, independent of the time parameterization (physical time or logSNR time λ); see the formal statement below. Proposition 9.4.2: DDIM is DPM-Solver-1 The update rule of DDIM, given in Equation (9.2.2), is identical to that of DPM-Solver-1, given in Equation (9.4.8). Proof for Proposition. By the definition of λ, we have σs αs = eλs and σt αt = eλt. (9.4.12) Substituting these expressions, along with = λt λs, into Equation (9.2.2) recovers the update rule in Equation (9.4.8), completing the equivalence. The above proposition may explain why DDIM outperforms traditional Euler methods in t-parametrization: it effectively exploits the semilinearity of the diffusion ODE under more suitable λ-reparametrization. Remark. When the Score SDE paper appeared, RungeKutta (RK45) was commonly used to solve the vanilla PF-ODE in Equation (4.2.5), but the semilinearity of its drift remained unexploited. Although DPM-Solver-k (k 2) is related to RungeKutta methods, it explicitly leverages this semilinearity via time reparameterization. This explains why DPM-Solver attains higher-order accuracy with far fewer function evaluations, reducing typical DDIM schedule of several hundred steps to about 1015 steps while preserving high sample quality. 9.4. DPM-Solver 291 9.4.5 Discussion on DPM-Solver-2 and Classic Heun updates In Section 9.2.2, we saw that different parameterizations of the PFODE lead to different interpretations of classical Eulertype updates: v-prediction: Euler = DDIM, ϵ-, x-, or s-prediction: expEuler = DDIM = plain Euler. In this subsection, we further illustrate the connection by examining the analogous relationship between the classic Heuns method and the 2ndorder DPMSolver across the four parameterizations. To set the stage, we briefly recall Heuns method (see also Section A.1.4). Heuns method is 2ndorder solver that refines Eulers method using predictorcorrector scheme: it first makes an Euler prediction to the end of the step, evaluates the slope there, and then updates using the average of the starting and predicted slopes. Intuitively, it advances along the curve by following the mean slope over the interval (the area of trapezoid), achieving much higher accuracy than plain Euler. We work in the logSNR time λ, where the PFODE can be expressed in simple linear + nonlinear form: dˆx(λ) dλ = L(λ)ˆx(λ) } {z linear part + N(cid:0)ˆx(λ), λ(cid:1) } {z nonlinear part , where the scalar L(λ) is determined by the noise schedule and N(, λ) collects the nonlinear part. This structure naturally arises from Equation (6.3.2): the ϵ-, x-, and s-prediction parameterizations yield nonzero L(λ), resulting in semilinear form. In contrast, v-prediction corresponds to L(λ) 0 (so = v), leaving no explicit linear term. In the remainder of our discussion, we first recall the plain Heun update without considering any semilinear structure, and then introduce the exponential Heun update, which is designed for semilinear ODEs and treats the linear part exactly, analogous to the exponential Euler step in Equations (9.1.7) and (9.1.8). Finally, we relate both Heun updates to DPM-Solver-2 under the four parameterizations and conclude: v-prediction: Heun = DPM-Solver-2, ϵ-, x-, or s-prediction: exp-Heun = DPM-Solver-2 = plain Heun. 292 Sophisticated Solvers for Fast Sampling Figure 9.3: Plain Heun update in log-SNR time. Starting from the previous state ˆxi1 at λi1, the predictor step (blue arrow) performs an explicit Euler move hF(ˆxi1, λi1) to obtain the intermediate estimate ˆxmid . At this predicted point, the corrector step evaluates the new slope hF(ˆxmid , λi) (green arrow) and combines both slopes through parallelogram construction: the dashed orange diagonal represents the vector sum h(cid:0)F(ˆxi1, λi1)+F(ˆxmid , λi)(cid:1) starting from ˆxi1, and the solid orange arrow is its half-diagonal, having the same direction but half the length. This procedure realizes the plain Heun integration of the PF-ODE trajectory in log-SNR time. i Plain Heun update. Denote λi := λti, then {λi}M i=0 is an increasing grid in the log-SNR domain, and set := λi λi1 > 0. Let ˆxi1 denote the previous iterate in log-SNR time. Applied directly to the full drift F(ˆx, λ) := L(λ)ˆx + N(ˆx, λ), the plain Heun update in log-SNR-time is given by Predict: Correct: ˆxmid = ˆxi1 + hF(ˆxi1, λi1), (cid:16) ˆxi = ˆxi1 + 2 F(ˆxi1, λi1) + F(ˆxmid (cid:17) , λi) . (9.4.13) Exponential Heun update (for Semilinear PF-ODE). With the exponential integrator technique, the idea is to treat the linear and nonlinear parts of the ODE differently. The linear term L(λ)ˆx is integrated exactly over the step, while the nonlinear term N(ˆx, λ) is only approximated by averaging its effect across the step. 9.4. DPM-Solver 293 To express this neatly, we introduce the quantity := λi λi1 L(τ ) dτ, which represents the total contribution of the linear coefficient L(λ) over the interval [λi1, λi]. Using E, we define two helper coefficients c1(E) and c2(E) that handle both cases: when is nonzero and when it vanishes: c1(E) = eE 1 , if = 0, 1, if = 0, c2(E) = eE 1E 2 , 1 2 , if = 0, if = 0. The second case simply ensures continuity when the linear term disappears (L(λ) = 0), so that the formulas remain valid and reduce smoothly to the standard Heun update as in Equation (9.4.13). With these coefficients, one update step of the exponentialHeun scheme can be written as: = eE ˆxi1 + hc1(E)N(ˆxi1, λi1), ˆxmid ˆxi = eE ˆxi1 + hc1(E)N(ˆxi1, λi1) Correct: Predict: (9.4.14) + hc2(E) (cid:16) N(ˆxmid , λi) N(ˆxi1, λi1) (cid:17) . When L(λ) 0, the coefficients simplify to c1 = 1 and c2 = 1 2 , and the method reduces to the plain Heun solver in Equation (9.4.13). When L(λ) = 0, the exponentialintegrator form of the update integrates the linear term exactly, while the plain Heun method only provides an approximation. To see this, expand the exponential term for small stepsize = λi λi1 > 0. Since = λi λi1 L(τ ) dτ = hL(λi1) + O(h2), eE = 1+E + 2 6 +O(E 3), c2(E) = 2 +O(E 3), c1(E) = 1+ we can treat as small quantity of order O(h). The Taylor expansions give: 2 + 2 24 +O(E 3). Substituting these approximations into Equation (9.4.14) and keeping terms up to 2 (that is, up to order h2 since = O(h)), the update simplifies exactly to the plain Heun form (Equation (9.4.13)). The remaining difference between the two schemes appears only in higher-order terms of size O(E 3) = O(h3). Intuitively, when the step size is small, is also small, so the exponential factors reduce to 6 + 2 2 + eE 1 + E, c1(E) 1, c2(E) 1 2 . 294 Sophisticated Solvers for Fast Sampling The linearhandled exponentialHeun update thus collapses to the plain Heun step. Connection of Heuns Updates to DPMSolver-2 Under the Four Predictions. We highlight that, in the ϵ-prediction form of the PF-ODE (see Equation (9.4.5)), the dynamics in logSNR time λ naturally take the required semilinear form: dˆxλ dλ = α λ αλ {z} =:L(λ) ˆxλ + (cid:0) σλ ˆϵϕ(ˆxλ, λ)(cid:1) } {z =:N(ˆxλ,λ) . Consequently, for ϵ-prediction in logSNR time λ, the exponentialHeun update in Equation (9.4.14) is exactly equivalent to DPM-Solver-2 (with midpoint parameter γ = 1 2 ; see Algorithm 5). Similarly, under the xand s-prediction parameterizations in logSNR time, their PF-ODEs also take the same semilinear structure. Hence, the DPMSolver-2 under the ϵ-, x-, or s-prediction is identical to the exponentialHeun update in Equation (9.4.14). In contrast, the v-prediction form naturally removes the linear term, so its PF-ODE does not require an exponential integrator; the plain Heun method in logSNR time already provides the correct 2ndorder update. Similar to the case of Euler versus exponential Euler in DDIM, we therefore conclude the following: Observation 9.4.1: Heun and DPM-Solver-2 Updates Given the PF-ODEs in log-SNR time λ, v-prediction: Heun = DPM-Solver-2, ϵ-, x-, or s-prediction: exp-Heun = DPM-Solver-2 = plain Heun, where, in the ϵ-, x-, or s-prediction cases, the plain Heun step is not equivalent to DPM-Solver-2, since the linear term is only approximated instead of being integrated exactly. 9.5. DPM-Solver++ 9.5 DPM-Solver++ 295 9.5.1 From DPM-Solver to DPM-Solver++ for Guidance High-order solvers enable faster sampling without guidance. However, diffusion models are prized for their controllable and flexible generation, typically achieved via guidance (see Chapter 8 for details). DPM-Solver++ (Lu et al., 2022c) identifies key limitation of prior highorder solvers: they suffer from stability issues and may become slower than DDIM under large guidance scales (stronger condition). The authors attribute this instability to the amplification of both the output and its derivatives by large guidance scales. Since high-order solvers depend on higher-order derivatives, they are especially sensitive to this effect, resulting in diminished efficiency and stability. 9.5.2 DPM-Solver++s Methodology To address the aforementioned issues, DPM-Solver++ proposes: 1. adopting x-prediction parameterization instead of ϵ-prediction; 2. applying thresholding methods (e.g., dynamic thresholding (Saharia et al., 2022)) to keep the predicted data within the training data bounds (mitigating the train-test mismatch at large guidance scales). We elaborate on the first point. Recall from Equation (6.3.1) that the data and noise parameterizations are linearly related: ϵϕ(xt, t) = xt αtxϕ(xt, t) σt . Using this relation, DPM-Solver++ rewrites the exact solution eΨst(xs) of the empirical PF-ODE (originally expressed in the noise parameterization in Equation (9.4.4)), starting from any xs: eΨst(xs) = αt αs xs αt into the data parameterization as eΨst(xs) = σt σs xs + σt λt λs λt λs eλ ˆϵϕ(ˆxλ, λ)dλ, eλ ˆxϕ(ˆxλ, λ)dλ, where we follow the notations in Equation (9.4.3) and further denote: ˆxλ := xtλ(λ), ˆxϕ(ˆxλ, λ) := xϕ(xtλ(λ), tλ(λ)). 296 Sophisticated Solvers for Fast Sampling Based on the x-prediction, DPM-Solver++ provides two solver variants: Higher-Order Single-Step Solver: Introduced in Section 9.5.3. This approach is analogous to that in DPM-Solver, which leverages higher-order Taylor expansions to approximate the integration, but here formulated with the x-prediction. The update uses only one previous point to estimate the next step. Multistep (Two-Step) Solver: Introduced in Section 9.5.4. The design philosophy is similar to DEIS (also multistep); however, DPM-Solver++ specifically reuses two previous points (whereas DEIS allows general order) to estimate the next step. Each update requires only single new diffusion model evaluation. 9.5.3 DPM-Solver++ Single-Step by Taylor Expansion Following similar approach to Section 9.4.3, DPM-Solver++ derives higherorder solvers in the x-parameterization. For 0, denote the n-th total derivative of ˆxϕ with respect to λ, evaluated at λi1, by ˆx(n) ϕ(ˆxλi1, λi1) := (cid:12) dn (cid:12) dλn ˆxϕ(ˆxλ, λ) (cid:12) (cid:12)λ=λi1 . Given the previous estimate xti1 at time ti1, using the (n 1)-th Taylor expansion at λti1 to approximate ˆxϕ(ˆxλ, λ) for λ [λti1, λti] (with = ti1 and = ti) yields the following approximation of eΨst(xs): xti = σti σti1 xti1 + σti n1 k= ˆx(k) ϕ(ˆxλi1, λi1) {z } estimated via finite difference λti λti1 eλ (λ λti1)k k! dλ {z analytically computable } + O(hn+1 ). where hi := λti λti1 > 0. As in Equation (9.4.9), the integral admits the closed form λti λti1 eλ (λ λti1)k k! dλ = eλti1 k+1 φk+1(hi), φm(h) := eh Pm1 j=0 hm hj j! . This yields the DPM-Solver++s single-step update (one previous point to estimate the next). When = 1, it reduces to the DDIM update. When = 2 and ˆx(1) ϕ(ˆxλi1, λi1) is approximated via finite difference, it gives DPMSolver++(2S), an update analogous to DPM-Solver-2 in Algorithm 5 but using the x-prediction. DPM-Solver++(2S)s algorithm is shown in Algorithm 6. 9.5. DPM-Solver++ Algorithm 6 DPM-Solver++(2S): midpoint special case. Input: initial value xT , time steps {ti}M 1: xt0 xT ; λti log(αti/σti) 2: ˆx0 ˆxϕ(xt0, t0) 3: for 1 to do hi λti λti1; 4: smid σti1 (cid:0)1 ehi/2(cid:1) ˆxi1 (cid:16) λti1 +λti 2 xti1 + αsmid smid tλ 5: (cid:17) σ i=0, data-prediction model ˆxϕ log-SNR at the grid cache at start forecast to midpoint one new model call at the midpoint (cid:0)ehi 1(cid:1)Dmid cache for next step 6: 7: ˆxϕ(ui, smid ) xti1 αti ui Dmid xti σti σti1 ˆxi ˆxϕ(xti, ti) 8: 9: end for 10: return xtM 9.5.4 DPM-Solver++ Multistep by Recycling History Highorder singlestep solvers rely (explicitly or implicitly) on higher derivatives of the model output; under strong CFG these derivatives can be strongly amplified and destabilize the update. DPM-Solver++ mitigates this with multistep (Adamstype) strategy in log-SNR time λ: it reuses short history of past data-prediction evaluations along the trajectory to approximate the needed derivatives via finite differences. This reuse requires only one new model call per step. As with DEIS, we separate the presentation into: Case 1. the warm start with no history (first step); Case 2. subsequent steps with two history anchors. Case I. DPM-Solver++ with One History Anchor (i = 1). For the first step (i = 1; no history), use the firstorder DPM-style update (which matches the deterministic DDIM step in data prediction). Let h1 = λ1 λ0. xt1 = σt1 σt0 xt0 + σt1 eλ0 (eh1 1)ˆxϕ(xt0, t0) Case II. DPM-Solver++ with Two History Anchors (i 2). After the warm start, the twostep multistep update reuses the estimations at time ti2 with xti2 and at time ti1 with xti1. At each step 2, these provide the two most recent anchors, equivalently in λtime: (λi1, ˆxϕ(xti1, ti1)) and (λi2, ˆxϕ(xti2, ti2)), 298 Sophisticated Solvers for Fast Sampling to compute the update xti using only these cached anchors (no fresh model call is needed to form the update). After obtaining xti, we evaluate the model once at (xti, ti) and cache ˆxϕ(xti, ti). This evaluation is performed during step and is used as an anchor in the subsequent step i+1. Namely, we aim for onecallperstep update that remains stable under large guidance by discretizing the exact xprediction form eΨst(xs) = σt σs xs + σt λt λs eλ ˆxϕ(ˆxλ, λ) dλ. Over single step [λi1, λi], we treat the linear ODE part exactly and approximate the residual integral by approximating the integrand as function linear in λ (since there are two anchor points). Concretely, we approximate on [λi1, λi] by the affine model λ 7 ˆxϕ(ˆxλ, λ) ˆxϕ(ˆxλ, λ) L(λ) := a0 + a1(λ λi1), λ [λi1, λi], where λi = λti, hi = λi λi1 > 0, and the coefficients a0 and a1 are uniquely specified by the straight line passing through the two most recent anchors: a0 = ˆxϕ(xti1, ti1), a1 = ˆxϕ(xti1, ti1) ˆxϕ(xti2, ti2) hi . Substituting L(λ) into the integral thus yields6 λi λi eλ ˆxϕ(xλ, λ) dλ σti eλL(λ) dλ σti λi1 λi1 λi ! eλ dλ σti λi a0 + σti eλ(λ λi1) dλ a1 ! = = (cid:16) λi1 (cid:17) αti(1 ehi) (cid:16) a0 + β(hi)a1 a0 + = αti(1 ehi) (cid:16) (cid:17) , λi1 αti(hi 1 + ehi) (cid:17) a1 6The second identity follows from straightforward algebra. The two needed exponential moments are λi λi1 eλ dλ = eλi1 (ehi 1), λi λi eλ(λ λi1) dλ = eλi1 (cid:0)hiehi ehi + 1(cid:1). Multiplying by the prefactor σti from the exact form and using αt = σteλt (so σti eλi1 = αti ehi ) gives the convenient coefficients σti λi λi1 eλ dλ = αti (1 ehi ), σti λi λi1 eλ(λ λi1) dλ = αti (hi 1 + ehi ). 9.5. DPM-Solver++ where β(h) := h1+eh estimate for xti as: 1eh . Until this point, we have already reached valid xti = σti σti1 xti1 + αti (cid:0)1 ehi (cid:1)Di, with Di = a0 + β(hi)a1. In practice, we can obtain simplified update rule with the same local truncation error (provided the step ratios are bounded) as the above one: xti = σti σti1 xti1 + αti (cid:0)1 ehi (cid:1)Dsim (xti1, xti2). Here, we define the step ratio ri = hi/hi1, and Dsim (xti1, xti2) := (cid:16) 1 + 1 2 ri (cid:17) ˆxϕ(xti1, ti1) 1 2 ri ˆxϕ(xti2, ti2). with local error O(h ) under standard smoothness assumptions. To see why, for notational simplicity, we write a0 = ˆxϕ(xti1, ti1) =: ˆxi1, a1 = ˆxi1 ˆxi2 hi1 . Then = ˆxi1 + Di := a0 + β(hi) a1 β(hi) hi1 (cid:17) ˆxi1 ri (cid:17) 1 + ri 2 h(cid:16) = (cid:16) = 1 + 1 2 ri + O(h2 ) = Dsim (cid:0)ˆxi1 ˆxi2 (cid:1) 2 ˆxi2 + (cid:16) β(hi) hi1 ˆxi1 1 2 ri ˆxi2 + O(h2 ) ri (cid:17)(cid:0)ˆxi1 ˆxi2 (cid:1) Here, we use that for small steps, Taylor expansion of β(h) at = 0 gives β(h) = 2 + O(h2) = β(hi) hi1 = hi 2hi1 + O(h2 /hi1) = ri + O(h2 /hi1), and that ˆxi1 ˆxi2 = O(hi1) under some smoothness assumption. Remark. If the log-SNR steps are uniform (every step has the same size h, so hi Sophisticated Solvers for Fast Sampling and ri = hi/hi1 = 1), then the two-anchor blend Dsim = (cid:16) 1 + 2 ri (cid:17) ˆxi1 1 2 ri ˆxi2 reduces to the classic constants Dsim = (cid:16) 1 + 1 2 1 (cid:17) ˆxi1 2 1 ˆxi2 = 3 2 ˆxi1 1 2 ˆxi2. Those ( 3 i.e., the standard two-step linear multistep coefficients. 2 ) are exactly the Adams-Bashforth 2 weights for uniform steps, 2 , Algorithm 7 DPM-Solver++(2M). Input: initial value xT , time steps {ti}M i=0, model ˆxϕ 1: xt0 xT ; λti log(αti/σti); hi λti λti1 2: ˆx0 ˆxϕ(xt0, t0) cache at start Case I. Warm start (i = 1) with one anchor (DDIM in x-pred.) 3: xt1 σt1 xt0 αt1 σt0 4: ˆx1 ˆxϕ(xt1, t1) (cid:0)eh1 1(cid:1) ˆx0 One model call & cache Case II. Using two history cached anchors (multistep) 5: for 2 to do 6: (cid:16) 1 + 1 ri hi/hi1 Dsim xti σti σti1 ˆxi ˆxϕ(xti, ti) 2 ri 7: 8: 9: 10: end for 11: return xtM (cid:17) ˆxi1 1 2 ri ˆxi2 (cid:0)1 ehi (cid:1) Dsim xti1 + αti step ratio One model call & cache 9.6. PF-ODE Solver Families and Their Numerical Analogues 9.6 PF-ODE Solver Families and Their Numerical Analogues In this section, we first place the PF-ODE solvers introduced so far (DDIM, DEIS, DPM-Solver, DPM-Solver++) into the context of classical numerical integration methods. We then turn to closer examination of two representative higherorder solvers, DEIS and DPM-Solver++, and compare their respective designs. 9.6.1 PF-ODE Solver Families and Classical Counterparts The diverse families of PF-ODE samplers can be understood through the lens of classical numerical analysis. Once the linear drift is treated by an integrating factor, each sampler aligns naturally with an established timestepping scheme: Euler-type methods, AdamsBashforth (AB) multistep schemes, or RungeKutta (RK) singlestep integrators. We summarize these correspondences in Table 9.1. Table 9.1: PF-ODE samplers and their numerical-analysis analogues. exp. denotes integrating-factor (semilinear) treatment of the linear term (see Equation (9.1.6)). AB = Adams-Bashforth, RK = Runge-Kutta. See Algorithm 5 for DPM-Solver-2. PF-ODE Solver Type Classical Numerical Analogue DDIM DEIS single step v-prediction: plain Euler; ϵ/x/s-prediction: exp. Euler multistep exp. AB (nth-order) DPM-Solver-n single step exp. RK (nth-order) in log-SNR DPM-Solversingle step v-prediction: plain Heun in log-SNR (2nd-order); ϵ/x/s-prediction: exp. Heun in log-SNR (2nd-order) DPM-Solver++ 2S single step exp. RK (2nd-order) DPM-Solver++ 2M multistep exp. AB (2nd-order) We highlight two representative examples in Table 9.1: the DDIM and DPMSolver2 cases. With fixed scheduler (αt, σt), we emphasize the illustrative results from Sections 9.2.2, 9.3.3 and 9.4.4: regardless of whether we use logSNR time or the original physical time, v-prediction: DDIM = DPM-Solver-1 = DEIS-1 = Euler, ϵ-, x-, or s-prediction: DDIM = DPM-Solver-1 = DEIS-1 = exp Euler. In Section 9.4.5, we extended this analogy by examining how DPM-Solver-2 Sophisticated Solvers for Fast Sampling relates to the classic Heun solver under the four parameterizations: v-prediction: DPM-Solver-2 = Heun, ϵ-, x-, or s-prediction: DPM-Solver-2 = exp-Heun = plain Heun. more general correspondence between DPM-Solver-n and classical RK methods can be understood in the same way. 9.6.2 Discussion on DEIS and DPM-Solver++ Aspect DEIS DPM++ Core Viewpoint Exponentialintegrator: integrates the linear term exactly; approximates the nonlinear residual by polynomial over past nodes. Same logSNR time λ with data prediction. integrator idea; formulated in Step type Multistep only Singlestep (2S) and Multistep (2M) Polynomial Basis Lagrange interpolation across past anchors (highorder multistep). Solvers Order Highorder multistep (general r). History Use Uses r+1 past evaluations to build highorder update. divided differences (NewBackward ton/Adamstype) 2M; algebraically spans the same polynomial space as Lagrange, but not presented as Lagrange fit. in λtime for Higherorder singlestep methods exist (though 2S is the main focus), and 2ndorder multistep (2M) scheme is provided; higherorder multistep variants are not covered. 2S: one intermediate eval (singlestep). 2M: reuses two anchors; after warm start, one model call per step. DEIS vs. DPM-Solver++. Both DEIS and DPM++ are exponential integrator samplers that integrate the linear part exactly and approximate the residual integral by lowdegree polynomial. In unconditional generation, both can achieve high fidelity with as few as 1020 ODE steps. For conditional generation with CFG, however, DPM++ is often preferable due to its stability under large guidance scales. We summarize the comparison between DEIS and DPM++, and provide further discussion below. DEIS. It is multistep method obtained by fitting polynomial to the nonlinear term across past nodes in the Lagrange basis (interpolation through anchors). DPM-Solver++. It works with data prediction in logSNR time: its singlestep (2S) variant uses Taylor/exponentialintegrator step with one 9.6. PF-ODE Solver Families and Their Numerical Analogues 303 intermediate evaluation, while its multistep (2M) variant reuses history via backward divided differences, which produce the same interpolating polynomial but expressed in the Newton (finitedifference) basis. In other words, for the same anchor points and function values, the Lagrange and Newton forms are two different coordinate systems for the same polynomial interpolant: Lagrange expresses it as sum of function values times cardinal basis polynomials, whereas Newton expresses it as product expansion with coefficients given by divided differences (finitedifference ratios that are easy to update in multistep schemes). The DPM++ paper emphasizes secondorder (2S/2M); higherorder multistep extensions can, in principle, be constructed using higherorder Newton bases. 304 Sophisticated Solvers for Fast Sampling 9.7 (Optional) DPM-Solver-v3 Both DPM-Solver and DPM-Solver++ design their solvers based on specific parameterizations of the diffusion model (ϵ-/x-prediction), which lacks principled approach for selecting the parametrization and may not represent the optimal choice. In this section, we introduce DPM-Solver-v3 (Zheng et al., 2023), which addresses this issue and enhances sample quality with fewer timesteps or at large guidance scales. DPM-Solver-v3 can be regarded as the culmination of insights of the entire DPM-Solver family (Lu et al., 2022b; Lu et al., 2022c). High-Level Overview of DPM-Solver-v3. We continue to focus on the key principle of DPM-Solver (Lu et al., 2022b), namely the time reparametrization in SRN as expressed in Equation (9.4.5): dxλ dλ = α λ αλ xλ σλ ˆϵϕ(xλ, λ). The core idea of DPM-Solver-v3 (Zheng et al., 2023) is to introduce three additional underdetermined/free variables into Equation (9.4.5), enabling the original ODE solution to be reformulated equivalently with new model parameterization. An efficient search method is then proposed to identify an optimal set of these variables, computed on the pre-trained model, with the objective of minimizing discretization errors. 9.7.1 Insight 1: Adjusting the Linear Term in Equation (9.4.5) The PF-ODE is stiff ODE with distinct timescales in each direction of temporal evolution, complicating its solution with fewer timesteps. Drawing from classic stiff ODE theory (Hochbruck and Ostermann, 2010), Zheng et al. (2023) propose modifying the linear part of the ODE to better handle these stiff dynamics. We begin by motivating this approach from classic numerical ODE theory. Motivation: From Classic Stiff ODE Solvers. We begin by considering an abstract form of Equation (9.4.5): dxλ dλ = v(xλ, λ), where v(x, λ) represents the vector field. 9.7. (Optional) DPM-Solver-v3 305 Rosenbrock-type exponential integrators are class of methods developed to efficiently solve stiff ODEs. The key feature of these methods is the flexibility in selecting the linear operator L, which decomposes the vector field as: v(x, λ) = Lx + N(x, λ), where N(x, λ) denotes the nonlinear remainder. This leads to the following update, starting from xλs, by applying the exponential integrator technique (as usual): eΨλsλt(xλs) = ehLxλs + λt λs e(λtτ )LN(xτ , τ ) dτ, with := λt λs. We observe that the linear part is in exponential form. The choice of is typically made by utilizing preconditioning information to handle stiffness more efficiently, with the goal of (1) ensuring the stability of the method, (2) improving the convergence rate of the numerical solution, and (3) ensuring that eλL remains computationally efficient. Applying the Above Idea to PF-ODE in Equation (9.4.5). We apply the introduced concept to Equation (9.4.5): dxλ dλ = α λ αλ xλ σλ ˆϵϕ(xλ, λ) , } {z v(xλ,λ) which we rewrite as: dxλ dλ = (cid:17) ℓλ (cid:16) α λ αλ {z linear part xλ } (cid:0)σλ ˆϵϕ(xλ, λ) ℓλxλ {z nonlinear part (cid:1) . } (9.7.1) Here, ℓλ is D-dimensional free/undetermined variable that depends solely on λ. For notational convenience, we denote the linear and nonlinear parts as L(λ) := α λ αλ ℓλ, Nϕ(xλ, λ) := σλ ˆϵϕ(xλ, λ) ℓλxλ. (9.7.2) Zheng et al. (2023) propose selecting ℓλ by solving the following simple least-squares problem: ℓ λ = arg min ℓλ xλpϕ λ (xλ) xNϕ(xλ, λ)2 , (9.7.3) 306 Sophisticated Solvers for Fast Sampling where denotes the Frobenius norm, and pϕ distribution of samples along the ODE trajectory in Equation (9.7.4) at λ. represents the marginal λ We note that ℓλ = ℓ λ can be solved analytically. This selection leverages preconditioning information from pre-trained models, conceptually making Nϕ less sensitive to errors in (as the Lipschitzness of Nϕ, which is approximately the x-gradient, is reduced), and cancels the linearity of Nϕ. 9.7.2 Insight 2: Introducing Free Variables in Model Parameterization to Further Minimize Discretization Error The PF-ODE exhibits semilinear structure (see Equation (9.7.1) and Equation (9.7.2)). For the sake of notational clarity, we consider the following (abstract) formulation of the empirical PF-ODE: dxλ dλ = L(λ)xλ + Nϕ(xλ, λ). (9.7.4) Motivation: Understanding Discretization Errors and Strategies for Their Minimization. As usual, the exact solution to this empirical ODE over the interval [λs, λt] can be expressed using the variation-of-parameters formula: eΨλsλt(xλs) = E(λs λt)xλs + (cid:1) λt λs E(λ (cid:1) λs)Nϕ(xλ, λ) dλ, (9.7.5) where E(s (cid:1) t) := t L(u) du. By using the estimation Nϕ(xλs, λs) Nϕ(xλ, λ) for λ [λs, λt], we can obtain an approximate solution which is given by: xλt = E(λs λt)xλs + (cid:1) λt λs E(λ (cid:1) λs)Nϕ(xλs, λs) dλ. (9.7.6) Subtracting Equation (9.7.5) and Equation (9.7.6), and expanding Nϕ(xλs, λs) in Taylor series as: Nϕ(xλs, λs) = Nϕ(xλ, λ) + (λs λ)N(1) ϕ(xλ, λ) + O((λs λ)2), we can quantify the first-order discretization error: xλt eΨλsλt(xλs) = λt λs E(λ (cid:1) λs)(λs λ)N(1) ϕ(xλ, λ) dλ + O(h3), where := λt λs. This observation reveals that the discretization error depends on N(1) ϕ(xλ, λ). 9.7. (Optional) DPM-Solver-v3 307 To reduce this error, Zheng et al. (2023) propose rewriting Equation (9.7.5) ϕ (xλ, λ), such that into an equivalent form using new parameterization, Nnew the error term retains the similar structure: xλt eΨλsλt(xλs) = λt λs E(λ (cid:1) λs)(λs λ)Nnew,(1) ϕ (xλ, λ) dλ + O(h3). Furthermore, the λ-derivative of Nnew ϕ (xλ, λ) satisfies: Nnew,(1) ϕ (xλ, λ) N(1) ϕ(xλ, λ) (cid:0)aλNϕ(xλ, λ) + bλ (cid:1) . (9.7.7) Here, aλ and bλ are free/undetermined variables. The goal is then to determine the optimal values of aλ and bλ that minimize (xλ, λ). This can be accomplished the discretization error by reducing Nnew,(1) by solving the following least squares optimization problem: ϕ (a λ, λ) = arg min aλ,bλ xλpϕ λ (xλ) N(1) ϕ(xλ, λ) (cid:0)aλNϕ(xλ, λ) + bλ (cid:1) 2 2 . (9.7.8) Notably, Equation (9.7.8) admits an analytical solution, depending on the pre-trained diffusion model, which can be precomputed. Realizing the Strategy for Minimizing Discretization Error. We begin by considering linearly transformed version of Nϕ(xλ, λ), defined as: ϕ (xλ, λ) := λ Nnew λs au duNϕ(xλ, λ) λ r λs λs au dubr dr. (9.7.9) We can then easily compute its λ-derivative given by: Nnew,(1) ϕ (xλ, λ) = λ λs au duh N(1) ϕ(xλ, λ) (cid:0)aλNϕ(xλ, λ) + bλ (cid:1)i , (9.7.10) which takes the desired form as in Equation (9.7.7). Using this, Nϕ(xλ, λ) can be rewritten as: Nϕ(xλ, λ) = λ λs au duh λs λ λ au duNϕ(xλ, λ) {z ϕ (xλ,λ) λ Nnew λs + λ λs = au duh Nnew ϕ (xλ, λ) + λ r λs λs λs au dubr dr r λs au dubr dr } r λs au dubr dr Sophisticated Solvers for Fast Sampling With this reformulation, we can rewrite Equation (9.7.5) and Equation (9.7.6) as: eΨλsλt(xλs) = E(λs λt)xλs + (cid:1) λt λs E(λ (cid:1) λ λs λs)e au du Nnew ϕ (xλ, λ) + λ r λs λs au dubr dr dλ (9.7.11) xλt = E(λs λt)xλs + (cid:1) λt λs E(λ (cid:1) λ λs λs)e au du Nnew ϕ (xλs, λs) + λ r λs λs au dubr dr dλ (9.7.12) Subtracting these two equations and employing the Taylor expansion: Nnew ϕ (xλs, λs) = Nnew ϕ (xλ, λ) + (λs λ)Nnew,(1) ϕ (xλ, λ) + O(cid:0)(λs λ)2(cid:1), we arrive at: xλt eΨλsλt(xλs) λt = E(λ λs)(λs λ)e λs λt λs = E(λ (cid:1) (cid:1) λ λs au duNnew,(1) ϕ (xλ, λ) dλ + O(cid:0)h3(cid:1) λs)(λs λ) N(1) ϕ(xλ, λ) (cid:0)aλNϕ(xλ, λ) + bλ (cid:1)i dλ + O(cid:0)h3(cid:1) Here, the last equality follows from Equation (9.7.10), which is central to our λ λs design, and it cancels out the factor au du. Thus, by solving Equation (9.7.8), we can determine the optimal coefficients (a λ, λ), effectively minimizing the discretization error. 9.7.3 Combining Both Insights. We now summarize the discussion so far. Procedure in Theory. For any λ, we first compute ℓ the least squares problem in Equation (9.7.3): λ analytically by solving ℓ λ = arg min ℓλ xλpϕ λ (xλ) xNϕ(xλ, λ)2 , where Nϕ(xλ, λ) is defined in Equation (9.7.2). Next, we compute (a λ, λ) analytically by solving the least squares problem in Equation (9.7.8), with 9.7. (Optional) DPM-Solver-v 309 ℓλ = ℓ λ fixed: (a λ, λ) = arg min aλ,bλ xλpϕ λ (xλ) N(1) ϕ(xλ, λ) (cid:0)aλNϕ(xλ, λ) + bλ (cid:1) 2 2 . Consequently, the resulting xλt, as defined in Equation (9.7.12) (with ℓλ, aλ, and bλ replaced by ℓ λ, λ in Equation (9.7.9)), serves as the desired estimation of eΨλsλt(xλs). λ, and Implementation Considerations. Although ℓ λ have analytical solutions involving the Jacobian-vector product of the pre-trained diffusion model ϵϕ (as detailed in Appendix C.1.1 of Zheng et al. (2023)), their computation requires evaluating expectations over pϕ λ . λ, and λ, In practice, these quantities are estimated via Monte Carlo (MCMC) approach. Specifically, batch of datapoints xλ pϕ (roughly 1K-4K samples) is drawn by applying an alternative solver (e.g., the 200-step DPM-Solver++ (Lu et al., 2022c)) to Equation (9.4.5), after which the relevant terms related to ϵϕ are computed analytically. Importantly, these statistics can all be precomputed, ensuring that when DPM-Solver-v3 is applied, the computational overhead associated with these calculations is avoided. λ 9.7.4 Higher-Order DPM-Solver-v The precomputed statistics ℓ λ, which are derived by analyzing the first-order discretization error, can also be employed to construct higher-order solvers (see also Section 9.7.5 for further interpretation). λ, and λ, To obtain the (n + 1)-th order approximation of Equation (9.7.11), we utilize the n-th order Taylor expansion of Nnew ϕ (xλ, λ) with respect to λ at λs, neglecting terms of order O(cid:0)(λs λ)(n+1)(cid:1). This allows us to approximate Nnew ϕ (xλ, λ) for λ [λs, λt]: Nnew ϕ (xλ, λ) Nnew ϕ (xλs, λs) + k=1 (λ λs)k k! Nnew,(k) ϕ (xλs, λs), 310 Sophisticated Solvers for Fast Sampling where this approximation leads to the estimated solution to Equation (9.7.11): xλt = E(λs λt)xλs + (cid:1) λt λs E(λ \" k=0 (λ λs)k k! λs)e (cid:1) {z =:E(λs λ λs udu } λ) (cid:1) Nnew,(k) ϕ (xλs, λs) + λ λs λs r {z =:B(λs udub # rdr } λ) (cid:1) dλ. (9.7.13) = E(λs (cid:1) + λt)xλs + λt λs E(λs (cid:1) λ)B(λs (cid:1) λ) dλ k=0 Nnew,(k) ϕ (xλs, λs) λt λs E(λs (cid:1) λ) (λ λs)k k! dλ. In manner similar to the derivation of higher-order DPM in Section 9.4.3, for the (n + 1)-th order approximation, we utilize the finite difference of Nnew,(k) (xλ, λ) at the previous + 1 steps, λin, . . . , λi1, λi0 := λs, to estimate ϕ each Nnew,(k) (xλs, λs). This approach is designed to match the coefficients in the Taylor expansions. ϕ Below, we demonstrate this method with an example for = 2: Example: Estimating Higher-Order Derivatives (n = 2 Case). When = 2, we aim to estimate the derivatives Nnew,(k) = 1, 2 with previous 3 timesteps λi2, λi1, λs. ϕ (xλ, λ) with Linear System for Approximated Derivatives. Let δk = λik λs (k = 1, 2). We expand Nnew ϕ (xλ, λ) around λs using Taylor series. Evaluating at λ = λi1 and λ = λi2, and rearranging to isolate the derivative terms, we obtain: Nnew ϕ (xλi , λi1) Nnew ϕ (xλs, λs) δ1Nnew,(1) ϕ (xλs, λs) + Nnew ϕ (xλi , λi2) Nnew ϕ (xλs, λs) δ2Nnew,(1) ϕ (xλs, λs) + δ2 1 2! δ2 2 2! Nnew,(2) ϕ (xλs, λs), Nnew,(2) ϕ (xλs, λs). Here, higher-order terms O(δ3 1) and O(δ 2) are neglected, respectively. This 9.7. (Optional) DPM-Solver-v3 311 forms the linear system: \" δ1 δ2 # δ2 1 δ2 2 eNnew,(1) (xλs ,λs) ϕ 1! eNnew,(2) (xλs ,λs) ϕ 2! = \"Nnew Nnew ϕ (xλi1 ϕ (xλi2 , λi1) Nnew , λi2) Nnew # ϕ (xλs, λs) ϕ (xλs, λs) with the approximated derivatives eNnew,(k) ϕ (xλs, λs) to be solved; hence, eNnew,(k) ϕ (xλs, λs) Nnew,(k) ϕ (xλs, λs). Solving for the Approximated Derivatives eNnew,(k) ϕ . Let: R2 = \" δ1 δ2 # δ2 1 δ2 2 , = \"Nnew Nnew ϕ (xλ, λi2) Nnew ϕ (xλ, λi1) Nnew ϕ (xλ, λs) ϕ (xλ, λs) # . The approximated derivatives are: eNnew,(1) (xλs ,λs) ϕ 1! eNnew,(2) (xλs ,λs) ϕ 2! = R1 2 b, which can be computed explicitly. Building upon the spirit of the illustrative example, we can easily extend it to the case of the k-th derivatives for with general n: δ1 δ2 ... δn δ2 1 δ2 2 ... δ2 . . . {z Rn eNnew,(1) (xλs ,λs) ϕ 1! eNnew,(2) (xλs ,λs) ϕ 2! ... eNnew,(n) (xλs ,λs) ϕ n! δn 1 δn 2 ... δn } = Nnew Nnew ϕ (xλi1 ϕ (xλi2 Nnew ϕ (xλin , λi1) Nnew , λi2) Nnew ... , λin) Nnew ϕ (xλs, λs) ϕ (xλs, λs) ϕ (xλs, λs) . By inverting the Vandermonde matrix Rn, we can analytically solve for the approximated derivatives eNnew,(k) (xλs, λs) for n. Therefore, by replacing ϕ (xλs, λs) in Equation (9.7.13) with eNnew,(k) Nnew,(k) (xλs, λs), we obtain an ϕ approximated solution, which we still denote as xλt: ϕ (9.7.14) 312 Sophisticated Solvers for Fast Sampling xλt = E(λs (cid:1) + λt)xλs + λt λs E(λs (cid:1) λ)B(λs (cid:1) λ) dλ (9.7.15) k= eNnew,(k) ϕ (xλs, λs) λt λs E(λs (cid:1) λ) (λ λs)k k! dλ. 9.7.5 More Interpretations of DPM-Solver-v3 Minimizing First-order Discretization Error Can Help Higher-order Solvers. λ and a λ are derived by reducing the first-order discretization error; however, in theory, they can also contribute to controlling errors in higher-order solvers. This result is summarized in the following proposition. Proposition 9.7.1: Reducing First-Order Discretization Error Helps Higher-Order Solvers Starting from the same initial condition xλs, let the approximated solution xλt be defined as in Equation (9.7.15), and the exact solution eΨλsλt(xλs) as in Equation (9.7.11). Then the discretization error is given by: xλt eΨλsλt(xλs) = λt (cid:16) λ λs + λs Nnew,(1) ϕ (xu, u) du (cid:17) E(λs λ) dλ (cid:1) (R1 )kj λij λs Nnew,(1) ϕ (xλ, λ) dλ ! k=1 j=1 λt λs E(λs (cid:1) λ) (λ λs)k k! dλ. Proof for Proposition. eΨλsλt(xλs) can be rewritten into the following expression: eΨλsλt(xλs) = E(λs λt)xλs + (cid:1) λt λs E(λs (cid:1) λt + λs λ)B(λs λ) dλ (cid:1) Nnew ϕ (xλ, λ)E(λs λ) dλ. (cid:1) 9.7. (Optional) DPM-Solver-v3 313 Subtracting Equation (9.7.15) by eΨλsλt(xλs): xλt eΨλsλt(xλs) = Nnew ϕ (xλ, λ) Nnew (cid:17) ϕ (xλs, λs) E(λs λ) dλ λt (cid:16) λs + k=1 eNnew,(k) ϕ (xλs, λs) λt λs E(λs λ) (cid:1) (cid:1) (λ λs)k k! dλ. By inverting the matrix Rn in Equation (9.7.14), the solution for any = 1, . . . , is given by: eNnew,(k) ϕ (xλs, λs) = (cid:16) (R1 )kj j=1 Nnew ϕ (xλij , λij ) Nnew ϕ (xλs, λs) (cid:17) . The results are obtained by applying the Fundamental Theorem of Calculus, yielding Nnew ϕ (xλ, λ) Nnew ϕ (xλs, λs) = Nnew ϕ (xλij , λij ) Nnew ϕ (xλs, λs) = λ λs λij λs Nnew,(1) ϕ (xu, u) du Nnew,(1) ϕ (xλ, λ) dλ. From the above proposition and Equation (9.7.10), controlling reduces (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)xλt eΨλsλt(xλs) (cid:13)2 , assuming sufficient smoothness. (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)Nnew,(1) (cid:13) ϕ Expressive Power of the Generalized Parameterization Nnew Equation (9.7.2) and Equation (9.7.9), we can rewrite Nnew form: ϕ . Utilizing ϕ in the following ϕ (xλ, λ) = σλe λ Nnew λs au du ˆϵϕ(xλ, λ) ℓλe λ r λ λs λs au duxλ au dubr dr, (9.7.16) which is conceptually of the following form: λs Tϕ(xλ, λ) := α(λ)ˆϵϕ(xλ, λ) + β(λ)xλ + γ(λ). (9.7.17) Indeed, for fixed λ, Ψϕ(xλ, λ) can be expressed in terms of Nnew ϕ (xλ, λ) through linear transformation depending on λs (see (Zheng et al., 2023)s Appendix I.1 for more details). 314 Sophisticated Solvers for Fast Sampling 9.7.6 Connection of DPM-Solver-v3 to Other Methods DPM-Solver-v3s Nnew ϕ is General Parametrization. By comparing DPMSolver-v3 with previous ODE formulations and their corresponding ϵ-/xprediction, we can easily identify that they are special cases of our approach by setting ℓλ, aλ, and bλ to specific values: ϵ-prediction: (cid:0)ℓλ, aλ, bλ x-prediction: (cid:0)ℓλ, aλ, bλ (cid:1) = (cid:0)0D, 1D, 0D (cid:1) (cid:1) = (cid:0)1D, 0D, 0D (cid:1) First-Order Discretization as an Improved DDIM. Since Nnew ϕ represents neither noise nor data parameterization but an improved parameterization aimed at minimizing the first-order discretization error, the first-order DPMSolver-v3 update in Equation (9.7.12) differs from the DDIM update in Equation (9.2.2): xti1ti = αti αti1 xti1 αti ! σti1 αti1 σti αti ϵϕ(xti1, ti1). 9.8. (Optional) ParaDiGMs 315 9.8 (Optional) ParaDiGMs xT xT 1 xT 2 . . . x0 xk xk 1 xk 2 . . . xk xk+1 xk+1 1 xk+1 2 . . . xk+1 0 (a) Sequential sampling by time-stepping estimation in generation process. (b) Picard iterations with skip dependencies. Figure 9.4: Comparisons of two computation graphs. Left: conventional time-stepping ODE solving, where the solution is propagated sequentially across time. Right: Picard iteration, which enables parallel computation by updating all time nodes simultaneously using the results from the previous iteration, thereby avoiding the strictly sequential nature of time-stepping. 9.8.1 From Time-Stepping to Time-Parallel Solver In the previous sections, we focused on the timestepping approach, which estimates the trajectory by evolving from the prior time toward an arbitrary [0, ]. Let vϕ(x, t) := (x, t) 1 g2(t)sϕ(x, t) denote the empirical PFODE drift from pre-trained diffusion model. The exact evolution from to any intermediate time is: eΨT (cid:0)x(T )(cid:1) = x(T ) + T vϕ (cid:0)x(τ ), τ (cid:1) dτ, x(T ) pprior. (9.8.1) Timestepping schemes approximate this integral using discrete updates based on past timesteps. In this section, we turn to the timeparallel approach, exemplified by ParaDiGMS, which builds on classical Picard iteration to enable parallel integration across time. The key idea behind ParaDiGMS is to trade computational resources for faster simulation. Sophisticated Solvers for Fast Sampling 9.8.2 Methodology of ParaDiGMS From Trajectories to Picard Iteration as Fixed-Point Update. The integral expression in Equation (9.8.1) can be understood as map that takes in an entire trajectory and produces new one. Formally, given any candidate trajectory {y(τ )}τ [0,T ], we define the operator by (L[y()])(t) = y(T ) + T vϕ (cid:0)y(τ ), τ (cid:1) dτ, [0, ]. That is, takes the terminal point y(T ) and extends it backward in time by integrating the prescribed velocity field vϕ along the path. true solution trajectory x() of the PF-ODE is precisely one that remains unchanged under this mapping. In other words, x() is fixed point of L: x(t) = L[x()](t) x(t) = x(T ) + T vϕ (cid:0)x(τ ), τ (cid:1) dτ. This reformulation shifts the problem from solving an ODE step by step to finding trajectory that is consistent with the operator L. Building on the operator view above, once we have the trajectory-totrajectory map L, natural way to find its fixed point is by successive substitution (Picard iteration): apply repeatedly on while evaluating the integral using the trajectory from the previous iterate. More precisely, starting from any initial path x(0)() (in practice, constant path x(0)(t) x(0)(T ) with fixed x(0)(T ) pprior), the update reads x(k+1)(t) :=L(k)[x(0)()](t) =x(k)(T ) + vϕ (cid:0)x(k)(τ ), τ (cid:1) dτ, = 0, 1, 2, . . . (9.8.2) This formula preserves the correct time anchoring: the iterate always starts from the prior-drawn state x(k)(T ), and then accumulates the drift as time decreases from down to t. Discrete Picard on to 0 Grid. To turn Equation (9.8.2) into practical algorithm, we place uniform, decreasing grid on [0, ] by choosing step count , setting := /M , and defining tj := jt, = 0, 1, . . . , M, so t0 = and tM = 0. Denote sampled iterates by x(k) := x(k)(tj). 9.8. (Optional) ParaDiGMs 317 Because the grid runs reversely in time, the integral from to tj has negative orientation . Approximating it by left endpoints on the partition {[ti+1, ti]}j1 i=0 gives tj vϕ(x(k)(τ ), τ ) dτ j1 i=0 vϕ (cid:0)x(k) ti (cid:1), , ti since each small integral over [ti+1, ti] equals ti+1 approximation into Equation (9.8.2) yields the discrete Picard update dτ . Substituting this ti x(k+1) = x(k) 0 j1 vϕ (cid:0)x(k) (cid:1) , , ti i=0 {z cumulative sum of drifts } = 1, . . . , M. (9.8.3) This scheme is simple and parallel-friendly: each drift evaluation vϕ , ti depends only on the previous iterate at the same time node ti, so all = 0, . . . , 1 evaluations can be computed independently across the grid. The integral is then recovered by cumulative sum, performed either serially or via parallel prefix-sum (scan/sliding windows). (cid:0)x(k) (cid:1) Figure 9.5: Compute the drift of x(k) ℓ:ℓ+p on batch window of size = 4, in parallel Figure 9.6: Update the values to x(k+1) ℓ:ℓ+p using the cumulative drift of points in the window Figure 9.7: Determine how far to slide the window forward, based on the error x(k+1) 2. x(k) Sliding Windows and Parallel Evaluation. The discrete Picard update Equation (9.8.3) expresses each x(k+1) 0 minus cumulative sum of drifts. To limit memory and exploit parallel hardware, it is convenient to apply the same idea locally on short, sliding blocks of indices. Fix window length and left index ℓ; the window then covers = as the leftanchored value x(k) ℓ, . . . , ℓ + with tℓ > tℓ+1 > > tℓ+p. During iteration k: Step 1. Parallel Drift Evaluation on the Window. Compute, in parallel and using only the previous iterate, vϕ (cid:0)x(k) ℓ+i, tℓ+i (cid:1), = 0, 1, . . . , 1. 318 Sophisticated Solvers for Fast Sampling These are the local increments needed to advance from the left edge tℓ across the window. Step 2. LeftAnchored Cumulative Updates. Form the windowed updates by anchoring at = ℓ and accumulating the drift across subintervals: ℓ+j+1 = x(k) x(k+1) ℓ X i=0 vϕ (cid:0)x(k) ℓ+i, tℓ+i (cid:1), = 0, 1, . . . , 1. (9.8.4) This is precisely Equation (9.8.3) restricted to the window, with the minus sign reflecting the decreasing time direction. The inner sum is prefix-sum (scan) over the windowed drifts, so all partial sums can be produced efficiently on parallel hardware. Step 3. Progress Control and Window Advance. Having formed the leftanchored cumulative updates on the current window (Step 2), we now decide how far to slide that window. We measure local convergence by the pointwise Picard change errorj := (cid:13) (cid:13)x(k+1) ℓ+j x(k) ℓ+j (cid:13) 2, (cid:13) = 1, . . . , 1, and compare it against prescribed tolerances tolℓ+j. That is, errorj measures how much the iterate at node ℓ + changed during the last Picard update. If this number is small, it indicates local agreement between the two successive approximations and hence local convergence of the fixed-point iteration at that node. If it is large, that node has not settled yet and needs more Picard smoothing. The stride is chosen as the first index in the window that fails this test (or the full window length if none fail): stride := min (cid:16) { 1 : errorj > tolℓ+j } {p} (cid:17) . We then slide the window by setting ℓ ℓ + stride. In words: we accept all nodes from the left edge up to (but not including) the first one that has not converged; if all nodes have converged, we accept the entire window. We then slide the window by that many accepted nodes, ℓ ℓ + stride, and continue. This advances by at most the window length p, never skipping any node that has not met its tolerance. If sliding would overrun the grid end , we truncate the window to min{p, ℓ} and proceed. When the window moves forward it uncovers new time nodes that have no values yet. To start Picard iteration there, we simply copy the value from 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 9.8. (Optional) ParaDiGMs Algorithm 8 ParaDiGMS with Sliding Windows Input: Drift vϕ(x, t); {tj}M Output: Approximate trajectory {x(k) 1: 0, ℓ 0 2: Sample x(0) 0 pprior; set x(0) x(0) }M 0 j=0; window length p; {tolj}M j= j=0 with x(k) at = 0 for = 1, . . . , min(p, ) 3: while ℓ < do 4: min(p, ℓ) Step 1: Parallel For = 0, . . . , 1: gi vϕ constant extrapolation current window length (cid:0)x(k) ℓ+i, tℓ+i (cid:1) Compute prefix sums Sj Pj i=0 gi for = 0, . . . , drifts from previous iterate (Picard freezing) scan over windowed drifts Step 2: Cumulative Updates For = 0, . . . , 1: x(k+1) ℓ+j+1 x(k) ℓ Sj left-anchored update; cf. Equation (9.8.4) Step 3: Progress Control and Window Advance For = 1, . . . , 1: errorj (cid:13) (cid:13)x(k+1) ℓ+j x(k) (cid:13) 2 (cid:13) { {1, . . . , 1} : errorj > tolℓ+j } {J} ℓ+j pointwise Picard change (cid:17) (cid:16) stride min Initialize New Nodes For = 1, . . . , stride: x(k+1) ℓ+J+r x(k+1) ℓ+J constant extrapolation into newly exposed indices ℓ ℓ + stride; + 1 15: 16: end while 17: return {x(k) }M j=0 the left boundary of the window and use it as an initial guess. This constant extrapolation is cheap and stable, and will be corrected by later updates. If desired, one can replace it by more accurate guesses, such as linear or polynomial extrapolations from past points. This completes the procedure of ParaDiGMS. We summarize the algorithm in Algorithm 8. 9.8.3 Relation to Time-Stepping Solvers Selection of Sliding Window Size. To place the sliding window scheme in context, note first what happens at the smallest window size. When = 1, the 320 Sophisticated Solvers for Fast Sampling window contains single step, so Equation (9.8.4) collapses to firstorder time-stepping update of the PFODE. The method reduces to, for instance, DDIM, if we use the same way of writing the ODE (e.g. data vs. noise prediction) and choose the same schedule of discrete timesteps as in DDIM. Increasing expands parallelism (more nodes advanced per window) without changing the overall step count . Consequently, sample quality continues to be determined by the base discretization (choice of grid/parameterization and perstep formula) together with Picard convergence on each window, which we monitor via the local tolerances. Compatibility with HigherOrder Solvers (e.g., DPM). The slidingwindow Picard structure controls how increments are computed (in parallel and accumulated by scan), not which local formula defines those increments. Consequently, one may replace the leftendpoint rule by any consistent higherorder quadrature without changing the parallel layout. For example, trapezoidal variant of Equation (9.8.4) reads ℓ+j+1 = x(k) x(k+1) ℓ 1 2 vϕ (cid:0)x(k) ℓ (cid:1) + , tℓ j1 i=1 vϕ (cid:0)x(k) ℓ+i, tℓ+i (cid:1) + 2 vϕ (cid:0)x(k) ℓ+j, tℓ+j (cid:1)i , where all drifts are still taken from the previous Picard iterate, so the pernode evaluations remain independent and the inner sum remains prefixsum. Likewise, multistep or exponentialintegrator updates used by DPM solvers family (e.g., DPMSolver++ 2M in logSNR time) can be inserted by replacing each windowed increment with the corresponding higherorder linear combination of past model evaluations (xor ϵ-predictions with precomputed coefficients). The scan then accumulates those weighted increments across the window exactly as before. In short: the parallel scheme is independent of the solver (discretization) choice to approximate the integral. Accuracy comes from the base solver; the windowed prefixsum just makes it fast. 9.9. Closing Remarks 321 9.9 Closing Remarks This chapter has confronted one of the most significant practical limitations of diffusion models: their slow, iterative sampling process. We have explored powerful class of training-free solutions that accelerate generation by leveraging the rich field of numerical methods for differential equations. The core strategy has been to more efficiently solve the PF-ODE, which defines the deterministic generative trajectory from noise to data: 1. We began with the foundational DDIM, which can be understood as first-order exponential Euler method. 2. We then moved to higher-order multi-step methods like DEIS, which improve accuracy by using history of past evaluations. 3. Finally, we examined the highly efficient DPM-Solver family, which achieves remarkable performance by introducing crucial log-SNR time reparameterization. Through these sophisticated solvers, the number of function evaluations (NFEs) required for high-quality generation has been dramatically reduced from hundreds or thousands to as few as 10-20, making diffusion models significantly more practical. However, these training-free methods are still fundamentally iterative. They approximate continuous path step-by-step. This raises natural and ambitious question: can we achieve high-quality generation in just one or very few discrete steps? The final part of this monograph will explore this question through training-based acceleration. We will investigate two main strategies: 1. First, in Chapter 10, we will examine distillation-based methods, where fast student generator is trained to replicate the output of slow, pre-trained teacher diffusion model in far fewer steps. 2. Then, in Chapter 11, we will push this idea further by exploring methods that learn fast, few-step generators from scratch, such as Consistency Training, which define standalone training principle without relying on any pre-trained model. This shift from improving the solver to learning the solution map itself represents the frontier of efficient generative modeling, aiming to combine the quality of diffusion models with the speed of one-step generators."
        },
        {
            "title": "Part D",
            "content": "Toward Learning Fast Diffusion-Based Generators 10 Distillation-Based Methods for Fast Sampling This chapter introduces training-based approaches that accelerate diffusion model sampling by teaching new generators to produce samples in only one or few steps. The central idea, called distillation, is to let fast student model learn from slow, pre-trained diffusion model (teacher) sampler. While the teacher may require hundreds of steps, the student can achieve comparable quality in only few steps1. Unlike solver-based acceleration, which improves the numerical integration scheme, distillation directly trains generator to take efficient shortcuts. We highlight two main paradigms: distribution level distillation, which skips simulating the full trajectory and instead aligns the students output distribution with the teachers, and flow map level distillation, which trains the student to reproduce the teachers sampling path in faster and more compact way. 1Here, distillation refers to reducing the number of sampling steps, not to shrinking the model size. 323 324 Distillation-Based Methods for Fast Sampling 10.1 Prologue central bottleneck of diffusion models is their slow sampling speed. As shown through Tweedies formula (Section 6.3.1), diffusion model can be interpreted as an x-prediction model, xϕ(xt, t), trained to recover the expected clean data from noisy input xt at noise level t: xϕ(xt, t) E[x0xt], where the expectation is taken with respect to p(x0xt), representing all plausible clean data corresponding to xt. natural idea is to use xϕ(xt, t) for one step generation. Yet because this denoiser averages over many plausible outcomes, the prediction becomes overly smooth, and generation with only few denoising steps leads to blurry, low quality samples. On the other hand, as discussed in Section 4.2.2, diffusion sampling follows an ODE or SDE trajectory through long sequence of iterative steps. This produces high fidelity samples, but the large number of steps required makes the process inherently slow. Reducing the NFE (i.e., the number of sampling steps times model calls) speeds up generation but inevitably reduces fidelity. Each solver step introduces an integration error of order O(hn), where is the solver order and = maxi ti ti1 is the step size. Fewer steps imply larger time increment h, which in turn increases the accumulated sampling error and leads to less accurate trajectory. This creates fundamental trade off between quality and efficiency in diffusion sampling. To overcome this bottleneck, major line of research is distillation, which assumes access to well trained diffusion model (the teacher) and trains generator (the student) to reproduce its behavior through single feed forward or few step computation. This compresses the teachers many sampling steps into fast process, effectively bypassing slow iterative solvers while maintaining high sample fidelity. Below, we introduce two perspectives on distillation: distribution level distillation and flow map level distillation2. 2Chronologically, flow map level distillation, represented by Knowledge Distillation (KD) (Luhman and Luhman, 2021) and Progressive Distillation (PD) (Ho et al., 2020), was proposed earlier in 2021, preceding the family of distribution level distillation approaches that emerged around 2023. For smoother exposition and connection to the next chapter, however, we present distribution level distillation first. 10.1. Prologue 325 10.1.1 Distribution Level Distillation The goal of distribution based distillation is to train one-step generator Gθ(z) that maps noise pprior to sample ˆx = Gθ(z), inducing distribution pθ(ˆx) that approximates the target data distribution pdata(x). This is typically achieved by minimizing statistical divergence min θ D(pθ(ˆx), pdata(ˆx)) , where denotes suitable divergence measurement such as KL. In practice, distribution based methods align the generators distribution with the empirical distribution pϕ(x) produced by pre-trained diffusion model: D(cid:0)pθ(ˆx), pϕ(ˆx)(cid:1) , min θ where pϕ serves as surrogate for pdata. Rather than evaluating this divergence explicitly, these methods approximate its gradient, which can be computed directly from the pre trained teacher model. This enables the student to align its distribution with the teachers without requiring full divergence evaluation. This formulation distills multi-step generative processes of diffusion models into single step model through distributional alignment. We detail this approach in Section 10.2. 10.1.2 Flow Map Level Distillation We consider the PF-ODE, which can be expressed for any prediction model (see Equation (6.3.1)): dx(τ ) dτ = (τ )x(τ ) 1 2 g2(τ )x log pτ (x(τ )) =: v(x(τ ), τ ). (10.1.1) Its solution map, starting from xs at time and evolving reversely to time s, is denoted by Ψst(xs); that is, Ψst(xs) := xs + s v(x(τ ), τ ) dτ, (10.1.2) where the integral solves the PF-ODE. Intuitively, Ψst transports, xs, noise at time to less noisy states at time (ultimately data at = 0). Sampling from diffusion model corresponds to evaluating ΨT 0(xT ) for xT pprior. Typically, this integral is approximated by iterative numerical solvers leveraging the velocity field (see Chapter 9), but requires many steps (e.g., at least 10 steps even in DPM-Solver), making sampling slower than 326 Distillation-Based Methods for Fast Sampling classic one-step generative models such as GAN. This motivates natural question: Question 10.1.1 Can we learn the solution map Ψst(xs) directly? In particular, learning map ΨT 0(xT ) with xT pprior enables one-step generation. Trajectory Distillation. Trajectory distillation seeks to train neural generator that approximates the solution map at the instance level. Since the PF-ODE integral rarely admits closed form, it must be approximated numerically during training. To formalize, we introduce the general solver notation Solverst(xs; ϕ) or simply Solverst(xs), (10.1.3) denoting numerical integration of the empirical PF-ODE from to starting at xs, with teacher parameters ϕ (omitted when clear from context). An Early Approach: Direct Knowledge Distillation. To enable few step or even one step generation, direct approach is to train generator Gθ(xT , T, 0) to imitate the output of numerical solver evaluated along the full trajectory: Gθ(xT , T, 0) SolverT 0(xT ), xT pprior. This idea underlies one of the earliest trajectory distillation methods, Knowledge Distillation (Luhman and Luhman, 2021), which uses the regression loss LKD(θ) := ExT pprior Gθ(xT , T, 0) SolverT 0(xT )2 2 . While this approach provides direct supervision from the pre-trained teacher, it cannot leverage the strong supervision available in the original training data. In addition, it is computationally expensive if ODE integration is invoked within the training loop, since each parameter update requires solving the ODE to form targets. Finally, because the generator learns only global mapping from to 0, it may lose controllability for steering the generation process from intermediate states. Consequently, most controllable generation techniques introduced in Chapter 8 cannot be directly applied. Preface to Progressive Distillation. Progressive Distillation (PD) (Salimans and Ho, 2021) trains timeconditional Student using local supervision from 10.1. Prologue 327 Teacher fragments. Let t0 = > t1 > > tN = 0 be fixed time grid. The Teacher provides timestepping maps Teachertktk+1 for = 0, . . . , 1. Rather than supervising only the one-jump 0, PD trains the Student twostep skip map to match two consecutive Teacher steps: Studenttktk+2 Teachertk+1tk+2 Teachertktk+1, for = 0, 2, 4, . . . . The matching is performed using simple regression loss (e.g., mean squared error). After training on locally paired fragments, the Student no longer follows every time interval of the original grid. Instead, it advances on every other time point, t0 t2 t4 tN , which means that each Student step effectively covers two consecutive Teacher steps. Consequently, the Student completes the same overall time span [0, ] using only N/2 transitions. After this stage, the trained Student replaces the Teacher to serve as the new reference model. The entire procedure is then repeated on the coarser grid, where the time step doubles (N N/2 N/4 ), progressively distilling the trajectory into fewer and fewer steps until the desired number of inference steps is reached. This iterative halving preserves the global time horizon while continually compressing the temporal resolution of the generative process. Unified Perspective of Flow Map Learning. Various methods, including KD and PD, can be expressed within unified loss framework: Loracle(θ) := Es,tExsps (cid:2)w(s, t)d(cid:0)Gθ(xs, s, t), Ψst(xs)(cid:1)(cid:3) , (10.1.4) where Ψst is the oracle flow map, w(s, t) 0 specifies how different time pairs (s, t) are weighted, d(, ) is discrepancy measure such as d(x, y) = y2 2 or d(x, y) = y1, and ps denotes the forward noised marginal at time s. Because Ψst is not available in closed form, one must rely on approximations, typically through pre-trained diffusion model (teacher) or another tractable surrogate. KD appears as simple instance of Equation (10.1.4). Selecting degenerate weighting w(s, t) = δ(sT ) δ(t0) and using the prior distribution pT = pprior 3, the oracle loss Loracle(θ) reduces to: ExT pT (cid:13)Gθ(xT , T, 0) ΨT 0(xT )(cid:13) (cid:13) 2 2 LKD(θ), (cid:13) 3This assumption holds for large enough or with appropriate noise schedules (αt, σt). Distillation-Based Methods for Fast Sampling with SolverT 0 ΨT 0. An alternative perspective on this formulation is presented in Section D.5. PD also fits this template, but instead of supervising only with the single extreme pair (T, 0), it uses many nearby time pairs and enforces simple local consistency rule: short step followed by another short step should match the direct twostep move. We return to this in Equation (10.3.3). In practice, the main challenge is that the oracle flow map Ψst generally has no closed-form expression, making direct supervision infeasible. range of methods have been developed to approximate this target efficiently, but their success often hinges on the quality of the teacher model. We will return to Equation (10.1.4) in Chapter 11, presenting principled framework for training-from-scratch methods that eliminate the teacher from the learning loop. 10.2. Distribution-Based Distillation 10.2 Distribution-Based Distillation Several works have pursued this distribution-based distillation concurrently under different names, including Distributional Matching Distillation (DMD) (Yin et al., 2024b; Yin et al., 2024a), Variational Score Distillation (VSD) (Poole et al., 2023; Wang et al., 2023; Luo et al., 2023; Lu and Song, 2024), and Score Identity Distillation (SiD) (Zhou et al., 2024). Despite technical differences, they share the same principle: train generator whose forward-noised marginals match those of the teacher. We focus on VSD as representative formulation, since the others follow similar principles. 10.2.1 Formulation of VSD as Representative Approach Forward Process. Let {pt}t[0,T ] denote the marginal densities of forward diffusion process induced by xt = αtx0 + σtϵ, ϵ (0, I), with initial distribution p0 = pdata. In contrast, let pθ 0 denote the distribution of synthetic samples generated by deterministic one-step generator Gθ(z) from latent variables pprior(z). Define {pθ }t[0,T ] as the marginal densities obtained by applying the same forward diffusion process to pθ 0, that is, xθ := αtGθ(z) + σtϵ, (10.2.1) where pprior and ϵ (0, I). Thus, both pt and pθ share the same Gaussian diffusion kernel pt(xtx0) but differ in their starting distributions (pdata vs. pθ 0 of one-step synthetic samples). Training Objective and Gradient. The literature typically adopts the KL divergence to match the distributions pt and pθ , commonly by minimizing LVSD(θ) := Et ω(t)DKL(pθ (cid:16) pt) (xθ log pθ ω(t) = Et,z,ϵ ) log pt(xθ ) (cid:17)i , where ω(t) is time-dependent weighting function. We will discuss in Section 10.2.3 why the KL divergence plays special role in distribution-level distillation. As shown in (Wang et al., 2023), the optimum is achieved when pθ 0 = pdata, indicating that the generators distribution matches the data distribution, and the training objective serves as valid loss for learning the data distribution. 330 Distillation-Based Methods for Fast Sampling However, the density-based formulation of the objective lacks an efficient training mechanism. Fortunately, by taking the gradient with respect to θ, we arrive at the expression in Equation (10.2.2), which is summarized in the following proposition. For notational simplicity, we denote ˆxt := xθ as defined in Equation (10.2.1). Proposition 10.2.1: θ-Gradient of LVSD We have θLVSD(θ) =Et,z,ϵ ω(t)αt (cid:16) log pθ (cid:17) (ˆxt) log pt(ˆxt) θGθ(z) . (10.2.2) Proof for Proposition. The derivation applies the chain rule: θEt = Et,z,ϵ = Et,z,ϵ pt) (cid:0) log pθ DKL(pθ θ θ log pθ {z first (ˆxt) } (ˆxt) log pt(ˆxt)(cid:1)i +(x log pθ (ˆxt))θ ˆxt (x log pt(ˆxt))θ ˆxt . The first term vanishes by the score-function identity: ˆxtpθ θ log pθ (ˆxt) = θpθ (x) dx = θ pθ (x) dx = θ(1) = 0. Using the reparameterization ˆxt = αtGθ(z) + σtϵ gives θ ˆxt = αtθGθ(z), hence θLVSD(θ) = Et,z,ϵ ω(t)αt (cid:0)x log pθ (ˆxt) log pt(ˆxt)(cid:1)θGθ(z) This proves Equation (10.2.2). See Section D.5 for details. . We observe that the score functions naturally emerge when taking the gradient with respect to θ. Consequently, we require approximations of the score log pθ (ˆxt) for the one-step generator and log pt(ˆxt) for the data distribution, as will be detailed in the following subsection. 10.2. Distribution-Based Distillation 331 10.2.2 Training Pipeline of VSD Existing works (Yin et al., 2024b; Yin et al., 2024a; Poole et al., 2023; Wang et al., 2023; Luo et al., 2023; Lu and Song, 2024) typically address this via bi-level optimization approach: training new diffusion model on samples from Gθ(z) to approximate log pθ (ˆxt), and employing pre-trained diffusion model as proxy for the intractable oracle score function log pt(ˆxt) on synthetic samples ˆxt (i.e., the teachers score). More precisely, training proceeds by alternating between two phases: Score Estimation Phase. Fix θ. Let ˆx0 = Gθ(z) and ˆxt = αt ˆx0 + σtϵ with pprior, ϵ (0, I). Train sζ by DSM using the known Gaussian diffusion kernel pt(xtx0): LDSM(ζ; θ) = Et,z,ϵ (cid:13) (cid:13) (cid:13) sζ(ˆxt, t) xt log pt(ˆxtˆx0) 2 (cid:13) (cid:13) (cid:13) , which yields sζ(, t) log pθ () at optimum (for fixed θ). Generator Update Phase. With sζ frozen (stop-grad), θ is updated by using the gradient in Equation (10.2.2), replacing the individual score terms by their respective proxies: sζ(ˆxt, t) log pθ (ˆxt), and sϕ(ˆxt, t) log pt(ˆxt) (teacher). Equation (10.2.2) then approximately becomes: θLVSD(θ) Et,z,ϵ ω(t)αt (cid:0)sζ(ˆxt, t) sϕ(ˆxt, t)(cid:1)θGθ(z) . pϕ These two phases repeat until, for all t, sζ(, t) sϕ(, t) on the support of pθ , so the plug-in gradient in Equation (10.2.2) vanishes. In this convergence regime, we have pθ (the teachers marginal) for all > 0. Since the forward noising operator (Gaussian convolution) is injective for any fixed > 0, it follows that pθ (the teachers = 0 distribution). Thus, the learned one-step generator Gθ matches the teachers distribution at = 0; when the teacher closely approximates pdata, this further implies pθ 0 pdata. 0 pϕ 10.2.3 Additional Discussion: Divergence Choices and VSD Applications Beyond KL: Can We Use General Divergences? In principle, one may replace the forward KL term DKL(pθ pt) in VSD with more general divergence family, such as the -divergence (see Equation (1.1.4)): Df (pθ pt) = pt(x)f ! pθ (x) pt(x) dx. 332 Distillation-Based Methods for Fast Sampling However, the gradient θDf (pθ pt) depends on the density ratio rt(x) = pθ (x) pt(x) , through (rt), which is intractable for an implicit student generator. Here the student is called implicit because it can produce samples ˆxt through stochastic mapping ˆxt = αtGθ(z) + σtϵ, but it does not provide closedform expression or likelihood for its induced density pθ (x). Consequently, computing the functional derivative of Df requires pointwise access to rt(x) or its log-gradient, both of which cannot be evaluated in this setting. common workaround is to introduce an auxiliary critic or discriminator that approximates the density ratio via the variational formulation of -divergences, as in -GAN (Nowozin et al., 2016), although this introduces an extra network and nested minimax optimization. By contrast, for the forward KL, the pathwise gradient simplifies neatly to score-difference form (Equation (10.2.2)): θDKL(pθ pt) = Eh(cid:0)x log pθ (ˆxt) log pt(ˆxt)(cid:1)θ ˆxt . This structure enables tractable score-only update. The teachers pre-trained diffusion model already provides log pt(), so we can reuse it directly without learning an auxiliary density-ratio estimator. This formulation yields non-adversarial training objective that remains fully differentiable and computationally efficient. VSD for 3D Generation Using Only 2D pre-trained Diffusion Model. VSD (Wang et al., 2023), together with its earlier special case SDS (Poole et al., 2023) where the generator is Dirac parameterized by θ, was originally introduced for 3D scenarios without paired supervision between 3D and 2D data (that is, without ground-truth 3D labels). Let θ Rd denote the parameters of 3D scene, and let R(θ) be differentiable renderer that produces an image ˆx0 := R(θ). The forward noising process is defined as ˆxt = αt R(θ) + σtϵ, ϵ (0, I). pre-trained 2D (image) diffusion teacher provides scores sϕ(ˆxt, tc) ˆxt log pt(ˆxtc), optionally conditioned on text c. The goal is to align the distribution of noisy renderings with the teachers marginals at each t. minimal formulation is 10.2. Distribution-Based Distillation 333 the score-alignment (VSD) objective under the rendering distribution: L3D VSD(θ) := Et,ϵ ω(t) (cid:13) (cid:13)sζ(ˆxt, t) sϕ(ˆxt, tc)(cid:13) 2 (cid:13) 2 , ˆxt = αtR(θ) + σtϵ, which transfers image-space score guidance to the 3D parameters through the renderer. Treating both scores as stop gradients with respect to ˆxt during the update of θ yields θL3D VSD(θ) = Et,ϵ (cid:20) ω(t) αt (cid:0)sζ sϕ (cid:1) θ (cid:21) . (θ) When the student score sζ is suppressed (Dirac generator), the formulation reduces to SDS (Poole et al., 2023). In practice, optimization alternates exactly as described in Section 10.2.2: first updating the student score on noisy renderings, and then updating θ with stop gradients through both scores. Further mathematical details are omitted here for brevity. 334 Distillation-Based Methods for Fast Sampling 10.3 Progressive Distillation Progressive Distillation (PD) (Salimans and Ho, 2021) consists of two procedures that together enable diffusion model to learn the PF-ODE trajectory more efficiently. The key idea is to progressively reduce the number of integration steps required for high-quality sampling while retaining fidelity to the teacher trajectory. Distillation Operation: Distills deterministic sampler (e.g., DDIM) based on pre-trained teacher model (initially diffusion model) into student model that reproduces the same trajectory using only half as many sampling steps. Progressive Operation: Repeats this distillation process iteratively, each time halving the number of steps, until the student can generate high-quality samples within small fixed budget (typically 14 steps). Figure 10.1: Illustration of Progressive Distillation (PD). At each round, the student model is trained so that single step reproduces the effect of two adjacent teacher steps. This process distills teacher steps into N/2 student steps, and repeating the procedure progressively halves the trajectory length until the desired step count is reached. The arrows indicate how multi-step teacher transitions are compressed into fewer student steps, moving from data to noise. We first introduce the distillation operation of PD in Section 10.3.1, and then summarize the entire training pipeline in Section 10.3.2. Section 10.3.4 presents an extension for CFG guidance. 10.3. Progressive Distillation 335 10.3.1 Distillation Operation in PD In this section, we fix DDIM in the x-prediction parameterization as the timestepping rule and still write Solverst for the deterministic map obtained by plugging the current teachers x-denoiser into DDIM. In the first PD round (teacher = pretrained diffusion model), this coincides with integrating the diffusion PFODE via DDIM; in later rounds (teacher = previous student), Solverst is simply the DDIM transition induced by the current teacher, not the original diffusion PFODE. The distillation step is as follows: starting from noisy input xs (a perturbed version of clean data, xs = αsx0 + σsϵ), the student is trained to predict target so that single student step reproduces the teachers two consecutive steps t. Let xϕ(x, τ ) denote the teachers x-prediction denoiser in this round. Applying the teacherinduced DDIM transition twice gives xu := Solversu (cid:0)xs; xϕ (cid:1) , xt := Solverut (cid:0)xu; xϕ (cid:1) . Here, we use the notation of Equation (10.1.3) to denote the deterministic transition map from to (starting at xs) induced by plugging xϕ into DDIM. Question 10.3.1 What is the pseudo-clean at time such that the solver produces the same output xt when stepping directly as it does via t? Specifically, determine satisfying: xt = Solverst (xs; x) . Once closed-form expression for is obtained, we train student model fθ(xs, s) (also an x-prediction model here) to approximate the two-steps-inone target by minimizing EsExsps min θ w(λs) (cid:13) (cid:13)fθ(xs, s) x(cid:13) 2 (cid:13) 2 . (10.3.1) In the following, we show that the DDIM rule yields in closed form through elementary algebra (note that the result holds for both discrete and continuous time): 336 Distillation-Based Methods for Fast Sampling Lemma 10.3.1: Two-Steps-in-One Target of DDIM Starting from an initial condition xs, if the solver is taken as DDIM, then the two-step-in-one target can be computed as σs αtσs αsσt σt αtσs αsσt xt = xs. Here, xt is obtained by applying DDIM (in Equation (9.2.3)) twice, from t: : : xu = xt = Proof for Lemma. σu σs σt σu xs + αs xu + αu (cid:18) αu αs (cid:18) αt αu (cid:19) (cid:19) σu σs σt σu xϕ(xs, s) xϕ(xu, u). xt must be matched with the one-step DDIM from to t, t, expressed as: : = σt σs xs + αs (cid:18) αt αs (cid:19) x. σt σs By equating and xt, we can solve for in terms of xt, s, and t: (cid:18) αt αs (cid:19) σt σs xt = = xs + αs xt = σt σs xt σt xs σs (cid:16) αt σt σs αs σs αtσs αsσt αs = (cid:17) (10.3.2) xt σt αtσs αsσt xs. With this formula, PD computes the pseudo-clean target at time whose single DDIM step lands exactly at the two-step output xt. Practical Discrete Time Grids and Loss. In practice, we fix decreasing grid t0 = > t1 > > tN = 0 and, for brevity, write := tk, := tk+1, := tk+2. The teacher provides one step maps Teachertktk+1, and the student learns two step skip map that matches the teacher composition: Studenttktk+2 Teachertk+1tk+2 Teachertktk+1. 10.3. Progressive Distillation 337 We sample triplets (s, u, t) = (tk, tk+1, tk+2) with {0, . . . , 2}. The objective Equation (10.3.1) becomes kU[[0,N 2]] xtk ptk min θ w(λtk ) (cid:13) (cid:13)fθ(xtk , tk) x(k)(cid:13) 2 (cid:13) 2 , where the teacher two-step target x(k) is computed via Lemma 10.3.1. If the grid is uniform, one may write tk = (1 k/N ) so that (cid:16) (cid:16) (cid:16) (cid:17) (cid:17) (cid:17) = 1 , = 1 , = 1 + 2 , + 1 corresponding to evenly spaced time steps of size = /N . 10.3.2 Entire Training Pipeline of PD and Its Sampling After training on locally paired fragments via Equation (10.3.1), the Student no longer follows every interval of the original grid. Instead, each learned step covers two consecutive Teacher steps, so the Student advances on every other time point, t0 t2 t4 tN , and thus traverses the same horizon [0, ] using only N/2 transitions. After this stage, the trained Student replaces the Teacher as the new denoiser model. The procedure is then repeated on the coarser grid (the time step doubles), yielding the progression N/2 N/4 , until the desired number of inference steps is reached. At each iteration, the new Student is initialized from the updated Teacher. This iterative halving preserves the global time horizon while progressively compressing the temporal resolution of the generative process. Sampling. At inference time, using the (DDIM) solver with the current Student as the denoiser, the sampler advances on the coarser grid induced by training. After the first round it takes skip-2 jumps (t0 t2 tN ), after the next round skip-4 (t0 t4 tN ), and so on, halving the number of sampling steps at each iteration while keeping the same start and end times. 10.3.3 Additional Discussion: Local Semigroup Matching and the Possibility of Generalized Solvers Progressive Distillation as Local Semigroup Matching. Within the unified objective Equation (10.1.4), the intractable oracle target Ψs0 is replaced 338 Distillation-Based Methods for Fast Sampling by teacherinduced surrogate that uses the semigroup property of the ODE flow (see more details later in Equation (11.2.1)): evolving from to should be equivalent to going from to any intermediate and then from to t, Ψst = Ψut Ψsu. PD enforces this locally by training the students onestep map to match the teachers composition of two adjacent onestep fragments: EsExsps (cid:13) (cid:13) Gθ(xs, s, 2s) } {z student one-step Solversss2s (cid:0)Solversss(xs)(cid:1) } {z teacher two-step composition (cid:13) 2 2. (cid:13) Minimizing Equation (10.3.3) instantiates the semigroup identity on short decreasing grid (take > > with = and = 2s): (10.3.3) Ψss2s = Ψsss2s Ψsss Solversss2s Solversss, so training only requires short teacher fragments, rather than full rollout from time all the way to 0. To connect back to the fewstep denoiser view in Equation (10.1.4), define the students fewstep map as composition of learned jumps: Gθ(xs, s, 0) } {z few-step denoiser = (cid:0)Gθ( , 2s, 0) Gθ( , s, 2s)(cid:1)(xs). Conceptually, Equation (10.3.3) provides an efficient local surrogate for the global regression Es,xs (cid:13) (cid:13)Gθ(xs, s, 0) (Solver) (cid:13) 2 (cid:13) (cid:13) s0(xs) (cid:13) 2 , where (Solver) grid with step size s, serving as proxy for Ψs0. s0 denotes the teachers full composition from to 0 on Can we Use Other Solvers? In the PD introduction above, we focused on DDIM in the x-prediction parameterization as concrete PFODE sampler. The local semigroup matching with grid halving is solver-agnostic at the level of deterministic state-to-state maps and extends to the time-stepping methods in Chapter 9 after standard conversions between parameterizations (x, ϵ, v, score). However, the closed-form pseudo-target here relies on single-step, explicit update whose one-step map is affine in the regression target (as with 10.3. Progressive Distillation 339 DDIM and explicit one-step schemes such as exponentialEuler or explicit RK applied to the PFODE). For multi-step or implicit solvers, which require step history or inner solves, one should instead match the corresponding transition map directly (cf. Equation (10.3.3)) and provide the necessary history or warm start; comparable closed-form inversion generally does not exist. If the sampler is stochastic, freeze the noise sequence per example to obtain deterministic transition Teacher(ω) st (with ω the fixed noise seed). In that case, PD regresses to fixed transition map; closed-form pseudo-targets generally require single step explicit affine update; otherwise, use direct matching as in Equation (10.3.3). 10.3.4 PD with Guidance Meng et al. (2023) proposed two-stage pipeline for distilling classifier-free guided (CFG) diffusion models: (1) distill the guidance into single network that takes the guidance weight as input, and (2) apply progressive distillation (PD) to reduce the sampling steps. They demonstrated this both in pixel space and in latent space (e.g., Stable Diffusion). Stage-One Distillation: Distilling Guidance. Let xϕ(xs, s, c) denote the (pre-trained) conditional diffusion model output in the x-prediction parameterization (i.e., clean estimate) at time and condition c; the condition can also be null, = (unconditional branch). The ω-weighted CFG combination in Equation (8.3.3) can be written as ω ϕ(xs, s, c) := (1 + ω) xϕ(xs, s, c) ω xϕ(xs, s, ), (10.3.4) where ω pω(ω) for some CFG weighting distribution pω, typically pω(ω) = U[ωmin, ωmax]. Stage-one introduces new model xθ1(xs, s, c, ω) that directly takes ω as ϕ(xs, s, c) by supervised input and learns to reproduce the CFG output ω regression: min θ1 Eωpω,s,xpdata,xsp(xsx)λ(s)(cid:13) (cid:13)xθ1(xs, s, c, ω) xω ϕ(xs, s, c)(cid:13) 2 2. (cid:13) Here λ(s) is standard schedule-dependent weighting; sampling ω each iteration teaches single network to emulate CFG at arbitrary guidance strengths. Stage-Two Distillation: PD. The stage-one model xθ1(xs, s, c, ω) serves as the teacher in PD and is progressively distilled into student xθ2(xs, s, c, ω) with fewer sampling steps, following Section 10.3.2. At each iteration, the number of steps is halved (e.g., N/2 N/4 ). 340 Distillation-Based Methods for Fast Sampling 10.4 Closing Remarks This chapter has introduced our first major paradigm for training-based acceleration. Having exhausted training-free improvements via numerical solvers, we shifted our focus to new strategy: training fast student generator that learns to replicate the behavior of slow, pre-trained teacher diffusion model. We explored two primary distillation philosophies. First, in distributionbased distillation, represented by methods like Variational Score Distillation (VSD), the students output distribution is trained to match the teachers. This is achieved by aligning their respective score functions across different noise levels, providing stable, non-adversarial objective. Second, in flow map distillation, we saw how methods like Progressive Distillation (PD) train the student to directly approximate the teachers solution trajectory. PDs iterative approach, where each round halves the number of sampling steps, proved to be powerful and practical method for compressing long iterative process into just few steps. These distillation techniques successfully bridge the gap between the high sample quality of iterative diffusion models and the inference speed of one-step generators, offering compelling pathway to efficient, high-fidelity synthesis. However, the reliance on pre-trained teacher model introduces twostage pipeline: first train slow but powerful teacher, then distill it into fast student. This raises fundamental question at the forefront of generative modeling research: Can we bypass the teacher entirely? Is it possible to design standalone training principle that learns these fast, few-step generators directly from data? The final chapter of this monograph will address this question. 1. We will explore pioneering methods such as Consistency Models that learn the mapping from any point on an ODE trajectory to its destination point. 2. We will delve into generalized concepts of Consistency Models which learn to map any point on an ODE trajectory to another point in single step. This shift from improving the solver or distilling solution to learning the solution map itself represents significant step toward new class of generative models that are both principled and highly efficient by design."
        },
        {
            "title": "Learning Fast Generators from Scratch",
            "content": "Truth is ever to be found in simplicity, and not in the multiplicity and confusion of things. Isaac Newton In Chapter 10 we saw that slow iterative samplers in diffusion models can be compressed into few step generators through distillation. From an engineering perspective, two-stage pipelines are practical because they divide complex generative training task into clear, independent objectives. The first stage learns the data distribution, while the second accelerates sampling or enhances quality. This separation allows each stage to be optimized independently, making the overall system easier to manage, more stable, and more reliable. In this chapter, however, the focus shifts to central question driving the progress of deep generative modeling: Question 11.0.1 Can we design standalone generative principle that trains in stable and efficient way, fast sampling, and allows users to easily guide or control what is produced? In this chapter we pursue this direction and discuss an alternative approach: training few-step diffusion-based generators without relying on pre-trained model. Our focus is the flow map model, which learns direct transformation that moves samples across time by approximating the oracle flow map of the 341 342 Learning Fast Generators from Scratch PF-ODE. This formulation provides principled way to transport probability mass from the prior distribution pprior to the data distribution pdata, while preserving the marginal distributions pt specified by the forward diffusion process at each intermediate time. 11.1. Prologue 11.1 Prologue 343 2021/01 2022/02 2023/03 2023/ 2024/10 2025/05 10.1 KD Section 10.3 PD Section 11.2 CM Section 11.4 CTM Section 11.3 sCM Section 11.5 MF Section Figure 11.1: Timeline of Flow Map Modeling. We use blue for the special case Ψs0 and orange for the general map Ψst. In Chapter 10 we showed how the inMotivation of Flow Map Models. accessible regression target in the oracle flow-map loss Loracle(θ) (see Equation (10.1.4)) can be estimated by distilling knowledge from pre-trained diffusion model to obtain few-step generators. This route is effective and practical: two-stage pipeline can be engineered for robustness and often remains competitive in both data and compute efficiency. In this chapter, we shift focus to broader challenge at the core of deep generative modeling: Can we establish standalone generative principle that enables stable, scalable, and efficient training, fast sampling, and generation that can be easily steered by user intentions, without relying on pre-trained model? Designing such standalone principles lies at the center of generative modeling. Diffusion models offer useful design principle: start with continuous-time forward process that gradually transforms data into simple prior (noise) as reference, and frame the modeling task as learning the reverse-time transport that restores this process to match the desired marginal distributions. This time-dependent formulation also makes it easier to steer the generation process at intermediate steps, compared to one-shot generative maps. Specializing to diffusion-motivated methods, this leads to the question: Question 11.1.1 Can we learn the flow map Ψst() with network Gθ(, s, t) (a flow map model) without access to pre-trained models, while maintaining high-fidelity generation? This chapter develops methods toward this goal, organized around single objective that also underlies distillation and provides unified view of flow-map formulations (Boffi et al., 2024; Hu et al., 2025): 344 Learning Fast Generators from Scratch Loracle(θ) := Es,t Exsps w(s, t)d(cid:0)Gθ(xs, s, t), Ψst(xs)(cid:1)i . (10.1.4) Here s, are sampled from some time distribution (e.g., uniform), w(s, t) 0 assigns weights to the time pairs (s, t), and d(, ) is discrepancy measure such as the squared ℓ2 norm. The oracle flow map Ψst represents the ideal transformation that takes state xs at time and transports it directly to time t: Ψst(xs) = xs + where the oracle drift is given as s v(xu, u) du, v(xu, u) = (cid:2)α ux0 + σ uϵxu (cid:3) , while equivalent parametrizations are also possible (see Chapter 6), with common choices including the x-prediction and v-prediction forms. At the optimum of the oracle loss, the learned model recovers the true flow map exactly: G(xs, s, t) = Ψst(xs), for all s, t, and xs ps. Because the flow map Ψst cannot be expressed in closed form, it must be approximated. One option, discussed in Chapter 10, is to rely on pre-trained diffusion model. Alternatively, as we will see in this chapter, new and more tractable surrogates can be introduced. For clarity, existing approaches can be broadly categorized according to whether the training procedure queries teacher during the loop: distillation, which explicitly calls teacher model, and training from scratch, which avoids teacher calls by constructing self-contained surrogates. Building on this principled objective, we now turn to systematic approaches for learning flow map models, with the aim of developing methods that are practical while also producing generations that more accurately reflect the true data distribution and are computationally efficient. We begin with high level introduction to this paradigm. Special Flow Map: Consistency Functions. Consistency Models (Song et al., 2023) represent one of the earliest pioneering approaches to flow-map learning. They learn few-step denoiser fθ(, s) that approximates the special case of the flow map to the origin: Ψs0(), (0, ]. 11.1. Prologue 345 The key idea is that every noisy sample xs should be mapped back to the clean data point x0 at the end of its trajectory. Formally, the oracle training objective for the CM family (Song et al., 2023; Song and Dhariwal, 2024; Geng et al., 2024; Lu and Song, 2024) is Loracle-CM(θ) := EsExsps [w(s)d (fθ(xs, s), Ψs0(xs))] . (11.1.0) In practice, however, the oracle Ψs0(xs) is unavailable. It is therefore replaced by stop-gradient target, denoted as fθ, taken from slightly earlier step Ψsss(xs) on the same trajectory: Ψs0(xs) fθ (Ψsss(xs), s) , > 0, where Ψsss(xs) itself must also be approximated. Two practical strategies are available: (i) distillation, which relies on pre-trained diffusion model, and (ii) training from scratch, which uses one-point estimate without teacher guidance. General Flow Map. Two representative approaches are the Consistency Trajectory Model (CTM) and Mean Flow (MF). Consistency Trajectory Models. Consistency Trajectory Model (CTM) (Kim et al., 2024a) is the first work to learn the general flow map Ψst for arbitrary start and end times, and can be viewed as concrete instance under the unified objective of Equation (10.1.4). CTM adopts an Euler-inspired parametrization by expressing the oracle flow map as Ψst(xs) := xs + s v(xu, u) du = xs + s xs + v(xu, u) du {z gθ , } which motivates the neural parameterization s where gθ is neural network trained so that Ψst(xs) Gθ(xs, s, t). Gθ(xs, s, t) := gθ(xs, s, t), xs + Since the oracle Ψst(xs) is inaccessible, CTM trains against stopgradient target evaluated at an intermediate time u: Ψst(xs) Gθ (cid:0)Ψsu(xs), u, t(cid:1), [t, s], where the intermediate state Ψsu(xs) is approximated in one of two ways: (i) distillation, which uses few-step solver applied to pre-trained diffusion teacher, or (ii) training from scratch, which constructs self-induced teacher directly through the Gθ parametrization. Learning Fast Generators from Scratch Mean Flow. Mean Flow (MF) (Geng et al., 2025a) builds on flow matching by modeling the average drift over an interval [t, s] (with s): hθ(xs, s, t) h(xs, s, t) := 1 t v(xu, u) du, also aligning with Equation (10.1.4). Differentiating the identity (t s) h(xs, s, t) = s v(xu, u) du with respect to yields self-referential relation that motivates the MF objective LMF(θ) := Es Exsps with stop-gradient target w(s) (cid:13) (cid:13)hθ(xs, s, t) htgt θ(xs, s, t)(cid:13) 2 (cid:13) 2 , htgt θ(xs, s, t) := v(xs, s) (s t) (v(xs, s) xhθ + shθ) . In practice, the oracle velocity v(xs, s) must also be approximated. Two common strategies are: (i) distillation, which leverages pre-trained diffusion model trained with flow matching, or (ii) training from scratch, which uses the one-point conditional velocity α ϵ derived from the forward corruption process xs = αsx0 + σsϵ. x0 + σ Relationship Between CTM and MF. CTM and MF approximate the same path integral but parameterize different surrogates of it: Ψst(xs) := xs + v(xu, u) du = xs + s xs + h {z gθ v(xu, u) du } . = xs + (t s) 1 t i v(xu, u) du {z hθ } In words, CTM learns an slope displacement through gθ, while MF learns the average drift hθ; both are consistent ways to approximate the same integral that defines Ψst. 11.1. Prologue 347 What Happens Next? We begin with the CM family, which focuses on the specific flow map Ψs0. This part covers both its discrete time origin in Section 11.2 and its continuous time extension in Section 11.3. We then move on to the general flow map and provide detailed discussion of two key representatives, CTM and MF. Their parameterizations, training strategies, and practical approximations are presented in Section 11.4 and Section 11.5, respectively. We remark that the Elucidating Diffusion Model (EDM) introduced in Section D.6 offers systematic guidelines for designing the network parameterization of the x-prediction model and has demonstrated strong empirical performance. Although this section can be considered optional, the EDM formulation serves as valuable foundation for CM-style models. For clarity of exposition later on, we do not strictly follow the chronological order in which these approaches appeared. Instead, we organize the discussion by conceptual relationships. Nevertheless, to acknowledge originality and respect chronology, we provide the historical timeline in Figure 11.1. 348 Learning Fast Generators from Scratch 11.2 Special Flow Map: Consistency Model in Discrete Time Figure 11.2: Illustration of the flow map semigroup property. This property states that transitioning from to and then from to is equivalent to transitioning directly from to t. An Important Principle of Flow Maps: The Semigroup Property. Consistency Models (introduced in Sections 11.2 and 11.3) and their generalization, the Consistency Trajectory Model (Section 11.4), define their regression targets by exploiting key mathematical structure of flow maps. This structure is the fundamental semigroup property: Ψut Ψsu = Ψst, Ψss = I, for all s, u, [0, ]. (11.2.1) Intuitively, this means that if we first evolve state from to (through Ψsu) and then from to (through Ψut), we end up at exactly the same point as if we had evolved directly from to t. This is nothing more than the basic principle of ODE solving1: once the starting point of flow is specified, its future evolution is completely determined, and it follows single well-defined path. Whether we follow this path in one long step or divide it into smaller intervals, we still move along the same trajectory and arrive at the same final state. 1The semigroup property follows from the uniqueness theorem for ODE initial value problems (see Chapter A). 11.2. Special Flow Map: Consistency Model in Discrete Time 349 To build further intuition for the semigroup property, consider the solution trajectory {x(s)}s[0,T ] of the PF-ODE dx(τ ) dτ = v(x(τ ), τ ), with fixed initial condition x(T ) at time , solved backward in time. If we fix the terminal time at = 0, the corresponding flow map can be written more simply as (, s) := Ψs0(), which is referred to as the consistency function. By construction, this function inherits several fundamental properties directly from the semigroup identity of Equation (11.2.1) with = 0: (i) Global Consistency: every point along the trajectory maps to the same clean endpoint, This is because (x(s), s) = x(0), for all [0, ]. (x(s), s) = Ψs0 (cid:0)Ψ0s(x(0))(cid:1) = (cid:0)Ψs0 Ψ0s (cid:1)(x(0)) = Ψ00(x(0)) = x(0). (ii) Self Consistency: any two points along the same trajectory must give identical outputs, (x(s), s) = (x(u), u), for all s, [0, ]. (11.2.2) This is direct re-interpretation of the semigroup identity: Ψs0Ψ0s = Ψu0 Ψ0u. (iii) Local Consistency: the consistency function is invariant with respect to when evaluated along the trajectory, ds (x(s), s) = 0, (x(0), 0) = x(0). (11.2.3) This follows from global consistency, which states that (x(s), s) does not change with along the trajectory. The three properties are all equivalent. Each states that along any solution trajectory 7 x(s), the flow-to-origin/consistency map (x(s), s) = Ψs0(x(s)) yields the same terminal point x(0), independent of the starting time. 350 Learning Fast Generators from Scratch Goal of Consistency Models. CM aims to train neural network fθ : RD [0, ] RD to approximate the special flow map Ψs0, i.e., consistency function2. The key idea is to enforce the semigroup property across multiple trajectories of the PF-ODE, ensuring that different noisy versions of the same data point consistently map back to the same clean origin (more precisely, this corresponds to the special case = 0 and = in Equation (11.2.1)). There are, however, multiple ways to realize this goal. The choice depends on whether pre-trained diffusion model is available and whether training is carried out in discrete-time or continuous-time regime. We begin by summarizing these variants in Table 11.1 and illustrating their objectives in Figure 11.3. The subsequent sections (Sections 11.2 and 11.3) then gradually develop the details of each approach. Table 11.1: Training Objectives of Consistency Models Distillation From Scratch Discrete-time Equation (11.2.4) Equation (11.2.6) Continuous-time Equation (11.3.5) Equation (11.3.6) 11.2.1 Discrete-Time Approximations for Learning Consistency Function In principle, consistency function can be learned by minimizing the oracle loss Equation (11.1.0): Loracle-CM(θ) := EsExsps [w(s)d (fθ(xs, s), Ψs0(xs))] . This objective enforces that every noisy sample xs is mapped back to its clean endpoint Ψs0(xs). The challenge is that the oracle map Ψs0(xs) is not available in practice. To overcome this, Song et al. (2023) exploit the semigroup property: any noisy state and its consecutive step along the same PF-ODE trajectory must map to the same clean endpoint. Concretely, the oracle target is replaced by 2The concept of consistency function for an ODE generalizes to function (x, t) for an SDE such that (xt, t) is (local) martingale with respect to the SDEs natural filtration, i.e. E[f (xt, t)xs] = (xs, s), for all s. This generalization was proposed/observed in (Daras et al., 2023; Lai et al., 2023a), and the theoretical connections are summarized in (Lai et al., 2023b). 11.2. Special Flow Map: Consistency Model in Discrete Time 351 stop-gradient target taken from slightly earlier point on the trajectory: Ψs0(xs) = Ψss0 (Ψsss(xs)) fθ (Ψsss(xs), s) , > 0, where θ are parameters under the stop-gradient operator. further difficulty is that the intermediate state Ψsss(xs) has no closed form either and must itself be approximated. Two practical regimes have been proposed: With Pre-trained Diffusion Model (Consistency Distillation). Suppose we have access to pre-trained diffusion model. Consistency Distillation (CD) leverages the teacher model to approximate the intermediate state Ψsss(xs) by simulating only single backward ODE step: Ψsss(xs) Solversss(xs). More concretely, pre-trained diffusion model provides an estimate of the score function sϕ(xs, s) xs log ps(xs). Using this, one can perform one-step DDIM update from xs to obtain an approximation of the state at = s: Ψsss(xs) xs + σ2 xs + σ2 αs αs αs αs := xϕ . (cid:18) αs αs (cid:18) αs αs (cid:19) (cid:19) σs σs σs σs xs log ps(xs) sϕ(xs, s) Combining this construction with the stop-gradient target yields practical discrete-time proxy for the oracle loss Loracle-CM(θ). Formally, over partition 0 = s1 < s2 < < sN = , the CD training objective is given by CD(θ, θ; ϕ) := Ex0,ϵ,i LN ω(si) d(cid:0)fθ(xsi+1, si+1), fθ(xϕ si , si)(cid:1)i . (11.2.4) Here, ω() is time-dependent weight, d(, ) is distance measurement, and θ indicates stop-gradient parameters, which prevent collapse to trivial solutions (e.g., constant predictions). 352 Learning Fast Generators from Scratch Without Pre-Trained Diffusion Model (Consistency Training). When no pre-trained diffusion model is available, the oracle score log ps(xs) can still be estimated directly using simple one-point approximation (albeit with high variance). Recall that it admits the conditional expectation form: xs log ps(xs) = Ex0p(x0xs) [xs log p(xsx0)] = Ex0p(x0xs) (cid:20) (cid:21) . xs αsx0 σ2 The identity above suggests simple one-sample estimator. If xs is obtained from paired sample (x0, ϵ) via xs = αsx0 + σsϵ, then (cid:92)x log ps(xs) := ϵ σs = xs αsx0 σ2 serves as an unbiased estimator of the score at xs (conditionally unbiased with respect to p(x0xs)). It corresponds exactly to the conditional score used as the regression target in denoising score matching. Plugging this estimate into the DDIM one-step update from to = ss (see Equation (9.2.3)) yields Ψss(xs) xs + σ2 αs αs αs αs αs αs xs + σ2 (cid:18) αs αs = αsx0 + σsϵ, xs = σs σs σs σs (cid:19) (cid:18) αs αs (cid:18) αs αs σs σs (cid:19) xs log ps(xs) (cid:19) (cid:92)xs log ps(xs) (DDIM) (1-pt score) (11.2.5) (xs αsx0) where3 x0 is the same data sample and ϵ is the same Gaussian noise used to construct xs. This leads to teacher-free discrete-time surrogate of the oracle objective Loracle-CM, written as CT(θ, θ) := Ex0,ϵ,i LN (cid:2)ω(si) (cid:0)fθ(xsi+1, si+1), fθ(xsi, si)(cid:1)(cid:3) , (11.2.6) with xsi = αsix0 + σsiϵ and xsi+1 = αsi+1x0 + σsi+1ϵ. 3The last identity follows directly from the forward corruption process xs = αsx0 + σsϵ by elementary algebra. 11.2. Special Flow Map: Consistency Model in Discrete Time 353 Using αsx0 + σsϵ directly as an approximation of Ψss(xs) without expectation introduces high variance4. Recall, however, the analogous case in denoising score matching (see Section 6.1), where single conditional score sample serves as the training target yet becomes unbiased once averaged over x0, ϵ in the loss. By the same reasoning, the expectations over x0 and ϵ in LN CT average out this sampling noise, yielding an unbiased loss-level approximation. The following theorem formalizes this expectation-level justification of the one-point estimator. Theorem 11.2.1: CM-CT Equivalence up to Error O(s2) Let := s, and define LCM(θ, θ) := Es,x0,ϵ LCT(θ, θ) := Es,x0,ϵ (cid:2)w(s) d(cid:0)fθ(xs, s), fθ(xDDIM (cid:2)w(s)d(cid:0)fθ(xs, s), fθ(xs, s)(cid:1)(cid:3), , s)(cid:1)(cid:3), where := xDDIM (cid:18) αs αs is the oracle DDIM update. Both xs = αsx0 +σsϵ and xs = αsx0 +σsϵ share the same pair (x0, ϵ) with x0 pdata and ϵ (0, I). Then, xs log ps(xs) xs + σ2 αs αs σs σs (cid:19) LCM(θ, θ) = LCT(θ, θ) + O(s2). Proof for Theorem. First, note that the DDIM update with the oracle score equals the conditional mean, xDDIM = E[xsxs], which can also be verified from Equation (11.2.5) by taking the expectation over p(xs). Next, perform Taylor expansion of d(fθ(xs, s), fθ(, s)) = E[xsxs]. The linear term of Taylor expansion vanishes around xDDIM because the inner expectation is taken over xsxs, satisfying E[xs 4The one-point (conditional) score estimate (cid:92)x logps(xs) can be viewed as one-sample Monte Carlo estimator, which is conditionally unbiased given xs: averaging this estimator over the (generally intractable) clean posterior p(xs) recovers the true score as xs log ps(xs) = Ex0p(xs) (cid:92)x logps(xs) . 354 Learning Fast Generators from Scratch xDDIM xs] = 0. This shows that, by reparameterizing the conditional as Ex0,ϵxs[] with xs = αsx0 + σsϵ, the DDIM update using the 1-pt score exactly recovers xs pathwise for the same (x0, ϵ) and therefore leaves the inner expectation unchanged. The remaining term is quadratic, O(s2), hence LCT = LCM + O(s2). detailed derivation is provided in Section D.5. In summary, CD leverages teacher model for initialization and guidance, which often stabilizes optimization and reduces variance. In contrast, Consistency Training (CT) requires no pre-trained model and can therefore be trained entirely from scratch. Despite this difference, CT serves as fully standalone generative model. In practice, Song et al. (2023) adopt the EDM forPractical Considerations. mulation of Karras et al. (2022) (see Section D.6) with the forward corruption kernel xs = x0 + sϵ, and use the neural network parameterization proposed therein (cf. Equation (D.6.1)): fθ(x, s) = cskip(s)x + cout(s) Fθ (cin(s) x, cnoise(s)) , where Fθ is neural network and the coefficients follow Equation (D.6.5). This parameterization has the important boundary property fθ(x, 0) = x, which enforces consistency at time zero and ensures the network output matches its input when no noise is present. 11.2.2 Sampling with Consistency Model Once consistency model fθ is trained, either in continuous or discrete time, it can be used to generate samples in single step or few steps. The algorithm is summarized in Algorithm 9. One-Step Generation. Given an initial latent ˆxT sampled from the prior distribution (in practice, (0, 2I)), clean sample can be generated via single function evaluation: fθ(ˆxT , ). 11.2. Special Flow Map: Consistency Model in Discrete Time 355 Multi-Step Generation. With pre-selected timesteps > τ1 > τ2 > > τM 1 = 0, start from initial noise ˆxT and alternate between noise injection and large clean jumps via the consistency model at earlier time points, gradually refining the sample: ˆxT long jump get clean fθ(ˆxT , ) add noise to level τ1 ˆxτ1 long jump get clean fθ(ˆxτ1, τ1) add noise to level τ2 . Algorithm 9 CMs Sampling with One-Step or Multi-Step Generation Input: Consistency model fθ (, ), sequence of time points > τ1 > τ2 > > τM 1 = 0, initial noise ˆxT fθ (ˆxT , ) 1: if one-step then 2: 3: else 4: 5: 6: 7: 8: 9: 10: end if Output: end for fθ (ˆxT , ) for = 1 to 1 do Sample ϵ (0, I) ˆxτm ατmx + στmϵ fθ (ˆxτm , τm) 356 Learning Fast Generators from Scratch 11.3 Special Flow Map: Consistency Model in Continuous Time We now move beyond the discrete-time setting of consistency models and consider continuous-time perspective. Unlike the discrete approach, which fixes time grid and trains only on those sampled points, the continuous formulation treats the flow map as defined for all times. This shift eliminates the dependence on an arbitrary discretization and provides more principled alignment with the underlying dynamics. It also helps reduce the approximation errors that naturally arise from discretized integration, and ensures consistency is enforced globally rather than only at selected steps. 11.3.1 Continuous-Time Consistency Model To motivate the continuous time formulation, we first revisit Equation (11.2.3), which describes the condition under which time derivatives can be taken. Using the chain rule, we arrive at ds (x(s), s) = 0 (xf ) (x(s), s) x(s) ds {z } ODE velocity + (cid:19) (cid:18) (x(s), s) = 0, (11.3.1) where the trajectory x(s) follows the PF-ODE ds x(s) = v(x(s), s). This relationship shows that the consistency function remains constant along any solution trajectory of the ODE. The velocity field can be estimated in practice either from pre-trained diffusion model (when such model is available) or from direct one point approximation, such as α sϵ, as explained in Section 11.2. sx0 + σ Equation (11.3.1) suggests natural way to design training objective in continuous time. One approach is to enforce the condition by minimizing the residual in manner similar to physics informed neural networks (PINNs) (Raissi, 2018; Boffi et al., 2024): Es,x0,ϵ min θ \"(cid:13) (cid:13) (cid:13) (cid:13) ds (cid:13) 2 (cid:13) fθ(xs, s) (cid:13) (cid:13) 2 # . In practice, however, Song et al. (2023) and Lu and Song (2024) observed that different formulation works better in training. Instead of directly 11.3. Special Flow Map: Consistency Model in Continuous Time 357 enforcing the differential condition, they consider the continuous time limit of the discrete approximation as 0: CM(θ, θ) := Ls ω(s)(cid:13) (cid:13)fθ(xs, s) fθ (cid:0)Ψsss(xs), s(cid:1)(cid:13) 2 (cid:13) 2 . (11.3.2) Taking the limit 0 in Equation (11.3.2) is equivalent to letting the number of time steps in Equations (11.2.4) and (11.2.6). We summarize this key idea in the following proposition. Proposition 11.3.1: Continuous-Time Consistency Training The following convergence result holds: lim s0 1 Here, θLs CM(θ, θ) = θL CM(θ, θ). CM(θ, θ) := Es,x0,ϵ (cid:20) 2ω(t)f θ (xs, s) (cid:21) fθ(xs, s) , ds and the total differentiation identity, ds fθ(xs, s) = sfθ(xs, s) + (cid:0)xfθ(xs, s)(cid:1)v(xs, s). (11.3.3) Proof for Proposition. first-order Taylor expansion of the stop-gradient target around (xs, s) CM behaves, up to O(s2), like an inner product shows that the loss Ls between the student update θfθ(xs, s) and the tangent change ds fθ(xs, s). Consequently, the scaled gradient satisfies lim s0 1 θLs CM = θE (cid:20) ω(s)f θ (xs, s) (cid:21) fθ(xs, s) , ds which is the claimed identity. We defer the proof to Section D.5. The result above is written under the gradient operator θ so that terms involving θ vanish, since θ is treated as constant under stop-gradient. Note that ds fθ(xs, s) denotes the total derivative along the oracle trajectory, rather than simple partial time derivative. In summary, the continuous time consistency model can be trained by minimizing the following objective (ignoring the factor 2): 358 Learning Fast Generators from Scratch CM(θ, θ) := Es,x0,ϵ (cid:20) ω(s)f θ (xs, s) (cid:21) fθ(xs, s) . ds (11.3.4) 11.3.2 Training Continuous-Time Consistency Model Similar to the discrete time case discussed in Section 11.2.1, we now clarify the practical approximation of the tangent term in Equation (11.3.4), which involves the inaccessible oracle velocity v: ds fθ(xs, s) = sfθ(xs, s) + (cid:0)xfθ(xs, s)(cid:1) v(xs, s). After training continuous-time CM, sampling follows the same procedure as in the discrete time case (Section 11.2.2). Continuous-Time Consistency Distillation. is available such that vϕ v, then the tangent vector Equation (11.3.3) can be approximated by the surrogate If pre-trained diffusion model ds fθ(xs, s) in ds fθ(xs, s) sfθ(xs, s) + (cid:0)xfθ(xs, s)(cid:1) vϕ(xs, s). (11.3.5) We denote the resulting objective as tion 11.3.1 can be restated as CM(θ, θ; ϕ). Accordingly, Proposilim θ LN CD(θ, θ; ϕ) = θ CD(θ, θ; ϕ). Continuous-Time Consistency Training (from Scratch). On the other hand, if pre-trained diffusion model is not available, the oracle velocity can be approximated using the one point conditional estimate α sϵ. In this case, the tangent vector ds fθ(xs, s) in Equation (11.3.3) is replaced by the surrogate sx0 + σ ds fθ(xs, s) sfθ(xs, s) + (cid:0)xfθ(xs, s)(cid:1) (cid:0)α sx0 + σ sϵ(cid:1) . (11.3.6) CT(θ, θ), which corresponds to the We denote the resulting objective as training from scratch setting. Accordingly, Proposition 11.3.1 can be restated as lim θLN CT(θ, θ) = θL CT(θ, θ). So far, we have introduced all the fundamental approaches listed in Table 11.1 to realize the learning of the consistency function Ψs0. To provide 11.3. Special Flow Map: Consistency Model in Continuous Time 359 clearer overview, Figure 11.3 summarizes the relationships among the different loss functions for training consistency functions. The figure also indicates whether each method relies on pre-trained diffusion model and distinguishes between continuous time and discrete time objectives. Discrete-Time CD LCD(θ, θ) = LCT(θ, θ) + O(s) (Theorem 11.2.1) Discrete-Time CT lim θLN CD = θL CD (Theorem 5) lim N θLN CD(θ, θ; ϕ) = θL CT(θ, θ) (Theorem 6) lim N θLN CT = θL CT (Theorem 6) Continuous-Time CD CD(θ, θ; ϕ) = CT(θ, θ) Continuous-Time CT Figure 11.3: Diagram showing relationships between discrete/continuous-time CD and CT under the ℓ2 distance metric: d(x, y) = y2 2. The marked theorems follow the labeling in (Song et al., 2023). Whenever the theorems involve CT, we assume perfect score: sϕ (x, t) log pt(x). CD is defined in Equation (11.3.5). CT is defined in Equation (11.3.4), while However, the tangent vector ds fθ often causes instability during training. In the following optional section, we present techniques from Simplifying, Stabilizing and Scaling Continuous Time Consistency Models (sCM) (Lu and Song, 2024) that mitigate these issues. 11.3. (Optional) Practical Considerations of Continuous-Time Consistency Training Our interest lies in the training from scratch scenario, since it yields standalone generative model that does not rely on external pre-trained diffusion models. Hence, we focus our discussion on the continuous time case. In practice, however, training directly with Equation (11.3.4) is often unstable, as the term ds fθ can exhibit large or unbounded time derivatives, leading to exploding gradients during optimization. To overcome this, suitable parameterizations and stabilization strategies are typically required (Geng et al., 2025b; Lu and Song, 2024). As summarized in Section 6.2.2, the main factors that influence stable training include the diffusion process, parameterization choices, time weighting function, and time sampling distribution, all of which should be carefully designed and disentangled also in continuous-time CM. 360 Learning Fast Generators from Scratch Diffusion Process. Instead of using the standard diffusion parameterization xs = αsx0 + σsϵ with ϵ (0, I), Lu and Song (2024) adopt trigonometric schedule. This schedule, although mathematically equivalent to the original form (as shown in Equation (6.3.4)), provides cleaner structure and better separation in the training objective, which contributes to improved stability during training 5. In addition, they incorporate the standard deviation σd of the data distribution pdata, in line with EDMs design in Section D.6.1: xs := cos(s)x0 + sin(s)z, where (0, σ2 dI). (11.3.7) This formulation is fully general. For any diffusion process of the form xs = αsx0 + σsϵ with ϵ (0, I), we can equivalently write: xs = αsx0 + (σdϵ), σs σd := σs σd by defining := σdϵ, α s, σ s) can then be mapped to the trigonometric form (cos(s), sin(s)) using the normalization described in Equation (6.3.5). . The transformed pair (α := αs, and σ Parametrizations. By considering the analogous principles of EDM in Section D.6.1, Lu and Song (2024) propose the following parametrization for the neural network similar to Equation (D.6.1): fθ(x, s) := cskip(s)x + cout(s)Fθ (cin(s)x, cnoise(s)) . Here, cskip(s), cout(s), and cin(s) can be derived using the same criteria presented in Section D.6.1 (see Appendix of Lu and Song (2024) for detailed derivations), and are given by cskip(s) = cos(s), cout(s) = σd sin(s), cin(s) 1 σd . This is considered along with the default choice cnoise(s) = s, where scnoise(s) is bounded to ensure training stability, as will be discussed around Equation (11.3.10). This leads to the following parametrization under the trigonometric schedule: fθ(x, s) = cos(s)x sin(s)σdFθ , cnoise(s) (cid:19) . (cid:18) σd (11.3.8) We note that this parametrization also ensures that the neural network always satisfies the boundary condition fθ(x, 0) for all x, which is an essential property of consistency function. 5Intuitively, both the trigonometric functions and their derivatives are bounded, which helps prevent scale explosion in terms like ds fθ . detailed discussion is provided later. 11.3. Special Flow Map: Consistency Model in Continuous Time 361 Techniques for Stabilizing Tangent Training. Under the trigonometric schedule and the network parametrization described in Equation (11.3.8), the gradient of the loss in Equation (11.3.4) becomes θL CT(θ, θ) = θEs,x0,ϵ (cid:20) ω(s)σd sin(s)F θ (cid:19) , (cid:18) xs σd dfθ ds (cid:21) (xs, s) . (11.3.9) In theory, training with the gradient update in Equation (11.3.9) may be sufficient to learn consistency function. However, Lu and Song (2024) empirically observed that the training process can become unstable in practice due to the behavior of the tangent function, given by dfθ(xs, s) ds {z A. } = cos(s) (cid:18) σdxsFθ (cid:19) , (cid:18) xs σd (cid:19) dxs ds sin(s) (cid:18) xs + σd dFθ ds (cid:18) xs σd , cnoise(s) (cid:19)(cid:19) . In particular, instability was observed in the term dFθ ds sin(s) (cid:18) xs σd {z B. (cid:19) , cnoise(s) } = sin(s)xsFθ dxs ds + sin(s)sFθ. More specifically, the instability arises from the component sin(s)sFθ = sin(s) cnoise(s) emb(cnoise) cnoise Fθ emb(cnoise) . (11.3.10) {z C. } Here, we follow common practice in the DM and CM literature by applying positional or Fourier embedding, denoted by emb(), to the time variable cnoise(s): 7 cnoise(s) 7 emb(cnoise(s)) 7 Fθ , emb(cnoise(s)) (cid:19) . (cid:18) xs σd Therefore, some additional empirical techniques are introduced to mitigate the instability: A. Tangent Normalization. Explicitly normalize the tangent function by replacing ds fθ with ds fθ ds fθ , where > 0 is constant set empirically. +c Alternatively, clipping the tangent within [1, 1] can also effectively cap its variance. 362 Learning Fast Generators from Scratch B. Tangent Warm-Up. Since the term sin(s)(xs + σd ds Fθ) may induce instability, an optional technique can be applied by replacing the coefficient sin(s) with sin(s), where linearly increases from 0 to 1 over the first few training iterations. C. Time Embedding. In light of the derivative chain in Equation (11.3.10), Lu and Song (2024) opted for smaller magnitude parameter to control the derivative emb(cnoise) . For similar reason, cnoise(s) = is chosen, cnoise where scnoise(s) = 1a bounded constant. On top of these, architectural changes for improved normalization (for stability) and efficient JVP-based computation of ds fθ are often necessary, but beyond our scope. Time-Weighting Function. Manual design of the time-weighting function ω(s) may lead to suboptimal performance. To address this, following similar approach to EDM-2 (Karras et al., 2024), Lu and Song (2024) learn an adaptive weighting function ωφ(s) to balance the training loss variance across different times (see Equation (11.3.11) for the desired outcome). To elaborate further, we observe that the objective function in Equation (11.3.9) takes the form Es,x0,ϵ F θ , with = ω(s)σd sin(s) dfθ ds . Since is vector independent of θ, Equation (11.3.9) is equivalent to θEs,x0,ϵ F θ = 1 2 θEs,x0,ϵ Fθ Fθ + y2 2 . Based on this observation, Lu and Song (2024) propose additionally training an adaptive weighting network ωφ(s) to estimate the loss norm, formulated as the following minimization problem: min φ Es,x0,ϵ \" eωφ(s) Fθ Fθ + y2 # 2 ωφ(s) . To understand the effect of the adaptive weighting, observe that the optimal solution ω(s) (obtained by taking the partial derivative of the above objective with respect to ωφ) satisfies Es,x0,ϵ \" eω(s) Fθ Fθ + y2 2 # = 1. (11.3.11) 11.3. Special Flow Map: Consistency Model in Continuous Time 363 That is, after rescaling, the expected (weighted) loss across different is kept uniform. As result, the adaptive weighting effectively reduces the variance of the training loss across different time steps, leading to more balanced and stable training. Time Sampling Distribution. Lu and Song (2024) opt to sample tan(s) from log-normal proposal distribution (Karras et al., 2022), that is, eσd tan(s) (; Pmean, 2 std). (11.3.12) Here, Pmean and Pstd are two hyper-parameters. Summary of Training Objective. sion, the final training loss is expressed as: In summary of the aforementioned discusLsCM(θ, φ) := \" eωφ(s) Es,x0,ϵ (cid:13) (cid:13) Fθ (cid:13) (cid:13) (cid:18) xs σd (cid:19) , Fθ (cid:19) , (cid:18) xs σd cos(s) dfθ ds (cid:13) 2 (cid:13) (xs, s) (cid:13) (cid:13) 2 # ωφ(s) . Here, is sampled according to Equation (11.3.12), and xs is computed via Equation (11.3.7). The model trained with this loss is referred to as sCM, and its training procedure is summarized in Algorithm 10. Learning Fast Generators from Scratch Algorithm 10 Training of Continuous-time Consistency Models (sCM) Input: dataset with std. σd, pre-trained DM Fpretrain with parameter θpretrain, model Fθ, weighting ωφ, learning rate η, proposal (Pmean, Pstd), constant c, warmup iteration 1: Init: θ θpretrain, Iters 0 2: Repeat dI), τ (Pmean, 2 std), arctan x0 D, (0, σ2 xs cos(s)x0 + sin(s)z if consistency training then dxs ds cos(s)z sin(s)x0 (cid:17) else dxs ds σdFpretrain (cid:16) xs σd , (cid:17) (cid:16) eτ σd (cid:1) end if min (cid:0)1, Iters cos2(s)(σdF w+c LsCM(θ, φ) eωφ(s) θ dxs ds ) cos(s) sin(s) Tangent warmup (cid:16) xs + σd dFθ ds (cid:17) Tangent normalization (cid:13) (cid:13) (cid:13)Fθ (cid:16) xs σd (cid:17) , Fθ (cid:16) xs σd (cid:17) , (cid:13) 2 (cid:13) ωφ(s) (cid:13) 2 Adaptive weighting 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: (θ, φ) (θ, φ) ηθ,φLsCM(θ, φ) Iters Iters + 14: 15: 16: until convergence 11.4. General Flow Map: Consistency Trajectory Model 365 11.4 General Flow Map: Consistency Trajectory Model Consistency Trajectory Model (CTM) (Kim et al., 2024a) is among the first methods to learn general flow map Ψst. Setup of CTM in Practice. Similar to the CM family, CTM originally follows the formulation of EDM (Karras et al., 2022) (Section D.6), using the PF-ODE in x-prediction form with the noise schedule αt = 1 and σt = t. Under this setup, the PF-ODE becomes dx(τ ) dτ = x(τ ) E[xx(τ )] τ . Starting from xs at time and evolving to later time s, the corresponding flow map (solution) can be written equivalently as xs + s xτ E[xxτ ] τ dτ. CTM adopts an Euler-inspired parameterization: applying single-step Euler solver (equivalently, DDIM; see Equation (9.2.4)) to the PF-ODE yields xEuler st = xs (s t) xs E[xxs] = xs + (cid:16) 1 (cid:17)E[xxs], where xEuler st approximates the solution at time given the state xs at time s. While the EDM setup provides simple illustrative case, CTM allows broader noise schedules defined by an arbitrary linear Gaussian forward kernel (αt, σt) and expresses the PF-ODE in v-prediction form: Ψst(xs) = xs + s v(xu, u) du. In the discussion that follows, we focus on this general formulation. 11.4.1 CTM Parametrization for Flexible Transition Learning Following the single-step Euler solver of the PF-ODE above, CTM rewrites the oracle flow map Ψst as convex combination of the input xs and residual function g: Ψst(xs) := xs + v(xu, u) du = xs + s h xs + v(xu, u) du s {z =: . } 366 Learning Fast Generators from Scratch where the residual term is defined as g(xs, s, t) := xs + Z v(xu, u) du. (11.4.1) This motivates the neural parameterization Gθ(xs, s, t) := xs + gθ(xs, s, t), (11.4.2) where gθ is neural network that aims at gθ g, hence Gθ(xs, s, t) is trained to approximate the oracle flow map, Gθ(xs, s, t) Ψst(xs). Therefore, CTM naturally fits within the general consistency-mapping framework of Equation (10.1.4), which aligns the learned mapping with the oracle flow map. Moreover, this formulation inherently satisfies the initial condition Gθ(xs, s, s) = xs, without requiring any explicit enforcement during training. Advantages of CTMs Parametrizations. crucial characteristic of becomes evident when taking the limit as approaches (i.e., the same ending time as the starting time): Proposition 11.4.1: Properties of (i) Recovering Diffusion Model: g(xs, s, s) = lim ts g(xs, s, t) = xs sv(xs, s). (ii) Integration Representation: g(xs, s, t) = xs sv(xs, s) + O(t s). Proof for Proposition. From the definition of g, we obtain lim st g(xs, s, t) = xt lim st 1 s v(xτ , τ ) dτ = xs sv(xs, s). 11.4. General Flow Map: Consistency Trajectory Model 367 This proved the first identity. For the second claim, from the Taylor expansion, we have g(xs, s, t) = xs = xs s s v(xτ , τ ) dτ (cid:20) (s t)v(xs, s) + O((t s)2) = xs sv(xs, s) + O(t s). (cid:21) From this proposition, we can conclude that 1. Estimating enables approximating not only the finite s-to-t transition (for t) but also the infinitesimal s-to-s transition characterized by the instantaneous velocity v. 2. g(xs, s, t) is interpreted as the oracle velocity added with residual term of the Taylor expansion. Therefore, by leveraging CTMs parameterization in Equation (11.4.2), learning Gθ Ψst (or equivalently, gθ g) enables both long-jump capability via Gθ, and recovery of the diffusion models velocity (or equivalently, the score function/denoiser) via gθ. This parameterization is thus key: by learning g, CTM unifies the strengths of diffusion models and consistency model (special flow map) under single framework. In the next two sections, we first present CTMs consistency loss (Section 11.4.2), which supports both distillation and training-from-scratch, and enforces the semigroup property to achieve Gθ(, s, t) Ψst(, s, t). We then describe auxiliary losses (Section 11.4.3) that arise naturally from the parametrization in Equation (11.4.2), including diffusion model loss and GAN loss, which further improve CTMs performance significantly. 11.4.2 Consistency Loss in CTM CTM aims to approximate the oracle solution map Gθ(, s, t) Ψst(, s, t), for any t. Since the oracle Ψst is usually not available in closed form, CTM builds feasible regression target by enforcing the semigroup property (Equation (11.2.1)): for any t, Ψut Ψsu = Ψst. Learning Fast Generators from Scratch Depending on whether pre-trained diffusion model is available, the flow map Ψst can be approximated in different ways. Throughout, we assume [0, ]. Training via Distillation. Assume access to pre-trained diffusion model producing vϕ(xs, s) v(xs, s). Then the PF-ODE is approximated by the empirical dynamics dx(τ ) dτ = vϕ(xτ , τ ). (11.4.3) CTM trains Gθ to match numerical solver Solverst(xs; ϕ) applied to this empirical ODE, which serves as computable proxy for the oracle: Gθ(xs, s, t) Solverst(xs; ϕ) Ψst(xs, s, t). With strong teacher, the solver can recover Ψst up to discretization error, so the optimal student closely matches the ground truth (see (Kim et al., 2024a), Propositions 3 and 4). However, solving across the full interval [t, s] during training loop can be costly when and are far apart. To improve efficiency and provide smoother signal, CTM introduces soft consistency matching, which operationalizes the semigroup property. As illustrated in Figure 11.4, CTM compares two predictions at time t: the direct student output Gθ(xs, s, t), and mixed teacherstudent path that first advances the teacher from to random U[t, s), then lets the student jump from to t: Gθ (cid:0)Solversu(xs; ϕ), u, t(cid:1). The student is trained to match this composite prediction: Gθ(xs, s, t) } {z Ψst(xs) Gθ (cid:0)Solversu(xs; ϕ), u, t(cid:1) } {z Ψut(Ψsu(xs)) , (11.4.4) where θ is stop gradient copy of Gθ. By varying u, CTM interpolates between global and local supervision: Global Consistency (u = s): the student mimics the teacher over the full interval (t, s), receiving the most informative teacher signal. Local Consistency (u = s): the student learns from short teacher step near s; when = 0, this reduces to consistency distillation. 11.4. General Flow Map: Consistency Trajectory Model 369 To reinforce sample quality while aligning trajectories, both predictions are mapped to time 0 by the stop gradient student and compared in feature space metric d: xest(xs, s, t) := Gθ xtarget(xs, s, u, t) := Gθ (cid:0)Gθ(xs, s, t), t, 0(cid:1), (cid:0)Gθ(Solversu(xs; ϕ), u, t), t, 0(cid:1). The CTM consistency loss is Lconsist(θ; ϕ) := Es[0,T ]Et[0,s]Eu[t,s)Ex0 Exsx0 d(cid:0)xest, xtarget (cid:1)i , (11.4.5) which encourages the student to match the empirical PF-ODE solution while preserving generation quality. Training from Scratch. Leveraging CTMs special parameterization (Proposition 11.4.1(i)), g(xτ , τ, τ ) = xτ τ v(xτ , τ ) = v(xτ , τ ) = xτ g(xτ , τ, τ ) τ . We can therefore replace the oracle residual function g(, τ, τ ) with CTMs own estimate gθ(, τ, τ ) for τ [0, ], which yields self-induced empirical PF-ODE: dx(τ ) dτ = x(τ ) gθ (x(τ ), τ, τ ) τ . (11.4.6) We then approximate the oracle solution map by solving this ODE and training the student to match the solver output: Gθ(xs, s, t) Solverst(xs; θ) Ψst(xs, s, t). As in the distillation case Equation (11.4.4), full integration over [t, s] can be costly when and are far apart. CTM therefore enforces the semigroup property to obtain shorter supervision path: Gθ(xs, s, t) } {z Ψst(xs) Gθ (cid:0)Solversu(xs; θ), u, t(cid:1) , } {z Ψut(Ψsu(xs)) where U[t, s) and θ is stop gradient copy of the student. The only change from distillation is that the external teacher vϕ is replaced by the self-induced teacher gθ. 370 Learning Fast Generators from Scratch Figure 11.4: Illustration of CTMs semigroup property. For any t, CTM enforces (cid:0)Solversu(xs), u, t(cid:1), i.e., short solver segment followed by Gθ(xs, s, t) Gθ CTM jump to matches the direct CTM map t. The solver may be pre-trained diffusion or CTMs self-induced teacher. To couple trajectory matching with sample quality, both predictions are mapped to time 0 using the stop gradient student and compared in feature space. The target without any pre-trained model is ˆxtarget := Gθ (cid:0)Gθ(Solversu(xs; θ), u, t), t, 0(cid:1), which replaces xtarget in Equation (11.4.5), and leads to: Lconsist(θ; θ) := Es[0,T ]Et[0,s]Eu[t,s)Ex Exsx0 d(cid:0)xest, ˆxtarget (cid:1)i , (11.4.7) Conceptually, this is self-distillation within CTM: the model supplies its own short horizon teacher signals while the student learns the full transition. 11.4.3 Auxiliary Losses in CTM (Self-)distillation can underperform the teacher because it optimizes only teacher generated targets, lacking direct supervision from real data. By contrast, CTM can naturally incorporate data driven regularizers, for example by augmenting its objective with denoising score matching and an adversarial (GAN) term (Goodfellow et al., 2014), to better learn the flow map. 11.4. General Flow Map: Consistency Trajectory Model 371 Natural Integration of Diffusion Loss. The diffusionmodel loss (more precisely, the conditional flow matching loss; see Equation (5.2.9)) integrates naturally into CTM and provides fixed regression target that facilitates the learning of the flow map model. To see this, note that we have v(xs, s) = xs g(xs, s, s) , g(xs, s, s) gθ(xs, s, s). This naturally induces velocity parametrization through gθ: vθ(xs, s) := 1 (cid:0)xs gθ(xs, s, s)(cid:1). Using the linear Gaussian path xs = αsx0 + σsϵ, x0 pdata, ϵ (0, I), the diffusion model loss can be written as LDM(θ) := Ex0,ϵ,s w(s)(cid:13) (cid:13)vθ(xs, s) (cid:0)α sx0 + σ sϵ(cid:1)(cid:13) 2 (cid:13) 2 . (11.4.8) LDM improves accuracy when is close to by explicitly supervising small jumps along the trajectory. In this regime, the factor 1 in Equation (11.4.2) approaches zero, which can weaken gradients and slow learning; LDM supplies stronger local signal and stabilizes training. Conceptually, Equations (11.4.5) and (11.4.7) enforce trajectory matching (zeroth order), while Equation (11.4.8) enforces slope matching (first order). (Optional) GAN Loss. While consistency and diffusion model loss provide strong regression signals, they can yield overly smooth outputs. CTM therefore optionally adds an adversarial term to encourage sharper, more realistic samples by aligning the generator distribution with the data distribution. With discriminator Dζ that distinguishes real x0 pdata from generated xest(xs, s, t), the objective is LGAN(θ, ζ) := Ex0 (cid:2) log Dζ(x0)(cid:3) + Es[0,T ]Et[0,s]Ex Exsx0 (cid:2) log(1 Dζ(xest(xs, s, t)))(cid:3), where Dζ is maximized and Gθ is minimized. Intuitively, the discriminator acts as an adaptive perceptual distance that encourages realistic detail. Theoretically, the GAN term drives distributional matching (JensenShannon divergence) between pdata and the model distribution induced by Gθ (Goodfellow et al., 2014), which can raise fidelity beyond the teacher. 372 Learning Fast Generators from Scratch Overall CTM Objective. sion, and GAN losses into single training framework: In summary, CTM unifies (self-)distillation, diffuLCTM(θ, ζ) := Lconsist(θ; ϕ/θ) + λDMLDM(θ) + λGANLGAN(θ, ζ), where the teacher is either an external pre-trained model ϕ or the self-induced teacher θ. The regression style components Lconsist and LDM act as strong regularizers, while the optional GAN term improves fine scale detail without sacrificing stability (Kim et al., 2024b). 11.4.4 Flexible Sampling with CTM CTM learns the general flow map Ψst for any > t, which means it supports anytime to anytime transitions. This property enables flexible sampling strategies. For example, CTM proposes γ sampling, where the hyperparameter γ controls the stochasticity during generation. In addition, CTM can reuse standard inference techniques developed for diffusion models, such as ODE based solvers and exact likelihood computation. In what follows, we fix discrete time grid for sampling = τ0 > τ1 > τ2 > > τM = 0. Algorithm 11 CTMs γ-sampling Input: Trained CTM Gθ, γ [0, 1], = τ0 > τ1 > τ2 > > τM = 0. 1: Start from xτ0 pprior = (0, 2I) 2: for = 0 to 1 do τn+1 p1 γ2τn+1 3: Denoise xτn+1 Gθ(xτn, τn, τn+1) Diffuse xτn+1 xτn+1 + γτn+1ϵ, where ϵ (0, I) 4: 5: 6: end for Output: xτM Methodology of γ-Sampling. CTMs γ-sampling introduces unified family of samplers that arises naturally from learning general flow map model. It encompasses prior approaches, such as CMs multistep sampling (see Algorithm 9) and time-stepping-style sampling, which is conceptually similar to ODE solvers. The parameter γ directly controls the degree of semantic change during generation, making γ sampling flexible and task aware strategy for diverse downstream applications. 11.4. General Flow Map: Consistency Trajectory Model 373 Figure 11.5: Illustration of γ-sampling with varying γ value. The procedure alternates Denoise between denoising with network evaluation and adding noise in reverse, (τn p1 γ2τn+1 n=0 . The leftmost panel illustrates γ = 1, corresponding to the fully stochastic case. The rightmost panel shows γ = 0, corresponding to the fully deterministic case. The middle panel depicts intermediate values γ (0, 1), which interpolate between these two extremes. Noisify τn+1)M 1 Figure 11.5-(Left): When γ = 1, it coincides to the multistep sampling introduced in CM (i.e., special flow map Ψs0), which is fully stochastic and results in semantic variation when the number of steps changes. Figure 11.5-(Right): When γ = 0, it reduces to fully deterministic timestepping, which estimates the solution trajectory of the PF-ODE. key distinction between γ sampling with γ = 0 and conventional timestepping ODE-based sampling is that CTM avoids the discretization errors of numerical solvers. Figure 11.5-(Middle): When 0 < γ < 1, γ-sampling interpolates between the two extremes by allowing controlled amount of stochasticity to be injected during sampling. We highlight that the ability to realize samplers with γ (0, 1] is possible only when the model learns the general flow map Ψst. Analysis of γ-Sampling. CTM empirically observed that CMs multistep sampling degrades in quality once the number of steps 4. To explain this phenomenon, CTM analyzed the underlying cause: when γ = 0, each neural jump introduces small mismatch, and these mismatches accumulate as the model iteratively maps states toward time zero. This error accumulation 374 Learning Fast Generators from Scratch explains why long multi-step runs can perform poorly. We formalize this idea in the following proposition. Proposition 11.4.2: (Informal) 2-steps γ-sampling Let τ (0, ) and γ [0, 1]. Let pθ,2 denote as the density obtained from the γ-sampler with the optimal CTM, following the transition sequence p1 γ2τ τ 0, starting from pprior. Then DTV (cid:0)pdata, pθ,2 (cid:1) = q 1 γ2τ + τ ! . Here, DTV denotes the total variation between distributions (see Equation (1.1.4)). Proof for Proposition. We refer the reader to Theorem 8 of Kim et al. (2024a) for the general case when the number of sampling steps is . The insights from the above theorem can be summarized as follows: When γ = 1 (corresponding to CMs multistep sampling): The method performs iterative long-range transitions from τn to 0 at each step n. This leads to error accumulation on the order of (cid:16)pT + τ1 + + τM (cid:17) . When γ = 0 (corresponding to CTMs deterministic multistep sampling): Such temporal overlap between transitions is eliminated. This avoids error accumulation and yields tighter bound of O( ). Empirically, CTM with γ = 0 provides favorable trade-off between sampling speed and sample quality: increasing the number of sampling steps improves generation quality without introducing instability. CTM Supports Diffusion Inference. Since CTM learns the score function (or denoiser) directly through gθ, thanks to its parametrization in Equation (11.4.2), it is compatible with inference techniques originally developed for diffusion models. For instance, one can compute exact likelihoods (Section 4.2.2) or apply advanced samplers such as DDIM or DPM (Chapter 9) for generation, by using gθ(, s, s). 11.5. General Flow Map: Mean Flow 11.5 General Flow Map: Mean Flow Just as diffusion models admit many equivalent parameterizations and training objectives, general flow map Ψst can also be learned in multiple plausible ways. In this section, we introduce Mean Flow (MF) (Geng et al., 2025a), later representative of the general flow map family Ψst that illustrates an alternative yet principled perspective on how such maps can be effectively learned. 11.5.1 Modeling and Training of Mean Flow In contrast to CM and CTM, which build on the EDM framework, MF is based on the flow matching formulation (αt = 1 and σt = for [0, 1]). Rather than directly parameterizing the flow map, MF learns the average drift over an interval [t, s] (with < s): hθ(xs, s, t) h(xs, s, t) := 1 Z v(xu, u) du. The corresponding oracle loss is Et<sExsps w(s)hθ(xs, s, t) h(xs, s, t)2 2 . (11.5.1) In particular, when t, the loss function reduces to the flow matching loss: EtExtpt w(t)hθ(xt, t, t) v(xt, t)2 2 , (11.5.2) learning the instantaneous velocity. We will see later in Section 11.5.3 that MF remains consistent with the general objective in Equation (10.1.4), but approaches it from different (while equivalent) perspective. Since the oracle regression target h(xs, s, t) does not admit closed form in general, MF constructs surrogate by exploiting an identity obtained from differentiating (t s) h(xs, s, t) = s v(xu, u) du with respect to s. This yields ds h(xs, s, t) h(xs, s, t) = v(xs, s) (s t) (xh)(xs, s, t)v(xs, s) + sh(xs, s, t) = v(xs, s) (s t) , where the second line applies the chain rule together with ds xs = v(xs, s). Motivated by this identity, MF replaces the intractable oracle with stop-gradient surrogate, leading to the practical training objective Learning Fast Generators from Scratch LMF(θ) := Et<sExsps w(s) hθ(xs, s, t) htgt θ(xs, s, t)2 , (11.5.3) where the regression target is defined as htgt θ(xs, s, t) := v(xs, s) (s t) (xhθ)(xs, s, t)v(xs, s) + shθ(xs, s, t) {z } JVP . In practice, the oracle velocity cannot be computed in closed form and must instead be approximated. Two common strategies are available: relying on pre-trained diffusion model (distillation) or constructing direct estimator from data (training from scratch). Regardless of the choice, one ultimately needs to compute Jacobianvector product (JVP) of the target network hθ: [xhθ, shθ, thθ] [v, 1, 0] Distillation. Use pre-trained diffusion model with flow matching backbone, vϕ v. Training from scratch. Use the one point conditional velocity α sϵ, obtained from the forward noise injection xs = αsx0 + σsϵ with ϵ (0, I). This gives an unbiased single sample estimate of the instantaneous drift at level when evaluated at paired (x0, ϵ). sx0 + σ 11.5.2 Sampling of Mean Flow Once MF hθ is trained, it naturally recovers proxy of the flow map. For any starting point xs, the map from to is (approximately) given by Ψst(xs) = xs + (t s) h(xs, s, t) xs + (t s) hθ(xs, s, t). This enables both one-step and multi-step sampling. For example, drawing xT pprior, the one-step generation of clean sample is x0 xT + hθ(xT , T, 0). Alternatively, multi-step generation can be performed by preparing time grid and applying the map sequentially, in the same time-stepping manner used in CTM. Since MF learns general flow map, it also supports γ-sampling as in CTM, where controllable hyperparameter γ injects stochasticity into the sampling process. 11.5. General Flow Map: Mean Flow 377 11.5.3 Equivalence of CTM and MF At first sight CTM and MF may appear unrelated. In fact, both are simply different parameterizations of the same oracle flow map Ψst, with their training losses (CTMs consistency loss versus Equation (11.5.1)) differing only in time weighting (Hu et al., 2025). Relationship of Parameterizations. Both methods operate under the same general framework but represent the learned function in distinct ways. The flow map can be written equivalently as Ψst(xs) = xs + v(xu, u) du = xs + s \" xs + Z # v(xu, u) du {z gθ } = xs + (t s) \" 1 s # v(xu, u) du . {z hθ } Here, the first is the definition of the flow map, the second form highlights the CTM parametrization through gθ (see Equations (11.4.1) and (11.4.2)), while the last highlights the MF parametrization through hθ. Relationship of Training Loss. Given the above reinterpretation of the oracle flow map Ψst in terms of the CTM parametrization gθ(xs, s, t) g(xs, s, t) := xs + t s v(xu, u) du and the MF parametrization hθ(xs, s, t) h(xs, s, t) := 1 Z v(xu, u) du, we now show that the training losses of CTM and MF are in fact equivalent. Consider the relation gθ(xs, s, t) := xs shθ(xs, s, t), and take d(x, y) := xy2 as an example. Substituting into Equation (10.1.4) and viewing Gθ as CTMs flow-map parameterization (Equation (11.4.2)) 378 gives Learning Fast Generators from Scratch d(cid:0)Gθ(xs, s, t), Ψst(xs)(cid:1) = Gθ(xs, s, t) Ψst(xs)2 (cid:19) gθ(xs, s, t) xs + = (cid:13) (cid:18) (cid:13) (cid:13) (cid:13) (cid:18) (cid:18) (cid:18) = = = (cid:18) xs + xs + s 2 (cid:19)(cid:13) (cid:13) (cid:13) (cid:13) v(xu, u) du 2 i(cid:19)(cid:13) (cid:13) (cid:13) (cid:13) (11.5.4) (cid:18) xs + (cid:19)2 (cid:13) (cid:13) gθ(xs, s, t) (cid:13) (cid:13) (cid:19)2 (cid:13) (cid:13) (xs shθ(xs, s, t)) (cid:13) (cid:13) (cid:19)2 (cid:13) (cid:13) (xs shθ(xs, s, t)) (cid:13) (cid:13) s (cid:18) xs + (cid:18) xs + v(xu, u) du Z s v(xu, u) du v(xu, u) du 2 (cid:19)(cid:13) (cid:13) (cid:13) (cid:13) 2 (cid:19)(cid:13) (cid:13) (cid:13) (cid:13) (11.5.5) = (s t)2 (cid:13) (cid:13) hθ(xs, s, t) (cid:13) (cid:13) (cid:18) 1 s v(xu, u) du Hence, 2 (cid:19)(cid:13) (cid:13) (cid:13) (cid:13) 1 s2 (cid:13) (cid:13) gθ(xs, s, t) g(xs, s, t) (cid:13) (cid:13) (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) = (cid:13) (cid:13) hθ(xs, s, t) h(xs, s, t) (cid:13) (cid:13) (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) . Thus CTM and MF losses are fundamentally equivalent up to weighting function. Moreover, setting = 0 in either case recovers the CM setting (Ψs0), where each state maps directly to the clean data. Auxiliary Loss in Practice. In CTM, training is performed with the consistency loss in Equation (11.4.7) jointly with its self-defined diffusion model loss in Equation (11.4.8). similar strategy is adopted in MF. As shown in Equation (11.5.2), when t, the MF loss reduces to the standard flow matching objective. In practice, MF controls the ratio between pairs with = and those with = t; consequently, the overall optimization becomes mixture of the MF objective in Equation (11.5.3) and the flow matching objective in Equation (11.5.2). Both parametrizations are able to provide smooth transition from diffusion-model training, which learns instantaneous velocity with fixed regression target, to flow-map learning, which employs stop-gradient pseudoregression target. Both CTM and MF Parameterizations Enable Flexible Inference. Both CTM (Gθ(xs, s, t)) and MF (hθ(xs, s, t)) aim to approximate the underlying 11.5. General Flow Map: Mean Flow 379 flow map Ψst: Gθ(xs, s, t) Ψst, and xs + (t s)hθ(xs, s, t) Ψst. Since both models learn an explicit mapping between any two time steps, they naturally support CTMs γ-sampling and remain compatible with inference techniques originally developed for diffusion models, such as guidance (Chapter 8), exact likelihood computation (Equation (4.2.7)), and accelerated sampling with higher-order solvers (Chapter 9). This compatibility arises because their parameterizations recover the instantaneous diffusion drift in the infinitesimal limit s: g(xs, s, s) = xs v(xs, s), and h(xs, s, s) = v(xs, s). This property is not shared by specialized flow map formulations Ψs0, such as those in the CM family. Thus, both CTM and MF can be regarded as flexible and general flow map formulations that generalize diffusion-based inference to direct time-to-time mappings. Conclusion. This equivalence between CTM and MF is similar to the situation in diffusion models (Section 6.3), where different parameterizations ultimately describe the same underlying oracle target. In principle, these formulations are mathematically identical. In practice, however, their behavior can differ because of factors such as loss weighting, network design, or optimization dynamics, which may cause one approach to perform better than another under specific conditions. This perspective suggests that CTM and MF are not the only possibility: other parametrizations of the flow map may also enable efficient and stable training, opening the door to new standalone generative models. Exploring these alternatives could further enrich the landscape of diffusion models and their flow map extensions, ultimately pushing the boundaries of what few-step generation can achieve. Learning Fast Generators from Scratch 11.6 Closing Remarks This final chapter has brought our exploration full circle, culminating in new paradigm for generative modeling: learning fast, few-step generators from scratch. Moving beyond the approaches of improving numerical solvers or distilling pre-trained models, we have focused on designing standalone training principles that are both principled and highly efficient by design. The core innovation presented here is the direct learning of the flow map (Ψst) of the underlying probability flow ODE. The key to making this tractable without teacher was leveraging the fundamental semi-group property of the ODE flow. This property, which dictates that long trajectory can be decomposed into shorter segments, provides powerful self-supervisory signal for training. We began with Consistency Models (CMs), which pioneered this approach by learning the special flow map that transports any noisy state back to its clean origin (Ψs0). We then saw this idea generalized by Consistency Trajectory Models (CTM) and Mean Flow (MF), which learn the complete, anytime-to-anytime flow map Ψst for all s, satisfying t. While appearing different in their parameterization, we showed that these methods are fundamentally equivalent ways of approximating the same path integral that defines the flow map. These flow map models represent powerful synthesis of the principles developed throughout this monograph. They inherit the continuous-time foundation of the Score SDE framework and the deterministic transport view of Flow Matching, but reformulate the training objective to be self-contained and efficient. By learning the solution map directly, these standalone models successfully unite the high sample quality of iterative diffusion processes with the inference speed of one-step generators. They resolve the fundamental trade-off between fidelity and speed, marking significant milestone in generative modeling. This achievement represents not an end, but the beginning of new chapter in the design of powerful, efficient, and controllable generative AI. The important thing is not to stop questioning. Curiosity has its own reason for existence. Albert Einstein"
        },
        {
            "title": "Appendices",
            "content": "A"
        },
        {
            "title": "Crash Course on Differential Equations",
            "content": "Differential equations (DEs) are fundamental tools for modeling dynamic systems and can be broadly categorized into ordinary differential equations (ODEs), stochastic differential equations (SDEs), and partial differential equations (PDEs). ODEs describe how systems state changes over time according to precise rule, so that knowing the starting point determines the future path exactly. SDEs add randomness to this evolution, modeling how noise or uncertainty influences the systems behavior, making the outcome probabilistic rather than fixed. PDEs explain how functions depending on several variables, such as time and space, evolve together, capturing phenomena like heat spreading, waves moving, or the time evolution of probability densities in stochastic systems (Spoiler: Fokker-Planck equation). These types of differential equations form fundamental language for understanding how systems evolve over time and space under both deterministic and random influences. In this chapter, we provide essential prerequisites on differential equations. 382 A.1. Foundation of Ordinary Differential Equations A.1 Foundation of Ordinary Differential Equations This section introduces the fundamental theory of ODEs, emphasizing the uniqueness of solutions given an initial condition. It also covers practical methods for solving ODEs using numerical solvers. A.1.1 Intuition of Ordinary Differential Equation The deterministic process is called an ordinary differential equation (ODE). In the multivariate case, we consider systems of the form: dx(t) dt = v(x(t), t), (A.1.1) where x(t) RD is vector-valued function representing the state of the system at time t, and : RD RD is vector field specifying the direction and magnitude of change at each point in space and time. Figure A.1: ODE illustration. velocity field v(x, t) assigns drift vector at every point. solution trajectory x(t) is path whose tangent always matches the local drift. The left panel shows step-by-step solver updates (dots and arrows) approximating the path, while the right panel shows the exact trajectories (black) flowing consistently with the velocity field. Without specifying an initial state x(0), there are infinitely many trajectories whose instantaneous changes match the same velocity field. Once x(0) is fixed, however, the ODE determines unique path x(t) that flows according to the drift. High-Level Intuition for Solving ODEs. To build intuition, imagine the vector field v(x, t) as dynamic landscape of arrows that tells you how point should move at any given time t. Solving the differential equation means tracing out curve x(t) through this field such that the tangent (i.e., the instantaneous velocity) of the curve at any point aligns with the vector given by v(x(t), t). Crash Course on Differential Equations Vector Field Perspective: The function v(x, t) defines how things should move: it gives the local instructions for motion or change. Trajectory Perspective: The solution x(t) is path that particle would follow if it obeys the rule set by the vector field at every instant. Thus, solving an ODE is like placing particle in flow field and observing where it goes over time. A.1.2 Existence and Uniqueness of Ordinary Differential Equations So far, we have seen that solving an ODE means finding path that follows the directions given by the vector field at every point. Intuitively, this is like tracing the trajectory of particle as it moves along the flow defined by the velocities. But this picture leads to an important question: Question A.1.1 If we pick starting point, can we be sure there really is path that follows these directions? And if there is, is that path unique, or could the particle suddenly jump onto different trajectory? Answering these questions is essential because it tells us whether the systems behavior can be reliably predicted from its starting position. The Existence and Uniqueness Theorem provides conditions on the vector field that guarantee exactly one path starting from any given initial point. This ensures the solution behaves consistently and forms cornerstone of the theory of ODEs. Local (in Time) Existence and Uniqueness Theorem. Below, we state local version of the theorem, which asserts existence and uniqueness of solution in neighborhood of the initial time for given initial condition. A.1. Foundation of Ordinary Differential Equations 385 Theorem A.1.1: Local Existence and Uniqueness Let v(x, t) be continuous function with respect to and in domain RD R. If satisfies the Lipschitz condition with respect to x: v(x1, t) v(x2, t) Lx1 x2 (x1, t), (x2, t) D, where > 0 is constant, then for every initial condition x(t0) = x0, there exists unique solution x(t) to Equation (A.1.1) defined on some interval [t0 δ, t0 + δ]. Proof for Theorem. (Proof Outline) The Existence and Uniqueness Theorem can be demonstrated constructively using the Picard-Lindelöf iteration method. The method generates sequence of functions {xn(t)} that converges to the solution x(t). The iteration is defined as: xn+1(t) = x0 + t0 v(xn(s), s) ds. Start with an initial guess x0(t) = x0. Iteratively refine xn(t) using the integral form. Convergence is guaranteed under the Lipschitz condition by applying Contraction Mapping Theorem. The essence of the proof is rooted in the PicardLindelöf iteration method, whose core idea is also leveraged in Section 9.8 to accelerate the sampling process of diffusion models. Global (in Time) Existence and Uniqueness Theorem. While the Local Existence and Uniqueness Theorem guarantees the existence of solutions on small time interval, the global (in time) existence and uniqueness theorem extends this result to the entire interval [t0, ] under additional regularity conditions. well-known result in this category is the Carathéodory theorem, which ensures the global existence and uniqueness of solutions to ODEs under two key assumptions: local Lipschitz continuity in the state variable and linear growth bound. 386 Crash Course on Differential Equations (i) Local Lipschitz condition in x: There exists function Lip(t), integrable on [0, ], such that for all x1, x2 RD, v(x1, t) v(x2, t) Lip(t)x1 x2. (ii) Linear growth condition: There exists function (t), integrable on [0, ], such that for all RD, v(x, t) (t)(1 + x). We refer the reader to (Reid, 1971) for comprehensive discussion of the assumptions, formal statement, and detailed proof of the theorem. Remark. To apply these theorems to the probability flow ODE in diffusion models (see Equation (4.1.7)), it may be necessary to impose additional assumptions, such as conditions (i) and (ii), on the score function log pt(x). These assumptions can be reasonably accepted without further justification by readers not focused on technical details. In summary, when an initial condition is given to an ODE defined by time-dependent velocity field, the trajectory of the particle flow is uniquely determined. Uniqueness Implies Non-Intersection of Solutions The uniqueness of solutions in ODEs, as guaranteed by the Local Existence and Uniqueness Theorem, implies fundamental property: two different solution trajectories, starting from different initial conditions, cannot cross each other. This reflects the deterministic nature of ODEs, ensuring that each state evolves along unique path. The following corollary formalizes this result. Corollary A.1.1: Non-Intersection of Solutions Consider two solutions x1(t) and x2(t) to the ODE dx(t) dt = (x(t), t) , [0, ]. Suppose they have distinct initial values x1(0) = x2(0). Then, these solutions do not intersect on [0, ], i.e., x1(t) = x2(t) for all [0, ]. A.1. Foundation of Ordinary Differential Equations Proof for Corollary. Assume, for the sake of contradiction, that there exists some (0, ] such that x1(t) = x2(t). Define the first time at which the two solutions meet as t0 := inf{t [0, ]x1(t) = x2(t)}. Since x1(0) = x2(0) and is contained in this set, it follows that t0 > 0. By continuity of x1 and x2, we have x1(t0) = x2(t0). Consider the initial value problem dx(t) dt = v(x(t), t), x(t0) = x1(t0). By the uniqueness theorem for ODEs, both x1 and x2 must coincide on the interval [t0, ]. Applying uniqueness backward in time similarly implies that the two solutions coincide on [0, t0]. Therefore, the solutions satisfy x1(t) = x2(t) for all [0, ], which contradicts the assumption that x1(0) = x2(0). Hence, we conclude that x1(t) = x2(t) for all [0, ]. By guaranteeing non-intersecting solution paths, this theorem offers hidden yet crucial support for the flow map model (see Chapters 10 and 11). A.1.3 Exponential Integration Factor Even ODE determined by general time-varying velocity does not admit closed-form solution, in some special case, we can solve them analytically or reducing its formulation to better structural one. An Illustrative Example. Consider the following linear scalar ODE: dx(t) dt = L(t)x(t), 388 Crash Course on Differential Equations where L(t) is continuous function. This equation is solvable in closed form, and its solution is well known (for any and t): x(t) = x(s) exp L(τ ) dτ (cid:19) . (cid:18)Z This formula demonstrates how the solution evolves according to an exponential factor that accumulates the effect of the time-dependent coefficient L(t). This motivates the use of exponential integration factors: t) := exp E(s (cid:1) L(τ ) dτ (cid:19) , (cid:18)Z (A.1.2) especially in more general settings where the dynamics include both linear and nonlinear components. Semilinear ODEs and Exponential Integration Factors. We now consider broader class of ODEs known as semilinear ODEs. These equations separate the dynamics into linear part (in the state variable) and nonlinear remainder: dx(t) dt = L(t)x(t) + N(x(t), t), (A.1.3) where x(t) RD is the state vector, L(t) is scalar-valued continuous function, and : RD [0, ] RD is nonlinear vector field. This semilinear structure arises naturally in many physical and engineering systems. In particular, it also appears in the probability flow ODE formulation of diffusion models (see Equation (4.1.7)). Recognizing this structure enables the use of exponential integration factors, which not only simplify analysis but also improve numerical stability. Specifically, this technique plays central role in the design of fast diffusion ODE solvers (see Chapter 9). Step 1: Isolate the Non-Linear Term via an Integration Factor. Observing that we can isolate the nonlinear part by subtracting the linear drift from the semiliner ODE in Equation (A.1.3): dx(t) dt L(t)x(t) = N(x(t), t). To absorb the linear term, we multiply both sides by the inverse integration factor: 1(s (cid:1) t) = exp (cid:18) (cid:19) L(τ ) dτ . A.1. Foundation of Ordinary Differential Equations 389 Now apply the product rule to the left-hand side: 1(s t) (cid:18) dx(t) dt (cid:1) (cid:19) L(t)x(t) = dt 1(s (cid:1) t)x(t) . Hence, the equation becomes: dt 1(s (cid:1) t)x(t) = 1(s t)N(x(t), t). (cid:1) This transformation simplifies the original equation by isolating the nonlinear component, allowing us to focus entirely on the nonlinear dynamics in transformed coordinate system. Step 2: Integrate Over Time. We now integrate both sides from to t: d dτ 1(s (cid:1) τ )x(τ ) dτ = E 1(s (cid:1) τ )N(x(τ ), τ ) dτ. The left-hand side is simply the difference of the transformed variable evaluated at and s: Hence, we obtain: 1(s (cid:1) t)x(t) x(s). 1(s (cid:1) t)x(t) = x(s) + s 1(s (cid:1) τ )N(x(τ ), τ ) dτ. Step 3: Solve for x(t). Multiplying both sides by the exponential flow t) gives the solution: E(s (cid:1) x(t) = E(s t)x(s) } (cid:1) {z linear part + E(τ t)N(x(τ ), τ ) dτ . (A.1.4) (cid:1) {z nonlinear part } The solution naturally separates into linear and nonlinear component. Exponential integrators exploit this structure by solving the linear part in exactly closed form and discretizing only the nonlinear residual. This ensures that the step size is dictated by the nonlinear dynamics rather than by the potentially large linear coefficient, yielding updates that are both stable and accurate even with fewer steps (see the comparison between the exponential Euler update Equation (9.1.7) and the vanilla Euler update Equation (9.1.8)). Crash Course on Differential Equations A.1.4 Numerical Solvers of Ordinary Differential Equations We consider the ODE in Equation (A.1.1) with an initial condition x(0). Solving this ODE involves finding continuous trajectory x(t) that satisfies the equation for all [0, ]. Ideally, closed-form solution is desirable, though it is rarely attainable in practice. useful perspective is to rewrite the ODE in its integral form: x(t) = x(0) + 0 v(x(τ ), τ ) dτ, (A.1.5) which expresses the solution as the initial state plus the accumulated effect of the velocity over time. However, the integral is often intractable due to the nonlinear and time-dependent nature of v, making closed-form solutions unavailable. In such cases, we turn to numerical methods, which discretize time and iteratively approximate x(t). Common approaches include Eulers method, RungeKutta methods, and specialized integrators for stiff systems. These methods simulate the system step by step, providing practical approximations of the true trajectory. Remark. When takes the semilinear form in Equation (A.1.3), the solution admits an integral representation involving an exponential integration factor (Equation (A.1.4)), which separates the linear and nonlinear components. This structure enables efficient numerical solvers that focus solely on approximating the nonlinear term, reducing computational complexity and motivating tailored algorithms (see Chapter 9). Key Concepts. Numerical solvers approximate the continuous dynamics of ODEs by discretizing time and estimating the state using the slope of the ODE. This involves: Discretization: Partition the time domain into discrete steps t0, t1, . . . , tn. Step Size: The interval ti = ti+1 ti is called the step size. Approximation: The solution at each step is estimated numerically; the accuracy depends on the step size and the method used. Error Control: Errors from discretization and approximation are monitored and controlled. A.1. Foundation of Ordinary Differential Equations 391 High-Level Categorization of Numerical Solvers. ODE solvers can be broadly categorized as: Time-Stepping Methods: These methods advance the solution step by step, e.g., explicit/implicit Euler, Runge-Kutta. Time-Parallel Methods: These methods leverage parallelism to compute solutions over different time intervals simultaneously, useful for largescale problems. Common Numerical Solvers. Among these, Euler, Heun, and RungeKutta are single-step methods, since each update uses only the current state (tn, xn). In contrast, multi-step methods (such as AdamsBashforth or AdamsMoulton) compute xn+1 using not only the current state xn but also several previous values xn1, xn2, . . . . They save work by reusing past information (history anchors) instead of re-evaluating everything within the current step. Such methods are not covered here, though related schemes (e.g., AdamsBashforth, discussed in Sections 9.3 and 9.5) also exploit multiple past states. Picard iteration, on the other hand, is of different nature: it serves as theoretical fixed-point construction, whose idea will be revisited in Section 9.8. Eulers Method. Eulers method is the simplest time-stepping scheme: xn+1 = xn + hv(xn, tn), where is the step size. It has first-order accuracy: local error O(h2), global error O(h). While easy to implement, it requires small for stability and accuracy. Heuns Method (Improved Euler). Heuns method is second-order predictor-corrector scheme: Predict: xpred = xn + hv(xn, tn), Correct: xn+1 = xn + (cid:0)v(xn, tn) + v(xpred, tn + h)(cid:1). It achieves local error O(h3) and global error O(h2). Karras et al. (2022) advocate Heuns method for solving ODEs in diffusion models, though higherorder methods such as DPM-Solvers (see Sections 9.4 and 9.5) typically yield better performance. 392 Crash Course on Differential Equations Runge-Kutta Methods. Runge-Kutta (RK) methods generalize Euler by using weighted averages of intermediate slopes. The fourth-order method (RK4) is standard choice: k1 = v(xn, tn), 2 k1, tn + k2 = v(xn + 2 ), 2 k2, tn + k3 = v(xn + 2 ), k4 = v(xn + hk3, tn + h), xn+1 = xn + 6 (k1 + 2k2 + 2k3 + k4). RK4 balances accuracy and cost, making it widely used. DPM-Solver builds on similar ideas to achieve higher-order accurate integration tailored to diffusion models, leveraging their semilinear structure (see (Lu et al., 2022b)s Appendix B.6 for comparison). Picard Iteration. Picard iteration refines successive approximations to the solution via: x(k+1)(t) = x(0) + 0 v(cid:0)x(k)(s), s(cid:1) ds, starting from an initial guess function x(0)(t) with x(0)(0) = x(0). While theoretically foundational, Picard iteration often converges slowly due to its strong dependence on the initial guess. Moreover, each iteration involves computing an integral over time, which can be computationally expensive. Solving ODEs in Forward and Reverse Time. So far, we have considered solving the ODE in Equation (A.1.1) forward in time, evolving the solution from an initial condition x(0) to later times > 0. In contrast, reverse-time integration computes the solution by stepping backward from terminal condition x(T ) toward earlier times < . Reparameterizing time as transforms the ODE into: dx(t) dt = v(cid:0)x(t), t(cid:1), x(0) = x(T ). Reverse-time integration applies the same methods as forward-time integration, but on decreasing time grid. With Euler and step size > 0, starting from t0 = with x0 = x(T ), the updates are tn+1 = tn h, xn+1 = xn v(xn, tn). A.1. Foundation of Ordinary Differential Equations 393 Care must be taken to ensure numerical stability, especially for stiff problems (i.e., when some components of the state vector evolve much faster than others, requiring very small time steps for stable integration), as commonly encountered in PF-ODE sampling for diffusion models. While time reversal for ODEs is theoretically straightforward, as it only requires reparameterization of time due to the bijective mapping between x(0) and x(T ), this does not hold for SDEs. Their intrinsic randomness precludes direct time reversal, point we elaborate on in the next section. 394 Crash Course on Differential Equations A.2 Foundation of Stochastic Differential Equations Stochastic Differential Equations (SDEs) are an extension of ordinary differential equations (ODEs) that incorporate randomness, providing mathematical framework for modeling systems affected by uncertainty. This chapter introduces SDEs, beginning with the discretization of ODEs, extending to the discretization of SDEs, and culminating in discussion of general SDEs, including Itos calculus and Itos formula. A.2.1 From ODEs to SDEs: An Intuitive Introduction Let us begin with ODE describing the deterministic evolution of state variable x(t) RD: dx(t) dt = (x(t), t), x(0) = x0. (A.2.1) Here, : RD [0, ] RD is time-dependent velocity field that governs the dynamics of x(t). The solution to this ODE is smooth trajectory 7 x(t), fully determined by the initial condition x0. Discretization Perspective. To build intuition, consider an Euler discretization of Equation Equation (A.2.1) over small time steps t: xt+t = xt + (xt, t)t. This approximation becomes more accurate as 0, converging (under standard regularity conditions on ) to the exact solution of the ODE. Introducing Randomness: From ODE to SDE. In many real-world systems, perfect knowledge of the dynamics is unrealistic. Noise, uncertainty, or unmodeled interactions may affect the evolution. To incorporate such randomness, we augment the ODE with stochastic term: xt+t = xt + (xt, t)t + g(t) ϵt, (A.2.2) where : [0, ] is diffusion coefficient (possibly dependent on both state and time, though here assumed time-dependent only), ϵt (0, ID) are i.i.d. standard Gaussian vectors. A.2. Foundation of Stochastic Differential Equations 395 This modified update rule reflects not just deterministic drift, but also t. The scaling ensures that the stochastic random perturbations scaled by perturbation remains finite in the limit 0. Importantly, this formulation gives rise to continuous-time stochastic process as 0, which leads us to the framework of SDE. Stochastic Differential Equations. Formally, the limit of the discrete update Equation (A.2.2) as 0 defines the SDE: dx(t) = (x(t), t) dt + g(t) dw(t). (A.2.3) Here, w(t) RD is Wiener process (standard Brownian motion), continuoustime stochastic process characterized by: Initial State: w(0) = 0 almost surely; Independent Increments: for 0 < t, the increment w(t) w(s) is independent of the past; Gaussian Increments: w(t) w(s) (cid:0)0, (t s)ID (cid:1) (A.2.4) Continuity: Sample paths 7 w(t) are almost surely continuous but nowhere differentiable. In addition, the notation dw(t) := w(t + dt) w(t) is often used to denote the infinitesimal increment of the Wiener process. While suggestive, this notation is heuristic and should not be interpreted as classical differential (e.g., in the Riemann or Lebesgue sense), since Brownian paths are almost surely nowhere differentiable. Instead, it serves as formal shorthand to express the Gaussian increments property: dw(t) (0, dt ID), meaning that over an infinitesimal time interval of length dt, the increment of the Wiener process behaves like Gaussian random variable with zero mean and covariance dt ID. 396 Crash Course on Differential Equations A.2.2 Further Explanation of Equation (A.2.3) The SDE in Equation (A.2.3) should be understood in its integral form: x(t) = x(0) + 0 (x(s), s) ds + 0 g(s) dw(s), (A.2.5) interpreted in the Itô sense. Here, the first term is classical (Riemann or Lebesgue) integral representing the accumulated deterministic drift, while the second term is an Itô stochastic integral, which integrates with respect to the Wiener process w(t). We do not provide full rigorous construction of the Itô integral, but offer the following intuition. Intuition for Itô Integration. The Itô integral can be viewed as the limit (in probability) of discrete sums: g(ti)(cid:0)w(ti+1) w(ti)(cid:1), where the integrand g(t) is evaluated at the left endpoint ti of each subinterval. This left-point evaluation is crucial and distinguishes Itô integration from classical integrals, which often use midpoints or other evaluation rules. Because Brownian paths are continuous yet almost surely nowhere differentiable, classical integration fails to apply. The Itô integral handles this irregularity, capturing the cumulative effect of stochastic fluctuations over time. Use of Differential Notation. Expressions such as dx(t), dt, and dw(t) are not classical differentials. Instead, they are formal notations representing infinitesimal increments of the respective processes. While heuristic, they are widely used for their convenience in expressing SDEs analogously to ODEs and facilitate formal manipulations within Itô calculus. How Itô calculus is applied in diffusion models will be explained in Chapter C. Comparison with ODEs. In ODEs, e.g., the integral form dx(t) dt = (x(t), t), x(t) = x(0) + f (x(τ ), τ ) dτ A.2. Foundation of Stochastic Differential Equations 397 is justified by the Fundamental Theorem of Calculus, which ensures that differentiable functions can be recovered from their derivatives. By contrast, in SDEs such as Equation (A.2.3), there is no direct analog of this theorem because Brownian motion lacks differentiability, and stochastic integrals do not follow the classical chain rule. Instead, Itô calculus introduces alternative tools (e.g., Itôs lemma) to analyze and manipulate stochastic dynamics. Thus, while the differential notation for SDEs is compact and intuitive, rigorous understanding depends on interpreting them via their integral formulation using Itô integrals. A.2.3 Numerical Solver for SDE. Like ODEs, the SDE in Equation (A.2.3) admits unique solution1 if (, t) and g() satisfy some smoothness conditions: (, t) is Lipschitz and of linear growth in x, and g() is square integrable. For general SDEs as in Equation (A.2.3), closed-form solutions are generally unavailable, so numerical methods are necessary. common approach is the EulerMaruyama method, which generalizes Eulers method for ODEs and, indeed, we have already seen it in Equation (A.2.2). It approximates the drift term (x(t), t) over time step and simulates the stochastic noise g(t) dw(t) using Gaussian increments ϵt with ϵt (0, I). Later, in Section C.1.5, we will see that linear SDE admits closed-form solution. 1The solution is in the strong sense, meaning that x(t) satisfies the SDE in its integral form (see Equation (A.2.5)) with respect to the given Brownian motion w(t) on fixed probability space. We omit the detailed technical definitions here. Density Evolution: From Change of Variable to FokkerPlanck Understanding how probability densities evolve under transformations is fundamental in both probability theory and generative modeling. In particular, diffusion models aim to construct generative processes whose induced density paths reverse pre-defined forward process. This evolution is governed by the continuity equation or, in the stochastic case, the FokkerPlanck equation. Although these names may sound unfamiliar or intimidating, they are in fact continuous-time analogues of the change-of-variable formula from basic calculus. In Section B.1, it builds up to them by presenting progression of change-of-variable formulas, starting from deterministic bijections, and culminating in stochastic differential equations. This progression naturally bridges discrete mappings and continuous-time flow dynamics. See Figure B.1 for an overview of this unified framework. In Section B.2, we provide physical and intuitive interpretation of the continuity equation, emphasizing its connection to the conservation of density in dynamical systems. 398 B.1. Change-of-Variable Formula: From Deterministic Maps to Stochastic Flows 399 Transform Density Φ x1 p0(x0) = p1(x1) (cid:12) (cid:12)det Φ(x0) (cid:12) x0 (cid:12) (cid:12) (cid:12) Multiple Bijections x0 Φ1 x1 Φ2 ΦL xL log p0(x0) = log pL(xL)+ (cid:12) (cid:12) (cid:12)det Φk+1(x) PL1 (cid:12) (cid:12) (cid:12) k=0 log xk Continuous-Time Limit dx(t) dt = (x(t), t), defining the flow map Φ0t tpt(x) = (f (x, t)pt(x)) With Gaussian Noise dx(t) = (x(t), t) dt+g(t) dw(t) tpt(x) = (f (x, t)pt(x)) 2 g2(t)pt(x) + 1 Figure B.1: unified change-of-variables formula. From top to bottom: (1) single bijection and the ; (2) composition of multiple bijections; (3) continuous-time deterministic flow governed by an ODE and the associated continuity equation; (4) stochastic flow modeled by an SDE and the corresponding FokkerPlanck equation. B.1 Change-of-Variable Formula: From Deterministic Maps to Stochastic Flows In this section, we aim to demystify the continuity equation and the Fokker Planck equation by drawing analogies to the classic change-of-variable formula from calculus. We begin with the familiar single-variable case, extend it to the multivariate setting and to probability densities (Section B.1.1), then 400 Density Evolution: From Change of Variable to FokkerPlanck generalize to compositions of bijective maps whose continuous-time limit leads to the continuity equation (Section B.1.2). Finally, we incorporate stochasticity by introducing random noise, which naturally extends the continuity equation to the FokkerPlanck equation (Section B.1.3). B.1.1 Change-of-Variable Formula for Deterministic Maps We move particles according to deterministic map and study how their law (density) evolves. The key principle is conservation of probability mass, grounded in fundamental result from calculus and probability: the change-ofvariable formula. This formula describes how integrals, and therefore probability densities, transform under smooth bijective mappings. To build intuition, we first consider single update step, and then extend the discussion to sequential transformations. Single Update. Think of single update rule induced by applying vector field (analogous to force) Ψ : RD RD for one unit of time. Starting from an initial particle state x0, its next state is given by x1 = Ψ(x0). Underlying Pattern (a Density) and How it Moves. If the initial states follow an underlying law/pattern described by density p0 (i.e., x0 p0), then applying Ψ produces new density p1 for x1 (i.e., x1 p1). Assuming Ψ is smooth bijection, p1 is obtained from p0 via the standard change-ofvariables formula: p1(x1) = p0(Ψ1(x1)) (cid:12) (cid:12) (cid:12) det (cid:12) (cid:12) Ψ1 x1 !(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) . (B.1.1) Here Ψ original coordinates, is the Jacobian matrix of Ψ, denoted xΨ. Equivalently, in the (cid:12) det xΨ(x0)(cid:12) (cid:12). p0(x0) = p1 (cid:12) det xΨ(cid:12) In words, Ψ reshapes the density p0 into p1. The factor (cid:12) (cid:12) represents the local change in volume; since probability mass is conserved, the density compensates by its inverse. (cid:0)Ψ(x0)(cid:1) (cid:12) As simple case, if Ψ is linear with an invertible matrix (i.e., x1 = Ax0), then p1(x1) = p0(A1x1) (cid:12) (cid:12) det A1(cid:12) (cid:12). B.1. Change-of-Variable Formula: From Deterministic Maps to Stochastic Flows Schematically, we can read it as: 401 Sample: Density: px0(x0) x0 Ψ Ψ px1(x1) x1 Why is Equation (B.1.1) the Change-of-Variables Formula? This comes directly from the familiar rule in calculus. Single-Variable Case. Let = Ψ(x) be smooth and invertible. Rewriting an integral over in terms of gives g(y) dy = g(Ψ(x)) Ψ(x) dx, where Ψ(x) compensates for interval stretching or compression, ensuring area preservation. Multivariate Case. For Ψ : RD RD with = Ψ(x), g(y) dy = g(Ψ(x)) (cid:12) (cid:12) det(xΨ)(cid:12) (cid:12) dx, so infinitesimal volumes transform as dy = (cid:12) (cid:12) det(xΨ)(cid:12) (cid:12) dx. From this, the density formula in Equation (B.1.1) follows: py(y) = RD δ(y Ψ(x)) px(x) dx = px (cid:0)Ψ1(y)(cid:1) (cid:12) (cid:12) (cid:12) det Ψ1 ! (cid:12) (cid:12) (cid:12). Composing Multiple Bijections. We now apply several updates in sequence. Let xk = Ψk(xk1) for = 1, . . . , L; that is, Ψ2 where each Ψk : RD RD is smooth bijection. If the initial state follows density p0 (i.e., x0 p0), then the sequence of updates induces densities p1, . . . , pL for x1, . . . , xL. ΨL xL, Ψ1 x0 Because probability mass is conserved at each step, the densities evolve according to pk(xk) = pk1(xk1) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) det xk1Ψk(xk1) (cid:12) , = 1, . . . , L. By recursion, the final density at xL is 402 Density Evolution: From Change of Variable to FokkerPlanck pxL(xL) = px0(x0) k=1 (cid:12) (cid:12) (cid:12) (cid:12) det (cid:18) Ψk xk1 (cid:19)(cid:12) (cid:12) (cid:12) (cid:12) 1 . (B.1.2) Equivalently, in log-density form: log pxL(xL) = log px0(x0) k=1 log (cid:12) (cid:12) (cid:12) (cid:12) det (cid:18) Ψk xk1 (cid:19)(cid:12) (cid:12) (cid:12) (cid:12) . This expression reflects how each transformation Ψk stretches or contracts volume, as captured by the Jacobian determinant. The accumulation of these local volume changes along the transformation path determines the final probability density under the composed map. Equation (B.1.2) serves as the core principle underlying Normalizing Flows (see Section 5.1.2). B.1. Change-of-Variable Formula: From Deterministic Maps to Stochastic Flows B.1.2 Continuous-Time Limit: Continuity Equation (a) Vector field illustrations. The arrows represent forces that would drag particles through space, deforming the underlying grid accordingly. (b) Particle-cloud dynamics. predefined vector field (interpreted as force) generates flow that transports particles from their initial state. (c) Density evolution. As particles are advected by the vector field, the density contours deform accordingly, reflecting how the flow reshapes the underlying distribution. Figure B.2: Illustrations of particle and density dynamics under vector field. Each column shows successive time snapshots (left to right). These illustrations are adapted from Lipman et al. (2024) with the authors permission. We now pass from discrete updates to continuous description. Suppose the particle motion is driven by time-varying velocity field : RD [0, ] RD. Imagine evolving particle x0 p0 through infinitely many small bijective updates. At each step of length > 0, the update is xt+t = Ψ(xt) := xt + (xt, t). 404 Density Evolution: From Change of Variable to FokkerPlanck As 0, the composition of these updates converges to continuous flow governed by velocity field : RD [0, ] RD: dx(t) dt = (x(t), t), x(0) = x0 p0. (B.1.3) Under suitable smoothness assumptions (see Chapter A), this ODE admits unique solution for each initial condition, which defines deterministic flow map Ψ0t : RD RD. In other words, Ψ0t brings the initial state x0 to the solution of Equation (B.1.3) at time t: Ψ0t(x0) = x0 + f (x(τ ), τ ) dτ. As result, the whole distribution also moves: the initial density p0 is transported into the new density pt, the law of x(t). Formally, this is written as pushforward: pt = (cid:0)Ψ0t (cid:1) #p0. When Ψ0t is smooth and invertible, this reduces to the familiar change-ofvariables rule: pt(x) = p0 (cid:0)Ψt0(x)(cid:1) (cid:12) (cid:12) det xΨt0(x)(cid:12) (cid:12) = δ(x Ψt0(x0)) p0(x0) dx0. Continuity Equation: How the Density Moves in Time. Rather than writing separate formula for the density at each time, we can describe how it moves continuously using differential equation in space and time t. The idea is simple: probability mass is conserved, and the velocity field only redistributes it in space. This gives the continuity equation: pt(x) + (cid:0)pt(x) (x, t)(cid:1) = 0. (B.1.4) Here the divergence term (ptf ) measures how the flow locally expands or compresses the density, ensuring total probability remains 1. This partial differential equation (PDE) ensures that probability mass is conserved as the flow moves particles. In fact, it can be viewed as the continuous-time analogue of the change-of-variables formula. Derivation of Continuity Equation via Change-of-Variables Formula. Conceptually, the continuity equation can also be obtained by taking the continuoustime limit of Equation (B.1.2). Here, however, we adopt more direct derivation based on Equation (B.1.1). B.1. Change-of-Variable Formula: From Deterministic Maps to Stochastic Flows 405 Discretization and Change-of-Variable Formula. Consider xt+t := Ψ(xt) = xt + (xt, t), which is actually the forward Euler discretization of the ODE in Equation (B.1.3) over small time interval > 0. The Jacobian of the map Ψ with respect to xt expands as Ψ xt = + txf (xt, t) + O(t2), so its determinant satisfies (cid:18) Ψ xt det (cid:19) = 1 + (xt, t) + O(t2). This uses the standard expansion det(I + A) = 1 + Tr(A) + O(t2) as 0, along with = Tr(xf ). Applying the change-of-variables formula, the log-density evolves as log pt+t(xt+t) = log pt(xt) (xt, t) + O(t2). Applying the change-of-variables formula, the log-density evolves as log pt+t(xt+t) = log pt(xt) (xt, t) + O(t2). That is, log pt+t(xt+t) log pt(xt) = (xt, t) + O(t2). (B.1.5) Using Taylor Expansion. Now, we expand the left-hand side via multivariate Taylor expansion: log pt+t(xt+t) log pt(xt) =t log pt(xt) + (xt+t xt)xt log pt(xt) + O(t2). Substituting xt+t xt = (xt, t)t yields: log pt+t(xt+t) log pt(xt) =t log pt(xt) + (xt, t)xt log pt(xt) + O(t2). Matching terms with Equation (B.1.5) and letting 0, we conclude that log pt(xt) = xt (xt, t) (xt, t)xt log pt(xt). Exponentiating and using the product rule yields the continuity equation. 406 Density Evolution: From Change of Variable to FokkerPlanck Velocity First (Lagrangian) vs. Density First (Eulerian). It is important to note key asymmetry between particle dynamics and density dynamics. Starting from velocity field gives unique flow of particles and hence unique density evolution. In contrast, prescribing only the density path does not pin down single velocity field: many different flows can lead to the same sequence of densities. Velocity-First (Eulerian: Flow Density). So far, we have assumed that the velocity field is given. The particle ODE dx(t) dt = (x(t), t) describes how each particle moves, while the density PDE tpt + (cid:0)ptft (cid:1) = 0 describes how the entire distribution of particles evolves. These two views are connected: moving particles according to the ODE automatically produces density that satisfies the PDE. In this case, the particle flow Ψ0t is uniquely determined: starting from x(0) p0, each trajectory x(t) is fixed, and the resulting density pt follows the continuity equation. Here, particle dynamics and density dynamics are fully consistent. Density-First (Eulerian: Density Unique Flow). If instead we begin only with the density path 7 pt (e.g., Section 5.3.2 in flow matching), the velocity field is no longer uniquely determined. For example, if vector field wt satisfies (cid:0)pt(x) wt(x)(cid:1) = 0 (no net flux w.r.t. pt), then both ft and ft + wt give rise to the same density evolution. Thus single density path may correspond to many different flows, and choosing one particular particle flow Ψ0t amounts to picking specific velocity field among these possibilities. Not every given path pt can actually arise from particles moving under some velocity field. The continuity equation (Equation (B.1.4)) provides the consistency check for whether density path can be generated by flow. We say that pt is realizable (or generated by ) if there exists velocity field such that particles following dx(t) dt = (x(t), t) B.1. Change-of-Variable Formula: From Deterministic Maps to Stochastic Flows 407 produce exactly the densities pt through the flow map Ψ0t. That is, realizability holds when pt and together satisfy Equation (B.1.4). Intuitively, realizability means that the snapshots of pt over time can be explained by particles moving under some velocity field, rather than being an arbitrary sequence of distributions. When this condition holds, the density pt is nothing more than the pushforward of the initial density p0 along the flow map Ψ0t. In this case, the familiar change-of-variables formula applies: pt = (cid:0)Ψ0t (cid:1) #p0 (cid:0)Ψt0(x)(cid:1) (cid:12) = p0 = (cid:12) det xΨt0(x)(cid:12) (cid:12) δ(x Ψt0(x0)) p0(x0) dx0. (Optional) Conditioning. introduced, the same reasoning applies for each fixed z: If an additional conditioning variable π(z) is dx(t) dt = vt(x(t)z) with pushforward pt(z) = (Ψ0t(; z))#p0, and continuity equation tpt(xz) + (cid:0)pt(xz) vt(xz)(cid:1) = 0 The marginal density is then pt(x) = pt(xz)π(z) dz. B.1.3 Stochastic Processes: FokkerPlanck Equation When noise is added, the dynamics follow the SDE as in Equation (A.2.3): dx(t) = (x(t), t) dt + g(t) dw(t). Then, the density pt(x) satisfies the FokkerPlanck equation: pt(x) = (f (x, t) pt(x)) + (cid:18)(cid:18) = (x, t) 1 2 1 2 g2(t) pt(x) g2(t)x log pt(x) (cid:19) (cid:19) pt(x) . 408 Density Evolution: From Change of Variable to FokkerPlanck Here, pt = xpt is the Laplacian operator. Here, the first term describes transport of probability mass by the deterministic drift , while the second term models the spreading (diffusion) of the density due to stochastic noise with variance proportional to 1 2 g2(t). The derivation of the FokkerPlanck equation is more involved; we refer it to Section C.1.4. B.2. Intuition of the Continuity Equation 409 B.2 Intuition of the Continuity Equation In this section, we give physical interpretation of the continuity equation, highlighting its role as conservation law for probability density in dynamical system. B.2.1 Physical Interpretation of the Continuity Equation Consider small fixed control volume (a rectangular box) in 3D space centered at = (x, y, z) with side lengths x, y, and z. Let p(x, t) denote the density of conserved quantity (e.g., mass or probability) at position and time t. The total amount of the quantity inside the box is: Total quantity in box = p(x, t)xyz. How Does the Total Change? Changes in the total quantity can only arise from flux across the boxs boundary. Let j(x, t) denote the flux vector, representing the amount of quantity flowing per unit area per unit time. Flux in the x-Direction. The inflow through the left face (at x) is approximately: and the outflow through the right face (at + x) is: jx(x, y, z, t)yz, jx(x + x, y, z, t)yz. Thus, the net flux in the x-direction is: [jx(x, y, z, t) jx(x + x, y, z, t)] yz. Net Flux in All Directions. Analogous terms arise in the yand z-directions: [jy(x, y, z, t) jy(x, + y, z, t)] xz, [jz(x, y, z, t) jz(x, y, + z, t)] xy. Summing all contributions, the total net outflux from the box is: j(x, t)xyz. Rate of Change Inside the Box. The rate of change of the total quantity within the box is: t (x, t)xyz. 410 Density Evolution: From Change of Variable to FokkerPlanck Conservation Principle. Assuming the quantity is conserved (e.g., total mass or probability is constant in time), the rate of change equals the negative of the net outflux: t (x, t)xyz = j(x, t)xyz. Local Form. Canceling the common volume factor (valid for any small box), we obtain the local form of the continuity equation: t + = 0. B.2.2 Derivation of the Continuity Equation from Conservation Laws The continuity equation formalizes the conservation of physical quantity, such as mass or charge, in dynamical system. Let p(x, t) denote the density of the conserved quantity at position RD and time [0, ], and let v(x, t) denote the velocity field. Step 1: Rate of Change within Control Volume. Consider an arbitrary control volume RD with boundary . The total amount of the conserved quantity in is p(x, t) dV, whose time derivative gives the rate of accumulation: V p(x, t) dV. Step 2: Net Flux Through the Boundary. The quantity exits through with outward normal vector n. The net outward flux is V p(x, t)v(x, t) dS. Step 3: Conservation Principle. Conservation implies that the rate of accumulation within equals the negative of the net outward flux: V dV + pv dS = 0. B.2. Intuition of the Continuity Equation 411 Step 4: Divergence Theorem. Applying the divergence theorem to convert the surface integral to volume integral: Hence, pv dS = (pv) dV. V dV + (pv) dV = 0. Step 5: Local Form. Since the control volume is arbitrary, the integrand must vanish pointwise. This yields the continuity equation. Behind the Scenes of Diffusion Models: Itôs Calculus and Girsanovs Theorem (Score-based) Diffusion models are built on SDEs: drift that pushes states and Brownian term that jitters them. Unlike ODE paths, Brownian paths are nowhere differentiable, so the ordinary chain rule fails. In this section, we introduce two fundamental tools that make the math precise: Itôs Formula is the correct chain rule for stochastic trajectories. It tells us how function h(xt, t) evolves when xt follows an SDE. It enables derivations of the FokkerPlanck equation, moment dynamics, the Itô product rule, and the identities used in score-based training. Girsanovs Theorem is change-of-measure result on path probabilities. It quantifies how likelihoods change when the noise is fixed but the drift is altered. This links score matching to path-space KL divergence and explains why learning the score in the reverse SDE corresponds to maximizing the data likelihood. With these tools, the standard diffusion model derivations (FokkerPlanck, reverse time SDE, training objectives, and likelihood relations) follow cleanly and without hand waving. C.1. Itôs Formula: The Chain Rule for Random Processes 413 C.1 Itôs Formula: The Chain Rule for Random Processes Standard calculus does not directly apply to stochastic processes because Wiener processes are not differentiable in the classical sense. Instead, we use Itôs calculus, which provides rules for working with stochastic integrals. C.1.1 Motivation: Why Do We Need Special Chain Rule? Consider deterministic time-varying function yt that evolves smoothly with time (e.g., an ODE). If we have function h(yt, t), the usual chain rule tells us: dh dt = t + yh dyt dt . Here, yh is the Jacobian of h. This works perfectly for deterministic paths yt. Question C.1.1 But what happens if xt is stochastic process, say, driven by an SDE dxt = (xt, t) dt + g(t) dwt as in Equation (A.2.3)? What SDE does the process h(xt, t) satisfy? Why the Ordinary Chain Rule Fails? Naïvely applying the classical chain rule yields t However, this neglects that Brownian increments satisfy dwt = O( dt + yh dxt. dh = dt) and (dwt)2 = dt. Thus, second-order terms in dwt do not vanish in stochastic calculus, unlike classical calculus where (dt)2 terms are negligible. Example: Simple Exampleh(xt) = x2 To see the intuition, let us consider the simple real-valued function h(xt) = where the random variable xt satisfies x2 dxt = σ dwt, with constant σ > 0. If we try the classical chain rule, dh = 2xt dxt = 2xtσ dwt. 414 Behind the Scenes of Diffusion Models: Itôs Calculus and Girsanovs Theorem If this were true, the expectation of h(xt) would be constant in time because E[dh] = 2σE[xt dwt] = 2σE[xt] E[dwt] {z } =0 = 0. But we know from classical Brownian motion properties (see Equation (A.2.4)) that E[x2 ] = σ2t, which grows linearly in time. So the ordinary chain rule misses an important term. C.1.2 Deriving 1D Itôs Formula from Taylor Expansion Deterministic Chain Rule via Taylor Expansion. To understand why the classical chain rule fails for stochastic processes defined by SDEs, we first revisit it in the deterministic setting using Taylor expansion. We consider the scalar case: yt and h(, ) R. Formally treating dyt = yt+dt yt, with dt 0, we expand: h(yt+dt, + dt) h(yt, t) 2h y2 (dyt)2 + t y dyt + dt + 1 2 = 2h ty dt dyt + ! 2h t2 (dt)2 + O(dt3), Here, dt dyt = all the gray parts are ignorable, and the full differential is: (dt)2 = O(dt2), and similarly (dt)2 = O(dt2). Therefore, (cid:17) (cid:16) dyt dt dh = t dt + y dyt + O(dt2). Itôs Formula via Stochastic Taylor Expansion. Now consider stochastic process xt governed by the SDE: dxt = (xt, t) dt + g(t) dwt, where wt is standard Brownian motion. We aim to compute the differential of scalar-valued function h(xt, t). Using the stochastic Taylor expansion (Kloeden et al., 1992), which retains second-order terms in dxt, we have: h(xt+dt, + dt) h(xt, t) 2h x2 (dxt)2 + 2 x t dxt + dt + 1 2 = 2h tx dt dxt + ! 2h t2 (dt) + C.1. Itôs Formula: The Chain Rule for Random Processes 415 Negligible Cross Terms. By the scaling property of Brownian motion (Equation (A.2.4)), dwt = O( dt) dt dwt = O((dt)3/2). Therefore, dt dxt = dt (f dt + dwt) = (dt)2 + dt dwt = O((dt)3/2). So, the gray terms are negligible: O((dt)3/2) or smaller. Second-Order Term (dxt)2. Expanding using the SDE: (dxt)2 = (f dt + dwt)2 = 2(dt)2 + 2f dt dwt + g2(dwt)2 = O((dt)2) + O((dt)3/2) + g2O(dt) = g2(t) dt + O((dt)3/2). Combining terms, we obtain the differential: dh(xt, t) = t dt + x dxt + 1 2 2h x2 g2(t) dt. Substituting dxt = (xt, t) dt + g(t) dwt yields: g2 2h dh(xt, t) = t x + 1 2 + ! dt + x dwt. This is the 1D version of Itôs formula. Example: Simple Exampleh(xt) = x2 We revisit the simple example: h(xt) = x2 xt satisfies , where the stochastic process dxt = σ dwt, with constant σ > 0. Applying Itôs formula correctly to h(xt) = x2 obtain: , we dh(xt) = d(x ) = 2xt dxt + σ2 dt. Substituting dxt = σ dwt, this becomes: d(x2 ) = 2xtσ dwt + σ2 dt. Behind the Scenes of Diffusion Models: Itôs Calculus and Girsanovs Theorem C.1.3 Itôs Formula: The Chain Rule for SDEs We summarize the one-dimensional Itôs formula derived above. Using similar arguments, the result extends naturally to the multi-dimensional setting. While we omit the detailed derivation, we state the general formula for completeness. Finally, we illustrate an application of Itôs formula by deriving the Itô yt) for stochastic processes product rule, which enables computation of d(x xt and yt. 1D Itôs Formula. Let xt be stochastic process satisfying the SDE: dxt = (xt, t) dt + g(t) dwt. For scalar function : [0, ] R, the process h(xt, t) satisfies: dh(xt, t) = t + x + 1 2 g2 2h x2 ! dt + x dwt. Multidimensional Itôs Formula with Scalar Output. Let xt RD satisfy the SDE: dxt = (xt, t) dt + g(t) dwt, where : RD [0, ] RD, : [0, ] R, and wt RD is D-dimensional Brownian motion. Let : RD [0, ] be scalar-valued function. Then h(xt, t) satisfies: dh(xt, t) = (cid:18) t + xhf + g2(t) Tr (cid:16) 1 2 (cid:17)(cid:19) 2 xh dt + g(t)xh dwt, (C.1.1) where xh RD is the gradient and 2 with respect to x. xh RDD is the Hessian matrix of Example: Itôs Product Rule Let xt, yt RD be vector-valued stochastic processes governed by the SDEs: dxt = a(xt, t) dt + b(t) dwt, dyt = c(yt, t) dt + d(t) dwt, where a, : RD [0, ] RD are vector fields, and b(t), d(t) are C.1. Itôs Formula: The Chain Rule for Random Processes 417 scalar-valued functions. Here, wt RD denotes standard D-dimensional Brownian motion. We aim to derive the SDE for the scalar-valued process z(t) := t yt. Applying the multivariate Itô formula to the bilinear function h(x, y) := xy, we obtain: d(xy) = (dx)y + dy + Tr dx (dy)i . The Itô correction term is computed as: dx (dy) = b(t) dwt [d(t) dwt] = b(t)d(t) dwt dw = b(t)d(t) dt ID. Thus, dx (dy)i Tr = b(t)d(t) Tr(ID) dt = Db(t)d(t) dt. Putting everything together, the resulting SDE is: d(xy) = (dx)y + dy + Db(t)d(t) dt (C.1.2) C.1.4 Itôs Formulas Application: Derivation of Fokker-Planck Equation In this section, we apply Itôs formula from Equation (C.1.1) to derive the FokkerPlanck equation, PDE that characterizes the time evolution of the probability density pt(x) associated with the D-dimensional diffusion process defined by the SDE in Equation (A.2.3). Step 1: Apply Itôs Formula. Let ϕ(x, t) be smooth test function ϕ : RD [0, ] R. By Itôs Formula: dϕ(xt, t) = (cid:18) ϕ + xϕf (xt, t) + g2(t) Tr[2 xϕ] (cid:19) 1 2 dt + g(t)xϕ dwt. Step 2: Take Expectation. Taking expectation over pt(x) and noting E[dwt] = 0: E[dϕ(xt, t)] = (cid:20)(cid:18) ϕ + xϕf (xt, t) + 1 2 g2(t) Tr[ xϕ] (cid:19) (cid:21) dt . Behind the Scenes of Diffusion Models: Itôs Calculus and Girsanovs Theorem Step 3: Express Expectation via Density. This expectation can be written as: E[dϕ(xt, t)] = (cid:18) ϕ + xϕf (x, t) + 1 g2(t) Tr[2 xϕ] (cid:19) pt(x) dx dt. Step 4: Integrate by Parts. Use integration by parts (divergence theorem) in RD: Z xϕf pt dx = Tr[2 xϕ]pt dx = ϕx (f pt) dx, ϕpt dx. Step 5: Substitute and Rearrange. Substituting back: E[dϕ(xt, t)] = ϕ(x, t) (cid:20) pt + (f pt) g2(t)pt (cid:21) dx dt. 1 Step 6: Conclude Fokker-Planck Equation. Since ϕ is arbitrary, the integrand must vanish: pt = (f (x, t)pt(x)) + 1 2 g2(t)pt(x), which completes the derivation of the Fokker-Planck equation. C.1.5 Itôs Formula Application: Closed-Form Solution of Linear SDE This subsection demonstrates how to obtain closed-form solution for linear SDE by using an integration factor (similar to the ODE case) and Itôs formula. The approach mirrors classical techniques for solving linear ODEs, but adapted to the stochastic setting. We consider linear SDE of the form dxt = (t)xtdt + g(t)dwt, (C.1.3) where (t) and g(t) are deterministic functions and wt is standard Wiener process. Closed-Form Solution of Linear SDE. We derive the explicit solution to the linear SDE in Equation (C.1.3) using the method of integrating factors. This type of forward SDE commonly arises in diffusion models (see Section 4.1). C.1. Itôs Formula: The Chain Rule for Random Processes 419 Step 1: Define an Integration Factor. Let Ψ(t) := exp (cid:18) 0 (cid:19) (s)ds , and define yt := Ψ(t)xt. Step 2: Apply Itôs Formula. We apply Itôs formula to the function h(x, t) := Ψ(t)x. This is actually special case of the Itô product rule in Equation (C.1.2). Since Ψ(t) is deterministic, there is no cross-variation term, and the formula simplifies to: dyt = d[Ψ(t)xt] = Ψ(t)xtdt + Ψ(t)dxt = (t)Ψ(t)xtdt + Ψ(t) [f (t)xtdt + g(t)dwt] = Ψ(t)g(t)dwt. Hence, yt = y0 + 0 Ψ(s)g(s)dw(s) = x0 + 0 Ψ(s)g(s)dw(s), since Ψ(0) = 1. Step 3: Solve for xt. Using xt = Ψ(t)1yt, we obtain (cid:20) x0 + (r)drg(s)dw(s) s xt = (s)ds 0 0 (cid:21) . (C.1.4) This provides an explicit solution to the vector-valued SDE. Below, we demonstrate two alternative approaches to compute the analytical form of pt(xtx0). Analysis of the Closed-Form Solution. Equation (C.1.4) reconfirms that pt(xtx0) is Gaussian. To see this, define ϕ(s) := s (u) dug(s), 0 which is deterministic matrix-valued function of time (assuming (u) and g(s) are deterministic). The Itô integral 0 ϕ(s) dws is then zero-mean Gaussian random variable, as it is the stochastic integral of deterministic function with respect to Brownian motion. Therefore, xtx0 is an affine transformation of Gaussian random variable and hence itself Gaussian. Its distribution is fully characterized by its conditional mean and covariance. We define the conditional mean and covariance (given initial condition x0) as m(t) := E[xtx0], P(t) := E[(xt m(t))(xt m(t))x0]. 420 Behind the Scenes of Diffusion Models: Itôs Calculus and Girsanovs Theorem Mean. Using linearity of expectation and the fact that the Itô integral of deterministic function has zero mean: m(t) = (cid:20) 0 (cid:18) (s) ds x0 + 0 ϕ(s) dws (cid:21) 0 = (cid:19) (cid:12) (cid:12) (cid:12) (cid:12) (s) dsx0. Covariance. Let zt := 0 ϕ(s) dws. Then xt m(t) = A(t)zt, so By Itô isometry1, P(t) = e2 0 (s) dsE[ztz ]. E[ztz ] = (cid:18)Z (cid:19) ϕ2(s) ds ID, hence, P(t) = e2 f (s) ds (cid:18) s 0 (cid:19)2 (u) dug(s) ! ds ID. This shows the conditional covariance is isotropic. Derivation of Mean and Variance ODEs in Equation (4.3.3). Alternatively, we can derive the moment evolution equations directly from the linear SDE Equation (C.1.3). Mean Evolution. Taking the conditional expectation of both sides of the SDE and using linearity: dm(t) dt = E[f (t)xtx0] = (t)E[xtx0] = (t)m(t). 1Itôs isometry links stochastic integrals to standard integrals in expectation; we omit the proof as it requires the full machinery of Itô calculus. For process ψ : [0, ] RDD, Itô isometry states E \"(cid:13) (cid:13) (cid:13) (cid:13) ψ(t)dwt 0 2# (cid:13) (cid:13) (cid:13) (cid:13) = (cid:20)Z 0 ψ(t)2 dt (cid:21) , where ψ(t)2 integral is scalar and the isometry simplifies to = PD i,j=1 ψij(t)2 is the Frobenius norm. For ψ(t) RD (a vector), the \"(cid:18)Z E 0 (cid:19)2# (cid:20)Z (cid:21) ψ(t)2dt . = ψ(t)dwt 0 C.1. Itôs Formula: The Chain Rule for Random Processes Covariance Evolution. Define the centered process xt := xt m(t). Applying Itôs product rule (see Equation (C.1.2)): (cid:16) xt (cid:17) = dxt + xt dx + dxt dx . From the SDE, we compute: dxt = dxt dm(t) = (t)xt dt + g(t) dwt. Substituting into the product rule and taking expectation: dP(t) dt = E[f (t)xt + xt = 2f (t)P(t) + g2(t)ID. (t) + g2(t)ID] Thus, we recover the moment evolution equations in Equation (4.3.3). Behind the Scenes of Diffusion Models: Itôs Calculus and Girsanovs Theorem C.2 Change-of-Variable For Measures: Girsanovs Theorem in Diffusion Models Diffusion models harness SDEs to transform simple noise into rich data distributions. At the heart of this transformation lies profound idea: we can reinterpret randomness by modifying only the deterministic part of an SDE (the drift) while preserving its underlying stochasticity. This is precisely where Girsanovs theorem enters the picture. The Core Idea. Consider an observed continuous trajectory that describes the datas evolution from time = 0 to = , denoted as x0:T := {xtt [0, ]}. Girsanovs theorem addresses fundamental question: Question C.2. Given this single observed path, what is its likelihood if we assume it was generated by one SDE, versus if we assume it was generated by different SDE? We compare two hypothetical models for generating the same trajectory. Both of these assumed SDEs share the same underlying pure randomness, represented by standard Wiener process (Brownian motion) wt, but differ only in their deterministic push or drift function. We assume x0 has the same initial distribution for both assumed generating processes. To build intuition, imagine x0:T as wiggly line drawn on paper. One hypothesis is that it was produced by robot painter guided by drift and perturbed by random noise scaled by g(t), yielding likelihood pf (x0:T ). Alternatively, we imagine second robot, with different drift but using the same noise process, generating the same line with likelihood pf (x0:T ). Girsanovs theorem gives us precise way to compare these two likelihoods for the exact same observed path. It quantifies how change in drift affects the probability of generating particular trajectory, while holding the randomness fixed. The Setup. Let xt RD be our single, fixed, continuous path. We consider its likelihood under two SDE models, which differ only in their drift functions and . They share the same diffusion coefficient g(t) and the same underlying Wiener process wt: dxt = (xt, t) dt + g(t) dwt dxt = (xt, t) dt + g(t) dwt (Model with drift ) (Model with drift ) C.2. Change-of-Variable For Measures: Girsanovs Theorem in Diffusion Models423 Let δt := (xt, t) (xt, t) represent the difference in drifts for the given path xt. Girsanovs Likelihood Ratio. Girsanovs theorem provides fundamental likelihood ratio between these two ways of interpreting the same observed path. It states: pf (x0:T ) pf (x0:T ) = exp 0 g(t)1 dwt δ ! g(t)1δt2 dt . 1 0 This compact formula is an exponential of two integrals. The first is an Itô integral, while the second is standard Riemann integral. This ratio is crucial in diffusion models, allowing us to bridge between different data generation processes and to evaluate model likelihoods. Girsanovs theorem is best understood as change-of-variable formula for measures. Just as change of variables in calculus transforms an integral between coordinate systems via the Jacobian determinant, Girsanovs theorem provides the corresponding factor (the RadonNikodym derivative) to transform probabilities or expectations between two stochastic processes, when the drift changes but the diffusion remains the same. C.2.1 Girsanovs Theorem as Bridge Between Likelihood Training and Score Matching After understanding how Girsanovs theorem relates the likelihoods of single path under different drift assumptions, we now delve into its implications for diffusion models. Recall the forward SDE in diffusion models: dxt = (xt, t) dt + g(t) dwt, which induces path distribution over full trajectories x0:T := {xt}T t=0 (that is, the joint law of the process over the entire time interval). The reverse-time SDE, parameterized by learnable score function sϕ(xt, t), is given by dxt = (cid:2)f (xt, t) g2(t) sϕ(xt, t)(cid:3) dt + g(t) wt, which in turn defines another path distribution Pϕ over trajectories. 424 Behind the Scenes of Diffusion Models: Itôs Calculus and Girsanovs Theorem Two Notions in Diffusion Models. In diffusion models, we navigate between two core perspectives for describing the stochastic process x0:T : the forward process and its reverse-time counterpart. These perspectives give rise to two distinct but related objectives: Concept 1. Marginal Distribution Matching: This goal constructs reverse-time process whose marginals pt(xt) match those of the forward SDE, starting from noise at time and recovering the data distribution at = 0. As emphasized, the FokkerPlanck equation ensures this marginal consistency for the reverse-time SDE. Concept 2. Joint Path Distribution Matching: This stronger objective seeks to match the full joint distribution over the entire trajectory = p(x0:T ). Rather than just matching snapshots at individual time steps, this condition ensures that the entire sequence of states and their temporal dependencies are faithfully reproduced. Matching the full path distribution ensures all marginals match. Formally, let x0:T := {xtt [0, ]} be stochastic process with joint distribution p(x0:T ). Suppose another process with joint q(x0:T ) satisfies p(x0:T ) = q(x0:T ). Then for any [0, ], the marginal distributions are pt(xt) = p(x0:T ) dx[0,T ]{t}, qt(xt) = q(x0:T ) dx[0,T ]{t}, which implies pt(xt) = qt(xt), [0, ]. Thus, joint path matching implies marginal matching. However, the reverse is not true: two processes may share identical marginals at every time step yet differ significantly in their temporal correlations. Marginal matching lacks the ability to capture these inter-time dependencies, which are encoded only in the joint distribution. Girsanov Bridges the Two Goals. While reverse-time SDEs are primarily designed for marginal matching (Concept 1), Girsanovs theorem reveals deeper connection: score matching across time also encourages joint path matching (Concept 2). More precisely, Girsanovs theorem relates the forward path distribution and the learned reverse path distribution Pϕ. The objective function in C.2. Change-of-Variable For Measures: Girsanovs Theorem in Diffusion Models425 score-based diffusion models is the KL divergence between these path measures: DKL(P Pϕ) = 1 2 EP \"Z 0 g2(t)(cid:13) (cid:13)sϕ(xt, t) log pt(xt)(cid:13) 2 dt (cid:13) # + Const., (C.2.1) Here, the constant does not depend on ϕ, and we use the fact that the Itô integral has zero expectation under . This expression shows that minimizing KL divergence between joint paths is equivalent to learning score function sϕ that approximates the true score xt log pt(xt). Thus, score matching, although framed as marginal objective, effectively promotes alignment of the entire joint path distribution. Implicit Likelihood Training. Beyond just matching path distributions, score matching implicitly allows diffusion models to achieve fundamental goal of generative modeling: approximating the data likelihood (Song et al., 2021). The connection becomes clear through powerful concept called the change-of-measure formula. This formula, illuminated by Girsanovs theorem, allows us to express the logarithm of the marginal likelihood of the data at = 0 (pϕ(x0)) under our learned model. log pϕ(x0) = log pT (xT ) pϕ(x0:T ) p(x0:T ) p(x0:T ) dx0:T . (C.2.2) Here, pT (xT ) is the known distribution of noise at time given by the forward SDE. The term pϕ(x0:T ) is the density ratio between the learned reverse process p(x0:T ) and the forward process for given path x0:T an object precisely quantified by Girsanovs theorem. Essentially, this formula calculates the likelihood of generated data by re-weighting the known likelihood of noise based on how well our learned reverse dynamics explain the observed paths trajectory. We further draw connection back to the KL minimization in Equation (C.2.1) which concerns the discrepancy between the full forward path distribution and the learned reverse path distribution. The two, Equation (C.2.1) and Equation (C.2.2) are deeply intertwined: optimizing this score matching objective (the training loss) directly translates to learning the Girsanov density ratio, thereby implicitly maximizing the data likelihood (pϕ(x0)). This elegant connection beautifully ties together Girsanovs theorem, score-based learning, and the ultimate generative modeling goal of assigning high probability to real data. D"
        },
        {
            "title": "Supplementary Materials and Proofs",
            "content": "D.1 Variational Perspective D.1.1 Theorem 2.2.1: Equivalence Between Marginal and Conditional KL Minimization Proof. Derivation of Equation (2.2.3). We start by expanding the right-hand side expectation: Ep(x0,xi) (cid:2)DKL (cid:0)p(xi1xi, x0)pϕ(xi1xi)(cid:1)(cid:3) = p(x0, xi)DKL (cid:0)p(xi1xi, x0)pϕ(xi1xi)(cid:1) dx0 dxi. By the definition of KL divergence, DKL (cid:0)p(xi1xi, x0)pϕ(xi1xi)(cid:1) = p(xi1xi, x0) log p(xi1xi, x0) pϕ(xi1xi) dxi1. Substituting this into the expectation, we have p(x0, xi)p(xi1xi, x0) log p(xi1xi, x0) pϕ(xi1xi) dxi1 dx0 dxi. Using the chain rule of probability, p(x0, xi) = p(xi)p(x0xi), we rewrite the integral as p(xi) p(x0xi) p(xi1xi, x0) log p(xi1xi, x0) pϕ(xi1xi) dxi1 dx0 dxi. 426 D.1. Variational Perspective 427 This allows us to express the expectation in nested form: \" \" \" Ep(xi) Ep(x0xi) Ep(xi1xi,x0) log p(xi1xi, x0) pϕ(xi1xi) ### . Next, we apply the decomposition of the logarithm: log p(xi1xi, x0) pϕ(xi1xi) = log p(xi1xi, x0) p(xi1xi) + log p(xi1xi) pϕ(xi1xi) . Substituting this back into the expectation gives two terms: Ep(xi) (cid:20) Ep(x0xi) (cid:20) Ep(xi1xi,x0) (cid:20) log + Ep(xi) \" Ep(x0xi) \" Ep(xi1xi,x0) \" p(xi1xi, x0) p(xi1xi) (cid:21)(cid:21)(cid:21) log p(xi1xi) pϕ(xi1xi) ### . Since the second logarithmic term does not depend on x0, by the law of total probability Ep(x0xi) \" Ep(xi1xi,x0) \" log ## p(xi1xi) pϕ(xi1xi) \" = Ep(xi1xi) log p(xi1xi) pϕ(xi1xi) # . Similarly, the first term is the KL divergence Ep(x0xi) (cid:2)DKL (cid:0)p(xi1xi, x0)p(xi1xi)(cid:1)(cid:3) . Putting it all together, we obtain the decomposition: Ep(x0,xi) =Ep(xi) (cid:2)DKL hEp(x0xi) (cid:2)DKL + Ep(xi) (cid:0)p(xi1xi, x0)pϕ(xi1xi)(cid:1)(cid:3) (cid:0)p(xi1xi, x0)p(xi1xi)(cid:1)(cid:3)i (cid:2)DKL (cid:0)p(xi1xi)pϕ(xi1xi)(cid:1)(cid:3) . Proof of Optimality. To prove: p(xi1xi) = p(xi1xi) = Ep(xxi) [p(xi1xi, x)] , xi pi. The first identity follows from the fact that the KL divergence DKL(ppϕ) is minimized when = p, assuming the parameterization is sufficiently expressive. The second identity follows directly from the law of total probability. D.1.2 Theorem 2.2.3: ELBO of Diffusion Model Proof. For notational simplicity, we denote x0:L := (x0, x1, . . . , xL). 428 by: Supplementary Materials and Proofs Step 1: Apply Jensens Inequality. The marginal log-likelihood is given log pϕ(x) = log pϕ(x, x0:L) dx0 dxL, with the joint distribution: pϕ(x, x0:L) = pprior(xL) i=1 pϕ(xi1xi) pϕ(xx0). We introduce the variational distribution p(x0:Lx) and rewrite: log pϕ(x) = log p(x0:Lx) pϕ(x, x0:L) p(x0:Lx) dx0 dxL. Applying Jensens inequality (log E[Z] E[log Z]), we obtain the ELBO: log pϕ(x) Ep(x0:Lx) (cid:20) log (cid:21) pϕ(x, x0:L) p(x0:Lx) =: LELBO, and thus, log pϕ(x) LELBO. Step 2: Expand the ELBO. Assume the variational distribution factorizes as: p(x0:Lx) = p(xLx) i=1 p(xi1xi, x). Substituting the joint and variational distributions into the ELBO: LELBO = Ep(x0:Lx) log pprior(xL) + log pϕ(xi1xi) + log pϕ(xx0) i=1 log p(xLx) i=1 log p(xi1xi, x) . We now compute the negative ELBO by grouping terms according to their dependencies and applying marginalization: LELBO =Ep(x0x) [ log pϕ(xx0)] + Ep(xLx) log \" # p(xLx) pprior(xL) + i=1 Ep(xix) \" Ep(xi1xi,x) \" log p(xi1xi, x) pϕ(xi1xi) ## . To justify the last term, we use the factorization: p(xi, xi1x) = p(xix) p(xi1xi, x), D.1. Variational Perspective 429 which leads to: \" Ep(x0:Lx) log # p(xi1xi, x) pϕ(xi1xi) = p(xix) \"Z p(xi1xi, x) log p(xi1xi, x) pϕ(xi1xi) # dxi1 dxi =Ep(xix) \" Ep(xi1xi,x) \" log p(xi1xi, x) pϕ(xi1xi) ## . We may refer to the three terms in LELBO as: Lrecon., Lprior, Ldiffusion, corresponding respectively to the reconstruction loss, the prior KL, and the stepwise diffusion KL. This completes the derivation. 430 Supplementary Materials and Proofs D.2 Score-Based Perspective D.2.1 Proposition 3.2.1: Tractable Score Matching via Integration by Parts Proof. Expanding LSM(ϕ). Let us expand the squared difference inside the expectation: 1 2 1 2 LSM(ϕ) = Expdata(x) = Expdata(x) 1 2 We now focus on the cross-product term: Expdata(x) + . sϕ(x)2 sϕ(x)2 2 s(x)2 2 2 2sϕ(x), s(x) + s(x)2 Expdata(x) [sϕ(x), s(x)] 2 Using the fact that Expdata(x) [sϕ(x), s(x)] . log pdata(x) = xpdata(x) pdata(x) , and assuming pdata(x) is not zero (e.g., on its support), the cross-product term becomes: Expdata(x) [sϕ(x), s(x)] = = = sϕ(x)x log pdata(x)pdata(x) dx sϕ(x)xpdata(x) dx Z i=1 s(i) ϕ (x)xipdata(x) dx, where s(i) ϕ (x) is the i-th component of the score function (cid:16) ϕ , s(2) s(1) ϕ , . . . , s(D) sϕ = (cid:17) ϕ . Integration by Parts. We use the following integration-by-parts formula (Evans, 2010), which is derived from standard calculus: Lemma. Let u, be differentiable real-valued functions on ball B(0, R) RD of radius > 0. Then for = 1, . . . , D, the formula holds: B(0,R) uxiv dx = B(0,R) vxiu dx + B(0,R) uvνi dS, where ν = (ν1, . . . , νD) is the outward unit normal to the boundary B(0, R)a sphere with radius > 0, and dS is the surface measure on B(0, R). D.2. Score-Based Perspective We apply this formula to u(x) := s(i) ϕ (x) and v(x) = pdata(x) for all = 1, . . . , D, assuming that u(x)v(x) 0 as . Summing the results over all = 1, . . . , D, we get: Expdata(x) [sϕ(x), s(x)] = Z i=1 xis(i) ϕ (x)pdata(x) dx = Expdata(x) [Tr (xsϕ(x))] . Combining all results, we have: LSM(ϕ) = Expdata(x) (cid:20) Tr (xsϕ(x)) + 1 2 sϕ(x)2 2 (cid:21) } {z eLSM(ϕ) s(x)2 2 , } + 1 2 Expdata(x) {z =:C where depends only on the distribution pdata, which concludes the proof. D.2.2 Theorem 3.3.1: Equivalence Between SM and DSM Minimization Proof. Expanding both LSM(ϕ; σ) and LDSM(ϕ; σ), we have: LSM(ϕ; σ) = Expσ(x) 1 2 sϕ(x; σ)2 2 2sϕ(x; σ)x log pσ(x) + log pσ(x)2 2 , LDSM(ϕ; σ) = Epdata(x)pσ(xx) 1 2 sϕ(x; σ) 2 2sϕ(x; σ)x log pσ(xx) + log pσ(xx)2 2 . 432 Supplementary Materials and Proofs Subtracting the two equations yields: LSM(ϕ; σ) LDSM(ϕ; σ) = 1 2 Expσ(x) sϕ(x; σ)2 2 Epdata(x)pσ(xx) sϕ(x; σ)2 2 ! Expσ(x) sϕ(x; σ)x log pσ(x) Epdata(x)pσ(xx) i sϕ(x; x)x log pσ(xx) ! Expσ(x) log pσ(x)2 2 Epdata(x)pσ(xx) log pσ(xx)2 2 ! . + 1 2 Next, we address the three terms one at time. For the first term, since pσ(x) = pσ(xx)pdata(x) dx, we can rewrite it as: Expσ(x) sϕ(x; σ)2 2 = (cid:16) pσ(xx)pdata(x) dx (cid:17) sϕ(x; σ)2 2 dx = pdata(x) pσ(xx) sϕ(x; σ)2 2 dx dx = Epdata(x)pσ(xx) sϕ(x; σ)2 2 . Thus, the first term is zero. For the second term: sϕ(x; σ)x log pσ(x) pσ(x)sϕ(x; σ) xpσ(x) pσ(x) dx sϕ(x; σ)x pσ(xx)pdata(x) dx dx Expσ(x) = = = sϕ(x; σ)xpσ(xx)pdata(x) dx dx sϕ(x; σ)x log pσ(xx) =Epdata(x)pσ(xx) (D.2.1) . Thus, it is also zero. For the third term, note that: := 1 Expσ(x) log pσ(x)2 2 Epdata(x)pσ(xx) log pσ(xx)2 2 ! depends only on pdata(x) and pσ(xx), and hence it is constant with respect to ϕ. D.2. Score-Based Perspective 433 D.2.3 Lemma 3.3.2: Tweedies Formula We first state more general form of Tweedies formula, which considers time dependent Gaussian perturbations, and we provide its proof below. Tweedies Identity with Time-Dependent Parameters. Let xt (cid:0); αtx0, σ2 be Gaussian random vector. Then Tweedies formula says I(cid:1) αtEx0p(x0xt)[x0xt] = xt + σ xt log pt(xt), where the expectation is taken over the posterior distribution p(x0xt) of x0 given the observed xt, and pt(xt) is the marginal density of xt. Proof. Marginal Density and Its Score. We recall that the marginal density of xt is given by pt(xt) = pt(xtx0)p0(x0) dx0. We now compute the score function: xt log pt(xt) = xtpt(xt) pt(xt) = 1 pt(xt) xtpt(xtx0)p0(x0) dx0. We therefore need to compute the gradient of the conditional density. Gradient of the Conditional and Rearrangement. The gradient of the conditional Gaussian density is: xtpt(xtx0) = pt(xtx0) σ2 (xt αtx0). Substituting this into the previous expression, we have: xtpt(xt) = xtpt(xtx0)p0(x0) dx0 Z = σ2 = σ2 (xt αtx0)pt(xtx0)p0(x0) dx (xt αtx0)p(x0xt)pt(xt) dx0 (cid:17) (cid:16) xt αtEp(x0xt)[x0xt] . = pt(xt)σ2 Dividing both sides by pt(xt), we obtain: xt log pt(xt) = σ (cid:16) (cid:17) xt αtEp(x0xt)[x0xt] . 434 Supplementary Materials and Proofs Rearranging yields: xt + σ2 xt log pt(xt) = αtEp(x0xt)[x0xt]. This completes the derivation. D.2.4 Steins Identity and Surrogate SURE Objective Steins Identity. Steins identity is the integration-by-parts technique that turns expectations under an unknown density into expectations of observable functions and their derivatives, which cancels the partition function and enabling unbiased, tractable objectives and tests without ever evaluating the unknown density or the partition function. We begin with the simplest onedimensional case and then extend it to the form needed to prove the surrogate loss for SURE. 1D, Standard Normal Case. If (0, 1) and has suitable decay, then Steins identity states: E[f (z)] = E[Zf (z)]. ez2/2, the one-dimensional standard normal density. The Denote ϕ(z) := 1 2π proof follows by integration by parts, using ϕ(z) = zϕ(z), together with the vanishing boundary term. To see this precisely, we compute E[f (Z)] = (z)ϕ(z) dz. By integration by parts, with = (z) and dv = ϕ(z) dz, we obtain (z)ϕ(z) dz = (z)ϕ(z) f (z)ϕ(z) dz. Since ϕ(z) = zϕ(z) and (z)ϕ(z) 0 as (decay condition), the boundary term vanishes and we have E[f (z)] = (z)zϕ(z) dz = E[zf (z)]. This completes the derivation and proves the one-dimensional case of Steins identity. Multivariate, Standard Normal Case. If (0, ID) and : RD R, then Steins identity is E[g(z)] = E[zg(z)]. Equivalently, for : RD RD, E[x u(z)] = E[zu(z)]. (D.2.2) D.2. Score-Based Perspective 435 Identity Needed for SURE. With = + σz, where (0, ID), and any vector function of suitable regularity, E(cid:2)(x x)a(x)(cid:12) (cid:12)x(cid:3) = σ2E(cid:2)x a(x)(cid:12) (cid:12)x(cid:3). (D.2.3) This is obtained by applying Equation (D.2.2) and using the chain rule. Deriving SURE from the Conditional MSE. Let : RD RD be denoiser and define R(D; x) := D(x) x2 2x . Expand around x: R(D; x) = = D(x) x2(cid:12) (cid:12)x D(x) x2(cid:12) (cid:12)x + 2E (D(x) x)(x x)(cid:12) (cid:12)x + + 2 (cid:16) E[(x x)D(x)x] {z } σ2E[xD(x)x] by Equation (D.2.3) E[(x x) xx] {z } σ2D x x2(cid:12) (cid:12)x (cid:17) + E[x x2x] {z } σ2D D(x) x2 + 2σ2x D(x) Dσ2x . = Therefore the observable surrogate SURE(D; x) := D(x) x2 2 + 2σ2x D(x) Dσ2 satisfies (cid:2)SURE(D; x)(cid:12) (cid:12)x(cid:3) = R(D; x). Minimizing SURE (in expectation or empirically) is thus equivalent to minimizing the true conditional MSE while using only noisy observations. D.2.5 Theorem 4.1.1: Marginal Alignment via FokkerPlanck Proof. Part 1: Fokker-Planck Equation for the Forward SDE. Consider the forward SDE: dx(t) = (x(t), t) dt + g(t) dw(t). The diffusion matrix is σ(t) = g(t)ID, so σ(t)σ(t)T = g2(t)ID. The FokkerPlanck equation for the marginal density pt(x) of x(t) is: tpt(x) = (cid:2)f (x, t)pt(x)(cid:3) + 1 2 i,j=1 2 xixj (cid:2)(g2(t)δij)pt(x)(cid:3). 436 Supplementary Materials and Proofs Compute the diffusion term: i,j=1 2 xixj (cid:2)g2(t)δijpt(x)(cid:3) = i= 2 x2 (cid:2)g2(t)pt(x)(cid:3) = g2(t)xpt(x). Thus: tpt(x) = (cid:2)f (x, t)pt(x)(cid:3) + 1 2 g2(t)xpt(x). Now, rewrite using: (x, t) = (x, t) 1 2 g2(t)x log pt(x). Since log pt(x) = xpt(x) pt(x) , compute: (cid:2)f (x, t)pt(x)(cid:3) = (cid:20) (x, t)pt(x) g2(t) 1 2 xpt(x) pt(x) (cid:21) . pt(x) The second term is: (cid:20) 1 2 (cid:21) g2(t)xpt(x) = 1 2 g2(t)xpt(x). Thus: (cid:2)f (x, t)pt(x)(cid:3) = (cid:2)f (x, t)pt(x)(cid:3) 1 2 g2(t)xpt(x). Therefore: tpt(x) = (cid:2)f (x, t)pt(x)(cid:3), verifying the Fokker-Planck equation in both forms. Part 2: PF-ODE Marginal Densities. Consider the PF-ODE: dx(t) dt = (x(t), t), (x, t) = (x, t) 1 2 g2(t)x log pt(x). Forward Direction: x(0) p0. Let x(t) follow the PF-ODE with x(0) p0. The flow map Ψt : RD RD is defined by: dt Ψt(x0) = (Ψt(x0), t), Ψ0(x0) = x0. Since x(t) = Ψt(x(0)), the density pt(x) of x(t) satisfies the continuity equation: pt(x) = (cid:2)f (x, t)pt(x)(cid:3). Since x(0) p0, we have p0(x) = p0(x). From Part 1, pt(x) satisfies: tpt(x) = (cid:2)f (x, t)pt(x)(cid:3). D.2. Score-Based Perspective Both pt and pt satisfy the same continuity equation with the same initial condition p0. Assuming sufficient smoothness (e.g., C1), the solution is unique in some appropriate function space, so pt = pt. Thus, x(t) pt for all [0, ]. Backward Direction: x(T ) pT . Now, let x(t) follow the PF-ODE backward from = to = 0, with x(T ) pT . The ODE is: dt x(t) = (x(t), t). Let = t, so the backward ODE becomes: ds x(T s) = (x(T s), s). The density pT s(x) of x(T s) satisfies: pT s(x) = (cid:2)f (x, s)pT s(x)(cid:3). Since x(T ) pT , we have pT = pT . The Fokker-Planck equation for pt at = is: tpT s(x) = (cid:2)f (x, s)pT s(x)(cid:3). Since = s, we get: spT s(x) = (cid:2)f (x, s)pT s(x)(cid:3). Both pT and pT satisfy the same PDE with the same initial condition at = 0 (pT = pT ). Uniqueness implies pT = pT s, so x(t) = x(T s) pT = pt, for all [0, ]. Part 3: Reverse-Time SDE Marginal Densities. Consider the reverse-time SDE: dx(t) = (cid:2)f (x(t), t) g2(t)x log pt(x(t))(cid:3) dt + g(t) w(t), with x(0) pT , where w(t) is standard Wiener process in reverse time, defined as w(t) = w(T t) w(T ). We need to show x(t) pT t. Rewrite the drift: (x, t) = (x, t) + 1 2 g2(t)x log pt(x), so: (x, t) g2(t)x log pt(x) = (x, t) 1 2 g2(t)x log pt(x). The reverse-time SDE is: dx(t) = (cid:20) (x(t), t) (cid:21) g2(t)x log pt(x(t)) 1 2 dt + g(t) w(t). 438 Supplementary Materials and Proofs Let = t, so x(t) = x(T s), and dt = ds. The SDE becomes: dx(T s) = (cid:20) (x(T s), s) + g2(T s)x log pT s(x(T s)) (cid:21) ds 1 2 + g(T s) w(T s). Since w(t) = w(T t) w(T ), we have w(T s) = dw(s), where w(s) is standard Wiener process. Thus, let w(s) = w(s), standard Wiener process, so: dx(s) = (cid:20) (x(s), s) 1 2 (cid:21) g2(T s)x log pT s(x(s)) ds + g(T s) w(s). The Fokker-Planck equation for the density ps(x) of x(s) is: ps(x) = (cid:20)(cid:18) (x, s) 1 2 g2(T s)x log pT s(x) (cid:19) (cid:21) ps(x) + 1 2 g2(T s)x ps(x). Assume ps = pT s. The Fokker-Planck equation for pT is: tpT s(x) = (cid:2)f (x, s)pT s(x)(cid:3). Since = s: spT s(x) = (cid:2)f (x, s)pT s(x)(cid:3). Substitute ps = pT s: spT = (cid:20) (x, s)pT s(x) g2(T s) 1 xpT s(x) pT s(x) (cid:21) pT s(x) + 1 2 g2(T s)xpT s(x). The extra term is: (cid:20) 1 2 (cid:21) g2(T s)xpT s(x) + 1 g2(T s)xpT s(x) = 1 2 g2(T s)xpT s(x) 1 2 g2(T s)xpT s(x) = 0. Thus, ps = pT satisfies the Fokker-Planck equation. Since x(0) pT , we have p0 = pT , matching the initial condition. Uniqueness (under sufficient smoothness) ensures ps = pT s, so x(t) = x(T s) pT t. D.2. Score-Based Perspective 439 D.2.6 Proposition 4.2.1: Minimizer of SM and DSM Proof. To find the minimizer s, we first consider fixed time and analyze the inner expectation in the objective function: (t, ϕ) := Ex0pdata Extpt(x0) sϕ(xt, t) xt log pt(xtx0)2 2 . For this expectation to be minimized, we need to find sϕ(xt, t) that minimizes the expected squared error for each xt. We can rewrite this expectation using the joint distribution of X0 and Xt: ZZ (t, ϕ) = pdata(x0)pt(xtx0) sϕ(xt, t) xt log p(xtx0)2 2 dx0 dxt. For each fixed xt, we need to minimize: p(x0Xt = xt)pt(xt) sϕ(xt, t) xt log p(xtx0)2 2 dx0. Since pt(xt) is constant with respect to sϕ(xt, t), this is equivalent to minimizing: p(x0Xt = xt) sϕ(xt, t) xt log p(xtx0)2 2 dx This is minimized when sϕ(xt, t) equals the conditional expectation: s(xt, t) = EX0p(X0Xt=xt) [xt log p(xtX0)] . Now we need to prove that this equals xt log pt(xt). By Bayes rule and the definition of marginal probability: pt(xt) = pt(xtx0)pdata(x0) dx0. Taking the logarithm and then the gradient with respect to xt: xt log pt(xt) = xtpt(xt) pt(xt) = xt pt(xtx0)pdata(x0) dx pt(xtx0)pdata(x0) dx0 . Under suitable regularity conditions, we can exchange the gradient and integral: xt log pt(xt) = xtpt(xtx0)pdata(x0) dx0 pt(xtx0)pdata(x0) dx0 . 440 Supplementary Materials and Proofs D.2.7 Closed-Form Score Function of Gaussian For future reference, we summarize the formula for the score of general multivariate normal distribution as the following lemma: Lemma D.2.1: Score of Gaussian Let RD and consider the multivariate normal distribution p(xx) := (x; µ, Σ), where µ RD is the mean and Σ RDD is an invertible covariance matrix. Its score function is log p(xx) = Σ1(x µ). (D.2.4) Proof for Lemma. The density function of p(xx) is given by: p(xx) = 1 (2π)D/2Σ1/2 exp (cid:18) 1 2 (x µ)Σ1(x µ) (cid:19) . To compute the score function, we first take the log of p(xx): log p(xx) = 2 log(2π) 1 2 log Σ 1 2 (x µ)Σ1(x µ). Now, we compute the gradient of log p(xx) with respect to x: log p(xx) = (cid:16) 1 2 (x µ)Σ1(x µ) (cid:17) . Using the chain rule, we get: (cid:16) (x µ)Σ1(x µ) (cid:17) = 2Σ1(x µ). Thus, the score function is: log p(xx) = Σ1(x µ). (D.2.5) D.3. Flow-Based Perspective 441 D.3 Flow-Based Perspective D.3.1 Lemma 5.1.1: Instantaneous Change of Variables Proof. Approach 1: Change-of-Variables Formula. We denote p(x(t), t) by pt(xt). Starting from the ODE discretization zt+t = zt + tF(zt, t), the change-of-variables formula for normalizing flows (Equation (5.1.1)) gives log pt+t(zt+t) = log pt(zt) log (cid:16) (cid:12) (cid:12) (cid:12) det (cid:0)I + tzF(zt, t)(cid:1)(cid:12) (cid:12) (cid:12) (cid:17) log(I + tzF(zt, t)) = log pt(zt) Tr = log pt(zt) Tr (cid:0)zF(zt, t)(cid:1) + O(t2), where we used log det = Tr log and the expansion for small t. Taking the limit 0 yields the continuous-time differential equation for the log-density. Indeed, the same trick is applied in Equation (5.1.6). Approach 2: Continuity Equation. We can also leverage the continuity equation, which essentially serves as the change-of-variables formula: tp(z, t) = (cid:0)F(z, t)p(z, t)(cid:1). Expanding the divergence, tp = (cid:0)(z F)p + zp(cid:1). Along trajectories z(t) satisfying dz dt = F(z(t), t), the total time derivative is dt p(z(t), t) = zp + tp dz dt = zp (cid:0)(z F)p + zp(cid:1) = (z F)p. Dividing by p(z(t), t), we conclude dt log p(z(t), t) = F(z(t), t). 442 Supplementary Materials and Proofs D.3.2 Theorem 5.2.2: Mass Conservation Criterion Some Prerequisites: Flow Map and Flow-Induced Density. For any initial position x0 RD, the flow map Ψt : RD RD is the unique solution of the ODE dt Ψt(x0) = vt (cid:0)Ψt(x0)(cid:1), Ψ0(x0) = x0. Under our regularity assumptions, Ψt is continuously differentiable in both and x0. The flow-induced density pfwd is the pushforward of the initial density p0 by Ψt: (x)(cid:1)(cid:12) This gives the density at and time of particles that started from p0 = pdata and evolved under the velocity field vt. (cid:12)det(cid:0)Ψ1 (cid:12) (x)(cid:1)(cid:12) (cid:12) (cid:12). (x) = p0 (cid:0)Ψ1 pfwd Informal Proof: Sufficient Condition: pfwd = pt Continuity Equation. In Section B.1.2 we obtained strong solution of the continuity equation by taking the continuous time limit of the discrete change of variable formula. In that approach, the density pt is assumed smooth enough that all derivatives exist classically and the PDE holds pointwise. Here, we offer complementary derivation in the weak sense: the continuity equation is imposed only after integrating against arbitrary smooth test functions, which relaxes the regularity requirements on both pt and the velocity field vt. This weak formulation is not only more rigorous since it accommodates less regular solutions but is also the standard framework in PDE theory and numerical analysis. For any smooth test function φ(x) with compact support, the pushforward property gives: pfwd (x)φ(x) dx = = Z p0(Ψ1 (x)) (cid:16) (cid:12) (cid:12) (cid:12)det Ψ (x) (cid:17)(cid:12) (cid:12) (cid:12) φ(x) dx p0(y)φ(Ψt(y)) dy, where the second equality follows from the change of variables = Ψt(y), with dy = Ψ1 (cid:16) (x) Differentiate both sides with respect to t: (cid:17)(cid:12) (cid:12) (cid:12) dx. (cid:12) (cid:12) (cid:12)det dt pfwd (x)φ(x) dx = dt p0(y)φ(Ψt(y)) dy. The left-hand side is: pfwd t (x)φ(x) dx. D.3. Flow-Based Perspective 443 On the right-hand side: p0(y)φ(Ψt(y)) vt(Ψt(y)) dy, since Ψt (y) = vt(Ψt(y)). Change variables to = Ψt(y), so dy = (cid:16) (cid:12) (cid:12) (cid:12)det Ψ1 (x) (cid:17)(cid:12) (cid:12) (cid:12) dx, and p0(y) = pfwd (x) det (Ψt(y)) = (cid:12) (cid:12) (cid:12)det Thus, the right-hand side becomes: pfwd (x) (cid:16) Ψ1 (x) . (cid:17)(cid:12) (cid:12) (cid:12) pfwd (x)φ(x) vt(x) dx. Apply integration by parts, using the compact support of φ: pfwd (x)φ(x) vt(x) dx = φ(x) (pfwd (x)vt(x)) dx. Equating both sides: \" pfwd (x) + (pfwd # (x)vt(x)) φ(x) dx = 0. Since φ is arbitrary, we conclude: pfwd + (pfwd vt) = 0. Given pfwd = pt, this implies: pt Necessary Condition: Continuity Equation pfwd + (ptvt) = 0. = pt. Suppose pt satisfies the continuity equation: pt with the initial condition p0(x) = pdata(x). We know pfwd continuity equation, as shown above, and: (cid:16) + (ptvt) = 0, pfwd 0 (x) = p0(Ψ1 0 (x)) (cid:12) (cid:12) (cid:12)det Ψ1 0 (x) (cid:17)(cid:12) (cid:12) (cid:12) = p0(x), satisfies the same since Ψ0(x) = x. Thus, both densities share the same initial condition p0 = pdata. Supplementary Materials and Proofs The continuity equation can be rewritten as: t + vt + vt = 0. This is first-order linear PDE. Assuming vt is continuously differentiable and globally Lipschitz, and pt is sufficiently smooth, the method of characteristics guarantees unique solution in the space of smooth functions. Since pt and pfwd satisfy the same PDE and initial condition, we conclude: pt(x) = pfwd (x) for all [0, 1] and RD. This completes the proof of the equivalence. D.4. Theoretical Supplement: Unified and Systematic View on Diffusion Models 445 D.4 Theoretical Supplement: Unified and Systematic View on Diffusion Models D.4.1 Proposition 6.3.1: Equivalence of Parametrizations Proof: As in Theorem 4.2.1 on the DSM loss, the global optimum of the matching loss Et ω(t) Ex0,ϵ ii 2 2 is attained when the inner expectation Ex0,ϵ 2 2 is minimized for each fixed t. Since this is standard mean squared error problem, the minimizer is unique. From denoising score matching (Vincent, 2011), Theorem 4.2.1 shows the optimal score function satisfies s(xt, t) = Ep(x0xt) [x log pt(xtx0)] = xt log pt(xt). Using the identity log pt(xtx0) = 1 σt ϵ for ϵ (0, I), we obtain s(xt, t) = 1 σt ϵ(xt, t), where ϵ(xt, t) = Ep(x0xt)[ϵxt] is the optimal ϵ-prediction for Lnoise(ϕ). For the x-prediction loss Lclean, the optimal estimator is x(xt, t) = Ep(x0xt)[x0xt], which, by Tweedies formula, relates to the score via αt x(xt, t) = xt + σ2 s(xt, t). For the v-prediction loss Lvelocity, the optimal estimator is tx0 + σ v(xt, t) = Ep(x0xt)[α tϵ. tx + σ = α tϵxt] Substituting the expressions for and ϵ in terms of yields v(xt, t) = α αt xt + = (t)xt s(xt, t) (cid:19) σtσ σ2 (cid:18) α αt 1 g2(t)s(xt, t), 2 which completes the derivation. 446 Supplementary Materials and Proofs D.5 Theoretical Supplement: Learning Fast Diffusion-Based Generators D.5.1 Knowledge Distillation Loss as an Instance of the General Framework Equation (10.1.4) We begin with the oracle KD loss Loracle KD (θ) = ExT pT (cid:13)Gθ(xT , T, 0) ΨT 0(xT )(cid:13) (cid:13) 2 2, (cid:13) with pT = pprior. For the deterministic ODE flow map Ψ (semigroup, bijective along the trajectory), the marginals satisfy the pushforward identity pt = Ψ0t pdata = ΨT pprior; hence, ΨsT ps = pT and ΨT 0 ΨsT = Ψs0. Changing variables xT = ΨsT (xs) with xs ps gives Loracle KD (θ) = Exsps (cid:13) (cid:13) (cid:13)Gθ 2 (cid:13) (cid:0)ΨsT (xs), T, 0(cid:1) Ψs0(xs) (cid:13) (cid:13) 2 . Define the pulled-back student eGθ(xs, s, 0) := Gθ(ΨsT (xs), T, 0) to express the same loss in the unified flow-map form (at = 0): Loracle KD (θ) = Exsps (cid:13) eGθ(xs, s, 0) Ψs0(xs)(cid:13) (cid:13) 2 2. (cid:13) This derivation relies on change of variables through the oracle flow and the semigroup property. D.5.2 ParameterFlow Interpretation to Proposition 10.2. From the derivation of Proposition 10.2.1, we can interpret the gradient of VSD as parameter-induced transport flow, where adjusting the model parameters moves particles in data space to align their motion with the score mismatch between the student and teacher distributions. Let p(t), p(z), ϵ (0, I) and Define the sample (particle) velocity ˆxt = αt Gθ(z) + σt ϵ. bvθ(ˆxt) := θ ˆxt = αt θGθ(z), and the velocity field in xspace as the conditional average vθ(x) := E(cid:2) (cid:12)ˆxt = x(cid:3). bvθ(ˆxt)(cid:12) D.5. Theoretical Supplement: Learning Fast Diffusion-Based Generators 447 With this definition, at each fixed the density obeys the parameterflow continuity equation θpθ (x) + (cid:0)pθ (x)vθ(x)(cid:1) = 0. Here vθ(ˆxt) = θ ˆxt is the parameterinduced particle velocity in data space (with fixed). Equivalently, at each fixed the density satisfies the continuity equation in θ: θpθ (x) + (cid:0)pθ (x)vθ(x)(cid:1) = 0. Thus the gradient of LVSD takes transport form, θLVSD = E(cid:2)ω(t)x log pθ log pt , } {z score mismatch at fixed vθ {z} parameterflow velocity (cid:3), which says: adjust the parameterflow so that particle motion aligns with the local score mismatch, reducing the divergence along the trajectory. D.5.3 Theorem 11.2.1: CM Equals CT up to O(s) Proof: Step 1: DDIM Update (with Oracle Score) Is the Conditional Mean. ˆxs := = αs αs αs αs xs + σ2 (cid:18) αs αs (cid:19) σs σs xs log ps(xs) (cid:0)xs + σ xs log ps(xs)(cid:1) σsσsxs log ps(xs) = αsE[x0xs] + σs (cid:16) xs αsE[x0xs] σs (cid:17) = αsE[x0xs] + σsE[ϵxs] = E[αsx0 + σsϵxs] = E[xsxs]. Here, we use the Tweedies formula E[x0xs] = xs+σ2 αsx0 + σsϵ in the third and fourth equalities. xs log ps(xs) αs and xs = Step 2: Expand CT Around the Conditional Mean ˆxs. 448 Supplementary Materials and Proofs LCT = Es,xs Exs xs (1) = Es,xs Exs xs w(s)d(cid:0)fθ(xs, s), fθ(xs, s)(cid:1)i w(s)d(cid:0)fθ(xs, s), fθ(ˆxs, s)(cid:1) + w(s)2d(cid:0)fθ(xs, s), fθ(ˆxs, s)(cid:1)(cid:2)1fθ(ˆxs, s)(xs ˆxs)(cid:3) + w(s)O(cid:0)xs ˆxs2(cid:1)i (2) = Es,xs + Es,xs + Es,xs (3) = Es,xs w(s)d(cid:0)fθ(xs, s), fθ(ˆxs, s)(cid:1)i w(s)2d(cid:0)fθ(xs, s), fθ(ˆxs, s)(cid:1)(cid:2)1fθ(ˆxs, s)Exs xs(xs ˆxs)(cid:3)i w(s)O(cid:0)xs ˆxs2(cid:1)i Exs xs w(s)d(cid:0)fθ(xs, s), fθ(ˆxs, s)(cid:1)i w(s)d(cid:0)fθ(xs, s), fθ(ˆxs, s)(cid:1)i Exs xsxs ˆxs2(cid:1) Exs xsxs ˆxs2(cid:17) w(s)O(cid:0)xs ˆxs2(cid:1)i (cid:16)Es,xs Exs xs + Es,xs + = Es,xs = LCM + O(cid:0)Es,xs (4) = LCM + O(s) Here, (1) applies second-order Taylor expansion of h(x) := d(fθ(xs, s), fθ(x, s)) around ˆxs in its second argument. (2) applies the tower property Es,xs Exs xs[] = Es,xs[] and notes that, inside Exs xs, the only xs-dependence is through (xs ˆxs), so all other factors are treated as constants and the inner expectations reduce to Exs xs(xs ˆxs) and Exs xsxs ˆxs2. (3) uses E[xs ˆxsxs] = 0 because ˆxs = E[xsxs]. (4) uses the linearGaussian scheduler, which gives E[xs ˆxs2xs] = O(s2), thus leading to total remainder of O(s). D.5.4 Proposition 11.3.1: Continuous-Time Limit of the CT Gradient Proof: We simplify the notation by proving for Equation (11.3.2): CM(θ, θ) := Ls ω(s)(cid:13) (cid:13)fθ(xs, s) fθ (cid:0)Ψsss(xs), s(cid:1)(cid:13) 2 (cid:13) . D.5. Theoretical Supplement: Learning Fast Diffusion-Based Generators 449 For notational simplicity, we write xss := Ψsss(xs) Expanding the MSE loss, we will have fθ(xs, s) fθ(xss, s)2 2 = (fθ(xs, s) fθ(xs, s)) } {z =:δf + (fθ(xs, s) fθ(xss, s)) 2 2 = δf 2 + 2δf ds fθ(xs, s)s + (cid:13) (cid:13) (cid:13) (cid:13) ds (cid:13) 2 (cid:13) fθ(xs, s) (cid:13) (cid:13) s2 + (cid:16) s3(cid:17) . Here, we apply Taylor expansion at (xs, s): fθ(xs, s) fθ (cid:0)xss, s(cid:1) = ds fθ(xs, s) + O(s2), together with the first-order expansion of the oracle flow map, xs Ψsss(xs) = v(xs, s) + O(s2), and the total differentiation identity, ds fθ(xs, s) = sfθ(xs, s) + (cid:0)xfθ(xs, s)(cid:1) v(xs, s), to simplify the expression. Since θ is treated as constant and just the same copy as θ (i.e., no gradient through it), the gradient of Ls CM(θ, θ) with respect to θ is: θLs CM(θ, θ) = 2E (cid:20) ω(s) θfθ(xs, s) (cid:21) fθ(xs, s) ds + (cid:16) s2(cid:17) . Dividing by and taking the limit yields: lim 1 θLs CM(θ, θ) = θE (cid:20) 2ω(t)f θ (xs, s) (cid:21) fθ(xs, s) . ds This proves the identity. 450 Supplementary Materials and Proofs D.6 (Optional) Elucidating Diffusion Model (EDM) We introduce specific criteria for neural network parameterization design in the x-prediction model, as proposed in Elucidating Diffusion Models (EDM) (Karras et al., 2022). EDM provides simple recipe that makes the training process easier to optimize and more reliable. The x-prediction model is expressed as time dependent skip connection combined with scaled residual (Equation (D.6.1)). The central idea is to normalize both the inputs and the regression targets to unit variance at all times, and to adjust the skip path so that residual errors are not amplified as time evolves. This recipe has become widely adopted default in diffusion model implementations and extends naturally to flow map learning, especially the family of Consistency Models. D.6.1 Criteria for Neural Network xϕ Design EDM considers parametrization of the x-prediction model, denoted with slight abuse of notation as xϕ(x, t)1, in the following form: xϕ(x, t) := cskip(t)x + cout(t)Fϕ (cin(t)x, cnoise(t)) . (D.6.1) Here, cskip(t), cout(t), cin(t), and cnoise(t) are time-dependent functions. They are chosen to enhance stability and performance during training, based on practical considerations that will be introduced shortly. Plugging this in Equation (6.2.5), then the objective function becomes after straightforward algebraic manipulation2: Ex0,ϵ,t ω(t)c2 out(t) Fϕ (cin(t)xt, cnoise(t)) xtgt(t)2 2 . (D.6.2) 1As discussed, all four prediction types are equivalent and can be reduced to the xprediction case. EDM adopts this formulation, which is both well-studied and naturally aligned with the goal of generating clean samples of flow map models. 2We start from the x-prediction diffusion loss by substituting the parameterization given in Equation (D.6.1): Ex0,ϵ,t = Ex0,ϵ,t = Ex0,ϵ,t (cid:2)ω(t) xϕ(xt, t) x02 2 (cid:3) (cid:13) (cid:13) (cid:13)cskip(t) (αtx0 + σtϵ) ω(t) } {z xt +cout(t)Fϕ (cin(t)xt, cnoise(t)) x0 (cid:13) 2 (cid:13) (cid:13) 2 ω(t)c2 out(t) (cid:13) (cid:13) (cid:13)Fϕ (cin(t)x, cnoise(t)) (cid:18) (1 cskip(t)αt) x0 cskip(t)σtϵ cout(t) {z xtgt(t) (cid:19) } (cid:13) 2 (cid:13) (cid:13) 2 = Equation (D.6.1). D.6. (Optional) Elucidating Diffusion Model (EDM) Here, the regression target xtgt(t) is obtained as: xtgt(t) = (1 cskip(t)αt) x0 cskip(t)σtϵ cout(t) . By incorporating the standard deviation of pdata, denoted as σd, EDM proposes the following design criterion for network parameterization, which may be described as the unit variance criterion. Unit Variance of Input. Varx0,ϵ [cin(t)xt] = 1 c2 in(t)Varx0,ϵ [αtx0 + σtϵ] = 1 1 + σ2 dα2 σ2 , cin(t) = taking the positive root. Unit Variance of Training Target. Varx0,ϵ [xtgt(t)] = 1 c2 out(t) = (1 cskip(t)αt)2 σ2 + skip(t)σ2 , (D.6.3) with centered data (E[x0] = 0). Minimize the Error Amplification from Fϕ to xϕ. EDM aims to mitigate the amplification of the networks learning error from Fϕ to xϕ. This is achieved by selecting cskip to minimize cout: skip arg min cskip c2 out. Using the standard approach of solving cout cskip we obtain = 0 for the critical point skip, skip(t) = αtσ2 + σ2 σ2 α2 . Substituting this into Equation (D.6.3), the optimal value is given by out(t) = σtσd σ2 α2 + σ2 . 452 Supplementary Materials and Proofs By convention, we use the nonnegative branch for the output scale: out(t) = σtσd σ2 α2 + σ2 ( 0), which ensures cout(0) = 0 and cout(t) σd as σt is sufficiently large, yielding the intuitive limits xϕ(xt, 0) xt and xϕ(xt, t) σd Fϕ(). We summarize these coefficients as follows: Denoting Rt := α σ2 + σ2 , we have the following selections: cin(t) = 1 Rt , cskip(t) = αtσ2 Rt , cout(t) = σtσd Rt . (D.6.4) With Equation (D.6.4), the regression target xtgt(t) simplifies to xtgt(t) = 1 σd σtx0 αtσ2 dϵ Rt . Additionally, substituting these expressions into Equation (D.6.1) and Equation (D.6.2) allows us to simplify both the parametrization and the loss function, yielding: xϕ(x, t) = αtσ2 Rt + σtσd Rt Fϕ (cid:18) 1 Rt x, cnoise(t) (cid:19) , and ω(t) Ex0,ϵ,t σ2 Rt (cid:13) (cid:13) (cid:13) σdFϕ (cid:13) (cid:13) (cid:18) 1 Rt (cid:19) xt, cnoise(t) σtx0 αtσ2 dϵ Rt . !(cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) 2 With this parameterization and the conditions α0 1 and σ0 0, we observe that cskip(0) 1 and cout(0) 0. Selection of cnoise(t). to-one mapping of the noise level σt (e.g., cnoise(t) = log σt) is suitable. It provides noise-level embedding to Fϕ; any oneD.6.2 Common EDM Special Case: αt = 1, σt = We consider the simplified noise schedule used in EDM, where αt = 1 and σt = t, which also appears in the discussion of CTM in Section 11.4. Under this setting, the forward process becomes xt = x0 + tϵ, with x0 pdata, ϵ (0, I), D.6. (Optional) Elucidating Diffusion Model (EDM) 453 corresponding to the perturbation kernel pt(xtx0) = (xt; x0, t2I). Accordingly, the prior distribution at the terminal time is set as pprior(xT ) := (xT ; 0, 2I). The marginal density induced by the perturbation kernel is given by the convolution: pt(x) = (; 0, t2I)pdata(x0) dx0. Under this setup, the PF-ODE based on x-prediction xϕ (see Equation (6.3.2)) simplifies to dx(t) dt = x(t) xϕ(x(t), t) . Substituting this formulation into Equation (D.6.4), the neural network parameterization coefficients become cin(t) = 1 σ2 + t2 , cskip(t) = σ2 + t2 , σ2 cout(t) = tσd σ2 + t2 . (D.6.5) From these expressions, we observe: When 0, the noise level is negligible, so cskip 1 and cout 0. In this limit, the skip path dominates and the network essentially passes through its input, xϕ(x, t) x. When 0, the input is heavily corrupted by noise, so cskip 0 and cout σd. In this regime, the skip path vanishes and the model output is determined entirely by the learned residual, xϕ(x, t) σdFϕ (cin(t)x, cnoise(t)) , meaning that the network Fϕ predicts scaled proxy of the clean signal from normalized noisy input; at high noise levels the model output is therefore determined entirely by the learned denoising function. In short, the parameterization smoothly interpolates from an identity map at small to scaled residual predictor on standardized inputs at large t."
        },
        {
            "title": "References",
            "content": "Ackley, D. H., G. E. Hinton, and T. J. Sejnowski. (1985). learning algorithm for Boltzmann machines. Cognitive science. 9(1): 147169. Albergo, M. S., N. M. Boffi, and E. Vanden-Eijnden. (2023). Stochastic interpolants: unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797. Albergo, M. S. and E. Vanden-Eijnden. (2023). Building Normalizing Flows with Stochastic Interpolants. In: The Eleventh International Conference on Learning Representations. Altschuler, J., J. Niles-Weed, and P. Rigollet. (2017). Near-linear time approximation algorithms for optimal transport via Sinkhorn iteration. Advances in neural information processing systems. 30. Anderson, B. D. (1982). Reverse-time diffusion equation models. Stochastic Processes and their Applications. 12(3): 313326. Atkinson, K., W. Han, and D. E. Stewart. (2009). Numerical solution of ordinary differential equations. Vol. 81. John Wiley & Sons. Bansal, A., H.-M. Chu, A. Schwarzschild, S. Sengupta, M. Goldblum, J. Geiping, and T. Goldstein. (2023). Universal guidance for diffusion models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 843852. Behrmann, J., W. Grathwohl, R. T. Chen, D. Duvenaud, and J.-H. Jacobsen. (2019). Invertible residual networks. In: International conference on machine learning. PMLR. 573582. Benamou, J.-D. and Y. Brenier. (2000). computational fluid mechanics solution to the Monge-Kantorovich mass transfer problem. Numerische Mathematik. 84(3): 375393. 454 References 455 Boffi, N. M., M. S. Albergo, and E. Vanden-Eijnden. (2024). Flow Map Matching. arXiv preprint arXiv:2406.07507. Bradley, R. A. and M. E. Terry. (1952). Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika. 39(3/4): 324 345. Caluya, K. F. and A. Halder. (2021). Wasserstein proximal algorithms for the Schrödinger bridge problem: Density control with nonlinear drift. IEEE Transactions on Automatic Control. 67(3): 11631178. Chen, R. T., J. Behrmann, D. K. Duvenaud, and J.-H. Jacobsen. (2019). Residual flows for invertible generative modeling. Advances in Neural Information Processing Systems. 32. Chen, R. T., Y. Rubanova, J. Bettencourt, and D. K. Duvenaud. (2018). Neural ordinary differential equations. Advances in neural information processing systems. 31. Chen, T., G.-H. Liu, and E. Theodorou. (2022). Likelihood Training of Schrödinger Bridge using Forward-Backward SDEs Theory. In: International Conference on Learning Representations. Chen, Y., T. T. Georgiou, and M. Pavon. (2016). On the relation between optimal transport and Schrödinger bridges: stochastic control viewpoint. Journal of Optimization Theory and Applications. 169: 671691. Chen, Y., T. T. Georgiou, and M. Pavon. (2021). Stochastic control liaisons: Richard sinkhorn meets gaspard monge on schrodinger bridge. Siam Review. 63(2): 249313. Choi, J., S. Kim, Y. Jeong, Y. Gwon, and S. Yoon. (2021). ILVR: Conditioning Method for Denoising Diffusion Probabilistic Models. In: 2021 IEEE/CVF International Conference on Computer Vision (ICCV). IEEE. 14347 14356. Chung, H., J. Kim, M. T. Mccann, M. L. Klasky, and J. C. Ye. (2022). Diffusion posterior sampling for general noisy inverse problems. arXiv preprint arXiv:2209.14687. Chung, H., J. Kim, M. T. Mccann, M. L. Klasky, and J. C. Ye. (2023). Diffusion Posterior Sampling for General Noisy Inverse Problems. In: The Eleventh International Conference on Learning Representations. url: https://openreview.net/forum?id=OnD9zGAGT0k. Csiszár, I. (1963). Eine informationstheoretische Ungleichung und ihre Anwendung auf den Beweis der Ergodizität von Markoffschen Ketten. Magyar Tudományos Akadémia Matematikai Kutató Intézetének Közleményei. 8(12): 85108. 456 References Cuturi, M. (2013). Sinkhorn distances: Lightspeed computation of optimal transport. Advances in neural information processing systems. 26. Dai Pra, P. (1991). stochastic control approach to reciprocal diffusion processes. Applied mathematics and Optimization. 23(1): 313329. Daras, G., H. Chung, C.-H. Lai, Y. Mitsufuji, J. C. Ye, P. Milanfar, A. G. Dimakis, and M. Delbracio. (2024). survey on diffusion models for inverse problems. arXiv preprint arXiv:2410.00083. Daras, G., Y. Dagan, A. Dimakis, and C. Daskalakis. (2023). Consistent diffusion models: Mitigating sampling drift by learning to be consistent. Advances in Neural Information Processing Systems. 36: 4203842063. De Bortoli, V. (2022). Convergence of denoising diffusion models under the manifold hypothesis. arXiv preprint arXiv:2208.05314. De Bortoli, V., J. Thornton, J. Heng, and A. Doucet. (2021). Diffusion schrödinger bridge with applications to score-based generative modeling. Advances in Neural Information Processing Systems. 34: 1769517709. Dhariwal, P. and A. Nichol. (2021). Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems. 34: 8780 8794. Efron, B. (2011). Tweedies formula and selection bias. Journal of the American Statistical Association. 106(496): 16021614. Esser, P., S. Kulal, A. Blattmann, R. Entezari, J. Müller, H. Saini, Y. Levi, D. Lorenz, A. Sauer, F. Boesel, et al. (2024). Scaling rectified flow transformers for high-resolution image synthesis. In: Forty-first International Conference on Machine Learning. Evans, L. C. (2010). Partial differential equations. Providence, R.I.: American Mathematical Society. Fournier, N. and A. Guillin. (2015). On the rate of convergence in Wasserstein distance of the empirical measure. Probability theory and related fields. 162(3): 707738. Frey, B. J., G. E. Hinton, and P. Dayan. (1995). Does the wake-sleep algorithm produce good density estimators? Advances in neural information processing systems. 8. Genevay, A., G. Peyré, and M. Cuturi. (2018). Learning generative models with sinkhorn divergences. In: International Conference on Artificial Intelligence and Statistics. PMLR. 16081617. Geng, Z., M. Deng, X. Bai, J. Z. Kolter, and K. He. (2025a). Mean flows for one-step generative modeling. arXiv preprint arXiv:2505.13447. References 457 Geng, Z., A. Pokle, W. Luo, J. Lin, and J. Z. Kolter. (2025b). Consistency Models Made Easy. In: The Thirteenth International Conference on Learning Representations. url: https : / / openreview . net / forum ? id = xQVxo9dSID. Geng, Z., A. Pokle, W. Luo, J. Lin, and J. Z. Kolter. (2024). Consistency models made easy. arXiv preprint arXiv:2406.14548. Goodfellow, I., J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. (2014). Generative adversarial nets. Advances in neural information processing systems. 27. He, Y., N. Murata, C.-H. Lai, Y. Takida, T. Uesaka, D. Kim, W.-H. Liao, Y. Mitsufuji, J. Z. Kolter, R. Salakhutdinov, et al. (2023). Manifold preserving guided diffusion. In: International Conference on Learning Representations. He, Y., N. Murata, C.-H. Lai, Y. Takida, T. Uesaka, D. Kim, W.-H. Liao, Y. Mitsufuji, J. Z. Kolter, R. Salakhutdinov, and S. Ermon. (2024). Manifold Preserving Guided Diffusion. In: The Twelfth International Conference on Learning Representations. url: https://openreview.net/forum?id= o3BxOLoxm1. Heitz, E., L. Belcour, and T. Chambon. (2023). Iterative α-(de) blending: minimalist deterministic diffusion model. In: ACM SIGGRAPH 2023 Conference Proceedings. 18. Hertrich, J., A. Chambolle, and J. Delon. (2025). On the Relation between Rectified Flows and Optimal Transport. arXiv preprint arXiv:2505.19712. Hinton, G. E. (2002). Training products of experts by minimizing contrastive divergence. Neural computation. 14(8): 17711800. Ho, J., A. Jain, and P. Abbeel. (2020). Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems. 33: 6840 6851. Ho, J. and T. Salimans. (2021). Classifier-Free Diffusion Guidance. In: NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications. Hochbruck, M. and A. Ostermann. (2005). Explicit exponential RungeKutta methods for semilinear parabolic problems. SIAM Journal on Numerical Analysis. 43(3): 10691090. Hochbruck, M. and A. Ostermann. (2010). Exponential integrators. Acta Numerica. 19: 209286. Hu, Z., C.-H. Lai, Y. Mitsufuji, and S. Ermon. (2025). CMT: Mid-Training for Efficient Learning of Consistency, Mean Flow, and Flow Map Models. arXiv preprint arXiv:2509.24526. 458 References Huang, C.-W., R. T. Chen, C. Tsirigotis, and A. Courville. (2021). Convex Potential Flows: Universal Probability Distributions with Optimal Transport and Convex Optimization. In: International Conference on Learning Representations. Hutchinson, M. F. (1989). stochastic estimator of the trace of the influence matrix for Laplacian smoothing splines. Communications in StatisticsSimulation and Computation. 18(3): 10591076. Hyvärinen, A. and P. Dayan. (2005). Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research. 6(4). Iserles, A. (2009). first course in the numerical analysis of differential equations. No. 44. Cambridge university press. Karras, T., M. Aittala, T. Aila, and S. Laine. (2022). Elucidating the design space of diffusion-based generative models. Advances in Neural Information Processing Systems. 35: 2656526577. Karras, T., M. Aittala, J. Lehtinen, J. Hellsten, T. Aila, and S. Laine. (2023). Analyzing and improving the training dynamics of diffusion models. arXiv preprint arXiv:2312.02696. Karras, T., M. Aittala, J. Lehtinen, J. Hellsten, T. Aila, and S. Laine. (2024). Analyzing and Improving the Training Dynamics of Diffusion Models. In: IEEE Conference on Computer Vision and Pattern Recognition. IEEE. 2417424184. Kim, D., C.-H. Lai, W.-H. Liao, N. Murata, Y. Takida, T. Uesaka, Y. He, Y. Mitsufuji, and S. Ermon. (2024a). Consistency trajectory models: Learning probability flow ode trajectory of diffusion. In: International Conference on Learning Representations. Kim, D., C.-H. Lai, W.-H. Liao, Y. Takida, N. Murata, T. Uesaka, Y. Mitsufuji, and S. Ermon. (2024b). PaGoDA: Progressive Growing of OneStep Generator from Low-Resolution Diffusion Teacher. arXiv preprint arXiv:2405.14822. Kim, D., S. Shin, K. Song, W. Kang, and I.-C. Moon. (2022). Soft truncation: universal training technique of score-based diffusion model for high precision score estimation. In: International Conference on Machine Learning. PMLR. 1120111228. Kingma, D., T. Salimans, B. Poole, and J. Ho. (2021). Variational diffusion models. Advances in neural information processing systems. 34: 21696 21707. Kingma, D. P. and R. Gao. (2023). Understanding Diffusion Objectives as the ELBO with Simple Data Augmentation. In: Thirty-seventh Conference on Neural Information Processing Systems. References 459 Kingma, D. P. and M. Welling. (2013). Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114. Kloeden, P. E., E. Platen, P. E. Kloeden, and E. Platen. (1992). Stochastic differential equations. Springer. Lai, C.-H., Y. Takida, N. Murata, T. Uesaka, Y. Mitsufuji, and S. Ermon. (2023a). FP-Diffusion: Improving score-based diffusion models by enforcing the underlying score fokker-planck equation. In: International Conference on Machine Learning. PMLR. 1836518398. Lai, C.-H., Y. Takida, T. Uesaka, N. Murata, Y. Mitsufuji, and S. Ermon. (2023b). On the Equivalence of Consistency-Type Models: Consistency Models, Consistent Diffusion Models, and Fokker-Planck Regularization. arXiv preprint arXiv:2306.00367. Larochelle, H. and I. Murray. (2011). The neural autoregressive distribution estimator. In: Proceedings of the fourteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings. 2937. Lavenant, H. and F. Santambrogio. (2022). The flow map of the fokkerplanck equation does not provide optimal transport. Applied Mathematics Letters. 133: 108225. LeCun, Y., S. Chopra, R. Hadsell, M. Ranzato, F. Huang, et al. (2006). tutorial on energy-based learning. Predicting structured data. 1(0). Léonard, C. (2012). From the Schrödinger problem to the MongeKantorovich problem. Journal of Functional Analysis. 262(4): 18791920. Léonard, C. (2014). survey of the Schrödinger problem and some of its connections with optimal transport. Discrete and Continuous Dynamical Systems-Series A. 34(4): 15331574. Lipman, Y., R. T. Chen, H. Ben-Hamu, M. Nickel, and M. Le. (2022). Flow Matching for Generative Modeling. In: The Eleventh International Conference on Learning Representations. Lipman, Y., M. Havasi, P. Holderrieth, N. Shaul, M. Le, B. Karrer, R. T. Chen, D. Lopez-Paz, H. Ben-Hamu, and I. Gat. (2024). Flow matching guide and code. arXiv preprint arXiv:2412.06264. Liu, G.-H., A. Vahdat, D.-A. Huang, E. Theodorou, W. Nie, and A. Anandkumar. (2023). Iˆ2SB: Image-to-Image Schrödinger Bridge. In: International Conference on Machine Learning. PMLR. 2204222062. Liu, Q. (2022). Rectified flow: marginal preserving approach to optimal transport. arXiv preprint arXiv:2209.14577. 460 References Liu, X., C. Gong, et al. (2022). Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow. In: The Eleventh International Conference on Learning Representations. Lou, A., C. Meng, and S. Ermon. (2024). Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution. In: International Conference on Machine Learning. PMLR. 3281932848. Lu, C. and Y. Song. (2024). Simplifying, stabilizing and scaling continuoustime consistency models. arXiv preprint arXiv:2410.11081. Lu, C., K. Zheng, F. Bao, J. Chen, C. Li, and J. Zhu. (2022a). Maximum Likelihood Training for Score-based Diffusion ODEs by High Order Denoising Score Matching. In: International Conference on Machine Learning. PMLR. 1442914460. Lu, C., Y. Zhou, F. Bao, J. Chen, C. Li, and J. Zhu. (2022b). Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems. 35: 57755787. Lu, C., Y. Zhou, F. Bao, J. Chen, C. Li, and J. Zhu. (2022c). Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095. Luan, V. T. (2021). Efficient exponential RungeKutta methods of high order: construction and implementation. BIT Numerical Mathematics. 61(2): 535560. Luhman, E. and T. Luhman. (2021). Knowledge distillation in iterative generative models for improved sampling speed. arXiv preprint arXiv:2101.02388. Luo, W., T. Hu, S. Zhang, J. Sun, Z. Li, and Z. Zhang. (2023). Diff-instruct: universal approach for transferring knowledge from pre-trained diffusion models. Advances in Neural Information Processing Systems. 36: 76525 76546. Ma, N., M. Goldstein, M. S. Albergo, N. M. Boffi, E. Vanden-Eijnden, and S. Xie. (2024). Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In: European Conference on Computer Vision. Springer. 2340. Maoutsa, D., S. Reich, and M. Opper. (2020). Interacting particle solutions of fokkerplanck equations through gradientlogdensity estimation. Entropy. 22(8): 802. Meng, C., K. Choi, J. Song, and S. Ermon. (2022). Concrete score matching: Generalized score matching for discrete data. Advances in Neural Information Processing Systems. 35: 3453234545. References Meng, C., R. Rombach, R. Gao, D. Kingma, S. Ermon, J. Ho, and T. Salimans. (2023). On distillation of guided diffusion models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1429714306. Meng, C., Y. Song, W. Li, and S. Ermon. (2021a). Estimating high order gradients of the data distribution by denoising. Advances in Neural Information Processing Systems. 34: 2535925369. Meng, C., Y. Song, J. Song, J. Wu, J.-Y. Zhu, and S. Ermon. (2021b). Sdedit: Image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073. Mikami, T. and M. Thieullen. (2006). Duality theorem for the stochastic optimal control problem. Stochastic processes and their applications. 116(12): 18151835. Mikami, T. and M. Thieullen. (2008). Optimal transportation problem by stochastic optimal control. SIAM Journal on Control and Optimization. 47(3): 11271139. Mokady, R., A. Hertz, K. Aberman, Y. Pritch, and D. Cohen-Or. (2023). Null-text inversion for editing real images using guided diffusion models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 60386047. Neklyudov, K., R. Brekelmans, D. Severo, and A. Makhzani. (2023). Action matching: Learning stochastic dynamics from samples. In: International Conference on Machine Learning. PMLR. 2585825889. Nowozin, S., B. Cseke, and R. Tomioka. (2016). f-gan: Training generative neural samplers using variational divergence minimization. Advances in neural information processing systems. 29. Øksendal, B. (2003). Stochastic differential equations. In: Stochastic differential equations. Springer. 6584. Onken, D., S. W. Fung, X. Li, and L. Ruthotto. (2021). Ot-flow: Fast and accurate continuous normalizing flows via optimal transport. In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. No. 10. 92239232. Pavon, M. and A. Wakolbinger. (1991). On free energy, stochastic control, and Schrödinger processes. In: Modeling, Estimation and Control of Systems with Uncertainty: Proceedings of Conference held in Sopron, Hungary, September 1990. Springer. 334348. Peters, J., K. Mulling, and Y. Altun. (2010). Relative entropy policy search. In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 24. No. 1. 16071612. 462 References Peyré, G., M. Cuturi, et al. (2019). Computational optimal transport: With applications to data science. Foundations and Trends in Machine Learning. 11(5-6): 355607. Pontryagin, L. S. (2018). Mathematical theory of optimal processes. Routledge. Poole, B., A. Jain, J. T. Barron, and B. Mildenhall. (2023). DreamFusion: Text-to-3D using 2D Diffusion. In: The Eleventh International Conference on Learning Representations. Pra, P. D. and M. Pavon. (1990). On the Markov processes of Schrödinger, the Feynman-Kac formula and stochastic control. In: Realization and Modelling in System Theory: Proceedings of the International Symposium MTNS-89, Volume I. Springer. 497504. Radford, A., J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. (2021). Learning transferable visual models from natural language supervision. In: International conference on machine learning. PMLR. 87488763. Rafailov, R., A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn. (2023). Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems. 36: 5372853741. Raissi, M. (2018). Deep hidden physics models: Deep learning of nonlinear partial differential equations. Journal of Machine Learning Research. 19(25): 124. Reid, W. (1971). Ordinary Differential Equations. Applied mathematics series. Wiley. Rezende, D. and S. Mohamed. (2015). Variational inference with normalizing flows. In: International conference on machine learning. PMLR. 1530 1538. Saharia, C., W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes, B. Karagol Ayan, T. Salimans, et al. (2022). Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems. 35: 3647936494. Salimans, T. and J. Ho. (2021). Progressive Distillation for Fast Sampling of Diffusion Models. In: International Conference on Learning Representations. Särkkä, S. and A. Solin. (2019). Applied stochastic differential equations. Vol. 10. Cambridge University Press. Schulman, J., F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. (2017). Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347. References Shih, A., S. Belkhale, S. Ermon, D. Sadigh, and N. Anari. (2023). Parallel Sampling of Diffusion Models. arXiv preprint arXiv:2305.16317. Sinkhorn, R. (1964). relationship between arbitrary positive matrices and doubly stochastic matrices. The annals of mathematical statistics. 35(2): 876879. Sohl-Dickstein, J., E. Weiss, N. Maheswaranathan, and S. Ganguli. (2015). Deep unsupervised learning using nonequilibrium thermodynamics. In: International Conference on Machine Learning. PMLR. 22562265. Song, J., C. Meng, and S. Ermon. (2020a). Denoising Diffusion Implicit Models. In: International Conference on Learning Representations. Song, Y. and P. Dhariwal. (2024). Improved Techniques for Training Consistency Models. In: The Twelfth International Conference on Learning Representations. Song, Y., P. Dhariwal, M. Chen, and I. Sutskever. (2023). Consistency models. arXiv preprint arXiv:2303.01469. Song, Y., C. Durkan, I. Murray, and S. Ermon. (2021). Maximum likelihood training of score-based diffusion models. Advances in Neural Information Processing Systems. 34: 14151428. Song, Y. and S. Ermon. (2019). Generative modeling by estimating gradients of the data distribution. Advances in Neural Information Processing Systems. 32. Song, Y., S. Garg, J. Shi, and S. Ermon. (2020b). Sliced score matching: scalable approach to density and score estimation. In: Uncertainty in Artificial Intelligence. PMLR. 574584. Song, Y., J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole. (2020c). Score-Based Generative Modeling through Stochastic Differential Equations. In: International Conference on Learning Representations. Su, X., J. Song, C. Meng, and S. Ermon. (2022). Dual diffusion implicit bridges for image-to-image translation. arXiv preprint arXiv:2203.08382. Tabak, E. G. and E. Vanden-Eijnden. (2010). Density estimation by dual ascent of the log-likelihood. Commun. Math. Sci. 8(1): 217233. Tong, A., K. FATRAS, N. Malkin, G. Huguet, Y. Zhang, J. Rector-Brooks, G. Wolf, and Y. Bengio. (2024). Improving and generalizing flow-based generative models with minibatch optimal transport. Transactions on Machine Learning Research. Turner, C. V. (2013). family of nonparametric density estimation algorithms. Communications on Pure and Applied Mathematics. 66(2): 145 164. 464 References Uria, B., M.-A. Côté, K. Gregor, I. Murray, and H. Larochelle. (2016). Neural autoregressive distribution estimation. Journal of Machine Learning Research. 17(205): 137. Vahdat, A. and J. Kautz. (2020). NVAE: deep hierarchical variational autoencoder. Advances in neural information processing systems. 33: 1966719679. Villani, C. et al. (2008). Optimal transport: old and new. Vol. 338. Springer. Vincent, P. (2011). connection between score matching and denoising autoencoders. Neural computation. 23(7): 16611674. Wallace, B., M. Dang, R. Rafailov, L. Zhou, A. Lou, S. Purushwalkam, S. Ermon, C. Xiong, S. Joty, and N. Naik. (2024). Diffusion model alignment using direct preference optimization. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 82288238. Wang, Z., C. Lu, Y. Wang, F. Bao, C. Li, H. Su, and J. Zhu. (2023). Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. Advances in Neural Information Processing Systems. 36: 84068441. Xu, Y., M. Deng, X. Cheng, Y. Tian, Z. Liu, and T. S. Jaakkola. (2023). Restart Sampling for Improving Generative Processes. In: Thirty-seventh Conference on Neural Information Processing Systems. url: https : / / openreview.net/forum?id=wFuemocyHZ. Ye, H., H. Lin, J. Han, M. Xu, S. Liu, Y. Liang, J. Ma, J. Y. Zou, and S. Ermon. (2024). Tfg: Unified training-free guidance for diffusion models. Advances in Neural Information Processing Systems. 37: 2237022417. Yin, T., M. Gharbi, T. Park, R. Zhang, E. Shechtman, F. Durand, and B. Freeman. (2024a). Improved distribution matching distillation for fast image synthesis. Advances in neural information processing systems. 37: 4745547487. Yin, T., M. Gharbi, R. Zhang, E. Shechtman, F. Durand, W. T. Freeman, and T. Park. (2024b). One-Step Diffusion with Distribution Matching Distillation. In: 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE. 66136623. Yu, J., Y. Wang, C. Zhao, B. Ghanem, and J. Zhang. (2023). Freedom: Training-free energy-guided conditional diffusion model. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. 23174 23184. Zhang, Q. and Y. Chen. (2022). Fast Sampling of Diffusion Models with Exponential Integrator. In: The Eleventh International Conference on Learning Representations. References 465 Zheng, K., C. Lu, J. Chen, and J. Zhu. (2023). Dpm-solver-v3: Improved diffusion ode solver with empirical model statistics. Advances in Neural Information Processing Systems. 36: 5550255542. Zhou, M., H. Zheng, Z. Wang, M. Yin, and H. Huang. (2024). Score identity distillation: Exponentially fast distillation of pretrained diffusion models for one-step generation. In: Forty-first International Conference on Machine Learning."
        }
    ],
    "affiliations": [
        "Sony AI",
        "Sony Corporation"
    ]
}