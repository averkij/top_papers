{
    "paper_title": "Scalable Chain of Thoughts via Elastic Reasoning",
    "authors": [
        "Yuhui Xu",
        "Hanze Dong",
        "Lei Wang",
        "Doyen Sahoo",
        "Junnan Li",
        "Caiming Xiong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large reasoning models (LRMs) have achieved remarkable progress on complex tasks by generating extended chains of thought (CoT). However, their uncontrolled output lengths pose significant challenges for real-world deployment, where inference-time budgets on tokens, latency, or compute are strictly constrained. We propose Elastic Reasoning, a novel framework for scalable chain of thoughts that explicitly separates reasoning into two phases--thinking and solution--with independently allocated budgets. At test time, Elastic Reasoning prioritize that completeness of solution segments, significantly improving reliability under tight resource constraints. To train models that are robust to truncated thinking, we introduce a lightweight budget-constrained rollout strategy, integrated into GRPO, which teaches the model to reason adaptively when the thinking process is cut short and generalizes effectively to unseen budget constraints without additional training. Empirical results on mathematical (AIME, MATH500) and programming (LiveCodeBench, Codeforces) benchmarks demonstrate that Elastic Reasoning performs robustly under strict budget constraints, while incurring significantly lower training cost than baseline methods. Remarkably, our approach also produces more concise and efficient reasoning even in unconstrained settings. Elastic Reasoning offers a principled and practical solution to the pressing challenge of controllable reasoning at scale."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 5 1 3 5 0 . 5 0 5 2 : r a"
        },
        {
            "title": "Scalable Chain of Thoughts via Elastic Reasoning",
            "content": "Yuhui Xu Hanze Dong Lei Wang Doyen Sahoo Junnan Li Caiming Xiong"
        },
        {
            "title": "Abstract",
            "content": "Large reasoning models (LRMs) have achieved remarkable progress on complex tasks by generating extended chains of thought (CoT). However, their uncontrolled output lengths pose significant challenges for real-world deployment, where inference-time budgets on tokens, latency, or compute are strictly constrained. We propose Elastic Reasoning, novel framework for scalable chain of thoughts that explicitly separates reasoning into two phasesthinking and solutionwith independently allocated budgets. At test time, Elastic Reasoning prioritize that completeness of solution segments, significantly improving reliability under tight resource constraints. To train models that are robust to truncated thinking, we introduce lightweight budget-constrained rollout strategy, integrated into GRPO, which teaches the model to reason adaptively when the thinking process is cut short and generalizes effectively to unseen budget constraints without additional training. Empirical results on mathematical (AIME, MATH500) and programming (LiveCodeBench, Codeforces) benchmarks demonstrate that Elastic Reasoning performs robustly under strict budget constraints, while incurring significantly lower training cost than baseline methods. Remarkably, our approach also produces more concise and efficient reasoning even in unconstrained settings. The code will be made publicly available after completing the necessary review process."
        },
        {
            "title": "Introduction",
            "content": "Large reasoning models (LRMs) [4, 25] have demonstrated remarkable performance on complex reasoning tasks by producing extended Chain-of-Thought (CoT) outputs, which facilitate effective problem-solving in domains such as mathematics and programming. Reinforcement learning (RL) techniques [29, 43, 27, 5, 30], have been employed to optimize these reasoning trajectories, enabling LRMs to generate longer, more informative chains. These RL-driven methods scale effectively across diverse benchmarks [44, 6, 21, 39, 20], yielding substantial gains in both solution accuracy and robustness; while they often incur significantly longer inference chains [4, 7, 41, 26, 38]. Notably, the length of the reasoning trajectory remains uncontrolled, making it difficult to allocate fixed compute budget at inference time while maintaining desired performance level. Two primary lines of research have been proposed to address this challenge. The first, known as Long2Short [34, 14], seeks to reduce reasoning length through reinforcement learning with trajectory penalties or compression-aware fine-tuning, where the model is trained on shortened trajectories to preserve performance while minimizing inference cost. The second line of work focuses on length control [24, 1, 42]. S1 [24] introduces simple mechanism that prompts the model to emit special tokens (e.g., Wait, Final Answer) to regulate reasoning length. However, this approach significantly degrades performance, as it overlooks the critical role of the solution segment. L1 [1] proposes reinforcement learning framework that enforces explicit length constraints over the entire trajectory. While more flexible, this method demands substantial training resources and still results in noticeable performance degradation compared to the original model. Preprint. We propose Elastic Reasoning, simple yet effective method that enables large reasoning models to achieve scalable and adaptive length control. As illustrated in Figure 1, the S1 approachgenerating the answer by emitting special token such as Final Answerperforms better than directly truncating the full reasoning trajectory, underscoring the importance of preserving the solution segment. Motivated by this, we propose separate budgeting which explicitly divides the total token budget into two parts: tokens for the thinking phase and tokens for the solution phase, where = + s. Once the model consumes tokens in the thinking phase, we forcibly terminate it by appending the special token </think> and transition to solution generation. Separate budgeting outperforms S1 under varying generation budgets. To further improve solution quality under incomplete reasoning, we introduce novel training strategy called budget-constrained rollout, which teaches the model to generate high-quality answers even with partial CoT trajectories. This method is integrated into GRPO training and is highly efficientrequiring only 200 training steps on math tasks with maximum response length of 2K tokens (t = 1K, = 1K), compared to 700 steps for L1-Exact and 820 steps for L1-Max with 4K response length. Moreover, models trained with Elastic Reasoning generalize effectively to arbitrary reasoning budgets without the need for further fine-tuning. Figure 1: Separating thinking and solution phases enables better length control. We evaluate Elastic Reasoning on both mathematical and programming reasoning tasks, introducing two models: E1-Math-1.5B and E1Code-14B. (1) E1-Math-1.5B outperforms both L1-Exact and S1, and achieves performance comparable to L1-Max, while requiring significantly fewer training steps. For instance, on the AIME2024 dataset, our method achieves 35.0% accuracy, compared to 27.1% for L1-Max, 24.2% for L1-Exact, and 41.0% for the original model. (2) E1-Code-14B demonstrates strong scaling with varying inference budgets, achieving Codeforces rating of 1987 and placing in the 96.0 percentilecomparable to O1-2024-12-17 (Low), which scores 1991 and ranks in the 96.1 percentile. (3) surprising observation is that, after training, the trajectories generated by our models are significantly shorter than those from the original DeepScaleR and DeepCoder models across both math and code tasks. This suggests that budget-constrained rollout not only improves length control but also encourages the model to reason more concisely and generate more efficient solutions."
        },
        {
            "title": "2 Related Works",
            "content": "2.1 Test-Time Scaling in Large Language Models Increasing computation during inference, often referred to as test-time scaling (TTS), has been shown to improve the reasoning capabilities of LLMs [36, 35, 33, 4, 34, 24]. Early works, such as chain-of-thought prompting [36], show that producing series of intermediate reasoning steps significantly improves LLMs performance on complex reasoning tasks. Building on this, selfconsistency [35] further boosts performance by sampling diverse set of reasoning paths and selecting the most consistent answer. Recent studies have formalized these findings into test-time inference scaling laws [33, 37]. Wu et al. [37] explore the trade-offs between model size and inference-time computation. Snell et al. [33] investigated how fixed but non-trivial inference-time budgets can significantly boost LLM performance. The remarkable successes of advanced reasoning models, such as o1 [25] and R1 [4], have further amplified interest in leveraging TTS techniques. While much of the existing works primarily focuses on improving performance by increasing inference-time computation, our work takes different perspective: How can we enable LLMs to perform effective long reasoning under strict output length constraints? 2 2.2 Length Control in Large Language models Controlling the generation length of an LLM directly affects both latency and monetary cost at inference time. Earlier approaches to length control are designed mainly for general text generation [13, 42]. Typical methods include (i) manipulating positional encodings to achieve exact sequence lengths [3], (ii) modifying training objectives to penalize deviations from length targets [13, 32], and (iii) fine-tuning on instructions that explicitly state the desired output length [42]. Although effective for tasks such as summarization or constrained writing, these techniques generally aim to verbosity or enforce maximum-length limits, and overlook the intricate, step-by-step reasoning processes required for many reasoning tasks. Recent works have begun to exploring efficiency in reasoning by encouraging shorter chains [14, 2]; however, they typically lack mechanisms for precise, user-defined length targets that align with explicit compute budgets. One notable attempt, budget forcing [24], enforces strict token caps by truncating or padding with special tokens. This can yield incomplete reasoning or unnatural, forced outputs, ultimately harming both accuracy and interpretability. Additionally, L1 [1] uses reinforcement learning to let models dynamically allocate inference compute based on constraints provided in the prompt. Our approach doesnt need to include length instructions in the prompt. Instead, we truncate reasoning trajectories to meet given budget and train the model under these constraints via reinforcement learning. 2.3 Efficient Reasoning in Large Language Models Making complex reasoning in LLMs more efficient, particularly by shortening the reasoning process, is crucial to reducing computational costs and making these models practical for real-world deployment. This has become vibrant research area with several promising directions to encourage more concise and effective reasoning strategies [14, 40, 10, 17, 19]. One common strategy involves incorporating explicit rewards into RL to encourage the model to find shorter reasoning paths [34, 19]. Some focus on creating datasets with examples of concise reasoning paths and then using SFT teach models how to generate compact and knowledgeable reasoning steps [14, 41]. Instead of relying solely on explicit textual reasoning, methods exploring latent reasoning aim to compress these intermediate steps into more compact, internal representations [10, 31, 28]. Efficiency can also be improved during inference, without needing to retrain the model. These training-free techniques dynamically adapt the reasoning strategy based on the specific input or task demands [17, 8]. In this work, we introduce training approach using reinforcement learning under strict budget constraints to encourage the model to balance reasoning quality with cost efficiency."
        },
        {
            "title": "3 Methodology",
            "content": "3.1 Preliminaries: Reasoning Language Models We consider reasoning-augmented language models that generate outputs consisting of two distinct segments: thinking part and solution part. Following prior work, we denote the reasoning phase using special tokens such as <think> and </think> to explicitly mark the models intermediate thoughts. Formally, given an input prompt x, the model generates an output sequence = (ythink, ysolution), where ythink contains the intermediate reasoning steps (enclosed between <think> and </think>) and ysolution contains the final solution. Typically, ythink accounts for over 90% of the total tokens, while ysolution provides concise summary and final answer. The overall generation structure is: = (<think> intermediate reasoning </think>, solution) 3.2 Elastic Reasoning 3.2.1 Budget-Constrained Inference In many real-world applications, inference cost must be carefully controlled due to constraints on latency, computation, or memory. common approach is to truncate generation after fixed number of tokens c, enforcing: 3 Figure 2: The framework of Elastic Reasoning. Elastic Reasoning comprises two key components: (1) GRPO training with budget-constrained rollout, and (2) separate budgeting for inference. Left: During training, the model is optimized using GRPO under fixed token budget (t, s). Right: At inference time, the trained E1 model can generalize to arbitrary token budgets ci = ti + s, enabling flexible and efficient reasoning. where denotes the number of generated tokens. However, naïvely truncating the output often results in incomplete or missing ysolution, leading to invalid or unusable predictions. 3.2.2 Separate Budgeting for Thinking and Solution To address this limitation, we propose Separate Budgeting, method that explicitly allocates independent budgets for the reasoning and solution phases. key observation is that even when the reasoning phase is forcibly terminated (e.g., by inserting </think>), the model is still capable of producing coherentand often correctsolution. Given total generation budget c, we divide it into two components: budget for the thinking phase and budget for the solution phase, such that = + s. During inference: The model begins generating thinking segment within <think> block. If the model emits </think> before reaching the budget t, we transition immediately to the solution phase. If the budget is exhausted before </think> is emitted, we forcibly terminate the reasoning by appending </think>. The model then continues generating the solution segment, up to maximum of tokens. This approach ensures that both the reasoning and solution components are explicitly accommodated within the total budget c, thereby avoiding unintended truncation of the solution segment. The thinking budget can be flexibly adjusted at inference time to match different application scenarios, while the solution phase always retains guaranteed allocation. As shown in Figure 1, Separate Budgeting outperforms both vanilla budgeting (naïve truncation) and S1 (budget forcing). By dedicating fixed token budget for solution generation, Separate Budgeting significantly improves the reliability and quality of model outputs under tight inference-time constraints. 3.2.3 Budget-Constrained Rollout While Separate Budgeting ensures dedicated budgets for both reasoning and solution phases, we observe that naively truncating the thinking partespecially on complex tasks such as code generationcan lead to significant performance degradation. To mitigate this issue, we propose reinforcement learning (RL) fine-tuning procedure that explicitly trains the model under reason- (a) Pass@1 on AIME2024 over steps (b) Reward over steps Figure 3: Validation accuracy and reward curves of E1-Math-1.5B over training steps. ing budget constraints, allowing it to produce more effective and concise reasoning within limited budgets. We adopt GRPO as our RL algorithm. Let πθ denote the policy of language model parameterized by θ, which generates response = (ythink, ysolution) for given input x, subject to total budget constraint + = c. During training, we simulate the Separate Budgeting procedure used at inference time: the policy rolls out reasoning segment ythink up to maximum of tokens. If the model emits the </think> token before reaching this limit, it proceeds to generate the solution segment as usual. Otherwise, we forcibly append </think> once the budget is reached. The model then generates the solution segment ysolution using the remaining tokens. Let r(y) denote task-specific reward function. The training objective is to maximize the expected reward: J(θ) = ExD, yπθ(x; t,s) [r(y)] We optimize J(θ) using GRPO with the following gradient estimator: θJ(θ) = Ex,y [A(x, y)θ log πθ(y x; t, s)] , A(x, y) = r(y) Eyπθ(x; t,s)[r(y)] (cid:112)Vyπθ(x)[r(y)] In our training setup, we fix the budget pair to (t, s) = (1K, 1K) for simplicity and efficiency. Surprisingly, we find that the learned policy generalizes well to wide range of unseen budget configurations at test time, without requiring any additional fine-tuning. As shown in Figure 1, the E1-Math-1.5B model achieves substantial improvements and generalizes robustly across various generation budgets. This indicates that Elastic Reasoning encourages the model to internalize flexible reasoning strategy that adapts to different resource constraints. This RL-based adaptation helps the model prioritize informative reasoning content earlier in the generation process, thereby improving both robustness and solution quality under test-time truncation."
        },
        {
            "title": "4 Experiment Results",
            "content": "4.1 Models and Datasets Our base models are DeepScaleR-1.5B-Preview [21] and DeepCoder-14B-Preview [20], which are fine-tuned from DeepSeekR1-Distill-Qwen-1.5B and 14B [4] through iterative context lengthening. For training data, we follow the same datasets used in [21, 20]. In the math domain, the training set consists of AIME (1984-2023), AMC, Omni-Math [9], and STILL [23]. For code training, we use TACO [16], SYNTHETIC-1 [22], and LiveCodeBench (2023/05/01-2024/07/31) [12]. For evaluation, we use AIME 2024, MATH500 [11], AMC, Olympiad-Bench [9], and Minerva Math [15] for mathematical reasoning. For code-related tasks, we evaluate on LiveCodeBench (2024/08/012025/02/01) [12], Codeforces, and HumanEval+ [18]. 5 Figure 4: Comparison of E1-Math-1.5B with L1 and S1 baselines under varying generation budgets. 4.2 Mathematical Reasoning Results We visualize the reward and validation Pass@1 performance on AIME2024 every 10 steps during training in Figure 3. It can be observed that the reward steadily increases during the initial training phase and begins to converge after approximately the 150th step. Meanwhile, the validation accuracy (Pass@1) improves rapidly, rising from around 0.07 to 0.20 over the course of training. This demonstrates that, through budget-constrained rollout, the model can quickly learn to reason effectively when the thinking phase is incomplete. We report Pass@1 accuracy versus the number of tokens used across five math benchmarks AIME, AMC, Olympiad-Bench, MATH500, and Minerva Math in Figure 4. Our proposed method, E1-Math1.5B, under both budget-constrained and 24K-token settings (red stars), consistently outperforms S1 (Budget Forcing) and L1-Exact, and performs competitively with L1-Max, while requiring significantly fewer training steps. On MATH500, E1-Math-1.5B achieves Pass@1 accuracy of 83.6% using only 1619 tokens per question, whereas L1-Exact and L1-Max yield lower or comparable performance with more tokens (L1-Exact: 79.9% with 1959 tokens; L1-Max: 83.6% with 1796 tokens). Notably, when evaluated without inference-time budget constraints, E1-Math-1.5B achieves higher accuracy than all baseline methods across all benchmarks. For example, on AIME2024, E1Math-1.5B exhibits performance degradation of only 6.0% relative to the original model, compared to 12.9% for L1-Max and 16.8% for L1-Exact. These results demonstrate that our method is not only effective in enforcing inference-time budget constraints but also preserves most of the original models performance. When compared with the original DeepScaleR-1.5B, E1-Math-1.5B reduces the average number of tokens used across datasets by more than 30%, including 32.1% reduction on AIME2024. Furthermore, similar to L1, S1, and O1, we observe clear log-linear scaling pattern in E1: performance improves approximately linearly with respect to the logarithm of the number of generated reasoning tokens. 4.3 Code Reasoning Results As shown in Figure 5, we visualize the Pass@1 accuracy on LiveCodeBench under varying generation budgets, comparing our method to simple separate budgeting strategy for thinking and solution. We observe that the original DeepCoder-14B-Preview fails to generate correct outputs when reasoning is incomplete, consistently achieving less than 10% accuracy when inference budget is less than 4K even using separate budgeting. In contrast, our E1-Code-14B model demonstrates impressive scalability: 6 Table 1: Comparison of models across LiveCodeBench, Codeforces, HumanEval+, and AIME benchmarks. E1-code-14B variants trained exclusively on code data; their AIME scores, obtained on math problems unseen during training, demonstrate that E1-code-14B retains strong math performance. Model LiveCodeBench Codeforces Rating Codeforces Percentile HumanEval+ AIME O1-2024-12-17 (Low) O3-Mini-2025-1-31 (Low) O1-Preview DeepSeek-R1 DeepSeek-R1-Distill-Qwen-14B DeepCoder-14B-Preview1 E1-code-14B (t = 1k, = 1k) E1-code-14B (t = 2k, = 1k) E1-code-14B (t = 3k, = 1k) E1-code-14B (t = 4k, = 1k) E1-code-14B 59.5 60.9 42.7 62.8 53.0 58.1 37.3 41.6 44.1 47.0 58.4+0.3 1991 1918 1658 1948 1791 1945 1457 1604 1711 1771 1987+ 96.1 94.9 88.5 95.4 92.7 95.4 78.1 85.4 90.6 92.3 96.0+0.6 90.8 92.6 89.0 92.6 92.0 90.8 88.3 89.6 90.8 92.0 91.4+0.6 74.4 60.0 40.0 79.8 69.7 71.7 17.9 28.5 35.4 41.9 70. its performance improves steadily as the inference budget increases, highlighting the effectiveness of our training strategy in enabling the model to reason adaptively under constrained thinking. Notably, E1-Code-14B also achieves performance improvement of 0.3% on LiveCodeBench even in the unconstrained setting, while simultaneously reducing the average number of generated tokens by 37.4%from 17,815 to 11,145 tokens. This indicates that our method not only scales well with inference budgets but also promotes more concise and efficient reasoning. In Table 1, we report the performance of E1-Code-14B on four benchmarks: LiveCodeBench, Codeforces, HumanEval Plus, and AIME2024. We observe consistent test-time scaling behavior across all benchmarks under constrained inference budgets. Beyond scalability, our model also demonstrates strong performance in the unconstrained setting. Specifically, we observe performance improvements on LiveCodeBench, Codeforces, and HumanEval Plus, and only slight performance drop on AIME2024. On Codeforces, E1-Code-14B achieves 42-point improvement in rating and 0.6 percentile gain, outperforming O3-Mini2025-1-31 (Low) and performing comparably to O1-2024-12-17 (Low). These results highlight that our method not only enables efficient, budget-constrained reasoning but also enhances overall reasoning capability, even in unconstrained scenarios. 4.4 Analysis and Discussions 4.4.1 Which part is enhanced after training? Figure 5: Pass@1 accuracy on LiveCodeBench under varying reasoning budgets. Both of the models inference with separate budgeting strategy. To better understand which components of the reasoning process are enhanced through training, we conduct ablation experiments on DeepScaleR-1.5B-Preview and E1-Math-1.5B using the AIME2024 benchmark. Specifically, we separately generate the thinking and solution segments using both models under varying generation budgets. For example, we use DeepScaleR-1.5B-Preview to generate the thinking part, and then use E1-Math-1.5B to generate the corresponding solution based on that reasoning. This setup allows us to isolate the contributions of each model to the reasoning pipeline and assess how training improves each component. As shown in Table 2, we observe that both the thinking and solution are enhanced after training. Notably, the improvement in the solution component is more substantial, particularly under constrained thinking budgets. For instance, using the E1 model to generate only the solution segment yields 1Results are reproduced using the authors official code and model with the same evaluation protocol. 7 Table 2: Ablation of enhanced thinking and solution on DeepScaleR-1.5B-Preview and E1-Math-1.5. Budget is in format thinking+solution (in thousands of tokens). DeepScalaR-1.5B E1-Math-1.5B Pass@1 (%) Thinking Solution Thinking Solution 0.5K+1K 1K+1K 2K+1K 3K+1K 2.10 3.50+1.4 10.8+8.7 13.5+11. 4.80 7.90+3.1 14.2+9.4 17.5+12.7 12.5 20.6+8.1 21.9+9.4 24.8+12.3 20.0 24.0+4.0 26.4+6.4 27.9+7.9 Figure 6: Comparison of E1-Math-1.5B with L1 and S1 baselines under varying generation budgets. an 8.7% gain in accuracy compared to using the original DeepScaleR model, under generation budget of (0.5K + 1K) tokens. This highlights the effectiveness of budget-constrained rollout in strengthening the models ability to produce high-quality solutions based on incomplete reasoning. This observation also helps explain why training with fixed budget constraint (e.g., (1K, 1K)) enables the model to generalize effectively to wide range of budget configurations. We hypothesize that the improvement in solution generation plays central role in this generalization, allowing the model to adapt even when the available thinking tokens are reduced. 4.4.2 Ablation of Training Budget To further investigate the role of the thinking budget in our proposed budget-constrained rollout, we conduct experiments to evaluate the models performance under four settings: {0.5K, 1K, 2K, 3K}, while keeping the solution budget fixed at = 1K. We evaluate on five math benchmarks: AIME, AMC, Olympiad-Bench, MATH500, and Minerva Math (Figure 6). Across all configurations, the model demonstrates strong generalization to varying inference budgets on all benchmarks. Among the tested values, = 1K consistently achieves the best performance, while also maintaining low maximum generation length of 2K tokens, making it highly efficient and effective setting. Based on this trade-off between performance and computational cost, we adopt (t = 1K, = 1K) as our default configuration. 4.4.3 Thinking Solution Distribution Figure 7 visualizes the distribution of thinking and solution tokens within generated trajectories under different generation budget constraints. We select AIME2024 for the math task and LiveCodeBench for the coding task. 8 Figure 7: Distribution of tokens for thinking and solution across different generation budgets. For AIME2024, as the inference budget decreases, the number of tokens used in the thinking segment decreases accordingly, while the number of tokens in the solution segment slightly increases. similar trend is observed on LiveCodeBench, where the thinking tokens decrease with tighter budgets, while the number of solution tokens remains relatively stable. Notably, even when evaluated without budget constraints, our trained E1 models demonstrate substantial token efficiency: they reduce total token usage by 32.1% on AIME2024 and 37.4% on LiveCodeBench, while maintaining strong performance (even slightly better than the baseline model). This suggests that the model has learned to reason more concisely and generate efficient solutions post training. 4.4.4 Iterative Training Table 3: Pass@1(%) on AIME 2024 across different budget configurations in two iterations Iteration 1st 2nd 0.5K+1K 1K+1K 2K+1K 3K+1K 13.5 11.5 17.5 17.1 24.8 22.9 27.9 26. In this section, we investigate whether the model can benefit from iterative trainingthat is, performing second round of training with expanded budgets after initial training. Specifically, we first train the model using budget of (t = 1K, = 1K), and then continue training from the resulting checkpoint using larger thinking budget of (t = 3K, = 1K). The results on AIME2024 are reported in Table 3. Surprisingly, the second round of training does not improve performance. In fact, we observe slight degradation in accuracy, suggesting that once the model has learned to reason effectively under shorter budget, further training with longer budget may not provide additional benefits."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduce Elastic Reasoning, unified framework for enabling large reasoning models to generate accurate and efficient chain-of-thought outputs under strict inference-time constraints. By explicitly separating the reasoning process into thinking and solution phases, and training with novel budget-constrained rollout strategy, our approach ensures robustness to truncated reasoning while preserving or even improving overall performance. Elastic Reasoning significantly reduces token usage during inference, generalizes across unseen budget configurations, and outperforms prior length control baselines in both mathematical and programming domains. Our findings offer scalable and principled solution for real-world deployment of reasoning LLMs where computation budgets are limited. We believe this framework opens new directions for budget-aware reasoning."
        },
        {
            "title": "References",
            "content": "[1] Pranjal Aggarwal and Sean Welleck. L1: Controlling how long reasoning model thinks with reinforcement learning. URL 2025. [2] Daman Arora and Andrea Zanette. Training language models to reason efficiently, 2025. URL https://arxiv.org/abs/2502.04463. [3] Bradley Butcher, Michael OKeefe, and James Titchener. Precise length control in large language models, 2024. URL https://arxiv.org/abs/2412.11937. [4] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. [5] Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. Raft: Reward ranked finetuning for generative foundation model alignment. arXiv preprint arXiv:2304.06767, 2023. [6] Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo, Caiming Xiong, and Tong Zhang. Rlhf workflow: From reward modeling to online rlhf. arXiv preprint arXiv:2405.07863, 2024. [7] Yifan Du, Zikang Liu, Yifan Li, Wayne Xin Zhao, Yuqi Huo, Bingning Wang, Weipeng Chen, Zheng Liu, Zhongyuan Wang, and Ji-Rong Wen. Virgo: preliminary exploration on reproducing o1-like mllm. arXiv preprint arXiv:2501.01904, 2025. [8] Yichao Fu, Junda Chen, Yonghao Zhuang, Zheyu Fu, Ion Stoica, and Hao Zhang. Reasoning without self-doubt: More efficient chain-of-thought through certainty probing. In ICLR 2025 Workshop on Foundation Models in the Wild, 2025. [9] Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, Zhengyang Tang, Benyou Wang, Daoguang Zan, Shanghaoran Quan, Ge Zhang, Lei Sha, Yichang Zhang, Xuancheng Ren, Tianyu Liu, and Baobao Chang. Omni-math: universal olympiad level mathematic benchmark for large language models, 2024. URL https://arxiv.org/abs/2410.07985. 10 [10] Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. Training large language models to reason in continuous latent space. arXiv preprint arXiv:2412.06769, 2024. [11] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset, 2021. URL https://arxiv.org/abs/2103.03874. [12] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code, 2024. URL arXivpreprintarXiv: 2403.07974. [13] Renlong Jie, Xiaojun Meng, Lifeng Shang, Xin Jiang, and Qun Liu. Prompt-based length controlled generation with reinforcement learning. arXiv preprint arXiv:2308.12030, 2023. [14] Yu Kang, Xianghui Sun, Liangyu Chen, and Wei Zou. C3ot: Generating shorter chain-of-thought without compromising effectiveness, 2024. URL https://arxiv.org/abs/2412.11664. [15] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models, 2022. URL arXivpreprintarXiv: 2206.14858. [16] Rongao Li, Jie Fu, Bo-Wen Zhang, Tao Huang, Zhihong Sun, Chen Lyu, Guang Liu, Zhi Jin, and Ge Li. Taco: Topics in algorithmic code generation dataset, 2023. URL arXivpreprintarXiv: 2312.14852. [17] Baohao Liao, Yuhui Xu, Hanze Dong, Junnan Li, Christof Monz, Silvio Savarese, Doyen Sahoo, and Caiming Xiong. Reward-guided speculative decoding for efficient llm reasoning. arXiv preprint arXiv:2501.19324, 2025. [18] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatGPT really correct? rigorous evaluation of large language models for code generation, 2023. URL https://openreview.net/forum?id=1qvx610Cu7. [19] Haotian Luo, Li Shen, Haiying He, Yibo Wang, Shiwei Liu, Wei Li, Naiqiang Tan, Xiaochun Cao, and Dacheng Tao. O1-pruner: Length-harmonizing fine-tuning for o1-like reasoning pruning. arXiv preprint arXiv:2501.12570, 2025. [20] Michael Luo, Xiaoxiang Shi Sijun Tan, Roy Huang, Rachel Xin, Colin Cai, Ameen Patel, Alpay Ariyak, Qingyang Wu, Ce Zhang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepcoder: fully open-source 14b coder at o3-mini level. https://pretty-radio-b75.notion.site/ DeepCoder-A-Fully-Open-Source-14B-Coder-at-O3-mini-Level-1cf81902c14680b3bee5eb349a512a51, 2025. Notion Blog. [21] Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, et al. Deepscaler: Surpassing O1-preview with 1.5 model by scaling reinforcement learning, 2025. URL https://pretty-radio-b75.notion.site/ DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2. Notion blog post. [22] Justus Mattern, Sami Jaghouar, Manveer Basra, Jannik Straube, Matthew Di Ferrante, Felix Gabriel, Jack Min Ong, Vincent Weisser, and Johannes Hagemann. Synthetic-1: Two million collaboratively generated reasoning traces from deepseek-r1, 2025. URL https: //www.primeintellect.ai/blog/synthetic-1-release. [23] Yingqian Min, Zhipeng Chen, Jinhao Jiang, Jie Chen, Jia Deng, Yiwen Hu, Yiru Tang, Jiapeng Wang, Xiaoxue Cheng, Huatong Song, Wayne Xin Zhao, Zheng Liu, Zhongyuan Wang, and Ji-Rong Wen. Imitate, explore, and self-improve: reproduction report on slow-thinking reasoning systems, 2024. URL https://arxiv.org/abs/2412.09413. [24] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling, 2025. URL https://arxiv.org/abs/2501.19393. [25] OpenAI, :, Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko, Alex Tachard Passos, Alexander Neitz, Alexander Prokofiev, Alexander Wei, Allison Tam, Ally Bennett, Ananya Kumar, Andre Saraiva, Andrea Vallone, Andrew Duberstein, Andrew Kondrich, Andrey Mishchenko, Andy Applebaum, Angela Jiang, Ashvin Nair, Barret Zoph, Behrooz Ghorbani, Ben Rossen, Benjamin Sokolowsky, Boaz Barak, Bob McGrew, Borys Minaiev, Botao Hao, Bowen Baker, Brandon Houghton, Brandon McKinzie, Brydon Eastman, Camillo Lugaresi, Cary Bassin, Cary Hudson, Chak Ming Li, Charles de Bourcy, Chelsea Voss, Chen Shen, Chong Zhang, Chris Koch, Chris Orsinger, Christopher Hesse, Claudia Fischer, Clive Chan, Dan Roberts, Daniel Kappler, Daniel Levy, Daniel Selsam, David Dohan, David Farhi, David Mely, David Robinson, Dimitris Tsipras, Doug Li, Dragos Oprica, Eben Freeman, Eddie Zhang, Edmund Wong, Elizabeth Proehl, Enoch Cheung, Eric Mitchell, Eric Wallace, Erik Ritter, Evan Mays, Fan Wang, Felipe Petroski Such, Filippo Raso, Florencia Leoni, Foivos Tsimpourlas, Francis Song, Fred von Lohmann, Freddie Sulit, Geoff Salmon, Giambattista Parascandolo, Gildas Chabot, Grace Zhao, Greg Brockman, Guillaume Leclerc, Hadi Salman, Haiming Bao, Hao Sheng, Hart Andrin, Hessam Bagherinezhad, Hongyu Ren, Hunter Lightman, Hyung Won Chung, Ian Kivlichan, Ian OConnell, Ian Osband, Ignasi Clavera Gilaberte, Ilge Akkaya, Ilya Kostrikov, Ilya Sutskever, Irina Kofman, Jakub Pachocki, James Lennon, Jason Wei, Jean Harb, Jerry Twore, Jiacheng Feng, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joaquin Quiñonero Candela, Joe Palermo, Joel Parish, Johannes Heidecke, John Hallman, John Rizzo, Jonathan Gordon, Jonathan Uesato, Jonathan Ward, Joost Huizinga, Julie Wang, Kai Chen, Kai Xiao, Karan Singhal, Karina Nguyen, Karl Cobbe, Katy Shi, Kayla Wood, Kendra Rimbach, Keren Gu-Lemberg, Kevin Liu, Kevin Lu, Kevin Stone, Kevin Yu, Lama Ahmad, Lauren Yang, Leo Liu, Leon Maksin, Leyton Ho, Liam Fedus, Lilian Weng, Linden Li, Lindsay McCallum, Lindsey Held, Lorenz Kuhn, Lukas Kondraciuk, Lukasz Kaiser, Luke Metz, Madelaine Boyd, Maja Trebacz, Manas Joglekar, Mark Chen, Marko Tintor, Mason Meyer, Matt Jones, Matt Kaufer, Max Schwarzer, Meghan Shah, Mehmet Yatbaz, Melody Y. Guan, Mengyuan Xu, Mengyuan Yan, Mia Glaese, Mianna Chen, Michael Lampe, Michael Malek, Michele Wang, Michelle Fradin, Mike McClay, Mikhail Pavlov, Miles Wang, Mingxuan Wang, Mira Murati, Mo Bavarian, Mostafa Rohaninejad, Nat McAleese, Neil Chowdhury, Neil Chowdhury, Nick Ryder, Nikolas Tezak, Noam Brown, Ofir Nachum, Oleg Boiko, Oleg Murk, Olivia Watkins, Patrick Chao, Paul Ashbourne, Pavel Izmailov, Peter Zhokhov, Rachel Dias, Rahul Arora, Randall Lin, Rapha Gontijo Lopes, Raz Gaon, Reah Miyara, Reimar Leike, Renny Hwang, Rhythm Garg, Robin Brown, Roshan James, Rui Shu, Ryan Cheu, Ryan Greene, Saachi Jain, Sam Altman, Sam Toizer, Sam Toyer, Samuel Miserendino, Sandhini Agarwal, Santiago Hernandez, Sasha Baker, Scott McKinney, Scottie Yan, Shengjia Zhao, Shengli Hu, Shibani Santurkar, Shraman Ray Chaudhuri, Shuyuan Zhang, Siyuan Fu, Spencer Papay, Steph Lin, Suchir Balaji, Suvansh Sanjeev, Szymon Sidor, Tal Broda, Aidan Clark, Tao Wang, Taylor Gordon, Ted Sanders, Tejal Patwardhan, Thibault Sottiaux, Thomas Degry, Thomas Dimson, Tianhao Zheng, Timur Garipov, Tom Stasi, Trapit Bansal, Trevor Creech, Troy Peterson, Tyna Eloundou, Valerie Qi, Vineet Kosaraju, Vinnie Monaco, Vitchyr Pong, Vlad Fomenko, Weiyi Zheng, Wenda Zhou, Wes McCabe, Wojciech Zaremba, Yann Dubois, Yinghai Lu, Yining Chen, Young Cha, Yu Bai, Yuchen He, Yuchen Zhang, Yunyun Wang, Zheng Shao, and Zhuohan Li. Openai o1 system card, 2024. URL https://arxiv.org/abs/2412.16720. [26] Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector Liu, Yuanzhi Li, et al. O1 replication journey: strategic progress reportpart 1. arXiv preprint arXiv:2410.18982, 2024. [27] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. [28] Nikunj Saunshi, Nishanth Dikkala, Zhiyuan Li, Sanjiv Kumar, and Sashank Reddi. Reasoning with latent thoughts: On the power of looped transformers. arXiv preprint arXiv:2502.17416, 2025. [29] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [30] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [31] Xuan Shen, Yizhou Wang, Xiangxi Shi, Yanzhi Wang, Pu Zhao, and Jiuxiang Gu. Efficient reasoning with hidden thinking. arXiv preprint arXiv:2501.19201, 2025. [32] Prasann Singhal, Tanya Goyal, Jiacheng Xu, and Greg Durrett. long way to go: Investigating length correlations in rlhf, 2024. URL https://arxiv.org/abs/2310.03716. [33] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters, 2024. URL https://arxiv. org/abs/2408.03314. [34] Kimi Team, Du, Gao, Xing, Jiang, Chen, Li, Xiao, Du, Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms, 2025. URL https://arxiv.org/abs/2501. 12599. [35] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models, 2023. URL https://arxiv.org/abs/2203.11171. [36] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023. URL https://arxiv.org/abs/2201.11903. [37] Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models, 2024. URL https://arxiv.org/abs/2408.00724. [38] Wei Xiong, Jiarui Yao, Yuhui Xu, Bo Pang, Lei Wang, Doyen Sahoo, Junnan Li, Nan Jiang, Tong Zhang, Caiming Xiong, et al. minimalist approach to llm reasoning: from rejection sampling to reinforce. arXiv preprint arXiv:2504.11343, 2025. [39] Wei Xiong, Hanning Zhang, Chenlu Ye, Lichang Chen, Nan Jiang, and Tong Zhang. Selfrewarding correction for mathematical reasoning. arXiv preprint arXiv:2502.19613, 2025. [40] Yuhui Xu, Zhanming Jie, Hanze Dong, Lei Wang, Xudong Lu, Aojun Zhou, Amrita Saha, Caiming Xiong, and Doyen Sahoo. Think: Thinner key cache by query-driven pruning. arXiv preprint arXiv:2407.21018, 2024. [41] Ping Yu, Jing Xu, Jason Weston, and Ilia Kulikov. Distilling system 2 into system 1. arXiv preprint arXiv:2407.06023, 2024. [42] Weizhe Yuan, Ilia Kulikov, Ping Yu, Kyunghyun Cho, Sainbayar Sukhbaatar, Jason Weston, and Jing Xu. Following length constraints in instructions, 2024. URL https://arxiv.org/ abs/2406.17744. [43] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:1547615488, 2022. [44] Yuxiang Zhang, Shangxi Wu, Yuqi Yang, Jiangming Shu, Jinlin Xiao, Chao Kong, and Jitao Sang. o1-coder: an o1 replication for coding. arXiv preprint arXiv:2412.00154, 2024."
        }
    ],
    "affiliations": []
}