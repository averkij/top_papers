{
    "paper_title": "Evolution and The Knightian Blindspot of Machine Learning",
    "authors": [
        "Joel Lehman",
        "Elliot Meyerson",
        "Tarek El-Gaaly",
        "Kenneth O. Stanley",
        "Tarin Ziyaee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper claims that machine learning (ML) largely overlooks an important facet of general intelligence: robustness to a qualitatively unknown future in an open world. Such robustness relates to Knightian uncertainty (KU) in economics, i.e. uncertainty that cannot be quantified, which is excluded from consideration in ML's key formalisms. This paper aims to identify this blind spot, argue its importance, and catalyze research into addressing it, which we believe is necessary to create truly robust open-world AI. To help illuminate the blind spot, we contrast one area of ML, reinforcement learning (RL), with the process of biological evolution. Despite staggering ongoing progress, RL still struggles in open-world situations, often failing under unforeseen situations. For example, the idea of zero-shot transferring a self-driving car policy trained only in the US to the UK currently seems exceedingly ambitious. In dramatic contrast, biological evolution routinely produces agents that thrive within an open world, sometimes even to situations that are remarkably out-of-distribution (e.g. invasive species; or humans, who do undertake such zero-shot international driving). Interestingly, evolution achieves such robustness without explicit theory, formalisms, or mathematical gradients. We explore the assumptions underlying RL's typical formalisms, showing how they limit RL's engagement with the unknown unknowns characteristic of an ever-changing complex world. Further, we identify mechanisms through which evolutionary processes foster robustness to novel and unpredictable challenges, and discuss potential pathways to algorithmically embody them. The conclusion is that the intriguing remaining fragility of ML may result from blind spots in its formalisms, and that significant gains may result from direct confrontation with the challenge of KU."
        },
        {
            "title": "Start",
            "content": "JOEL LEHMAN1, ELLIOT MEYERSON2, TAREK EL-GAALY1, KENNETH O. STANLEY3, and TARIN ZIYAEE1 This paper claims that machine learning (ML) largely overlooks an important facet of general intelligence: robustness to qualitatively unknown future in an open world. Such robustness relates to the concept of Knightian uncertainty (KU) in economics, i.e. uncertainty that cannot be quantified, which is excluded from consideration in MLs key formalisms. This paper aims to identify this blind spot, argue its importance, and catalyze research into addressing it, which we believe is necessary to create truly robust open-world AI. To help illuminate the blind spot, we contrast one area of ML, reinforcement learning (RL), with the process of biological evolution. Despite staggering ongoing progress, RL still struggles in open-world situations, often failing under unforeseen situations. For example, the idea of zero-shot transferring self-driving car policy trained only in the US to the UK currently seems exceedingly ambitious. In dramatic contrast, biological evolution routinely produces agents that thrive within an open world, sometimes even to situations that are remarkably out-of-distribution (e.g. invasive species; or humans, who do undertake such zero-shot international driving). Interestingly, evolution achieves such robustness without explicit theory, formalisms, or mathematical gradients. We explore the assumptions underlying RLs typical formalisms, showing how they limit RLs engagement with the unknown unknowns characteristic of an everchanging complex world. Further, we identify mechanisms through which evolutionary processes foster robustness to novel and unpredictable challenges, and discuss potential pathways to algorithmically embody them, highlighting the promise of artificial life, open-endedness, and revisiting RLs core formalisms in service of managing Knightian uncertainty. The conclusion is that the intriguing remaining fragility of ML may result from blind spots induced by its formalisms, and that significant gains may result from direct confrontation with the challenge of KU."
        },
        {
            "title": "Contents",
            "content": "Reinforcement Learnings Formalisms Limit Robustness to Knightian Uncertainty Abstract Contents 1 2 2.1 2.2 3 3.1 3.2 4 4.1 4.2 4.3 4.4 5 5.1 5.2 5.3 5.4 6 References"
        },
        {
            "title": "Knightian Uncertainty\nMachine Learning in Open Worlds",
            "content": "Biological Evolution and Knightian Uncertainty Evolution as Driven by Scientific Falsifiability Evolution as Open-ended Generator of Novelty RLs Core Formalism: The Markov Decision Process RL is Time-blind RL has Thin Conception of Risk Implicit Limiting Assumptions of RL Algorithms"
        },
        {
            "title": "Conclusion",
            "content": "Implications for Foundation Models, RLHF, and AI Safety The Promise of Artificial Life The Promise of Open-endedness Revising the RL Formalism Corresponding author: lehman.154@gmail.com; Second Nature AI1, Cognizant AI Labs2, Unaffiliated3 1 1 2 6 6 8 11 13 14 15 15 16 19 20 21 21 24 25 26 27 28 5 2 0 2 2 ] . [ 1 5 7 0 3 1 . 1 0 5 2 : r 2 Lehman et al."
        },
        {
            "title": "Introduction",
            "content": "Seeking an improvement that makes difference in the shorter term, researchers seek to leverage their human knowledge of the domain, but the only thing that matters in the long run is the leveraging of computation. [...] We have to learn the bitter lesson that building in how we think we think does Richard Sutton not work in the long run. [187] The above quote reflects the ascendancy in machine learning (ML) of methods that leverage increased computation through search and learning, potentially bitter lesson for those tailoring ML algorithms through human domain knowledge [187]. For example, many sophisticated feature-construction methods for machine vision were made obsolete by deep learning on raw pixels [102, 215], just as natural language processing was upended by the transformer revolution [24, 199]. Yet unanswered by this bitter lesson is exactly how little we should design by hand into ML systems. In other words, do there remain blind spots that exist within ML as result of human thinking that could be overcome if yet more was given over to search [34]? If so, perhaps such blind spots could be remedied by taking the insights from the bitter lesson further (e.g. automating the search for architectures or learning algorithms), or illuminated by studying algorithms that already do so. This latter thread is pursued here. In particular, this paper argues that there are subtle lessons still to digest from the algorithm that takes the bitter lesson to its logical conclusion: biological evolution, which imposed no prior domain knowledge, and instead effectively leveraged extreme amounts of computation to invent human intelligence on one leaf of its massively divergent search. The main idea is to highlight, through contrast with evolution, an intriguing blind spot in the formalisms of ML: the importance of robustness to an open-ended future. We hope to go beyond critique alone through suggesting mechanisms from nature, human intelligence, and threads of current ML research that may unlock future progress. Human engineering has surpassed biological evolution in many ways (e.g. the cargo capacity or speed of jumbo jet, compared to birds). However our ingenuity has yet to match evolution in others, such as in engineering self-replicating machines, or, of central interest here: in matching the robustness of evolutions products. For example, in the late 1980s, zebra mussels made their way from Europe to the great lakes in the US, hitchhiking in the ballast tanks of ships [163]. They rapidly spread through the waterways, outcompeting native species and clogging water intake pipes. Such invasive species are not uncommon: Animals evolved for one environment, when placed into significantly different one, can be successful, sometimes frustratingly so (to us) [140]. From an ML or robotics perspective, this level of out-of-distribution robustness is an amazing accomplishment; it is hard to imagine, for example, successfully zero-shot transferring self-driving policy trained only on data from the US to the UK which us humans (another of evolutions products) regularly accomplish, albeit with visceral discomfort. Like evolution, ML aspires towards agents that robustly function in open-world environments, an important challenge spanning many critical applications, including social networks, chatbot assistants, home-assistance robots, self-driving cars, and robots acting in unstructured environments more broadly; indeed, competence within such open worlds is fundamental to MLs quest for artificial general intelligence (AGI) [83, 151]. Yet despite their unprecedented pace of improvement in capabilities, ML systems still struggle in environments much different from those they were trained in: self-driving cars falter with rare situations that are mundane to humans (like understanding that traffic lights in the back of trailer are not worrisome; or that moving plastic bag is not problem) [72, 73, 125, 159, 173]; generative AI agents remain surprisingly fragile [28, 81, 87, 182, 203, 208, 216]; and chain-of-thought reasoning in 100 billion+ parameter language models frequently goes off the rails [43]. Others have remarked that this is peculiar situation [196, 205, 206]: An agent that is remarkably knowledgeable and capable of e.g. fluid philosophical discussion, can yet make rudimentary mistakes of generalization [206] Evolution and The Knightian Blindspot of Machine Learning 3 or reliability [46, 80]. One explanation to resolve this paradox is that facets of intelligence that are coupled in humans may be decoupled in machines: one or more such facets may still be missing from ML. This paper thus aims to draw attention to an important property of intelligence largely ignored by ML: Robustness to qualitatively unknown future in an open world, which we relate to the idea of Knightian uncertainty (KU; [44, 92, 94, 100, 185]), or roughly, unknown unknowns. What emerges next in the real world (i.e. the next fashion, scientific idea, power outage, edge-case driving scenario, natural disaster, stock market boom or crash, etc.) is the vastly-complex product of billions of creative agents acting in an interconnected, complicated physical and digital world; there is much we reasonably cannot anticipate. Yet, the preponderance of work in robust ML aims to address known unknowns, i.e. encouraging robustness to limited classes of experimenter-anticipated risks. For example, to apply techniques from safe reinforcement learning [56] an experimenter generally must anticipate the ways an environment can vary, and specify the type of risk within such environments they wish the policy to optimize. Such work is obviously useful and important. However, one pillar of general intelligence is robustness to the novel situations and risks that continually arise in an open world can we truly claim to be approaching AGI without such capacity, which is central to the survival of animal species as well as human civilization? We define an agents robustness to KU as its capacity to succeed or at least fail gracefully in the face of novel unforeseeable situations. For example, the first zebra mussels that arrived from Europe in the great lakes faced situations far outside their window of direct evolutionary exposure, and yet thrived. In particular, this paper investigates assumptions in ML made for conceptual and mathematical convenience that limit robustness to plausible, if unlikely, future environmental disturbances. These assumptions are then contrasted with mechanisms from biological evolution that enable broader robustness. We focus centrally on reinforcement learning (RL), where the typical objective is to find policy for an agent that maximizes expected reward, although many facets of this analysis apply also to the settings of unsupervised and supervised learning. central conclusion is that RLs engagement with the problem of KU is obscured by two main factors. First, thought in RL is often shaped by the fields formalisms, which tend to axiomatically rule out KU as possibility; it is human to take for granted the water that immerses us [2, 103]. For example, Markov decision processes (MDPs) and typical RL optimization objectives make simplifying closed-world assumptions: that the deployment environment is the same as in training and will remain so, that boundaries between episodes are absolute, that agents are by definition indifferent to impacts of their actions over long time-horizons, and so on. In short, RL (and ML more broadly) most often seeks optimal solutions to fixed problems. We later address (in Section 2.2) extensions of RL (such as meta-learning) that in theory, but (we argue) not in practice, meet this critique. Secondly, RL research is often driven by implicit assumptions about how existing agendas will naturally lead to superhuman robustness. For example, that the combination of increasing scale and the generalization capabilities of large neural networks will address all relevant robustness issues; or that it will do so when combined with increasingly sophisticated hand-designed RL algorithms derived to fit closed-world formalisms. Yet it may be optimistic that robustness to unknown unknowns will arise as byproducts of formalisms and methods that implicitly disavow their existence. In contrast, biological evolution allows all aspects of an agents architecture to adapt; nothing is sacrosanct. There are no explicit formalisms underpinning the many divergent implementations of biological reinforcement learning, nor any fixed time horizon beyond which feedback cannot shape evolutions trajectory. Evolution continually seeks new and diverse ways of surviving and reproducing, which implicitly represent bets on the nature of the future (e.g. perhaps dinosaur-sized reptiles are robust; or small mammals); this diversification can continually refresh the many bets culled by reality (with no fixed time horizon), such that time favors bets (lineages) more robust to the continually unfolding unknown (which itself is partially created by the diversification of life, in how one species contributes to the environment of others). Figure 1 highlights these conditions through 4 Lehman et al. which evolution becomes robust to KU; and figure 2 contrasts at high level how evolution and ML approach robustness to the open unknown. Fig. 1. Interlocking principles enabling evolutions robustness to Knightian Uncertainty. (a) Evolution happens within search space that is open-ended enough such that vast array of complex adaptations can be encoded, e.g. the human brain, multicellularity, developmental systems as whole, and photosynthesis. (b) Diversification pressure in evolution continually creates new behaviors and adaptations from the set of open-ended possibilities, which implicitly can be seen as bets about how the organism and its lineage can persist into the future. (c) Because organisms form part of the environment of other organisms, novel behaviors and adaptations in one lineage create novel unforeseen situations for other organisms as an externality, e.g. the high branches of tree provide novel situation giraffe can exploit. (d) Organisms unable to persist across the uncertainty created by other organisms are filtered away, in effect invalidating their bets about how to persist through KU; the image shows coelacanth, fish that has persisted for 400 million years. In concert, these factors can be seen as form of open-ended generation and falsification of bets about how to deal with KU. We believe there may be ways to adapt these principles to ML research (see discussion in Section 5). One possibility is that, similar to the argument for AI-generating algorithms (AI-GAs; [34]), there remains further bitter lessons [187] that ML has yet to absorb. Dealing with KU is central to intelligence and remains fundamental challenge for ML, and to achieve it, computation may trump clever combinations of individual algorithmic advances. That is, it may be more effective to scale simpler algorithms that make fewer assumptions, taking inspiration from the genesis of robust human learning, i.e. biological evolution. That more [computation] is different [7] extends beyond deep learning alone [167, 184], and less attention has been placed upon fields such as artificial life [3, 108] that while highly ambitious, have revolutionary potential. Or conversely, perhaps there are ways to tackle KU through novel RL algorithms that deeply integrate insights from evolution or how humans and society successfully navigate KU [89, 92, 190]. In total, this paper adds to thread of counter-intuitive critique, that despite dramatic recent progress, ML and RL may yet still be skirting core feature of intelligence [29, 83, 137, 168]. Importantly, while this paper presents critique of ML, we are not skeptics of algorithmic intelligence nor dogmatically committed to mimicking biological evolution or the Evolution and The Knightian Blindspot of Machine Learning 5 Fig. 2. Two Strategies for Dealing with an Open World. This figure describes two possible strategies for coping with an open changing world. In (a) diversify-and-filter, process continually refreshes and adapts its diverse hypotheses about how to persist through the open-ended future. Such hypotheses are filtered through empirical success at tackling later unanticipated problems. Evolution, market competition, and science can be seen to largely operate through this paradigm. There is no explicit formalism, although robustness implicitly relies on the Lindy effect [190], i.e. an adaptable solution long-tested by time is more likely than an untested one to persist yet longer. In (b) anticipate-and-train, diverse problems are first collected, and augmented through human anticipation about what novel situations might later arise. Then, single policy is trained to solve the problems to convergence, and that policy is then deployed into changing world. Much of current ML adopts this paradigm; although the closed-world formalism adopted in training mismatches the open world it is deployed into, the hope is that generalization will enable sufficient robustness to unforeseen challenges. One conclusion is that nothing precludes machine learning from more deeply integrating diversify-and-filter approaches into its methods [86, 106, 112, 115]. Another conclusion is that diversify-and-filter leverages the temporal structure of when novel problems arise, and forces agents to directly grapple with the issue of KU (if they do not, they are discarded). primacy of evolutionary algorithms; instead, we aim to draw focus to blind spot of ML, in service of moving its formalisms and algorithms forward. The paper proceeds first by reviewing background on KU and on the tension between ML and open worlds. Next, Section 3 explores the means through which biological evolution is able to encourage robustness to KU is explored, and Section 4 contrasts them with the formalisms of RL. Finally, Section 5 discusses the possibilities for the fields of artificial life and open-endedness to contribute to reckoning with unknown unknowns; implications of KU for large models and AI safety; and on possibilities for more deeply integrating KU into RL. 6 Lehman et al."
        },
        {
            "title": "2.1 Knightian Uncertainty",
            "content": "Before the wheel was invented [...] no one could talk about the probability of the invention of the wheel, and afterwards there was no uncertainty to discuss [...]. To identify probability of inventing John Kay and Mervyn King [92] the wheel is to invent the wheel. What gets us into trouble is not what we dont know. Its what we know for sure that just aint so. Mark Twain The attempt to manage risk through formalizing it is common practice in many fields such as finance [33], economics [139], and machine learning [141, 186]. Yet when formalizing risk and optimizing against it, we can delude ourselves into false sense of security by thinking we have the world pinned down [92]. Much escapes formalization, however, and in optimizing against what we formalize, we may indeed decrease the narrow form of quantified risk, while potentially exacerbating true risk. Figure 3 provides high-level depiction of this phenomenon. Fig. 3. Optimizing for known unknowns can exacerbate risk from Knightian uncertainty. An optimization formalism that makes closed-world assumptions will indeed improve an agents performance on the situations an experimenter anticipates. However, if such closed-world optimizer aggressively trains an open-world agent, the agent may perversely become more brittle to Knightian uncertainty, as it is incentivized to internalize the closed-world assumptions as true. In other words, Goodharts law applies: Optimization pressure decouples simplified metric from the broader quality it is intended to capture [61, 181]. prominent example is the subprime mortgage crisis of 2007, which at least partially resulted from incorrectly assuming away long-tail risks (in particular, assuming that defaults on high-risk loans could not become highly-correlated), and optimizing against that faulty model with abandon, as if true risk were contained [92, 131]. Evolution and The Knightian Blindspot of Machine Learning 7 It is likely impossible to anticipate all important aspects of the future given the complexity of the world, which grows daily through the decentralized actions of billions of creative agents, who create new products, organizations, media, software, technology, ideas, etc. We can model known unknowns, i.e. the things we know we do not know but by definition cannot predict unknown unknowns, i.e. what we do not know that we do not know (see Table 1). Formalizing the idea of unknown unknowns is near-paradoxical, and the closest concept is that of Knightian uncertainty in economics, which was independently posed in 1921 by John Maynard Keyes and Frank Knight [44, 100]. Knight makes distinction between what he calls uncertainty and risk: Uncertainty must be taken in sense radically distinct from the familiar notion of Risk ... measurable uncertainty, or risk proper, as we shall use the term, is so far different from an unmeasurable one that it is not in effect an uncertainty at all [100]. [emphasis added]"
        },
        {
            "title": "Unknowns",
            "content": "Known Unknown Implicit Knowledge (Unknown Knowns) Knightian Uncertainty (Unknown Unknowns) Deterministic (Known Knowns) Risk (Known Unknowns) Table 1. Types of Risk and Uncertainty. Optimization is most straightforward in deterministic settings with complete information, and more challenging in situations of risk, where there are unknowns, but their parameters are known or estimable. More challenging still are situations where there exist unknown unknowns, i.e. future possibilities that are qualitatively novel and difficult or impossible to anticipate (though they may seem obvious in hindsight) the central topic of this paper. The fourth quadrant of unknown knowns (e.g. where an agent or system has implicit knowledge that is not explicitly known) is also important but less relevant to arguments here. In short, machine learning excels at situations of determinism or risk, but struggles to model Knightian uncertainty. Note that Knights usage of uncertainty differs from common usage in ML. Rather than uncertainty in Bayesian statistics [20], Knightian uncertainty intuitively relates to unknown unknowns, where some important outcomes cannot be probabilistically quantified or anticipated. Such KU contrasts with what Knight calls risk, wherein uncertainty can be modeled (i.e. known unknowns). MLs treatment of uncertainty rarely contends with unknown unknowns; Bayesian methods require, for instance, defining the space of possible hypotheses to calculate uncertainty across [58, 89] and are subject to restrictive closed-world formalizations in the same way as ML more broadly (discussed later in Section 4). recent work has also highlighted KU as an important property for AI [168] in the context of games; and other recent critiques of ML and RL can be interpreted from the lens of KU [2, 29]. Because KU makes negative claim (i.e. it deals with situations where uncertainty cannot be modeled), it is understandably controversial concept [36, 44]. While the idea of unknown unknowns is intuitive, as each of us has been buffeted by events we reasonably failed to anticipate, it cuts against many fields tendencies to create formalisms that by fiat rule them out as possibilities, e.g. neoclassical economics, game theory, and subfields of machine learning. For example, the formalization of supervised learning assumes test distribution identical to that in training [26, 64]. In such frozen world, there is no room for unknown unknowns; yet assuming away their existence need not make them less real or important. As representative example of KU skeptic, the economist Milton Friedman wrote have not referred to this distinction [between Knights risk and uncertainty] because do not believe it is valid. [...] We may treat people as if they assigned numerical probabilities to every conceivable event [55]. [emphasis added] In other words, as in many economic formalizations [13, 129, 171], Friedman assumes that all uncertainty can be framed probabilistically, and that effectively the agent should be blamed for not anticipating all emergent possibilities 8 Lehman et al. in an open world (regardless of computational intractability). similar assumption is often made within ML, but with blame instead implicitly assigned to the experimenter who fails to anticipate all future scenarios when designing the training environment. Limits of anticipation of course exist: Taken to an extreme, what basis does caveman have for estimating the year in which GPUs will be invented, lacking even the concept of computer at all? We do often find ourselves in situations of true uncertainty (what are the long-term consequences of taking this job rather than the other? of having child? of starting this business? of moving to new city?), and the qualitative distinction between business as usual and situations of true uncertainty matters (see longer counter-arguments in [92, 185]). In ML terms, neural network (NN) policy can of course, given any possible input, output probability distribution across actions. Such policy is trained over some experimenter-provided distribution of training situations. When deployed, and facing an unknown unknown, one can query the network to output probability. Yet, there is no theoretical reason to assume sensible generalization from the underlying function approximator (e.g. NN) when querying it far out-of-distribution. More broadly, it is wishful thinking to assume that the best policy for qualitatively novel situation is an interpolative generalization from previously-encountered situations. For example, it is better not to eat mushroom species you have never encountered before, even if it exhibits combinations of features shared by mushrooms you know to be edible (see Figure 4). Subtle differences in input can often be of outsize importance when generalizing, meaning that uncertainty quantification methods may not mitigate this failure [1], especially when the situation violates the assumptions of the training algorithm (to be discussed more in the next section). Note than even if uncertainty quantification methods reveal that the situation is novel, what to do to gracefully handle such situation can be both high stakes and highly context-dependent (i.e. not simple matter of generalization). This paper thus takes as given that the distinction between risk and KU is meaningful, especially as it highlights distinctions between algorithms that treat risk explicitly in terms of formal probabilities, e.g. the expected value under stochastic policy, and those that do not (e.g. evolution as whole, or an animal learning to deal with new human invention). One might then concede that KU exists, but remain skeptical of its practical implications, i.e. whether any algorithm could handle it with grace; perhaps in situations of true uncertainty there are no reasonable principles of action. Yet in practice some decision policies are indeed more fragile than others in unexpected situations [89, 92, 100, 190]. For example, while in normal conditions it may be of marginal advantage for an airplane to carry only enough fuel to reach its destination, under many unexpected situations, having additional fuel will allow more flexibility to handle them without catastrophe. Similarly, Nassim Taleb highlights the benefits of antifragile systems [190], i.e. systems that become stronger in the aftermath of unpredictable but impactful events (black swan events; [189]). Indeed, Taleb describes biological evolution as prototypical example of system that benefits from unexpected shocks. We will return to the theme of evolution later, after next reviewing the challenge of ML in open worlds."
        },
        {
            "title": "2.2 Machine Learning in Open Worlds",
            "content": "There are more things in heaven and earth, Horatio, / Than are dreamt of in your philosophy. William Shakespeare The field of ML most often assumes closed world, where representative situations are provided in training distribution, and that the world will not thereafter change [218]. That is, the dominant abstractions in supervised and unsupervised learning assume that data is independently and identically distributed (IID; [26, 64]). similar assumption in most RL formalisms is that the deployment environment is the same as the training environment [96, 213]. However, the world in fact is open-ended, ever-changing, and presents many rare situations that may effectively be impossible to fully anticipate priori [42, 70, 127, 132, 157]. An open world entails unknown unknowns, Evolution and The Knightian Blindspot of Machine Learning 9 Fig. 4. Neural network generalization is not general cure for Knightian Uncertainty. Imagine as part of larger reinforcement learning policy, an agent decides whether to eat certain mushrooms, which can either be deadly or edible, and can be separated through features learned in training that correspond to the cap size of the mushroom and its thickness. (a) In closed world, it is safe to assume that the distribution of mushrooms encountered during training (red and green +s) reflects that encountered during testing, and the (b) NN decision boundary on whether to eat or not eat the mushroom learned through training will likely reflect this assumption. However, in an open world, not all mushroom varieties are known, the policy might be deployed in slightly different ecosystem, or new variety of mushroom might evolve or be bred. If encountering the unanticipated mushroom (question mark symbol) at the center of (a), it is likely rational for an open-world agent to forgo eating it, given its novelty and the risk of death. The claim is that simple generalization from what is known does not address Knightian uncertainty. fact sometimes embraced in other fields of study, such as economics, complexity theory, business strategy, and risk management [10, 100, 127, 165, 189], but rarely directly confronted within ML [29, 74]. The divergence between strong formal assumptions in ML and real-world situations is well-known and matters in practice [50, 146, 198, 217], as it is central cause for the challenge of real-world ML deployments. As result, there are many strategies for relaxing the assumption of closed world or directly improving the robustness of ML solutions, which are surveyed next. Scaling and Generalization. For example, accumulating training data can increase coverage of unlikely 2.2.1 situations. Such accumulation, coupled with NN generalization, does indeed lead to surprising and powerful capabilities [24, 204]. Yet as impressive as such large models are, they still are susceptible to jail-breaks (novel adversarial inputs), hallucination of references (an interesting failure of robustness), and surprising generalization failures [29, 206]. In short, relying on the capabilities of NNs to generalize beyond their training distribution, no matter how much data is accumulated, does not yet seem to solve the problem of robustly dealing with unknown unknowns (see also Figure 4, and later discussion in Section 4.4.1). We fully acknowledge that the failures of current models are moving target, and that scaling has proven an incredibly powerful paradigm; at the same time, the arguments in this paper highlight subtle but important facet of robust behavior that scaling alone seemingly does not address. 10 Lehman et al."
        },
        {
            "title": "2.2.2 Meta-learning and Continual Learning. Other mitigation strategies include explicitly relaxing the assump-\ntion of a static data distribution, through meta-learning [198] or continual learning [151]. Meta-learning, especially\nwhen called “learning how to learn,” appears a natural paradigm for handling unknown unknowns; however, it is\nworth examining what actually is entailed by common meta-learning formalisms. In practice, the assumption of\nIID, characteristic of closed worlds, is shifted from a static training distribution to a meta-distribution of possible\ntasks. The world remains closed, but the researcher is now responsible for specifying that meta-distribution, and\nthere is no reason that the learned policy should generalize to qualitative variants of tasks the researcher fails to\nanticipate [4, 97, 161]. As a representative example, Figure 5 highlights how optimal behavior in meta-RL can\nmeaningfully differ from commonsense notions of “learning how to learn.” Continual learning makes distinct but\nrelated assumptions [95, 151, 201], focusing more centrally on the challenge of catastrophic forgetting rather\nthan of robustness to novel situations; although some frame it as inclusive of meta-learning [95]. In conclusion,\nboth meta-learning and continual learning are exciting directions, although neither approach (to our knowledge)\nhas successfully tackled many qualitatively new situations across long deployments; nor do such methods play\nmuch role in current foundation models deployments (with exception of in-context learning, which can be seen\nas a form of emergent meta-learning [37]).",
            "content": "Fig. 5. Typical metalearning setups do not incentivize learning how to solve unforeseen tasks. This figure offers caricature of optimal behavior under typical meta-RL formalism, where an agent is trained across fixed distribution of problems; this setup is similar to e.g. [45]. In meta-RL, it is common for an agent to be exposed many times to training tasks covering all major necessary task-relevant skills. Thus the agent is incentivized to learn in training all qualitative skills needed to solve the tasks; after many iterations of training, there need be no significant remaining surprise for the agent when solving new tasks drawn from an IID test distribution (which is the formal goal of the algorithm). At completion of training, an optimal agents behavior is sketched as: (1) it encounters task drawn from the IID test distribution, which is ambiguous (as this characterizes the need for metalearning); (2) the agent takes actions that optimally disambiguate the sampled task; and (3) having identified the task, which it has encountered many similar variants of before in training, the agent executes its previously-learned optimal solution. In practice, optimal behavior will entail mixing steps (2) and (3) together, but nowhere in this process does optimality under the formalism require the generalized ability to learn how to learn. The conclusion is that if then deployed into changing world where it encounters an unknown unknown, the agent may struggle to handle it gracefully. Evolution and The Knightian Blindspot of Machine Learning 11 Safe Reinforcement Learning. Another family of mitigations focuses more directly on the issues of risk and 2.2.3 robustness. For instance, the area of robust reinforcement learning [141] aims to discover policies for agents that can better cope with uncertainty, shocks, and change. Nearly all such methods rely on formalizing robustness through making quantitative assumptions about how reward function or state transition dynamics might change, or what kind of observation or action noise an agent might encounter; while effective and useful for closed world situations, the challenge in open worlds relates more to qualitative changes. Indeed, recent work has suggested that common framings of RL under risk are insufficient to deal with black swan events [110]. Most such approaches implicitly assume that qualitative changes in e.g. state transition dynamics (like new type of actor in the world) can be handled through adding quantitative noise to the system; and it is left to the experimenter to decide what kind of quantitative robustness to encourage."
        },
        {
            "title": "2.2.6 Open-endedness. Finally, the field of open-endedness [179, 181] attempts to study and engineer processes of\nopen-ended creativity, often inspired by biological evolution. The idea is to create long-running search processes\nthat generate continual innovation, as happens in culture and science. Such open-endedness aims in effect to\ncreate and continually contribute to an open world, and work has explored how evolutionary open-endedness\nleads to increasing evolvability [117, 120], i.e. the ability for the system or individuals to quickly adapt. However\nto our knowledge, no work has attempted to directly tackle the problem of KU through open-endedness, although\nit has been applied to increase the capacity and adaptability of RL policies in impressive ways [41, 192, 193, 202],\nmost often within a fixed class of environment where tasks require relatively short time horizons.",
            "content": "Summary. In general, across most such approaches, there is an intriguing tension between the desire to 2.2.7 derive algorithms from formal problem definitions and the seeming impossibility of formalizing the ways in which the future will qualitatively differ from the present (and how to respond to such qualitative differences). The proliferation of problem settings, whether meta-learning, continual learning, stochastic games, robust RL, gives the impression that ML is marching towards solving its deepest issues, if only enough progress is made in each subproblem and then combined [34], yet it is also possible that little research actively targets what is core: The past does not in general provide straight-forward guide to the future [181]."
        },
        {
            "title": "3 Biological Evolution and Knightian Uncertainty",
            "content": "Darwins idea is universal acid; it eats through just about every traditional concept, and leaves in its wake revolutionized worldview. 12 Lehman et al. Daniel Dennett This paper argues that the formalisms of ML have blind-spot for unknown unknowns. This section proceeds through proof-by-example that greater robustness is possible through different mechanisms: Biological evolution, though it unfolds without volition or formalisms, does indeed create organisms (such as ourselves) that are surprisingly robust to novel and outlier situations. This section argues that evolutions products are often robust to KU, and highlights the mechanisms through which such robustness arises. The subsequent section then examines limiting assumptions made in RL, the ML paradigm most relevant for learning to handle the unknown. To begin, we sometimes take for granted the incredible complexity and surprising robustness of biological evolutions products, especially given the dearth of information that drives its search. To first approximation, evolution blindly pulls mutational levers and receives as feedback only whether the levers pulled result in persistence across time [62]. Evolution has no foresight, makes no formal assumptions, and has no conceptual understanding of the complex mechanisms through which an organism survives and reproduces. And yet its creations often act as if they are making complex plans about the future: e.g. some cicadas have life cycles that synchronize to prime numbers (e.g. 13 or 17 years), thereby making it more difficult to be predated by organisms with shorter lifecycles (as prime numbers are not divisible by smaller numbers); leafcutter ants in effect farm fungi by cutting leaves (too tough for them to eat) that they feed to underground symbiotic fungal gardens; migratory birds can travel thousands of miles to exploit seasonal food resources; some pine species have cones that lie dormant and only release seeds after being exposed to fire, ensuring that the environment is cleared of competition and nutrient-rich; and cyanobacteria living only hours can keep track of changing seasons across generations [85]. To quote Cass Sunstein [185]: Contrary to standard view in economics, Knightian uncertainty is real. Dogs face Knightian uncertainty; horses and elephants face it; human beings face it [...] to which we could add: biological evolution most certainly faces it. Evolution is fundamentally unable to assign explicit probabilities to novel outcomes, and yet seems also to be remarkably successful at navigating uncertainty. On the whole its creations are surprisingly robust in the face of unforeseen challenges [99, 166]; and more deeply, evolution as whole is incredibly robust it is unlikely to be extinguished on Earth, even given full-scale nuclear war. As consequence of its lack of volition, evolution necessarily must wrangle with KU, in contrast with ML algorithms derived in explicitly probabilistic terms from closed-world formalisms. There is no theoretical formalism of risk that evolution imposes upon the world; instead, it can be viewed an ongoing anarchic, creative, and divergent search for interdependent structures that persist across time, subject to the stringent constraints of the environment and interactions with other organisms. More concretely, the argument is that evolutions robustness in the face of KU is due to four interlocking factors (see also Figure 1; note that these principles relate to previously-proposed conditions for enabling continuing open-ended evolution [178], but aim instead to explain the emergence of robustness to KU): (1) The selection criterion of persisting across (potentially vast) swaths of time filters bets for robustness. (2) The drive to accumulate an abundant diversity of solutions results in many diverse bets on how to persist through future. (3) Adaptations of one organism create novel situations for others that tests their robustness in new ways. (4) An open-ended search space enables revising any part of the mechanisms of learning in service of greater robustness. In short, an organism implicitly encodes bet about how it and its offspring can persist through the indefinite future, which is tested across unforeseeable situations generated in part by the physical environment and in part by the adaptations of other organisms. Because evolution explores diverse and ever-refreshing set of such bets, and bad (or unlucky) bets are culled across both the short and long-term, as whole it tends to favor organisms relatively robust to the unforeseen. Further, because the search space includes arbitrarily complex adaptations, Evolution and The Knightian Blindspot of Machine Learning 13 including revising any aspect of neural architecture, learning algorithms, and development as whole, there is no commitment in the system to any particular formalism, algorithm, or architecture. Instead, reality serves as filter for what works empirically in the face of often-extreme uncertainty. To expand on this argument, we next connect evolution to Popperian falsifiability in science (to highlight how evolution filters losing bets), and also expand on evolutions open-ended drive towards diversity (to highlight how its generates both new bets and novel situations for testing them). Readers with less interest in evolution can proceed directly to Section 4; what is most critical is that the mechanisms described above could plausibly be adapted and abstracted into ML algorithms as means of encouraging KU robustness."
        },
        {
            "title": "3.1 Evolution as Driven by Scientific Falsifiability",
            "content": "We are standing in the doorway of library of life, but the books are burning as we speak. GPT-4o hallucination attributed to E.O. Wilson Popperian falsifiability is the idea that scientific theory must be testable in way that could prove it false [157], and provides useful frame for thinking about what it means for an evolutionary lineage to persist across (potentially vast) time. The scientific community creates many divergent hypotheses across its diverse fields of study (e.g. physics, psychology, biology), which can be falsified through experiments that generate results that cleave between hypotheses (e.g. Francesco Redis disproval of the spontaneous generation theory within biology [162]). We can similarly view the diverse gambles of evolution from the lens of scientific hypotheses, experimentation, and falsification [40, 157]. An organisms current genome represents range of possible phenotypes (expressible through development and environmental influence); this range of possibilities can be viewed as hypothesis about how an organism can persist the future, which can be falsified through experiments with reality if it is not able to survive and reproduce. This analogy with science is helpful, because it clarifies that evolution is in effect an automated and diversifying research and development process [179], and that like science, it pursues many independent tentative bets which at all times are subject to falsification, no matter how long they have so far persisted. Importantly, beyond scientific paradigms ability to explain current experimental data, its longevity depends also on its potential to be fleshed out and expanded in the future the extent to which it is generative stepping stone [181]. For example, the beauty of Darwins insights into evolution persist today, through being muchexpanded and refined into the current extended evolutionary synthesis [155]; in similar way, genome is stepping stone to wider range of possible future phenotypes accessible through genetic mutation, i.e. its adjacent possible [91]. In other words, persistence of an organisms lineage depends not only on the behaviors it exhibits during its life (and their robustness to KU), but to its potential for future adaptation and diversification (e.g. to realize adaptive radiation [183]), as new situations and opportunities emerge across generations."
        },
        {
            "title": "3.1.1 Bets on Robust Behavior in an Organism’s Lifetime. Biological organisms have evolved diverse strategies\nfor robustly navigating uncertainty during their lifetimes. For example, many animals have developed avoidance\nstrategies that are robust through their simplicity, e.g. toads that attempt to eat small moving things, and avoid\nlarge moving things [164]; and mice have a generalized fear of open spaces where they are vulnerable [123].\nMammals more generally have a variety of sophisticated fear and fear-learning mechanisms [148], often biased\ntowards rapid response (e.g. that relies on simple visual features), and favoring false positives over false negatives,\ngiven the life-and-death stakes of predation. Notice in this example, that evolution moves outside the abstractions\ngenerally available to a typical NN approach to RL: It leverages a faster response to a dangerous situation by\ndynamically using less neural processing. Such rapidity is a fit for avoiding predators, and less rapid responses\nare culled by evolution as they result in greater predation. The point is not rapid response per se, but that in\ngeneral, evolution benefits from lacking a firm commitment to any particular ingredient or formalism (unlike a\nsingle NN that follows a fixed path of computation from input to output).",
            "content": "14 Lehman et al."
        },
        {
            "title": "3.1.2 Bets on Robust Behavior in an Organism’s Future Lineage. A lineage’s evolutionary success extends far\nbeyond the lifetime of one organism, requiring continuing persistence over long swaths of time [62]. Any\norganism alive today has an unbroken track record of persistence dating back to the first replicator, across\nwhich its ancestors undoubtedly successfully met the challenges of countless unanticipated situations. If an\norganism’s genome is overly-optimized for expected present reproductive success, but its entire lineage falls\nprey to a catastrophic change in conditions, then that bet on the future is culled. Thus, even if an organism is\nsuccessful in the present environment, to persist it must be adaptable to potentially dramatic changes.",
            "content": "Beyond its present behavior, the potential future behaviors of an organism are also encoded by implicit bets within its genome. That is, organisms are predisposed to certain kinds of variation at the expense of others [98]; mutations of genome result in range of accessible changes to behavior and morphology, and that range of accessible variation can itself be selected for. Such facilitated variation [57] operates through evolved mechanisms like development [27], canalization [174], and genomic organization [90]; and other mechanisms could yet evolve in the future. For example, there exist mutations that give person six functioning fingers on each hand, which are enabled by the developmental programs encoded in the human genome; these kinds of modular high-level mutations are more evolvable than if the design for new finger required independent re-invention. Such evolvability of genome is also subject to selection, insofar as it enables or hinders lineages ability to persist. genomes accessible variation implicitly encodes bet about the future opportunities and situations an organisms lineage may encounter; e.g. the insect body plans dependence on an exoskeleton limits the size of organisms evolved in different way than that of animals with an endoskeleton, and different body plans enable different flavors of phenotypic variation (e.g. wing size in birds; tail size in lizards). This can be seen somewhat analogously to how ML algorithms themselves develop over time as researchers extend them and apply them to new domains and contexts (e.g. the many different variants of LLM activation functions, positional encodings, and variants of attention layers); the difference is that evolution explores such architectures and systems autonomously. Thus in total, genome implicitly encodes hypothesis about the present and possible futures, which can be falsified if it ceases to persist (either through it not reproducing, or through all of its childrens lineages not persisting). As in science, such hypothesis is never completely validated, as tomorrows environmental conditions or the adaptation of an organism vying for the same niche may undermine lineages prospects. Importantly, there is no fixed time horizon for falsification (in contrast with current RL, as we will later see)."
        },
        {
            "title": "3.2 Evolution as Open-ended Generator of Novelty",
            "content": "The beauty of the living world was trying to photograph was spread before me like tapestry of evolutionary innovation, with one outlandish strategy after another. flower shaped like bucket to trap insects. fish that farms algae. Another that stabs its prey with its snout. Life evolved to fill every conceivable niche. Claude 3.5 Sonnet hallucination attributed to Frans Lanting Evolution, like science, is antifragile [190]: the system as whole improves from shocks (e.g. in science, from experiments that empirically refute hypothesis, or in evolution, from environmental changes that cause extinction [114]). More broadly, there are pressures in evolution to diversify and expand through possible behaviors and niches [59, 181, 183], such as to escape competition [59, 153] or predation [88]. Over evolutionary time, this antifragility and pressure to diversify causes evolution to accelerate, which is known as the evolution of evolvability [38, 152]. Common critiques of evolutionary algorithms [211] tend not to recognize this selfaccelerating antifragility; while evolution may start slow, like science, it accumulates an expanding repertoire of powerful building blocks (e.g. like individual proteins, multicellularity, developmental systems, and neural Evolution and The Knightian Blindspot of Machine Learning computation) that can be flexibly recombined to speed up evolutionary search and realize products beyond what humans can currently design (e.g. robust human-level intelligence). An important condition for such acceleration is that opportunities for meaningful adaptation and diversification must not permanently halt [118]. This condition highlights key difference between biological evolution and most ML algorithms (including evolutionary algorithms): that biological evolution is an open-ended search process [179, 181] with no fixed end point, and tends to creatively and endlessly diverge and innovate. The tree of life bootstrapped itself into vastness, and individual ecosystems intricately intertwine the interests of diverse organisms; adaptation and diversification of one species provides novel opportunities and challenges for others [153], and so on. Because an organisms environment is largely shaped by other evolving organisms, and it is impossible for one organism to anticipate the possible adaptations of all others that interact with it, novel situations of KU become unavoidable. Thus evolution is the self-same accelerating engine of generating KU and selecting for it: novel adaptations of one species generate KU for other ones, which tests their robustness. In this way, evolutions open-ended creativity is important for two separate reasons: Continual diversification of creatures continually generates (1) diverse implicit bets about how to persist through future and (2) new adaptations that constitute unforeseen situations to filter the bets of other organisms. In conclusion, the open-ended and accelerating creative divergence of evolution continually refreshes the set of diverse bets made about strategies for persisting the future. In contrast, ML algorithms rarely include continual and expansive diversify-and-filter strategies for dealing with the unknown future; nothing in principle prevents them from doing so."
        },
        {
            "title": "4 Reinforcement Learning’s Formalisms Limit Robustness to Knightian Uncertainty",
            "content": "A picture held us captive. And we could not get outside it, for it lay in our language and language seemed to repeat it to us inexorably. Ludwig Wittgenstein This section explores the formalisms underlying reinforcement learning (RL) as representative example of how convenient assumptions implicitly limit MLs engagement with the problem of KU; without loss of generality, similar assumptions may apply also to unsupervised and supervised learning."
        },
        {
            "title": "4.1 RL’s Core Formalism: The Markov Decision Process\nThe foundational formalism of RL is the Markov decision process (MDP; [188]), wherein an agent in a fixed\nenvironment iteratively observes a state and reward, chooses an action in response, and transitions to a new\nstate. The MDP formalism makes many assumptions that are often violated in practice, which has motivated\nmany variations fit to particular settings. For example, the partially-observable MDP (POMDP; [180]) models\nenvironments where an agent must act without complete information; Markov games [124] apply to mixed-\nincentive multi-agent environments; and bounded-parameter MDPs (BMDPs;[60]) extend MDPs to environments\nwhere transition probabilities and rewards are not precisely known.",
            "content": "That RL as field continues to diversify its formalisms might seem to undermine the argument that core assumptions limit RLs ability to deal with unknown unknowns. In some sense, RL has no core assumptions, and there is strong agenda within RL to deal with its shortcomings by relaxing assumptions in different ways to tackle more realistic problem settings. Yet as others have pointed out [2], there are implicit dogmas in RL that may sometimes cloud its larger ambitions. For example, from the perspective of open-world unknown unknowns, nearly all MDP relaxations suffer from the same pathology: They require researcher to quantitatively anticipate important properties about qualitatively unknown future risk. 16 Lehman et al. In more detail, the price of relaxing an assumption is that it makes deriving an effective and efficient algorithm more difficult especially one benefiting from formal guarantees; thus in practice, assumptions are only partially relaxed into new unrealistic assumptions, often through offloading onto the experimenter the tall challenge of anticipating all possible domain variation. For example, BMDPs require an experimenter to explicitly describe uncertainty in the distribution of transitions and rewards, which is poor fit to describe the qualitative uncertainties inherent in an unknown future [189]. More directly, does the functional robustness of the zebra mussel result from its robustness to certain level of uniform perturbations to its observations? Certainly robustness to input signals is useful; but much more pertinent is the robustness of its general living strategy (e.g. how well does its conception of threat and its response to it generalize to rare but high-stakes situations)? Notice, in contrast, that biological evolution proceeds without any explicit formalism, of risk or otherwise. Organisms implicitly encode within their genomes how they will deal with risk, and unsuccessful bets are culled through natural selection. This is not so different from how humans and corporations deal with risk over long time-horizons; individuals take different approaches, and whichever works in practice tends to be elevated. An interesting question this raises is whether RLs attachment to formal basis could at times prove an obstacle to the fields progress, an issue that we return to later. The next sections examine consequences of RLs formalisms in more detail, highlighting how specific aspects of them contribute to blindness to KU. Because there are too many variations of formalisms and algorithms to address comprehensively, in what follows we assume the RL algorithms most popular in practice, operating under the typical POMDP formulation of RL. We do not intend in what follows to deny the progress and promise of RL algorithms or undercut RLs many economically and scientifically exciting successes; instead, the hope is to point towards subtle emergent issue, which is that many independent small assumptions as whole paint world where KU does not exist, or if it does, it does not require direct engagement. Readers less interested in specifics of the RL formalism can proceed to Section 5, where more general implications of this paper are discussed; what is most critical below is that many assumptions in RL are fitting for closed world, but definitionally preclude engaging with messy facets of KU in an open one."
        },
        {
            "title": "4.2.1 Deployment and training environments are assumed identical and static. In RL, while generalization is equally\nas important as in supervised learning, most formalizations make no distinction between the train and deployment\nenvironment [35]. As a result, it is not surprising that many RL solutions are brittle [71, 82, 177], as overfitting is\nto be expected from training on the test set. Domain randomization, procedural content generation, adversarial\ntraining, and other methods can encourage generalization [5, 35, 156], although for KU the higher-order problem\nthen becomes identifying the types of variation representative of novel risks an open-world agent will plausibly\nface in the future. In other words, the issue of KU is not engaged with by the formalism to be learned by data;\ninstead it is off-loaded onto the experimenter.",
            "content": "The fundamental challenge is that it is hard to characterize the flavor of generalization that is desired from first principles when dealing with out-of-distribution extrapolation in the abstract [96] (without notion of larger Evolution and The Knightian Blindspot of Machine Learning 17 world evolving in time that generates new situations). Heuristics such as NN simplicity [35], or disentangled NN representations [76] can perhaps often be useful [96], but it is unclear that perfect such heuristic or combination thereof is universally appropriate. Indeed it seems doubtful that humans have yet arrived upon ideal principles for dealing with open-world KU. Evolutions strategy (see Figure 2) or human ones perhaps can provide inspiration; and perhaps there is way to frame the problem that brings more of the temporal unfolding of an open world inside the formalism. Fixed Time Horizon. The standard objective function for reinforcement learning is to find policy that 4.2.2 maximizes the expected return 𝐺, which is discounted sum across time 𝑡 of rewards 𝑅𝑡 : 𝐺 = 𝑡 =0 𝛾𝑡 𝑅𝑡 +1 = 𝑅1 + 𝛾𝑅2 + 𝛾 2𝑅3 + . . . (1) Prominent in this equation is the discount factor 𝛾 [0, 1), whereby the importance of reward or punishment decays across time. With increasing 𝑡, 𝛾𝑡 approaches 0, and the algorithm becomes indifferent to outcomes, implying that (1) credit assignment becomes impossible when actions and consequences are sufficiently distant in time, and (2) equivalently, an RL algorithm is indifferent to catastrophic events beyond its time horizon. As result, RL is most frequently applied to environments requiring only short time-scales, like games [84]. While time horizons can be extended by drawing 𝛾 closer to 1, larger 𝛾 can slow convergence of RL due to increased variance [84, 172], and credit assignment across long time-horizons is significantly challenging in itself [9, 84]. While time horizons can also be extended through the use of hierarchical RL (HRL; [12]), HRL is rarely adopted in practice for high-profile results; one reason is that HRL induces challenging exploration and optimization problems [65]. Even given HRL, or other methods for long-term credit assignment in RL [9, 84], it is challenging (and to our knowledge far beyond the state of the art) to scale them to timescales easily managed by evolution, i.e. managing risk across many years. Some RL algorithms maximize average reward, which circumvents these issues, but for theoretical reasons they then often assume ergodicity [133], which implies that irrecoverable failures in the domain are impossible; this assumption directly contradicts the aim of robustness to KU (which often involves unrecoverable mistakes). Interestingly, the time horizon across which evolution manages risk has no intrinsic limit. An organism can survive for years before reproducing, and actions taken near the beginning of its life can impact whether it reproduces or not. More broadly, branch on the tree of life can end up being dead-end, even after persisting for many, many generations, if an implicit assumption it encodes (e.g. qualitative approach to making its living) is invalidated by change in its environment or the adaptation of another species. For example, the lifestyle of the trilobite was successful for over 200 million years, before it went extinct, driven into decline by predation from better-adapted species such as early jawed fish, which had more efficient feeding mechanisms and locomotion [53]."
        },
        {
            "title": "4.2.3 Episodes are Hermetic. Relatedly, most RL algorithms break evaluations into distinct episodes, which is a\ncomplete sequence of states, actions, and rewards spanning an initial state to a terminal state [188]. For example,\na legged robot may navigate from a starting location to a goal location. The assumption is that each such episode\nis strictly independent from others, and generally, that all episodes are drawn from the same timeless distribution.\nHowever, in the real world, each episode changes the environment in ways both subtle (e.g. a robot’s motors will\nwear differentially from turning left more than right) and obvious (e.g. where the robot ends up after the ‘end’\nof the episode is where it will ‘begin’ at the next [128]); further, in open worlds, the environment is constantly\nchanging, often in response to adaptations of the agent [23, 122, 210].",
            "content": "In open-world situations, where an agent is deployed in an on-going basis, and interacts with other adaptive agents (e.g. humans or other learning agents) in persistent environment, formalism that imposes episodic 18 Lehman et al. limits is at odds with the agents actual situation, and is therefore likely to cause fragility. That is, optimizing as if episodic boundaries are real will likely yield behavior over-optimized for episodic boundaries; in effect, internalizing incremental performance gains while externalizing risk and fragility. For example, robot may complete task marginally faster if it sacrifices motor wear for increased speed, which may have negligible impact within an episode but compound across the robots life. Evolution, in contrast, has no rigid episodes. While the life-cycle of an organism is natural unit of evolutionary time, past life-cycles impact present and future ones in important ways, unlike the strict boundaries of episodes in RL. That is, parent organism may help its offspring, the starting state of an offspring (e.g. where it begins its life, and how much energy it begins life with) is dependent upon its parent, and environmental changes from past organisms can persist across time (e.g. beavers dam, the composition of soil, or the population dynamics between predator and prey)."
        },
        {
            "title": "4.2.4 Data is Temporally Undifferentiated. From the point of view of an e.g. off-policy RL algorithm like DQN\n[138], within a training batch there is no distinction between data from recent episodes and from far earlier ones.\nIn other words, the algorithm cannot distinguish mistakes made by “younger” and “older” versions of the agent.\nFor on-policy algorithms, past experience collected from prior policies has limited value. In both cases, neither\nthe RL algorithm nor the policy can explicitly generalize from the trajectory of prior versions of itself’s mistakes\n(e.g. a person can make higher-level abstractions about the kinds of mistakes they have previously made years\nago, because they can recall those mistakes, and how they later learned to avoid them). Similarly an LLM trained\nwith RLHF does not learn from an explicit history of their past mistakes earlier in training, nor do deployed\nLLMs reason explicitly from mistakes they made during RLHF. The point is that the abstractions applied in RL\ntraining cut off important temporal information that evolution has access to and often does exploit.",
            "content": "RL algorithms can struggle to learn quickly from rare catastrophic events (e.g. the rare event may be swamped in gradient updates by more mundane data; whereas child will not put their hand on the hot stove after encountering it once). To address this point, researchers have explored policies based on episodic memory [21, 121], wherein explicit memories of past episodes serve to guide present agent action. However, as of yet they are not common in practice, and the integration of episodic memory into larger RL framework often requires experimenter-designed heuristics for what memories to place into memory, how to represent them, and when to recall them. Interestingly, RL agents themselves rarely receive time as an input [150], meaning the agents themselves are fundamentally blind to absolute time (although policy classes like recurrent NNs, or positional encodings in LLMs enable relative time-keeping within an episode), i.e. to know how long they have been trained for, or how many episodes they have seen. In contrast, the RL algorithms embodied by intelligent animals make flexible use of time and history. For example, wide mix of model-free, model-based, and episodic learning mechanisms have evolved across different families of animals [207]. Because biological evolution is not tied to mathematical gradients, it is not logically constrained by the memory or computational costs of attempting to calculate gradients across the lifetime of an animal, nor by the greater problems of vanishing or exploding gradients that often complicate gradientbased meta-learning algorithms [8, 200]. Of course, there is no free lunch, and evolution cannot transcend the information-theoretic challenges of credit assignment or the energetic costs of computation [93], yet it is hard to deny that the learning algorithms evolution has uncovered are surprisingly efficient and robust relative to those in RL. Moreover, on an evolutionary time-scale, there are evolutionary equivalents of the notion of generalization [101], and evolution can learn from rare catastrophic events like extinctions, which can serve to accelerate it [114]. Evolution and The Knightian Blindspot of Machine Learning"
        },
        {
            "title": "4.3.1 Counter-intuitive Implications of Expected Value. Most RL algorithms explicitly optimize for the policy\nthat maximizes the expected reward of an agent [188], i.e. the reward an agent attains when averaged over\nindependent episodes. While seemingly an intuitive notion of performance, it has counter-intuitive implications\nand subtleties. For example, expected reward can incentivize risk-taking and fragile optimizations that achieve\nslight benefits in training environments over broader robustness [136].",
            "content": "In particular, expected reward does not consider the variance of rewards [51, 136]. That is, in most real-world situations not only the average outcome, but the variance across outcomes is instrumentally important. For example, it is obvious that when deciding between two options, one that leads to predictable outcome is significantly different from one that has wildly divergent outcomes (e.g. if one taxi offered an average arrival time of 40 minutes, plus or minus 5 minutes; and another offered the same average arrival time, plus or minus 20). Yet the most common RL objectives and evaluation metrics are blind to the real qualitative differences underlying similar average score [51]."
        },
        {
            "title": "4.3.2 Optimization of Risk Measures can Externalize Risk. The subfield of safe RL [56] does explore many ways of\nformalizing decision-making under risk, such as optimizing for the worst-case or the variance of returns, although\nsuch risk-sensitive optimization is rarely observed within RL’s highest-profile results (as it is a more difficult\noptimization target). Distributional RL [16], while most often is used to optimize for expected reward, does model\nthe distribution of reward, which is a promising broadening of the RL formalism. However, while RL is making\nprogress on practical RL algorithms for managing risk, such progress obscures two deeper philosophical issues.\nThe first issue is that such risk-sensitive approaches are limited by the problem settings they are applied\nwithin. In other words, if a risk-sensitive RL algorithm is applied under the assumption of a static distribution of\nenvironments (as is most common), what risk it models will not include those of an open future. That is, the\nalgorithm will take e.g. the variation of returns into account, but not with respect to unforeseen situations. The\nresult then is a policy that manages known unknowns, but remains vulnerable (and perhaps more so, due to\noverconfidence) to unknown unknowns. Indeed, recent theoretical work in RL has argued that such formalisms\nof decision under risk are insufficient to deal with black swan events, even given a static environment [110].",
            "content": "The second issue is that the design of the reward function itself (which serves to quantify how catastrophic an outcome is), the risk criterion (which serves to quantify how to trade-off risk and reward), and the environment distribution (which serves to represent the variety of circumstances an agent will experience) are relegated to the experimenter. That is, while pragmatic and useful tools for human-guided risk management, the challenge of dealing with an unknown future is not actually confronted, but instead handed off to experimenter intuition and iteration. In contrast, evolution empirically tunes risk relative to persistence, with no temporal limit (i.e. it is capable of managing geologically-long time horizons). Insensitivity to Correlated Risk Outside Formalism. The most common RL formulation, of training single 4.3.3 policy in fixed environment, is so common that we are often blinded to significant difference between the train and test environment, namely that many copies of the same policy are often deployed into the wild post-training. Now, what was previously an uncorrelated risk (e.g. of one policy failing in the training environment) can become highly-correlated risk (e.g. if hundreds of self-driving cars simultaneously face the same rare weather condition, and all fail on the same day; or if the numerous instances of globally-deployed LLM are all susceptible to the 20 Lehman et al. same jail-break). In other words, optimizing for safety in the single-agent case can obtain it narrowly, while creating unexpected emergent situations at many-agent deployment and externalizing systematic risk. In similar way, if catastrophic risk is correlated across episodes (e.g. as in Russian roulette), then RL will be insensitive to compounding risks if it is then deployed in situations consisting in effect of many sequential episodes. For example, one challenge in the LLM agents paradigm is that of error compounding [46], where long chains of independent LLM calls end up reliably failing, due to small errors that compound (i.e. risk is correlated across LLM calls), even if many of the independent calls in themselves are fairly reliable. This concern is related to the assumption of episodic independence discussed in section 4.2.3."
        },
        {
            "title": "4.4.1 Reliance upon NN generalization to OOD observations. In most current RL algorithms, the policy output by\ntraining is a NN that maps observations to actions. The neural network serves as a function approximator, e.g. in\ndeep RL, a parameterized NN approximates an optimal policy or Q-function. While the learned NN can accept\ninputs that span the entire space of possible observations (e.g. for a vision-driven policy, the space of all images,\nor for a language-driven policy, the space of all possible input text), the training data for the network is drawn\nfrom only a sliver of that greater space.",
            "content": "As argued in Figure 1, an unforeseen circumstance will often present an observation to the NN that is out-of-distribution (OOD) relative to training environments [176], and NN generalization seems potentially over-optimistic solution to the issue of KU. In RL in particular this assumption is suspect, as rare or novel situations can have dramatic implications for what sequence of actions are thereafter appropriate (e.g. in driving policies for cars, in program synthesis, or in games like Go). That is, successful generalization may be highly context-specific. More broadly, the particular NN architecture and its learning algorithm (e.g. gradient descent) are core dependency of an RL algorithm for generalization, and the implicit assumption is that generalization capabilities of such NNs are appropriate and sufficient to deal with rare or novel situations, and that gradient descent is sufficient algorithm for the job. However, reliable extrapolation is difficult and fundamental challenge for any function approximator (including deep learning and foundation models; [19, 29, 67, 104, 109, 209]), and out-of-distribution generalization is an under-specified problem. Furthermore, dependence on the convenience of well-behaved gradients and on the hardware lottery of GPUs [78] is an invisible hand that implicitly constrains the neural architectures explored by researchers. That is, while in biology it is common to have many feedback connections from higher layers to lower layers [25], nearly all dominant NN architectures structurally limit recurrent connections (i.e. to layer feeding back into itself, rather than to lower layers), due to computational and gradient stability issues. It is clear that the streetlight effect [54] (i.e. searching only where is convenient to look) is not sound principle of search, and we would not knowingly design our algorithms to fall prey to such bias; of course, for many complex reasons as research communities we do not necessarily organize our resources in ways consistent with our domain knowledge [181]. In contrast, evolution is not committed to single mechanism for generalizing to unseen states (e.g. it does not depend on action as mapping from observation by way of single monolithic NN), and often invents and adapts learning mechanisms to handle OOD situations more robustly (e.g. surprise, caution, and fear can Evolution and The Knightian Blindspot of Machine Learning 21 indicate when one is in novel situation and temper ones actions) and generalize better from single examples (e.g. robust episodic memory). Evolution, unlike RL algorithms with fixed architectures, can adjust all facets of neural architecture, connectivity, and neural learning algorithm, and often composes many separate learning processes together."
        },
        {
            "title": "4.4.2 RL algorithms are developed to be domain-agnostic. In a related vein, robustness to KU is also limited by\nthe aim within RL to create general, fixed RL algorithms, i.e. algorithms that apply equally to any possible world\nexpressible within its formalism (e.g. a POMDP). While a benefit for practitioners seeking to quickly tackle a new\ndomain, and for algorithm designers to have a convenient theoretical base, the generality of an RL algorithm\n(e.g. PPO or Q-learning) implies that the RL algorithm itself cannot adapt as the agent incrementally encounters\nunforeseen situations, especially ones that trigger shortcomings of the underlying RL algorithm (which will\nactively be sought out in open-world situations where some agents are adversarial).",
            "content": "That is, research in RL often proceeds by human researchers uncovering latent failure mode of an RL algorithm, and then patching it in some way. For example, the discovery that deep Q-learning suffers from over-optimism motivated double Q-learning [69]; or that gradient updates to policy may be too large or small, motivating the use of trust region [48]. Yet in an open world, foreseeing all such failures priori may be implausible. While one could argue that algorithmic generality respects the bitter lesson [188] because it relies on data gathered from the world rather than hand-coded assumptions, the problem is that it is not data-driven enough: The RL algorithm itself is critical bias to search that does not update with data or compute, and cannot learn from its failures in unforeseen situations. Fundamental shortcomings built into the algorithm (e.g. the POMDP formalism; the implied time horizon beyond which it is indifferent; higher-order patterns in learning failures, like catastrophic forgetting or gradient spikes) remain limiting, and the RL algorithm can benefit only from whatever set of hand-programmed mechanisms were included within it [75]. For example, the performance profile of different RL algorithms often diverges across even relatively-similar tasks (e.g. different Atari games; [184, 212]), meaning that the different heuristics, assumptions, and implementation details underlying such algorithms hold varying benefit in different contexts; yet in an open world, what qualitatively new contexts will be encountered is unknown, leaving the system designer in difficult position. In contrast, biological reinforcement learning algorithms embody rich priors about agent-specific risks and the qualitative world they occupy, themselves learned from evolutionary experience (meaning that across evolutionary time they have survived encounters with unknown unknowns and adversaries motivated to exploit shortcomings in their adaptations). The overall conclusion is that many independent assumptions in RLs formalism, and implicit in the field more generally, contribute to clouding the importance to general intelligence of robustness to an unknown open-ended future."
        },
        {
            "title": "5 Discussion\nThis paper highlights the importance of robustness to unknown unknowns for general intelligence, how biological\nevolution successfully wrangles with KU, and the intriguing blind spot that ML has for it. This section discusses\nthe implications of these ideas. First, its implications for foundation models, RLHF, and AI safety, then what\noutside approaches and fields are naturally most promising for confronting the challenge of KU. Finally, the\npaper concludes with possibilities for synthesizing KU into RL’s formalisms.",
            "content": "Implications for Foundation Models, RLHF, and AI Safety"
        },
        {
            "title": "5.1\nA possible tension in the argument about blind spots in RL’s formalisms comes from the obvious success of\nfoundation models, included those fine-tuned using RL through human feedback (RLHF; [32, 160]). If KU is so",
            "content": "22 Lehman et al. important, why are foundation models useful across many open-world situations? We believe ultimately there is no contradiction, in that such models are indeed immensely useful, and as argued in the introduction, they still struggle with robustness in intriguing ways [28, 43, 73, 81, 87, 125, 159, 173, 182, 203, 208, 216]. More broadly, there has been much concern with the safety of large models [6, 72, 144]; many of such concerns relate to issues of robustness [6, 72], but KU is rarely mentioned in such contexts. This section thus discusses what KU implies for foundation models in general, RLHF algorithms in particular, and ends with reflection with implications for AI safety."
        },
        {
            "title": "5.1.1 KU and Foundation Models. One powerful role of AI is as empirical philosophy, providing direct evidence\ntowards settling deep questions in linguistics and cognitive science that were previously argued only rhetorically\nor from our current understanding of human intelligence [52, 134, 154], e.g. can intelligent behavior be learned\nthrough symbols alone, without grounding in experience? In short, AI broadens our understanding of intelligence\nby creating divergent implementations of it [119]. ML grants the freedom to isolate particular abstractions of\nintelligence into algorithms, and explore how they compare and contrast with human intelligence, thereby\nhelping us to better understand the broader space of possible intelligent agents. We learn, for example, that what\ncomes easily in human development is difficult for some types of machines, or vice versa (e.g. Moravec’s paradox\n[142]). By leaving KU out of our formalisms, we thus learn how far we can nonetheless get, which is interesting\nin its own right; the space of failures left after scaling methods blind to KU may then reveal, through negation,\nwhat robustness to KU ultimately contributes to general intelligence.",
            "content": "From this point of view, one interpretation of the success of RLHF and current LLMs more generally is that an extremely broad distribution of training data (e.g. the internet and large collections of human feedback), coupled with the surprising generalization capabilities of increasingly large NNs, does indeed capture large range of adaptive (and impressive) behavior even when the training formalism mismatches the open-world nature of the deployment setting. In other words, seemingly much of language-based behavior can be captured by form of interpolation in highly-abstracted space, given sufficient data. This is no doubt an incredible scientific and philosophical discovery, and one of great economic importance. For many business-as-usual situations, the ability to deal with KU may not be necessary for adequate behavior, given sufficient coverage of similar situations in training. Conversely (and as argued by others [29, 137]), the failures of large models [28, 43, 73, 81, 87, 125, 159, 173, 182, 203, 208, 216] have interesting implications as well. For example, the ARC prize has (until recently) stymied LLMs despite the seeming simplicity of its task [31]; its design was explicitly motivated by the theory that ML models struggle to acquire skills that have little support within their training data [29]. This theory fits well with the narrative in this paper around KU, as qualitatively new task with little support in the training set is effectively an unknown unknown from the perspective of the model. Yet of course, general intelligence must include the ability to acquire new skills on the fly; and many other intriguing failure patterns of LLMs can be related to KU-blindness, such as the continued fragility of AI agents (see Section 4.3.3). In conclusion, the arguments in this paper apply also to foundation models, which do remain interestingly fragile, especially to unforeseen situations; we should expect so if the formalisms motivating both their pretraining (supervised learning) and post-training (reinforcement learning) exclude by definition the possibility of worlds with unknown unknowns. The exciting possibility is that through revising such formalisms (discussed later) or otherwise bringing KU within the realm of ML, it might be possible to train large models that are much more robust to the open world."
        },
        {
            "title": "5.1.2 KU and Reinforcement Learning through Human Feedback. Because RLHF is among the most impactful\ncurrent RL algorithms, this section explores the implications of KU for it in particular. In the most common\nRLHF setups, a reward model that represents human preferences is trained from preferences collected across a",
            "content": "Evolution and The Knightian Blindspot of Machine Learning 23 representative set of tasks (the task distribution). This reward model then acts as RLs reward function, enabling RL to fine-tune foundation model to produce more-preferred responses for all the tasks in the task distribution. RLHF has been highly successful in practice, and yet the arguments detailed so far also apply to it. For example, the task distribution used in RLHF is an important and messy consideration, and if qualitatively novel tasks emerge after model is deployed, the ability of model to gracefully handle such tasks depends on its robustness to KU; this issue may underlie e.g. fragility to novel jailbreak attacks and underperformance on tasks like ARC that are designed to represent new but learnable skills. Given the breadth of the world, it seems intractable to anticipate priori the complete space of qualitative tasks widely-used model may later be asked to tackle. Interestingly, KU may also impact the RLHF training process itself, because RLHF depends upon model of human preferences. In particular, the KU perspective suggests tension between the creativity we might want from RLHF (i.e. for learning to uncover qualitatively novel superhuman solutions to given tasks), and the ability for the reward model to successfully recognize such creativity. That is, truly novel strategy for solving problem may be qualitatively out-of-distribution for the reward model (which is trained to differentiate better and worse responses under closed-world assumptions). If highly-creative output effectively becomes KU for the reward model, then to realize more of RLHFs potential may require addressing KU in their design. An intriguing empirical study to investigate this hypothesis is to extend reward model benchmark suite [107, 126], and measure if reward models struggle to recognize divergently creative (but objectively correct) solutions. Note that these concerns apply also to methods like RL from AI feedback [111] and constitutional AI [11], wherein foundation models themselves act as reward models for an RL process. Another interesting consideration is that while the RL formalism underlying RLHF does not directly incentivize wrangling with KU (as per earlier arguments), the training data of LLMs does include semantic high-level information about KU itself and the challenges of navigating an uncertain future (e.g. Knights work on KU is likely in common training corpuses, as will be this paper). Thus in theory, models may internalize the problem of KU on some level, and perhaps can be more attuned to KU if so prompted (an idea which is worth future study). Yet it remains an open empirical and logical question to what extent training model on the semantic information necessary to transcend its training paradigm can indeed remedy such limitations; one point of optimism is that humans are able to sometimes understand and transcend their evolutionary biases. Additionally, LLM training data includes within it transcripts of human reasoning, and reasoning indeed is part of the process through which humans deal with KU. Thus models with reliable and general reasoning may help in some ways with KU [30]. However, for the many reasons articulated earlier in this paper, we believe that reasoning by itself is insufficient for addressing KU; for one, reasoning is general tool that can easily enable operating in ignorance of KU as much as grappling with it it deeply matters the nuance with which reasoning is applied [92]. Secondly, if LLM reasoning itself is simplistically integrated into the language model paradigm (i.e. with more tokens relating to reasoning added to training corpuses), an open question then becomes how robust to unforeseen reasoning scenarios it will be (i.e. is it indeed general reasoning?)."
        },
        {
            "title": "5.1.3 KU and AI Safety. Intuitively, robustness is of particular interest in safety-critical domains (e.g. self-driving\ncars, mental health counseling, medical advice and analysis, etc.), and in adversarial situations in which creative,\nunforeseen attacks are incentivized (e.g. as in LLM jail-breaking, or exploits of self-driving cars). More deeply,\nrobustness to distributional shift between training and deployment is a general concern within AI safety [6, 72],\nand we thus argue that KU has research implications for that community.",
            "content": "Perhaps unsurprisingly, one conclusion is that current framings of robustness to distributional shift could benefit from more direct confrontation with unknown unknowns; in particular, through (1) more fully acknowledging the creative process through which situations of KU emerge in the real world, (2) giving up on the possibility of fully anticipating and training upon all possible domain shifts, and instead (3) encouraging the ability to recognize situations of KU, and to adapt dynamically to them at test-time. 24 Lehman et al. That is, in open-world situations, qualitatively novel situations and improbable edge-cases emerge naturally across time, often due to the creativity and adaptability of other intelligent agents (the billions of humans on Earth, markets, governments, and other institutions). As alluded to above, for foundation model deployed through an API, it seems plausibly intractable to anticipate the open-ended decentralized and collaborative creativity of those seeking to leverage LLMs for new applications, or to elicit interesting failures from them, or jailbreak them to cause misuse. Furthermore, the outputs of LLMs themselves impact the digital world (e.g. the rise of LLM-generated webpages, enable new apps that LLMs feasibly will interact with), which other actors in the world accordingly adjust to as well. In other words, the real world is in constant state of bubbling creative distribution shift. In acknowledging that such models will inexorably encounter many unforeseeable situations and edge-cases when deployed in an open world, the consequence is that it may be necessary to find approaches to handle KU other than the dominant strategy of anticipate-and-train. That is, the dominant approach for robustness in large real-world models is simply to accumulate as much diverse and relevant data as possible (i.e. anticipate), and then to train models upon them, in hopes that NN generalization will be sufficient to handle KU; an extension is to incrementally patch models through human or automated red-teaming [170], in effect, another round of anticipate and train. Methods developed specifically to encourage robustness to distributional shift similarly attempt to create many environmental variants (as in domain randomization [195] or adversarial robustness [156]) and train upon them. As described in Section 2.2, shifting to the meta-learning paradigm does not in practice necessitate moving beyond this anticipate and train paradigm. Such existing methods are practical and useful, and the critique is not that we should not attempt to use our current tools to help models be as robust as possible. Rather, the overall problem with such approaches is that there is difference between (1) training in parallel upon anticipated situations, practicing upon them repeatedly to learn an optimal policy that is then deployed, and (2) adapting sequentially across lifetime of qualitatively novel challenges in way that improves ones core ability to recognize and work with situations of true uncertainty. The claim is that there is hard-to-pin-down hole in the current way we formalize our methods that we hope this paper helps bring to light. Finally, to end on broader note, as researchers in ML we ourselves lie in position of Knightian uncertainty with respect to the development of AGI or superintelligence, what their nature would be if developed, and their safety implications for society [185]. That is, the unfurling of science is unpredictable, futurology is littered with failed predictions, and the injection into society of strange and powerful new technology is guaranteed to have unforeseen consequences, both bad and good. This is not to discount our laudable efforts to anticipate the risks to come [6, 22, 68, 72, 144], but instead to suggest that we may need to yet broaden them. If the past is any guide, we should embrace that whatever our beliefs about how close or far we are from cracking intelligence, or what machine agent much smarter than us would be like in practice, that the realities are likely to greatly surprise us."
        },
        {
            "title": "5.2 The Promise of Artificial Life\nOne way of interpreting the argument in this paper is that it advocates for evolutionary algorithms [39] over\nother ML methods, in analogy with biological evolution’s robustness in dealing with KU; we do not intend to take\nthis position. Biological evolution is quite distinct from traditional evolutionary algorithms (EAs), which most\noften abstract evolution as a black box optimization (BBO) process [115]. Indeed, from the lens of KU, the setting\nof BBO could be critiqued in much the same way as RL, because BBO is often formalized as global optimization,\ni.e. the search for an optimal policy in a fixed environment; and while EAs are population-based, and thus are\ncapable of exploring diverse policies simultaneously, in practice, in most EAs the population quickly converges\n[116] due to global competition among all individuals. In other words, most EAs would not satisfy the four factors\nwe hypothesized are responsible for biological evolution’s robustness to KU (see Figure 1).",
            "content": "Evolution and The Knightian Blindspot of Machine Learning 25 However, there are many ways to abstract evolution into an algorithm [115, 181]. That is, many processes optimize, but few continually produce interesting novelty for billion years, weaving together diverse, adaptable solutions into complex ecosystems. In contrast to EAs for optimization, the field of artificial life (ALife; [3, 14, 108]), especially within the open-ended evolution community [149, 179, 191], attempts to create evolutionary processes more directly inspired by biological evolutions tendency towards ongoing open-ended creativity. Simulated ALife worlds, such as Polyworld [194], EvoSphere [135], or Avida [147] instantiate 2-D or 3-D simulations where creatures evolve to compete for resources. The hope is that it is possible to set up the initial conditions (often with seed organism capable of replication) in rich simulated world such that the resulting dynamics of competition and evolution produce an ongoing proliferation of diverse innovation in complex ecologies, as happened on Earth. If successful, such systems should produce more KU-aware solutions, as they would satisfy the criteria hypothesized here. However, it remains an unsolved challenge how to engineer such rich world wherein running the system results in long-running open-ended evolution [15, 179]. Yet in comparison to ML, relatively few resources have been expended in pursuit of ALife, especially when juxtaposed with the intellectual grandeur of what it represents: The understanding of the principles for universe-design that satisfy the necessary conditions for continual and unbounded creativity [178, 179]. Given increases in the scale of possible computation, and the possibilities for making inroads into core aspects of intelligence that RL may be skirting (i.e. robustness to KU), we believe this field of research is notably underappreciated; intriguing recent work has explored the possibility for foundation models to help accelerate ALife experimentation [105, 145], and much more synthesis between ML and ALife may be possible [145]. An interesting possibility is that ALife may benefit from scale in similar ways that foundation models have; more can be qualitatively different. While recent work has demonstrated that ALife populations can adapt to sudden changes in domain [77], to our knowledge simulated ALife worlds have not been investigated for their potential to produce robustness to the open unknown. Future work can attempt to engineer ALife worlds that encourage robustness to KU, perhaps informed by the four conditions proposed in this paper (Figure 1). An interesting if speculative question, is if evolved ALife architectures and learning rules, potentially quite alien from ML architectures, could handle KU better than general-purpose deep RL algorithm? And if so, could lessons be learned from them to inspire new ones?"
        },
        {
            "title": "5.3 The Promise of Open-endedness\nBeyond creating artificial worlds, the field of open-endedness [83, 179] attempts to abstract the engineering\nprinciples for ongoing creative search in a way that is domain-independent, similar to the way that human\nopen-ended search can be generalized to almost any design space, e.g. to create on-going novelty in architecture,\nengineering, songs, beverages, scientific papers, artwork, algorithms and inventions. For example, the POET\nalgorithm attempts to continually create new problems for itself to solve [202], in a similar way to how the ML\nresearch community challenges itself with new benchmark tasks. The intuition is that successful open-ended\nsearch is highly-related to KU, as it will continually create qualitative novelty (as happens in e.g. science or art);\nin the POET example, a new benchmark may provide a unforeseen challenge for the solver.",
            "content": "A possible benefit of this kind of domain-independent approach over ALife worlds is that open-endedness more directly relates to real-world problems. That is, even if open-ended evolution in ALife is successful, how to make practical use of the agents evolved in simulated worlds may not be trivial [47]. Further, open-endedness can leverage recent advances in ML (such as LLMs) to circumvent the need to evolve intelligence from scratch [113, 214], which could make such an approach more computationally feasible. Indeed, there is some evidence that open-endedness can help with the problem of generalizing out-ofdistribution [41, 169] in specific RL tasks, albeit in limited ways that do not include new qualitative dimensions. 26 Lehman et al. We believe that generalization to new qualitative dimensions may require open-ended invention of new and specialized learning algorithms and architectures for agents, similar to the spirit of AI-GAs [34]; in other words, there is no reason to expect that the out-of-the-box generalization capabilities of deep learning architectures should be optimal for unknown unknowns. As described earlier, biological organisms have many special adaptations that let them generalize and fail gracefully (e.g. generalized fear of the unknown, specific learning mechanisms for near-catastrophes and painful situations); we could leverage open-endedness techniques to continually invent such mechanisms specifically fitted to the affordances of particular agents in particular domain. Hybrid-evolutionary methods like population-based training [86] are promising step towards allowing aspects of RL algorithms themselves to adapt to their circumstances, and recent work has learned agent architectures through an LLM-based open-endedness method [79]; perhaps such techniques could be adapted to target the problem of robustness to unknown unknowns. One challenge for open-endedness research with respect to KU is that most applications of open-endedness involve single-agent or few-agent learning in relatively simple episodic world across short time horizons. Further, the highest-profile open-endedness results generally leverage out-of-the-box RL algorithms to train agents, and thus we believe are subject to many of the criticisms described so far with respect to KU, even if embedded within larger open-endedness loop. What may still be lacking in open-endedness research is deep synthesis between simulated ALife worlds (of many diverse agents co-evolving in persistent complex world) and that of the abstracted open-endedness environments (of single agents solving tasks within domain of real-world relevance). For example, games such as minecraft in theory offer open-ended possibilities within fairly-rich world capable of hosting many agents [63]. An intriguing open question is if there exists an interpolative synthesis somewhere on the continuum between ALife and domain-independent open-endedness that enables practical and fruitful engagement with the problem of KU."
        },
        {
            "title": "5.4 Revising the RL Formalism\nWhile it is surprisingly difficult conceptual work to invent formalisms that enfold previous blind spots, RL has a\nrich history of such innovations: inverse RL [143] seeks to identify the implicit objective of an agent; unsupervised\nenvironment design [41] poses the optimization of challenging environments as part of a larger RL problem that\nhelps single agents to better learn and generalize; distributional RL [16] models the distribution of reward rather\nthan simply the average; and inverse reward design [66] attempts to bring the human design process of the RL\nreward itself into the RL formalism. There is also a history within RL of critique of potential dogmas [2] and bold\nhypotheses about what directions will or will not propel RL forward towards its grandest goals [34, 175, 187, 197].\nThus while this paper largely provides a critique of current RL from the lens of unknown unknowns, one hope\nis that it can spur new ways of viewing RL and new RL algorithms that better cope with KU. Here we provide\nsuggestions of future work towards that end, starting from more immediate to the more theoretical.",
            "content": "While it may not attack the core of KU, one immediate possibility is to leverage advances in foundation models to generate qualitative variations of RL training environments, for direct training, meta-learning, or post-hoc evaluation. That is, one critique of RL is that it substitutes robustness to quantitative unknowns (e.g. noisy transitions, observations, or actions; or to quantifications of risk like value-at-risk) for the qualitative unknown of the future. If LLMs have greater understanding of qualitative dimensions in which environments may realistically vary, they can likely be applied to brainstorm range of qualitative variations (e.g. rare but realistic situations) and then implement them by editing the code of training environment; some work has begun to explore this, although more from the lens of extending agent capabilities [49] than encouraging robustness to KU. Further promise is shown by work highlighting the benefits of qualitative priors from LLMs to aid policy robustness [130, 170]. One important question would be how to best leverage such additional scenarios to Evolution and The Knightian Blindspot of Machine Learning 27 encourage robustness to further unanticipated variation; training directly on imagined scenarios may encourage greater robustness, but still relies on the anticipate-and-train paradigm. In this way, complementary possibility is to synthesize what enabled biological evolutions robustness into RL methods. For example, one aspect of KU is to continually accumulate wide diversity of bets on qualitative futures, which can then be culled by novel situations. One way to realize this in RL would be to apply diversityseeking method such as SMERL [106] to find many solutions to business-as-usual training environments, which could be iteratively culled on outlier test-cases that are hand-curated (or are generated by LLMs, as described above). Another synthesis is to explore many-agent forms of unsupervised environment design [169, 202], where the system and agents continually provide novel challenges for each other, and adapt them towards the diversify-and-filter paradigm (from Figure 2). Finally, there is the intriguing challenge of how to formalize KU and robustness to it within the paradigm of RL. The argument here is that unknown unknowns are not well-captured by the current range of extensions to the MDP, and in fact, may be very difficult to capture mathematically at all, as they seem entangled with the nebulous nature of how complex open world unfolds into the future. In some ways, formalizing KU seems paradoxical: If KU is valid concept, how can one meaningfully measure the robustness of policy to what cannot be fully anticipated? Yet, perhaps there are incremental ways to capture at least some of it. For example, incorporating non-stationarity into foundation model pretraining by ordering data relative to when it was generated, could grant an environment for competing LLMs to diverify and be filtered over time, potentially encouraging their robustness to culturally new tasks. However, ultimately we believe new insights are needed. Perhaps the Lindy effect [190] could be formalized to good effect; or new convincing and practical formalization of what makes situation qualitatively distinct from others could be derived; or meta-learning could be reposed such that it better captures the qualitative sense of learning how to learn, which includes graceful handling of unforeseen situations at inference-time. For moment, however, assume that KU cannot be formalized. The challenge of KU might then be blind spot of ML precisely because ML tends to organize its sub-fields by anchoring them on distinct formal problem statements; and progress might result counter-intuitively from lessening our reliance upon them (a phenomenon arguably already unfolding in the era of LLMs). While fitting and efficient for closed world problems, this sociological bias of the ML community may not help in tackling the nebulous open-ended world [92, 158, 181]. That is, if evolution has produced solutions much more robust to KU than ML, and has done so without any formalisms, obviously they are not logically requisite to its achievement. In this way, it is conceivable that critical facet of intelligence (robustness to the qualitatively unknown future) potentially lies beyond precise formalization. While such an idea could seem unlikely, or intellectually or aesthetically dissatisfying, it at least bears consideration: The assumption that formalization must lie at the core of ML is itself unproven, and may never be so. We pose this gauntlet for the RL community: Whether or not KU can be mathematically formalized in productive way, how to handle it is challenging and important question; we look forward to what philosophical, algorithmic, or practical advances may result from disproving our thesis, or from attempts to address it."
        },
        {
            "title": "6 Conclusion\nThis paper highlights how the concept of Knightian uncertainty is a necessary component of general intelligence,\nhow mechanisms from biological evolution contend with it, and how it challenges popular formalisms in ML.\nThe conclusion is that the ability to deal with unknown unknowns is core to some of machine learning’s most\nambitious goals, and that truly impressive recent advances in ML may yet skirt the challenge of KU, thus providing\na possible explanation for why machine intelligence at times still seems fragile relative to the biological. While a\nnegative result, the exciting consequence is that progress in algorithms and our understanding of intelligence\nmay lie on the other side of direct confrontation with this intriguing problem.",
            "content": "28 Lehman et al."
        },
        {
            "title": "References",
            "content": "[1] Moloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Rezazadegan, Li Liu, Mohammad Ghavamzadeh, Paul Fieguth, Xiaochun Cao, Abbas Khosravi, Rajendra Acharya, et al. 2021. review of uncertainty quantification in deep learning: Techniques, applications and challenges. Information fusion 76 (2021), 243297. [2] David Abel, Mark Ho, and Anna Harutyunyan. 2024. Three Dogmas of Reinforcement Learning. arXiv preprint arXiv:2407.10583 (2024). [3] Christoph Adami. 1998. Introduction to artificial life. Springer Science & Business Media. [4] Anurag Ajay, Abhishek Gupta, Dibya Ghosh, Sergey Levine, and Pulkit Agrawal. 2022. Distributionally adaptive meta reinforcement learning. Advances in Neural Information Processing Systems 35 (2022), 2585625869. [5] Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, et al. 2019. Solving rubiks cube with robot hand. arXiv preprint arXiv:1910.07113 (2019). [6] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané. 2016. Concrete problems in AI safety. arXiv preprint arXiv:1606.06565 (2016). [7] Philip Anderson. 1972. More Is Different: Broken symmetry and the nature of the hierarchical structure of science. Science 177, 4047 (1972), 393396. [8] Antreas Antoniou, Harri Edwards, and Amos Storkey. 2019. How to train your MAML. In Seventh international conference on learning representations. [9] Jose Arjona-Medina, Michael Gillhofer, Michael Widrich, Thomas Unterthiner, Johannes Brandstetter, and Sepp Hochreiter. 2019. Rudder: Return decomposition for delayed rewards. Advances in Neural Information Processing Systems 32 (2019). [10] Brian Arthur. 2021. Foundations of complexity economics. Nature Reviews Physics 3, 2 (2021), 136145. [11] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. 2022. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073 (2022). [12] Andrew Barto and Sridhar Mahadevan. 2003. Recent advances in hierarchical reinforcement learning. Discrete event dynamic systems 13 (2003), 341379. [13] Gary Becker. 1976. The Economic Approach to Human Behavior. The University of Chicago Press. [14] Mark Bedau. 2007. Artificial life. In Philosophy of biology. Elsevier, 585603. [15] Mark Bedau, John McCaskill, Norman Packard, Steen Rasmussen, Chris Adami, David Green, Takashi Ikegami, Kunihiko Kaneko, and Thomas Ray. 2000. Open problems in artificial life. Artificial life 6, 4 (2000), 363376. [16] Marc Bellemare, Will Dabney, and Mark Rowland. 2023. Distributional reinforcement learning. MIT Press. [17] Abhijit Bendale and Terrance Boult. 2015. Towards open world recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition. 18931902. [18] Abhijit Bendale and Terrance Boult. 2016. Towards open set deep networks. In Proceedings of the IEEE conference on computer vision and pattern recognition. 15631572. [19] Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, and Owain Evans. 2023. The reversal curse: Llms trained on\" is b\" fail to learn\" is a\". arXiv preprint arXiv:2309.12288 (2023). [20] Christopher Bishop and Nasser Nasrabadi. 2006. Pattern recognition and machine learning. Vol. 4. Springer. [21] Charles Blundell, Benigno Uria, Alexander Pritzel, Yazhe Li, Avraham Ruderman, Joel Leibo, Jack Rae, Daan Wierstra, and Demis Hassabis. 2016. Model-free episodic control. arXiv preprint arXiv:1606.04460 (2016). [22] Rishi Bommasani, Drew Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. 2021. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258 (2021). [23] Rodney Brooks. 2024. Unexpected Consequences of Self-Driving Cars. https://rodneybrooks.com/unexpected-consequences-of-selfdriving-cars/. Accessed: 2024-10-14. [24] Tom Brown. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165 (2020). [25] Edward Callaway. 2004. Feedforward, feedback and inhibitory connections in primate visual cortex. Neural Networks 17, 5-6 (2004), 625632. [26] Longbing Cao. 2016. Non-iid recommender systems: review and framework of recommendation paradigm shifting. Engineering 2, 2 (2016), 212224. [27] Sean Carroll. 2005. Endless Forms Most Beautiful: The New Science of Evo Devo and the Making of the Animal Kingdom. WW Norton and Co. [28] Yanan Chen, Ali Pesaranghader, Tanmana Sadhu, and Dong Hoon Yi. 2024. Can We Rely on LLM Agents to Draft Long-Horizon Plans? Lets Take TravelPlanner as an Example. arXiv preprint arXiv:2408.06318 (2024). [29] François Chollet. 2019. On the measure of intelligence. arXiv preprint arXiv:1911.01547 (2019). Evolution and The Knightian Blindspot of Machine Learning [30] François Chollet. 2024. OpenAI o3 Breakthrough High Score on ARC-AGI-Pub. https://arcprize.org/blog/oai-o3-pub-breakthrough Accessed: 2025-01-06. [31] Francois Chollet, Mike Knoop, Gregory Kamradt, and Bryan Landers. 2024. Arc prize 2024: Technical report. arXiv preprint arXiv:2412.04604 (2024). [32] Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017. Deep reinforcement learning from human preferences. Advances in neural information processing systems 30 (2017). [33] Peter Christoffersen. 2011. Elements of financial risk management. Academic press. [34] Jeff Clune. 2019. AI-GAs: AI-generating algorithms, an alternate paradigm for producing general artificial intelligence. arXiv preprint arXiv:1905.10985 (2019). [35] Karl Cobbe, Oleg Klimov, Chris Hesse, Taehoon Kim, and John Schulman. 2019. Quantifying generalization in reinforcement learning. In International conference on machine learning. PMLR, 12821289. [36] Alan Coddington. 1982. Deficient foresight: troublesome theme in Keynesian economics. The American Economic Review 72, 3 (1982), 480487. [37] Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei. 2022. Why can gpt learn in-context? language models implicitly perform gradient descent as meta-optimizers. arXiv preprint arXiv:2212.10559 (2022). [38] Richard Dawkins. 2019. The evolution of evolvability. In Artificial life. Routledge, 201220. [39] Kenneth De Jong. 2017. Evolutionary computation: unified approach. In Proceedings of the Genetic and Evolutionary Computation Conference Companion. 373388. [40] Daniel Dennett. 1995. Darwins dangerous idea. The Sciences 35, 3 (1995), 3440. [41] Michael Dennis, Natasha Jaques, Eugene Vinitsky, Alexandre Bayen, Stuart Russell, Andrew Critch, and Sergey Levine. 2020. Emergent complexity and zero-shot transfer via unsupervised environment design. Advances in neural information processing systems 33 (2020), 1304913061. [42] David Dequech. 2001. Bounded rationality, institutions, and uncertainty. Journal of economic issues 35, 4 (2001), 911929. [43] Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston. 2023. Chain-ofverification reduces hallucination in large language models. arXiv preprint arXiv:2309.11495 (2023). [44] Robert Dimand. 2021. Keynes, Knight, and fundamental uncertainty: double centenary 19212021. Review of Political Economy 33, 4 (2021), 570584. [45] Yan Duan, John Schulman, Xi Chen, Peter Bartlett, Ilya Sutskever, and Pieter Abbeel. 2016. RL2: Fast reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779 (2016). [46] Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jiang, Bill Yuchen Lin, Sean Welleck, Peter West, Chandra Bhagavatula, Ronan Le Bras, et al. 2024. Faith and fate: Limits of transformers on compositionality. Advances in Neural Information Processing Systems 36 (2024). [47] Adrien Ecoffet, Jeff Clune, and Joel Lehman. 2020. Open questions in creating safe open-ended AI: tensions between control and creativity. In Artificial Life Conference Proceedings 32. MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info . . . , 2735. [48] Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry Rudolph, and Aleksander Madry. 2020. Implementation matters in deep policy gradients: case study on ppo and trpo. arXiv preprint arXiv:2005.12729 (2020). [49] Maxence Faldor, Jenny Zhang, Antoine Cully, and Jeff Clune. 2024. OMNI-EPIC: Open-endedness via Models of human Notions of Interestingness with Environments Programmed in Code. arXiv preprint arXiv:2405.15568 (2024). [50] Abolfazl Farahani, Sahar Voghoei, Khaled Rasheed, and Hamid Arabnia. 2021. brief review of domain adaptation. Advances in data science and information engineering: proceedings from ICDATA 2020 and IKE 2020 (2021), 877894. [51] Manon Flageat, Bryan Lim, and Antoine Cully. 2024. Beyond Expected Return: Accounting for Policy Reproducibility When Evaluating Reinforcement Learning Algorithms. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 1202412032. [52] Luciano Floridi. 2023. AI as agency without intelligence: on ChatGPT, large language models, and other generative models. Philosophy & technology 36, 1 (2023), 15. [53] Richard Fortey. 2010. Trilobite: Eyewitness to evolution. Vintage. [54] David Freedman. 2010. Why scientific studies are so often wrong: The streetlight effect. Discover Magazine 26 (2010), 14. [55] Milton Friedman. 1976. Price theory. Routledge. [56] Javier Garcıa and Fernando Fernández. 2015. comprehensive survey on safe reinforcement learning. Journal of Machine Learning Research 16, 1 (2015), 14371480. [57] John Gerhart and Marc Kirschner. 2007. The theory of facilitated variation. Proceedings of the National Academy of Sciences 104, suppl_1 (2007), 85828589. [58] Mohammad Ghavamzadeh, Shie Mannor, Joelle Pineau, Aviv Tamar, et al. 2015. Bayesian reinforcement learning: survey. Foundations and Trends in Machine Learning 8, 5-6 (2015), 359483. [59] Owen Gilbert. 2020. Natural reward drives the advancement of life. Rethinking Ecology 5 (2020). 30 Lehman et al. [60] Robert Givan, Sonia Leach, and Thomas Dean. 2000. Bounded-parameter Markov decision processes. Artificial Intelligence 122, 1-2 (2000), 71109. [61] Charles AE Goodhart and CAE Goodhart. 1984. Problems of monetary management: the UK experience. Springer. [62] Steve Grand. 2001. Creation: Life and how to make it. Harvard University Press. [63] Djordje Grbic, Rasmus Berg Palm, Elias Najarro, Claire Glanois, and Sebastian Risi. 2021. Evocraft: new challenge for open-endedness. In Applications of Evolutionary Computation: 24th International Conference, EvoApplications 2021, Held as Part of EvoStar 2021, Virtual Event, April 79, 2021, Proceedings 24. Springer, 325340. [64] Yu Gu, Sue Kase, Michelle Vanni, Brian Sadler, Percy Liang, Xifeng Yan, and Yu Su. 2021. Beyond IID: three levels of generalization for question answering on knowledge bases. In Proceedings of the Web Conference 2021. 34773488. [65] Abhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine, and Karol Hausman. 2019. Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning. arXiv preprint arXiv:1910.11956 (2019). [66] Dylan Hadfield-Menell, Smitha Milli, Pieter Abbeel, Stuart Russell, and Anca Dragan. 2017. Inverse reward design. Advances in neural information processing systems 30 (2017). [67] Pamela Haley and DONALD Soloway. 1992. Extrapolation limitations of multilayer feedforward neural networks. In [Proceedings 1992] IJCNN international joint conference on neural networks, Vol. 4. IEEE, 2530. [68] Robin Hanson. 2016. The age of Em: Work, love, and life when robots rule the earth. Oxford University Press. [69] Hado Hasselt. 2010. Double Q-learning. Advances in neural information processing systems 23 (2010). [70] Friedrich August Hayek. 2013. The use of knowledge in society. In Modern understandings of liberty and property. Routledge, 2738. [71] Douglas Heaven et al. 2019. Why deep-learning AIs are so easy to fool. Nature 574, 7777 (2019), 163166. [72] Dan Hendrycks, Nicholas Carlini, John Schulman, and Jacob Steinhardt. 2021. Unsolved problems in ml safety. arXiv preprint arXiv:2109.13916 (2021). [73] Mario Herger. 2024. Waymo Confused Behind Trailer With Tree. https://thelastdriverlicenseholder.com/2024/05/14/waymoconfused-behind-a-trailer-with-a-tree/ Accessed: 2024-11-25. [74] José Hernández-Orallo. 2017. Evaluation in artificial intelligence: from task-oriented to ability-oriented measurement. Artificial Intelligence Review 48 (2017), 397447. [75] Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. 2018. Rainbow: Combining improvements in deep reinforcement learning. In Proceedings of the AAAI conference on artificial intelligence, Vol. 32. [76] Irina Higgins, Arka Pal, Andrei Rusu, Loic Matthey, Christopher Burgess, Alexander Pritzel, Matthew Botvinick, Charles Blundell, and Alexander Lerchner. 2017. Darla: Improving zero-shot transfer in reinforcement learning. In International Conference on Machine Learning. PMLR, 14801490. [77] Babak Hodjat, Hormoz Shahrzad, and Risto Miikkulainen. 2024. Domain-Independent Lifelong Problem Solving Through Distributed ALife Actors. Artificial Life 30, 2 (2024), 259276. [78] Sara Hooker. 2021. The hardware lottery. Commun. ACM 64, 12 (2021), 5865. [79] Shengran Hu, Cong Lu, and Jeff Clune. 2024. Automated design of agentic systems. arXiv preprint arXiv:2408.08435 (2024). [80] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al. 2023. survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. ACM Transactions on Information Systems (2023). [81] Qian Huang, Jian Vora, Percy Liang, and Jure Leskovec. 2023. Benchmarking large language models as ai research agents. In NeurIPS 2023 Foundation Models for Decision Making Workshop. [82] Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel. 2017. Adversarial attacks on neural network policies. arXiv preprint arXiv:1702.02284 (2017). [83] Edward Hughes, Michael Dennis, Jack Parker-Holder, Feryal Behbahani, Aditi Mavalankar, Yuge Shi, Tom Schaul, and Tim Rocktaschel. 2024. Open-Endedness is Essential for Artificial Superhuman Intelligence. arXiv preprint arXiv:2406.04268 (2024). [84] Chia-Chun Hung, Timothy Lillicrap, Josh Abramson, Yan Wu, Mehdi Mirza, Federico Carnevale, Arun Ahuja, and Greg Wayne. 2019. Optimizing agent behavior over long time scales by transporting value. Nature communications 10, 1 (2019), 5223. [85] Maria Luísa Jabbur, Benjamin Bratton, and Carl Hirschie Johnson. 2024. Bacteria can anticipate the seasons: photoperiodism in cyanobacteria. Science 385, 6713 (2024), 11051111. [86] Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech Czarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, et al. 2017. Population based training of neural networks. arXiv preprint arXiv:1711.09846 (2017). [87] Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. 2023. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770 (2023). [88] Jerald Johnson and Mark Belk. 2020. Predators as agents of selection and diversification. Diversity 12, 11 (2020), 415. [89] Samuel GB Johnson, Avri Bilovich, and David Tuckett. 2023. Conviction narrative theory: theory of choice under radical uncertainty. Behavioral and Brain Sciences 46 (2023), e82. Evolution and The Knightian Blindspot of Machine Learning 31 [90] Nadav Kashtan, Elad Noor, and Uri Alon. 2007. Varying environments can speed up evolution. Proceedings of the National Academy of Sciences 104, 34 (2007), 1371113716. [91] Stuart Kauffman. 1993. The origins of order: Self-organization and selection in evolution. Oxford University Press. [92] John Kay and Mervyn King. 2020. Radical uncertainty: Decision-making beyond the numbers. WW Norton & Company. [93] Christopher Kempes, David Wolpert, Zachary Cohen, and Juan Pérez-Mercader. 2017. The thermodynamic efficiency of computations made in cells across the range of life. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 375, 2109 (2017), 20160343. [94] John Maynard Keynes. 1921. treatise on probability. Courier Corporation. [95] Khimya Khetarpal, Matthew Riemer, Irina Rish, and Doina Precup. 2022. Towards continual reinforcement learning: review and perspectives. Journal of Artificial Intelligence Research 75 (2022), 14011476. [96] Robert Kirk, Amy Zhang, Edward Grefenstette, and Tim Rocktäschel. 2023. survey of zero-shot generalisation in deep reinforcement learning. Journal of Artificial Intelligence Research 76 (2023), 201264. [97] Louis Kirsch, Sjoerd van Steenkiste, and Jürgen Schmidhuber. 2019. Improving generalization in meta reinforcement learning using learned objectives. arXiv preprint arXiv:1910.04098 (2019). [98] Marc Kirschner and John Gerhart. 2005. The plausibility of life: Resolving Darwins dilemma. Yale University Press. [99] Hiroaki Kitano. 2004. Biological robustness. Nature Reviews Genetics 5, 11 (2004), 826837. [100] Frank Knight. 1921. Risk, uncertainty and profit. Hart, Schaffner and Marx (1921). [101] Loizos Kounios, Jeff Clune, Kostas Kouvaris, Günter Wagner, Mihaela Pavlicev, Daniel Weinreich, and Richard Watson. 2016. Resolving the paradox of evolvability with learning theory: How evolution learns to improve evolvability on rugged fitness landscapes. arXiv preprint arXiv:1612.05955 (2016). [102] Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. 2012. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems 25 (2012). [103] Thomas Kuhn. 1997. The structure of scientific revolutions. Vol. 962. University of Chicago press Chicago. [104] Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. 2019. Stabilizing off-policy q-learning via bootstrapping error reduction. Advances in neural information processing systems 32 (2019). [105] Akarsh Kumar, Chris Lu, Louis Kirsch, Yujin Tang, Kenneth Stanley, Phillip Isola, and David Ha. 2024. Automating the Search for Artificial Life with Foundation Models. arXiv preprint arXiv:2412.17799 (2024). [106] Saurabh Kumar, Aviral Kumar, Sergey Levine, and Chelsea Finn. 2020. One solution is not all you need: Few-shot extrapolation via structured maxent rl. Advances in Neural Information Processing Systems 33 (2020), 81988210. [107] Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. 2024. Rewardbench: Evaluating reward models for language modeling. arXiv preprint arXiv:2403.13787 (2024). [108] Christopher Langton. 1997. Artificial life: An overview. (1997). [109] Angeliki Lazaridou, Adhi Kuncoro, Elena Gribovskaya, Devang Agrawal, Adam Liska, Tayfun Terzi, Mai Gimenez, Cyprien de Masson dAutume, Tomas Kocisky, Sebastian Ruder, et al. 2021. Mind the gap: Assessing temporal generalization in neural language models. Advances in Neural Information Processing Systems 34 (2021), 2934829363. [110] Hyunin Lee, David Abel, Ming Jin, Javad Lavaei, and Somayeh Sojoudi. 2024. Black Swan Hypothesis in Markov Decision Process via Irrationality. arXiv preprint arXiv:2407.18422 (2024). [111] Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Ren Lu, Thomas Mesnard, Johan Ferret, Colton Bishop, Ethan Hall, Victor Carbune, and Abhinav Rastogi. [n. d.]. Rlaif: Scaling reinforcement learning from human feedback with ai feedback. ([n. d.]). [112] Yoonho Lee, Huaxiu Yao, and Chelsea Finn. 2023. Diversify and disambiguate: Out-of-distribution robustness via disagreement. In The Eleventh International Conference on Learning Representations. [113] Joel Lehman, Jonathan Gordon, Shawn Jain, Kamal Ndousse, Cathy Yeh, and Kenneth Stanley. 2023. Evolution through large models. In Handbook of Evolutionary Machine Learning. Springer, 331366. [114] Joel Lehman and Risto Miikkulainen. 2015. Extinction events can accelerate evolution. PloS one 10, 8 (2015), e0132886. [115] Joel Lehman and Kenneth Stanley. 2010. Revising the evolutionary computation abstraction: minimal criteria novelty search. In Proceedings of the 12th annual conference on Genetic and evolutionary computation. 103110. [116] Joel Lehman and Kenneth Stanley. 2011. Abandoning objectives: Evolution through the search for novelty alone. Evolutionary computation 19, 2 (2011), 189223. [117] Joel Lehman and Kenneth Stanley. 2011. Improving evolvability through novelty search and self-adaptation. In 2011 IEEE congress of evolutionary computation (CEC). IEEE, 26932700. [118] Joel Lehman and Kenneth Stanley. 2013. Evolvability is inevitable: Increasing evolvability without the pressure to adapt. PloS one 8, 4 (2013), e62186. [119] Joel Lehman and Kenneth Stanley. 2015. Investigating biological assumptions through radical reimplementation. Artificial Life 21, (2015), 2146. 32 Lehman et al. [120] Joel Lehman, Bryan Wilder, and Kenneth Stanley. 2016. On the critical role of divergent selection in evolvability. Frontiers in Robotics and AI 3 (2016), 45. [121] Máté Lengyel and Peter Dayan. 2007. Hippocampal contributions to control: the third way. Advances in neural information processing systems 20 (2007). [122] Dirk Lewandowski, Sebastian Sünkler, and Nurce Yagci. 2021. The influence of search engine optimization on Googles results: multi-dimensional approach for detecting SEO. In Proceedings of the 13th ACM Web Science Conference 2021. 1220. [123] Richard Lister. 1987. The use of plus-maze to measure anxiety in the mouse. Psychopharmacology 92 (1987), 180185. [124] Michael Littman. 1994. Markov games as framework for multi-agent reinforcement learning. In Machine learning proceedings 1994. Elsevier, 157163. [125] Henry Liu and Shuo Feng. 2024. Curse of rarity for autonomous vehicles. nature communications 15, 1 (2024), 4808. [126] Yantao Liu, Zijun Yao, Rui Min, Yixin Cao, Lei Hou, and Juanzi Li. 2024. RM-bench: Benchmarking reward models of language models with subtlety and style. arXiv preprint arXiv:2410.16184 (2024). [127] Giuseppe Longo, Maël Montévil, and Stuart Kauffman. 2012. No entailing laws, but enablement in the evolution of the biosphere. In Proceedings of the 14th annual conference companion on Genetic and evolutionary computation. 13791392. [128] Kevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch. 2020. Reset-free lifelong learning with skill-space planning. arXiv preprint arXiv:2012.03548 (2020). [129] Robert Lucas Jr. 1972. Expectations and the Neutrality of Money. Journal of economic theory 4, 2 (1972), 103124. [130] Yecheng Jason Ma, William Liang, Hung-Ju Wang, Sam Wang, Yuke Zhu, Linxi Fan, Osbert Bastani, and Dinesh Jayaraman. 2024. DrEureka: Language Model Guided Sim-To-Real Transfer. arXiv preprint arXiv:2406.01967 (2024). [131] Donald MacKenzie and Taylor Spears. 2014. device for being able to book P&L: The Organizational Embedding of the Gaussian Copula. Social Studies of Science 44, 3 (2014), 418440. [132] Olivia Macmillan-Scott and Mirco Musolesi. 2023. (Ir) rationality in AI: State of the Art, Research Challenges and Open Questions. arXiv preprint arXiv:2311.17165 (2023). [133] Sridhar Mahadevan. 1996. Average reward reinforcement learning: Foundations, algorithms, and empirical results. Machine learning 22, 1 (1996), 159195. [134] Simon McGregor. 2023. Is ChatGPT Really Disembodied?. In Artificial Life Conference Proceedings 35, Vol. 2023. MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info . . . , 133. [135] Thomas Miconi. 2008. Evosphere: evolutionary dynamics in population of fighting virtual creatures. In 2008 IEEE Congress on Evolutionary Computation (IEEE World Congress on Computational Intelligence). IEEE, 30663073. [136] Oliver Mihatsch and Ralph Neuneier. 2002. Risk-sensitive reinforcement learning. Machine learning 49 (2002), 267290. [137] Melanie Mitchell. 2021. Why AI is harder than we think. arXiv preprint arXiv:2104.12871 (2021). [138] Volodymyr Mnih. 2013. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602 (2013). [139] Philippe Mongin. 1998. Expected utility theory. (1998). [140] Harold Mooney and Elsa Cleland. 2001. The evolutionary impact of invasive species. Proceedings of the National Academy of Sciences 98, 10 (2001), 54465451. [141] Janosch Moos, Kay Hansel, Hany Abdulsamad, Svenja Stark, Debora Clever, and Jan Peters. 2022. Robust reinforcement learning: review of foundations and recent advances. Machine Learning and Knowledge Extraction 4, 1 (2022), 276315. [142] Hans Moravec. 1988. Mind Children: The Future of Robot and Human Intelligence. Harvard UP (1988). [143] Andrew Ng, Stuart Russell, et al. 2000. Algorithms for inverse reinforcement learning.. In Icml, Vol. 1. 2. [144] Bostrom Nick. 2014. Superintelligence: Paths, dangers, strategies. (2014). [145] Eleni Nisioti, Claire Glanois, Elias Najarro, Andrew Dai, Elliot Meyerson, Joachim Winther Pedersen, Laetitia Teodorescu, Conor Hayes, Shyam Sudhakaran, and Sebastian Risi. 2024. From text to life: On the reciprocal relationship between artificial life and large language models. In Artificial Life Conference Proceedings 36, Vol. 2024. MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info . . . , 39. [146] Roman Novak, Yasaman Bahri, Daniel Abolafia, Jeffrey Pennington, and Jascha Sohl-Dickstein. 2018. Sensitivity and generalization in neural networks: an empirical study. arXiv preprint arXiv:1802.08760 (2018). [147] Charles Ofria and Claus Wilke. 2004. Avida: software platform for research in computational evolutionary biology. Artificial life 10, 2 (2004), 191229. [148] Arne Öhman and Susan Mineka. 2001. Fears, phobias, and preparedness: toward an evolved module of fear and fear learning. Psychological review 108, 3 (2001), 483. [149] Norman Packard, Mark Bedau, Alastair Channon, Takashi Ikegami, Steen Rasmussen, Kenneth Stanley, and Tim Taylor. 2019. An overview of open-ended evolution: Editorial introduction to the open-ended evolution ii special issue. Artificial life 25, 2 (2019), 93103. [150] Fabio Pardo, Arash Tavakoli, Vitaly Levdik, and Petar Kormushev. 2018. Time limits in reinforcement learning. In International Conference on Machine Learning. PMLR, 40454054. Evolution and The Knightian Blindspot of Machine Learning 33 [151] German Parisi, Ronald Kemker, Jose Part, Christopher Kanan, and Stefan Wermter. 2019. Continual lifelong learning with neural networks: review. Neural networks 113 (2019), 5471. [152] Joshua Payne and Andreas Wagner. 2019. The causes of evolvability and their evolution. Nature Reviews Genetics 20, 1 (2019), 2438. [153] David Pfennig and Karin Pfennig. 2012. Evolutions wedge: competition and the origins of diversity. Vol. 12. Univ of California Press. [154] Steven Piantadosi. 2023. Modern language models refute Chomskys approach to language. From fieldwork to linguistic theory: tribute to Dan Everett (2023), 353414. [155] Massimo Pigliucci, Gerd Müller, et al. 2010. Evolution: the extended synthesis. [156] Lerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav Gupta. 2017. Robust adversarial reinforcement learning. In International conference on machine learning. PMLR, 28172826. [157] Karl Popper. 2005. The logic of scientific discovery. Routledge. [158] Theodore Porter. 2020. Trust in numbers: The pursuit of objectivity in science and public life. (2020). [159] The Associated Press. 2024. U.S. to probe Teslas Full Self-Driving system after pedestrian killed. https://www.npr.org/2024/10/19/g-s129030/us-probe-tesla-full-self-driving-system Accessed: 2024-11-25. [160] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. 2024. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems 36 (2024). [161] Janarthanan Rajendran, Alexander Irpan, and Eric Jang. 2020. Meta-learning requires meta-augmentation. Advances in Neural Information Processing Systems 33 (2020), 57055715. [162] Francesco Redi. 1909. Experiments on the Generation of Insects. Open court publishing Company. [163] Anthony Ricciardi. 2003. Predicting the impacts of an introduced species from its invasion history: an empirical approach applied to zebra mussel invasions. Freshwater biology 48, 6 (2003), 972981. [164] Mark Ridley. [n. d.]. Animal behavior: An introduction to behavioral mechanisms, development, and ecology. (No Title) ([n. d.]). [165] Violina Rindova and Hugh Courtney. 2020. To shape or adapt: Knowledge problems, epistemologies, and strategic postures under Knightian uncertainty. Academy of Management Review 45, 4 (2020), 787807. [166] Alan Roberts and Ken Tregonning. 1980. The robustness of natural systems. Nature 288, 5788 (1980), 265266. [167] Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. 2017. Evolution strategies as scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864 (2017). [168] Spyridon Samothrakis, Dennis JNJ Soemers, and Damian Machlanski. 2024. Games of Knightian Uncertainty. arXiv preprint arXiv:2406.18178 (2024). [169] Mikayel Samvelyan, Akbir Khan, Michael Dennis, Minqi Jiang, Jack Parker-Holder, Jakob Foerster, Roberta Raileanu, and Tim Rocktäschel. 2023. MAESTRO: Open-ended environment design for multi-agent reinforcement learning. arXiv preprint arXiv:2303.03376 (2023). [170] Mikayel Samvelyan, Sharath Chandra Raparthy, Andrei Lupu, Eric Hambro, Aram Markosyan, Manish Bhatt, Yuning Mao, Minqi Jiang, Jack Parker-Holder, Jakob Foerster, et al. 2024. Rainbow teaming: Open-ended generation of diverse adversarial prompts. arXiv preprint arXiv:2402.16822 (2024). [171] Leonard Savage. 1972. The foundations of statistics. Courier Corporation. [172] John Schulman. 2016. Optimizing expectations: From deep reinforcement learning to stochastic computation graphs. Ph. D. Dissertation. UC Berkeley. [173] Faiz Siddiqui and Jeremy B. Merrill. 2022. Tesla drivers report surge in phantom braking. The Washington Post (2022). https: //www.washingtonpost.com/technology/2022/02/02/tesla-phantom-braking/ [174] Mark Siegal and Aviv Bergman. 2002. Waddingtons canalization revisited: developmental stability and evolution. Proceedings of the National Academy of Sciences 99, 16 (2002), 1052810532. [175] David Silver, Satinder Singh, Doina Precup, and Richard Sutton. 2021. Reward is enough. Artificial Intelligence 299 (2021), 103535. [176] Anoopkumar Sonar, Vincent Pacelli, and Anirudha Majumdar. 2021. Invariant policy optimization: Towards stronger generalization in reinforcement learning. In Learning for Dynamics and Control. PMLR, 2133. [177] Xingyou Song, Yiding Jiang, Stephen Tu, Yilun Du, and Behnam Neyshabur. 2019. Observational overfitting in reinforcement learning. arXiv preprint arXiv:1912.02975 (2019). [178] Lisa Soros and Kenneth Stanley. 2014. Identifying necessary conditions for open-ended evolution through the artificial life world of chromaria. In Artificial Life Conference Proceedings. MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info . . . , 793800. [179] Lisa Soros, Joel Lehman, and Kenneth Stanley. 2017. Open-endedness: The last grand challenge youve never heard of. [180] Matthijs TJ Spaan. 2012. Partially observable Markov decision processes. In Reinforcement learning: State-of-the-art. Springer, 387414. [181] Kenneth Stanley and Joel Lehman. 2015. Why greatness cannot be planned: The myth of the objective. Springer. [182] Ian Stewart, Sameera Horawalavithana, Brendan Kennedy, Sai Munikoti, and Karl Pazdernik. 2024. Surprisingly Fragile: Assessing and Addressing Prompt Instability in Multimodal Foundation Models. arXiv preprint arXiv:2408.14595 (2024). 34 Lehman et al. [183] James Stroud and Jonathan Losos. 2016. Ecological opportunity and adaptive radiation. Annual Review of Ecology, Evolution, and Systematics 47, 1 (2016), 507532. [184] Felipe Petroski Such, Vashisht Madhavan, Edoardo Conti, Joel Lehman, Kenneth Stanley, and Jeff Clune. 2017. Deep neuroevolution: Genetic algorithms are competitive alternative for training deep neural networks for reinforcement learning. arXiv preprint arXiv:1712.06567 (2017). [185] Cass Sunstein. 2023. Knightian uncertainty. Available at SSRN 4662711 (2023). [186] Sundaram Suresh, Narasimhan Sundararajan, and Paramasivan Saratchandran. 2008. Risk-sensitive loss functions for sparse multicategory classification problems. Information Sciences 178, 12 (2008), 26212638. [187] Richard Sutton. 2019. The bitter lesson. Incomplete Ideas (blog) 13, 1 (2019), 38. [188] Richard Sutton and Andrew Barto. 2018. Reinforcement learning: An introduction. MIT press. [189] Nassim Nicholas Taleb. 2010. The Black Swan: The Impact of the Highly Improbable\". Vol. 2. Random house trade paperbacks. [190] Nassim Nicholas Taleb. 2014. Antifragile: Things that gain from disorder. Vol. 3. Random House Trade Paperbacks. [191] Tim Taylor, Mark Bedau, Alastair Channon, David Ackley, Wolfgang Banzhaf, Guillaume Beslon, Emily Dolson, Tom Froese, Simon Hickinbotham, Takashi Ikegami, et al. 2016. Open-ended evolution: Perspectives from the OEE workshop in York. Artificial life 22, 3 (2016), 408423. [192] Adaptive Agent Team, Jakob Bauer, Kate Baumli, Satinder Baveja, Feryal Behbahani, Avishkar Bhoopchand, Nathalie Bradley-Schmieg, Michael Chang, Natalie Clay, Adrian Collister, et al. 2023. Human-timescale adaptation in an open-ended task space. arXiv preprint arXiv:2301.07608 (2023). [193] Open Ended Learning Team, Adam Stooke, Anuj Mahajan, Catarina Barros, Charlie Deck, Jakob Bauer, Jakub Sygnowski, Maja Trebacz, Max Jaderberg, Michael Mathieu, et al. 2021. Open-ended learning leads to generally capable agents. arXiv preprint arXiv:2107.12808 (2021). [194] Kurt Thearling and Thomas Ray. 1994. Evolving multi-cellular artificial life. (1994). [195] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. 2017. Domain randomization for transferring deep neural networks from simulation to the real world. In 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS). IEEE, 2330. [196] Keyon Vafa, Ashesh Rambachan, and Sendhil Mullainathan. 2024. Do large language models perform the way people expect? measuring the human generalization function. arXiv preprint arXiv:2406.01382 (2024). [197] Peter Vamplew, Benjamin Smith, Johan Källström, Gabriel Ramos, Roxana Rădulescu, Diederik Roijers, Conor Hayes, Fredrik Heintz, Patrick Mannion, Pieter JK Libin, et al. 2022. Scalar reward is not enough: response to silver, singh, precup and sutton (2021). Autonomous Agents and Multi-Agent Systems 36, 2 (2022), 41. [198] Joaquin Vanschoren. 2018. Meta-learning: survey. arXiv preprint arXiv:1810.03548 (2018). [199] Vaswani. 2017. Attention is all you need. Advances in Neural Information Processing Systems (2017). [200] Anna Vettoruzzo, Mohamed-Rafik Bouguelia, Joaquin Vanschoren, Thorsteinn Rognvaldsson, and KC Santosh. 2024. Advances and challenges in meta-learning: technical review. IEEE Transactions on Pattern Analysis and Machine Intelligence (2024). [201] Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu. 2024. comprehensive survey of continual learning: theory, method and application. IEEE Transactions on Pattern Analysis and Machine Intelligence (2024). [202] Rui Wang, Joel Lehman, Jeff Clune, and Kenneth Stanley. 2019. Paired open-ended trailblazer (poet): Endlessly generating increasingly complex and diverse learning environments and their solutions. arXiv preprint arXiv:1901.01753 (2019). [203] Ruoyao Wang, Graham Todd, Ziang Xiao, Xingdi Yuan, Marc-Alexandre Côté, Peter Clark, and Peter Jansen. 2024. Can Language Models Serve as Text-Based World Simulators? arXiv preprint arXiv:2406.06485 (2024). [204] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. 2022. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682 (2022). [205] Peter West, Ximing Lu, Nouha Dziri, Faeze Brahman, Linjie Li, Jena Hwang, Liwei Jiang, Jillian Fisher, Abhilasha Ravichander, Khyathi Chandu, et al. 2023. The Generative AI Paradox:What It Can Create, It May Not Understand. In The Twelfth International Conference on Learning Representations. [206] Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Akyürek, Boyuan Chen, Bailin Wang, Najoung Kim, Jacob Andreas, and Yoon Kim. 2023. Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks. arXiv preprint arXiv:2307.02477 (2023). [207] Clive DL Wynne and Monique AR Udell. 2020. Animal cognition: Evolution, behavior and cognition. Bloomsbury Publishing. [208] Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. 2024. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. arXiv preprint arXiv:2404.07972 (2024). [209] Keyulu Xu, Mozhi Zhang, Jingling Li, Simon Du, Ken-ichi Kawarabayashi, and Stefanie Jegelka. 2020. How neural networks extrapolate: From feedforward to graph neural networks. arXiv preprint arXiv:2009.11848 (2020). Evolution and The Knightian Blindspot of Machine Learning 35 [210] Zihao Xu, Yi Liu, Gelei Deng, Yuekang Li, and Stjepan Picek. 2024. LLM Jailbreak Attack versus Defense TechniquesA Comprehensive Study. arXiv preprint arXiv:2402.13457 (2024). [211] Roman Yampolskiy. 2018. Why we do not evolve software? Analysis of evolutionary algorithms. Evolutionary Bioinformatics 14 (2018), 1176934318815906. [212] Weirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, and Yang Gao. 2021. Mastering atari games with limited data. Advances in neural information processing systems 34 (2021), 2547625488. [213] Chiyuan Zhang, Oriol Vinyals, Remi Munos, and Samy Bengio. 2018. study on overfitting in deep reinforcement learning. arXiv preprint arXiv:1804.06893 (2018). [214] Jenny Zhang, Joel Lehman, Kenneth Stanley, and Jeff Clune. 2023. Omni: Open-endedness via models of human notions of interestingness. arXiv preprint arXiv:2306.01711 (2023). [215] Peng Zhang, Jiuling Wang, Ali Farhadi, Martial Hebert, and Devi Parikh. 2014. Predicting failures of vision systems. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 35663573. [216] Yukun Zhao, Lingyong Yan, Weiwei Sun, Guoliang Xing, Shuaiqiang Wang, Chong Meng, Zhicong Cheng, Zhaochun Ren, and Dawei Yin. 2024. Improving the Robustness of Large Language Models via Consistency Alignment. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), Nicoletta Calzolari, Min-Yen Kan, Veronique Hoste, Alessandro Lenci, Sakriani Sakti, and Nianwen Xue (Eds.). ELRA and ICCL, Torino, Italia, 89318941. https://aclanthology.org/2024.lrec-main.782 [217] Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and Chen Change Loy. 2022. Domain generalization: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence 45, 4 (2022), 43964415. [218] Fei Zhu, Shijie Ma, Zhen Cheng, Xu-Yao Zhang, Zhaoxiang Zhang, and Cheng-Lin Liu. 2024. Open-world machine learning: review and new outlooks. arXiv preprint arXiv:2403.01759 (2024)."
        }
    ],
    "affiliations": [
        "Cognizant AI Labs",
        "Second Nature AI"
    ]
}