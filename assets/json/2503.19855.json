{
    "paper_title": "Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time Thinking",
    "authors": [
        "Xiaoyu Tian",
        "Sitong Zhao",
        "Haotian Wang",
        "Shuaiting Chen",
        "Yunjie Ji",
        "Yiping Peng",
        "Han Zhao",
        "Xiangang Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in large language models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have demonstrated the effectiveness of test-time scaling, where extended reasoning processes substantially enhance model performance. Despite this, current models are constrained by limitations in handling long texts and reinforcement learning (RL) training efficiency. To address these issues, we propose a simple yet effective test-time scaling approach Multi-round Thinking. This method iteratively refines model reasoning by leveraging previous answers as prompts for subsequent rounds. Extensive experiments across multiple models, including QwQ-32B and DeepSeek-R1, consistently show performance improvements on various benchmarks such as AIME 2024, MATH-500, GPQA-diamond, and LiveCodeBench. For instance, the accuracy of QwQ-32B improved from 80.3% (Round 1) to 82.1% (Round 2) on the AIME 2024 dataset, while DeepSeek-R1 showed a similar increase from 79.7% to 82.0%. These results confirm that Multi-round Thinking is a broadly applicable, straightforward approach to achieving stable enhancements in model performance, underscoring its potential for future developments in test-time scaling techniques. The key prompt: {Original question prompt} The assistant's previous answer is: <answer> {last round answer} </answer>, and please re-answer."
        },
        {
            "title": "Start",
            "content": "Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time Thinking Xiaoyu Tian, Sitong Zhao, Haotian Wang, Shuaiting Chen, Yunjie Ji, Yiping Peng, Han Zhao, and Xiangang Li a-m-team Abstract Recent advances in large language models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have demonstrated the effectiveness of test-time scaling, where extended reasoning processes substantially enhance model performance. Despite this, current models are constrained by limitations in handling long texts and reinforcement learning (RL) training efficiency. To address these issues, we propose simple yet effective test-time scaling approachMulti-round Thinking. This method iteratively refines model reasoning by leveraging previous answers as prompts for subsequent rounds. Extensive experiments across multiple models, including QwQ-32B and DeepSeek-R1, consistently show performance improvements on various benchmarks such as AIME 2024, MATH-500, GPQA-diamond, and LiveCodeBench. For instance, the accuracy of QwQ-32B improved from 80.3% (Round 1) to 82.1% (Round 2) on the AIME 2024 dataset, while DeepSeek-R1 showed similar increase from 79.7% to 82.0%. These results confirm that Multiround Thinking is broadly applicable, straightforward approach to achieving stable enhancements in model performance, underscoring its potential for future developments in test-time scaling techniques. The key prompt: Original question prompt The assistants previous answer is: re-answer. <answer> last round answer </answer>, and please 5 2 0 2 5 2 ] . [ 1 5 5 8 9 1 . 3 0 5 2 : r Figure 1: Benchmark performance of QwQ-32B using Multi-round Thinking. 1 Figure 2: Benchmark performance of DeepSeek-R1 using Multi-round Thinking."
        },
        {
            "title": "Introduction",
            "content": "Inference test-time compute (Yang et al., 2025; Wu et al., 2025) refers to the computational resources utilized by large language models (LLMs) during the generation of prompt responses, distinct from the training compute used for model creation and refinement. Leveraging step-by-step reasoning has shown substantial improvements in solving complex tasks by explicitly providing models with intermediate reasoning steps(Lightman et al., 2023; Wei et al., 2023), significantly enhancing accuracy. In recent years, the performance improvements of language models have largely depended on massive-scale self-supervised pre-training (Kaplan et al., 2020; Hoffmann et al., 2022), scaling up training-time compute. However, as advancements in training-time scaling slow, increasing attention is turning towards scaling up test-time compute (Muennighoff et al., 2025; Chen et al., 2025). OpenAI (OpenAI, 2024a) pioneered this approach with their o1 series models (OpenAI, 2024b) using large-scale reinforcement learning (RL). DeepSeek further advanced test-time scaling by introducing the DeepSeek-R1 (DeepSeek-AI, 2025), successfully achieving performance comparable to OpenAIs o1 series. Prior approaches in inference test-time compute have included majority voting methods and external reward-based best-of-N strategies (Levi, 2024; Diao et al., 2024). Unlike repetitive sampling, sequential expansion approaches enable models to iteratively refine attempts based on prior outcomes. Many researchers have attempted to replicate or extend their methods, employing Monte Carlo Tree Search (MCTS) (Zhou et al., 2024; Choi et al., 2023), multi-agent approaches (Qin et al., 2024; Li et al., 2025), some work based on Process Reward Model(PRM) (Wang et al., 2024; Lightman et al., 2023). Despite these successes, existing methods exhibit critical limitations. PRM face challenges such as defining fine-grained reasoning steps clearly, verifying intermediate reasoning correctness, and mitigating reward hacking (Amodei et al., 2016; Langosco et al., 2023), making automated labeling challenging and manual labeling impractical for scaling. Similarly, MCTS methods encounter difficulties due to vast search spaces, often causing models to become trapped in local optima, and depend heavily on sophisticated scoring models that are challenging to train (DeepSeek-AI, 2025). Addressing these issues, DeepSeek introduced rule-based reward system combined with large-scale reinforcement learning (RL), enabling clearer guidance and promoting model self-reflection and deeper reasoning (DeepSeek-AI, 2025). However, consistently identifying optimal reasoning paths remains challenging. Inspired by human cognitive behaviors, we propose novel test-time scaling strategy named Multi-round Thinking. This method allows the model to iteratively reconsider previous answers independently, using only the final answer from previous rounds as input prompts, discarding prior reasoning steps. This approach 2 parallels human cognitive processes, breaking cognitive inertia and enabling the model to correct entrenched reasoning errors. Our experimental results demonstrate the effectiveness of this intuitive approach. For example, using the DeepSeek-R1 model (DeepSeek-AI, 2025), performance improvements were observed across multiple benchmarks: on AIME 2024 (MAA, 2024), pass@1 increased from 79.7% (Round 1) to 82.0% (Round 2); on GPQA-Diamond (Rein et al., 2023), it rose from 74.0% to 74.8%; and on LiveCodeBench (Jain et al., 2024), performance improved from 65.3% to 67.1%. These findings underscore the substantial potential of iterative thinking for further exploiting the benefits of test-time scaling."
        },
        {
            "title": "2 Approach",
            "content": "We introduce novel Multi-round Thinking approach designed to significantly enhance reasoning capabilities in large language models (LLMs). In contrast to traditional single-step reasoning methods, our approach iteratively refines answers through multiple rounds of inference. Each round takes the answer from the previous iteration (without intermediate reasoning steps) as part of new input prompt, encouraging independent reconsideration and correction. This iterative process helps models avoid cognitive inertia, analogous to human strategies in overcoming entrenched errors in reasoning. The Multi-round Thinking methodology operates explicitly as follows: Given an original user prompt Puser, the inference and refinement process proceeds iteratively: Initial Round (Round 1): The language model receives the initial prompt and generates the first round of reasoning and final answer: (Puser) {T hinking1, Answer1} (1) Subsequent Rounds (Round n, 2): In each subsequent inference round, intermediate reasoning traces (T hinkingn1) from the previous iteration are discarded, retaining only the final answer (Answern1). The prompt for the next round is constructed by concatenating the original user prompt and the previously obtained answer: Pn = Puser Answern1 (2) The model independently reevaluates the newly formed prompt and produces an updated reasoning trace and refined answer: This iterative refinement cycle can be formally represented as follows: (Pn) {T hinkingn, Answern} P1 = Puser, (P1) {T hinking1, Answer1} P2 = Puser Answer1, (P2) {T hinking2, Answer2} P3 = Puser Answer2, (P3) {T hinking3, Answer3} ... Pn = Puser Answern1, (Pn) {T hinkingn, Answern} (3) (4) (5) (6) (7) (8) In these equations, denotes the textual concatenation operation used to form the iterative prompts. Through this repeated refinement procedure, the model is encouraged to reconsider previous conclusions independently, effectively minimizing cognitive inertia and systematically improving the quality of reasoning outcomes. Specifically, for given question prompt , we first use reasoning model to answer the question, producing thought process and an answer A. Then, we concatenate the original question prompt and the answer using the following prompt: Original question prompt The assistants previous answer is: re-answer. <answer> last round answer </answer>, and please 3 We then send this prompt to the large model again to generate new answer. Using this method, we can obtain multi-turn responses."
        },
        {
            "title": "3.1.1 Benchmark",
            "content": "We evaluated the reasoning ability of the model using LiveCodeBench (Jain et al., 2024), GPQA-Diamond (Rein et al., 2023), AIME 2024 (MAA, 2024), and MATH-500 (Lightman et al., 2023). These benchmarks span multiple fields and difficulty levels, enabling thorough assessment of the models reasoning performance across diverse scenarios."
        },
        {
            "title": "3.1.2 Evaluation Methodology",
            "content": "We standardized the evaluation conditions by setting the maximum generation length at 32,768 tokens. For benchmarks requiring stochastic sampling, we uniformly set the temperature to 0.6 and the top-p value to 0.95. Specifically, for AIME 2024 (MAA, 2024), we generated 32 samples per query to calculate pass@1 accuracy. For LiveCodeBench (Jain et al., 2024) and GPQA-Diamond (Rein et al., 2023), we generated 8 samples per query to estimate pass@1. For MATH-500 (Lightman et al., 2023), we generated 4 responses per query, also to estimate pass@1 accuracy. The primary evaluation metric adopted was the global average accuracy across all benchmarks."
        },
        {
            "title": "3.2 Results and Analysis",
            "content": "3.2.1 Overall Results of Multi-round Thinking Experimental results comparing initial (Round 1) and Multi-round Thinking (Round 2) performance are summarized in Table 1. Table 1: Model Performance Comparison (pass@1 accuracy) Between Single-round (Round 1) and Multiround Thinking (Round 2-4) Across Different Benchmarks Model Round Deepseek-R QwQ-32B DeepSeek-R1-Distill-Qwen-32B DeepSeek-R1-Distill-Qwen-7B AM-Distill-Qwen-32B 1 2 1 2 3 4 1 2 1 2 1 2 AIME 2024 pass@1 79.7 82.0 80.3 82.1 82.8 83.1 72.0 75.1 56.9 58.4 72.8 76. MATH500 pass@1 97.6 97.6 97.2 97.8 97.8 97.7 96.0 96.3 93.4 93.9 96.2 97.2 GPQADiamond pass@1 74.0 74.8 65.9 67.2 67.5 68.1 60.1 61.3 49.2 49.4 62.3 62.8 LiveCodeBench pass@1 Average 65.3 67.1 63.0 64.7 65.2 66.0 57.0 57.6 35.0 36.7 58.3 60. 79.2 80.4 76.6 78.0 78.3 78.7 71.3 72.6 58.6 59.6 72.4 74.2 Experimental results consistently show that our proposed Multi-round Thinking method effectively enhances reasoning performance across diverse benchmarks. As detailed in Table 1, each evaluated model showed notable improvement when transitioning from Round 1 to Round 2 reasoning. 4 Specifically, for the Deepseek-R1 model, accuracy improved from 79.7% to 82.0% on the AIME 2024 benchmark, remained consistently high at 97.6% on MATH-500, increased from 74.0% to 74.8% on GPQADiamond, and improved from 65.3% to 67.1% on LiveCodeBench. For the QwQ-32B model (Team, 2025), notable gains were achieved, with accuracy rising from 80.3% to 82.1% on AIME 2024 (MAA, 2024), 97.2% to 97.8% on MATH-500, 63.0% to 64.7% on GPQA-Diamond, and 65.9% to 67.2% on LiveCodeBench. Further, we evaluated our self-trained AM-Distill-Qwen-32B model, 32B model built upon the Qwen2.532B (Qwen, 2024) architecture and trained using distilled data from the DeepSeek-R1 model (refer to (Zhao et al., 2025) for distillation details). Experimental results demonstrated robust performance improvements with Multi-round Thinking: accuracy increased from 72.8% to 76.7% on AIME 2024, from 96.2% to 97.2% on MATH-500, from 62.3% to 62.8% on GPQA-Diamond, and from 58.3% to 60.2% on LiveCodeBench. Building upon this, we further examine the performance trajectory of QwQ-32B across four rounds of iterative thinking, as visualized in Figure 1. The model exhibits clear and steady upward trend across all benchmarks. From Round 1 to Round 4, QwQ-32Bs performance on AIME 2024 improves from 80.3% to 83.1%, indicating enhanced capability in competition-level mathematical reasoning. On the MATH-500 dataset, performance remains consistently high, fluctuating slightly within the 97.2%97.8% range. Substantial gains are observed on reasoning-heavy benchmarks like GPQA-Diamond, where accuracy increases from 65.9% to 68.1% over four rounds. Similarly, LiveCodeBench scores rise from 63.0% to 65.9%, reflecting notable enhancement in code understanding and generation tasks. These empirical results highlight the consistent advantage provided by the Multi-round Thinking methodology, underscoring its efficacy in iteratively refining reasoning processes, correcting earlier mistakes, and substantially boosting model performance across challenging reasoning tasks. In summary, our results strongly indicate that Multi-round Thinking consistently improves the reasoning performance of LLMs across various tasks. Particularly notable is its effectiveness on tasks demanding complex, iterative reasoning such as mathematics competitions and coding benchmarks. Moreover, the incremental and sustained improvements observed over multiple rounds underscore the robustness of this simple yet effective test-time scaling strategy. These findings suggest that Multi-round Thinking offers an efficient pathway to enhance model accuracy without additional training overhead, thus highlighting its practical value for real-world deployment and opening promising avenues for future research in test-time scaling methods. 3.2.2 Analysis of Word Frequency Changes To better understand how the models reasoning behavior evolves through multi-round thinking, we conduct lexical analysis focusing on four discourse markers: but, wait, maybe, and therefore. These words serve as linguistic signals for hesitation (but, wait, maybe) or decisiveness (therefore), and tracking their usage reveals insights into the models confidence and reasoning dynamics. Figure 3 presents the overall average usage frequency of these keywords across all AIME 2024 test samples. From Round 1 to Round 2, we observe consistent declines in the frequency of hesitation-related words. Specifically, but decreases from 68.3 to 44.8, wait from 67.9 to 51.0, and maybe from 23.7 to 15.8. Even therefore, although more conclusive word, experiences drop from 43.8 to 29.5, but the relative reduction is smaller. This suggests that, overall, the model adopts more concise and assertive phrasing in Round 2 responses. To analyze this shift in greater detail, Figure 4 breaks down average keyword usage by answer trajectory: IncorrectCorrect (I-C), IncorrectIncorrect (I-I), CorrectCorrect (C-C), and CorrectIncorrect (C-I). In most groups, there is significant drop in but, wait, and maybe from Round 1 to Round 2, reinforcing the observation that models tend to suppress uncertain or self-interruptive phrasing after one round of reflection. For example, in the I-I group, but drops from 145.2 to 106.8 and wait from 131.4 to 110.8, indicating that even when the model fails in both rounds, it still shifts toward more direct expression. Interestingly, in the I-C group where the model corrects its earlier error, we observe an increase in the use of wait and therefore. This suggests more thoughtful and deliberate step-by-step reanalysis process. The 5 Figure 3: Overall change in word frequency across all AIME 2024 examples. Figure 4: Changes in average word frequency across different reasoning trajectories. Each subplot shows the average frequency of four indicative words but, wait, maybe, and therefore in Round 1 vs. Round 2, grouped by response type: I-C (Incorrect Correct), I-I (Incorrect Incorrect), C-C (Correct Correct), and C-I (Correct Incorrect). 6 rise in therefore (from 63.7 to 66.0) also reflects the models increased confidence in arriving at the correct conclusion. Together, these patterns suggest that multi-round thinking helps the model become more confident, fluent, and decisive in its responsesreducing hedging and strengthening clarity."
        },
        {
            "title": "3.2.3 Analysis of Response Length",
            "content": "Table 2: Analysis of response length across multiple reasoning rounds for QwQ-32B, demonstrating decreasing token lengths as the model iteratively refines answers. Model Round QwQ-32B 1 2 3 4 AIME 13566.1 9607.9 8630.0 8654.8 MATH500 GPQADiamond 8489.9 5540.4 5287.7 4948.0 13860.5 11043.9 10368.0 9674.5 LiveCodeBench Average 4473.0 3200.9 3012.7 2920.8 10097.4 7348.3 6824.6 6549.5 We analyzed the generation lengths of the model across different inference rounds, as shown in Table 2. As the number of inference rounds increases, the generation length tends to decrease. Moreover, there is correlation between performance improvement and the reduction in generation lengththe greater the performance gain, the more significant the reduction. For example, from Round 1 to Round 2, the average score improves by 1.4 points while the average generation length decreases by 2749.1 tokens; from Round 2 to Round 3, there is minimal improvement in performance, while the average generation length decreases by only 675.9 tokens. Figure 5: Changes in response length across reasoning rounds on the AIME 2024 dataset (QwQ-32B model). Labels represent the correctness trajectory from Round 1 to Round 2: = Correct, = Incorrect. For example, CI indicates responses that were correct initially but became incorrect in the next round. As shown in Figure 5, we analyzed the changes in response lengths for the same questions across different inference rounds. We found that when the model answered correctly in the previous round but incorrectly in the current round, the inference length increased significantly. This trend is consistent with the word frequency analysis in Figure 3, where the frequency of the word wait rises notably, suggesting increased 7 uncertainty in the models reasoning. In contrast, when the model answers correctly in both the previous and current rounds, it becomes more confident, leading to substantial reduction in inference length."
        },
        {
            "title": "3.3 A Preliminary Experiment with Supervised Fine-tuning (SFT)",
            "content": "To further enhance the robustness of Multi-round Thinking, we explored combining it with supervised finetuning (SFT). The key idea was to reduce error propagation from earlier reasoning rounds by explicitly training the model to rectify previously incorrect answers. Specifically, our supervised fine-tuning process involved the following steps: Data selection: We selected challenging mathematical and programming tasks from open datasets, ensuring each could be independently verified. Data generation: We employed the DeepSeek-R1 model iteratively on these tasks using Multi-round Thinking until correct answer was verified, forming dataset of approximately 100,000 examples. Model training: This dataset was used to fine-tune our AM-32B model in an initial round of supervised training. Table 3: Model Performance with Average(pass@1) Model Round AM-32B 1 2 2(SFT) AIME 2024 72.8 76.7 75.9 MATH500 GPQADiamond 96.2 97.2 97.0 62.3 62.8 63. LiveCodeBench Average 58.3 60.2 57.9 72.4 74.2 73.5 After SFT, we conducted further experiments based on the AM-32B Round2 reasoning outputs. While this preliminary fine-tuning did not lead to performance improvements in our current evaluation (see Table 3), it opens up promising directions for future research in leveraging high-quality reasoning data to enhance Multi-round Thinking."
        },
        {
            "title": "4 Discussion and Conclusion",
            "content": "In this study, we proposed Multi-round Thinking, straightforward yet effective test-time scaling strategy designed to enhance the reasoning capabilities of large language models (LLMs). Inspired by human cognitive processes, this iterative approach allows models to refine their reasoning by independently reconsidering their previous answers, significantly mitigating cognitive inertia and correcting initial reasoning errors. Our extensive experiments demonstrated consistent and substantial improvements across challenging benchmarks, including AIME 2024, GPQA-Diamond, MATH-500, and LiveCodeBench. For instance, accuracy improved by more than 2 percentage points on complex mathematical competition tasks, underscoring the broad applicability and practical value of this approach. Further analysis revealed that multi-round reasoning not only improved accuracy but also made the models reasoning more concise and confident. Specifically, we observed reduction in uncertainty markers (such as but, wait, and maybe) and shorter responses, reflecting increased model clarity and decisiveness in reasoning. These linguistic insights indicate that iterative thinking aligns closely with human cognitive patterns, enhancing the transparency and interpretability of LLM behaviors. While preliminary experiments integrating supervised fine-tuning (SFT) did not immediately yield additional improvements, they highlighted crucial considerations for future researchparticularly regarding the quality of training data and fine-tuning strategies tailored explicitly for iterative reasoning. Exploring these 8 directions further promises significant theoretical and practical benefits, potentially unlocking even greater reasoning capabilities in LLMs. In practical product applications, adopting think twice approach can conveniently incorporate the first-round response as part of the thinking process itself, effectively realizing performance gains. However, this inevitably introduces additional waiting time during the thinking phase. In summary, Multi-round Thinking represents practical, efficient, and universally applicable method for improving LLM reasoning without additional training overhead. This research opens valuable pathways for future exploration and offers immediate utility for both academia and industry in the ongoing quest for more robust, reliable, and explainable AI reasoning."
        },
        {
            "title": "References",
            "content": "Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mane. Concrete problems in ai safety, 2016. URL https://arxiv.org/abs/1606.06565. Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wanxiang Che. Towards reasoning era: survey of long chain-of-thought for reasoning large language models, 2025. URL https://arxiv.org/abs/2503.09567. Sehyun Choi, Tianqing Fang, Zhaowei Wang, and Yangqiu Song. Kcts: Knowledge-constrained tree search decoding with token-level hallucination detection, 2023. URL https://arxiv.org/abs/2310.09044. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Shizhe Diao, Pengcheng Wang, Yong Lin, Rui Pan, Xiang Liu, and Tong Zhang. Active prompting with chain-of-thought for large language models, 2024. URL https://arxiv.org/abs/2302.12246. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models, 2022. URL https://arxiv.org/abs/2203.15556. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020. URL https://arxiv.org/abs/2001.08361. Lauro Langosco, Jack Koch, Lee Sharkey, Jacob Pfau, Laurent Orseau, and David Krueger. Goal misgeneralization in deep reinforcement learning, 2023. URL https://arxiv.org/abs/2105.14111. Noam Levi. simple model of inference scaling laws, 2024. URL https://arxiv.org/abs/2410.16377. Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. Search-o1: Agentic search-enhanced large reasoning models, 2025. URL https://arxiv.org/abs/2501. 05366. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step, 2023. URL https://arxiv.org/abs/ 2305.20050. MAA. American invitational mathematics examination - aime. https://maa.org/math-competitions/ american-invitational-mathematics-examination-aime, feb 2024. Accessed in February 2024, from American Invitational Mathematics Examination - AIME 2024. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand`es, and Tatsunori Hashimoto. s1: Simple test-time scaling, 2025. URL https://arxiv.org/abs/2501.19393. OpenAI. Learning to reason with llms, 2024a. URL https://openai.com/index/learning-to-reason-with-llms/. OpenAI. Openai o1 system card, 2024b. URL https://arxiv.org/abs/2412.16720. Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector Liu, Yuanzhi Li, and Pengfei Liu. O1 replication journey: strategic progress report part 1, 2024. URL https://arxiv.org/abs/2410.18982. Qwen. Team qwen2.5: party of foundation models, September 2024. URL https://qwenlm.github.io/blog/ qwen2.5/. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: graduate-level google-proof q&a benchmark, 2023. URL https://arxiv.org/abs/2311.12022. Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. URL https://qwenlm. github.io/blog/qwq-32b/. Peiyi Wang, Lei Li, Zhihong Shao, R. X. Xu, Damai Dai, Yifei Li, Deli Chen, Y. Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations, 2024. URL https: //arxiv.org/abs/2312.08935. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023. URL https://arxiv.org/abs/2201.11903. Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models, 2025. URL https://arxiv. org/abs/2408.00724. Wenkai Yang, Shuming Ma, Yankai Lin, and Furu Wei. Towards thinking-optimal scaling of test-time compute for llm reasoning, 2025. URL https://arxiv.org/abs/2502.18080. Han Zhao, Haotian Wang, Yiping Peng, Sitong Zhao, Xiaoyu Tian, Shuaiting Chen, Yunjie Ji, and Xiangang Li. 1.4 million open-source distilled reasoning dataset to empower large language model traning, 2025. URL https://github.com/a-m-team/a-m-models/blob/main/docs/AM-DeepSeek-R1-Distilled-Dataset.pdf. Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. Language agent tree search unifies reasoning acting and planning in language models, 2024. URL https://arxiv.org/abs/ 2310.04406."
        },
        {
            "title": "A Example",
            "content": "Figure 6: Illustration of the Think Twice Strategy in Multi-round Reasoning. The model first provides an incorrect answer by following its initial reasoning chain.Upon invoking the Think Twice mechanism, it is explicitly prompted to reassess its prior response. The model then revisits its reasoning, identifies the over-simplified solution space, and produces corrected answer with significantly expanded and accurate enumeration. This process highlights the effectiveness of forcing self-reflection to catch subtle counting mistakes."
        }
    ],
    "affiliations": []
}