{
    "paper_title": "SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language Model",
    "authors": [
        "Loubna Ben Allal",
        "Anton Lozhkov",
        "Elie Bakouch",
        "Gabriel Martín Blázquez",
        "Guilherme Penedo",
        "Lewis Tunstall",
        "Andrés Marafioti",
        "Hynek Kydlíček",
        "Agustín Piqueres Lajarín",
        "Vaibhav Srivastav",
        "Joshua Lochner",
        "Caleb Fahlgren",
        "Xuan-Son Nguyen",
        "Clémentine Fourrier",
        "Ben Burtenshaw",
        "Hugo Larcher",
        "Haojun Zhao",
        "Cyril Zakka",
        "Mathieu Morlon",
        "Colin Raffel",
        "Leandro von Werra",
        "Thomas Wolf"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While large language models have facilitated breakthroughs in many applications of artificial intelligence, their inherent largeness makes them computationally expensive and challenging to deploy in resource-constrained settings. In this paper, we document the development of SmolLM2, a state-of-the-art \"small\" (1.7 billion parameter) language model (LM). To attain strong performance, we overtrain SmolLM2 on ~11 trillion tokens of data using a multi-stage training process that mixes web text with specialized math, code, and instruction-following data. We additionally introduce new specialized datasets (FineMath, Stack-Edu, and SmolTalk) at stages where we found existing datasets to be problematically small or low-quality. To inform our design decisions, we perform both small-scale ablations as well as a manual refinement process that updates the dataset mixing rates at each stage based on the performance at the previous stage. Ultimately, we demonstrate that SmolLM2 outperforms other recent small LMs including Qwen2.5-1.5B and Llama3.2-1B. To facilitate future research on LM development as well as applications of small LMs, we release both SmolLM2 as well as all of the datasets we prepared in the course of this project."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 7 3 7 2 0 . 2 0 5 2 : r SmolLM2: When Smol Goes Big Data-Centric Training of Small Language Model Loubna Ben Allal * Anton Lozhkov * Elie Bakouch * Gabriel Martín Blázquez * Guilherme Penedo Lewis Tunstall Andrés Marafioti Hynek Kydlíˇcek Agustín Piqueres Lajarín Vaibhav Srivastav Joshua Lochner Caleb Fahlgren Xuan-Son Nguyen Clémentine Fourrier Ben Burtenshaw Hugo Larcher Haojun Zhao Cyril Zakka Mathieu Morlon Colin Raffel Leandro von Werra Thomas Wolf"
        },
        {
            "title": "Hugging Face",
            "content": "https://hf.co/collections/HuggingFaceTB/smollm2-6723884218bcda64b34d7db9 Abstract While large language models have facilitated breakthroughs in many applications of artificial intelligence, their inherent largeness makes them computationally expensive and challenging to deploy in resource-constrained settings. In this paper, we document the development of SmolLM2, state-of-the-art small (1.7 billion parameter) language model (LM). To attain strong performance, we overtrain SmolLM2 on 11 trillion tokens of data using multi-stage training process that mixes web text with specialized math, code, and instruction-following data. We additionally introduce new specialized datasets (FineMath, Stack-Edu, and SmolTalk) at stages where we found existing datasets to be problematically small or low-quality. To inform our design decisions, we perform both small-scale ablations as well as manual refinement process that updates the dataset mixing rates at each stage based on the performance at the previous stage. Ultimately, we demonstrate that SmolLM2 outperforms other recent small LMs including Qwen2.5-1.5B and Llama3.2-1B. To facilitate future research on LM development as well as applications of small LMs, we release both SmolLM2 as well as all of the datasets we prepared in the course of this project. 1. Introduction Large language models (LMs) have become cornerstone of modern AI systems due to their ability to follow natural language instructions and flexibly perform huge range of tasks *Equal contribution . Correspondence to: Loubna Ben Allal <loubna@hf.co>, Leandro von Werra <leandro@hf.co>, Thomas Wolf <thomas@hf.co>. Preprint. Under review. 1 (Touvron et al., 2023; Bai et al., 2023; Brown et al., 2020; Dubey et al., 2024; Groeneveld et al., 2024; Chowdhery et al., 2023; Young et al., 2024; Taylor et al., 2022). LLMs are, by their nature, large, in the sense that they are models with many parameters (more than 10 billion, by current conventions). This enormity results in enormous computational costs, both during training and for inference, which can prevent LLMs from being used in resource-constrained settings. To address this issue, flurry of recent work has aimed to produce performant small (3 billion parameters or less) LMs (Gunter et al., 2024; Yang et al., 2024b; AI@Meta, 2024b; Team et al., 2024; Li et al., 2023b). These small LMs are computationally inexpensive and can be run on wider range of devices (e.g. mobile phones) while providing satisfactory performance on many important tasks. key factor in the performance and behavior of LMs is the data used to train them. While important for an LM of any size, data curation has an especially outsized influence for smaller models, as their limited capacity must be carefully optimized for learning core knowledge and fundamental capabilities rather than memorizing incidental facts (Abdin et al., 2024a; Rolnick et al., 2017). Most LMs are primarily trained on text crawled from the web (Radford et al., 2019; Raffel et al., 2020) and state-of-the-art pipelines include sophisticated filtering and processing stages that aim to improve data quality (Li et al., 2024c; Penedo et al., 2024b;a; Soldaini et al., 2024). Recently, it has become common to include specialized data from certain domains such as software code (Kocetkov et al., 2022; Lozhkov et al., 2024) and mathematics (Paster et al., 2023; Han et al., 2024), which can improve performance not only on those domains but also more generally on challenging tasks that require reasoning (Muennighoff et al., 2023; Aryabumi et al., 2024). Motivated by the above considerations, our contributions in this paper are as follows: First, we perform careful evaluation of existing web, code, math, and instructionfollowing datasets (Section 3) to help guide training data SmolLM2 design choices, ultimately training SmolLM2 via multistage manual rebalancing of different sources to maximize performance (Section 4). Such on-the-fly rebalancing is promising approach for large-scale training runs which can be sufficiently costly (around 1e23 FLOPs, or $250,000 USD worth of GPU compute for SmolLM2) to preclude running multiple full-scale training runs. Following standard practice, we also develop an instruction-tuned variant of SmolLM2 (Section 5). Additionally, after finding that existing datasets were too small and/or low-quality, we created the new datasets FineMath, Stack-Edu, and SmolTalk (for mathematics, code, and instruction-following respectively). Ultimately, we show that both the base and instruction-tuned variants of SmolLM2 are state-of-the-art among similarly sized models (Section 4.7 and Section 5.4). 2. Background Training modern LM typically begins with pretraining on large amount (e.g. trillions of tokens) of unstructured text. Pretraining helps the model fit the structure of language (Clark, 2019) and store factual knowledge (Petroni et al., 2019; Roberts et al., 2020) and therefore has proven to be vital part of LM training, which made the composition of the pretraining dataset key consideration. The datahungry nature of pretraining has led to the use of largescale web scrapes (com; ope; ant) which in their raw form can lead to poorly performing LMs (Penedo et al., 2024b). Consequently, the primary means of curation for modern LM pretraining datasets involves designing sophisticated pipelines for automatically filtering and reformatting web texts (Penedo et al., 2024a;b; Soldaini et al., 2024; Soboleva et al., 2023; Li et al., 2024c) that aim to keep enough data to avoid detrimental repetition (Muennighoff et al., 2023) while discarding any data that is not high-quality. Apart from web text, including specialized data from certain domains code (Kocetkov et al., 2022; Li et al., 2023a) and math (Paster et al., 2023; Han et al., 2024; Wang et al.; Azerbayev et al., 2023) in particular can improve model performance on tasks that involve reasoning and world knowledge (Muennighoff et al., 2023; Aryabumi et al., 2024; Lewkowycz et al., 2022; Shao et al., 2024). The contribution of small specialized datasets can be dwarfed by much larger web-based pretraining data sources, which has led to the design of multi-stage pretraining where specialized or high-quality datasets are incorporated later in training (Abdin et al., 2024b; Ai2, 2024; Blakeney et al., 2024; Singer et al., 2024). After pretraining, language models typically undergo two additional training stages before deployment: instruction tuning and preference learning. In instruction tuning, the model undergoes supervised training on instruction/response pairs that reflect the way that the language model should answer query (Wei et al., 2021; Mishra et al., 2021; Sanh et al., 2021; Wang et al., 2022). This process provides valuable way of tailoring LMs to provide helpful responses rather than simply attempting to continue the input (as taught during pretraining). During preference learning, language models are further aligned towards their intended use by being trained to distinguish between helpful and unhelpful responses (Ouyang et al., 2022; Bai et al., 2022). This final stage typically involves form of reinforcement learning (Bai et al., 2022; Lee et al.; Rafailov et al., 2024) on data labeled with human or synthetically generated preferences. 3. Pretraining datasets Pretraining data curation is especially important for small LMs due to their tendency to be more sensitive to noise in the training data (Rolnick et al., 2017; Abdin et al., 2024a). In addition, designing pretraining strategy involves not only selecting and curating data, but also determining how much to mix (i.e. sample) from different sources, which can be particularly important when including e.g. specialized math and code datasets. We therefore undertook careful evaluation of existing datasets and, wherever we deemed necessary, created new, improved, and larger datasets. 3.1. Ablation setup To compare English web datasets and find the best mixture for training our models, we followed an empirical approach similar to Penedo et al. (2024a). Specifically, we trained models on each dataset under identical conditions: model configuration, training hyperparameters, and token count. We trained 1.7B parameter Transformers (Vaswani et al., 2017) based on the Llama architecture (Touvron et al., 2023), with sequence length of 2048, global batch size of approximately 2 million tokens, the GPT-2 tokenizer (Radford et al., 2019), and cosine learning rate schedule (Loshchilov & Hutter, 2016) with learning rate of 3.0 104. Each dataset ablation model is trained on 350B tokens randomly sampled from the full dataset. For evaluation, we also followed Penedo et al. (2024a), and used lighteval to evaluate on variety of knowledge, reasoning, and text understanding benchmarks: MMLU (Hendrycks et al., 2021), HellaSwag (Zellers et al., 2019), OpenBook QA (Mihaylov et al., 2018), PIQA (Bisk et al., 2019), WinoGrande (Sakaguchi et al., 2019), ARC (Clark et al., 2018), and CommonSenseQA (Talmor et al., 2019). Math and code capabilities typically emerge only after extensive training, so similarly to Blakeney et al. (2024); Dubey et al. (2024); Ai2 (2024), when evaluating math and code datasets we started from mid-training checkpoint of SmolLM2 at 3T tokens (detailed in Section 4), which was trained primarily on web data. We then used an annealing approach: the learning rate linearly decays to 0 while training on mixture that includes the dataset 2 SmolLM Table 1. Evaluation of models trained on FineWeb-Edu and DCLM for 350B tokens. 40/60 and 60/40 denote the FW-Edu/DCLM ratio. Task FW-Edu DCLM 40/60 60/40 MMLU ARC OpenBookQA HellaSwag CommonsenseQA PIQA 37.5 57.5 41.9 60.1 36.2 76. 35.5 53.5 40.8 62.3 40.1 76.9 36.5 53.2 39.0 61.4 39.9 75.7 37.0 56.0 41.9 62.2 38.5 76.4 under evaluation. For math, we annealed on mixture of 60B tokens of the dataset under evaluation and 40B from the pre-checkpoint mixture. For code ablations, we performed annealing on 200B tokens, uniformly distributed across 15 of the most commonly used programming languages (14B tokens each). We evaluated the math ablation models on GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021) and MMLU-STEM to assess their math capabilities using lighteval, and we used HumanEval (Chen et al., 2021) and MultiPL-E (Cassano et al., 2022) to evaluate the code ablation models using the BigCode-Evaluation-Harness. 3.2. English web data Web text from Common Crawl has remained popular source of pretraining data, and recent classifier-based filtering techniques have significantly advanced pretraining data quality (Dubey et al., 2024; Abdin et al., 2024b;a; Kong et al., 2024). Two prominent examples of open datasets that use classifer-based filtering are FineWeb-Edu (Penedo et al., 2024a) and DCLM (Li et al., 2024c). FineWebEdu consists of 1.3T tokens that were deemed educational by classifier trained on annotations generated by Llama3-70B-Instruct (Dubey et al., 2024). DCLM comprises 3.8T tokens filtered using fastText classifier (Joulin et al., 2016a;b) trained on instruction-following data from OpenHermes 2.5 (Teknium, 2023a) and high-scoring posts from the r/ExplainLikeImFive (ELI5) subreddit. Training ablation models on 350B tokens each from FineWeb-Edu and DCLM attained performance shown in Table 1. We find that FineWeb-Edu achieves higher scores on the educational benchmarks MMLU, ARC, and OpenBookQA, while DCLM performs better on HellaSwag and CommonsenseQA. These results align with the datasets content: FineWeb-Edu prioritizes educational material, while DCLM captures more diverse, conversational styles. Given the complementary strengths of FineWeb-Edu and DCLM, we explored whether mixing them could further improve performance. After testing different ratios, we found that 60% FineWeb-Edu and 40% DCLM mix works well, as shown in Table 1: It nearly matches FineWeb-Edus performance on MMLU, ARC, and OpenBookQA while also 3 aligning with DCLMs results on HellaSwag and approaching its performance on CommonSenseQA. Combining these datasets yields 5.1T tokens of (English) text. 3.3. Math data Specialized math pretraining data is crucial for developing robust mathematical understanding. Recent research has shown that carefully curated mathematical content from Common Crawl, combined with targeted filtering techniques, can significantly enhance language models mathematical reasoning capabilities (Dubey et al., 2024; Yang et al., 2024c; Shao et al., 2024; Han et al., 2024). 3.3.1. COMPARISON OF EXISTING DATASETS We compare two leading publicly available math datasets: OpenWebMath (OWM) (Paster et al., 2023) and InfiMMWebMath (Han et al., 2024). OWM consists of 12B tokens, built by filtering math-specific content from Common Crawl and using specialized text extraction pipeline to preserve mathematical formatting and equations. InfiMMWebMath contains 40B text tokens, and its authors show that it matches the performance of the private dataset of DeepSeekMath (Shao et al., 2024). We performed annealing ablations (following the setup described in Section 3.1) on OWM and InfiMM-WebMath, finding that InfiMM-WebMath achieves peak accuracy of 14% on GSM8K compared to OWMs 10%, while OWM slightly outperforms InfiMM-WebMath on MATH. The full evaluation curves are available in Appendix C.1. Despite training on 60B math tokens (i.e., 5 epochs for OWM and 1.5 epochs for InfiMM-WebMath), performance still lagged behind proprietary state-of-the-art small models (Yang et al., 2024b). Further analysis highlighted two key limitations: insufficient dataset sizes, and insufficient focus on step-bystep mathematical reasoning, along with an overrepresentation of academic papers that focus on advanced concepts. 3.3.2. NEW DATASET: FINEMATH The aforementioned issues with OWM and InfiMMWebMath motivated us to develop FineMath1, collection of up to 54B tokens of math data focusing on mathematical deduction and reasoning through classifier-based filtering. We began by extracting text from Common Crawl WARC files using Resiliparse, focusing on all 5.8B unique URLs from the FineWeb dataset (a subset of Common Crawls 75B unique URLs). We then employed the FineWeb-Edu filtering approach, using Llama-3.1-70B-Instruct (Dubey et al., 2024) with prompt (Appendix C.2) that scores content on 3-point scale, where 1 indicates some mathematical 1https://huggingface.co/datasets/HuggingFaceTB/finemath SmolLM2 content and 3 indicates step-by-step problem solutions at an appropriate level. After training classifier on these silver labels, we identified domains containing at least 10 pages with quality score of 2 or higher. We expanded our domain coverage by including domains with at least 10 URLs from either OWM or InfiMM-WebMath. From the Common Crawl index, we retrieved total of 7.7B URLs belonging to this list of domains: 5.7B identified by our classifier, 0.6B from OWM, and 1.3B from InfiWebMath. We then re-extracted all identified pages using the OWM pipeline, preserving LaTeX formatting and removing all-boilerplate pages, yielding 7.1B pages containing 6.5T tokens. To retain only high-quality math content, we reapplied classifier trained on Llama-3.1-70B-Instruct annotations using 5-point scale prompt (Appendix C.3) specifically targeting pages with reasoning and middleto high-schoollevel content. We note that InfiMM-WebMath used similar classifier filtering pipeline, but their prompt did not target the same type of content. After classification, we performed deduplication using single-band MinHash LSH (Broder, 1997) with 10 hashes and applied fastText language classification (Joulin et al., 2016a;b) to retain only English content. Ultimately, we developed multiple variants of FineMath, including FineMath4+ (10B tokens, 6.7M documents) which retains only samples with scores of 4-5 and FineMath3+ (34B tokens, 21.4M documents) which includes scores 35. We additionally applied the same classifier to InfiMMWebMath, creating Infi-WebMath4+ (8.5B tokens, 6.3M documents) and Infi-WebMath3+ (20.5B tokens, 13.9M documents). Similarly to Yang et al. (2024c), we decontaminate each dataset against GSM8K, MATH and MMLU using 13-gram matching and minimum overlap ratio with the longest common subsequence of 0.6. Results Figure 1 presents our FineMath annealing ablations. All FineMath subsets consistently outperform OWM and InfiMM-WebMath on GSM8K, MATH, and MMLUSTEM. FineMath4+ achieves 2x improvement on GSM8K and 6x improvement on MATH compared to InfiMMWebMath, demonstrating the importance of retaining highquality mathematical content with reasoning. Additionally, Infi-WebMath4+ outperforms InfiMM-WebMath, but plateaus after 80B tokens (roughly 10 epochs), likely due to data repetition, trend not seen in FineMath4+. 3.4. Code data Code generation and understanding are becoming essential capabilities for modern LLMs, enabling diverse use cases such as code completion, debugging, and software design. While specialized code models (Lozhkov et al., 2024; Bai et al., 2023; Roziere et al., 2023) are optimized specifically for these tasks, general-purpose LLMs are increasingly deployed as coding assistants. Moreover, recent research has Figure 1. Performance of models trained on different subsets of FineMath and other math datasets. shown that including code data in pretraining enhances not only code-related capabilities but also improves natural language reasoning and world knowledge (Aryabumi et al., 2024). The Stack datasets are state-of-the-art open code datasets (Li et al., 2023a; Kocetkov et al., 2022), including Stack v1, 3TB of source code from public GitHub repositories; StarCoderData (Li et al., 2023a; Kocetkov et al., 2022; Lozhkov et al., 2024), filtered subset of 250 billion tokens across 80 programming languages; Stack v2, with 32TB of data sourced from the Software Heritage code archive; and StarCoder2Data, the training corpus for StarCoder2 models (Lozhkov et al., 2024) with 900 billion tokens spanning more than 600 programming languages. Stack-Edu Recent work has shown that the FineWeb-Edu classifier-based filtering strategy can be effective for code data (Wei et al., 2024b; Allal et al., 2024). We therefore constructed Stack-Edu, filtered variant of StarCoder2Data focusing on educational and well-documented code. Specifically, we selected the 15 largest programming languages from StarCoder2Data to match the capacity constraints of smaller models (Lozhkov et al., 2024) and ensure benchmark coverage for the ablations. This subset had 450 billion tokens. We then trained 15 language-specific classifiers using the StarEncoder model (Li et al., 2023a) on synthetic annotations generated by Llama3-70B-Instruct (Dubey et al., 2024) (prompt in Appendix D.1), which rated the educational quality on scale from 0 to 5. Each classifier was trained on 500,000 samples and achieved an F1 score above 0.7 for most languages when applying threshold of 3 for binary classification. To evaluate Stack-Edu, we performed annealing ablations as described in Section 3.1. Filtering with threshold of 3 improved performance across most languages while maintaining sufficient data, although Java performed better with threshold 2. Since Markdown is not included in the MultiPLE benchmark, we could not determine threshold for the dataset quantitatively; instead, we used threshold 3 based on qualitative analysis. The resulting Stack-Edu dataset contains 125B tokens across its 15 languages (see Appendix D.2). Table 2 shows the statistics of the top 4 programming languages in terms of size, and the impact of our 4 SmolLM Table 2. Stack-Edu dataset statistics and MultiPL-E scores for the top 4 (in terms of size) programming languages. We use HumanEval for Python evaluation. Language StarCoder2Data (B tokens) Stack-Edu (B tokens) MultiPL-E (Original Filtered) 20.7 25.6 16.7 24.8 18.2 22.4 17.6 22.7 Python C++ JavaScript Java 50.6 69.7 45.3 45. 21.8 16.0 11.1 42.1 educational filtering on MultiPL-E. 4. Pretraining Recent trends in language models pretraining show clear shift towards significantly longer training durations, especially for smaller models (Yang et al., 2024a;b; AI@Meta, 2024b). While this strategy deviates from the Chinchillaoptimal guidelines (Hoffmann et al., 2022), the resulting performance gains and reduced inference costs make extended training worthwhile trade-off (de Vries, 2023). For example, Qwen2-1.5B was trained on 7 trillion tokens, Qwen2.5-1.5B on 18 trillion tokens, and Llama3.2-1B, derived from pruned 8B model, was trained using distillation on 9 trillion tokens (Yang et al., 2024a;b; AI@Meta, 2024b). When building SmolLM2, we trained on 11 trillion tokens (approximately two epochs on our collected datasets), employing multi-stage training approach instead of fixed dataset mixture throughout pretraining. This design was guided by four key principles: (1) Performance-driven interventions, where we monitor evaluation metrics on key benchmarks and adapt dataset mixtures to address specific capability bottlenecks; (2) Upsampling high-quality math and code during the annealing phase, reserving datasets like FineMath and parts of Stack-Edu for the final stages to maximize their impact (Blakeney et al., 2024; Ai2, 2024); (3) Strategic introduction of medium-sized datasets, such as OWM, InfiMM-WebMath, and Stack-Edu, mid-training to avoid dilution by larger datasets early on; and (4) Avoiding excessive data repetition, in line with Muennighoff et al. (2023) we aimed to stay close to the recommended 45 epoch threshold for most datasets. While it might be fruitful to perform multiple from-scratch training runs to explore different data mixing schedules, the high cost of pretraining SmolLM2 (around $250,000 USD of GPU compute) motivated our online approach. In the following sections, we describe each stage of the training process, detailing the dataset mixtures, the rationale behind our choices, and the observations that guided our interventions. While some decisions were informed by established findings in the literature, others were driven by empirical insights gathered during training. The data mixtures of the four pretraining phases are available in Figure 2. 5 Figure 2. Dataset mixtures across training stages. Detailed descriptions are provided in Section 4. The x-axis represents the number of training tokens. 4.1. Training setup Our base model contains 1.7B parameters and follows the LLama2 (Touvron et al., 2023) architecture, outlined in Appendix A. We trained the model on 256 H100s using the nanotron framework and use AdamW optimizer with (β, β2) = (0.9, 0.95) with Warmup Stable Decay (WSD) (Hu et al., 2024; Zhai et al., 2022) learning rate schedule to avoid setting fixed training duration (see Figure 3, Appendix A). The schedule started with 2,000-step warmup phase, maintained peak learning rate of 5.0 104 (stable phase), and could transition to decay phase when needed, reducing the learning rate to zero over 10% of the total training steps (Hägele et al., 2024). We used the tokenizer from Allal et al. (2024), which has vocabulary size of 49,152 tokens and was trained on mixture of 70% of FineWeb-edu, 15% Cosmopedia-v2, 8% OpenWebMath, 5% StarCoderData and 2% StackOverflow. 4.2. Stable phase: stage 1 Data mixture In the first phase of SmolLM2s pretraining (0 to 6T tokens), we designed our dataset mixture based on insights from our English web ablations and existing literature. We adopted 60% FineWeb-Edu and 40% DCLM ratio (discussed in Section 2.2) for web data, which provided an optimal balance between educational content and diverse, real-world Q&A-style data. For code data, following Aryabumi et al. (2024), we incorporated StarCoderData, consisting of 250B tokens across 80 programming languages, and limited it to 10% of the total mixture to ensure approximately 4 epochs over 11T tokens with room for upsampling in later stages. We did not include math data in stage 1 due to our math datasets relatively small size. Findings After 6T tokens of training, we evaluated SmolLM2 on key benchmarks, as shown in Table 3. Knowledge and reasoning performance aligned with expectations based on our English web ablation results. However, we observed generally poor coding and mathematics performance. SmolLM2 Table 3. Average model performance on different benchmark categories after each training stage. Stages 1-3 are during stable phase (no decay). Full per-benchmark results in Appendix E.1. Tokens Stage 1 0-6T Stage 2 6-8T Stage 3 8-10T Stage 4 10-11T Knowledge/Reasoning Math Code Generative Tasks 55.50 3.21 8.87 31.54 56.76 3.7 10.56 31.30 57.47 7.27 16.75 34. 60.24 22.07 23.21 36.12 4.3. Stable phase: stage 2 Data mixture For stage 2 (6T to 8T tokens), we added OWM to the mixture at 5% ratio and increased the proportion of code data in hopes of maintaining strong knowledge retention while addressing observed gaps in coding and mathematical reasoning. Including OWM at low percentage reflects the datasets small size (12B tokens) and our gradual approach to incorporating math content. The final mixture for stage 2 consisted of 75% English web data (keeping the 60/40 FineWeb-Edu to DCLM ratio from stage 1), 20% code data, and 5% math data, as shown in Figure 2. Findings After stage 2, code performance improved across most languages, validating the decision to upsample StarCoderData. OWM integration had no significant impact on math performance, underscoring the need for larger, higher-quality math datasets in later stages. Beyond code and math performance, as shown in Figure 6 (Appendix E.2), we observed above-random (>25%) MMLU accuracy with multiple-choice formulation (MCF, i.e. explicitly outputting an option from A, B, C, or instead of computing the likelihood of different answers as in the cloze formulation). This contrasts prior work showing that small models struggle with the MCF (Gu et al., 2024; Du et al., 2024) and suggests that long trainings of small models can make them acquire abilities typically associated with larger models (Blakeney et al., 2024; Gu et al., 2024; Du et al., 2024). To further optimize MMLU performance, we revisited our English dataset mixture with additional annealing ablations and found that increasing DCLM relative to FineWeb-Edu slightly improves MMLU MCF at this stage. also added Jupyter Notebooks from StarCoder2 (Lozhkov et al., 2024), which provides rich, contextual examples of code interleaved with explanations, enhancing the models reasoning around programming tasks. Findings While the integration of these new datasets brought improvements across multiple benchmarks, we observed noticeable loss spike during this phase which remained even after rewinding training and skipping data associated with the spike (Chowdhery et al., 2023; Almazrouei et al., 2023). The exact cause remains undetermined but most evaluation metrics recovered by the end of the stage. 4.5. Decay phase: stage 4 Data mixture The final stage consisted of decaying the learning rate linearly to 0 for 10% of the total training duration (from 10T to 11T tokens) (Hägele et al., 2024). Following Blakeney et al. (2024), we introduced our highest quality mathematical datasets, InfiWebMath-3+, and FineMath 4+. We additionally allocated 0.08% of the mixture to OWM and 0.02% to AugGSM8K (Li et al., 2024a), an augmented version of the GSM8K benchmarks training set, which has become common component of recent pretraining datasets (Achiam et al., 2023; Dubey et al., 2024; Ai2, 2024). Overall, mathematical content totaled 14% of the mixture. We expanded Stack-Edu to include additional programming languages not covered in stage 3, and set the datasets contribution to 24% of the mixture. We maintained the natural distribution across programming languages, with higher allocation for Python. The remaining mixture consisted of English web data at 58% (maintaining the higher DCLM to FineWeb-Edu ratio) and Cosmopedia v2 (Allal et al., 2024) at 4%, which provides 30B tokens of highquality synthetic textbooks, blog posts, and stories. Findings While all benchmark tasks show improvements after stage 4, we observe substantial gains in coding performance and, most notably, in math performance, validating our data mixture specifically targeting these domains. 4.4. Stable phase: stage 3 4.6. Context Length extension Data mixture In the third and last stage of the stable phase (8T to 10T tokens, before annealing starts), we added the text-only English portion of InfiMM-WebMath with OWM, bringing the total proportion of math data to approximately 10%, as shown in Figure 2. For English web data, we revisited our ablation findings and adjusted the FineWeb-Edu to DCLM ratio to 40/60. For code, we replaced StarCoderData with Stack-Edu (Section 3.4). For languages with fewer than 4B tokens in Stack-Edu (TypeScript, Shell, Swift, Go, Rust and Ruby), we used their StarCoder2Data subsets. We To support long-context applications, we followed standard practice (Gao et al., 2024) and extended the context length from 2k to 8k tokens, by taking an intermediate checkpoint from stage 4 (before the final 75 billion tokens of training) and continuing training with different data mixture and RoPE value of 130k. The mixture was adjusted to include 40% long-context documents (8k tokens or more) sourced from DCLM (10%), FineWeb-Edu (10%), and the books subset of Dolma (20%) (Soldaini et al., 2024), while the remaining 60% followed the stage 4 mixture. After this step, we obtain the final SmolLM2 base model. 6 SmolLM2 Table 4. Performance comparison of SmolLM2 and other 1-2B base models across benchmarks. SmolLM2 demonstrates competitive results highlighting its generalization capabilities. Model family Parameters HellaSwag ARC PIQA CommonsenseQA Winogrande OpenBookQA MMLU-Pro (held-out) Natural Questions (held-out) TriviaQA (held-out) GSM8K (5-shot) MATH (4-shot) HumanEval SmolLM2 Llama3.2 Qwen2.5 1B 1.5B 1.7B 68.7 60.5 77.6 43.6 59.4 42. 19.4 8.7 36.7 31.1 11.6 22.6 61.2 49.2 74.8 41.2 57.8 38.4 11.7 6.2 28.1 7.6 3.3 18.9 66.4 58.5 76.1 34.1 59.3 40. 13.7 10.5 20.9 61.7 34.3 37.2 4.7. Base model evaluation We evaluate and compare the final base SmolLM2 model with existing state-of-the-art models of similar size, Qwen2.5-1.5B (Yang et al., 2024b) and Llama3.21B (AI@Meta, 2024a), on wide range of benchmarks. Evaluations are conducted using lighteval and in zeroshot setting unless otherwise specified. Evaluation results in Table 4 show the strong performance of base SmolLM2, outperforming the Qwen2.5 base model on HellaSwag, and ARC. SmolLM2 also delivers strong performance on held-out benchmarks not monitored during training, such as MMLU-Pro (Wang et al., 2024c), TriviaQA (Joshi et al., 2017), and Natural Questions (NQ, Kwiatkowski et al., 2019). Notably, the model outperforms Qwen2.5-1.5B by nearly 6 percentage points on MMLUPro, further validating its generalization capabilities. On math and coding benchmarks, SmolLM2 demonstrates competitive performance. While it lags behind Qwen2.5-1.5B, SmolLM2 outperforms Llama3.2-1B on GSM8K, MATH and HumanEval. Importantly, we see next to no degradation in performance after Context Length Extension, while the HELMET (Yen et al., 2024) and Needle in the Haystack (NIAH) (Kamradt, 2024) results show strong performance see Appendix G. These results highlight the effectiveness of our curated datasets, data mixtures, and training stages. 5. Post-training After training the base SmolLM2 model, we followed current standard practice for maximizing performance and utility via post-training through instruction tuning and preference learning. For post-training, we leveraged existing datasets in addition to new instruction tuning dataset called SmolTalk. 7 5.1. SmolTalk Although the SmolLM2 base model outperformed other state-of-the-art base models in the 1-2B parameter range, the base models performance after fine-tuning on public datasets like MagPie-Pro (Xu et al., 2024) or OpenHermes2.5 (Teknium, 2023b) was lower than the post-trained versions of these other models. This observation motivated the development of SmolTalk2, new instructionfollowing dataset that carefully combines selected existing datasets with new synthetic datasets we developed, including the Magpie-Ultra conversational dataset as well as other specialized datasets that address specific capabilities like Smol-Constraint, Smol-Rewrite, and Smol-Summarization. All datasets were generated using Distilabel (Bartolomé Del Canto et al., 2024). 5.1.1. CONVERSATIONAL DATA MagPie-Ultra is multi-turn dataset created using the twostep prompting method from (Xu et al., 2024). Unlike MagPie, which used Llama-3-70B-Instruct without specific system prompts to generate two-turn conversations, MagPieUltra leverages the larger, more powerful model Llama-3.1405B-Instruct-FP8 (Dubey et al., 2024). We also incorporate system prompts to guide generation, producing balanced dataset of 1M samples with three-turn conversations. The resulting dataset was further filtered using smaller Llama models (Llama-3.1-8B-Instruct and Llama-Guard-3-8B) to ensure quality and safety of the generated instructions. We also leveraged ArmoRM (Wang et al., 2024b;a) to score conversations for quality-based filtering, and gte-large-env1.5 (Zhang et al., 2024; Li et al., 2023c) to deduplicate semantically similar conversations. We compare MagPie-Ultra to existing public supervised fine-tuning (SFT) datasets in Table 10 (Appendix F). The evaluation suite included the instruction-following and conversation benchmarks IFEval (Zhou et al., 2023) and MTBench (Zheng et al., 2023); reasoning in ARC Challenge; knowledge in MMLU-Pro as well as GSM8K and MATH for math evaluations. Our dataset outperforms MagPie-Pro on most benchmarks, and largely surpasses OpenHermes2.5 and UltraChat (Ding et al., 2023) on IFEval and MT-Bench. 5.1.2. TASK-SPECIFIC DATA We developed additional task-specific datasets to further enhance model instruction-following with detailed constraints (Smol-Constraint), summarization (Smol-Summarization) and rewriting (Smol-Rewrite) capabilities. Smol-Constraint contains 36k instructions with detailed constraints similar to the ones found in IFEval (Zhou et al., 2023). Using the method from (Xu et al., 2024) with targeted system prompt, 2https://huggingface.co/datasets/HuggingFaceTB/smoltalk SmolLM2 we generated 550k instructions and responses for these instructions using Qwen2.5-72B-Instruct (Yang et al., 2024b). We then filtered out generated instructions that contained conflicting constraints or incorrect responses, resulting in 56.3k instruction-response pairs, which after decontaminating against IFEval (10 n-gram overlap), yielded 36k pairs. For Smol-Summarization and Smol-Rewrite, we first generated high-quality source texts that would serve as the basis for summarization and rewriting tasks. We synthesized diverse collection of emails, tweets, LinkedIn posts, and notes using PersonaHub (Ge et al., 2024) and personas from the FinePersonas dataset (Argilla, 2024; Chan et al., 2024). This allowed us to generate diverse content by prompting Qwen2.5-72B-Instruct with specific system prompts and persona description, obtaining texts with various writing styles, topics and perspectives. We then prompted Qwen2.572B-Instruct to summarize and rewrite the given texts, obtaining around 1M summaries and 600k rewritten texts. Adding the 3 Smoldatasets to MagPie-Ultra, (MagPieUltra+) further improves IFEval performance as shown in Table 10 (Appendix F). 5.1.3. MATH DATA To improve mathematical reasoning, we evaluated public math instruction datasets by fine-tuning on mixtures with 80% general instruction data (MagPie Ultra + SmolConstraint, Smol-Rewrite, Smol-Summarization) and 20% math data from various sources. Results in Table 10 (Appendix F) highlight complementary dataset strengths: NuminaMath-CoT (Li et al., 2024b) demonstrated strong performance on MATH and MT-Bench, while MetaMathQA (Yu et al., 2023), which is also included in OpenHermes2.5, improved results on GSM8K. Based on these findings, we incorporated combination of both datasets into SmolTalk. 5.1.4. OTHER SPECIALIZED DATA For code generation, we used Self-OSS-Starcoder2Instruct (Wei et al., 2024a), containing 50k high-quality Python instruction-response pairs. To support system prompts, we included 30k randomly selected samples from SystemChats2.0 (Computations, 2024), and for function calling, we added 80k samples from APIGen-FunctionCalling (Liu et al., 2024). Additionally, to maintain strong performance on long-context tasks, we incorporated an English subset of LongAlign (Bai et al., 2024) (3.7k samples with 8k16k tokens). We also added 100k randomly selected OpenHermes2.5 samples due to its strong performance in knowledge (MMLU-Pro), Everyday-Conversations (Face, 2024), 2.2k casual multi-turn interactions, and ExploreInstruct (Wan et al., 2023) for rewriting. We found that incorporating these datasets with the specified number of samples effectively enhanced their target capabilities while preserving strong performance across other benchmarks. Table 5. Comparison of 1-2B instruction-tuned models across benchmarks. SmolLM2-1.7B-Instruct exhibits strong performance in instruction-following, reasoning, and math. Model SmolLM2-1.7B Llama3.2-1B Qwen2.5-1.5B IFEval (Average) MT-Bench OpenRewrite-Eval ARC BBH (3-shot) MMLU-Pro HellaSwag PIQA GSM8K (5-shot) MATH (4-shot) HumanEval 56.7 6.13 44.9 51.7 32.2 19.3 66.1 74. 48.8 21.0 28.1 53.5 5.48 39.2 41.6 27.6 12.7 56.1 72.3 37.4 19.5 33.5 47.4 6.52 46.9 46.2 35.3 24.2 60.9 73.2 63.3 19.6 30.5 5.2. Supervised fine-tuning (SFT) Table 9 (Appendix F) shows the final composition of SmolTalk. We performed supervised fine-tuning of our base SmolLM2 on SmolTalk for 2 epochs, using global batch size of 128, sequence length of 8192, and learning rate of 3.0 104. The evaluation results after this SFT phase are available in Table 10 (Appendix F). 5.3. Alignment For preference learning, we used Direct Preference Optimization (DPO) (Rafailov et al., 2024). We experimented with various public synthetic feedback datasets (Ivison et al., 2024) including UltraFeedback (Cui et al., 2024), UltraInteract (Yuan et al., 2024), Capybara (Daniele & Suphavadeeprasit, 2023), and ORCA (Lv et al., 2023). UltraFeedback proved the most consistently effective across benchmarks, improving MT-Bench, MMLU-Pro, and MATH. We trained for 2 epochs with learning rate of 1.0 106, beta of 0.5, global batch size of 128, and sequence length of 1024 tokens. After this final stage of DPO training, we obtain the instruct SmolLM2 model. As noted in Dubey et al. (2024), using short-context data for DPO did not impact the models 8k context ability. 5.4. Instruct model evaluation We evaluate the final instruct version of SmolLM2 and compare it with the instruct variants of Qwen2.5-1.5B and Llama3.2-1B, with results shown in Table 5. SmolLM2Instruct shows strong instruction following capabilities, strongly outperforming Qwen2.5-1.5B-Instruct on IFEval; our model is competitive on MT-Bench and OpenRewriteEval (Shu et al., 2024) for text rewriting, and demonstrates strong mathematical capabilities as evidenced by the GSM8K and MATH scores. These results highlight SmolLM2s ability to generalize across variety of tasks, showcasing its potential as capable chat assistant. SmolLM2 6. SmolLM2 135M and 360M In addition to SmolLM2-1.7B, we also trained two smaller models: SmolLM2-360M (360M parameters, trained on 4T tokens) and SmolLM2-135M (135M parameters, trained on 2T tokens), which are similarly state-of-the-art in their size class. Given their smaller capacity and reduced training cost, we re-ran data ablations at the target training length to determine the most effective data mixture. We found that filtering DCLM with the FineWeb-Edu classifier, removing samples with score 0, and downsampling those with scores 1 and 2 worked best. Unlike SmolLM2-1.7B, where we leveraged multi-stage training strategy, these smaller models benefited from single-stage training approach with consistently high-quality data. We incorporated Stack-Edu from the start, alongside InfiMM-WebMath, FineMath, and Cosmopedia. These models share the same architecture as SmolLM2-1.7B but use Grouped Query Attention (GQA) and were trained using the WSD scheduler with 20% decay and learning rate of 3.0 103. For post-training, we applied SFT using filtered version of SmolTalk3, removing complex instruction-following tasks (e.g., function calling) and hard examples from MagPie-Ultra to better align with the models capacity. Finally, we performed DPO training using UltraFeedback, optimizing the models for instruction-following while preserving coherence and helpfulness. More details about SmolLM2-360M and 135M can be found in their respective model cards45. 7. Conclusion SmolLM2 advances the state-of-the-art for open small LMs through combination of careful dataset curation and multistage training. Our approach highlights the critical role of high-quality, specialized datasets in enabling smaller models to achieve strong performance across variety of benchmarks. The development of FineMath, StackEdu, and SmolTalk addressed limitations in existing public datasets, improving capabilities in reasoning, mathematics, and instruction-following tasks. To support future research and development, we release SmolLM2 alongside the datasets and code used in its training. These resources provide comprehensive foundation for training performant small language models, making them accessible to broader range of researchers and applications."
        },
        {
            "title": "Acknowledgments",
            "content": "This work would not have been possible without the contributions and support of our collaborators and colleagues: 3https://huggingface.co/datasets/HuggingFaceTB/smolsmoltalk 4SmolLM2-360M model card 5SmolLM2-135M model card We thank Nouamane Tazi, Phuc Nguyen, Ferdinand Mom, and Haojun Zhao for designing and building our training framework, Nanotron. We thank Guilherme Penedo and Hynek Kydlíˇcek for building our data pipeline framework, Datatrove. We thank Clémentine Fourrier and Nathan Habib for developing our evaluation framework, LightEval. We thank all our colleagues who participated in discussions that contributed to the development and refinement of SmolLM2. We thank Muhammed Emin Baslak and Pierre-Carl Langlais for their work on enhancing SmolLM2 with Entropix. We also extend our gratitude to the broader research community and open-source ecosystem for fostering collaboration and innovation, without which this project would not have been possible."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Claudebot documentation. https://darkvisitors. com/agents/claudebot. Accessed: 2024-06-05. Common crawl. https://commoncrawl.org/. Accessed: 2024-06-05. Openai gptbot documentation. https://platform. openai.com/docs/gptbot. Accessed: 2024-0605. Abdin, M., Aneja, J., Awadalla, H., Awadallah, A., Awan, A. A., Bach, N., Bahree, A., Bakhtiari, A., Bao, J., Behl, H., et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024a. Abdin, M., Aneja, J., Behl, H., Bubeck, S., Eldan, R., Gunasekar, S., Harrison, M., Hewett, R. J., Javaheripi, M., Kauffmann, P., et al. Phi-4 technical report. arXiv preprint arXiv:2412.08905, 2024b. Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 9 Ai2. Olmo 2: The best fully open language model to date. https://allenai.org/blog/olmo2, 2024. Blog post. Bisk, Y., Zellers, R., Bras, R. L., Gao, J., and Choi, Y. Piqa: Reasoning about physical commonsense in natural language, 2019. SmolLM2 Llama 3.2: AI@Meta. and 2024a. llama-3-2-connect-2024-vision-edge-mobile-devices/. Revolutionizing edge ai customizable models, URL https://ai.meta.com/blog/ Blakeney, C., Paul, M., Larsen, B. W., Owen, S., and Frankle, J. Does your data spark joy? performance gains from domain upsampling at the end of training. arXiv preprint arXiv:2406.03476, 2024. vision with open, AI@Meta. Llama 3.2 model card, 2024b."
        },
        {
            "title": "URL",
            "content": "https://github.com/meta-llama/ llama-models/blob/main/models/llama3_ 2/MODEL_CARD.md. Broder, A. Z. On the resemblance and containment of documents. Proceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No.97TB100171), pp. 2129, 1997. Allal, L. B., Lozhkov, A., Bakouch, E., von Werra, L., and Wolf, T. Smollm - blazingly fast and remarkably powerful, 2024. Almazrouei, E., Alobeidli, H., Alshamsi, A., Cappelli, A., Cojocaru, R., Debbah, M., Goffinet, É., Hesslow, D., Launay, J., Malartic, Q., et al. The falcon series of open language models. arXiv preprint arXiv:2311.16867, 2023. Argilla. Finepersonas-v0.1 dataset. https: //huggingface.co/datasets/argilla/ FinePersonas-v0.1, 2024. Available on Hugging Face Datasets. Aryabumi, V., Su, Y., Ma, R., Morisot, A., Zhang, I., Locatelli, A., Fadaee, M., Üstün, A., and Hooker, S. To code, or not to code? exploring impact of code in pre-training. arXiv preprint arXiv:2408.10914, 2024. Azerbayev, Z., Schoelkopf, H., Paster, K., Santos, M. D., McAleer, S., Jiang, A. Q., Deng, J., Biderman, S., and Welleck, S. Llemma: An open language model for mathematics. arXiv preprint arXiv:2310.10631, 2023. Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan, Y., Ge, W., Han, Y., Huang, F., et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKinnon, C., et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022. Bai, Y., Lv, X., Zhang, J., He, Y., Qi, J., Hou, L., Tang, J., Dong, Y., and Li, J. Longalign: recipe for long context alignment of large language models. arXiv preprint arXiv:2401.18058, 2024. Bartolomé Del Canto, Á., Martín Blázquez, G., Piqueres Lajarín, A., and Vila Suero, D. Distilabel: An AI feedback (AIF) framework for building datasets with and for LLMs. https://github.com/argilla-io/ distilabel, 2024. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. Cassano, F., Gouwar, J., Nguyen, D., Nguyen, S., PhippsCostin, L., Pinckney, D., Yee, M.-H., Zi, Y., Anderson, C. J., Feldman, M. Q., et al. Multipl-e: scalable and extensible approach to benchmarking neural code generation. arXiv preprint arXiv:2208.08227, 2022. Chan, X., Wang, X., Yu, D., Mi, H., and Yu, D. Scaling synthetic data creation with 1,000,000,000 personas, 2024. URL https://arxiv.org/abs/2406.20094. Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. D. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1113, 2023. Clark, K. What does bert look at? an analysis of berts attention. arXiv preprint arXiv:1906.04341, 2019. Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try arc, the ai2 reasoning challenge, 2018. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Computations, C. Systemchat-2.0. https: //huggingface.co/datasets/ cognitivecomputations/SystemChat-2.0, 2024. 10 SmolLM2 Cui, G., Yuan, L., Ding, N., Yao, G., He, B., Zhu, W., Ni, Y., Xie, G., Xie, R., Lin, Y., et al. Ultrafeedback: Boosting language models with scaled ai feedback. In Forty-first International Conference on Machine Learning, 2024. Gunter, T., Wang, Z., Wang, C., Pang, R., Narayanan, A., Zhang, A., Zhang, B., Chen, C., Chiu, C.-C., Qiu, D., et al. Apple intelligence foundation language models. arXiv preprint arXiv:2407.21075, 2024. Daniele, L. and Suphavadeeprasit. Amplify-instruct: Synthetically generated diverse multi-turn conversations for arXiv preprint arXiv:(coming efficient llm training. soon), 2023. URL https://huggingface.co/ datasets/LDJnr/Capybara. de Vries, H. Go smol or go home. https://www.harmdevries.com/post/ model-size-vs-compute-overhead/, 2023. Ding, N., Chen, Y., Xu, B., Qin, Y., Zheng, Z., Hu, S., Liu, Z., Sun, M., and Zhou, B. Enhancing chat language models by scaling high-quality instructional conversations. arXiv preprint arXiv:2305.14233, 2023. Du, Z., Zeng, A., Dong, Y., and Tang, J. Understanding emergent abilities of language models from the loss perspective. arXiv preprint arXiv:2403.15796, 2024. Dua, D., Wang, Y., Dasigi, P., Stanovsky, G., Singh, S., and Gardner, M. Drop: reading comprehension benchmark requiring discrete reasoning over paragraphs. arXiv preprint arXiv:1903.00161, 2019. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Face, H. Everyday for conversations https://huggingface. llms. co/datasets/HuggingFaceTB/ everyday-conversations-llama3.1-2k, 2024. Gao, T., Wettig, A., Yen, H., and Chen, D. How to train long-context language models (effectively), 2024. URL https://arxiv.org/abs/2410.02660. Ge, T., Chan, X., Wang, X., Yu, D., Mi, H., and Yu, D. Scaling synthetic data creation with 1,000,000,000 personas. arXiv preprint arXiv:2406.20094, 2024. Groeneveld, D., Beltagy, I., Walsh, P., Bhagia, A., Kinney, R., Tafjord, O., Jha, A. H., Ivison, H., Magnusson, I., Wang, Y., et al. Olmo: Accelerating the science of language models. arXiv preprint arXiv:2402.00838, 2024. Hägele, A., Bakouch, E., Kosson, A., Allal, L. B., Von Werra, L., and Jaggi, M. Scaling laws and computeoptimal training beyond fixed training durations. arXiv preprint arXiv:2405.18392, 2024. Han, X., Jian, Y., Hu, X., Liu, H., Wang, Y., Fan, Q., Ai, Y., Huang, H., He, R., Yang, Z., et al. Infimm-webmath-40b: Advancing multimodal pre-training for enhanced mathematical reasoning. arXiv preprint arXiv:2409.12568, 2024. Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding, 2021. Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark, A., et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Hu, S., Tu, Y., Han, X., He, C., Cui, G., Long, X., Zheng, Z., Fang, Y., Huang, Y., Zhao, W., Zhang, X., Thai, Z. L., Zhang, K., Wang, C., Yao, Y., Zhao, C., Zhou, J., Cai, J., Zhai, Z., Ding, N., Jia, C., Zeng, G., Li, D., Liu, Z., and Sun, M. Minicpm: Unveiling the potential of small language models with scalable training strategies, 2024. URL https://arxiv.org/abs/2404.06395. Ivison, H., Wang, Y., Liu, J., Wu, Z., Pyatkin, V., Lambert, N., Smith, N. A., Choi, Y., and Hajishirzi, H. Unpacking dpo and ppo: Disentangling best practices for learning from preference feedback. arXiv preprint arXiv:2406.09279, 2024. Joshi, M., Choi, E., Weld, D. S., and Zettlemoyer, L. Triviaqa: large scale distantly supervised challenge arXiv preprint dataset for reading comprehension. arXiv:1705.03551, 2017. Joulin, A., Grave, E., Bojanowski, P., Douze, M., Jégou, H., and Mikolov, T. Fasttext.zip: Compressing text classification models. arXiv preprint arXiv:1612.03651, 2016a. Joulin, A., Grave, E., Bojanowski, P., and Mikolov, T. Bag of tricks for efficient text classification. arXiv preprint arXiv:1607.01759, 2016b. Gu, Y., Tafjord, O., Kuehl, B., Haddad, D., Dodge, J., and Hajishirzi, H. Olmes: standard for language model evaluations. arXiv preprint arXiv:2406.08446, 2024. Kamradt, G. Needle in haystack - pressure testhttps://github.com/gkamradt/ ing llms. LLMTestNeedleInAHaystack, 2024. 11 SmolLM Kocetkov, D., Li, R., Allal, L. B., Li, J., Mou, C., Ferrandis, C. M., Jernite, Y., Mitchell, M., Hughes, S., Wolf, T., et al. The stack: 3 tb of permissively licensed source code. arXiv preprint arXiv:2211.15533, 2022. Li, Y., Bubeck, S., Eldan, R., Del Giorno, A., Gunasekar, S., and Lee, Y. T. Textbooks are all you need ii: phi1.5 technical report. arXiv preprint arXiv:2309.05463, 2023b. Kong, X., Gunter, T., and Pang, R. Large language arXiv preprint model-guided document selection. arXiv:2406.04638, 2024. Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., et al. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7, 2019. Lee, H., Phatale, S., Mansoor, H., Mesnard, T., Ferret, J., Lu, K. R., Bishop, C., Hall, E., Carbune, V., Rastogi, A., et al. Rlaif vs. rlhf: Scaling reinforcement learning from human feedback with ai feedback. In Forty-first International Conference on Machine Learning. Lewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H., Ramasesh, V., Slone, A., Anil, C., Schlag, I., Gutman-Solo, T., et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:38433857, 2022. Li, C., Yuan, Z., Yuan, H., Dong, G., Lu, K., Wu, J., Tan, C., Wang, X., and Zhou, C. Mugglemath: Assessing the impact of query and response augmentation on math reasoning. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1023010258, 2024a. Li, J., Beeching, E., Tunstall, L., Lipkin, B., Soletskyi, R., Huang, S. C., Rasul, K., Yu, L., Jiang, A., Shen, Z., Qin, Z., Dong, B., Zhou, L., Fleureau, Y., Lample, G., and Polu, S. Numina- [https://huggingface.co/AI-MO/ math. NuminaMath-CoT](https://github.com/ project-numina/aimo-progress-prize/ blob/main/report/numina_dataset.pdf), 2024b. Li, J., Fang, A., Smyrnis, G., Ivgi, M., Jordan, M., Gadre, S., Bansal, H., Guha, E., Keh, S., Arora, K., et al. DatacompLM: In search of the next generation of training sets for language models. arXiv preprint arXiv:2406.11794, 2024c. Li, R., Allal, L. B., Zi, Y., Muennighoff, N., Kocetkov, D., Mou, C., Marone, M., Akiki, C., Li, J., Chim, J., et al. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023a. Li, Z., Zhang, X., Zhang, Y., Long, D., Xie, P., and Zhang, M. Towards general text embeddings with multi-stage contrastive learning. arXiv preprint arXiv:2308.03281, 2023c. Liu, Z., Hoang, T., Zhang, J., Zhu, M., Lan, T., Kokane, S., Tan, J., Yao, W., Liu, Z., Feng, Y., et al. Apigen: Automated pipeline for generating verifiable and diverse function-calling datasets. arXiv preprint arXiv:2406.18518, 2024. Loshchilov, I. and Hutter, F. dient descent with warm restarts. arXiv:1608.03983, 2016. SGDR: Stochastic graarXiv preprint Lozhkov, A., Li, R., Allal, L. B., Cassano, F., Lamy-Poirier, J., Tazi, N., Tang, A., Pykhtar, D., Liu, J., Wei, Y., et al. Starcoder 2 and the stack v2: The next generation. arXiv preprint arXiv:2402.19173, 2024. Lv, K., Zhang, W., and Shen, H. preference Supervised optimization fine-tuning https://medium.com/ on intel-analytics-software/a1197d8a3cd3, 2023. Intel Corporation. and gaudi2. direct intel Mihaylov, T., Clark, P., Khot, T., and Sabharwal, A. Can suit of armor conduct electricity? new dataset for open book question answering. In EMNLP, 2018. Mishra, S., Khashabi, D., Baral, C., and Hajishirzi, H. Crosstask generalization via natural language crowdsourcing instructions. arXiv preprint arXiv:2104.08773, 2021. Llm foundry - jeopardy dataset, 2024. https://github.com/mosaicml/ MosaicML. URL llm-foundry/blob/main/scripts/eval/ local_data/world_knowledge/jeopardy_ all.jsonl. Accessed: 2024-11-10. Muennighoff, N., Rush, A. M., Barak, B., Scao, T. L., Piktus, A., Tazi, N., Pyysalo, S., Wolf, T., and Raffel, C. Scaling data-constrained language models, 2023. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. Paster, K., Santos, M. D., Azerbayev, Z., and Ba, J. Openwebmath: An open dataset of high-quality mathematical web text. arXiv preprint arXiv:2310.06786, 2023. 12 SmolLM2 Penedo, G., Kydlícek, H., Allal, L. B., Lozhkov, A., Mitchell, M., Raffel, C., von Werra, L., and Wolf, T. The fineweb datasets: Decanting the web for the finest text data at scale. ArXiv, abs/2406.17557, 2024a. URL https://api.semanticscholar. org/CorpusID:270711474. Penedo, G., Malartic, Q., Hesslow, D., Cojocaru, R., Alobeidli, H., Cappelli, A., Pannier, B., Almazrouei, E., and Launay, J. The RefinedWeb dataset for Falcon LLM: Outperforming curated corpora with web data only. In Advances in Neural Information Processing Systems, 2024b. Petroni, F., Rocktäschel, T., Lewis, P., Bakhtin, A., Wu, Y., Miller, A. H., and Riedel, S. Language models as arXiv preprint arXiv:1909.01066, knowledge bases? 2019. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsupervised multitask learners. 2019. Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21 (140):167, 2020. Rajpurkar, P., Jia, R., and Liang, P. Know what you dont know: Unanswerable questions for squad. arXiv preprint arXiv:1806.03822, 2018. Reddy, S., Chen, D., and Manning, C. D. Coqa: conversational question answering challenge. Transactions of the Association for Computational Linguistics, 7:249266, 2019. Roberts, A., Raffel, C., and Shazeer, N. How much knowledge can you pack into the parameters of language model? arXiv preprint arXiv:2002.08910, 2020. Rolnick, D., Veit, A., Belongie, S., and Shavit, N. Deep learning is robust to massive label noise. arxiv 2017. arXiv preprint arXiv:1705.10694, 2017. Roziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., Adi, Y., Liu, J., Sauvestre, R., Remez, T., et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023. Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y. Winogrande: An adversarial winograd schema challenge at scale, 2019. Sanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L., Alyafeai, Z., Chaffin, A., Stiegler, A., Scao, T. L., Raja, A., et al. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207, 2021. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Shu, L., Luo, L., Hoskere, J., Zhu, Y., Liu, Y., Tong, S., Chen, J., and Meng, L. Rewritelm: An instruction-tuned large language model for text rewriting. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 1897018980, 2024. Singer, P., Pfeiffer, P., Babakhin, Y., Jeblick, M., Dhankhar, N., Fodor, G., and Ambati, S. S. H2o-danube-1.8 technical report. arXiv preprint arXiv:2401.16818, 2024. Soboleva, D., Al-Khateeb, F., Myers, R., Steeves, J. R., Hestness, J., and Dey, N. Slimpajama: 627b token cleaned and deduplicated version of redpajama, June 2023. URL https://huggingface.co/ datasets/cerebras/SlimPajama-627B. Soldaini, L., Kinney, R., Bhagia, A., Schwenk, D., Atkinson, D., Authur, R., Bogin, B., Chandu, K., Dumas, J., Elazar, Y., Hofmann, V., Jha, A. H., Kumar, S., Lucy, L., Lyu, X., Lambert, N., Magnusson, I., Morrison, J., Muennighoff, N., Naik, A., Nam, C., Peters, M. E., Ravichander, A., Richardson, K., Shen, Z., Strubell, E., Subramani, N., Tafjord, O., Walsh, P., Zettlemoyer, L., Smith, N. A., Hajishirzi, H., Beltagy, I., Groeneveld, D., Dodge, J., and Lo, K. Dolma: an open corpus of three trillion tokens for language model pretraining research, 2024. Talmor, A., Herzig, J., Lourie, N., and Berant, J. Commonsenseqa: question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 41494158, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1421. URL https://aclanthology.org/N19-1421. Taylor, R., Kardas, M., Cucurull, G., Scialom, T., Hartshorn, A., Saravia, E., Poulton, A., Kerkez, V., and Stojnic, R. Galactica: large language model for science. arXiv preprint arXiv:2211.09085, 2022. Team, G., Riviere, M., Pathak, S., Sessa, P. G., Hardin, C., Bhupatiraju, S., Hussenot, L., Mesnard, T., Shahriari, B., Ramé, A., et al. Gemma 2: Improving open language models at practical size, 2024. URL https://arxiv. org/abs/2408.00118, 1(3), 2024. SmolLM2 Teknium. Openhermes 2.5: An open dataset of llm assistants, 2023a. https://huggingface.co/datasets/ synthetic data for generalist URL teknium/OpenHermes-2.5. Teknium. Openhermes 2.5: An open dataset of llm assistants, 2023b. https://huggingface.co/datasets/ synthetic data for generalist URL teknium/OpenHermes-2.5. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lample, G. Llama: Open and efficient foundation language models, 2023. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. Attention is all you need. In Advances in Neural Information Processing Systems, 2017. Wan, F., Huang, X., Yang, T., Quan, X., Bi, W., and Shi, S. Explore-instruct: Enhancing domain-specific instruction coverage through active exploration. arXiv preprint arXiv:2310.09168, 2023. Wang, H., Lin, Y., Xiong, W., Yang, R., Diao, S., Qiu, S., Zhao, H., and Zhang, T. Arithmetic control of llms for diverse user preferences: Directional preference alignment with multi-objective rewards. In ACL, 2024a. Wang, H., Xiong, W., Xie, T., Zhao, H., and Zhang, T. Interpretable preferences via multi-objective reward modeling and mixture-of-experts. In EMNLP, 2024b. Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H. Self-instruct: Aligning language models with self-generated instructions. arXiv preprint arXiv:2212.10560, 2022. Wang, Y., Ma, X., Zhang, G., Ni, Y., Chandra, A., Guo, S., Ren, W., Arulraj, A., He, X., Jiang, Z., et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. arXiv preprint arXiv:2406.01574, 2024c. Wang, Z., Li, X., Xia, R., and Liu, P. Mathpile: billiontoken-scale pretraining corpus for math. In The Thirtyeight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021. Wei, Y., Cassano, F., Liu, J., Ding, Y., Jain, N., Mueller, Z., de Vries, H., von Werra, L., Guha, A., and Zhang, L. Selfcodealign: Self-alignment for code generation. arXiv preprint arXiv:2410.24198, 2024a. Wei, Y., Han, H., and Samdani, R. Arctic-snowcoder: Demystifying high-quality data in code pretraining. arXiv preprint arXiv:2409.02326, 2024b. Xu, Z., Jiang, F., Niu, L., Deng, Y., Poovendran, R., Choi, Y., and Lin, B. Y. Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing. arXiv preprint arXiv:2406.08464, 2024. Yang, A., Yang, B., Hui, B., Zheng, B., Yu, B., Zhou, C., Li, C., Li, C., Liu, D., Huang, F., Dong, G., Wei, H., Lin, H., Tang, J., Wang, J., Yang, J., Tu, J., Zhang, J., Ma, J., Xu, J., Zhou, J., Bai, J., He, J., Lin, J., Dang, K., Lu, K., Chen, K.-Y., Yang, K., Li, M., Xue, M., Ni, N., Zhang, P., Wang, P., Peng, R., Men, R., Gao, R., Lin, R., Wang, S., Bai, S., Tan, S., Zhu, T., Li, T., Liu, T., Ge, W., Deng, X., Zhou, X., Ren, X., Zhang, X., Wei, X., Ren, X., Fan, Y., Yao, Y., Zhang, Y., Wan, Y., Chu, Y., Cui, Z., Zhang, Z., and Fan, Z.-W. Qwen2 technical report. ArXiv, abs/2407.10671, 2024a. URL https://api.semanticscholar. org/CorpusID:271212307. Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024b. Yang, A., Zhang, B., Hui, B., Gao, B., Yu, B., Li, C., Liu, D., Tu, J., Zhou, J., Lin, J., et al. Qwen2.5-math technical report: Toward mathematical expert model via selfimprovement. arXiv preprint arXiv:2409.12122, 2024c. Yen, H., Gao, T., Hou, M., Ding, K., Fleischer, D., Izsak, P., Wasserblat, M., and Chen, D. Helmet: How to evaluate long-context language models effectively and thoroughly, 2024. URL https://arxiv.org/abs/ 2410.02694. Young, A., Chen, B., Li, C., Huang, C., Zhang, G., Zhang, G., Li, H., Zhu, J., Chen, J., Chang, J., et al. Yi: Open foundation models by 01.ai. arXiv preprint arXiv:2403.04652, 2024. Yu, L., Jiang, W., Shi, H., Yu, J., Liu, Z., Zhang, Y., Kwok, J. T., Li, Z., Weller, A., and Liu, W. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023. Yuan, L., Cui, G., Wang, H., Ding, N., Wang, X., Deng, J., Shan, B., Chen, H., Xie, R., Lin, Y., et al. Advancing llm reasoning generalists with preference trees. arXiv preprint arXiv:2404.02078, 2024. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag: Can machine really finish your senIn Korhonen, A., Traum, D., and Màrquez, tence? SmolLM L. (eds.), Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4791 4800, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1472. URL https://aclanthology.org/P19-1472. Zhai, X., Kolesnikov, A., Houlsby, N., and Beyer, L. Scaling vision transformers, 2022. URL https://arxiv. org/abs/2106.04560. Zhang, X., Zhang, Y., Long, D., Xie, W., Dai, Z., Tang, J., Lin, H., Yang, B., Xie, P., Huang, F., et al. mgte: Generalized long-context text representation and reranking models for multilingual text retrieval. arXiv preprint arXiv:2407.19669, 2024. Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36: 4659546623, 2023. Zhou, J., Lu, T., Mishra, S., Brahma, S., Basu, S., Luan, Y., Zhou, D., and Hou, L. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023. A. Training setup Table 6 shows the architecture details of SmolLM2 1.7B. SmolLM2 Table 6. Overview of the architecture of SmolLM2. This is before extending the context to 8k tokens. Parameter Value Layers Model Dimension FFN Dimension Attention Heads Sequence Length Token per batch Tied embedding Positional Embeddings RoPE (θ = 10, 000) Activation Function 24 2,048 8,192 32 2,048 2M Yes"
        },
        {
            "title": "SwiGLU",
            "content": "Figure 3 shows the progression of the learning rate through the training using WSD scheduler. Figure 3. Learning rate during SmolLM2 training. We used WSD scheduler with 2000 steps warmup, learning rate 5.0 104 and 10% decay. 16 B. English web ablations SmolLM2 Figure 4 shows the evaluation curves of ablation models trained on DCLM, FineWeb-Edu and their mix for 350B tokens. Figure 4. Evaluation of models trained on FineWeb-Edu and DCLM for 350B tokens. FineWeb-Edu excels at knowledge and reasoning tasks, while DCLM demonstrates stronger performance on commonsense reasoning benchmarks. 60/40 mixture of FineWeb-Edu and DCLM achieves balanced performance across all tasks. 17 C. FineMath C.1. Public datasets comparison SmolLM2 Figure 5 Shows the performance of ablation models trained on OWM and InfiMM-WebMath on GSM8k and MATH. Figure 5. Results of annealing ablations comparing OWM and the text component of InfiMM-WebMath. InfiMM-WebMath consistently outperforms OWM on GSM8K, while OWM has slight advantage on MATH. Despite training on 60B math tokens (equivalent to 5 epochs for OWM and 1.5 epochs for InfiMM-WebMath), performance remains far below state-of-the-art LLMs, highlighting the need for new math dataset. C.2. Annotation Prompt (3-scale) We used the following prompt template to generate the silver 3-scale annotations for FineMath using the Llama3 model: Evaluate the following text extract for its potential usefulness for studying mathematics up to high school and early undergraduate levels. Use the following 3-point scoring system described below. Points are accumulated based on the satisfaction of each criterion: - Add 1 point if the extract contains some mathematical content, even if its not very useful for studying or is an academic paper that is too advanced. - Add another point if the extract demonstrates logical reasoning in mathematical context, even if it lacks step-by-step explanations or is too advanced. - Award third point if the extract is at an appropriate level (up to high school and early undergraduate levels) and contains clear mathematical deductions and step-by-step solutions to mathematical problems. Question-answer formats (e.g., from educational websites or forums) are acceptable if they meet the criteria. Ignore any formatting errors or missing equations and make assumptions based on the overall content. The text extract: <EXTRACT> After examining the extract: - Briefly justify your total score, up to 100 words. - Conclude with the score using the format: \"Final score: <total points>\". C.3. Annotation Prompt (5-scale) We used the following prompt template to generate the 5-scale annotations for FineMath using the Llama3 model during the second filtering stage: Evaluate the following text extract for its potential usefulness for studying mathematics up to high school and early undergraduate levels. Use the following 5-point scoring system described below. Points are accumulated based on the satisfaction of each criterion: 18 SmolLM - Add 1 point if the extract contains some mathematical content, even if its not very useful for studying, or if it contains non-academic content such as advertisements and generated pages for converting weight and currencies. - Add another point if the extract touches on mathematical topics, even if its poorly written if its too complex such as an academic paper that is too advanced. - Award third point if the extract demonstrates problem solving or logical reasoning in mathematical context, even if it lacks step-by-step explanations. - Grant fourth point if the extract is at an appropriate level (up to high school and early undergraduate levels) and contains clear mathematical deductions and step-by-step solutions to mathematical problems. It should be similar to chapter from textbook or tutorial. - Give fifth point if the extract is outstanding in its educational value for teaching and studying mathematics in middle school and high school. It should include very detailed and easy to follow explanations. Question-answer formats (e.g., from educational websites or forums) are acceptable if they meet the criteria. The text extract: <EXTRACT> After examining the extract: - Briefly justify your total score, up to 100 words. - Conclude with the score using the format: Final score: <total points>. D. Stack-Edu D.1. Annotation Prompt SmolLM2 We used the following prompt template to generate the 5-scale annotations for Stack-Edu (Python in this case) using the Llama3 model: Below is an extract from Python program. Evaluate whether it has high educational value and could help teach coding. Use the additive 5-point scoring system described below. Points are accumulated based on the satisfaction of each criterion: - Add 1 point if the program contains valid Python code, even if its not educational, like boilerplate code, configs, and niche concepts. - Add another point if the program addresses practical concepts, even if it lacks comments. - Award third point if the program is suitable for educational use and introduces key concepts in programming, even if the topic is advanced (e.g., deep learning). The code should be well-structured and contain some comments. - Give fourth point if the program is self-contained and highly relevant to teaching programming. It should be similar to school exercise, tutorial, or Python course section. - Grant fifth point if the program is outstanding in its educational value and is perfectly suited for teaching programming. It should be well-written, easy to understand, and contain step-by-step explanations and comments. The extract: <EXAMPLE> After examining the extract: - Briefly justify your total score, up to 100 words. - Conclude with the score using the format: \"Educational score: <total points> We use similar prompts for the other 14 programming languages in Stack-Edu, adjusting the examples in the third criterion to reflect language-specific topics. For instance, in the JavaScript prompt, we replace \"deep learning\" with \"asynchronous programming\". D.2. Stack-Edu language statistics Table 7 shows the size of each programming language in Stack-Edu before and after the educational filtering. Initially, we also included HTML, but the classifier performed poorly, so we retained StarCoder2Data. Table 7. Stack-Edu dataset statistics across programming languages. The table shows the original dataset size (from StarCoder2Data) and filtered Stack-Edu size for each programming language. Language StarCoder2Data (B tokens) Stack-Edu (B tokens) Python Cpp Markdown JavaScript Java SQL PHP C-Sharp TypeScript Shell Swift Go Rust Ruby 50.6 69.7 80.4 38.4 45.3 45.6 13.7 44.9 33.4 12.2 4.17 3.71 3.67 3.39 5.76 21.8 16.0 14.0 11.1 11.1 42.1 9.62 9.07 8.87 3.03 3.13 1.83 1.80 1.75 1. 20 E. Detailed pretraining results E.1. Evaluation after each training stage SmolLM2 Table 8 shows the evaluation results of SmolLM2 at the end of each training stage. In addition to the benchmarks used during the ablations, we added four generative tasks: CoQA (Reddy et al., 2019), DROP (Dua et al., 2019), Jeopardy (MosaicML, 2024) and SQuAD v2 (Rajpurkar et al., 2018) Table 8. Per-benchmark model performance across training stages. Stages 1-3 are during stable phase (no learning rate decay)."
        },
        {
            "title": "Tokens",
            "content": "MMLU (MCF) HellaSwag ARC OpenBookQA WinoGrande PIQA GSM8K MATH HumanEval Multiple-E Java Multiple-E JS CoQA DROP Jeopardy SQuAD v2 Stage 1 0-6T Stage 2 6-8T Stage 3 8-10T Stage 4 10-11T 29.62 66.17 59.95 42.00 58.88 76.39 4.32 2.1 10.97 5.70 9.94 33.43 13.69 23.1 55. 37.96 65.29 60.08 42.40 58.33 76.50 4.62 2.78 9.15 10.12 12.42 33.98 11.36 22.4 57.45 42.54 66.29 58.66 41.40 58.64 77.26 10.01 4. 17.68 14.56 18.01 38.82 17.19 25.54 57.26 48.87 69.26 60.99 43.60 61.09 77.64 32.60 11.54 22.60 23.42 23.60 40.45 19.22 23.35 61. E.2. MMLU progression Figure 6 shows the progression of MMLU scores throughout the stable phase. Figure 6. Progression of MMLU MCF and MLU CF during the training. We observe above-random (>25%) accuracy on MMLU MCF after 6T tokens of training, while MMLU CF appears to plateau. 21 F. Post-training Table 9 shows the final composition of SmolTalk dataset. SmolLM2 Table 9. Composition of the SmolTalk dataset. The total dataset contains 1.1M instruction-response pairs from different data sources. Dataset source Number of samples in SmolTalk MagPie-Ultra Smol-Rewrite Smol-Constraints Smol-Summarization NuminaMath-CoT MetaMathQA"
        },
        {
            "title": "Other",
            "content": "Self-OSS-Starcoder2-Instruct APIGen-Function-Calling SystemChats2.0 LongAlign Everyday-Conversations Explore-Instruct-Rewriting OpenHermes2.5 Total 431k 56.2k 36.2k 101k 112k 50k 50.7k 87.5k 35.9k 3.73k 2.38k 32k 100k 1.1M Table 10 shows the performance after training on the different components of SmolTalk we consider. The top section compares the results of fine-tuning SmolLM2 base on different instruction datasets, while the bottom section evaluates the impact of adding 20% specialized math data to base mixture of 80% MagPie-Ultra+ during the SFT. The last row, SmolLM2-SFT, represents the final SFT checkpoint of SmolLM2 before DPO, trained for two epochs on the full SmolTalk dataset. 22 SmolLM2 Table 10. Performance on instruction-tuning datasets. MagPie-Ultra+ refers to MagPie-Ultra combined with Smol-Constraints, SmolRewrite, and Smol-Summarization. MagPie-Pro-MT is multi-turn while MagPie-Pro is the single turn version. All comparisons were performed by fine-tuning the SmolLM2 base model on each dataset for 1 epoch. SmolLM2-SFT, the final supervised fine-tuned version of SmolLM2, was trained for 2 epochs on SmolTalk. Dataset IFEval MTB GSM8K MATH ARC-C MMLU-Pro OpenHermes UltraChat MagPie-Pro MagPie-Pro-MT MagPie-Ultra MagPie-Ultra+ Instruction datasets comparison 30.01 27.26 30.45 31.66 35.49 48.16 1.02 4.66 4.31 5.40 5.22 5.28 42.91 30.40 14.56 20.55 24.34 19.94 Math datasets comparison MagPie-Ultra+ + MathInstruct MagPie-Ultra+ + MetaMathQA MagPie-Ultra+ + NuminaMath-CoT 47.05 44.98 46.27 5.43 5.02 5.99 30.1 47.08 25.32 Full SmolTalk 12.76 9.06 6.64 7.84 13.56 12. 14.0 17.56 18.00 40.27 41.21 36.01 36.69 37.71 38.91 38.99 36.77 37.88 SmolTalk SmolLM2-SFT 46.67 57.09 5.49 6. 43.75 47.54 18.60 19.64 40.02 42.49 20.32 15.79 12.19 11.97 12.01 12.43 13.65 12.18 12.58 18.19 19. 23 G. Long context evaluations Figure 7 shows the evaluation results on the Needle in the Haystack benchmark. SmolLM2 Figure 7. Needle in the Haystack evaluation of SmolLM2 with 8192 context length. Table 11 shows the evaluation results on the HELMET benchmark. Table 11. Evaluation results of the base models on the HELMET benchmark using 8k maximum input length. Metric SmolLM2-1.7B Llama3.2-1B Qwen2.5-1.5B Average-Real Average-All Recall RAG ICL Re-rank LongQA 31.67 32.61 36.38 47.17 23.20 23.31 33.00 35.56 39.61 55.81 42.13 51.20 26.93 21. 38.76 44.40 66.94 47.54 52.00 29.29 26."
        }
    ],
    "affiliations": [
        "HuggingFaceTB"
    ]
}