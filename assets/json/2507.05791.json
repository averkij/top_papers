{
    "paper_title": "GTA1: GUI Test-time Scaling Agent",
    "authors": [
        "Yan Yang",
        "Dongxu Li",
        "Yutong Dai",
        "Yuhao Yang",
        "Ziyang Luo",
        "Zirui Zhao",
        "Zhiyuan Hu",
        "Junzhe Huang",
        "Amrita Saha",
        "Zeyuan Chen",
        "Ran Xu",
        "Liyuan Pan",
        "Caiming Xiong",
        "Junnan Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Graphical user interface (GUI) agents autonomously operate across platforms (e.g., Linux) to complete tasks by interacting with visual elements. Specifically, a user instruction is decomposed into a sequence of action proposals, each corresponding to an interaction with the GUI. After each action, the agent observes the updated GUI environment to plan the next step. However, two main challenges arise: i) resolving ambiguity in task planning (i.e., the action proposal sequence), where selecting an appropriate plan is non-trivial, as many valid ones may exist; ii) accurately grounding actions in complex and high-resolution interfaces, i.e., precisely interacting with visual targets. This paper investigates the two aforementioned challenges with our GUI Test-time Scaling Agent, namely GTA1. First, to select the most appropriate action proposal, we introduce a test-time scaling method. At each step, we sample multiple candidate action proposals and leverage a judge model to evaluate and select the most suitable one. It trades off computation for better decision quality by concurrent sampling, shortening task execution steps, and improving overall performance. Second, we propose a model that achieves improved accuracy when grounding the selected action proposal to its corresponding visual elements. Our key insight is that reinforcement learning (RL) facilitates visual grounding through inherent objective alignments, rewarding successful clicks on interface elements. Experimentally, our method establishes state-of-the-art performance across diverse benchmarks. For example, GTA1-7B achieves 50.1%, 92.4%, and 67.7% accuracies on Screenspot-Pro, Screenspot-V2, and OSWorld-G, respectively. When paired with a planner applying our test-time scaling strategy, it exhibits state-of-the-art agentic performance (e.g., 45.2% task success rate on OSWorld). We open-source our code and models here."
        },
        {
            "title": "Start",
            "content": "2025-07-09 GTA1: GUI Test-time Scaling Agent Yan Yang1,2 Zhiyuan Hu1 Dongxu Li (cid:12)2 Junzhe Huang2 Yutong Dai1 Amrita Saha1 Caiming Xiong1 Junnan Li (cid:12) Yuhao Yang3 Zeyuan Chen1 Ziyang Luo1 Ran Xu1 Zirui Zhao1 Liyuan Pan2 1Salesforce AI Research 2The Australian National University 5 2 0 2 8 ] . [ 1 1 9 7 5 0 . 7 0 5 2 : r 3University of Hong Kong (cid:12) Corresponding Author dongxuli1005@gmail.com junnan.li@salesforce.com"
        },
        {
            "title": "Abstract",
            "content": "Graphical user interface (GUI) agents autonomously operate across platforms (e.g., Linux) to complete tasks by interacting with visual elements. Specifically, user instruction is decomposed into sequence of action proposals, each corresponding to an interaction with the GUI. After each action, the agent observes the updated GUI environment to plan the next step. However, two main challenges arise: i) resolving ambiguity in task planning (i. e., the action proposal sequence), where selecting an appropriate plan is non-trivial, as many valid ones may exist; ii) accurately grounding actions in complex and high-resolution interfaces, i. e., precisely interacting with visual targets. This paper investigates the two aforementioned challenges with our GUI Test-time Scaling Agent, namely GTA1. First, to select the most appropriate action proposal, we introduce test-time scaling method. At each step of task execution, we sample multiple candidate action proposals and leverage judge model to evaluate and select the most suitable one. It trades off computation for better decision quality by concurrent sampling, shortening task execution steps, and improving Second, we propose model that achieves improved accuracy when overall performance. grounding the selected action proposal to its corresponding visual elements. Our key insight is that reinforcement learning (RL) facilitates visual grounding through inherent objective alignments, rewarding successful clicks on interface elements. Experimentally, our GUI grounding model establishes state-of-the-art performance across diverse benchmarks. For example, GTA1-7B achieves 50.1%, 92.4%, and 67.7% accuracies on ScreenspotPro, Screenspot-V2, and OSWorld-G benchmarks, respectively. Furthermore, when paired with planner applying our test-time scaling strategy, it exhibits state-of-the-art agentic performance (e.g., 45.2% task success rate on OSWorld benchmark). We open-source our code and models here."
        },
        {
            "title": "Introduction",
            "content": "Automating user task execution across diverse platforms through GUI agents represents significant milestone toward general artificial intelligence, supporting users in tasks ranging from placing online orders to performing complex, expert-level professional workflows [1]. To solve task, GUI agent translates user instructions into multi-steps of interactions such as action proposals consisting of clicks or keystrokes [2]. This introduces ambiguity in planning, as multiple valid action proposal sequences may exist for the same user task. The challenge is further intensified by the high-resolution (up to 4K), complex, and hierarchical layouts of GUI [36], requiring accurately identifying the coordinates to interact with the target user interface (UI) elements. This work aims to address both challenges (i. e., planning and grounding) towards performant GUI agent. Formally, existing works [1, 69] often pair GUI grounding model with planner (e.g., Gemini 2.5 [10], Claude 3.7 [11], or o3 [12]). The planner is responsible for determining the action proposal required at each step, while the grounding model is used to locate the target interface elements when an action must be visually grounded. However, due to the inherent flexibility of user tasks, multiple feasible action proposal sequences may exist for completing the same task. Some of these plans are more direct and efficient, while others may involve unnecessarily long or complex action sequences. This makes the agent highly susceptible to cascading failures, i. e., errors in early grounding or planning steps can derail the entire task. This fragility is often reflected in the high performance variance among multiple runs of the same agent [6, 13]. straightforward idea to avoid it is rollouting the full action proposal sequences in advance. However, unlike domains such as mathematical problem-solving, GUI environments lack"
        },
        {
            "title": "Salesforce AI Research",
            "content": "2025-07-09 Figure 1: Performance comparisons of our method and state-of-the-art methods. Each method is labeled with its name next to its icon; red dot represents our method. (a) Grounding accuracy (%) on the ScreenSpot-Pro benchmark [3] across various model scales in billions (B) of parameters. (b) Task success rate (%) on the OSWorld benchmark [14], shown as it evolves over time. lookahead capability: agents cannot rollout several multi-step plans and retrospectively choose the best one, because executing actions in GUI environments usually has irreversible state effects. Therefore, we pose central question: How can we design GUI agents that remain robust in planning, considering the lack of lookahead and the presence of multiple plausible action proposal sequences? Beyond planning, GUI grounding models usually rely on supervised fine-tuning (SFT) [1, 2, 46]. It rigidly trains models to predict the exact center of the target element. While this approach can be effective, it often struggles to generalize in complex GUI environments, particularly in out-of-domain professional interfaces [3]. Moreover, the formulation of SFT introduces mismatch with the nature of the GUI grounding task: any coordinate within the target element should be considered valid prediction, yet SFT optimizes only for the center, limiting the model flexibility and robustness, and preventing the model from perceiving appropriate supervision signals. In the alternative, motivated by the success of DeepSeek-R1-Zero [15], RL, specifically Group Relative Policy Optimization (GRPO) [16], has been explored for the GUI grounding tasks. Following [15], past works [1719] formulates by first performing thinking (i. e., textual CoT reasoning), and then deriving coordinate prediction. In general, the model is trained using format function that enforces the presence of textual reasoning, and click reward that verifies whether the predicted coordinate falls within the target element region. Meanwhile, some works [19, 20] also extend this formulation to predict the bounding box of the target UI element. While these approaches have shown performance improvements over SFT, we ponder another fundamental question: Is explicit thinking or auxiliary bounding box reward truly necessary for effective GUI grounding? We investigate these two questions by proposing two complementary strategies: i) test-time scaling strategy for planning, which works in tandem with grounding model to execute user tasks more robustly; ii) RL-based grounding model to predict interaction coordinates. Specifically, we develop test-time scaling strategy that addresses planning ambiguity without requiring lookahead. For an agent consisting of grounding model and planner, our method avoids committing to single action proposal sequence to complete user task. Instead, at each step of task execution, we sample multiple candidate action proposals from the planner. These candidates are then evaluated using multimodal large language model, which act as judge to select the most contextually appropriate option. Before executing the chosen action proposal in the GUI environment, we ground the actions to the target visual element by predicting its associated coordinates using the grounding model if the action is coordinate-based, ensuring accurate interaction with the target interface element. This strategy enables the agent to navigate ambiguous tasks more robustly by exploring short-term alternatives without relying on forward rollout of the action proposal sequence. Then, we introduce simple and straightforward approach that optimizes GUI grounding model by designing it to directly predict coordinate, and rewarding the model based on whether the predicted point falls within the target UI element. Due to the objective alignment between the training signal and the grounding task, this method is highly efficient. Despite its simplicity, this approach achieves state-of-the-art performance across diverse GUI grounding"
        },
        {
            "title": "Salesforce AI Research",
            "content": "2025-07-09 benchmarks. Interestingly, we also find that thinking in the form of reasoning over the task object, past trajectories, and user instructions can be beneficial in dynamic environments where context evolves over time. However, such reasoning is usually difficult to generalize in broader grounding scenarios, limiting its practicality. Our contributions are summarized as follows: we conduct comprehensive study of GUI agents, focusing on the key challenges of grounding and planning in real-world, high-resolution, and dynamic UI environments; we propose simple yet effective GUI grounding model that directly predicts interaction coordinates without requiring explicit reasoning; we introduce test-time scaling strategy for the planner that pairs with the grounding model, improving planning robustness, and resolving execution uncertainty. performance overview is shown in Fig. 1. Our method consistently outperforms previous approaches across diverse GUI grounding benchmarks. Furthermore, using o3 [12] as planner and judge for test-time scaling, GTA1 demonstrates precise interaction in real environments to accomplish user tasks, establishing lightweight and effective pathway toward agentic GUI behavior."
        },
        {
            "title": "2 Related Work",
            "content": "We review recent achievements of GUI agents, with primary focus on multimodal large language model-based agents. We exclude studies that do not rely on visual capabilities (e.g., those using HTML or a11y trees). We first reviews the GUI grounding, and then categorize existing agents into two broad types: native GUI agents and two-stage GUI agents. GUI Grounding. GUI grounding refers to the task of mapping user instructions (usually at low level) to specific coordinates corresponding to target UI elements. Early works [1, 2, 5, 9] primarily focus on SFT, training models to predict the center point of the intended UI element. However, this approach does not fully align with the nature of the GUI grounding task, where any coordinate within the target element should be considered success. As result, SFT-based models often exhibit poor generalization, particularly when evaluating on high-resolution and visually complex user interfaces [3]. With the success of DeepSeek-R1-Zero [15], RL (specifically GRPO [16]) has drawn increased attention in this domain. However, many recent efforts naively replicate techniques from other domains [1719], such as prompting the model to generate thinking (i. e., CoT reasoning) before producing an answer, and the answer is rewarded only if the predicted coordinates fall within the target element region. This strategy overlooks an important insight: the thinking degrade performance in GUI grounding, where precise spatial perception matters more than multi-step verbal reasoning. The main benefit of RL in the GUI grounding task lies in objective alignment, rewarding any coordinate within the correct region rather than producing textual thinking. concurrent study [20] makes similar observations, noting that CoT reasoning (i. e., thinking) is not required for RL training in GUI grounding and may even hinder grounding accuracy. Our work further distinguishes itself in the following ways: i) we clarify that thinking is not necessary for GUI grounding in static environments; ii) we demonstrate that thinking improves grounding performance in dynamic, real-world environments when provided with past trajectories and task objectives; iii) we conduct comprehensive study of RL-based GUI grounding across models of various scales. Going beyond prior work, we also evaluate how the model, when paired with planner, performs in realistic and dynamic environments, critical aspect that existing studies largely overlook. Two-stage GUI Agent. One major challenge in GUI grounding is accurately locating the coordinates of UI elements intended for interaction. To address this, two-stage GUI agents modularize into planning and action, each handled by separate models [2]. They usually leverage advanced reasoning models, such as GPT-4o [21] and Claude 3.7 [11], as planners to generate an action proposal for each step from the user task instruction, using real-time UI screenshot and past trajectories as context. separate visual grounding module then maps these instructions to specific UI elements, enabling the development of vision-only GUI agents [1, 2, 5]. While most existing work primarily focuses on GUI grounding, more complex components, such as memory management and external knowledge bases, are also being explored to enhance agent performance [8]. This paper follows the two-stage method on establishing GUI agent. Native GUI Agent. native GUI agent completes user tasks in an end-to-end manner. Four main aspects are studied: i) perception, interpreting the user interface to understand the current state; ii) memory, storing knowledge and historical experiences to support making decisions; ii) planning, analyzing the task and reflecting on progress to generate action proposals; iv) action, performing atomic operations based on the action proposal to effectively progress toward the task goal. We refer the reader to [13] for more details. Some examples of native GUI agents can be specified to CUA [12, 21] and Claude Computer Use [11]. One of the main challenges for native GUI agents lies in long-context"
        },
        {
            "title": "Salesforce AI Research",
            "content": "2025-07-09 Figure 2: Overview of our GUI agent architecture. At each step, the trajectory, current UI screenshot, and user instruction are sent to planner, which samples multiple action proposals. multimodal large language model judge is then used to select the best candidate action proposal. When the candidate action proposal is coordinate-based action (e.g., click), the grounding model predicts precise interaction point on the GUI for executing the action. For non-coordinate-based actions (e.g., key presses), the action can be executed directly without grounding. learning. To address this, some approaches employ sliding window mechanism [13], while others maintain textual description of past trajectories to manage contextual information [9]. In practice, end-to-end native GUI agents have demonstrated strong performance in completing user tasks, as shown by benchmarks that reflect dynamic and realistic UI environments, such as OSWorld [14]. However, this paper is the first to show that two-stage GUI agent can achieve competitive performance in such environments, challenging the assumption that end-to-end approaches are inherently superior."
        },
        {
            "title": "3 Method",
            "content": "Overview. Our method adopts two-stage GUI agent framework composed of planner and grounding model, and focuses on improving planning robustness and grounding accuracy through the following key components: i) test-time scaling for planning, which introduces scaling strategy during inference to effectively handle planning ambiguity in complex GUI environments; ii) grounding model training, filtering out training samples with annotation errors to improve supervision quality, and optimizing grounding model using RL (e.g., GRPO) to directly predict coordinates without relying on any intermediate thinking (i. e., CoT reasoning) on the derived data."
        },
        {
            "title": "3.1 Test-time Scaling for Planning",
            "content": "At each step of executing user instructions in real-world environments, planner is provided with the user instruction (i. e., task objective), the trajectory so far, and the current UI screenshot. Based on this context, we sample 𝐾 candidate action proposals, denoted as {𝒂𝑘 }𝐾 𝑘=1, where 𝒂𝑘 represents corresponding action (e.g., clicking the blue button or performing keystroke). Then, multimodal large language model judge is used to evaluate the 𝐾 candidates {𝒂𝑘 }𝐾 𝑘=1 based on their alignment with the user intent and the current GUI state. The judge, which can be the planner model itself, picks the best candidate action proposal 𝒂𝑘 from {𝒂𝑘 }𝐾 𝑘=1, allowing the agent to select the most contextually appropriate option. Once the best candidate action proposal 𝒂𝑘 is selected, the grounding model 𝜋(, ) takes 𝒂𝑘 and the current screenshot 𝒙 as the input. If 𝒂𝑘 is coordinate-based action (e.g., click), the grounding model predicts precise interaction point on the GUI, which is then used to execute the action. For non-coordinate-based actions (e.g., key presses or text input), the action can be executed directly without grounding. This process is repeated step by step until the task is completed or the agent reaches termination condition. By incorporating sampling, judging, and grounding into each step, our agent avoids overcommitting to suboptimal action proposals and demonstrates improved robustness in complex, ambiguous, and dynamic GUI environments. We show an overview in Fig. 2."
        },
        {
            "title": "3.2 Grounding",
            "content": "Data Cleaning. We leverage dataset collections curated by Aria-UI [1] and OS-Atlas [4]. To train with reward signal that verifies whether the predicted coordinates fall within the target UI element, we require dataset that provides accurate bounding boxes for annotated interactive elements. For data points from desktop and web domains, the collection usually involves screenshots paired with accessibility tools such as A11y or HTML parsers to extract element structure and bounding box annotations. However, these bounding boxes can sometimes be misaligned with"
        },
        {
            "title": "Salesforce AI Research",
            "content": "2025-07-09 Figure 3: Examples from the Aria-UI dataset collection [1]. The blue bounding box shows the derived annotation 𝑏ann, while the red bounding boxes are detected using OmniParser [22]. large green arrow is used to draw attention to the misaligned blue bounding box. Our lightweight cleaning strategy filters out such cases where the annotation does not match the actual UI element. the actual visual rendering due to UI animations, timing inconsistencies, or rendering delays, introducing noise into the training signal. Therefore, to improve data quality, we apply lightweight cleaning strategy. Given UI screenshot 𝒔, we use OmniParser [22], denoted as (), to detect all UI elements in 𝒔, resulting in set of bounding boxes {𝑏𝑖 } = (𝒔). Each 𝑏𝑖 represents the bounding box of detected UI element. For data point 𝒔 with an annotated bounding box 𝑏ann, we discard the sample if the maximum Intersection over Union between 𝑏ann and any 𝑏𝑖 {𝑏𝑖 } is smaller than predefined threshold 𝜏, max 𝑏𝑖 (𝒔) IoU(𝑏ann, 𝑏𝑖) < 𝜏 , (1) where IoU(, ) computes the overlap between two bounding boxes, defined as the area of their intersection divided by the area of their union. This helps ensure that the annotation 𝑏ann in the training data remains consistent with the actual visual target, thereby reducing noise caused by misaligned annotations. We show some samples in Fig. 3. Training. In our RL training, we follow the GRPO framework [15, 16] to sample 𝑁 responses {𝒐𝑛} 𝑁 𝑛=1 from the policy multimodal large language model 𝜋(, ), given screenshot 𝒔 and an action proposal 𝒂 as input. Here, each response 𝑜𝑛 represents pair of pixel coordinates on the screen, i. e., 𝒐𝑛 = (𝑥𝑛, 𝑦𝑛), where 𝑥𝑛 and 𝑦𝑛 denote the horizontal and vertical positions, respectively. Unlike prior approaches, we do not prompt the model to generate thinking (i. e., intermediate CoT reasoning) before producing response. Instead, the model directly outputs the predicted coordinates, aligning more closely with the nature of the GUI grounding task. Then, each response is evaluated by checking whether the predicted coordinate (𝑥𝑛, 𝑦𝑛) falls within the annotated bounding box bann = (𝑥min, 𝑦min, 𝑥max, 𝑦max). This yields set of 𝑁 binary rewards {𝑟𝑛} 𝑁 𝑛=1, where each reward is 𝑟𝑛 = (cid:26)1, 0, if 𝑥min 𝑥𝑛 𝑥max and 𝑦min 𝑦𝑛 𝑦max , otherwise . (2)"
        },
        {
            "title": "Salesforce AI Research",
            "content": "2025-07-09 Table 1: Comparison with state-of-the-art methods on the ScreenSpot-Pro dataset [3]. We report the grounding accuracy (%) across various task domains, categorizing results by grounding target type: Text, Icon, and the overall average (Avg). We use - to denote unavailability, and to denote the results evaluated by us (which will be updated if improved evaluation scripts become available). The final average scores are highlighted in dark blue. Agent Model Proprietary Models GPT-4o [21] Claude 3.7 Sonnet [11] Operator [23] Seed-1.5-VL [24] UI-TARS-1.5 [13] Development Creative CAD Scientific Office OS Avg Text Icon Avg Text Icon Avg Text Icon Avg Text Icon Avg Text Icon Avg Text Icon Avg Text Icon Avg - - 1.3 0.0 0.7 1.0 0.0 0.6 2.0 0.0 1.5 2.1 0.0 1.2 1.1 0.0 0.9 0.0 0.0 0.0 1.3 0.0 0.8 - 27.7 - - 50.0 19.3 35.1 51.5 23.1 39.6 16.8 14.1 16.1 58.3 24.5 43.7 60.5 28.3 53.0 34.6 30.3 32.7 45.0 23.0 36.6 - 60.9 - 61.6 - 74.8 - - 79.6 - - 61.4 - - 69.3 - - 60.2 - - 51.0 - - 59.2 - - 50.4 - - 59.0 - - 58.2 - - 53.8 - - 63.9 - - - - - - - - - - - - - - - - - - - 11.2 6.3 - 27.3 3.5 - 42.4 11.8 - 32.2 11.3 - 13.1 4.5 Open-Source Models 0.6 0.0 0.3 1.0 0.0 0.6 2.5 0.0 1.9 3.5 0.0 2.0 1.1 0.0 0.9 2.8 0.0 1.5 1.8 0.0 1.1 SeeClick [5] 2.6 0.0 1.3 1.5 0.0 0.9 0.5 0.0 0.4 6.3 0.0 3.5 3.4 1.9 3.0 0.9 0.0 0.5 2.5 0.2 1.6 Qwen2-VL-7B [25] 16.9 1.4 9.4 9.1 0.0 5.3 2.5 0.0 1.9 13.2 7.3 10.6 15.3 7.5 13.5 10.3 2.2 6.6 10.8 2.6 7.7 ShowUI-2B [26] 14.9 0.7 8.0 9.6 0.0 5.6 7.1 3.1 6.1 22.2 1.8 13.4 13.0 0.0 10.0 5.6 0.0 3.1 12.0 0.8 7.7 CogAgent-18B [27] 16.2 0.0 8.4 23.7 2.1 14.7 7.6 1.6 6.1 27.1 6.4 18.1 20.3 1.9 16.1 4.7 0.0 2.6 17.1 2.0 11.3 Aria-UI [1] - 17.8 22.7 4.1 UI-R1-3B [18] 33.1 1.4 17.7 28.8 2.8 17.9 12.2 4.7 10.3 37.5 7.3 24.4 33.9 5.7 27.4 27.1 4.5 16.8 28.1 4.0 18.9 OS-Atlas-7B [4] UI-TARS-2B [28] 47.4 4.1 26.4 42.9 6.3 27.6 17.8 4.7 14.6 56.9 17.3 39.8 50.3 17.0 42.6 21.5 5.6 14.3 39.6 8.4 27.7 Qwen2.5-VL-3B [25] 38.3 3.4 21.4 40.9 4.9 25.8 22.3 6.3 18.4 44.4 10.0 29.5 48.0 17.0 40.9 33.6 4.5 20.4 37.8 6.6 25.9 Qwen2.5-VL-7B [25] 51.9 4.8 29.1 36.9 8.4 24.9 17.8 1.6 13.8 48.6 8.2 31.1 53.7 18.9 45.7 34.6 7.9 22.4 39.9 7.6 27.6 - 31.1 UGround-7B [2] UGround-72B [2] - 34.5 58.4 12.4 36.1 50.0 9.1 32.8 20.8 9.4 18.0 63.9 31.8 50.0 63.3 20.8 53.5 30.8 16.9 24.5 47.8 16.2 35.7 UI-TARS-7B [28] 51.3 12.4 32.4 44.9 7.0 29.0 33.0 14.1 28.4 58.3 20.0 41.7 65.5 28.3 57.0 43.9 12.4 29.6 49.1 14.1 35.7 InfiGUI-R1-3B [19] 55.8 7.6 35.1 47.0 4.9 29.0 38.1 12.5 31.8 61.8 16.4 43.3 59.9 24.5 50.9 40.2 12.4 25.5 50.4 11.8 35.9 SE-GUI-3B [29] 61.0 13.8 38.1 53.5 8.4 34.6 27.4 9.4 23.0 54.2 18.2 38.6 64.4 32.1 57.0 38.3 9.0 25.0 49.8 13.7 36.1 Jedi-3B [6] 50.7 10.3 31.1 36.6 11.9 26.6 39.6 9.4 32.2 61.8 30.0 48.0 67.2 32.1 59.1 23.5 10.6 16.1 49.5 16.8 37.1 GUI-G1-3B [20] 63.0 17.3 40.8 57.1 15.4 39.6 18.8 12.5 17.2 64.6 20.9 45.7 63.3 26.4 54.8 42.1 15.7 30.1 50.9 17.5 38.1 UI-TARS-72B [28] Jedi-7B [6] 42.9 11.0 27.4 50.0 11.9 34.0 38.0 14.1 32.2 72.9 25.5 52.4 75.1 47.2 68.7 33.6 16.9 26.0 52.6 18.2 39.5 UI-TARS-1.5-7B [28] 58.4 12.4 31.8 58.1 15.4 40.2 38.6 11.0 31.8 66.7 21.9 47.2 74.6 35.9 65.6 49.5 13.5 33.2 57.5 16.9 42.0 Qwen2.5-VL-32B [25] 74.0 21.4 48.5 61.1 13.3 41.1 38.1 15.6 32.6 78.5 29.1 57.1 76.3 37.7 67.4 55.1 27.0 42.3 63.2 22.5 47.6 68.2 19.3 44.5 57.6 9.1 37.2 51.3 42.2 42.1 75.0 28.2 54.7 78.5 43.4 70.4 49.5 25.8 38.8 63.5 21.0 47.3 SE-GUI-7B [29] - 53.3 Qwen2.5-VL-72B [25] - 48.8 - - 51.3 - - 26.1 - - 25.5 - - 38.8 - - 50.0 - - 13.5 - - 13.8 - - 35.5 - - 31.1 - - 27.8 - - 35.8 - - 44.4 - - 49.5 - - 59.1 - - 72.6 - - 44.9 - - 53.5 - - - - - - GTA1-7B GTA1-32B GTA1-72B 66.9 20.7 44.5 62.6 18.2 44.0 53.3 17.2 44.4 31.8 76.4 57.1 82.5 50.9 75.2 48.6 25.9 38.3 65.5 25.2 50.1 82.5 28.3 56.2 69.2 14.7 46.3 43.7 23.4 38.7 79.9 31.8 59.1 80.8 43.4 72.2 70.1 32.6 53.1 69.9 27.2 53.6 79.9 33.1 57.2 73.2 20.3 51.0 56.9 28.1 49.8 81.9 38.2 63.0 85.3 49.1 77.0 73.8 37.1 57.1 74.5 32.5 58.4 We then normalize the rewards {𝑟𝑛} 𝑁 Finally, the model is optimized by 𝑣𝑛 = 1 𝑁 𝑛=1 into advantages using Z-score normalization, (cid:205)𝑁 𝑟𝑛 1 𝑁 𝑛=1 𝑟𝑛 (cid:205)𝑁 (cid:205)𝑁 𝑛=1(𝑟𝑛 1 𝑁 𝑛=1 𝑟𝑛)2 . (3) (4) = 1 𝑁 𝑁 𝑛=1 𝜋(𝑜𝑛 𝒙, 𝒂) 𝜋old(𝑜𝑛 𝒙, 𝒂) 𝑣𝑛 , where 𝜋old( , ) denotes the old policy, and 𝑣𝑛 is the advantage associated with the prediction 𝑜𝑛. The advantage serves as weight to encourage the model to increase the likelihood of high reward predictions while suppressing low-reward ones."
        },
        {
            "title": "4 Experiment",
            "content": "Implementation Detail. We train the grounding models using dataset collections from Aria-UI [1] and OS-Atlas [4], applying data cleaning threshold of 𝜏 = 0.3. Our model is initialized from UI-TARS-1.5-7B [13], Qwen2.5-VL-32BInstruct [25], and Qwen2.5-VL-72B-Instruct [25]. We use learning rate of 106 and sample 𝑁 = 8 responses per"
        },
        {
            "title": "Salesforce AI Research",
            "content": "2025-07-09 Table 2: Comparison with state-of-the-art methods on the ScreenSpot-V2 dataset [4, 5] across mobile, desktop, and web domains. We report grounding accuracy (%) categorized by grounding target type: Text, Icon/Widget, and the overall Average (Avg). We use - to denote unavailability, and to denote the results evaluated by us (which will be updated if improved evaluation scripts become available). The final average scores are highlighted in dark blue. Agent Model Proprietary Models Operator [23] Claude 3.7 Sonnet [11] UI-TARS-1.5 [13] Seed-1.5-VL [24] Open-Source Models SeeClick [24] OmniParser-v2 [22] Qwen2.5-VL-3B [25] UI-TARS-2B [13] OS-Atlas-Base-7B [4] OS-Atlas-Base-7B [4] Jedi-3B [6] Qwen2.5-VL-7B [25] UI-TARS-1.5-7B [13] UI-TARS-72B [13] UI-TARS-7B [13] Jedi-7B [6] Qwen2.5-VL-32B [25] Qwen2.5-VL-72B [25] GTA1-7B GTA1-32B GTA1-72B Mobile Desktop Web Text Icon/Widget Text Icon/Widget Text Icon/Widget 47.3 - - - 78.4 95.5 93.4 95.2 95.2 96.2 96.6 97.6 95.9 94.8 96.9 96.9 98.3 99.0 99.0 98.6 99. 41.5 - - - 50.7 74.6 73.5 79.1 75.8 83.4 81.5 87.2 84.8 86.3 89.1 87.2 86.7 90.1 88.6 89.1 92.4 90.2 - - - 70.1 92.3 88.1 90.7 90.7 89.7 96.9 90.2 94.9 91.2 95.4 95.9 94.3 96.4 94.9 96.4 97. 80.3 - - - 29.3 60.9 58.6 68.6 63.6 69.3 78.6 74.2 80.7 87.9 85.0 87.9 83.6 87.1 89.3 86.4 89.3 92.8 - - - 55.2 88.0 88.0 87.2 90.6 94.0 88.5 93.2 90.6 91.5 93.6 94.4 93.6 96.6 92.3 95.7 95. 84.3 - - - 32.5 59.6 71.4 78.3 77.3 79.8 83.7 81.3 86.2 87.7 85.2 84.2 89.7 90.6 86.7 88.7 91.6 Avg 70.5 87.6 94.2 95.2 55.1 80.7 80.9 84.7 85.1 87.1 88.6 88.8 89.7 90.3 91.6 91.7 91.9 94. 92.4 93.2 94.8 input during training. The batch sizes are set to 256, 128, and 128 for the 7B, 32B, and 72B models, respectively. The 7B and 32B models usually converge around 250 iterations, while the 72B model converges around 50 iterations. When testing on the real-world dynamic environment, we use the action space from [8] and o3 as the planner [12]. Dataset. We evaluate our method on two sets of benchmarks: i) GUI Grounding: we use ScreenSpot-V2 [4, 5], ScreenSpotPro [3], and OSWorld-G [14] datasets, evaluating using the metric of accuracy; ii) Agent Task Execution: we use OSWorld [14] benchmark, measuring performance by task success rate. Baseline. We compare with various state-of-the-art methods: CogAgent [27], OminiParser [22], Qwen2.5-VL [25], Aria-UI [1], OS-Atlas [4], UGround [2], ShowUI [26], Aguvis [9], Jedi [6], GUI-G1 [20], SE-GUI [29], GUI-R1 [17], UI-R1 [18], InfiGUI-R1 [19], UI-TARS [13], Seed-1.5-VL [24], UI-TARS-1.5 [13], GPT-4o [21], o3 [12], Claude 3.7 Sonnet [11], and Gemini-2.5 [10]."
        },
        {
            "title": "4.1 Grounding Performance",
            "content": "We compare our method with state-of-the-art approaches on the ScreenSpot-Pro [3], ScreenSpot-V2 [4, 15], and OSWorld-G [14] datasets, as shown in Tab. 1, Tab. 2, and Tab. 3, respectively. Among the three benchmarks, the ScreenSpot-Pro benchmark is the most challenging, designed for high-resolution, complex, and professional GUI grounding scenarios. It spans 23 applications across 5 industries and 3 operating systems. The ScreenSpotV2 benchmark evaluates grounding capability across mobile, desktop, and web domains, while the OSWorldG benchmark focuses on the Linux environment, providing comprehensive benchmark for measuring diverse capabilities such as text matching, element recognition, layout understanding, and precise manipulation. Our method consistently demonstrates the best performance among open-source models, while remaining competitive with the leading proprietary models. On the ScreenSpot-Pro [3] benchmark, our 7B model even outperforms significantly larger alternatives, for example, achieving 50.1% scores compared to 34.5% scores from UGround-72B. On the ScreenSpot-V2 benchmark, our best-performing model, GTA1-72B, achieves performance comparable to the proprietary Seed-1.5-VL [24], with only 0.3% grounding accuracy difference. Similarly, on the OSWorld-G [6] benchmark, our method surpasses all previous state-of-the-art approaches, setting new benchmark with grounding accuracy of 66.7%."
        },
        {
            "title": "Salesforce AI Research",
            "content": "2025-07-09 Table 3: Performance comparison of state-of-the-art models on the OSWorld-G [6] dataset. We report grounding accuracy (%) categorized by different capabilities, along with the overall average (Avg). We use to denote the results evaluated by us (which will be updated if improved evaluation scripts become available). The final average scores on the benchmark are highlighted in dark blue. Agent Model Proprietary Models Operator [23] Gemini-2.5-Pro [30] Seed1.5-VL [24] Open-Source Models Qwen2.5-VL-3B [25] OS-Atlas-7B [4] Qwen2.5-VL-7B [25] UGround-7B [2] Aguvis-7B [9] UI-TARS-7B [13] Qwen2.5-VL-32B [25] Jedi-3B [6] Jedi-7B [6] UI-TARS-72B [13] Qwen2.5-VL-72B [25] UI-TARS-1.5-7B [13] GTA1-7B GTA1-32B GTA1-72B Text Matching Element Recognition Layout Understanding Fine-grained Manipulation Refusal Avg 51.3 59.8 73.9 41.4 44.1 45.6 51.3 55.9 60.2 57.9 67.4 65.9 69.4 52.6 52.6 63.2 52.6 57.9 42.4 45.5 66.7 28.8 29.4 32.7 40.3 41.2 51.8 70.2 53.0 55.5 60.6 74.6 75.4 82.1 73.1 76. 46.6 49.0 69.6 34.8 35.2 41.9 43.5 43.9 54.9 73.8 53.8 57.7 62.9 74.7 72.4 74.2 72.0 77.3 31.5 33.6 47.0 13.4 16.8 18.1 24.8 28.2 35.6 49.2 44.3 46.9 45.6 55.3 66.7 70.5 59.9 66. 0.0 38.9 18.5 0.0 7.4 0.0 0.0 0.0 0.0 0.0 7.4 7.4 0.0 0.0 0.0 0.0 0.0 0.0 40.6 45.2 62.9 27.3 27.7 31.4 36.4 38.7 47.5 59.6 50.9 54.1 57.1 62.2 64.2 67.7 61.9 66. Table 4: Comparison with state-of-the-art methods on the OSWorld [14] benchmark. We show the number of steps used by different agent in the second column. We report success rate (%) as the evaluation metric. Agent Model Proprietary Models Claude 3.7 Sonnet [11] OpenAI CUA 4o [12] UI-TARS-1.5 [13] OpenAI CUA o3 [12] Open-Source Models Aria-UI w/ GPT-4o [1] Aguvis-72B w/ GPT-4o [9] UI-TARS-72B-SFT [13] Agent w/ Claude-3.5-Sonnet [7] Agent w/ GPT-4o [7] UI-TARS-72B-DPO [13] UI-TARS-72B-DPO [13] UI-TARS-1.5-7B [13] Jedi-7B w/ GPT-4o [6] Agent S2 w/ Claude-3.7-Sonnet [8] Agent S2 w/ Gemini-2.5-Pro [8] GTA1-7B w/ o3 GTA1-32B w/ o3 GTA1-72B w/ o3 Step Success Rate 100 200 100 200 15 15 50 15 15 15 50 100 100 50 50 100 100 100 28.0 38.1 42.5 42.9 15.2 17.0 18.8 20.5 20.6 22.7 24.6 26.9 27.0 34.5 41. 45.2 42.1 43.4 Overall, our method is trained using simple strategy that determines whether click correctly lands on the target element. Although this is straightforward approach, it achieves state-of-the-art performance among open-source models, highlighting its robustness and effectiveness. This demonstrates the potential of our method as strong foundation for grounding models in agents that interact with complex GUI environments."
        },
        {
            "title": "Salesforce AI Research",
            "content": "2025-07-09 Table 5: Ablation of optimization rewards. We consider three reward types: click reward (i. e., whether the prediction selects the correct target element), IoU reward (i. e., enforcing predictions of the bounding box of the target element), and format reward (i. e., enforcing thinking) Click Reward IoU Reward Format Reward ScreenSpot-Pro ScreenSpot-V2 OSWorld-G 44.5 42.2 46.9 50.1 89.3 89.2 93.2 92.4 59.9 59.2 67.0 67. Figure 4: Scalability of 𝐾. Subplots (a) shows the scalability of 𝐾 using 15 steps on the OSWorld benchmark [14], while (b) use 50 steps, and (c) use 100 steps. We vary 𝐾 over for test-time scaling, and measure 1, 8, 16, 32 { performance using task success rate (%). When 𝐾 = 1, the performance degrades to the setting where the test-time scaling strategy is not used. We color the performance of 𝐾 = 1 over steps on the OSWorld benchmark with red dotted lines. 15, 50, 100 } { }"
        },
        {
            "title": "4.2 Agent Performance",
            "content": "We compare our method with state-of-the-art approaches on the OSWorld [14] benchmark in Tab. 4. It consists of 369 tasks distributed across real-world web and desktop applications, providing diverse and challenging testbed for assessing agent capabilities to complete user tasks in Linux environments. We assess various scales of our grounding model using o3 as the planner and the judge for our test-time scaling strategy, forming the GTA1 agent series. Among the three model scales, GTA1-7B achieves the highest task success rate of 45.2% on the OSWorld benchmark, outperforming all state-of-the-art methods. It is worth highlighting that, even when using the o3 planner, GTA1-7B significantly outperforms its native agent variant, CUA o3, while operating with shorter execution horizon (i. e., 45.2% from our method with 100-step horizon vs. 42.9% from CUA o3 with 200-step horizon [23]). The strong performance of the GTA1 agent demonstrates its effectiveness in handling complex and real-world user tasks across diverse scenarios. This highlights the robustness and generalization capability of our approach. In the following subsections, we present more detailed analysis of the GTA1 agent to examine it."
        },
        {
            "title": "4.3 Discussion and Ablation",
            "content": "Click Reward Works the Best. We explore the impact of different optimization objectives for GTA1, focusing on widely studied rewards such as the format reward (i. e., enforcing thinking) and IoU rewards (i. e., enforcing prediction of bounding box of the target element) in Tab. 5 (a). Specifically, we evaluate three settings: i) optimizing the model using the format reward, IoU reward, and click reward (Eq. (2)); ii) using the IoU reward and click reward; ii) applying the format reward and click reward. The three settings achieve performance of 44.5%/42.2%/46.9%, 89.3%/89.2%/93.2%, and 59.9%/59.2%/67.0% on the ScreenSpot-Pro [3], ScreenSpot-V2 [4, 5], and OSWorld-G [14] benchmarks, respectively. However, all of these combinations generally underperform compared to using the click reward alone (with the exception of the ScreenSpot-V2 benchmark), which yields the accuracies of 50.1%, 92.4%, and 67.7% on the respective benchmarks. thinking Benefits Grounding in Dynamic Environment Only. Across various benchmarks, we observe minimal performance differences between grounding models trained with and without thinking. However, they often succeed on different samples, likely due to training instability rather than systematic reasoning gains. We find that thinking can be effective in dynamic environments such as the AndroidWorld benchmark [31], where the model is provided"
        },
        {
            "title": "Salesforce AI Research",
            "content": "2025-07-09 with the task object, past trajectories, and the user instruction. For example, by training an in-domain 7B model based on [25] using the AndroidControl dataset [32], using or not using thinking have similar grounding performance on the AndroidControl test fold. However, the task success rate on the AndroidWorld benchmark increased from 39% to 44% when using thinking. This improvement is attributed to the increased complexity of the textual prompts (i. e., combination of task object, past trajectories, and the user instruction), which encourages the model to engage in thinking when operating under challenging and dynamic conditions. Test-time Scaling Generalizes Well. We demonstrate the generalization capability of our test-time scaling strategies through two sets of experiments. First, when using 𝐾 = 8, increasing the horizon from 50 to 100 steps on the OSWorld benchmark [14] improves the success rate from 43.4% to 45.2%, indicating robustness across varying execution In comparison, the baseline performance with 𝐾 = 1 is 41.3% and 43.4% for the 50-step and 100-step lengths. horizons, respectively. Second, we show that our test-time scaling strategies generalize to other agents, as evidenced by the scalability of 𝐾 on UI-TARS-1.5-7B [13], shown in Fig. 4. We assess performance with 15-, 50-, and 100-step horizons in Fig. 4 (a), Fig. 4 (b), and Fig. 4 (c) respectively, by varying 𝐾 over . The red dotted lines mark the baseline performance of UI-TARS-1.5-7B without any test-time scaling (i. e., 𝐾 = 1) at each horizon on the OSWorld benchmark. Our test-time scaling strategy consistently boosts performance, yielding two main insights: i) with our test-time scaling, UI-TARS-1.5-7B executed for only 15 steps and 𝐾 = 32 already outperforms the baseline that executes for 100 steps without scaling. Since the 𝐾 candidate action proposals are sampled concurrently, this also cuts wall-clock time substantially; ii) the greatest overall gain occurs with 50-step horizon. Using 15-step horizon is occasionally insufficient to complete certain tasks, whereas 100-step horizon provide excessive slack, diluting the benefit of additional steps. We present qualitative comparisons of UI-TARS-1.5-7B with and without test-time scaling strategies in Fig. 5 and Fig. 6. As shown, without the test-time strategy, errors in early grounding or planning stages can propagate and derail the entire task execution, making the agent highly susceptible to cascading failures. 1, 8, 16, 32 { }"
        },
        {
            "title": "5 Conclusion",
            "content": "This paper investigates two key challenges toward building intelligent GUI agents: disambiguation in task planning and precise visual grounding in complex interfaces. We address these challenges with two strategies. First, to improve task planning, we introduce scalable test-time strategy that concurrent samples multiple action proposals at each step and use multimodal large language model judge to select the most suitable one. Second, we propose grounding model, which employs simple RL-based optimization approach that directly rewards successful clicks on target elements, bypassing the explicit thinking required by prior methods. Overall, GTA1 achieves state-of-the-art performance on standard visual grounding benchmarks and demonstrates robust behavior when integrated with planner and our test-time scaling strategy as part of complete agent. We hope this work inspires further research on GUI agents."
        },
        {
            "title": "References",
            "content": "[1] Y. Yang, Y. Wang, D. Li, Z. Luo, B. Chen, C. Huang, and J. Li, Aria-ui: Visual grounding for gui instructions, arXiv preprint arXiv:2412.16256, 2024. [2] B. Gou, R. Wang, B. Zheng, Y. Xie, C. Chang, Y. Shu, H. Sun, and Y. Su, Navigating the digital world as humans do: Universal visual grounding for gui agents, arXiv preprint arXiv:2410.05243, 2024. [3] K. Li, Z. Meng, H. Lin, Z. Luo, Y. Tian, J. Ma, Z. Huang, and T.-S. Chua, Screenspot-pro: Gui grounding for professional high-resolution computer use, arXiv preprint arXiv:2504.07981, 2025. [4] Z. Wu, Z. Wu, F. Xu, Y. Wang, Q. Sun, C. Jia, K. Cheng, Z. Ding, L. Chen, P. P. Liang, et al., Os-atlas: foundation action model for generalist gui agents, arXiv preprint arXiv:2410.23218, 2024. [5] K. Cheng, Q. Sun, Y. Chu, F. Xu, Y. Li, J. Zhang, and Z. Wu, Seeclick: Harnessing gui grounding for advanced visual gui agents, arXiv preprint arXiv:2401.10935, 2024. [6] T. Xie, J. Deng, X. Li, J. Yang, H. Wu, J. Chen, W. Hu, X. Wang, Y. Xu, Z. Wang, et al., Scaling computer-use grounding via user interface decomposition and synthesis, arXiv preprint arXiv:2505.13227, 2025. [7] S. Agashe, J. Han, S. Gan, J. Yang, A. Li, and X. E. Wang, Agent s: An open agentic framework that uses computers like human, arXiv preprint arXiv:2410.08164, 2024. [8] S. Agashe, K. Wong, V. Tu, J. Yang, A. Li, and X. E. Wang, Agent s2: compositional generalist-specialist framework for computer use agents, arXiv preprint arXiv:2504.00906, 2025."
        },
        {
            "title": "Salesforce AI Research",
            "content": "2025-07-09 [9] Y. Xu, Z. Wang, J. Wang, D. Lu, T. Xie, A. Saha, D. Sahoo, T. Yu, and C. Xiong, Aguvis: Unified pure vision agents for autonomous gui interaction, arXiv preprint arXiv:2412.04454, 2024. [10] Deepmind, Gemini 2.5: Our most intelligent ai model, technical report, Deepmind, 2025. [11] Anthropic, Claude 3.7 sonnet and claude code, technical report, Anthropic, 2025. System Card. [12] OpenAI, Openai o3 and o4-mini system card, technical report, OpenAI, 2025. System Card. [13] Y. Qin, Y. Ye, J. Fang, H. Wang, S. Liang, S. Tian, J. Zhang, J. Li, Y. Li, S. Huang, et al., Ui-tars: Pioneering automated gui interaction with native agents, arXiv preprint arXiv:2501.12326, 2025. [14] T. Xie, D. Zhang, J. Chen, X. Li, S. Zhao, R. Cao, T. J. Hua, Z. Cheng, D. Shin, F. Lei, et al., Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments, Advances in Neural Information Processing Systems, vol. 37, pp. 5204052094, 2024. [15] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, et al., Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, arXiv preprint arXiv:2501.12948, 2025. [16] Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. Li, Y. Wu, et al., Deepseekmath: Pushing the limits of mathematical reasoning in open language models, arXiv preprint arXiv:2402.03300, 2024. [17] R. Luo, L. Wang, W. He, and X. Xia, Gui-r1: generalist r1-style vision-language action model for gui agents, arXiv preprint arXiv:2504.10458, 2025. [18] Z. Lu, Y. Chai, Y. Guo, X. Yin, L. Liu, H. Wang, H. Xiao, S. Ren, G. Xiong, and H. Li, Ui-r1: Enhancing action prediction of gui agents by reinforcement learning, arXiv preprint arXiv:2503.21620, 2025. [19] Y. Liu, P. Li, C. Xie, X. Hu, X. Han, S. Zhang, H. Yang, and F. Wu, Infigui-r1: Advancing multimodal gui agents from reactive actors to deliberative reasoners, arXiv preprint arXiv:2504.14239, 2025. [20] Y. Zhou, S. Dai, S. Wang, K. Zhou, Q. Jia, et al., Gui-g1: Understanding r1-zero-like training for visual grounding in gui agents, arXiv preprint arXiv:2505.15810, 2025. [21] A. Hurst, A. Lerer, A. P. Goucher, A. Perelman, A. Ramesh, A. Clark, A. Ostrow, A. Welihinda, A. Hayes, A. Radford, et al., Gpt-4o system card, arXiv preprint arXiv:2410.21276, 2024. [22] Y. Lu, J. Yang, Y. Shen, and A. Awadallah, Omniparser for pure vision based gui agent, arXiv preprint arXiv:2408.00203, 2024. [23] OpenAI, Computer-using agent: Introducing universal interface for ai to interact with the digital world, 2025. [24] D. Guo, F. Wu, F. Zhu, F. Leng, G. Shi, H. Chen, H. Fan, J. Wang, J. Jiang, J. Wang, et al., Seed1. 5-vl technical report, arXiv preprint arXiv:2505.07062, 2025. [25] S. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang, et al., Qwen2. 5-vl technical report, arXiv preprint arXiv:2502.13923, 2025. [26] K. Q. Lin, L. Li, D. Gao, Z. Yang, S. Wu, Z. Bai, S. W. Lei, L. Wang, and M. Z. Shou, Showui: One vision-language-action model for gui visual agent, in Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1949819508, 2025. [27] W. Hong, W. Wang, Q. Lv, J. Xu, W. Yu, J. Ji, Y. Wang, Z. Wang, Y. Dong, M. Ding, et al., Cogagent: visual language model for gui agents, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14281 14290, 2024. [28] C. Qin, A. Zhang, Z. Zhang, J. Chen, M. Yasunaga, and D. Yang, Is chatgpt general-purpose natural language processing task solver?, arXiv preprint arXiv:2302.06476, 2023. [29] X. Yuan, J. Zhang, K. Li, Z. Cai, L. Yao, J. Chen, E. Wang, Q. Hou, J. Chen, P.-T. Jiang, et al., Enhancing visual grounding for gui agents via self-evolutionary reinforcement learning, arXiv preprint arXiv:2505.12370, 2025. [30] Deepmind, Introducing gemini 2.0: our new ai model for the agentic era, technical report, Deepmind, 2025. [31] C. Rawles, S. Clinckemaillie, Y. Chang, J. Waltz, G. Lau, M. Fair, A. Li, W. Bishop, W. Li, F. Campbell-Ajala, et al., Androidworld: dynamic benchmarking environment for autonomous agents, arXiv preprint arXiv:2405.14573, 2024. [32] W. Li, W. E. Bishop, A. Li, C. Rawles, F. Campbell-Ajala, D. Tyamagundlu, and O. Riva, On the effects of data scale on ui control agents, Advances in Neural Information Processing Systems, vol. 37, pp. 9213092154, 2024."
        },
        {
            "title": "Salesforce AI Research",
            "content": "2025-07-09 Figure 5: Example trajectories improved by our test-time scaling strategy. We show key steps of completing the user task using UI-TARS-1.5-7B. (a) Without our test-time scaling strategy, UI-TARS-1.5-7B shifts its action proposal from modifying the search field to scrolling the page to find the ticket. This occurs due to early planning and grounding errors in the From field. (b) With our strategy, it consistently modifies the search information to complete the task."
        },
        {
            "title": "Salesforce AI Research",
            "content": "2025-07-09 Figure 6: Example trajectories improved by our test-time scaling strategy. We show key steps of completing the user task using UI-TARS-1.5-7B. (a) Without our strategy, UI-TARS-1.5-7B attempts to save shortcut by closing Chrome, encountering authentication, and getting stuck. This results from early planning and grounding errors in locating the shortcut panel. (b) With our strategy, it focuses on opening Chrome settings and successfully completes the task."
        }
    ],
    "affiliations": [
        "Salesforce AI Research",
        "The Australian National University",
        "University of Hong Kong"
    ]
}