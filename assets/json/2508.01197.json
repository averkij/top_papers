{
    "paper_title": "A Coarse-to-Fine Approach to Multi-Modality 3D Occupancy Grounding",
    "authors": [
        "Zhan Shi",
        "Song Wang",
        "Junbo Chen",
        "Jianke Zhu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Visual grounding aims to identify objects or regions in a scene based on natural language descriptions, essential for spatially aware perception in autonomous driving. However, existing visual grounding tasks typically depend on bounding boxes that often fail to capture fine-grained details. Not all voxels within a bounding box are occupied, resulting in inaccurate object representations. To address this, we introduce a benchmark for 3D occupancy grounding in challenging outdoor scenes. Built on the nuScenes dataset, it integrates natural language with voxel-level occupancy annotations, offering more precise object perception compared to the traditional grounding task. Moreover, we propose GroundingOcc, an end-to-end model designed for 3D occupancy grounding through multi-modal learning. It combines visual, textual, and point cloud features to predict object location and occupancy information from coarse to fine. Specifically, GroundingOcc comprises a multimodal encoder for feature extraction, an occupancy head for voxel-wise predictions, and a grounding head to refine localization. Additionally, a 2D grounding module and a depth estimation module enhance geometric understanding, thereby boosting model performance. Extensive experiments on the benchmark demonstrate that our method outperforms existing baselines on 3D occupancy grounding. The dataset is available at https://github.com/RONINGOD/GroundingOcc."
        },
        {
            "title": "Start",
            "content": "A Coarse-to-Fine Approach to Multi-Modality 3D Occupancy Grounding Zhan Shi1, Song Wang1, Junbo Chen2 and Jianke Zhu1, Senior Member, IEEE 5 2 0 2 2 ] . [ 1 7 9 1 1 0 . 8 0 5 2 : r Abstract Visual grounding aims at identifying objects or regions in scene based on natural language descriptions, which is essential for spatially aware perception in autonomous driving. However, existing visual grounding tasks typically depend on bounding boxes that often fail to capture fine-grained details. Not all voxels within bounding box are occupied, resulting in inaccurate object representations. To address this, we introduce benchmark for 3D occupancy grounding in challenging outdoor scenes. Built on the nuScenes dataset, it fuses natural language with voxel-level occupancy annotations, offering more precise object perception compared to the traditional grounding task. Moreover, we propose GroundingOcc, an end-to-end model designed for 3D occupancy grounding through multimodal learning. It combines visual, textual, and point cloud features to predict object location and occupancy information from coarse to fine. Specifically, GroundingOcc comprises multimodal encoder for feature extraction, an occupancy head for voxel-wise predictions, and grounding head for refining localization. Additionally, 2D grounding module and depth estimation module enhance geometric understanding, thereby boosting model performance. Extensive experiments on the benchmark demonstrate that our method outperforms existing baselines on 3D occupancy grounding. The dataset is available at https://github.com/RONINGOD/ GroundingOcc. I. INTRODUCTION Holistic 3D scene understanding is key challenge in computer vision, especially for autonomous vehicles [1], [2]. However, current research [3][5] struggles with finegrained spatial reasoning and cognitive scene interpretation. Visual grounding [6] is one of the most promising tasks in this domain, which focuses on localizing objects from natural language descriptions. Traditional bounding boxbased methods [7], [8] often fail to capture full complexity of real-world objects. This is particularly evident for irregularly shaped or partially occluded objects [9]. As illustrated in Fig. 1, bounding boxes are insufficient for accurately representing complex objects, such as excavators, with non-rectangular structures. In contrast, 3D occupancy representations provide more precise way to capture the object shapes and details. Existing datasets have made significant progress in visual grounding, with ScanRefer [10] and Sr3D [11] used for indoor object localization, and datasets such as Mono3DRefer [7] and Talk2Car [12] for autonomous driving. Emerging datasets such as Talk2Radar [13] and Talk2LiDAR [14] 1Zhan Shi is with the College of Software Technology, Zhejiang University. Song Wang and Jianke Zhu are with the College of Computer Science, Zhejiang University, Hangzhou 310027, China. Jianke Zhu is the corresponding author. (email: {zscoisini,songw, jkzhu}@zju.edu.cn). 2Junbo Chen is with the Udeer.ai, Hangzhou 310000, China. (email: junbo@udeer.ai). Fig. 1. Examples of 3D occupancy grounding. Conventional bounding boxes cannot accurately describe irregularly shaped vehicles in daily driving scenes, such as the excavators in (a) and (c). In contrast, 3D occupancy representations provide more precise depiction of complex obstacles, as shown in (b) and (d). contribute but are not publicly available. Moreover, these datasets are limited to bounding box-based understanding and do not incorporate voxel-level occupancy prediction, which is essential for fine-grained spatial understanding. We bridge this gap by integrating occupancy prediction into 3D visual grounding. As shown in Fig. 2, our 3D occupancy grounding task combines natural language descriptions with voxel-level spatial precision, allowing more detailed understanding of complex environments. This task enhances both localization and fine-grained occupancy perception, which are crucial for autonomous decision-making. To advance this research, we introduce Talk2Occ, new benchmark designed for 3D occupancy grounding in autonomous driving scenarios. Talk2Occ combines the Talk2Car [12] dataset with the Occ3D [15] dataset, linking them through the nuScenes dataset [16]. The Talk2Occ dataset includes surround-view images, LiDAR point clouds, 3D occupancy ground truth, and corresponding natural language descriptions. This new task presents unique challenges, including the need for more precise localization and detailed spatial understanding, which motivates the development of advanced models for 3D occupancy grounding. To address the challenges of 3D occupancy grounding, we propose GroundingOcc, an end-to-end model that integrates occupancy prediction with grounding. Our approach combines 2D grounding and depth estimation module, which is supervised by occupancy-rendered depth maps, enabling it to utilize spatial information from multiple modalities. Our main contributions are summarized as below. We introduce occupancy prediction into 3D visual grounding to enable text-guided human-machine interIllustration for 3D occupancy grounding. (a) Our proposed 3D occupancy grounding aims to perceive the 3D occupancy of the referred object in Fig. 2. scene using language descriptions. (b) The traditional 3D visual grounding intends to estimate the location and size of the referred object. (c) Monocular 3D visual grounding relies only on single image to localize the 3D extent of the referred object. (d) The counterpart 2D task does not capture the 3D extent of the referred object. action for fine-grained occupancy perception. novel benchmark for 3D occupancy grounding in autonomous driving, Talk2Occ, bridges natural language with voxel-level spatial understanding. An end-to-end multi-branch network, GroundingOcc, that integrates visual, textual, and point-cloud features for precise voxel-level occupancy grounding. It leverages multi-branch architecture with 2D grounding and depth estimation to improve accuracy. Our framework achieves state-of-the-art performance in 3D occupancy grounding and sets new benchmark for voxel-level perception. Extensive experiments demonstrate that our method significantly improves localization accuracy, outperforming the baseline by 18.13%. II. RELATED WORK 2D Visual Grounding. Localizing the referred objects in 2D images from natural language descriptions is key research area in both computer vision and natural language processing. Early methods employed two-stage pipelines, where region proposals were first generated and then matched with linguistic features [6], [17]. Subsequent advancements introduced tree structures [18], [19] and graph neural networks (GNNs) [20], [21] to better model object relationships. Recently, the trend shifted toward single stage models that directly regressed bounding boxes from fused multi-modal features [22], [23]. Transformer-based methods [24], [25] set new state-of-the-art benchmarks. Moreover, relation-aware grounding techniques, such as expression parsing [26], [27] and graph alignment [21], further enhanced performance. 3D Visual Grounding. Identifying referred objects in 3D scenes using natural language gained momentum with datasets like ScanRefer [10] and SR3D/NR3D [11]. Early methods relied on two-stage pipelines that used pre-trained detectors [28] to extract 3D features. In contrast, more recent models moved towards transformer-based approaches [29], [30], graph-based techniques [8], and unified frameworks tailored for dense 3D tasks [31], [32]. Methods that integrated 2D semantic information [33] and efficient single stage grounding models like 3D-SPS [8] also demonstrated significant improvements. Additionally, research expanded towards outdoor scene grounding [34] and cost-effective monocular methods, such as Mono3DVG [7], which aimed to reduce reliance on LiDAR. Multi-Modality Occupancy Perception. Fusing data from multiple sensor modalities has been proven essential for robust occupancy perception, particularly in autonomous driving. Cameras provided rich semantic details, while LiDAR offered precise sparse geometric information. Many methods aligned 2D image features to 3D space and fused them with LiDAR data [35][37]. Radar integration further improved prediction, as demonstrated by TEOcc [38]. Recent advances incorporated natural language, with models like CLIP [39], POP3D [3], and LangOcc [40] enabling open-vocabulary perception through vision-language alignment. Additionally, OVO [4] aligned voxel predictions with feature maps, though it lacked geometry supervision and was primarily suited for simple scenes. III. THE TALK2OCC BENCHMARK Dataset Construction. The Talk2Occ dataset is created by combining Talk2Car [12] and Occ3D-nuScenes [15], both derived from the nuScenes dataset [16]. This dataset supports 3D occupancy grounding by linking each Talk2Car sample to r d a Objects Object Number 4515 2603 Average Voxel Number 32 r 981 446 y 54 i 11 22 a 22 . C - 254 36 b 95 . - o 83 323 i T 124 772 e m 24 196 R B 60 206 r 166 39 y t 246 56 344 871 TABLE STATISTICS OF REFERENT OBJECT COUNT AND AVERAGE NUMBER OF OCCUPIED VOXELS IN TALK2OCC. matching nuScenes frame. Using sample tokens as unique identifiers, we retrieve 3D bounding box annotations and occupancy labels from Occ3D-nuScenes, while reusing all 11,959 natural language prompts from Talk2Car. To ensure data quality, we apply two filters: bounding box centers lie within the range of [-40m, 40m], [-40m, 40m], and [-1m, 5.4m], and bounding boxes contain occupied voxels. We also perform coordinate transformation to align the data with the ego vehicle coordinate system. This process is fully automated without extra manual labeling. Dataset Statistics. Table shows the distribution of objects and average number of occupied voxels in Talk2Occ, which contains 9,925 objects across 15 categories. Cars make up 45.49% of instances, followed by pedestrians and trucks. Buses occupy the most voxels, with an average of 871, while smaller objects like pedestrians and traffic cones occupy fewer. The diversity of dataset reflects varying spatial complexities, posing challenges for 3D occupancy grounding. For robust evaluation, the dataset is split into 8,949 training and 976 validation samples after data filtering. Metrics. We evaluate 3D occupancy grounding using accuracy at different IoU thresholds, specifically Acc@0.25 and Acc@0.5, as established in prior work [10], [34]. For 3D occupancy grounding, we compute the IoU between the ground truth occupancy set Ogt and the predicted occupancy set Opred. The ground truth set Ogt is defined as the occupied voxels Vocc within the referred objects bounding box B: Ogt = {vi vi Vocc B, ℓ(vi) = free}. (1) In two-stage methods, Opred includes the occupied voxels predicted within the predicted bounding box. In single-stage methods, is directly derived from the models overall predicted scene occupancy. We calculate the IoU as: it IoU = Ogt Opred Ogt Opred . The accuracy at given IoU threshold θ is defined as: Acc@θ = (cid:88) (IoUi > θ) ,"
        },
        {
            "title": "1\nN",
            "content": "i=1 where is the total number of samples. (IoUi > θ) evaluates to 1 if the IoU for the i-th sample exceeds the threshold θ, and 0 otherwise. These metrics provide comprehensive evaluation of the capability of model to localize and predict 3D occupancy accurately. IV. METHODOLOGY A. Problem Definition Multi-modality 3D occupancy grounding is challenging task in autonomous driving. It involves localizing and infer- (2) (3) ring the 3D occupancy of referred object within scene using textual prompt , point cloud data , and multiview images I. The prompt = {w1, . . . , wL} describes the target object, while = {p1, . . . , pN } represents the 3D scene structure. The set of images = {I1, . . . , IK} provides multiple perspectives of the scene. The goal is to predict the occupancy state {0, 1, 2}HW for each voxel. Here, 0 represents free space, 1 indicates voxel occupied by general objects, and 2 denotes voxel occupied by the referred object. The dimensions H, , and correspond to the spatial dimensions of the voxel grid. Performance is evaluated using the mean Intersection over Union (mIoU) for the voxels occupied by the referred object. This approach allows for more detailed voxel-level understanding and improves object perception beyond the limitations of traditional bounding box localization. B. Talk2Occ Baselines There are currently no methods specifically designed for the 3D occupancy grounding task. Previous language-guided multimodal occupancy perception methods often rely on complex pipelines, first extracting semantic information from images with pre-trained models, then projecting it into 3D space for voxel feature learning [3], [4]. In contrast, we propose three efficient and effective two-stage baseline methods, as illustrated in Fig. 3. a) LiDAR-based baseline: In the first stage, as shown in the top-left diagram of Fig. 3, we begin by utilizing parameterized voxelization to embed LiDAR points into voxelized features. To enhance computational efficiency, we apply 3D sparse convolutions. This process generates LiDAR voxel features, denoted as H D, where is the stride, leading to reduction in the spatial dimensions. Next, text encoder is used to extract textual features from the prompts. These textual features then interact with the voxel features in the voxel decoder. The voxel features are subsequently decoded using 3D convolutions, which produce multi-scale voxel representations. The occupancy head reduces the feature channels, resulting in the occupancy prediction OL. softplus function [41] is applied to generate semantic probabilities. Additionally, we follow [42] to learn an occupancy mask OM {0, 1}HW via binary classification head, which indicates whether positions on OL are occupied. Finally, grounding query Qg is used to predict 3D bounding box from the voxel features. In the second stage, the occupancy predictions are classified. The labels marked as free are denoted as Of ree, while the remaining labels are classified as Oocc. Using the 3D bounding box B, we identify the occupied voxels inside Fig. 3. Overall architecture of three proposed baselines. The LiDAR-based baseline (top) employs parameterized voxelization and sparse 3D convolutions to generate LiDAR voxel features, while the camera-based baseline (bottom) utilizes 3D voxel query mechanism to extract features from multi-view images. The multi-modal baseline combines both modalities through an adaptive fusion module to leverage complementary information. the box, which are marked as Ogrounding. This leads to the final occupancy grounding prediction, OOGL RHW Z. b) Camera-based baseline: As depicted at the bottom of Fig. 3, we first employ an image encoder to extract multiview features mv. Inspired by the querying paradigm in recent works [42], [43], we introduce 3D voxel query mechanism(Qvq). This mechanism extracts features from the multi-view features mv and prompt embeddings RLd. This process outputs camera voxel features C, maintaining the same volumetric size as L. Following the LiDAR-based baseline, we apply the voxel decoder, occupancy head, and grounding head to generate the final occupancy grounding results, denoted as OOGC RHW Z. c) Multi-modal baseline: The LiDAR voxel features and camera features are both effective representations for occupancy prediction. Following [35], we employ an adaptive model that dynamically integrates these features. The fusion of these features results in combined voxel features . The final occupancy grounding prediction, OOGF , is generated using the voxel decoder, occupancy head, and grounding head as described earlier. To train these baselines, we utilize combination of loss functions. These include cross-entropy loss Lce, focal loss Lmask [44], and Lovasz-softmax loss Lls [45]. Focal loss is applied for classification, and L1 loss is used for bounding box regression, following [43]. The total loss function is then defined as: Lbaseline = Lce + Lmask + Lls + Lcls + Lreg. (4) C. GroundingOcc point clouds, and text in the same manner as the multi-modal baseline. These multi-modal features are then passed through the Vision-Language PAN module to enhance the interaction between visual and textual representations. To guide feature learning, we incorporate two auxiliary 2D task branches for depth estimation and 2D visual grounding. The fused features are then sent through voxel encoder and voxel fusion module, generating the final voxel features. The output of GroundingOcc includes the 3D occupancy prediction, along with an optional 3D grounding bounding box that can be refined during post-processing. b) Occupancy Encoder: The occupancy encoder takes in voxel queries Qvq, along with the extracted image, text, and point cloud features. It then generates the fused voxel features in the same manner as above. Vision-Language Pan. Inspired by YOLO-World [46], we use the Vision-Language PAN module to create feature pyramids {P3, P4, P5, P6} from multi-scale image features {C3, C4, C5, C6}. For each scale {3, 4, 5, 6}, we aggregate the text embeddings and image features Xl using sigmoid activation function σ(). This is done as follows: = Xl + Xl σ (cid:18) max j{1,...,L} (XlF ) (cid:19) , (5) 2D Grounding Head. At this stage, transformer decoder is used to process the multi-scale image features and the text embedding . The extracted image features X2d are defined as below: X2d = FFN(MHCA(MHSA(X ), )). (6) a) Overview: The baseline methods use two-stage framework, while their complexity in training and deployment presents challenges for practical use. To address these issues, we introduce GroundingOcc, novel single-stage framework designed for 3D occupancy grounding in autonomous driving. As shown in Fig. 4, our approach processes images, where FFN, MHCA, and MHSA refer to the feed-forward network, multi-head cross-attention, and multi-head selfattention layers, respectively. Following [47], convolutional layers predict the 2D grounding outputs, which include the 2D bounding box coordinates(l, r, t, b), center (x2D, y2D), and centerness measure d. Fig. 4. The overview framework of the multi-modal GroundingOcc. GroundingOcc replaces semantic supervision with geometric supervision for more precise occupancy grounding prediction, utilizing voxel encoder and fusion module to generate final occupancy grounding results. The architecture outputs both 3D occupancy grounding predictions and optional 3D bounding boxes that can be refined in post-processing. Here, T, 1, . . . , denote the current image frame and its preceding frames. The integration of 2D spatial locations with multi-scale features and object queries occurs in two stages: In the first stage, the 2D coordinates (H, ) in the feature map are normalized as follows: (x, y){i,j} = (cid:18) + s/2 Wpad , + s/2 Hpad (cid:19) , (7) where is the stride, and (Wpad, Hpad) refer to the padded dimensions of the input image. In the second stage, the object queries are fused with spatial and feature information using transformer-based architecture. This fusion process is defined as below: Qfusion = Fusion(Qg, TopK(F ), PE(p), Qpos). (8) where Qpos and Qg represent the positional and content components of the grounding queries, respectively. The function TopK(F ) selects the top-k sampled multi-scale features, while PE(p) denotes the position encoding for normalized 3D coordinates. Depth Predictor. Depth estimation is crucial for 3D object detection and occupancy perception, significantly enhancing representation learning [48][51]. Traditional methods generate depth ground truth by projecting sparse point clouds onto image coordinates, which often results in inaccuracies due to the sparsity of the LiDAR data. Therefore, we explore to fuse multi-frame point clouds for denser depth ground truth. However, this introduces spatial misalignment, where distant points may project incorrectly onto nearby regions (see Fig. 5(c)). To address this concern, we propose novel depth ground truth generation method based on occupancy results. We generate depth maps by converting 3D occupancy grid, which has the same resolution as in Occ3D [15], into camera-view depth maps through ray-casting. For each pixel (u, v), ray is formed using the Fig. 5. Comparison between different depth ground truth generation methods. Our occupancy-based ray-casting pipeline generates more complete and geometrically accurate depth maps. inverse camera intrinsic matrix: [Xc, Yc, 1]T = 1[u, v, 1]T . (9) Next, we march along the ray at fixed intervals and transform the points to ego coordinates to get Pego: Pego = Tcamera to ego[dXc, dYc, d, 1]T . (10) (cid:106) PegoPmin vsize The ego coordinates are mapped to voxel indices using (cid:107) , where Pmin is the minimum point Vindex = cloud bound, and vsize is the voxel size. The depth for each pixel is then calculated as the minimum distance along the ray to non-free voxel: D(u, v) = min{d O(Vindex) = lfree}. (11) c) Occupancy Grounding Decoder: The occupancy grounding decoder takes fused voxel features and produces occupancy predictions and 3D bounding box estimates"
        },
        {
            "title": "Method",
            "content": "GT-Rand Box-Rand L-baseline C-baseline M-baseline GroundingOcc GroundingOcc-Refine"
        },
        {
            "title": "Type",
            "content": "Two-Stage Two-Stage Two-Stage Two-Stage Two-Stage One-Stage Two-Stage"
        },
        {
            "title": "Overall",
            "content": "Acc@0.25(%) 4.39 5.49 3.29 4.39 7.69 15.38 19.78 Acc@0.5(%) 3.29 5.49 0.00 1.09 2.19 4.39 4.39 Acc@0.25(%) 4.85 5.19 11.86 17.17 22.48 28.58 34.01 Acc@0.5(%) 3.50 4.97 1.58 1.80 2.59 7.68 9.49 Acc@0.25(%) 4.81 5.22 11.06 15.98 21.10 27.35 32.68 Acc@0.5(%) 3.48 5.02 1.43 1.74 2.46 7.47 9. TABLE II QUANTITATIVE RESULTS OF 3D OCCUPANCY GROUNDING ON TALK2OCC. THE TERMS UNIQUE AND MULTIPLE ARE BASED ON WHETHER THE OBJECT CATEGORIES REFERRED TO IN THE SCENE ARE UNIQUE OR MULTIPLE. THE C/L/M-BASELINE METHODS ARE PROPOSED IN IV-B. through two branches. The grounding head refines 3D occupancy grounding by improving spatial localization. Occupancy Head. This module upsamples voxel features H using 3D deconvolution to generate high-resolution occupancy features RHW ZD .A multilayer perceptron (MLP) head then predicts occupancy from the derived features OOG. In addition, binary voxel mask OM is produced to identify the occupied regions. Unlike baseline methods, we use geometry grounding label for supervision. Grounding Head. To predict the 3D bounding box, we first apply average pooling along the height dimension to generate 2D Birds Eye View (BEV) embedding BEV from the voxel features O: BEV = Poolavg(F O). (12) We then use transformer-based decoder, as in [52], to predict the 3D bounding box attributes, including the dimensions (l, h, w), the 3D box center (x3D, y3D, z3D), and the yaw angle ψ along the z-axis. The predicted bounding box refines the results of 3D occupancy grounding, similar to baseline methods. D. Loss Function The total loss Lours for training consists of four components: Lours = L2D + L3D + Ldmap + Locc. (13) The 2D loss L2D includes box size and center, defined as L2D = λ1Llrtb+λ2Lxy2D+λ3Lcenterness+λ4LGIoU. Here, λ14 are hyperparameters that are set to (5, 10, 1, 2). The losses Llrtb and Lxy2D use L1 loss, and LGIoU uses GIOU loss. The 3D loss L3D is the same as baseline methods, for attributes like box size, center, orientation, category, and depth, is defined as L3D = λ5Lcls +λ6Lbbox, where λ56 are set to (2, 0.25). For the depth map, we adopt focal loss to supervise Ldmap. The occupancy loss Locc consists of several terms: Locc = λ7Lce + λ8Lmask + λ9LLovasz + λ10Lgeo scal + λ11Lsem scal, where λ711 are set to (10, 2, 1, 1, 1). Compared to baseline methods, GroundingOcc adds scene-class affinity losses Lgeo scal and Lsem scal from MonoScene [53]. V. EXPERIMENTS A. Implementation Details a) Setting: Our implementation is based on the nuScenes dataset [16], like Occ3D-nuScenes [15]. The point cloud range is [40.0 m, 40.0 m] the and axes, and for [1.0 m, 5.4 m] for the axis. We use voxel grid size of (0.4 m, 0.4 m, 0.4 m) for loss supervision and an initial voxel query resolution of 50 50 16, with embedding dimension = 256. After upsampling, the resolution becomes 200 200 32 with = 64. For the image backbone, we use ResNet101-DCN [54], providing multi-scale features at resolutions of 1/8, 1/16, 1/32, and 1/64. The text encoder is pre-trained RoBERTabase model [55]. Depth maps are estimated from the P3 feature map at 1/8 resolution. The voxel encoder consists of 3 layers with 4 sampling points per voxel query. We fuse data from 4 consecutive frames (0.5s apart) with single-frame LiDAR point clouds to generate fused voxel embeddings. The occupancy head uses two MLP layers with 128 hidden units and softplus activation, while the grounding head has 3 layers with grounding queries of dimension 256. b) Training: We train both the baselines and GroundingOcc model on 4 NVIDIA RTX 4090 GPUs, with batch size of 1 per GPU. The AdamW optimizer is adopted with an initial learning rate of 2 104, cosine annealing schedule, and weight decay of 0.01. Data augmentation techniques such as image cropping, color distortion, and GridMask [56] are applied. Input images are cropped and resized to 320 800 for training due to GPU memory constraints. B. Main Results a) Quantitative Analysis: Since no existing methods are specifically designed for the 3D occupancy grounding task, we compare our method with several baselines to demonstrate its effectiveness. (1) GT-Rand: This method randomly selects ground truth box and generates an occupancy voxel label within the same class range. It serves as baseline for random performance. (2) Box-Rand: Similar to GTRand, it uses ground truth occupancy results. It provides stronger comparison. (3) GroundingOcc: This is the method we propose in Section IV-C. (4) GroundingOcc-Refine: incorporating 3D refined version of GroundingOcc, bounding box for more accurate results. The C/L/M-baseline methods are introduced in Section IV-B. We report the results on the Talk2Occ dataset in Table II. Our method, GroundingOcc-Refine, shows significant improvements across all metrics. It achieves 32.68% Acc@0.25 and 9.01% Acc@0.5, outperforming strong baseline methods. This confirms the superior performance of our approach in the 3D occupancy grounding task. Fig. 6. Qualitative results from baseline method and our GroundingOcc. Red, blue, and purple boxes denote the ground truth occupancy grounding, prediction, and ground truth bounding box, respectively. M-Baseline Multi Frames Depth Predictor 2D Grounding Head Acc@0.25 Acc@0.5 21.10 21.90 24.46 26.94 2.46 3.07 5.40 5.94 TABLE III IMPACT OF THE PROPOSED MODULE IN GROUNDINGOCC FRAMEWORK. Semantic Supervision Geometric Supervision Lgeo scal Lsem scal Acc@0.25 Acc@0.5 26.94 29.30 31.67 32.68 5.94 8.07 8.68 9.01 TABLE IV ABLATION STUDY OF THE OCCUPANCY LOSS FUNCTION. b) Qualitative Analysis: Figure 6 presents the 3D occupancy grounding results for M-baseline and GroundingOccRefine. The baseline method offers rough estimates of object range and occupancy but struggles to pinpoint precise locations due to limited depth sensitivity. GroundingOcc addresses this by using depth predictor, improving distance perception. Our method combines 2D Grounding Head, depth prediction, and multi-frame fusion. This multi-modal approach effectively integrates both appearance and geometric information. However, there are some failure cases. In scenes with ambiguous instructions or multiple similar objects, both the baseline and GroundingOcc methods fail to provide accurate results. These cases highlight the challenges posed by complex environments. C. Ablation Study Impact of Each Module. We evaluate the contribution of each component in the GroundingOcc framework using the Talk2Occ validation set. Performance is measured at Acc@0.25 and Acc@0.5. The results are shown in Table III. Adding multi-frame fusion improves performance slightly. It helps the model capture richer scene information by merging multiple frames. The depth predictor brings more noticeable improvement, enhancing the models ability to estimate distances accurately. The 2D grounding head further boosts accuracy. By refining 3D queries with 2D spatial priors, it helps generate better grounding queries. Impact of Occupancy Losses. We study the role of different loss components in occupancy learning using the GroundingOcc-Refine model. The results are shown in Table IV. Semantic supervision alone provides baseline performance. Adding geometric supervision improves accuracy by capturing structural details. The geometric scaling loss further enhances spatial consistency. Finally, incorporating the semantic scaling loss results in the best performance. Notably, key distinction between GroundingOcc and baseline methods is the use of geometric supervision, which significantly enhances performance by focusing more on geometric quality. VI. CONCLUSION In this paper, we introduce Talk2Occ, new benchmark for multi-modality 3D occupancy grounding in autonomous driving. To tackle this task, we propose GroundingOcc, an end-to-end model that integrates visual, textual, and point cloud features for occupancy grounding. It consists of multimodal encoder for feature extraction, an occupancy head for voxel-wise predictions, and grounding head to refine localization. Additionally, 2D grounding module and depth estimation module enhance geometric understanding. Extensive experiments on Talk2Occ demonstrate that GroundingOcc outperforms the baseline methods, bridging the gap between bounding box-based grounding and voxellevel occupancy grounding. ACKNOWLEDGMENTS This work is supported by National Natural Science Foundation of China under Grant No.62376244."
        },
        {
            "title": "REFERENCES",
            "content": "[1] E. Arnold, O. Y. Al-Jarrah, M. Dianati, S. Fallah, D. Oxtoby, and A. Mouzakitis, survey on 3d object detection methods for autonomous driving applications, IEEE TITS, 2019. [2] Y. Hu, J. Yang, L. Chen, K. Li, C. Sima, X. Zhu, S. Chai, S. Du, T. Lin, W. Wang et al., Planning-oriented autonomous driving, in CVPR, 2023. [3] A. Vobecky, O. Simeoni, D. Hurych, S. Gidaris, A. Bursuc, P. Perez, and J. Sivic, Pop-3d: Open-vocabulary 3d occupancy prediction from images, NeurIPS, 2024. [4] Z. Tan, Z. Dong, C. Zhang, W. Zhang, H. Ji, and H. Li, Ovo: Openvocabulary occupancy, arXiv preprint, 2023. [5] H. Yu, W. Li, S. Wang, J. Chen, and J. Zhu, Inst3d-lmm: Instanceaware 3d scene understanding with multi-modal instruction tuning, in CVPR, 2025. [6] H. Zhang, Y. Niu, and S.-F. Chang, Grounding referring expressions in images by variational context, in cvpr, 2018. [7] Y. Zhan, Y. Yuan, and Z. Xiong, Mono3dvg: 3d visual grounding in monocular images, in AAAI, 2024. [8] J. Luo, J. Fu, X. Kong, C. Gao, H. Ren, H. Shen, H. Xia, and S. Liu, 3d-sps: Single-stage 3d visual grounding via referred point progressive selection, in CVPR, 2022. [9] W. Tong, C. Sima, T. Wang, L. Chen, S. Wu, H. Deng, Y. Gu, L. Lu, P. Luo, D. Lin et al., Scene as occupancy, in ICCV, 2023. [10] D. Z. Chen, A. X. Chang, and M. Nießner, Scanrefer: 3d object localization in rgb-d scans using natural language, in ECCV, 2020. [11] P. Achlioptas, A. Abdelreheem, F. Xia, M. Elhoseiny, and L. Guibas, Referit3d: Neural listeners for fine-grained 3d object identification in real-world scenes, in ECCV, 2020. [12] T. Deruyttere, S. Vandenhende, D. Grujicic, L. Van Gool, and M.- F. Moens, Talk2car: Taking control of your self-driving car, arXiv preprint, 2019. [13] R. Guan, R. Zhang, N. Ouyang, J. Liu, K. L. Man, X. Cai, M. Xu, J. Smith, E. G. Lim, Y. Yue et al., Talk2radar: Bridging natural language with 4d mmwave radar for 3d referring expression comprehension, arXiv preprint, 2024. [14] Y. Liu, B. Sun, G. Zheng, Y. Wang, J. Wang, and F.-Y. Wang, Talk to parallel lidars: human-lidar interaction method based on 3d visual grounding, arXiv preprint, 2024. [15] X. Tian, T. Jiang, L. Yun, Y. Mao, H. Yang, Y. Wang, Y. Wang, and H. Zhao, Occ3d: large-scale 3d occupancy prediction benchmark for autonomous driving, NeurIPS, 2024. [16] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom, nuscenes: multimodal dataset for autonomous driving, in CVPR, 2020. [17] R. Hu, M. Rohrbach, J. Andreas, T. Darrell, and K. Saenko, Modeling relationships in referential expressions with compositional modular networks, in CVPR, 2017. [18] D. Liu, H. Zhang, F. Wu, and Z.-J. Zha, Learning to assemble neural module tree networks for visual grounding, in ICCV, 2019. [19] R. Hong, D. Liu, X. Mo, X. He, and H. Zhang, Learning to compose and reason with language tree structures for visual grounding, IEEE TPAMI, 2019. [20] S. Yang, G. Li, and Y. Yu, Dynamic graph attention for referring expression comprehension, in ICCV, 2019. [21] P. Wang, Q. Wu, J. Cao, C. Shen, L. Gao, and A. v. d. Hengel, Neighbourhood watch: Referring expression comprehension via languageguided graph attention networks, in CVPR, 2019. [22] A. Sadhu, K. Chen, and R. Nevatia, Zero-shot grounding of objects from natural language queries, in ICCV, 2019. [23] Z. Yang, B. Gong, L. Wang, W. Huang, D. Yu, and J. Luo, fast and accurate one-stage approach to visual grounding, in ICCV, 2019. [24] Y. Du, Z. Fu, Q. Liu, and Y. Wang, Visual grounding with transformers, in ICME, 2022. [25] J. Deng, Z. Yang, T. Chen, W. Zhou, and H. Li, Transvg: End-to-end visual grounding with transformers, in ICCV, 2021. [26] S. Chen and B. Li, Multi-modal dynamic graph transformer for visual grounding, in CVPR, 2022. [27] Y. Liu, B. Wan, X. Zhu, and X. He, Learning cross-modal context graph for visual grounding, in AAAI, 2020. [28] C. R. Qi, L. Yi, H. Su, and L. J. Guibas, Pointnet++: Deep hierarchical feature learning on point sets in metric space, NeurIPS, 2017. [29] L. Zhao, D. Cai, L. Sheng, and D. Xu, 3dvg-transformer: Relation modeling for visual grounding on point clouds, in ICCV, 2021. [30] D. He, Y. Zhao, J. Luo, T. Hui, S. Huang, A. Zhang, and S. Liu, Transrefer3d: Entity-and-relation aware transformer for fine-grained 3d visual grounding, in ACM MM, 2021. [31] D. Z. Chen, Q. Wu, M. Nießner, and A. X. Chang, 3 net: unified speaker-listener architecture for 3d dense captioning and visual grounding, in ECCV, 2022. [32] D. Cai, L. Zhao, J. Zhang, L. Sheng, and D. Xu, 3djcg: unified framework for joint dense captioning and visual grounding on 3d point clouds, in CVPR, 2022. [33] Z. Yang, S. Zhang, L. Wang, and J. Luo, Sat: 2d semantics assisted training for 3d visual grounding, in ICCV, 2021. [34] Z. Lin, X. Peng, P. Cong, Y. Hou, X. Zhu, S. Yang, and Y. Ma, Wildrefer: 3d object localization in large-scale dynamic scenes with multi-modal visual data and natural language, arXiv preprint, 2023. [35] X. Wang, Z. Zhu, W. Xu, Y. Zhang, Y. Wei, X. Chi, Y. Ye, D. Du, J. Lu, and X. Wang, Openoccupancy: large scale benchmark for surrounding semantic occupancy perception, in ICCV, 2023. [36] Z. Ming, J. S. Berrio, M. Shan, and S. Worrall, Occfusion: straightforward and effective multi-sensor fusion framework for 3d occupancy prediction, arXiv preprint, 2024. [37] S. Sze and L. Kunze, Real-time 3d semantic occupancy prediction for autonomous vehicles using memory-efficient sparse convolution, arXiv preprint, 2024. [38] Z. Lin, H. Jin, Y. Wang, Y. Wei, and N. Dong, Teocc: Radarcamera multi-modal occupancy prediction via temporal enhancement, in ECAI, 2024. [39] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., Learning transferable visual models from natural language supervision, in ICML, 2021. [40] S. Boeder, F. Gigengack, and B. Risse, Langocc: Self-supervised open vocabulary occupancy estimation via volume rendering, arXiv preprint, 2024. [41] H. Zheng, Z. Yang, W. Liu, J. Liang, and Y. Li, Improving deep neural networks using softplus units, in IJCNN. IEEE, 2015. [42] Y. Wang, Y. Chen, X. Liao, L. Fan, and Z. Zhang, Panoocc: Unified occupancy representation for camera-based 3d panoptic segmentation, in CVPR, 2024. [43] Z. Li, W. Wang, H. Li, E. Xie, C. Sima, T. Lu, Y. Qiao, and J. Dai, Bevformer: Learning birds-eye-view representation from transformers, in ECCV, multi-camera images via spatiotemporal 2022. [44] T.-Y. Ross and G. Dollar, Focal loss for dense object detection, in proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 29802988. [45] M. Berman, A. R. Triki, and M. B. Blaschko, The lovasz-softmax loss: tractable surrogate for the optimization of the intersectionover-union measure in neural networks, in CVPR, 2018. [46] T. Cheng, L. Song, Y. Ge, W. Liu, X. Wang, and Y. Shan, Yolo-world: Real-time open-vocabulary object detection, in CVPR, 2024. [47] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, End-to-end object detection with transformers, in ECCV, 2020. [48] X. Jiang, S. Li, Y. Liu, S. Wang, F. Jia, T. Wang, L. Han, and X. Zhang, Far3d: Expanding the horizon for surround-view 3d object detection, in AAAI, 2024. [49] R. Miao, W. Liu, M. Chen, Z. Gong, W. Xu, C. Hu, and S. Zhou, Occdepth: depth-aware method for 3d semantic scene completion, arXiv preprint, 2023. [50] S. Boeder, F. Gigengack, and B. Risse, Occflownet: Towards selfsupervised occupancy estimation via differentiable rendering and occupancy flow, arXiv preprint, 2024. [51] C. Zhang, J. Yan, Y. Wei, J. Li, L. Liu, Y. Tang, Y. Duan, and J. Lu, Occnerf: Self-supervised multi-camera occupancy prediction with neural radiance fields, arXiv preprint, 2023. [52] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, Deformable detr: Deformable transformers for end-to-end object detection, arXiv preprint, 2020. [53] A.-Q. Cao and R. De Charette, Monoscene: Monocular 3d semantic scene completion, in CVPR, 2022. [54] J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei, Deformable convolutional networks, in ICCV, 2017. [55] Y. Liu, Roberta: robustly optimized bert pretraining approach, arXiv preprint, vol. 364, 2019. [56] P. Chen, S. Liu, H. Zhao, X. Wang, and J. Jia, Gridmask data augmentation, arXiv preprint, 2020."
        }
    ],
    "affiliations": [
        "College of Computer Science, Zhejiang University, Hangzhou 310027, China",
        "College of Software Technology, Zhejiang University",
        "Udeer.ai, Hangzhou 310000, China"
    ]
}