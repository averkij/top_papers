{
    "paper_title": "NitroFusion: High-Fidelity Single-Step Diffusion through Dynamic Adversarial Training",
    "authors": [
        "Dar-Yen Chen",
        "Hmrishav Bandyopadhyay",
        "Kai Zou",
        "Yi-Zhe Song"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce NitroFusion, a fundamentally different approach to single-step diffusion that achieves high-quality generation through a dynamic adversarial framework. While one-step methods offer dramatic speed advantages, they typically suffer from quality degradation compared to their multi-step counterparts. Just as a panel of art critics provides comprehensive feedback by specializing in different aspects like composition, color, and technique, our approach maintains a large pool of specialized discriminator heads that collectively guide the generation process. Each discriminator group develops expertise in specific quality aspects at different noise levels, providing diverse feedback that enables high-fidelity one-step generation. Our framework combines: (i) a dynamic discriminator pool with specialized discriminator groups to improve generation quality, (ii) strategic refresh mechanisms to prevent discriminator overfitting, and (iii) global-local discriminator heads for multi-scale quality assessment, and unconditional/conditional training for balanced generation. Additionally, our framework uniquely supports flexible deployment through bottom-up refinement, allowing users to dynamically choose between 1-4 denoising steps with the same model for direct quality-speed trade-offs. Through comprehensive experiments, we demonstrate that NitroFusion significantly outperforms existing single-step methods across multiple evaluation metrics, particularly excelling in preserving fine details and global consistency."
        },
        {
            "title": "Start",
            "content": "NitroFusion: High-Fidelity Single-Step Diffusion through Dynamic Adversarial Training Dar-Yen Chen1,2 Hmrishav Bandyopadhyay1 Kai Zou2 Yi-Zhe Song1 1SketchX, CVSSP, University of Surrey 2NetMind.AI {dc00872, h.bandyopadhyay, y.song}@surrey.ac.uk kz@netmind.ai https://chendaryen.github.io/NitroFusion.github.io 4 2 0 2 ] . [ 1 0 3 0 2 0 . 2 1 4 2 : r Figure 1. Our one-step diffusion pipeline generates vibrant and photorealistic images with exceptional detail in single inference step, broadening the potential for text-to-image synthesis in applications like real-time interactive systems."
        },
        {
            "title": "Abstract",
            "content": "We introduce NitroFusion, fundamentally different approach to single-step diffusion that achieves high-quality generation through dynamic adversarial framework. While one-step methods offer dramatic speed advantages, they typically suffer from quality degradation compared to their multi-step counterparts. Just as panel of art critics provides comprehensive feedback by specializing in different aspects like composition, color, and technique, our approach maintains large pool of specialized discriminator heads that collectively guide the generation process. Each discriminator group develops expertise in specific quality aspects at different noise levels, providing diverse feedback that enables high-fidelity one-step generation. Our framework combines: (i) dynamic discriminator pool with specialized discriminator groups to improve generation quality, (ii) strategic refresh mechanisms to prevent discriminator overfitting, and (iii) global-local discriminator heads for multi-scale quality assessment, and unconditional/conditional training for balanced generation. Additionally, our framework uniquely supports flexible deployment through bottom-up refinement, allowing users to dynamically choose between 1-4 denoising steps with the same model for direct quality-speed trade-offs. Through comprehensive experiments, we demonstrate that NitroFusion significantly outperforms existing single-step methods across multiple evaluation metrics, particularly excelling in preserving fine details and global consistency. 1. Introduction Recent advances in accelerated diffusion models [14, 15, 21, 27, 29, 49, 51, 58] have demonstrated that high-quality image generation is possible with dramatically reduced step counts. While several approaches now achieve one-step generation [23, 32, 37, 42, 52, 53, 56], they face significant challenges in matching the quality of multi-step methods, particularly in preserving fine details and ensuring global coherence. This quality gap has limited the practical adoption of single-step methods, especially in applications requiring both speed and high fidelity. The core challenge in single-step diffusion lies in compressing an entire denoising trajectory [25, 57] into single transformation. Traditional approaches based on distillation [39, 46] struggle because they attempt to directly match intermediate states or distributions, leading to blurry outputs and loss of detail. Recent adversarial methods [13, 42, 43, 52] show promise but face training instability and diversity collapse when pushed to single-step generation. NitroFusion introduces fundamentally different approach to single-step diffusion through dynamic adversarial framework. Consider how panel of art critics evaluates painting each critic specializes in different aspects like composition, color, technique, and detail. Similarly, rather than relying on single discriminator that can quickly become overconfident [8, 12, 30, 31], we maintain large, dynamic pool of specialized discriminator groups that operate on top of frozen UNet backbone [38]. Just as diverse panel of critics provides more comprehensive feedback than single judge, our ensemble of discriminators guides the generator toward high-quality outputs by providing specialized feedback at different noise levels [23] and spatial scales. Our framework implements this insight through three technical innovations: (i) dynamic discriminator pool architecture where we leverage the teacher models UNet encoder as frozen feature extractor, with multiple lightweight discriminator groups Ht specialized for different noise levels to improve generation quality, (ii) strategic refresh mechanism that randomly re-initializes 1% of discriminator heads while preserving the collective knowledge distribution across the pool to prevent discriminator overfitting common failure mode in GAN training while maintaining stable adversarial feedback, and (iii) multi-scale strategy with dual training objectives where global heads and local heads are compartmentalized in 1:2 ratio, with global heads assessing overall image coherence at resolution HW and local heads examining fine-grained details in patches of size hw. These are further divided as unconditional and prompt-conditional discriminator heads (dual-training) effectively balancing prompt alignment with image coherence. These technical components work together to solve the fundamental challenges of single-step generation. The dynamic discriminator pool and refresh mechanism work in tandem to maintain balanced feedback system throughout training as established heads provide consistent feedback, the periodic introduction of new heads prevents the system from becoming too rigid or predictable. The multi-scale strategy then complements this dynamic feedback system, enabling our generator to achieve what previous approaches could not: transforming noise into high-quality images in single step while avoiding the artifacts and quality degradation that typically plague fast generation methods. Notably, unlike existing approaches [23, 37, 52] that require separate models for different step counts, our framework uniquely supports flexible deployment through bottom-up refinement. While we optimize primarily for single-step generation, our model uniquely enables dynamic refinement users can simply add steps (up to 4) on-demand if higher quality is desired, all with the same model weights. Through extensive experimentation, we demonstrate that NitroFusion consistently produces sharper, more detailed images than existing single-step methods. Our approach not only matches but often exceeds the quality metrics of recent fast diffusion models while maintaining the speed advantages of single-step generation. Human evaluation studies further confirm the superior visual quality of our results, particularly in challenging areas like face detail and texture preservation. Our key contributions include: (i) dynamic discriminator pool with specialized discriminator groups to improve generation quality, (ii) strategic refresh mechanisms to prevent discriminator overfitting, and (iii) multi-scale strategy with dual training objectives to effectively balance prompt alignment and image coherence. Additionally, we uniquely enable flexible deployment by supporting 1-4 denoising steps with the same model weights. 2 2. Related Works 2.1. Timestep Distillation Timestep distillation accelerates inference in diffusion models by reducing the required sampling steps for high-quality output. Standard approaches [14, 15, 27, 29, 32, 49, 51, 56, 58] distil multi-step teacher model into student model with fewer steps. common strategy is to approximate the sampling trajectory, modeled as an ordinary differential equation (ODE), of the teacher model in reduced step count. This can be implemented by either preserving [57] the original ODE path at each timestep, or reformulating [25, 42] and learning more efficient trajectory directly from the final outputs. Recent works train series of such student models that progressively lower sampling steps [28, 39], while enforcing self-consistency [21, 46]. Hyper-SD [37] further combines ODE-preserving and -reformulating methods. However, these models often face quality degradation due to limited model fitting capacity. Different from flow-guided distillation, Distribution Matching Distillation (DMD) [52, 53] minimizes the Kullback-Leibler (KL) divergence between generated and target distributions to directly match distributions on the sample domain. Despite these advancements, achieving high fidelity in one-step distillation remains challenging, as these models frequently struggle with degradation and instability in extreme low-step settings. 2.2. Adversarial Distillation Adversarial Diffusion Distillation [42, 43] (ADD) incorporates GAN training to address the limitations of MSE-based distillation in the few-step generation, which often leads to blurry outputs. Generally, pretrained feature extractor [33] is used as the discriminator backbone to obtain stable, discriminative features [41]. SDXL-Lightning [23] for instance, uses the encoder of pretrained diffusion model as the discriminator backbone, injecting noise prior to the realvs-fake judgment as form of augmentation [23]. Recent works [9, 21, 52] further integrate adversarial loss with distillation objectives to improve image fidelity. However, adversarial loss introduces its own challenges, including training instability and reduced diversity [9]. Rapid discriminator learning can lead to overconfident assessments, limiting constructive feedback for the generator and causing suboptimal training dynamics. Overcoming these challenges is primary goal of our work. 2.3. Multi-Discriminator Training GANs with multiple discriminators have reduced mode collapse and enhanced training stability through the incorporation of diverse adversarial feedback. Various strategies have been developed to balance multiple discriminator objectives, including softmax-weighted ensembles [12] and three-player minimax games [31]. To address overconfidence in discriminators, Neyshabur et al. [30] applies lower-dimensional random projection for each discriminator, while MCL-GAN [8] incorporates multiple choice learning. StyleGAN-XL [40] and StyleGAN-T [41] use multiple discriminator heads alongside frozen, pretrained backbone, enabling feedback across feature pyramids to capture various levels of detail. While these multidiscriminator methods address challenges in GAN training, they remain under-explored in diffusion distillation. Our approach builds upon these insights, introducing robust adversarial framework to provide diverse and dynamic feedback for high-fidelity one-step diffusion distillation. 3. Methodology To perform one-step diffusion, we utilize the concept of timestep distillation. In here, one-step student model is trained to perform at par with pre-trained multi-step teacher. After training, the one-step student can be used independently for super-fast inference. Unlike conventional methods that rely on score matching [53] or flow matching [25] to align student and teacher quality, our approach uses adversarial loss only for critiquing teacher and student predictions - akin to panel of critics that evaluate paintings. This helps us align teacher and student distributions for the student to mimic the teacher in single step without quality degradation. Specifically, we propose Dynamic Adversarial Framework, as: (i) huge pool of discriminator heads with specialized discriminators for different levels of noise and quality, reducing feedback bias from an otherwise single discriminator set-up. (ii) periodic pool refresh to randomly re-initialize sampled set of discriminators to prevent overfitting, and (iii) multi-scale dual-objective GAN training to reduce artifacts and balance image coherence with prompt alignment. Figures 2 and 3 illustrate our training pipeline. Preliminaries: Diffusion Models [19] iteratively refine noise in data sample by reversing forward process that progressively transforms an input sample x0 into noise. In this forward process, each noisy sample xt is obtained from x0 using Gaussian noise ϵ (0, I) at timestep {1, . . . , } as: xt = αtx0 + 1 αtϵ, (1) where αt is variance schedule controlling the noise level [19, 45]. The reverse process, parameterized by neural network Gθ, is trained to predict the noise ϵ from xt to reconstruct x0. Using the predicted noise ˆϵ = Gθ(xt, t), x0 is reconstructed as: xt x0 = 1 αtˆϵ αt . (2) 3.1. One-Step Adversarial Diffusion Distillation Our training pipeline consists of one-step student (generator) Gθ, and pretrained multi-step teacher model Gψ. 3 Figure 2. Our method distils multi-step teacher model into an efficient one-step student generator. The Dynamic Adversarial Framework provides dynamic, stable feedback via large dynamic Discriminator Head Pool, dynamically sampling subset of heads in each iteration to provide unbiased and stable feedback to judge real or fake, effectively balancing one-step efficiency with high-quality generation. We initialize the student with pre-trained one-step weights [37, 52] θ, to reduce the time to converge. During each training iteration, Gθ and Gψ denoise noisy sample xT (0, I) to ˆx0 and x0 respectively. While this denoising takes multiple steps for the teacher Gψ, our student Gθ directly denoises xT to x0 in one step only (see Fig. 2). The discriminator attempts to distinguish x0 as real and ˆx0 as fake, constructing the adversarial loss Ladv. adv = E[D(ˆx0)] LG adv = E[D(ˆx0) D(x0))] LD (3) (4) 3.2. Dynamic Discriminator Pool Building on previous works [52], we utilize the teachers [38] UNet encoder and mid-block as frozen discriminator backbone that extracts image features (see Fig. 3). This generally entails first noising inputs x0 to pre-defined noise levels as xt and then using their denoising signals E(xt , t) as visual features. Different levels of the UNet encoder provide feature representations at different levels, spanning from low-level details to high-level semantics. lightweight trainable discriminator head is attached at each such level of the backbone for the discriminator to perform real/fake classification. As core building block of our pipeline, we use dynamic discriminator pool to source these discriminator heads. This discriminator pool is huge pool of constantly evolving discriminator heads that can be attached to for our pipelines multi-head discriminator. The lightweight design of these heads allows us to scale the pool without significant computational or memory overhead. For training the pool, we sample subset of heads from the pool at every training iteration, computing the adversarial loss Ladv with this subset. We backpropagate gradients from Ladv to optimize the sampled heads D. After the update, we release the heads back into the pool to evolve the global knowledge of the pool dynamically. The stochasticity of this process through random sampling ensures varied feedback, preventing any single head from dominating the generators learning and reducing bias. This diversifies feedback and enhances stability [6, 8] in GAN training. To construct specialized discriminator heads we compartmentalize the pool based on the noise level of the discriminator timestep as {Pt t}. This helps us sample discriminator heads Dt Pt that are specialized for specific noise level at discriminator timestep t. Unlike prior approaches that treat timestep-dependent discriminators as augmentation or smoothing techniques [23, 47], each head in our pool functions as an expert on its designated noise level, providing precise, nuanced critiques targeting specific image characteristics. We calculate the adversarial loss as: adv = E[ΣHDt (E(ˆxt , t))] LG adv = E[ΣHDt (E(ˆxt , t)) (E(xt , t))] LD (5) (6) where the frozen UNet encoder extracts features for sampled discriminator heads Dt . Intermediate outputs from each trainable-head are aggregated for real/fake discriminator predictions. 3.3. Discriminator Pool Refresh Early overfitting in GAN training limits the discriminators feedback diversity, reducing the quality and variation of generated images [9, 23, 42]. To address this, we introduce random re-initialization strategy for our dynamic discriminator pool: at each training iteration, we discard (flush) random subset (1%) of discriminator heads, replacing (refreshing) them with re-initialized discriminators. Refreshing discriminator subsets helps maintain balance between stable feedback from retained heads and variability from reinitialized ones to enhance generator performance. 4 } , local, uncond , {P local, cond Algorithm 1 Dynamic Adversarial Framework 1: Input: Teacher Gψ, Student Gθ, Pool P, timesteps all 2: for each timestep {tall} do Initialize global, uncond 3: 4: end for 5: while not converged do ϵ (0, I) 6: Sample Timestep: {t all} 7: Teacher output: x0 Gψ(ϵ) 8: Student output: ˆx0 Gθ(ϵ) αt x0 + xt ˆxt αt ˆx0 + for compartment type Dt type adv = Dt (E(ˆxt , t)) Dt (E(xt , t)) LD LG adv = Dt (E(ˆxt , t)) Optimize: Gθ α.LG adv Optimize: type 1 αt ϵ 1 αt ϵ in Pt do 9: 10: 11: 12: 13: 14: optim α.LD adv 15: 16: 17: 18: 19: 20: end for {P, Poptim} Prefresh (0, I) {P, Prefresh} 21: 22: end while 23: Return: Trained student model Gθ on one-step or multiple steps (up to 4) to have higher-quality generated images with the same model weights. We support this by using bottom-up refinement approach, where we optimize the network for one step, and iteratively refine for multiple steps one by one. This significantly differs from the more traditional top-down approaches that iteratively refine for 8, 4, 2, and then 1 step in that order. Using bottomup refinement approach allows users to use the same model for multiple steps, and obtain gradually improving results from 1 to 4 steps. 4. Experiments Implementation Details: Each discriminator head comprises 4 4 convolution layers with stride of 2, group normalization [48], and SiLU activation [16, 36]. 10 heads work on 10 feature maps at different feature levels from pretrained diffusion models frozen backbone. We employ specific discriminator timesteps {10, 250, 500, 750} [23]. We use pool of 480 heads, using 160 for each of the task types (global conditional / local conditional / local unconditional). We train using the AdamW [26] optimizer with batch size of 5 and gradient accumulation over 20 steps on single NVIDIA A100 GPU. Each iteration samples discriminator heads for real/fake classification from pool, with 1% reinitialized (during pool refresh) to maintain dynamic feedback. To demonstrate generalization across teacher models, we train two networks with distinct visual 5 Figure 3. Our discriminator employs frozen UNet backbone with dynamic pool of discriminator heads. At each iteration, subset of heads is sampled and trained, with 1% of all heads randomly reinitialized to maintain diverse signals and prevent overfitting. 3.4. Multi-Scale and Dual-Objective GAN Training The generalization potential of diffusion models to multiple resolutions [38] allows us to further use the pre-trained UNet encoder for both global and local (patch) discrimination. For this, we divide the pool into local and global heads, training them with adversarial feedback - to judge either the entire image, or fine-grained details respectively. This setup enables global-focused heads to assess structure and localfocused heads to capture textures, balancing macro and micro image details. We also introduce dual-objective GAN training which applies both conditional and unconditional adversarial loss. We motivate this training following prior analysis [23] that confirms conditional generation to introduce Janus artifacts while struggling to align images with text features. Janus artifacts present repeated patterns, such as faces or hands, within local area. To reduce such artifacts that manifest more in single-step diffusions, we use local discriminator heads to perform conditional and unconditional discrimination. Unconditional local heads provide feedback solely based on image coherence. This dualobjective approach prevents overfitting to specific promptdriven features, reducing the likelihood of artifacts and delivering balanced, generalized adversarial signal. To summarize, we compartmentalize our pool of weights for each timestep t, where further boundaries are created for different training settings: (i) global images with conditional discrimination, (ii) local patches with conditional discrimination and (iii) local patches with unconditional discrimination. Each of these pools has the same number of discriminator heads. 3.5. Bottom-Up Multi-Step Refinement Unlike previous step-reduction algorithms, we offer quality v/s speed trade-off, where users can perform denoising Figure 4. Visual comparison of our models (NitroSD-Realism and NitroSD-Vibrant) against multi-step SDXL [34], our teacher models (4-step DMD2 [52] and 8-step Hyper-SDXL [37]), and selected 1-step state-of-the-art baselines [23, 42]. Figure 5. User preferences study with other baseline models. goals: NitroSD-Realism, optimized for photorealism with the 4-step DMD2 [52] teacher; and NitroSD-Vibrant, for vivid colors with the 8-step Hyper-SDXL [37] teacher. Data: Following the hypothesis [42] that synthetic images offer superior text alignment than real images, we train our models on synthetic samples only, generated by multi-step teacher models - without paired prompt-image data. Prompts are sourced from the Pick-a-Pic [22] and LAION [44] datasets, totaling one million. Baseline Models and Evaluation Metrics: We compare our models to DMD2 [52], Hyper-SDXL [37], the SDXL base model [34], and additional timesteps distillation methods like izSDXL-Turbo [42] and SDXL-Lightning [23]. DMD2 [52] proposes distribution matching distillation using KL-divergence to address limitations in flow-guided distillation. Hyper-SDXL [37] uses human feedback [50, 55] to improve visual appeal of outputs. SDXL-Turbo [42] and SDXL-Lighting [23] introduce adversarial loss and 6 Figure 6. Visual comparison of our models (NitroSD-Realism and NitroSD-Vibrant) with other approaches across multiple steps, highlighting the clarity and improving quality of our method from 1-step to 4-step inference. timestep-dependent discriminator for low-step inference. 4.1. Qualitative Comparison Figure 4 provides qualitative comparison of our models NitroSD-Realism and NitroSD-Vibrant against stateof-the-art diffusion models for one-step inference. Models SDXL-Turbo [42] and SDXL-Lightning [23], show limSDXL-Turbo exhibits occaitations in visual fidelity. sional text misalignment (e.g., 4th row), while SDXLLightning often lacks sharpness in fine details. In contrast, NitroSD-Realism and NitroSD-Vibrant exhibit greater clarity, richer textures, and fewer artifacts than all onestep benchmarks, including teacher models DMD2 [52] and Hyper-SDXL [37]. We also note that our models can pick up visual detail and texture fidelity of multi-step teachers, specifically Hyper-SDXLs 8-step and DMD2s 4-step models. NitroSD-Realism aligns closely with the photorealistic detail of DMD2, reproducing fine-grained realism even in single inference step. NitroSD-Vibrant captures the vibrant, saturated color characteristic of Hyper-SDXLs vivid style. This strong alignment in style and quality highlights the effectiveness of our proposed adversarial framework in distilling distinctive teacher attributes. Finally, we note in comparisons with SDXL [34]s 25-step results that NitroSD achieves competitive detail and texture fidelity, effectively compressing SDXLs extensive process into streamlined, one-step model without sacrificing visual quality. 4.2. User Study We conduct two-choice preference-based user study, illustrated in Figure 5, where participants compare images generated by NitroSD-Realism and NitroSD-Vibrant against other one-step and multi-step methods. Our single-step results indicate that NitroSD-Vibrant consistently outperforms all models, including SDXL with 25 steps, showcasing superior color vibrancy and richness. NitroSD-Realism also demonstrates strong performance, outperforming all one-step approaches. We also evaluate our 2-step results against 4-step outputs from the same competitors observing preference of our 2-step method against even 4-step baselines. This demonstrates NitroSD to achieve superior quality with fewer steps, and highlights the practical advantage of our framework for high-fidelity generation. 4.3. Quantitative Comparison We conduct quantitative evaluation on the COCO-5K validation dataset [24], using several key metrics in Table 1: CLIP score [35] (ViT-B/32 [11]), which assesses prompt alignment by measuring the similarity between generated images and textual descriptions; Frechet Inception Distance (FID) [17], which evaluates image quality and diversity by comparing feature distributions of generated and real images; Aesthetic Score [1], which is trained on user preferences to quantify visual appeal; and ImageReward score [50], which reflects potential user preferences. While FID and CLIP scores for our models are competitive, NitroSD particularly excels in advanced metrics: Aesthetic Score and Image Reward. NitroSD-Realism outperforms its teacher DMD2 [52] both in Aesthetic Score and Image Reward, two metrics capturing image appeal and text alignment based on user preference. NitroSD-Vibrant also 7 achieves one of the highest scores in these two metrics, reflecting its capability to produce visually engaging images that align with user preferences. These advanced metrics highlight NitroSDs strengths in subjective quality, critical factor in text-to-image generation. When paired with our user study findings, these results confirm that NitroSD effectively balances fast inference with high user satisfaction, offering practical solution for applications that demand both efficiency and aesthetic appeal. Model Steps () CLIP () FID () Aesthetic Score () Image Reward() SDXL-Base [34] 25 0.320 23.30 SDXL-Turbo [42] SDXL-Lightning [23] Hyper-SDXL [37] DMD2 [52] NitroSD-Realism NitroSD-Vibrant SDXL-Turbo [42] SDXL-Lightning [23] Hyper-SDXL [37] DMD2 [52] NitroSD-Realism NitroSD-Vibrant 4 4 4 4 4 4 1 1 1 1 1 1 0.317 0.312 0.314 0.316 0.313 0.312 0.318 0.313 0.317 0.320 0.320 0.314 29.07 28.95 34.49 24.57 29.09 39.76 28.99 29.23 36.77 23.91 25.61 38. 5.58 5.51 5.75 5.87 5.54 5.60 5.85 5.38 5.65 6.00 5.47 5.56 5.92 0.782 0.848 0.749 1.091 0.880 0.945 1.034 0.782 0.557 1.169 0.825 0.856 0. Table 1. Quantitative Comparisons with State-of-the-Art Methods. 4.4. Comparison on Multiple-Step Samples We conduct comparisons on multi-step samples, as shown in Figure 6. Notably, models like SDXL-Lightning [23] and DMD2 [52] lack unified model for both one-step and multi-step inference, resulting in layout inconsistencies that limit users ability to refine one-step outputs. HyperSDXL sacrifices one-step performance to achieve unified model. All approaches [23, 37, 42, 52] aside from ours exhibit noticeable artifacts on complex scenes, particularly in areas with intricate textures, such as in lush vegetation or in the space suit of the astronaut in Fig. 6. When inference is extended to 4 steps, SDXL-Turbo demonstrates significant degradation, showing its limitation at higher inference steps. In contrast, our models NitroSD-Realism and NitroSD-Vibrant exhibit high levels of image clarity and steadily improve fidelity from 1-step to 4-step. 4.5. Ablation Study To assess the impact of each component in our Dynamic Adversarial Framework, we conduct an ablation study by removing specific elements, as shown in Figure 7. We note Figure 7. Qualitative study of ablative configurations 8 Figure 8. Results from applying NitroSD-Realism to anime [3] and oil painting [5] base models. Our model effectively adapts to different artistic styles. that (i) The absence of Multi-Scale Dual-Objective GAN Training reduces fine-grained details and introduces prominent triple-eyes Janus artifacts, highlighting the importance of balanced feedback. (ii) Without Pool Refresh, artifacts persist and sharpness is lost, yielding poorer image quality. This suggests overfitting and lack of adaptiveness in the discriminator. (iii) Removing Dynamic Discriminator Pool further reduces sharpness, indicating the pivotal role of the huge discriminator pool in our framework. 4.6. Extending to Diverse Teacher Models Although NitroFusion is trained as full model rather than as LoRA [2, 20], it can adapt to other SDXL [34] checkpoints through weight adjustment. This is achieved by applying the weight difference between NitroFusion and SDXL [34] to new custom model. Figure 8 illustrates results from adapting NitroSD-Realism to custom SDXL models having anime [3] and oil painting [5] styles from the CivitAI [4] community. Without additional training, NitroCustom-ZS (zero-shot) retains each styles distinct characteristics using weight adjustments. NitroFusions independence from natural image data for training further allows easy adaptation to new styles (last column in Fig. 8) 5. Conclusion In this paper, we propose Dynamic Adversarial Framework for one-step diffusion distillation, using huge pool of specialized discriminator heads to judge generation quality on multiple aspects - akin to panel of art critics. We introduce periodic refresh strategy for this pool, wherein part of the pool is re-initialized to prevent discriminator overfitting and adversarial collapse. Finally, we train our entire setup with multi-scale dual-objective strategy to focus on image detail at various scales (local v/s global) and balance prompt alignment with image coherence. Our model outperforms state-of-the-art low-step and one-step baselines in both qualitative and quantitative analysis. We perform extensive user studies and demonstrate that the majority of users prefer our one-step and two-step models, often even over 25-step high resolution diffusion pipelines."
        },
        {
            "title": "References",
            "content": "[1] Clip+mlp aesthetic https : / / github.com/christophschuhmann/improvedaesthetic-predictor, 2022. 7 score predictor. [2] Low-rank adaptation for fast text-to-image diffusion finehttps : / / github . com / cloneofsimo / tuning. lora, 2022. 8 [3] Animagine xl v3.1. https://civitai.com/models/ 260267/animagine-xl-v31, 2024. 8 [4] Civitai. https://civitai.com/, 2024. 8 [5] Painters checkpoint (oil paint / oil painting art style) v1.1. https://civitai.com/models/240154/ painters - checkpoint - oil - paint - oil - painting-art-style, 2024. 8 [6] Isabela Albuquerque, Joao Monteiro, Thang Doan, Breandan Considine, Tiago Falk, and Ioannis Mitliagkas. MultiObjective Training of Generative Adversarial Networks with Multiple Discriminators. In ICML, 2019. 4 [7] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-sigma: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. arXiv preprint arXiv:2403.04692, 2024. 11 [8] Jinyoung Choi and Bohyung Han. MCL-GAN: Generative adversarial networks with multiple specialized discriminators. In NeurIPS, 2022. 2, 3, 4 [9] Trung Dao, Thuan Hoang Nguyen, Thanh Le, Duc Vu, Khoi Nguyen, Cuong Pham, and Anh Tran. Swiftbrush v2: Make your one-step diffusion model better than its teacher. In ECCV, 2024. 3, [10] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In NeurIPS, 2021. 12 [11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. 7 [12] Ishan Durugkar, Ian Gemp, and Sridhar Mahadevan. Generative multi-adversarial networks. In ICLR, 2017. 2, 3 [13] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative Adversarial Nets. In NeurIPS, 2014. 2 [14] Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Lingjie Liu, and Joshua Susskind. Boot: Data-free distillation of denoising diffusion models with bootstrapping. In ICMLW, 2023. 2, 3 [15] Jonathan Heek, Emiel Hoogeboom, and Tim Salimans. Multistep consistency models. arXiv preprint arXiv:2403.06807, 2024. 2, 3 [16] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2023. 5 [17] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by 9 two time-scale update rule converge to local nash equilibrium. In NeurIPS, 2017. 7 [18] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In arXiv preprint arXiv:2207.12598, 2022. 12 [19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. 3 [20] Edward Hu, yelong shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In ICLR, 2022. 8 [21] Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, and Stefano Ermon. Consistency trajectory models: Learning probability flow ode trajectory of diffusion. In ICLR, 2024. 2, [22] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. In NeurIPS, 2023. 6 [23] Shanchuan Lin, Anran Wang, and Xiao Yang. Sdxllightning: Progressive adversarial diffusion distillation. arXiv preprint arXiv:2402.13929, 2024. 2, 3, 4, 5, 6, 7, 8, 11, 12 [24] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014. 7 [25] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In ICLR, 2023. 2, [26] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2019. 5 [27] Eric Luhman and Troy Luhman. Knowledge distillation in iterative generative models for improved sampling speed. arXiv preprint arXiv:2101.02388, 2021. 2, 3 [28] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Latent consistency models: Synthesizing highZhao. resolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023. 3, 12 [29] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. In CVPR, 2023. 2, 3 [30] Behnam Neyshabur, Srinadh Bhojanapalli, and Ayan Chakrabarti. Stabilizing gan training with multiple random projections. arXiv preprint arXiv:1705.07831, 2018. 2, 3 [31] Tu Nguyen, Trung Le, Hung Vu, and Dinh Phung. Dual Discriminator Generative Adversarial Nets. In NeurIPS, 2017. 2, 3 [32] Thuan Hoang Nguyen and Anh Tran. Swiftbrush: One-step text-to-image diffusion model with variational score distillation. In CVPR, 2024. 2, 3 [33] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, ShangWen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, eration with sub-path linear approximation model. arXiv preprint arXiv:2404.13903, 2024. 2, 3 [50] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for textto-image generation. NeurIPS, 2024. 6, [51] Hanshu Yan, Xingchao Liu, Jiachun Pan, Jun Hao Liew, Qiang Liu, and Jiashi Feng. Perflow: Piecewise rectified flow as universal plug-and-play accelerator. arXiv preprint arXiv:2405.07510, 2024. 2, 3 [52] Tianwei Yin, Michael Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and William Freeman. Improved distribution matching distillation for fast image synthesis. In NeurIPS, 2024. 2, 3, 4, 6, 7, 8, 11, 12 [53] Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In CVPR, 2024. 2, 3 [54] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2022. 11 [55] Jiacheng Zhang, Jie Wu, Yuxi Ren, Xin Xia, Huafeng Kuang, Pan Xie, Jiashi Li, Xuefeng Xiao, Weilin Huang, Min Zheng, Lean Fu, and Guanbin Li. Unifl: Improve stable diffusion via unified feedback learning. CoRR, 2024. 6 [56] Yifan Zhang and Bryan Hooi. Hipa: Enabling one-step textto-image diffusion models via high-frequency-promoting adaptation. arXiv preprint arXiv:2311.18158, 2023. 2, 3 [57] Jianbin Zheng, Minghui Hu, Zhongyi Fan, Chaoyue and Tat-Jen Wang, Changxing Ding, Dacheng Tao, Cham. Trajectory consistency distillation. arXiv preprint arXiv:2402.19159, 2024. 2, [58] Mingyuan Zhou, Huangjie Zheng, Zhendong Wang, Mingzhang Yin, and Hai Huang. Score identity distillation: Exponentially fast distillation of pretrained diffusion models for one-step generation. In ICML, 2024. 2, 3 Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 3 [34] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 6, 7, 8, 12 [35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 7 [36] Prajit Ramachandran, Barret Zoph, and Quoc V. Le. arXiv preprint Searching for activation functions. arXiv:1710.05941, 2017. 5 [37] Yuxi Ren, Xin Xia, Yanzuo Lu, Jiacheng Zhang, Jie Wu, Pan Xie, Xing Wang, and Xuefeng Xiao. Hyper-sd: Trajectory segmented consistency model for efficient image synthesis. arXiv preprint arXiv:2404.13686, 2024. 2, 3, 4, 6, 7, 8, 11, [38] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 2, 4, 5 [39] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In ICLR, 2022. 2, 3 [40] Axel Sauer, Katja Schwarz, and Andreas Geiger. Styleganxl: Scaling stylegan to large diverse datasets. In SIGGRAPH, 2022. 3 [41] Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, and Timo Aila. StyleGAN-T: Unlocking the power of GANs for fast large-scale text-to-image synthesis. In ICML, 2023. 3 [42] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. arXiv preprint arXiv:2311.17042, 2023. 2, 3, 4, 6, 7, 8, 12 [43] Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattmann, Patrick Esser, and Robin Rombach. Fast highresolution image synthesis with latent adversarial diffusion distillation. arXiv preprint arXiv:2403.12015, 2024. 2, 3, 11 [44] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. In NeurIPS, 2022. 6 [45] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In ICLR, 2021. 3 [46] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In ICML, 2023. 2, 3 [47] Zhendong Wang, Huangjie Zheng, Pengcheng He, Weizhu Chen, and Mingyuan Zhou. Diffusion-gan: Training gans with diffusion. In ICLR, 2023. [48] Yuxin Wu and Kaiming He. Group normalization. In IJCV, 2018. 5 [49] Chen Xu, Tianhui Song, Weixin Feng, Xubin Li, Tiezheng Ge, Bo Zheng, and Limin Wang. Accelerating image gen10 NitroFusion: High-Fidelity Single-Step Diffusion through Dynamic Adversarial Training"
        },
        {
            "title": "Supplementary Material",
            "content": "Figure 9. 1to 4-step refinement process of our NitroSD-Realism and -Vibrant, illustrating the progressive enhancement of image quality and detail across steps. A. Additional Implementation Details Timestep Shift: Following prior works [7] and our base models, DMD2 [52] and Hyper-SD [37], we adopt the timestep shift technique, shifting the original = 1000 to 500 and 250. NitroSD-Realism and -Vibrant are trained on timesteps {250, 188, 125, 63} and {500, 375, 250, 125}, respectively, for multi-step generation. Both models were trained over approximately 20 NVIDIA A100 days. User Study Details: We evaluate user preferences using 128 prompts from the LADD [43] subset of PartiPrompts [54], gathering 2,884 votes from 170 participants. B. Additional Ablation Study The ablation study in Section 4.5 employs the 8-step HyperSDXL [37] as the teacher, with 30 hours of training. Table 2 presents the quantitative results. Model CLIP () Patch Teacher FID () Aesthetic Score () Image Reward() 0.315 Our Full w/o M-S D-O GAN 0.316 0.316 w/o Pool Refresh 0.316 w/o Dynamic Pool 18.70 18.99 18.78 19. 5.87 5.83 5.98 5.98 1.020 1.035 1.054 1.010 Table 2. Quantitative results of ablation study. In particular, we introduce the Patch Teacher FID metric, which measures the FID score between 299 299 centercropped patches from student and teacher samples [23], assessing how well high-resolution details are preserved. This metric serves as critical index for evaluating the effectiveness of GAN training, as it emphasizes the generators ability to represent fine-grained features and maintain fidelity to the teacher model. Table 2 shows that removing each component causes varying levels of degradation in Patch Teacher FID, highlighting the unique contributions 11 of each to the overall performance of our Dynamic Adversarial framework. C. Discussion and Limitation Classifier-Free Guidance (CFG): Like most few-step distillation methods [28, 37], our framework does not support CFG [10, 18]. While we achieve competitive results in onestep generation, incorporating CFG could enhance alignment with prompts, particularly for complex or ambiguous text. Future work could focus on integrating CFG into the adversarial framework to enhance controllability. Training with Natural Images: Training on natural images offers the potential for improved quality by leveraging diverse, high-resolution data beyond teacher-generated samples. However, poorly aligned image-prompt pairs pose significant risk of text-image misalignment, reducing adversarial training effectiveness. Future research will explore strategies for training with natural images while addressing image-prompt misalignment. Training Efficiency: Our framework highlights the potential of adversarial training in one-step diffusion distillation, an area that remains underexplored. Future directions include optimizing adversarial strategies, such as more efficient adaptive learning schedules, to further boost training efficiency. D. Additional Qualitative Results We provide additional qualitative results in this section. Figure 9 showcases the 1to 4-step refinement process of NitroSD, while Figure 10 presents further comparisons with baseline methods [23, 34, 37, 42, 52]. Additionally, Figure 11 and Figure 12 include more single-step samples generated by NitroSD-Realism and NitroSD-Vibrant, respectively. 12 Figure 10. Additional visual comparison with state-of-the-art approaches. 13 Figure 11. Additional single-step samples from NitroSD-Realism. 14 Figure 12. Additional single-step samples from NitroSD-Vibrant."
        }
    ],
    "affiliations": [
        "NetMind.AI",
        "SketchX, CVSSP, University of Surrey"
    ]
}