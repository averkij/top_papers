{
    "paper_title": "Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for Visual Reasoning",
    "authors": [
        "Yana Wei",
        "Liang Zhao",
        "Jianjian Sun",
        "Kangheng Lin",
        "Jisheng Yin",
        "Jingcheng Hu",
        "Yinmin Zhang",
        "En Yu",
        "Haoran Lv",
        "Zejia Weng",
        "Jia Wang",
        "Chunrui Han",
        "Yuang Peng",
        "Qi Han",
        "Zheng Ge",
        "Xiangyu Zhang",
        "Daxin Jiang",
        "Vishal M. Patel"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The remarkable reasoning capability of large language models (LLMs) stems from cognitive behaviors that emerge through reinforcement with verifiable rewards. This work investigates how to transfer this principle to Multimodal LLMs (MLLMs) to unlock advanced visual reasoning. We introduce a two-stage paradigm built on Qwen2.5-VL-7B: a massive linguistic cold-start fine-tuning, followed by multimodal reinforcement learning (RL) spanning nearly 1,000 steps, surpassing all previous open-source efforts in scale. This pioneering work reveals three fundamental insights: 1) Behavior transfer emerges surprisingly early in cold start due to linguistic mental imagery. 2) Cold start broadly memorizes visual behaviors, while RL critically discerns and scales up effective patterns. 3) Transfer strategically favors high-utility behaviors such as visual reflection. Our resulting model, Open-Vision-Reasoner (OVR), achieves state-of-the-art performance on a suite of reasoning benchmarks, including 95.3% on MATH500, 51.8% on MathVision and 54.6% on MathVerse. We release our model, data, and training dynamics to catalyze the development of more capable, behavior-aligned multimodal reasoners."
        },
        {
            "title": "Start",
            "content": "7-8-2025 Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for Visual Reasoning Yana Wei1, Liang Zhao2, Jianjian Sun2, Kangheng Lin3, Jisheng Yin4, Jingcheng Hu5, Yinmin Zhang2, En Yu6, Haoran Lv2, Zejia Weng2, Jia Wang2, Chunrui Han2, Yuang Peng5, Qi Han2, Zheng Ge2, Xiangyu Zhang2, Daxin Jiang2, Vishal M. Patel1 1Johns Hopkins University 2StepFun 3BUPT 4UCAS 5THU 6HUST"
        },
        {
            "title": "Abstract",
            "content": "The remarkable reasoning capability of large language models (LLMs) stems from cognitive behaviors that emerge through reinforcement with verifiable rewards. This work investigates how to transfer this principle to Multimodal LLMs (MLLMs) to unlock advanced visual reasoning. We introduce two-stage paradigm built on Qwen2.5-VL-7B: massive linguistic cold-start fine-tuning, followed by multimodal reinforcement learning (RL) spanning nearly 1,000 stepssurpassing all previous open-source efforts in scale. This pioneering work reveals three fundamental insights: 1) Behavior transfer emerges surprisingly early in cold start due to linguistic mental imagery. 2) Cold start broadly memorizes visual behaviors, while RL critically discerns and scales up effective patterns. 3) Transfer strategically favors high-utility behaviors such as visual reflection. Our resulting model, Open-Vision-Reasoner (OVR), achieves state-ofthe-art performance on suite of reasoning benchmarks, including 95.3% on MATH500, 51.8% on MathVision and 54.6% on MathVerse. We release our model, data, and training dynamics to catalyze the development of more capable, behavior-aligned multimodal reasoners. 5 2 0 2 7 ] . [ 1 5 5 2 5 0 . 7 0 5 2 : r Figure 1: Performance comparison with state-of-the-art models on both textual (AIME 2024, AIME 2025 [1], MATH500 [2]) and multimodal (MathVista [3], MathVision [4], MathVerse [5]) math reasoning benchmarks. Open Vision Reasoner (OVR) demonstrates superior results among open-source models and performs competitively with commercial counterparts. * Core contribution Corresponding authors: zhaoliang02@stepfun.com, vpatel36@jhu.edu 1. Introduction The eye sees only what the mind is prepared to comprehend. Robertson Davies Shifting Reinforcement Learning from Human Feedback (RLHF) [6] to Reinforcement Learning from Verifiable Reward (RLVR) [7, 8] has endowed LLMs [9, 7] with unexpectedly powerful reasoning across mathematics, code, and general problem-solving. At its core, verifiable rewardwhere correctness is determined by objective, often rule-based criteriais inherently less susceptible to \"reward hacking\" [10, 11] than learned reward model. This robustness proves instrumental in large-scale RL, enabling the internalization and activation of what recent studies [12, 13, 14] term cognitive behaviorspatterns like backtracking and subgoal decomposition that are empirically crucial for advanced reasoning. The multimodal domain, inherently grounded in verifiable visual facts [15, 16], is uniquely suited for this paradigm. Yet, early multimodal RL efforts paradoxically adopted RLHF, relying on learned reward models to approximate objective correctness [17, 18, 19]. Inspired by the success of RLVR in language models, recent efforts have started exploring rule-based rewards in the multimodal setting. Perception-R1 [16] incorporates supervisions such as IoU and Euclidean distance to enhance the perceptual alignment of MLLMs, while works such as R1-OneVision [20] and VLAA-Thinking [21] construct behavior-rich visual reasoning trajectories through complex pipelines including iterative distillation and synthesizing. Recently, ReVisual-R1 [22] adopts effective language-only cold start as foundation for visual reasoning. Despite this encouraging progress, these approaches still leave foundational question unanswered: How can linguistic cognitive behaviors transfer to MLLMs for advanced visual reasoning? To address this, we build upon the \"RL with cold start\" paradigm [7] by conducting large-scale training on Qwen2.5-VL-7B [23], establishing it as powerful testbed to systematically analyze how such behaviors emerge and scale in the multimodal domain. To this end, we introduce robust two-stage methodology designed to first instill linguistic cognitive patterns and then activate them for visual reasoning. Our process begins with large-scale cold start, fine-tuning Qwen2.5-VL-7B on over 2 million examples to build strong foundation. This is followed by prolonged reinforcement learning phase under the OpenReasoner-Zero [24] framework, leveraging over 0.3 million mixed-modality examples. To the best of our knowledge, this represents the largest open-source RL practice on this model. The resulting model, Open-Vision-Reasoner (OVR), validates our approach by achieving strong performance across both language and multimodal benchmarks. As shown in Fig. 1, it achieves 63.5% on AIME2024 and 95.3% on MATH500 for math reasoning, as well as 51.8% on MathVision and 54.6% on MathVerse for visual reasoning. To further trace the transfer and evolution of cognitive patterns throughout training, we develop in-depth visual cognitive behavior analysis. Three central insights are worth highlighting: (1) Behavior transfer emerges remarkably early in cold start, driven by linguistic patterns encoding mental imagery [25, 26] as illustrated in Fig. 11. (2) Cold start broadly memorizes diverse visual cognitive behaviors, while RL critically discerns and scales up effective patterns. (3) Transfer follows strategic path, favoring behaviors with high utility such as visual reflection. These findings deepen the understanding on visual intelligence scaffolded by linguistic reasoning [27]. We further examine how this paradigm impacts foundational capability of MLLMsvisual perception. While linguistic cold start introduces perceptual degradation, our study shows that multimodal RL can effectively recover this loss. However, we also observe the limited scalability of Table 1: Visual Cognitive Behaviors and Linguistic Counterparts. We define four key visual cognitive behaviors, providing formal definitions, illustrative examples, and their corresponding linguistic counterparts. Visual Behavior Example Definition Visual Reflection Let me see the image again. The model explicitly revisits the image after identifying potential mistake or inconsistency in its reasoning, indicating an effort to correct course. Linguistic Counterpart Backtracking Divide-andConquer Visual Verification Lets first look at the numbers on the left. The model breaks down complex visual problem into sub-components or regions, each addressed sequentially to reach the final answer. Subgoal Setting will now verify this against the image. The model confirms that its intermediate conclusions are visually grounded by cross-referencing with the image before proceeding. Verification Goal-driven Visual Tracing To get this answer, need to find an object that... The model starts from desired visual conclusion and reasons backwards to identify relevant image evidence that supports it. Backward Chaining RL when focused solely on perceptual tasks, as reward signals increase without corresponding growth in reasoning complexity (e.g., token length). This limitation motivates more deliberate integration of diverse, primitive visual cognitive behaviors. Such efforts represent promising direction toward unlocking the potential of more advanced RL frameworksmulti-turn or even agentic RL built upon visual manipulation and imagination. In summary, this paper advances the field through the following three key contributions: We construct two-stage training pipeline consisting of linguistic cold start followed by large-scale multimodal RL, enabling effective transfer of cognitive behaviors in MLLMs. Our Open Vision Reasoner, the largest open-source RL practice on Qwen2.5-VL-7B, achieves superior performance on both linguistic and multimodal reasoning benchmarks. We conduct an in-depth analysis of visual cognitive behaviors in OVR and provide valuable insights into their transfer and evolution across training stages. 2. Cognitive Behavior Preliminaries Recent studies have highlighted that the emergence of robust reasoning in LLMs is closely tied to the acquisition of certain cognitive behaviors [13]. These behaviors reflect structured internal reasoning patterns akin to human problem-solving. Key examples include: (1) Backtracking revising previously chosen strategy upon identifying inconsistencies (e.g., This approach wont work because. . . ), (2) Verification checking intermediate steps or partial results, (3) Subgoal Setting breaking down complex problems into manageable components (e.g., First, we need to. . . ), and (4) Backward Chaining reasoning from the desired outcome back to required inputs (e.g., To get 75, we need number divisible by. . . ). The four patterns form kind of textual inner monologue that emerges naturally in language-based reasoning. Figure 2: Multiple Cognitive Behaviors in Single Response. This case shows triggered visualspecific cognitive behaviors like visual divide-and-conquer, reflection, goal-driven visual tracing, along with the linguistic behavior backtracking. Based on this, we investigate the transfer of cognitive behaviors from language to vision. We define the visual extensions of the aforementioned behaviorsvisual reflection, divide-andconquer, visual verification, and goal-driven visual tracing. Their formal definitions, examples, and corresponding linguistic counterparts are provided in Table 1, while Fig. 2 presents multimodal example encompassing both linguistic and visual cognitive behaviors. In the following sections, we present simple yet effective MLLM training pipeline comprising linguistic cold start followed by multimodal RL (Section 3), and systematically analyze the transfer and scaling of these visual cognitive behaviors (Section 5.2). 3. Open-Vision-Reasoner In this section, we introduce Open-Vision-Reasoner (OVR), strong multimodal reasoning model build from Qwen2.5-VL-7B [28], from perspectives of training pipeline (Section 3.1), RL algorithm (Section 3.2) and data construction (Section 3.3). 3.1. Training Pipeline To facilitate efficient cognitive development and cross-modal generalization, we employ the popular \"RL with cold start\" paradigm [7] with two sequential training stages: Stage 1: Linguistic Cold Start. The LLM module is supervised fine-tuned on language-only reasoning datasets distilled from DeepSeek-R1 [7], establishing core cognitive behaviors such as backtracking and subgoal decomposition within purely linguistic setting. Stage 2: Multimodal RL. We apply reinforcement learning with Open-Reasoner-Zero [24] setting on both text and multimodal tasks using verifiable match rewards. This promotes reasoning generalization and aligns previously learned cognitive patterns with visual contexts, enabling effective cross-modal transfer. 3.2. RL Algorithm For the RL stage of our training pipeline, we adopt lightweight Proximal Policy Optimization (PPO) [29] with Generalized Advantage Estimation (GAE) [30], following the policy and reward design used in Open-Reasoner-Zero [24]. We detail the RL for multimodal tasks below: 4 Proximal Policy Optimization For each input, consisting of an image 洧냪 and textual prompt 洧, the policy network 洧랢洧랚 generates 洧녵 responses {洧녶1, . . . , 洧녶洧녵}. Each response 洧녶洧녰 is trajectory 洧녢洧녰 1, 洧녩(洧녰) 洧랦洧녰 = (洧(洧녰) includes 洧 (and potentially encoded 洧냪 features) and previously generated tokens; 洧녩(洧녰) computed at each timestep 洧노 of trajectory 洧랦洧녰. is the token generated at step 洧노. reward 洧 (洧녰) ) of length 洧녢洧녰. The state 洧(洧녰) 0 , . . . , 洧(洧녰) 0 , 洧녩(洧녰) 洧녢洧녰 1 is 洧노 洧노 洧노 We use GAE to balance bias and variance in advantage estimation. The advantage 틙洧냢(洧녰) 洧노 state-action pair (洧(洧녰) 洧노 , 洧녩(洧녰) 洧노 ) in trajectory 洧랦洧녰 is: 틙洧냢洧노 = 洧녢 洧노1 洧녳=0 (洧쮫롚)洧녳洧洧노+洧녳, where 洧洧노 = 洧洧노 + 洧쮫롐괧롚(洧멇롐+1) 洧녤洧랯(洧멇롐). for (1) 洧녤洧랯 is the value function, 洧, 洧랝 are discount and GAE factors, and 洧녤洧랯(洧멇롐 ) = 0 for terminal states. 洧랢洧랚 is updated by maximizing JPPO(洧랚) using experiences (洧멇롐, 洧녩洧노, 틙洧냢洧노) sampled under an older policy 洧랢 old: JPPO(洧랚) = 틙E洧랢 old (cid:2)min (cid:0)洧랣洧노 (洧랚) 틙洧냢洧노, clip (洧랣洧노 (洧랚), 1 洧랬, 1 + 洧랬) 틙洧냢洧노(cid:1)(cid:3) . (2) Here, 洧랣洧노 (洧랚) = 洧랢洧랚 (洧녩洧노 洧멇롐 ) 洧랢 average over samples from 洧랢 using the empirical discounted returns 洧녠洧노 = (cid:205)洧녢 洧노1 old (洧녩洧노 洧멇롐 ) and 洧랬 is clipping parameter (e.g., 0.2). old. 洧녤洧랯 is trained by minimizing Jvalue(洧랯) on samples from 洧랢 틙E洧랢 old denotes the empirical old, 洧녲=0 洧쮫롐떯롐洧노+洧녲: (cid:104)(cid:0)洧녤洧랯(洧멇롐) 洧녠洧노(cid:1) 2(cid:105) Jvalue(洧랯) = 틙E洧랢 old . (3) Reward Function. We adopt the minimalist rule-based reward design, which evaluates only the correctness of model outputs while ignoring formatting or stylistic preferences. Specifically, we extract the predicted answer encapsulated within boxed{} in the models output and compare it against the reference answer. binary reward is assigned1 for exact matches, and 0 otherwiseenabling clear, scalable and unhackable reward signal for reinforcement learning. 3.3. Dataset Construction To support cognitive transfer, we carefully curate datasets specifically tailored to each training stage, encompassing both language-only and multimodal domains. Data Collection. We firstly broadly collect prompt-answer pairs to develop both language and multimodal reasoning skills across mathematical, scientific, and logical domains. For language-only scenarios, we utilize public benchmarks including AIME (up to 2023), MATH [2], Numina-Math [31], Tulu3 MATH [32], and OpenR1-Math-220k [33], and other open-source datasets. We also synthesize general logical problems via programmatic generation to further enrich reasoning diversity. Multimodal scenarios incorporate datasets covering geometry problem solving (Geometry3k [34], GeoQA [35], Geos [36]), visual discrimination (IconQA [37], Pixmo [38], ChartQA [39]), visual puzzles (PuzzleVQA [40], AlgoPuzzleVQA [41]), STEM (TQA [42], ScienceQA [43], K12 from [44]) and multimodal math (AtomThink [45], in-house curated math). Data Curation. To refine data quality, we employ multi-step curation process. Firstly, we employ pre-trained model to automatically filter out samples with high training loss, which typically indicate noise or excessive complexity. Secondly, rule-based and model-assisted methods then identify and remove undesirable patterns [31]. Thirdly, we apply reweighting 5 Table 2: Comparison on Language Reasoning and General Benchmarks. Best results are bold and the second-best are underlined for open-source models. indicates metrics reproduced by ourselves. Model AIME 2024 AIME 2025 MATH500 GPQA Diamond MMLU MMLU-Pro Open-source Models Qwen2.5-7B [28] Qwen2.5-VL-7B [28] Open-Reasoner-Zero-7B [24] DeepSeek-R1-Distill-Qwen-7B [7] QwQ-32B-Preview [46] Skywork-R1V-38B [47] ReVisual-R1 [22] Close-source Models Gemini-2.0-Flash [48] OpenAI-o1-mini [9] Claude 3.7 Sonnet [49] Doubao-1.5-vision-pro-32k [50] OVR-7B 6.7 6.7 17.9 55.5 50.0 72.0 53.3 33.4 63.6 20.0 26.7 63.5 6.7 6.7 15.6 39.2 33.5 - 43. 36.7 - 13.3 20.0 52.1 77.6 67.4 81.4 92.8 90.6 94.0 89.2 69.0 90.0 80.4 85.2 95.3 32.8 31.8 36.6 49.1 54.5 61.6 47. 35.4 60.0 61.1 56.1 49.8 72.6 69.6 - - - - - - 85.2 - - 77.2 57.5 51.7 - - - - - - 80.3 80.0 - 67.9 to balance coverage, down-weighting overrepresented categories while emphasizing rare but valuable instances. To the end, we distill responses from DeepSeek-R1 [7] to construct approximately 2 million cold-start data. To ensure the unhackability and stability during RL, we further exclude problems incompatible with our reward functions (e.g., proof-style questions) and apply difficulty-based heuristic filtering, removing both overly trivial and infeasible samples to ensure well-calibrated learning. This leaves around 300k multimodal RL data. Further details refer to the appendix. 4. Experiments In this section, we first elaborate our implementation of Open-Vision-Reasoner (OVR). Then, we present superior performance across textual benchmarks (Section 4.2) and multimodal scenerios (Section 4.3). 4.1. Implementation Details Our model is based on Qwen2.5-VL-7B [23] and employs two-stage training strategy. In the first stage of cold start, we independently fine-tune the LLM module for 5 epochs with batch size of 640, sequence length of 64k, and learning rate of 2 104 leveraging the default Qwen2.5 configuration [28]. During the subsequent stage of reinforcement learning, following Open-Reasoner-Zero [24], we utilize PPO and configure GAE with 洧 = 1 and 洧랝 = 1 to fully capture long-term dependencies crucial for reasoning tasks, enabling stable training. This RL phase proceeds for 900 iterations, during which we adopt curriculum for the sequence length: it begins at 24k for the first 300 iterations, increases to 32k through iteration 700, and expands to 48k thereafter, with our latest models continuously undergoing this refinement process. We adhere to strict on-policy updates for the policy model and undertake multiple optimization steps for the critic model. Please note that our final model is an uniform average of several representative intermediate checkpoints, ensuring balanced and robust performance across various benchmarks. Additional details can be found in the appendix. 6 Table 3: Evaluation Results on Visual Reasoning Benchmarks. Best results are bold and the second-best are underlined for open-source models. Indicates results reproduced by ourselves. Model SFT Methods LLaVA-OneVision-7B [51] InternLM-XComposer2.5 [52] InternVL3-8B [53] InternVL2.5-8B [54] InternVL2-8B [55] Qwen2-VL-7B [56] Qwen2.5-VL-7B [28] QvQ-72B-Preview [57] Kimi-VL-16B [58] Close-source Models Gemini-2.0-Flash [48] OpenAI-GPT-4o [59] Claude 3.7 Sonnet [49] GPT-4o mini [60] doubao-1.5-vision-pro-32k [50] RL-based Methods VLAA-Thinker-Qwen2-7B [21] VLAA-Thinker-Qwen2.5-7B [21] R1-Onevision-7B [20] OpenVLThinker-7B [61] MM-Eureka-Qwen-7B [44] MMR1-Math-v0 [62] ThinkLite-7B-VL [63] R1-VL-7B [64] X-REASONER [65] VL-Rethinker-7B [66] ReVisual-R1 [22] WeThink [67] Skywork-R1V-38B [47] OVR-7B MathVista MathVision MathVerse DynaMath WeMath LogicVista MMMU-Pro worst strict loose vision-only 62.6 64.0 70.5 64.5 58.3 61.6 69.2 70.3 66.0 70.4 59.9 66.8 55.1 78.6 59.6 68.0 64.1 65.3 72.6 69.8 71.6 63.5 69.0 73.7 73.1 70.9 60.6 72.1 17.6 17.8 28.6 17.0 20.0 19.2 25.5 34.9 21. 47.8 31.1 41.3 27.3 51.5 19.8 26.4 29.9 23.0 28.1 30.7 24.6 24.7 29.6 28.4 48.8 27.2 42.1 51.8 17.6 16.2 33.9 22.8 20.4 25.4 41.1 48.2 34.1 43.6 40.6 52.0 30.0 64.7 33.9 48.2 40.0 38.1 45.4 42.8 42.9 40.0 - 46.4 53.6 44.7 40.4 54.6 9.0 8.2 23.0 9.4 9.2 11.0 21.8 30.7 18. 42.1 34.5 39.7 31.6 44.9 15.2 22.4 - 16.8 23.0 17.4 16.5 - - 17.8 27.5 24.4 - 33.5 - - - - - - 17.7 14.1 37.5 23.5 20.2 22.3 31.2 53.1 39.0 32.3 - - - - - 47.4 42.9 58.2 31.4 48.8 64.2 - 30.5 41.5 - 35.2 21.8 31.9 41.8 - - 36.3 42.0 48.0 34.1 44.6 - - 61.8 - - - - - - - - - - 64.8 32.0 34.7 43.6 36.0 33.6 33.3 47.9 58.2 42.7 52.3 64.4 49.3 41.4 65. 36.0 48.5 - 44.5 46.3 46.8 42.7 - - 42.7 52.3 53.0 50.6 54.8 24.1 - - 34.3 29.0 30.5 - - - - - - 37.6 - - - - - - - - 43.0 41.7 - 50.2 CharXiv reas. desc. 23.6 - 37.6 32.9 - 34.6 36.4 - - 48.7 - 73.6 68.6 - 58.0 67.3 - - - - - - - - 34.10 74.92 - - - - - - - - - - - - - - - - - - - - - 44.5 - 73.6 4.2. Enhanced Language Reasoning and General Capabilities Our model is initially evaluated on variety of language benchmarks, which cover mathematical reasoning and general problem-solving skills. Specifically, we include AIME 2024, AIME 2025 [1], MATH500 [2], GPQA Diamond [68], MMLU [69], and MMLU-Pro [70]. We compare Open-VisionReasoner (OVR) with strong LLM baselines, including Qwen2.5-7B [71], DeepSeek-R1-Distill-Qwen7B [7] and Open-Reasoner-Zero-7B [24]. OVR demonstrates exceptional language reasoning capabilities. On the challenging AIME 2024 and 2025 benchmarks, it dramatically surpasses other 7B open-source models by an average of over 10%, achieving performance comparable to leading 32B models. This superiority extends to general reasoning tasks, with significant gains of +4.6% on MMLU and +10.4% on MMLU-Pro over parameter-matched competitors. These results highlight the effectiveness of our curated, high-quality cold-start training data. 4.3. Superior Visual Reasoning Abilities To evaluate whether the introduced cognitive behavior transfer leads to cross-modal benefits, we further assess the model on suite of multimodal reasoning benchmarks. These tasks involve image-grounded mathematical reasoning, general multimodal reasoning, and chart understanding. Specifically, we include MathVista [3], MathVision [4], MathVerse [5], DynaMath [72], WeMath [73], LogicVista [74], MMMU-Pro [75], and CharXiv [76] for evaluation. We compare our model against strong MLLM baselines, including SFT-based methods such as LLaVA-OneVision [77] and Qwen2.5-VL [23], as well as recent rule-based RL methods like Open7 Figure 3: Training Dynamics. (a) The cold-start stage shows step-wise loss decrease. (b) In the RL stage, reward (purple, left axis) and average response length (orange, right axis) grow steadily, with sharp surges after each sequence length expansion. VLThinker [61], MM-Eureka [44] and ReVisual-R1 [22]. As shown in Table 3, our model sets new breakthrough for 7B models in visual reasoning. It is the first post-trained Qwen2.5-VL-7B-based model to surpass the 50% performance on MathVision, while also achieving state-of-the-art results among 7B models on DynaMath and MathVerse. This strong overall performance is further underscored by substantial gain on MMMU-Pro (+7.2% over prior SOTA methods). These results indicate that reasoning capabilities acquired through language training can effectively transfer to multimodal tasks, leading to notable improvements in visual reasoning. 5. Discussion 5.1. Analysis of Training Dynamics In this section, we present comprehensive overview of the training dynamics as illustrated in Fig. 3, and provide detailed analysis of how text and multi-modal reasoning metrics evolve throughout the process as shown in Fig. 4. During the initial cold-start phase (Fig. 3 (a)), the models loss rapidly descends to below 0.5. Subsequently, across multiple training epochs, the loss exhibits step-wise, gradual decrease. In parallel, we observe corresponding surge in performance across all benchmarks  (Fig. 4)  , which first ascend sharply before transitioning to phase of slower, more incremental improvement toward their peak. noteworthy observation is that the aggressive training strategy detailed in Section 4.1employing large batch size in concert with high learning rateproves to be essential. This approach is critical for breaking the models inherent constraints, thereby successfully imbuing it with new cognitive paradigms and sculpting more favorable landscape for reinforcement learning. It is prerequisite that enables our model, which originates from an instruction-tuned base, to ultimately achieve text performance that is comparable to, or even surpasses models initialized from base [28] or math-specific checkpoints [78]. Furthermore, Fig. 3 (b) reveals how the models reward and average token length in the RL phase steadily advance from an initial 7k to exceed 12k. Owing to the stability of the training configuration inherited from Open-Reasoner-Zero [24], OVR is successfully trained on diverse corpus of over 20 multi-modal and language-only datasets without encountering any training collapse or performance degradation. Critically, whenever the token length begins to plateau or even decline, we strategically switch to longer context length, which invariably catalyzes the Figure 4: Performance Evolution on Reasoning Benchmarks. OVR demonstrates sustained and convergent growth across both linguistic and multi-modal benchmarks throughout the cold start (left) and RL (right) training phases. next wave of rapid reward growth. Fig. 4 captures the coincident yet unsurprising convergent growth trajectory shared by all eight reasoning benchmarks, spanning both text and multi-modal domains, as they progressively ascend towards their zenith amidst fluctuations. 5.2. Multimodal Cognitive Behavior Analysis Recent studies have highlighted the emergence of cognitive behaviors in MLLMs during visual reasoning tasksphenomena often dubbed visual aha moments [79, 21]. In this work, we move beyond plain observations and systematically investigate how these behaviors are transferred from their linguistic counterparts. Our analysis centers on the four pivotal visual cognitive behaviors introduced in Section 2 which are drawn from foundational research on cognitive patterns [13]. To quantify this process, we employ GPT-4o [60] to analyze the emergence of each behavior within the inference traces of our OVR model. Visual behaviors emerge remarkably early from cold start. Following Section 5.1, we tracked the dynamics of visual reflection, significant behavior mentioned in previous studys [80, 81], throughout OVRs training. As depicted in Fig. 5, this vision-specific behavior emerges in significant quantities from the very beginning of the cold-start phase and fluctuates throughout subsequent training steps. Strikingly, we observed that even in linguistic problems, DeepSeekR1s responses [7] frequently exhibited signs of mental imagery [25, 26] as shown in Fig. 11(a). The model appeared to construct internal visualizations to support mathematical reasoning, often articulated through phrases such as let me visualize. . . or let me see the image. Once this linguistic scaffolding was introduced into our MLLM, these mental images were rapidly grounded in actual visual input, enabling their rapid and effective generalization within OVR. Cold-start learns broadly, large-scale RL discerns critically. We further investigate how cognitive behaviors scale during large-scale RL. As shown in Fig. 5(a), after an initial, rapid instillation 9 Figure 5: Multimodal Cognitive Behavior Analysis. (a) Emergence of visual reflection across the cold start and RL training steps. (b) Emergence and transfer rates of four visual cognitive behaviors across base models and training stages. Numerical values denote the language-tovision transfer rates for each behavior. of patterns during the aggressive cold-start phase, their prevalence is first suppressed then amplified to unprecedented levels during multimodal RL. This counter-intuitive dynamic suggests clear division of labor: the cold-start phase learns broadly, indiscriminately memorizing all available patterns. In contrast, RL discerns critically, acting as strategic filter for the crucial tokens [82] and scaling up pivotal behaviors. This process of RLdiscarding the dross to select the essenceis significant for achieving superior generalization. Visual transfer of cognitive behaviors is strategic. To analyze the transition from linguistic to visual cognition, we track the emergence and transfer rates (detailed in Appendix C.1) of four core cognitive behaviors across both language and vision modalities. As shown in Fig. 5(b), the emergence of backtracking and verification steadily increases across training stages, underscoring their growing importance. Among these, the transfer rate of backtracking shows consistent growthfrom 2.5% to 17.3%while verification exhibits near-zero transfer throughout both the cold-start and RL phases. This indicates that transfer is strategic process, for which we posit two potential explanations: (1) Backtracking transfers more readily due to DeepSeek-R1s [7] inherent mental imagination capabilities, while verification, lacking direct linguistic precursor, is more difficult for the MLLM to internalize. (2) Mirroring how humans naturally and instinctively process visual information [80], backtracking is more fundamental component of complex visual reasoning, making its amplification higher priority during the strategic RL phase. We will investigate these hypotheses in greater depth in our future work. 5.3. Beyond Behavior: Visual Perception Analysis and Future Work Beyond behavioral dynamics, we extend our discussion to essential capability of MLLMs: visual perception under the cold start plus large-scale RL paradigm. In particular, we investigate two key areas of interestperceptual hallucination and scaling propertiesthrough dedicated study on our OVR model. Cold start impairs perception, while RL enhances. We evaluated both stages of OVR, along with the base model Qwen2.5-VL-7B, on comprehensive set of multimodal benchmarks targeting visual perception and recognition (MMBench [83], BLINK [84], MMStar [85], HallusionBench [86], POPE [87], RealWorldQA [88], PhyX [89], MME [90], MMVet [91]). As shown in Table 4, performance steadily improves across tasks such as MMBench and PhyX, under10 Table 4: Model Performance on Perception-centric Benchmarks. Model MMBench en cn 85.3 84.6 82.1 86.1 +Multimodal RL 86.6 84.2 Qwen2.5-VL +Cold Start BLINK MMStar HallusionBench POPE RealWorldQA PhyX MME MMVet 53.7 51.5 54.1 62.1 62.4 62.7 49.0 55.0 53.6 86.1 82.5 83.2 69.3 63.1 65. 37.5 47.7 50.0 1659.7 1549.8 1559.1 63.9 61.8 63.6 scoring the effectiveness of our training paradigm. The cold-start model shows declines on several tasks, notably increased hallucinations [92, 93], likely due to token distribution shifts from large-scale linguistic data [87]. However, the regained performance on benchmarks such as MMBench and BLINK demonstrate that long-term multimodal RL can effectively mitigate these issues by discerning perceptual capabilities that are critically for multimodal tasks. Looking ahead, degradation from cold start can be mitigated either by incorporating the linguistic data into model pretraining [94, 95], or by introducing more multimodal supervision during the cold start to establish stronger visual foundation. The current unscalability of RL for perception policy. Throughout the multimodal RL, we observed strong correlation between the reward and the average response length in Fig. 3, which is finding consistent with prior practices [7, 24]. This reinforces response length as an effective reward proxy, indicative of scaling property tied to reasoning depth and computational resources. However, when focusing on specific discriminative perceptual tasks like OCR and counting, we observe clear divergence. As shown in Fig. 6, while the reward can be effectively increased, the average response length remains largely stagnant. This unscalable training dynamic on such challenging tasks hints at more fundamental issue: the absence of certain core visual cognitive behaviors. Addressing this fundamental capability gap is paramount for achieving robust multimodal scaling. Emerging research offers promising avenues, such as multi-turn RL with agentic tool-use (e.g., OpenAI-o3 [96]) and the integration of intrinsic imagining through mental images [97, 98]. These approaches hold the potential to bridge current limitations and unlock more scalable multimodal reasoning. 6. Related Work Figure 6: Training Dynamics on perception tasks including OCR and counting. Recent breakthroughs like OpenAIs o1 [9] have highlighted the power of RL in unlocking and scaling reasoning capabilities [99, 100, 101] within LLMs. DeepSeek-R1-Zero [7] showed that reasoning capabilities can emerge purely through large-scale RL, leading to complex behaviors like self-verification and reflection. Open-source efforts like Open-Reasoner-Zero [24] further demonstrates that even minimalist RL approaches, such as vanilla PPO [29] with GAE [30] and simple rule-based rewards, can drive scaling in response length and benchmark performance on open-source models [28, 78]. MLLMs [102, 56, 23, 103, 104] have rapidly progressed from basic image captioning [105, 106] to more challenging reasoning tasks [107, 108, 109]. Early efforts primarily relied on 11 supervised fine-tuning with Chain-of-Thought (CoT) datasets [110], while some explored explicit reflection [80] and self-correction [111] mechanisms to emulate human-like reasoning patterns. More recently, methods such as PerPO [18] and MDPO [17] adopt RL-based post-training approaches like DPO [112], where alignment is learned from paired positive/negative responses. These approaches generally follow the RL from Human Feedback (RLHF)[6, 12] or RL from AI Feedback (RLAIF) [113] paradigms, where signals from learned reward models or preference labels are utilized for optimization. Inspired by the success of RLVR [7] in language models, MLLM research has shifted toward rule-based RL like GRPO [114] into the multimodal domain. This has led to two major lines of efforts: (1) designing task-specific reward objectives [16, 21, 33], and (2) constructing multimodal thinking datasets that embed cognitive behaviors within CoT sequences [115, 116, 61, 44]. Additionally, recent powerful MLLMs adopt language-only cold start [22, 94], using verbal reasoning sequences as foundation for subsequent multimodal learning. These approaches encourages human-like behaviors [13] or so-called visual aha moments in the model responses. Despite these advances in MLLM, fine-grained understanding of the underlying reasoning mechanisms remains less explored. In contrast, recent study [13] centered on LLMs posit that effective reasoning is causally linked to the models acquisition of certain cognitive behaviors, such as verification, backtracking, subgoal setting, and backward chaining. The test-time studies have observed that invoking these patterns improve performance [117]. Entropy-based analysis further reveals that regions associated with cognitive tokens are critical for diverse and highquality reasoning [82]. The multimodal work like Long-Perceptual-Thoughts [118] attempts to explicitly instill such patterns by synthesizing long-form multimodal CoT data. 7. Conclusion In this paper, we propose two-stage training framework to investigate cognitive behavior in the multimodal domain. By combining linguistic cold start followed by large-scale multimodal RL, our approach enables effective cross-modal transfer and scaling of cognitive patterns. Our model Open Vision Reasoner, the largest open-source RL practice built on Qwen2.5VL-7B, demonstrates strong performance across both linguistic and perceptual benchmarks. Beyond performance, we provide systematic analysis of visual cognitive behaviors, revealing how they emerge and evolve through different training stages. We hope our findings inspire future research on cognitively aligned multimodal agents and open up new possibilities for scaling vision-language reasoning through behavior-centered learning."
        },
        {
            "title": "References",
            "content": "[1] Mislav Balunovic, Jasper Dekoninck, and Martin Vechev Ivo Petrov, Nikola Jovanovic. Matharena: Evaluating llms on uncontaminated math competitions, February 2025. [2] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, 2021. [3] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. [4] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural Information Processing Systems, 37:9509595169, 2024. [5] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, and Hongsheng Li. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems?, 2024. [6] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [7] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [8] Kimi Team. Kimi k1.5: Scaling reinforcement learning with llms. 2025. [9] OpenAI. Learning to reason with llms. https://openai.com/index/learning-t o-reason-with-llms/, 2025. [10] Joar Skalse, Nikolaus H. R. Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and characterizing reward hacking, 2025. [11] En Yu, Kangheng Lin, Liang Zhao, Yana Wei, Zining Zhu, Haoran Wei, Jianjian Sun, Zheng Ge, Xiangyu Zhang, Jingyu Wang, et al. Unhackable temporal rewarding for scalable video mllms. arXiv preprint arXiv:2502.12081, 2025. [12] Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, et al. Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1380713816, 2024. [13] Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and Noah Goodman. Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars. arXiv preprint arXiv:2503.01307, 2025. 13 [14] Rosie Zhao, Alexandru Meterez, Sham Kakade, Cengiz Pehlevan, Samy Jelassi, and Eran Malach. Echo chamber: Rl post-training amplifies behaviors learned in pretraining. arXiv preprint arXiv:2504.07912, 2025. [15] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llms referential dialogue magic. arXiv preprint arXiv:2306.15195, 2023. [16] En Yu, Kangheng Lin, Liang Zhao, Jisheng Yin, Yana Wei, Yuang Peng, Haoran Wei, Jianjian Sun, Chunrui Han, Zheng Ge, et al. Perception-r1: Pioneering perception policy with reinforcement learning. arXiv preprint arXiv:2504.07954, 2025. [17] Fei Wang, Wenxuan Zhou, James Huang, Nan Xu, Sheng Zhang, Hoifung Poon, and Muhao Chen. mdpo: Conditional preference optimization for multimodal large language models. arXiv preprint arXiv:2406.11839, 2024. [18] Zining Zhu, Liang Zhao, Kangheng Lin, Jinze Yang, En Yu, Chenglong Liu, Haoran Wei, Jianjian Sun, Zheng Ge, and Xiangyu Zhang. Perpo: Perceptual preference optimization via discriminative rewarding. arXiv preprint arXiv:2502.04371, 2025. [19] Ke Zhu, Liang Zhao, Zheng Ge, and Xiangyu Zhang. Self-supervised visual preference alignment. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 291300, 2024. [20] Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, et al. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615, 2025. [21] Hardy Chen, Haoqin Tu, Fali Wang, Hui Liu, Xianfeng Tang, Xinya Du, Yuyin Zhou, and Cihang Xie. Sft or rl? an early investigation into training r1-like reasoning large vision-language models. arXiv preprint arXiv:2504.11468, 2025. [22] Shuang Chen, Yue Guo, Zhaochen Su, Yafu Li, Yulun Wu, Jiacheng Chen, Jiayu Chen, Weijie Wang, Xiaoye Qu, and Yu Cheng. Advancing multimodal reasoning: From optimized cold start to staged reinforcement learning, 2025. [23] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [24] Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model. arXiv preprint arXiv:2503.24290, 2025. [25] Philip Johnson-Laird. Mental models in cognitive science. Cognitive science, 4(1):71115, 1980. [26] Marilyn Ford. Mental models: towards cognitive science of language, inference, and consciousness, 1985. [27] Yifan Du, Zikang Liu, Yifan Li, Wayne Xin Zhao, Yuqi Huo, Bingning Wang, Weipeng Chen, Zheng Liu, Zhongyuan Wang, and Ji-Rong Wen. Virgo: preliminary exploration on reproducing o1-like mllm, 2025. 14 [28] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. [29] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [30] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. Highdimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015. [31] Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu. Numinamath. [https://hugg ingface.co/AI-MO/NuminaMath-CoT](https://github.com/project-numina/ aimo-progress-prize/blob/main/report/numina_dataset.pdf), 2024. [32] Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. Tulu 3: Pushing frontiers in open language model post-training, 2025. [33] Loubna Ben Allal, Lewis Tunstall, Anton Lozhkov, Elie Bakouch, Guilherme Penedo, and Gabriel Mart칤n Bl치zquez Hynek Kydlicek. Open r1: Evaluating llms on uncontaminated math competitions, February 2025. [34] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and SongChun Zhu. Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning. arXiv preprint arXiv:2105.04165, 2021. [35] Jiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang, Lingbo Liu, Eric Xing, and Liang Lin. Geoqa: geometric question answering benchmark towards multimodal numerical reasoning. arXiv preprint arXiv:2105.14517, 2021. [36] Liangyu Xu, Yingxiu Zhao, Jingyun Wang, Yingyao Wang, Bu Pi, Chen Wang, Mingliang Zhang, Jihao Gu, Xiang Li, Xiaoyong Zhu, et al. Geosense: Evaluating identification and application of geometric principles in multimodal reasoning. arXiv preprint arXiv:2504.12597, 2025. [37] Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. Iconqa: new benchmark for abstract diagram understanding and visual language reasoning. arXiv preprint arXiv:2110.13214, 2021. [38] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024. [39] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. 15 [40] Yew Ken Chia, Vernon Toh Yan Han, Deepanway Ghosal, Lidong Bing, and Soujanya Poria. Puzzlevqa: Diagnosing multimodal reasoning challenges of language models with abstract visual patterns. arXiv preprint arXiv:2403.13315, 2024. [41] Deepanway Ghosal, Vernon Toh Yan Han, Chia Yew Ken, and Soujanya Poria. Are language models puzzle prodigies? algorithmic puzzles unveil serious challenges in multimodal reasoning. arXiv preprint arXiv:2403.03864, 2024. [42] Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi, and Hannaneh Hajishirzi. Are you smarter than sixth grader? textbook question answering for multimodal machine comprehension. In Proceedings of the IEEE Conference on Computer Vision and Pattern recognition, pages 49995007, 2017. [43] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. [44] Meng, Du, Liu, Zhou, Lu, Fu, Shi, Wang, He, Zhang, et al. Mm-eureka: Exploring visual aha moment with rule-based large-scale reinforcement learning. arXiv preprint arXiv:2503.07365, 2025. [45] Kun Xiang, Zhili Liu, Zihao Jiang, Yunshuang Nie, Runhui Huang, Haoxiang Fan, Hanhui Li, Weiran Huang, Yihan Zeng, Jianhua Han, et al. Atomthink: slow thinking framework for multimodal mathematical reasoning. arXiv preprint arXiv:2411.11930, 2024. [46] Qwen Team. Qwq-32b: Embracing the power of reinforcement learning. URL: https://qwenlm. github. io/blog/qwq-32b, 2025. [47] Yi Peng, Xiaokun Wang, Yichen Wei, Jiangbo Pei, Weijie Qiu, Ai Jian, Yunzhuo Hao, Jiachun Pan, Tianyidan Xie, Li Ge, et al. Skywork r1v: Pioneering multimodal reasoning with chain-of-thought. arXiv preprint arXiv:2504.05599, 2025. [48] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [49] Anthropic. Claude. https://www.anthropic.com/index/introducing-claude, 2023. [50] Doubao AI. Doubao 1.5 vision pro 32k, 2025. [51] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [52] Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, Lin Chen, Qipeng Guo, Haodong Duan, Bin Wang, Linke Ouyang, et al. Internlm-xcomposer-2.5: versatile large vision language model supporting long-contextual input and output. arXiv preprint arXiv:2407.03320, 2024. [53] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. 16 [54] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. [55] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. [56] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [57] Qwen Team. Qvq: To see the world with wisdom, 2024. [58] Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, et al. Kimi-vl technical report. arXiv preprint arXiv:2504.07491, 2025. [59] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [60] GPT-4o. Hello gpt-4o, 2024. [61] Yihe Deng, Hritik Bansal, Fan Yin, Nanyun Peng, Wei Wang, and Kai-Wei Chang. Openvlthinker: An early exploration to complex vision-language reasoning via iterative selfimprovement. arXiv preprint arXiv:2503.17352, 2025. [62] Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, and Xu Yang. Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl. arXiv preprint arXiv:2503.07536, 2025. [63] Xiyao Wang, Zhengyuan Yang, Chao Feng, Hongjin Lu, Linjie Li, Chung-Ching Lin, Kevin Lin, Furong Huang, and Lijuan Wang. Sota with less: Mcts-guided sample selection for data-efficient visual reasoning self-improvement. arXiv preprint arXiv:2504.07934, 2025. [64] Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu, Xikun Zhang, Shijian Lu, and Dacheng Tao. R1-vl: Learning to reason with multimodal large language models via step-wise group relative policy optimization. arXiv preprint arXiv:2503.12937, 2025. [65] Qianchu Liu, Sheng Zhang, Guanghui Qin, Timothy Ossowski, Yu Gu, Ying Jin, Sid Kiblawi, Sam Preston, Mu Wei, Paul Vozila, et al. X-reasoner: Towards generalizable reasoning across modalities and domains. arXiv preprint arXiv:2505.03981, 2025. [66] Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, and Wenhu Chen. Vl-rethinker: Incentivizing self-reflection of vision-language models with reinforcement learning. arXiv preprint arXiv:2504.08837, 2025. [67] Jie Yang, Feipeng Ma, Zitian Wang, Dacheng Yin, Kang Rong, Fengyun Rao, and Ruimao Zhang. Wethink: Toward general-purpose vision-language reasoning via reinforcement learning, 2025. 17 [68] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level googleproof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023. [69] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations. [70] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. In Advances in Neural Information Processing Systems, NeurIPS 2024, 2024. [71] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [72] Chengke Zou, Xingang Guo, Rui Yang, Junyu Zhang, Bin Hu, and Huan Zhang. Dynamath: dynamic visual benchmark for evaluating mathematical reasoning robustness of vision language models, 2024. [73] Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma GongQue, Shanglin Lei, Zhe Wei, Miaoxuan Zhang, et al. We-math: Does your large multimodal model achieve human-like mathematical reasoning? arXiv preprint arXiv:2407.01284, 2024. [74] Yijia Xiao, Edward Sun, Tianyu Liu, and Wei Wang. Logicvista: Multimodal llm logical reasoning benchmark in visual contexts, 2024. [75] Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, Yu Su, Wenhu Chen, and Graham Neubig. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark, 2024. [76] Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, et al. Charxiv: Charting gaps in realistic chart understanding in multimodal llms. Advances in Neural Information Processing Systems, 37:113569113697, 2024. [77] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [78] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024. [79] Liang Chen, Lei Li, Haozhe Zhao, Yifan Song, and Vinci. R1-v: Reinforcing super generalization ability in vision-language models with less than $3. https://github.com/Dee p-Agent/R1-V, 2025. Accessed: 2025-02-02. [80] Yana Wei, Liang Zhao, Kangheng Lin, En Yu, Yuang Peng, Runpei Dong, Jianjian Sun, Haoran Wei, Zheng Ge, Xiangyu Zhang, et al. Perception in reflection. arXiv preprint arXiv:2504.07165, 2025. 18 [81] Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, Lei Zhang, Kay McKinney, Disha Shrivastava, Cosmin Paduraru, George Tucker, Doina Precup, Feryal Behbahani, and Aleksandra Faust. Training language models to self-correct via reinforcement learning, 2024. [82] Daixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, and Furu Wei. Reasoning with exploration: An entropy perspective, 2025. [83] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pages 216233. Springer, 2024. [84] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. In European Conference on Computer Vision, pages 148166. Springer, 2024. [85] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024. [86] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et al. Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1437514385, 2024. [87] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355, 2023. [88] X.AI. Grok-2 beta release. https://x.ai/blog/grok-2, 2024. Accessed on: 2024-07-02. [89] Hui Shen, Taiqiang Wu, Qi Han, Yunta Hsieh, Jizhou Wang, Yuyue Zhang, Yuxin Cheng, Zijian Hao, Yuansheng Ni, Xin Wang, et al. Phyx: Does your model have the\" wits\" for physical reasoning? arXiv preprint arXiv:2505.15929, 2025. [90] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: comprehensive evaluation benchmark for multimodal large language models, 2024. [91] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. [92] Zijun Yao, Yantao Liu, Yanxu Chen, Jianhui Chen, Junfeng Fang, Lei Hou, Juanzi Li, and Tat-Seng Chua. Are reasoning models more prone to hallucination? arXiv preprint arXiv:2505.23646, 2025. [93] Chengzhi Liu, Zhongxing Xu, Qingyue Wei, Juncheng Wu, James Zou, Xin Eric Wang, Yuyin Zhou, and Sheng Liu. More thinking, less seeing? assessing amplified hallucination in multimodal reasoning models. arXiv preprint arXiv:2505.21523, 2025. 19 [94] Core Team, Zihao Yue, Zhenru Lin, Yifan Song, Weikun Wang, Shuhuai Ren, Shuhao Gu, Shicheng Li, Peidian Li, Liang Zhao, Lei Li, Kainan Bao, Hao Tian, Hailin Zhang, Gang Wang, Dawei Zhu, Cici, Chenhong He, Bowen Ye, Bowen Shen, Zihan Zhang, Zihan Jiang, Zhixian Zheng, Zhichao Song, Zhenbo Luo, Yue Yu, Yudong Wang, Yuanyuan Tian, Yu Tu, Yihan Yan, Yi Huang, Xu Wang, Xinzhe Xu, Xingchen Song, Xing Zhang, Xing Yong, Xin Zhang, Xiangwei Deng, Wenyu Yang, Wenhan Ma, Weiwei Lv, Weiji Zhuang, Wei Liu, Sirui Deng, Shuo Liu, Shimao Chen, Shihua Yu, Shaohui Liu, Shande Wang, Rui Ma, Qiantong Wang, Peng Wang, Nuo Chen, Menghang Zhu, Kangyang Zhou, Kang Zhou, Kai Fang, Jun Shi, Jinhao Dong, Jiebao Xiao, Jiaming Xu, Huaqiu Liu, Hongshen Xu, Heng Qu, Haochen Zhao, Hanglong Lv, Guoan Wang, Duo Zhang, Dong Zhang, Di Zhang, Chong Ma, Chang Liu, Can Cai, and Bingquan Xia. Mimo-vl technical report, 2025. [95] Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1. 5-vl technical report. arXiv preprint arXiv:2505.07062, 2025. [96] OpenAI. Introducing openai o3 and o4-mini. https://openai.com/index/introdu cing-o3-and-o4-mini/, 2025. [97] Zeyuan Yang, Xueyang Yu, Delin Chen, Maohao Shen, and Chuang Gan. Machine mental imagery: Empower multimodal reasoning with latent visual tokens. arXiv preprint arXiv:2506.17218, 2025. [98] Baiqiao Yin, Qineng Wang, Pingyue Zhang, Jianshu Zhang, Kangrui Wang, Zihan Wang, Jieyu Zhang, Keshigeyan Chandrasegaran, Han Liu, Ranjay Krishna, et al. Spatial mental modeling from limited views. arXiv preprint arXiv:2506.21458, 2025. [99] Mislav Balunovic, Jasper Dekoninck, Ivo Petrov, Nikola Jovanovic, and Martin Vechev. Matharena: Evaluating llms on uncontaminated math competitions, february 2025. URL https://matharena. ai. [100] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. [101] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level googleproof q&a benchmark. In First Conference on Language Modeling, 2024. [102] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. [103] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, et al. Dreamllm: Synergistic multimodal comprehension and creation. arXiv preprint arXiv:2309.11499, 2023. [104] Junyu Zhang, Runpei Dong, Han Wang, Xuying Ning, Haoran Geng, Peihao Li, Xialin He, Yutong Bai, Jitendra Malik, Saurabh Gupta, and Huan Zhang. Alphaone: Reasoning models thinking slow and fast at test time. arXiv preprint arXiv:2505.24863, 2025. [105] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. 20 [106] Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao, Zheng Ge, Jinrong Yang, Jianjian Sun, Chunrui Han, and Xiangyu Zhang. Vary: Scaling up the vision vocabulary for large vision-language model. In European Conference on Computer Vision, pages 408424. Springer, 2024. [107] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multidiscipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. [108] En Yu, Liang Zhao, Yana Wei, Jinrong Yang, Dongming Wu, Lingyu Kong, Haoran Wei, Tiancai Wang, Zheng Ge, Xiangyu Zhang, et al. Merlin: Empowering multimodal llms with foresight minds. In European Conference on Computer Vision, pages 425443. Springer, 2024. [109] Liang Zhao, En Yu, Zheng Ge, Jinrong Yang, Haoran Wei, Hongyu Zhou, Jianjian Sun, Yuang Peng, Runpei Dong, Chunrui Han, et al. Chatspot: Bootstrapping multimodal llms via precise referring instruction tuning. arXiv preprint arXiv:2307.09474, 2023. [110] Guowei Xu, Peng Jin, Li Hao, Yibing Song, Lichao Sun, and Li Yuan. Llava-o1: Let vision language models reason step-by-step. arXiv preprint arXiv:2411.10440, 2024. [111] Jiayi He, Hehai Lin, Qingyun Wang, Yi Fung, and Heng Ji. Self-correction is more than refinement: learning framework for visual and language reasoning tasks. arXiv preprint arXiv:2410.04055, 2024. [112] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. [113] Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, and Sushant Prakash. Rlaif vs. rlhf: Scaling reinforcement learning from human feedback with ai feedback, 2024. [114] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [115] Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. [116] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. [117] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand칟s, and Tatsunori Hashimoto. s1: Simple test-time scaling, 2025. [118] Yuan-Hong Liao, Sven Elflein, Liu He, Laura Leal-Taix칠, Yejin Choi, Sanja Fidler, and David Acuna. Longperceptualthoughts: Distilling system-2 reasoning for system-1 perception. arXiv preprint arXiv:2504.15362, 2025. 21 [119] Jian Hu, Xibin Wu, Zilin Zhu, Xianyu, Weixun Wang, Dehao Zhang, and Yu Cao. Openrlhf: An easy-to-use, scalable and high-performance rlhf framework. arXiv preprint arXiv:2405.11143, 2024. [120] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [121] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. [122] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [123] Richard Sutton. The bitter lesson. Incomplete Ideas (blog), 13(1):38, 2019. [124] Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Tianjun Zhang, Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl. https://pretty-radio-b75 .notion.site/DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Model-b y-Scaling-RL-19681902c1468005bed8ca303013a4e2, 2025. Notion Blog. [125] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [126] Chengqi Lyu, Songyang Gao, Yuzhe Gu, Wenwei Zhang, Jianfei Gao, Kuikun Liu, Ziyi Wang, Shuaibin Li, Qian Zhao, Haian Huang, Weihan Cao, Jiangning Liu, Hongwei Liu, Junnan Liu, Songyang Zhang, Dahua Lin, and Kai Chen. Exploring the limit of outcome reward for learning mathematical reasoning, 2025. [127] Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. Demystifying long chain-of-thought reasoning in llms. arXiv preprint arXiv:2502.03373, 2025. [128] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. [129] OpenThoughts Team. Open Thoughts. https://open-thoughts.ai, January 2025. [130] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [131] Yingqian Min, Zhipeng Chen, Jinhao Jiang, Jie Chen, Jia Deng, Yiwen Hu, Yiru Tang, Jiapeng Wang, Xiaoxue Cheng, Huatong Song, Wayne Xin Zhao, Zheng Liu, Zhongyuan Wang, and Ji-Rong Wen. Imitate, explore, and self-improve: reproduction report on slow-thinking reasoning systems. arXiv preprint arXiv:2412.09413, 2024. [132] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. [133] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023. [134] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024. [135] Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: comparative study of foundation model post-training. arXiv preprint arXiv:2501.17161, 2025. [136] Lev Vygotsky. Thought and language, volume 29. MIT press, 2012. [137] Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, Alexis Chevalier, Sanjeev Arora, and Danqi Chen. Charxiv: Charting gaps in realistic chart understanding in multimodal llms, 2024. [138] OpenAI. GPT-4o Mini: Advancing Cost-Efficient Intelligence. http://openai.com/ind ex/gpt-4o-mini-advancing-cost-efficient-intelligence/, 2025. Accessed: 2025-05-16. [139] Daixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, and Furu Wei. Reasoning with exploration: An entropy perspective. arXiv preprint arXiv:2506.14758, 2025. [140] Baiqiao Yin, Qineng Wang, Pingyue Zhang, Jianshu Zhang, Kangrui Wang, Zihan Wang, Jieyu Zhang, Keshigeyan Chandrasegaran, Han Liu, Ranjay Krishna, Saining Xie, Manling Li, Jiajun Wu, and Li Fei-Fei. Spatial mental modeling from limited views, 2025. 23 The appendix includes extended details on data curation (Appendix A), implementation (Appendix B), cognitive behavior evaluation (Appendix C), and additional case studies (Appendix D). A. Cold-Start Data Curation Details As mentioned in Section 3.3, critical component of our initial policy development is the curation of the cold-start SFT dataset. This stage serves as the foundation for subsequent learning, particularly in shaping the models ability to exhibit structured reasoning and cognitive behavior. To this end, we adopt multi-stage curation pipeline consisting of data collection, filtering, cleaning, and strategic reweighting. Data Acquisition. We begin by assembling broad set of prompt-response pairs that span diverse reasoning domains. These include math, science, and general logical reasoning tasks such as puzzles, deductive tasks, and constraint satisfaction problems. Our sources include mix of public datasets as illustrated in Section 3.3. Automated Filtering. To improve the signal-to-noise ratio, we filter the collected data using lightweight pretrained LLM as proxy for quality estimation. Each instance is passed through this model, and those with abnormally high training loss are flagged as noisy, ambiguous, or misaligned. We further apply rule-based and model-assisted pattern detectors to identify and eliminate undesirable data characteristics. Difficulty Stratification. We explicitly incorporate samples from AMC, AIME, Olympiads, and AoPS forums to ensure difficulty levels. We then stratify the collected samples based on their source and inherent problem complexity to balance coverage across easy, intermediate, and challenging reasoning scenarios. Reweighting and Balance. To address imbalances across domains and formats, we apply reweighting scheme based on coverage and reasoning relevance. Over-represented formats are down-weighted, while rare but cognitively rich categories are given higher sampling probabilities. This ensures more uniform distribution of reasoning challenges and minimizes overfitting to dominant patterns. B. More Implementation Details Model and Optimization Setup Our model is based on the Qwen2.5-VL [23]. During RL, both the policy and critic networks are initialized from the cold-start model. The value head is initialized from uniform distribution ( 5) without bias. The policy and critic networks do not share weights during training. We use the AdamW optimizer with 洧띻 = [0.9, 0.95] and no weight decay. Learning rates are set to 1 106 for the policy and 5 106 for the critic. We use constant learning rates with linear warm-up of 50 steps, and employ sample packing for improved throughput. No KL regularization or entropy bonus is used, demonstrating that vanilla PPO remains stable under our setup. 5, 24 PPO Training Dynamics Each PPO update is based on 512 unique prompts, each generating 16 sampled responses using temperature and top-p sampling (both set to 1.0). To ensure training stability, we enforce strict on-policy updates for the policy: each prompt generation corresponds to single optimization step. In contrast, the critic performs 4 optimization steps per PPO update. We apply batch-level advantage normalization to stabilize training further. C. Details for Coginitive Behavior Evaluation In this section, we detail definitions of metrics in cognitive behavior analysis (Section 5.2) and show the prompt for evaluation. C.1. Behavior Transfer Rate To quantify how well language-acquired cognitive behaviors generalize to the visual modality, we define the Behavior Transfer Rate (BTR) for each behavior type introduced in Fig. 5. BTR is calculated as the ratio between the emergence rate of visual behaviors and that of their linguistic counterparts. Formally, we compute the Cognitive Behavior Emergence Rate in the visual modality (CBRv) and in the language modality (CBRl), both evaluated on the multimodal benchmark MathVision (mini). The BTR is then defined as: BTR = CBRv CBRl This metric reflects the cross-modal transfer efficiency of cognitive behaviors, with higher values indicating stronger behavioral generalization from language to vision. C.2. Evaluation Prompt We design prompts for the LLM-based evaluation. Fig. 7 showcases the prompt template for the cognitive behavior Backtracking as an example. Figure 7: Prompt design for evaluating Backtracking. 25 D. More Case Studies We present examples of OVRs reasoning process during complex multimodal tasks. In Fig. 10, the model begins by perceiving the image and applying visual divide-and-conquer to interpret each emoji individually. It then engages in subgoal setting, breaking down the task to explore multiple candidate answers based on each emoji. Throughout the process, the model exhibits backtracking, revising earlier hypotheses by cross-verifying them with elements in the image. The reasoning process concludes with final, well-justified prediction. Figure 8: Case Study on OCR-based Document Understanding. Red highlights some of the cognitive behaviors. 26 Figure 9: Case Study on Math Reasoning. Red highlights some of the cognitive behaviors. 27 Figure 10: Case Study on Emoji Quiz. Red highlights some of the cognitive behaviors. The model first performs visual decomposition to interpret each emoji individually. It then engages in subgoal setting to explore multiple candidate answers, accompanied by backtracking to revise earlier hypotheses. The reasoning concludes with final, justified prediction. 28 Figure 11: Case Study on DeepSeek-R1. Red highlights the mental imagery mentioned in Section 5.2."
        }
    ],
    "affiliations": [
        "BUPT",
        "HUST",
        "Johns Hopkins University",
        "StepFun",
        "THU",
        "UCAS"
    ]
}