{
    "paper_title": "DataDecide: How to Predict Best Pretraining Data with Small Experiments",
    "authors": [
        "Ian Magnusson",
        "Nguyen Tai",
        "Ben Bogin",
        "David Heineman",
        "Jena D. Hwang",
        "Luca Soldaini",
        "Akshita Bhagia",
        "Jiacheng Liu",
        "Dirk Groeneveld",
        "Oyvind Tafjord",
        "Noah A. Smith",
        "Pang Wei Koh",
        "Jesse Dodge"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Because large language models are expensive to pretrain on different datasets, using smaller-scale experiments to decide on data is crucial for reducing costs. Which benchmarks and methods of making decisions from observed performance at small scale most accurately predict the datasets that yield the best large models? To empower open exploration of this question, we release models, data, and evaluations in DataDecide -- the most extensive open suite of models over differences in data and scale. We conduct controlled pretraining experiments across 25 corpora with differing sources, deduplication, and filtering up to 100B tokens, model sizes up to 1B parameters, and 3 random seeds. We find that the ranking of models at a single, small size (e.g., 150M parameters) is a strong baseline for predicting best models at our larger target scale (1B) (~80% of com parisons correct). No scaling law methods among 8 baselines exceed the compute-decision frontier of single-scale predictions, but DataDecide can measure improvement in future scaling laws. We also identify that using continuous likelihood metrics as proxies in small experiments makes benchmarks including MMLU, ARC, HellaSwag, MBPP, and HumanEval >80% predictable at the target 1B scale with just 0.01% of the compute."
        },
        {
            "title": "Start",
            "content": "How to Predict Best Pretraining Data with Small Experiments Ian Magnusson, Nguyen Tai, Ben Bogin, David Heineman, Jena Hwang, Luca Soldaini, Akshita Bhagia, Jiacheng Liu, Dirk Groeneveld, Oyvind Tafjord, Noah A. Smith, Pang Wei Koh, Jesse Dodge Allen Institute for AI University of Pennsylvania University of Washington equal contribution"
        },
        {
            "title": "Abstract",
            "content": "Because large language models are expensive to pretrain on different datasets, using smaller-scale experiments to decide on data is crucial for reducing costs. Which benchmarks and methods of making decisions from observed performance at small scale most accurately predict the datasets that yield the best large models? To empower open exploration of this question, we release models, data, and evaluations in DATADECIDEthe most extensive open suite of models over differences in data and scale. We conduct controlled pretraining experiments across 25 corpora with differing sources, deduplication, and filtering up to 100B tokens, model sizes up to 1B parameters, and 3 random seeds. We find that the ranking of models at single, small size (e.g., 150M parameters) is strong baseline for predicting best models at our larger target scale (1B) ( 80% of comparisons correct). No scaling law methods among 8 baselines exceed the compute-decision frontier of single-scale predictions, but DATADECIDE can measure improvement in future scaling laws. We also identify that using continuous likelihood metrics as proxies in small experiments makes benchmarks including MMLU, ARC, HellaSwag, MBPP, and HumanEval > 80% predictable at the target 1B scale with just 0.01% of the compute. 5 2 0 2 5 1 ] . [ 1 3 9 3 1 1 . 4 0 5 2 : r Figure 1: Which pretraining data to use? Ideally, compare performance of large models with fixed configurations averaged over random seeds (left). In practice, cheaper, smaller-scale experiments are used (center). Here DATADECIDE measures accuracy of pairwise decisions between 25 pretraining corpora to find efficient prediction methods (right)."
        },
        {
            "title": "Introduction",
            "content": "The cost of training large language models (LMs) necessitates methods of trying out options at small scale, but it also makes it expensive to validate the accuracy of development decisions made with such methods. We focus on the question of choosing between pretraining datasets to useone of the most impactful development decisions. Common practice (e.g., Li et al., 2024) uses single, small scale of experiments to cheaply test pretraining data intended for larger-scale models, where scale is determined by number of model parameters and training tokens. The other predominant approach is to fit scaling laws (Kaplan et al., 2020; Hoffmann et al., 2022; Choshen et al., 2024) to the trend in performance observed over multiple small scales, with recent work extending this to the prediction of downstream performance instead of language modeling loss (Gadre et al., 2024; Dubey et al., 2024; Bhagia et al., 2024). So far decision-making approaches have only been validated without observing the counterfactual outcome, either by producing single large model on the chosen decision with impressive performance or by low error in predicting the magnitude of observed performance of small number of large models. Knowing what amount of error in predicting performance over scale is low enough to actually make correct decision among datasets, requires suite of comparable models trained on many datasets. Although wide variety of open-source pretraining corpora are available, the scaling behavior of data is difficult to assess from off-the-shelf models that vary simultaneously in data, optimizer, and modeling decisions. To make it possible to empirically study what methods make the best decisions over data, we build DATADECIDE1a suite of models we pretrain on 25 corpora up to 100B tokens, over 14 different model sizes ranging from 4M parameters up to 1B parameters (more than 30K model checkpoints in total). We evaluate all models across suite of 10 downstream tasks and calculate how accurately small models predict which pretraining corpora lead to better performance at our largest scale. Our conclusions provide practical recommendations for the best benchmarks, prediction methods, and metrics to use to make decisions. We call the 25 corpora we train on data recipes as they range across popular corpora including Dolma (Soldaini et al., 2024), DCLM (Li et al., 2024), RefinedWeb (Penedo et al., 2023), C4 (Raffel et al., 2019), and FineWeb (Penedo et al., 2024) as well as combinations of interventions on these datasets such as source mixing, deduplication, and filtering. Previous work has considered only 2 (Biderman et al., 2023) or 6 recipes (Magnusson et al., 2024; Brandfonbrener et al., 2024). We also offer novel affordance by including 3 random seed reruns for even our largest runs, to help quantify whether variation occurs due to random initialization and data order or differences in the distribution of data. Concretely, DATADECIDE allows analyses such as Figure 1 (right), which shows the relationship between compute used to predict ranking of datasets and how accurately that ranking reflects mean performance over 3 seed runs (quantified here by OLMES; Gu et al., 2024) for models fully trained on those datasets at the target (1B) scale. We measure the accuracy of decisions as the percent of compared pairs of datasets where the prediction identifies the correct winner. Each point represents the average decision accuracy of given method over 3 prediction attempts using small models with different random seeds, and shading shows standard deviation. Measuring the tradeoff of compute cost to better decisions lets us make the following recommendations about small experiments for making data decisions: 3.1 The amount of compute you need to allocate for given decision accuracy depends heavily on task. MMLU and ARC are much cheaper to predict than HellaSwag and some tasks such as SocialIQA are difficult to predict at all scales. 3.2 8 baseline scaling law methods do not exceed the compute to decision accuracy frontier set by ranking single scale experiments. 1DataDecide collection on HuggingFace 2 Source / Recipe Description Dolma1.7 Original, No code, No math/code, No Reddit, No Flan 2.3T-token corpus (Dolma 1.7 Soldaini et al., 2024) sampling common LM sources for open research. We ablate code, math/code, Reddit, or Flan subsets. Dolma1.6++ Original Dolma 1.6 plus additional sources from Dolma 1.7: RedPajamas arxiv subset, openwebmath, algebraic stack, flan, starcoder, falcon. C4 Original The C4 dataset (Raffel et al., 2019) as prepared in Dolma 1.7, heuristically filtered from the April 2019 Common Crawl. FineWeb-Pro Original The FineWeb Pro corpus (Zhou et al., 2024), featuring model-driven data cleaning on FineWeb. FineWeb-Edu Original The deduplicated FineWeb-Edu subset of SmolLM-Corpus (Ben Allal et al., 2024), focused on educational web pages. Falcon Original The Falcon RefinedWeb corpus (Penedo et al., 2023) in Dolma 1.7, derived from Common Crawl through June 2023 and more aggressively filtered/deduplicated than C4. Falcon+CC Original, QC 10%, QC 20%, QC Orig 10%, QC Tulu 10% Falcon and Dolma 1.7s Common Crawl. We quality filter to top 10% or 20% documents with reproduced or original Li et al. (2024) filter or retrain filter on pre-release version of Tulu-v3 (Lambert et al., 2024). DCLM-Baseline Original, QC 7% FW2, QC 7% FW3, QC FW 3%, QC FW 10%, QC 10%, QC 20% SOTA Common Crawl corpus using best ablated deduplication, cleaning heuristics, and quality filter. We quality filter to top 7% of DCLM classified documents and further take 2+ or 3+ scores with FineWeb-edu classifier; or filter to top 3% or 10% with FineWeb-edu classifier; or take top 10% or 20% with reproduced DCLM classifier. λ% DCLM-Baseline + 1 λ% Dolma1. Fractional combinations of Dolma1.7 and DCLM-Baseline mixing different proportions of the two datasets for λ {25%, 50%, 75%}. Table 1: DATADECIDE enables the study of data differences over scales through controlled pretraining experiments on 25 data recipes. These take different source datasets and apply interventions from ablating domains, deduplication, mixing, to quality filtering with different classifiers and thresholds. We release all pretraining corpora, as well as models trained on each recipe and each of the 14 model configurations in Table 2 with 3 random seeds. 3.3 At small scales, continuous metrics using answer likelihood are better or equivalent predictors of decisions than using the same discrete accuracy target metric. 3.4 Better decisions can be explained in part by low run-to-run variance and wide spread of benchmark performance values for different data, traits which can be improved by proxy metrics. Future research can extend DATADECIDE with little extra compute by running new evaluations on our checkpoints, pretraining additional small models to compare against the large target models we provide, or trying new prediction methods with lightweight manipulations such as smoothing and curve fitting on top of our released evaluation results."
        },
        {
            "title": "2 Methods",
            "content": "Our aim is to empirically test the predictability of downstream performance at larger, target scale using small experiments. We describe DATADECIDE 2.1, the prediction methods we examine 2.2, the metrics we use to assess predictions 2.3, how we measure downstream performance 2.4, and proxy metrics for our performance evaluations 2.5. We will release all models, checkpoints, pretraining corpora, and evaluations. 3 2.1 The DATADECIDE Suite We pretrain suite of 1,050 models using 25 data recipes 14 model scales 3 random seeds for initialization and data order. Table 1 describes the 25 data recipes included in DATADECIDE that aim to provide coverage of common data preparation choices such as deduplication, ablating domains, mixes of existing datasets, as well as quality filters with different implementations, training data, and thresholds for quality classifiers. We select token to parameter ratio of 100, which at 5 Chinchilla (5 C) optimal ratio (Hoffmann et al., 2022) captures the typical overtraining favored for inference savings. All 1B (target size) models have 3 full reruns with different seeds, while other model sizes have second and third seed runs that are terminated early after 25% of the target compute budget. We train the 1B reruns all the way to completion to allow our target gold predictions to account for run-to-run variance in evaluations due to weight initialization and data order. For instance, we find that the standard deviation between runs at the 1B 5C scale can be as high as 2% points of accuracy for some recipes on most tasks. Meanwhile, at the non-target scales we wish to make predictions with small fraction of the target compute, so we avoid reruns that would use an impractically large prediction budget. Whether for extrapolating scaling laws or ranking single scale experiments, it is important to select reasonable hyperparameters for each scale to avoid confounding in performance differences that are simply due to suboptimal hyperparameters. We use OLMos model ladder (Groeneveld et al., 2024; OLMo et al., 2025; Bhagia et al., 2024) to programmatically create LM pretraining configurations for specified parameter size and token-parameter ratio to enable running grid of model scaling experiments. The model ladder uses heuristics from the literature (Porian et al., 2024) to set global batch size and learning rate based on scaling factors. The hyperparameters that determine parameter count (layers, hidden dimension, number of heads, MLP dimension) were handpicked by OLMo developers for each scale to achieve the desired number of parameters. Appendix Table 2 details the configurations of all our models. 2.2 Prediction Methods Broadly, there are two approaches in the literature to predicting large-scale performance based on small-scale experiments. We use straightforward implementations of each to assess where they succeed and fail at making decisions about which data recipes to use. Ranking Single Scale Experiments (Single Scale) This simple approach is employed by work such as Li et al. (2024) and consists of running set of ablations or experiments over data recipe options while holding constant all other modeling variables including scale. The winning data recipe by downstream accuracy (or proxies) at the small experimental scale is assumed to extrapolate to the target scale. Extrapolating Scaling Laws (Multi Scale) Another approach to making decisions with predictions across scales used in works such as Dubey et al. (2024) is to fit scaling laws to multiple small experiments across range of scales for each of the data recipes. The winning recipe is decided as the one whose scaling law shows the highest extrapolated performance at the target scale. Although scaling laws were first observed for language modeling loss (Kaplan et al., 2020; Hoffmann et al., 2022), they have been extended to predict downstream performance through two-step approach that also fits function from loss to downstream performance (Gadre et al., 2024; Bhagia et al., 2024). We follow method from Bhagia et al. (2024). Their proposed approach incorporates separate parameters for number of model parameters and number of tokens trained to account for over or undertrained models. But as our suite only includes one token-parameter ratio, we use the simplified 3 parameter baseline, L(C), as first step which we chain with second step, Acc(L), defined as follows where A, α, E, a, b, k, L0 are optimized parameters: L(C) = Acc(L) = Cα + 1 + ek(LL0) + (1) (2) Following Bhagia et al. (2024) we fit Equation 1 only on observations of final, fully trained checkpoints as accounting for the learning rate schedules impact on intermediate checkpoints would require further parameters in the equation increasing the required number of observations and cost. To account for step-to-step noise in evaluation we average the last 10% of checkpoints as the final observed loss. Equation 2, however, is fit on all observations including intermediate checkpoints. We explore variations for total of 8 multi scale approaches defined in Appendix C; none of these make for substantially better decisions than the method defined in this section. 2.3 Prediction Metrics Our predictive task is to forecast which of pair of data recipes will perform better at some target scale based on small-scale experiments. We use the following metrics to measure the quality of these predictions. Prediction Error Scaling laws literature (Bhagia et al., 2024; Gadre et al., 2024) typically evaluates success from predicted and actual downstream performance, using relative error predictedactual 100%) or absolute error (predicted actual 100%). We call these actual ( absolute or relative prediction error to distinguish from the following metric. Decision Accuracy Unlike previous work, we also measure the impact of predictions on decisions about which data recipe is better than another. The metric we use to capture this is decision accuracy, an accuracy over all pairs of data recipes and where either or is defined as the correct winner based on which achieves higher performance at the target scale. This is nearly equivalent to Kendalls τ, but ranges from 0 to 1. We define the target-scale winner based on mean downstream performance over 3 random seeds. Thus decision accuracy can be formalized as follows. Let be the set of all data recipe pairs (A, B) with observed mean performance yA, yB and predicted performance ˆyA, ˆyB, respectively, then decision accuracy is: 1 (A,B)P I(cid:0)sign( ˆyA ˆyB) = sign(yA yB)(cid:1) (3) Percent of Target Compute Budget (%C) We measure compute in terms of theoretical FLOPs following the simplifying assumption made in most scaling literature that the costs associated with training model are captured well enough by FLOPs = 6ND, based solely on the number of parameters (N) and tokens trained (D) (Kaplan et al., 2020). We consider the efficiency of prediction based on the ratio of the experimental budget and the target budget in FLOPs, %C = 100%. 2.4 Performance Evaluation with OLMES We use the OLMES suite of 10 multiple choice question answering benchmarks (Gu et al., 2024): MMLU (Hendrycks et al., 2021), HellaSwag (Zellers et al., 2019), ARC Challenge (Clark et al., 2018), ARC Easy (Clark et al., 2018), PIQA (Bisk et al., 2020), CommonsenseQA (Talmor et al., 2019),SocialIQA (Sap et al., 2019), OpenBookQA (Mihaylov et al., 2018), BoolQ (Clark et al., 2019), and WinoGrande (Sakaguchi et al., 2020). These tasks are well suited for the model scales we examine with all but BoolQ receiving non-trivial performance. Unless otherwise noted, we consider the macro average of these ten tasks. The underlying metric for each task is accuracy, for which OLMES specifies different length normalization scheme per task. Our target gold rankings which we aim to predict are always based on the cloze formulation (CF) accuracy with curated normalization per task, which we refer Figure 2: Accuracy in pairwise decisions on best data when evaluating on the 10 OLMES tasks with ACCURACY (shown aggregated in Figure 1). Specific tasks have very distinct ranges of sensitivity, with some like ARC Easy being predictable at small scales and others like HellaSwag requiring substantially more compute to predict. to as ACCURACY. We diverge from OLMES only in that we make use of all available items in the specified split of each benchmark rather than subsampling them, to reduce variance over the task distribution. Note that while we focus just on OLMES multiple choice evaluations in this work, our method of validating decisions made through predictions can be applied to other benchmarks. We chose these tasks based on their appropriateness to our range of model scales, and one would have to select different tasks when targeting larger scale. Moreover, DATADECIDE could be used to identify new evaluations that are sensitive within our range of scales. 2.5 Proxy Metrics for Performance Evaluation Previous work has noted how discrete metrics such as accuracy can cause jumps in performance across scale that otherwise see more predictable improvements with scale for continuous metrics (Schaeffer et al., 2023). We experiment with using continuous metrics at small scale as proxies of the accuracies selected by OLMES for each task (ACCURACY) at the target scale to improve decision accuracy. We use the following metrics: CORRECT PROB is the average probabilities of the correct continuations. MARGIN is the average difference between the probability of the correct continuation and the most likely incorrect continuation. NORM CORRECT PROB is the average probability of the correct continuation conditioned on the response being in the set of correct or incorrect continuations. TOTAL PROB is the average of the sum of probabilities of all correct and incorrect continuations. ACCURACY is the fraction of instances where the correct continuation has the highest probability. Each of these can be computed with likelihoods normalized by number of tokens or characters; unless otherwise specified we use character length normalization. Appendix Table 3 shows formal definitions. Figure 3: Decision accuracy over 8 baseline scaling law variants. At best, these approaches reach only the same compute to decision accuracy frontier as ranking single scale experiments. DATADECIDE can be used to iterate on future scaling law prediction methods."
        },
        {
            "title": "3 Results",
            "content": "3.1 What is the best way to spend compute for data decisions? More compute makes better decisions. Decisions from intermediate checkpoints are as good as compute equivalent final checkpoints. The amount of compute needed to make good predictions varies between tasks. ARC and MMLU are predictable with much less compute than HellaSwag. The rest of OLMES tasks give markedly less reliable predictions across the scales we examine. First looking at the aggregation of all 10 OLMES tasks (Figure 1 right), we see that there is positive and roughly log-linear relationship between experimental compute and decision accuracy. Specifically, this figure illustrates the relationship between the compute used for predicting best data recipes and the decision accuracy those predictions achieve against targets ranked by OLMES performance at the 1B scale. Each point represents the average decision accuracy over three runs with different random seeds, with shading indicating standard deviation. Points with the same color show all intermediate checkpoints from given parameter size. The color shows each model size for predicting using ranking single scale experiments. The stars show predictions from extrapolating scaling laws using our default 3-parameter approach, the details of which are discussed further in 3.2. The ease of prediction is greatly influenced by which evaluation benchmark we use. In Figure 2, we show the relationship of compute and decision accuracy for each of the tasks in OLMES individually. The predictive sensitivity of tasks at given compute varies significantly, with ARC Easy being consistently predictable with 5 orders of magnitude less compute and BoolQ only reaching beyond trivial decision accuracy for intermediate checkpoints of the target runs. HellaSwag, SocialIQA, WinoGrande show distinct periods of insensitivity followed by roughly log-linear increase after hitting some compute threshold. 3.2 How does extrapolating scaling laws compare to ranking single scale experiments? selection of 8 baseline scaling law methods are no more efficient than ranking single scale experiments. Future scaling law methods can be assessed on DATADECIDE. Figure 3 contrasts different approaches to fitting scaling laws over multiple scales of small experiments. Each of the 8 approaches is shown in different color. Multi-scale predictions have compute budget equal to the training cost of the model sizes used 7 Figure 4: Per-task decision accuracy using character normalized proxy metrics for ACCURACY targets. 5 tasks benefit at smaller scales from using raw likelihood of answers (CORRECT PROB and TOTAL PROB), as opposed to discrete ACCURACY or continuous metrics that penalize probability on incorrect answers (NORM CORRECT PROB, MARGIN). to make the prediction. We try the following combinations of models sizes: We use {{s1, . . . , sk} 3 14}, where is the ordered set of sizes, to explore the improvements of progressively adding larger model sizes beyond the minimum 3 required for fitting. We also use {{sk, . . . , s14} 2 11} to try removing potentially noisy information from small models. Unlike single scale results, we make only one prediction attempt with the default fully trained random seed, as final checkpoints are required for fitting the first step of these scaling law variants but are not available for all seeds. Our scaling law approaches vary in the number of parameters fit, using hard coded points to define the minimum and maximum performance, using only the second half of intermediate checkpoints for fitting the second step, or fitting function directly from compute to accuracy in single step. Each of the scaling law variants are defined formally in Appendix C. The 2 and 3 parameter variants all achieve among the top decision accuracy. priori we know that ranking single scale experiments cannot correctly predict when the scaling trend of one data recipe overtakes another at scales between our small experiments and target scale. Such crossovers bound the decision accuracy of this constant approximation of performance. Nevertheless ranking single scale experiments sets high baseline decision accuracy, implying relatively little crossover occurs. It is difficult to distinguish evaluation variance from true crossovers, but the scaling trends we empirically observe cross over frequently. Improved future scaling laws may be able to advance the Pareto frontier on DATADECIDE as they are not bound by crossovers. 3.3 What proxy metrics give better signal for predictions at small scale? At small scales, continuous metrics using the character normalized likelihood of correct or all answer options serve as better or equivalent predictors of decisions than using the same ACCURACY as used at the target scale. Figure 4 shows the decision accuracy over different proxy metrics. Here we chose single length normalization, * PER CHAR. Metrics follow similar trends regardless of length normalization and this one is empirically optimal for most of the tasks that we observe. Using CORRECT PROB or TOTAL PROB leads to decision accuracy at least as good as any other metric for most small scales. These continuous metrics are simple likelihoods over 8 Figure 5: Why do some tasks or metrics get better or worse decision accuracy? At 150M with CORRECT PROB tasks like HellaSwag succeed with low run-to-run variance and tasks like SocialIQA widely spread the performance assigned to different pretraining data. answer strings. In particular, TOTAL PROB may be interpretable as signal of model having exposure to the domain of given task in the form of higher likelihoods on incorrect but presumably relevant additional answers. We notice two very distinct types of trends over the different tasks. Either the different proxy metrics are nearly indistinguishable and increase in decision accuracy with compute or CORRECT PROB and TOTAL PROB are flat with respect to scale and the other metrics only rise up to that level of decision accuracy towards the full target compute budget. In the last order of magnitude below the target compute ACCURACY and the other metrics tend to overtake CORRECT PROB and TOTAL PROB, while these two metrics sometimes even decrease in decision accuracy. Notably these other metrics that trend with ACCURACY include continuous metrics that penalize probability assigned to incorrect answers, NORM CORRECT PROB and MARGIN."
        },
        {
            "title": "3.4 How can we make evaluation benchmarks more predictable?",
            "content": "The decision accuracy on task is driven in part by low run-to-run variance and wide spread of performance values for different data recipes. Using CORRECT PROB sees wider spreads or reduced noise for many tasks. Using this metric enables predicting rankings for code tasks that are too hard for accuracy metrics at small scales. What underlies differences in decision accuracy when benchmarks and metrics change? The evaluation must separate pairs of data recipes by an amount greater than combined noise from run-to-run variance of each of the pairs runs. In Figure 5, we plot tasks with given metric using fully trained 150M models over these two characteristics: 1) noisethe standard deviation over 3 random seed runs averaged over all recipes, and 2) spreadthe standard deviation among the mean performance of the different data recipes. Each point also shows the decision accuracy. We see that some highly predictable tasks (e.g., MMLU) are characterized by having low run-to-run noise, while others (e.g., ARC Easy) widely spread the different data recipes. We also see that improvements from using CORRECT PROB often align with improvements in one of these two characteristics. 9 Figure 6: Code tasks such as humaneval and MBPP go from trivial decision accuracy to largely predictable when using using continuous CORRECT PROB instead of discrete ACCURACY. Meanwhile common math tasks remain near trivial decision accuracy regardless of metric. As practical application of these insights, we demonstrate that change of proxy metric makes predictable two code tasks (Austin et al., 2021; Chen et al., 2021) that are otherwise too challenging for our small models. Figure 6 shows how decision accuracy goes from trivial to 80% when using CORRECT PROB. The switch of metric allows small models to get above the noise floor for these tasks, while still predicting large-scale accuracy metrics. Notably, two math benchmarks (Lewkowycz et al., 2022; Cobbe et al., 2021) do not see such benefit. They do however give decision accuracy above 80% if we switch the target metric to CORRECT PROB, raising question for future work to explore whether changing the target metric can be justified."
        },
        {
            "title": "4 Related Work",
            "content": "Prediction Much work studies scaling behavior in language models. Initially this focused on predicting LM loss from scale as determined by parameter count and tokens trained (Kaplan et al., 2020; Hoffmann et al., 2022). Special consideration is also given to the case of data constrained scaling (Muennighoff et al., 2023; Goyal et al., 2024). Unlike predicting loss, predicting downstream performance from scale is generally harder (Schaeffer et al., 2024). However, recent work has demonstrated it can be done based on two step prediction that chains together predictions from scale to loss and loss to downstream performance (Gadre et al., 2024; Bhagia et al., 2024; Dubey et al., 2024), sometimes using training loss (Du et al., 2024) or transferring losses from different data recipes (Brandfonbrener et al., 2024; Ruan et al., 2024). The one line of work targeting pretraining data considers the special case of deciding mixing proportions of several data sources optimized through scaling laws (Kang et al., 2024; Ye et al., 2024). Most relevant to our work, Choshen et al. (2024) consider practical methods for better scaling prediction error such as how much compute to use or whether to include intermediate checkpoints. Orthogonally to these findings, we propose way to assess the accuracy of decisions made with such predictions. Suites over Data Differences DATADECIDE follows in the footsteps of the Pythia Suite (Biderman et al., 2023) which was the first to offer controlled comparison of 2 data recipes, using compute scales up to 2 1022 FLOPs. Subsequent suites have offered 6 data recipes at 9 1020 scale (Magnusson et al., 2024) and 6 data recipes over range of scales up to 1021 (Brandfonbrener et al., 2024). Our DATADECIDE offers range of 14 scales up to 7 1020 FLOPs, while including an order of magnitude more fine-grained data differences. Meanwhile, DCLM also makes extensive use of ranking single scale experiments to drive improvement in data recipes (Li et al., 2024). They release their best data and model trained on it, but do not release models from their decision making experiments and do not search over multiple recipes at their largest scale. Where their goal is creating proposed 10 best recipe, our DATADECIDE enables the assessment of whether method for decision making really does find the best among proposed recipes."
        },
        {
            "title": "5 Limitations",
            "content": "The scope of our work is limited to just one ratio of tokens to parameters, 100 or 5 Chinchilla optimal ratio (Hoffmann et al., 2022). We believe this captures the typical case, as most models now favor overtraining for inference savings. Due to compute limitations and the need for standardized set of model configurations over long period of time in which compute became available for pretraining, we opt for 14 specific configurations from 4M1B parameter scale. While observations across more configurations would always be better, this must be traded off with exploring the other dimensions of data recipes and random seed reruns. Likewise, while our 25 data recipes is an order of magnitude more than previous suites, there is always the possibility that findings across these will not be representative of future data recipes. In our evaluations we focus on multiple choice tasks with cloze formulation as we find these to be good fit for our range of scales. Using DATADECIDE, new evaluations can be assessed easily by others without any additional pretraining."
        },
        {
            "title": "Acknowledgments",
            "content": "We would like to thank Dave Wadden, Kyle Lo, Valentin Hofmann, and Hannaneh Hajishirzi for fruitful conversations. This material is based upon work supported by the U.S. National Science Foundation under Grant No. 2313998. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the U.S. National Science Foundation. IM is supported by the NSF CSGrad4US Fellowship. PWK is supported by the Singapore National Research Foundation and the National AI Group in the Singapore Ministry of Digital Development and Information under the AI Visiting Professorship Programme (award number AIVP-2024-001) and by the AI2050 program at Schmidt Sciences."
        },
        {
            "title": "Ethics Statement",
            "content": "Training large language models is computationally expensive, especially when investigating thoroughly over dimensions of pretraining data composition, model scale, random initialization, and data order. The pretraining experiments in our DATADECIDE required approximately 820K H100 GPU hours. We share the benefit of this cost through releasing all of our models, data, and evaluations so that others will not have to repeat this expenditure. Moreover, our findings can guide efficient and cost-effective model development through the application of decision making with small-scale experiments. While DATADECIDE does not present direct ethical concerns beyond opportunity cost, we acknowledge that decisions about pretraining data heavily impact downstream model behavior. We encourage future research to explore potential biases in data selection methods and their implications for models deployed in the real world."
        },
        {
            "title": "References",
            "content": "Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Loubna Ben Allal, Anton Lozhkov, Guilherme Penedo, Thomas Wolf, and Leandro von Werra. Smollm-corpus, 2024. URL https://huggingface.co/datasets/HuggingFaceTB/ smollm-corpus. Akshita Bhagia, Jiacheng Liu, Alexander Wettig, David Heineman, Oyvind Tafjord, Ananya Harsh Jha, Luca Soldaini, Noah A. Smith, Dirk Groeneveld, Pang Wei Koh, Jesse 11 Dodge, and Hannaneh Hajishirzi. Establishing task scaling laws via compute-efficient model ladders, 2024. URL https://arxiv.org/abs/2412.04403. Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. Pythia: suite for analyzing large language models across training and scaling, 2023. URL https://arxiv. org/abs/2304.01373. Yonatan Bisk, Rowan Zellers, Ronan Le bras, Jianfeng Gao, and Yejin Choi. PIQA: Reasoning about physical commonsense in natural language. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):74327439, Apr. 2020. doi: 10.1609/aaai.v34i05.6239. URL https://ojs.aaai.org/index.php/AAAI/article/view/6239. David Brandfonbrener, Nikhil Anand, Nikhil Vyas, Eran Malach, and Sham Kakade. Lossto-loss prediction: Scaling laws for all datasets, 2024. URL https://arxiv.org/abs/2411. 12925. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021. URL https://arxiv.org/abs/2107.03374. Leshem Choshen, Yang Zhang, and Jacob Andreas. hitchhikers guide to scaling law estimation, 2024. URL https://arxiv.org/abs/2410.11840. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. pp. 29242936, Minneapolis, Minnesota, June 2019. doi: 10.18653/v1/N19-1300. URL N19-1300. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. ArXiv, 2018. URL http://arxiv.org/abs/1803.05457. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Zhengxiao Du, Aohan Zeng, Yuxiao Dong, and Jie Tang. Understanding emergent abilities of language models from the loss perspective. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id= 35DAviqMFo. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony S. Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozi`ere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Cant on Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, 12 Ehab A. AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriele Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guanglong Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Laurens Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Ju-Qing Jia, Kalyan Vasuden Alwala, K. Upasani, Kate Plawiak, Keqian Li, Ken-591 neth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren RantalaYeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Babu Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melissa Hall Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri S. Chatterji, Olivier Duchenne, Onur cCelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Chandra Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whit ney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yiqian Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zhengxu Yan, Zhengxing Chen, Zoe Papakipos, Aaditya K. Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adi Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Ben Leonhardi, Po-Yao (Bernie) Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, Shang-Wen Li, Danny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, Francisco Guzman, Frank J. Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory G. Sizov, Guangyi Zhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang, Han Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Igor Molybog, Igor Tufanov, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kaixing(Kai) Wu, KamHou, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin 13 Chen, Lakshya Garg, Lavender, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikolay Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shiva Shankar, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Sung-Bae Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Andrei Poenaru, Vlad T. Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xia Tang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu Wang, Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, and Zhiwei Zhao. The llama 3 herd of models. ArXiv, abs/2407.21783, 2024. URL https://api.semanticscholar.org/CorpusID:271571434. Samir Yitzhak Gadre, Georgios Smyrnis, Vaishaal Shankar, Suchin Gururangan, Mitchell Wortsman, Rulin Shao, Jean Mercat, Alex Fang, Jeffrey Li, Sedrick Keh, Rui Xin, Marianna Nezhurina, Igor Vasiljevic, Jenia Jitsev, Luca Soldaini, Alexandros G. Dimakis, Gabriel Ilharco, Pang Wei Koh, Shuran Song, Thomas Kollar, Yair Carmon, Achal Dave, Reinhard Heckel, Niklas Muennighoff, and Ludwig Schmidt. Language models scale reliably with over-training and on downstream tasks, 2024. URL https://arxiv.org/abs/2403.08540. Sachin Goyal, Pratyush Maini, Zachary C. Lipton, Aditi Raghunathan, and J. Zico Kolter. Scaling laws for data filtering - data curation cannot be compute agnostic. CoRR, abs/2404.07177, 2024. doi: 10.48550/ARXIV.2404.07177. URL https://doi.org/10. 48550/arXiv.2404.07177. Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, and Hannaneh Hajishirzi. Olmo: Accelerating the science of language models, 2024. URL https://arxiv.org/abs/2402.00838. Yuling Gu, Oyvind Tafjord, Bailey Kuehl, Dany Haddad, Jesse Dodge, and Hannaneh Hajishirzi. Olmes: standard for language model evaluations, 2024. URL https://arxiv. org/abs/2406.08446. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021. 14 Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models, 2022. URL https://arxiv.org/abs/2203.15556. Feiyang Kang, Yifan Sun, Bingbing Wen, Si Chen, Dawn Song, Rafid Mahmood, and Ruoxi Jia. Autoscale: Automatic prediction of compute-optimal data composition for training llms. ArXiv, abs/2407.20177, 2024. URL https://api.semanticscholar.org/CorpusID: 271533897. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020. URL https://arxiv.org/abs/2001.08361. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. ulu 3: Pushing frontiers in open language model post-training. 2024. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models, 2022. URL https://arxiv.org/abs/2206.14858. Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas Muennighoff, Reinhard Heckel, Jean Mercat, Mayee Chen, Suchin Gururangan, Mitchell Wortsman, Alon Albalak, Yonatan Bitton, Marianna Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba Ghosh, Josh Gardner, Maciej Kilian, Hanlin Zhang, Rulin Shao, Sarah Pratt, Sunny Sanyal, Gabriel Ilharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan, Jieyu Zhang, Khyathi Chandu, Thao Nguyen, Igor Vasiljevic, Sham Kakade, Shuran Song, Sujay Sanghavi, Fartash Faghri, Sewoong Oh, Luke Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari, Alexander Toshev, Stephanie Wang, Dirk Groeneveld, Luca Soldaini, Pang Wei Koh, Jenia Jitsev, Thomas Kollar, Alexandros G. Dimakis, Yair Carmon, Achal Dave, Ludwig Schmidt, and Vaishaal Shankar. Datacomp-lm: In search of the next generation of training sets for language models, 2024. URL https://arxiv.org/abs/2406. 11794. Ian Magnusson, Akshita Bhagia, Valentin Hofmann, Luca Soldaini, Ananya Harsh Jha, Oyvind Tafjord, Dustin Schwenk, Evan Pete Walsh, Yanai Elazar, Kyle Lo, Dirk Groeneveld, Iz Beltagy, Hannaneh Hajishirzi, Noah A. Smith, Kyle Richardson, and Jesse Dodge. Paloma: benchmark for evaluating language model fit, 2024. URL https: //arxiv.org/abs/2312.10523. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. pp. 23812391, Brussels, Belgium, October-November 2018. doi: 10.18653/v1/D18-1260. URL D18-1260. Niklas Muennighoff, Alexander Rush, Boaz Barak, Teven Le Scao, Nouamane Tazi, Aleksandra Piktus, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language models. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 5035850376. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper files/paper/ 2023/file/9d89448b63ce1e2e8dc7af72c984c196-Paper-Conference.pdf. Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, Nathan Lambert, Dustin Schwenk, Oyvind Tafjord, Taira Anderson, David Atkinson, Faeze Brahman, Christopher Clark, 15 Pradeep Dasigi, Nouha Dziri, Michal Guerquin, Hamish Ivison, Pang Wei Koh, Jiacheng Liu, Saumya Malik, William Merrill, Lester James V. Miranda, Jacob Morrison, Tyler Murray, Crystal Nam, Valentina Pyatkin, Aman Rangapur, Michael Schmitz, Sam Skjonsberg, David Wadden, Christopher Wilhelm, Michael Wilson, Luke Zettlemoyer, Ali Farhadi, Noah A. Smith, and Hannaneh Hajishirzi. 2 olmo 2 furious, 2025. URL https://arxiv.org/abs/2501.00656. Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra-Aimee Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: Outperforming curated corpora with web data, and web data only. ArXiv, abs/2306.01116, 2023. URL https://api.semanticscholar. org/CorpusID:259063761. Guilherme Penedo, Hynek Kydlıˇcek, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf. The fineweb datasets: Decanting the web for the finest text data at scale, 2024. URL https://arxiv.org/abs/2406.17557. Tomer Porian, Mitchell Wortsman, Jenia Jitsev, Ludwig Schmidt, and Yair Carmon. Resolving discrepancies in compute-optimal scaling of language models. ArXiv, abs/2406.19146, 2024. URL https://api.semanticscholar.org/CorpusID:270764838. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. arXiv e-prints, 2019. Yangjun Ruan, Chris J. Maddison, and Tatsunori Hashimoto. Observational scaling laws and the predictability of langauge model performance. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id= On5WIN7xyD. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: An adversarial winograd schema challenge at scale. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):87328740, Apr. 2020. doi: 10.1609/aaai.v34i05.6399. URL https://ojs.aaai.org/index.php/AAAI/article/view/6399. Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social IQa: Commonsense reasoning about social interactions. pp. 44634473, Hong Kong, China, November 2019. doi: 10.18653/v1/D19-1454. URL D19-1454. Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. Are emergent abilities of large language models mirage?, 2023. URL https://arxiv.org/abs/2304.15004. Rylan Schaeffer, Hailey Schoelkopf, Brando Miranda, Gabriel Mukobi, Varun Madan, Adam Ibrahim, Herbie Bradley, Stella Biderman, and Sanmi Koyejo. Why has predicting downstream capabilities of frontier AI models with scale remained elusive? In Trustworthy Multi-modal Foundation Models and AI Agents (TiFA), 2024. URL https: //openreview.net/forum?id=AbHHrj9afB. Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A. Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo. Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research. arXiv preprint, 2024. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: question answering challenge targeting commonsense knowledge. pp. 41494158, Minneapolis, Minnesota, June 2019. doi: 10.18653/v1/N19-1421. URL N19-1421. 16 Model name Batch size Hidden dim. LR Model size Heads Layers Training 32 4M 32 6M 32 8M 32 10M 32 14M 32 16M 64 20M 96 60M 90M 160 150M 192 300M 320 530M 448 750M 576 704 1B 64 96 128 144 192 208 192 384 528 768 1,024 1,344 1,536 2, 1.4e-02 1.2e-02 1.1e-02 1.0e-02 9.2e-03 8.9e-03 8.4e-03 5.8e-03 4.9e-03 4.2e-03 3.3e-03 2.8e-03 2.5e-03 2.1e-03 3.7M 8 6.0M 8 8.5M 8 9.9M 8 14.4M 8 16.0M 8 19.1M 8 57.1M 12 97.9M 12 151.9M 12 320.0M 16 530.1M 16 681.3M 16 1176.8M 16 8 8 8 8 8 8 16 16 16 12 16 16 16 16 steps 5,725 9,182 13,039 15,117 21,953 24,432 14,584 29,042 29,901 38,157 45,787 57,786 63,589 69,369 Tokens trained 0.4B 0.6B 0.9B 1.0B 1.4B 1.6B 1.9B 5.7B 9.8B 15.0B 30.0B 53.0B 75.0B 100.0B Table 2: DATADECIDE uses OLMos model ladder (Groeneveld et al., 2024; OLMo et al., 2025; Bhagia et al., 2024) to programmatically create configurations for 14 model sizes with hyperparameters determined by heuristics in Porian et al. (2024). All models have sequence length of 2024 and MLP ratio of 8. Each configuration is pretrained over 25 data recipes  (Table 1)  . Each recipe and configuration is also trained for 3 random seeds where model sizes < 1B are stopped early at 25% of the compute used to train the 1B model for all but the default seed. Model size is number of non-embedding parameters. Batch size is the number of sequences per batch. Jiasheng Ye, Peiju Liu, Tianxiang Sun, Yunhua Zhou, Jun Zhan, and Xipeng Qiu. Data mixing laws: Optimizing data mixtures by predicting language modeling performance. ArXiv, abs/2403.16952, 2024. URL https://api.semanticscholar.org/CorpusID:268681464. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can machine really finish your sentence? pp. 47914800, Florence, Italy, July 2019. doi: 10.18653/v1/P19-1472. URL P19-1472. Fan Zhou, Zengzhi Wang, Qian Liu, Junlong Li, and Pengfei Liu. Programming every example: Lifting pre-training data quality like experts at scale. arXiv preprint arXiv:2409.17115, 2024."
        },
        {
            "title": "A Hyperparameters",
            "content": "Table 2 provides OLMo model ladder configurations for all models in DATADECIDE."
        },
        {
            "title": "B Proxy Metric Definitions",
            "content": "Table 3 provides formal definitions for our proxy metrics (2.5)."
        },
        {
            "title": "C Scaling Law Variants",
            "content": "Baseline 3-parameter fit. Our default setup (described in 2.2) follows the two-step fit from (Bhagia et al., 2024) and uses Equation 1 to map compute to task loss L, and Equation 2 to map task loss to metric score. This variant fits three parameters (A, α, E) in the first step. 2-parameter fit. This is restricted version of the baseline where the irreducible loss term is removed from Equation 1, leaving only two parameters: L(C) = Cα 17 (4) N i=1 P(c (cid:0)P(c i=1 (i) correct contexti) (i) correct contexti) max correctC(i) P(c contexti)(cid:1) (i) c=c Metric Name Equation 1 1 1 1 CORRECT PROB MARGIN NORM CORRECT PROB TOTAL PROB ACCURACY per token per char i=1 i=1 (i) P(c correctcontexti) cC(i) P(ccontexti) cC(i) P(c contexti) I(cid:0) arg maxcC(i) P(c contexti) = (i) correct (cid:1) i=1 1 P(ccontext)/tokens(c) P(ccontext)/chars(c) Table 3: Proxy metrics used as alternative inputs to our prediction methods, C(i) is the set of possible continuations for item and is the number of items in benchmark. Each each of the first 5 metrics have * per token and * per char variants in which likelihoods are normalized as defined in the bottom two rows. Scaling Law Variant 3-parameter with helpers and >50% checkpoints 3-parameter with helper points 3-parameter step 2 fit with >50% checkpoints 3-parameter 2-parameter 5-parameter, single step 3-parameter, single step 5-parameter Relative Error Absolute Error 5.6 6.0 5.9 6.5 6.5 42.8 42.9 230.8 2.6 2.8 2.9 3.1 3.2 17.4 42.3 65.4 Table 4: Average prediction error for 1B targets for the different scaling law setups across tasks and recipes on ACCURACY fit to all models but 1B. We see that other than the single step and 5-parameter variants errors are comparable, and these variants also roughly follow the compute-decision frontier in Figure 3. 5-parameter (N, D) fit. Instead of modeling loss as function of compute C, this variant uses both number of tokens and number of parameters directly in the loss function: L(N, D) = Nα + Dβ + (5) This introduces five parameters: A, α, B, β, and E. Single-step prediction. In this variant, the two-stage fitting procedure is replaced with single step that directly maps compute to accuracy: Acc(C) = 1 + exp (cid:16) (cid:16) Cα + L0 (cid:17)(cid:17) + (6) This combines the loss and accuracy mapping into one function. 5-parameter, single step. We also test single-step variant that directly maps from (N, D) to accuracy using logistic function over the predicted loss. This merges Equations 5 and 2 into: Acc(N, D) = 1 + exp (cid:16) (cid:16) Nα + Dβ + (cid:17)(cid:17) + (7) This formulation retains the same five parameters from the two-step (N, D) loss function. Following Bhagia et al. (2024), we merge the parameters and L0 from the second-stage sigmoid into the loss-side parameters (A, B, E), yielding simplified single-stage fit with 7 total free parameters: {A, α, B, β, E, a, b}. Use of helper points. Following Bhagia et al. (2024), we optionally include an extra point (L = 0.0, Acc = 1.0) in the second-stage fit. This helper point anchors the upper asymptote of the accuracy prediction. Filtering early checkpoints. We experiment with excluding the first 50% of intermediate checkpoints when fitting the second-stage sigmoid. This reduces noise from high-loss early training points and often improves the fit for extrapolation. Helpers and > 50% checkpoints. Lastly we experiment with combining the previous two techniques on the baseline 3-parameter fit. Prediction Error. We report prediction errors in Table 4 for each setup. As the best scaling laws variants are all roughly comparable to the simple 3-parameter set up, we use this one as our baseline."
        }
    ],
    "affiliations": [
        "Allen Institute for AI",
        "University of Pennsylvania",
        "University of Washington"
    ]
}