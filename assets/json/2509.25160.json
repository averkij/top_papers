{
    "paper_title": "GSM8K-V: Can Vision Language Models Solve Grade School Math Word Problems in Visual Contexts",
    "authors": [
        "Fan Yuan",
        "Yuchen Yan",
        "Yifan Jiang",
        "Haoran Zhao",
        "Tao Feng",
        "Jinyan Chen",
        "Yanwei Lou",
        "Wenqi Zhang",
        "Yongliang Shen",
        "Weiming Lu",
        "Jun Xiao",
        "Yueting Zhuang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision language models (VLMs) achieve unified modeling of images and text, enabling them to accomplish complex real-world tasks through perception, planning, and reasoning. Among these tasks, reasoning is particularly representative, with mathematical reasoning serving as a prominent example. It highlights the high-level capability of VLMs to comprehend mathematical information in images and to perform sophisticated reasoning. Recently, numerous visual mathematical reasoning benchmarks have been proposed, but they are often restricted to geometry, lack coverage of math word problems, and rarely assess reasoning across multiple images. To address these gaps, we introduce GSM8K-V, a purely visual multi-image mathematical reasoning benchmark. GSM8K-V is built by systematically mapping each sample from the widely used text-based GSM8K into visual form. Through a carefully designed automated image-generation pipeline combined with meticulous human annotation, we curate 1,319 high-quality samples. We evaluate a wide range of open-source and closed-source models on GSM8K-V. Results show that although existing VLMs have nearly saturated performance on text-based GSM8K, there remains substantial room for improvement on GSM8K-V. For example, the best-performing model, Gemini-2.5-Pro, achieves 95.22% accuracy on GSM8K but only 46.93% on GSM8K-V. We conduct a comprehensive analysis of GSM8K-V, examining the limitations of current models as well as potential directions for improvement. GSM8K-V offers a new perspective on visual mathematical reasoning and establishes a benchmark to guide the development of more robust and generalizable VLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 0 6 1 5 2 . 9 0 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "GSM8K-V: CAN VISION LANGUAGE MODELS SOLVE GRADE SCHOOL MATH WORD PROBLEMS IN VISUAL CONTEXTS Fan Yuan1,, Yuchen Yan1,, Yifan Jiang1, Haoran Zhao1, Tao Feng1, Wenqi Zhang1, Yongliang Shen1,, Weiming Lu1, 1Zhejiang University yuanfan7777777@gmail.com {yanyuchen,syl}@zju.edu.cn Jinyan Chen1, Yanwei Lou1, Jun Xiao1, Yueting Zhuang1 GitHub: https://github.com/ZJU-REAL/GSM8K-V (cid:128) Project: Ä± Data: https://zju-real.github.io/GSM8K-V https://huggingface.co/datasets/ZJU-REAL/GSM8K-V"
        },
        {
            "title": "ABSTRACT",
            "content": "Vision language models (VLMs) achieve unified modeling of images and text, enabling them to accomplish complex real-world tasks through perception, planning, and reasoning. Among these tasks, reasoning is particularly representative, with mathematical reasoning serving as prominent example. Mathematical reasoning highlights the high-level capability of VLMs to comprehend mathematical information embedded in images and to perform sophisticated reasoning processes. Recently, numerous visual mathematical reasoning benchmarks have been proposed to evaluate the mathematical reasoning capabilities of VLMs. However, these benchmarks suffer from several limitations: they are typically restricted to geometry problems, lack comprehensive evaluation on math word problems, and rarely assess the ability to reason across multiple images. To fill this gap, we introduce GSM8K-V, purely visual multi-image mathematical reasoning benchmark. GSM8K-V is constructed by systematically mapping each sample from the widely used text-based mathematical reasoning benchmark GSM8K into visual form. Through carefully designed automated image-generation pipeline combined with meticulous human annotation, we curate benchmark comprising 1,319 high-quality samples. We evaluate wide range of open-source and closesource models on the proposed GSM8K-V benchmark. Our results reveal that, although existing VLMs have achieved nearly saturated performance on the textbased GSM8K, there remains substantial room for improvement on the purely visual GSM8K-V. For instance, the best-performing model, Gemini-2.5-Pro, attains 95.22% accuracy on GSM8K but only 46.93% on GSM8K-V. We conduct comprehensive and detailed analysis on GSM8K-V, systematically examining the limitations of existing models on this benchmark as well as potential directions for improvement. GSM8K-V provides new perspective on visual mathematical reasoning and establishes novel evaluation benchmark that can guide the research community toward developing more robust and generalizable VLMs."
        },
        {
            "title": "INTRODUCTION",
            "content": "The rapid development of vision language models (VLMs) has led to remarkable achievements, yielding breakthroughs in visual perception, multimodal interaction, and problem solving (OpenAI, 2024; Comanici et al., 2025; Wang et al., 2025b; Team et al., 2025). Among the diverse capabilities * The first two authors have equal contributions. Corresponding author."
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Left: GSM8K-V is constructed by converting the text-based mathematical reasoning benchmark GSM8K, which contains 1,319 math word problems, into comic-style purely visual format for evaluating VLMs under visual contexts. Right: Performance (%) of vision language models on GSM8K-V. of VLMs, one of the most challenging and advanced is their reasoning ability, which reflects the potential of VLMs to tackle complex visual problems. Within the broad spectrum of reasoning tasks, mathematical reasoning serves as highly representative benchmark: it not only evaluates the visual understanding capacity of VLMs but also challenges their abilities in planning, reasoning, and reflection (Wang et al., 2025a; Shi et al., 2024). To better evaluate the mathematical reasoning capability of VLMs, researchers have proposed variety of mathematical reasoning benchmarks, which assess VLMs mathematical competence from multiple dimensions. Representative benchmarks include MathVista (Lu et al., 2023), which aggregates diverse math-vision tasks under unified taxonomy; MathVerse (Zhang et al., 2024), which controls how information is apportioned between diagrams and text; MATHVision (Wang et al., 2024), which extends competition-style problems with visual statements; and MMMATH (Sun et al., 2024), which assesses process quality in addition to final answers. Robustness efforts such as DynaMath (Zou et al., 2024) further show sensitivity to presentation changes. However, existing vision-based mathematical benchmarks exhibit several limitations. First, most benchmarks focus predominantly on geometric problems, where models are required to answer questions based on diagrams. Such narrow task scope restricts the comprehensiveness of evaluating the mathematical capabilities of VLMs. Second, these benchmarks rarely assess math word problems under visual contexts, thus overlooking the evaluation of VLMs in solving real-world problems. Third, current benchmarks lack evaluation settings involving multiple images, especially when semantic dependencies exist across images, which limits the assessment of VLMs ability to capture inter-image relationships. Language serves as high-level abstraction of the real world, providing an efficient way to describe existing entities (Li et al., 2024; Lupyan & Winter, 2018). Motivated by this, we pose the following question: can math word problems expressed in natural language be transformed into purely visual representations consisting of multiple images, thereby enabling more realistic evaluation of the mathematical reasoning ability of VLMs? Specifically, we construct new vision-based benchmark for mathematical reasoning, GSM8K-V, by systematically mapping each case in the widely used math word problem benchmark GSM8K (Cobbe et al., 2021) into its visual counterpart. GSM8K-V preserves the exact set of problems as GSM8K, differing only in that the problem statements are expressed in visual rather than textual form. The introduction of GSM8K-V effectively alleviates the limitations of existing benchmarks in assessing the mathematical reasoning capacity of VLMs. To construct GSM8K-V, we design an automated pipeline that systematically transforms each textbased problem into multi-scene visual form. The pipeline consists of three stages. First, we extract mathematical information from GSM8K samples, apply classification and introduce distractors, and then allocate the extracted information across different scenes to provide the foundation for generating accurate images. Next, we employ carefully designed prompts to generate textual descriptions for each scene. Finally, we use an image generation model to produce the corresponding visual con-"
        },
        {
            "title": "Preprint",
            "content": "tent for each scene based on the generated descriptions. During this process, we conduct meticulous human annotation to ensure the accuracy and reliability of our benchmark. In our constructed GSM8K-V, we conducted comprehensive evaluation of multiple open-source and closed-source models. We found that the performance of existing models on GSM8K-V remains unsatisfactory: the best-performing model, Gemini-2.5-Pro, achieved only an accuracy of 46.93%. In contrast, these models consistently reach much higher performance (80%90%) on the text-based GSM8K benchmarks. This discrepancy demonstrates that current VLMs still face significant challenges in understanding and reasoning over images when solving mathematical problems grounded in real-world scenarios. In this paper, we provide detailed analysis of the underlying causes of such underperformance, offering valuable insights for researchers to advance the mathematical reasoning capabilities of future VLMs. GSM8K-V enables clean, within-item comparison across modalities: models can be evaluated on the original text question and on the visualized version of the same question, allowing us to isolate modality effects on mathematical reasoning. Our benchmark complements prior visual-math evaluations that synthesize broad taxonomies of skills and sources (Lu et al., 2023), and it is orthogonal to recent work that escalates the difficulty of textual mathematics to Olympiad level (Gao et al., 2024). In contrast, GSM8K-V focuses on real-world applications and tests whether LVMs achieve parity with LLMs when equivalent information is conveyed visually. Our main contributions are summarized as follows. We propose an automated framework that converts text-based math word problems into visual form. Specifically, we construct detailed multi-scene textual descriptions and leverage image generation models to produce corresponding visual representations. Building on the proposed data construction framework and careful human annotation, we introduce vision-based mathematical reasoning benchmark GSM8K-V, which enables the evaluation of VLMs on more realistic mathematical problem-solving scenarios. We perform thorough evaluation and analysis of the existing VLMs in GSM8K-V. The results reveal substantial room for improvement, and our analysis provides valuable insights for enhancing the mathematical reasoning capabilities of future VLMs."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "2.1 MATHEMATICAL REASONING OF VISION LANGUAGE MODELS Reasoning ability is core dimension of intelligence, and in multi-modal settings it provides stringent lens for evaluating higher-order cognition. VisionLanguage Models (VLMs) such as GPT-4o (OpenAI, 2024), Gemini-2.5 Pro (Comanici et al., 2025), Qwen2.5-VL (Bai et al., 2025), GLM4.5V (Team et al., 2025), LLaMA 4 (Meta AI, 2024), Ovis 2.5 (Lu et al., 2025), and MiniCPM-V (Yao et al., 2024) exemplify the rapid progress of multimodal systems. Nevertheless, their capacity for mathematical reasoning remains limited: unlike perception-focused tasks, it requires grounding symbolic relations in visual contexts, integrating multimodal information, and performing multi-step logical inference (Hendrycks et al., 2021; Cobbe et al., 2021). To address this, recent post-training paradigms emphasize reasoning-specific supervision. On the SFT side, long chain-of-thought annotations and interleaving visual cues into reasoning steps yield consistent gains (Zhang et al., 2025b; Wang et al., 2025d). On the RL side, Group-Relative Policy Optimization (GRPO) has become central recipe for math and coding reasoning (Shao et al., 2024; Guo et al., 2025), with refinements such as S-GRPO and stable RL addressing overthinking and token efficiency (Dai et al., 2025a;b), while RL with Verifiable Rewards achieves strong math gains even in low-shot settings (Wang et al., 2025c). Surveys further consolidate these trends for multimodal reasoning (Zhang et al., 2025a; Zhou et al., 2025). Despite these advances, existing evaluations under-probe multi-image, visually grounded word problems, motivating benchmarks like GSM8K-V that isolate modality effects and test whether VLMs can achieve parity with text-only LLMs under equivalent information."
        },
        {
            "title": "Preprint",
            "content": "Figure 2: An overview of our pipeline that converts the text-based mathematical reasoning dataset GSM8K into its purely visual version, GSM8K-V. 2.2 MATHEMATICAL BENCHMARKS FOR VISION LANGUAGE MODELS To evaluate mathematical reasoning in visual contexts, variety of vision-based benchmarks have been proposed. Early efforts targeted specific domains such as geometry or multiple-choice reasoning (Lu et al., 2021; Chen et al., 2021), while more recent benchmarks have expanded scope and methodology. MathVista (Lu et al., 2023) aggregates diverse math-vision tasks under unified taxonomy, MATH-Vision (Wang et al., 2024) compiles competition-level problems with visual contexts, MathVerse (Zhang et al., 2024) redistributes information between text and diagrams to probe genuine visual understanding, and MM-MATH (Sun et al., 2024) emphasizes process evaluation, revealing diagram misinterpretation as major failure source. Beyond static evaluation, DynaMath (Zou et al., 2024) introduces perturbed variants to test robustness, while Omni-MATH (Gao et al., 2024) escalates difficulty with Olympiad-level text problems. Most recently, VisuLogic (Xu et al., 2025) further broadens coverage by targeting systematic visual reasoning across multiple domains of multimodal tasks. In contrast, our proposed GSM8K-V isolates modality effects by rendering the GSM8K word problems into visual form while preserving their distribution, enabling direct within-problem comparisons and focusing on everyday application-style math rather than narrow or ultra-hard settings."
        },
        {
            "title": "3 BENCHMARK CONSTRUCTION",
            "content": "In this paper, we propose GSM8K-V, comic-style, purely visual reasoning benchmark derived by systematically converting the textual reasoning benchmark GSM8K into visual form. The overall pipeline is illustrated in Figure 2. Our core idea is to transform each textual problem into multiple visually expressed scenes. Specifically, the pipeline consists of three steps. For each sample in GSM8K, we first decompose its mathematical information, introduce controlled interference, and allocate them across different scenes (Section 3.1). Next, we design prompts according to the problem category to generate scene descriptions (Section 3.2). Finally, we employ an image generation model to produce the corresponding scene images, which are further validated and refined through human annotation. The resulting dataset constitutes the GSM8K-V benchmark (Section 3.3). 3.1 STEP 1: PROBLEM DECOMPOSITION AND ALLOCATION Math information decomposition. Each sample in GSM8K contains multiple pieces of mathematical information. To ensure that all details are faithfully preserved during image genera-"
        },
        {
            "title": "Preprint",
            "content": "tion, we first decompose textual problems into structured mathematical representations. Specifically, we employ GPT-4.1 (OpenAI, 2025a) to parse each instance into set of triples (object, math value, semantic). Here, object denotes the entity described in the problem, math value specifies its associated numerical attribute, and semantic captures the contextual role of the information. This structured representation provides systematic foundation for subsequent processing. Math information classification. To better represent the mathematical information of the original problems in the generated images, we categorize the mathematical elements contained in GSM8K into 13 typical classes (details in Appendix C.1). These 13 classes are rendered in distinct ways within the images, ensuring that the visualizations preserve the mathematical information while aligning more naturally with real-world scenarios. Scene allocation. Since single mathematical problem often contains multiple pieces of mathematical information, we allocate them across different scenes such that each scene contains only 1-2 pieces of information. This allocation follows three guiding principles: Contextual grouping: Information that refers to the same object type, spatial setting, or functional purpose is grouped into one scene. Final isolation: The problem question itself is always reserved for the last scene, which contains no additional numerical values or objects, serving as the reasoning target. Atomic fidelity: No new values inferred or merged. Only atomic math facts extracted in Step 1 are retained, ensuring that reasoning is not prematurely performed during scene construction. We employ GPT-4.1 to perform this allocation of mathematical information, with further details provided in Appendix C.2). Multi-dimensional interference. To further increase task difficulty and better probe the reasoning ability of VLMs, controlled interference is introduced during scene construction. perception interference enriches the visual environment by adding irrelevant but visually salient objects, while semantic interference introduces distractors that are contextually close to the mathematical problem. These strategies enhance the robustness of the benchmark and create stronger reasoning challenges (details in Appendix D.4). 3.2 STEP 2: SCENE DESCRIPTION GENERATION Meta description strategy definition. To enhance the quality of image generation, we first construct high-quality scene descriptions in textual form. Based on the mathematical information introduced in Section 3.1, we design series of meta description strategies for each type of mathematical information. Each mathematical information category is mapped to corresponding meta description strategy, defined through pre-specified prompts (e.g. time & clock, percentage, measurement, sign board & icon). These strategies provide high-level templates that ensure the mathematical content is faithfully represented in the visual modality. The complete set of templates is documented in Appendix D.2. Scene description generation. Based on the selected meta strategy, we use GPT-4.1 to produce structured scene description for each allocated unit of information. Each description follows tripartite schema (object, action, composition): Object: the concrete entities that must appear in the image and carry mathematical information. Action: the state or activity that defines how an object is presented and conveys semantic cues. Composition: the spatial arrangement or positional relations among all elements. This structured representation enforces consistency across scenes and provides explicit guidance for subsequent image generation. 3.3 STEP 3: IMAGE GENERATION Multi-scene image generation. With the generated structured scene descriptions in Section 3.2, we generate multi-scene images with the GPT-Image-1 model (OpenAI, 2025b). The specific gener-"
        },
        {
            "title": "Preprint",
            "content": "ation prompts for each category are provided in Appendix D.3, ensuring reproducibility of the visual pipeline. For each scene, we generate 1024 1024 pixel image. Human cross check and refinement. To ensure benchmark reliability, all generated images undergo dual human cross-validation process conducted by fully trained annotators. This iterative procedure is guided by three principles: Consistency: visual scenes must faithfully preserve the entities, quantities, and constraints specified in the original text, ensuring semantic equivalence across modalities. Completeness: all information necessary to solve the problem must be visually accessible, without requiring additional assumptions beyond what is explicitly stated. Compliance: images must adhere to safety and formatting standards, including the exclusion of sensitive content, clarity of object identities, and legibility of numerical symbols. Whenever violations are identified, the images are refined; in severe cases, the underlying scene descriptions are manually corrected. Detailed protocols of the dual cross-checking process are provided in Appendix C.3. Table 1: Benchmark statistics. Statistic Total samples Total categories Total sub-categories Answer type Total images Average images Maximum images Minimum images Value 1,319 6 13 Integer 5,343 4.05 11 2 Figure 3: Categogy distribution. Benchmark construction. This iterative generationverification loop is repeated until every problemimage pair meets the established requirements. The final outcome is GSM8K-V, benchmark that preserves the semantic fidelity of GSM8K while offering systematically constructed visual counterparts, thereby enabling rigorous evaluation of mathematical reasoning in multimodal models. With GSM8K-V benchmarks, we are enabled with instance-level cross-modal evaluation against GSM8K text counterparts. We present the statistics of GSM8K-V in Table 1 and the distribution of categories in Figure 3."
        },
        {
            "title": "4 EVALUATIONS RESULTS",
            "content": "4.1 EVALUATION SETUP We evaluate broad range of both open-source and closed-source VLMs on the GSM8K benchmark. The closed-source models include Gemini-2.5-Pro (Comanici et al., 2025), GPT-5 (OpenAI, 2024), GPT-4o (OpenAI, 2024), and QVQ-Max-Latest (Qwen Team, 2025), while the opensource models cover the Llama-4 series (Meta AI, 2024), InternVL3.5 series (Wang et al., 2025b), Qwen2.5-VL series (Bai et al., 2025), Ovis2.5 series (Lu et al., 2025), as well as Step-3 (StepFun et al., 2025), Kimi-VL (Du et al., 2025), MiniCPM (Yao et al., 2024), and GLM-4.5V (Team et al., 2025), which represent diverse set of strong VLMs. For closed-source models, evaluation is conducted through their official APIs, whereas for open-source models, inference is performed using the vLLM framework (Kwon et al., 2023). Details of the hyperparameter configurations for each model, ensuring consistency and reproducibility, are provided in Appendix B. We further conduct evaluations on both the text-only GSM8K benchmark and our visual counterpart, GSM8K-V. For GSM8K-V, models are instructed to reason step by step over the images and report only the final numeric answer in the required format (integer, decimal, or fraction). Comprehensive definitions and representative prompt templates are included in Appendix B.2. In addition, during"
        },
        {
            "title": "Preprint",
            "content": "Table 2: Performance (%) comparison of vision language models across six categories and overall average accuracy on GSM8k-V. The rightmost column shows the performance in text-only GSM8K. Model GSM8K-V (Visual) GSM8K (Text) Signboard & Icon Measurement Physical Metric Temporal Ratio & Percentage Other Avg. Avg. Human 90.27 86.76 96.05 91.20 88.57 93.25 91. - Closed-Source VLMs Gemini-2.5-Pro GPT-5 GPT-4o QVQ-Max-Latest Open-Source VLMs Llama-4-17B-16E-Instruct Llama-4-17B-128E-Instruct InternVL3.5-8B InternVL3.5-38B InternVL3.5-30B-A3B InternVL3.5-241B-A28B Qwen2.5-VL-7B-Instruct Qwen2.5-VL-32B-Instruct Qwen2.5-VL-72B-Instruct Ovis2.5-2B Ovis2.5-9B Step3 Kimi-VL-A3B-Thinking-2506 MiniCPM-V 4.5 GLM-4.5V 52.94 41.54 41.91 30. 36.76 42.28 16.91 36.03 29.41 36.76 24.26 30.51 28.31 19.12 37.13 34.19 17.28 26.10 34.93 52.43 46.60 42.72 39.81 36.89 41.75 24.27 38.84 37.86 46.60 24.27 38.83 36.89 27.18 38.83 34.95 20.39 29.13 42.72 56.30 26.89 31.93 21.85 47.57 28.65 28.11 25.95 30.25 28.57 10.92 31.09 21.85 28.57 17.65 18.49 21.85 15.13 31.09 33.61 10.92 18.49 26. 25.41 34.59 8.65 25.41 25.41 27.03 11.35 20.54 24.86 13.51 24.86 33.51 6.49 18.92 31.35 42.05 21.91 21.91 16.25 19.79 22.61 5.30 17.67 13.43 21.20 7.07 13.07 13.43 6.01 18.02 15.19 6.71 12.01 22.26 41.18 46.93 24.93 30.10 22.41 29.57 17.37 23.20 21.01 26.69 27.45 31.69 7.56 10.77 22.97 26.84 19.89 22.82 21.29 27.90 13.17 15.16 18.77 21.76 17.93 21.91 10.64 13.50 24.37 27.45 18.77 25.85 11.48 11.60 18.49 19.56 22.97 28.28 95.22 95.00 94.92 95. 94.16 95.30 90.67 93.86 93.18 95.22 78.32 88.70 94.09 86.28 94.16 83.09 84.15 90.83 90.60 the human annotation process, we instructed annotators to provide answers directly based on the generated images, and then computed the overall human accuracy on GSM8K-V. 4.2 OVERALL PERFORMANCE We present our main evaluation results in Table 2, from which we derive several key observations: GSM8K-V is challenging visual benchmark. After conducting extensive evaluations of various VLMs on GSM8K-V, we observed that current models exhibit suboptimal performance. The bestperforming model, Gemini-2.5-Pro, achieved only 46.93% accuracy, while other modelsincluding flagship ones such as GPT-5 and Llama-4-17B-128Ereached merely around 30%. This highlights that existing VLMs still face significant challenges in solving real-world mathematical reasoning problems, particularly in multi-image understanding and reasoning. GSM8K-V thus provides sustainable and reliable benchmark for future research aimed at advancing the mathematical reasoning capabilities of VLMs. Near-saturated textual reasoning, but substantial room for visual reasoning. As shown in Table 2, even the weakest model (Qwen2.5-VL-7B-Instruct) on text-only GSM8K still achieves 78.32%, and the vast majority of VLMs (14 out of 19) exceed 90% accuracy. In sharp contrast, the visual setting of GSM8K-V reveals dramatic performance drop: the best closed-source model, Gemini-2.5-Pro, achieves only 46.93%, while the next best, the open-source Llama-4-17B-128EInstruct, falls to 31.69%. Across all evaluated VLMs, only 3 models surpass 30% in the visual"
        },
        {
            "title": "Preprint",
            "content": "setting, compared to near-universal success in the text-only case. This stark discrepancy highlights that while textual reasoning on GSM8K is nearly saturated, visual reasoning in GSM8K-V remains an unsolved and substantially more challenging problem. Human-VLM differences in visual reasoning. Humans achieve far higher average accuracy (91.15%) than all VLMs, with the best closed-source system (Gemini-2.5-Pro) reaching 46.93% and the strongest open-source model (Llama-4-17B-128E-Instruct) remaining below 31.69%. In addition, humans maintain uniformly high performance across categories (86.7696.05%), while VLMs show pronounced variability. For example, GPT-5 performs relatively well on Signboard & Icon (41.54%) and Measurement (46.60%) but drops sharply on Other (24.93%), and InternVL3.5-241BA28B shows similar imbalance (46.60% vs. 21.29%). Moreover, different VLMs exhibit divergent strengths: the open-source Step3 model performs competitively on Physical Metric (33.61%) and Temporal (33.51%), ranking just behind Gemini-2.5-Pro, yet its performance in other categories falls well below that of comparable models, highlighting uneven specialization. Crucially, the categories where humans and VLMs perform well are not aligned. Humans excel in Other (93.25%), even surpassing their accuracy on Signboard & Icon (90.27%) and Measurement (86.76%). However, VLMs show the opposite trend, performing better on Signboard & Icon and Measurement while consistently struggling the most on Other, with accuracies often below 2030%. This divergence suggests that human visual reasoning is more balanced and generalizable, whereas current VLMs rely on category-specific cues that fail to capture the broader semantic grounding humans naturally employ."
        },
        {
            "title": "5 ANALYSIS",
            "content": "5.1 ABLATION ON THE FORMAT OF INPUT QUERY Traditional vision benchmarks typically treat images as feature inputs and pose questions about them through textual descriptions. In contrast, GSM8K-V embeds the actual problem to be solved directly within the image. To investigate whether models can capture such problem statements from images, we conducted an ablation study to examine whether explicitly providing the question in textual form can improve model performance. We compare two input modes: implicit, where only scene images are provided, and explicit, where images are paired with masked question linking visual entities to semantics. Results show that explicit input yields slightly higher accuracy across most models, as textual grounding reduces ambiguity in reference resolution. However, even with explicit support, performance remains far below text-only scores (e.g., Gemini-2.5-Pro: 47.6% vs. 95.2%), underscoring that robust multimodal mathematical reasoning is still an open challenge. detailed illustration of the evaluation modes is provided in Appendix B.2.2. 5.2 ABLATION ON MULTI-IMAGE INPUTS Table 3: Performance (%) under single-image and multi-image settings. In practical applications, visual math problem can be presented either as single concatenated image merging all scenes or as multiple sequential images corresponding to each scene. Table 3 reports model performance under these two settings. Multi-image input yields slightly higher accuracy across models, suggesting that scene separation helps models preserve temporal and logical order. By contrast, concatenated single-image input tends to obscure sequential dependencies, especially for temporally grounded reasoning, and often prevents models from capturing critical information. Appendix B.4 further illustrates the concrete input formats. GPT-4o GLM-4.5V Qwen2.5-VL-72B Qwen2.5-VL-7B Single-image Multi-image 29.57 28.28 21.91 15.16 27.45 26.76 20.47 15.39 Model"
        },
        {
            "title": "5.3 SENSITIVITY ON IMAGE STYLE",
            "content": "Our constructed GSM8K-V benchmark adopts Pixar-style rendering as the primary visual format, chosen for its clarity, sharp edges, and natural lighting. To examine whether visual style significantly affects model reasoning, we conduct an ablation study on small subset of problems re-rendered in Giphli-style, which features softer tones and painterly textures. Table 4: Performance (%) under different image rendering styles. Model Pixar Style Giphli Style GPT-4o GLM-4.5V Qwen2.5-VL-72B Qwen2.5-VL-7B 27.27 28.10 18.18 12. 28.93 26.45 17.36 12.40 Table 4 demonstrates that performance differences between Pixar and Giphli styles are marginal, confirming that stylistic variation exerts minimal influence on reasoning; Appendix B.5 presents side-by-side illustration."
        },
        {
            "title": "5.4 MODALITY VALID",
            "content": "Table 5 examines whether GSM8K-V truly requires visual reasoning by comparing performance across text-only, image-only, OCR, and caption inputs. The OCR refers to text transcribed by QwenOCR, while caption inputs are structured scene descriptions. Results show that OCR-based inputs yield extremely low accuracy, demonstrating that models cannot simply rely on text transcriptions of the images to solve the problems. This validates that the benchmark genuinely requires vision, since critical cues are embedded in visual form rather than surface text. By contrast, caption-based inputs significantly outperform the image-only setting, yet still fall short of text-only performance. This indicates that explicitly converting scene descriptions into structured text helps bridge the gap, but models still fail to capture the implicit references and grounded semantics conveyed visually. Representative cases are provided in Appendix B.3, highlighting how OCR loses essential information and how captions only partially close the modality gap. Table 5: Performance (%) on text-only, image-only, OCR and caption settings, image-only is the baseline, and OCR/caption show gain/loss relative to image-only. Model Text only Image only OCR Caption GPT-4o GLM-4.5V Qwen2.5-VL-72B Qwen2.5-VL-7B 94.92 90.60 94.09 78.32 29.57 28.28 21.91 15.16 11.98 (-17.59 ) 9.33 (-18.95 ) 9.63 (-12.28 ) 3.11 (-12.05 ) 55.11 (+25.54 ) 44.96 (+16.68 ) 47.31 (+25.40 ) 31.92 (+16.76 ) 5.5 ERROR ANALYSIS Perception-Calculation Errors. This error type is particularly prevalent in GSM8K-V, arising when multiple visually similar objects coexist or when fine-grained details are required for quantitative reasoning. Models often misperceive counts or confuse object categories, leading to cascading calculation mistakes. For example, they may misinterpret package sizes or mismap quantities to prices (i.e., flower packaging or lumber resale). Such perception-calculation errors frequently flip the outcome of arithmetic reasoning and underscore key weakness in visual perception fused with quantitative binding. Representative cases are provided in the Appendix E.1. Instrument-Reading Errors. second prominent error category arises from failures in recognizing and interpreting everyday instruments, such as clocks, gauges, or dashboards. These devices are frequently used in GSM8K-V to encode numerical values, yet models often misread the visual cues or map them inconsistently to quantities. As result, downstream reasoning collapses even when the underlying arithmetic is straightforward. For instance, models misinterpreted pie-chart percentages in scheduling problem or confused the reading of an exercise gauge, yielding answers far from the ground truth. Such errors reveal bottleneck in visually grounded numeracy, where reliable"
        },
        {
            "title": "Preprint",
            "content": "perception of real-world measurement devices is essential. Detailed case studies are provided in the Appendix E.2."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper, we introduce purely visual benchmark for mathematical reasoning, GSM8K-V, which is derived from the widely used text-based dataset GSM8K. GSM8K-V consists of 1,319 comic-style multi-image evaluation samples covering diverse range of grade school math word problems. We evaluate variety of both closedand open-source models, and find that although existing VLMs achieve near-perfect performance on the text-based GSM8K, there remains substantial room for improvement on vision-based GSM8K-V. We further conduct extensive experiments and analyses, and discuss the underlying reasons why current models underperform on GSM8K-V. To the best of our knowledge, GSM8K-V is the first benchmark to focus on the reasoning capabilities of VLMs in math word problems, providing the research community with new direction and reliable evaluation standard for advancing mathematical reasoning in VLMs."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "In this paper, all evaluation samples in our constructed benchmark GSM8K-V are derived through one-to-one mapping from the original GSM8K (Cobbe et al., 2021) benchmark, which has already undergone ethical review. Consequently, the content of GSM8K-V poses negligible ethical risks. During the human annotation process, we provided thorough training for all annotators. Their tasks involved only objective judgments, without engagement in subjective human preferences or exposure to ethical concerns. All annotators received fair compensation aligned with their workload. Furthermore, we collected feedback regarding their annotation experience, and they unanimously reported that they were able to accomplish the task satisfactorily and were appropriately rewarded."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "Our constructed benchmark will be fully open-sourced to facilitate the research community in evaluating and analyzing VLMs. We provide all hyperparameter settings used in our evaluations in Appendix to ensure reproducibility of our results."
        },
        {
            "title": "REFERENCES",
            "content": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, et al. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Jiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang, Lingbo Liu, Eric P. Xing, and Liang Lin. Geoqa: geometric question answering benchmark towards multimodal numerical reasoning. arXiv preprint arXiv:2105.14517, 2021. URL https://arxiv.org/abs/2105.14517. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. URL https://arxiv.org/abs/2110.14168. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, Luke Marris, and Sam Petulla et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. URL https://arxiv.org/abs/2507.06261. Minheng Dai et al. Early exit via reinforcement learning in reasoning models: S-GRPO. arXiv preprint arXiv:2505.07686, 2025a. URL https://arxiv.org/abs/2505.07686."
        },
        {
            "title": "Preprint",
            "content": "Minheng Dai et al. Stable Reinforcement Learning for Efficient Reasoning. arXiv preprint arXiv:2505.18086, 2025b. URL https://arxiv.org/abs/2505.18086. Kimi Team: Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, et al. Kimi-vl technical report. arXiv preprint arXiv:2504.07491, 2025. URL https://arxiv.org/abs/2504. 07491. Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, Zhengyang Tang, Benyou Wang, Daoguang Zan, Shanghaoran Quan, Ge Zhang, Lei Sha, Yichang Zhang, Xuancheng Ren, Tianyu Liu, and Baobao Chang. OmniMATH: universal olympiad-level mathematic benchmark for large language models. arXiv preprint arXiv:2410.07985, 2024. URL https://arxiv.org/abs/2410.07985. Dong Guo et al. DeepSeek-R1: Incentivizing reasoning capability in llms via pure reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. URL https://arxiv.org/abs/2501. 12948. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. MATH: Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. URL https://arxiv.org/abs/2103.03874. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, et al. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pp. 611626, Koblenz Germany, October 2023. ACM. doi: 10.1145/3600006.3613165. Zichao Li, Yanshuai Cao, and Jackie CK Cheung. Do LLMs build world representations? probIn The Thirty-eighth Annual Conference on Neural ing through the lens of state abstraction. Information Processing Systems, 2024. URL https://openreview.net/forum?id= lzfzjYuWgY. Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning. arXiv preprint arXiv:2105.04165, 2021. URL https://arxiv.org/abs/2105.04165. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, KaiWei Chang, Michel Galley, and Jianfeng Gao. MathVista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. URL https: //arxiv.org/abs/2310.02255. Shiyin Lu, Yang Li, Yu Xia, Yuwei Hu, Shanshan Zhao, Yanqing Ma, et al. Ovis2.5 technical report. arXiv:2508.11737, 2025. Gary Lupyan and Bodo Winter. Language is more abstract than you think, or, why arent languages more iconic? Philosophical Transactions of the Royal Society B: Biological Sciences, 373(1752): 20170137, 2018. Meta AI. Introducing LLaMA 4: Advancing Multimodal Intelligence. Meta AI Blog, 2024. URL https://ai.meta.com/blog/llama-4-multimodal-intelligence/. OpenAI. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. URL https://arxiv. org/abs/2410.21276. OpenAI. Learning to reason with llms, 2024. URL https://openai.com/index/ learning-to-reason-with-llms/. OpenAI. Introducing gpt-4.1 in the api. https://openai.com/index/gpt-4-1/, April 2025a. OpenAI. Introducing our latest image generation model in the api. https://openai.com/ index/image-generation-api/, April 2025b. Qwen Team. QVQ-Max: Think with evidence. https://qwenlm.github.io/blog/ qvq-max-preview/, 2025."
        },
        {
            "title": "Preprint",
            "content": "Zhenyu Shao et al. DeepSeekMath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. URL https://arxiv.org/abs/2402. 03300. Wenhao Shi, Zhiqiang Hu, Yi Bin, Junhua Liu, Yang Yang, See-Kiong Ng, Lidong Bing, and Roy Ka-Wei Lee. Math-LLaVA: Bootstrapping mathematical reasoning for multimodal large lanIn Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Findings of guage models. the Association for Computational Linguistics: EMNLP 2024, pp. 46634680, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024. findings-emnlp.268. URL https://aclanthology.org/2024.findings-emnlp. 268/. StepFun et al. Step-3 is large yet affordable: Model-system co-design for cost-effective decoding, 2025. Kai Sun, Yushi Bai, Ji Qi, Lei Hou, and Juanzi Li. MM-MATH: Advancing multimodal math evaluation with process evaluation and fine-grained classification. arXiv preprint arXiv:2404.05091, 2024. URL https://arxiv.org/abs/2404.05091. GLM-V Team, Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, et al. GLM-4.1V-Thinking and GLM-4.5V: Towards versatile multimodal reasoning with scalable reinforcement learning. arXiv preprint arXiv:2507.01006, 2025. URL https://arxiv.org/abs/2507.01006. Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Mingjie Zhan, and Hongsheng Li. MATHVision: Measuring multimodal mathematical reasoning with visual statements. arXiv preprint arXiv:2402.14804, 2024. URL https://arxiv.org/abs/2402.14804. Ke Wang, Junting Pan, Linda Wei, Aojun Zhou, Weikang Shi, Zimu Lu, Han Xiao, Yunqiao Yang, Houxing Ren, Mingjie Zhan, and Hongsheng Li. MathCoder-VL: Bridging vision and code for enhanced multimodal mathematical reasoning. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Findings of the Association for Computational Linguistics: ACL 2025, pp. 25052534, Vienna, Austria, July 2025a. Association for Computational Linguistics. ISBN 979-8-89176-256-5. doi: 10.18653/v1/2025.findings-acl.128. URL https://aclanthology.org/2025.findings-acl.128/. Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Internvl3.5: Advancing open-source multimodal Linglin Jing, Shenglong Ye, Jie Shao, et al. models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025b. Yuwei Wang et al. Reinforcement Learning with Verifiable Rewards (RLVR) improves mathematical reasoning. arXiv preprint arXiv:2504.20571, 2025c. URL https://arxiv.org/abs/ 2504.20571. Yuxuan Wang et al. MINT-CoT: Enabling interleaved visual tokens in multimodal chain-of-thought. arXiv preprint arXiv:2506.05331, 2025d. URL https://arxiv.org/abs/2506.05331. Weiye Xu, Jiahao Wang, Weiyun Wang, Zhe Chen, Wengang Zhou, Aijun Yang, Lewei Lu, Houqiang Li, Xiaohua Wang, Xizhou Zhu, Wenhai Wang, Jifeng Dai, and Jinguo Zhu. VisuLogic: benchmark for evaluating visual reasoning in multi-modal large language models. arXiv preprint arXiv:2504.15279, 2025. URL https://arxiv.org/abs/2504.15279. Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. Kai Zhang et al. Survey of Reinforcement Learning for Large Reasoning Models. arXiv preprint arXiv:2509.08827, 2025a. URL https://arxiv.org/abs/2509.08827. Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, and Hongsheng Li. MathVerse: Does your multi-modal llm truly see the diagrams in visual math problems? arXiv preprint arXiv:2403.14624, 2024. URL https://arxiv.org/abs/2403.14624."
        },
        {
            "title": "Preprint",
            "content": "Ruidan Zhang et al. Improve Vision-Language Model Chain-of-Thought Training with Outcome Rewards and Preference Optimization. In Proceedings of the 63rd Annual Meeting of the ACL, 2025b. URL https://aclanthology.org/2025.acl-long.82.pdf. Guang Zhou et al. Reinforced MLLM: survey on RL-based multimodal reasoning in large visionlanguage models. arXiv preprint arXiv:2504.21277, 2025. URL https://arxiv.org/abs/ 2504.21277. Chengke Zou, Xingang Guo, Rui Yang, Junyu Zhang, Bin Hu, and Huan Zhang. DynaMath: dynamic visual benchmark for evaluating mathematical reasoning robustness of visionlanguage models. arXiv preprint arXiv:2411.00836, 2024. URL https://arxiv.org/abs/2411. 00836."
        },
        {
            "title": "CONTENTS",
            "content": "1 Introduction 2 Related Works"
        },
        {
            "title": "2.2 Mathematical Benchmarks for Vision Language Models",
            "content": ". . . . . . . . . . . . . . 3 Benchmark Construction"
        },
        {
            "title": "3.3 Step 3: Image Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "4 Evaluations Results 4.1 Evaluation Setup . . . 4.2 Overall Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Analysis 5.1 Ablation on the format of input query . . . . . . . . . . . . . . . . . . . . . . . . 5.2 Ablation on multi-image inputs . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.3 Sensitivity on Image Style . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4 Modality Valid . 5.5 Error Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 Conclusion LLM Usage Detailed Evaluation Setup B.1 Model Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Evaluation Mode . B.2.1 Text Only . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2.2 Implicit and Explicit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3 Modality Valid . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.4 Single Images vs. Multi Images . . . . . . . . . . . . . . . . . . . . . . . . . . . B.5 Image Style Effects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . GSM8K-V Construction Details C.1 Math Information Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 Assign Category . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.3 Human cross check and refinement . . . . . . . . . . . . . . . . . . . . . . . . . . C.4 API Token Usage and Cost of GSM8K-V . . . . . . . . . . . . . . . . . . . . . . 14 1 3 4 4 4 5 5 6 7 8 8 8 9 9 10 16 16 17 17 18 21 23 29 29 31"
        },
        {
            "title": "Preprint",
            "content": "D Prompt Templates D.1 Step1 Prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 Meta Description Strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.3 Image Generation Prompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.4 Controlled Interference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.5 Image Generation Prompts and Verification Principles . . . . . . . . . . . . . . . . Error Analysis E.1 Perception Calculation Errors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2 Instrument Reading Errors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Category Case Study 34 34 36 40 44 44 44 50"
        },
        {
            "title": "A LLM USAGE",
            "content": "In writing this paper, we only used LLMs for polishing. The generation of ideas in this work did not involve any assistance from LLMs. The experimental design and manuscript writing were not directly produced by LLMs either. The models were used solely as polishing tool: specifically, we first drafted the manuscript, then refined it with the help of an LLM, and finally the authors conducted another round of verification after polishing. Regarding the construction of the benchmark, we leverage multiple LLMs and image generation models to automate the data creation process. After the final stage of image generation, we conduct meticulous human annotation, where each case undergoes dual manual verification to ensure correctness and reliability."
        },
        {
            "title": "B DETAILED EVALUATION SETUP",
            "content": "In this section, we report the detailed evaluation settings and mode. B.1 MODEL HYPERPARAMETERS We set temperature = 0.2 for closed-source models and temperature = 0.1 for open-source models to reduce the randomness in the model generation. Table refappendix:hyperparams displays the parameters we used for generation in VLMs. Table 6: Hyperparameter settings for all evaluated models. For closed-source VLMs, all unlisted configurations follow official defaults. Model Hyperparameters Closed-Source VLMs (other settings follow official defaults) Gemini-2.5-Pro (Comanici et al., 2025) GPT-5 (OpenAI, 2024) GPT-4o (OpenAI, 2024) QVQ-Max-Latest (Qwen Team, 2025) Open-Source VLMs temperature = 0.2, max tokens = 4096 temperature = 1.0, max tokens = 4096 reasoning effort = minimal, verbosity = low temperature = 0.1, max tokens = 2048 temperature = 0.1, max tokens = 4096 Llama-4-17B-16E-Instruct (Meta AI, 2024) Llama-4-17B-128E-Instruct (Meta AI, 2024) InternVL3.5-8B (Wang et al., 2025b) InternVL3.5-38B (Wang et al., 2025b) InternVL3.5-30B-A3B (Wang et al., 2025b) InternVL3.5-241B-A28B (Wang et al., 2025b) Qwen2.5-VL-7B-Instruct (Bai et al., 2025) Qwen2.5-VL-32B-Instruct (Bai et al., 2025) Qwen2.5-VL-72B-Instruct (Bai et al., 2025) Ovis2.5-2B (Lu et al., 2025) Ovis2.5-9B (Lu et al., 2025) GLM-4.5V (Team et al., 2025) MiniCPM-V 4.5 (Yao et al., 2024) Kimi-VL-A3B-Thinking-2506 (Du et al., 2025) temperature = 0.1, max tokens = 2048 temperature = 0.1, max tokens = 2048 temperature = 0.1, max tokens = 2048 temperature = 0.1, max tokens = 2048 temperature = 0.1, max tokens = 2048 temperature = 0.1, max tokens = 2048 temperature = 0.1, max tokens = 2048 temperature = 0.1, max tokens = 2048 temperature = 0.1, max tokens = 2048 temperature = 0.1, max tokens = 2048 temperature = 0.1, max tokens = 2048 temperature = 0.1, max tokens = 2048 temperature = 0.1, max tokens = 2048 temperature = 0.1, max tokens ="
        },
        {
            "title": "Preprint",
            "content": "B.2 EVALUATION MODE We provide detailed definitions and examples for each evaluation mode, and illustrate the input format, example questions, and the exact prompts used in our experiments. B.2.1 TEXT ONLY Answer Format Prompt. When providing your final answer: - If the answer can be expressed as whole number (integer), provide it as an integer - If the answer requires decimals and can be expressed with finite decimal, use decimal format - Otherwise, use fraction format if appropriate Please think step by step. After your reasoning, output your final answer on new line starting with \"FINAL ANSWER: \" followed by the number only. Text Only Mode Meta Data: { } \"question_id\": \"195\", \"original_question\": \"Violetta wants to buy new crayons. She needs them in 5 different colors and prepared $20 for this purchase. One crayon costs $2. How much change will she get?\", \"math_ground_truth\": \"10\" Input (question): pen costs as much as pencil and eraser combined. pencil costs $1.20 and an eraser costs $0.30. How much will 8 pens cost? Mode Prompt. You are an expert at solving mathematical word problems. Please solve the following problem step by step, showing your reasoning. Problem: {question} # Answer Prompt: {Answer Format Prompt}"
        },
        {
            "title": "Preprint",
            "content": "B.2."
        },
        {
            "title": "IMPLICIT AND EXPLICIT",
            "content": "To better illustrate the two evaluation settings, we provide the visualization in Figure 4. In the implicit setting, models are given only the visual scenes, requiring them to infer semantic relations directly from images. In contrast, the explicit setting supplements the scenes with masked question, which helps ground entities and clarify their semantic roles. This reduces ambiguity but does not eliminate the fundamental difficulty of multimodal reasoning. Figure 4: Illustration of performance (%) comparison between explicit and implicit evaluation settings. As shown in Figure 4, when the final query is provided to the model in textual form, VLMs generally achieve slight improvement in performance. This indicates that VLMs still face challenges in accurately understanding the underlying problems embedded in query images. However, even with textual queries, the performance of existing models on GSM8K-V remains limited, with the best accuracy still below 50%. This highlights the deficiencies of current models in handling multiimage reasoning tasks and further demonstrates that VLMs have substantial room for improvement in solving real-world mathematical problems, thereby validating the practical value of our proposed benchmark. In the following, we provide the detailed formats of both implicit and explicit inputs and prompts."
        },
        {
            "title": "Preprint",
            "content": "In the implicit mode, for each mathematical problem, we only provide the model with several images and textual prompt. The prompt does not contain any mathematical information or the actual problem statement; it solely specifies the required output format. After receiving the prompt and the input images, the model generates an output according to the prescribed format. We then parse and verify the models output, and finally evaluate the correctness of the predicted answer. Implicit Mode Meta Data: { } \"question_id\": \"195\", \"original_question\": \"Violetta wants to buy new crayons. She needs them in 5 different colors and prepared $20 for this purchase. One crayon costs $2. How much change will she get?\", \"math_ground_truth\": \"10\", \"pic_ids\": [ \"195_1.jpg\", \"195_2.jpg\", \"195_3.jpg\", \"195_4.jpg\" ] Input(Only Images): Prompt. You are an expert at solving mathematical problems based on visual information. Ill show you some images that contain math problem or story . Answer the question in the images. # Answer Prompt: {Answer Format Prompt}"
        },
        {
            "title": "Preprint",
            "content": "In the explicit mode, for each mathematical problem, we provide the model with several images, pre-processed textual representation of the problem (which excludes any mathematical information), and textual prompt. The prompt also contains no mathematical information or actual problem statement; it merely specifies the required output format. After receiving the prompt and inputs, the model produces an output according to the prescribed format. We then parse and verify the models response, and finally evaluate the correctness of the predicted answer. Explicit Mode Meta Data: { } \"question_id\": \"195\", \"original_question\": \"xx(the same as text only mode)\", \"math_ground_truth\": \"10\", \"masked_question\": \"How much change will Violetta (long wavy brown hair, pink backpack, light green shirt) get after buying her crayons?\", \"pic_ids\": [ \"195_1.jpg\", \"195_2.jpg\", \"195_3.jpg\", \"195_4.jpg\"] Input(Images + Masked Question) The model receives four scene images representing the problem, together with masked version of the original text question. masked question: How much change will Violetta (long wavy brown hair, pink backpack, light green shirt) get after buying her crayons? Prompt. You are an expert at solving mathematical problems based on visual information. Ill show you one or more images related to math problem, along with question. Please analyze the images and answer the math question step by step. Question to answer: {masked question} {Answer Format Prompt}"
        },
        {
            "title": "Preprint",
            "content": "B.3 MODALITY VALID OCR Mode Meta Data: { } \"question_id\": \"196\", \"ocr_text\": \"Here are thenprices for ournschool supplies .nPRICE LISTn$2n$1n$1,50 want these. Thank you.n20 How muchnchange will Inget afternbuying thesencrayons? n20\", \"math_ground_truth\": \"12\" Input (OCR Text): Here are the prices for our school supplies. PRICE LIST $2 $1 $1,50 want these. Thank you. 20 How much change will get after buying these crayons? 20 Prompt. You are an expert at solving mathematical problems extracted from {ocr_text}. The input is raw OCR transcription of images. Carefully read the text, resolve formatting artifacts (e.g., misplaced line breaks, currency symbols, punctuation), and then solve the math problem it describes. # Answer Prompt: {Answer Format Prompt}"
        },
        {
            "title": "Preprint",
            "content": "Caption Mode Input (Scene Description): Scene 1: Objects: - visually clear and attractive price list with textual label price list - The price list line1 states: - The price list line2 states: - The price list line3 states: - Shop assistant (short black hair, blue vest, white shirt, name tag).A white speech bubble extending from the shop assistants mouth, containing the text: Here are the prices for our school supplies. $1.50 $2 $1 Composition: - The price list is mounted on stand or board near the store entrance, positioned at eye level and angled slightly toward the viewer for full visibility. - Each line of the price list is clearly separated, with the crayon, eraser, and pencil icons on the left and their respective prices to the right, all on separate lines. Action: The shop assistant extends their right hand, pointing directly at the price list to visually anchor their spoken statement to the displayed prices. Scene 2: ... FINAL SCENE 4(Problem Demonstration) ... white speech bubble extending from Violettas mouth, containing the text: How much change will get after buying these crayons? ... Prompt. You are an expert at analyzing visual scenes and solving mathematical problems based on interconnected information across multiple scenes. Analyze the following sequence of scenes carefully. Each scene provides information that connects to the others, building toward the final question. You need to understand the relationships between scenes and track quantities, actions, and information flow. Now analyze these scenes: {scene_description} # Answer Prompt: {Answer Format Prompt}"
        },
        {
            "title": "Preprint",
            "content": "B.4 SINGLE IMAGES VS. MULTI IMAGES Single vs. Multi Images (Temporal Reasoning) Text Problem. Lloyd earns $10 an hour on Math tutoring. He tutored 5 hours for the first week and 8 hours for the second week. How much did he earn for the first two weeks? Single-Image (One Image). Single-Image (Concatenated) Multi-Image (6 Scenes). Scene 1 Scene 2 Scene 3 Scene 4 Scene 5 Scene Ground Truth: 130 Hourly rate = $10. Total hours = 5 + 8 = 13. Total earnings = $10 13 = 130."
        },
        {
            "title": "Preprint",
            "content": "Single vs. Multi Images (Temporal Reasoning) GPT-4o, Multi-Image (q Wrong) Answer: To determine the earnings for the first two weeks, we need to calculate the total hours worked and multiply by the hourly rate. 1. **Hourly Rate**: $10/hour 2. **First Week**: - Start time: 9:00 AM - End time: 3:00 PM - Total hours per day = 3:00 PM - 9:00 AM = 6 hours 3. **Second Week**: - Start time: 9:00 AM - End time: 3:00 PM - Total hours per day = 3:00 PM - 9:00 AM = 6 hours - **Total Hours for Two Weeks**: 6 hours (first week) + 6 hours (second week) = 12 hours 5. **Total Earnings**: 12 hours * $10/hour = $120 FINAL ANSWER: 120 GPT-4o, Single-Image (q Wrong) Answer: To determine how much Lloyd earned for the first two weeks of tutoring, we need to follow these steps: 1. **Identify the hourly rate**: The price list shows that Lloyd charges $10 per hour for tutoring. 2. **Determine the number of hours worked each week**: - The first image indicates the start of the first week, and the third image indicates the end of the first week. However, the number of hours worked is not directly shown. - The fourth image indicates the start of the second week, and the fifth image indicates the end of the second week. Again, the number of hours worked is not directly shown. 3. **Calculate the total earnings**: Since the number of hours worked each week is not specified in the images, we cannot calculate the exact earnings without additional information. Given the lack of specific hours worked, we cannot provide numerical answer based on the images alone. FINAL ANSWER: 0 Analysis. Both inputs lead to incorrect answers, but for different reasons. In the multi-image setting, GPT-4o misread clock times and produced an incorrect total of 120. In the single-image setting, the concatenation obscured temporal order, preventing the model from retrieving critical information, and it defaulted to zero answer. This contrast indicates that concatenated input may obscure critical details, whereas multi-image input better preserves scene-specific information."
        },
        {
            "title": "Preprint",
            "content": "B."
        },
        {
            "title": "IMAGE STYLE EFFECTS",
            "content": "Image Style Example1 Text Problem. fog bank rolls in from the ocean to cover city. It takes 10 minutes to cover every 3 miles of the city. If the city is 42 miles across from the oceanfront to the opposite inland edge, how many minutes will it take for the fog bank to cover the whole city? GSM8K-V Images (Pixar vs. Giphli). Pixar Scene 1 Pixar Scene 2 Pixar Scene Giphli Scene 1 Giphli Scene 2 Giphli Scene 3 Ground Truth: 140 The fog covers 3 miles in 10 minutes. The city is 42 miles wide. Thus, total time = 42 3 10 = 14 10 = 140 minutes."
        },
        {
            "title": "Preprint",
            "content": "Image Style Example1 GPT-4o, Pixar Style (q Wrong) Answer: To solve the problem, lets analyze the images step by step: 1. **First Image:** - The clock shows 10:0. - The sign indicates the beach is 3 miles away and the city is 42 miles away. 2. **Second Image:** - The clock shows 11:00. - The sign indicates the park. 3. **Third Image:** - The question asks how many minutes until the fog covers the whole city. **Analysis:** - The time difference between the first and second images is 60 minutes (from 10:00 to 11:00). - The fog has already covered the park by 11:00. - The distance from the beach to the city is 42 miles, and the park is 3 miles from the beach. - Assuming the fog moves at constant speed, it took 60 minutes to cover 3 miles. **Calculation:** - Speed of fog = Distance / Time = 3 miles / 60 minutes = 0.05 miles per minute. - Remaining distance to cover the city = 42 miles - 3 miles = 39 miles. - Time to cover 39 miles = Distance / Speed = 39 miles / 0.05 miles per minute = 780 minutes. FINAL ANSWER: 780 GPT-4o, Giphli Style (q Wrong) Answer: To solve the problem, we need to determine how long it will take for the fog to cover the entire city. 1. **Understand the Distances:** - The first image shows that the distance from the beach to the park is 3 miles. - The distance from the beach to the city is 42 miles. 2. **Analyze the Clocks:** - The first clock shows 10:00. - The second clock shows 11:00. 3. **Determine the Time Taken for Fog to Cover 3 Miles:** - The fog covers the distance from the beach to the park (3 miles) in 1 hour (from 10:00 to 11:00). 4. **Calculate the Speed of the Fog:** - Speed of the fog = Distance / Time = 3 miles / 1 hour = 3 miles per hour. 5. **Calculate the Time to Cover 42 Miles:** - Time = Distance / Speed = 42 miles / 3 miles per hour = 14 hours. 6. **Convert Hours to Minutes:** - 14 hours = 14 60 minutes = 840 minutes. Therefore, it will take 840 minutes for the fog to cover the whole city. FINAL ANSWER: 840 Analysis. Although both predictions are incorrect, the differences between Pixar and Giphli renderings do not significantly affect recognition. The errors stem from divergent reasoning paths rather than visual style, suggesting that stylistic variation has minimal impact on model understanding."
        },
        {
            "title": "Preprint",
            "content": "Image Style Example2 Text Problem. Alice is 7 years older than Beth, who is 5 years older than Erica. What is the difference between the ages of Alice and Erica, if Erica is 30 years old? GSM8K-V Images (Pixar vs. Giphli). Pixar Scene 1 Pixar Scene 2 Pixar Scene Pixar Scene 4 Giphli Scene 1 Giphli Scene 2 Giphli Scene 3 Giphli Scene 4 Ground Truth: 12 - Ericas age = 30. - Beths age = 30 + 5 = 35. - Alices age = 35 + 7 = 42. - Difference = 42 30 = 12. GPT-4o, Pixar Style ( Correct) Answer: To solve the problem, lets determine the ages of each person step by step: 1. **Ericas Age:** - In 2024, Erica is 30 years old. 2. **Beths Age:** - Beth is 5 years older than Erica. - Beths age = Ericas age + 5 = 30 + 5 = 35 years old. 3. **Person in Greens Age:** - The person in green is 7 years older than Beth. - Person in greens age = Beths age + 7 = 35 + 7 = 42 years old. 4. **Age Difference Between Person in Green and Erica:** - Age difference = Person in greens age - Ericas age = 42 - 30 = 12 years. FINAL ANSWER:"
        },
        {
            "title": "Preprint",
            "content": "Image Style Example2 GPT-4o, Giphli Style ( Correct) Answer: To find the age difference between Alice and Erica, lets analyze the information step by step: 1. **Ericas Age:** - From the first image, we know that Erica is 30 years old. 2. **Beths Age:** - From the second image, we know that the person in the red sweater is 5 years older than Erica. - Therefore, Beth is 30 + 5 = 35 years old. 3. **Alices Age:** - From the third image, we know that Alice is 7 years older than Beth. - Therefore, Alice is 35 + 7 = 42 years old. 4. **Difference in Ages:** - The difference between Alices age and Ericas age is 42 30 = 12. FINAL ANSWER: 12 Analysis. Both Pixar and Giphli renderings lead to the same correct reasoning process and answer, reinforcing that stylistic differences exert negligible influence on model performance when semantic cues are preserved."
        },
        {
            "title": "Preprint",
            "content": "C GSM8K-V CONSTRUCTION DETAILS C.1 MATH INFORMATION CLASSIFICATION This step involves the classification of mathematical information extracted from GSM8K into 13 predefined categories, ensuring the mathematical content is properly represented in the visual format. These categories include types like weight, distance, price, and count, each of which is associated with specific rendering strategies to preserve mathematical fidelity. The 13 categories are as follows. Weight Weight refers to the measurement of how heavy an object is, usually expressed in units such as grams, kilograms, or pounds. Examples: An apple weighs 200 g, The suitcase shows 15 kg. Length/Area/Volume Length, area, and volume capture spatial dimensions of objects or regions. Examples: The pencil is 12 cm long, The field covers 500 m2, The box has volume of 2 liters. Distance Distance denotes the separation between two locations or points. Examples: The school is 3 km from home, The two trees are 20 apart. Speed Speed refers to the rate of motion, often read from dashboard or speedometer. Examples: The car moves at 60 km/h, The bike shows 15 mph. Price Price expresses the cost of an item, usually displayed on signboard or tag. Examples: The bread costs $2.50, The toy is labeled 15 yuan. Group Group information involves the use of icons to represent collections or categories, where each icon aligns with specific textual meaning. Examples: - The icon of person holding cloth represents the action of washing. - The icon of teacher standing in front of board signifies the action of teaching. These icons are used to represent common tasks, roles, or collections that are easily understood through visual semantics. Calendar/Age Calendar and age capture temporal aspects such as dates, years, or personal age. Examples: Today is March 15, The child is 8 years old."
        },
        {
            "title": "Preprint",
            "content": "Clock Clock information refers to specific times of day shown on analog or digital clocks. Examples: The clock shows 7:30, The watch displays 12:45. Graph Graphs depict ratios, percentages, or quantities visually, often with bars, lines, or pie charts. Examples: 40% of the pie chart is shaded, The bar reaches value 10. Statistics Statistics summarize data in tables or charts, highlighting counts or distributions. Examples: The table shows 25 students in Class A, The chart lists 300 voters. Count Count refers to the number of discrete, visible objects in scene. Examples: There are 8 apples on the table, The image shows 3 books. Label Labels are direct textual annotations attached to objects, carrying mathematical values. Examples: box labeled 10 kg, jar with tag 2 L. Dialogue Dialogue involves characters communicating contextually relevant text in speech bubbles. The dialogue should never directly provide numbers. Examples: customer says Id like large pizza, teacher says Lets start the exam."
        },
        {
            "title": "Preprint",
            "content": "C.2 ASSIGN CATEGORY The allocation process, carried out with the assistance of GPT-4.1, organizes the information in way that facilitates accurate visual representation. This step is critical for ensuring that each element is positioned correctly in the generated scene. For detailed look at how the extracted information is categorized and allocated, see Figure 5, which illustrates the assignment process. Figure 5: An detailed example that assigns the extracted mathematical information into corresponding categories."
        },
        {
            "title": "Preprint",
            "content": "C.3 HUMAN CROSS CHECK AND REFINEMENT To ensure the visual scenes accurately reflect the original mathematical problems, dual human cross-checking process is applied. Two independent annotators review each problem and its corresponding scene, ensuring consistency, completeness, and compliance with the benchmark requirements. This process is documented through the visual scenes shown in Figure 6. Annotator Details Instructions. Questions are automatically assigned to you in consistent random order. Review the question and images carefully. Solve the problem yourself (Human Solution). Add your comment (reasons) if you choose Incorrect or Skip. Choose your annotation: Correct: images match the question and ground truth. Incorrect: images do not match the question or ground truth. Skip: uncertain or problematic case. Annotator Description. We employed total of seven annotators to conduct human verification. Among them, five were undergraduate students and two were doctoral students. All annotators had solid background in mathematics and computer science, ensuring that their knowledge base was sufficient to understand the problems, inspect the generated scenes, and evaluate correctness. The combination of undergraduate and doctoral perspectives provided balance between practical annotation skills and advanced reasoning expertise, strengthening the reliability of the human evaluation process. Training Procedure. Before annotation began, all annotators underwent one-hour training session specifically designed for GSM8K-V. The training introduced the project background, the dataset structure, and the end-to-end construction pipeline. Special emphasis was placed on the error taxonomy, so that annotators could consistently identify mismatches between images, textual questions, and ground truth solutions. This preparation ensured that annotators were not only familiar with the annotation interface but also equipped to provide high-quality and consistent judgments."
        },
        {
            "title": "Preprint",
            "content": "Figure 6: Illustration of the dual human cross-checking process in GSM8K-V construction. Each problem is independently validated by two annotators, who verify scene fidelity under the 3C principles: Consistency (preserving entities, quantities, and constraints from the original text), Completeness (ensuring all problem-relevant information is visually available), and Compliance (adhering to safety and formatting standards). Annotators also solve each problem to confirm correctness of the derived answer, ensuring benchmark reliability. C.4 API TOKEN USAGE AND COST OF GSM8K-V We summarize the average token usage and estimated cost for each stage of data construction. Stage 1 and Stage 2 used GPT-4.1 inference APIs, while Stage 3 used GPT-Image-1 for rendering. Pricing details are based on official documentation at the time of experiment. Details are in Table 7. Table 7: Average token usage and estimated costs across three stages of dataset construction. Stage 1/2 use GPT-4.1 inference rates ($2.00/1M input, $8.00/1M output). Stage 3 uses GPT-Image1 ($10.00/1M input, $0.167 per imagequality:high, size:1024 1024). Stage # Samples Avg. Input Tokens Avg. Output Tokens / Images Avg. Cost / Sample Total Cost Stage 1 (GPT-4.1) Stage 2 (GPT-4.1) Stage 3 (GPT-Image-1) Total 1319 1319 1319 - 16,582 13,093 1,038 33 946 1,039 4.05 images $0.041 $0.035 $0.686 $54.1 $46.2 $904.8 $1005."
        },
        {
            "title": "D PROMPT TEMPLATES",
            "content": "D.1 STEP1 PROMPT Step 1: problem decomposition and allocation Input (Original Textual Question). Prompt. You are math information extractor. Given an math word problem, identify only the mathematical information required to solve the final question, and represent it as triples: (object, math value, semantic) =============================== TRIPLE DEFINITIONS =============================== - object: the entity described (e.g., product, character, item , tool, vehicle). - math value: the associated numeric attribute (integer, decimal, ratio, unit). - semantic: the contextual role of this fact (e.g., placement, relation, or action). =============================== CLASSIFICATION RULES =============================== 1. Concrete math information: - Object is countable, physical entity. - Math value is pure integer (no units/decimals/ percentages). - Example: (\"apple\", 3, \"in the basket\") 2. Abstract math information: - Object is abstract (time, distance, price, weight, score, duration). - OR math value is not an integer (e.g., 1.5, 20% - Example: (\"distance\", \"12 miles\", \"between and B\") =============================== RENDERING STRATEGY ==============================="
        },
        {
            "title": "Preprint",
            "content": "Step 1: problem decomposition and allocation - Small countable values (<=10): -> Render directly as visual elements (visual_detail). - Large integers (>10): -> Decompose into = [op] with <= 10. Show \"a\" visually, \"b\" via speech bubble or text. - Abstract values: -> Express textually or with meta-descriptions (e.g., sign board for prices, calendar for dates, ratio graph). =============================== SCENE ALLOCATION =============================== - Group triples into scenes based on context. - Related facts appear in the same scene. - The final scene contains only the target question (no numeric values), using final_scene_description. =============================== INTERFERENCE (OPTIONAL) =============================== - perception: add visually unrelated but irrelevant objects. - semantic: insert distractors that are contextually similar but not required for solving the problem. =============================== OUTPUT FORMAT (JSON) =============================== { \"scenes\": [ { \"scene_id\": <int>, \"interfere\": \"none\" \"perception\" \"semantic\", \"scene_math_information\": [ { \"raw_math_information\": <string>, \"object\": <string>, \"math_value\": <string>, \"semantic\": <string>, \"use_strategy\": Distance Object Measurement Weight Graph Ratio Cross Scene Clock Dashboard Day Sign Board Action Visual Detail Speech Bubble Label }, ... ] }, ... ] }"
        },
        {
            "title": "Preprint",
            "content": "D.2 META DESCRIPTION STRATEGY Distance Meta Description Input: { \"object\": \"object field content\", \"math_value\": \"location location ...\", \"semantic\": \"semantic field content\", } Output: Object : - road sign board (reference to object field content). - Two distinct real-world location icons (from math value: location and B) are shown on the road sign board. - double-headed horizontal arrow is placed between the two icons, representing the distance. Composition: - Two location icons are horizontally aligned on opposite sides of the board. - Arrow spans between them, midpoint centered. - Label sits exactly at the midpoint, bold and unobstructed. Object Measurement Meta Description Input: { \"object\": \"object field content\", \"math_value\": \"numeric label\", \"semantic\": \"width/length/height/depth/thick\", } Output: Object : - single concrete object (from object field content). - dimension line (based on semantic) as bold double-headed arrow. - Numeric label (math value) at the midpoint. Composition: - Object centered, with space around. - Arrow aligns with relevant edges (length, width, height). - Arrow tips flush with boundaries. - Numeric label centered, bold, legible. Weight Meta Description Input : { \"object\": \"object field content\", \"math_value\": \"math_value field content\", \"semantic\": \"semantic field content\", } Output: Object : - Concrete object (object field content) on digital scale. - Scales screen shows weight: [math value] in black digits. Composition: - Object centered on weighing surface. - Display at front, angled to viewer. - Numeric weight clearly visible, no glare or obstruction. - No overlaps or extra labels."
        },
        {
            "title": "Preprint",
            "content": "Graph Ratio Meta Description Input : { \"object\": \"[color1] [color2] ...\", \"math_value\": \"N ...\", \"semantic\": \"color1 legend color2 legend ...\", } Output: Object: - single circle visually displayed as pie chart representing the ratio of [color1], [color2], ... segments. - The circle is evenly divided into equal radial segments, where = + + ... . Starting at the top center and proceeding clockwise. - Below the pie chart is legend consisting of horizontal items: - small [color1] square followed by the text [semantic1]. - small [color2] square followed by the text [semantic2]. Composition: - The pie chart is displayed at the position indicated by the semantic. - All objects mentioned in the scene are clearly visible and not overlapping. Cross Scene Clock Meta Description Input : { \"object\": \"clock\", \"math_value\": \"clock time\", \"semantic\": \"semantic field content\", } Output: Time Rule: If > 12:00, subtract 12:00. Example: 15:00 3:00. Object: - Wall clock with light beige face, dark blue border, black tick marks, bold numerals 112. - Distinct black hour & minute hands (from math value). Composition: - Clock placed on upper right of [semantic position]. Dashboard Meta Description Input : { \"object\": \"dashboard position\", \"math_value\": \"math_value field content\", \"semantic\": \"NUMA NUMB NUMC\", } Output: Object : - Dashboard speedometer: perfect circle, no needle. - Scale: 0NUMA, increment NUMB, every NUMC labeled. - Tick marks evenly spaced. - Numbers: black sans-serif font near ticks. - Label: bold unit (e.g. MPH, KMH), centered at bottom. Composition: - Dashboard placed on [dashboard position]."
        },
        {
            "title": "Preprint",
            "content": "Day Meta Description Input : { \"object\": \"calendar\", \"math_value\": \"specific day(s)\",(day1 day2 ...) \"semantic\": \"semantic field content\", } Output: Object : - wall-mounted monthly calendar(The background color is light beige, while the header area (where JULY is written in all capital letters, centered in bold black sans-serif font) is bright orange, creating clear visual separation). - The calendar grid has 7 columns labeled with single-letter weekday initials: S, M, T, W, T, F, (with Sunday on the far left). - calendar has 5 full rows of square date cells representing the days of the month. - Each date from 1 to 31 is shown as bold black numeral, centered inside its square cell. The first date (1) appears in the third cell(i.e., Tuesday column) in the first row. The last date (31) appears in the fifth cell(i.e., Thursday column) in the fifth row. Composition: composition: - The clock is diaplayed on the upper left of [position](based on the semantic field content). - The calendar is cleanly structured with even spacing and clearly separated date cells. - The grid is rectangular, with consistent column widths and row heights to maintain visual balance. Sign Board Strategy Input : { \"object\": \"object field content\", \"math_value\": \"math_value field content\", \"semantic\": \"semantic field content\", } Output: Object: - [semantic field content] (add some appearance description). - icon (convert the [object field content] into corresponding icon) + [math value field content]. Composition: - The icon/emoji (must convert the object into corresponding icon) should be displayed on separate line above the math value field content. Example: Input: { \"object\": \"apple\", \"math_value\": \"$3\", \"semantic\": \"price list\", } { } \"object\": \"banana\", \"math_value\": \"$2/kg\", \"semantic\": \"price list\", Output: Output: - visually clear and attractive price list with textual label price list. - The price list line1 states: Composition: - The $2/kg are displayed on separate line on the price list. $3. - The price list line2 states: $3 and $2/kg."
        },
        {
            "title": "Preprint",
            "content": "Action Strategy (VisualTextual Linking) Input : { \"object\": \"\", \"math_value\": \"\", \"semantic\": \"The character specific action.\", } Output: Description: Action is link between visual-detail-description and textual-information-detail. Approved Cross-Modal Visual Linkage Strategies: 1. Pointing Gesture The character must clearly point toward the specific visual group. 2. Gaze Direction The eyes of the character must align with the referenced object(s). Gaze must be directed and specific not ambiguous or generic. 3. Torso Orientation The characters upper body must be turned toward the relevant visual target, reinforcing gaze or pointing. 4. Holding or Touching The character physically holds or touches an object that semantically links to the textual math content. Use only when unambiguous. Visual Detail Strategy Input : { \"object\": \"object field content\", \"math_value\": \"math_value field content\", \"semantic\": \"semantic field content\", } Output: Object: - [math value field content] [object field content], each rendered with distinct visual features (e.g., color, shape, texture) are placed evenly on the where (semantic field content). Composition: - [math value field content] [object field content] must be described in fully visible, non-overlapping, and easily countable manner. - Objects must exhibit slight variation in position, size, or orientation to enhance perceptual clarity. Speech Bubble Strategy Input : { \"object\": \"object field content\", \"math_value\": \"\", \"semantic\": \"semantic field content\", } Output: Object: - Character (must be the same as object field content) facing forward or slightly angled. - clearly visible speech bubble extending from the characters mouth or face, fully and samely stating semantic field content. Composition: - The speech bubble must be connected to the speaking character with visible tail. - The speech bubble should not overlap with the other object."
        },
        {
            "title": "Preprint",
            "content": "Label Strategy Input : { \"object\": \"object field content\", \"math_value\": \"math_value field content\", \"semantic\": \"\", } Output: Object: - The object (must be the same as object field content). - The label stating must be the same as math value field content. Composition: - The label must be attached to the surface of an object (must be the same as object field content) or be part of the inherent surface marking of the object. D. IMAGE GENERATION PROMPTS Image Generation Prompt You are **skilled and consistent image illustrator**. You will continuously generate {num_scenes} distinct and separate images, each corresponding to exactly one of the scenes described below. You must follow the detailed instructions and generation process with no exception. ***STRICT GENERATION RULES:*** - You MUST generate **exactly one image per scene**. - You MUST generate the images in the exact given order: from **Scene 1** to **Scene {num_scenes}**. - You MUST NOT generate multiple images for the same scene. - You MUST NOT skip any scenes. - You MUST NOT repeat or regenerate previously generated scenes. ***IMAGE STYLE & FORMAT RULES:*** - All {num_scenes} images must follow **Pixar-style** with clear, sharp, and naturally lit rendering, no warm tints, soft color haze, textures, or yellow glow. - Use consistent character design, background tone, and visual quality across all scenes. - Every image must strictly match all numeric and descriptive details from its corresponding scene. - Each image must be square format (exactly 1024 1024 pixels (square, 1:1)). --- Now begin generating the images **scene by scene**, in strict order. Generate the image corresponding to the current scene. Each new image must correspond to the **next numbered scene** and must not duplicate any earlier scenes."
        },
        {
            "title": "Preprint",
            "content": "Image Generation Prompt Scene 1 Special Start Instruction: Before generating Scene 1 image, explicitly output: \"Now generating Scene 1 image (Pixar-style, at exactly 1024 1024 pixels (square, 1:1)).\" Then explicitly output the Scene 1 description in full. After that, generate Scene 1 image. After finishing any scene image, explicitly output: \"Scene image finished, now generate Scene x+1 image (Pixarstyle, at exactly 1024 1024 pixels (square, 1:1)).\" Then explicitly output the Scene X+1 description in full. After that, generate Scene X+1 image. After all scenes images generated, explicitly output: \"All finished!\" ####################################### The {num_scenes} scenes are described below: ... D.4 CONTROLLED INTERFERENCE To increase reasoning difficulty and avoid trivial visual shortcuts, we introduce two forms of controlled interference during scene allocation: Perception interference: additional objects or backgrounds are inserted that enrich the scene visually but have no mathematical relevance. For example, extra decorative items in shop scene. Semantic interference: distractor entities are introduced that are semantically close to the target objects, such as displaying both apples and pears when only apples matter for the computation. This makes the reasoning process more challenging while preserving solvability. Controlled Interference Example Original Problem: Violetta wants to buy new crayons. She needs them in 5 different colors and prepared $20 for this purchase. One crayon costs $2. How much change will she get? Structured JSON: { \"scenes\": [ { \"scene_id\": 1, \"interfere\": \"semantic\", \"scene_math_information\": [ {\"raw_math_information\": \"One crayon costs $2.\", \" object\": \"crayon\", \"math_value\": \"$2\", \"semantic\": \"one crayons price\", },"
        },
        {
            "title": "Preprint",
            "content": "Controlled Interference Example { \"raw_math_information\": \"interference: One eraser costs $1.\", \"object\": \"eraser\", \"math_value\": \"$1\", \"semantic\": \"one erasers price\", }, { \"raw_math_information\": \"interference: One pencil costs $1.50.\", \"object\": \"pencil\", \"math_value\": \"$1.50\", \"semantic\": \"one pencils price\", } ] }, { \"scene_id\": 2, \"interfere\": \"perception\", \"scene_math_information\": [ { \"raw_math_information\": \"She needs them in 5 different colors.\", \"object\": \"crayon\", \"math_value\": \"5\", \"semantic\": \"Violettas need for different color crayon\", }, { \"raw_math_information\": \"interference: There is blue backpack on the counter.\", \"object\": \"backpack\", \"math_value\": \"1\", \"semantic\": \"on the counter next to the crayons\", } ] }, { \"scene_id\": 3, \"interfere\": \"none\", \"scene_math_information\": [ { \"raw_math_information\": \"Violetta prepared $20 for this purchase.\", \"object\": \"us bill\", \"math_value\": \"$20\", \"semantic\": \"Violetta prepared money\", } ] },"
        },
        {
            "title": "Preprint",
            "content": "Controlled Interference Example { \"scene_id\": 4, \"interfere\": \"none\", \"scene_math_information\": [ { get?\", \"raw_math_information\": \"How much change will she \"object\": \"Violetta\", \"math_value\": \"\", \"semantic\": \"How much change will get after buying these crayons?\", } ] } ] } Rendered Scenes: (a) Scene 1: Semantic interference (b) Scene 2: Perception interference (c) Scene 3: No interference (d) Scene 4: No interference Explanation. Scene 1 inserts semantically close distractors (eraser, pencil) as semantic interference. Scene 2 adds irrelevant but visually salient objects (backpack) as perception interference. Scenes 3 and 4 contain no interference and faithfully encode the original problem."
        },
        {
            "title": "Preprint",
            "content": "D."
        },
        {
            "title": "IMAGE GENERATION PROMPTS AND VERIFICATION PRINCIPLES",
            "content": "The prompts for GPT-Image-1 (OpenAI, 2025b) are constructed from the structured scene descriptions, ensuring that each (object, action, composition) tuple is faithfully rendered. To maintain benchmark quality, we apply iterative human-in-the-loop verification guided by the 3C principles: Consistency: generated images must preserve the entities, quantities, and constraints of the original text, with any modification remaining semantics-preserving. Completeness: all information required to solve the problem must be visually available without relying on external assumptions. Compliance: images must satisfy safety and formatting standards, avoid sensitive content, and ensure clarity of objects and legible numerals. Items violating these principles are refined or, in severe cases, re-rendered with modified scene descriptions."
        },
        {
            "title": "E ERROR ANALYSIS",
            "content": "E.1 PERCEPTION CALCULATION ERRORS Error Case Study: Flower Package Text Problem. Vincent can buy flowers in packages of 3 for $2.50 or in packages of 2 for $1. How much money does he save by buying 18 flowers at the better price? GSM8K-V Images. Scene 1 Scene Ground Truth: 6 Better option is buying 9 packs of 2 flowers at $1 each (total $9) instead of 6 packs of 3 flowers at $2.50 each (total $15). Thus, saving = $15 - $9 = 6."
        },
        {
            "title": "Preprint",
            "content": "Error Case Study: Flower Package Gemini-2.5-Pro (q Wrong) Answer: Here is the step-by-step solution: 1. **Determine the price and quantity per pack for each flower type.** * The price list shows blue flowers (blue heart) cost $2.50 per pack and red flowers (red heart) cost $1 per pack. * The images of the flower arrangements show that there are 3 flowers in each pack (3 blue flowers in the larger vase and 3 red flowers in the smaller vase). 2. **Identify the better price.** * The cost for pack of 3 blue flowers is $2.50. * The cost for pack of 3 red flowers is $1. * Comparing the two, the red flowers are cheaper and therefore the better price. 3. **Calculate the cost of 18 flowers at the more expensive price.** * The more expensive flowers are the blue ones. * To buy 18 flowers, you would need 18 / 3 = 6 packs. * The total cost for 6 packs of blue flowers is 6 * $2.50 = $15. 4. **Calculate the cost of 18 flowers at the better price.** * The better priced flowers are the red ones. * To buy 18 flowers, you would need 18 / 3 = 6 packs. * The total cost for 6 packs of red flowers is 6 * $1 = $6. 5. **Calculate the total savings.** * The savings are the difference between the more expensive option and the cheaper option. * Savings = $15 - $6 = $9. FINAL ANSWER: 9 Analysis: The model assumed red packages contained 3 flowers (instead of 2), leading to miscalculation of packs and an overestimated saving. GPT-4o (q Wrong) Answer: To determine how much is saved by buying 18 flowers at the better price, we need to calculate the cost of buying 18 flowers at each price and find the difference. 1. **Price for blue flowers**: $2.50 per pack 2. **Price for red flowers**: $1.00 per pack Assuming each pack contains the same number of flowers, buying at the cheaper price (red flowers) will save money. **Calculate the cost for 18 flowers:** - **Cost at $2.50 per pack**: - Total cost = 18 flowers $2.50 = $45.00 - **Cost at $1.00 per pack**: - Total cost = 18 flowers $1.00 = $18.00 **Calculate the savings**: - Savings = Cost at $2.50 per pack - Cost at $1.00 per pack - Savings = $45.00 - $18.00 = $27.00 FINAL ANSWER: 27 Analysis: The model ignored the pack sizes altogether, treating prices as if they were per-flower."
        },
        {
            "title": "Preprint",
            "content": "Error Case Study: Flower Package Qwen2.5-VL-72B (q Wrong) Answer: To solve the problem, we need to determine how much money is saved by buying 18 flowers at the better price. Lets break it down step by step. 1. Identify the prices of the flower packages: - Blue flowers: $2.50 per pack - Red flowers: $1.00 per pack 2. Determine which price is better: - The red flowers are cheaper at $1.00 per pack compared to $2.50 per pack for blue flowers. 3. Calculate the cost of buying 18 flowers at the higher price (blue flowers): - Cost = 18 packs * $2.50/pack = $45.00 4. Calculate the cost of buying 18 flowers at the lower price (red flowers): - Cost = 18 packs * $1.00/pack = $18.00 5. Determine the savings by subtracting the lower cost from the higher cost: - Savings = $45.00 - $18.00 = $27.00 Thus, the amount saved by buying 18 flowers at the better price is: FINAL ANSWER: 27 Analysis: Like GPT-4o, this model ignored the actual number of flowers per pack, conflating pack cost with per-flower pricing."
        },
        {
            "title": "Preprint",
            "content": "Error Case Study: Lumber Resale Text Problem. Sasha notices that prices for lumber have gone up 50% in the last few months after she bought some lumber. She has leftover thirteen 2x4x10 boards that cost her $10 each. She also has six 4x4x10 boards she bought for $16 each. If she sells them all, how much profit does she make? GSM8K-V Images. Scene 1 Scene 2 Scene Scene 4 Ground Truth: 113 Total cost = 1310+616 = 208. Selling price after +50% increase = 2081.5 = 321. Profit = 321 208 = 113. Gemini-2.5-Pro (q Wrong) Answer: To find the profit from selling all the leftover boards, we need to follow these steps: 1. **Determine the Number of Boards:** - From the first image, there are 4 boards shown, plus 1 more mentioned. So, there are 5 boards of size 2 4 10. - From the second image, there are 4 boards of size 4 4 10. 2. **Calculate the Selling Price:** - The 2 4 10 boards sell for $10 each. - The 4 4 10 boards sell for $16 each. 3. **Calculate the Total Selling Price:** - Total for 2 4 10 boards: 5 10 = 50 dollars. - Total for 4 4 10 boards: 4 16 = 64 dollars. - Total selling price for all boards: 50 + 64 = 114 dollars. 4. **Determine the Original Cost:** - The pie chart indicates that the current price is 60% of the original price, meaning the original price was 40% higher. - Let be the original price. Then, 0.6x = current price. - Solving for x, we get = current price 5. **Calculate the Original Cost for Each Board:** - Original cost for 2 4 10 0.6 = 16.67 dollars each. - Original cost for 4 4 10 boards: 16 boards: 10 0.6 = 26.67 dollars each. 6. **Calculate the Total Original Cost:** - Total original cost for 2 4 10 boards: 5 16.67 = 83.35 dollars. - Total original cost for 4 4 10 boards: 4 26.67 = 106.68 dollars. - Total original cost for all boards: 83.35 + 106.68 = 190.03 dollars. 7. **Calculate the Profit:** - Profit = Total selling price - Total original cost - Profit = 114 190.03 = 76.03 dollars. The person will incur loss, not profit. FINAL ANSWER: -76.03 0. . Analysis: The model undercounted boards (5 vs. 13) and inverted the meaning of the price chart, producing large negative profit."
        },
        {
            "title": "Preprint",
            "content": "Error Case Study: Lumber Resale GPT-4o (q Wrong) Answer: To find the profit from selling all the leftover boards, we need to follow these steps: 1. **Determine the Number of Boards:** - From the first image, there are 4 boards shown, plus 1 more mentioned. So, there are 5 boards of size 2 4 10. - From the second image, there are 4 boards of size 4 4 10. 2. **Calculate the Selling Price:** - The 2 4 10 boards sell for $10 each. - The 4 4 10 boards sell for $16 each. 3. **Calculate the Total Selling Price:** - Total for 2 4 10 boards: 5 10 = 50 dollars. - Total for 4 4 10 boards: 4 16 = 64 dollars. - Total selling price for all boards: 50 + 64 = 114 dollars. 4. **Determine the Original Cost:** - The pie chart indicates that the current price is 60% of the original price, meaning the original price was 40% higher. - Let be the original price. Then, 0.6x = current price. - Solving for x, we get = current price 5. **Calculate the Original Cost for Each Board:** - Original cost for 2 4 10 boards: 10 0.6 = 16.67 dollars each. - Original cost for 4 4 10 boards: 16 0.6 = 26.67 dollars each. 6. **Calculate the Total Original Cost:** - Total original cost for 2 4 10 boards: 5 16.67 = 83.35 dollars. - Total original cost for 4 4 10 boards: 4 26.67 = 106.68 dollars. - Total original cost for all boards: 83.35 + 106.68 = 190.03 dollars. 7. **Calculate the Profit:** - Profit = Total selling price - Total original cost - Profit = 114 190.03 = 76.03 dollars. The person will incur loss, not profit. FINAL ANSWER: -76.03 0.6 . Analysis: GPT-4o followed nearly identical faulty reasoning as Gemini, showing consistent perception and price interpretation errors."
        },
        {
            "title": "Preprint",
            "content": "Error Case Study: Lumber Resale Qwen2.5-VL-72B (q Wrong) Answer: To solve the problem, lets break it down step by step: 1. **Identify the number and price of each type of board:** - The woman has 3 boards that are 2x4x10, priced at $10 each. - She also has 4 boards that are 4x4x10, priced at $16 each. 2. **Calculate the total revenue from selling all the boards:** - Revenue from 2x4x10 boards: 3 10 = 30 dollars. - Revenue from 4x4x10 boards: 4 16 = 64 dollars. - Total revenue: 30 + 64 = 94 dollars. 3. **Determine the original cost of the lumber:** - The current price is 60% of the original price, meaning the original price is 100 60 times the current price. - For 2x4x10 boards: - Current price per board: $10 - Original price per board: 10 100 3 16.67 dollars. - Total original cost for 3 boards: 3 50 3 = 50 dollars. - For 4x4x10 boards: - Current price per board: $16 - Original price per board: 16 100 3 26.67 dollars. - Total original cost for 4 boards: 4 80 3 106.67 dollars. - Total original cost: 50 + 320 4. **Calculate the profit:** - Profit = Total revenue - Total original cost - Profit = 94 470 Since the profit is negative, it indicates loss. However, the problem asks for the profit, so we consider the absolute value. FINAL ANSWER: -62.67 3 = 320 3 156.67 dollars. 3 62.67. 3 = 188 3 = 282 3 = 150 3 = 470 3 + 320 3 60 = 50 60 = 80 Analysis: The model drastically undercounted the number of boards (3 instead of 13), which reduced total revenue, and also misapplied the price-scaling rule. Both perception and arithmetic propagation errors compounded, flipping the profit into loss."
        },
        {
            "title": "Preprint",
            "content": "E."
        },
        {
            "title": "INSTRUMENT READING ERRORS",
            "content": "Error Case Study: Premiere Arrival Text Problem. Wayne and Bernadette are movie stars heading to the premiere of their latest film. Bernadette wants to arrive 5 minutes before Wayne. Wayne is staying at hotel close to the premiere theater, and Bernadette is staying at her high-rise apartment in the same city. The drive from Bernadettes apartment takes four times as long as the drive from Waynes hotel. If it takes Wayne four minutes to be driven to the theater, how much earlier should Bernadette leave than Wayne to get to the theater first? GSM8K-V Images. Scene 1 Scene 2 Scene Scene 4 Scene 5 Ground Truth: 17 Waynes drive time is 4 minutes. Bernadettes drive time is four times longer, i.e., 4 4 = 16 minutes. Since she wants to arrive 5 minutes earlier than Wayne, she must leave 16 + 5 = 16 + 5 = 2121 minutes earlier than Wayne. Thus, the required difference is 17 minutes."
        },
        {
            "title": "Preprint",
            "content": "Error Case Study: Premiere Arrival Gemini-2.5-Pro (q Wrong) Answer: To determine how much earlier Bernadette should leave than Wayne, we need to analyze the given information: 1. Bernadette wants to arrive 5 minutes earlier than Wayne. 2. The pie chart shows that Bernadettes drive time is 80 The difference in their drive times is 80 Since Bernadette wants to arrive 5 minutes earlier, this 60 To find the relationship between their drive times: - Let Waynes drive time be . - Bernadettes drive time is 0.8T . The difference in drive times is 0.8T = 0.2T . Since Bernadettes drive time is longer, we have: 0.8T = + 5 0.2T = 5 = 25 minutes So, Waynes drive time is 25 minutes, and Bernadettes drive time is 0.8 * 25 = 20 minutes. The difference is 5 minutes, which is what Bernadette wants. Therefore, Bernadette should leave 5 minutes earlier than Wayne to arrive 5 minutes earlier. FINAL ANSWER: Analysis: The model misinterpreted the pie chart proportions, treating 80 GPT-4o (q Wrong) Answer: To solve the problem, lets break down the information: 1. **Waynes Travel Time:** - Wayne leaves at 8:00 and arrives at 8:30. - Total travel time for Wayne = 30 minutes. 2. **Drive Time Distribution:** - Bernadettes drive time is 80Waynes drive time is 20 3. **Calculate Total Drive Time:** - Waynes drive time (20Therefore, 1Total drive time = 1.5 minutes 100 = 150 minutes. 4. **Bernadettes Drive Time:** - Bernadettes drive time (80 5. **Bernadettes Arrival Time:** - Bernadette wants to arrive 5 minutes before Wayne, so she arrives at 8:25. 6. **Calculate Bernadettes Departure Time:** - Bernadettes travel time = 120 minutes. - Bernadettes arrival time = 8:25. - Bernadettes departure time = 8:25 - 120 minutes = 6:25. 7. **Determine How Much Earlier Bernadette Should Leave:** - Wayne leaves at 8:00. - Bernadette leaves at 6:25. - Difference in departure times = 8:00 - 6:25 = 95 minutes. FINAL ANSWER: 95 Analysis: Here the model over-relied on absolute timestamps, hallucinating Waynes departure/arrival times from clocks. This misreading of clock faces inflated Bernadettes required lead time by more than 5."
        },
        {
            "title": "Preprint",
            "content": "Error Case Study: Premiere Arrival Qwen2.5-VL-72b (q Wrong) Answer: To solve the problem, lets break down the information given in the images step by step: 1. **Image 1 and Image 2**: These images show two characters, Bernadette and Wayne, who are planning to arrive at premiere. The clock in both images shows the same time, indicating that they are leaving at the same time. 2. **Image 3**: This image provides pie chart showing the drive times for Bernadette and Wayne. Bernadettes drive time is 80 3. **Image 4**: Bernadette wants to arrive 5 minutes before Wayne. 4. **Image 5**: We need to determine how much earlier Bernadette should leave compared to Wayne. Lets denote: - as the total drive time. - TB as Bernadettes drive time. - TW as Waynes drive time. From the pie chart: TB = 0.8T TW = 0.2T Since Bernadette wants to arrive 5 minutes before Wayne: Substitute the values of TB and TW : TB 5 = TW Solve for : 0.8T 5 = 0.2T 0.8T 0.2T = 5 0.6T = = 5 0.6 = 25 3 8.33 minutes Now, we need to find out how much earlier Bernadette should leave: TB = 0.8 TW = 0.2 25 3 25 3 = = 20 3 5 6.67 minutes 1.67 minutes Bernadette needs to leave: 20 3 5 + 5 = 15 3 + 5 = 5 + 5 = 10 minutes earlier FINAL ANSWER: 10 Analysis: The model partially recognized the 4 ratio but failed to align it with Waynes absolute drive time (4 minutes). Miscalibration of proportional reasoning with clockbased adjustments produced near-miss answer."
        },
        {
            "title": "Preprint",
            "content": "Error Case Study: Dog Exercise Text Problem. Bernie has 4 dogs. They each need certain amount of exercise per day. The first needs to walk 1 mile. The second needs to walk 4 miles. The third needs to walk 3 miles. On average, they need to walk 3 miles per day. How many miles does the last dog need? GSM8K-V Images. Scene 1 Scene 2 Scene Scene 4 Scene 5 Scene 6 Ground Truth: 4 The first three dogs walk 1 + 4 + 3 = 8 miles. For an average of 3 miles across 4 dogs, the total required is 4 3 = 4 3 = 1212 miles. Thus, the fourth dog must walk 12 8 = 12 8 = 44 miles. Gemini-2.5-Pro ( Correct) Answer: Here is step-by-step solution to the problem: 1. **Identify the total number of dogs:** The first image shows man with his four dogs. So, the total number of dogs is 4. 2. **Gather the known information:** The next three images provide the walking distances for three of the four dogs. * Dog 1 (beagle) walks 1 mile. * Dog 2 (black dog) walks 4 miles. * Dog 3 (white dog) walks 3 miles. 3. **Determine the target average distance:** The fifth image states, Each dog should get the average amount of exercise. The man is pointing to gauge where the needle points to the third major tick mark. The unit on the gauge is miles. This indicates that the target average walking distance for each dog is 3 miles. 4. **Set up the problem mathematically:** We need to find the distance the fourth dog must walk so that the average distance for all four dogs is 3 miles. Let be the distance the last dog needs to walk. 5. **Use the formula for the average (mean):** Average = (Sum of all values) / (Number of values) 6. **Substitute the known values into the formula:** 3 = (1 + 4 + 3 + x) / 4 7. **Solve the equation for x:** * First, sum the known distances: 1 + 4 + 3 = 8. * The equation becomes: 3 = (8 + x) / 4. * Multiply both sides by 4: 12 = 8 + x. * Subtract 8: = 4. The last dog needs to walk 4 miles for the average distance of all dogs to be 3 miles. FINAL ANSWER: 4 Analysis: Gemini-2.5-Pro correctly read the gauge (3 miles) and aligned it with the arithmetic, giving the correct result."
        },
        {
            "title": "Preprint",
            "content": "Error Case Study: Dog Exercise GPT-4o (q Wrong) Answer: To solve the problem, we need to determine how many miles the last dog needs to walk so that each dog gets the average amount of exercise. 1. **Identify the distances each dog walks:** - First dog: 1 mile - Second dog: 4 miles - Third dog: 3 miles 2. **Calculate the total distance walked by the first three dogs:** 1 + 4 + 3 = 8 miles 3. **Determine the average distance each dog should walk:** - There are 4 dogs in total. - The average distance per dog is 0.4 miles. 4. **Calculate the total distance all dogs should walk:** 4 0.4 = 1.6 miles 5. **Determine the distance the last dog needs to walk:** - Total distance needed for all dogs: 1.6 miles - Distance already walked by the first three dogs: 8 miles - Distance the last dog needs to walk: 1.6 8 = 6.4 miles Since negative distance doesnt make sense, it indicates that the average distance of 0.4 miles per dog is not achievable with the given distances. Therefore, the problem might be interpreted differently, or there might be an error in the setup. FINAL ANSWER: -6.4 Analysis: GPT-4o misread the gauge, interpreting the 3 miles tick as 0.4 miles. This perception error cascaded into nonsensical negative result."
        },
        {
            "title": "Preprint",
            "content": "Error Case Study: Dog Exercise Qwen2.5-VL-72b (q Wrong) Answer: To solve the problem, lets break down the information given in the images: 1. There are four dogs, each with specified distance they need to walk: - The first dog needs to walk 1 mile. - The second dog needs to walk 4 miles. - The third dog needs to walk 3 miles. - The fourth dogs distance is unknown. 2. The man wants each dog to get the average amount of exercise. This means the total distance walked by all dogs should be evenly distributed among them. Lets denote the unknown distance for the fourth dog as x. The total distance walked by all dogs should be: The average distance each dog should walk is: 1 + 4 + 3 + 1 + 4 + 3 + 4 Since the average distance each dog should walk is the same as the distance each dog actually walks, we set up the equation: 1 + 4 + 3 + 4 = 8 + 4 This average distance should be equal to the distance each dog walks. Therefore, we need to find such that the total distance divided by 4 equals the average distance. Given that the man has already walked 0.4 miles, we need to find the remaining distance for the fourth dog to make the average correct. The total distance walked by the first three dogs is: Lets denote the total distance for all four dogs as D: 1 + 4 + 3 = 8 miles The average distance each dog should walk is: = 8 + 4 = 8 + 4 Since the man has already walked 0.4 miles, the remaining distance for the fourth dog is: = 4 Solving for x: (cid:19) (cid:18) 8 + 8 = 8 + 8 = 2 Therefore, the fourth dog needs to walk 2 miles. FINAL ANSWER: 2 Analysis: Qwen2.5-VL-72B misread the gauge (treating it inconsistently as 0.4 miles), then tangled the algebra, outputting 2 instead of 4. The failure stems from incorrect gauge interpretation rather than arithmetic weakness."
        },
        {
            "title": "F CATEGORY CASE STUDY",
            "content": "Case Study 1: Measurement (Length / Area / Volume) Text Problem. The rug is 5 feet wider than the chair. The couch is 2 feet longer than twice the width of the rug. If the chair is 3 feet wide. How many feet long is the couch? GSM8K-V Images. Scene 1 Scene 2 Scene Ground Truth: 18 Chair = 3 = 33 feet; ### Rug = 3 + 5 = 3 + 5 = 88 feet; ### Couch = 2 8 + 2 = 2 8 + 2 = 1818 feet; ### The couch is 18 = 1818 feet long. Gemini-2.5-Pro (q Wrong) Answer: Step 1: Determine the length of one rug. The first image shows single blue rug. chair is placed on the left side of it. The image shows the rug is composed of two sections: The length covered by the chair, which is 3 feet. The remaining length of the rug, which is 5 feet. Total length of one rug: 3 + 5 = 8 feet. Step 2: Determine the total length of the two rugs combined. The second image shows two identical rugs placed side-by-side along their length. Total length: 2 8 = 16 feet. Step 3: Calculate the length of the couch. There is space of 2 feet on the left and 2 feet on the right. Couch length = 16 2 2 = 12 feet. Final Answer:"
        },
        {
            "title": "Preprint",
            "content": "Case Study 1: Measurement (Length / Area / Volume) GPT-4o (q Wrong) Answer: Step 1: Analyze the first image. The chair width is labeled 3 feet. The visible rug width (horizontal, beside the chair) is labeled 5 feet. Therefore, the length of the rug is also 5 feet. Step 2: Analyze the second image. couch is on two rugs, each measuring 2 feet wide. Total width under the couch: 2 + 2 = 4 feet. Step 3: Analyze the third image. The question explicitly asks for the length of the couch. Since the rugs are placed side by side under the couch, the model concludes that the length of the couch equals the total width of the rugs. Final Answer: 4 Qwen2.5-VL-72b (q Wrong) Answer: Step 1: Determine. To determine the length of the couch, we need to analyze the given images and the information provided. Step 2: Images Content Analyze The first image shows chair with dimensions 3 ft on rug with dimensions by 5 ft. The second image shows couch on rug with dimension of 2 ft along one side. The third image asks how many feet long the couch is. Step 3: Calculate From the second image, we can see that the couch is placed on rug, and the rugs dimension along the length of the couch is 2 ft. Since the couch fits within this dimension, it implies that the length of the couch is equal to or less than 2 ft. Given that the couch appears to fully occupy this dimension, we can conclude that the length of the couch is 2 ft. Final Answer:"
        },
        {
            "title": "Preprint",
            "content": "Case Study 2: Signboard & Icon (Price) Text Problem. Tyler wants to buy dictionary that costs $18, dinosaur book that costs $13, and childrens cookbook that costs $8. He has saved $14 from his allowance. If Tyler earns $5 per hour by washing, how many hours does he have to work to afford his books? GSM8K-V Images. Scene 1 Scene 2 Scene Scene 4 Scene 5 Ground Truth: 5 Total cost = 18 + 13 + 8 = 18 + 13 + 8 = 3939; ### Money still needed = 39 14 = 39 14 = 2525; ### Hours needed = 25 5 = 25/5 = 55. Gemini-2.5-Pro (q Wrong) Answer: Here is the step-by-step solution: 1. **Calculate the total cost of the books:** * The boy wants to buy three books: dictionary, dinosaur book, and cookbook. * The price list shows: * Red book (Dictionary): $18 * Green book (Dinosaur): $13 * Yellow book (Cookbook): $8 * Total cost = $18 + $13 + $8 = $39. 2. **Determine how much more money is needed:** * The boy has already saved $14. * Money still needed = Total cost - Money saved * Money still needed = $39 - $14 = $25. 3. **Identify the hourly wage for the job:** * In the fourth image, the woman is pointing to the job associated with the dog emoji, which pays $4 per hour. She confirms this is the boys wage. * The boy then asks about the hours needed for washing. Combining these facts, we can conclude his job is washing dogs for $4 per hour. 4. **Calculate the number of hours needed to work:** * To find out how many hours the boy needs to work, divide the amount of money he needs by his hourly wage. * Hours to work = $25 / $4 = 6.25 hours. FINAL ANSWER: 6."
        },
        {
            "title": "Preprint",
            "content": "Case Study 2: Signboard & Icon (Price) GPT-4o (q Wrong) Answer: To determine how many hours Tyler needs to work, we need to follow these steps: 1. **Identify the Cost of the Books:** - Dictionary: $18 - Dinosaur book: $13 - Cookbook: $8 2. **Calculate the Total Cost of the Books:** 18 + 13 + 8 = 39 The total cost of the books is $39. 3. **Determine Tylers Current Savings:** - Tyler has saved $14. 4. **Calculate the Additional Money Needed:** 39 14 = 25 Tyler needs an additional $25. 5. **Identify Tylers Hourly Wage for Washing:** - Washing pays $4 per hour. 6. **Calculate the Number of Hours Needed:** 25 4 = 6.25 Since Tyler cannot work fraction of an hour, he will need to work 7 hours to afford his books. FINAL ANSWER: 7 Qwen2.5-VL-72B ( Correct) Answer: To solve this problem, we need to follow these steps: 1. Calculate the total cost of the books Tyler wants to buy. The dictionary costs $18, the dinosaur book costs $13, and the childrens cookbook costs $8. Adding these amounts together gives us the total cost of the books: 18 + 13 + 8 = 39 So, the total cost of the books is $39. 2. Determine how much more money Tyler needs. Tyler has already saved $14 from his allowance. 39 14 = 25 So, Tyler needs an additional $25. 3. Calculate how many hours Tyler needs to work. Tyler earns $5 per hour. So, Tyler needs to work 5 hours. FINAL ANSWER: 5 25 5 ="
        },
        {
            "title": "Preprint",
            "content": "Case Study 3: Physical Metric (Weight) Text Problem. The basketball team went to the steakhouse to eat dinner. The first player ate 6-ounce steak. The second player ate beef tips, containing 12 beef tips, each an ounce in size. The third player ate one-pound steak. And the fourth and fifth players ordered vegetarian meals. In total, how many ounces of meat were consumed by the team? GSM8K-V Images. Scene 1 Scene 2 Scene Scene 4 Scene 5 Ground Truth: 34 Player 1: 6 ounces; ### Player 2: 12 1 = 12 1 = 1212 ounces; ### Player 3: 1 pound = 1 lb = 16 oz16 ounces; ### Total = 6 + 12 + 16 = 34 = 3434 ounces. Gemini-2.5-Pro (q Wrong) Answer: * **Step 1: Analyze the first image.** The first image shows man who has eaten steak. The steak is on scale, and the scale reads 6 ounces. So, the first person ate 6 ounces of meat. * **Step 2: Analyze the second image.** The second image shows girl eating beef tips. One beef tip is on scale, which reads 1 ounce. On the plate, there are 9 more beef tips. In total, there are 1 + 9 = 10 beef tips. Since each beef tip weighs 1 ounce, the total weight of the beef tips is 10 * 1 = 10 ounces. So, the second person ate 10 ounces of meat. * **Step 3: Analyze the third image.** The third image shows boy who has eaten large steak. The steak is on scale, and the scale reads 1 pound. The question asks for the total weight in ounces, so we need to convert pounds to ounces. There are 16 ounces in 1 pound. So, the third person ate 16 ounces of meat. * **Step 4: Analyze the fourth image.** The fourth image shows two people eating salads. Their meals consist of vegetables and grains, with no visible meat. Therefore, they did not eat any meat. Their contribution to the total is 0 ounces. * **Step 5: Calculate the total amount of meat eaten.** Person 1: 6 ounces Person 2: 10 ounces Person 3: 16 ounces Total = 6 + 10 + 16 = 32 FINAL ANSWER:"
        },
        {
            "title": "Preprint",
            "content": "Case Study 3: Physical Metric (Weight) GPT-4o (q Wrong) Answer: To find the total ounces of meat eaten by the basketball team, lets analyze each image: 1. **First Image**: The steak weighs 6 ounces. 2. **Second Image**: Each beef tip weighs 1 ounce. There are 10 beef tips on the plate, so the total weight is: 10 1 = 10 ounces 3. **Third Image**: The steak weighs 1 pound. Since 1 pound equals 16 ounces, the weight is: 16 ounces 4. **Fourth Image**: There is no meat shown, so we assume no meat was consumed by these individuals. Now, add up all the ounces of meat: FINAL ANSWER: 32 6 + 10 + 16 = 32 ounces Qwen2.5-VL-72B (q Wrong) Answer: To solve the problem, we need to determine the total amount of meat consumed by the team. Lets break down the information provided in the images: 1. The first image shows person who ate steak weighing 6 ounces. 2. The second image shows person who ate beef tips weighing 1 ounce. 3. The third image shows person who ate steak weighing 1 pound (which is equivalent to 16 ounces). Now, lets add up the weights of the meat consumed: - Steak from the first image: 6 ounces - Beef tips from the second image: 1 ounce - Steak from the third image: 16 ounces Adding these together: 6 + 1 + 16 = 23 ounces So, the total amount of meat consumed by the team is 23 ounces. FINAL ANSWER:"
        },
        {
            "title": "Preprint",
            "content": "Case Study 4: Temporal(Clock) Text Problem. Every day Charisma meditates for 15 minutes when she first wakes up and again before she goes to sleep. 5 days week she practices 1 hour of yoga. In 4 weeks, how much time has she spent on meditation/yoga practice? GSM8K-V Images. Scene 1 Scene 2 Scene Scene 4 Scene 5 Scene 6 Scene 7 Ground Truth: 34 She meditates for 15 minutes in the morning and 15 minutes at night for total of 15 + 15 = 15 + 15 = 3030 minutes. She meditates 30 minutes daily so in 7 days/1 week she meditates 30 7 = 30 7 = 210210 minutes. She meditates 210 minutes in 1 week so in 4 weeks she meditates 210 4 = 210 4 = 840840 minutes. 60 minutes are in an hour so she meditates for 840/60 = 840/60 = 1414 hours. She practices yoga for 1 hour 5 days week so thats 1 5 = 1 5 = 55 hours. Over 4 weeks she practices yoga for 4 5 = 4 5 = 2020 hours. She spends 14 hours meditating and 20 hours practicing yoga for total of 14 + 20 = 14 + 20 = 3434 hours."
        },
        {
            "title": "Preprint",
            "content": "Case Study 4: Temporal(Clock) Gemini-2.5-Pro (q Wrong) Answer: Here is the step-by-step solution: 1. **Calculate the duration of each morning meditation session:** * Start time: 6:00 (from the first image). * End time: 6:15 (from the second image). * Duration: 6:15 - 6:00 = 15 minutes. 2. **Calculate the duration of each night meditation session:** * Start time: 9:45 (from the third image). * End time: 10:00 (from the fourth image). * Duration: 10:00 - 9:45 = 15 minutes. 3. **Calculate the total daily meditation time:** * Total daily time = Morning meditation + Night meditation * Total daily time = 15 minutes + 15 minutes = 30 minutes. 4. **Calculate the total meditation time over 4 weeks:** * Meditation is performed every day. * Number of days in 4 weeks = 4 weeks * 7 days/week = 28 days. * Total meditation time = 30 minutes/day * 28 days = 840 minutes. 5. **Calculate the duration of each yoga session:** * Start time: 6:15 (from the fifth image). * End time: 6:45 (from the sixth image). * Duration: 6:45 - 6:15 = 30 minutes. 6. **Calculate the total yoga time over 4 weeks:** * The calendar shows yoga is done on weekdays (Monday to Friday). * Number of yoga sessions in 4 weeks = 5 days/week * 4 weeks = 20 sessions. * Total yoga time = 30 minutes/session * 20 sessions = 600 minutes. 7. **Calculate the total time spent on all activities:** * Grand total time = Total meditation time + Total yoga time * Grand total time = 840 minutes + 600 minutes = 1440 minutes. 8. **Convert the total time from minutes to hours:** * There are 60 minutes in 1 hour. * Total hours = 1440 minutes / 60 minutes/hour = 24 hours. The total time spent meditating and doing yoga in 4 weeks is 24 hours. FINAL ANSWER:"
        },
        {
            "title": "Preprint",
            "content": "Case Study 4: Temporal(Clock) GPT-4o (q Wrong) Answer: To solve this problem, we need to calculate the total time spent meditating and doing yoga over 4 weeks. 1. **Morning Meditation:** - Starts at 6:00 AM and ends at 6:30 AM. - Duration: 30 minutes. 2. **Night Meditation:** - Starts at 9:00 PM and ends at 9:30 PM. - Duration: 30 minutes. 3. **Yoga Session:** - Starts at 5:00 PM and ends at 5:30 PM. - Duration: 30 minutes. Each activity is done daily. **Total Daily Time:** - Morning Meditation: 30 minutes - Night Meditation: 30 minutes - Yoga Session: 30 minutes Total daily time spent = 30 + 30 + 30 = 90 minutes **Total Time for 4 Weeks:** - 4 weeks = 28 days - Total time = 90 minutes/day 28 days = 2520 minutes Convert minutes to hours: - 2520 minutes 60 = 42 hours FINAL ANSWER: 42 Qwen2.5-VL-72b (q Wrong) Answer: To solve the problem, we need to determine how much time the person has spent meditating and doing yoga over period of 4 weeks. Step-by-Step Reasoning: 1. **Identify the daily meditation and yoga times:** - Morning meditation: The clock shows 7:00 AM. - Night meditation: The clock shows 9:00 PM. - Yoga session: The clock shows 6:00 AM. 2. **Calculate the duration of each activity:** - Morning meditation: Assume it lasts for 30 minutes (common duration). - Night meditation: Assume it also lasts for 30 minutes. - Yoga session: Assume it lasts for 60 minutes (common duration). 3. **Total daily time spent on activities:** - Morning meditation: 30 minutes - Night meditation: 30 minutes - Yoga session: 60 minutes - Total daily time = 30 + 30 + 60 = 120 minutes 4. **Convert daily time to hours:** - 120 minutes = 2 hours 5. **Calculate the total time over 4 weeks:** - Number of days in 4 weeks = 4 * 7 = 28 days - Total time = 2 hours/day * 28 days = 56 hours FINAL ANSWER:"
        },
        {
            "title": "Preprint",
            "content": "Case Study 5: Ratio & Percentage (Graph) Text Problem. There are 1.5 times as many boys as girls at Dr. Wertzs school. If there are 60 girls and 5 students to every teacher, how many teachers are there? GSM8K-V Images. Scene 1 Scene 2 Scene Scene 4 Ground Truth: 30 There are 1.5 60 = 1.5 60 = 9090 boys. Total students = 90 + 60 = 90 + 60 = 150150. Each teacher has 5 students, so teachers = 150/5 = 150/5 = 3030. Gemini-2.5-Pro ( Correct) Answer: Here is step-by-step solution to the problem: 1. **Find the total number of girls.** * The first image shows group of girls. By counting them, we see there are 10 girls in the picture (5 in the back row and 5 in the front row). * The teacher says, besides these, there are 50 more girls at the school. * So, the total number of girls is the sum of the girls in the picture and the additional girls: 10 + 50 = 60 girls. 2. **Find the total number of students.** * The second image shows pie chart representing the proportion of boys and girls at the school. * The chart is divided into slices. Lets count them: * Pink slices (girls): 8 * Blue slices (boys): 12 * The total number of slices is 8+12 = 20. * This means that girls make up 8/20 = 2/5 of the student population. * We know from step 1 that there are 60 girls. * (2/5) = 60 = 150. * There are 150 students in total at the school. 3. **Find the number of teachers.** * The third image shows pie chart representing the proportion of students and teachers at the school. * Lets count the slices in this chart: * Blue slices (students): 10 * Gray slices (teachers): 2 * This shows that the ratio of students to teachers is 10 to 2, which simplifies to 5 to 1. * We know from step 2 that there are 150 students. * 150/T = 5/1 = 30. 4. **Answer the final question.** * The last image asks, How many teachers are at the school? * Based on our calculation, there are 30 teachers at the school. FINAL ANSWER:"
        },
        {
            "title": "Preprint",
            "content": "Case Study 5: Ratio & Percentage (Graph) GPT-4o (q Wrong) Answer: To solve the problem, lets analyze the images step by step: 1. **First Image**: There are 10 girls shown, and the speech bubble mentions there are 50 more girls at the school. So, the total number of girls is: 10 + 50 = 60 2. **Second Image**: The pie chart shows the ratio of boys to girls. There are 12 sections in total, with 3 sections for girls and 9 sections for boys. This means the ratio of boys to girls is 9:3 = 3:1. Therefore, the number of boys is: 3 60 = 180 3. **Third Image**: The pie chart shows the ratio of students to teachers. There are 10 sections in total, with 8 sections for students and 2 sections for teachers. This means the ratio of students to teachers is 8:2 = 4:1. 4. **Calculate Total Students**: 180 + 60 = 240 5. **Calculate Number of Teachers**: 240/4 = 60 FINAL ANSWER: Qwen2.5-VL-72b (q Wrong) Answer: Lets break down the problem step by step using the information provided in the images. Step 1: Determine the number of girls and boys at the school. - From the first image, we see group of 10 girls. The speech bubble indicates that there are 50 more girls at the school. - Total number of girls = 10 (in the image) + 50 (additional girls) = 60 girls. - The second image shows pie chart with sections for boys and girls. The section for girls is larger than the section for boys, but it doesnt give exact numbers. - Lets assume the total number of students is . - If the pie chart is divided into 10 segments and girls occupy 6 segments, then boys occupy 4 segments. - This means the ratio of girls to boys is 6:4 = 3:2. - So, if girls are 60, boys would be (2/3) 60 = 40. - Total number of students = 60 + 40 = 100. Step 2: Determine the number of teachers. - The third image shows pie chart with small grey section representing teachers. - If the grey section is 1/10 of the pie chart, then the number of teachers is (1/10) 100 = 10. FINAL ANSWER:"
        },
        {
            "title": "Preprint",
            "content": "Case Study 6: Other (Count) Text Problem. Janets ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers market? GSM8K-V Images. Scene 1 Scene 2 Scene Scene 4 Scene 5 Ground Truth: 18 Janet sells 1634 = 1634 = 99 duck eggs day. She makes 92 = $92 = 1818 every day at the farmers market. Gemini-2.5-Pro (q Wrong) Answer: Here is step-by-step solution based on the information in the images: 1. **Calculate the total number of eggs collected each day. ** * In the first image, there are 7 eggs in the nest. * The woman states that besides those, there are 8 more eggs each day. * Total daily eggs = 7 + 8 = 15 eggs. 2. **Calculate the number of eggs used each day. ** * In the second image, she eats 3 eggs for breakfast. * In the third image, she uses 4 eggs to bake muffins. * Total eggs used = 3 + 4 = 7 eggs. 3. **Determine the number of eggs available to sell. ** * Eggs for sale = 15 - 7 = 8 eggs. 4. **Calculate the total earnings from the market. ** * Each egg is sold for $2. * Total earnings = 8 2 = 16. FINAL ANSWER:"
        },
        {
            "title": "Preprint",
            "content": "Case Study 6: Other (Count) GPT-4o (q Wrong) Answer: To solve the problem, lets analyze the images step by step: 1. **First Image**: There are 8 eggs in the nest, and 8 more eggs are added each day. 2. **Second Image**: She eats 3 eggs for breakfast. 3. **Third Image**: She uses 4 eggs to make muffins. 4. **Fourth Image**: She sells eggs at $2 each. 5. **Fifth Image**: We need to calculate how much she makes every day at the market. **Calculations**: - Total eggs produced each day: 8 eggs. - Eggs used for breakfast: 3 eggs. - Eggs used for muffins: 4 eggs. - Total eggs used: 3 + 4 = 7 eggs. - Eggs available for sale: 8 - 7 = 1 egg. - Income from selling eggs: 1 $2 = $2. FINAL ANSWER: 2 Qwen2.5-VL-72b (q Wrong) Answer: To solve the problem, lets break down the information given in the images step by step: 1. **Eggs in the nest**: The first image states that there are 8 more eggs each day in the nest. 2. **Breakfast**: The second image shows that some eggs are used for breakfast, but it doesnt specify how many. 3. **Muffins**: The third image indicates that some eggs are used to make muffins for friends, but again, it doesnt specify the number. 4. **Selling eggs**: The fourth image shows that she sells fresh duck eggs for $2 each. 5. **Daily earnings**: The fifth image asks how much she makes every day at the market. Since the exact number of eggs used for breakfast and muffins is not specified, well assume all 8 eggs are sold each day for simplicity. - Each egg is sold for $2. - She has 8 eggs each day. Now, calculate the daily earnings: Daily earnings = 8 2 = 16 Thus, her daily earnings from selling eggs are $16. FINAL ANSWER:"
        }
    ],
    "affiliations": [
        "Zhejiang University"
    ]
}