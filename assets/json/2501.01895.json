{
    "paper_title": "EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation",
    "authors": [
        "Siyuan Huang",
        "Liliang Chen",
        "Pengfei Zhou",
        "Shengcong Chen",
        "Zhengkai Jiang",
        "Yue Hu",
        "Peng Gao",
        "Hongsheng Li",
        "Maoqing Yao",
        "Guanghui Ren"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce EnerVerse, a comprehensive framework for embodied future space generation specifically designed for robotic manipulation tasks. EnerVerse seamlessly integrates convolutional and bidirectional attention mechanisms for inner-chunk space modeling, ensuring low-level consistency and continuity. Recognizing the inherent redundancy in video data, we propose a sparse memory context combined with a chunkwise unidirectional generative paradigm to enable the generation of infinitely long sequences. To further augment robotic capabilities, we introduce the Free Anchor View (FAV) space, which provides flexible perspectives to enhance observation and analysis. The FAV space mitigates motion modeling ambiguity, removes physical constraints in confined environments, and significantly improves the robot's generalization and adaptability across various tasks and settings. To address the prohibitive costs and labor intensity of acquiring multi-camera observations, we present a data engine pipeline that integrates a generative model with 4D Gaussian Splatting (4DGS). This pipeline leverages the generative model's robust generalization capabilities and the spatial constraints provided by 4DGS, enabling an iterative enhancement of data quality and diversity, thus creating a data flywheel effect that effectively narrows the sim-to-real gap. Finally, our experiments demonstrate that the embodied future space generation prior substantially enhances policy predictive capabilities, resulting in improved overall performance, particularly in long-range robotic manipulation tasks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 5 9 8 1 0 . 1 0 5 2 : r EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation Siyuan Huang2,4, Liliang Chen1,, Pengfei Zhou1,5, Shengcong Chen1, Zhengkai Jiang6, Yue Hu1,7, Peng Gao2, Hongsheng Li2,3, Maoqing Yao1,, Guanghui Ren1, 1AgiBot, 2Shanghai AI Lab, 3CUHK, 4SJTU, 5FDU, 6HKUST, 7HIT, * Indicates equal contribution, indicates project leader and indicates corresponding author. We introduce EnerVerse, comprehensive framework for embodied future space generation specifically designed for robotic manipulation tasks. EnerVerse seamlessly integrates convolutional and bidirectional attention mechanisms for inner-chunk space modeling, thereby ensuring low-level consistency and continuity. Recognizing the inherent redundancy in video data, we propose sparse memory context in conjunction with chunkwise unidirectional generative paradigm to facilitate the generation of infinitely long sequences. To further augment robotic capabilities, we introduce the Free Anchor View (FAV) space, which offers flexible perspectives that enhance observation and analysis. The FAV space mitigates motion modeling ambiguity, removes physical constraints in confined environments, and significantly improves the robots generalization and adaptability across variety of tasks and settings. To address the prohibitive costs and labor intensity associated with acquiring multi-camera observations, we present data engine pipeline that integrates generative model with 4D Gaussian Splatting (4DGS). This pipeline capitalizes on the generative models robust generalization capabilities and the spatial constraints provided by 4DGS, enabling an iterative enhancement of data quality and diversity, thus creating data flywheel effect that effectively narrows the sim-to-real gap. Finally, our experiments demonstrate that the embodied future space generation prior substantially enhances policy predictive capabilities, resulting in improved overall performance, particularly in long-range robotic manipulation tasks. Date: January 01, 2025 Website: https://sites.google.com/view/enerverse"
        },
        {
            "title": "1 Introduction",
            "content": "High-capacity foundational models have achieved remarkable success across various modalities, including language Wang et al. (2024b), images Rombach et al. (2022), and videos Blattmann et al. (2023). These models are extensively pretrained on large datasets, allowing fine-tuning for specific downstream tasks. In robotics, these foundation models offer promising framework for addressing complex tasks by leveraging their pretrained capabilities and adapting to task-specific applications. This paradigm allows robots to handle diverse tasks based on varied input data. However, planning future actions based on real-time observations remains core challenge in robotics. Unlike language or vision domains, robotic systems must interact with the physical world in real time, requiring precise action planning and execution. This complexity arises from two primary challenges: (1) Explicit Alignment Across Modalities: Aligning task-instructions, observations, and action spaces is inherently intricate. Earlier works Goyal et al. (2024); Shridhar et al. (2022), used pretrained models for language-visual alignment and complex algorithms to map these to actions. Recently, LLM-based methods Liang et al. (2023); Huang et al. (2023, 2024) have been introduced to generate code policies using pretrained visual models. Although simplifying alignment, these approaches are limited by the representational capacity of plain language, making them unsuitable for complex tasks. (2) Data Scarcity: EnerVerse - Technical Report Figure 1 The overview of our proposed EnerVerse model, consisting of three key components. First, Initial Reconstruction uses observation images from cameras mounted on the robot to build an initial 3D point cloud, with anchor views set to adapt to the environment and meet task-specific requirements. Second, Free Anchor View Renders generates rendered images from these anchor perspectives to provide comprehensive scene representations. Finally, Chunk-wise Autoregressive Generation employs multi-view video diffusion to produce image sequences in chunks based on task instructions. When integrated with policy head, this module can generate robotic actions to execute the given task. The lack of large-scale task-observation-action datasets hinders implicit learning of mapping relationships. Although large-scale datasets ONeill et al. (2023); Khazatsky et al. (2024) have been introduced, they remain less diverse and lower in quality than those in language and vision domains. Methods Kim et al. (2024)attempt to address this limitation by combining these datasets with pretrained LLM knowledge, but they require enormous amounts of data, making them resource-intensive. Recent studies Wen et al. (2024); Rigter et al. (2024); Cheang et al. (2024); Guo et al. (2024) have yielded promising results by integrating video generation with policy planning, utilizing large-scale video datasets without action labels. However, many of these methods Rigter et al. (2024) simply adapt general-purpose video generation models to embodied tasks, overlooking the distinct requirements inherent to robotics. It is crucial to emphasize that video generation is not synonymous with generating embodied future spaces. Robotic tasks are characterized by specific demands, including causal logic that delineates the initiation and completion of task, the capability to achieve precise outcomes, and context-memory capabilities. Furthermore, robots operate within real-world three-dimensional environments, whereas video generation is constrained to 2D projection sequence that inadequately capture the complexities of embodied future spaces. Consequently, the relationship between video representations and three-dimensional spatial actions remains ambiguous. For high-information-density language tasks, causal modeling paradigms Vaswani (2017); Achiam et al. (2023), which leverage unidirectional attention, have exhibited exceptional performance. However, identifying the optimal modeling paradigm for visual tasks, characterized by significant information redundancy, remains an unresolved research challenge. In contrast to language tasks, visual generation tasks often benefit from bidirectional computational mechanisms, such as bidirectional attention and convolution, which have proven highly effective across various domains, including image generation Chi et al. (2023); Li et al. (2024b); Chang et al. (2022), general video generation Xing et al. (2025); Ho et al. (2022), and 3D generation Gao et al. (2024b); Wu et al. (2024b). Building on these insights, EnerVerse adopts hybrid framework for modeling the embodied future space. First, to model minimal unit of future space, defined as chunk space, we employ combination of convolution and bidirectional attention to ensure low-level consistency and continuity. Second, for long-range spatiotemporal sequences, we utilize chunk-wise unidirectional generative paradigm to encapsulate task execution logic, thereby enhancing the models reasoning capabilities. Moreover, we 2 EnerVerse - Technical Report observe that consecutive memory can often lead to unexpected model collapse during robotic manipulations. In such instances, future space generation is more reliant on logical execution than merely extending single scene. This indicates that fewer redundant context frames are critical for inducing coherent sequential understanding. To address this challenge, we design sparse context memory mechanism that preserves prior content throughout the generation process in non-redundant manner, leading to theoretical capability to generate sequences of indefinite length. By integrating bidirectional mechanisms for local modeling with unidirectional paradigms for long-range reasoning, the proposed framework effectively resolves the unique challenges associated with embodied future space generation. We further introduce the concept of the Free Anchor View (FAV) space. In robotics, camera observations are typically obtained using hardware-mounted cameras, which provide fixed perspectives associated with the robotic setup. In contrast, anchor views can be configured arbitrarily and can be adjusted according to task requirements or environmental conditions. Defined relative to the robots base system, FAVs remain consistent as long as the robots base position is fixed. As illustrated in Figure 1, anchor views are not physically affixed but exist freely in space, offering flexible perspectives that enhance observation and analysis. This \"free anchor view\" characteristic is particularly advantageous for robotic tasks due to the following reasons: (1) most robotic tasks benefit from multi-view observations, which strongly reduces metric ambiguity and deepens environmental understanding; (2) in complex robotic tasks involving whole-body motion, bodymounted cameras experience extrinsic changes as the robot moves, introducing motion modeling ambiguity and significantly complicating policy learning; (3) in certain task environments, such as narrow kitchens, body-mounted cameras can impose physical constraints, as the camera or its mounting apparatus may collide with walls during operation; and (4) although static, workspace-mounted cameras can mitigate some of these challenges, they simultaneously reduce the robots generalization and flexibility across diverse tasks and environments. Furthermore, multiple FAVs provide richer visual information thus implicitly construct 3D spatial representation, which significantly enhances the policys performance Goyal et al. (2024). However, acquiring meticulously calibrated multi-camera observations, mounted not only on the robot hardware but also in the surrounding environment, alongside robotic actions is both costly and labor-intensive in realworld scenarios. This limitation prevents the FAV system from obtaining sufficient training data in physical environments. In contrast, simulators can readily address this challenge by generating diverse and abundant synthetic data. Nevertheless, the sim-to-real gap, the disparity between simulated and real-world environments, remains significant obstacle that cannot be overlooked. To address this issue, we propose data engine pipeline that integrates generative model with 4D Gaussian Splatting (4DGS). This pipeline capitalizes on the generative models robust generalization capabilities and the spatial constraints and optimization strengths implied by 4DGS. Together, these two components synergistically establish data flywheel effect, self-reinforcing process that continuously enhances data quality and diversity, ultimately diminishing the sim-to-real gap and augmenting the effectiveness of the FAV system. The contributions of this work are as follows. We develop an innovative chunk-wise autoregressive diffusion model architecture capable of logically reasoning for future space generation, benefiting from sparse contextual memory mechanism. We propose novel FAV-based 4D future space generation method associated with policy planning, significantly enhancing spatial understanding. We are the first to construct data flywheel in the robotics domain that integrates 4D Gaussian Splatting optimization and multi-view video generation, enabling iterative improvements in data quality and model performance. The diffusion generator, combined with naive policy head, achieves state-of-the-art (SOTA) performance, particularly in long-range manipulation tasks, demonstrating the effectiveness of future space generation prior. 3 EnerVerse - Technical Report"
        },
        {
            "title": "2.1 Video Generation Models.",
            "content": "Diffusion-based video generation models have achieved significant progress in recent years Blattmann et al. (2023); Ho et al. (2020); Song et al. (2020), particularly in the domain of text-to-video (T2V) generation. Early T2V models Zhang et al. (2023); Chen et al. (2023); Ren et al. (2024); Zhang et al. (2024) leverage the strong priors established by text-to-image (T2I) models, incorporating temporal modules trained on video data to enable video generation. For instance, AnimateDiff Guo et al. (2023) introduces plug-and-play motion module that seamlessly integrates into existing personalized T2I diffusion models, effectively animating static images. Similarly, DynamicCrafter Xing et al. (2025) adapts motion priors from text-to-video diffusion models to the image-to-video (I2V) setting, generating animated clips by conditioning noise on input still images. Recent advancements Kong et al. (2024); Zheng et al. (2024); Bao et al. (2024) have explored replacing the traditional U-Net with Diffusion Transformer architectures in the denoising process, inspired by the recent success of diffusion transformer in image generation Peebles and Xie (2023a); Liu et al. (2024b); Zhuo et al. (2024). Additionally, some works Gao et al. (2024a) extend the original video diffusion paradigm by incorporating causal mechanisms, enabling the generation of long-sequence videos. Furthermore, other studies Hu et al. (2023); Wang et al. (2023); Zhao et al. (2024) expand video generation models into the realm of world modeling, where future states are predicted. In this paper, we adopt DynamicCrafter as our base model for the image-to-video task, as it is widely-used and open-sourced I2V framework in the research community. Our implementation is also compatible with modern Transformer-based Diffusion models (DiT), which is not our current focus for this paper."
        },
        {
            "title": "2.2 Video Pretraining for Robotics.",
            "content": "The concurrent work GR-2 Cheang et al. (2024) introduces versatile and generalizable robot manipulation framework that relies on pretraining with large collection of videos sourced from the internet. GR-2 is fine-tuned for both video generation and action prediction using robotic trajectories. LAPA Ye et al. (2024) also leverages internet-scale videos to learn pretraining representations from non-robot action videos. It first learns discrete latent actions between image frames using VQ-VAE, then pretrains latent Vision-Language Action (VLA) model to predict these latent actions from observations and task descriptions. Finally, the VLA is fine-tuned on small-scale robot manipulation datasets to map latent actions to robotic actions. SEER Tian et al. (2024) expands on LAPA by incorporating additional inverse dynamics pretraining to further improve performance. AVID Rigter et al. (2024) utilizes DynamicCrafter Xing et al. (2025) as its base video generation model and employs an adapter to transfer the original model to the robotics domain. VidMan Wen et al. (2024), built upon OpenSora Zheng et al. (2024), leverages the environment prediction capabilities of video diffusion models prior to action generation but remains limited to the 2D image space. In contrast to the aforementioned pretraining schemes, our approach focuses on generating long-sequence future spaces using novel data generation engine. Long-sequence data inherently captures motion information, making it particularly suitable for robotic tasks. 2.3 4D Reconstruction and Generation. There has been substantial progress Chen and Wang (2024) in reconstructing dynamic scenes from 2D videos using techniques such as 3D Gaussian Splatting (GS) Kerbl et al. (2023) and Neural Radiance Fields (NeRF) Mildenhall et al. (2021). Previous works have formulated this task by approximating scenes underlying spatio-temporal 4D volume using set of 4D Gaussians Yang et al. (2023). Wu et al. Wu et al. (2024a) proposed modeling the geometry and dynamics of scenes through the joint optimization of Gaussians in canonical space and deformation field. More recent advancements in 4D generation have focused on crafting sampling schemes for diffusion models that generate multi-view videos Li et al. (2024a), primarily targeting the modeling of single dynamic objects. DimensionX Sun et al. (2024) employed multiple LoRAs Hu et al. (2021), each designed for specific camera motion, to generate multi-view videos, which were subsequently utilized for 4D scene reconstruction. Concurrently, Cat4D Wu et al. (2024b) used single multi-view video diffusion model to generate multi-view videos, which were then employed to reconstruct dynamic 3D scenes as deforming 3D Gaussians. In contrast, our approach generates videos from the Free Anchor View, specifically 4 EnerVerse - Technical Report tailored for robotic manipulation tasks. Additionally, Gaussian Splatting in our framework is primarily involved in the offline data flywheel stage, where the video generation model and GS model complement each other to reduce the Sim2Real gap."
        },
        {
            "title": "3 Methods",
            "content": "Our EnerVerse model is designed with several components tailored for future space generation, incorporating Gaussian Splatting (GS) data factory pipeline. First, we employ chunk-wise autoregressive strategy integrated with video diffusion models, enabling the prediction of future frames by analyzing preceding sequences in manageable chunks and leveraging the capabilities of video diffusion models. Second, we propose free anchor view-based 4D generation method, where the ray direction map of each anchor is provided as prior knowledge to facilitate the efficient generation of target novel-view videos. Finally, we implement data engine pipeline comprising generative model and 4DGS, which produces diverse novel view video sequences with specified camera poses. These generated video sequences are then utilized to drive Sim2Real adaptation."
        },
        {
            "title": "3.1 Next Chunk Diffusion",
            "content": "Figure 2 The architecture of our proposed next-chunk diffusion model. As shown in Figure (a), sequence of observational frames, captured by camera and accompanied by the corresponding ray direction map, is utilized as observation priors. Leveraging these camera observations, an initial 3D reconstruction is obtained through depth wrapping and rendering Lassner and Zollhofer (2021), then several Free Anchor Views are established accordingly. In addition to camera observational frames, render frame from the FAV is also employed as context priors for the subsequent chunk diffusion models. To synthesize the anchor view + 1 sequence, the respective ray direction map is concatenated with the video latent. Notably, the observational image from the camera is optional and used only when the camera is static. If all sensors are in motion, the rendered image alone can serve as the context prior. In the context of the chunk-wise autoregressive training process, as depicted in Figure (b), clean frames selected at random from consecutive sequences are concatenated with noisy frames to forecast denoised latents. During the inference phase, once denoised frames are produced, they are utilized as the new set of clean frames for the following inference step. This iterative process persists until the predefined End-Of-Sequence (EOS) frame is encountered. Notably, we visualize only one view in Figure (b) to simplify the demonstration of the autoregressive generation process, but multi-view generation is fully supported by the model. Chunk-wise Autoregressive Generation: As shown in Figure 2, the observed latent sequence is represented as o1:K 0 ] RKHW C, encoded using pre-trained Variational Autoencoder (VAE). Here, denotes the number of observed frames, represents the downsampled spatial resolution, and indicates the number of color channels. Similarly, the latent representation of the rendered image is denoted as r0 R1HW C. The predicted latent sequence is expressed as z1:N 0, . . . , z0N ] RN HW C. The objective is to develop video diffusion model that generates predicted latents conditioned on observed latents, , r0). rendered latents, and textual input, following the conditional probability distribution pθ(z1:N 0 = [o1 0 = [z1 0, . . . , oK c, o1:K 0 5 EnerVerse - Technical Report Here, denotes the textual condition, and θ are the parameters of the denoising network, represented as , r0, t). For consistency, we refer to both observed and rendered frames as the clean frame ϵθ(z1:N context. The denoising network is trained to predict the ground truth noise ϵ from the noisy frame targets, optimized with the objective: , c, o1:K 0 Et,zzdata,ϵN (0,I)ϵ ϵθ (cid:0)z1:N , o1:K 0 , r0, t(cid:1) 2 2, min θ where ϵ is the sampled ground truth noise and θ denotes the learnable network parameters. In pratice, we predict following previous work Salimans and Ho (2022). Upon completion of model training, denoised data z0 can be derived from random noise zT through iterative denoising. During inference, clean frames, combined with noisy frames, are input into the diffusion generator to produce noisy frames. The most recently generated frames are then used as the new clean frames for the next iteration of inference. This iterative process continues until the predefined End-Of-Sequence (EOS) frame is detected. Since our diffusion generation operates on consecutive latent representation frames, we calculate the generated result of each frame and its L1 distance to the EOS during the inference process. predefined threshold is applied to determine when to terminate the process. In practice, this threshold-based method for detecting the EOS has proven to be highly accurate. Context Frame Mechanism. Instead of the conventional approach of using consecutive frames as the clean frame context for chunk prediction during training, we propose an alternative method that involves sparse sampled frames as the clean frames. For video data, which often contains significant redundant information, our method can drop approximately 80% of the frames while still retaining sufficient information for effective training. Furthermore, dropping frames at high ratio improves the models robustness, enabling it to better handle out-of-distribution (OOD) scenarios, particularly the covariant shift problem commonly encountered in the robot learning domain. Overall, from representation learning perspective, this randomized selection strategy encourages more comprehensive understanding of chunk prediction, potentially leading to improved outcomes compared to methods relying on continuous frames. During inference, clean frames are obtained from observed or rendered frames and denoised using sliding window approach. This method ensures seamless transition from observed to generated frames while improving inference efficiency and reducing GPU memory usage. 3.2 4D Generation Why Choose Free Anchor View Representation? Single-view video generation methods face substantial challenges in addressing occlusion, common and unavoidable issue in robotic manipulation tasks Huang et al. (2024). Previous approaches often adopt single top-down perspective, such as the birds-eye view (BEV). However, this setup is insufficient for manipulation tasks due to the complexity of occlusion relationships in 3D environments. Similarly, fixed multi-anchor view representations are overly constrained by the environment. For example, in narrow and confined spaces, such as kitchens, fixed camera positions may become physically infeasible, with pre-mounted overhead cameras potentially intersecting walls or other obstacles. In contrast, free multi-view video generation provides more practical and flexible alternative. By enabling generative models to focus on the physical properties of objects, such as their shapes and positions, this approach enhances object-level reasoning, which is particularly important for manipulation tasks that require precise modeling of physical interactions. Another common observation setup involves relative motion, such as cameras mounted on robots wrist. However, this setup introduces drawbacks by coupling environmental dynamics with the robots own motion, complicating policy learning. Free Anchor View Video Generation Pipeline. As illustrated in Figure 2, the objective of our approach is to 0 RN HW C, where denotes the number of directly generate multi-view latents, represented as z1:N views. In multi-camera or multi-view setting, each camera inherently captures distinct perspective of the same scene. Without explicitly accounting for these varying perspectives, the model may struggle to produce consistent outputs. To resolve this, we enhance the embedding of the initial images, Einit, by appending ray direction map along the channel dimension. The ray direction map encodes observation view information, including intrinsic and extrinsic camera parameters. Through ray casting, the generation process becomes 6 EnerVerse - Technical Report view-aware, conditioning the model on camera parameters. This enables the model to reflect the unique perspective of each anchor view while capturing 3D spatial relationships and occlusion propertiescritical for manipulation tasks. In addition, we extend the original 2D spatial attention to 3D cross-view attention to further enhance the 3D spatial perception ability of the model. This approach ensures consistency and coherence across different views while preserving the geometric relationships among objects in the scene. By leveraging free anchor view representation, the pipeline effectively addresses occlusion, enhances spatial reasoning, and adapts seamlessly to complex 3D environments. Figure 3 The pipeline for EnerVerse as data engine. Observation images captured from multiple cameras, along with rendered images from anchor views, are processed by the multi-view video generator to produce denoised multi-view videos. These videos, paired with their corresponding camera poses, are utilized in 4D Gaussian Splatting (4D GS) for 4D scene reconstruction. The reconstructed content is rendered from anchor views to generate high-precision images, which are iteratively fed back into the pipeline to enhance motion consistency and reconstruction quality. This iterative loop combines geometric consistency with generative refinement, delivering high-fidelity outputs for tasks such as robotic manipulation. Real-World Data Flywheels with EnerVerse and 4DGS. Obtaining carefully calibrated multi-camera observations in the real world is both costly and labor-intensive. As result, we primarily rely on data sourced from simulators. However, video data from simulated environments or academic benchmarks often suffers from domain gaps when applied to real-world scenarios. These gaps typically manifest as discrepancies in visual appearance, scale perception, and metric accuracy, which hinder direct applicability. To address these challenges, we propose data generation engine pipeline that leverages sparse or isolated observations to generate multi-perspective views of given scene. By utilizing Gaussian Splatting for 4D reconstruction from these multi-view observations, we ensure geometric and optical consistency, thereby improving alignment and coherence across different viewpoints. Our EnerVerse model serves as an advanced data engine, employing multi-stage process to enhance video generation and reconstruction. Initially, we train base model, EnerVerse, using data from simulator. This model is then fine-tuned into EnerVerse-D, conditioned on complete offline observation sequences, where clean, noise-free videos are captured from multiple mounted cameras. These videos include both robotic arm motion and scene dynamics, ensuring motion consistency across views. Subsequently, these multi-view videos are used to construct 4D Gaussian representation via Gaussian Splatting. After completing the 3D scene reconstruction, the content is rendered from the anchor views to obtain higher-precision observations. The rendered observations, which are denoised and geometrically consistent, are iteratively refined using EnerVerse-D to generate pseudo-ground truth. After collecting sufficient real-world multi-view video data with the data engine, we further fine-tune the multi-view video generator using this data. This iterative process reduces noise, improves reconstruction quality, and facilitates Sim2Real domain adaptation, ultimately producing large-scale, high-quality video datasets essential for training 4D generation models. 7 EnerVerse - Technical Report Figure 4 Visualization of FAVs generation on the LIBERO benchmark. Anchor View 1 represents the observation image captured by mounted camera. Anchor View 2 and Anchor View 3 are generated by rendering from point cloud reconstructed from Anchor View 1 using depth wrapping."
        },
        {
            "title": "4.1 Experiment Settings",
            "content": "To demonstrate the effectiveness of proposed method, we evaluate EnerVerse in two different domains, e.g. video generation quality and robotic policy performance. 4.1.1 Implementation Details Training Data. We selected several publicly available datasets characterized by well-defined task logic, including RT-1 Brohan et al. (2022), Taco-Play Rosete-Beas et al. (2022), ManiSkill Gu et al. (2023), BridgeV1 Walke et al. (2023), LanguageTable Lynch et al. (2023) and RoboTurk Mandlekar et al. (2019), for the purpose of pretraining. During this pretraining phase, only video frames were utilized for video generation training. Furthermore, we constructed dataset containing multi-anchor view video ground truths using the Isaac Sim simulator Mittal et al. (2023). The FAV generation model was trained by leveraging the weights derived from the single-view video generation model. For the policy planning task, fine-tuning with limited quantity of demonstration data from specific scenarios proved sufficient to attain state-of-the-art performance. To mitigate domain gaps encountered when training with heterogeneous data, we employed domain embeddings inspired by Wang et al. (2024a). Specifically, distinct domain embeddings were allocated to each sub-dataset. In subsequent space generation and policy planning, these embeddings were integrated with the diffusion timestep embeddings prior to input into the diffusion model. This methodology effectively alleviated conflicts arising from discrepancies in entities, task types, and visual styles. 8 EnerVerse - Technical Report Training Details. Our model is conducted based on UNet-based Video Diffusion Models (VDM) Xing et al. (2025), and can be easily adapted to DiT Peebles and Xie (2023b) architectures. In our experiments on generating embodied future spaces, we identified that chunk size significantly influences model performance. Comparative analyses utilizing chunk sizes of 1, 4, 8, and 16 revealed that the model exhibited optimal robustness when employing chunk size of 8 (further details regarding these experiments can be found in the supplementary material). Following the methodology outlined in Bruce et al. (2024), we introduced corruptive noise to the frames within the memory context. To alleviate degradation in autoregressive generation, the intensity of this noise was modulated in cosine-related manner relative to the distance from the current moment. In the policy prediction experiment, the action head adopts the Diffusion Policy (DP) architecture Chi et al. (2023), with total of 190M parameters. For the condition of the DP head, we utilize the feature before middle block of the UNet in the first denoise step, and calculate the mean value over spatial dimension, resulting in final shape of C, where is the length of video and is the number of channels before middle block. The rendered FAV images are with 512 320 image sizes and the action header predicts the delta pose."
        },
        {
            "title": "4.2 Comparison Results\nEmbodied Future Space Generation. Following the approach established in AVID Rigter et al. (2024), we assess\nvideo generation quality utilizing the RT-1 Brohan et al. (2022) dataset. To create a comparable baseline, we\nfine-tune DynamicCrafter on the RT-1 dataset and run inference iteratively with FreeNoise Qiu et al. (2023)\nto enable long video generation(DC-FN). For evaluation, we generate 200 synthetic videos with varied lengths\nby conditioning the models on the initial frame and task instructions, subsequently comparing the generated\nvideos against the ground truth using standard metrics such as PSNR and FVD. However, while these metrics\nprimarily evaluate visual quality, embodied tasks necessitate additional considerations, including semantic\nalignment with instructions, workspace consistency across frames, and motion continuity. To address these\nhigher-order aspects, we execute a user study involving robotics experts, who assess the generated videos\nbased on semantic accuracy, frame consistency, and motion continuity.",
            "content": "Method Atomic Task Long Task PSNR FVD Quality Seman. Consist. Continuity Ability DC-FN 25. 445.94 EnerVerse 26.1 404.65 54 97 97 92 89 80 90 Table 1 Performance comparison between DynamiCrafter (FN) and our proposed approach across Atomic Task metrics (Quantitative Comparison and User Study) and Long Task ability. The proposed method outperforms DynamiCrafter (FN) in most metrics, demonstrating its effectiveness in video generation and task performance. Table 1 illustrates that our method substantially outperforms DynamicCrafter (FN) in both quantitative and qualitative evaluations. In terms of quantitative metrics, our approach achieves higher PSNR and lower FVD. These findings indicate that our method produces videos of superior visual quality and enhanced temporal dynamics. In the user study, our method secures higher quality score and exceeds DynamicCrafter in motion continuity, which is essential for robotic manipulation tasks. Although both methods attain equivalent semantic accuracy, this suggests that our approach effectively preserves instruction alignment while delivering superior overall performance. Moreover, our method uniquely accommodates long tasks, as evidenced by its successful execution of long-range manipulation scenarios, whereas DynamicCrafter falters in this domain. We also provide qualitative comparison in Figure 5. Multi-View Consistency. In this section, we qualitatively demonstrate the capability of EnerVerse to generate multi-view videos of the same scene while ensuring consistency across anchor views. Furthermore, each view attains high-quality image generation, thereby highlighting the robustness of our approach. As shown in Figure 6, EnerVerse could generate high-quality multi anchor view videos in both simulator and real-world settings. Robotic Policy Evaluation Following the methodology in OpenVLA Kim et al. (2024), we evaluate robotic policies EnerVerse - Technical Report Figure 5 Qualitative comparison for single view video generation between EnerVerse and DynamiCrafter(FN) on RT-1 dataset. Since EnerVerse predict EOS frame at 42th frame for this task, we visualize 8th, 16th, 24th and 41th frame sampled from both generated sequence. The sequences generated by DynamiCrafter(FN) did not maintain the logic of the long-range task, producing many hallucinations as the sequence grew. In contrast, the sequence generated by EnerVerse was logically coherent, continuously and completely generating the future space of the entire task, and accurately predicting the EOS (End of Sequence) frame. using the LIBERO Liu et al. (2024a) benchmark, which consists of four distinct task suites: LIBERO-Spatial, LIBERO-Object, LIBERO-Goal, and LIBERO-Long. Each suite contains 10 tasks, each with 50 human demonstrations, designed to evaluate various aspects of robotic learning. For each task suite, separate policy model is fine-tuned. We compare our method against five baselines: Diffusion Policy Chi et al. (2023), direct action learning policy trained from scratch; Octo Team et al. (2024), transformer-based policy model fine-tuned on the target dataset; OpenVLA Kim et al. (2024), 7B vision-language-action model fine-tuned on the target dataset; MDT Reuss et al. (2024),a diffusion transformer-based policy with an auxiliary MAE loss; MAIL Jia et al. (2024), policy model with Mamba Gu and Dao (2023) in an encoder-decoder structure. For evaluation, all models are tested across tasks using 50 rollouts per task, with results averaged over three random seeds. Experiments with EnerVerse are conducted under two setups: single static-camera configuration, consistent with OpenVLA, and three-static-camera configuration (e.g., three FAVs as shown in Figure 4), as our method supports multiple visual inputs. Model Visual Input Spatial Object Goal Long Avg. Diffusion Policy Octo OpenVLA MDT MAIL EnerVerse EnerVerse One Third View One Third View One Third View One Third & One Wrist View One Third & One Wrist View One FAV Three FAVs 78.3 78.9 84.7 78.5 74.3 92. 91.2 92.5 85.7 88.4 87.5 90. 93.2 97.7 68.3 84.6 79.2 73.5 81. 78.1 50.5 51.1 53.7 64.8 78. 72.4 75.1 76.5 76.1 83.5 73. 84.1 85.0 80.0 88.5 Table 2 Evaluation results on the LIBERO benchmark across four task suites. Our method achieves superior performance in both single and multi-visual input settings. As shown in Table 2, EnerVerse achieves state-of-the-art performance across the LIBERO benchmark, significantly surpassing all baselines. With One Third View input, it achieves an average score of 84.0, outperforming strong baselines like MAIL (83.5) and OpenVLA (76.5). The Three Third View configuration further enhances performance, achieving the highest average score of 88.5, demonstrating the value of richer 10 EnerVerse - Technical Report Figure 6 Qualitative results for multi anchor view generation on LIBERO benchmark (left) and real-world manipulation data (right), collected from AgiBot World AgiBot (2024). One view is overlapped with fixed RGB sensor and other views are manully set. Visualized Frames are uniformly sampled from generated sequence. We emphasize the consistency of objects across views by highlighting them with red rectangle. visual input. Task-specific results highlight the models strengths, achieving top scores in Spatial (92.1 and 91.2 for One and Three Third View, respectively), Object (93.2 and 97.7), and Long (73.0 and 80.0). The models balanced performance across all tasks, particularly excelling in Object and Goal tasks, underscores its robustness and adaptability. These results validate the effectiveness of the proposed architecture and its ability to leverage multi-view input for enhanced understanding and task performance."
        },
        {
            "title": "4.3 Further Studies",
            "content": "In this section, we explore several key design choices for EnerVerse. First, we examine the significance of the proposed sparse memory mechanism, which plays critical role in both policy learning and video generation. Second, we discuss the training strategy utilized in EnerVerse. Third, we analyze the alignment between the predicted action spaces and visual spaces through attention map analysis. Finally, we introduce the real-world experiment setup. Sparse Memory Mechanism. We evaluate the effectiveness of our sparse memory mechanism in both policy learning and video generation. The evaluation is conducted on the LIBERO-Long task suite, as this suite involves significantly longer task execution steps, requiring the policy to exhibit strong long-range memory and task reasoning capabilities. The evaluation is performed with single visual input. As shown in Table 3, the absence of the sparse memory results in significant performance degradation, with the policy achieving only 30.8 compared to 73 when the sparse memory mechanism is applied. Similarly, Figure 7 demonstrates that when the video generator operates without sparse memory, the model experiences unexpected collapse and fails to recover in out-of-distribution (OOD) scenarios. In contrast, the sparse memory mechanism ensures 11 EnerVerse - Technical Report robust performance while also saving computational resources. Setup w/o Sparse Memory Sparse Memory LIBERO-Long-SV 30.8 73 Table 3 Performance comparison on the LIBERO-Long task with and without Sparse Memory. Figure 7 Ablation results for context memory mechanism in video generation. Providing history information to the generation model with consecutive context (first line) often leads to unexpected model collapse while the model with sparse memory (second line) shows robust performance and save mush computing resources. Training Strategy Analysis. To analyze the impact of different training strategies on robotic policy learning, we trained four robotic policies on the LIBERO-Spatial task suite using the following approaches: (1) training the entire EnerVerse from scratch using only policy loss optimization; (2) training the entire EnerVerse as in (1) but initialized with pretrained weights from general video generator, e.g. DynamiCrafter(DC) Xing et al. (2025), which is trained with the general natural videos; (3) co-training EnerVerse by optimizing both the robotic policy action loss and the video generation loss simultaneously; and (4) the default two-stage training strategy, where the video generator is pretrained first, followed by fine-tuning EnerVerse using only robotic policy loss optimization. Strategy All-Scratch With DC Pretrain. One-Stage Co-Train Two-Stage Finetune LIBERO-Spatial Failed 79 86.3 92.1 Table 4 Performance comparison of different training strategies on the LIBERO-Spatial task suite. The metrics are the task success rates. As shown in Table 4, training EnerVerse from scratch without loading pretrained weights failed to converge, underscoring the importance of robust initialization. Another possible reason for this failure could be the relatively limited training data compared to the number of network parameters. Initializing with pretrained weights improved performance (79), while jointly optimizing the policy loss and video generation loss in one-stage co-training setup further increased performance to 86.3. This demonstrates that the video generation task enhances policy learning. Our default Two-Stage Fine-tuning strategy, which involves pretraining the video generator followed by fine-tuning EnerVerse with policy loss optimization, achieved the best performance overall. Attention Map Analysis. To further analyze the alignment between the predicted action space and the visual space, including the visual observations cached by our Sparse Memory Mechanism and the generated future space, we calculated and visualized the attention maps from the first several layers of the Cross-Attention Block in our policy head, as described in Section 3.3. Figure 8 illustrates attention maps from different heads and layers, showcasing the models hierarchical focus and the impact of our proposed embodied future space generation in facilitating robust action prediction. In 12 Figure 8(a), attention is distributed almost entirely across the future space, reflecting the models ability to leverage sparse memory conditions and generated predictions from the outset. In contrast, Figure 8(d) shows the attention sharply focused on the sparse memory space, with minimal reliance on the generated future space, indicating that the model has transitioned to memory-based reasoning. Interestingly, Figures 8(c,e) demonstrate that the model effectively integrates information from both the sparse memory space and the predicted future space. Moreover, these attention maps reveal that earlier decision steps tend to prioritize sparse memory, while later action steps shift focus to the generated future space. These results validate that our generative pretraining effectively enhances the models ability to integrate temporal information, align predicted actions with future visual contexts, and make robust decisions. Figure 8 Attention maps from different attention heads and layers of the model. The y-axis represents the predicted action space (the Query), spanning 8 steps, while the x-axis represents the Key-Value space. The first 4 columns in the KV space correspond to information from the Sparse Memory space, while the last 8 columns correspond to the predicted future space. These maps highlight how the model attends to sparse memory conditions (left) and future space conditions (right) when predicting actions. The bright yellow indicates higher attention score while dark red indicates lower one. Real-World Experiments. To evaluate the manipulation capabilities of EnerVerse, we conducted real-world experiments using AgiBot-A2W and AgiBot-A2D robotics in two challenging industrial scenarios. Unlike the evaluation tasks described in Section 4.2, these scenarios required precise manipulation and robust decision-making. In the first task, the robot placed blocks into designated compartments of foam worktable, demanding accuracy due to the tight fit and visual similarity between the foam and table. In the second task, the robot sorted several transparent plastic objects, including measuring cup and plate, where the transparency added complexity to object recognition and manipulation. For detailed task descriptions and additional visualizations, please refer to our project page."
        },
        {
            "title": "5 Conclusion",
            "content": "In conclusion, we present EnerVerse, pioneering framework that addresses the challenges of multi-view video generation and long-range policy execution for robotic manipulation by modeling embodied future spaces. Leveraging sparse contextual memory and the novel Free Anchor View space, EnerVerse enhances spatial reasoning and provides flexible perspectives critical for complex tasks. Our data engine pipeline, combining generative modeling with 4D Gaussian Splatting, bridges the Sim2Real gap by producing high-quality synthetic data, reducing reliance on costly real-world data collection, and enabling seamless transitions to real-world scenarios. The diffusion generator, integrated with naive policy head, achieves state-of-the-art performance in long-range manipulation tasks, highlighting the effectiveness of future space generation prior. Overall, EnerVerse delivers scalable, generalizable solution for robotics, paving the way for robust advancements across diverse applications. Acknowledge: This paper was created using the Meta FAIR pre-prints template. EnerVerse - Technical Report"
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. AgiBot. Agibot world. https://agibot-world.com, 2024. Fan Bao, Chendong Xiang, Gang Yue, Guande He, Hongzhou Zhu, Kaiwen Zheng, Min Zhao, Shilong Liu, Yaole Wang, and Jun Zhu. Vidu: highly consistent, dynamic and skilled text-to-video generator with diffusion models. arXiv preprint arXiv:2405.04233, 2024. Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022. Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative interactive environments. In Forty-first International Conference on Machine Learning, 2024. Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1131511325, 2022. Chi-Lam Cheang, Guangzeng Chen, Ya Jing, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Hongtao Wu, Jiafeng Xu, Yichu Yang, et al. Gr-2: generative video-language-action model with web-scale knowledge for robot manipulation. arXiv preprint arXiv:2410.06158, 2024. Guikun Chen and Wenguan Wang. survey on 3d gaussian splatting. arXiv preprint arXiv:2401.03890, 2024. Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter1: Open diffusion models for high-quality video generation, 2023. Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. The International Journal of Robotics Research, page 02783649241273668, 2023. Kaifeng Gao, Jiaxin Shi, Hanwang Zhang, Chunping Wang, and Jun Xiao. Vid-gpt: Introducing gpt-style autoregressive generation in video diffusion models. arXiv preprint arXiv:2406.10981, 2024a. Ruiqi Gao, Aleksander Holynski, Philipp Henzler, Arthur Brussee, Ricardo Martin-Brualla, Pratul Srinivasan, Jonathan Barron, and Ben Poole. Cat3d: Create anything in 3d with multi-view diffusion models. arXiv preprint arXiv:2405.10314, 2024b. Ankit Goyal, Valts Blukis, Jie Xu, Yijie Guo, Yu-Wei Chao, and Dieter Fox. Rvt-2: Learning precise manipulation from few demonstrations. arXiv preprint arXiv:2406.08545, 2024. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. Jiayuan Gu, Fanbo Xiang, Xuanlin Li, Zhan Ling, Xiqiang Liu, Tongzhou Mu, Yihe Tang, Stone Tao, Xinyue Wei, Yunchao Yao, Xiaodi Yuan, Pengwei Xie, Zhiao Huang, Rui Chen, and Hao Su. Maniskill2: unified benchmark for generalizable manipulation skills. In International Conference on Learning Representations, 2023. Yanjiang Guo, Yucheng Hu, Jianke Zhang, Yen-Jen Wang, Xiaoyu Chen, Chaochao Lu, and Jianyu Chen. Prediction with action: Visual policy learning via joint denoising process. arXiv preprint arXiv:2411.18179, 2024. Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. 14 EnerVerse - Technical Report Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:86338646, 2022. Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fedoseev, Alex Kendall, Jamie Shotton, and Gianluca Corrado. Gaia-1: generative world model for autonomous driving. arXiv preprint arXiv:2309.17080, 2023. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. Siyuan Huang, Zhengkai Jiang, Hao Dong, Yu Qiao, Peng Gao, and Hongsheng Li. Instruct2act: Mapping multimodality instructions to robotic actions with large language model. arXiv preprint arXiv:2305.11176, 2023. Wenlong Huang, Chen Wang, Yunzhu Li, Ruohan Zhang, and Li Fei-Fei. Rekep: Spatio-temporal reasoning of relational keypoint constraints for robotic manipulation. arXiv preprint arXiv:2409.01652, 2024. Xiaogang Jia, Qian Wang, Atalay Donat, Bowen Xing, Ge Li, Hongyi Zhou, Onur Celik, Denis Blessing, Rudolf Lioutikov, and Gerhard Neumann. Mail: Improving imitation learning with mamba. arXiv preprint arXiv:2406.08234, 2024. Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, et al. Droid: large-scale in-the-wild robot manipulation dataset. arXiv preprint arXiv:2403.12945, 2024. Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. Christoph Lassner and Michael Zollhofer. Pulsar: Efficient sphere-based neural rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14401449, 2021. Bing Li, Cheng Zheng, Wenxuan Zhu, Jinjie Mai, Biao Zhang, Peter Wonka, and Bernard Ghanem. Vivid-zoo: Multi-view video generation with diffusion model. arXiv preprint arXiv:2406.08659, 2024a. Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. arXiv preprint arXiv:2406.11838, 2024b. Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as policies: Language model programs for embodied control. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 94939500. IEEE, 2023. Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, and Peter Stone. Libero: Benchmarking knowledge transfer for lifelong robot learning. Advances in Neural Information Processing Systems, 36, 2024a. Dongyang Liu, Shitian Zhao, Le Zhuo, Weifeng Lin, Yu Qiao, Hongsheng Li, and Peng Gao. Lumina-mgpt: Illuminate flexible photorealistic text-to-image generation with multimodal generative pretraining. arXiv preprint arXiv:2408.02657, 2024b. Corey Lynch, Ayzaan Wahid, Jonathan Tompson, Tianli Ding, James Betker, Robert Baruch, Travis Armstrong, and Pete Florence. Interactive language: Talking to robots in real time. IEEE Robotics and Automation Letters, 2023. Ajay Mandlekar, Jonathan Booher, Max Spero, Albert Tung, Anchit Gupta, Yuke Zhu, Animesh Garg, Silvio Savarese, and Li Fei-Fei. Scaling robot supervision to hundreds of hours with roboturk: Robotic manipulation dataset through human reasoning and dexterity. In 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 10481055. IEEE, 2019. Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. 15 EnerVerse - Technical Report Mayank Mittal, Calvin Yu, Qinxi Yu, Jingzhou Liu, Nikita Rudin, David Hoeller, Jia Lin Yuan, Ritvik Singh, Yunrong Guo, Hammad Mazhar, Ajay Mandlekar, Buck Babich, Gavriel State, Marco Hutter, and Animesh Garg. Orbit: unified simulation framework for interactive robot learning environments. IEEE Robotics and Automation Letters, 8 (6):37403747, 2023. doi: 10.1109/LRA.2023.3270034. Abby ONeill, Abdul Rehman, Abhinav Gupta, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, et al. Open x-embodiment: Robotic learning datasets and rt-x models. arXiv preprint arXiv:2310.08864, 2023. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023a. William Peebles and Saining Xie. Scalable diffusion models with transformers, 2023b. https://arxiv.org/abs/2212. 09748. Haonan Qiu, Menghan Xia, Yong Zhang, Yingqing He, Xintao Wang, Ying Shan, and Ziwei Liu. Freenoise: Tuning-free longer video diffusion via noise rescheduling, 2023. Weiming Ren, Harry Yang, Ge Zhang, Cong Wei, Xinrun Du, Stephen Huang, and Wenhu Chen. Consisti2v: Enhancing visual consistency for image-to-video generation. arXiv preprint arXiv:2402.04324, 2024. Moritz Reuss, Ömer Erdinç Yağmurlu, Fabian Wenzel, and Rudolf Lioutikov. Multimodal diffusion transformer: Learning versatile behavior from multimodal goals. In First Workshop on Vision-Language Models for Navigation and Manipulation at ICRA 2024, 2024. Marc Rigter, Tarun Gupta, Agrin Hilmkil, and Chao Ma. Avid: Adapting video diffusion models to world models. arXiv preprint arXiv:2410.12822, 2024. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. Erick Rosete-Beas, Oier Mees, Gabriel Kalweit, Joschka Boedecker, and Wolfram Burgard. Latent plans for task agnostic offline reinforcement learning. 2022. Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512, 2022. Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Cliport: What and where pathways for robotic manipulation. In Conference on robot learning, pages 894906. PMLR, 2022. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. Wenqiang Sun, Shuo Chen, Fangfu Liu, Zilong Chen, Yueqi Duan, Jun Zhang, and Yikai Wang. Dimensionx: Create any 3d and 4d scenes from single image with controllable video diffusion. arXiv preprint arXiv:2411.04928, 2024. Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, et al. Octo: An open-source generalist robot policy. arXiv preprint arXiv:2405.12213, 2024. Yang Tian, Sizhe Yang, Jia Zeng, Ping Wang, Dahua Lin, Hao Dong, and Jiangmiao Pang. Predictive inverse dynamics models are scalable learners for robotic manipulation. https://arxiv.org/abs/2412.15109, 2024. Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. Homer Walke, Kevin Black, Abraham Lee, Moo Jin Kim, Max Du, Chongyi Zheng, Tony Zhao, Philippe HansenEstruch, Quan Vuong, Andre He, Vivek Myers, Kuan Fang, Chelsea Finn, and Sergey Levine. Bridgedata v2: dataset for robot learning at scale. In Conference on Robot Learning (CoRL), 2023. Lirui Wang, Xinlei Chen, Jialiang Zhao, and Kaiming He. Scaling proprioceptive-visual learning with heterogeneous pre-trained transformers. arXiv preprint arXiv:2409.20537, 2024a. Xiaofeng Wang, Zheng Zhu, Guan Huang, Xinze Chen, Jiagang Zhu, and Jiwen Lu. Drivedreamer: Towards real-world-driven world models for autonomous driving. arXiv preprint arXiv:2309.09777, 2023. Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024b. EnerVerse - Technical Report Youpeng Wen, Junfan Lin, Yi Zhu, Jianhua Han, Hang Xu, Shen Zhao, and Xiaodan Liang. Vidman: Exploiting implicit dynamics from video diffusion model for effective robot manipulation. arXiv preprint arXiv:2411.09153, 2024. Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang. 4d gaussian splatting for real-time dynamic scene rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2031020320, 2024a. Rundi Wu, Ruiqi Gao, Ben Poole, Alex Trevithick, Changxi Zheng, Jonathan Barron, and Aleksander Holynski. Cat4d: Create anything in 4d with multi-view video diffusion models. arXiv preprint arXiv:2411.18613, 2024b. Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Gongye Liu, Xintao Wang, Ying Shan, and Tien-Tsin Wong. Dynamicrafter: Animating open-domain images with video diffusion priors. In European Conference on Computer Vision, pages 399417. Springer, 2025. Zeyu Yang, Hongye Yang, Zijie Pan, and Li Zhang. Real-time photorealistic dynamic scene representation and rendering with 4d gaussian splatting. arXiv preprint arXiv:2310.10642, 2023. Seonghyeon Ye, Joel Jang, Byeongguk Jeon, Sejune Joo, Jianwei Yang, Baolin Peng, Ajay Mandlekar, Reuben Tan, Yu-Wei Chao, Bill Yuchen Lin, et al. Latent action pretraining from videos. arXiv preprint arXiv:2410.11758, 2024. Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qing, Xiang Wang, Deli Zhao, and Jingren Zhou. I2vgen-xl: High-quality image-to-video synthesis via cascaded diffusion models, 2023. Yiming Zhang, Zhening Xing, Yanhong Zeng, Youqing Fang, and Kai Chen. Pia: Your personalized image animator via plug-and-play modules in text-to-image models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 77477756, 2024. Guosheng Zhao, Chaojun Ni, Xiaofeng Wang, Zheng Zhu, Xueyang Zhang, Yida Wang, Guan Huang, Xinze Chen, Boyuan Wang, Youyi Zhang, et al. Drivedreamer4d: World models are effective data machines for 4d driving scene representation. arXiv preprint arXiv:2410.13571, 2024. Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all, March 2024. https://github.com/hpcaitech/ Open-Sora. Le Zhuo, Ruoyi Du, Han Xiao, Yangguang Li, Dongyang Liu, Rongjie Huang, Wenze Liu, Lirui Zhao, Fu-Yun Wang, Zhanyu Ma, et al. Lumina-next: Making lumina-t2x stronger and faster with next-dit. arXiv preprint arXiv:2406.18583, 2024."
        }
    ],
    "affiliations": [
        "AgiBot",
        "CUHK",
        "FDU",
        "HIT",
        "HKUST",
        "SJTU",
        "Shanghai AI Lab"
    ]
}