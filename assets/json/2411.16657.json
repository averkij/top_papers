{
    "paper_title": "DreamRunner: Fine-Grained Storytelling Video Generation with Retrieval-Augmented Motion Adaptation",
    "authors": [
        "Zun Wang",
        "Jialu Li",
        "Han Lin",
        "Jaehong Yoon",
        "Mohit Bansal"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Storytelling video generation (SVG) has recently emerged as a task to create long, multi-motion, multi-scene videos that consistently represent the story described in the input text script. SVG holds great potential for diverse content creation in media and entertainment; however, it also presents significant challenges: (1) objects must exhibit a range of fine-grained, complex motions, (2) multiple objects need to appear consistently across scenes, and (3) subjects may require multiple motions with seamless transitions within a single scene. To address these challenges, we propose DreamRunner, a novel story-to-video generation method: First, we structure the input script using a large language model (LLM) to facilitate both coarse-grained scene planning as well as fine-grained object-level layout and motion planning. Next, DreamRunner presents retrieval-augmented test-time adaptation to capture target motion priors for objects in each scene, supporting diverse motion customization based on retrieved videos, thus facilitating the generation of new videos with complex, scripted motions. Lastly, we propose a novel spatial-temporal region-based 3D attention and prior injection module SR3AI for fine-grained object-motion binding and frame-by-frame semantic control. We compare DreamRunner with various SVG baselines, demonstrating state-of-the-art performance in character consistency, text alignment, and smooth transitions. Additionally, DreamRunner exhibits strong fine-grained condition-following ability in compositional text-to-video generation, significantly outperforming baselines on T2V-ComBench. Finally, we validate DreamRunner's robust ability to generate multi-object interactions with qualitative examples."
        },
        {
            "title": "Start",
            "content": "DREAMRUNNER: Fine-Grained Storytelling Video Generation with Retrieval-Augmented Motion Adaptation"
        },
        {
            "title": "Zun Wang",
            "content": "Han Lin UNC Chapel Hill {zunwang,jialuli,hanlincs,jhyoon,mbansal}@cs.unc.edu"
        },
        {
            "title": "Mohit Bansal",
            "content": "4 2 0 2 5 2 ] . [ 1 7 5 6 6 1 . 1 1 4 2 : r https://dreamrunner-story2video.github.io/ Figure 1. Overall pipeline for DREAMRUNNER. (1) plan generation stage: we employ an LLM to craft hierarchical video plan (i.e., High-Level Plan and Fine-Grained Plan) from user-provided generic story narration. (2.1) motion retrieval and prior learning stage: we retrieve videos relevant to the desired motions from video database for learning the motion prior through test-time finetuning. (2.2) subject prior learning stage: we use reference images for learning the subject prior through test-time fine-tuning. (3) video generation with region-based diffusion stage: we equipt diffusion model with novel spatial-temporal region-based 3D attention and prior injection module (i.e., SR3AI) for video generation with fine-grained control."
        },
        {
            "title": "Abstract",
            "content": "Storytelling video generation (SVG) has recently emerged as task to create long, multi-motion, multi-scene videos that consistently represent the story described in the input text script. SVG holds great potential for diverse content creation in media and entertainment; however, it also presents significant challenges: (1) objects must exhibit range of fine-grained, complex motions, (2) multiple objects need to appear consistently across scenes, and (3) subjects may require multiple motions with seamless transitions within single scene. To address these challenges, we propose DREAMRUNNER, novel story-to-video generation method: First, we structure the input script using large language model (LLM) to facilitate both coarsegrained scene planning as well as fine-grained objectlevel layout and motion planning. Next, DREAMRUNNER presents retrieval-augmented test-time adaptation to capture target motion priors for objects in each scene, supporting diverse motion customization based on retrieved videos, thus facilitating the generation of new videos with complex, scripted motions. Lastly, we propose novel spatialtemporal region-based 3D attention and prior injection module SR3AI for fine-grained object-motion binding and frame-by-frame semantic control. We compare DREAMRUNNER with various SVG baselines, demonstrating stateof-the-art performance in character consistency, text alignment, and smooth transitions. Additionally, DREAMRUNNER exhibits strong fine-grained condition-following ability in compositional text-to-video generation, significantly outperforming baselines on T2V-ComBench. Finally, we validate DREAMRUNNERs robust ability to generate multiobject interactions with qualitative examples. 1 1. Introduction Storytelling video generation (SVG) models [16, 17, 68, 74] capable of creating multi-scene, multi-object, motion-based long videos are essential for advancing real-world applications of video generation, enabling the creation of rich, immersive narratives with realistic and engaging interactions. Unlike existing short-form video generation approaches [8, 11, 12, 14, 19, 25, 41, 46, 50, 56, 65, 67], these models allow characters and objects to evolve across scenes, enhancing the coherence of generated content to align more closely with human storytelling. Such capabilities hold vast potential in media, gaming, and interactive storytelling. SVG presents unique challenges in seamlessly translat- (1) Ening text scripts into long-form videos, such as: suring fine-grained, complex object motions: subjects in storytelling videos often need to display fine-grained motions that align with the narratives demands rather than relying on basic or repetitive movement patterns; (2) Maintaining multiple objects consistency across multiple scenes: characters introduced in one scene need to retain recognizable features (e.g., appearance and position) throughout the story, despite dynamic movements and changing environments; and (3) Managing smooth transitions for multiple motions within single scene: robust SVG model needs to represent seamless transitions between different actions or states of subject within the same scene, ensuring continuity and coherence to enhance the flow of the story, such as character running across beach before transitioning to calm walk. However, recent SVG approaches [16, 17, 36, 68, 71, 74], while effective at preserving the main character across multiple scenes, are limited in generating diverse and natural object motions, resulting in restricted movement throughout storytelling sequences. They also struggle to incorporate multiple objects and multiple concurrent motions, preventing dynamic interactions and transitions that are essential for richer narratives. To address these challenges, we propose DREAMRUNNER, novel retrieval-augmented story-to-video generative approach that produces long-form, multi-character, multimotion videos across multiple scenes by incorporating new motion and subject prior learning strategy along with region-based diffusion. DREAMRUNNER facilitates finegrained video generation capabilities through test-time finetuning with retrieved motion-oriented videos and reference images, enabling consistent character appearance across multiple scenes while enhancing motion quality and transitions. DREAMRUNNER presents three essential processes in the framework (see Fig. 1): 1) Dual-Level Video Plan Generation, 2) Motion Retrieval and Subject/Motion Prior Learning, and 3) Spatial-Temporal Region-Based 3D Attention and Prior Injection (SR3AI). In (1) plan generation stage, given generic story narration provided by the user, we use large language model (LLM) for hierarchical planning by first generating high-level sequence of character-driven, motion-rich events across scenes (see High-Level Plan in Fig. 1), and then building detailed, entity-specific plans for each frame within single scene (see Fine-Grained Plan in Fig. 1). This two-step approach allows us to define precise entity motion and layout for each frame, ensuring that each scene is narratively coherent. In (2) motion retrieval and motion/subject priors learning stage, to enable the video diffusion model to accurately follow these scene-level scripts, especially for localized and complex motions involving multiple objects, we enhance the test-time motion adaptation approach in [69] with novel automatic retrieval pipeline which retrieves videos relevant to the desired motions from video database [52] for learning the motion prior. The prior is learned by only updating parameter-efficient modules [21] within the specific layers of DiT [40], while similar design is employed for learning the character priors [45]. In (3) video generation with region-based diffusion stage, we introduce novel spatial-temporal region-based 3D attention and prior injection module, SR3AI, designed for video generation with fine-grained control. Our SR3AI enables control over several aspects of video generation: (a) Detailed frame-by-frame semantics, supporting the smooth transition of multiple actions and across frames. (b) Regional motion and character control within the 3D attention layers, improving object-motion binding and reducing interference between the generation of multiple objects and motions. To achieve this, we first encode multiple conditions involved in the fine-grained plan. Next, in our SR3AI, for each condition, we compute the corresponding latents within the provided spatial-temporal layouts in the latent space, then for each condition, we mask its attention to unrelated regional latents, while unrelated conditions will be masked for regional latents as well. Such spatial-temporal-region-based attention allows fine-grained object-and-motion-level control of the generation process. Additionally, we inject the learned character and motion priors exclusively into their corresponding regions within the diffusion model, allowing multiple priors to be integrated seamlessly, avoiding conflicts and ensuring coherent results. We validate the effectiveness of DREAMRUNNER on two tasks: story-to-video generation and compositional text-tovideo generation. For story-to-video generation, we collect story dataset, DreamStorySet, and compare DREAMRUNNER against previous SoTA approaches (i.e., VideoDirectGPT [33] and VLogger [74]). DREAMRUNNER achieves 13.1% relative improvement in character consistency (CLIP score) and an 8.56% relative gain in text-following capability (ViCLIP score) over these approaches. Additionally, DREAMRUNNER enhances smooth event transitions within single scene, with 27.2% relative improvement In comin DINO score, underscoring its effectiveness. 2 positional text-to-video generation, DREAMRUNNER, built on CogVideoX-2B [62], outperforms baseline methods on T2V-CompBench [48] across all metrics, including attribute binding and motion, highlighting its strength in compositional generation. Notably, although based on an opensource model, DREAMRUNNER achieves the highest dynamic attribute binding and comparable results in spatial relationships and character interactions compared to closedsource models. This demonstrates the potential of our approach to elevate open-source models toward closed-source performance levels. Lastly, we present qualitative example to demonstrate the effectiveness of DREAMRUNNER in multi-character generation and motion binding. 2. Related Work Storytelling Video Generation. Storytelling video generation [16, 17, 35, 36, 68, 71, 74] aims to create multiscene videos based on given scripts. VideoDirectorGPT [33] and Vlogger [74] use LLMs for high-level planning, decomposing scripts into multi-scene conditions, and generating videos scene by scene. Animate-A-Story [17] further improves motion control by using retrieval-augmented videos as depth conditions. Recently, DreamStory [16] and MovieDreamer [68] generate keyframes using text-toimage models, which are then animated with image-tovideo models [9] to create coherent flow. Customization methods [13, 26, 29, 39, 45, 47, 54, 63, 70] are also involved to keep the consistency of reference images. Unlike these approaches, our work is designed to address the videocentric challenge of generating multi-characters, motionrich videos with smooth, natural transitions between different actions using dual-level LLM planning. Motion Customization. Motion customization is fundamental challenge in video generation. One line of research focuses on pixel-level motion learning from video editing perspective [23, 34, 43, 57, 66], where the goal is to generate videos that replicate low-level motions present in the original images, ensuring consistency in movement across frames. Another approach emphasizes learning generic motion priors, such as human or camera movements, from manually-curated collection of related videos [53, 58, 69, 73], capturing high-level semantic actions essential for realistic motion depiction. Most of these methods leverage testtime fine-tuning using specific motion LoRAs or adapters to adapt to particular motions. Our method follows similar pipeline but enhances it by learning motion priors from retrieved videos sourced from large-scale video databases, enabling more contextually relevant and diverse motion customization for generating realistic and dynamic scenes. Compositional Diffusion. Recent advances in diffusion models have opened new possibilities for compositional text-to-video generation by improving video coherence, semantic alignment, and user control. Several methods have explored utilizing large language models (LLMs) for finegrained scene planning [12, 32, 33]. With the fine-grained planning for video scenes, some approaches employ regional masks to control multi-object generation [22, 49, 55, 60, 64], improving both visual and semantic continuity in video generation. Other works explore frame-level semantic controls by applying different text conditions for different frames [7, 59]. Additionally, compositional techniques that integrate multiple LoRA modules have been developed to introduce diverse concepts seamlessly within the generation process [15, 30, 61, 72]. However, these methods do not specifically address the binding between objects and their corresponding actions spatial-temporally. Our approach focuses on fine-grained control over both objects and motions, with an emphasis on maintaining cohesive link between objects and their actions throughout the video. 3. Methodology Task Setup. Storytelling video generation focuses on creating multi-scene, character-driven videos based on given topic. The characters are defined by reference images (e.g., images of witch), and the topic is presented as an instructional prompt (e.g., \"witchs one day\"). The generated videos should align with the given topic and accurately reflect the characteristics and behavior of the characters. Method Overview. Our approach employs hierarchical system where an LLM generates event-based scripts across multiple scenes, followed by detailed plans specifying the layout and motion transitions of key objects per scene (Sec. 3.1). video diffusion model then synthesizes each scene step by step. We train motion priors from retrieval videos aligned with the LLM-generated plans, sourced from large-scale video-language database, and character priors using the reference images (Sec. 3.2). Finally, we inject these priors and detailed plans into the video generation process in zero-shot manner using our spatial-temporal regional diffusion module SR3AI (Sec. 3.3). Base Generation Model. We leverages CogVideoX2B [62] as the foundational text-to-video backbone. CogVideoX employs DiT-based architecture that integrates full 3D attention, rather than seperated spatial and temporal attentions. The model generates 6-second videos at 8 fps conditioned on input text. In our method, we extend CogVideoX by training character and motion priors in distinct layers (see Sec. 3.2) and by modifying its 3D attention (see Sec. 3.3) for better motion and character binding. 3.1. Generating Dual-Level Plans with LLMs Story-Level Croase-Grained Planning. We use an LLM to generate sequence of narrations that span multiple scenes. Specifically, given the task requirements, sin3 Figure 2. Implementation details for region-based diffusion. We extend the vanilla self-attention mechanism to spatial-temporalregion-based 3D attention (see upper orange part), which is capable of aligning different regions with their respective text descriptions via region-specific masks. The region-based character and motion LoRAs (see lower yellow and blue parts) are then injected interleavingly to the attention and FFN layers in each transformer block (see the right part). Note that although we resize the visual tokens into sequential 2D latent frames for better visualization, they are flattened and concatenated with all conditions when performing region-based attention. gle in-context example, and the specified story topic, we prompt GPT-4o [37] to generate six to eight narratives that focus on character-driven, motion-rich events. Each event is in the format of scene, motions, narrations, where we first define the related motions then generate corresponding event narration (the box of High-Level Plan in the middle in Fig. 1). This event list serves as high-level catalog that directs the storys progression across scenes, ensuring that each event is visuality and consistent with the narrative flow of the video. Scene-Level Fine-Grained Planning. After generating the list of single-scene events with narrations, we subsequently create detailed, entity-level plans for each frame within the single-scene event. Each plan begins with an overall background description, followed by entity-level details for each frame. As shown in the yellow Frame-level Plan box at the top of Fig. 2, the background serves as an overall scene description (e.g., \"A large garden with moonlit flowers\") formatted as Background: background description. Entity-level details specify each entitys description along with its motion (e.g., \"A [v1] witch is walking among the moonlit flowers in her garden\") and bounding box layout formatted as: Frame:[entity name,entity motion,entity description], [x0,y0,x1,y1]. Here, [x0,y0,x1,y1] represents the top-left and bottom-right corners of the bounding box, with each coordinate normalized to r0, 1s range. Entities without motion are labeled with \"none\" for motion. Each scene comprises plans for six key frames and each frame will guide each second to generate the six-second video with CogVideoX. Detailed prompt templates for coarseand fine-grained planning are provided in the appendix. 3.2. Motion Retrieval and Prior Learning Retrieving Motion-Related Videos from Database. We employ retrieval-augmented approach to fine-tune motion priors at test time, enhancing the models capacity to generate complex and diverse motions. Based on motion descriptions generated from the LLM planning, we retrieve relevant videos from large-scale video database [52]. For instance, for the query motion sitting, our retrieval pro4 cess consists of the following steps: videos. Formally, 1) Initial Retrieval with BM25: We use the text-only BM25 score [44] based on video captions in the dataset to retrieve 400 candidate videos for the query. To ensure the retrieved videos are human motion-centric, we add \"person is\" at the beginning of the query (\"person is sitting\"). 2) Attribute-Based Filtering: We refine the candidate pool by filtering videos based on key attributes such as duration (at least 2 second), frame count (at least 40 frames), and aspect ratio (width/height at least 0.9). This ensures that the selected videos align with the requirements of the video generator, excluding videos that are too short or have extreme aspect ratios. 3) Clip Segmentation via Object Tracking: We track individuals within videos using YOLOv5 [24] and segment clips into human-centered segments based on the tracking results, keeping meaningful human-focused content. 4) Scoring with CLIP and ViCLIP [42, 51]: To ensure the fidelity between the segmented video clips and the query, we compute semantic similarity scores to the query text (e.g., the person is sitting) using CLIP and ViCLIP for each segmented clip. The CLIP score is computed by sampling eight frames and averaging frame-query scores, while the ViCLIP score is directly computed on the full video and query. We select the top 20 videos that satisfy the average scores of CLIP and ViCLIP > 0.2. We retain the top four videos based on their ranking if fewer than four videos meet this threshold. By following this process, we retrieve 4 20 video clips per motion, which are then used for learning motion priors. Motion Prior Training. Following MotionDirector [69], we apply test-time fine-tuning to learn specific motions. In MotionDirector, one or more retrieved videos are used to parameter-efficiently fine-tune video diffusion model [1, 50] with LoRA. Temporal LoRA are injected into layers where temporal attention is applied to capture motion patterns (e.g., jumping), while per-video-specific LoRAs are injected into the spatial layers with spatial attention to capture unique characteristics of each video, while only the temporal LoRA are injected during inference. In our case, since we use CogVideoX [62] with 3D full attention instead of separate spatial and temporal attention, we manually designate the even layers as spatial layers and the odd layers as temporal layers to separate learning of spatial and temporal LoRAs. We train the LoRAs on our filtered topranked videos with all other backbone parameters frozen, using two diffusion losses: standard diffusion loss Lorg, which is reconstruction loss of all the video frames, and an appearance-debiased temporal loss Lad, which decouples the motion space from appearance space in the latent space, focusing on only reconstructing the motions in the Lorg Ez0,y,ϵN p0,1q,tU p0,T qrϵ ϵθpzt, t, yq2s (1) where z0 is the latent encoding of the training videos, is the text prompt condition, ϵ is the Gaussian noise added to the latent space, ϵθ is the predicted noise, and is the denoising time step. The appearance-debiased temporal loss optimizes the normalized latent space: ϕpϵq β2 ` 1ϵ βϵanchor (2) where ϵanchor is the anchor among the frames from the same training data, and β is the strength factor that controls the strength of the debiasing. Lad is defined as: Lad Ez0,y,ϵN p0,1q,tU p0,T qrϕpϵq ϕpϵθpzt, t, yqq2s (3) In the end, we update the model using combined motion loss function defined as Lmotion Lorg ` Lad. Notably, we do not apply scaling to each loss term throughout experiments in the paper, highlighting the robustness and simplicity of hyperparameter selection in DREAMRUNNER. Subject Prior Learning. We learn the subjects appearance by injecting LoRA modules into the spatial transformer layers. To train these LoRAs, we create videos by repeating reference images multiple times (48 time, similar to the output frame number of CogVideoX) and focus on reconstructing the first frame of the video during training, preventing overfitting to the static, repeated video. Notably, the subject priors are learned within spatial LoRAs, while the motion priors are learned within temporal LoRAs. Since their injections target different layers, there is no overlap, effectively avoiding conflicts between multiple LoRAs. 3.3. Sapatial-Temporal-Region-Based Diffusion Region-Based 3D Attention. We build our model on CogVideoX-2B [62], 2 billion text-to-video generation model designed on top of Diffusion Transformer (DiT). Unlike methods that use separate spatial and temporal attention for efficient video modeling, CogVideoX employs 3D full attention module, integrating self-attention across concatenated embeddings of all visual latents and the text condition embeddings. We extend this module to enable region-specific conditioning via masking, aligning different regions with their respective text descriptions. Specifically, given fine-grained plan with region-specific text descriptions C1, C2, . . . , CN and corresponding layouts L1, L2, . . . , LN across frames, we encode each text condition Ci to produce embeddings T1, T2, . . . , TN (Fig. 2 top right). At each attention layer, we identify the visual tokens corresponding to each layout Li in the latent space. We then perform masked self-attention on the concatenation of T1, T2, . . . , TN and L1, L2, . . . , LN . The self-attention 5 Method Image Fine-Grained Text Full Text VideoDirectorGPT [33] VLogger [74] DREAMRUNNER (Ours) CLIP 54.3 62.5 70.7 (+13.1%) DINO 9.5 41.3 55.1 (+33.4%) CLIP 23.7 23.5 24.7 (+5.11%) ViCLIP 21.7 23.1 23.7 (+2.60%) CLIP 22.4 22.5 24.2 (+7.56%) ViCLIP 22.5 22.2 24.1 (+8.56%) Transition DINO 63.5 73.6 93.6 (+27.2%) Table 1. Evaluation of story-to-video generation on DreamStorySet. We compare our model with VideoDirectorGPT [33] and VLogger [74] on character consistency (via CLIP and DINO scores), text instructions following and full prompt adherence (via CLIP and ViCLIP scores), and event transitions smoothness (via DINO score). Our relative improvement over VLogger is highlighted in blue. mask is defined as follows: for each regions visual latents Li, attention is allowed to its corresponding text condition Ti and all visual tokens L1, L2, . . . , LN . Conversely, for each condition Ti, attention is restricted to itself and its corresponding latents Li. This design ensures each region is conditioned on its specific textual description while maintaining interactions among visual latents through unmasked attention among L1, L2, . . . , LN . No modifications are made to other modules in the base model, preserving the integrity of its original architecture. Visualization examples of such masking strategy are contained in the appendix. Region-Based LoRA Injection. We adopt similar regionbased strategy for injecting LoRA priors into diffusion models. For each LoRA, we first identify the corresponding regions of latent tokens based on the associated text description and layout information. LoRA injection is then applied exclusively to these regions, ensuring precise alignment between the priors and their designated areas. This approach enables handling multiple LoRAs simultaneously while avoiding conflicts between them, preserving the integrity of each injected prior. 4. Experiments In this section, we first introduce the evaluation datasets and evaluation metrics details in Sec. 4.1, then compare our DREAMRUNNER with prior methods on story-to-video generation in Sec. 4.2. Next, we present detailed ablation studies on the necessity of RAG and effectiveness of SR3AI in Sec. 4.3, and demonstrate the generalizability of our DREAMRUNNER to improve compositional text-to-video generation on T2V-CompBench [48] in Sec. 4.4. Moreover, we show the effectiveness of RAG for learning the motion prior on more comprehensive motion dataset we collect in Sec. 4.5. Lastly, we present qualitative comparison between our DREAMRUNNER and previous approaches in Sec. 4.6. 4.1. Datasets and Evaluation Metrics Evaluation Datasets. We evaluate DREAMRUNNER on two tasks: (1) story-to-video generation, and (2) compositional text-to-video generation. The first task focuses on the models ability to follow the text closely while maintaining character and scene consistency throughout the story. RAG SR3AI Fine-Grained Text Full Text ˆ ˆ ˆ ˆ CLIP 23.8 23.9 24.7 24.7 ViCLIP 22.5 22.1 23.5 23.7 CLIP ViCLIP 22.2 22.5 23.9 24.2 22.1 22.4 24.0 24.1 Trans. 87.1 92.5 84.6 93.6 Table 2. Ablation studies for the effectiveness of RAG and SR3AI in DREAMRUNNER. Default DREAMRUNNER achieves the best text-following ability and event transition smoothness. The second task assesses various aspects of compositionality in video generation. For (1) story-to-video generation, we collect and introduce new benchmark dataset, DreamStorySet. Specifically, we collect 10 characters, including 6 from existing customization datasets (CustomConcept101 [27] and Dreambooth [45]), and 4 with generation models (FLUX [4]). (featuring two motions per scene) and three multi-character stories (featuring two or three motions per scene). Each story comprises 5 to 8 scenes, incorporating total of 64 diverse motions throughout. We focus on single-character stories for quantitative evaluation of SVG models and reserve multi-character stories for qualitative evaluation. For (2) compositional text-to-video generation, we use the T2V-CompBench [48] to benchmark the performance of DREAMRUNNER, where we select six dimensions except numeracy. Evaluation Metrics. We evaluate our generated storytelling videos across multiple dimensions, includ- (Frame-Reference-Image ing Character Consistency CLIP/DINO scores), Full-Narration-Text Alignment per Scene (Image/Video-Text CLIP/ViCLIP scores), FineGrained-Text Alignment per Scene (Image/Video-Text CLIP/ViCLIP scores), and Transition Smoothness (FrameFrame DINO score). Detailed descriptions of the metrics and computation methods can be found in the appendix. We follow similar metrics to T2V-ComBench [48] for evaluating compositional text-to-video generation. 4.2. Story-To-Video Generation Evaluation We compare our approach with previous SoTAs (VideoDirectorGPT [33] and VLogger [74]) for story-to-video generation on our DreamStorySet dataset. For VideoDirectorGPT and Vlogger, to promote better alignment between 6 Figure 3. Qualitative comparison of DREAMRUNNER on multi-scene story generation with multiple objects. DREAMRUNNER generates videos with significantly better character consistency compared to other strong baseline methods, while other methods either fail to maintain consistency for the same object across scenes (e.g., VLogger), fail to generate objects that match the reference images (e.g., VideoDirectorGPT), or fail to generate multiple objects correctly (e.g., CogVideoX w/ Character LoRAs). Model Consist-attr Dynamic-attr Spatial Motion Action Interaction Gen-3 [5] Dreamina [2] PixVerse [3] Kling [6] VideoCrafter2 [10] Open-Sora 1.2 [20] Open-Sora-Plan v1.1.0 [28] VideoTetris [49] LVD [31] CogVideoX-2B [62] CogVideoX-2B+Ours [62] 0.7045 0.8220 0.7370 0.8045 0.6750 0.6600 0.7413 0.7125 0.5595 0.6775 0.7350 0.2078 0.2114 0.1738 0.2256 0.1850 0.1714 0.1770 0.2066 0.1499 0.2118 0. 0.5533 0.6083 0.5874 0.6150 0.4891 0.5406 0.5587 0.5148 0.5469 0.4848 0.6040 0.3111 0.2391 0.2178 0.2448 0.2233 0.2388 0.2187 0.2204 0.2699 0.2379 0.2608 0.6280 0.6660 0.6960 0.6460 0.5800 0.5717 0.6780 0.5280 0.4960 0.5700 0. 0.7900 0.8175 0.8275 0.8475 0.7600 0.7400 0.7275 0.7600 0.6100 0.7250 0.8225 Table 3. T2V-CompBench evaluation results. Best/2nd best scores for open-sourced models are bolded/underlined. gray indicates close-sourced models, and yellow indicates the best score for close-sourced models. Method CogVideoX-2B CLIP ViCLIP 20.84 23.39 23.04 CogVideoX-2B + RAG 24. Table 4. Effectiveness of our retrieval-augmented test-time adaptation for learning better motion prior. the generated videos and motions, we decouple the narration for each scene into two consecutive single-motion descriptions. Videos are generated separately for each description and combined to form the final single-scene video. As shown in Table 1, DREAMRUNNER improves the CLIP score and DINO score by relative gain of 13.1% and 33.4% respectively compared to VLogger, showing the effectiveness of our learned subject prior and our regionbased LoRA injection technique to maintain character consistency throughout the video. Furthermore, we evaluate the models text-following capability across two dimensions: fine-grained text-following and full-prompt adherence. In DreamStorySet, each scene is designed to contain two distinct events, enabling more thorough assessment of both event transitions within single scene and the models capacity to follow detailed event descriptions. For full-prompt following, we measure the similarity between the complete prompt describing both events and the generated video. For fine-grained textfollowing, we assess the alignment between each events individual description and the corresponding portions of the generated video. Our DREAMRUNNER demonstrates improved performance in both CLIP and ViCLIP scores by up to 7.56% and 8.56% respectively compared to VLogger. Lastly, we evaluate the transition smoothness between events within single scene. Our goal is to maintain scene consistency while ensuring that events unfold naturally and smoothly. To quantify this, we uniformly sample four frames from the full video and calculate the average DINO score between frames, defining this as the Transition score. Our DREAMRUNNER significantly outperforms previous SoTAs in generating smoother event transitions, improving the Transition score by 27.2% over VLogger. This result suggests that our SR3AI design, which conditions generation across both spatial and temporal dimensions, effectively enhances the fluidity of transitions between events. 4.3. Ablation Studies In this section, we demonstrate the effectiveness of RAG for automatic video retrieval in motion prior learning and SR3AI for achieving fine-grained control over objects and their motion. As shown in Table 2, leveraging SR3AI for enhanced object and motion binding (2nd row) significantly improves the smooth transition between events within single scene (87.1ÝÑ92.5). Besides, incorporating retrieval-augmented motion prior learning (3rd row) boosts the video-text similarity (fine-grained text following ViCLIP score: 22.5ÝÑ23.5, and full prompt following ViCLIP score: 22.1ÝÑ24.0). Lastly, we show that RAG and SR3AI can be combined effectively (last row), with the combined model achieving the best performance in both text alignment and smooth event transitions. 4.4. Compositional T2V Generalization In this section, we demonstrate that DREAMRUNNER can be adapted to help the general compositional text-to-video generation task, as evaluated on T2V-CompBench [48]. Specifically, we employ an LLM (i.e., GPT-4o) to generate fine-grained, hierarchical plans based on the prompt, while SR3AI enables regional control over objects and their motions. Due to computational constraints, we did not include motion LoRA or spatial LoRA for learning motion or character priors for each action in the benchmark. As shown in Table 3, DREAMRUNNER significantly outperforms the baseline approach, CogVideoX-2B [62], across all categories. Notably, DREAMRUNNER enhances consistent attribute binding by 0.0575 and dynamic attribute binding by 0.0554, indicating that the detailed planning by LLM improves attribute binding during video generation. Additionally, DREAMRUNNER improves spatial relationship accuracy by 0.1192 and motion binding by 0.0229, showing that SR3AI aids in maintaining spatial relationships between objects and binding their motions. Furthermore, DREAMRUNNER enhances multi-object interactions by 0.0975, underscoring its capability to manage interactions among multiple objects effectively. Beyond baseline comparisons, DREAMRUNNER built on CogVideoX2B achieves superior or comparable performance compared to other open-sourced models (e.g., Open-Sora 1.2 [20], VideoTetris [49]), and achieves performance on par with some closed-sourced models (e.g., Gen-3 [5], PixVerse [3]), especially in dynamic attribute binding and maintaining This demonstrates our apgood spatial relationships. proachs effectiveness in advancing closed-sourced models to match or exceed open-sourced benchmarks and highlights its potential for adaptation to even more robust models, pushing the SoTA in compositional T2V generation. 4.5. Effect of RAG for Learning Motion Prior We investigate the effectiveness of retrieval-augmented testtime fine-tuning for learning an enhanced motion prior. Specifically, for each motion, we use an LLM [37] to provide 6 prompts for each motion in the whole 64-motion set, then evaluate the average CLIP/ViCLIP scores using these prompts. As shown in Table 4, applying our approach to CogVideoX-2B results in both CLIP and ViCLIP score improvements, with an increase of 1.28 in CLIP and 2.20 in ViCLIP. The substantial ViCLIP gain indicates stronger alignment between the story and the generated video, highlighting significant enhancements in motion accuracy. Additionally, the improvement in CLIP score shows that our approach better preserves semantic alignment within individual frames. These results validate that RAG effectively retrieves videos with relevant motions, aiding in learning more accurate motion prior for the model. 4.6. Qualitative Comparison Fig. 3 presents one qualitative comparison with other apSpecifically, we compare with VideoDirecproaches. torGPT [33], Vlogger [74], and CogVideoX [62] with character LoRA. We observe that other approaches fail to maintain consistent character appearances (e.g., the bear and robot) across generated videos. VideoDirectorGPT fails to preserve character appearance, Vlogger exhibits character appearance interference, and CogVideoX merges the two characters into single character with mixed features. In contrast, DREAMRUNNER successfully maintains the distinct appearances of both characters and generates realistic character interactions, which demonstrates that SR3AI significantly enhances fine-grained control over object-motion binding and reduces interference between different character LoRAs. Additionally, DREAMRUNNER achieves smoother event transitions within single scene, showing the effectiveness of LLM planning for detailed scene descriptions and frame-level semantic injection with SR3AI. 5. Conclusion In this work, we present DREAMRUNNER, novel framework for story-to-video generation. Specifically, DREAMRUNNER first utilizes LLM to structure hierarchical video plan, then introduces retrieval-augmented test-time adaptation to capture target motion priors, and finally generates videos using novel spatial-temporal region-based 3D attention and prior injection module for fine-grained object motion binding and frame-level semantic control. Experiments on both story-to-video and compositional T2V 8 generation benchmarks show that DREAMRUNNER outperforms strong baselines and SoTAs in tackling fine-grained complex motions, maintaining multi-scene consistency of multiple objects, and ensuring seamless scene transitions."
        },
        {
            "title": "Acknowledgments",
            "content": "This work was supported by DARPA ECOLE Program No. HR00112390060, NSF-AI Engage Institute DRL2112635, DARPA Machine Commonsense (MCS) Grant N6600119-2-4031, ARO Award W911NF2110220, ONR Grant N00014-23-1-2356, Accelerate Foundation Models Research program, and Bloomberg Data Science PhD Fellowship. The views contained in this article are those of the authors and not of the funding agency."
        },
        {
            "title": "References",
            "content": "[1] Zeroscope. https : / / huggingface . co / cerspense/zeroscope_v2_576w, 2023. 5 [2] Dreamina. https://dreamina.capcut.com/aitool/platform, 2024. 7 [3] Pixverse. https://app.pixverse.ai, 2024. 7, 8 [4] Flux. https : / / github . com / black - forest - labs/flux, 2024. 6, 13 [5] Gen-3. https : / / runwayml . com / blog / introducing-gen-3-alpha/, 2024. 7, 8 [6] Kling. https://kling.kuaishou.com/, 2024. 7 [7] Hritik Bansal, Yonatan Bitton, Michal Yarom, Idan Szpektor, Aditya Grover, and Kai-Wei Chang. Talc: Time-aligned captions for multi-scene text-to-video generation. arXiv preprint arXiv:2405.04682, 2024. [8] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Guanghui Liu, Amit Raj, et al. Lumiere: spacetime diffusion model for video generation. arXiv preprint arXiv:2401.12945, 2024. 2 [9] Wenhao Chai, Xun Guo, Gaoang Wang, and Yan Lu. Stablevideo: Text-driven consistency-aware diffusion video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2304023050, 2023. 3 [10] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. arXiv preprint arXiv:2401.09047, 2024. 7 [11] Weifeng Chen, Yatai Ji, Jie Wu, Hefeng Wu, Pan Xie, Jiashi Li, Xin Xia, Xuefeng Xiao, and Liang Lin. Control-a-video: Controllable text-to-video generation with diffusion models. arXiv preprint arXiv:2305.13840, 2023. 2 [12] Hao Fei, Shengqiong Wu, Wei Ji, Hanwang Zhang, and TatSeng Chua. Dysen-vdm: Empowering dynamics-aware textIn Proceedings of the IEEE to-video diffusion with llms. International Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 2, 3 [13] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. In Proceedings of the International Conference on Learning Representations (ICLR), 2023. [14] Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi Yin, Devi Parikh, and Ishan Misra. Emu video: Factorizing text-to-video generation by explicit image conditioning. arXiv preprint arXiv:2311.10709, 2023. 2 [15] Yuchao Gu, Xintao Wang, Jay Zhangjie Wu, Yujun Shi, Yunpeng Chen, Zihan Fan, Wuyou Xiao, Rui Zhao, Shuning Chang, Weijia Wu, et al. Mix-of-show: Decentralized lowrank adaptation for multi-concept customization of diffusion models. Advances in Neural Information Processing Systems, 36, 2024. 3 [16] Huiguo He, Huan Yang, Zixi Tuo, Yuan Zhou, Qiuyue Wang, Yuhang Zhang, Zeyu Liu, Wenhao Huang, Hongyang Chao, and Jian Yin. Dreamstory: Open-domain story visualization by llm-guided multi-subject consistent diffusion. arXiv preprint arXiv:2407.12899, 2024. 2, 3 [17] Yingqing He, Menghan Xia, Haoxin Chen, Xiaodong Cun, Yuan Gong, Jinbo Xing, Yong Zhang, Xintao Wang, Chao Weng, Ying Shan, et al. Animate-a-story: Storytelling with retrieval-augmented video generation. arXiv preprint arXiv:2307.06940, 2023. 2, 3 [18] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021. 12 [19] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. [20] hpcaitech. Open-sora: Democratizing efficient video production for all, 2024. 7, 8 [21] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 2 [22] Yash Jain, Anshul Nasery, Vibhav Vineet, and Harkirat Behl. Peekaboo: Interactive video generation via maskedIn Proceedings of the IEEE/CVF Conference diffusion. on Computer Vision and Pattern Recognition, pages 8079 8088, 2024. 3 [23] Hyeonho Jeong, Geon Yeong Park, and Jong Chul Ye. Vmc: Video motion customization using temporal attention adaption for text-to-video diffusion models. arXiv preprint arXiv:2312.00845, 2023. 3 [24] Glenn Jocher. YOLOv5 by Ultralytics, 2020. 5 [25] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Textto-image diffusion models are zero-shot video generators. In Proceedings of the International Conference on Computer Vision (ICCV), 2023. [26] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In Proceedings of the IEEE Inter9 national Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 3 [27] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19311941, 2023. 6, 13 [28] PKU-Yuan Lab and Tuzhan AI etc. Open-sora-plan, 2024. 7 [29] Dongxu Li, Junnan Li, and Steven C. H. Hoi. Blip-diffusion: Pre-trained subject representation for controllable text-toimage generation and editing. In Advances in Neural Information Processing Systems (NeurIPS), 2023. 3 [30] Jialu Li, Jaemin Cho, Yi-Lin Sung, Jaehong Yoon, and Mohit Bansal. Selma: Learning and merging skill-specific textto-image experts with auto-generated data. arXiv preprint arXiv:2403.06952, 2024. [31] Long Lian, Baifeng Shi, Adam Yala, Trevor Darrell, and arXiv Boyi Li. Llm-grounded video diffusion models. preprint arXiv:2309.17444, 2023. 7 [32] Long Lian, Baifeng Shi, Adam Yala, Trevor Darrell, and In The Boyi Li. Llm-grounded video diffusion models. Twelfth International Conference on Learning Representations, 2024. 3 [33] Han Lin, Abhay Zala, Jaemin Cho, and Mohit Bansal. Videodirectorgpt: Consistent multi-scene video generation via llm-guided planning. arXiv preprint arXiv:2309.15091, 2023. 2, 3, 6, 8 [34] Han Lin, Jaemin Cho, Abhay Zala, and Mohit Bansal. Ctrladapter: An efficient and versatile framework for adapting diverse controls to any diffusion model. arXiv preprint arXiv:2404.09967, 2024. 3 [35] Fuchen Long, Zhaofan Qiu, Ting Yao, and Tao Mei. Videostudio: Generating consistent-content and multi-scene videos, 2024. [36] Gyeongrok Oh, Jaehwan Jeong, Sieun Kim, Wonmin Byeon, Jinkyu Kim, Sungwoong Kim, and Sangpil Kim. Mevg: Multi-event video generation with text-to-video models. In European Conference on Computer Vision, pages 401418. Springer, 2025. 2, 3 [37] OpenAI. Hello, gpt-4 turbo, 2024. 4, 8, 12 [38] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 12 [39] Jae Wan Park, Sang Hyun Park, Jun Young Koh, Junha Lee, and Min Song. CAT: contrastive adapter training for personalized image generation. CoRR, abs/2404.07554, 2024. 3 [40] William Peebles and Saining Xie. Scalable diffusion modIn Proceedings of the International els with transformers. Conference on Computer Vision (ICCV), 2023. 2 [41] Zhiwu Qing, Shiwei Zhang, Jiayu Wang, Xiang Wang, Yujie Wei, Yingya Zhang, Changxin Gao, and Nong Sang. Hierarchical spatio-temporal decoupling for text-to-video generation. In Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [42] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language superIn Proceedings of the International Conference on vision. Machine Learning (ICML), 2021. 5 [43] Yixuan Ren, Yang Zhou, Jimei Yang, Jing Shi, Difan Liu, Feng Liu, Mingi Kwon, and Abhinav Shrivastava. Customize-a-video: One-shot motion customization of textto-video diffusion models. arXiv preprint arXiv:2402.14780, 2024. 3 [44] Stephen Robertson, Hugo Zaragoza, et al. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends in Information Retrieval, 3(4):333389, 2009. 5, 12 [45] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven In Proceedings of the IEEE/CVF Conference generation. on Computer Vision and Pattern Recognition, pages 22500 22510, 2023. 2, 3, 6, 13 [46] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. 2 [47] Kihyuk Sohn, Lu Jiang, Jarred Barber, Kimin Lee, Nataniel Ruiz, Dilip Krishnan, Huiwen Chang, Yuanzhen Li, Irfan Essa, Michael Rubinstein, Yuan Hao, Glenn Entis, Irina Blok, and Daniel Castro Chin. Styledrop: Text-to-image synthesis of any style. In Advances in Neural Information Processing Systems (NeurIPS), 2023. [48] Kaiyue Sun, Kaiyi Huang, Xian Liu, Yue Wu, Zihan Xu, Zhenguo Li, and Xihui Liu. T2v-compbench: comprehensive benchmark for compositional text-to-video generation. arXiv preprint arXiv:2407.14505, 2024. 3, 6, 8 [49] Ye Tian, Ling Yang, Haotian Yang, Yuan Gao, Yufan Deng, Jingmin Chen, Xintao Wang, Zhaochen Yu, Xin Tao, Pengfei Wan, Di Zhang, and Bin Cui. Videotetris: Towards compositional text-to-video generation, 2024. 3, 7, 8 [50] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. 2, 5 [51] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, Sen Xing, Guo Chen, Junting Pan, Jiashuo Yu, Yali Wang, Limin Wang, and Yu Qiao. Internvideo: General video foundation models via generative and discriminative learning. arXiv preprint arXiv:2212.03191, 2022. 5 [52] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, et al. Internvid: large-scale video-text dataset for multimodal understanding and generation. arXiv preprint arXiv:2307.06942, 2023. 2, 4, 12 [53] Yujie Wei, Shiwei Zhang, Zhiwu Qing, Hangjie Yuan, Zhiheng Liu, Yu Liu, Yingya Zhang, Jingren Zhou, and 10 hit Bansal. Zero-shot controllable image-to-video animation via motion decomposition. ACM, Multimedia, 2024. 3 [65] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng Shou. Show-1: Marrying pixel and latent diffusion models for text-to-video generation. International Journal of Computer Vision, pages 115, 2024. 2 [66] Yuxin Zhang, Fan Tang, Nisha Huang, Haibin Huang, Chongyang Ma, Weiming Dong, and Changsheng Xu. Motioncrafter: One-shot motion customization of diffusion models. arXiv preprint arXiv:2312.05288, 2023. 3 [67] Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng Zhang, Wangmeng Zuo, and Qi Tian. Controlvideo: Training-free controllable text-to-video generation. arXiv preprint arXiv:2305.13077, 2023. 2 [68] Canyu Zhao, Mingyu Liu, Wen Wang, Jianlong Yuan, Hao Chen, Bo Zhang, and Chunhua Shen. Moviedreamer: Hierarchical generation for coherent long visual sequence. arXiv preprint arXiv:2407.16655, 2024. 2, 3 [69] Rui Zhao, Yuchao Gu, Jay Zhangjie Wu, David Junhao Zhang, Jiawei Liu, Weijia Wu, Jussi Keppo, and Mike Zheng Shou. Motiondirector: Motion customization of text-tovideo diffusion models. arXiv preprint arXiv:2310.08465, 2023. 2, 3, [70] Matthew Zheng, Enis Simsar, Hidir Yesiltepe, Federico Tombari, Joel Simon, and Pinar Yanardag. Stylebreeder: Exploring and democratizing artistic styles through text-toimage models. CoRR, abs/2406.14599, 2024. 3 [71] Sixiao Zheng and Yanwei Fu. Temporalstory: Enhancing consistency in story visualization using spatial-temporal attention. arXiv preprint arXiv:2407.09774, 2024. 2, 3 [72] Ming Zhong, Yelong Shen, Shuohang Wang, Yadong Lu, Yizhu Jiao, Siru Ouyang, Donghan Yu, Jiawei Han, and Weizhu Chen. Multi-lora composition for image generation. arXiv preprint arXiv:2402.16843, 2024. 3 [73] Junchen Zhu, Lianli Gao, Jingkuan Song, et al. Echoreel: Enhancing action generation of existing video diffusion models. arXiv preprint arXiv:2403.11535, 2024. 3 [74] Shaobin Zhuang, Kunchang Li, Xinyuan Chen, Yaohui Wang, Ziwei Liu, Yu Qiao, and Yali Wang. Vlogger: Make your dream vlog. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88068817, 2024. 2, 3, 6, 8 Hongming Shan. Dreamvideo: Composing your dream videos with customized subject and motion. arXiv preprint arXiv:2312.04433, 2023. 3 [54] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. ELITE: encoding visual concepts into textual embeddings for customized text-to-image generation. In Proceedings of the International Conference on Computer Vision (ICCV), 2023. [55] Yujie Wei, Shiwei Zhang, Hangjie Yuan, Xiang Wang, Haonan Qiu, Rui Zhao, Yutong Feng, Feng Liu, Zhizhong Huang, Jiaxin Ye, et al. Dreamvideo-2: Zero-shot subjectdriven video customization with precise motion control. arXiv preprint arXiv:2410.13830, 2024. 3 [56] Wenming Weng, Ruoyu Feng, Yanhui Wang, Qi Dai, Chunyu Wang, Dacheng Yin, Zhiyuan Zhao, Kai Qiu, Jianmin Bao, Yuhui Yuan, et al. Art-v: Auto-regressive text-tovideo generation with diffusion models. In Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 2 [57] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 76237633, 2023. 3 [58] Ruiqi Wu, Liangyu Chen, Tong Yang, Chunle Guo, Chongyi Li, and Xiangyu Zhang. Lamp: Learn motion pattern for few-shot-based video generation. arXiv preprint arXiv:2310.10769, 2023. 3 [59] Zhen Xing, Qi Dai, Zejia Weng, Zuxuan Wu, and YuGang Jiang. Aid: Adapting image2video diffusion models for instruction-guided video prediction. arXiv preprint arXiv:2406.06465, 2024. 3 [60] Shiyuan Yang, Liang Hou, Haibin Huang, Chongyang Ma, Pengfei Wan, Di Zhang, Xiaodong Chen, and Jing Liao. Direct-a-video: Customized video generation with userIn Spedirected camera movement and object motion. cial Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers 24 (SIGGRAPH Conference Papers 24), page 12, New York, NY, USA, 2024. ACM. [61] Yang Yang, Wen Wang, Liang Peng, Chaotian Song, Yao Chen, Hengjia Li, Xiaolong Yang, Qinglin Lu, Deng Cai, Boxi Wu, and Wei Liu. Lora-composer: Leveraging lowrank adaptation for multi-concept customization in trainingfree diffusion models. arXiv preprint arXiv: 2403.11627, 2024. 3 [62] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 3, 5, 7, 8 [63] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. CoRR, abs/2308.06721, 2023. 3 [64] Shoubin Yu, Jacob Zhiyuan Fang, Skyler Zheng, Gunnar Sigurdsson, Vicente Ordonez, Robinson Piramuthu, and Mo11 A. Ablations of RAG pipeline We provide additional experiments in Section 3.2 to demonstrate the effectiveness of our data processing approach within the retrieval pipeline. Table 5 primarily evaluates the impact of the maximal number of retrieved videos and the use of CLIP and ViCLIP for filtering. For efficiency, we evaluate subset of eight motions selected from the full pool of 64 motions. The results indicate that, compared to the CogVideoX zero-shot baseline (Row 1), retrieving videos without any filtering (Row 2) improves performance, even though BM-25 [44] retrieval introduces some noise. This highlights the importance of retrieval itself. Furthermore, adding CLIP and ViCLIP as filters (Row 4) further enhances performance, showcasing the benefit of using semantically aligned videos for improved motion learning. Additionally, retrieving sufficient number of videos is critical, as evidenced by Row 3, where limiting retrieval to maximum of three videos results in poorer performance compared to retrieving 20 videos (Row 4). Max. #Retrieval CLIP+ViCLIP filter CLIP ViCLIP 20.56 22.51 22.80 23.66 23.42 24.01 24.45 25.47 0 20 3 20 ˆ ˆ Table 5. Pipeline component ablation on retrieval-augmented testtime adaptation for learning better motion prior. B. Evaluation Metrics We provide detailed evaluation metrics for evaluating generated storytelling videos in multiple dimensions in Section 4.1. Similarity to Reference Images: We assess the alignment between the generated videos and reference images using Image-Image CLIP [18] and DINO [38] scores. The evaluation is conducted by averaging the CLIP/DINO similarity scores with each frame of the generated video and the reference images, with CLIP-L14 (336px) [18] and DINOv2-B16 [38] as image encoders. Similarity to Full Narration per scene: We measure the alignment between the full narration and the generated videos for each scene using Image-Text CLIP [18] and Video-Text ViCLIP [52] scores. For the CLIP score, we uniformly sample eight frames from the single-scene video and compute the average score between each frame and the full narration. For the ViCLIP score, we directly use the alignment score between the video and the narration. Per-scene scores are averaged to obtain the overall score for the multi-scene storytelling video. Fine-Grained Text Alignment per Scene: We also assess fine-grained alignment to textual descriptions using Image-Text CLIP [18] and Video-Text ViCLIP [52] 12 In our story, each narration contains two moscores. tions, which we decouple into two consecutive singlemotion descriptions using an LLM [37]. For each generated single-scene video, we divide it into two segments at the temporal midpoint. We then compute the CLIP/ViCLIP scores between the first segment and the first description and between the second segment and the second description. The average of these two scores provides the per-scene score, and the per-scene scores are further averaged to compute the overall score for the storytelling video. Transition: We assess whether the single-scene video achieves smooth transitions between two motions. To evaluate this, we uniformly sample four frames from the video per scene and calculate the average DINO similarity between adjacent frames. higher transition score indicates smoother transitions, as it reflects minimal changes in the background across frames. C. Compositional T2V Examples In this section, we present qualitative examples demonstrating the capabilities of DREAMRUNNER in compositional text-to-video generation. DREAMRUNNER effectively generates videos with accurate action binding to different characters, consistent attributes across objects, dynamic attribute changes, motion control, object interactions, and appropriate spatial relationships between objects. For action binding, as shown in Figure 4, DREAMRUNNER generates distinct motions for two objects (e.g., wolf howling into microphone and fox playing the drums), ensuring the actions are correctly bound to their respective objects without interference. For consistent attribute binding, as illustrated in Figure 5, our approach maintains separate attributes for different objects (e.g., spherical globe and cube-shaped clock) without any overlap or inconsistency. For dynamic attribute changes, as shown in Figure 6, DREAMRUNNER naturally transitions object attributes over time (e.g., metal gradually rusting throughout the video). For motion control, as depicted in Figure 7, DREAMRUNNER successfully directs object movements in different trajectories (e.g., kite moving from left to right and cyclist traveling from right to left). For object interactions, as illustrated in Figure 8, interactions are accurately modeled, adhering to physical rules (e.g., pottery shattering upon hitting the floor). Finally, for spatial relationships, as shown in Figure 9, DREAMRUNNER generates rare or imaginative configurations (e.g., duck positioned under spacecraft) while maintaining spatial coherence. These examples highlight the strong performance of DREAMRUNNER in generating high-quality compositional text-to-video outputs. D. Characters. We provide character examples in Figure 10, where the first four are generated using FLUX [4], and the others are collected from existing customization datasets (CustomConcept101 [27] and Dreambooth [45]). E. Single-Character Examples In this section, we present qualitative examples of video generation featuring single main character. As shown in Figure 12 and Figure 11, DREAMRUNNER generates consistent characters throughout the entire story. Additionally, in each scene, DREAMRUNNER effectively captures multiple events, such as the mermaid first wandering through the plants and then examining unique shells. F. Multi-Character Examples In this section, we present qualitative examples of video generation featuring multiple characters. As illustrated in Figure 13 and Figure 14, DREAMRUNNER generates multiscene, multi-character videos where each character retains its own motion and interacts seamlessly with others, without any interference. For instance, in Figure 13, the witch is shown pouring ingredients while the cat wanders around the room. Even as the cat approaches the witch, their motions remain independent, and the appearance of both characters is consistently preserved throughout the scene. G. Region-Based 3D Attention Masks Figure 15 shows and example of the attention Mask of our Spatial-Temporal Region-based 3D Attention introduced in Section 3.3. Different text colors represent different conditions, while the white region indicates the masked areas. Note that for simplicity, we reduce each condition to two words, each frame to three segments, and display only three conditions and two frames in the figure. In practice, conditions can be longer and more numerous, frames can have more segments, and there are 12 latent frames in total. H. LLM Prompts We provide detailed LLM prompts for both high-level plans and fine-grained plans in Listing 1 and Listing 3. respectively. For high-level plans, we use simple in-context example with instructions, while fine-grained plans require reasoning before generating the output. Example outputs are shown in Listings 2 and 4. For multi-character scenarios, we modify character-related words and examples and limit required motions to maximum of four. 13 Figure 4. Qualitative results of DREAMRUNNER generated with prompts characterizing action binding. 14 Figure 5. Qualitative results of DREAMRUNNER generated with prompts characterizing consistent attribute binding. 15 Figure 6. Qualitative results of DREAMRUNNER generated with prompts characterizing dynamic attribute binding. 16 Figure 7. Qualitative results of DREAMRUNNER generated with prompts characterizing motion binding. 17 Figure 8. Qualitative results of DREAMRUNNER generated with prompts characterizing object interactions. 18 Figure 9. Qualitative results of DREAMRUNNER generated with prompts characterizing spatial relationships. 19 Figure 10. Qualitative results of DREAMRUNNER generated with single character (mermaid). 20 Figure 11. Qualitative results of DREAMRUNNER generated with single character (astronaut). 21 Figure 12. Qualitative results of DREAMRUNNER generated with single character (mermaid). 22 Figure 13. Qualitative results of DREAMRUNNER generated with multiple characters (witch and cat 1). 23 Figure 14. Qualitative results of DREAMRUNNER generated with multiple characters (warrior and dog 2). 24 Figure 15. Visualization of spatial-temporal region-based 3D attention masks. Different text colors represent different conditions, while the white region indicates the masked areas. For simplicity, we reduce each condition to two words, each frame to three segments, and display only three conditions and two frames in the figure. In practice, conditions can be longer and more numerous, frames can have more segments, and there are 12 latent frames in total. 25 Listing 1. The LLM prompt for high-level planning (Section 3.1, Story-Level Croase-Grained Planning). User may input the high-level planning to generate at the [Input] highlighted in blue. We highlight scene, motions, narrations using same colors as Section 3.1. Consider you are an expert in writing stories. will provide you with topic, and you need to create multi-scene story with 5 to 8 scenes. Each scene should describe the events taking place, emphasizing highlighting human actions or motions. Limit each scene to maximum of 2 distinct human motions. Your output should include both the scene and motions as well as the narration, where the motion should be in present progressive. [Input] Teddys one day [Output] Scene 1: bedroom Motions: waking up, stretching arms Narration: Teddy starts his day in his bedroom. He wakes up from the bed and stretches his arms beside it, getting ready for new day. Scene 2: kitchen Motions: pouring cereal, drinking Narration: Teddy is in the kitchen for breakfast. He pours cereal into bowl in the kitchen, and drinks the soup, preparing his morning meal. Scene 3: backyard Motions: watering plants, examining flower Narration: Teddy spends some time outside in the backyard. He waters the garden plants and kneels down to examine flower, enjoying the fresh air and tending to the plants. Scene 4: park Motions: jogging, breathing Narration: Teddy visits the park for some exercise. He jogs in the park, then he becomes very tired so he breaths heavily, regaining his energy. Scene 5: forest Motions: hiking, sitting Narration: Teddy goes hiking on forest trail. He hikes up the trail and then sits on rock to rest, appreciating the tranquility of nature. Scene 6: living room Motions: sitting, reclining Narration: Teddy winds down at the end of the day in the living room. He sits on the couch and then reclines to relax, ending his day in comfort and peace. [Input] Mermaids Adventure 26 Listing 2. The example of the output from the high-level planning prompt, Listing 1. We highlight scene, motions, narrations using same colors as Section 3.1. [Output] Scene 1: Coral Reef Motions: swimming, touching Narration: The Mermaid begins her day through the vibrant coral reef. She swims smoothly between colorful corals and touches the corals with her curiosity. Scene 2: Sunken Ship Motions: diving, exploring Narration: The Mermaid diving toward the old, mysterious sunken ship, exploring its hidden corners. Her movements are careful as she examines every detail, wondering what secrets the ship might reveal. Scene 3: Open Ocean Motions: gliding, waving Narration: In the open ocean, the Mermaid glides effortlessly through the water. She encounters friendly dolphin and waves as it playfully circles around her before swimming off. Scene 4: Kelp Forest Motions: wandering, inspecting Narration: The Mermaid enters dense kelp forest, wandering through the tall, flowing plants. She pauses to inspect some unique shells caught on kelp strand. Scene 5: Sea Cave Motions: swimming, picking up Narration: The Mermaid ventures into shadowy sea cave, swimming cautiously. She finds glowing stone on the cave floor and picks it up, mesmerized by its mysterious light. Scene 6: Lagoon Motions: lying, splashing Narration: The Mermaid finishes her journey by secluded lagoon, lying near the shore. With playful motion, she splashes water around, basking in the warm sunlight to end her day. 27 Listing 3. LLM prompt for fine-grained frame-level plan (Section 3.1, Scene-Level Fine-Grained Planning) per narration. We highlight Background, [entity, motion, caption] and layout [left, top, right, bottom] using same colors as Section 3.1. Assuming the frame size is normalized to the range 0-1, you need to give possible 6-frame layout plan at 1fps with the relevant regions, containing entity with corresponding motion, caption and bounding box involved in the input example motion and narration. You should follow these instructions: 1. [Background and Regions] You need to give background of the videos, then list all regions with related entity, motion, caption, and bounding box, for each frame. 2. [Bounding Box Size] Each bounding box of the region is one rectangle or square box in the layout, and the size of boxes should be AS LARGE AS POSSIBLE. The width and height of each bounding box should be at least 0.2. 3. [Bounding Box Format] Every region should contain one related motion and caption, with the bounding boxes in the format of [[entity1, motion1, caption1], [left, top, right, bottom]]. If the entity doesn involve any motion, use \"none\" as the motion (e.g. [\"table\", \"none\", \"a table in the forest\"]). 4. [Captions and Motions)] \"IMPORTANT\" For each entity, you should give caption containing the entity and motion, with the provided background. 5. [Allowing Overlaps for Interaction Contexts] Regions can overlap if necessary, particularly when entities are interacting. For example, in \"Teddy is pouring water into bowl,\" you can have region for \"a bowl\" overlapping or near Teddys region. Similarly, in \"Teddy is sitting by the river,\" you can have region for \"river bank\" overlapping or positioned beneath Teddys region. 5. [Allowing Overlaps for Interaction Contexts] Regions can overlap if necessary, particularly when entities are interacting. For example, in \"Teddy is pouring water into bowl,\" you can have region for \"a bowl\" overlapping or near Teddys region. Similarly, in \"Teddy is sitting by the river,\" you can have region for \"river bank\" overlapping or positioned beneath Teddys region. 6. [Interaction with Objects] For an object listed in previous frames, when the entity is interacting with it in the current frames, you can still list the interacted object, then also use whole region describing the character interacting with the object (e.g. [[\"rock\", \"none\", \"a rock in the forest\"], [0.0, 0.8, 0.2, 1.0]] before entity Teddy is interacting with it, [[\"rock\", \"none\", \"a rock in the forest\"], [0.0, 0.8, 0.2, 1.0]], [[\"Teddy\", \"sitting\", \"Teddy is sitting on the rock in the forest\"], [0.0, 0.0, 0.4, 1.0]] when interacting). 7. [Static Interaction Objects] In most cases, the interacted objects should be static. For example, \" Teddy is greeting to people\" then \"people\" should have no bounding box changes. For some non-static objects like \"the bottle\" of \"Teddy is drinking water\", ignore such objects to be listed seperatedly. 8. [Motion Limitation] You should not use other motions outside the provided narration. The motion and caption should all be in PRESENT PROGRESSIVE. 9. [Motion Duration] One motion should last at least two frames. 10. [Reasoning] Add reasoning before you generate the region plan, explaining how you will allocate different events/motions to different frames and how the entity will be moving (e.g., left to right or staying static). 11. [Smart Motion Allocation] Allocate motions smartly by reasoning the frames they need (e.g., static motions like standing will require fewer frames but walking may need more). The background should not contain information about the characters (e.g., Teddys xxx is not allowed). 12. [Common Sense in Layout] Make sure the locations of the generated bounding boxes are consistent with common sense. You need to generate layouts from the close-up camera view of the event. The layout difference between two adjacent frames should not be too large, considering the small interval. 13. [Motions for Large Position Changes] If you want to move the entity largely by changing its bounding boxes (e.g., from most right to most left), make sure the motion naturally involves position changes (e.g., walking, running, flying, riding bike). Otherwise, avoid big changes in related bounding boxes (e.g., cooking, jumping, playing guitar, etc., where bounding boxes should not change significantly). 14. [Big Regions Preference] We prioritize using large regions for main entities. For example, [0.2, 0.0, 0.8, 1.0] for main teddy bear, as long as it fits the scene. Do not use small regions (like with only 0.20.3 width and height) as possible as you can. 15. [Motion Caption Consistency] Ensure that each caption accurately reflects the stated motion. For example, if the motion is \"walking\" the caption should match this exactly, such as \"Teddy is walking in the park.\" Avoid any inconsistencies where the motion described in the caption does not match the stated motion, such as \"Teddy is running in the park\" when the motion is \"walking.\" This consistency maintains clarity and alignment with the narration. Use format: *Reasoning* reason *Plan* Background: Frame_1: [[entity1, motion1, caption1], [left, top, right, bottom]], [[entity2, motion2, background 28 caption2], [left, top, right, bottom]], ..., [[entity3, motion3, caption3], [left, top, right, bottom]] Frame_2: [[entity1, motion1, caption1], [left, top, right, bottom]], [[entity2, motion2, caption2], [left, top, right, bottom]], ..., [[entity3, motion3, caption3], [left, top, right, bottom]] ... Frame_6: [[entity1, motion1, caption1], [left, top, right, bottom]], [[entity2, motion2, caption2], [left, top, right, bottom]], ..., [[entity3, motion3, caption3], [left, top, right, bottom]] Reasoning: ... Example 1: [Input] Motion: walking, sitting Narration: Teddy goes to forest. He walks on the trail and then sits on rock to rest, appreciating the tranquility of nature. [Output] *Reasoning* Listed motions are: walking, sitting. Related entities and motions: Teddy (from walking to sitting), rock (will be sit by Teddy in the end, but no related motions). Motion frames allocation: The main entity involve motion changes is Teddy. Teddys motion changes from walking to sitting. As sitting takes less time, we should allocate more frames to walking. so the plan is 4 frames for walking and 2 frames for sitting. (Following bullet point 11 [Smart Motion Allocation]) Bounding box changes: For Teddy, the first motion is walking, which involves position changes little bit. And the second motion is sitting, which should be bounding-box-static motion. So the Teddy bear can have bounding boxes changes at first and finally interacts with the involve any motion, so it should be static from far from to close to, and finally interacting with Teddy. (Following bullet point 13 [Motions for Large Position Changes]) Interaction: This input narration involve interaction between the rock and Teddy and happens in late frames, so the rock and Teddy will have one merged-region where the entity is Teddy and the caption is about Teddy sitting on the rock. ALso the rock will also be listed from frame 1 to 3, while maintained from 4 to 6, as static object. (Following bullet point 5 [Allowing Overlaps for Interaction Contexts ], 6 [Interaction with Objects], 7 [Static Interaction Objects]) For the rock, it doesnt In conclusion, the plan can be: Teddy is at the right side at the beginning of the video, and theres also rock on the left right corner. From Frame 1 to Frame 4, the Teddy bear is hiking on the trail moving from right to left to approaching the rock. In Frame 5 and Frame 6, it reaches the rock and sits on the rock (which is on the left corner) to rest, in the left side of the video. the forest *Plan* Background: Frame_1: [[\"Teddy\", \"hiking\", \"Teddy is hiking on trail in the forest\"], [0.6, 0.0, 1.0, 1.0]], [[\"rock\", \"none\", \"a rock in the forest\"], [0.0, 0.8, 0.2, 1.0]] Frame_2: [[\"Teddy\", \"hiking\", \"Teddy is hiking on trail in the forest\"], [0.47, 0.0, 0.87, 1.0]], [[\"rock\", \"none\", \"a rock in the forest\"], [0.0, 0.8, 0.2, 1.0]] Frame_3: [[\"Teddy\", \"hiking\", \"Teddy is hiking on trail in the forest\"], [0.33, 0.0, 0.73, 1.0]], [[\"rock\", \"none\", \"a rock in the forest\"], [0.0, 0.8, 0.2, 1.0]] Frame_4: [[\"Teddy\", \"hiking\", \"Teddy is hiking on trail in the forest\"], [0.2, 0.0, 0.6, 1.0]], [[\"rock\", \"none\", \"a rock in the forest\"], [0.0, 0.8, 0.2, 1.0]] Frame_5: [[\"Teddy\", \"sitting\", \"Teddy is sitting on the rock in the forest\"], [0.0, 0.0, 0.4, 1.0]], [[\"rock\", \"none\", \"a rock in the forest\"], [0.0, 0.8, 0.2, 1.0]] Frame_6: [[\"Teddy\", \"sitting\", \"Teddy is sitting on the rock in the forest\"], [0.0, 0.0, 0.4, 1.0]], [[\"rock\", \"none\", \"a rock in the forest\"], [0.0, 0.8, 0.2, 1.0]] [Input] Motions: swimming, touching Narration: The Mermaid begins her day through the vibrant coral reef. She swims smoothly between colorful corals and touches the corals with her curiosity. 29 Listing 4. The example output of fine-grained planning prompt 3. We highlight Background, [entity, motion, caption] and layout [left, top, right, bottom] using same colors as Section 3.1. [Output] *Reasoning* Listed motions are: swimming, touching. Related entities and motions: Mermaid (from swimming to touching), corals (being touched by the mermaid ). Motion frames allocation: The main entity involving motion changes is the Mermaid. The first motion, swimming, can last relatively longer with more frame coverage as it involves position changes. Touching corals happens afterward and should occupy fewer frames with minimal position changes. Bounding box changes: The swimming motion involves position changes, so the bounding box for the Mermaid can have slight shifts, swimming through the corals from left to right. The touching motion of the Mermaid should be static, focusing on the interaction at the coral, which is near the center of the frame. Interaction: The interaction between the Mermaid and the corals happens later in the narration, so combined region featuring the Mermaid touching the corals will be involved in the last two frames. Non-interacting corals will appear in all frames, having stable bounding boxes across the video to show the environment and the Mermaids interaction context. The plan is as follows: The Mermaid swims through the vibrant coral reef, starting from near the left, moving towards the center, where the corals are located. On reaching the center, she begins touching the corals with fascination. the vibrant coral reef full of colors and life *Plan* Background: Frame_1: [[\"Mermaid\", \"swimming\", \"The Mermaid is swimming through the vibrant coral reef\"], [0.0, 0.0, 0.4, 1.0]], [[\"corals\", \"none\", \"Colorful corals in the reef\"], [0.5, 0.3, 0.8, 0.6]] Frame_2: [[\"Mermaid\", \"swimming\", \"The Mermaid is swimming through the vibrant coral reef\"], [0.15, 0.0, 0.55, 1.0]], [[\"corals\", \"none\", \"Colorful corals in the reef\"], [0.5, 0.3, 0.8, 0.6]] Frame_3: [[\"Mermaid\", \"swimming\", \"The Mermaid is swimming through the vibrant coral reef\"], [0.3, 0.0, 0.7, 1.0]], [[\"corals\", \"none\", \"Colorful corals in the reef\"], [0.5, 0.3, 0.8, 0.6]] Frame_4: [[\"Mermaid\", \"swimming\", \"The Mermaid is swimming through the vibrant coral reef\"], [0.45, 0.0, 0.85, 1.0]], [[\"corals\", \"none\", \"Colorful corals in the reef\"], [0.5, 0.3, 0.8, 0.6]] Frame_5: [[\"Mermaid\", \"touching\", \"The Mermaid is touching the corals with her curiosity\"], [0.4, 0.1, 0.7, 0.9]], [[\"corals\", \"none\", \"Colorful corals in the reef\"], [0.5, 0.3, 0.8, 0.6]] Frame_6: [[\"Mermaid\", \"touching\", \"The Mermaid is touching the corals with her curiosity\"], [0.4, 0.1, 0.7, 0.9]], [[\"corals\", \"none\", \"Colorful corals in the reef\"], [0.5, 0.3, 0.8, 0.6]]"
        }
    ],
    "affiliations": [
        "UNC Chapel Hill"
    ]
}