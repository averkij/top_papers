{
    "paper_title": "MASS: Motion-Aware Spatial-Temporal Grounding for Physics Reasoning and Comprehension in Vision-Language Models",
    "authors": [
        "Xiyang Wu",
        "Zongxia Li",
        "Jihui Jin",
        "Guangyao Shi",
        "Gouthaman KV",
        "Vishnu Raj",
        "Nilotpal Sinha",
        "Jingxi Chen",
        "Fan Du",
        "Dinesh Manocha"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision Language Models (VLMs) perform well on standard video tasks but struggle with physics-driven reasoning involving motion dynamics and spatial interactions. This limitation reduces their ability to interpret real or AI-generated content (AIGC) videos and to generate physically consistent content. We present an approach that addresses this gap by translating physical-world context cues into interpretable representations aligned with VLMs' perception, comprehension, and reasoning. We introduce MASS-Bench, a comprehensive benchmark consisting of 4,350 real-world and AIGC videos and 8,361 free-form video question-answering pairs focused on physics-related comprehension tasks, with detailed annotations including visual detections, sub-segment grounding, and full-sequence 3D motion tracking of entities. We further present MASS, a model-agnostic method that injects spatial-temporal signals into the VLM language space via depth-based 3D encoding and visual grounding, coupled with a motion tracker for object dynamics. To strengthen cross-modal alignment and reasoning, we apply reinforcement fine-tuning. Experiments and ablations show that our refined VLMs outperform comparable and larger baselines, as well as prior state-of-the-art models, by 8.7% and 6.0%, achieving performance comparable to close-source SoTA VLMs such as Gemini-2.5-Flash on physics reasoning and comprehension. These results validate the effectiveness of our approach."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 3 7 3 8 1 . 1 1 5 2 : r MASS: Motion-Aware Spatialtemporal Grounding for Physics Reasoning and Comprehension in Vision-Language Models Xiyang Wu1,2 * Zongxia Li1 Jihui Jin2 Guangyao Shi3 Gouthaman KV2 Vishnu Raj2 Nilotpal Sinha2 Jingxi Chen1 Fan Du2 Dinesh Manocha1 1University of Maryland 2Dolby Laboratories 3University of Southern California wuxiyang@umd.edu"
        },
        {
            "title": "Abstract",
            "content": "Vision Language Models (VLMs) perform well on standard video tasks but struggle with physics-driven reasoning involving motion dynamics and spatial interactions. This limitation reduces their ability to interpret real or AI-Generated Content (AIGC) videos and to generate physically consistent content. We present novel approach to address this gap by translating physical-world context cues into interpretable representations aligned with VLMs perception, comprehension, and reasoning. We introduce comprehensive benchmark, MASS-Bench, consisting of 4, 350 real-world and AIGC videos and 8, 361 free-form video questionanswering pairs focused on physics-related comprehension tasks, with detailed annotations including visual detections and grounding over sub-segments, as well as full-sequence 3D motion tracking of entities. We further present MASS, modelagnostic approach that injects spatialtemporal signals into the VLM language space via depth-based 3D encoding and visual grounding, coupled with motion tracker for object dynamics. To strengthen cross-modal alignment and reasoning, we apply reinforcement fine-tuning. Experiments and ablations show that our refined VLMs outperform comparable and larger baselines, and prior state-of-the-art models, by 8.7% and 6.0%, achieving performance comparable to close-source SoTA VLMs like Gemini-2.5-Flash on physics reasoning and comprehension, validating the effectiveness of our approach. 1. Introduction Vision Language Models (VLMs) demonstrate strong reasoning and comprehension in standard video tasks such as captioning [25], event recognition [29] and scene understanding [58]. However, they struggle with complex visual cues that involve intertwined 3D spatial layouts [8, 54], motion patterns [10], and temporal dynamics [19, 62]. Achieving *Work done during an internship at Dolby Laboratories. robust physical understanding requires VLMs not only to perceive visual cues, but also to internalize real-world physical principles and commonsense expectations about object behavior [13]. Such physical reasoning, central to humanlevel video understanding, remains challenging, as models must connect visual evidence with underlying physical dynamics and reason about whether observed events align with or violate real-world physics. central difficulty lies in the implicit nature of physical laws. Unlike tasks such as segmentation or object grounding, where the supervision is explicit and localized, real-world physics must be inferred from indirect and often ambiguous visual evidence. Many phenomena governed by the same underlying principle can manifest in drastically different visual forms. For example, an apple falling and person standing still are both influenced by gravity but differ substantially in spatial configuration, motion patterns, and temporal structure. As result, VLMs often fail to generalize across diverse physical processes, challenge further magnified by the scarcity of datasets with dense spatiotemporal and motion-level annotations. Without such supervision, models tend to memorize superficial correlations rather than develop physics-grounded reasoning. Effective physics comprehension requires several demanding prerequisites, spatial and temporal understanding, motion tracking, object detection, and visual grounding, yet these components are rarely annotated in sufficient detail. This lack of rich spatiotemporal data widens the gap between physics-driven video dynamics and VLM cognition, ultimately degrading their ability to reason coherently about real-world physical behavior. Compounding these issues, existing VLMs are typically trained on large collections of real-world videos, where object motions naturally conform to physical laws. While such data can help models implicitly acquire certain motion patterns, it also encourages strong language and visual priors: VLMs tend to assume that observed actions are physically plausible simply because they resemble frequent patterns in the training set. This reliance on priors becomes particularly problematic in the era of AI-generated (AIGC) videos, 1 Figure 1. Physics-Centric Video Question Answering. Physics-aware video comprehension is challenging, as VLMs must capture finegrained spatialtemporal cues and integrate them for higher-level reasoning. MASS introduces motion-aware spatialtemporal grounding module that explicitly encodes object motions and scene dynamics into the language space. By enriching VLMs with structured spatial, temporal, and semantic signals, MASS significantly improves downstream reasoning, including motion and action understanding, physicalprocess inference, and abnormality detection (e.g., identifying the counterfactual upward motion of basketball). MASS outperforms strong SoTA models such as GPT-4o and Gemini-2.5-Flash, demonstrating robust physics comprehension and reasoning across diverse tasks. where implausible trajectories, inconsistent depth cues, or temporally incoherent motions are common. Prior work has shown that VLMs frequently hallucinate or overlook physics abnormalities [4, 32]; for instance, model may conclude that an apple must be falling simply because it appears near tree, reflecting prior expectations rather than actual visual evidence. These challenges highlight the need for methods that go beyond raw video pixels and high-level textual supervision. Instead of training VLMs from scratch, which is infeasible and very costly in time and computational resource, one more data-efficient way is to augment VLMs with structured spatial and temporal representations that capture fine-grained object motions, interactions, and geometry. Recent advances in motion grounding [14], visual grounding, detailed temporal modeling [57], and action analysis [15, 21] demonstrate that smaller specialized models can provide highly accurate spatialtemporal cues. However, existing approaches rarely explore how such modules can be systematically integrated into VLMs to elevate higher-level physics reasoning. Meanwhile, prior physics-oriented datasets [4, 27, 32] primarily rely on coarse annotations, limiting their capacity to induce deep, mechanism-level understanding. Main Results: Our approach, MASS, aims to bridge the gap between the structured dynamics of the physical world and VLMs perception, comprehension, and reasoning abilities. We leverage expert models spatial and motion representations to explicitly ground entities and encode their 3D trajectories, enabling VLMs to reason about motion, interactions, and spatial constraints rather than relying on coarse priors. To evaluate these capabilities, we design comprehensive free-form video QA benchmark covering physics phenomena with both factual and critical-thinking questions. For post-training, we explore both supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT) using Group Relative Policy Optimization (GRPO) to strengthen cross-modal alignment and reasoning under complex physical contexts. The novel contributions of our work include: We propose benchmark, MASS-Bench, of 4, 350 real and AIGC videos with 8, 361 free-form QA pairs on spatialtemporal comprehension, physics-related reasoning, and It includes entity-level motion abnormality detection. grounded annotation with visual grounding over spatial and temporal dimension and dense spatial-motion representations across videos. We introduce MASS, model-agnostic, motion-aware spatialtemporal grounding algorithm for physics reasoning and comprehension. MASS explicitly grounds and rep2 resents spatialmotion information of entities in video, integrating visual grounding with structured spatial and temporal representations through spatiotemporal awareness module. This enables VLMs to capture and encode object dynamics that are otherwise inaccessible from raw prompt inputs. We post-train VLMs using GRPO to improve their comprehension and reasoning and cross-modality alignment over physics-related video phenomena. Experiments and ablations show that our refined VLMs outperform comparable and larger baselines, and prior state-of-the-art models, by 8.7% and 6.0%, achieving performance close to closedsource SoTA VLMs like Gemini-2.5-Flash and validating the effectiveness of our approach. 2. Related Work Physical Reasoning and Abnormality Detection in Videos: With the rise of AIGC video generation, abnormalities such as prompt misalignment and violations of physics or commonsense remain prevalent across models from LaVIE [47], SORA [6], and CogVideoX [56] to newer systems like VEO3 [45], Wan2.2 [46], and COSMOS [1]. Detecting and reducing these issues has become major area of research. Early work such as VideoScore [20] trained synthetic evaluators aligned with human judgment to assess abnormalities, while large benchmarks [13, 27, 32, 38, 39, 59] provide annotated data for physics and commonsense reasoning. WorldScore [16] offers unified evaluation across controllability, quality, and dynamics, and CRAVE [44] supports content-rich assessment using texttemporal fusion and motion-fidelity modeling. Recent efforts aim at mitigation via prompt engineering or external physical cues: [22] uses physics-based features, PhyT2V [52] applies chainof-thought reasoning, and VideoPhy-2 [4] provides actioncentric evaluation with corresponding fixes. TRAVL [37] enhances motion-aware physical plausibility judgment, offering unified framework for improving physical realism. Despite this progress, physical and commonsense abnormalities remain challenging, and our approach addresses this persistent gap. Spatial, Temporal and Motion Understadning in VLM: As VLMs evolve, their limitations in video-based perception have inspired more research in terms of spatial and temporal reasoning. For spatial understanding, SpatialRGPT [11] enhances 3D perception through regional representations and depth cues; VG-LLM [61] extracts 3D geometry directly from videos; and LayoutVLM [43] uses semantic priors with differentiable layout optimization to generate plausible 3D arrangements. For temporal reasoning, SlowFast-LLaVA [51] captures detailed spatial and long-range temporal cues through slowfast design, while TVS [17] improves alignment between queries and focuscritical segments. Long-video search is further advanced by T* [57], which reframes temporal retrieval as spatial search on LV-Haystack. Other works address action and motion understanding: OpenMixer [5] enables open-vocabulary action detection; Video-MME [21] exposes weaknesses in long-video comprehension; MotionSight [15] introduces zero-shot prompts for fine-grained motion perception; and GroundMoRe [14] supports motion-grounded reasoning via spatiotemporal masks. Despite these advances, most methods do not explicitly address physics-centric reasoning, which requires jointly integrating spatial, temporal, and motion understanding with physics-based commonsense. Video Understanding in VLM: State-of-the-art VLMs like Qwen-VL [3] and InternVL [63] exhibit strong general video understanding capabilities, but struggle in terms of complex spatial, temporal, and long-video reasoning [31]. LongVLM [49] enhances long video comprehension via hierarchical token merging, while VideoMind [36] uses role-based agent with Chain-of-LoRA for efficient temporal reasoning. Video-Holmes [12] benchmarks complex reasoning from suspense films, showing VLMs can still miss multi-segment clues. EgoLife [53] supports egocentric video assistants with long-context retrieval. To mitigate hallucinations, MASH-VLM [2] disentangles spatial-temporal attention, and MVoT [26] enables visual reasoning traces for spatial reasoning. Scaling the data also helps: [9] builds 2Bsample dataset for 3D spatial VQA and robotics. Training strategies further advance reasoning. Video-R1 [18] applies R1-style reinforcement learning with T-GRPO, surpassing GPT-4o on VSI-Bench, while VideoChat-R1 [30] uses reinforcement fine-tuning to boost spatio-temporal tasks like grounding and tracking. 3. MASS-Bench: Motion-Grounded Physics Reasoning and Comprehension Benchmark In this section, we introduce the details of our benchmark. High-quality training data is essential for enabling video models to understand the physical world, yet existing resources fall short when tasks extend beyond scene-level captioning to entity-level spatiotemporal reasoning and factual comprehension, such as physics understanding, commonsense reasoning, or abnormality detection in AIGC videos. This gap largely stems from the lack of datasets that (1) provide fine-grained, spatially and temporally consistent annotations of entities across frames, and (2) include balanced mixture of positive examples that follow real-world physical dynamics and negative examples that intentionally violate physical laws. In the absence of such enriched and balanced supervision, VLMs remain limited to surface-level perception; by contrast, datasets meeting these criteria enable more robust, physics-aware reasoning over dynamic video content. Data Collection: MASS-Bench is free-form video questionanswering dataset aimed at strengthening VLMs Figure 2. Data Exhibition of MASS-Bench. MASS-Bench provides two question typesfactual and critical-thinkingto evaluate physics-driven video understanding. For each videoquestionanswer pair, we supply rich motion-grounding annotations, including temporal segmentation, entity-level visual grounding, temporal profiles across the full video, and motion attributes such as first/last positions and 3D displacement vectors. These structured spatialtemporal cues transform complex physics-related perception into interpretable representations that support more reliable physical reasoning. Additional dataset details are provided in the Appendix A. physics understanding, built from real-world videos in Motion-Sight [15] and ActivityNet [7], as well as AIGC videos from VideoPhy2 [4], VideoHallu [32], and models such as COSMOS [1], Wan2.2 [46], and Sora [6]. MASSBench contains 4, 350 videos and 8, 361 free-form QA pairs covering diverse physics-related reasoning tasks. Each example includes positive and negative demonstrations with detailed human annotations, enabling models to learn correct physical processes from real videos while identifying artifacts and violations commonly found in AIGC content. Questions: As shown in Figure 2, our dataset contains two types of reasoning questions: Factual questions that assess models grounding of real versus hypothetical scenarios, asking whether physical process follows real-world principles or identifying the underlying mechanism, and, for AIGC videos, determining whether the physics aligns with reality or exhibits artifact-driven abnormalities. and Criticalthinking questions that require inferring causes, intentions, or detecting physical or commonsense abnormalities beyond explicit annotations. We re-annotate multiple-choice and labeled physics tasks into open-ended QA using Claude-4Sonnet, and all ground-truth answers are further verified by humans to ensure physical consistency and accessibility to both humans and VLMs. Categorization: To capture the spectrum of physics-related cognition required in MASS-Bench, we group all questions into five categories reflecting the underlying visual and physCategory SU (Spatial Understanding) TU (Temporal Understanding) PA (Physical Abnormality Detection) PC (Physics Comprehension) MAR (Motion & Action Recognition) Count Pct. 2,785 1,633 1,432 1,304 1,205 33.31% 19.53% 17.13% 15.60% 14.41% Table 1. Distribution of question categories in MASS-Bench. We group all questions into five categories that reflect the underlying visual and physics-driven reasoning processes, based on the type of physical phenomena probed and the level of reasoning required. ical reasoning processes: (a) Spatial Understanding (SU). Identifying objects and their geometric relationships, positions, and scene layouts. (b) Temporal Understanding (TU). Interpreting how events evolve over time, including ordering, duration, and temporal dependencies. (c) Motion & Action Recognition (MAR). Detecting and characterizing object motions and agent actions across frames. (d) Physics Comprehension (PC). Applying physical principles to infer, explain, or predict real-world dynamics. (e) Physical Abnormality Detection (PA). Identifying motions or events that violate physical laws or exhibit implausible behavior. We classify these categories from easy to challenging based on the depth of reasoning involved. VLMs must first es4 tablish spatial and temporal understanding, then recognize motion patterns, before progressing to higher-level physics comprehension and detecting real-world violations or abnormalities. The breakdown of the questions among different categories is highlighted in Table 1. Motion Grounding. For each videoquestionanswer pair, we provide comprehensive motion-grounding annotations in addition to the question, video, and ground-truth answer. These include: (1) temporal video segmentation indexed by frame ranges; (2) visual grounding for each queried entity specified by the entity name and its bounding box; and (3) an entity-level temporal profile that tracks each grounded entity across the entire videopersisting once detected and left blank in segments where the entity is absent. Within each video segment, we further provide (4) motion-grounding attributes, including the entitys first and last observed positions to capture coarse spatial layout, and (5) 3D motion vectors representing the entitys temporal displacement. By encoding these spatialtemporal cues, the dataset converts physics-intensive perceptual challenges into structured textual and mathematical representations, enabling more reliable physical reasoning in multimodal models. Data Metadata. Our dataset has 6093 examples for training and 2268 examples for testing. Our dataset covers videos with average 545.8 frames per video, corresponding to approximately 19.62 seconds at an average framerate of 27.37 FPS. Frame resolution averages 1120 702 pixels across all videos. 4. MASS: Model-Agnostic Approach High-quality human annotations are essential for strengthening VLMs understanding of physics-related visual contexts, yet data alone cannot address their core limitations in modeling spatial layouts and motion dynamics. Effective physical reasoning requires isolating relevant cues from noisy video content. While one solution is to retrain models with heavy preprocessing pipelines, this incurs substantial cost. Instead, we propose more data-efficient alternative: augmenting VLMs with dedicated spatial and motion encoders that explicitly extract and represent key visual signals. As shown in Figure 3, our model-agnostic designmotivated by dataset insights and observed VLM weaknesses, combines lightweight architectural refinement with targeted training to enhance spatiotemporal perception, cross-modal alignment, and physics-aware reasoning, ultimately reducing hallucinations and improving physical comprehension. 4.1. Entity-Centric Visual Grounding Understanding physics-related dynamics in videos typically requires identifying and tracking one or more specific entities throughout the sequence. However, videos contain rich, highdimensional spatialtemporal context, making it challenging to isolate and follow these entities across frames. Besides, language queries are often ambiguous about which entities they refer to. Grounding is therefore essential for isolating the correct objects and preserving their spatialtemporal dynamics, which would otherwise be lost or distorted by high-level VLM encoders. Establishing video-scale grounding profiles for the queried entities helps align modalities, filter out irrelevant or distracting information, and mitigate the information loss introduced by LLaVA-style VLMs [34], which compress visual signals into limited textual embeddings. We begin by semantically extracting the key components from each probing questionthose most relevant to physics, motion, or other spatialtemporal dynamicsto narrow the target for visual grounding. Using Grounding-DINO [35], we detect bounding boxes for the queried entities and apply dynamic temporal-resolution scheme that automatically selects an appropriate sub-sequence length based on the input videos duration. This segmentation strategy increases temporal coherence within each chunk, improving object tracking and downstream processing while balancing accuracy and efficiency. It also preserves grounding consistency over time and reveals temporal artifacts such as sudden appearance or disappearance, which are common in AIGC videos. For each detected entity, we construct time-aligned grounding profile. Finally, we apply SAM2 [40, 41] to generate segmentation masks for the grounded regions, enabling subsequent spatialtemporal analysis. 4.2. Spatial Motion Feature Extraction Physical reasoning depends on accurate knowledge of where entities are, how they move, and how their interactions evolve over time. After identifying the key entities referenced by the probing questions, the main challenge lies in the VLMs limited ability to comprehend their spatial positions, appearances, and dynamics across frames. Without explicit spatialtemporal understanding, VLMs struggle to track entities, recognize actions, and maintain frame-level consistency, ultimately hindering higher-level physical reasoning. To address these limitations, we extract explicit spatialtemporal representations for each queried entity by integrating domain-specialized visual encoders capable of translating raw spatialmotion cues into structured features accessible to the VLM. Leveraging the narrowed-down, visually grounded entities identified by the probing questions, we perform motion tracking for each entity within its video sub-sequences using CoTracker3 [23], which tracks arbitrary point trajectories inside the grounded regions. For each entity, we compute the spatial deviations across time, including averaged motion vectors and the first/last tracked positions. In parallel, we estimate per-frame depth using Depth Anything V2 [55] to provide spatial awareness. By aligning sampled RGB frames with their corresponding depth maps, we project the Figure 3. Overview of MASS: We use model-agnostic approach to enhance visual recognition with explicit spatial and motion awareness. Beyond standard visual transformer encoders that process video inputs (e.g., LLaVA-OneVision [24], Qwen2.5-VL [3]), we introduce visual grounding module to strengthen correlations between queried entities and corresponding visual cues. Depth estimation captures spatial geometry, while motion tracking encodes temporal dynamics across frames. These spatial and temporal signals are fused into motion traces for each entity and tokenized with grounding and temporal features to align them with the language domain. During post-training, we freeze the spatialtemporal encoders and apply reinforcement fine-tuning (RFT) to improve the LLM backbones comprehension of the additional multimodal information. tracked motion vectors over each tracking points throughout the entity into 3D space, yielding both 3D positions and full spatio-temporal motion trajectories for each entity. effectively. We provide the visual feature representation template incorporated into the questionanswering format used for VLM training in Appendix C. 4.3. Visual Feature Representation 4.4. Training Pipeline After encoding visual features across different modules, the key challenge lies in integrating heterogeneous spatialmotion outputs into the VLM pipeline without disrupting its native architecture. Prior work [32, 50] shows that LLM backbones better comprehend abstract, factual, and physics-related concepts when such information is expressed in the language space rather than as raw visual features. Motivated by this, we fuse visual cues through naturallanguage representations aligned with the tokenized questions. We compute these features because VLMs cannot reliably infer fine-grained spatial layouts or motion dynamics from compressed visual embeddings; providing explicit, structured cues supplies the physical information needed for stable and interpretable reasoning. Our visual-feature serialization converts spatialmotion signals into structured motion-trajectory sequences for each entity, including 3D start/end positions, motion vectors, bounding boxes within each video sub-segment, and associated temporal indices. These attributes are expressed through predefined naturallanguage templates, enabling the VLM to access precise spatialtemporal information in the modality it handles most Although visual grounding and spatialmotion features provide essential cues, VLMs cannot directly leverage these signals for higher-level reasoning because their representations are misaligned with those learned during pre-training. As result, even precise motion trajectories, spatial positions, and temporal indicessuch as knowing an object is intact at position at time 1 and broken at position at time 2cannot be meaningfully synthesized without additional alignment. The model must bridge this representational gap to integrate structured cues, filter irrelevant details, and reason over scene evolution rather than merely retrieving or describing visual content. To address this gap, we perform post-training to enhance the VLMs reasoning capabilities. Chain-of-thought (CoT) reasoning [48] helps the model process complex temporal and physical concepts more effectively. By encouraging intermediate reasoning steps, CoT aligns the VLMs decision process with the multi-stage deductions needed for physical comprehension. This training enables the VLM to operate on grounded spatialmotion features, integrate temporal changes, and produce coherent, physics-aware explanations 6 rather than treating cues as isolated descriptors. We evaluate both Supervised Fine-tuning (SFT) and Reinforcement Fine-tuning (RFT). For RFT, we adopt Temporal Group Relative Policy Optimization (T-GRPO) [18], which adds temporal-aware rewards on top of vanilla GRPO [42], key component for video understanding. We further incorporate ROUGE [33] for semantic coherence, temporal reward to strengthen temporal reasoning, and format reward to encourage proper CoT generation. Implementation details of our method are provided in Appendix B. 5. Empirical Results For our experiments, we evaluate 14 widely used state-of-theart VLMs as baselines (e.g., GPT-4o and Gemini-2.5-Flash), covering 7B and 13B parameter scales. We report performance comparisons between these baselines and VLMs augmented with MASS in Table 2, and present ablations on our backbone models and post-training strategies in Table 3. For evaluation, we use GPT-4.1-Mini as an automated judge to assess the factual correctness between each models answer and the ground-truth annotations regarding these physical comprehension and reasoning questions provided. We provide the evaluation template in Appendix D. We provide additional qualitative analyses of questionanswering results and extended evaluations in the Appendix. Appendix reports results on general-purpose real-world video QA benchmarks, while Appendix includes qualitative video question answering examples. 5.1. Baseline Insights: Why Motion-Aware Spatialtemporal Grounding Matters SpatialTemporal Signals Are Critical for Physics Understanding. We evaluate baseline VLMs in the 7B8B range, including LLaVA [34], Qwen-VL [3], and InternVL [63], with additional comparisons to larger InternVL3.5 models. Surprisingly, newer or larger variants (e.g., Qwen2.5-VL7B vs. Qwen3-VL-8B, or InternVL3-8B vs. InternVL3.58B-Flash) do not consistently yield better performance under identical settings. Scaling further offers limited benefit: within the InternVL3.5 family, increasing from 8B to 38B yields only modest 4.0% gain, and Qwen3-VL-32B performs similarly to its 8B counterpart. These results indicate that model scale alone does not reliably strengthen physics reasoning. In contrast, integrating MASS produces substantial improvements. Qwen2.5-VL-7B and LLaVAOneVision-7B outperform the previous best model (Qwen3VL-8B) by 8.7% and 6.0%, respectively. Notably, Qwen2.5VL-7B+MASS matches Gemini-2.5-Flash and surpasses it on physics abnormality detectionthe most reasoningintensive category. These findings demonstrate that explicit spatialtemporal representations provide the structured physical cues VLMs inherently lack, enabling more faithful physics-driven reasoning. Model SU TU MAR PC PA Overall VideoLLaVA InternVL3.5-8B-Flash InternVL3-8B InternVL3.5-14B-Flash LLaVA-OneVision-7B Qwen2.5-VL-7B Qwen3-VL-8B Baselines 36.39 47.22 48.20 47.68 49.18 55.87 57.29 31.76 45.25 49.89 49.23 51.99 53.03 50.77 21.01 39.40 39.29 41.96 38.12 54.89 51.92 38.55 58.65 57.20 57.52 56.02 52.36 58.65 17.76 32.76 32.47 31.03 30.46 30.21 37. 30.72 44.68 45.59 45.71 45.79 52.30 53.23 R1-finetuned Reasoning VLMs VideoChat-R1 Video-R1 51.20 57.49 48.66 55.19 47.34 47. 52.30 63.53 31.09 33.91 47.05 53.08 Larger / Close-Source InternVL3.5-38B-Flash GPT-4o Qwen3-VL-32B Gemini-2.5-Flash 52.19 54.47 58.87 65. 49.88 55.13 50.59 59.65 43.29 55.62 48.05 68.57 61.45 60.15 59.84 65.15 32.71 31.32 38.63 43.80 48.71 52.81 53.44 63.18 LLaVA-OneVision-7B + MASS Qwen2.5-VL-7B + MASS 61.26 63.52 65.41 63.06 58.15 64.07 61.45 65.06 42.37 45.79 59.24 61. VLMs with MASS Table 2. Accuracy (%) across categories in MASS-Bench: Performance of VLMs across five physics-related categoriesSpatial Understanding (SU), Temporal Understanding (TU), Motion & Action Recognition (MAR), Physics Comprehension (PC), and Physical Abnormality Detection (PA). Models are grouped by family and sorted by overall accuracy. Integrating MASS with Qwen2.5-VL7B and LLaVA-OneVision-7B substantially improves performance over their baselines and achieves accuracy comparable to Gemini2.5-Flash, the current SoTA in video understanding, by leveraging explicit spatialtemporal representations that enhance physics comprehension and reasoning. Training Data Quality is Crucial for Reasoning. We also include two R1-finetuned VLMs, Video-R1 and VideoChatR1, in Table 2, both trained with GRPO to enhance reasoning. However, neither model shows clear improvement on physics comprehension tasks compared with nonGRPO fine-tuned VLMs. In contrast, integrating MASS with Qwen2.5-VL-7B and LLaVA-OneVision-7B yields substantially higher performance, highlighting the importance of high-quality training data. Detailed annotations, along with both positive (successful) and negative (failure) demonstrations, are essential for strengthening reasoning and counteracting language priors when tackling out-of-domain physics tasks. Without such data, RFT alone offers limited gains and may even reinforce hallucinations arising from misinterpreting visual evidence. 5.2. Ablation Study: The Role of Reasoning Reasoning Dominates Beyond the Base Model. Table 3 compares post-training results on two backbone VLMs, Qwen2.5-VL-7B [3] and LLaVA-OneVision-7B [24]. Across both models, enhanced reasoning contributes far more than the backbone choice itself: pairing Qwen2.5VL-7B with MASS yields 9.6% improvement, while LLaVA-OneVision-7B combined with MASS achieves larger 13.5% gain. After post-training, the base models influence becomes secondary, as physics comprehension, especially challenging tasks like physics abnormality detection, demands integrated reasoning over visual physics cues, Model SU TU MAR PC PA Overall Qwen2.5-VL-7B + SFT + GRPO LLaVA-OneVision-7B + SFT + GRPO 55.87 45.08 63. 49.18 46.45 61.26 53.03 49.88 63.06 51.99 51.29 65.41 54.89 38.15 64.07 38.12 29.87 58.15 52.36 55.82 65. 56.02 58.23 61.45 30.21 32.09 45.79 30.46 31.78 42.37 52.30 43.74 61.93 45.79 42.94 59.24 Table 3. Ablation of Post-Training Strategies. Performance comparison of base VLMs, SFT variants, and GRPO-enhanced models across five physics-related categories. GRPO consistently provides substantial gains over both the base models and their SFT counterparts, whereas SFT generally degrades performance. spatialtemporal structure, and commonsense priors. This synergy, rather than raw model capacity, ultimately drives the performance improvements. Why RFT Matters: SFT Alone Is Not Enough. Table 3 compares SFT and RFT strategies. While RFT (via GRPO) consistently boosts performance, SFT leads to notable degradation: 8.6% on Qwen2.5-VL-7B and 2.9% on LLaVAOneVision-7B. This highlights that simple supervised alignment cannot equip VLMs to handle complex physics reasoning. Physics comprehension, especially detecting abnormal or non-intuitive dynamics, requires models to integrate grounded visual cues with spatialtemporal structure and commonsense priors. RFT explicitly strengthens this reasoning process, whereas SFT merely memorizes correlations between inputoutput pairs, limiting its effectiveness on physics-intensive tasks. 5.3. Discussion The Impact of Motion-aware Spatiotemporal Grounding. In Table 2 and Table 3, we observe that across model architectures, scales, and post-training strategies, VLMs equipped with our motion-aware spatiotemporal grounding outperform their baselines and reach performance comparable to Gemini2.5-Flash. These gains stem from explicitly integrating spatiotemporal signals, as standard VLM encoders struggle to utilize or correlate spatial and temporal cues effectively. Providing structured representations of key video dynamics enables stronger physics comprehension and reasoning while filtering irrelevant or distracting visual information. for Understanding Physics. Reasoning is Essential Motion-aware spatiotemporal grounding is necessary but not sufficient for physics comprehension. Reasoning is the key mechanism that integrates and interprets the grounded information. As shown in Table 3, comparing SFT and GRPO reveals that even with motion-aware spatiotemporal signals, learning purely from inputoutput correlations (as in SFT) does not helpand often degradesperformance on physics reasoning tasks. Given the complexity and interdependence of physical processes, VLMs cannot grasp the underlying dynamics through grounding alone and may lose focus during question answering. In contrast, reasoning via chain-of-thought encourages VLMs to reflect on grounded cues and combine them with commonsense knowledge, effectively bridging the gap between visual perception and real-world physical understanding. Challenges in Physical Abnormality Detection Remain. key observation from Table 2 and Table 3 is that Qwen2.5VL-7B and LLaVA-OneVision-7B integrated with MASS outperform all other VLMs, including Gemini-2.5-Flash, on physical abnormality detection. However, compared with more grounded physics comprehension tasks, abnormality detection is inherently more difficult. It requires VLMs to overcome hallucinations rooted in their learned priors about real-world physics [32] and to reason about why visual process violates physical laws. While our method yields notable 12% improvement on this task, indicating the promise of motion-aware spatiotemporal grounding for enhancing perception and reasoning, further advances are needed to robustly handle these challenging cases. 6. Conclusion, Limitations and Future Work We present model-agnostic framework that bridges raw video content and physics-aware reasoning by converting physical cues into structured, interpretable inputs suitable for VLM perception and inference. To support this, we introduce comprehensive benchmark of physics-oriented video QA pairs with fine-grained annotations, including visual detections, sub-segment grounding, and full-sequence 3D motion tracking. To our knowledge, MASS-Bench is the first dataset providing both positive and negative demonstrations paired with questions on spatialtemporal understanding, physics reasoning, and abnormality detection. Our method, MASS, injects spatialtemporal signals into the VLM language space through depth-based 3D encoding, visual grounding, and motion-aware trajectory modeling, yielding consistent gains over strong baselines and recent SoTA VLMs on real-world and AIGC videos. Our approach has limitations. Although post-training improves physics reasoning, models still struggle with comprehensive realworld dynamics and commonsense cues. The current implementation also faces challenges in crowded scenes, where multi-object tracking increases computational overhead. In addition, subtle cues in reasoning-heavy cases may be overshadowed by language priors, leading to hallucinations. Future work includes enhancing long-range motion grounding and temporal reasoning to better capture extended physical dynamics, improving multi-object tracking via more scalable spatialtemporal profiling, and expanding high-quality training data with diverse positive/negative examples. These efforts aim to further reduce hallucinations, improve physical fidelity, and broaden applicability to complex physicscentered video reasoning."
        },
        {
            "title": "References",
            "content": "[1] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025. 3, 4 [2] Kyungho Bae, Jinhyung Kim, Sihaeng Lee, Soonyoung Lee, Gunhee Lee, and Jinwoo Choi. Mash-vlm: Mitigating actionscene hallucination in video-llms through disentangled spatialtemporal representations. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 13744 13753, 2025. 3 [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 3, 6, 7 [4] Hritik Bansal, Clark Peng, Yonatan Bitton, Roman Goldenberg, Aditya Grover, and Kai-Wei Chang. Videophy-2: challenging action-centric physical commonsense evaluation in video generation. arXiv preprint arXiv:2503.06800, 2025. 2, 3, 4, 1 [5] Wentao Bao, Kai Li, Yuxiao Chen, Deep Patel, Martin Renqiang Min, and Yu Kong. Exploiting vlm localizability and In 2025 semantics for open vocabulary action detection. IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 82918301. IEEE, 2025. 3 [6] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, et al. Video generation models as world simulators. OpenAI Blog, 1(8):1, 2024. 3, [7] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: large-scale video benchmark for human activity understanding. In Proceedings of the ieee conference on computer vision and pattern recognition, pages 961970, 2015. 4 [8] Zhongang Cai, Yubo Wang, Qingping Sun, Ruisi Wang, Chenyang Gu, Wanqi Yin, Zhiqian Lin, Zhitao Yang, Chen Wei, Oscar Qian, Hui En Pang, Xuanke Shi, Kewang Deng, Xiaoyang Han, Zukai Chen, Jiaqi Li, Xiangyu Fan, Hanming Deng, Lewei Lu, Bo Li, Ziwei Liu, Quan Wang, Dahua Lin, and Lei Yang. Holistic evaluation of multimodal llms on spatial intelligence, 2025. 1 [9] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1445514465, 2024. 3 [10] Ling-Hao Chen, Shunlin Lu, Ailing Zeng, Hao Zhang, Benyou Wang, Ruimao Zhang, and Lei Zhang. Motionllm: Understanding human behaviors from human motions and videos. arXiv preprint arXiv:2405.20340, 2024. 1 [11] An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision-language models. Advances in Neural Information Processing Systems, 37: 135062135093, 2024. 3 [12] Junhao Cheng, Yuying Ge, Teng Wang, Yixiao Ge, Jing Liao, and Ying Shan. Video-holmes: Can mllm think like holmes for complex video reasoning? arXiv preprint arXiv:2505.21374, 2025. 3 [13] Wei Chow, Jiageng Mao, Boyi Li, Daniel Seita, Vitor Guizilini, and Yue Wang. Physbench: Benchmarking and enhancing vision-language models for physical world understanding. arXiv preprint arXiv:2501.16411, 2025. 1, 3 [14] Andong Deng, Tongjia Chen, Shoubin Yu, Taojiannan Yang, Lincoln Spencer, Yapeng Tian, Ajmal Saeed Mian, Mohit Bansal, and Chen Chen. Motion-grounded video reasoning: Understanding and perceiving motion at pixel level. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 86258636, 2025. 2, [15] Yipeng Du, Tiehan Fan, Kepan Nan, Rui Xie, Penghao Zhou, Xiang Li, Jian Yang, Zhenheng Yang, and Ying Tai. Motionsight: Boosting fine-grained motion understanding in multimodal llms. arXiv preprint arXiv:2506.01674, 2025. 2, 3, 4, 1 [16] Haoyi Duan, Hong-Xing Yu, Sirui Chen, Li Fei-Fei, and Jiajun Wu. Worldscore: unified evaluation benchmark for world generation. arXiv preprint arXiv:2504.00983, 2025. 3 [17] Zheyu Fan, Jiateng Liu, Yuji Zhang, Zihan Wang, Yi Fung, Manling Li, and Heng Ji. Video-llms with temporal visual screening. arXiv preprint arXiv:2508.21094, 2025. 3 [18] Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Junfei Wu, Xiaoying Zhang, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025. 3, 7 [19] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2410824118, 2025. 1 [20] Xuan He, Dongfu Jiang, Ge Zhang, Max Ku, Achint Soni, Sherman Siu, Haonan Chen, Abhranil Chandra, Ziyan Jiang, Aaran Arulraj, et al. Videoscore: Building automatic metrics to simulate fine-grained human feedback for video generation. arXiv preprint arXiv:2406.15252, 2024. 3 [21] Wenyi Hong, Yean Cheng, Zhuoyi Yang, Weihan Wang, Lefan Wang, Xiaotao Gu, Shiyu Huang, Yuxiao Dong, and Jie Tang. Motionbench: Benchmarking and improving fine-grained video motion understanding for vision language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 84508460, 2025. 2, 3 [22] Bingyi Kang, Yang Yue, Rui Lu, Zhijie Lin, Yang Zhao, Kaixin Wang, Gao Huang, and Jiashi Feng. How far is video generation from world model: physical law perspective. arXiv preprint arXiv:2411.02385, 2024. [23] Nikita Karaev, Iurii Makarov, Jianyuan Wang, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker3: Simpler and better point tracking by pseudolabelling real videos. arXiv preprint arXiv:2410.11831, 2024. 5 9 [24] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 6, 7 [25] Boyi Li, Ligeng Zhu, Ran Tian, Shuhan Tan, Yuxiao Chen, Yao Lu, Yin Cui, Sushant Veer, Max Ehrlich, Jonah Philion, et al. Wolf: Dense video captioning with world summarization framework. arXiv preprint arXiv:2407.18908, 2024. 1 [26] Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vulic, and Furu Wei. Imagine while reasoning in space: Multimodal visualization-ofthought. arXiv preprint arXiv:2501.07542, 2025. 3 [27] Dacheng Li, Yunhao Fang, Yukang Chen, Shuo Yang, Shiyi Cao, Justin Wong, Michael Luo, Xiaolong Wang, Hongxu Yin, Joseph Gonzalez, et al. Worldmodelbench: Judging video generation models as world models. arXiv preprint arXiv:2502.20694, 2025. 2, [28] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22195 22206, 2024. 2, 3 [29] Pengteng Li, Yunfan Lu, Pinghao Song, Wuyang Li, Huizai Yao, and Hui Xiong. Eventvl: Understand event streams arXiv preprint via multimodal large language model. arXiv:2501.13707, 2025. 1 [30] Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, and Limin Wang. Videochat-r1: Enhancing spatio-temporal perception via reinforcement fine-tuning. arXiv preprint arXiv:2504.06958, 2025. 3 [31] Zongxia Li, Xiyang Wu, Hongyang Du, Fuxiao Liu, Huy Nghiem, and Guangyao Shi. survey of state of the art large vision language models: Alignment, benchmark, evaluations and challenges. arXiv preprint arXiv:2501.02189, 2025. 3 [32] Zongxia Li, Xiyang Wu, Guangyao Shi, Yubin Qin, Hongyang Du, Tianyi Zhou, Dinesh Manocha, and Jordan Lee Boyd-Graber. Videohallu: Evaluating and mitigating multimodal hallucinations on synthetic video understanding. arXiv preprint arXiv:2505.01481, 2025. 2, 3, 4, 6, 8, 1 [33] Chin-Yew Lin. Rouge: package for automatic evaluation of summaries. In Text summarization branches out, pages 7481, 2004. [34] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. 5, 7 [35] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European conference on computer vision, pages 3855. Springer, 2024. 5 [37] Saman Motamed, Minghao Chen, Luc Van Gool, and Iro Laina. Travl: recipe for making video-language models better judges of physics implausibility. arXiv preprint arXiv:2510.07550, 2025. 3 [38] Saman Motamed, Laura Culp, Kevin Swersky, Priyank Jaini, and Robert Geirhos. Do generative video models understand physical principles? arXiv preprint arXiv:2501.09038, 2025. 3 [39] Yiran Qin, Zhelun Shi, Jiwen Yu, Xijun Wang, Enshen Zhou, Lijun Li, Zhenfei Yin, Xihui Liu, Lu Sheng, Jing Shao, et al. Worldsimbench: Towards video generation models as world simulators. arXiv preprint arXiv:2410.18072, 2024. 3 [40] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman RÃ¤dle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. 5 [41] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang, Hongyang Li, Qing Jiang, and Lei Zhang. Grounded sam: Assembling open-world models for diverse visual tasks, 2024. [42] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv. org/abs/2402.03300, 2(3):5, 2024. 7 [43] Fan-Yun Sun, Weiyu Liu, Siyi Gu, Dylan Lim, Goutam Bhat, Federico Tombari, Manling Li, Nick Haber, and Jiajun Wu. Layoutvlm: Differentiable optimization of 3d layout via vision-language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 29469 29478, 2025. 3 [44] Shangkun Sun, Xiaoyu Liang, Bowen Qu, and Wei Gao. Content-rich aigc video quality assessment via intricate text alignment and motion-aware consistency. arXiv preprint arXiv:2502.04076, 2025. 3 [45] Veo-Team. Veo 3. DeepMind Blog, 2025. 3 [46] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 3, 4 [36] Ye Liu, Kevin Qinghong Lin, Chang Wen Chen, and Mike Zheng Shou. Videomind: chain-of-lora agent for long video reasoning. arXiv preprint arXiv:2503.13444, 2025. 3 [47] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation 10 with cascaded latent diffusion models. International Journal of Computer Vision, 133(5):30593078, 2025. 3 Computational Linguistics (Volume 1: Long Papers), pages 64356462, 2025. 1 [59] Chenyu Zhang, Daniil Cherniavskii, Andrii Zadaianchuk, Antonios Tragoudaras, Antonios Vozikis, Thijmen Nijdam, Derck WE Prinzhorn, Mark Bodracska, Nicu Sebe, and Efstratios Gavves. Morpheus: Benchmarking physical reasoning of video generative models with real physical experiments. arXiv preprint arXiv:2504.02918, 2025. 3 [60] Yilun Zhao, Haowei Zhang, Lujing Xie, Tongyan Hu, Guo Gan, Yitao Long, Zhiyuan Hu, Weiyuan Chen, Chuhan Li, Zhijian Xu, et al. Mmvu: Measuring expert-level multidiscipline video understanding. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 8475 8489, 2025. 2, 3 [61] Duo Zheng, Shijia Huang, Yanyang Li, and Liwei Wang. Learning from videos for 3d world: Enhancing mllms with 3d vision geometry priors. arXiv preprint arXiv:2505.24625, 2025. [62] Shijie Zhou, Alexander Vilesov, Xuehai He, Ziyu Wan, Shuwang Zhang, Aditya Nagachandra, Di Chang, Dongdong Chen, Xin Eric Wang, and Achuta Kadambi. Vlm4d: Towards spatiotemporal awareness in vision language models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 86008612, 2025. 1 [63] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. 3, 7 [48] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-ofthought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824 24837, 2022. 6 [49] Yuetian Weng, Mingfei Han, Haoyu He, Xiaojun Chang, and Bohan Zhuang. Longvlm: Efficient long video understanding via large language models. In European Conference on Computer Vision, pages 453470. Springer, 2024. 3 [50] Xiyang Wu, Tianrui Guan, Dianqi Li, Shuaiyi Huang, Xiaoyu Liu, Xijun Wang, Ruiqi Xian, Abhinav Shrivastava, Furong Huang, Jordan Lee Boyd-Graber, et al. Autohallusion: Automatic generation of hallucination benchmarks for vision-language models. arXiv preprint arXiv:2406.10900, 2024. 6 [51] Mingze Xu, Mingfei Gao, Zhe Gan, Hong-You Chen, Zhengfeng Lai, Haiming Gang, Kai Kang, and Afshin Slowfast-llava: strong training-free baseDehghan. arXiv preprint line for video large language models. arXiv:2407.15841, 2024. [52] Qiyao Xue, Xiangyu Yin, Boyuan Yang, and Wei Gao. Phyt2v: Llm-guided iterative self-refinement for physicsIn Proceedings of the grounded text-to-video generation. Computer Vision and Pattern Recognition Conference, pages 1882618836, 2025. 3 [53] Jingkang Yang, Shuai Liu, Hongming Guo, Yuhao Dong, Xiamengwei Zhang, Sicheng Zhang, Pengyun Wang, Zitang Zhou, Binzhu Xie, Ziyue Wang, et al. Egolife: Towards In Proceedings of the Computer egocentric life assistant. Vision and Pattern Recognition Conference, pages 28885 28900, 2025. 3 [54] Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1063210643, 2025. 1 [55] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. arXiv:2406.09414, 2024. 5 [56] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 3 [57] Jinhui Ye, Zihan Wang, Haosen Sun, Keshigeyan Chandrasegaran, Zane Durante, Cristobal Eyzaguirre, Yonatan Bisk, Juan Carlos Niebles, Ehsan Adeli, Li Fei-Fei, et al. Rethinking temporal search for long-form video understanding. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 85798591, 2025. 2, [58] Linhao Yu, Xingguang Ji, Yahui Liu, Fanheng Kong, Chenxi Sun, Jingyuan Zhang, Hongzhi Zhang, Fuzheng Zhang, Deyi Xiong, et al. Evaluating multimodal large language models In Proon video captioning via monte carlo tree search. ceedings of the 63rd Annual Meeting of the Association for 11 MASS: Motion-Aware Spatialtemporal Grounding for Physics Reasoning and Comprehension in Vision-Language Models"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Additional Details of MASS-Bench This section supplements the dataset description in Section 3 by providing expanded definitions and illustrative examples for each video question-answering pairs. A.1. Dataset Composition MASS-Bench aggregates diverse physics-centric video sources to ensure broad coverage across spatial, temporal, and motion-driven phenomena. In total, the dataset comprises 1,229 samples (14.63%) from MotionSight [15], 4,009 samples (47.95%) from VideoPhy2 [4], and 2,862 samples (34.23%) from VideoHallu [32]. A.2. Example Type Composition To characterize the types of physics reasoning required in MASS-Bench, we follow the definitions introduced in Section 3 and categorize each video question-answer pair into one of two groups. Positive Examples capture scenarios in which the physical dynamics depicted in the video are consistent with real-world physics. These include both real and synthetically generated videos whose motion patterns follow physically plausible trajectories. Questions in this category primarily evaluate models ability to comprehend and reason about correct physical processes. MASS-Bench contains total of 3,436 such examples (41.10%). Negative Examples correspond to cases where at least one aspect of the video exhibits physical abnormality, such as trajectories or interactions that violate real-world physics. Successfully answering these questions requires VLMs to overcome language priors and reason directly from observed visual evidence. This category includes 4,925 examples (58.90%). Overall, the two groups jointly evaluate physics comprehension under both physically valid and physically inconsistent conditions. A.3. Question Type Composition To assess the range of reasoning skills required by VLMs, we categorize all videoquestionanswer pairs in MASS-Bench into two types. Representative examples of each are shown in Figure 2. Factual Questions focus on concrete, visually grounded information related to spatial layout, temporal ordering, object motion, or physical abnormalities. Examples include queries such as What is the spatial position of the watermelon in the video? or How does the watermelon move after it is shot? These questions typically contain explicit cues that narrow the perceptual scope, enabling the model to identify the relevant entities and perform targeted grounding and reasoning. This category consists of 5,427 examples (67.0%). Critical Thinking Questions require higher-level inference and more abstract reasoning. They often omit explicit grounding cues and instead ask broadly scoped questions such as What is the watermelon doing in this video? or What is abnormal in the scene? Successfully answering them requires the model to infer intent, identify salient events, and interpret physical dynamics without direct guidance from the query. This category contains 2,673 examples (33.0%). Together, these two question types enable comprehensive evaluation of both grounded perception and higher-order physics reasoning in VLMs. B. Implementation Details All experiments, including ablations, are conducted using full-parameter fine-tuning on 8 NVIDIA H100 (80GB) GPUs. Both Qwen2.5-VL-7B and LLaVA-OneVision-7B are trained under identical settings for fair comparison. The GRPO post-training phase for each model requires approximately 912 hours, while the supervised fine-tuning (SFT) stage in our ablation studies completes within 23 hours. C. Visual Feature Representation Template Figure 4 illustrates the prompt template used during both post-training and inference for VLMs in MASS-Bench. This template is designed to elicit structured reasoning for freeform video question answering. The motion-grounding information incorporated into the prompt is derived from the spatialmotion feature extraction module and the visual representation pipeline described in Section 4. For each detected entity, denoted as <Entity Name>, we generate motion-grounding descriptors for every video segment in which the entity appears. These descriptors include the entitys first and last 3D positions within the segment (<first_position> and <last_position>), the corresponding 3D motion vector (<motion>), the bounding box in the segments first frame (<bbox>), and the segments temporal extent indicated by the starting and ending frame indices (<first_frame> and <last_frame>). 1 Prompt Template for Video-Language Models Task Description: The model receives system instruction enforcing explicit reasoning and final answer formatting. Given video, question, and motion-grounding metadata, the model must produce detailed reasoning inside <think> tags and concise final answer inside <answer> tags. Core Requirements: Use natural internal dialogue in the <think> section (e.g., let me think, hmm, wait). Perform step-by-step reasoning validating spatialtemporal cues. Place the final answer only inside <answer> tags. Use free-form answer format: Provide short text answer within the answer tags. Motion-Grounding Information: We use the following template to represent the motion-grounding information generated for VLMs. This template is used to fill the {motion_grouding_info} in the QA prompt template below: Entity #1: * Segment #1 First Position <first_position>, Motion Vector <motion>, Last Position <last_position>, Bounding Box <bbox>, Frame <first_frame>...<last_frame> * Segment #2 First Position <first_position>, Motion Vector <motion>, Last Position <last_position>, Bounding Box <bbox>, Frame <first_frame>...<last_frame> <Entity Name> ... Entity #2: <Entity Name> ... Actual Prompt Used for Video QA: Conversation Setup: conversation between User and Assistant. Assistant solves it. The Assistant first thinks about the reasoning process inside <think> </think> tags, then provides the final answer inside <answer> </answer> tags. The user asks question, and the Question: <Question> {question} </Question> Motion-Grounding Information: {motion_grouding_info} Reasoning Instruction: Please think about this question as if you were human pondering deeply. internal dialogue such as let me think, wait, hmm, see, and include verification or self-reflection in the reasoning process. reasoning in <think> </think>, then provide the final answer in <answer> </answer>. Provide detailed Use Free-form Answer Instruction: Please provide your text answer within the <answer> </answer> tags. Figure 4. Prompt template used for motion-aware video question answering. The template first serializes entity-level motion grounding (positions, motion vectors, bounding boxes, and frame ranges) into text, then injects this context into chain-of-thought style prompt that guides the VLM to reason in <think> tags and output its final prediction in standardized <answer> tags. D. Evaluation Template E. Experiments on Real-world Video QA We provide the LLM-as-a-judge evaluation template used in our experiments in Figure 5. This template standardizes how we assess the correctness of model predictions by comparing each VLMs answer against the ground-truth annotation and the corresponding question. As supplement to the experiments in the main paper, we provide additional results on two real-world video questionanswering benchmarks, MMVU [60] and MVBench [28], to further assess the generalization ability of VLMs augmented with MASS. MMVU evaluates models on expert-level tem-"
        },
        {
            "title": "Prompt Template for Evaluation",
            "content": "Task Description: You are an intelligent teacher whose task is to evaluate the correctness of models answer to question, given reference ground-truth answer. Inputs: Question: wrapped in <Question> ... Ground-truth answer: wrapped in <GT> ... Model prediction: wrapped in <Answer> ... Evaluation Criteria: If the prediction does not conflict with the ground truth, output <Eval> Correct </Eval>. If the prediction conflicts with the ground truth, output <Eval> Incorrect </Eval>. If the correctness of the prediction is unclear, output <Eval> Unclear </Eval>. Reason carefully about the relationship between the prediction and the ground truth, but keep the final evaluation very </Question> </GT> </Answer> brief. Output Format: Produce only one of the following tokens as the final output: <Eval> Correct </Eval> <Eval> Incorrect </Eval> <Eval> Unclear </Eval> Figure 5. Prompt template used for automatic evaluation of model answers against ground-truth references. The template presents the question, ground truth, and model output provided for LLM-as-a-judge evaluation and instructs the evaluator to output one of three outcomesCorrect, Incorrect, or Unclearensuring reliable and consistent scoring across predictions. Model MMVU (%) MVBench (%) InternVL3.5-8B-Flash Qwen3-VL-8B LLaVA-OneVision GPT-4o Gemini-2.5-Flash Baselines 55.04 64.00 50.41 Larger / Close-Source 75.96 75.27 VLMs with MASS LLaVA-OneVision-7B + MASS Qwen2.5-VL-7B + MASS 48.95 65.76 45.78 51.30 55.24 58.50 54.32 48.33 61.59 Table 4. Overall accuracies (%) on MMVU and MVBench. We report overall performance on two comprehensive, real-world video question-answering benchmarks, MMVU [60] and MVBench [28]. GPT-4o achieves the highest accuracy on both datasets. Notably, open-source VLMs augmented with MASS exhibit strong generalization, narrowing the gap to close-source models and transferring effectively from physics-oriented spatialtemporal grounding to broader video QA tasks. dynamics, providing clear reference for evaluating model behavior and identifying failure modes. poral, procedural, and interaction-centric reasoning, while MVBench is widely used benchmark for temporal video understanding and action-centric tasks. Both datasets contain diverse video-based QA pairs captured from real-world scenarios. Table 4 reports the performance of VLMs integrated with MASS alongside state-of-the-art baselines. On MMVU, large-scale close-source models achieve substantially higher accuracy due to the benchmarks emphasis on expert-level, interdisciplinary reasoninghighlighting the limitations of smaller 7B models. Nevertheless, VLMs enhanced with MASS attain performance comparable to open-source models of similar scale, demonstrating strong generalization across tasks. On MVBench, models equipped with MASS consistently outperform all baselines, underscoring the effectiveness of motion-aware spatialtemporal grounding for general-purpose video understanding when capturing temporal information and leveraging such information in reasoning and question-answering. F. Case Study In this section, we present additional qualitative examples of video question answering produced by several SoRA VLMs across the categories defined in MASS-Bench, as shown in Figures 610. Hallucinated or incorrect predictions are highlighted in red. Each example is accompanied by expert human annotations describing the ground-truth physics-driven 3 Figure 6. Video question-answering example from the Spatial Understanding (SU) category. We present physics reasoning and comprehension cases from state-of-the-art VLMs evaluated on spatial understanding tasks. Each example includes the video-generation prompt and human expert annotations, with visual grounding annotated (Red), the corresponding questions (Purple), and model responses from GPT-4o (Orange), Gemini-2.5-Flash (Blue), Qwen2.5-VL (Green), and Qwen2.5-VL + MASS (Gray). Hallucinated content and critical contextual errors are highlighted in Red. Figure 7. Video question-answering example from the Temporal Understanding (TU) category. We present physics reasoning and comprehension cases from state-of-the-art VLMs evaluated on temporal understanding tasks. Each example includes the video-generation prompt and human expert annotations, with visual grounding annotated (Red), the corresponding questions (Purple), and model responses from GPT-4o (Orange), Gemini-2.5-Flash (Blue), Qwen2.5-VL (Green), and Qwen2.5-VL + MASS (Gray). Hallucinated content and critical contextual errors are highlighted in Red. 4 Figure 8. Video question-answering example from the Motion and Action Recognition (MAR) category. We present physics reasoning and comprehension cases from state-of-the-art VLMs evaluated on motion and action recognition tasks. Each example includes the video-generation prompt and human expert annotations, with visual grounding annotated (Red), the corresponding questions (Purple), and model responses from GPT-4o (Orange), Gemini-2.5-Flash (Blue), Qwen2.5-VL (Green), and Qwen2.5-VL + MASS (Gray). Hallucinated content and critical contextual errors are highlighted in Red. Figure 9. Video question-answering example from the Physics Comprehension (PC) category. We present physics reasoning and comprehension cases from state-of-the-art VLMs evaluated on physics comprehension tasks. Each example includes the video-generation prompt and human expert annotations, with visual grounding annotated (Red), the corresponding questions (Purple), and model responses from GPT-4o (Orange), Gemini-2.5-Flash (Blue), Qwen2.5-VL (Green), and Qwen2.5-VL + MASS (Gray). Hallucinated content and critical contextual errors are highlighted in Red. 5 Figure 10. Video question-answering example from the Physics Abnormality Detection (PA) category. We present physics reasoning and comprehension cases from state-of-the-art VLMs evaluated on physics abnormality detection tasks. Each example includes the video-generation prompt and human expert annotations, with visual grounding annotated (Red), the corresponding questions (Purple), and model responses from GPT-4o (Orange), Gemini-2.5-Flash (Blue), Qwen2.5-VL (Green), and Qwen2.5-VL + MASS (Gray). Hallucinated content and critical contextual errors are highlighted in Red."
        }
    ],
    "affiliations": [
        "Dolby Laboratories",
        "University of Maryland",
        "University of Southern California"
    ]
}