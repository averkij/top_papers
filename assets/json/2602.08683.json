{
    "paper_title": "OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence",
    "authors": [
        "Feilong Tang",
        "Xiang An",
        "Yunyao Yan",
        "Yin Xie",
        "Bin Qin",
        "Kaicheng Yang",
        "Yifei Shen",
        "Yuanhan Zhang",
        "Chunyuan Li",
        "Shikun Feng",
        "Changrui Chen",
        "Huajie Tan",
        "Ming Hu",
        "Manyuan Zhang",
        "Bo Li",
        "Ziyong Feng",
        "Ziwei Liu",
        "Zongyuan Ge",
        "Jiankang Deng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Hypothesis. Artificial general intelligence is, at its core, a compression problem. Effective compression demands resonance: deep learning scales best when its architecture aligns with the fundamental structure of the data. These are the fundamental principles. Yet, modern vision architectures have strayed from these truths: visual signals are highly redundant, while discriminative information, the surprise, is sparse. Current models process dense pixel grids uniformly, wasting vast compute on static background rather than focusing on the predictive residuals that define motion and meaning. We argue that to solve visual understanding, we must align our architectures with the information-theoretic principles of video, i.e., Codecs. Method. OneVision-Encoder encodes video by compressing predictive visual structure into semantic meaning. By adopting Codec Patchification, OV-Encoder abandons uniform computation to focus exclusively on the 3.1%-25% of regions rich in signal entropy. To unify spatial and temporal reasoning under irregular token layouts, OneVision-Encoder employs a shared 3D RoPE and is trained with a large-scale cluster discrimination objective over more than one million semantic concepts, jointly capturing object permanence and motion dynamics. Evidence. The results validate our core hypothesis: efficiency and accuracy are not a trade-off; they are positively correlated. When integrated into LLM, it consistently outperforms strong vision backbones such as Qwen3-ViT and SigLIP2 across 16 image, video, and document understanding benchmarks, despite using substantially fewer visual tokens and pretraining data. Notably, on video understanding tasks, OV-Encoder achieves an average improvement of 4.1% over Qwen3-ViT. Codec-aligned, patch-level sparsity is a foundational principle, enabling OV-Encoder as a scalable engine for next-generation visual generalists."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 9 ] . [ 1 3 8 6 8 0 . 2 0 6 2 : r OneVision-Encoder: Codec-Aligned Sparsity as Foundational Principle for Multimodal Intelligence Glint Lab, AIM for Health Lab, MVP Lab Hypothesis. Artificial general intelligence is, at its core, compression problem (Sutskever, 2023). Effective compression demands resonance: deep learning scales best when its architecture aligns with the fundamental structure of the data. These are the fundamental principles. Yet, modern vision architectures have strayed from these truths: visual signals are highly redundant, while discriminative information, the surprise, is sparse. Current models process dense pixel grids uniformly, wasting vast compute on static background rather than focusing on the predictive residuals that define motion and meaning. We argue that to solve visual understanding, we must align our architectures with the information-theoretic principles of video, i.e., Codecs. Method. OneVision-Encoder encodes video by compressing predictive visual structure into semantic meaning. By adopting Codec Patchification, OneVision-Encoder abandons uniform computation to focus exclusively on the 3.1%-25% of regions rich in signal entropy. To unify spatial and temporal reasoning under irregular token layouts, OneVision-Encoder employs shared 3D RoPE and is trained with large-scale cluster discrimination objective over more than one million semantic concepts, jointly capturing object permanence and motion dynamics. Evidence. The results validate our core hypothesis: efficiency and accuracy are not trade-off; they are positively correlated. By resolving the dichotomy between dense grids and sparse semantics, OV-Encoder redefines the performance frontier. When integrated into large multimodal models, it consistently outperforms strong vision backbones such as Qwen3-ViT and SigLIP2 across 16 image, video, and document understanding benchmarks, despite using substantially fewer visual tokens and pretraining data. Notably, on video understanding tasks, OneVision-Encoder achieves an average improvement of 4.1% over Qwen3-ViT. Under attentive probing, it achieves state-of-the-art representation quality, with 17.1% and 8.1% Top-1 accuracy improvements over SigLIP2 and DINOv3, respectively, on Diving-48 under identical patch budgets. These results demonstrate that codec-aligned, patch-level sparsity is not an optimization trick, but foundational principle for next-generation visual generalists, positioning OneVision-Encoder as scalable engine for universal multimodal intelligence. Date: February 10, 2026 Code: https://github.com/EvolvingLMMs-Lab/OneVision-Encoder Data: https://github.com/EvolvingLMMs-Lab/OneVision-Encoder/blob/main/docs/data_card.md Model: https://huggingface.co/collections/lmms-lab-encoder/onevision-encoder"
        },
        {
            "title": "1 Introduction",
            "content": "Transformer-based methods have achieved significant improvements in video understanding (Carreira et al., 2024; Wang et al., 2024; Soldan et al., 2025; Assran et al., 2025; Shu et al., 2025; Yang et al., 2025). By representing videos as sequences of visual tokens, these models have demonstrated strong capacity to capture long-range spatial and temporal dependencies (Weng et al., 2024; Song et al., 2025). Reconstruction-based self-supervised frameworks (e.g., MAE (He et al., 2021), V-JEPA (Bardes et al., 2024)) emphasize pixelor feature-level prediction, which is effective for capturing low-level spatial and temporal correlations but often lacks explicit semantic structuring. In contrast, contrastive learning paradigms (e.g., CLIP (Radford et al., 2021), SigLIP (Zhai et al., 2023)) focus on instance-level discrimination and typically rely on external language supervision to induce semantic separation, limiting their ability to model intra-class consistency and fine-grained inter-class relationships. Recent cluster discrimination methods (An et al., 2023, 2024; Xie 1 Figure 1 Visual intelligence as codec-aligned predictive compression. Visual intelligence as compression problem, where scalable learning emerges from alignment with the predictive structure of the world. Video exemplifies this principle: most visual content is redundant and predictable, while meaningful information arises sparsely as motion and residual change. Video codecs make this structure explicit by decomposing visual signals into stable spatial context and sparse temporal updates. Grounded in this codec principle, OV-Encoder reframes visual modeling as predictive compression, serving as scalable engine for universal multimodal intelligence that sees, updates, and reasons over time. et al., 2025; Tang et al., 2025) address this gap by encouraging semantically related entities to form coherent clusters, pointing toward structured object-centric representation learning. Despite these advances, existing video transformers predominantly rely on representations constructed from sparsely sampled frames, leading to dense token sequences that implicitly assume equivalence across spatial regions and temporal frames. Such frame-centric design reflects prevailing modeling assumption in video pretraining and motivates rethinking how visual evidence is structured in video signals. At its core, general intelligence is compression problem. Natural videos are highly redundant, exhibiting strong spatial and temporal regularities. As result, the majority of the visual content in video is predictable from its surrounding context rather than constituting new discriminative evidence. However, standard video pretraining strategies rely on uniform computation over dense pixel grids, expending substantial capacity on static or easily inferred background regions. Discriminative information, the surprise, is sparse. This sparsity is not modeling artifact, but property of the signal itself. Video compression makes this explicit. Video codecs such as H.264 and H.265/HEVC (High Efficiency Video Coding) decompose video signals into spatially complete intra-coded frames (I-frames) that establish global context and predicted frames (P-frames) that encode inter-frame variations via motion compensation and residuals (Sullivan et al., 2012). This codec-driven formulation reveals that the vast majority of the video signal corresponds to motion-driven incremental updates to existing spatial context rather than independently discriminative visual evidence. In other words, visual understanding is governed by sparse, localized evidence that defines motion and meaning, rather than dense grids of uniformly processed pixels. These observations lead to unified conclusion: to scale visual intelligence, architectures must align with the information-theoretic structure of the data. In this work, we present OneVision-Encoder (OV-Encoder), 2 HEVC-style Vision Transformer that aligns spatiotemporal representation learning with the intrinsic predictive structure of video signals, as illustrated in Figure 1. Rather than uniformly processing dense pixel grids, OV-Encoder explicitly determines which visual signals constitute independent evidence and selectively encodes only the regions rich in signal entropy. To enable this, we introduce Codec Patchification, codec-inspired input formulation that leverages temporal signals exposed by video codecs to organize visual tokens at the patch level, together with 3D Rotary Position Embedding (RoPE) (Su et al., 2024) that jointly encodes spatial and temporal positions to support coherent attention over irregular spatiotemporal layouts. Specifically, (a) Dense Video-Codec Patchification: codec-inspired video encoding formulation that leverages motion-centric temporal signals exposed by P-frames to patchify selected visual regions (3.1%-25%) in dense video inputs, while preserving dense temporal coverage. (b) Chunk-wise Patchification: codec-inspired temporal patchification scheme that partitions video streams into fixed-length chunks and constructs patch-level representations with chunk-level positional encoding. (c) Single-Image Spatial Patchification: spatial instantiation of Codec Patchification that constructs patch-level representations for single-image input, enabling structured modeling of static visual content. Furthermore, explicitly modeling visual evidence at the patch level requires training objective that enforces semantic structure. We adopt self-supervised cluster discrimination objective based on large-scale semantic clustering over concept bank with more than one million clusters, jointly capturing object-level permanence and motion dynamics. In particular, OV-Encoder provides bi-directional attention-based vision encoder that effectively supports image and video understanding. Extensive experiments demonstrate the efficacy of OV-Encoder across both multimodal and representationlevel evaluation protocols. For comparison with SigLIP2 (Tschannen et al., 2025), all models are assessed under identical multimodal fine-tuning conditions, employing 1.5M-scale LLaVA-Next (single-image instruction) (Liu et al., 2024a) and LLaVA-Next-Videos (Zhang et al., 2024) instruction-tuning corpus, together with native-resolution processing strategy. Within this experimental configuration, OV-Encoder outperforms SigLIP2 across 16 benchmarks spanning video, image, and document understanding tasks, when evaluated using large multimodal models (LMMs) built upon Qwen3-4B (Bai et al., 2025). For comparison with Qwen3-ViT (Bai et al., 2025), we adopt controlled evaluation protocol. Specifically, we integrate OneVision-Encoder with the Qwen3-1.7B language model and train it under the LLaVA-OneVision1.5 (An et al., 2025) framework, completing both Stage 1 and Stage 1.5 to adapt the encoder to native-resolution inputs. The trained OneVision-Encoder is then decoupled and compared with Qwen3-ViT under the same LLaVA-Next-Videos instruction-tuning training setting, where OV-Encoder outperforms Qwen3-ViT across 16 understanding benchmarks under an LMM built upon Qwen3-4B. In particular, despite having been pretrained on substantially fewer visualtext tokens (approximately 100B caption tokens), OV-Encoder outperforms Qwen3-ViT, which is pretrained on more than 2.1T caption and instruction-aligned tokens. Under attentive probing on 7 benchmarks, OV-Encoder achieves state-of-the-art performance, including 17.1% and 8.1% Top-1 accuracy improvements over SigLIP2 and DINOv3 (Siméoni et al., 2025), respectively, on Diving-48 under an identical patch budget of 2048. Moreover, OV-Encoder outperforms strong vision baselines such as DINOv3, SigLIP2, MetaCLIP2 (Chuang et al., 2025), and AIMv2 (Fini et al., 2025) under dense-patch evaluation. We release our data, training protocols, and model parameters to support transparent, reproducible, and cost-effective visionlanguage research. Our contributions are as follows: We present OneVision-Encoder (OV-Encoder), HEVC-style vision transformer that aligns spatiotemporal representation learning with the intrinsic predictive structure of video signals through Codec patch-level encoding. We introduce Codec Patchification, codec-inspired input formulation that leverages codec-derived temporal signals to selectively encode informative visual patches (3.1%-25%) from dense video, while unifying video, chunk-wise sampling, and single-image inputs with 3D-RoPE. We adopt self-supervised cluster discrimination objective that jointly models object-level and motionlevel semantics with large-scale concept bank, enabling structured and modality-agnostic visual representation learning. Extensive experiments establish the effectiveness of OV-Encoder across evaluation settings. Under LLM-based probing, the model consistently surpasses strong vision backbones, including Qwen3-ViT and SigLIP2, across multimodal benchmarks. Under attentive probing, OV-Encoder outperforms SigLIP2 3 and DINOv3 across 7 benchmarks, indicating robust representation learning."
        },
        {
            "title": "2 Approach",
            "content": "Although most previous video encoders focus on short clips of 16 frames (roughly seconds) (Bardes et al., 2023; Wang et al., 2024), we explore training with longer clips of up to 64 frames at higher spatial resolutions. Let Vi = {Ii,t}Ti denote the i-th input video of spatial size , where Ti is the total number of frames t=1 in Vi. Our objective is to process raw video inputs within the proposed different input configurations and jointly encode them using shared ViT for unified spatiotemporal representation learning."
        },
        {
            "title": "2.1 HEVC Guided Patch Selection",
            "content": "Codec Based Video Factorization. Following the standard High Efficiency Video Coding formulation (Sullivan et al., 2012), each video Vi is divided into Ni Groups of Pictures (GOP), {Si,n}Ni . Applying the HEVC n=1 codec to each Groups of Pictures yields one intra-coded frame and (Ki,n1) predicted frames: i,n, {F i,n RHW Cimg is the I-frame (RGB, Cimg = 3), and each where represented in the bitstream by motion vectors and residual signal. Ki,n denotes the GOP length. (cid:1) = CHEVC(Si,n), i,n,τ }Ki,n1 (cid:0)F i,n,τ τ =1 denotes P-frame, which is (1) Motion and Residual Signals. In HEVC, motion is represented by motion vectors di,n,τ that encode block level displacements between the current frame and its motion compensated prediction from the reference frame. Concretely, P-frames are partitioned into coding units (CUs) with variable sizes ranging from 44 to 6464, and all pixels within CU share the same motion vector. To align codec signals with ViT patchification, we first broadcast each CU motion vector to its covered pixels, obtaining dense pixel level motion field. The magnitude di,n,τ 2 reflects the intensity of local motion, with larger values indicating stronger or more complex dynamics. In addition to motion, each P-frame is associated with residual signal that captures appearance changes not explained by motion compensation; we decode the luma residual into the pixel domain and measure its energy as complementary cue for unpredictable visual variation. At the patch level, we aggregate motion magnitude and residual energy over all pixels inside each ViT patch, so the two signals jointly characterize the amount of new visual evidence introduced by region. We use these codec exposed signals as principled proxy for unpredictability, enabling the identification of salient regions that contribute new spatiotemporal information. Sparse Patch Selection. Let denote the patch size (e.g., p=14). We define the patch grid = {(y, x) 0 < H/p, 0 < W/p}, with cardinality P0 = (H/p)(W/p). For each P-frame, we compute patch level saliency score by aggregating the codec exposed motion magnitude and residual energy defined above. Based on the aggregated saliency score, we construct binary mask Ωi,n,τ by selecting fixed proportion of the most salient patches. The sparsity ratio is fixed throughout training and inference by selecting fixed proportion of the most salient patches, i.e., Ωi,n,τ = rP0."
        },
        {
            "title": "2.2 Codec Patchification",
            "content": "Dense Video-Codec Patchification. Following the codec formulation, each video Vi is partitioned into Ni GOP. For the n-th GOP, the HEVC encoder produces one intra-coded frame and (Ki,n 1) predicted i,n that are defined by motion vectors and residual signal after motion compensation. Let frames {F Ωi,n,τ denote the codec-derived binary mask that selects dynamically informative patches based on motion magnitude and residual energy, and let Πp() denote patchification with patch size p. The HEVC-compressed input sequence is defined as i,n,τ }Ki,n1 τ =1 (hevc) i,n (cid:104) Πp(F i,n) (cid:8)Πp( i,n,τ )[Ωi,n,τ ](cid:9)Ki,n1 τ =1 (cid:105) , (2) where i,n,τ RHW Cimg denotes the decoded RGB P-frame in the pixel domain obtained by decoding the HEVC bitstream. The binary mask Ωi,n,τ is computed from the codec-exposed motion vectors di,n,τ and 4 Figure 2 Overview of the OneVision-Encoder framework. Left: Input formulation. The framework integrates three Codec Patchification strategies: Dense Video-Codec Patchification, Chunk-wise Patchification, and (Sigle-Image/Frame) Spatial Patchification. All inputs are processed by shared-parameter OneVision-Encoder. Right: Unified cluster discrimination objective. Image and video embeddings are aligned through contrastive learning against global set of class centers, jointly optimizing object-level and action-level representations within single encoder. the associated motion-compensated residual signal of the same P-frame, and is used only for salient patch selection. Therefore, the tokens fed to the encoder for P-frames are RGB patches from indexed by Ωi,n,τ . Ki,n is the GOP length. [Ω] denotes masked patch selection along the patch dimension. denotes concatenation along the token dimension. Let Mi,n be the number of tokens in (hevc) ; the total tokens over the video satisfy: i,n,τ i,n Ni(cid:88) Mi = Mi,n = Ni(cid:88) (cid:16) P0 + Ki,n1 (cid:88) (cid:17) Ωi,n,τ , (3) n=1 which yields pixel/token compression ratio per GOP, γi,n = 1 Mi,n/(Ki,nP0). Under our default setting (64 frames, GOP size 32, token budget 2048, P0 = 256), the overall clip-level token reduction is 1 2048/(64P0) = 87.5%. n=1 τ =1 Chunk-wise Patchification. To unify Codec patchification with sparse temporal sampling, each video Vi is uniformly partitioned into temporal chunks of Ti/C consecutive frames. From every chunk, one frame is randomly sampled, resulting in temporally stratified subsequence πi = {tc}C , where tc [(c1)Ti/C, cTi/C). The corresponding chunk-wise sampling sequence is defined as c=1 (chunk) (cid:8)Πp(Ii,tc) tc πi (cid:9) = Πp(Vi[πi]). (4) Single-Image Spatial Patchification. To achieve spatial scalability, each image Ii,t RHW Cimg is processed independently as static image input. Each image is directly patchified in row-wise manner from top to bottom to ensure deterministic spatial ordering of visual tokens, preserving the original spatial layout. The resulting patch sequence is defined as (image) (cid:8)Πp(Ii,t) = 1, . . . , Ti (cid:9), (5) where Πp() denotes the patchification operator applied to single frame, and Ti = 1 for single-image inputs. detailed spatial bias analysis is provided in the supplementary material. Tokenization and Transformer Encoding. After generating the three types of input sequences, namely the Codec Patchification formulation, we uniformly tokenize and encode them using shared Transformer backbone is processed by ϕ(). For each video input vid {hevc, chunk, image}, the token sequence vid = {xvid i,k }M vid k=1 5 Figure 3 Contrastive learning vs. cluster discrimination. (a) Standard contrastive learning contrasts samples against batchlocal negatives, constraining the view of the embedding space. (b) Cluster discrimination contrasts samples against global concept bank of clustered centers at scale, yielding discriminative and structurally separated representations. the encoder ϕ() followed by linear projection head RdD where and denote hidden and latent dimensions of the encoder, respectively, to obtain latent embeddings: = ϕ(cid:0)F vid Evid (cid:1)W = vid W. (6) = ϕ(F vid which defines vid as the encoder output features before projection. Finally, function (), such as attentive pooling, integrates all token embeddings into compact video-level representation = (Evid evid ) RD. ) RM vid i"
        },
        {
            "title": "2.3 Image and Video Clustering",
            "content": "Iterative clustering-discrimination approaches commonly suffer from substantial computational overhead (Caron et al., 2018). To address this issue, we adopt single-step offline clustering to efficiently capture both objectlevel semantics from images and motion-level semantics from videos. Notably, embeddings used for clustering are extracted using separate frozen vision encoder (e.g., metaclip-h14 (Xu et al., 2024)) For the image modality, we follow the clustering formulation in MLCD (An et al., 2024) to capture objectlevel semantics. Given an image embedding eobj RD, we learn set of object-level semantic centroids Cobj = {cobj k=1 RD by minimizing the within-cluster distance: }Kobj Cobj = arg min {cobj } Nobj (cid:88) i=1 min 1kKobj (cid:13) (cid:13)eobj cobj (cid:13) 2 2, (cid:13) (7) where Nobj denotes the number of image samples. For the video modality, we extend this formulation to model motion-level dynamics. Video embeddings are derived from fixed-length 16-frame inputs, where frame-level features are concatenated to form single video-level representation evid k=1 RD, and formulate the clustering objective for videos as RD. We then define set of video semantic centroids Cvid = {cvid }Kvid Cvid = arg min {cvid } Nvid(cid:88) i=1 min 1kKvid (cid:13) (cid:13)evid cvid (cid:13) 2 2, (cid:13) (8) }Kobj k=1 {cvid where Nvid denotes the number of video samples. We define set of shared semantic centroids Cuni = {cobj , and = Kobj + Kvid represents the total number of clusters across both image and video modalities. These semantic centroids are later used as supervision signals in the cluster discrimination objective, where image embeddings are supervised by object-level centroids and video embeddings are supervised by motion-level centroids. }Kvid k=1 Figure 4 3D-RoPE for Codec Patchification. unified relative positional encoding scheme is adopted for Codec Patchification. (a) encodes full spatiotemporal offsets (t, x, y) over I/P-frame sequences to preserve motion-driven inter-frame structure. (b) defines temporal offsets at the chunk level, enabling structured reasoning under non-uniform temporal sampling. (c) degenerates the formulation to purely spatial offsets (0, x, y) for static inputs. 3D-RoPE preserves structural consistency, enabling coherent attention over sparse and irregular token layouts."
        },
        {
            "title": "2.4 Training Objective",
            "content": "Visual samples commonly exhibit multiple semantic components, including object-level semantics from images and motion-level semantics from videos, rendering single-label assignments inadequate for unified representation learning. To capture both semantic structures, we introduce contrastive objective that leverages these semantic clusters as pseudo-label supervision to explicitly enforce structural constraints, as illustrated in Figure 3. Specifically, for each visual embedding ei RD, we identify multiple positive semantic labels from unified semantic centroid set Cuni, which consists of both object-level centroids Cobj and motion-level centroids Cvid. We compute the training objective separately for each semantic granularity {obj, vid}, where negative labels are drawn from the corresponding centroid set Cm. The remaining centroids within the same granularity are treated as negative labels. Subsequently, the joint multi-label semantic discrimination objective is formulated as: = (cid:88) m{obj,vid} E(u,k)Cm log(cid:0)1 + exp(ym u,k σm u,k)(cid:1), (9) cm u,k = denotes the similarity score between the embedding eu and its semantic centroid cm where σm , and indexes visual embeddings while indexes centroids in Cm. For each granularity m, (u, k) Cm samples visual embeddings eu and semantic centroids cm from the corresponding cluster set, with pseudo-labels u,k {+1, 1} indicating positive or negative semantic associations. This unified formulation integrates both ym objectand motion-level clustering signals, enforcing spatiotemporal consistency and promoting discriminative representation learning."
        },
        {
            "title": "2.5 Architecture",
            "content": "For the OneVision-Encoder, we adopt the Vision Transformer (ViT) architecture (Dosovitskiy et al., 2020). 3D-RoPE for Codec Patchification. Unlike absolute encoding = (t, x, y), 3D-RoPE adopts relative positional scheme (Su et al., 2024) represented as = (t1 t2, x1 x2, y1 y2), as illustrated in Figure 4. The relative offsets for the three inputs are defined as: Dense Video-Codec Patchification: pcodec = (titj, xixj, yiyj), which emphasizes inter-frame (I/P) residual alignment via the temporal offset titj. 7 Table 1 OneVision-Encoder Pretraining Dataset. The pretraining corpus combines large-scale image and video datasets for unified visual representation learning. Image datasets primarily provide broad visual coverage, while video datasets support temporal modeling and videolanguage alignment. We use ExoVideo to denote large-scale third-person web videos, and ActionVideo to denote curated action recognition datasets. Source Samples Type Modality Temporal Curation LAION-400M (Schuhmann et al., 2021) COYO-700M (Byeon et al., 2022) OBELICS (Laurençon et al., 2023) Zero250M (Xie et al., 2023) ImageNet-21K (Deng et al., 2009) HowTo100M (Miech et al., 2019) Panda-70M (Chen et al., 2024b) Kinetics-710 (Li et al., 2022b) SSV2 (Goyal et al., 2017) 250M 400M 15M 15M 14M 50M 50M 658K 221K WebImages WebImages Documents CuratedImages Images ExoVideo ExoVideo ActionVideo ActionVideo Image Image Image Image Image Video Video Video Video Short Long Short Short Yes Yes Yes Yes Yes No Yes Yes Yes Chunk-wise Patchification: for frames ti, tj belonging to chunks ci, cj {1, . . . , C}, the relative offset is defined as pchunk = (c, x, y), capturing inter-chunk temporal disparity under non-uniform sampling, where = ci cj. Single-Image Spatial Patchification: for two patches within the same frame with spatial coordinates (xi, yi) and (xj, yj), the relative positional offset is defined as pspatial = (0, x, y), where = xi xj and = yi yj, encoding spatial positional relationships without temporal shifts. Attentive Pooling Head. We employ multi-head attention pooling module, adapted from SigLIP (Zhai et al., 2023), to aggregate spatiotemporal tokens into compact class embeddings through learnable token-to-class attention weights, emphasizing salient regions and enabling unified global contextual representation across image and video modalities."
        },
        {
            "title": "3 Pretraining Dataset",
            "content": "Data Annotation and Processing. This section details the annotation and processing pipeline for the OneVision-Encoder pretraining data. Our core objective is to generate high-quality supervision signals for massive-scale data through automated means. Image Data Annotation. For image data, we primarily process LAION-400M (Schuhmann et al., 2022) and COYO-700M (Byeon et al., 2022). First, we employ Union-Find algorithm to strictly deduplicate the dataset. Subsequently, we utilize the metaclip-h14-fullcc2.5b (Xu et al., 2024) model to extract image features and cluster all images into two million classes. Based on this clustering, each image sample is annotated with the nearest Top-10 class centers as its multi-label supervision signal. Furthermore, we incorporate the OBELICS (Laurençon et al., 2023) and Zero250M (Xie et al., 2023) datasets. We utilize PaddleOCR (Sarkar et al., 2024) to recognize text within images and perform word segmentation on the recognized content; the resulting vocabulary is used as multi-labels to construct supervision signal containing exactly 100 fine-grained tags per image. Video Data Annotation. The video data construction encompasses HowTo100M (Miech et al., 2019), Panda70M (Chen et al., 2024b), Kinetics-710 (Li et al., 2022b), and Something-Something-V2 (SSV2) (Goyal et al., 2017). We uniformly adopt metaclip-h14-fullcc2.5b as the video encoder, performing uniform frame sampling to extract features from fixed 8-frame clip. In the feature processing stage, we adopt L2 Normalize Concatenate L2 Normalize strategy to generate video-level representations. We then cluster the video representations into 400k classes and assign each video clip the nearest Top-10 class centers as its multi-labels."
        },
        {
            "title": "4.1 Pretraining OneVision-Encoder",
            "content": "We adopt two-stage pretraining pipeline using large-scale image, video, and OCR data, trained on 128 A800 GPUs (16 nodes 8 GPUs). Stage 1: For the image model, we use images at resolution of 224. We adopt the AdamW optimizer with learning rate of 0.001 and weight decay of 0.2. The number of classes (k) is two million, the ratio of sampled negative class centers (r) is 0.1, and the number of positive labels (l) assigned to each image (and each video) is 10. In this stage, we trained on 13B samples using only image data. Stage 2: In the second pre-training stage, we introduced OCR data and video data, training on 4B samples with an image resolution of 448 and video resolution of 224 (frame sampling). For video processing, we randomly adopted one of Codec Patchification formulations. For dense video-Codec inputs, each training sample corresponds to fixed-length clip of 64 consecutive frames. We follow an HEVC-style GOP configuration with an I-frame every 32 frames, resulting in two I-frames and sixty-two P-frames per clip. All I-frames are fully encoded, retaining all spatial patches to establish complete spatial context. For the remaining P-frames, codec-derived motion vectors and residuals are used to identify temporally salient regions, and patches are selected sparsely across all P-frames. Importantly, the patch budget is enforced at the clip level rather than the GOP level. While GOPs define the I/P frame structure, the total number of visual tokens for the entire 64-frame clip is fixed to 2,048. Specifically, 512 tokens are allocated to the two I-frames (256 patches each), and the remaining 1,536 tokens are distributed across all P-frames by selecting motion-relevant residual patches. This results in an 87.5% reduction compared to dense processing of all 64 frames (16,384 patches), while preserving full temporal coverage. All samples shared the same ViT backbone, with loss calculated separately for each modality based on their annotations. The video-to-image ratio was 1:1, and the learning rate was reduced to 0.0001. Training Strategy. During the training phase, all data sources share the same OneVision-Encoder for feature extraction. However, when computing the loss, the loss is calculated separately for each data type and then aggregated. Detailed training configurations and implementation specifics are provided in the supplementary material."
        },
        {
            "title": "4.2 LMM Probing Evaluation - Language Alignment",
            "content": "In this section, we evaluate the effectiveness of OV-Encoder when integrated into LMMs. Our goal is to assess whether the learned visual representations transfer robustly to multimodal reasoning settings, while isolating the contribution of the vision encoder from language model capacity and training protocol. We lift these features through alignment tuning to construct new codec encoder, OV-Encoder-Lang, specialized for MLLMs. The controlled evaluation pipeline is illustrated in Figure 7 in the supplementary material. MLLM Evaluation Tasks. We adapt vision encoders to large multimodal models and evaluate downstream performance on set of imageand video-centric benchmarks using LMMs-Eval (Zhang et al., 2025) with the default prompt. Image tasks include ChartQA (Masry et al., 2022), DocVQA (Mathew et al., 2021), InfoVQA (Mathew et al., 2022), AI2D (Kembhavi et al., 2016), MMBench-EN (Fu et al., 2023), OCRBench (Liu et al., 2024b), OCRBench v2 (Liu et al., 2024b), MMStar (Chen et al., 2024a), and RealWorldQA (Corp., 2024). Video tasks include MVBench (Li et al., 2023a), MLVU-dev (Zhou et al., 2024), NExT-QA (Xiao et al., 2021), VideoMME (Fu et al., 2024), PerceptionTest (Patraucean et al., 2023), TOMATO (Shinoda et al., 2025), and LongVideoBench-Val-Video (Wu et al., 2024). Experimental Setup. Following probing-based fine-tuning paradigm, we keep the language model architecture fixed and vary only the visual encoder. Unless otherwise specified, all experiments are conducted with Qwen3-4B-Instruct2507 as the language backbone. We adopt two-stage training pipeline: first, Stage-1 alignment phase, followed by Stage-2 instruction-tuning phase. The instruction-tuning corpus consists of approximately 1.5M samples, including 740K single-image instruction from LLaVA-Next and 800K samples from LLaVA-Next-Videos. This unified instruction-following setting enables controlled comparison across different vision encoders under identical multimodal supervision. 9 Table 2 Comparison of different vision encoders on multimodal benchmarks. All models are evaluated on unified multimodal setting using Qwen3-4B-Instruct2507 as the language backbone. OV-Encoder-Lang denotes the language-aligned variant of the OV-Encoder architecture. Qwen3-ViT is extracted from Qwen3-VL-4B. SigLIP2 uses siglip2-so400mpatch16-naflex. (Codec) indicates codec-guided visual encoding using motion vectors and residual signals, while (Frame) indicates frame-based visual encoding with dense spatial patchification. Bold values indicate the best performance under the same evaluation setting. Results reported in the left columns correspond to encoders trained with caption supervision, whereas results in the right columns correspond to encoders trained without caption supervision. Task Benchmark Qwen3-4B-Instruct2507 OV-Encoder-Lang Qwen3-ViT OV-Encoder OV-Encoder-Frame SigLIP2 (Codec) (Frame) (Codec) (Frame) (Frame) Video Image MVBench MLVU-dev NExT-QA (MC) VideoMME Perception Test TOMATO LongVideoBench-Val-Video AI2D ChartQA DocVQA InfoVQA MMBench-EN OCRBench OCRBench v2 MMStar RealWorldQA 53.2 47.4 76.1 54.1 60.6 21.8 51. 80.2 80.1 83.2 51.6 80.2 657 30.8 56.6 66.1 47.4 47.2 70.1 47.2 57.1 22.2 45. 77.8 79.6 85.1 49.0 79.4 706 30.6 56.6 63.3 52.4 46.3 75.6 53.4 60.3 22. 50.4 75.7 76.5 78.4 43.1 77.2 605 26.3 52.1 60.8 49.8 49.4 71.9 49.3 56.7 21.8 45.5 76. 77.8 79.5 45.5 78.5 630 26.1 54.3 61.2 47.2 48.4 70.6 46.8 56.0 22.3 45. 78.6 76.4 75.0 42.0 79.6 621 26.1 55.0 62.1 4.2.1 Native-Resolution Evaluation. We adopt native-resolution evaluation strategy following LLaVA-Next, with key distinction: input frames matching the native resolution of the vision encoder are processed directly without spatial tiling or cropping. For video inputs, we use fixed per-frame resolution, set to 512512 for SigLIP2 and Qwen3-ViT, and 504504 for OneVision-Encoder. This design avoids resolution-induced artifacts and enables direct assessment of the encoders native-resolution modeling capability under realistic multimodal inference conditions. Codec-based Patch Sampling. We evaluate the same codec-guided patchification principle as described in Stage 2 of Section 4.1 under strictly controlled token budget. For each test video, we uniformly sample 64 frames over the full duration to obtain broad temporal coverage. We do not re-encode or transcode benchmark videos; compressed-domain signals (e.g., motion vectors and residuals) are extracted directly from the original bitstreams, thus preserving the native GOP structures and codec parameters of each dataset. Based on these signals, we compute lightweight saliency score to estimate temporally informative regions and select patches sparsely across the sampled frames. The selected patches are then packed into fixed number of visual tokens. Importantly, we keep the total token budget identical to the dense baseline that encodes only 8 frames, ensuring that performance differences reflect improved token allocation rather than increased token count. Comparison with SigLIP2. We first compare OneVision-Encoder with SigLIP2 under identical multimodal fine-tuning conditions, as shown in Table 2. All models share the same instruction-tuning corpus, data preprocessing pipelines, training schedules, decoding strategies, and visual token budgets. Under this setting, OneVision-Encoder consistently outperforms SigLIP2 across 16 video, image, and document understanding benchmarks when integrated into an LMM built upon Qwen3-4B, indicating stronger multimodal transfer from the learned visual representations. Comparison with Qwen3-ViT. We further conduct comparison with Qwen3-ViT. Specifically, we integrate 10 Table 3 Comparison of OV-Encoder training stages on image understanding benchmarks. In this study, we evaluate two training variants of the same ViT architecture under an identical data scale: OV-Encoder-stage1 is trained with image-only data, while OV-Encoder-stage2 continues training from Stage 1 and incorporates OCR and video data with codec-style patch selection. Bold values indicate the best performance among the compared encoders. Benchmark OV-Encoder-stage1 OV-Encoder-stage SigLIP2-sig AI2D ChartQA DocVQA InfoVQA MMBench-EN MMStar RealWorldQA OCRBench 73.6 73.6 74.3 34.7 74.7 49.3 61.8 551.0 74.5 76.2 78.5 41.4 76.3 49.7 61.3 601. 77.5 77.0 74.4 37.7 78.1 52.6 62.2 590.0 OneVision-Encoder with the Qwen3-1.7B language model and train it under the LLaVA-OneVision-1.5 framework, completing both Stage 1 and Stage 1.5 to adapt the encoder to native-resolution inputs. After this adaptation, the trained OneVision-Encoder is decoupled and compared with Qwen3-ViT under the same LLaVA-Next-Videos instruction-tuning setting. Under this setting, OneVision-Encoder outperforms Qwen3ViT across 16 understanding benchmarks when evaluated with an LMM built upon Qwen3-4B, as shown in Table 2. Notably, these gains are achieved despite OneVision-Encoder being pretrained on substantially fewer visualtext tokens (approximately 100B caption tokens), whereas Qwen3-ViT is pretrained on more than 2.1T caption and instruction-aligned tokens. This result suggests that the observed improvements arise from more effective visual representation learning, rather than increased pretraining scale or architectural specialization. 4.2.2 Stage-wise Multimodal Training Analysis. We further analyze how stage-wise multimodal training contributes to the observed performance in LMM probing. In the first stage (OV-Encoder-Stage1), the visual encoder is trained using image-only data, focusing on general-purpose visual representation learning. In the second stage (OV-Encoder-Stage2), OCR and video data are introduced on top of Stage1, together with Codec patch selection and chunk-wise temporal sampling strategies. As shown in Table 3, the Stage2 model consistently outperforms its Stage1 counterpart on multimodal and OCR-related benchmarks, while maintaining strong performance on general visual reasoning tasks. This comparison demonstrates that injecting OCR and video supervision plays critical role in enhancing the ViT-based encoders suitability as unified visual backbone for LMMs. Together with the native-resolution results above, this analysis highlights that stage-wise multimodal training is key factor enabling both robust native-resolution generalization and effective multimodal reasoning."
        },
        {
            "title": "4.3 Attentive Probing Evaluation",
            "content": "We evaluate the quality of visual representations learned by OV-Encoder using an attentive probing protocol, which has been widely adopted to assess backbone-level spatiotemporal modeling capacity without task-specific adaptation. In this setting, the visual encoder is frozen and lightweight attention-based classifier head is trained on top of the extracted features for downstream video classification. This protocol isolates the intrinsic representational strength of the encoder and enables fair comparison across architectures with different tokenization and temporal modeling strategies. Experimental Setup. All models are evaluated under controlled and unified attentive probing protocol. Following prior work on vision-language pretraining and attentive pooling (Tschannen et al., 2025), we employ multi-head attention pooling classifier to aggregate spatiotemporal features into video-level representations. The same classifier architecture is used for all methods to ensure architectural consistency, and the probing head is trained with an identical number of epochs, optimization settings, and learning rate schedules across 11 Table 4 Comparison with state-of-the-art methods on video understanding benchmarks. We report top-1 accuracy (%) using an attentive probe with frozen backbones, evaluated under two input configurations: 8 Frames / 2048 Patches and 16 Frames / 4096 Patches. For OV-Encoder (Codec), inputs are constructed using Dense Video-codec Patchification, which selectively encodes temporally salient patches from dense video inputs under the corresponding patch budgets. Bold indicates the best performance and underline indicates the second-best. Model Setup Video Benchmarks (Acc. %) Method Backbone Res. Avg. 8 Frames / 2048 Patches CLIP SigLIP MetaCLIP MetaCLIP2 AIMv2 SigLIP2 DINOv3 OV-Encoder (Frame) OV-Encoder (Codec) 16 Frames / 4096 Patches SigLIP MetaCLIP2 AIMv2 SigLIP2 DINOv3 OV-Encoder (Frame) OV-Encoder (Codec) ViT-L/14 ViT-L/16 ViT-L/14 ViT-L/14 ViT-L/14 ViT-L/16 ViT-L/14 ViT-L/14 ViT-L/14 ViT-L/16 ViT-L/14 ViT-L/14 ViT-L/16 ViT-L/14 ViT-L/14 ViT-L/14 224 256 224 224 224 256 224 224 224 256 224 224 256 224 224 e i c 52.2 48.9 49.8 47.7 55.1 52.7 59.3 58.3 8 4 v 46.6 43.9 28.9 48.0 43.6 50.1 58.6 57.6 2 48.2 50.7 50.6 47.2 55.1 52.6 57.4 57. 50.5 50.1 48.5 50.2 53.8 53.1 58.0 58.4 60.2 58.5 67.2 60.0 52.8 51.0 56.4 55.7 59.1 59. 61.4 52.7 49.3 57.2 58.2 58.3 58.7 60.1 54.7 42.1 55.7 56.7 61.3 63.2 51.0 51.1 56.4 53.3 60.8 60.3 69. 60.9 s r 10.8 10.9 10.4 11.0 12.0 11.6 13.2 12.1 12.3 11.7 11.2 12.4 11.9 14.0 12.6 12. e - E 52.8 52.2 54.1 48.0 56.6 54.2 62.5 61.4 62.3 54.1 49.2 58.3 56.4 63.2 62.9 63.3 N - E 36.1 39.1 37.1 40.9 45.6 43.8 51.7 52.5 0 0 4 - t i 79.3 78.2 79.6 82.4 81.1 80.9 82.9 84.3 1 5 H 78.0 77.0 77.1 76.3 81.3 79.1 78.6 83. 53.9 84.4 83.4 40.2 43.2 46.2 45.2 51.9 54.5 54.4 79.1 84.0 82.2 82.7 83.9 85. 78.8 78.2 82.6 81.2 79.7 81.6 85.4 85.3 all experiments. All experiments are conducted on cluster of 8 NVIDIA A800 GPUs. We therefore evaluate our model by assessing the quality of the models learned representation on set of seven benchmarks: SSV2, Diving48 (Li et al., 2018), Perception Test (Patraucean et al., 2023), CharadesEgo (Sigurdsson et al., 2018), Epic-Kitchens-100 (Kay et al., 2017), Kinetics-400 (Damen et al., 2022), HMDB51 (Kuehne et al., 2011). Batch sizes are determined on per-dataset basis to balance computational efficiency and training stability: batch size of 32 is used for SSV2, Diving48, and Perception Test, 16 for HMDB51, and 128 for all remaining datasets. During evaluation, we adopt single-crop inference protocol, using one temporal crop and one spatial crop per video clip, resulting in single prediction for each input video. Input Configuration. For frame-centric baselines, including SigLIP2, DINOv3, and AIMv2, we evaluate both 8-frame and 16-frame inputs using uniform temporal sampling. For OV-Encoder, we evaluate two instantiations under an identical patch budget of 512 visual tokens. Specifically, the codec-guided variant operates on dense 64-frame video inputs, where codec-derived signals determine the visible patch indices, while the chunk-wise variant samples frames within fixed temporal chunks. This design ensures that all methods are compared under fixed token budget, decoupling representational capacity from raw input resolution or frame count. Evaluation Protocol. During inference, all crops belonging to the same video are aggregated by averaging logits across crops. Labels are shared across crops of the same video, and no test-time augmentation beyond the single-crop setting is applied. For codec-based models, visible patch indices are provided as part of the input to ensure consistent patch selection across crops. Results. Table 4 shows that OV-Encoder consistently outperforms SigLIP2, DINOv3, and AIMv2 across all evaluated video benchmarks under both 8-frame and 16-frame settings. In particular, OV-Encoder achieves 12.7% absolute Top-1 accuracy improvement over SigLIP2 on Diving48 at an identical patch budget, demonstrating superior motion modeling capability. Notably, these gains are achieved without sacrificing 12 Table 5 Effect of patch budget scaling under attentive probing. Patch budgets of 512/1024/2048/4096 correspond to 2/4/8/16 video frames, respectively. Dense SigLIP2 processes all spatial patches per frame, while OV-Encoder (Codec) selectively retains motion-relevant patches guided by codec-derived temporal signals. Setting Video Benchmarks (Acc. %) T t r o e a r - E N - E 0 0 4 - t K 1 5 H 2 AVG 8 4 v Patch Budget = 512 SigLIP2 47.0 OV-Encoder (Codec) 55.1 Patch Budget = 1024 SigLIP 53.4 OV-Encoder (Codec) 62.4 Patch Budget = 2048 SigLIP2 56.6 OV-Encoder (Codec) 66.5 Patch Budget = 4096 SigLIP2 59.9 OV-Encoder (Codec) 67.5 42.8 50. 37.3 46.5 22.5 50.5 46.4 50.6 42.9 50.3 37.4 41.7 74.5 76. 72.4 74.7 49.0 56.6 48.5 54.9 35.2 61.6 50.0 57.4 50.6 58. 42.0 48.4 78.8 81.8 76.6 80.3 53.1 60.2 52.6 58.5 50.1 67. 52.7 60.0 54.2 62.3 43.8 53.9 80.9 84.4 79.1 83.4 55.7 61. 58.2 60.1 56.7 69.4 53.3 60.9 56.4 63.3 45.2 54.4 82.7 85. 81.2 85.3 performance on appearance-dominated datasets such as Kinetics-400, confirming that Codec patch-level encoding yields more discriminative and efficient spatiotemporal representations. We further observe that for 64-frame inputs under fixed token budget, allocating only one to two I-frames is sufficient to establish stable spatial context, with subsequent frames contributing primarily through sparse motion-driven updates."
        },
        {
            "title": "4.4 Patch-Efficient Video Understanding Comparison",
            "content": "We conduct an efficiency analysis comparing SigLIP2 with dense full-frame patch processing and OV-Encoder (Codec) under fixed token budget, as shown in Table 5. It is important to emphasize that OV-Encoder (Codec) does not perform temporal downsampling of the input video. All results are obtained from the same 64-frame (16384 patches) source video, where codec-native motion vectors and residuals are used to selectively extract fixed number of spatiotemporal patches distributed across the entire temporal extent. For fair comparison, SigLIP2 is evaluated under the same token budgets and adopts traditional frame sampling strategy, where each group of 256 patches corresponds to contiguous RGB frame. Under fixed token budget, OV-Encoder (Codec) redistributes patches across time while preserving their spatial positions, enabling long-range temporal coverage. As result, it outperforms SigLIP2 on Diving48 and Perception Test while reducing patch processing by 75.0%96.9% compared to dense processing of 16,384 patches. Specifically, the reduction ratio is computed relative to the dense baseline that processes all 64 256 = 16,384 patches from the full video. Using token budgets of 4096, 2048, 1024, and 512 patches corresponds to retaining 25.0%, 12.5%, 6.25%, and 3.1% of the dense patches, respectively, yielding patch reduction of 75.0%96.9%."
        },
        {
            "title": "4.5 Ablation of Codec-guided Patch Selection",
            "content": "Although OV-Encoder (Codec) consistently outperforms frame-centric baselines under attentive probing, as shown in Table 4, performance gains alone do not establish codec-guided patch selection as functional mechanism. To isolate its causal role, we conduct set of controlled interventions that explicitly manipulate 13 Table 6 Controlled interventions on codec-selected motion patches. All settings use identical token budgets and visual content unless otherwise specified. Setting Video Benchmarks (Acc. %) T t r o e a r - E N - E 0 0 4 - t K 1 5 H 2 AVG 8 4 v OV-Encoder (Codec) 61.4 60.1 69.4 60. 12.9 63.3 54.4 85.4 85.3 Non-motion Patch Replacement (50%) (Same Video, Same Position) Counterfactual Motion Replacement (50%) (Motion Patch from Other Video) PatchPosition Shuffle (Content Preserved, Positions Shuffled) 55. 52.1 55.4 54.9 11.6 56.3 50. 83.1 79.4 54.9 50.6 57.2 53. 11.3 55.1 49.6 82.7 79.0 48. 41.8 46.3 45.1 8.7 48.2 42. 78.4 73.6 patch content while holding token count, spatiotemporal positions, positional encodings, and model parameters fixed. Patch Content Necessity. We first examine whether the content of codec-selected motion patches is necessary for the observed gains. In this setting, motion-heavy patches identified by the codec are replaced with non-motion patches sampled from the same video, while preserving their original spatiotemporal positions. As shown in Table 6, this intervention leads to substantial performance degradation across all benchmarks. The drop is particularly pronounced on motion-sensitive datasets, with accuracy decreasing, while appearance-dominated datasets such as Kinetics-400 exhibit smaller but consistent declines. These results indicate that the benefits of OV-Encoder (Codec) cannot be attributed to token sparsity or positional bias alone, but critically depend on the motion-centric content encoded by the selected patches. Semantic Specificity of Motion Cues. To further assess whether the model relies on semantically aligned motion rather than generic motion signals, we perform counterfactual replacement in which codec-selected motion patches are substituted with motion patches drawn from unrelated videos. Despite preserving motion magnitude and patch positions, this intervention results in even larger performance drops on fine-grained temporal benchmarks, as shown in Table 6. The consistent degradation relative to non-motion replacement demonstrates that OV-Encoder (Codec) is sensitive to the semantic correctness of motion cues, rather than merely benefiting from the presence of motion energy or stochastic perturbations. Negative Control: Patch-Position Shuffle. As sanity check, we additionally evaluate patch-position shuffle intervention, in which the visual content of codec-selected patches is preserved but their spatiotemporal positions are randomly permuted. This intervention causes substantially larger performance drop across all benchmarks, as shown in Table 6, confirming that coherent spatial and temporal alignment is critical for effective representation learning. Importantly, this experiment is not intended to demonstrate the causal role of codec guidance itself, but rather to rule out degenerate explanations in which patch content alone suffices without positional structure. Discussion. Taken together, these interventions establish consistent ordering across benchmarks: preserving semantically correct motion patches yields the strongest performance, followed by non-motion substitutions, while semantically mismatched motion patches and patch-position shuffling are most detrimental. This hierarchy rules out alternative explanations based on regularization, noise injection, or attention disruption, and provides converging evidence that codec-guided patch selection captures motion-centric visual evidence that is both semantically meaningful and structurally aligned with the underlying video content. 14 Figure 5 Visualization of Iand P-frame decomposition in HEVC. I-frames retain complete spatial structure, whereas P-frames encode motion-compensated residuals highlighting motion. Bright areas denote high residual magnitudes, while dark areas indicate static content."
        },
        {
            "title": "4.6 Qualitative Analysis",
            "content": "To understand how our model leverages temporal information for selective patch processing, we visualize the patch selection mechanism guided by residual magnitudes and motion vectors. As illustrated in Figure 5, we compare an I-frame (reference frame) with subsequent P-frames in video sequence, where patch colors indicate their selection priority based on residual and motion strength. In the I-frame (top-left), all patches are processed uniformly since no temporal prior exists. In contrast, the P-frames show selective emphasis on patches with large residuals and strong motion vectors, corresponding to regions with significant appearance changes or object movements. The highlighted patches, shown in warmer colors (red, orange, yellow), primarily correspond to dynamic foreground objects such as moving pedestrians, whereas static background regions (trees, buildings) appear in cooler colors (blue, green), indicating reduced computational focus. Across the sequence, the model consistently tracks salient motion regions, as pedestrians maintain high activation throughout their trajectories, demonstrating that the residualmotion criterion effectively identifies temporally informative patches. This visualization confirms that our approach achieves spatial selectivity by concentrating computation on motion-rich areas and allocating representational capacity according to temporal saliency, leading to more efficient video understanding. Figure 6 contrasts conventional frame-centric video processing with the proposed Codec patch extraction. While dense 64-frame inputs preserve full temporal context at high computational cost, uniform frame sampling reduces computation by sparsely selecting frames but inevitably discards fine-grained inter-frame motion, particularly for fast or subtle actions. Temporal saliency detection instead analyzes all frames to identify motionand event-centric regions. Leveraging this signal, Codec patch extraction selectively encodes only temporally salient patches using codec-inspired ordering, achieving substantial token reduction (75%96.9%) while preserving critical motion dynamics. This formulation decouples temporal coverage from token density, enabling efficient and scalable spatiotemporal modeling without reliance on sparse frame sampling. 15 Figure 6 Comparison of video processing pipelines for spatiotemporal representation learning. (a) original dense video input with full temporal context, (b) uniform frame sampling that sparsely selects evenly spaced frames, (c) temporal saliency detection that identifies motionand event-centric regions across all frames, and (d) Codec patch extraction that selectively retains temporally salient patches under fixed token budget."
        },
        {
            "title": "5.1 Advances in Visual Representation Learning",
            "content": "Vision Transformers (Dosovitskiy et al., 2021; Li et al., 2022a) have emerged as dominant approach in visual representation learning. DINOv2 (Jose et al., 2025) and DINOv3 (Siméoni et al., 2025) introduce self-supervised framework that does not require labels, achieving state-of-the-art results across wide range of vision tasks. Simultaneously, equivariant self-supervised methods (Devillers and Lefort, 2023; Park et al., 2022; Garrido et al., 2023; Gupta et al., 2024; Dangovski et al., 2111) have been developed to predict structured transformations consistent with group-theoretic principles. Masked image modeling techniques (He et al., 2021; Bao et al., 2021; El-Nouby et al., 2024; Xie et al., 2022) learn visual representations by reconstructing masked regions within the pixel domain. Furthermore, Joint-Embedding Predictive Architecture (Assran et al., 2023, 2025; Baevski et al., 2022) showed that predicting learned latent space instead of the pixel space leads to more powerful, higher-level features. Contrastive Language-Image Pretraining (CLIP) (Bolya et al., 2025; Sun et al., 2023; Li et al., 2023c; Fang et al., 2024; Xu et al., 2024; Schuhmann et al., 2022) aligns images and texts within shared embedding space through instance-level contrastive supervision. However, existing methods mainly focus on either pixel-level reconstruction or instance-level contrastive alignment, limiting their ability to capture structured semantic relationships across samples and modalities. In this work, we adopt the self-supervised cluster discrimination paradigm (e.g., UNICOM (An et al., 2023), MLCD (An et al., 2024), RICE (Xie et al., 2025), UniViT (Tang et al., 2025)), which learns structured semantics by jointly clustering similar instances and discriminating between clusters."
        },
        {
            "title": "5.3 Video Codec Compression",
            "content": "Video codec compression has been extensively studied, with large body of work devoted to designing efficient and effective video coding systems (Girod et al., 2005; Adami et al., 2007; Kumar, 2019). Classical standards such as H.264/AVC (Sullivan et al., 2012; Zhao and Liang, 2006) established the foundation of modern video compression by introducing key techniques including motion compensation, transform coding, and entropy coding, leading to substantial gains in compression efficiency. Building upon this framework, the High Efficiency Video Coding (HEVC) standard, also known as H.265 (Sullivan and Wiegand, 2005; Sullivan et al., 2012), further improved coding performance through more expressive block partitioning, enhanced motion modeling, and refined entropy coding mechanisms. Beyond traditional hand-crafted codecs, recent research has explored deep learning-based video compression methods (Li et al., 2021; Mentzer et al., 2022; Yang et al., 2021; Li et al., 2023b; Zhang et al., 2023; Wang et al., 2025), which employ neural networks to model spatial and temporal redundancies directly from data. These approaches have demonstrated promising compression performance and have also motivated perceptually driven coding strategies that allocate bits according to human visual sensitivity. Collectively, prior work in video compression has made significant progress toward compactly representing video content by exploiting its inherent temporal predictability and structural redundancy. In this work, we do not aim to improve compression efficiency or replace existing codecs; instead, we draw inspiration from the structural principles underlying modern video codecs, particularly their explicit decomposition of spatial context and temporal variation, to guide the design of spatiotemporal representations for visual transformers."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we introduced OV-Encoder, unified self-supervised vision framework that departs from frame-centric modeling and aligns representation learning with the predictive structure of video signals. By treating discriminative visual evidence as patch-level and motion-centric, OneVision-Encoder selectively 17 encodes informative regions while preserving dense temporal coverage under fixed token budgets. Central to this design is Codec Patchification, which constructs sparse yet structure-preserving spatiotemporal token layouts and naturally extends to chunk-wise temporal modeling and single-image inputs within unified attention-based encoder, supported by 3D rotary positional encoding. Combined with cluster discrimination objective that jointly models object-level and motion-level semantics without external supervision, OneVisionEncoder achieves state-of-the-art performance under both LMM probing and attentive probing. These results highlight codec-inspired patch-level sparsity as an effective and scalable foundation for general-purpose visual representation learning. Project Leaders Bo Li Ziyong Feng Ziwei Liu Zongyuan Ge Jiankang Deng"
        },
        {
            "title": "7 Contributors",
            "content": "Contributors core contributors are in bold Feilong Tang Xiang An Yunyao Yan Yin Xie Bin Qin Kaicheng Yang Yifei Shen Yuanhan Zhang Chunyuan Li Shikun Feng Changrui Chen Huajie Tan Ming Hu Manyuan Zhang"
        },
        {
            "title": "8 Implementation Details",
            "content": "Model Architecture and Configuration. OneVision-Encoder Large is implemented as Vision Transformer with 24 transformer layers, hidden dimension of 1024, and 16 attention heads. The model uses patch size of 14 14 and adopts GELU activations with Layer Normalization throughout the network. Attention computation is accelerated using Flash Attention 2, enabling efficient training and inference at scale. summary of the core architectural hyperparameters is provided in Table 7. Table 7 Architecture configuration of OneVision-Encoder Large. Component Setting Transformer layers Hidden dimension Attention heads Patch size MLP expansion ratio Position encoding 24 1024 16 14 14 4 3D RoPE (T:H:W = 4:6:6) Unified Patch-based Input Representation. All inputs are converted into patch tokens and processed by single ViT backbone. Images are treated as single-frame videos (T =1), while videos are represented in 5D tensor format. To ensure consistent temporal reasoning, all video inputs are mapped to virtual temporal grid of 64 frames, regardless of the actual number of frames processed. This mapping enables uniform temporal position encoding across dense, sparse, and Codec inputs. For inputs that do not cover all 64 frames explicitly, visible indices mechanism is used to associate each selected patch with its corresponding temporal position in the virtual grid. This design decouples temporal coverage from token density and allows sparse inputs to preserve long-range temporal structure. Codec-style Patch Selection. Codec-style processing operates on dense videos of 64 frames at spatial resolution of 224 224. Motion vectors and prediction residuals are extracted from the HEVC codec and used to estimate patch-level temporal saliency. Motion vectors capture object displacement at sub-pixel precision, while residuals encode fine-grained appearance changes not explained by motion compensation. These signals are fused into unified saliency score for each patch across all frames. Patches are ranked globally by their saliency scores, and only the top-K patches are retained. This selection typically preserves between 3.1% and 25% of all patches, corresponding to compression ratio of 75%96.9% relative to dense processing. Selected patches are then reassembled into compact video representation and passed to the ViT using sparse visible indices. Table 8 summarizes the codec-style selection procedure. Table 8 Codec-style patch selection pipeline. Description Decode HEVC motion vectors with camera motion compensation Step Motion extraction Residual extraction Obtain prediction residuals for fine-grained changes Energy fusion Top-K selection Sparse encoding Combine motion and residual energies into saliency score Retain globally most salient patches across all frames Process selected patches with sparse visible indices Video Processing Modes and Batch Composition. During pretraining, OneVision-Encoder employs mixedmodality batch that includes multiple video processing modes. This design exposes the model to diverse temporal structures and encourages robust representation learning. Video samples within batch are split into three processing modes: Codec patchification, uniform frame sampling, and Tiling-style spatial concatenation. All modes produce inputs that are compatible with the same ViT backbone and position encoding. Position Encoding Consistency. All input modes share the same 3D Rotary Position Embedding. The temporal 19 Table 9 Video processing modes used during training. Mode Batch Ratio Input Form Output Shape Codec Frame sampling Tiling 50% 37.5% 12.5% Dense video + saliency Uniform temporal bins Vertical frame concatenation [B, 3, 8, 224, 224] [B, 3, 8, 224, 224] [B, 3, 1792, 224] dimension of the RoPE encodes the position of each patch within the 64-frame virtual grid, while spatial dimensions encode patch row and column indices. For Codec inputs, patches may originate from arbitrary frames but are positioned correctly via their temporal indices. For uniformly sampled frames, temporal gaps are explicitly encoded. Tiling inputs are treated as single-frame inputs with fixed temporal positions. This unified encoding scheme enables the model to reason coherently over heterogeneous spatiotemporal layouts. Training and Inference Behavior. The model is trained using unified optimization pipeline across all modalities. No modality-specific parameters or task-specific encoders are introduced. At inference time, the same preprocessing logic is applied, and the model can flexibly switch between Codec sparse processing and conventional frame sampling depending on computational constraints. This design allows efficient deployment across wide range of image and video understanding tasks without architectural modification."
        },
        {
            "title": "9 Controlled Evaluation Pipeline",
            "content": "As shown in Figure 7, we adopt controlled evaluation protocol to compare OneVision-Encoder with Qwen3ViT and SigLIP2 under LMM probing. For comparison with Qwen3-ViT, OneVision-Encoder is first integrated with the Qwen3-1.7B language model and trained through Stage 1 and Stage 1.5 under the LLaVA-OneVision1.5 framework to adapt the encoder to native-resolution multimodal inputs. After alignment, the trained vision encoder is decoupled and evaluated under the same LLaVA-Next-Videos instruction-tuning setting as Qwen3-ViT, ensuring fair comparison under identical downstream supervision. For comparison with SigLIP2, all models are directly evaluated under identical multimodal fine-tuning conditions using unified 1.5M-scale instruction-tuning corpus, while keeping the language model backbone fixed. This decoupled and unified evaluation pipeline isolates the contribution of visual representation learning and avoids confounding effects from language model capacity, instruction data leakage, or differing alignment procedures. Figure 7 Controlled evaluation pipeline decoupling the encoder for fair comparison against Qwen3-ViT and SigLIP"
        },
        {
            "title": "10 Spatial Bias Analysis",
            "content": "As shown in Figure 8, the reported statistics are computed from random subset of 200,000 video samples drawn from the training data. When using codec-guided patch selection alone, the selected visual tokens exhibit pronounced spatial center bias, with the majority of tokens concentrated in the central regions of the 20 Figure 8 Spatial Bias Analysis of Selected Visual Patches under given setting. frame. This behavior reflects intrinsic statistical properties of video data: due to camera framing and subject placement, salient motion cues and residual signals are typically denser near the image center. While such bias enables the model to focus on regions with strong motion signals, it also leads to insufficient coverage of peripheral areas, thereby weakening the representation of global scene structure and fine-grained action cues. With the introduction of chunk-wise patchification, the spatial distribution of selected tokens becomes markedly more uniform, redistributing tokens toward the image periphery and boundary regions and effectively mitigating the center bias induced by codec-driven selection. This effect arises because chunk-wise sampling partitions the video along the temporal dimension and performs global selection over visible patch indices, resulting in more structurally balanced spatial coverage. Importantly, this rebalancing is achieved without increasing the token budget, but through principled reallocation of patch selection that enhances spatial diversity and complements motion-centric evidence with global contextual information."
        },
        {
            "title": "11 Token Allocation Case Study",
            "content": "For each case, we plot time (x-axis) versus accumulated visual tokens (y-axis) under fixed token budget. Stepwise growth indicates when method allocates non-trivial number of tokens to timestamp. Vertical rugs at the bottom show the actual timestamps observed by each strategy, and triangles denote I-frame anchors in the codec-style layout. Case 1 uses 2048-token budget (uniform 8-frame baseline), and Case 2 uses 4096-token budget (uniform 16-frame baseline). Detailed experiments and evaluation protocols for these two motion regimes will be reported in LLaVA-OneVision 2. 21 Figure 9 Case study 1 (Diving): dense evidence for continuous motion. Under fixed budget of 2048 tokens, codec-style patch extraction distributes tokens over 64 sampled frames to capture dense pose transitions, while uniform sampling allocates the same budget to only 8 frames and may miss brief but critical instants. Phases: P1 0.01.4s (takeoff + angular momentum), P2 1.43.4s (tuck/pike set; body shape), P3 3.411.6s (twist+somersault progression; dense cues), P4 11.614.2s (spot water + open/align), P5 14.215.0s (entry alignment; line/splash). Diving is high-speed and continuous, so discriminative cues are temporally dense and benefit from dense coverage."
        },
        {
            "title": "11.1 Case 1",
            "content": "We consider videos whose semantics are carried by smooth, persistent motion, where essentially every moment contributes evidence. In this regime, missing intermediate timestamps breaks trajectory continuity and harms recognition. Figure 9 visualizes this behavior on Diving example. Under the same fixed token budget of 2048 tokens, uniform 8-frame sampling concentrates all tokens on only eight sparse timestamps (leaving large temporal gaps), whereas codec-style allocation first samples 64-frame timeline and then reallocates tokens through saliency-guided P-frame patch selection, preserving dense motion evidence across time. This is particularly important for high-angular-velocity actions such as diving, where discriminative cues are temporally dense and distributed across many brief pose transitions rather than confined to single transition."
        },
        {
            "title": "11.2 Case 2",
            "content": "We consider videos whose semantics concentrate in short, discrete transitions, where the true key frames are sparse events such as brief pours, quick tool interactions, or sudden scene changes. In this regime, the primary risk of uniform sampling is misplacement: even with sufficient token budget, the 16 sampled frames can easily miss short segment entirely, causing decisive evidence to be absent from the visual input. Figure 10 illustrates this effect on 130-second cooking video. Notably, several pour segments are extremely short (e.g., P4: 38.039.0s), making them easy to skip under uniform sampling. In contrast, codec-style allocation first samples 64-frame timeline and then reallocates the same token budget toward high-saliency moments via P-frame patch selection, increasing the probability of capturing the decisive evidence that later multi-turn QA may depend on. 22 Figure 10 Case study 2 (Cooking): sparse key frames under instantaneous motion. We analyze 130-second cooking video where decisive evidence concentrates in short, discrete transitions (e.g., brief pours) rather than being uniformly distributed over time. Under fixed budget of 4096 tokens, codec-style allocation spreads tokens over 64-frame timeline and reallocates P-frame tokens toward high-saliency moments, while uniform sampling allocates the same budget to only 16 frames and may miss brief segments entirely (misplacement). Phases: P1 0.025.0s (context), P2 25.027.0s (pour raw materials), P3 28.037.0s (mix raw materials), P4 38.039.0s (pour raw materials), P5 40.045.0s (mix raw materials), P6 46.048.0s (pour raw materials), P7 49.051.0s (mix raw materials), P8 52.054.0s (pour raw materials), P9 55.071.0s (mix raw materials), P10 72.098.0s (pour raw materials), P11 98.0130.0s (context). 23 Figure 11 Comparison of video processing pipelines for spatiotemporal representation learning. (a) original dense video input with full temporal context, (b) uniform frame sampling that sparsely selects evenly spaced frames, (c) temporal saliency detection that identifies motionand event-centric regions across all frames, and (d) Codec patch extraction that selectively retains temporally salient patches under fixed token budget."
        },
        {
            "title": "References",
            "content": "Nicola Adami, Alberto Signoroni, and Riccardo Leonardi. State-of-the-art and trends in scalable video compression with wavelet-based approaches. IEEE Transactions on Circuits and Systems for Video Technology, 17(9):12381255, 2007. Xiang An, Jiankang Deng, Kaicheng Yang, Jaiwei Li, Ziyong Feng, Jia Guo, Jing Yang, and Tongliang Liu. Unicom: Universal and compact representation learning for image retrieval. In ICLR, 2023. Xiang An, Kaicheng Yang, Xiangzi Dai, Ziyong Feng, and Jiankang Deng. Multi-label cluster discrimination for visual representation learning. In ECCV, pages 428444. Springer, 2024. Xiang An, Yin Xie, Kaicheng Yang, Wenkang Zhang, Xiuwei Zhao, Zheng Cheng, Yirui Wang, Songcen Xu, Changrui Chen, Didi Zhu, et al. Llava-onevision-1.5: Fully open framework for democratized multimodal training. arXiv preprint arXiv:2509.23661, 2025. Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, and Nicolas Ballas. Self-supervised learning from images with joint-embedding. CVPR, 2023. Mido Assran, Adrien Bardes, David Fan, Quentin Garrido, Russell Howes, Matthew Muckley, Ammar Rizvi, Claire Roberts, Koustuv Sinha, Artem Zholus, et al. V-jepa 2: Self-supervised video models enable understanding, prediction and planning. arXiv:2506.09985, 2025. Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, and Michael Auli. Data2vec: general framework for self-supervised learning in speech, vision and language. arXiv:2202.03555, 2022. Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, Chunjiang Ge, Wenbin Ge, Zhifang Guo, Qidong Huang, Jie Huang, Fei Huang, Binyuan Hui, Shutong Jiang, Zhaohai Li, Mingsheng Li, Mei Li, Kaixin Li, Zicheng Lin, Junyang Lin, Xuejing Liu, Jiawei Liu, Chenglong Liu, Yang Liu, Dayiheng Liu, Shixuan Liu, Dunjie Lu, Ruilin Luo, Chenxu Lv, Rui Men, Lingchen Meng, Xuancheng Ren, Xingzhang Ren, Sibo Song, Yuchong Sun, Jun Tang, Jianhong Tu, Jianqiang Wan, Peng Wang, Pengfei Wang, Qiuyue Wang, Yuxuan Wang, Tianbao Xie, Yiheng Xu, Haiyang Xu, Jin Xu, Zhibo Yang, Mingkun Yang, Jianxin Yang, An Yang, Bowen Yu, Fei Zhang, Hang Zhang, Xi Zhang, Bo Zheng, Humen Zhong, Jingren Zhou, Fan Zhou, Jing Zhou, Yuanzhi Zhu, and Ke Zhu. Qwen3-vl technical report. arXiv preprint arXiv:2511.21631, 2025. Hangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv:2106.08254, 2021. Adrien Bardes, Quentin Garrido, Jean Ponce, Xinlei Chen, Michael Rabbat, Yann LeCun, Mido Assran, and Nicolas Ballas. V-jepa: Latent video prediction for visual representation learning. arXiv:2404.08471, 2023. Adrien Bardes, Quentin Garrido, Jean Ponce, Xinlei Chen, Michael Rabbat, Yann LeCun, Mahmoud Assran, and Nicolas Ballas. Revisiting feature prediction for learning visual representations from video, 2024. Daniel Bolya and Judy Hoffman. Token Merging for Fast Stable Diffusion. In CVPR Workshops, pages 45994603. IEEE, 2023. Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token Merging: Your ViT But Faster. In ICLR. OpenReview.net, 2023. Daniel Bolya, Po-Yao Huang, Peize Sun, Jang Hyun Cho, Andrea Madotto, Chen Wei, Tengyu Ma, Jiale Zhi, Jathushan Rajasegaran, Hanoona Rasheed, et al. Perception encoder: The best visual embeddings are not at the output of the network. arXiv:2504.13181, 2025. Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. Coyo-700m: Image-text pair dataset. https://github.com/kakaobrain/coyo-dataset, 2022. Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised learning of visual features. In ECCV, 2018. João Carreira, Dilara Gokay, Michael King, Chuhan Zhang, Ignacio Rocco, Aravindh Mahendran, Thomas Albert Keck, Joseph Heyward, Skanda Koppula, Etienne Pot, et al. Scaling 4d representations. arXiv:2412.15212, 2024. Wenhao Chai, Enxin Song, Yilun Du, Chenlin Meng, Vashisht Madhavan, Omer Bar-Tal, Jenq-Neng Hwang, Saining Xie, and Christopher Manning. Auroracap: Efficient, performant video detailed captioning and new benchmark. arXiv:2410.03051, 2024. 25 Lei Chen, Zhan Tong, Yibing Song, Gangshan Wu, and Limin Wang. Efficient Video Action Detection with Token Dropout and Context Refinement. In ICCV, pages 1035410365. IEEE, 2023. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv:2403.20330, 2024a. Tsai-Shien Chen et al. Panda-70m: Captioning 70m videos with multiple cross-modality teachers. CVPR, 2024b. Yung-Sung Chuang, Yang Li, Dong Wang, Ching-Feng Yeh, Kehan Lyu, Ramya Raghavendra, James Glass, Lifei Huang, Jason Weston, Luke Zettlemoyer, et al. Meta clip 2: worldwide scaling recipe. arXiv preprint arXiv:2507.22062, 2025. X.AI Corp. Grok-1.5 vision preview: Connecting the digital and physical worlds with our first multimodal model. https://x.ai/blog/grok-1.5v, 2024. Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Evangelos Kazakos, Jian Ma, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. Rescaling Egocentric Vision: Collection, Pipeline and Challenges for EPIC-KITCHENS-100. Int. J. Comput. Vis., 130(1):3355, 2022. Rumen Dangovski, Li Jing, Charlotte Loh, Seungwook Han, Akash Srivastava, Brian Cheung, Pulkit Agrawal, , and Marin Soljačić. Equivariant contrastive learning. arXiv:2111.00899, 2111. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In CVPR, 2009. Alexandre Devillers and Mathieu Lefort. Equimod: An equivariance module to improve self-supervised learning. arXiv:2211.01244, 2023. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv:2010.11929, 2020. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. Alaaeldin El-Nouby, Michal Klein, Shuangfei Zhai, Miguel Angel Bautista, Alexander Toshev, Vaishaal Shankar, Joshua Susskind, and Armand Joulin. Scalable pre-training of large autoregressive image models. arXiv:2401.08541, 2024. Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, and Vaishaal Shankar. Data filtering networks. In ICLR, 2024. Enrico Fini, Mustafa Shukor, Xiujun Li, Philipp Dufter, Michal Klein, David Haldimann, Sai Aitharaju, Victor Turrisi da Costa, Louis Béthune, Zhe Gan, et al. Multimodal autoregressive pre-training of large vision encoders. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 96419654, 2025. Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv:2306.13394, 2023. Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv:2405.21075, 2024. Quentin Garrido, Laurent Najman, and Yann Lecun. Self-supervised learning of split invariant equivariant. Proceedings of Machine Learning Research, pages 1097510996, 2023. B. Girod, A.M. Aaron, S. Rane, and D. Rebollo-Monedero. Distributed video coding. Proceedings of the IEEE, 93(1): 7183, 2005. Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzyńska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, Florian Hoppe, Christian Thurau, Ingo Bax, and Roland Memisevic. The \"something something\" video database for learning and evaluating visual common sense. In Proceedings of the IEEE international conference on computer vision, pages 58425850, 2017. Sharut Gupta, Joshua Robinson, Derek Lim, Soledad Villar, and Stefanie Jegelka. Structuring representation geometry with rotationally equivariant contrastive learning. In ICLR, 2024. 26 Tengda Han, Weidi Xie, and Andrew Zisserman. Turbo Training with Token Dropout. In BMVC, page 622. BMVA Press, 2022. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. arXiv:2111.06377, 2021. Cijo Jose, Théo Moutakanni, Dahyun Kang, Federico Baldassarre, Timothée Darcet, Hu Xu, Daniel Li, Marc Szafraniec, Michaël Ramamonjisoa, Maxime Oquab, Oriane Siméoni, Huy V. Vo, Patrick Labatut, and Piotr Bojanowski. DINOv2 meets text: unified framework for imageand pixel-level vision-language alignment. In CVPR, 2025. Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017. Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In ECCV, 2016. Wonkyun Kim, Changin Choi, Wonseok Lee, and Wonjong Rhee. An image grid can be worth video: Zero-shot video question answering using vlm. IEEE Access, 2024. Rajat Koner, Gagan Jain, Prateek Jain, Volker Tresp, and Sujoy Paul. Lookupvit: Compressing visual information to limited number of tokens. In ECCV (86), pages 322337. Springer, 2024. Hildegard Kuehne, Hueihan Jhuang, Estíbaliz Garrote, Tomaso Poggio, and Thomas Serre. Hmdb: large video database for human motion recognition. In ICCV, 2011. Dr Senthil Kumar. novel method for hdr video encoding, compression and quality evaluation. Journal of Innovative Image Processing, 1(2):7180, 2019. Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander Rush, Douwe Kiela, et al. Obelics: An open web-scale filtered dataset of interleaved image-text documents. NeurIPS, 2023. Seon-Ho Lee, Jue Wang, Zhikang Zhang, David Fan, and Xinyu Li. Video token merging for long-form video understanding. arXiv:2410.23782, 2024. Jiahao Li, Bin Li, and Yan Lu. Deep contextual video compression. NeurIPS, 34:1811418125, 2021. Kunchang Li, Yali Wang, Peng Gao, Guanglu Song, Yu Liu, Hongsheng Li, and Yu Qiao. Uniformer: Unified transformer for efficient spatiotemporal representation learning. arXiv:2201.04676, 2022a. Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Limin Wang, and Yu Qiao. Uniformerv2: Spatiotemporal learning by arming image vits with video uniformer. arXiv:2211.09552, 2022b. Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, Limin Wang, and Yu Qiao. Mvbench: comprehensive multi-modal video understanding benchmark, 2023a. Kunchang Li, Yali Wang, Yizhuo Li, Yi Wang, Yinan He, Limin Wang, and Yu Qiao. Unmasked teacher: Towards training-efficient video foundation models. In ICCV, 2023b. Xianhang Li, Zeyu Wang, and Cihang Xie. CLIPA-v2: Scaling CLIP training with 81.1% zero-shot imagenet accuracy within $10,000 budget; an extra $4,000 unlocks 81.8% accuracy. arXiv:2306.15658, 2023c. Yingwei Li, Yi Li, and Nuno Vasconcelos. Resound: Towards action recognition without representation bias. In Proceedings of the European conference on computer vision (ECCV), pages 513528, 2018. Youwei Liang, Chongjian Ge, Zhan Tong, Yibing Song, Jue Wang, and Pengtao Xie. Not All Patches are What You Need: Expediting Vision Transformers via Token Reorganizations. CoRR, abs/2202.07800, 2022. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024a. Yue Liu, Christos Matsoukas, Fredrik Strand, Hossein Azizpour, and Kevin Smith. PatchDropout: Economizing Vision Transformers Using Patch Dropout. In WACV, pages 39423951. IEEE, 2023. Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu, Lianwen Jin, and Xiang Bai. Ocrbench: on the hidden mystery of ocr in large multimodal models. Science China Information Sciences, 2024b. 27 Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. In ACL, 2022. Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In WACV, 2021. Minesh Mathew, Viraj Bagal, Rubèn Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In WACV, 2022. Fabian Mentzer, George Toderici, David Minnen, Sung-Jin Hwang, Sergi Caelles, Mario Lucic, and Eirikur Agustsson. Vct: video compression transformer. arXiv:2206.07307, 2022. Antoine Miech et al. Howto100m: Learning text-video embedding by watching hundred million narrated video clips. ICCV, 2019. Jung Yeon Park, Ondrej Biza, Linfeng Zhao, Jan Willem van de Meent, and Robin Walters. Learning symmetric embeddings for equivariant world models. arXiv:2204.11371, 2022. Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adria Recasens, Larisa Markeeva, Dylan Banarse, Skanda Koppula, joseph heyward, Mateusz Malinowski, Yi Yang, Carl Doersch, Tatiana Matejovicova, Yury Sulsky, Antoine Miech, Alexandre Fréchette, Hanna Klimczak, Raphael Koster, Junlin Zhang, Stephanie Winkler, Yusuf Aytar, Simon Osindero, Dima Damen, Andrew Zisserman, and Joao Carreira. Perception test: diagnostic benchmark for multimodal video models. In NeurIPS, pages 4274842761. Curran Associates, Inc., 2023. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021. Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, and Cho-Jui Hsieh. Dynamicvit: Efficient vision transformers with dynamic token sparsification. NeurIPS, 34:1393713949, 2021. Oshmita Sarkar, Satyam Sinha, Ajay Kumar Jena, Ajaya Kumar Parida, Nirupama Parida, and Raj Kumar Parida. Automatic number plate character recognition using paddle-ocr. In 2024 International Conference on Innovations and Challenges in Emerging Technologies (ICICET), pages 17. IEEE, 2024. Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv:2111.02114, 2021. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5b: An open large-scale dataset for training next generation image-text models. In NeurIPS Datasets and Benchmarks, 2022. Kazutoshi Shinoda, Nobukatsu Hojo, Kyosuke Nishida, Saki Mizuno, Keita Suzuki, Ryo Masumura, Hiroaki Sugiyama, and Kuniko Saito. Tomato: Verbalizing the mental states of role-playing llms for benchmarking theory of mind. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 15201528, 2025. Yan Shu, Zheng Liu, Peitian Zhang, Minghao Qin, Junjie Zhou, Zhengyang Liang, Tiejun Huang, and Bo Zhao. Video-xl: Extra-long vision language model for hour-scale video understanding. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2616026169, 2025. Gunnar Sigurdsson, Abhinav Gupta, Cordelia Schmid, Ali Farhadi, and Karteek Alahari. Charades-ego: large-scale dataset of paired third and first person videos. arXiv preprint arXiv:1804.09626, 2018. Oriane Siméoni, Huy V. Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Michaël Ramamonjisoa, Francisco Massa, Daniel Haziza, Luca Wehrstedt, Jianyuan Wang, Timothée Darcet, Théo Moutakanni, Leonel Sentana, Claire Roberts, Andrea Vedaldi, Jamie Tolan, John Brandt, Camille Couprie, Julien Mairal, Hervé Jégou, Patrick Labatut, and Piotr Bojanowski. DINOv3, 2025. Mattia Soldan, Fabian Caba Heilbron, Bernard Ghanem, Josef Sivic, and Bryan Russell. Residualvit for efficient temporally dense video encoding. In ICCV, pages 2230522315, 2025. Enxin Song, Wenhao Chai, Shusheng Yang, Ethan Armand, Xiaojun Shan, Haiyang Xu, Jianwen Xie, and Zhuowen Tu. Videonsa: Native sparse attention scales video understanding. arXiv:2510.02295, 2025. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 28 Gary Sullivan and Thomas Wiegand. Video compression-from concepts to the h. 264/avc standard. Proceedings of the IEEE, 93(1):1831, 2005. Gary Sullivan, Jens-Rainer Ohm, Woo-Jin Han, and Thomas Wiegand. Overview of the high efficiency video coding (hevc) standard. IEEE Transactions on circuits and systems for video technology, 22(12):16491668, 2012. Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. EVA-CLIP: Improved training techniques for clip at scale. arXiv:2303.15389, 2023. Ilya Sutskever. An observation on generalization. Talk at the Workshop on Large Language Models and Transformers, Simons Institute for the Theory of Computing, 2023. Accessed: 2023-XX-XX. Feilong Tang, Xiang An, Haolin Yang, Yin Xie, Kaicheng Yang, Ming Hu, Zheng Cheng, Xingyu Zhou, Zimin Ran, Imran Razzak, Ziyong Feng, Behzad Bozorgtabar, Jiankang Deng, and Zongyuan Ge. Univit: Unifying image and video understanding in one vision encoder. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, Olivier Hénaff, Jeremiah Harmsen, Andreas Steiner, and Xiaohua Zhai. SigLIP 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv:2502.14786, 2025. Chenting Wang, Kunchang Li, Tianxiang Jiang, Xiangyu Zeng, Yi Wang, and Limin Wang. Make your training flexible: Towards deployment-efficient video models. arXiv:2503.14237, 2025. Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, et al. Internvid: large-scale video-text dataset for multimodal understanding and generation. In ICLR, 2023. Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Zun Wang, Yansong Shi, Tianxiang Jiang, Songze Li, Jilan Xu, Hongjie Zhang, Yifei Huang, Yu Qiao, Yali Wang, and Limin Wang. InternVideo2: Scaling foundation models for multimodal video understanding. In ECCV, 2024. Yuetian Weng, Mingfei Han, Haoyu He, Xiaojun Chang, and Bohan Zhuang. LongVLM: Efficient Long Video Understanding via Large Language Models. In ECCV (33), pages 453470. Springer, 2024. Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding, 2024. Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to explaining temporal actions. In CVPR, pages 97779786, 2021. Chunyu Xie, Heng Cai, Jincheng Li, Fanjing Kong, Xiaoyu Wu, Jianfei Song, Henrique Morimitsu, Lin Yao, Dexin Wang, Xiangzheng Zhang, et al. Ccmb: large-scale chinese cross-modal benchmark. In Proceedings of the 31st ACM International Conference on Multimedia, pages 42194227, 2023. Yin Xie, Kaicheng Yang, Xiang An, Kun Wu, Yongle Zhao, Weimo Deng, Zimin Ran, Yumeng Wang, Ziyong Feng, Roy Miles, et al. Region-based cluster discrimination for visual representation learning. In ICCV, pages 17931803, 2025. Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: simple framework for masked image modeling, 2022. Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph Feichtenhofer. Demystifying clip data. In ICLR, 2024. Weili Xu, Enxin Song, Wenhao Chai, Xuexiang Wen, Tian Ye, and Gaoang Wang. Auroralong: Bringing rnns back to efficient open-ended video understanding. arXiv:2507.02591, 2025. Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun, Bei Liu, Huan Yang, Jianlong Fu, and Baining Guo. Advancing high-resolution video-language representation with large-scale video transcriptions. In International Conference on Computer Vision and Pattern Recognition (CVPR), 2022. Haolin Yang, Feilong Tang, Lingxiao Zhao, Xiang An, Ming Hu, Huifa Li, Xinlin Zhuang, Yifan Lu, Xiaofeng Zhang, Abdalla Swikir, et al. Streamagent: Towards anticipatory agents for streaming video understanding. arXiv:2508.01875, 2025. 29 Ren Yang, Fabian Mentzer, Luc Van Gool, and Radu Timofte. Learning for video compression with recurrent auto-encoder and recurrent probability model. IEEE Journal of Selected Topics in Signal Processing, 15(2):388401, 2021. Hongxu Yin, Arash Vahdat, José M. Álvarez, Arun Mallya, Jan Kautz, and Pavlo Molchanov. A-ViT: Adaptive Tokens for Efficient Vision Transformer. In CVPR, pages 1079910808. IEEE, 2022. Wang Zeng, Sheng Jin, Wentao Liu, Chen Qian, Ping Luo, Wanli Ouyang, and Xiaogang Wang. Not All Tokens Are Equal: Human-centric Visual Analysis via Token Clustering Transformer. In CVPR, pages 1109111101. IEEE, 2022. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In ICCV, 2023. Guozhen Zhang, Yuhan Zhu, Haonan Wang, Youxin Chen, Gangshan Wu, and Limin Wang. Extracting motion and appearance via inter-frame attention for efficient video frame interpolation. In CVPR, pages 56825692, 2023. Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Lmms-eval: Reality check on the evaluation of large multimodal models. In ACL, 2025. Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llava-next: strong zero-shot video understanding model, 2024. Zhuo Zhao and Ping Liang. highly efficient parallel algorithm for h. 264 video encoder. In 2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings, pages VV. IEEE, 2006. Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv:2406.04264, 2024. Xingyi Zhou, Anurag Arnab, Chen Sun, and Cordelia Schmid. How can objects help action recognition? In CVPR, pages 23532362. IEEE, 2023."
        }
    ],
    "affiliations": [
        "AIM for Health Lab",
        "Glint Lab",
        "MVP Lab"
    ]
}