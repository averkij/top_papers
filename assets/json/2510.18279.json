{
    "paper_title": "Text or Pixels? It Takes Half: On the Token Efficiency of Visual Text Inputs in Multimodal LLMs",
    "authors": [
        "Yanhong Li",
        "Zixuan Lan",
        "Jiawei Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) and their multimodal variants can now process visual inputs, including images of text. This raises an intriguing question: can we compress textual inputs by feeding them as images to reduce token usage while preserving performance? In this paper, we show that visual text representations are a practical and surprisingly effective form of input compression for decoder LLMs. We exploit the idea of rendering long text inputs as a single image and provide it directly to the model. This leads to dramatically reduced number of decoder tokens required, offering a new form of input compression. Through experiments on two distinct benchmarks RULER (long-context retrieval) and CNN/DailyMail (document summarization) we demonstrate that this text-as-image method yields substantial token savings (often nearly half) without degrading task performance."
        },
        {
            "title": "Start",
            "content": "Text or Pixels? It Takes Half: On the Token Efficiency of Visual Text Inputs in Multimodal LLMs Yanhong Li* Allen Institute for AI yanhongl@allenai.org Zixuan Lan University of Chicago zixuanlan@uchicago.edu Jiawei Zhou Stony Brook University jiawei.zhou.1@stonybrook.edu 5 2 0 2 2 ] . [ 2 9 7 2 8 1 . 0 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) and their multimodal variants can now process visual inputs, including images of text. This raises an intriguing question: can we compress textual inputs by feeding them as images to reduce token usage while preserving performance? In this paper, we show that visual text representations are practical and surprisingly effective form of input compression for decoder LLMs. We exploit the idea of rendering long text inputs as single image and provide it directly to the model. This leads to dramatically reduced number of decoder tokens required, offering new form of input compression. Through experiments on two distinct benchmarksRULER (long-context retrieval) and CNN/DailyMail (document summarization)we demonstrate that this text-as-image method yields substantial token savings (often nearly half) without degrading task performance."
        },
        {
            "title": "Introduction",
            "content": "Running large language model (LLM) inference on long text inputs is computationally expensive due to the underlying architecture of the Transformer model, where the self-attention mechanisms complexity scales quadratically with the input length (Vaswani et al., 2017). This can be prohibitive when processing long documents (Liu et al., 2024), interactive dialogues (Zhou et al., 2022), or complex multi-step reasoning (Feng et al., 2025). Even with recent increases in context length, deploying LLMs at scale (e.g., in chat assistants or document analysis) is constrained by throughput and cost per token (Chowdhery et al., 2022; Xiao et al., 2024; Xu et al., 2025a; Li et al., 2025a). Reducing the token length of inputs without losing information is therefore highly desirable for *These authors contributed equally to this work. 1Code available at yanhong-lbh/text_or_pixels. is https://github.com/ Figure 1: Illustration of our text-as-image compression pipeline. Instead of feeding the entire 90-token context to the model (top), we convert the context into single image and supply only the textual query alongside the image (bottom). The multimodal LLM (MLLM) reads the context from the image, so it processes just 50 visual tokens as input to the LLM decodercutting token usage by nearly half while still providing all the information needed to answer the question. improving LLM efficiency (Xu et al., 2025b; Tan et al., 2025; Xing et al., 2025; Li et al., 2025b). One novel avenue for input compression is to leverage the ability of multimodal LLMs (MLLMs) (Liu et al., 2023; Fang et al., 2024) to read text from images. The adage picture is worth thousand words hints that visual representations might convey the equivalent of many text tokens in compact form. Modern multimodal models like GPT-4 Vision (OpenAI, 2023) and Google Gemini 2.5 (Comanici et al., 2025) can accept images as part of their input and reason over them. This raises the question: Can we feed an LLM an image of text in lieu of the text itself, to save input tokens and still get the correct output? Early explorations (discussed in Section 2) suggest that multimodal LLMs can interpret text-based images, but the impact on efficiency and downstream performance remains under-examined. In this paper, we present an empirical study of multimodal LLMs that explores using visual text inputs as form of input compression. By rendering long passages as single image, the vision encoder produces compact set of visual tokens for the decoder, directly reducing sequence length without fine-tuning or supervision. On the RULER needle-in-a-haystack task, GPT-4.1-mini and Qwen2.5-VL-72B sustain 9799% accuracy with up to 58% fewer decoder tokens, and on CNN/DailyMail summarization, this approach outperforms two specialized pruning baselines at matched or higher compression rates. Although vision encoding adds some overhead on smaller models, the shorter decoder sequence yields up to 45% endto-end speedup on larger ones, demonstrating that off-the-shelf multimodal LLMs can treat images as an implicit compression layer, preserving performance at nearly half the original text-token cost."
        },
        {
            "title": "2 Related Work",
            "content": "Multimodal Variations of LLMs Recent advances in LLMs have extended their capabilities beyond text to handle images (Liu et al., 2023), speech (Wang et al., 2023), and video (Tang et al., 2025). In the visual domain, numerous visionlanguage models (VLMs) have been developed, including BLIP(-2) (Li et al., 2022, 2023a), Flamingo (Alayrac et al., 2022), LLaVA (Liu et al., 2023), InternVL (Chen et al., 2024), Qwen-VL (Wang et al., 2024a), PaLI-Gemma (Beyer et al., 2024), and proprietary systems such as GPT-4V (Achiam et al., 2023) and Gemini (Team et al., 2023). widely adopted architecture comprises vision encoder that extracts image features, projection layer that maps these features into the LLMs token space, and text decoder that jointly processes visual and textual tokens (Liu et al., 2023; Beyer et al., 2024; Wang et al., 2024a). Importantly, the number of visual tokens produced by this pipeline is usually smallconstrained by image size and model designand can be far fewer than the text tokens needed to represent the same information. This suggests that multimodal LLMs already embody form of token compression, motivating our exploration of representing rich texts directly as images to reduce decoder token usage. Text as Image Several works have explored the idea of providing text to LLMs via images. Lyu et al. (2025) proposed pixel-input benchmark, finding that some advanced VLMs can interpret and reason over text in images, though performance can drop without specialized training. Others, such as Lu et al. (2024), have investigated representing large documents as visual patches for handling extended context windows. While these efforts highlight the feasibility of text-as-image inputs, they have not systematically evaluated the trade-off between token usage and reasoning performance on multi-step tasks. Our work also relates to the concept of hybrid text-vision prompting, where instructions are given partially in text and partially as an image (Aggarwal and Welleck, 2025). However, prior studies focus more on the novelty of multimodal usage or coding from screenshots, rather than on compression. We instead emphasize the efficiency gains and cost savings arising from replacing sizeable chunk of text with an image for advanced reasoning tasks. Information Compression There are many works focusing on context compression (Pradeep et al., 2023; Xu et al., 2024; Jiang et al., 2024; Li et al., 2025b). Our approach is complementary to soft prompt line of work. Extreme RetrievalAugmented Generation (xRAG) replaces full documents with one dense embedding token, achieving 50 compression ratio without fine-tuning the LM (Cheng et al., 2024). Instruction-Aware Contextual Compression (IACC) filters noisy RAG passages based on the user query, halving context length while retaining QA accuracy (Hou et al., 2024). Prompt-centric surveys catalogue spectrum of hard pruning, abstractive summarization, and learned soft tokens that collectively reach 510 compression on reasoning benches (Li et al., 2025c; Jha et al., 2024). Broader reviews on extending LLM context windows emphasize that modality fusion and token dropping are orthogonal, and can be stacked for additive gains (Wang et al., 2024b; Li et al., 2024). Distinct from these token-level approaches, we compress entire text spans by offloading them to the vision encoder of an off-the-shelf multimodal LLMtreating thousands of tokens as single image, usually represented with fixed amounts of visual tokens or proportional to the image resolutionsand thus reduce context length to the the LLM decoder without any model finetuning. (a) GPT-4.1-mini (b) Qwen2.5-VL-72B-instruct Figure 2: Accuracy vs. text-token size (m) with fixed visual tokens (k) for GPT-4.1-mini (left) and Qwen2.5VL-72B-Instruct (right). Each curve varies the rendered text length at fixed k; vertical dashed lines mark = k, where text and visual tokens are equal. Both models degrade as text density increases, though Qwens larger decoder sustains higher ratios before sharper drop. The text-token tolerance (largest within 3 points of the text-only baseline) is shaded in the plots and reported in Table 1a. Beyond these limits, accuracy falls rapidly, revealing the maximum achievable compression without loss."
        },
        {
            "title": "3 Methodology",
            "content": "The corresponding token budget is 3.1 Problem Formulation Timg = k+q, with possible in practice. the context Let (e.g., document or multiturn dialogue) be sequence of text tokens, = (t1, t2, . . . , tm), and let the accompanying question be (usually short) sequence = (q1, q2, . . . , qq) . Text-only baseline. In the standard setting we feed the concatenated token sequence stext = (c, q) to text-only LLM. The input-length budget is therefore Ttext = + q. Textimage (hybrid) input. To compress long context, we first render it into an image via = R(cid:0)c(cid:1), using LaTeX-based typesetting pipeline that preserves the layout and line-breaks of the original text (see Figure 1). frozen vision encoder2 Φ : (cid:55) (v1, v2, . . . , vk), vj Rd, transforms the image into fixed-length sequence of visual tokens. These embeddings are passed through projection layer ψ (e.g., linear map) and become part of the language decoders input:3 simg = (cid:0)ψ(v1), . . . , ψ(vk), q(cid:1). 2For all experiments we use the native vision module shipped with each multimodal LLM. No fine-tuning or additional supervision is applied. 3The number of visual tokens passed to the language decoder depends on the VLM architecture: some models (e.g., LLaVA (Liu et al., 2023)) output fixed number, while others (e.g., Qwen-VL (Wang et al., 2024a)) vary with image sizes. In our experiments, for Qwen models, we define as the number of visual embeddings passed to the text decoder. For GPT-4.1-mini, we use the input token count returned by the API, which accounts for the visual input. Compression ratio. We define the compression ratio ρ = Ttext Timg = + + k when & q. higher ρ indicates greater token savings. 3.2 Evaluation Protocol For each example (c, q) we run both modes: (1) Text-only: Evaluate the LLM on stext to obtain answer atext; (2) Hybrid: Evaluate the multimodal LLM on simg to obtain answer aimg. We measure accuracy on task-specific metrics; token usage (Ttext, Timg) and thus ρ; and throughput and latency (wall-clock time per example). Unless stated otherwise, is fixed by the vision encoder, so any reduction in directly translates to lower LLM decoder token cost."
        },
        {
            "title": "4 Experiments and Results",
            "content": "We test to what extent visual inputs of texts can reduce discrete token consumption in LLM decoders without performance degradation. Long context tasks are especially targeted, including information retrieval and summarization, with configurable context lengths. We test two prominent multimodal LLMs, the proprietary GPT-4.1-mini (OpenAI, 2025) and the open-weight Qwen2.5-VL72B-Instruct (Bai et al., 2025).4 4Experiments on smaller model Qwen2.5-VL-7B are also presented in Figure 3 and Appendix B. All text-as-image rendering is performed with pdflatex followed by rasterization at 300 dpi. Algorithmic details can be found in Appendix D. During inference, we use the temperature=0 setting for deterministic outputs and truncate responses to the first newline to obtain concise answers. 4.1 Long-Context Retrieval Setup. We evaluate our text-as-image compression strategy on the RULER S-NIAH (single needle-in-a-haystack) task (Hsieh et al., 2024), where single target number (needle) is hidden in long distractor passage (haystack). The model must return the exact number, testing long-context retention. For each model, we generate 100 random passages and report accuracy (percentage of correct extractions). Since the query is short, the effective token budgets simplify to Ttext = and Timg = k, giving compression ratio ρ = m/k. How much can we compress? Figures 2 sweep while holding fixed. Accuracy remains stable until critical point m(k), after which it drops sharply. We define as the largest within three percentage points of the text-only baseline, referring to this threshold as the text-token tolerance. These values are highlighted in the plots (shaded) and reported in Table 1a. For example, at = 783, GPT-4.1-mini tolerates 1,300 tokensequivalent to ρ 1.9 compressionwithout measurable degradation. Larger visual budgets (k = 998, 1,258) increase tolerance to over 2,300 tokens while still saving 4258% of the decoder context.5 Qwen2.5-VL follows the same trend but exhibits steeper accuracy drop once is exceeded, underscoring that text-token tolerance is both modeland k-dependent. Token savings and Latency analysis. Table 1a collates (k, m) and confirms that hybrid prompts cut the decoder context nearly in half while matching text-only accuracy across both models.6 To better understand this relationship, Figure 3 plots the text-token tolerance as function of the visual token budget k. For all models tested, there is strong positive correlation: increases with at steady compression ratio ρ around 2. The larger Qwen2.5-VL-72B model consistently demonstrates superior compression ratio compared to both the 7B model and GPT-4.1-mini. The 5Some visual examples and details in Appendix A. 6Additional results on different long-context reasoning task with Gemini (Appendix C) also confirm this observation. Figure 3: Text-token tolerance vs. visual token count. The maximum text tokens that can be preserved without accuracy loss, plotted against the visual tokens generated from the image. Results show consistent reduction of roughly 1/2 in decoder tokens. approximately linear relationship suggests predictable trade-off between visual budget and text compression capacity. We next compare wall-clock generation time (Table 1b). For GPT-4.1-mini the vision processing adds modest < 1.5s overhead,7 whereas for Qwen2.5-VL the shorter sequence length outweighs that cost, yielding 2545% faster inference. Across accuracy and latency, converting long textual contexts into images yields substantial token savings without sacrificing RULER retrieval performance, demonstrating simple yet effective way to reduce inference cost on large-context tasks. 4.2 DocumentLevel Summarization While RULER stresses models ability to retrieve single item from long context, it is not expressly designed as compression benchmarkevery token in the haystack is, by construction, irrelevant to the answer. To gauge how well textas-image compression fares on genuine compression task, we turn to CNN/DailyMail longdocument summarization, where all input sentences may contribute to the final summary. Setup. We compare our approach against two widely used tokenpruning techniques that operate purely in the text modality: (1) SelectContext (Li et al., 2023b): keeps tokens whose self-information (estimated by small LLM) exceeds learned 7Not exact on model due to API overhead. See Apendix A. Model GPT QWEN TextImage (hybrid) Text-only Image size Acc. (%) m(k) Acc. (%) k/m(k) 600800 783 6001000 998 7501000 1,258 600800 635 6001000 782 7501000 998 99 97 98 98 99 98 1,272.4 1,752.5 2,352.2 1,289.6 1,769.7 2,369.4 100 100 100 100 100 100 0.61 0.57 0.53 0.49 0.44 0.42 (a) RULER accuracy and token statistics. is the visual token count, m(k) the maximum text-token tolerance, and k/m(k) the relative token footprint. Text-image input reduces the decoder tokens by 3858%. Model GPT QWEN Timeimg (s) Timetext (s) 783 998 1,258 635 782 1.29 1.75 1.95 2.81 3.04 3.35 0.60 0.61 0.58 3.61 4.16 5.09 (b) End-to-end latency. Hybrid inputs add modest overhead for GPT but reduce total time for Qwen thanks to smaller decoder contexts. measured from API responses; see Appendix A. Table 1: RULER S-NIAH long-context retrieval accuracy, text-as-image compression statistics, and model latency. GPT refers to GPT-4.1-mini and QWEN denotes Qwen2.5-VL-72B-Instruct. Model Method Remaining BERTScore ROUGE-L ROUGE-1 ROUGE-2 ROUGE-Lsum GPT QWEN Baseline (text-only) Text-as-image (ours) Select-Context LLMLingua-2 Baseline (text-only) Text-as-image (ours) Select-Context LLMLingua-2 m=693 225 (67%) 295 (57%) 265 (62%) m=726 279 (62%) 181 (75%) 258 (64%) 86.25 85.33 85.01 85.25 86.37 85.64 84.60 85.46 16.26 15.31 12.79 13.75 17.70 15.53 13.40 15. 23.78 21.98 18.82 20.42 25.18 23.28 20.36 22.95 8.60 7.40 5.30 7.00 9.47 7.54 5.13 7.09 18.91 17.75 15.71 16.60 20.77 19.16 17.63 18.94 Table 2: CNN/DailyMail document summarization with token compression baselines. GPT refers to GPT-4.1-mini and QWEN denotes Qwen2.5-VL-72B-Instruct. is the uncompressed text length, the remaining decoder tokens after compression. Percentage shows token reduction relative to m. At similar compression rates, rendering the document as an image beats both token-level baselines on every metric. threshold. (2) LLMLingua-2 (Pan et al., 2024): trains Transformer encoder to predict, token by token, whether to retain or discard. to learned token-selection approaches. Future work could further combine text token pruning before rendering, stacking the benefits of both paradigms. We compute the average input length in CNN/- DailyMail and use it to set the image resolution in our text-to-image pipeline, yielding visual token inputs at roughly half the original context lengththe optimal ratio identified in previous section. This image size is applied consistently across the task. For fair comparison, baselines are configured with the same decoder token compression ratio. Summary quality is evaluated with ROUGE (Lin, 2004) and BERTSCORE (Zhang et al., 2020). Results and Discussion. The evidence in Table 2 shows that, even though textimage compression was not tailored for summarization, it yields stronger summaries than two specialized pruning methods while retaining only 40% of the original tokens. We stress that the aim of this paper is to characterize the compression capacity of visual inputs, not to introduce yet another SOTA compression model. Nevertheless, the unexpectedly strong results suggest that our simple rendering trick is competitiveand orthogonalalternative"
        },
        {
            "title": "5 Conclusion",
            "content": "Our primary goal is to answer the question: How many tokens can be saved by replacing text with an image without harming downstream performance? By converting long contexts into visual form, we achieved nearly two-fold reductions in decoder token count while preserving task accuracy on both retrieval (RULER S-NIAH) and generation (CNN/DailyMail summarization) benchmarks. The approach is modeland task-agnostic, requires no parameter updates, and can even lower latency on large decoders. Our findings suggest several directions for future work: (i) applying token-level pruning before visual rendering to further compound compression gains; and (ii) expanding the approach to other domains (e.g. math) where most prompt tokens are critical and thus difficult to prune at the token level. We hope this study will spark broader exploration of modality shifting as complementary axis for scaling the usability and efficiency of Large language models."
        },
        {
            "title": "Limitations",
            "content": "Despite showing promising token savings on short to medium context scenarios, our work has not yet fully evaluated the impact of text-as-image prompting on extremely large contexts that span tens of thousands of tokens or more. Real-world applications such as document analysis or in-depth conversational histories may require specialized approaches (e.g., chunking, retrieval) to ensure reliable performance at these larger scales. Furthermore, our experiments focus on limited number of benchmarks, leaving open questions about performance on other domains (e.g., medical, legal) and tasks (e.g., coding, translation)."
        },
        {
            "title": "Acknowledgment",
            "content": "We thank the Google Gemma Academic Program for their partial support of Jiawei Zhou and for providing computational resources."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, and 1 others. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Pranjal Aggarwal and Sean Welleck. 2025. Programming with pixels: Computer-use meets software engineering. Preprint, arXiv:2502.18525. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, and 1 others. 2022. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:23716 23736. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, and 1 others. 2025. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923. Lucas Beyer, Andreas Steiner, André Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, and 1 others. 2024. Paligemma: versatile 3b vlm for transfer. arXiv preprint arXiv:2407.07726. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, and 1 others. 2024. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2418524198. Xin Cheng, Xun Wang, Xingxing Zhang, Tao Ge, SiQing Chen, Furu Wei, Huishuai Zhang, and Dongyan Zhao. 2024. xrag: Extreme context compression for retrieval-augmented generation with one token. In Advances in Neural Information Processing Systems (NeurIPS). Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, and 48 others. 2022. Palm: Scaling language modeling with pathways. Preprint, arXiv:2204.02311. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, and 1 others. 2025. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. Preprint, arXiv:2507.06261. Yixiong Fang, Ziran Yang, Zhaorun Chen, Zhuokai Zhao, and Jiawei Zhou. 2024. From uncertainty to trust: Enhancing reliability in vision-language models with uncertainty-guided dropout decoding. arXiv preprint arXiv:2412.06474. Yiyang Feng, Yichen Wang, Shaobo Cui, Boi Faltings, Mina Lee, and Jiawei Zhou. 2025. Unraveling misinformation propagation in llm reasoning. arXiv preprint arXiv:2505.18555. Haowen Hou, Fei Ma, Binwen Bai, Xinxin Zhu, and Fei Yu. 2024. Enhancing and accelerating large language models via instruction-aware contextual compression. arXiv preprint arXiv:2408.15491. Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris Ginsburg. 2024. RULER: Whats the real context size of your long-context language models? In First Conference on Language Modeling. Siddharth Jha, Lutfi Eren Erdogan, Sehoon Kim, Kurt Keutzer, and Amir Gholami. 2024. Characterizing prompt compression methods for long context inference. arXiv preprint arXiv:2407.08892. Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2024. LongLLMLingua: Accelerating and enhancing LLMs in long context scenarios via prompt compression. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16581677, Bangkok, Thailand. Association for Computational Linguistics. Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Ivan Rodkin, Dmitry Sorokin, Artyom Sorokin, and Mikhail Burtsev. 2024. Babilong: Testing the limits of llms with long context reasoning-in-a-haystack. Preprint, arXiv:2406.10149. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023a. Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR. Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022. Blip: Bootstrapping language-image pretraining for unified vision-language understanding and generation. In International conference on machine learning, pages 1288812900. PMLR. Yanhong Li, Karen Livescu, and Jiawei Zhou. 2025a. In The ThirChunk-distilled language modeling. teenth International Conference on Learning Representations. Yanhong Li, David Yunis, David McAllester, and Jiawei Zhou. 2025b. Context-efficient retrieval with factual decomposition. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers), pages 178194, Albuquerque, New Mexico. Association for Computational Linguistics. Yucheng Li, Bo Dong, Frank Guerin, and Chenghua Lin. 2023b. Compressing context to enhance inference In Proceedefficiency of large language models. ings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 63426353, Singapore. Association for Computational Linguistics. Zhuowan Li, Cheng Li, Mingyang Zhang, Qiaozhu Mei, and Michael Bendersky. 2024. Retrieval augmented generation or long-context LLMs? comprehensive study and hybrid approach. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 881 893, Miami, Florida, US. Association for Computational Linguistics. Zongqian Li, Yixuan Su, and Nigel Collier. 2025c. 500xCompressor: Generalized prompt compression for large language models. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2508125091, Vienna, Austria. Association for Computational Linguistics. Chin-Yew Lin. 2004. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 7481, Barcelona, Spain. Association for Computational Linguistics. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual instruction tuning. Advances in neural information processing systems, 36:34892 34916. Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157173. Yujie Lu, Xiujun Li, Tsu-Jui Fu, Miguel Eckstein, and William Yang Wang. 2024. From text to pixel: Advancing long-context understanding in mllms. Preprint, arXiv:2405.14213. Zhiheng Lyu, Xueguang Ma, and Wenhu Chen. 2025. Pixelworld: Towards perceiving everything as pixels. Preprint, arXiv:2501.19339. OpenAI. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. OpenAI. 2025. Gpt-4.1-mini. https://platform. Acopenai.com/docs/models/gpt-4.1-mini. cessed: 2025. Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue Zhang, Qingwei Lin, Victor Rühle, Yuqing Yang, Chin-Yew Lin, H. Vicky Zhao, Lili Qiu, and Dongmei Zhang. 2024. LLMLingua-2: Data distillation for efficient and faithful task-agnostic prompt compression. In Findings of the Association for Computational Linguistics: ACL 2024, pages 963981, Bangkok, Thailand. Association for Computational Linguistics. Ronak Pradeep, Sahel Sharifymoghaddam, and Jimmy Lin. 2023. RankVicuna: Zero-shot listwise document reranking with open-source large language models. arXiv:2309.15088. Xudong Tan, Peng Ye, Chongjun Tu, Jianjian Cao, Yaoxin Yang, Lin Zhang, Dongzhan Zhou, and Tao Chen. 2025. Tokencarve: Information-preserving visual token compression in multimodal large language models. Preprint, arXiv:2503.10501. Yunlong Tang, Jing Bi, Siting Xu, Luchuan Song, Susan Liang, Teng Wang, Daoan Zhang, Jie An, Jingyang Lin, Rongyi Zhu, and 1 others. 2025. Video understanding with large language models: survey. IEEE Transactions on Circuits and Systems for Video Technology. Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, and 1 others. 2023. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc. Mingqiu Wang, Wei Han, Izhak Shafran, Zelin Wu, Chung-Cheng Chiu, Yuan Cao, Nanxin Chen, Yu Zhang, Hagen Soltau, Paul Rubenstein, and 1 others. 2023. Slm: Bridge the thin gap between speech and text foundation models. In 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 18. IEEE. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, and 1 others. 2024a. Qwen2vl: Enhancing vision-language models perception arXiv preprint of the world at any resolution. arXiv:2409.12191. Xindi Wang, Mahsa Salmani, Parsa Omidi, Xiangyu Ren, Mehdi Rezagholizadeh, and Armaghan Eshaghi. 2024b. Beyond the limits: survey of techniques to extend the context length in large language models. Preprint, arXiv:2402.02244. Guangxuan Xiao, Jiaming Tang, Jingwei Zuo, Junxian Guo, Shang Yang, Haotian Tang, Yao Fu, and Song Han. 2024. Duoattention: Efficient long-context llm inference with retrieval and streaming heads. Preprint, arXiv:2410.10819. Ling Xing, Alex Jinpeng Wang, Rui Yan, Xiangbo Shu, and Jinhui Tang. 2025. Vision-centric token compression in large language model. arXiv preprint arXiv:2502.00791. Fangyuan Xu, Tanya Goyal, and Eunsol Choi. 2025a. RefreshKV: Updating small KV cache during longIn Proceedings of the 63rd Anform generation. nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 24878 24893, Vienna, Austria. Association for Computational Linguistics. Fangyuan Xu, Tanya Goyal, and Eunsol Choi. 2025b. Refreshkv: Updating small kv cache during longform generation. Preprint, arXiv:2411.05787. Fangyuan Xu, Weijia Shi, and Eunsol Choi. 2024. RECOMP: Improving retrieval-augmented LMs with context compression and selective augmentation. In The Twelfth International Conference on Learning Representations. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: EvalIn International uating text generation with bert. Conference on Learning Representations. Jiawei Zhou, Jason Eisner, Michael Newman, Emmanouil Antonios Platanios, and Sam Thomson. 2022. Online semantic parsing for latency reduction in task-oriented dialogue. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15541576."
        },
        {
            "title": "Image Sizes on RULER",
            "content": "A central challenge is how to balance visual and textual tokens under fixed sequence length. In many patch-based vision encoders used by VLMs such as Qwen, higher image resolution produces more patches, and thus more visual tokens, which offsets efficiency benefits of text-as-image processing for token reduction. Conversely, lowering resolution reduces token usage, but risks discarding semantically important visual details. This tradeoff becomes especially critical in long-context settings such as retrieval-style reasoning or the RULER benchmark, where even small shifts in token allocation can lead to large changes in accuracy. Our experiments demonstrate that model accuracy is highly sensitive to the effective resolution of image inputs, which directly controls the number of visual tokens generated. We observe consistent trend across benchmarks: when visual tokens account for roughly one half of the total context window, performance remains nearly indistinguishable from that achieved with the original, uncompressed textual context. This 1 2 allocation emerges as near-optimal operating point, striking balance between preserving visual fidelity and maintaining sufficient capacity for textual information. The finding aligns with broader evidence of redundancy in long-context models, where moderate compression often preserves accuracy despite substantial reductions in token count. To illustrate, Figure 57 present representative cases from the RULER needle-in-a-haystack task: at context length of 1500 with an image resolution of 600 800 pixels (Figure 5), at 2000 with 600 1000 pixels (Figure 6), and at 2500 with 7501000 pixels (Figure 7). In all three cases, the model successfully recovers nearly all embedded image information, achieving accuracy close to 100%. In contrast, when the context length is extended to 3000 tokens with 600 1000 image (Figure 8), accuracy degrades substantially despite the increase in available tokens. This illustrates that recognition accuracy depends not only on the absolute context length but also on preserving balanced allocation between textual and visual tokens. Simply enlarging the context window is therefore insufficient to ensure stable performance. Collectively, these results indicate that tuning image resolution relative to the available context budget is critical for multimodal reasoning. The observed 1 2 allocation (or compression ratio ρ of 2) emerges as practical heuristic for balancing efficiency and fidelity, though further validation is required to assess its robustness across tasks, datasets, and model architectures. Inference Latency Analysis To measure the inference latency of text-only input to the LLM decoder and text-image hybrid input to the full multimodal model (shown in Table 1b), we run the full RULER S-NIAH test set and report the average wall-clock time per sample. No batching is used. For the proprietary GPT-4.1-mini, latency is measured from API response times. For Qwen2.5VL-72B-Instruct which is open sourced, inference is performed on 8 Nvidia RTX A6000 GPUs using the standard Hugging Face implementation without batching, vLLM, or other speedups. Note that API response times for GPT-4.1-mini may not reflect pure model latency, as they include system overhead such as request queuing, routing, network latency, and data transmission. Since image payloads are larger than text, this overhead is likely greater for text-as-image inputs, partially explaining the higher latency reported in Table 1b. Results on Qwen2.5-VL-7B-Instruct To understand the role of model scale in visual text compression, we replicate the long-context retrieval experiment from Section 4.1 on the much smaller Qwen2.5-VL-7B-Instruct model.8 The results, plotted in Figure 4, show that while the general trend of performance degradation holds, the 7B model is significantly more sensitive to text density than its 72B counterpart. The text-token tolerance of the 7B model is substantially lower across all corresponding visual token budgets (k). For instance, with = 998 visual tokens, the 72B model maintains over 97% accuracy up to nearly 2,400 text tokens, whereas the 7B models accuracy drops below 95% after only 2,000 text tokens. This highlights that larger model scale provides greater capacity to robustly process and reason over densely packed textual information presented visually, making it critical factor for achieving high compression ratios without performance loss. 8https://huggingface.co/Qwen/Qwen2. 5-VL-7B-Instruct. provides additional evidence that balancing image and text tokens at near-equal ratio offers practical operating point for multimodal long-context reasoning. ConTexImage: Text-to-Image"
        },
        {
            "title": "Conversion Pipeline",
            "content": "To standardize our experiments, we require consistent way to convert textual sequences into image inputs of controlled resolution. We therefore design lightweight text-to-image pipeline, which we term ConTexImage (Algorithm 1). ConTexImage renders arbitrary text into rasterized images while automatically adjusting font size to achieve target content density. This ensures that the generated images preserve readability, maintain consistent tokenization patterns, and remain comparable across different resolutions. The pipeline consists of three main stages: 1. Preprocessing: Input text is normalized by replacing typographic symbols (e.g., curly quotes, dashes, ellipses) and escaping LaTeX special characters. This step guarantees compatibility with the rendering backend. 2. LaTeX Rendering: The cleaned text is embedded into minimal LATEX document and compiled using tectonic9. The output PDF page is rasterized into an image at specified DPI and then resized to the target resolution (e.g., 600 800, 600 1000). 3. Adaptive Font Optimization: To maximize visual fidelity, the algorithm searches over candidate font sizes and evaluates the proportion of the image occupied by text (fill ratio). The largest font size that meets pre-defined target fill ratio (default 0.8) is selected, ensuring both legibility and balanced whitespace. The resulting images are consistent across different contexts and allow us to precisely control the number of image tokens relative to text tokens. More details are illustrated in Algorithm 1. 9https://github.com/tectonic-typesetting/ tectonic. Figure 4: Qwen2.5-VL-7B-Instruct: accuracy vs. text-token size (m) with fixed visual tokens (k). Each curve varies the amount of rendered text for fixed visual token size. Compared to the 72B version (Figure 2b), the smaller 7B model exhibits much steeper performance degradation as text density increases, indicating that model scale is critical factor for effective visual text processing. BABILong 1k Benchmark To further validate our findings, we evaluate on the BABILONG benchmark (Kuratov et al., 2024), which is specifically designed to test the limits of long-context reasoning in LLMs. The benchmark extends the classical bAbI tasks to much longer contexts, where the relevant information must be retrieved from sequences containing up to 1k distractor tokens. This makes it well suited for assessing whether multimodal token allocation strategies remain effective in long-context scenarios. We follow the standard setup of the BABILONG 1k variant, which consists of ten subsets (QA1QA10) covering range of reasoning tasks such as singleand multi-supporting fact retrieval, coreference, and induction. For this experiment, we use gemini-2.5-flash-preview-04-17 (Comanici et al., 2025). Since Gemini is closed-source, we estimate visual token usage from the input/output token statistics returned by the API. Specifically, we record both text and image token counts for each query, enabling us to analyze how token allocation is distributed between modalities. Table 3 shows that across QA1QA10, performance is stable when image tokens constitute roughly half the count of the original total context text tokens. In this configuration, the model achieves strong and well-aligned accuracies for both text (0.91) and image (0.83), confirming that the 1 2 allocation rule observed in the RULER experiments generalizes to BABILong as well. This Task TxtAcc ImgAcc TxtIn TxtOut ImgIn ImgOut QA1 QA2 QA3 QA4 QA5 QA6 QA7 QA8 QA9 QA10 Avg 1.00 0.85 0.87 1.00 0.92 0.99 0.54 0.99 1.00 0.98 0.91 0.96 0.65 0.59 0.98 0.89 0.96 0.48 0.95 0.96 0. 0.83 846.9 877.0 946.2 810.5 881.0 829.0 863.1 862.7 827.7 875.1 861.9 9.0 7.0 10.0 1.1 1.0 1.0 1.0 1.1 1.0 1.0 3.3 440.0 466.0 531.0 399.0 465.4 430.0 451.0 468.0 425.0 466. 454.1 9.1 7.0 10.0 1.1 1.0 1.0 1.0 1.1 1.0 1.0 3.3 Table 3: Gemini-2.5-flash-preview-04-17 on BABILONG 1k. TxtAcc is accuracy for text input. ImgAcc is accuracy for image input. TxtIn and TxtOut are the average text input/output tokens. ImgIn and ImgOut are the average image input/output tokens. Algorithm 1: ConTexImage: Context-Aware Text-to-Image Pipeline def escape_latex_special_chars(text): \"\"\"Replace LaTeX special characters with safe tokens.\"\"\" escape_map = {\"\": r\"textbackslash{}\", \"&\": r\"&\", \"%\": r\"%\", \"$\": r\"$\", \"#\": r\"#\", \"_\": r\"_\", \"{\": r\"{\", \"}\": r\"}\", \"\": r\"textasciitilde{}\", \"^\": r\"textasciicircum{}\"} for k, in escape_map.items(): text = text.replace(k, v) return text def text_to_image(text, width, height, dpi=300, target_fill_ratio=0.7): # Step 1: Preprocessing text = normalize_typography(text) text = escape_latex_special_chars(text) # replace curly quotes, dashes, ellipses # escape special symbols for LaTeX # Step 2: Font size search best_image, best_ratio = None, 0 for font_size in candidate_font_sizes(descending=True): tex_doc = build_latex_template(text, font_size, width, height, margin=10) pdf = compile_with_tectonic(tex_doc) image = convert_pdf_to_image(pdf, dpi=dpi, # LaTeX -> PDF resize=(width, height)) # PDF -> raster ratio = calculate_fill_ratio(image) # bounding box occupancy if ratio > best_ratio: # update best so far best_ratio, best_image = ratio, image if ratio >= target_fill_ratio: break return best_image def generate_images(dataset, output_dir, width, height, dpi=300): \"\"\"Batch conversion for all documents in dataset.\"\"\" for doc in dataset: text = doc[\"input\"] img = text_to_image(text, width, height, dpi=dpi) save_image(img, path=output_dir + f\"/{doc[doc_id]}.png\") Figure 5: Rendered image input for the RULER task at context length 1500 (600 800 image resolution). Here there is almost no accuracy degradation. This example illustrates how textual sequences are converted into rasterized images for multimodal processing. Figure 6: Rendered image input at context length 2000 (600 1000 image resolution). Here there is almost no accuracy degradation. The figure demonstrates scaling of resolution while preserving readability and model performance. Figure 7: Rendered image input at context length 2500 (750 1000 image resolution). Here there is almost no accuracy degradation. Increased resolution produces more visual tokens while maintaining comparable visual fidelity and model performance. Figure 8: Rendered image input at context length 3000 (600 1000 image resolution). Here accuracy degrades substantially. This setting illustrates the tradeoff between tolerable text token budget and image resolution to maintain model performance."
        }
    ],
    "affiliations": [
        "Allen Institute for AI",
        "Stony Brook University",
        "University of Chicago"
    ]
}