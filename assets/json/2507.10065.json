{
    "paper_title": "MoVieS: Motion-Aware 4D Dynamic View Synthesis in One Second",
    "authors": [
        "Chenguo Lin",
        "Yuchen Lin",
        "Panwang Pan",
        "Yifan Yu",
        "Honglei Yan",
        "Katerina Fragkiadaki",
        "Yadong Mu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present MoVieS, a novel feed-forward model that synthesizes 4D dynamic novel views from monocular videos in one second. MoVieS represents dynamic 3D scenes using pixel-aligned grids of Gaussian primitives, explicitly supervising their time-varying motion. This allows, for the first time, the unified modeling of appearance, geometry and motion, and enables view synthesis, reconstruction and 3D point tracking within a single learning-based framework. By bridging novel view synthesis with dynamic geometry reconstruction, MoVieS enables large-scale training on diverse datasets with minimal dependence on task-specific supervision. As a result, it also naturally supports a wide range of zero-shot applications, such as scene flow estimation and moving object segmentation. Extensive experiments validate the effectiveness and efficiency of MoVieS across multiple tasks, achieving competitive performance while offering several orders of magnitude speedups."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 5 6 0 0 1 . 7 0 5 2 : r MoVieS: Motion-Aware 4D Dynamic View Synthesis in One Second Chenguo Lin1, Yuchen Lin1,3, Panwang Pan2, Yifan Yu2, Honglei Yan2, Katerina Fragkiadaki3, Yadong Mu1 1Peking University, 2ByteDance, 3Carnegie Mellon University https://chenguolin.github.io/projects/MoVieS"
        },
        {
            "title": "Abstract",
            "content": "We present MoVieS, novel feed-forward model that synthesizes 4D dynamic novel views from monocular videos in one second. MoVieS represents dynamic 3D scenes using pixel-aligned grids of Gaussian primitives, explicitly supervising their time-varying motion. This allows, for the first time, the unified modeling of appearance, geometry and motion, and enables view synthesis, reconstruction and 3D point tracking within single learning-based framework. By bridging novel view synthesis with dynamic geometry reconstruction, MoVieS enables large-scale training on diverse datasets with minimal dependence on task-specific supervision. As result, it also naturally supports wide range of zero-shot applications, such as scene flow estimation and moving object segmentation. Extensive experiments validate the effectiveness and efficiency of MoVieS across multiple tasks, achieving competitive performance while offering several orders of magnitude speedups."
        },
        {
            "title": "Introduction",
            "content": "Humans and animals perceive continuous stream of observations from dynamic three-dimensional world, and effortlessly interpret its underlying 3D geometry and motion. Replicating this capability is essential for any embodied agent that must understand and act in the physical world. Recent advances have made great strides in individual 3D tasks, such as monocular depth estimation [1, 2, 3, 4, 5], 3D scene reconstruction [6, 7, 8, 9, 10], novel view synthesis [11, 12, 13, 14, 15] and point tracking [16, 17, 18, 19, 20]. However, besides treating each task in isolation, most existing view synthesis and reconstruction studies focus on static scenes and require costly per-scene optimization without learning prior knowledge. Real-world environments are inherently dynamic and diverse, and all the aforementioned 3D scene understanding tasks could share common underlying principles. Motivated by this, we introduce MoVieS, Motion-aware dynamic View Synthesis model for feedforward 4D reconstruction of videos in the wild, that jointly models scenes appearance, geometry and motion. MoVieS represents 3D dynamic scenes using renderable and deformable 3D particles, termed dynamic splatter pixels, and utilizes differentiable 3D Gaussian rendering framework [13]. Specifically, following recent practices on feed-forward view synthesis [21, 22, 23, 24, 25], each input pixel is mapped to 3D Gaussian primitive, with its 3D location determined by predicted depth. To model dynamics, MoVieS regresses per-pixel motion displacements toward arbitrary query timestamps, enabling temporal tracking of each splatter pixel. This design facilitates coherent reconstruction of both 3D geometry and appearance across camera viewpoints and temporal frames. As shown in Figure 1, MoVieS is build upon large-scale pretrained transformer backbone [26, 10], which encodes each video frame independently and aggregates their information via attentions [27]. The aggregated features are then processed by specialized prediction heads: (1) depth head estimates depth for each input frame, (2) splatter head predicts per-pixel 3D Gaussian [13] : Equal contribution; : Project lead; : Corresponding author. Preprint. Figure 1: Overview. MoVieS consists of shared image encoder, an attention-based feature backbone (Section 3.2.1), and three heads (Section 3.2.2) that simultaneously predict appearance, geometry and motion. Image shortcut for splatter head and time-varying Gaussian attributes are omitted for brevity. appearance attributes, such as color and opacity, for novel view rendering, (3) motion head estimates the time-conditioned movements of Gaussian primitives towards target timestamp, allowing us to track its temporal evolution. Benefiting from the unified architecture, MoVieS can be trained on large-scale datasets featuring both static [28, 29, 30] and dynamic [31, 32] scenes, as well as point tracking datasets [33, 34, 35]. At inference time, MoVieS takes monocular video, whether depicting static or dynamic scene, and reconstructs per-pixel 3D Gaussian primitives along with their motion attributes at any target timestamp, enabling novel view synthesis, depth estimation, and 3D point tracking in single model. Extensive experiments on diverse benchmarks [28, 36, 37, 38] show that MoVieS achieves competitive performance across variety of 4D perception tasks, while being several orders of magnitude faster than the existing state of the art. Furthermore, empowered by novel view synthesis as proxy task, MoVieS enables dense motion understanding from sparse tracking supervision. This naturally gives rise to variety of zero-shot applications, further broadening the potential of our approach. In summary, our main contributions include: We introduce MoVieS, the first feed-forward framework that jointly models appearance, geometry and motion for 4D scene perception from monocular videos. Dynamic splatter pixels are proposed to represent dynamic 3D scenes as renderable deforming 3D particles, bridging novel view synthesis and dynamic geometry reconstruction. MoVieS delivers strong performance and orders of magnitude speedups for 4D reconstruction, and naturally enables wide range of applications in zero-shot manner."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Feed-forward 3D Reconstruction Most traditional 3D reconstruction methods require dense multi-view supervision and per-scene optimization from scratch at test time [39, 40, 41, 42, 6, 11, 13]. Recent learning-based approaches leverage large-scale data priors to enable feed-forward 3D reconstruction that directly regresses geometric structures such as camera poses and depth [43, 44, 1, 45, 4, 3, 46]. With the advent of differentiable rendering techniques like 3D Gaussian Splatting (3DGS) [13], several works propose to predict pixel-aligned Gaussian attributes for novel view synthesis [21, 47, 23, 48, 24]. We build on these advances and extend them to dynamic 3DGS representations for novel timestamps. DUSt3R [8] pioneered direct regression of pixel-aligned pointmaps in canonical space from image pairs. Subsequent works improved this framework by incorporating feature matching [49], supporting multi-view inputs [50, 51, 52, 53], adapting to streaming video [54, 55], or bridging view synthesis with 3DGS [56, 57, 52, 53]. VGGT [10] further advances this line by using stronger image encoder [26] and multiple task-specific heads to predict all key 3D properties from as few as one to hundreds of views. We adopt VGGT as the backbone of MoVieS for its strong visual representations. However, all these methods are restricted to static scenes and overlook dynamic environments with moving objectsthe main focus of our work. 2 2.2 Dynamic Geometry Reconstruction Compared to static scene reconstruction, which has been studied for decades, dynamic scene reconstruction remains relatively underexplored. Some works extend the DUSt3R framework to dynamic settings in plug-and-play manner [58], or leverage foundation models [9, 59, 60] by incorporating monocular depth [4, 61], optical flow [62], or 2D point tracking [19]. Dynamic Point Map [63], concurrent work, proposes time-invariant DUSt3R variant by predicting each viewpoints pointmap at another timestamp. The most related work, DynaDUSt3R [35], adds pixel-level motion supervision to model dynamic geometry. However, all these methods are limited to two-frame inputs and produce only point clouds, without high-quality visual reconstructions. To go beyond two views, CUT3R [64] uses recurrent model to incrementally update scene representations from streaming video for static and dynamic scenes. Other methods [65, 66] treat dynamic reconstruction as conditional video generation, fine-tuning diffusion models to estimate pointmaps probabilistically. However, these require multiple network passes per video. 2.3 Dynamic Novel View Synthesis Instead of geometric structure, dynamic novel view synthesis emphasizes visual results and viewdependent effects. Early attempts utilize neural radiance field [11, 12, 67] to represent dynamic scenes, either from multi-view videos [68, 69, 70], or monocular ones [71, 72, 73, 74]. 3DGS [13], as more efficient renderable representation, has been applied for dynamic view synthesis using 4D primitives with time dimension [75, 76] or an additional deformable field [14, 15, 77, 78, 79]. Leveraging the low-dimensional nature of scene motion, several works [80, 81, 82, 83, 84] express motion explicitly and learn Gaussian trajectories across monocular frames. However, these methods are trained from scratch and require iterative optimization for scene adaptation. Off-the-shelf point tracking [17, 19] or optical flow [62] models are also essential to provide supervision signals. The most related works to ours are BTimer [85] and NutWorld [86], both of which utilize feedforward approach to estimate 3DGS attributes from monocular videos. However, BTimer predicts independent Gaussian chunks per timestamp without modeling frame relations and needs an enhancer module for smooth intermediate frames. NutWorld models Gaussian motion but lacks explicit supervision, relying heavily on pretrained depth [5] and flow [87], and uses an orthographic camera, which may further lead to projection distortions."
        },
        {
            "title": "3 Method",
            "content": "Following previous studies on monocular 4D reconstruction [74, 80, 79, 83, 85], given posed video i=1 with frames Ii R3HW , camera poses Pi SE(3), intrinsics Ki R33 := {Ii, Pi, Ki}N and timestamps ti [0, 1], we aim to train generic model that jointly reconstruct the underlying appearance, geometry and motion of dynamic scenes. 3.1 Dynamic Splatter Pixel To model 3D scenes with moving contents, we propose novel representation, namely dynamic splatter pixel, which decomposes dynamic scenes into set of static Gaussian primitives and their corresponding deformation fields. Given an input video V, each pixel in the i-th frame Ii is associated with splatter pixel gi [21, 22, 23] in shared canonical space of the first frame cameras coordinate system. Each := {x, a} is parameterized by its position R3 in the canonical space and other rendering attributes R11, including rotation quaternion R4, scale R3, opacity α R, and color crgb R3 [13]. Considering splatter pixels were originally designed for static scenes, we decouple motion from geometric structure to adapt them for dynamic scenes. An additional time-dependent deformation field is introduced, in which each splatter pixel is associated with m(t) := {x(t), a(t)}. x(t) R3 is the motion vector of splatter pixel at time with respect to the canonical 3D space, and a(t) is the change of the corresponding attributes of the splatter pixel at time t. Therefore, the splatter pixel is deformed to time as: + x(t), (1) By combining static splatter pixels and their deformation fields, we establish the correspondence between each Gaussian primitive and its temporal dynamics, thereby enabling dynamic scene modeling and dense motion estimation. In practice, we find that making only the scale and rotation attributes time-dependent, i.e., a(t) R7, is sufficient for representing dynamic splatter pixels. + a(t). 3 Figure 2: Motion Head. Given tq target timesteps, the proposed motion head is conditioned via adaptive layer normalization (AdaLN) and predicts 3D displacements for each input pixel. After rasterization using the corresponding query-time cameras, output images in shape 3H are rendered for supervision. Gaussian attribute deformation is omitted for brevity. 3.2 MoVieS: Unify Appearance, Geometry and Motion As illustrated in Figure 1, the framework of the proposed MoVieS provides unified approach to simultaneously model appearance, geometry and motion of dynamic scenes. It consists of feature backbone with camera and time information for extracting features from input video frames, followed by dedicated heads for depth, splatter and motion estimation. 3.2.1 Feature Backbone Given an input posed video = {Ii, Pi, Ki, ti}N i=1, we first patchify the input images Ii and use pretrained image encoder [26] to extract their features. To effectively incorporate camera information, we adopt two complementary strategies to embed the camera parameters into image features: (1) Plücker embedding: camera pose Pi and intrinsics Ki are encoded into pixel-aligned Plücker embeddings [88], which are then downsampled and fused with image features via spatial-wise addition; (2) Camera token: both Pi and Ki are passed through linear layer to generate camera token, which is appended to the sequence of image tokens. Plücker embedding provides dense and spatially aligned encoding of camera geometry, while the camera token interacts globally with image tokens through attention, enabling more holistic feature reasoning. Ablation study on these two camera injection manners is validated in Section 4.4. To inform the model that input images originate from temporally ordered video, we additionally encode each timestamp ti [0, 1] using sinusoidal positional encoding [11] to produce time token, which is then concatenated with the aforementioned image and camera tokens. After tokenizing input images, camera parameters, and timestamps, we apply geometrically pretrained attention blocks [27] from VGGT [10] to enable interactions among image tokens across video frames. This produces set of shared feature tokens enriched with inter-frame context as well as camera and temporal information, which are then used for predicting various properties of dynamic scenes. 3.2.2 Prediction Heads Shared aggregated video tokens from the feature backbone are fed into three parallel prediction heads that estimate the appearance, geometry, and motion of the dynamic scene, respectively. Each head adopts DPT-style architecture [89] to convert image tokens into dense predictions matching the input resolution, thereby producing dynamic splatter pixels. Depth and Splatter Head Different from previous feed-forward 3DGS reconstruction methods [21, 47, 23] that use single head to predict all splatter pixel attributes, we adopt decoupled design to better leverage geometric priors from the pretrained VGGT [10]. Specifically, dedicated depth head, initialized from VGGT, is used for geometry prediction to provide spatial grounding for splatter pixel construction, while separate DPT as splatter head is trained from scratch for appearance rendering. We further incorporate direct RGB shortcut [57] from the input image to the final convolution layer of the splatter head to preserve high-frequency details and enhance color fidelity. Motion Head To capture scene dynamics, novel motion head as shown in Figure 2 is introduced to predict dense deformation m(t) for each dynamic splatter pixel at any target moment. Temporal variation is enabled by injecting the sinusoidally encoded query time tq into the image tokens via 4 Table 1: Training Datasets. Nine datasets from diverse sources are utilized to train MoVieS at scale. #Repeat denotes dataset duplication count during integration to balance their contributions. Dataset Dynamic? Depth? Tracking? Real? #Scenes #Frames #Repeat RealEstate10K [28] TartanAir [29] MatrixCity [30] PointOdyssey [33] DynamicReplica [34] Spring [31] VKITTI2 [32] Stereo4D [35] 70K 0.4K 4.5K 0.1K 0.5K 0.03K 0.1K 98K 6.36M 0.49M 0.31M 0.18M 0.26M 0.003M 0.03M 19.6M 1 100 10 1000 100 2000 500 1 adaptive layer normalization [90] before applying DPT convolutions. For each input frame Ii at time ti, the motion head predicts its 3D movements toward tq in shared world coordinate system. Thus, for input frames and query timestamps, this yields motion map of shape 3 . 3.3 Training 3.3.1 Dataset Curation An ideal dataset for dynamic scene reconstruction would include synchronized multi-view videos with dense depth and point tracking annotations. However, such data is infeasible to capture and annotate at scale in practice. Instead, we leverage diverse set of open-source datasets [28, 29, 30, 31, 32, 33, 34, 35], each providing complementary supervision, as shown in Table 1. With the flexible model design, MoVieS can be jointly trained on these heterogeneous sources by aligning objectives to their respective annotations. More details about data curation are provided in Appendix A. 3.3.2 Objectives MoVieS is trained by multi-task objective that combines depth, rendering and motion losses: := λdLdepth + λrLrendering + λmLmotion. (2) Depth and Rendering Losses Depth loss is computed as the mean squared error (MSE) between the predicted and ground truth depth maps, along with their spatial gradients, after filtering out invalid values. Rendering loss combines pixel-wise MSE and perceptual loss [91] between 3DGS-rendered images under corresponding camera views and the video frames at target timestamps. Motion Loss Given 3D point tracking datasets, ground-truth motion is defined as the 3D displacement of each tracked point between any two frames. Since all 3D points are defined in the world coordinate and most tracked points remain static, implying that their corresponding motion vectors tend to zero. We apply point-wise L1 loss between the predicted and ground-truth motions to promote sparsity after filtering out points that are not visible in the input frames. Additionally, to complement direct point-to-point alignment, distribution loss is introduced that encourages the predicted motion vectors to preserve the internal relative distance structure within each frame. The final motion loss is defined as combination of point-wise and distribution-level supervision: Lmotion := λptLpt + λdistLdist (cid:88) = 1 iΩ λptˆxi xi1 + 1 2 (cid:88) (i,j)ΩΩ λdistˆxi ˆx xi 1, (3) (4) where Ω denotes the set of all valid tracked points, and Ω Ω is its Cartesian product. Ablation study on the effectiveness of these two types of motion supervision is presented in Section 4.4. Normalization Similar to VGGT [10], we normalize the 3D scene scale by the average Euclidean distance from each 3D point to the origin of the canonical world coordinate system. As result, unlike some other reconstruction methods [8, 49, 64], we do not apply additional normalization in the depth or motion loss. We also omit confidence-aware weighting [8, 10] for simplicity."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Settings Implementation MoVieS is built on geometrically pretrained transformer, VGGT [10], with splatter head and camera/time embeddings trained from scratch. AdamW [92] with cosine learning rate scheduling and linear warm-up is used for optimization. We observed that training MoVieS is particularly unstable, likely arising from sparse annotations and the heterogeneous nature of the training data. curriculum strategy is employed that gradually increases the training complexity, involving (1) pretraining on static scenes, (2) dynamic scenes with varying views, and (3) fine-tuning on high resolution. Several techniques, such as gsplat [93] rendering backend, DeepSpeed [94], gradient checkpointing [95], gradient accumulation, and bf16 mixed precision, are adopted to improve memory and computation efficiency. Training completes in approximately 5 days using 32 H20 GPUs. Please refer to Appendix for more details about implementations. Evaluation We evaluate the capabilities of MoVieS on two primary tasks: novel view synthesis (Section 4.2) and 3D point tracking (Section 4.3). Following prior works [21, 47, 24, 80, 81, 83, 85], RealEstate10K [28] is used to evaluate novel view synthesis performance on static scenes. DyCheck [37] and NVIDIA dynamic scene dataset [36] are adopted for dynamic scenes, in terms of PSNR, SSIM and LPIPS [91]. Since the DyCheck dataset contains invisible regions in the target novel views, we compute reconstruction metrics (denoted by prefix m) using the provided covisibility masks, ensuring fair comparison across methods. For 3D point tracking, the TAPVid-3D [38] benchmark serves as the evaluation protocol, covering both indoor and outdoor scenes across three datasets. End-point Euclidean error in the 3D space (EPE3D) and the percentage of 3D points within 0.05 and 0.10 units of the ground-truth locations (δ0.05 3D ) are utilized as metrics. 3D and δ0.10 4.2 Novel View Synthesis 4.2.1 Static Scene View Synthesis As special case of dynamic scenes, we first evaluate MoVieS on static dataset RealEstate10K [28], comparing against several state-of-the-art feed-forward static scene reconstruction methods, including DepthSplat [24], our reimplementation of GS-LRM [23], and our method pretrained solely on the first-stage static reconstruction. As shown in Table 2, although MoVieS is primarily designed for dynamic scenes, it maintains competitive performance on static scenes. Notably, when processing static inputs, our predicted motion naturally converges to zero, demonstrating the ability of MoVieS to implicitly differentiate between static and dynamic regions without explicit supervision. 4.2.2 Dynamic Scene View Synthesis Dynamic scene view synthesis is evaluated on two benchmarks: (1) DyCheck [37], with 7 daily videos captured by 3 synchronized cameras, and (2) NVIDIA [36], containing 9 dynamic scenes recorded by 12-camera static rig. For DyCheck, we sample 13 frames of size 518 518 from random clip from iPhone-record videos, and include the other two cameras for evaluation. For NVIDIA, 12 input views at 379 672 resolution are sampled in round-robin style [96, 97, 85], where the i-th frame comes from the i-th camera at timestep i. We compare against three state-of-the-art optimization-based methods [80, 81, 83] for dynamic 3D Gaussian reconstruction, and also report static feed-forward baselines for reference. As shown in Table 2, MoVieS achieves competitive or superior performance compared to baselines, while requiring only 0.93s per scene, which is orders of magnitude faster than prior approaches that rely on heavy pretrained models and complex multi-stage pipelines. Qualitative visualizations in Figure 3 further highlight the strength of our approach. While MoSca [83] demonstrates impressive performance, it struggles with sparse inputs and often overfits to seen poses, producing spiky and over-saturated artifacts under novel views and timesteps. In contrast, MoVieS leverages large-scale learned priors to generalize more effectively, yielding smoother and more realistic results. To ensure fair comparison and better reflect real-world scenarios, no video masks for dynamic objects are used in our experiments. It poses major challenge for optimization-based methods like Shape-of-Motion [80] and Splatter-a-Video [81], which rely heavily on explicit motion segmentation. The challenge is particularly evident on the NVIDIA dataset, where significant camera shake hinders disentangling scene dynamics, leading to notably degraded performance and even worse than static baselines. In contrast, our method exhibits strong robustness by directly learning to model motion. 6 Figure 3: Novel View Synthesis for Dynamic Scenes. Given monocular video, we compare synthesized views from novel view across different methods. Regions invisible in the input are rendered as black or white, depending on the rendering implementation. More results in Figure 6. Table 2: Evaluation on Novel View Synthesis. The best , second best and third best results are highlighted for clarity. indicates our reimplemented version of GS-LRM [23]. Novel View Synthesis Time Per Scene RealEstate10K DyCheck NVIDIA PSNR SSIM LPIPS mPSNR mSSIM mLPIPS PSNR SSIM LPIPS Static Feed-forward DepthSplat [24] GS-LRM [23] Ours (static) Optimization-based Splatter-a-Video [81] Shape-of-Motion [80] MoSca [83] Ours 0.60s 0.57s 0.84s 37min 10min 45min 26. 26.94 27.60 80.06 79.13 81.25 0. 0.139 0.113 - - - - - - - - - 0.93s 26.98 81.75 0.111 13.83 14.60 15. 13.61 17.96 18.24 18.46 0.4364 0. 0.4784 0.3131 0.5662 0.5514 0.3850 0. 0.3783 0.5706 0.3463 0.3698 17.16 17. 18.73 14.39 15.30 21.45 0.5002 0. 0.4988 0.3265 0.5042 0.2959 0.2538 0. 0.3169 0.5087 0.7123 0.2653 0.5887 0. 19.16 0.5141 0.3152 4.3 3D Point Tracking Trained on large-scale point tracking datasets [33, 34, 35], the proposed method can also densely track any 3D point corresponding to pixel across video frames (see Figure 7 in the appendix). We compare MoVieS against three strong baselines, including two state-of-the-art 2D point tracking methods, BootsTAP [98] and CoTracker3 [19], as well as native 3D point tracking approach, SpatialTracker [20]. For 2D trackers, recent video depth estimation model [5] and ground-truth camera intrinsics are used to unproject tracked points into 3D space. To account for scale differences across methods, we normalize all predicted 3D points by their median norm before evaluation. Quantitative results are reported in Table 3. While 3D-based SpatialTracker generally outperforms 2D-based approaches, all of them rely heavily on pretrained monocular depth estimators for geometry reasoning, introducing significant noise and inconsistency in the 3D space. In contrast, MoVieS directly estimates 3D point positions in shared world coordinate, enabling more accurate and robust 3D tracking, and achieves consistently superior or competitive performance across all datasets. 4.4 Ablation and Analysis Camera conditioning We investigate different camera conditioning strategies in the static pertaining stage. Quantitative comparisons are provided in Table 4. Camera tokens are injected throughout the feature backbone, enabling effective camera-aware modeling. In contrast, Plücker embeddings 7 Table 3: Evaluation on 3D Point Tracking. The best , second best and third best results are highlighted for clarity. denotes 3D points obtained by combining depth estimation model [5]. 3D Point Tracking BootsTAPIR [98] CoTracker3 [19] Aria Digital Twin DriveTrack Panoptic Studio EPE3D δ0.05 3D δ0.10 3D EPE3D δ0.05 3D δ0.10 3D EPE3D δ0.05 3D δ0.10 3D 0.5539 17.73% 32.97% 0.0617 55.82% 75.66% 0.0650 69.28% 87.95% 0.5614 19.88% 35.82% 0.0637 55.30% 77.55% 0.0617 69.27% 88.04% SpatialTracker [20] 0.5413 18.08% 38.23% 0.0648 56.58% 80.67% 0.0519 72.91% 89.86% Ours 0.2153 52.05% 71.63% 0.0472 60.63% 79.87% 0.0352 87.88% 94.61% Figure 4: Motion Visualization for Ablation Studies. We investigate key factors affecting motion learning in MoVieS, such as loss design and synergy with view synthesis. XYZ values in motion maps are normalized as RGB for visualization. Red arrows on video frames indicate motion directions. provide limited conditioning on their own and are merely comparable to having no camera information at all. However, as pixel-aligned representation, Plücker embeddings are complementary to camera tokens, and their combination yields the most effective camera conditioning. Motion Supervision To learn object 3D movements in dynamic scenes, we provide two kinds of motion supervision: (1) point-wise L1 loss and (2) distribution loss, as described in Equation 4. Their efficiency is evaluated via the 3D point tracking task in Table 5. Without any motion supervision, i.e., learning solely from novel view synthesis, training exhibits severe loss oscillations and frequent None gradients. The distribution loss captures only relative motion between pixels, while point-wise L1 loss produces more reasonable motion maps. Combining both leads to sharper boundaries. Qualitative results of estimated motions from different motion objectives are provided in Figure 4. Synergy of Motion and View Synthesis Thanks to the unified design of MoVieS, it supports simultaneous novel view synthesis (NVS) and motion estimation. We study their mutual benefits in Table 6. NVS w/o motion disables explicit motion supervision during training, relying solely on NVS as proxy to learn dynamics. As shown in Table 6 and Figure 4, this setting fails to learn meaningful motion and tends to model static scenes. Motion w/o NVS detaches the motion head from 3DGS rendering and instead conditions the depth head on time. Although explicit supervision enables some motion learning, the predictions are blurry and low-quality, as shown in Figure 4. Moreover, the depth head must now model both geometry and dynamics, increasing its burden and negatively affecting NVS. These results highlight the mutual reinforcement between NVS and motion estimation in MoVieS, where joint training leads to better performance on both. 4.5 Zero-shot Applications Scene Flow Estimation Scene flow can be naturally derived by transforming the estimated per-pixel motion vectors from world coordinates to the target cameras coordinates. Visualization results in Figure 5(a) present sharp edges and accurate motion directions. More results are provided in Figure 8. Moving Object Segmentation By thresholding the norm of per-pixel motion vectors, the estimated motion maps can be used to segment moving objects (Figure 5(b)), which is an essential task in computer vision and robotics [99, 100]. Remarkably, this is achieved without any explicit mask supervision, demonstrating the strong potential of our method. More results are provided in Figure 9. 8 Table 4: Ablation study on camera conditioning. Table 5: Ablation study on motion supervision. Camera Conditioning RealEstate10K PSNR SSIM LPIPS Motion Supervision Aria Digital Twin EPE3D δ0.05 3D δ0.10 3D N/A + Plücker embedding + Camera token Ours (static) 25.56 25.81 26.81 27.60 74. 74.44 78.65 81.25 0.150 0.143 0. 0.113 N/A + Point-wise L1 + Distribution loss Ours 0. 0.2262 0.2496 0.2153 19.58% 32.86% 48.74% 69.93% 45.98% 66.87% 52.05% 71.63% Table 6: Ablation Study on the synergy of motion estimation and novel view synthesis (NVS). Motion & View Synthesis DyCheck NVIDIA Aria Digital Twin mPSNR mSSIM mLPIPS PSNR SSIM LPIPS EPE3D δ0.05 3D δ0.10 3D NVS w/o motion Motion w/o NVS Ours 15.82 16.26 18.46 0. 0.4556 0.5887 0.3741 0.3461 0.3094 18. 18.98 19.16 0.4856 0.3010 0.7938 19.58% 32.86% 0.4939 0.3207 0.3801 24.72% 42.92% 0.5041 0. 0.2153 52.05% 71.63% Figure 5: Zero-shot Applications. The predicted pixel-aligned motion maps from our model can be directly applied to downstream tasks, such as (a) scene flow estimation and (b) moving object segmentation, in zero-shot manner, without any task-specific fine-tuning or supervision."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we introduce MoVieS, feed-forward model for dynamic novel view synthesis from monocular videos. Its trained on large-scale datasets from diverse sources and jointly models scene appearance, geometry and motion in unified and efficient network. Dynamic splatter pixels are proposed to represent dynamic scenes, enabling accurate and temporally coherent 4D reconstruction. Beyond novel view synthesis, MoVieS supports broad range of applications including depth estimation, 3D point tracking, scene flow estimation and moving object segmentation, demonstrating its versatility for dynamic scene perception. We hope this work could serve as step toward generalizable dynamic scene understanding, and support applications that require spatial and motion intelligence. More discussions on limitations, future work and broader impact are included in Appendix B."
        },
        {
            "title": "References",
            "content": "[1] René Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. Transactions on Pattern Analysis and Machine Intelligence (T-PAMI), 2020. [2] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth estimation. In Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [3] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. Advances in Neural Information Processing Systems (NeurIPS), 2024. [4] Aleksei Bochkovskii, AmaÃG, Delaunoy, Hugo Germain, Marcel Santos, Yichao Zhou, Stephan Richter, and Vladlen Koltun. Depth pro: Sharp monocular metric depth in less than second. In International Conference on Learning Representations (ICLR), 2024. [5] Sili Chen, Hengkai Guo, Shengnan Zhu, Feihu Zhang, Zilong Huang, Jiashi Feng, and Bingyi Kang. Video depth anything: Consistent depth estimation for super-long videos. In Conference on Computer Vision and Pattern Recognition (CVPR), 2025. [6] Johannes Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In Computer Vision and Pattern Recognition (CVPR), 2016. [7] Zachary Teed and Jia Deng. Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras. In Advances in Neural Information Processing Systems (NeurIPS), 2021. [8] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [9] Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, and Ming-Hsuan Yang. Monst3r: simple approach for estimating geometry in the presence of motion. In International Conference on Learning Representations (ICLR), 2025. [10] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. [11] Mildenhall, PP Srinivasan, Tancik, JT Barron, Ramamoorthi, and Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In European Conference on Computer Vision (ECCV), 2020. [12] Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahbæk Warburg, Benjamin Recht, and Angjoo Kanazawa. K-planes: Explicit radiance fields in space, time, and appearance. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [13] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuehler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. Transactions on Graphics (TOG), 2023. [14] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang. 4d gaussian splatting for real-time dynamic scene rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [15] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and Deva Ramanan. Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis. In International Conference on 3D Vision (3DV), 2024. [16] Adam Harley, Zhaoyuan Fang, and Katerina Fragkiadaki. Particle video revisited: Tracking through occlusions using point trajectories. In European Conference on Computer Vision (ECCV), 2022. 10 [17] Carl Doersch, Yi Yang, Mel Vecerik, Dilara Gokay, Ankush Gupta, Yusuf Aytar, Joao Carreira, and Andrew Zisserman. Tapir: Tracking any point with per-frame initialization and temporal refinement. In International Conference on Computer Vision (ICCV), 2023. [18] Qianqian Wang, Yen-Yu Chang, Ruojin Cai, Zhengqi Li, Bharath Hariharan, Aleksander Holynski, and Noah Snavely. Tracking everything everywhere all at once. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023. [19] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker: It is better to track together. In European Conference on Computer Vision (ECCV), 2024. [20] Yuxi Xiao, Qianqian Wang, Shangzhan Zhang, Nan Xue, Sida Peng, Yujun Shen, and Xiaowei Zhou. Spatialtracker: Tracking any 2d pixels in 3d space. In Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [21] David Charatan, Sizhe Lester Li, Andrea Tagliasacchi, and Vincent Sitzmann. pixelsplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [22] Stanislaw Szymanowicz, Chrisitian Rupprecht, and Andrea Vedaldi. Splatter image: Ultra-fast single-view 3d reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [23] Kai Zhang, Sai Bi, Hao Tan, Yuanbo Xiangli, Nanxuan Zhao, Kalyan Sunkavalli, and Zexiang Xu. Gs-lrm: Large reconstruction model for 3d gaussian splatting. In European Conference on Computer Vision (ECCV), 2024. [24] Haofei Xu, Songyou Peng, Fangjinhua Wang, Hermann Blum, Daniel Barath, Andreas Geiger, and Marc Pollefeys. Depthsplat: Connecting gaussian splatting and depth. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. [25] Chenguo Lin, Panwang Pan, Bangbang Yang, Zeming Li, and Yadong Mu. Diffsplat: Repurposing image diffusion models for scalable gaussian splat generation. arXiv preprint arXiv:2501.16764, 2025. [26] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. Transactions on Machine Learning Research (TMLR), 2024. [27] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, In Advances in Neural Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Information Processing Systems (NeurIPS), 2017. [28] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: learning view synthesis using multiplane images. Transactions on Graphics (TOG), 2018. [29] Wenshan Wang, Delong Zhu, Xiangwei Wang, Yaoyu Hu, Yuheng Qiu, Chen Wang, Yafei Hu, Ashish Kapoor, and Sebastian Scherer. Tartanair: dataset to push the limits of visual slam. In International Conference on Intelligent Robots and Systems (IROS), 2020. [30] Yixuan Li, Lihan Jiang, Linning Xu, Yuanbo Xiangli, Zhenzhi Wang, Dahua Lin, and Bo Dai. Matrixcity: large-scale city dataset for city-scale neural rendering and beyond. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023. [31] Lukas Mehl, Jenny Schmalfuss, Azin Jahedi, Yaroslava Nalivayko, and Andrés Bruhn. Spring: high-resolution high-detail dataset and benchmark for scene flow, optical flow and stereo. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 49814991, 2023. [32] Yohann Cabon, Naila Murray, and Martin Humenberger. Virtual kitti 2. arXiv preprint arXiv:2001.10773, 2020. 11 [33] Yang Zheng, Adam Harley, Bokui Shen, Gordon Wetzstein, and Leonidas Guibas. Pointodyssey: large-scale synthetic dataset for long-term point tracking. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023. [34] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and In Christian Rupprecht. Dynamicstereo: Consistent dynamic depth from stereo videos. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [35] Linyi Jin, Richard Tucker, Zhengqi Li, David Fouhey, Noah Snavely, and Aleksander Holynski. Stereo4d: Learning how things move in 3d from internet stereo videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. [36] Jae Shin Yoon, Kihwan Kim, Orazio Gallo, Hyun Soo Park, and Jan Kautz. Novel view synthesis of dynamic scenes with globally coherent depths from monocular camera. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. [37] Hang Gao, Ruilong Li, Shubham Tulsiani, Bryan Russell, and Angjoo Kanazawa. Monocular dynamic view synthesis: reality check. In Advances in Neural Information Processing Systems (NeurIPS), 2022. [38] Skanda Koppula, Ignacio Rocco, Yi Yang, Joe Heyward, João Carreira, Andrew Zisserman, Gabriel Brostow, and Carl Doersch. Tapvid-3d: benchmark for tracking any point in 3d. In Advances in Neural Information Processing Systems (NeurIPS), 2024. [39] Richard Hartley and Andrew Zisserman. Multiple view geometry in computer vision. Cambridge University Press, 2003. [40] Andrew Davison, Ian Reid, Nicholas Molton, and Olivier Stasse. Monoslam: Real-time single camera slam. Transactions on Pattern Analysis and Machine Intelligence (T-PAMI), 2007. [41] Sameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian Simon, Brian Curless, Steven Seitz, and Richard Szeliski. Building rome in day. Communications of the ACM, 2011. [42] Raul Mur-Artal, Jose Maria Martinez Montiel, and Juan Tardos. Orb-slam: versatile and accurate monocular slam system. IEEE Transactions on Robotics (T-RO), 2015. [43] Jianyuan Wang, Nikita Karaev, Christian Rupprecht, and David Novotny. Vggsfm: Visual geometry grounded deep structure from motion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [44] Ruicheng Wang, Sicheng Xu, Cassie Dai, Jianfeng Xiang, Yu Deng, Xin Tong, and Jiaolong Yang. Moge: Unlocking accurate monocular geometry estimation for open-domain images with optimal training supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. [45] Luigi Piccinelli, Yung-Hsu Yang, Christos Sakaridis, Mattia Segu, Siyuan Li, Luc Van Gool, and Fisher Yu. Unidepth: Universal monocular metric depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [46] Mu Hu, Wei Yin, Chi Zhang, Zhipeng Cai, Xiaoxiao Long, Hao Chen, Kaixuan Wang, Gang Yu, Chunhua Shen, and Shaojie Shen. Metric3d v2: versatile monocular geometric foundation model for zero-shot metric depth and surface normal estimation. Transactions on Pattern Analysis and Machine Intelligence (T-PAMI), 2024. [47] Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, and Jianfei Cai. Mvsplat: Efficient 3d gaussian splatting from sparse multi-view images. In European Conference on Computer Vision (ECCV), 2024. [48] Chen Ziwen, Hao Tan, Kai Zhang, Sai Bi, Fujun Luan, Yicong Hong, Li Fuxin, and Zexiang Xu. Long-lrm: Long-sequence large reconstruction model for wide-coverage gaussian splats. arXiv preprint arXiv:2410.12781, 2024. 12 [49] Vincent Leroy, Yohann Cabon, and Jérôme Revaud. Grounding image matching in 3d with mast3r. In European Conference on Computer Vision (ECCV), 2024. [50] Hengyi Wang and Lourdes Agapito. 3d reconstruction with spatial memory. In International Conference on 3D Vision (3DV), 2025. [51] Jianing Yang, Alexander Sax, Kevin Liang, Mikael Henaff, Hao Tang, Ang Cao, Joyce Chai, Franziska Meier, and Matt Feiszli. Fast3r: Towards 3d reconstruction of 1000+ images in one forward pass. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. [52] Shangzhan Zhang, Jianyuan Wang, Yinghao Xu, Nan Xue, Christian Rupprecht, Xiaowei Zhou, Yujun Shen, and Gordon Wetzstein. Flare: Feed-forward geometry, appearance and camera estimation from uncalibrated sparse views, 2025. [53] Zhenggang Tang, Yuchen Fan, Dilin Wang, Hongyu Xu, Rakesh Ranjan, Alexander Schwing, and Zhicheng Yan. Mv-dust3r+: Single-stage scene reconstruction from sparse views in 2 seconds. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. [54] Yuzheng Liu, Siyan Dong, Shuzhe Wang, Yanchao Yang, Qingnan Fan, and Baoquan Chen. Slam3r: Real-time dense scene reconstruction from monocular rgb videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. [55] Riku Murai, Eric Dexheimer, and Andrew Davison. Mast3r-slam: Real-time dense slam with 3d reconstruction priors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. [56] Brandon Smart, Chuanxia Zheng, Iro Laina, and Victor Adrian Prisacariu. Splatt3r: Zero-shot gaussian splatting from uncalibrated image pairs. arXiv preprint arXiv:2408.13912, 2024. [57] Botao Ye, Sifei Liu, Haofei Xu, Xueting Li, Marc Pollefeys, Ming-Hsuan Yang, and Songyou Peng. No pose, no problem: Surprisingly simple 3d gaussian splats from sparse unposed images. In International Conference on Learning Representations (ICLR), 2025. [58] Xingyu Chen, Yue Chen, Yuliang Xiu, Andreas Geiger, and Anpei Chen. Easi3r: Estimating disentangled motion from dust3r without training. arXiv preprint arXiv:2503.24391, 2025. [59] Jiahao Lu, Tianyu Huang, Peng Li, Zhiyang Dou, Cheng Lin, Zhiming Cui, Zhen Dong, Sai-Kit Yeung, Wenping Wang, and Yuan Liu. Align3r: Aligned monocular depth estimation for dynamic videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. [60] David Yifan Yao, Albert Zhai, and Shenlong Wang. Uni4d: Unifying visual foundation models for 4d modeling from single video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. [61] Luigi Piccinelli, Christos Sakaridis, Yung-Hsu Yang, Mattia Segu, Siyuan Li, Wim Abbeloos, and Luc Van Gool. Unidepthv2: Universal monocular metric depth estimation made simpler. arXiv preprint arXiv:2502.20110, 2025. [62] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In European Conference on Computer Vision (ECCV), 2020. [63] Edgar Sucar, Zihang Lai, Eldar Insafutdinov, and Andrea Vedaldi. Dynamic point maps: versatile representation for dynamic 3d reconstruction. arXiv preprint arXiv:2503.16318, 2025. [64] Qianqian Wang, Yifei Zhang, Aleksander Holynski, Alexei Efros, and Angjoo Kanazawa. In Proceedings of the IEEE/CVF Continuous 3d perception model with persistent state. Conference on Computer Vision and Pattern Recognition (CVPR), 2025. [65] Tian-Xing Xu, Xiangjun Gao, Wenbo Hu, Xiaoyu Li, Song-Hai Zhang, and Ying Shan. Geometrycrafter: Consistent geometry estimation for open-world videos with diffusion priors. arXiv preprint arXiv:2504.01016, 2025. 13 [66] Zeren Jiang, Chuanxia Zheng, Iro Laina, Diane Larlus, and Andrea Vedaldi. Geo4d: Leveraging video generators for geometric 4d scene reconstruction. arXiv preprint arXiv:2504.07961, 2025. [67] Ang Cao and Justin Johnson. Hexplane: fast representation for dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [68] Aayush Bansal, Minh Vo, Yaser Sheikh, Deva Ramanan, and Srinivasa Narasimhan. 4d visualization of dynamic events from unconstrained multi-view videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. [69] Liao Wang, Jiakai Zhang, Xinhang Liu, Fuqiang Zhao, Yanshun Zhang, Yingliang Zhang, Minye Wu, Jingyi Yu, and Lan Xu. Fourier plenoctrees for dynamic radiance field rendering in real-time. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [70] Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph Lassner, Changil Kim, Tanner Schmidt, Steven Lovegrove, Michael Goesele, Richard Newcombe, et al. Neural 3d video synthesis from multi-view video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [71] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural radiance fields for dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. [72] Keunhong Park, Utkarsh Sinha, Jonathan Barron, Sofien Bouaziz, Dan Goldman, Steven Seitz, and Ricardo Martin-Brualla. Nerfies: Deformable neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. [73] Zhengqi Li, Qianqian Wang, Forrester Cole, Richard Tucker, and Noah Snavely. Dynibar: Neural dynamic image-based rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [74] Xiaoming Zhao, Alex Colburn, Fangchang Ma, Miguel Angel Bautista, Joshua M. Susskind, and Alexander G. Schwing. Pseudo-generalized dynamic view synthesis from video. In International Conference on Learning Representations (ICLR), 2024. [75] Zeyu Yang, Hongye Yang, Zijie Pan, and Li Zhang. Real-time photorealistic dynamic scene In International Conference on representation and rendering with 4d gaussian splatting. Learning Representations (ICLR), 2024. [76] Zhan Li, Zhang Chen, Zhong Li, and Yi Xu. Spacetime gaussian feature splatting for real-time dynamic view synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [77] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, and Xiaogang Jin. Deformable 3d gaussians for high-fidelity monocular dynamic scene reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [78] Youtian Lin, Zuozhuo Dai, Siyu Zhu, and Yao Yao. Gaussian-flow: 4d reconstruction with dynamic 3d gaussian particle. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [79] Qingming Liu, Yuan Liu, Jiepeng Wang, Xianqiang Lyu, Peng Wang, Wenping Wang, and Junhui Hou. Modgs: Dynamic gaussian splatting from casually-captured monocular videos with depth priors. In International Conference on Learning Representations (ICLR), 2025. [80] Qianqian Wang, Vickie Ye, Hang Gao, Jake Austin, Zhengqi Li, and Angjoo Kanazawa. Shape of motion: 4d reconstruction from single video. In International Conference on Computer Vision (ICCV), 2025. 14 [81] Yang-Tian Sun, Yihua Huang, Lin Ma, Xiaoyang Lyu, Yan-Pei Cao, and Xiaojuan Qi. Splatter In Advances in Neural video: Video gaussian representation for versatile processing. Information Processing Systems (NeurIPS), 2024. [82] Colton Stearns, Adam Harley, Mikaela Uy, Florian Dubost, Federico Tombari, Gordon Wetzstein, and Leonidas Guibas. Dynamic gaussian marbles for novel view synthesis of casual monocular videos. In SIGGRAPH Asia Conference Papers, 2024. [83] Jiahui Lei, Yijia Weng, Adam Harley, Leonidas Guibas, and Kostas Daniilidis. Mosca: Dynamic gaussian fusion from casual videos via 4d motion scaffolds. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. [84] Yuchen Lin, Chenguo Lin, Jianjin Xu, and Yadong Mu. Omniphysgs: 3d constitutive gaussians for general physics-based dynamics generation. arXiv preprint arXiv:2501.18982, 2025. [85] Hanxue Liang, Jiawei Ren, Ashkan Mirzaei, Antonio Torralba, Ziwei Liu, Igor Gilitschenski, Sanja Fidler, Cengiz Oztireli, Huan Ling, Zan Gojcic, and Jiahui Huang. Feedforward bullet-time reconstruction of dynamic scenes from monocular videos. arXiv preprint arXiv:2412.03526, 2024. [86] Qiuhong Shen, Xuanyu Yi, Mingbao Lin, Hanwang Zhang, Shuicheng Yan, and Xinchao Wang. Seeing world dynamics in nutshell. arXiv preprint arXiv:2502.03465, 2025. [87] Haofei Xu, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, Fisher Yu, Dacheng Tao, and Andreas Geiger. Unifying flow, stereo and depth estimation. Transactions on Pattern Analysis and Machine Intelligence (T-PAMI), 2023. [88] Vincent Sitzmann, Semon Rezchikov, Bill Freeman, Josh Tenenbaum, and Fredo Durand. Light field networks: Neural scene representations with single-evaluation rendering. In Advances in Neural Information Processing Systems (NeurIPS), 2021. [89] René Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. [90] Jingjing Xu, Xu Sun, Zhiyuan Zhang, Guangxiang Zhao, and Junyang Lin. Understanding and improving layer normalization. In Advances in Neural Information Processing Systems (NeurIPS), 2019. [91] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2018. [92] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations (ICLR), 2018. [93] Vickie Ye, Ruilong Li, Justin Kerr, Matias Turkulainen, Brent Yi, Zhuoyang Pan, Otto Seiskari, Jianbo Ye, Jeffrey Hu, Matthew Tancik, et al. gsplat: An open-source library for gaussian splatting. Journal of Machine Learning Research (JMLR), 2025. [94] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pages 35053506, 2020. [95] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174, 2016. [96] Chen Gao, Ayush Saraf, Johannes Kopf, and Jia-Bin Huang. Dynamic view synthesis from dynamic monocular video. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. 15 [97] Yu-Lun Liu, Chen Gao, Andreas Meuleman, Hung-Yu Tseng, Ayush Saraf, Changil Kim, Yung-Yu Chuang, Johannes Kopf, and Jia-Bin Huang. Robust dynamic radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [98] Carl Doersch, Pauline Luc, Yi Yang, Dilara Gokay, Skanda Koppula, Ankush Gupta, Joseph Heyward, Ignacio Rocco, Ross Goroshin, João Carreira, et al. Bootstap: Bootstrapped training for tracking-any-point. In Proceedings of the Asian Conference on Computer Vision (ACCV), 2024. [99] Junyu Xie, Charig Yang, Weidi Xie, and Andrew Zisserman. Moving object segmentation: All you need is sam (and flow). In Proceedings of the Asian Conference on Computer Vision (ACCV), 2024. [100] Nan Huang, Wenzhao Zheng, Chenfeng Xu, Kurt Keutzer, Shanghang Zhang, Angjoo In Proceedings of the Kanazawa, and Qianqian Wang. Segment any motion in videos. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. [101] Zhengqi Li, Richard Tucker, Forrester Cole, Qianqian Wang, Linyi Jin, Vickie Ye, Angjoo Kanazawa, Aleksander Holynski, and Noah Snavely. Megasam: Accurate, fast, and robust structure and motion from casual dynamic videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. [102] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. In International Conference on Learning Representations (ICLR), 2016."
        },
        {
            "title": "A Implementation Details",
            "content": "A.1 Dataset Details As summarized in Table 1, the majority of training datasets used in our experiments are synthetic, providing rich annotations such as pixel-aligned depth maps and accurate camera intrinsics and extrinsics. Ground-truth 3D point trajectories are also available in PointOdyssey [33] and DynamicReplica [34]. Stereo4D [35], on the other hand, is large-scale dataset constructed from YouTube stereo videos and annotated using pretrained foundation models [62, 98]. Despite being partially noisy, its diversity and scale offer strong generalization benefits. The utilized backbone VGGT [10] requires all 3D scenes to be normalized to unit scale, which in turn necessitates depth maps and camera extrinsics to be defined in consistent metric space. This requirement is satisfied by the synthetic datasets and Stereo4D, whose camera poses and depths are metric-aligned by construction. In contrast, RealEstate10K [28] only provides relative camera parameters estimated via COLMAP [6], resulting in an unknown global scale. To address this, we re-estimate both depth maps and camera extrinsics using recent foundation models, Video Depth Anything [5] and MegaSaM [101], to recover aligned geometry across frames. A.2 Curriculum Training We observed that training MoVieS is particularly unstable: the training loss often fluctuates abruptly, and gradients are prone to becoming None. It may arise from sparse annotations and the heterogeneous nature of the training data, which mixes datasets from various sources with differing domains (e.g., indoor vs. outdoor, real vs. synthetic), camera fields of view, recording frame rates, etc. Thanks to the versatile design of MoVieS, we employ curriculum strategy that gradually increases the training complexity. It begins by pretraining the model on low-resolution (224 224) static datasets with only depth and photometric losses, then introduces dynamic datasets along with motion supervision. We found that static datasets play crucial role in stabilizing training for dynamic scenes, without which the loss would be highly unstable. Since modeling dynamic scenes requires reconstructing set of 3DGS for each query time, which results in high GPU memory usage, we start by training on 5 input views for dynamic scenes and then expand to 13 views for fine-tuning. Finally, the training resolution is increased to 518 518. A.3 Training Details Image encoder, feature backbone and depth head of MoVieS are initialized from geometrically pretrained transformer, VGGT [10]. Motion head is initialized from its point head. Remaining components, such as the splatter head and camera/time embeddings, are trained from scratch. We use AdamW optimizer [92] with weight decay of 0.05, and adopt cosine learning rate scheduler [102] with linear warm-up for all curriculum training stages. For static pretraining and dynamic scenes with 5 and 13 input views at resolution of 224 224, we use learning rates of 4e-4, 4e-4 and 4e-5, respectively, with batch size of 256. Training rates for parameters initialized from VGGT are multiplied by 0.1. With 32 H20 GPUs, training takes about 2 days for static and then dynamic scenes with 5 views, and 2 days for 13 views. MoVieS is then finetuned on 518 518 videos with 13 frames using learning rate of 1e-5, which takes around 1 day. To improve memory and computation efficiency, several techniques such as gsplat [93] rendering backend, DeepSpeed [94], gradient checkpointing [95], gradient accumulation, and bf16 mixed precision are adopted in this work."
        },
        {
            "title": "B Discussion",
            "content": "B.1 Limitations and Future Work Although MoVieS achieves competitive performance with inference speeds that are orders of magnitude faster than optimization-based methods, there remains noticeable gap in reconstruction quality. This gap arises partly because many optimization-based approaches benefit from multiple pretrained models that preprocess input videos to provide richer and more accurate cues. Incorporating such richer prior knowledge directly into MoVieS represents promising direction for future work to further improve reconstruction fidelity and robustness. 17 Currently, MoVieS depends on off-the-shelf tools for camera parameter estimation, which adds an external dependency and may limit end-to-end optimization. Seamlessly integrating camera pose estimation within the MoVieS pipeline could simplify the overall workflow, reduce error accumulation, and enhance adaptability to diverse scenarios. Moreover, scaling MoVieS to handle large-scale scenes and achieve high-resolution rendering remains challenge. The computational cost and memory demand grow significantly with scene complexity and resolution, which constrains practical deployment. Developing more compact and efficient dynamic scene representations, possibly through novel model architectures or sparse encoding strategies, is essential to push 4D reconstruction towards real-world applications. Addressing these challenges will not only bridge the performance gap but also unlock the full potential of fast and high-quality dynamic scene reconstruction. B.2 Broader Impact MoVieS provides substantial advantages for fields including robotics simulation, AR/VR, autonomous driving, and digital twins by enabling fast and generalizable dynamic scene understanding. Its ability to reconstruct dynamic environments efficiently can accelerate innovation and improve system performance in these areas. However, such powerful technology also raises risks related to the unauthorized generation of content and potential privacy violations. To mitigate these risks, it is essential to establish clear ethical guidelines and implement appropriate regulatory measures to ensure the responsible and safe use of this technology."
        },
        {
            "title": "C More Visualization Results",
            "content": "We provide more visualization results of MoVieS in Figure 6, 7, 8 and 9."
        },
        {
            "title": "D License Information",
            "content": "We employ several open-source implementations in our experimental comparisons, including: (1) DepthSplat [24]1 (MIT License), (2) Splatter-a-Video [81]2 (Apache License), (3) Shape-ofMotion [80]3 (MIT License), (4) MoSca [83]4 (MIT License), (5) BootsTAPIR [98]5 (Apache License), (6) CoTracker3 [19]6 (Creative Commons Attribution-NonCommercial 4.0 International Public License), and (7) SpatialTracker [76]7 (Attribution-NonCommercial 4.0 International). Several public datasets from diverse sources are utilized to train our models, including: (1) RealEstate10K [28]8 (Creative Commons Attribution 4.0 International License), (2) TartanAir [29]9 (Creative Commons Attribution 4.0 International License), (3) MatrixCity [30]10 (Apache License), (4) PointOdyssey [33]11 (MIT License), (5) DynamicReplica [34]12 (Attribution-NonCommercial 4.0 International License), (6) Spring [31]13 (CC BY 4.0), (7) VKITTI2 [32]14 (Creative Commons Attribution-NonCommercial-ShareAlike 3.0), and (8) Stereo4D [35]15 (CC0 1.0 Universal). 1https://github.com/cvg/depthsplat/tree/main 2https://github.com/SunYangtian/Splatter_A_Video 3https://github.com/vye16/shape-of-motion/ 4https://github.com/JiahuiLei/MoSca 5https://github.com/google-deepmind/tapnet 6https://github.com/facebookresearch/co-tracker 7https://github.com/henry123-boy/SpaTracker 8https://google.github.io/realestate10k/ 9https://theairlab.org/tartanair-dataset/ 10https://city-super.github.io/matrixcity/ 11https://pointodyssey.com/ 12https://github.com/facebookresearch/dynamic_stereo 13https://spring-benchmark.org/ 14https://europe.naverlabs.com/research/computer-vision/proxy-virtual-worlds-vkitti-2/ 15https://stereo4d.github.io/ 18 Figure 6: Qualitative results for reconstruction. Motion means the 3D points movements of the input frame with respect to the first one. Figure 7: Qualitative results for 3D point tracking. Motion means the 3D points movements of the input frame with respect to the first one. 20 Figure 8: More results for scene flow estimation. Flow means the optical flow of the input pixels with respect to the first frame. Arrows on pictures roughly mark directions. 21 Figure 9: More results for moving object segmentation. Masks for moving parts, which are filtered from motion maps, are highlighted across frames in light red."
        }
    ],
    "affiliations": [
        "ByteDance",
        "Carnegie Mellon University",
        "Peking University"
    ]
}