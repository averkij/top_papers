{
    "paper_title": "From Macro to Micro: Benchmarking Microscopic Spatial Intelligence on Molecules via Vision-Language Models",
    "authors": [
        "Zongzhao Li",
        "Xiangzhe Kong",
        "Jiahui Su",
        "Zongyang Ma",
        "Mingze Li",
        "Songyou Li",
        "Yuelin Zhang",
        "Yu Rong",
        "Tingyang Xu",
        "Deli Zhao",
        "Wenbing Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper introduces the concept of Microscopic Spatial Intelligence (MiSI), the capability to perceive and reason about the spatial relationships of invisible microscopic entities, which is fundamental to scientific discovery. To assess the potential of Vision-Language Models (VLMs) in this domain, we propose a systematic benchmark framework MiSI-Bench. This framework features over 163,000 question-answer pairs and 587,000 images derived from approximately 4,000 molecular structures, covering nine complementary tasks that evaluate abilities ranging from elementary spatial transformations to complex relational identifications. Experimental results reveal that current state-of-the-art VLMs perform significantly below human level on this benchmark. However, a fine-tuned 7B model demonstrates substantial potential, even surpassing humans in spatial transformation tasks, while its poor performance in scientifically-grounded tasks like hydrogen bond recognition underscores the necessity of integrating explicit domain knowledge for progress toward scientific AGI. The datasets are available at https://huggingface.co/datasets/zongzhao/MiSI-bench."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 7 6 8 0 1 . 2 1 5 2 : r From Macro to Micro: Benchmarking Microscopic Spatial Intelligence on Molecules via Vision-Language Models Zongzhao Li1* Xiangzhe Kong2,3* Songyou Li1 Yuelin Zhang1 Yu Rong6,7 Jiahui Su4 Zongyang Ma5 Mingze Li1 Tingyang Xu6,7 Deli Zhao6,7 Wenbing Huang1 1Gaoling School of Artificial Intelligence, Renmin University of China 2Dept. of Comp. Sci. & Tech., Tsinghua University 3Institute for AI Industry Research (AIR), Tsinghua University 4SKL-ESPC & SEPKL-AERM, College of Environmental Sciences and Engineering, Peking University 5MAIS, Institute of Automation, Chinese Academy of Sciences 6DAMO Academy, Alibaba Group, Hangzhou, China 7Hupan Lab, Hangzhou, China lizongzhao2023@ruc.edu.cn, jackie kxz@outlook.cn, hwenbing@126.com"
        },
        {
            "title": "Abstract",
            "content": "This paper introduces the concept of Microscopic Spatial Intelligence (MiSI), the capability to perceive and reason about the spatial relationships of invisible microscopic entities, which is fundamental to scientific discovery. To assess the potential of Vision-Language Models (VLMs) in this domain, we propose systematic benchmark framework MiSI-Bench. This framework features over 163,000 question-answer pairs and 587,000 images derived from approximately 4,000 molecular structures, covering nine complementary tasks that evaluate abilities ranging from elementary spatial transformations to complex relational identifications. Experimental results reveal that current state-of-the-art VLMs perform significantly below human level on this benchmark. However, fine-tuned 7B model demonstrates substantial potential, even surpassing humans in spatial transformation tasks, while its poor performance in scientifically-grounded tasks like hydrogen bond recognition underscores the necessity of integrating explicit domain knowledge for progress toward scientific AGI. The datasets are available at https://huggingface.co/ datasets/zongzhao/MiSI-bench. 1. Introduction Spatial intelligence [11, 46], critical component of advanced artificial intelligence, empowers systems to perceive, interpret, and interact with the three-dimensional world. Such capability necessitates profound comprehen- *Equal contribution. Corresponding author. sion of geometries, spacial relationships, and even fundamental physical rules. Contemporary efforts address this by utilizing Vision-Language Models (VLMs) [6, 27, 28], which jointly process visual and textual data to learn spatial properties and relationships from complex scenes [18, 45, 47, 49]. This reasoning capacity regarding object layouts, occlusions, and perspectives establishes vital foundation for embodied interaction in real-world environments [50]. Yet, beyond this visible macroscopic realm lies the microscopic world, composed of invisible particles (e.g., atoms and molecules) that constitute matter [16], where spatial reasoning takes on profoundly different form. In molecular sciences, experts routinely visualize and manipulate microscopic entities such as proteins and drugs using software tools (e.g., PyMOL [15], ChimeraX [34]) to explore geometric complementarity, analyze interactions, and design new functional molecules. This process relies on specialized form of spatial reasoning: the ability to reconstruct three-dimensional structures from two-dimensional projections and to infer physical relationships (e.g., hydrogen bonds). In this paper, we refer to this capability as Microscopic Spatial Intelligence (MiSI), the cognitive foundation underlying human expertise and discovery in scientific fields such as structural biology, drug discovery, and material design. The success of Large Language Models (LLMs) in general-purpose tasks has spurred their exploration in scientific discovery [8, 30, 39], motivating the use of VLMs to analyze scientific data. VLMs are uniquely suited for this role, as they can process both visual and textual modalities within unified architecture. Unlike conventional domain-specific systems, VLMs can perceive structural patterns while grounding their interpretations in scientific con1 Figure 1. Overview of our MiSI-Bench. Our dataset is derived from around 4,000 PDBs and comprises 9 distinct tasks. cepts. This cross-modal capability enables more humanlike, context-aware reasoning about molecular structures by seamlessly linking them with textual semantics. However, shifting from human-scale daily objects to atom-level invisible entities, MiSI requires exceptional scientific expertise to perceive spatial transformations and reason over relational identifications such as atomic interactions. It remains unclear whether the VLMs are ready for tackling the challenges in the microscopic scientific fields. To bridge the gap, we propose MiSI-Bench, systematical framework for training and evaluating microscopic spatial intelligence in VLMs. As illustrated in Fig. 1, MiSI-Bench contains 163,514 question-answer pairs and 587,975 images over diverse set of 9 complementary tasks, constructed from around 4,000 molecular structures [44]. We visualize these three-dimensional microscopic objects as two-dimensional orthographic projections, just like how human experts interpret them. We then disentangle the intelligence for spatial transformations and relational identifications into four elementary operations: translation, rotation, zooming, interaction. Subsequently, we design four unit tasks to evaluate these fundamental abilities independently, and further design five composite tasks which integrate multiple elementary operations to test the models high-order reasoning ability. Experimental results show that current advanced VLMs (e.g., o3 [33], Claude Sonnet4.5 [5]) perform well below human level on our benchmark. While human evaluators excel in some tasks, they struggle with continuous spatial transformation and 3D reconstruction. Remarkably, after SFT on our dataset, 7B model outperforms all leading VLMs and even surpasses humans in spatial transformation tasks, revealing VLMs untapped potential for spatial reasoning. However, its poor performance in biologicallygrounded tasks like hydrogen bond recognition highlights the need for injecting explicit scientific knowledge during pre-training to progress toward AGI. 2. Related Work Macroscopic Spatial Intelligence In recent years, researchers have developed variety of datasets and benchmarks with distinct focuses to evaluate the spatial intelligence of VLMs [20, 37, 51]. For instance, VIS-Bench [46] and MuriBench [43] emphasize the models ability to associate and reason across video/multi-images; LEGOPuzzles [40] examines multi-step spatial reasoning in synthetic block-building environment. Therefore, we propose MiSI-Bench to draw attention to this direction and to establish reliable benchmark for evaluating models microlevel spatial intelligence. This task is uniquely challenging, 2 as understanding microscopic entities demands exceptional expertise in both spatial transformations and relational reasoning. Three-Dimensional Molecular Understanding Conventional modeling of 3D molecules usually relies on Cartesian coordinates as model inputs. Starting from physical force fields [1, 38], models have evolved from 3D convolutional neural networks [35] to equivariant graph neural networks [23, 41] and transformers [22, 26] with the rise of geometric deep learning [9]. However, these approaches primarily operate within geometric coordinate space only, while MLLMs offer complementary, human-like perspective which can learn to reason about three-dimensional molecular geometry through visual abstractions and natural language grounding, unlocking new mode of molecular understanding that bridges microscopic visual perception and textual knowledge. 3. Definition of Major Concepts To explore whether current VLMs possess the ability to understand 3D molecular structures, which we term MiSI as above, we begin by defining the representations adopted for molecular structures, and then introduce the fundamental perceptual expertise humans rely on to comprehend the micro world. These elements together serve as the preliminaries for our benchmark design. Study Scope Our physical world is organized across multiple hierarchical levels, from macroscopic organisms to organs, cells, and to molecules such as proteins, and DNAs [16]. Thanks to the continuous scientific exploration, people now understand the phenomena observed in the macroscopic world are the results of microscopic particles. And advances in imaging technologies, such as cryoelectron microscopy, further allow us to visualize these particles at near-atomic resolution [7]. In this work, we shift the focus from the macroscopic world to its microscopic foundation, investigating how VLMs perceive and reason about 3D molecular structures composed of atoms. Orthographic Projection of Molecules Throughout human history, people have sought to represent the threedimensional world on two-dimensional media. One classical approach employs perpendicular rays of light to generate orthographic projections, and typically canonical views, such as the front, top, and left views, are employed to reconstruct the full 3D structure of an object [10]. Following this convention, we adopt orthographic views as 2D representations of microscopic 3D molecular structures. Taxonomy of Human Expertise Understanding microscopi molecules requires both spatial reasoning and domain expertise. Experts rely on fundamental spatial transformation abilities, such as translation, rotation, and zooming, to establish more complete panorama of molecular structures [12, 25]. Beyond geometric manipulation, they use domain knowledge to identify interaction patterns such as hydrogen bonds, which reveal the underlying physical principles of molecular organization [2]. We refer to this process as relational identification. In this work, we summarize the expert skills with the above-mentioned four elementary microspace operations, namely translation, rotation, zooming, and interaction, then design unit tasks to independently evaluate each capability. We further introduce combinatorial tasks that require integrating multiple operations, enabling more comprehensive assessment of VLMs in microspace understanding. 4. MiSI-Bench We construct our MiSI-Bench for evaluating VLMs using the refined PDBbind dataset [44], widely adopted benchmark for structure-based drug discovery [21, 24, 42]. Each entity in PDBbind dataset corresponds to complex composed of protein and ligand. We visualize all complexes in ChimeraX [34] to generate orthographic projections images as model inputs. After removing complexes with visualization issues, the final dataset contains 3,503 proteinligand complexes for Supervised Fine-Tuning (SFT) and 490 for testing, all with experimentally solved crystal structures. The benchmark encompasses nine tasks, including four unit tasks involving single elementary operations and five composite tasks involving combinations of multiple operations. For each task, the QA pairs are generated using fixed templates. The problem templates and brief illustration of all tasks are shown in Fig. 2. The overall pipeline for constructing MiSI-Bench is detailed in supplementary materials. Our benchmark contains total of 150,597 Question Answering (QA) pairs for training and 12,917 for testing, summing up to 538,015 and 49,960 images in the train and the test set, respectively. The statistics for all tasks are presented in Fig. 3. Two formats of questions are used to evaluate model performance. Cloze Questions require the model to complete partially specified instructions by filling in missing actions or parameters. These tasks assess the ability of the models to identify the correct operations with precise attributes (e.g., axes, distances, or angles). Multiple-Choice Questions present several candidate options, among which the model should identify the correct answer while rejecting distracting decoys. These tasks evaluate whether the models have discriminative understanding of spatial configurations and their ability to reason about the consequences of microspace operations. 3 Figure 2. brief demonstration of the MiSI-Bench dataset. The examples have been simplified for clarity; for complete examples, please refer to Appendix C. 4.1. Unit Task We first establish four unit tasks to evaluate the spatial understanding of elementary microspace operations, where each task isolates one essential ability involved in manipulating or interpreting microscopic 3D molecular structures. For translation, rotation, and zooming tasks, the three orthographic projections (i.e., the top, the front, and the left side views) of the initial complex and the front view of the complex after the operation are given to the models. For residueligand interaction task, since overlapping atom names might interfere with the performance, we give all six orthographic projections to the models. Figure 3. The statistics for all tasks. 4 Translation (Cloze) In this task, the molecular complex is translated along one of the axes parallel to the visualization plane (i.e., the or axis) by random distance between 4 and 4 angstrom ( A). The model must infer both the direction and the magnitude of motion, completing prompt 3 . To avoid being too harsh of the form: move on numerical precisions, the translation range is discretized into 1.0 bins. For SFT, two samples per complex are generated along each axis, yielding 14,012 training samples, and one per axis for evaluation, totaling 980 test samples. Rotation (Cloze) The complex is rotated along one of the three coordinate axes (x, y, or z) by random angle uniformly drawn from [90, 90]. Models must determine both the rotation axis and the degree of rotation, filling in 15 . Rotation angles are disthe prompt: roll cretized into 15 bins. Each complex results in two samples per axis for SFT (21,018 training samples) and one per axis for testing (1,470 samples). Zooming (Cloze) To simulate zooming operations, the complex is moved along the axis perpendicular to the visualization plane (i.e., the axis) by random depth between 40 and 60 A. This range corresponds to the magnification levels most suitable for visualizing the pocketligand interactions near the center of the view (See the distribution figure in Appendix for details). The model fills in prompts like: move 50 , where depth values are discretized into 1.0 bins. Four samples per complex are created for SFT (14,012 training samples) and two per complex for testing (980 samples). Residue-Ligand Interaction (Cloze) Given residue and the ligand, models must first identify whether the residue interacts with the ligand (Yes or No), and then output all atom pairs participating in the interaction: ARG NH2, O22; ARG N, O23. For this proof-ofconcept benchmark, we focus on hydrogen bonds as interactions of interest, with detailed geometric configurations provided in the Appendix A. The dataset includes 11,572 positive and 12,125 negative samples for SFT, and 1,499 positive and 1,603 negative samples for evaluation. 4.2. Composite Tasks We further design five composite tasks that require models to understand and reason on multiple microspace operations, the capability of which are commonly required for human experts during molecular structural analysis. 4.2.1. Spatial Transformation Reasoning This category evaluates the ability of the models to reason about sequential spatial transformations and generalize them across different molecular complexes. Denote two complexes as c1 and c2, and two spatial transformations as f1 and f2. The models are given the three orthographic projections of both c1 and c2, along with the front view of f2(f1(c1)), which is the result of applying f1 followed by f2 to c1. The task is to identify the correct front view of f2(f1(c2)) among four candidate images, where the other three are decoys. The decoys are constructed by exerting perturbation on the ground-truth transformations through one of three schemes: 1) Altering the magnitude (translation distance or rotation angle) of both f1 and f2; 2) Flipping the sign of f1 (e.g., clockwise to counterclockwise) and adjusting the magnitude of f2; 3) Changing the axis of f1 and modifying the magnitude of f2. Translation-Rotation Movement (Multiple-Choice) In this task, f1 is sampled from translational operations and f2 from rotational operations. For each complex in the dataset, we pair it with another random complex and construct six and three distinct transformation combinations for SFT and testing, respectively, yielding total of 21,018 and 1,470 questions for SFT and testing. (Multiple-Choice) Both Rotation-Rotation Movement f1 and f2 are sampled from rotational operations, constrained to different axes to prevent trivial correlations. The scale matches the previous task, with 21,018 questions for SFT and 1,470 for testing. 4.2.2. Local Relational Reasoning This category evaluates whether the models are capable of interpreting fine-grained, domain-specific spatial relations within molecular complexes, such as hydrogen bonds between atom pairs, and then reasoning about how to manipulate the visualization to highlight specific interactions. Interaction Location (Multiple-Choice) The model is provided with six orthographic projections of molecular complex, along with specified atom pair representing hydrogen bond (e.g., ARG 45 NH2 A, O1B, denoting the interaction between the NH2 atom of residue ARG45 on chain and the O1B atom on the ligand). The objective is to distinguish the correct transformation that repositions the corresponding interaction to the center of the visualization from three other decoys. The decoy transformations are generated by perturbing the sign and magnitude of the ground-truth translation parameters. 4.2.3. Global Relational Reasoning This series of tasks considers the overall spatial relations of the entire complex instead of single localized ones, which is inherently more difficult than previous tasks, as they test the ability of the models to reason high-order combinations of spatial operations. 5 Ligand Docking (Cloze) This task emulates the molecular docking process to evaluate whether the model can infer the complementary binding configuration and corresponding geometric transformations. The model is provided with three orthographic views of the ligand alone, the pocket alone, and one undocked complex view obtained by translating and rotating the ligand away from its pocket. Rotation angles are uniformly sampled from [90, 90], while translation distances are adaptively determined for each complex to minimize spatial overlap between the displaced ligand and pocket (Specific details can be found in Appendix A). The model must predict the sequence of transformations required to recover the native docking conformation, such as roll 45, move -12. Each complex generates six training samples (21,018 in total) and three test samples (1,470 in total). the full complex, Pocket-Ligand Interaction (Cloze) This task extends the ResidueLigand Interaction task to the entire binding pocket, requiring the model to reason about global intermolecular contact patterns. Given six orthographic the models are reprojections of quired to output all hydrogen-bond interactions between the ligand and the pocket in structured forlike ARG 221 NH2 A, O22; ARG 221 A, mat O23; ARG 221 NE A, O22. Each interaction is expressed as tuple specifying the residue type, residue index, interacting atom in the residue, chain identity, and interacting atom in the ligand, with semicolons separating multiple entries. 5. Experiments We first introduce the experimental setup in Sec. 5.1, and then report the main results of all compared models on our benchmark in Sec. 5.2. Finally, we conduct factor analysis and case study in Sec. 5.3. 5.1. Setup Benchmark Subsets Due to the expensive cost of closed-source models (especially reasoning models), we create MiSI-Bench(tiny) by randomly sampling 50 question-answer pairs from each task in our dataset. This tiny subset will be used for evaluating the performance of all closed-source models and open-source mixture-of-experts (MoE) models, ensuring an intuitive and fair comparison. All models are evaluated under few-shot settings to provide them with necessary scientific prior knowledge. For Cloze Questions, where answers might involve continuous numerical values and multiple entries, we employ weighted composite score, as inspired by previous literature [17, 29], to reflect the degree of correctness beyond exact matching. For tasks involving spatial transformations (i.e., move and roll), when the model predicts the correct axis, we will further assign scores based on the predicted values. Specifically, the score is determined by the normalized absolute error between the predicted ( ˆd) and groundtruth (d) magnitudes (i.e., distances and angles), ˆdd. For composite tasks involving multiple transformations, such as Ligand Docking, each component operation (e.g., move and roll) contributes equally to the total score, summing up to 1.0. In this task, since the axis for the move operation is fixed to x, scores are assigned only when the sign of the predicted ( ˆd) matches the sign of the ground-truth (d). For ResidueLigand Interaction and PocketLigand Interaction tasks, we compute the ratio of correctly predicted interactions among all provided outputs. To penalize cheating behaviors where models output all the correct interactions along with hallucinations of irrelevant ones, such cases are assigned with score of 0.5. Furthermore, if the number of hydrogen bonds in the models response exceeds twice the number in the ground-truth, we consider the model to be attempting to score through exhaustive enumeration, in which case the model receives score of 0. Furthermore, we also provide the results for exact matching for Cloze Questions in Appendix B. Benchmark Models We comprehensively evaluate ten VLMs spanning four major model families, including nine closed-source and one open-source representatives. From Open AI, we include GPT-5-mini (w/ mixed modes of reasoning) [32], o4-mini (w/ reasoning) [33], o3 (w/ reasoning) [33], and GPT-4.1 (w/o reasoning) [31]. From Anthropics Claude series, we test Claude 4.5 Sonnet (w/ reasoning) [5], Claude 4 Opus (w/ reasoning) [4], and Claude 3.5 Sonnet (w/o reasoning) [3]. From Googles Gemini series, we include Gemini-2.5-pro (w/ reasoning) [14] and Gemini-2.5-flash-lite (w/ reasoning) [13]. For open-source models, our preliminary experiments show that most models below 32B parameters achieve very low performance on MiSI-Bench. Therefore, we select the strong Qwen3-vl235b-a22b-thinking (w/ reasoning) [36] from larger opensource models as representative baseline. Furthermore, we propose Qwen2.5VL-7B-SFT, which is finetuned on the training split of our benchmark to investigate how to better trigger the Microscopic Spatial Intelligence in VLMs. For Multiple-Choice Questions and Zooming Metrics task, we follow the convention to adopt Accuracy (ACC) as the main metric [19, 48], which is calculated as the proportaion of answers exactly matching the ground truth. Human-Level Performance We estimate the performance of humans by recruiting PhD candidates in biology to complete the questions for residue-ligand interaction, ligand docking, and pocket-ligand interaction, which requires 6 Table 1. Evaluation on MiSI-Bench. Bold indicates the best result among all models. Since in the Trans-Rot. and Rot-Rot. tasks, the predictions of almost all models are close to random guessing, we exclude these two rows when calculating the average scores. t n Tr t R m s. e g - R g. e g - t. - Tr Rank Avg. 81.18 - 100.00 70. Unit Task 30.00 100.00 100.00 32.00 t. - n a n r t Composite Task 74. 92.00 26.00 27.71 28.55 33.65 29.94 16.00 33.13 34.37 23.34 47.71 39.71 52.29 50.15 36.29 57.43 45.71 46.36 30.55 36.36 43.82 38.88 22.55 24.73 44.18 25.21 4.00 2.08 2.00 0.00 4.00 6.00 6.00 6. 29.33 12.67 18.67 28.67 6.67 33.67 22.33 17.03 34.00 76.00 94.00 52.00 0.00 74.00 84.00 25.00 28.00 40.00 22.00 30.61 30.00 12.00 28.00 20.40 22.44 20.00 22.44 21.62 25.00 26.00 26.00 22.00 27.24 31.51 20.69 30.38 32.25 34.39 34.12 29.32 47.82 30.00 36.00 38.00 26.00 34.00 38.00 38. 29.20 31.23 29.71 47.14 37.45 37.11 2.00 10.00 7.33 27.50 80.00 70. 33.33 18.00 29.26 32.00 32.90 27.52 36.00 28.00 62.96 99. 99.71 27.14 63.46 89.52 88.44 89. 24.94 88.37 10.72 r. I - 82.78 1.01 0.00 1.71 1.44 0.25 0.77 0.60 0. 0.2 2.55 Methods Human Level Reasoning Models GPT-5-mini O4-mini O3 Gemini-2.5-pro Gemini-2.5-flash-lite Claude Opus4 Claude Sonnet4.5 Qwen3-vl-235b-a22b-thinking General Models GPT-41 Claude Sonnet3.5 Our Model Qwen2.5VL-7B-SFT 9 8 3 6 11 4 2 10 7 5 1 more domain expertise than the other tasks. For the rest of tasks, we employ PhD candidates in broader field of science, technology, engineering, and mathematics to propose answers. Each participant is required to answer questions in MiSI-Bench(tiny) independently, whose responses are later assessed with the same metrics as the models. 5.2. Main Results Human Level Performance. The evaluation results are shown in Tab. 1. Human evaluators perform well in most unit tasks, demonstrating strong 3D spatial modeling abilities and the potential to integrate biological knowledge with spatial reasoning for basic interactive tasks. However, their performance decline significantly in complex spatial reasoning tasks. They manage small-angle rotations by tracking key atomic changes, while large-scale rotationsrequiring maintained spatial continuity and multiatom trackingincrease cognitive load and impaired axis and angle judgment. Zooming tasks prove even more challenging, as their judgments rely on overall intuition regarding boundary shifts and atomic density changes, lacking clear reference points and resulting in greater estimation errors. In composite tasks, consecutive spatial operations (e.g., Trans-Rot., Rot-Rot.) lead to error accumulation and frequent reference frame shifts, significantly degrading human performance. Such tasks impose high demands on working memory and the stability of spatial mental simulation, forming bottleneck in human performance. In Docking task, performance is the poorest due to the need for both sequential spatial transformations and biological knowledge to determine hydrogen bond formation and optimal docking positions. For Poc-Lig Inter. task, the main challenge lies in integrating multiple 2D views to reconstruct the 3D conformation of occluded residues before identifying hydrogen bonds, making it highly demanding. In contrast, Inter Location. task are simpler: they involve no rotational operationsonly distance perceptionand provide hydrogen bond information upfront, eliminating the need for specialized knowledge. Advancing VLMs Performance. As evidenced by the results in the table, all advanced models perform suboptimally across various tasks in MiSI-Bench(tiny). Overall, the models exhibit better performance in distancerelated tasks compared to rotation-related ones. For instance, in tasks such as Translation versus Rotation, and Interaction Location versus RotationRotation Movement, the models consistently achieve higher scores in the former. This may stem from the fact that most current VLMs are primarily trained on two-dimensional data, making distanceas two-dimensional attributemore in the readily adaptable for the models. ResidueLigand InteractionPos and PocketLigand Interaction tasks, the performance gap between the models Furthermore, 7 model across different statistical intervals for these two tasks. As shown in Fig. 4(a), the prediction accuracy decreases sharply as the number of hydrogen bonds increases, indicating that the model struggles to identify all hydrogen bonds in scenarios with complex hydrogen-bonding interactions. In Fig. 4(b), the prediction error rate curve exhibits an initial increase followed by decrease. We hypothesize that this pattern may stem from the models failure to generalize uniformly across the entire scale space. The observed peak likely corresponds to visually critical scale in molecular structures, where discriminative structural information is minimal, thereby reducing the parsing efficiency of the models attention mechanism. Figure 5. Case study of the Rotation task. Case Study. In this section, we examine Claude Sonnet 4.5, the top-performing model among advancing VLMs, by analyzing its reasoning process in failure cases from the Rotation task. As shown in Fig. 5, the model demonstrates logically sound approach: it identifies conserved residues across structural changes as anchors and infers the rotation axis and angle accordingly. However, in the key region it localizes, although the model correctly detects spatial rearrangements of residues 221 and 225, it misinterprets the change as moved slightly backward, leading to an incorrect conclusion. In fact, simply observing the positional shift of residue 221 would suggest rotation around the yaxis. This case indicates that advancing VLMs still lack adequate spatial reasoning capabilities and require more effective mechanisms to elicit such skills. 8 Figure 4. Factor Analysis of MiSI-Bench. and human-level performance is most pronounced, highlighting their still-insufficient knowledge reserves in specialized domains such as biology. in the ResidueLigand InteractionNeg task, most models perform relatively well, likely because the greater spatial distance between residues and ligands in negative samples allow the models to make correct judgments based solely on spatial proximity. In contrast, SFT Model Performance. Experimental results demonstrate that after fine-tuning on the MiSI-Bench dataset, model performance improves significantly, surpassing mainstream VLMs across all tasks and exceeding humanlevel performance in complex spatial tasks such as Rotatask, where human pertion. Notably, in the RotRot. formance approaches random guessing, the model maintains approximately 90% accuracy, indicating that advanced VLMs possess potential for 3D spatial cognition. Previous underperformance of advancing VLMs may have stemmed from domain adaptation barriers: although models have generic spatial understanding, they lack visual priors for specialized structures such as proteins, hindering knowledge transfer. Appropriate fine-tuning can establish crossdomain mappings and unlock their spatial reasoning capabilities. However, in tasks such as Res/PocLig Inter., which rely on domain-specific knowledge, models still lag behind humans, suggesting that the absence of domain priors in foundational training remains bottleneck. Future work should focus on further exploring the spatial potential of models and investigating how to effectively integrate explicit knowledge from scientific fields such as structural biology. 5.3. Analysis Factor Analysis. In this section, we conduct detailed analysis of the suboptimal performance exhibited by the SFT model on the Res-Lig Inter Pos. and Zooming tasks. Fig. 4(a) and (b) present the prediction accuracy of the 6. Conclusion In this work, we establish Microscopic Spatial Intelligence (MiSI) as distinct and critical challenge for VisionLanguage Models (VLMs), extending beyond macroscopic understanding to the atom-level reasoning essential for scientific discovery. We propose systematic benchmark framework dubbed as MiSI-Bench, for valuating various advanced VLMs for MiSI. The experiments reveals significant performance gap between state-of-the-art VLMs and human expertise. Yet, the strong performance of fine-tuned 7B model underscores the substantial potential of VLMs to master complex spatial transformations, even surpassing humans on complex spatial tasks such as rotation. Ultimately, achieving robust MiSI will require not only scaling model architectures but also the explicit integration of scientific knowledge for real-world scientific applications."
        },
        {
            "title": "References",
            "content": "[1] Rebecca Alford, Andrew Leaver-Fay, Jeliazko Jeliazkov, Matthew OMeara, Frank DiMaio, Hahnbeom Park, Maxim Shapovalov, Douglas Renfrew, Vikram Mulligan, Kalli Kappel, et al. The rosetta all-atom energy function for macromolecular modeling and design. Journal of chemical theory and computation, 13(6):30313048, 2017. 3 [2] Amy Anderson. The process of structure-based drug de- [3] Anthropic. sign. Chemistry & biology, 10(9):787797, 2003. 3 Claude 3.5 sonnet. https : / / www . anthropic . com / news / claude - 3 - 5 - sonnet, 2025. 6 [4] Anthropic. Introducing claude 4. anthropic.com/news/claude-4, 2025. 6 https : / / www . [5] Anthropic. Introducing claude sonnet 4.5. https:// www.anthropic.com/news/claude-sonnet-45, 2025. 2, 6 [6] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 1 [7] Xiao-Chen Bai, Greg McMullan, and Sjors HW Scheres. How cryo-em is revolutionizing structural biology. Trends in biochemical sciences, 40(1):4957, 2015. 3 [8] Daniil Boiko, Robert MacKnight, Ben Kline, and Gabe Gomes. Autonomous chemical research with large language models. Nature, 624(7992):570578, 2023. 1 [9] Michael Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine, 34(4):1842, 2017. [10] Ingrid Carlbom and Joseph Paciorek. Planar geometric projections and viewing transformations. ACM Computing Surveys (CSUR), 10(4):465502, 1978. 3 [11] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In Proceedings of the IEEE/CVF Conference on Com9 puter Vision and Pattern Recognition, pages 1445514465, 2024. 1 [12] Vadim Cherezov, Daniel Rosenbaum, Michael Hanson, Søren GF Rasmussen, Foon Sun Thian, Tong Sun Kobilka, Hee-Jung Choi, Peter Kuhn, William Weis, Brian Kobilka, et al. High-resolution crystal structure of an engineered human β2-adrenergic proteincoupled receptor. science, 318(5854):12581265, 2007. 3 [13] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. [14] Google DeepMind. Gemini 2.5: Our most intelligent ai model. https://blog.google/technology/ google - deepmind / gemini - model - thinking - updates-march-2025/, 2025. 6 [15] Warren DeLano et al. Pymol: An open-source molecular graphics tool. CCP4 Newsl. protein crystallogr, 40(1):82 92, 2002. 1 [16] Simon Eidelman, KG Hayes, KA ea Olive, AguilarBenitez, Amsler, Asner, KS Babu, RM Barnett, Beringer, PR Burchat, et al. Review of particle physics. Physics letters B, 592(1-4):15, 2004. 1, 3 [17] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object International journal of computer classes (voc) challenge. vision, 88(2):303338, 2010. 6 [18] Kaituo Feng, Manyuan Zhang, Hongyu Li, Kaixuan Fan, Shuang Chen, Yilei Jiang, Dian Zheng, Peiwen Sun, Yiyuan Zhang, Haoze Sun, et al. Onethinker: All-inone reasoning model for image and video. arXiv preprint arXiv:2512.03043, 2025. [19] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2410824118, 2025. 6 [20] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. In European Conference on Computer Vision, pages 148166. Springer, 2024. 2 [21] Bowen Gao, Bo Qiang, Haichuan Tan, Yinjun Jia, Minsi Ren, Minsi Lu, Jingjing Liu, Wei-Ying Ma, and Yanyan Lan. Drugclip: Contrastive protein-molecule representation learning for virtual screening. Advances in Neural Information Processing Systems, 36:4459544614, 2023. 3 [22] Wenbing Huang, Rui Jiao, Xiangzhe Kong, Li Zhang, Ziyang Yu, Fangyuan Ren, Wenjuan Tan, and Yang Liu. An equivariant pretrained transformer for unified 3d molecular representation learning. 2025. 3 [23] Xiangzhe Kong, Wenbing Huang, and Yang Liu. Conditional antibody design as 3d equivariant graph translation. In The Eleventh International Conference on Learning Representations, 2023. [24] Xiangzhe Kong, Wenbing Huang, and Yang Liu. Generalist equivariant transformer towards 3d molecular interaction learning. In International Conference on Machine Learning, pages 2514925175. PMLR, 2024. 3 [25] Xiangzhe Kong, Rui Jiao, Haowei Lin, Ruihan Guo, Wenbing Huang, Wei-Ying Ma, Zihua Wang, Yang Liu, and Jianzhu Ma. Peptide design through binding interface mimicry with pepmimic. Nature biomedical engineering, pages 116, 2025. 3 [26] Xiangzhe Kong, Zishen Zhang, Ziting Zhang, Rui Jiao, Jianzhu Ma, Wenbing Huang, Kai Liu, and Yang Liu. Unimomo: Unified generative modeling of 3d molecules for de novo binder design. In Forty-second International Conference on Machine Learning, 2025. 3 [27] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 1 [28] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024. 1 [29] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740755. Springer, 2014. [30] Andres M. Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew White, and Philippe Schwaller. Augmenting large language models with chemistry tools. Nature Machine Intelligence, 6(5):525535, 2024. 1 [31] OpenAI. Introducing gpt-4.1 in the api. https : / / openai.com/index/gpt-4-1/, 2025. 6 [32] OpenAI. Introducing gpt-5. https://openai.com/ index/introducing-gpt-5/, 2025. 6 [33] OpenAI. Introducing o3 and o4 mini. https://openai. com / index / introducing - o3 - and - o4 - mini/, 2025. 2, 6 [34] Eric Pettersen, Thomas Goddard, Conrad Huang, Elaine Meng, Gregory Couch, Tristan Croll, John Morris, and Thomas Ferrin. Ucsf chimerax: Structure visualization for researchers, educators, and developers. Protein science, 30(1):7082, 2021. 1, 3 [35] Pedro Pinheiro, Arian Rokkum Jamasb, Omar Mahmood, Vishnu Sresht, and Saeed Saremi. Structure-based drug design by denoising voxel grids. In International Conference on Machine Learning, pages 4079540812. PMLR, 2024. 3 [36] QwenTeam. Qwen3-vl: Sharper vision, deeper thought, https : / / qwen . ai / blog ? id = broader action. 99f0335c4ad9ff6153e517418d48535ab6d8afef& from = research . latest - advancements - list, 2025. 6 [37] Arijit Ray, Jiafei Duan, Reuben Tan, Dina Bashkirova, Rose Hendrix, Kiana Ehsani, Aniruddha Kembhavi, Bryan Plummer, Ranjay Krishna, Kuo-Hao Zeng, et al. Sat: Spatial aptitude training for multimodal language models. arXiv e-prints, pages arXiv2412, 2024. 2 [38] Joost Schymkowitz, Jesper Borg, Francois Stricher, Robby Nys, Frederic Rousseau, and Luis Serrano. The foldx web server: an online force field. Nucleic acids research, 33 (suppl 2):W382W388, 2005. 3 [39] Kyle Swanson, Wesley Wu, Nash Bulaong, John Pak, and James Zou. The virtual lab of ai agents designs new sars-cov-2 nanobodies. Nature, pages 13, 2025. 1 [40] Kexian Tang, Junyao Gao, Yanhong Zeng, Haodong Duan, Yanan Sun, Zhening Xing, Wenran Liu, Kaifeng Lyu, and Kai Chen. Lego-puzzles: How good are mllms at multi-step spatial reasoning? arXiv preprint arXiv:2503.19990, 2025. 2 [41] Philipp Tholke and Gianni De Fabritiis. Equivariant transformers for neural network based molecular potentials. In International Conference on Learning Representations, 2022. 3 [42] Raphael John Lamarre Townshend, Martin Vogele, Patricia Adriana Suriana, Alexander Derry, Alexander Powers, Yianni Laloudakis, Sidhika Balachandar, Bowen Jing, Brandon M. Anderson, Stephan Eismann, Risi Kondor, Russ Altman, and Ron O. Dror. ATOM3d: Tasks on molecules in three dimensions. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021. [43] Fei Wang, Xingyu Fu, James Huang, Zekun Li, Qin Liu, Xiaogeng Liu, Mingyu Derek Ma, Nan Xu, Wenxuan Zhou, Kai Zhang, et al. Muirbench: comprehensive benchmark for robust multi-image understanding. arXiv preprint arXiv:2406.09411, 2024. 2 [44] Renxiao Wang, Xueliang Fang, Yipin Lu, Chao-Yie Yang, and Shaomeng Wang. The pdbbind database: methodologies and updates. Journal of medicinal chemistry, 48(12):4111 4119, 2005. 2, 3, 1 [45] Junfei Wu, Jian Guan, Kaituo Feng, Qiang Liu, Shu Wu, Liang Wang, Wei Wu, and Tieniu Tan. Reinforcing spatial reasoning in vision-language models with interwoven thinking and visual drawing. arXiv preprint arXiv:2506.09965, 2025. 1 [46] Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1063210643, 2025. 1, 2 [47] Baiqiao Yin, Qineng Wang, Pingyue Zhang, Jianshu Zhang, Kangrui Wang, Zihan Wang, Jieyu Zhang, Keshigeyan Chandrasegaran, Han Liu, Ranjay Krishna, et al. Spatial mental modeling from limited views. In Structural Priors for Vision Workshop at ICCV25, 2025. 1 [48] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for exIn Proceedings of the IEEE/CVF Conference pert agi. on Computer Vision and Pattern Recognition, pages 9556 9567, 2024. 6 [49] Jiahui Zhang, Yurui Chen, Yanpeng Zhou, Yueming Xu, Ze Huang, Jilin Mei, Junhui Chen, Yu-Jie Yuan, Xinyue Cai, Guowei Huang, et al. From flatland to space: Teaching vision-language models to perceive and reason in 3d. arXiv preprint arXiv:2503.22976, 2025. 1 [50] Baining Zhao, Ziyou Wang, Jianjie Fang, Chen Gao, Fanhang Man, Jinqiang Cui, Xin Wang, Xinlei Chen, Yong Li, and Wenwu Zhu. Embodied-r: Collaborative framework for activating embodied spatial reasoning in foundation In Proceedings of the models via reinforcement learning. 33rd ACM International Conference on Multimedia, pages 1107111080, 2025. 1 [51] Yiming Zuo, Karhan Kayan, Maggie Wang, Kevin Jeon, Jia Deng, and Thomas Griffiths. Towards foundation models In 2025 International for 3d vision: How close are we? Conference on 3D Vision (3DV), pages 12851296. IEEE, 2025. 2 11 A. Dataset Details In this section, we present additional details regarding the construction of the dataset. A.1. Data Generation Pipeline As shown in Fig. 6, our dataset construction process consists of three main steps. The first step is Dataset Collection. We download the PDBBind dataset [44], retaining only the ligand and pocket (collectively referred to as the complex) for each sample. As the first exploratory dataset for microscopic spatial intelligence, we reduce the overall complexity by removing the solvent and hiding the hydrogen. Subsequently, we color all oxygen, nitrogen, and carbon atoms red, blue, and gray, respectively, in accordance with common coloring standards in the field. Notably, for pocket residues, we apply alternating yellow and purple coloring to facilitate the models ability to distinguish adjacent residues in subsequent interaction (hydrogen bond) recognition tasks. The second step is Annotation. For each complex, we use the ChimeraX [34] command get sel screen to obtain the screen coordinates of all atoms. We then employ ChimeraXs built-in hydrogen bond calculation function to identify all hydrogen bonds between the pocket and ligand. All this information is annotated for every complex and stored for direct use in constructing subsequent subtaskspecific datasets. The third step is Subtask-Specific Data Generation. For each subtask, we first develop corresponding code according to its requirements. We then use ChimeraX to generate multiple image samples for every complex in the training and test sets. Finally, we design question-answering (QA) templates for each subtask and populate them with the metainformation of each sample to form complete data instances. A.2. Zooming For the zooming task, we compute the probability density function of the movement depth required to translate all interactions from their initial states to the screen center in the training set, as shown in Fig. 7. The results indicate that most movement depths fall within the range of 2080 angstroms ( A)). Through empirical testing, we observe that when the movement depth lies in the 2040 Ainterval, one-unit difference in the zooming operation (e.g., move 25 vs. move 26) produces only minor changes in the output, making them difficult to distinguish and thus unsuitable as training data (as illustrated in Fig. 8). On the other hand, for movement depths in the 6080 Arange, due to the varying spatial conformations of different PDB structures in their initial states, some structures become zoomed in to the extent that only single atom remains visible when depth values exceed 60 A. Such samples are likewise inadequate for training the model to discern specific zooming depth values. Therefore, we exclusively select depth values within the 4060 Ainterval to generate our dataset. A.3. Residue-Ligand Interaction In tasks related to interaction, the calculation of groundtruth hydrogen bonds follows the default protocol of the software ChimeraX [34]. The distance and angle cutoffs for hydrogen bonding are based on survey of small-molecule crystal structures, as described in ChimeraX. Additionally, the option to relax distance and angle criteriathat is, whether to incorporate tolerance values beyond the precise criteria for identifying hydrogen bonds (which involve several distinct distance and angle thresholds depending on the atom types involved)is also adopted from the reference provided in ChimeraX. Specifically, distance tolerance of 0.4 Aand an angle tolerance of 20 degrees are used. A.4. Ligand Docking In ligand docking task, to enable the model to better observe the respective conformational and geometric information of the protein pocket and the displaced ligand, it is essential to minimize the overlapping region between the displaced ligand and the pocket. In ChimeraX, most molecular complexes initially occupy the full vertical extent (e.g., the Yaxis) of the screen; therefore, we only consider translating the ligand horizontally (e.g., along the X-axis) to either the far left or far right side of the screen. We randomly select the first complex (PDB ID: 1ugx) from the training dataset as reference. In its native docking conformation, the mean screen coordinates of all atoms in this complex are denoted as (X base). When the ligand alone is moved to the far right side of the screen, the mean screen coordinates of all atoms in the ligand become (X base, base, base, base, base). max, xl For other complexes in the dataset, the mean screen coordinates of all atoms in each complex under the native docking conformation are obtained as (xc, yc, zc). Similarly, under the same conformation, the maximum and minimum X-axis screen coordinates of all atoms in the ligand are denoted as (xl min). Based on this, the distance required to move to the farthest point is approximated according to the Field of View. Specifically, the distance to move to the far-right is calculated as dstr = zc max, and the distance to move to the far-left is dstl = (zc base xl max). Subsequently, numerical values are randomly sampled from [0, 1, 2] and subtracted from or added to dstr and dstl, respectively. These adjusted distances are then combined with subsequent rotation operations to generate additional dataset samples. basexl base/Z base/Z B. More Results the Exact Matching AccuIn this section, we present racy of all evaluated models and human evaluators on the MiSI-Bench dataset. prediction is considered correct 1 Figure 6. Data generation pipeline. Our pipeline comprises three key stages: data collection and filtering, data annotation, and unified module for synthesizing both images and question-answer pairs using Chimera with fixed templates. Table 2. Evaluation on MiSI-Bench. We employ Exact Matching Accuracy as the evaluation metric. Bold indicates the best result among all models. Since in the Trans-Rot. and Rot-Rot. tasks, the predictions of almost all models are close to random guessing, we exclude these two rows when calculating the average scores. Methods Human Level Reasoning Models GPT-5-mini O4-mini O3 Gemini-2.5-pro Gemini-2.5-flash-lite Claude Opus4 Claude Sonnet4.5 Qwen3-vl-235b-a22b-thinking General Models GPT-41 Claude Sonnet3.5 Our Model Qwen2.5VL-7B-SFT 8 9 2 6 11 4 3 10 7 5 i s Tr t R m s. e g - g. e g - t. - Tr Rank Avg. 76.25 - 100.00 58.00 Unit Task 30.00 100. 100.00 32.00 t. - n a n r t Composite Task 60.00 92.00 26. 16.23 13.01 33.65 17.54 9.04 19.54 20.75 11.80 10.00 6.00 22.00 14.89 36.29 14.29 8.00 14.29 10.00 12.00 20.00 23.40 2.00 6.00 14.00 4.55 4.00 2.08 2.00 0.00 4.00 6.00 6.00 6.00 22.00 8.00 12.00 12.00 4.00 22.00 14.00 6.52 34.00 76.00 94.00 52.00 0.00 74.00 84.00 25. 28.00 40.00 22.00 30.61 30.00 12.00 28.00 20.40 22.44 20.00 22.44 21.62 25.00 26.00 26.00 22.00 17.50 18.02 4.00 10.00 16.00 8.16 2.00 10. 2.00 18.00 80.00 70.00 33.33 18.00 29.26 32.00 2.04 0.00 0.00 0.00 0.00 0.00 2.00 0.00 0.00 0. 47.82 30.00 36.00 38.00 26.00 34.00 38.00 38.00 36.00 28.00 57.56 98.88 97.48 27. 57.52 89.52 88.44 89.59 0.75 88. 0.78 2 r. I - 70.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0. C. Visualization In this section, we present complete examples for all nine tasks, as described in Figs. 9 to 17, respectively. For display purposes, the background color of the images has been changed from black to transparent. The highlighted content in each question represents unique information for that specific sample, while the remaining text in black constitutes the unified prompt for the subtask. Figure 7. The probability density function of the movement depth. Figure 8. Comparison of different movement depths. only if it exactly matches the ground-truth. The corresponding results are shown in Table Tab. 2. It should be noted that for the ProteinLigand Interaction task, while the original test set contains certain number of complexes without hydrogen bonds, there is only one such complex in MiSI-Bench(tiny). Given that predicting the absence of hydrogen bonds is considerably simpler than predicting all hydrogen bonds correctly, we report the Exact Matching Accuracy of the models only for complexes that contain hydrogen bonds. From the table, we can observe that when Exact Matching Accuracy is applied, the performance gap between all advancing VLMs and our SFT model becomes more pronounced. Particularly in the Translation and Rotation tasks, the transformation of the metric have limited impact on the SFT model, whereas the performance of advancing VLMs exhibits substantial decline. This indicates that while advancing VLMs possess strong potential for spatial understanding and reasoning, effective methods are required to activate this capability. For the SFT model, the performance gap with human evaluators is further widened in tasks related to interaction recgnition. This indicates that current models lack specialized domain knowledge, and suggests that incorporating such knowledge during the pre-training phase may be necessary for progressing toward more general artificial intelligence. 3 Figure 9. Sample visualization for the translation task. Zoom in for greater detail. 4 Figure 10. Sample visualization for the rotation task. Zoom in for greater detail. 5 Figure 11. Sample visualization for the zooming task. Zoom in for greater detail. 6 Figure 12. Sample visualization for the residue-ligand interaction task. Zoom in for greater detail. 7 Figure 13. Sample visualization for the translation-rotation movement task. Zoom in for greater detail. 8 Figure 14. Sample visualization for the rotation-rotation movement task. Zoom in for greater detail. 9 Figure 15. Sample visualization for the ligand docking task. Zoom in for greater detail. 10 Figure 16. Sample visualization for the interaction location task. Zoom in for greater detail. 11 Figure 17. Sample visualization for the pocket-ligand interaction task. Zoom in for greater detail."
        }
    ],
    "affiliations": [
        "DAMO Academy, Alibaba Group, Hangzhou, China",
        "Dept. of Comp. Sci. & Tech., Tsinghua University",
        "Gaoling School of Artificial Intelligence, Renmin University of China",
        "Hupan Lab, Hangzhou, China",
        "Institute for AI Industry Research (AIR), Tsinghua University",
        "MAIS, Institute of Automation, Chinese Academy of Sciences",
        "SKL-ESPC & SEPKL-AERM, College of Environmental Sciences and Engineering, Peking University"
    ]
}