{
    "paper_title": "Patient-Similarity Cohort Reasoning in Clinical Text-to-SQL",
    "authors": [
        "Yifei Shen",
        "Yilun Zhao",
        "Justice Ou",
        "Tinglin Huang",
        "Arman Cohan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Real-world clinical text-to-SQL requires reasoning over heterogeneous EHR tables, temporal windows, and patient-similarity cohorts to produce executable queries. We introduce CLINSQL, a benchmark of 633 expert-annotated tasks on MIMIC-IV v3.1 that demands multi-table joins, clinically meaningful filters, and executable SQL. Solving CLINSQL entails navigating schema metadata and clinical coding systems, handling long contexts, and composing multi-step queries beyond traditional text-to-SQL. We evaluate 22 proprietary and open-source models under Chain-of-Thought self-refinement and use rubric-based SQL analysis with execution checks that prioritize critical clinical requirements. Despite recent advances, performance remains far from clinical reliability: on the test set, GPT-5-mini attains 74.7% execution score, DeepSeek-R1 leads open-source at 69.2% and Gemini-2.5-Pro drops from 85.5% on Easy to 67.2% on Hard. Progress on CLINSQL marks tangible advances toward clinically reliable text-to-SQL for real-world EHR analytics."
        },
        {
            "title": "Start",
            "content": "Patient-Similarity Cohort Reasoning in Clinical Text-to-SQL Yifei ShenW Yilun Zhao Y"
        },
        {
            "title": "Arman Cohan Y",
            "content": "6 2 0 2 4 1 ] . [ 1 6 7 8 9 0 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Real-world clinical text-to-SQL requires reasoning over heterogeneous EHR tables, temporal windows, and patient-similarity cohorts to produce executable queries. We introduce CLINSQL, benchmark of 633 expertannotated tasks on MIMIC-IV v3.1 that demands multi-table joins, clinically meaningful filters, and executable SQL. Solving CLINSQL entails navigating schema metadata and clinical coding systems, handling long contexts, and composing multi-step queries beyond traditional text-to-SQL. We evaluate 22 proprietary and open-source models under Chain-ofThought self-refinement and use rubric-based SQL analysis with execution checks that prioritize critical clinical requirements. Despite recent advances, performance remains far from clinical reliability: on the test set, GPT-5-mini attains 74.7% execution score, DeepSeek-R1 leads open-source at 69.2% and Gemini-2.5Pro drops from 85.5% on Easy to 67.2% on Hard. Progress on CLINSQL marks tangible advances toward clinically reliable text-to-SQL for real-world EHR analytics. Data Code yifeis02/ClinSQL Barryshen1/ClinSQL"
        },
        {
            "title": "Introduction",
            "content": "Automating clinical data analysis requires bridging natural-language questions from clinicians to executable queries over complex electronic health record (EHR) databases. While large language models (LLMs) have recently excelled at text-toSQL and database reasoning on general-domain benchmarks (Yu et al., 2018; Wei et al., 2024; Yang et al., 2025), real-world clinical analysis presents distinct challenges: specialized medical terminology, fine-grained temporal reasoning across heterogeneous tables, and cohort-level clinical reasoning Equal Contributions. Correspondence: Yilun Zhao (yilun.zhao@yale.edu) Figure 1: Overview of the CLINSQL benchmark. that goes beyond point retrieval to compare similar patients under clinically meaningful constraints (Yu et al., 2018; Li et al., 2023; Wei et al., 2024). These requirements are not merely larger versions of the classic text-to-SQL problem; they demand workflows that integrate domain knowledge, temporal windows, coding systems, and outcome-aware analytics over longitudinal data (Johnson et al., 2023). Foundational text-to-SQL evaluations (e.g., WikiSQL, Spider 1.0, BIRD) catalyze progress on cross-domain parsing and database generalization (Zhong et al., 2017; Yu et al., 2018; Li et al., 2023). Recent enterprise-style benchmarks (i.e., Spider 2.0) further expose challenges from large schemas, diverse SQL dialects, and multi-step workflows (Wei et al., 2024). However, clinical settings introduce additional, domain-specific hurdles: temporal abstractions (e.g., first 24/48/72 hours), clinical ranges/units, ICD/medication coding, and cohort construction for outcome comparison. Prior clinical text-to-SQL datasets, notably MIMICSQL (Wang et al., 2020) and EHRSQL (Lee et al., 2022), demonstrate feasibility on EHR schemas but predominantly emphasize single-patient or statistical summaries and seldom require patientsimilarity cohort reasoning central to real-world clinical decision making. To bridge this gap, we introduce CLINSQL, Dataset Task Source Data Construction Patient_id Optional General Text-to-SQL Benchmarks Single-table Text-to-SQL Wikipedia and SQL tables Cross-domain, multi-table Text-to-SQL Diverse real DB schemas Real-world enterprise workflows WikiSQL (Zhong et al., 2017) Spider (Yu et al., 2018) Spider 2.0 (Wei et al., 2024) KaggleDBQA (Lee et al., 2021) Realistic DBs from Kaggle BIRD (Li et al., 2023) LiveBench (White et al., 2024) Large-scale Text-to-SQL Contamination-limited evaluation Enterprise-scale DBs Real-world multi-table DBs 95 DBs across 37 domains Mixed sources incl. DB tasks Crowdsourcing Expert annotation Expert + synthetic Author-written Qs Crowdsourcing + expert review Expert-authored, verifiable Healthcare Benchmarks Biomedical QA Exam-style multiple-choice QA Broad medical MCQ QA Medical QA w/ explanations PubMedQA (Jin et al., 2019a) MedQA (Jin et al., 2021) MedMCQA (Pal et al., 2022) MedExQA (Kim et al., 2024b) MedXpertQA (Zhang et al., 2025) Expert-level multimodal medical QA Specialty board Qs; multimodal clinical info Collection + filtering + synthesis; expert review emrQA (Pampari et al., 2018) DrugEHRQA (Wang et al., 2022) Medication-centric QA EHRXQA (Bae et al., 2023) Multi-modal EHR QA EHRNoteQA (Kweon et al., 2024) Discharge-summary QA DischargeQA (Ou et al., 2025) RadQA (Soni et al., 2022) Template generation (i2b2) Template generation + sample human check Derived from MIMIC-CXR-VQA & EHRSQL; curated GPT-4 generation + clinician review Generated from discharge data Physician-authored Qs + span annotation De-identified clinical notes i2b2 EHR notes + structured meds Notes + chest X-ray images Real EHR discharge summaries EHR discharge summaries Radiology reports Heuristic generation + manual labels Exam scrape Exam scrape Manual collection/cleaning PubMed abstracts Medical board exam questions Multi-subject exam questions Mock tests & online exams Discharge-related clinical QA Radiology report QA Template-driven clinical QA MIMICSQL (Wang et al., 2020) NL SQL clinical EHRSQL (Lee et al., 2022) EHRSQL-ST (Lee et al., 2024) EHR-SeqSQL (Ryu et al., 2024) NL SQL EHR Practical NL SQL Reliable Text-to-SQL evaluation EHR Text-to-SQL Benchmarks MIMIC-III structured tables Hospital EHR schemas Same family of EHR schemas Institutional EHR DB Auto-generated Qs + crowdsourcing filter Hospital-staff utterances + manual SQL annotation Organizer-curated evaluation splits Decomposition of EHRSQL into sequential tasks CLINSQL Text-to-SQL with advanced reasoning EHR tables Expert annotation + validation; fine-grained eval rubrics - - - - - - - - - - - Table 1: Comparison of CLINSQL with existing Text-to-SQL and Healthcare Benchmarks. The Patient_id Optional column indicates whether benchmark supports supplying an optional de-identified anchor patient identifier (e.g., MIMIC subject_id/hadm_id) alongside the question to ground patient-similarity or patient-specific queries. : supported; : not supported; -\": not applicable. benchmark of 633 expert-annotated clinical text-toSQL tasks on the MIMIC-IV v3.1 database (Johnson et al., 2023). high-level benchmark overview appears in Figure 1, and Figure 2 details the construction pipeline: we design six scenario types to reflect real clinical settings. Each example is grounded in concrete scenario and requires composing multi-table, temporally aware SQL with patient-similarity cohort construction. Difficulty is stratified by SQL and clinical reasoning complexity. We adopt rubric-based evaluation with critical-first aggregation and execution checks that verify result format and clinical plausibility while allowing equivalent formulations. We evaluate 22 proprietary and open-source models with Chain-of-Thought self-refinement and find that CLINSQL remains challenging: Gemini2.5-Pro drops from 85.5% execution on Easy to 67.2% on Hard; GPT-5-mini leads overall test execution at 74.7%, and DeepSeek-R1 tops opensource models at 69.1%. Even for these models, Hard split execution remains below 70%, underscoring the difficulty of CLINSQL. Our error analysis reveals that most failures stem from cohort specification drift (e.g., relaxed ICD/item constraints), schema or output mismatches, and mis-specified clinical aggregations, even for top-performing models. Guided by these findings, we further study schema-hinted inference setting that foregrounds clinically validated filters and expected outputs, yielding consistent execution gains, especially on medium and hard cases. We summarize our contributions as follows: We introduce clinically grounded text-to-SQL benchmark that requires patient-similarity cohort construction and multi-step temporal reasoning over heterogeneous EHR tables. We curate six families of realistic clinical scenarios and provide rubric-structured evaluation for reliable automated evaluation. We benchmark 22 proprietary and open-source models and release rubric-based error taxonomy that highlights the challenges that future clinical text-to-SQL systems must address."
        },
        {
            "title": "2 Related Work",
            "content": "General Text-to-SQL Benchmarks. Generaldomain Text-to-SQL has evolved through successive foundational benchmarks (Zhong et al., 2017; Yu et al., 2018; Lee et al., 2021; Li et al., 2023; Wei et al., 2024; White et al., 2025). However, healthcare Text-to-SQL presents fundamental challenges that distinguish it from general-domain applications, requiring medical terminology, complex temporal relationships, and clinical reasoning that extends beyond standard database operations to Figure 2: Overview of CLINSQL construction pipeline. The process begins with scenario design and patient selection, followed by question authoring. Annotators then perform database analysis and schema mapping on MIMIC-IV, write executable gold SQL, and construct tree-structured rubrics for SQL and results validation. incorporate medical decision-making logic (Lee et al., 2022; Wang et al., 2020). Most critically, the patient similarity reasoning paradigm central to CLINSQL represents fundamental departure from general Text-to-SQL evaluation, as healthcare queries require identifying patient cohorts based on multi-dimensional similarity criteria rather than simple retrieval or aggregation operations. get specific patients with known identifiers, thereby representing only limited subset of actual clinical analysis scenarios. Our work addresses these limitations by centering on patient-similarity cohort reasoning over MIMIC-IV v3.1, requiring models to define cohorts, apply temporal and phenotyping logic, and compute stratified cohort-level outcomes that mirror real clinical workflows. Healthcare NLP/ML Benchmarks. Healthcare NLP benchmarks have evolved from general medical knowledge QA (Jin et al., 2019b; Hendrycks et al., 2021; Jin et al., 2021; Pal et al., 2022; Singhal et al., 2022; Wang et al., 2024; Kim et al., 2024b) to sophisticated clinical QA tasks with expert-level capabilities and real clinical data utilization (Pampari et al., 2018; Saleh and Pecina, 2019; Suominen et al., 2020; Bardhan et al., 2022; Bae et al., 2023; Kweon et al., 2024; Kim et al., 2024b; Zhang et al., 2025; Chen et al., 2025; Ou et al., 2025). However, as shown in Table 1, existing clinical Text-to-SQL datasets (Wang et al., 2020; Lee et al., 2022; Ryu et al., 2024; Lee et al., 2024; Sivasubramaniam et al., 2024; Kim et al., 2024a) predominantly emphasize statistical analyses rather than addressing authentic clinical questions encountered in realworld practice, and consistently assume queries tar-"
        },
        {
            "title": "3 Benchmark Construction",
            "content": "CLINSQL is designed to comprehensively evaluate Text-to-SQL capabilities within realistic clinical scenarios. Our benchmark, built upon MIMIC-IV v3.1 (Johnson et al., 2023), incorporates complex analytical scenarios that require sophisticated clinical reasoning and the multi-step integration of diverse clinical data. Figure 2 provides an overview of our benchmark construction pipeline. Table 2 presents the six core clinical scenarios that reflect real-world healthcare data analysis needs and clinical decision-making. Concrete scenario examples and rubric trees are provided in Appendix B. In the following sections, we detail the query annotation, SQL annotation, evaluation guideline annotation, and data validation. The expert annotator biographies are summarized in Appendix A, and Clinical Scenarios Example Question Provided in Appendix B. Patient Demographics and Admissions Analysis of patient demographics and administrative data (admissions, length of stay), testing foundational SQL skills and understanding of clinical administrative workflow. For an 81-year-old female: among female Medicare patients aged 7686 transferred from another hospital with principal AMI (ICD-9 410*/ICD-10 I21*), report 30-day readmission rate; median index LOS for readmitted vs not; percent index stays > 4 days. Complexity: Hard (Appendix B.1) Vital Signs Monitoring Temporal analysis of vital signs (e.g., blood pressure, heart rate), designed to test timeseries reasoning, understanding of clinical normal ranges, and trend identification capabilities. have 60-year-old man in the ICU. In male ICU patients aged 5565 with HFNC within 24 hours versus condition-matched ICU controls, what are the instability score median and p25/p75/p95, tachycardia and hypotension burden, ICU LOS and mortality? Complexity: Medium (Appendix B.2) Laboratory Results Analysis Analysis of trends in laboratory results, designed to test knowledge of medical terminology, unit conversions, and the ability to correlate lab values with clinical conditions. have 51-year-old female with suspected ACS. Among female ACS admissions age 4656, what are counts, percentages, and mean hospital length of stay for first hs-TnT: Normal, Borderline, Myocardial Injury? Complexity: Medium (Appendix B.3) Medication Management Analysis of medication regimens (prescriptions, dosing, interactions), designed to test complex temporal reasoning and pharmacological knowledge to ensure medication safety. have 64-year-old female inpatient. Among females aged 5969, whats the IQR of single inpatient amiodarone prescription durations (days)? Complexity: Easy (Appendix B.4) Diagnostic Procedures Temporal sequencing of diagnostic procedures and interventions, designed to evaluate the understanding of clinical workflows, procedural relationships, and care coordination. Evaluating an 88-year-old man: among male patients aged 8393 with sepsis on their first ICU stay, stratify first-72-hour diagnostic intensity (distinct procedures) into quartiles and report mean procedure counts, mean ICU LOS in days, and mortality (%) per quartile. Complexity: Hard (Appendix B.5) Disease Diagnosis and Outcomes Analysis of diagnoses (ICD-9/10 codes), comorbidities, and clinical outcomes, designed to test knowledge of medical coding, integrated clinical reasoning, and the ability to assess treatment effectiveness and patient prognosis. have 75-year-old female inpatient with pulmonary embolism. For female inpatients aged 7080 with PE, stratify into risk-score quintiles and report per quintile: 90-day mortality, general 7080 female 90-day mortality (comparison), AKI and ARDS rates, and median survivor LOS. Complexity: Hard (Appendix B.6) Table 2: Definition of clinical scenario types in CLINSQL. the annotation interface is shown in Appendix I."
        },
        {
            "title": "3.1 Query Annotation",
            "content": "Clinical Scenario Development. Each annotator is assigned one of six scenario types and selects representative patient from MIMIC-IV v3.1 (Johnson et al., 2023). Sampling is stratified across five dimensions: (1) Age uses scenario-specific ranges spanning 2585 years; (2) Clinical condition covers major categories (e.g., cardiovascular, respiratory, metabolic, infectious, post-operative); (3) Healthcare utilization varies admission type (e.g., emergency, elective, urgent), insurance (e.g., Medicare, Medicaid, commercial), and length of stay (e.g., 215 days); (4) Acuity distinguishes settings (e.g., ward vs ICU) with risk strata and monitoring intensity; and (5) Temporal windows include early windows (e.g., first 24/48/72 hours), the full hospitalization, and procedure-specific periods. To prevent data contamination and ensure benchmark integrity, all information related to the selected patients is removed from the database prior to model evaluation. Natural Language Question Formulation. Annotators craft natural language questions that physicians would realistically ask given the provided patient information and scenario type. Each question must require database querying and cannot be answered through simple observation (e.g., questions requiring temporal analysis or aggregation across multiple records). Questions incorporate appropriate medical terminology while maintaining clarity and clinical authenticity."
        },
        {
            "title": "3.2 SQL Annotation",
            "content": "Our SQL annotation process is divided into two steps: database analysis and schema mapping, and gold-standard SQL construction. Each clinical question undergoes comprehensive database analysis by annotators, followed by the development of gold-standard SQL to produce high-quality executable queries. Database Analysis and Schema Mapping. For each natural language clinical question, annotators first conduct systematic database analysis to identify the required MIMIC tables and establish the necessary relationships between clinical entities. This process includes locating relevant database tables (e.g., patients, admissions, diagnoses_icd), identifying key features including specific columns and clinical values (e.g., gender=M, icd_code LIKE 410%), and mapping clinical concepts to database schema elements while considering temporal constraints and data integrity requirements. concise schema reference for MIMIC-IV is provided in Appendix P. Gold-Standard SQL Construction. Following the database analysis phase, expert annotators develop comprehensive gold-standard SQL queries Algorithm 1 Critical-First Score Aggregation Require: rubric tree , node weights , critical flags C, sequential flags function EVALUATENODE(node) if node is leaf then return LLM judge evaluation score {0, 1} end if critical_children {c children : C[c] = true} noncritical_children {c children : C[c] = false} for critical_children do score[c] EVALUATENODE(c) if score[c] = 1 then return 0 end if end for if noncritical_children = then return 1 end if sum 0, total_weight 0 for noncritical_children do score[c] EVALUATENODE(c) sum sum+W [c]score[c], total_weight total_weight + [c] if S[c] = true score[c] = 0 then break end if sum total_weight end for return end function inal_score EVALUATENODE(root) return inal_score Figure 3: Example of SQL evaluation rubric tree. Figure 4: Example of an executed result rubric tree. that accurately translate clinical questions into executable database operations. Each SQL implementation undergoes rigorous development processes including multi-table join construction with proper foreign key relationships to ensure data consistency, temporal constraint implementation using appropriate date functions and time-based filtering (e.g., DATE_DIFF for length of stay calculations, charttime-based temporal analysis), clinical value range validation incorporating medical domain knowledge and normal physiological parameters (e.g., hemoglobin levels between 7-18 g/dL, age calculations using anchor_age and anchor_year), edge case handling for common clinical database issues including null value management and data quality constraints, and query optimization to maintain computational efficiency while preserving clinical accuracy."
        },
        {
            "title": "3.3 Evaluation Guideline Annotation",
            "content": "To support reliable automated evaluation, each clinical question is accompanied by guideline comprising two rubric trees: one for SQL evaluation and one for executed results. Rubric Design Principles. Our evaluation framework employs tree-structured rubrics that hierarchically decompose complex evaluation tasks into granular, verifiable criteria (Gou et al., 2025). Each rubric tree consists of internal nodes representing high-level evaluation aspects and leaf nodes defining specific binary verification criteria. The SQL evaluation rubric (Figure 3) assesses query construction across four primary dimensions: Patient Cohort Construction, Medical Concept Implementation, Database Integration, and Clinical Analytics, while the results rubric (Figure 4) focuses on output validation and clinical value assessment. Following practices in automated evaluation (Starace et al., 2025), we implement three key structural components: (1) Critical vs. Non-Critical Nodes, where critical nodes represent essential requirements whose failure immediately causes parent failure, while non-critical nodes allow partial scoring; (2) Sequential Dependencies, where sequential nodes indicate dependencies and earlier failures short-circuit subsequent evaluations; (3) Weighted Scoring, where each node is assigned weight from 1 to 3 based on importance: 1 denotes basic supportive criteria, 2 indicates standard requirements, and 3 marks critical elements that are essential to validity and require substantial domain expertise. Our scoring system employs Critical-First ScorStatistics Total Examples Avg. Question Length Avg. SQL Tokens Evaluation Trees Easy 190 22.86 158.79 Med. 254 36.04 452. Hard 189 45.52 615.62 14.80 / 3.05 18.27 / 3.06 19.03 / 3.11 SQL (Nodes/Depth) SQL Avg. Words 17.74 Results (Nodes/Depth) 10.01 / 4.00 19.62 / 4.02 23.54 / 4.04 7.98 Results Avg. Words 15.55 16. 8.21 6.52 Table 3: Basic statistics of CLINSQL. ing adapted from recent agentic evaluation frameworks (Gou et al., 2025), detailed in Algorithm 1. 3.4 Data Validation Each annotated example undergoes comprehensive validation by an expert annotator within the medical research field. The validation framework examines four critical aspects: clinical question assessment evaluates real-world relevance, medical terminology accuracy, and linguistic quality; SQL implementation review verifies correct MIMIC-IV database (Johnson et al., 2023) standards and technical execution; output verification confirms structural integrity and medical plausibility; evaluation framework review ensures comprehensive component coverage and unambiguous scoring standards. Validators revise examples with minor issues or reject those with significant problems. Upon passing all validation checks, examples receive \"Validated\" status for dataset inclusion. To assess problem difficulty and provide fine-grained evaluation of model capabilities, we stratify our dataset into three difficulty levels based on SQL complexity and clinical reasoning requirements: (1) Easy (30%): Few-table queries with basic filtering and clinical concepts; (2) Medium (40%): Multi-table joins with temporal filtering and moderate reasoning; (3) Hard (30%): Complex multi-join queries with nested subqueries and advanced logic. Table 3 presents the data statistics of CLINSQL."
        },
        {
            "title": "4.1 SQL Analysis using Evaluation Guideline",
            "content": "SQL evaluation employs rubric-driven process that decomposes each candidate query into clinically relevant checks. The guideline distinguishes critical from supportive requirements and encodes sequential dependencies such that subsequent reasoning is evaluated only if prerequisite steps are satisfied. Candidates must construct an appropriate cohort and map clinical concepts to schema and codes; subsequently, table relationships, join keys, type handling, grouping, and aggregation are verified, followed by task-specific interpretation. Leaf criteria receive binary decisions. Scores are aggregated with critical-first rule: any failed critical node collapses its parent, whereas non-critical checks contribute via weighted averaging only after critical prerequisites are met. Consistency is ensured by employing GPT-5 as the judge, which leverages strong instruction following and longcontext capacity to compare rubric text, gold SQL, and schema hints. Concise rationales are recorded for each leaf decision to support error analysis. 4.2 SQL Execution Result Evaluation The execution-level evaluation examines CSV outputs produced by executing the candidate SQL. Format compliance is first enforced, including file presence, exact column names, absence of nulls, and basic type checks; clinical plausibility is then assessed using per-column value ranges grounded in the cohort and task definition. Plausible bands admit clinically equivalent answers, whereas acceptable bands tighten tolerance; these criteria support equivalence across alternative but valid formulations. Sequential gating prevents downstream clinical judgments from obscuring upstream format defects. Leaf decisions are binary and are aggregated under the same critical-first rule. The same GPT-5 judge issues decisions and concise rationales by comparing the CSV with the rubric and gold references, yielding interpretable discrepancies in schema adherence, unit handling, rounding, and cohort-conditioned statistics. The LLM-as-Judge prompts are provided in Appendix O. We provide the reliability and reconciliation analyses of our proposed rubric-based evaluation in Appendix and L, with SQL-execution divergence statistics summarized in Appendix M."
        },
        {
            "title": "5 Experiment",
            "content": "This section discusses the experiment setup and our experiment results and analysis."
        },
        {
            "title": "5.1 Experiment Setup",
            "content": "We evaluate all models on CLINSQL using rubricbased metrics specifically designed for clinical textto-SQL tasks. Our primary evaluation metrics are the SQL Score and the Execution Score. We consider the following categories of models: (1) Model GPT-5-mini Gemini-2.5-Pro GPT-5 GPT-4.1 Gemini-2.5-Flash OpenAI o4-mini Grok-4-Fast-Reason. GPT-5-nano Mistral-Medium Grok-4-Fast-Non-Reason. DeepSeek-R1 DeepSeek-V3.1 Qwen3-235B-A22B-Ins. Qwen3-Coder-480B-A35B-Ins. Qwen3-Next-80B-A3B-Ins. Qwen3-235B-A22B-Think. Llama-4-Maverick-17B-128E-Ins. Llama-4-Scout-17B-16E-Ins. Qwen3-Next-80B-A3B-Think. Baichuan-M2-32B MedGemma-27B SQLCoder-7B-2 Easy Test Set Medium Hard Avg. Validation Avg. Test SQL Exec SQL Exec SQL Exec SQL Exec SQL Exec 54.07 53.15 58.56 59.54 55.86 54.62 49.47 40.83 42.61 54.16 45.32 57.54 38.14 48.97 48.34 34.71 39.82 40.79 46.07 36.09 32.66 6.65 81.31 85.46 73.79 71.20 72.64 67.96 70.82 55.42 56.05 39.31 75.73 71.63 71.85 64.58 67.85 55.51 59.14 36.57 27.73 23.10 6.52 0. Proprietary Models 37.12 46.34 41.29 42.30 45.61 36.59 40.34 34.11 32.81 32.33 73.46 69.89 66.94 68.55 64.05 58.37 53.18 54.67 42.50 29.80 38.94 42.71 39.58 38.25 41.67 34.13 39.40 34.17 31.47 35.69 Open-source Models 45.91 38.78 37.72 36.00 29.93 43.41 27.23 24.04 37.30 30.44 16.60 6. 68.43 58.38 54.48 56.04 45.83 44.72 47.42 28.69 30.66 13.24 3.12 0.00 43.16 34.83 32.77 33.27 26.81 34.71 20.98 20.23 31.25 23.17 14.92 2.30 69.69 67.22 65.08 63.39 58.72 51.68 53.23 45.14 37.68 22.39 63.59 52.99 51.05 54.69 35.38 46.67 36.01 26.55 21.36 10.11 2.65 0.00 42.30 45.31 42.62 44.69 47.06 39.41 42.67 33.18 32.71 34.77 42.63 38.90 36.24 35.54 34.48 38.08 29.36 26.88 37.77 26.60 21.03 3. 75.16 76.14 66.52 69.92 66.75 59.07 56.58 51.58 43.81 30.10 69.79 61.46 58.15 60.51 43.41 51.11 51.63 31.44 29.06 11.40 4.46 0.00 42.72 47.28 45.93 46.23 47.48 41.23 42.78 36.13 35.33 39.85 44.91 43.19 36.36 39.05 34.48 38.20 29.11 27.89 38.13 29.97 20.92 5.29 74.67 73.73 68.42 67.79 65.01 59.22 58.46 52.03 45.10 30.41 69.15 60.71 58.63 58.18 49.26 48.54 47.49 30.40 27.01 15.27 4.00 0. Table 4: SQL score and execution score (%) on CLINSQL validation and test sets using CoT prompting with self-refinement. Scenario-level scores are presented in Appendix H. Open-source general-purpose LLMs: DeepSeekR1 (DeepSeek-AI et al., 2025a), DeepSeekV3.1 (DeepSeek-AI et al., 2025b), Qwen3-Coder series, Qwen3-Instruct series, Qwen3-Thinking series (Qwen Team, 2025), Llama-4 series (Meta (2) Proprietary models: GPT-5 AI, 2024). series (OpenAI, 2025b), Gemini-2.5 series (Comanici et al., 2025), GPT-4.1 (OpenAI, 2025a), o4-mini (OpenAI, 2025c), Grok-4-Fast series (xAI, 2025b,a), and Mistral-Medium (Mistral AI, 2025). (3) Text-to-SQL models: SQLCoder-7B-2 (De- (4) Medical-domain LLMs: fog.ai, 2024). MedGemma-27B (Google DeepMind, 2025) and Baichuan-M2-32B (Baichuan AI, 2025). For opensource models, we perform inference using vLLM pipeline (Kwon et al., 2023), while proprietary models are accessed through official APIs. We evaluate models under two prompting regimes: Direct Output and Chain-of-Thought (CoT). In both regimes, the model must return single executable BigQuery query. If execution fails, we run up to two self-refinement rounds that feed the question, the prior SQL, and the BigQuery error back to the model with minimal-edit instructions. We then extract the final fenced SQL block and execute it. We apply self-refinement to both regimes because many models have low first-pass execution success, and single correction round is insufficient; Appendix reports the attempt-wise success rates that motivate this choice. Prompt templates for both regimes and the refinement procedure are provided in Appendix D. Parameter settings and model configurations appear in Appendix E."
        },
        {
            "title": "5.2 Main Findings",
            "content": "Table 4 presents SQL and execution scores on CLINSQL. We highlight the following findings: CLINSQL presents substantial challenges for current foundation models. While GPT-5-mini achieves the best average execution score, performance on the Hard split remains modest: leading proprietary models stay under 70% (e.g., GPT-5mini 69.7% and Gemini-2.5-Pro 67.2%). Gemini2.5-Pro also drops by 18.24% from Easy to Hard. Open-sourced models performance. DeepSeekR1 attains 69.2% average test execution with 44.9% SQL score, and it surpasses several proprietary baselines, including o4-mini and both Grok-4 variants. However, open-source models still lag the strongest proprietary models: the best proprietary model (GPT-5-mini) reaches 74.7% execuSetting SQL Score Easy Medium Hard Baseline CoT Schema-hinted CoT 58.273.49 41.094.69 36.40 54.78 37.90 42.044. Execution Score Baseline CoT Schema-hinted CoT 82.742.78 85.8510.21 77.037.20 69.83 75.63 79.96 Figure 5: Execution score comparison on the validation set for representative models. Full SQL and execution comparisons for all models are provided in Appendix G. tion, about 5.5 points higher than DeepSeek-R1 and roughly 14 to 16 points ahead of DeepSeek-V3.1 (60.7%) and Qwen3-Coder-480B-A35B-Instruct (58.2%). Even so, these results show the gap is narrowing as techniques mature. CoT reasoning generally improves model performance compared to directly outputting the final SQL. As shown in Figure 5, the extent of improvement varies across models. Qwen3-235BA22B-Instruct increases from 53.6% to 58.2% and Qwen3-Coder-480B-A35B-Instruct from 56.1% to 60.5%, while Llama-4-Scout-17B-16E-Instruct rises only from 29.3% to 31.4% and Grok-4-FastNon-Reasoning from 27.4% to 30.1%. Strong proprietary models see modest gains (Gemini-2.5-Pro from 73.6% to 76.1%, GPT-5-mini from 72.7% to 75.2%). Even lower baselines benefit (MedGemma27B from 4.0% to 4.5%)."
        },
        {
            "title": "5.3 Error Analysis and Case Study",
            "content": "To better characterize failure modes, we randomly select 10 cases from each of six validation scenarios generated by GPT-5-mini and analyze rubric feedback from our judge model. We observe three common error types: Cohort specification & codin which explicit ICD or itemid ing (54%): constraints are replaced by keyword heuristics or key joins are relaxed, broadening cohorts; Output schema & formatting (24%): omitted required columns, invalid values, or naming mismatches that trigger schema checks; and Aggregation & clinical statistics (14%): mis-specified denominators or missing normalization leading to implausible rates. Other observed errors include occasional temporal boundary mistakes and pattern-specific issues seen in GPT-5-mini outputs, such as using quartiles instead of percentiles in APPROX_QUANTILES Table 5: Validation performance of GPT-5-mini under baseline and schema-hinted Chain-of-Thought configurations. Superscripts denote absolute percentage-point gains of the schema-hinted setting over the baseline. (e.g., employing quartile buckets rather than 100quantile offsets), which yields incorrect reported statistics. Examples for each error type are provided in Appendix C. 5.4 Schema-Hinted Inference Analysis The preceding failure analysis underscores cohort drift and schema mismatches as dominant error sources. This motivates our exploration of Schema-Hinted inference configuration designed to mitigate these common failures. The setting augments the standard Chain-of-Thought prompting and self-refinement schedule with schema hints, foregrounding clinically validated ICD filters and expected result columns. Full setup details are provided in Appendix F. We evaluate this configuration with GPT-5-mini on the validation split, and the results show consistent gains over the baseline. As summarised in Table 5, SQL and execution accuracy improve across all difficulty tiers. The most pronounced execution gains are observed on medium and hard queries, where providing clinically validated ICD filters and expected result columns better constrains the inference process."
        },
        {
            "title": "6 Conclusion",
            "content": "We propose CLINSQL, benchmark for realistic clinical text-to-SQL analytics. It captures core challenges of real EHR practice, including heterogeneous tables, temporal windows, and patientsimilarity cohort construction. We assess broad set of models using the developed rubric-based evaluation protocols and observe that, despite recent advances, performance remains well short of clinically reliable operation: execution frequently exceeds SQL correctness and errors cluster around cohort specification, schema/formatting, and aggregation/clinical statistics. CLINSQL establishes rigorous, domain-grounded target for clinical research and advance trustworthy EHR analytics."
        },
        {
            "title": "Limitations",
            "content": "While CLINSQL advances clinically grounded text-to-SQL evaluation, several limitations remain. First, the current benchmark is built on MIMIC-IV v3.1data from single health systemand targets single SQL environment, which may limit transferability to other EHR ecosystems, data models, and database backends. Second, CLINSQL depends on substantial domain-expert involvement for scenario specification, gold-standard SQL authoring, and rubric-aligned leaf rationales. Although this expert curation provides high-fidelity supervision, the associated training, annotation, and review burden reduces throughput and makes it difficult to scale to substantially larger datasets without additional tooling or alternative supervision strategies."
        },
        {
            "title": "References",
            "content": "Seongsu Bae, Sunjun Kweon, Taewoong Jang, and Edward Choi. 2023. Ehrxqa: multi-modal question answering dataset for electronic health records with chest x-ray images. arXiv preprint arXiv:2310.18652. Baichuan AI. 2025. Baichuan-M2-32B. https: //huggingface.co/baichuan-inc/ Baichuan-M2-32B. Model card. Jayetri Bardhan, Anthony Colas, Kirk Roberts, and Daisy Zhe Wang. 2022. DrugEHRQA: question answering dataset on structured and unstructured electronic health records for medicine related queries. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 10831097, Marseille, France. European Language Resources Association. Hanjie Chen, Zhouxiang Fang, Yash Singla, and Mark Dredze. 2025. Benchmarking large language models on answering and explaining challenging medical questions. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 35633599, Albuquerque, New Mexico. Association for Computational Linguistics. Gheorghe Comanici, many others, and the Gemini Team. 2025. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. 2025a. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wanjia Zhao, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaokang Zhang, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xingkai Yu, Xinnan Song, Xinxia Shan, Xinyi Zhou, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. X. Zhu, Yang Zhang, Yanhong Xu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Yu, Yi Zheng, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Ying Tang, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yu Wu, Yuan Ou, Yuchen Zhu, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yukun Zha, Yunfan Xiong, Yunxian Ma, Yuting Yan, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Z. F. Wu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhibin Gou, Zhicheng Ma, Zhigang Yan, Zhihong Shao, Zhipeng Xu, Zhiyu Wu, Zhongyu Zhang, Zhuoshu Li, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Ziyi Gao, and Zizheng Pan. 2025b. Deepseekv3 technical report. Defog.ai. 2024. https:// SQLCoder-7B-2. huggingface.co/defog/sqlcoder-7b-2. Model card. Google DeepMind. Text-only 2025. MedGemma (medgemma-27b-text-it). 27B https://huggingface.co/google/ medgemma-27b-text-it. Model card. Boyu Gou, Zanming Huang, Yuting Ning, Yu Gu, Michael Lin, Weijian Qi, Andrei Kopanev, Botao Yu, Bernal Jiménez Gutiérrez, Yiheng Shu, Chan Hee Song, Jiaman Wu, Shijie Chen, Hanane Nour Moussa, Tianshu Zhang, Jian Xie, Yifei Li, Tianci Xue, Zeyi Liao, Kai Zhang, Boyuan Zheng, Zhaowei Cai, Viktor Rozgic, Morteza Ziyadi, Huan Sun, and Yu Su. 2025. Mind2web 2: Evaluating agentic search with agent-as-a-judge. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. In International Conference on Learning Representations. Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. 2021. What disease does this patient have? large-scale open domain question answering dataset from medical exams. Applied Sciences, 11(14):6421. Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. 2019a. Pubmedqa: dataset for biomedical research question answering. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 25672577. Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. 2019b. PubMedQA: dataset for biomedical research question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2567 2577, Hong Kong, China. Association for Computational Linguistics. Alistair EW Johnson, Lucas Bulgarelli, Lu Shen, Alvin Gayles, Ayad Shammout, Steven Horng, Tom Pollard, Sicheng Hao, Benjamin Moody, Brian Gow, et al. 2023. Mimic-iv, freely accessible electronic health record dataset. Scientific Data, 10(1):19. Zong Ke, Yuqing Cao, Zhenrui Chen, Yuchen Yin, Shouchao He, and Yu Cheng. 2025a. Early warning of cryptocurrency reversal risks via multi-source data. Finance Research Letters, page 107890. Zong Ke, Jiaqing Shen, Xuanyi Zhao, Xinghao Fu, Yang Wang, Zichao Li, Lingjie Liu, and Huailing Mu. 2025b. stable technical feature with gru-cnnga fusion. Applied Soft Computing, page 114302. Hajung Kim, Chanhwi Kim, Hoonick Lee, Kyochul Jang, Jiwoo Lee, Kyungjae Lee, Gangwoo Kim, and Jaewoo Kang. 2024a. KU-DMIS at EHRSQL 2024 : Generating SQL query via question templatization in EHR. In Proceedings of the 6th Clinical Natural Language Processing Workshop, pages 672686, Mexico City, Mexico. Association for Computational Linguistics. Yunsoo Kim, Jinge Wu, Yusuf Abdulle, and Honghan Wu. 2024b. MedExQA: Medical question answering benchmark with multiple explanations. In Proceedings of the 23rd Workshop on Biomedical Natural Language Processing, pages 167181, Bangkok, Thailand. Association for Computational Linguistics. Sunjun Kweon, Jiyoun Lee, Joon Sung Yi, and Edward Choi. 2024. Ehrnoteqa: An llm benchmark for realworld clinical practice using discharge summaries. arXiv preprint arXiv:2402.16040. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. Chia-Hsuan Lee, Oleksandr Polozov, and Matthew Richardson. 2021. Kaggledbqa: Realistic evaluation of text-to-sql parsers. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 22612273. Anusri Pampari, Preethi Raghavan, Jennifer Liang, and Jian Peng. 2018. emrqa: large corpus for question answering on electronic medical records. Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 23572368. Qwen Team. 2025. Qwen3 technical report. Gyubok Lee, Hyeonji Hwang, Seongsu Bae, Yeonsu Kwon, Woncheol Shin, Seongjun Yang, Minjoon Seo, Jong-Yeup Kim, and Edward Choi. 2022. Ehrsql: practical text-to-sql benchmark for electronic health records. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Jaehee Ryu, Seonhee Cho, Gyubok Lee, and Edward Choi. 2024. EHR-SeqSQL : sequential text-toSQL dataset for interactively exploring electronic health records. In Findings of the Association for Computational Linguistics: ACL 2024, pages 16388 16407, Bangkok, Thailand. Association for Computational Linguistics. Gyubok Lee, Sunjun Kweon, Seongsu Bae, and Edward Choi. 2024. Overview of the EHRSQL 2024 shared task on reliable text-to-SQL modeling on electronic health records. In Proceedings of the 6th Clinical Natural Language Processing Workshop, pages 644 654, Mexico City, Mexico. Association for Computational Linguistics. Shadi Saleh and Pavel Pecina. 2019. An extended CLEF ehealth test collection for cross-lingual information retrieval in the medical domain. In Advances in Information Retrieval: 41st European Conference on Information Retrieval (ECIR 2019), volume 11438 of Lecture Notes in Computer Science, pages 188195. Springer. Jinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin Wang, Bowen Qin, Ruiying Geng, Nan Huo, et al. 2023. Can llm already serve as database interface? big bench for large-scale database grounded text-to-sql evaluation. Advances in Neural Information Processing Systems, 36. Meta AI. 2024. Introducing llama 4: Advancing multimodal intelligence. Mistral AI. 2025. Mistral medium: Model overview and documentation. Product Documentation. OpenAI. 2025a. Gpt-4.1 and gpt-4.1 mini release notes. Release notes (14 May 2025) announcing GPT-4.1 and GPT-4.1 mini; GPT-4.1 excels at coding and precise instruction following, while GPT-4.1 mini is fast, efficient model with 1 M-token context window. OpenAI. 2025b. Introducing GPT-5 for developers. Announces API model sizes including gpt-5-mini. OpenAI. 2025c. Openai o4-mini: reasoning model release. Blog post (16 Apr 2025) announcing the o3 and o4-mini models; o4-mini is small model optimized for fast, cost-efficient reasoning. Justice Ou, Tinglin Huang, Yilun Zhao, Ziyang Yu, Experience Peiqing Lu, and Rex Ying. 2025. retrieval-augmentation with electronic health records enables accurate discharge qa. Kaichen Ouyang, Zong Ke, Shengwei Fu, Lingjie Liu, Puning Zhao, and Dayu Hu. 2024. Learn from global correlations: Enhancing evolutionary algorithm via spectral gnn. arXiv preprint arXiv:2412.17629. Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. 2022. Medmcqa: large-scale multisubject multi-choice dataset for medical domain question answering. Proceedings of the Conference on Health, Inference, and Learning, pages 248260. Karan Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne, Martin Seneviratne, Paul Gamble, Chris Kelly, Nathaneal Scharli, Aakanksha Chowdhery, Philip Mansfield, Blaise Aguera Arcas, Dale Webster, Greg S. Corrado, Yossi Matias, Katherine Chou, Juraj Gottweis, Nenad Tomasev, Yun Liu, Alvin Rajkomar, Joelle Barral, Christopher Semturs, Alan Karthikesalingam, and Vivek Natarajan. 2022. Large language models encode clinical knowledge. Sithursan Sivasubramaniam, Cedric Osei-Akoto, Yi Zhang, Kurt Stockinger, and Jonathan Fuerst. 2024. SM3-text-to-query: Synthetic multi-model In The Thirtymedical text-to-query benchmark. eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Siddharth Soni, Kirk Roberts, Daqing Wang, Arvind Subburathinam, and Sivaramakrishnan Long. 2022. Radqa: question answering dataset to improve comprehension of radiology reports. In Proceedings of the Language Resources and Evaluation Conference, pages 65676577. Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon Maksin, Rachel Dias, Evan Mays, Benjamin Kinsella, Wyatt Thompson, Johannes Heidecke, Amelia Glaese, and Tejal Patwardhan. 2025. Paperbench: Evaluating ais ability to replicate ai research. Hanna Suominen, Liadh Kelly, Lorraine Goeuriot, and Martin Krallinger. 2020. Clef ehealth evaluation In Advances in Information Retrieval: lab 2020. 42nd European Conference on Information Retrieval (ECIR 2020), Part II, volume 12036 of Lecture Notes in Computer Science, pages 587594. Springer. Logesh Kumar Umapathi Wang, Ankit Pal, and Malaikannan Sankarasubbu. 2022. Drugehrqa: Yifan Zhang, Jingqing Chen, Qiao Yuan, Zhihao Ding, Huaishao Luo, Kaixiong Pan, Mengzhuo Zhang, Haiyang Yu, Qingyun Chen, Xiangru Wang, et al. 2025. Medxpertqa: Benchmarking expert-level medical reasoning and understanding. arXiv preprint arXiv:2501.18362. Victor Zhong, Caiming Xiong, and Richard Socher. 2017. Seq2sql: Generating structured queries from natural language using reinforcement learning. In arXiv preprint arXiv:1709.00103. question answering dataset on structured and unstructured electronic health records. In Proceedings of the Language Resources and Evaluation Conference, pages 27892797. Xiaoxuan Wang, Pavan Kapanipathi, Ryan Musa, Mo Yu, Kartik Talamadupula, Ibrahim Abdelaziz, Maria Chang, Achille Fokoue, Bassem Makni, Nicholas Mattei, et al. 2020. Mimicsql: Text-to-sql generation for question answering on electronic medical records. In Proceedings of The Web Conference 2020, pages 25062516. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. 2024. MMLU-pro: more robust and challenging multi-task language understanding benchmark. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Fangyu Wei, Jixuan Chen, Yuxiao Ye, Ruisheng Cao, Dongchan Shin, Hongjin Su, Zhaoqing Suo, Hongcheng Gao, Wenjing Hu, Pengcheng Yin, et al. 2024. Spider 2.0: Evaluating language models on real-world enterprise text-to-sql workflows. arXiv preprint arXiv:2411.07763. Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Ben Feuer, Siddhartha Jain, Ravid Shwartz-Ziv, Neel Jain, Khalid Saifullah, Siddartha Naidu, et al. 2024. Livebench: challenging, contamination-limited llm benchmark. arXiv preprint arXiv:2406.19314. Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Benjamin Feuer, Siddhartha Jain, Ravid Shwartz-Ziv, Neel Jain, Khalid Saifullah, Sreemanti Dey, ShubhAgrawal, Sandeep Singh Sandha, Siddartha Venkat Naidu, Chinmay Hegde, Yann LeCun, Tom Goldstein, Willie Neiswanger, and Micah Goldblum. 2025. Livebench: challenging, contamination-free LLM benchmark. In The Thirteenth International Conference on Learning Representations. xAI. 2025a. Grok 4 fast (non-reasoning) model overview. xAI Documentation. xAI. 2025b. Grok 4 fast (reasoning) model overview. xAI Documentation/News. Zheyuan Yang, Lyuhao Chen, Arman Cohan, and Yilun Zhao. 2025. Table-r1: Inference-time scaling for table reasoning tasks. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 2060520624, Suzhou, China. Association for Computational Linguistics. Tao Yu, Rui Zhang, Kai Yang, Michihiko Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, et al. 2018. Spider: large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 39113921."
        },
        {
            "title": "A CLINSQL Benchmark Construction",
            "content": "ID Year Major Assigned Scenario Author? 1 2 3 4 5 6 3rd-year PhD Health Informatics 5th-year PhD Biochemistry 3rd-year PhD Medicine 4th-year PhD Biomedical Engineering Diagnostic Procedures Patient Demographics & Admissions Laboratory Results Analysis Medication Management Table 6: Overview of the 6 expert annotators who contributed to the CLINSQL Benchmark Construction. Two annotators (IDs 2 and 6) are paper authors; to preserve confidentiality, their rows omit year, major, and scenario details."
        },
        {
            "title": "B Example Clinical Questions",
            "content": "B.1 Patient Demographics Example Query For an 81-year-old female: among female Medicare patients aged 7686 transferred from another hospital with principal AMI (ICD-9 410*/ICD-10 I21*), report 30-day readmission rate; median index LOS for readmitted vs not; percent index stays > 4 days. SQL As Figure 6 Figure 6: SQL evaluation rubric tree and Result for CLINSQL sample: Patient Demographics. B.2 Vital Signs Monitoring Example Query have 60-year-old man in the ICU. In male ICU patients aged 5565 with HFNC within 24 hours versus condition-matched ICU controls, what are the instability score median and p25/p75/p95, tachycardia and hypotension burden, ICU LOS and mortality? SQL As Figure 7 B.3 Laboratory Results Analysis Example Query have 51-year-old female with suspected ACS. Among female ACS admissions age 4656, what are counts, percentages, and mean hospital length of stay for first hs-TnT: Normal, Borderline, Myocardial Injury? SQL As Figure 8 B.4 Medication Management Example Query have 64-year-old female inpatient. Among females aged 5969, whats the IQR of single inpatient amiodarone prescription durations (days)? SQL As Figure 9 B.5 Diagnostic Procedures Example Query Evaluating an 88-year-old man: among male patients aged 8393 with sepsis on their first ICU stay, stratify first-72-hour diagnostic intensity (distinct procedures) into quartiles and report mean procedure counts, mean ICU LOS in days, and mortality (%) per quartile. SQL As Figure 10 B.6 Disease Diagnosis and Outcomes Example Query have 75-year-old female inpatient with pulmonary embolism. For female inpatients aged 7080 with PE, stratify into risk-score quintiles and report per quintile: 90-day mortality, general 7080 female 90-day mortality (comparison), AKI and ARDS rates, and median survivor LOS. SQL As Figure 11 Figure 7: SQL sample: Vital Signs Monitoring. Figure 8: SQL evaluation rubric tree and Result for CLINSQL sample: Laboratory Results Analysis. Figure 9: SQL evaluation rubric tree and Result for CLINSQL sample: Medication Management. Figure 10: SQL evaluation rubric tree and Result for CLINSQL sample: Diagnostic Procedures. Figure 11: SQL sample: Disease Diagnosis and Outcomes."
        },
        {
            "title": "C Error Analysis",
            "content": "Figure 12: sample of Error Analysis: Output Schema and Formatting Figure 13: sample of Error Analysis: Cohort Specification and Coding Figure 14: sample of Error Analysis: Aggregation and Clinical Statistics"
        },
        {
            "title": "D SQL Generation Prompts",
            "content": "We include the exact prompt templates used for model prompting and refinement in both Chain-of-Thought (CoT) and Direct Output regimes. SQL Generation Prompt (CoT) You are clinical data analyst expert specializing in the MIMIC-IV database. Your goal is to produce correct BigQuery SQL query for the question below. Constraints: - Target platform: Google BigQuery. - Use the correct datasets: physionet-data.mimiciv_3_1_hosp, physionet-data.mimiciv_3_1_icu. MIMIC-IV Schema Reference (HOSP + ICU): {schema text} Clinical question: {Question} Your output should be organized in the following two parts: Reasoning: - Think step by step about relevant tables, joins, filters, groupings, and edge cases. - Briefly justify important choices. SQL (wrap the final query in fenced code block using sql and ): Think step by step and then generate the complete SQL query. Figure 15: Chain-of-Thought SQL generation prompt used in our experiments. Refinement Prompt (CoT) You are clinical data analyst expert for the MIMIC-IV dataset. The following SQL failed to run on Google BigQuery. Refine it to resolve the error and better answer the question. Constraints: - Use valid BigQuery SQL. - Use the correct datasets: physionet-data.mimiciv_3_1_hosp, physionet-data.mimiciv_3_1_icu. - Modify only what is necessary; prefer minimal, correct fixes. MIMIC-IV Schema Reference (HOSP + ICU): {schema text} Clinical question: {Question} Previous SQL attempt (for reference): {Previous SQL (provided as fenced code block)} BigQuery error message: {Error message} Your output should be organized in the following two parts: Reasoning: - Step by step, explain the cause of the error and the fix. - Justify key changes briefly. SQL (wrap the final corrected query in fenced code block using sql and ): Think step by step and then generate the complete corrected SQL query. Figure 16: Chain-of-Thought SQL refinement prompt used in our experiments. SQL Generation Prompt (Direct Output) You are clinical data analyst expert specializing in the MIMIC-IV database. Your goal is to produce correct BigQuery SQL query for the question below. Constraints: - Target platform: Google BigQuery. - Use the correct datasets: physionet-data.mimiciv_3_1_hosp, physionet-data.mimiciv_3_1_icu. MIMIC-IV Schema Reference (HOSP + ICU): {schema text} Clinical question: {Question} Output format: - Return only single fenced SQL code block containing the final query (use sql and ). - Do not include explanations, or any text outside the fenced SQL block. Figure 17: Direct Output SQL generation prompt used in our experiments. Refinement Prompt (Direct Output) You are clinical data analyst expert for the MIMIC-IV dataset. The following SQL failed to run on Google BigQuery. Refine it to resolve the error and better answer the question. Constraints: - Use valid BigQuery SQL. - Use the correct datasets: physionet-data.mimiciv_3_1_hosp, physionet-data.mimiciv_3_1_icu. - Modify only what is necessary; prefer minimal, correct fixes. MIMIC-IV Schema Reference (HOSP + ICU): {schema text} Clinical question: {Question} Previous SQL attempt (for reference): {Previous SQL (provided as fenced code block)} BigQuery error message: {Error message} Output format: - Return only single fenced SQL code block containing the corrected query (use sql and ). - Do not include explanations, or any text outside the fenced SQL block. Apply the minimal fix internally and output only the final corrected SQL. Figure 18: Direct Output SQL refinement prompt used in our experiments."
        },
        {
            "title": "E Configuration of Evaluated Models",
            "content": "Organization Model Release Version # Inference Pipeline OpenAI Google xAI GPT-5-mini GPT-5-nano GPT-5 GPT-4.1 o4-mini Gemini-2.5-Pro Gemini-2.5-Flash Grok-4-Fast-Reason. Grok-4-Fast-Non-Reason. Mistral AI Mistral-Medium DeepSeek DeepSeek-R1 DeepSeek-V3.1 Qwen Team Meta AI Defog.ai Google Baichuan Qwen3-Coder-480B-A35B-Ins. Qwen3-235B-A22B-Ins. Qwen3-235B-A22B-Think. Qwen3-Next-80B-A3B-Ins. Qwen3-Next-80B-A3B-Think. Llama-4-Maverick-17B-128E-Ins. Llama-4-Scout-17B-16E-Ins. SQLCoder-7B-2 MedGemma-27B Baichuan-M2-32B 2025-08 2025-08 2025-08 2025-04 2025-04 2025-06 2025-06 2025-09 2025-09 2025-05 2025-01 2025-08 2025-07 2025-07 2025-07 2025-09 20252025-04 2025-04 2024-02 2025-06 2025-08 Proprietary Models gpt-5-mini-2025-08-07 gpt-5-nano-2025-08-07 gpt-5-chat-2025-08-07 gpt-4.1-2025-04-14 o4-mini-2025-04gemini-2.5-pro gemini-2.5-flash grok-4-fast-reasoning grok-4-fast-non-reasoning mistral-medium-2505 Open-Source Models deepseek-ai/DeepSeek-R1-0528 deepseek-ai/DeepSeek-V3.1 Qwen/Qwen3-Coder-480B-A35B-Instruct Qwen/Qwen3-235B-A22B-Instruct-2507 Qwen/Qwen3-235B-A22B-Thinking-2507-FP8 Qwen/Qwen3-Next-80B-A3B-Instruct Qwen/Qwen3-Next-80B-A3B-Thinking meta-llama/Llama-4-Maverick-17B-128E-Instruct meta-llama/Llama-4-Scout-17B-16E-Instruct defog/sqlcoder-7b-2 google/medgemma-27b-text-it baichuan-inc/Baichuan-M2-32B API API API API vLLM vLLM vLLM vLLM HF vLLM Table 7: Configuration of models evaluated in CLINSQL. We report official release month and canonical API/HF identifiers when available. Schema-Hinted Inference Setup This section describes the schema-hinted inference setup. Scope. We run GPT-5-mini on the CLINSQL validation set, covering all six clinical domains and the easy, medium, and hard difficulty tiers. The baseline remains the standard CoT pipeline with up to two execution-driven refinements. Prompt augmentation. For each query, we construct hint block from gold artifacts. We parse the reference SQL and extract ICD codes. We also read the header row of the reference result table to obtain the expected output column names. These hints are appended to the standard CoT prompt, instructing the model to include ICD filters and align SELECT aliases to the expected columns. Full schema-hinted CoT prompt templates are shown in Figure 19 and Figure 20. SQL Generation Prompt (Schema-Hinted CoT) You are clinical data analyst expert specializing in the MIMIC-IV database. Your goal is to produce correct BigQuery SQL query for the question below. Constraints: - Target platform: Google BigQuery. - Use the correct datasets: physionet-data.mimiciv_3_1_hosp, physionet-data.mimiciv_3_1_icu. MIMIC-IV Schema Reference (HOSP + ICU): {schema text} Schema-hinted context: Relevant ICD code filters observed in validated SQL examples: - {ICD code patterns} Incorporate the necessary ICD filters or joins when identifying the clinical cohort. Expected column names for the final CSV output: - {Column names} Align your SELECT aliases with these column names and preserve ordering when applicable. Clinical question: {Question} Your output should be organized in the following two parts: Reasoning: - Think step by step about relevant tables, joins, filters, groupings, and edge cases. - Briefly justify important choices. SQL (wrap the final query in fenced code block using sql and ): Think step by step and then generate the complete SQL query. Figure 19: Schema-hinted Chain-of-Thought SQL generation prompt used in our experiments. Refinement Prompt (Schema-Hinted CoT) You are clinical data analyst expert for the MIMIC-IV dataset. The following SQL failed to execute. Refine it to resolve the issues and better answer the question. Constraints: - Use valid BigQuery SQL. - Use the correct datasets: physionet-data.mimiciv_3_1_hosp, physionet-data.mimiciv_3_1_icu. - Modify only what is necessary; preserve previously correct logic. MIMIC-IV Schema Reference (HOSP + ICU): {schema text} Schema-hinted context: Relevant ICD code filters observed in validated SQL examples: - {ICD code patterns} Incorporate the necessary ICD filters or joins when identifying the clinical cohort. Expected column names for the final CSV output: - {Column names} Align your SELECT aliases with these column names and preserve ordering when applicable. Clinical question: {Question} Previous SQL attempt (for reference): {Previous SQL (provided as fenced code block)} Execution feedback: {Execution feedback} Your output should be organized in the following two parts: Reasoning: - Step by step, explain the cause of the error and the fix. - Justify key changes briefly. SQL (wrap the final corrected query in fenced code block using sql and ): Think step by step and then generate the complete corrected SQL query. Figure 20: Schema-hinted Chain-of-Thought SQL refinement prompt used in our experiments."
        },
        {
            "title": "G Validation Score Comparisons",
            "content": "Figure 21: Full validation execution score comparison between Chain-of-Thought reasoning and Direct Output for all models. Figure 22: Full validation SQL score comparison between Chain-of-Thought reasoning and Direct Output for all models. Scenario-Level Results Tables 8 and 9 extend the main results by reporting scenario-specific SQL and execution score on the CLINSQL validation and test splits. Scenario abbreviations Demog.=Patient Demographics and Admissions; Vitals=Vital Signs Monitoring; Labs=Laboratory Results Analysis; Meds=Medication Management; Dx Proc.=Diagnostic Procedures; Dx & Outc.=Disease Diagnosis and Outcomes. Model SQL Exec SQL Exec SQL Exec SQL Exec SQL Exec Demog. Test Set Vitals Labs Avg. Validation Avg. Test GPT-5-mini GPT-5-nano Gemini-2.5-Pro GPT-5 GPT-4.1 Gemini-2.5-Flash OpenAI o4-mini Grok-4-Fast-Reason. Grok-4-Fast-Non-Reason. Mistral-Medium DeepSeek-R1 DeepSeek-V3.1 Qwen3-Coder-480B-A35B-Ins. Qwen3-235B-A22B-Ins. Qwen3-Next-80B-A3B-Ins. Qwen3-235B-A22B-Think. Qwen3-Next-80B-A3B-Think. Llama-4-Maverick-17B-128E-Ins. Llama-4-Scout-17B-16E-Ins. Baichuan-M2-32B MedGemma-27B SQLCoder-7B50.76 44.44 53.41 54.01 56.15 58.82 52.15 48.81 49.15 46.07 49.15 54.21 54.33 45.67 52.77 45.25 44.56 43.56 40.69 37.17 32.74 9.85 80.70 49.51 79.22 76.63 77.94 70.53 67.35 65.90 37.25 51.97 74.86 65.66 71.97 69.72 59.68 59.19 31.36 55.67 37.54 22.68 9.27 0.00 Proprietary Models 35.06 31.39 38.06 35.54 34.70 40.35 35.71 34.83 34.34 29. 72.65 43.16 73.97 50.53 58.07 63.55 46.57 51.01 31.58 41.22 40.70 35.08 46.98 39.48 43.02 46.53 34.99 43.74 30.61 32.96 Open-source Models 36.24 34.36 32.53 31.90 30.63 33.11 38.95 25.14 24.16 23.60 18.07 2.96 62.57 54.83 38.48 41.63 41.24 45.09 15.68 42.47 30.52 5.29 0.22 0.00 53.47 40.77 32.92 38.59 28.23 42.74 35.33 22.40 21.03 26.57 16.51 5. 65.09 49.02 63.66 57.89 48.39 54.40 48.45 45.50 21.97 32.22 66.91 53.66 49.78 55.94 42.33 28.94 19.75 37.66 24.73 4.22 2.31 0.00 42.30 33.18 45.31 42.62 44.69 47.06 39.41 42.67 34.77 32.71 42.63 38.90 35.54 36.24 34.48 38.08 37.77 29.36 26.88 26.60 21.03 3.99 75.16 51.58 76.14 66.52 69.92 66.75 59.07 56.58 30.10 43.81 69.79 61.46 60.51 58.15 43.41 51.11 29.06 51.63 31.44 11.40 4.46 0. 42.72 36.13 47.28 45.93 46.23 47.48 41.23 42.78 39.85 35.33 44.91 43.19 39.05 36.36 34.48 38.20 38.13 29.11 27.89 29.97 20.92 5.29 74.67 52.03 73.73 68.42 67.79 65.01 59.22 58.46 30.41 45.10 69.15 60.71 58.18 58.63 49.26 48.54 27.01 47.49 30.40 15.27 4.00 0.00 Table 8: Scenario-level SQL and execution score (%) on CLINSQL validation and test sets. This table lists Demog., Vitals, and Labs scenarios. Test Set Avg. Validation Avg. Test Model SQL Exec SQL Exec SQL Exec SQL Exec SQL Exec Meds Dx Proc. Dx & Outc. GPT-5-mini GPT-5-nano Gemini-2.5-Pro GPT-5 GPT-4.1 Gemini-2.5-Flash OpenAI o4-mini Grok-4-Fast-Reason. Grok-4-Fast-Non-Reason. Mistral-Medium DeepSeek-R1 DeepSeek-V3.1 Qwen3-Coder-480B-A35B-Ins. Qwen3-235B-A22B-Ins. Qwen3-Next-80B-A3B-Ins. Qwen3-235B-A22B-Think. Qwen3-Next-80B-A3B-Think. Llama-4-Maverick-17B-128E-Ins. Llama-4-Scout-17B-16E-Ins. Baichuan-M2-32B MedGemma-27B SQLCoder-7B51.23 40.31 52.39 51.99 53.12 48.62 47.45 40.97 46.33 37.97 43.39 44.19 39.24 37.89 31.93 42.01 40.21 28.86 27.65 36.31 16.87 5.24 75.06 58.35 77.38 71.07 71.03 67.96 70.01 67.01 33.14 58.60 72.87 62.13 66.38 60.35 52.76 62.32 45.10 47.08 28.91 25.84 2.48 0.00 Proprietary Models 33.72 27.25 41.90 45.15 37.81 41.96 35.00 38.87 35.14 26. 75.64 58.29 72.36 75.38 75.44 67.10 66.04 63.81 32.71 45.59 44.90 38.28 50.97 50.05 52.74 48.83 42.53 49.44 44.13 38.95 Open-source Models 37.52 41.75 34.91 31.68 31.28 29.41 31.08 24.13 24.68 26.37 22.52 4.04 70.16 71.46 63.86 69.55 57.37 47.17 31.32 51.65 28.72 18.65 9.05 0.00 49.09 44.32 41.01 32.46 32.84 36.43 38.67 31.15 29.75 30.24 19.70 4. 79.55 54.28 76.43 80.22 77.52 67.32 58.19 58.75 26.61 41.97 67.97 57.61 59.94 55.79 43.37 50.04 19.95 51.33 32.37 16.28 1.35 0.00 42.30 33.18 45.31 42.62 44.69 47.06 39.41 42.67 34.77 32.71 42.63 38.90 35.54 36.24 34.48 38.08 37.77 29.36 26.88 26.60 21.03 3.99 75.16 51.58 76.14 66.52 69.92 66.75 59.07 56.58 30.10 43.81 69.79 61.46 60.51 58.15 43.41 51.11 29.06 51.63 31.44 11.40 4.46 0. 42.72 36.13 47.28 45.93 46.23 47.48 41.23 42.78 39.85 35.33 44.91 43.19 39.05 36.36 34.48 38.20 38.13 29.11 27.89 29.97 20.92 5.29 74.67 52.03 73.73 68.42 67.79 65.01 59.22 58.46 30.41 45.10 69.15 60.71 58.18 58.63 49.26 48.54 27.01 47.49 30.40 15.27 4.00 0.00 Table 9: Scenario-level SQL and execution score (%) on CLINSQL validation and test sets. This table lists Meds, Dx Proc., and Dx & Outc. scenarios."
        },
        {
            "title": "I Annotation Interface",
            "content": "We provide the graphical interface that annotators use while labeling CLINSQL samples, alongside the JSON file that is exported after an annotation is submitted. The pairing highlights how rubric items are surfaced during labeling and then captured in the structured log for future evaluation. Figure 23: Graphical annotation interface used by annotators when labeling single CLINSQL sample. Figure 24: JSON export generated from the completed annotation, preserving rubric selections and metadata."
        },
        {
            "title": "J Execution Success Rates",
            "content": "We benchmark execution reliability across all CLINSQL scenarios using the test split. Table 10 reports the queries that executed without errors for every model and scenario, while Table 11 and Table 12 break down execution success by the initial query (A1) and up to two refinement attempts (A2 and A3). Model Scenarios (%) Demog. Vitals Labs Meds Dx Proc. GPT-5-mini GPT-5-nano Gemini-2.5-Pro GPT-5 GPT-4.1 Gemini-2.5-Flash OpenAI o4-mini Grok-4-Fast-Reason. Grok-4-Fast-Non-Reason. Mistral-Medium DeepSeek-R1 DeepSeek-V3.1 Qwen3-Coder-480B-A35B-Ins. Qwen3-235B-A22B-Ins. Qwen3-Next-80B-A3B-Ins. Qwen3-235B-A22B-Think. Qwen3-Next-80B-A3B-Think. Llama-4-Maverick-17B-128E-Ins. Llama-4-Scout-17B-16E-Ins. Baichuan-M2-32B MedGemma-27B SQLCoder-7B-2 100.0 91.5 97.2 100.0 98.6 93.0 100.0 97.2 59.2 83.1 98.6 95.8 98.6 98.6 80.3 100.0 56.3 88.7 49.3 62.0 32.4 0.0 98.7 89.5 97.4 96.1 94.7 85.5 94.7 93.4 55.3 89.5 94.7 93.4 92.1 85.5 71.1 92.1 57.9 77.6 55.3 52.6 17.1 1.3 92.3 97.4 91.0 94.9 96.2 82.1 93.6 76.9 21.8 70.5 98.7 88.5 93.6 93.6 65.4 94.9 50.0 75.6 59.0 48.7 29.5 1. 93.2 85.1 97.3 100.0 93.2 93.2 97.3 90.5 33.8 90.5 95.9 87.8 95.9 86.5 74.3 95.9 77.0 77.0 47.3 55.4 20.3 0.0 97.1 94.3 95.7 97.1 98.6 87.1 97.1 84.3 34.3 82.9 94.3 98.6 100.0 91.4 70.0 85.7 70.0 80.0 51.4 51.4 28.6 1.4 Dx & Outc. 90.5 75.7 90.5 100.0 93.2 81.1 86.5 73.0 18.9 74.3 89.2 82.4 93.2 64.9 47.3 79.7 47.3 79.7 44.6 47.3 12.2 2.7 Table 10: Execution success rate (%) per model and scenario on the test split. Abbreviations defined in section H. Model GPT-5-mini GPT-5-nano Gemini-2.5-Pro GPT-5 GPT-4.1 Gemini-2.5-Flash OpenAI o4-mini Grok-4-Fast-Reason. Grok-4-Fast-Non-Reason. Mistral-Medium DeepSeek-R1 DeepSeek-V3.1 Qwen3-Coder-480B-A35B-Ins. Qwen3-235B-A22B-Ins. Qwen3-Next-80B-A3B-Ins. Qwen3-235B-A22B-Think. Qwen3-Next-80B-A3B-Think. Llama-4-Maverick-17B-128E-Ins. Llama-4-Scout-17B-16E-Ins. Baichuan-M2-32B MedGemma-27B SQLCoder-7B-2 A1 84.5 60.6 88.7 85.9 78.9 62.0 73.2 43.7 19.7 62.0 78.9 67.6 77.5 69.0 32.4 42.3 42.3 57.7 21.1 25.4 15.5 0.0 Demog. A2 14.1 23.9 5.6 8.5 18.3 19.7 25.4 36.6 23.9 12.7 11.3 22.5 16.9 23.9 26.8 52.1 11.3 28.2 19.7 21.1 5.6 0.0 1.4 7.0 2.8 5.6 1.4 11.3 1.4 16.9 15.5 8.5 8.5 5.6 4.2 5.6 21.1 5.6 2.8 2.8 8.5 15.5 11.3 0.0 A1 63.2 56.6 81.6 73.7 72.4 47.4 57.9 38.2 30.3 51.3 50.0 71.1 69.7 47.4 28.9 47.4 35.5 47.4 22.4 14.5 10.5 0.0 Scenarios (%) Vitals A2 27.6 22.4 13.2 15.8 19.7 26.3 26.3 38.2 11.8 25.0 36.8 14.5 19.7 31.6 30.3 32.9 14.5 18.4 22.4 27.6 6.6 0. A3 7.9 10.5 2.6 6.6 2.6 11.8 10.5 17.1 13.2 13.2 7.9 7.9 2.6 6.6 11.8 11.8 7.9 11.8 10.5 10.5 0.0 1.3 A1 41.0 53.8 71.8 73.1 74.4 33.3 59.0 9.0 3.8 43.6 69.2 70.5 71.8 52.6 15.4 42.3 30.8 48.7 16.7 19.2 19.2 0.0 Labs A2 41.0 32.1 16.7 12.8 15.4 30.8 26.9 33.3 11.5 17.9 24.4 12.8 16.7 29.5 29.5 47.4 16.7 23.1 24.4 19.2 9.0 0. A3 10.3 11.5 2.6 9.0 6.4 17.9 7.7 34.6 6.4 9.0 5.1 5.1 5.1 11.5 20.5 5.1 2.6 3.8 17.9 10.3 1.3 1.3 Table 11: Per-attempt execution success rate (%). This table lists Demog., Vitals, Labs. Attempts A1, A2 and A3 correspond to the initial query and up to two refinements. Model GPT-5-mini GPT-5-nano Gemini-2.5-Pro GPT-5 GPT-4.1 Gemini-2.5-Flash OpenAI o4-mini Grok-4-Fast-Reason. Grok-4-Fast-Non-Reason. Mistral-Medium DeepSeek-R1 DeepSeek-V3.1 Qwen3-Coder-480B-A35B-Ins. Qwen3-235B-A22B-Ins. Qwen3-Next-80B-A3B-Ins. Qwen3-235B-A22B-Think. Qwen3-Next-80B-A3B-Think. Llama-4-Maverick-17B-128E-Ins. Llama-4-Scout-17B-16E-Ins. Baichuan-M2-32B MedGemma-27B SQLCoder-7B-2 41.9 35.1 79.7 71.6 73.0 51.4 59.5 24.3 14.9 48.6 59.5 56.8 67.6 35.1 27.0 27.0 31.1 45.9 8.1 21.6 12.2 0.0 Meds A2 37.8 40.5 14.9 25.7 20.3 31.1 33.8 54.1 9.5 31.1 31.1 27.0 17.6 33.8 21.6 59.5 28.4 20.3 18.9 25.7 4.1 0.0 A3 13.5 9.5 2.7 2.7 0.0 10.8 4.1 12.2 9.5 10.8 5.4 4.1 10.8 17.6 25.7 9.5 17.6 10.8 20.3 8.1 4.1 0.0 51.4 47.1 75.7 84.3 75.7 45.7 57.1 32.9 20.0 48.6 70.0 75.7 61.4 60.0 21.4 20.0 41.4 61.4 11.4 21.4 21.4 0.0 Scenarios (%) Dx Proc. A2 37.1 40.0 18.6 10.0 21.4 30.0 37.1 37.1 4.3 28.6 18.6 17.1 32.9 28.6 30.0 52.9 21.4 17.1 18.6 24.3 5.7 0.0 A3 8.6 7.1 1.4 2.9 1.4 11.4 2.9 14.3 10.0 5.7 5.7 5.7 5.7 2.9 18.6 12.9 7.1 1.4 21.4 5.7 1.4 1. Dx & Outc. A2 32.4 32.4 9.5 13.5 13.5 29.7 25.7 28.4 5.4 17.6 27.0 8.1 23.0 24.3 13.5 43.2 9.5 20.3 23.0 16.2 2.7 0.0 A1 44.6 29.7 74.3 78.4 78.4 35.1 56.8 16.2 4.1 47.3 51.4 55.4 60.8 32.4 21.6 28.4 35.1 51.4 8.1 18.9 2.7 0.0 A3 13.5 13.5 6.8 8.1 1.4 16.2 4.1 28.4 9.5 9.5 10.8 18.9 9.5 8.1 12.2 8.1 2.7 8.1 13.5 12.2 6.8 2. Table 12: Per-attempt execution success rate (%). This table lists Meds, Dx Proc., Dx & Outc. HumanGPT Agreement Study Because our rubric-based evaluation relies on GPT-5 as the judge, we run humanGPT agreement study to validate the reliability of its rubric decisions. We randomly sample 100 CLINSQL validation/test examples and re-score them with two medically trained annotators from the same pool that constructs the dataset and rubrics. For each example, both annotators independently evaluate rubric leaf nodes for the SQL and results trees using the same annotation guidelines, without access to GPT-5 scores or each others labels. We compare GPT-5s leaf-level decisions and aggregated pass/fail outcomes against each annotator, and also compare the two annotators with each other. Table 13 reports agreement rates at the leaf level and at the final pass/fail level for both SQL and results. Disagreements are largely concentrated in borderline, partial-credit cases, and do not materially change relative model rankings, supporting the use of GPT-5 as reliable and scalable rubric judge for our tree-structured evaluation. Pair Leaf-level agreement (SQL, %) Leaf-level agreement (results, %) Pass/fail agreement (SQL, %) Pass/fail agreement (results, %) GPT-5 vs. Annotator 1 GPT-5 vs. Annotator 2 Annotator 1 vs. Annotator 2 83.4 82.1 86.8 87.1 85.9 89. 90.2 88.7 92.4 92.3 91.0 94.1 Table 13: HumanGPT agreement on 100 randomly sampled CLINSQL examples. Leaf-level agreement compares rubric leaf decisions, while pass/fail agreement compares aggregated outcomes for SQL and results. Inter-Annotator Agreement and Reconciliation Protocols The trustworthiness of an evaluation benchmark is essential for expert-domain assessment (Ke et al., 2025a,b; Ouyang et al., 2024). To this end, CLINSQL uses validator-based quality control to stabilize scenario design, gold SQL, and rubric leaves. Each scenario is created by primary annotator and independently reviewed by validator. Validators re-run the SQL in BigQuery, inspect result tables, and check rubric trees for coverage and correctness. For each item they record one of three outcomes: accept as-is, accept with minor edits, or major revision or reject. Across the benchmark, validators accept 87% of items as-is, request 9% minor edits (e.g., tightening time window, adjusting an inclusion criterion, or clarifying rubric leaf), and request 4% major revision or rejection. To quantify inter-annotator agreement (IAA), we additionally sample 50 validation/test scenarios and ask second annotator, distinct from both the original annotator and the validator, to re-annotate them independently at two levels. For gold SQL, the second annotator writes fresh query based only on the natural-language description and schema. We canonicalize both SQL queries and execute them on the redacted BigQuery database. In 46/50 cases (92%), the two queries produce identical result tables (up to row/column ordering). In 3/50 cases (6%), the results differ only by small numerical variations and are treated as near-miss agreements. In the remaining 1/50 case (2%), the second annotator interprets the clinical question differently, yielding clinically distinct cohort. Exact SQL-level agreement is 92%; counting near-miss cases as acceptable alternatives yields 98%. For rubric leaves, an independent annotator reconstructs the SQL and results rubrics using shared templates and guidelines. We treat each rubric as set of atomic checks (leaf presence and criticality) and compare the two annotators trees. Raw agreement on leaf presence and criticality is 91% with Cohens κ = 0.82, and the critical-first aggregation yields pass/fail agreement of 47/50 scenarios (94%). Table 14 summarizes these reconciliation and IAA statistics. Statistic Validator-based quality control (all CLINSQL items) Accept as-is Accept with minor edits Major revision / reject Double-annotation study (n=50 validation/test scenarios) SQL exact match (identical results) SQL near-miss (minor numeric differences) SQL distinct (clinically different) Rubric leaf agreement (presence + criticality) Rubric pass/fail agreement Value 87% 9% 4% 46/50 (92%) 3/50 (6%) 1/50 (2%) 91% (Cohens κ = 0.82) 47/50 (94%) Table 14: Inter-annotator agreement (IAA) and reconciliation statistics for CLINSQL. Validator outcomes are reported on the full benchmark; double-annotation statistics are computed on 50-scenario validation/test sample."
        },
        {
            "title": "M Reconciling Execution Passes with SQL Analysis Failures",
            "content": "SQL analysis and execution scoring capture complementary notions of correctness: SQL analysis checks whether the query encodes the intended cohort logic, whereas execution scoring evaluates whether the observed results are clinically plausible and consistent with the gold answer. At the model level, these signals are strongly aligned. Across all models in the main tables, the Pearson correlation between SQL Score and Execution Score is = 0.8597, and the Spearman rank correlation is ρ = 0.8554, indicating near-identical model rankings. To diagnose divergence cases, we run targeted outlier study. We identify modeldataset points with large absolute gap between SQL Score and Execution Score (at least 20 percentage points) and randomly sample 40 such outliers for manual inspection of both SQL and result tables. Most outliers (about 95%) exhibit high execution scores but low SQL scores, reflecting partial or imprecise cohort logic (e.g., missing secondary exclusion criteria, incomplete temporal logic, or incorrect aggregation granularity). In these cases, execution remains high because the resulting aggregates stay close to reference values despite logical deviations. The remaining outliers (about 5%) show high SQL scores but lower execution scores, typically when correct logic yields clinically implausible values in narrow subgroups. These findings indicate that discrepancies arise from meaningful error types rather than rubric misalignment. Table 15 summarizes the correlation statistics and outlier breakdown. Statistic Pearson correlation (SQL Score vs. Execution Score) Spearman rank correlation (SQL Score vs. Execution Score) Outlier threshold (SQL Execution 20 points) High Execution / low SQL among outliers High SQL / low Execution among outliers Value = 0.8597 ρ = 0.8554 40 cases sampled 95% 5% Table 15: Reconciliation of SQL analysis and execution scoring. Correlations are computed across all models in the main tables; outlier statistics are based on 40 sampled large-gap cases."
        },
        {
            "title": "N Annotation Guideline",
            "content": "N.1 Part I: Annotation Guidelines N.1.1 Overview Each example in our benchmark consists of: The type of realistic clinical scenario, representing one of the six scenario types defined in the paper natural language clinical question that requires database query to be solved gold standard SQL query that accurately translates the clinical question into executable database operations results table in CSV format, generated from the execution of the gold-standard SQL query An evaluation guideline for evaluating the accuracy of the SQL queries and executed results generated by other models You will be assigned the following three sequential roles: 1. Query Annotator: Develop the clinical scenario, provide the patient context, and formulate the natural language question. 2. SQL Annotator: Analyze database requirements, construct the gold-standard SQL implementation, execute the SQL queries, analyze the results table, and annotate the specific database tables that were used and key features in the results table (columns, values). 3. Evaluation Guideline Annotator: For each example, create tailored guideline for evaluating the SQL query and validating its results. While the annotation interface will guide you, it is essential to follow the instructions carefully. N.2 Step 1: Query Annotation As the Query Annotator, your task is to create realistic clinical scenarios and formulate natural language questions that require database queries to solve. You will develop questions that reflect authentic clinical decision-making processes and information needs. N.2.1 Clinical Scenario Development 1. Select and Understand Your Assigned Patient Information and Scenario Type You will be assigned one of six clinical scenario types and specific patient from the MIMIC-IV database, selected based on that scenario type for filtering: Patient Demographics & Admissions, Vital Signs Monitoring, Laboratory Results Analysis, Medication Management, Diagnostic Procedures, or Disease Diagnosis & Outcomes. Before you begin, familiarize yourself with the MIMIC tables and understand the information for the specific patient assigned, and review the scenario definition. N.2.2 Natural Language Question Formulation 1. Craft the Clinical Question Write natural language question that physician would realistically ask, given the information of the provided patient. Ensure the question requires database querying and cannot be answered through simple observation. Use appropriate medical terminology while maintaining clarity. N.3 Step 2: SQL Annotation Your task is to analyze the clinical question, identify database requirements, construct the gold-standard SQL query, and document the implementation details. N.3.1 Database Analysis and Schema Mapping 1. Clinical Question Analysis Identify the specific clinical data elements required to answer the question. 2. MIMIC-IV Database Mapping Identify all MIMIC-IV tables required to answer the question. Map clinical concepts in the question to specific database tables and columns. Determine relevant clinical thresholds, normal ranges, and medical domain knowledge. Map clinical conditions, procedures, and interventions mentioned in the question to their corresponding ICD codes. N.3.2 Gold-Standard SQL Annotation 1. SQL Query Development Write complete, executable SQL query that accurately answers the clinical question. Ensure the query handles edge cases and data quality issues common in clinical databases. 2. Query Execution and Result Generation Execute the SQL query against the MIMIC-IV database. Generate the complete results table in CSV format. Verify that results are clinically meaningful and interpretable. N.4 Evaluation Guideline Annotation As the Evaluation Guideline Annotator, your responsibility is to create comprehensive criteria for evaluating SQL queries and executed results generated by other models attempting to answer the clinical question. N.4.1 Understanding Evaluation Rubric Structure Before building evaluation rubrics, you must understand the foundational concepts that govern how evaluation scores are calculated in our benchmark system. Critical vs. Non-Critical Nodes Our evaluation system employs two types of assessment nodes: Critical Nodes [Critical]: Essential criteria whose failure immediately causes the parent node to fail, regardless of other sibling node performance. Critical nodes represent fundamental requirements that must be satisfied for meaningful evaluation. Non-Critical Nodes: Allow partial scoring at the parent level. When mixed with critical nodes, non-critical nodes contribute to averaging only after all critical nodes pass. Score = 1: The requirement is fully satisfied. The SQL query or result demonstrates correct and clinically appropriate implementation of this component. Score = 0: The requirement is not satisfied. The component is missing, incorrectly implemented, or produces clinically invalid output. For example: If Gender Selection [1] [Critical] and Age Range Selection [1] [Critical] Patient Cohort Construction [1] If Gender Selection [1] [Critical] and Age Range Selection [0] [Critical] Patient Cohort Construction [0] If Gender Selection [1] [Critical], Age Range Selection [1] [Critical], and Time Filter [0] (non-critical) Patient Cohort Construction = (0)/1 = 0 (average of non-critical nodes) Sequential Dependencies [sequential] Some evaluation nodes are marked as sequential, indicating logical dependencies among child nodes where failure at an earlier step renders subsequent evaluations meaningless. For example, if SQL query fails to correctly filter the patient cohort, evaluating the aggregation logic becomes pointless. For example: If Table Join Logic [1] [sequential] and Key Matching [1] [sequential] Data Integration = (1+1)/2 = 1 (all sequential steps succeed) If Table Join Logic [0] [sequential] and Key Matching [not evaluated] [sequential] Data Integration = (0)/1 = 0 (sequential failure stops evaluation) If Table Join Logic [1] [sequential], Key Matching [1] [sequential], and Final Validation [0] [sequential] Data Integration = (1+1+0)/3 = 0.67 (sequential failure after partial evaluation) Weight Assignment [Weight X] Each major evaluation category is assigned weight reflecting its relative importance in the overall assessment. Weights enable proportional scoring where more critical aspects (e.g., patient cohort construction) receive higher influence than secondary considerations. N.4.2 Quantified Weight Scale Our evaluation framework employs 3-point weight scale based on clinical importance: Weight 1: Basic Supportive Criteria Represents supplementary evaluation components that provide additional context. Examples: Output formatting, minor data type handling, non-essential temporal constraints. -- Output formatting and rounding SELECT ROUND(AVG(procedure_count), 2) as avg_imaging_procedures -- Column aliasing for readability COUNT(DISTINCT pr.icd_code) as procedure_count Typically assigned to elements that enhance quality but are not fundamental to clinical correctness. Weight 2: Standard Clinical Requirements Represents standard clinical database operations and moderate complexity reasoning. Examples: Medical concept implementation, aggregation functions, procedure identification. -- Medical concept implementation - ICD code pattern matching (pr.icd_version = 10 AND ( pr.icd_code LIKE B% OR pr.icd_code LIKE 3E0% OR pr.icd_code LIKE BW% OR pr.icd_code LIKE B3% -- Imaging procedures -- CT procedures -- X-ray procedures -- Ultrasound procedures )) -- Aggregation functions for clinical analytics COUNT(DISTINCT pr.icd_code) as procedure_count AVG(procedure_count) -- ICD version handling (pr.icd_version = 9 AND ( pr.icd_code LIKE 87% OR pr.icd_code LIKE 88% -- Diagnostic radiology -- Other diagnostic procedures )) Assigned to components that demonstrate competent clinical data analysis capabilities. Weight 3: Critical Clinical Elements Represents essential requirements whose failure undermines clinical validity and elements requiring substantial clinical domain knowledge and SQL proficiency. Examples: Core patient demographic filtering, critical medical code selection, fundamental table relationships, patient cohort construction, database integration with complex joins, clinical analytics. -- Critical patient cohort construction WHERE p.gender = AND p.anchor_age BETWEEN 60 AND 70 -- Fundamental table relationships FROM physionet-data.mimiciv_3_1_hosp.patients JOIN physionet-data.mimiciv_3_1_hosp.procedures_icd pr ON p.subject_id = pr.subject_id -- Essential grouping for per-patient analysis GROUP BY p.subject_id -- Critical medical filtering logic WHERE p.gender = AND p.anchor_age BETWEEN 60 AND 70 AND ( -- Comprehensive ICD version and code handling (pr.icd_version = 10 AND (...)) OR (pr.icd_version = 9 AND (...)) )"
        },
        {
            "title": "Reserved for components that are absolutely essential for producing clinically meaningful results and",
            "content": "require deep understanding of both clinical domain and advanced SQL capabilities. Scoring aggregation follows the critical-first protocol described in Algorithm 1. N.4.3 Build SQL Query Evaluation Rubric Create hierarchical evaluation tree tailored to your specific clinical question and SQL implementation. The structure should reflect the logical flow of SQL query construction while identifying critical checkpoints. See Figure 3 for an example sql rubric tree. N.4.4 Build Results Validation Rubric Create validation criteria based on the actual generated CSV file from your gold-standard SQL execution, combined with your clinical knowledge. See Figure 4 for an example results rubric tree. N.5 Part II: Validation Guidelines As Validator, your role is to ensure every clinical example meets our benchmark standards. To do this, you will perform comprehensive review of all its components: the natural language question, the SQL query, the executed results, and the evaluation guideline. N.5.1 Clinical Question Assessment Real-world Relevance: Question represents authentic clinical decision-making scenarios Medical Language: Accurate clinical terminology and healthcare concepts Scenario Match: Aligns with designated clinical category Linguistic Quality: Clear, grammatically sound, and unambiguous phrasing Query Requirement: Necessitates database analysis, not simple observation N.5.2 SQL Implementation Review Database Standards: Uses correct MIMIC-IV paths (physionet-data.mimiciv_3_1_hosp) Schema Validation: Accurate table references, columns, and join relationships Medical Logic: Valid age computation, ICD handling, and temporal analysis Technical Function: Error-free execution with proper NULL management Query Coverage: Comprehensively addresses clinical question requirements N.5.3 Output Verification Structure: Well-formed CSV with meaningful column labels Medical Plausibility: Values fall within clinically acceptable boundaries Data Integrity: Complete dataset without missing essential information Logic Alignment: Output corresponds to SQL query operations N.5.4 Evaluation Framework Review Component Coverage: SQL evaluation addresses all query elements Priority Identification: [Critical] labels properly applied to essential parts Order Dependencies: [Sequential] tags used where sequence matters Output Standards: Adequate value ranges and format specifications Assessment Clarity: Unambiguous binary scoring system N.5.5 Complexity Level Classification Difficulty Assessment: Evaluate and categorize the annotated querySQL pair according to the appropriate complexity level based on SQL complexity and clinical reasoning requirements. Classification Accuracy: Ensure each example is correctly assigned to Easy, Medium, or Hard difficulty levels to maintain uniform standards throughout the benchmark. N.5.6 Action Required If the example fails any of the above checks, revise it if corrections are minor (e.g., grammar fixes, small SQL adjustments, or evaluation refinements). If issues are significant (e.g., clinically inappropriate question, fundamentally incorrect SQL, or incomplete evaluation framework), you may reject the example or heavily revise. Provide brief justification when making revisions or rejections. N.5.7 Mark as Validated Once all checks have passed, mark the example as Validated. This confirms it is ready for inclusion in the final dataset."
        },
        {
            "title": "O Judge Prompt",
            "content": "We use an LLM-as-a-judge to score rubric leaf nodes with binary decisions and short explanations. The SQLand results-level prompt templates are shown in Figure 25 and Figure 26. Judge Prompt: SQL Evaluation You are evaluating SQL queries for clinical data analysis based on specific requirements. Evaluation Criteria: {node.requirements} Clinical Question: {query} SQL to Evaluate (fenced): {test_sql} Gold Standard SQL: {gold_sql} Instructions: 1. Evaluate if the SQL meets the specific requirement: \"{node.requirements}\". 2. Focus on whether the implementation satisfies the requirement, not on syntactic perfection. 3. Use the gold standard SQL as reference for best practices and expected approach. 4. Score: 1 if requirement is fully met, 0 if not met. 5. Provide brief explanation of your assessment. Response Format: Score: [0 or 1] Explanation: [Brief explanation of why the score was given] Figure 25: LLM judge prompt template for SQL-level rubric evaluation. Judge Prompt: Results Evaluation You are evaluating clinical query results based on specific requirements. Evaluation Criteria: {node.requirements} Clinical Question: {query} Results to Evaluate: {test_results} Gold Standard Results: {gold_results} Instructions: 1. Evaluate if the results meet the specific requirement: \"{node.requirements}\". 2. For \"CSV File Exists\" requirements: if results data is shown above and not empty, it means CSV file exists. 3. Use the gold standard results as reference for expected format and values. 4. Consider clinical plausibility, data format, and completeness. 5. Score: 1 if requirement is fully met, 0 if not met. 6. Provide brief explanation of your assessment. Response Format: Score: [0 or 1] Explanation: [Brief explanation of why the score was given] Figure 26: LLM judge prompt template for results-level rubric evaluation. MIMIC-IV Schema MIMIC-IV HOSP module admissions Columns: subject_id, hadm_id, admittime, dischtime, deathtime, admission_type, admit_provider_id, admission_location, discharge_location, insurance, language, marital_status, race, edregtime, edouttime, hospital_expire_flag patients Columns: subject_id, gender, anchor_age, anchor_year, anchor_year_group, dod transfers Columns: subject_id, hadm_id, transfer_id, eventtype, careunit, intime, outtime labevents Columns: labevent_id, subject_id, hadm_id, specimen_id, itemid, charttime, storetime, value, valuenum, valueuom, ref_range_lower, ref_range_upper, flag, priority, comments d_labitems Columns: itemid, label, fluid, category, loinc_code microbiologyevents Columns: microevent_id, subject_id, hadm_id, micro_specimen_id, order_provider_id, chartdate, charttime, spec_itemid, spec_type_desc, test_seq, storedate, storetime, test_itemid, test_name, org_itemid, org_name, isolate_num, quantity, ab_itemid, ab_name, dilution_text, dilution_comparison, dilution_value, interpretation, comments diagnoses_icd Columns: subject_id, hadm_id, seq_num, icd_code, icd_version d_icd_diagnoses Columns: icd_code, icd_version, long_title procedures_icd Columns: subject_id, hadm_id, seq_num, chartdate, icd_code, icd_version d_icd_procedures Columns: icd_code, icd_version, long_title emar Columns: subject_id, hadm_id, emar_id, emar_seq, poe_id, pharmacy_id, enter_provider_id, charttime, medication, event_txt, scheduletime, storetime emar_detail Columns: subject_id, emar_id, emar_seq, parent_field_ordinal, administration_type, pharmacy_id, barcode_type, reason_for_no_barcode, complete_dose_not_given, dose_due, dose_due_unit, dose_given, dose_given_unit, will_remainder_of_dose_be_given, product_amount_given, product_unit, product_code, product_description, prior_infusion_rate, infusion_rate, infusion_rate_adjustment, infusion_rate_adjustment_amount, infusion_rate_unit, route, infusion_complete, completion_interval, new_iv_bag_hung, continued_infusion_in_other_location, restart_interval, side, site, non_formulary_visual_verification prescriptions Columns: subject_id, hadm_id, pharmacy_id, poe_id, poe_seq, order_provider_id, starttime, stoptime, drug_type, drug, formulary_drug_cd, gsn, ndc, prod_strength, form_rx, dose_val_rx, dose_unit_rx, form_val_disp, form_unit_disp, doses_per_24_hrs, route pharmacy Columns: subject_id, hadm_id, pharmacy_id, poe_id, starttime, stoptime, medication, proc_type, status, entertime, verifiedtime, route, frequency, disp_sched, infusion_type, sliding_scale, lockout_interval, basal_rate, one_hr_max, doses_per_24_hrs, duration, duration_interval, expiration_value, expiration_unit, expirationdate, dispensation, fill_quantity poe Columns: poe_id, poe_seq, subject_id, hadm_id, ordertime, order_type, order_subtype, transaction_type, discontinue_of_poe_id, discontinued_by_poe_id, order_provider_id, order_status poe_detail Columns: poe_id, poe_seq, subject_id, field_name, field_value hcpcsevents Columns: subject_id, hadm_id, chartdate, hcpcs_cd, seq_num, short_description d_hcpcs Columns: code, category, long_description, short_description drgcodes Columns: subject_id, hadm_id, drg_type, drg_code, description, drg_severity, drg_mortality services Columns: subject_id, hadm_id, transfertime, prev_service, curr_service provider Columns: provider_id omr Columns: subject_id, chartdate, seq_num, result_name, result_value MIMIC-IV ICU module icustays Columns: subject_id, hadm_id, stay_id, first_careunit, last_careunit, intime, outtime, los chartevents Columns: subject_id, hadm_id, stay_id, caregiver_id, charttime, storetime, itemid, value, valuenum, valueuom, warning datetimesevents Columns: subject_id, hadm_id, stay_id, caregiver_id, charttime, storetime, itemid, value, valueuom, warning inputevents Columns: subject_id, hadm_id, stay_id, caregiver_id, starttime, endtime, storetime, itemid, amount, amountuom, rate, rateuom, orderid, linkorderid, ordercategoryname, secondaryordercategoryname, ordercomponenttypedescription, ordercategorydescription, patientweight, totalamount, totalamountuom, isopenbag, statusdescription, originalamount, originalrate ingredientevents Columns: subject_id, hadm_id, stay_id, caregiver_id, starttime, endtime, storetime, itemid, amount, amountuom, rate, rateuom, orderid, linkorderid, statusdescription, originalamount, originalrate outputevents Columns: subject_id, hadm_id, stay_id, caregiver_id, charttime, storetime, itemid, value, valueuom procedureevents Columns: subject_id, hadm_id, stay_id, caregiver_id, starttime, endtime, storetime, itemid, value, valueuom, location, locationcategory, orderid, linkorderid, ordercategoryname, ordercategorydescription, patientweight, isopenbag, continueinnextdept, statusdescription, originalamount, originalrate d_items Columns: itemid, label, abbreviation, linksto, category, unitname, param_type, lownormalvalue, highnormalvalue caregiver Columns: caregiver_id Concise notes (commonly confusing columns) hadm_id vs. stay_id: hadm_id is the hospital admission identifier; stay_id tracks an ICU stay within an admission. charttime vs. storetime: charttime captures when the event occurred; storetime records when it was entered or verified. itemid: numeric key for labs, measurements, or medications (lookup in d_labitems or d_items). value / valuenum / valueuom: textual value, numeric value, and unit respectively; use valuenum for calculations. seq_num: ordering field for diagnoses/procedures, where lower values often imply higher priority. poe_id / poe_seq: provider order identifier plus sequence; detailed attributes live in poe_detail. orderid / linkorderid: link infusion segments and associated orders over time in ICU inputs. interpretation: microbiology susceptibility call (e.g., S/I/R)."
        }
    ],
    "affiliations": [
        "Yale University"
    ]
}