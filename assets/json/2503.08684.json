{
    "paper_title": "Perplexity Trap: PLM-Based Retrievers Overrate Low Perplexity Documents",
    "authors": [
        "Haoyu Wang",
        "Sunhao Dai",
        "Haiyuan Zhao",
        "Liang Pang",
        "Xiao Zhang",
        "Gang Wang",
        "Zhenhua Dong",
        "Jun Xu",
        "Ji-Rong Wen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Previous studies have found that PLM-based retrieval models exhibit a preference for LLM-generated content, assigning higher relevance scores to these documents even when their semantic quality is comparable to human-written ones. This phenomenon, known as source bias, threatens the sustainable development of the information access ecosystem. However, the underlying causes of source bias remain unexplored. In this paper, we explain the process of information retrieval with a causal graph and discover that PLM-based retrievers learn perplexity features for relevance estimation, causing source bias by ranking the documents with low perplexity higher. Theoretical analysis further reveals that the phenomenon stems from the positive correlation between the gradients of the loss functions in language modeling task and retrieval task. Based on the analysis, a causal-inspired inference-time debiasing method is proposed, called Causal Diagnosis and Correction (CDC). CDC first diagnoses the bias effect of the perplexity and then separates the bias effect from the overall estimated relevance score. Experimental results across three domains demonstrate the superior debiasing effectiveness of CDC, emphasizing the validity of our proposed explanatory framework. Source codes are available at https://github.com/WhyDwelledOnAi/Perplexity-Trap."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 4 8 6 8 0 . 3 0 5 2 : r Published as conference paper at ICLR PERPLEXITY-TRAP: PLM-BASED RETRIEVERS OVERRATE LOW PERPLEXITY DOCUMENTS Haoyu Wang1, Sunhao Dai1, Haiyuan Zhao1, Liang Pang2, Xiao Zhang1 Gang Wang3, Zhenhua Dong 3, Jun Xu1, Ji-Rong Wen1 1Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China 2CAS Key Laboratory of AI Safety, Institute of Computing Technology, Beijing, China 3Huawei Noahs Ark Lab, Shenzhen, China {wanghaoyu0924,sunhaodai,junxu}@ruc.edu.cn,"
        },
        {
            "title": "ABSTRACT",
            "content": "Previous studies have found that PLM-based retrieval models exhibit preference for LLM-generated content, assigning higher relevance scores to these documents even when their semantic quality is comparable to human-written ones. This phenomenon, known as source bias, threatens the sustainable development of the information access ecosystem. However, the underlying causes of source bias remain unexplored. In this paper, we explain the process of information retrieval with causal graph and discover that PLM-based retrievers learn perplexity features for relevance estimation, causing source bias by ranking the documents with low perplexity higher. Theoretical analysis further reveals that the phenomenon stems from the positive correlation between the gradients of the loss functions in language modeling task and retrieval task. Based on the analysis, causal-inspired inferencetime debiasing method is proposed, called Causal Diagnosis and Correction (CDC). CDC first diagnoses the bias effect of the perplexity and then separates the bias effect from the overall estimated relevance score. Experimental results across three domains demonstrate the superior debiasing effectiveness of CDC, emphasizing the validity of our proposed explanatory framework 1."
        },
        {
            "title": "INTRODUCTION",
            "content": "The rapid advancement of large language models (LLMs) has driven significant increase in AIgenerated content (AIGC), leading to information retrieval (IR) systems that now index both humanwritten and LLM-generated contents (Cao et al., 2023; Dai et al., 2024b; 2025). However, recent studies (Dai et al., 2024a;c; Xu et al., 2024) have uncovered that Pretrained Language Model (PLM) based retrievers (Guo et al., 2022; Zhao et al., 2024) exhibit preferences for LLM-generated documents, ranking them higher even when their semantic quality is comparable to human-written content. This phenomenon, referred to as source bias, is prevalent among various popular PLM-based retrievers across different domains (Dai et al., 2024a). If the problem is not resolved promptly, human authors creative willingness will be severely reduced, and the existing content ecosystem may collapse. So its urgent to comprehensively understand the mechanism behind source bias, especially when the amount of online AIGC is rapidly increasing (Burtch et al., 2024; Liu et al., 2024). Existing studies identify perplexity (PPL) as key indicator for distinguishing between LLMgenerated and human-written contents (Mitchell et al., 2023; Bao et al., 2023). Dai et al. (2024c) find that although the semantics of the text remain unchanged, LLM-rewritten documents possess much lower perplexity than their human-written counterparts. However, its still unclear whether document perplexity has causal impact on the relevance score estimation of PLM-based retrievers (which may lead to source bias), and if so, why such causal impact exists. In this paper, we delve deeper into the cause of source bias by examining the role of perplexity in PLM-based retrievers. By manipulating sampling temperature when generating with LLMs, we Equal contributions. Corresponding author. 1Codes are available at https://github.com/WhyDwelledOnAi/Perplexity-Trap. 1 Published as conference paper at ICLR observe negative correlation between estimated relevance scores and perplexity. Inspired by this, we construct causal graph where document perplexity plays as treatment and document semantic plays as confounder (Figure 2). We adopt two-stage least squares (2SLS) regression procedure (Angrist and Pischke, 2009; Hartford et al., 2017) to eliminate the influence of confounders when estimating this biased effect, the experimental results indicate the effect is significantly negative. Based on these findings, the cause of source bias can be elucidated as the unexpected causal effect of perplexity on estimated relevance scores. For semantically identical documents, the documents with low perplexity causally get higher estimated relevance scores from PLM-based retrievers. Since LLM-generated documents typically have lower perplexity than human-written ones, they receive higher estimated relevance scores and are ranked higher, leading to the presence of source bias. To further understand why estimated relevance scores of PLM-based retrievers are influenced by perplexity, we provide theoretical analysis for the overlap between masked language modeling (MLM) task and mean-pooling retrieval task. Analysis in the linear decoder scenario shows that, the retrieval objectives gradients are positively correlated to the language modeling gradients. This correlation causes the retrievers to consider not only the document semantics required for retrieval but also the bias introduced by perplexity. Meanwhile, this correlation further explains the trade-off between retrieval performance and source bias observed in previous study (Dai et al., 2024a): the stronger the ranking performance of the PLM-based retrievers, the greater the impact of perplexity. Based on the analysis, we propose an inference-time debiasing method called CDC (Causal Diagnosis and Correction). With the proposed causal graph, we separate the causal effect of perplexity from the overall estimated relevance scores during inference, achieving calibrated unbiased relevance scores. Specifically, CDC first estimates the biased causal effect of perplexity on small set of training samples, which is then applied to de-bias the test samples at the inference stage. This debiasing process is inference-time and can be seamlessly integrated into existing trained PLM-based retrievers. We demonstrate the debiasing effectiveness of CDC with experiments across six popular PLM-based retrievers. Experimental results show that the estimated causal effect of perplexity can be generalized to other data domains and LLMs, highlighting its practical potential in eliminating source bias. We summarize the major contributions of this paper as follows: We construct causal graph and estimate the causal effect through experiments, demonstrating that PLM-based retrievers causally assign higher relevance scores to documents with lower perplexity, which is the cause of source bias. We provide theoretical analysis explaining that the effect of perplexity in PLM-based retrievers is due to the positive correlation between objective gradients of retrieval and language modeling. We propose CDC for PLM-based retrievers to counteract the biased effect of perplexity, with experiments demonstrating its effectiveness and generalizability in eliminating source bias."
        },
        {
            "title": "2 RELATED WORK",
            "content": "With the rapid development of LLMs (Zhao et al., 2023), the internet has quickly integrated huge amount of AIGC (Cao et al., 2023; Dai et al., 2024b; 2025). Potential bias may occur when these generated contents are judged by neural networks as competitor together with human works. For example, Dai et al. (2024c) are the first to highlight paradigm shift in information retrieval (IR): the content indexed by IR systems is transitioning from exclusively human-written corpora to coexistence of human-written and LLM-generated corpora. They then uncover an important finding that mainstream neural retrievers based on pretrained language models (PLMs) prefer LLM-generated content, phenomenon termed source bias (Dai et al., 2024a;c). Xu et al. (2024) further discover that this bias extends to text-image retrieval, and similarly, other works further observe the existence of source bias in other IR scenarios, such as recommender systems (RS) (Zhou et al., 2024), retrievalaugmented generation (RAG) (Chen et al., 2024) and question answering (QA) (Tan et al., 2024). In the context of LLMs-as-judges, similar bias is discovered as self-enhancement bias (Zheng et al., 2024), likelihood bias (Ohi et al., 2024), and familiarity bias (Stureborg et al., 2024), where LLM overates AIGC when serving as judge. Existing works provide intuitive explanations suggesting that this kind of bias may stem from coupling between neural judges and LLMs (Dai et al., 2024c; Xu et al., 2024), such as similarities in model 2 Published as conference paper at ICLR 2025 (a) DL19 (b) TREC-COVID (c) SCIDOCS Figure 1: Perplexity and estimated relevance scores of ANCE on positive query-document pairs in three dataset, where documents are generated by LLM rewriting with different sampling temperatures. The Pearson coefficients highlight the significant negative correlation between the two variables. architectures and training objectives. However, the specific nature of this coupling, how it operates to cause source bias, and why it exists remains unclear. Ohi et al. (2024) find the correlation between perplexity and bias, while our work is the first to systematically analyze the effect of perplexity for neural models preference. Given that both PLMs and LLMs are highly complex neural network models, investigating this question is particularly challenging and difficult."
        },
        {
            "title": "3 ELUCIDATING SOURCE BIAS WITH CAUSAL GRAPH",
            "content": "This section first conducts intervention experiments to illustrate the motivation. Subsequently, we construct causal graph to explain source bias and demonstrate the rationality of the causal graph. 3.1 MOTIVATION: INTERVENTION EXPERIMENTS ON TEMPERATURE Previous studies have revealed significant difference in the perplexity (PPL) distribution between LLM-generated content and human-written content (Mitchell et al., 2023; Bao et al., 2023), suggesting that PPL might be key indicator for analyzing the cause of source bias (Dai et al., 2024c). To verify whether perplexity causally affects estimated relevance scores, we use LLMs (in following chapters the LLMs we use are Llama2-7B-chat (Touvron et al., 2023) unless emphasized) to generate documents with almost identical semantics but varying perplexity, where semantics are expected as the only associated variable when retrieval. Specifically, we manipulate the sampling temperatures during generation to obtain LLM-generated documents with different PPLs but similar semantic content. Following the method of Dai et al. (2024c), we use the following simple prompt: Please rewrite the following text: {human-written text}. We also recruit human annotators to conduct evaluations to ensure the quality of the generated LLM content. The results, shown in Appendix E.2.1, indicate that there are fewer quality discrepancies between documents generated at different sampling temperatures compared to the original humanwritten documents. This ensures the reliability of the subsequent experiments. We then explore the relationship between perplexity and estimated relevance scores on the corpora generated with different temperatures, where perplexity is calculated by BERT masked language modelling following previous work (Dai et al., 2024c). Figure 1 presents the average perplexity and estimated relevance scores by ANCE across three datasets from different domains. As expected, lower sampling temperatures result in less randomness in LLM-generated content and thus lower PPL. Meanwhile, we find that documents generated with lower temperatures were also more likely to be assigned higher estimated relevance scores. The Pearson coefficients for the three datasets are all below -0.8, emphasizing the strong negative linear correlation between document perplexity and relevance score. Similar results for other PLM-based retrievers are provided in Appendix E.2.2. Since document semantics remain unchanged during rewriting, the synchronous variation between document perplexity and estimated relevance scores reflects causal effect. These findings offer an intuitive explanation for source bias: LLM-generated content typically has lower PPL, and since documents with lower perplexity are more likely to receive higher relevance scores, LLM-generated content is more likely to be ranked highly, leading to source bias. 3 Published as conference paper at ICLR"
        },
        {
            "title": "3.2 CAUSAL GRAPH FOR SOURCE BIAS",
            "content": "Inspired by the findings above, we propose causal graph to elucidate source bias (Fan et al., 2022), as illustrated in Figure 2. Let denotes the query set and denote the corpus. During the inference stage for certain PLM-based retriever, given query and document C, the estimated relevance score ˆRq,d is simultaneously determined by both the golden relevance score Rq,d and document perplexity Pd R+. Note that the fundamental goal of IR is to calculate the similarity between document semantics Md and query semantics Mq for document ranking, Rq,d ˆRq,d is considered an unbiased effect, while the influence of Pd ˆRq,d is considered as biased effect. Subsequently, we explain the rationale behind each edge in the causal graph as follows: First, let the document source Sd is binary variable where Sd = 1 denotes the document is generated by LLM and Sd = 0 denotes the document is written by human. As suggested in (Dai et al., 2024c), LLMgenerated documents through rewriting possess lower perplexity than their original documents, even though there is no significant difference in their semantic content. Thus, an edge Sd Pd exists. This phenomenon can be attributed to two main reasons: (1) Sampling strategies aimed at probability maximization, such as greedy algorithms, discard long-tailed documents during LLM inference. More detailed analysis and verification can be found in (Dai et al., 2024c). (2) Approximation error during LLM training causes the tails of the document distribution to be lost (Shumailov et al., 2023). Next, the document semantics Md reflect the topics of the document d, including domain, events, sentiment information, and so on. Since documents with different semantic meanings convey different amounts of information, their difficulties in masked token prediction vary. This means that different document semantics lead to different document perplexities. For example, colloquial conversations are more predictable than research papers due to their less specialized vocabulary. Thus, the content directly affects the perplexity, establishing the edge Md Pd. Figure 2: The proposed causal graph for explaining source bias. Finally, as retrieval models are trained to estimate ground-truth relevance, their outputs are valid approximations of the golden relevance scores, making Md Rq,d Mq natural unbiased effect. However, retrieval models may also learn non-causal features unrelated to semantic matching, especially high-dimensional features in deep learning. According to findings in Section 3.1, document perplexity Pd has emerged as potential non-causal feature learned by PLM-based retrievers, where higher relevance estimations coincide with lower document perplexity. Moreover, Since document perplexity is determined at the time of document generation, which temporally predates the existence of estimated relevance scores, document perplexity should be cause rather than consequence of changes in relevance. Hence, biased effect of Pd ˆRq,d exists. 3.3 EXPLAINING SOURCE BIAS VIA THE PROPOSED CAUSAL GRAPH Based on the causal graph constructed above, source bias can be explained as follows: Although the content generated by LLMs retains similar semantics to the human-written content, LLM-generated content typically exhibits lower perplexity. Coincidentally, retrievers learn and incorporate perplexity features into their relevance estimation processes, consequently assigning higher relevance scores to LLM-generated documents. This leads to the lower ranking of human-written documents. It is worth noting that source bias is an inherent issue in PLM-based retrievers. Before the advent of LLMs, these retrievers had already learned non-causal perplexity features from purely human-written corpora. However, because the document ranking was predominantly conducted on human-written corpora, the relationship between PLM-based retrievers and perplexity was not evident. As powerful LLMs have become more accessible, the emergence of LLM-generated content has accentuated the perplexity effect. The content generated by LLMs exhibits perceptibly different perplexity distribution compared to human-written content. This disparity in perplexity distribution causes documents from different sources to receive significantly different relevance rankings. 4 Published as conference paper at ICLR 2025 Table 1: Quantified causal effects (and corresponding p-value) for document perplexity on estimated relevance scores via two-stage regression. Bold indicates that the estimate can pass significance test with p-value< 0.05. Significant negative causal effects are prevalent across various PLM-based retrievers in different domain datasets. Dataset DL BERT -9.32 (1e-4) TREC-COVID -1.69 (2e-2) -2.44 (6e-2) SCIDOCS RoBERTa -28.15 (2e-12) 2.42 (8e-2) -6.42 (2e-3) ANCE -0.52 (9e-3) 0.09 (0.21) -0.23 (0.15) TAS-B -0.96 (1e-2) -0.48 (6e-3) -0.39 (0.10) Contriever -0.02 (0.33) -0.05 (7e-7) -0.02 (0.24) coCondenser -0.69 (3e-2) -0.32 (8e-3) -0.26 (0.41)"
        },
        {
            "title": "4 EMPIRICAL AND THEORETICAL ANALYSIS ON THE EFFECT OF PERPLEXITY",
            "content": "In this section, we conduct empirical experiments and theoretical analysis to substantiate that PLMbased retrievers assign higher relevance scores to documents with lower perplexity."
        },
        {
            "title": "4.1 EXPLORING THE BIASED EFFECT CAUSED BY PERPLEXITY",
            "content": "4.1.1 ESTIMATION METHODS From the temperature intervention experiments in Section 3.1, we observe clear negative correlation between document perplexity and estimated relevance scores. Despite human evaluation allows us to largely confirm that document semantics Md generated from different temperatures are almost the same, estimating the biased effect of Pd ˆRq,d directly is problematic due to inevitable minor variations in document semantics, which, though subtle, are significant in causal effect estimation. From the causal view, to robustly estimate the causal effect of Pd ˆRq,d, the document semantics Md, query semantics Mq and golden relevance scores Rq,d are considered as confounders. Therefore, directly estimating this biased causal effect is not feasible without addressing this confounding factor. We use 2SLS based on instrumental variable (IV) methods (Angrist and Pischke, 2009; Hartford et al., 2017) to more accurately evaluate the causal effect of document perplexity on estimated relevance scores, more details about the method can be found in Appendix D. According to the causal graph, document source Sd serves as an IV for estimating the effect of Pd ˆRq,d. The IV is independent of confounders: query semantics Mq, document semantics Md, and golden relevance scores Rq,d. In the first stage of the regression, we use linear regression to predict document perplexity Pd based on document source Sd: Pd = β1Sd + Pd, (1) where Pd is independent with document source Sd and therefore depends solely on document semantics Md. As result, we obtain coefficient ˆβ1 and the predicted document perplexity ˆPd. In the second stage, we substitute Pd with ˆPd = ˆβ1Sd to estimate the predicted relevance score ˆRq,d from the certain PLM-based retrievers: ˆRq,d = β2 ˆPd + Rq,d, . (2) where residual term Rq,d represents the part of the estimated relevance scores that cant be explained by document perplexity. Since ˆPd is independent of document semantics Md, the estimated coefficient ˆβ2 can accurately reflect the causal effect of perplexity on estimated relevance scores. 4.1.2 EXPERIMENTAL RESULTS AND ANALYSIS In this section, we apply the causal effect estimation method described previously to assess the impact of document perplexity Pd on the estimated relevance score ˆRq,d. Models. To comprehensively evaluate this causal effect, we select several representative PLM-based retrieval models from the Cocktail benchmark (Dai et al., 2024a), including: (1) BERT (Devlin et al., 2019); (2) RoBERTa (Liu et al., 2019); (3) ANCE (Xiong et al., 2020); (4) TAS-B (Hofstätter et al., 2021); (5) Contriever (Izacard et al., 2022); (6) coCondenser (Gao and Callan, 2022). We employ the officially released checkpoints. For more details, please refer to Appendix E.1. 5 Published as conference paper at ICLR 2025 Datasets. We select three widely-used IR datasets from different domains to ensure the broad applicability of our findings: (1) DL19 dataset (Craswell et al., 2020) for exploring retrieval across miscellaneous domains. (2) TREC-COVID dataset (Voorhees et al., 2021) focused on biomedical information retrieval. (3) SCIDOCS (Cohan et al., 2020) dedicated to the retrieval of scientific scholarly articles. Given that source bias arises from the ranking orders of positive samples from different sources, we only compare the estimated relevance scores of human-written and LLMgenerated relevant documents against their corresponding queries. Results and Analysis. The results across different datasets and different PLM-based retrievers are shown in Table 1. As we can see, in most cases, perplexity exhibits consistently negative causal effect on relevance estimation, with documents of lower perplexity more likely to receive higher relevance scores. Although this causal effect is relatively weak, it is statistically significant, with p-values < 0.05 in most instances. We also explore whether this causal effect changes with different sampling temperature. Results in Appendix Table 5 indicate that ˆβ2 is robust for temperature changes, that is, this causal effect is independent with generation temperature. This finding is crucial as retrieval tasks emphasize the relative ranking of relevance scores rather than their absolute values. Even slight preferential increase in estimated relevance scores for LLM-generated content over human-written content will lead to consistent trend of higher rankings for LLM-generated documents by PLM-based retrievers, further confirming the observations in Figure 1. Finding 1: For PLM-based retrievers, document perplexity has causal effect on estimated relevance scores. Lower perplexity can lead to higher relevance scores. 4.2 ANALYZING MECHANISM BEHIND THE BIASED EFFECT 4.2.1 WHY PERPLEXITY AFFECTS PLM-BASED RETRIEVERS? In Section 4.1, our empirical experiments have confirmed that PLM-based retrievers take perplexity features into account for document retrieval. However, the reason why perplexity-related features play role, particularly when these models are primarily designed for document ranking, remains unclear. Considering that PLM-based retrievers are generally fine-tuned from PLMs on retrieval tasks, we delve into the relationship between the mask language modeling task in the pre-training stage and the mean-pooling document retrieval task in the fine-tuning stage. Our formulation are as follows and explanations can be found in Appendix C.1. Model Architecture. To simplify our analysis, we assume common architecture for PLM-based retrievers, consisting of an encoder (t; θ) : LD (cid:55) RLN and one-layer decoder g(z; ) = σ(zW ), where denotes the set composed of one-hot vectors, is the length of query or document, is the dictionary size, is the dimension of embedding vector, and σ() maps real vectors to simplexes. For the ease of qualitative analysis, we replace softmax operation with linear operation, and zW is assumed positive to ensure the well-definition of the probability distribution. Masked Language Modeling (MLM) Task. The PLM is initially pre-trained on the MLM task with CrossEntropy loss: L1(d) = 1 L[d log g(f (d))]1D, where denotes the Hadamard product, 1 1L means averages over the length of the documents, [d log g(f (d))]1D is the expression of CrossEntropy using one-hot vectors. 1T Document Retrieval Task. In the fine-tuning stage for the document retrieval task, the retrieval model estimates the relevance for given query-document pairs by computing the dot product of the document embedding vectors demb = (d, θ) and query embedding vectors qemb = (q, θ). Without loss of generality, we assume demb = 1, . . . , L, which means the embeddings of each token is 2 = 1, normalized. The loss function can be written as L2(d, q) = tr[( 1 1Lqemb)], where 1 1L[] is the mean pooling operation of the embeddings over the document length L. With the formulation above, we further explore the theoretical underpinnings of why perplexity influences retrieval performance by examining the gradients of the loss functions for both the MLM task and the document retrieval task, as shown in the following Theorem 1: 1Ldemb)T ( 1 Theorem 1. Given the following three conditions: 6 Published as conference paper at ICLR 2025 Representation Collinearity: the embedding vectors of relevant query-document pairs are collinear after mean pooling, i.e., 1LLf (q) = λ1LLf (d), λ > 0. Semi-Orthogonal Weight Matrix: the weight matrix of the decoder is semi-orthogonal, i.e., = IN . Encoder-decoder Cooperation: fine-tuning does not disrupt the corresponding function between encoder and decoder, i.e., Then there exists matrix = (cid:104) λkl L(1kl) , kl = (cid:80)D (dembW )ld which satisfies (d) = g1(d). (cid:105) RLN + ln L2 demb = L1 demb . The three conditions made with their rationale are explained in Appendix C.1 and the proof of Theorem 1 can be found in Appendix C.2. From Theorem 1, we observe that the gradients of the two losses of MLM task and the retrieval task have positive linear relationship. Note that L1(d) actually represent the document perplexity Pd and L2(d, q) actually represent the negative estimated relevance score ˆRq,d. Then we can easily derive the following Corollary, which illustrates how the key conclusion L2/demb = L1/demb in Theorem 1 leads to the biased effect of document perplexity Pd on estimated relevance score ˆRq,d: Corollary 1. Consider human-written document d1 and its LLM-rewritten document d2, they are both relevant with query q. Assume LLM-rewritten documents possess lower perplexity at token level (Mitchell et al., 2023). Let rvec/vec be matrix-to-row/column-vector operator, Ll 1(d) denote the perplexity of the l-th token in the document, (demb )l denote the embedding of the l-th token, Ll 1(d1) Ll 1(d2) = L1(d2) (demb )l 2 )l (demb 2 d2 vec(d1 d2) > 0, = 1, . . . , L, where 1st-order approximation of Chain rule is taken as the surrogate function (Grabocka et al., 2019; Nguyen et al., 2009) for Ll 1(d). According to Theorem 1 and 1st-order approximation of L2(d), ˆRq,d1 ˆRq,d2 = [L2(d1) L2(d2)] = rvec(K = (cid:88) l=1 λkl L(1 kl) L1(d2) (demb )l 2 )l (demb 2 d2 ) ) demb 2 d2 vec(d1 d2) L1(demb 2 demb 2 (cid:88) λkl L(1 kl) l=1 vec(d1 d2) = (cid:0)Ll 1(d1) Ll 1(d2)(cid:1) < 0. Corollay 1 indicates that human-written document will receive lower relevance estimation than its LLM-written document, resulting in source bias. It is important to note that our theoretical analysis does not cover all situations in reality, we will discuss these limitations in Appendix B. Finding 2: For PLM-based retrievers, the gradients of MLM and IR loss functions (metrics) possess linear overlap, leading to the biased effect of perplexity on estimated relevance scores. 4.2.2 FURTHER VERIFICATION OF THEOREM 1 Theorem 1 reveals the linear relationship between language modeling gradients and retrieval gradients w.r.t. document embedding vectors demb. For more comprehensive verification for its reliability, we derive Corollay 2 from Theorem 1 and provide supporting experiments about the corollay. The derivation is similar with that in Corollary 1. Corollary 2. For two retrievers (t; θ1), (t; θ2) which share the same PLM, such as BERT. If retriever (t; θ1) possesses more powerful language modeling ability than (t; θ2), i.e., EdD[Ll 1(d; θ1)] EdD[Ll 1(d; θ2)] < 0, = 1, . . . , L, 7 Published as conference paper at ICLR 2025 then similar to the Corollary 1, we have EdD[L2(d; θ1)] EdD[L2(d; θ2)] = EdD (cid:20) rvec (cid:20) rvec(K =E L1(demb; θ2) demb ) demb θ2 (cid:21) vec(θ1 θ2) (cid:18) L2(demb; θ2) demb (cid:34) (cid:88) = λkl L(1 kl) l=1 (cid:19) demb θ2 (cid:21) vec(θ1 θ2) (cid:0)Ll 1(d; θ1) Ll 1(d; θ2)(cid:1) (cid:35) < 0. Note that EdD[L1(d; θ)] is typical measure of language modeling ability and EdD[L2(d; θ)] reflects the ranking performance, Corollary 2 indicates that if retriever possesses more powerful language modeling ability, its ranking performance will be better. To offer empirical support for the corollary, we evaluate the language modeling ability of PLM-based retrieval models with different ranking performances. By taking the retrieval model directly as PLM encoder to do MLM task, we calculate the average text perplexity of the retrieval corpus to evaluate their language modeling ability, which offers support for the encoder-decoder corporation assumption at the same time. As illustrated in Figure 3, there is clear correlation between text perplexity and retrieval accuracy (except Contriever). These results, demonstrating that language modeling capabilities are indeed correlated with retrieval performance, strengthen the practical reliability of our assumptions and conclusions as the deductive verification of the above hypothesis we used. This finding also explains why PLM dramatically improve the performance of retrievers over past years. Combining the previous findings, we can further understand the relationship between model retrieval performance and the degree of source bias. On one hand, if the PLM-based retriever demonstrates better MLM capability, it tends to be more sensitive to document perplexity, which leads to more severe source bias (Corollary 1). On the other hand, retriever with better MLM capabilities can also achieve more accurate relevance estimations, leading to better ranking performance (Corollary 2). Consequently, PLM-based retrievers encounter trade-off between accuracy in retrieval and the severity of source bias. Specifically, higher ranking performance is associated with more significant source bias. This relationship has been noted in previous research (Dai et al., 2024a), and we are the first to offer plausible explanation for this phenomenon. Figure 3: Model perplexity and ranking performance (NDCG@3) on averaged results of DL19, TREC-COVID, and SCIDOCS. Finding 3: Better language modeling improves PLM-based retrievers ranking performance, but also heightens its sensitivity to perplexity, thus increasing source bias severity."
        },
        {
            "title": "5 CAUSAL-INSPIRED SOURCE BIAS MITIGATION",
            "content": "In this section, we further propose causal-inspired debiasing method to eliminate the source bias, which can be naturally derived from our above causal analysis. We then conduct experiments to evaluate the effectiveness of the proposed debiasing method. 5.1 PROPOSED DEBIASING METHOD: CAUSAL DIAGNOSIS AND CORRECTION In Section 3 and 4, we have constructed causal graph and estimated the biased effect of perplexity on the final predicted relevance score. Based on these insights, we propose an inference-time debiased method via Causal Diagnosis Correction (CDC). The main procedure of CDC lies on two stage: (i) Bias Diagnosis: Employing the Instrumental Variable method for estimating the bias effect ˆβ2 of perplexity Pd to estimated relevance score ˆRq,d. (ii) Bias Correction: Separating the biased effect of document perplexity from the overall estimated relevance scores ˆRq,d. 8 Published as conference paper at ICLR 2025 Algorithm 1: The Proposed CDC: Debiasing with Causal Diagnosis and Correction Input: training set D, test query set Q, test corpus C, estimation budget Output: unbiased estimated relevance scores 1 // Bias Diagnosis 2 Initialize the estimation set for estimating biased effect De 3 for training pairs (qi, dH 4 ) and De < do 5 6 Instruct LLM to generate doc dG Predict the estimated relevance scores ˆrH Calculate perplexity pH Updating the estimation set De De (ˆrH for doc dH , pG , ˆrG and doc dG , ˆrG , pH , respectively , pG ) via rewriting the original human-written doc dH for pairs (qi, dH ) and (qi, dG ) 7 8 end 9 Estimate the biased effect coefficient ˆβ2 with 2-stage regression using Eq. (2) on De 10 // Bias Correction 11 for test query qt do 12 Predict the estimated relevance scores ˆrt for each pair (qt, dt) with dt Calculate document perplexity pt for each doc dt Debias the original model prediction ˆrt using Eq. (3), add the calibrated score rt to 14 15 end 16 return Table 2: Performance (NDCG@3) and bias (Relative (Dai et al., 2024c) on NDCG@3) of different PLM-based retrievers with and without our proposed CDC debiased method on three datasets. Note that more negative bias metric value indicates greater bias towards LLM-generated documents, while more positive value indicates greater bias towards human-written documents. Model BERT Roberta ANCE TAS-B Contriever coCondenser DL19 (In-Domain) TREC-COVID (Out-of-Domain) SCIDOCS (Out-of-Domain) Performance Bias Performance Bias Performance Bias Raw 75.92 72.79 69.41 74.97 72.61 75.50 +CDC 77.65 71.33 67.73 75.63 73.83 75.36 Raw -23.68 -36.32 -21.03 -49.17 -21.93 -18.99 +CDC 5.90 4.45 34.95 -9.97 -5.33 9.60 Raw 53.72 46.31 71.01 63.95 63.17 70. +CDC 45.88 45.86 69.94 62.84 61.35 71.07 Raw -39.58 -48.14 -33.59 -73.36 -62.26 -67.95 +CDC -18.40 -10.51 -1.94 -37.42 -31.33 -45.39 Raw 10.80 8.85 12.73 15.04 15.45 13.93 +CDC 10.44 8.24 12.31 14.15 15.09 13.79 Raw -2.85 -30.90 -1.57 -1.90 -6.96 -5. +CDC 29.19 32.13 26.26 23.48 1.63 1.06 Specifically, the final calibrated score Rq,d for document ranking can be formulated as follows: Rq,d = ˆRq,d ˆβ2Pd, (3) which can be derived by rearranging Eq. (2). In this formula, Rq,d is independent to document source and perplexity. Therefore, it serves as good proxy for semantic relevance ranking. Specifically, we first take samples from the training set to construct the estimation set De for estimating the biased effect ˆβ2 (lines 2-8), where is the estimation budget. To construct the estimation set De, we instruct an LLM to generate document dG by rewriting the original humanwritten document dH . For these two types of samples, we use the retriever to predict their relevance for the given query and calculate their document perplexities pH scores ˆrH . Further, following the practice in Section 4.1, we use two-stage IV regression on the estimation set De to estimate the biased coefficient ˆβ2 (line 9). During testing, we use Eq. (3) to correct the original model prediction ˆrt, obtaining the calibrated score rt for final document ranking (line 11-15). We summarize the overall procedure of CDC in Algorithm 1. and ˆrG and pG 5.2 EXPERIMENTS AND ANALYSIS To evaluate the effectiveness of CDC, we implement it across various retrievers in simulated-realistic debiasing scenarios, where generated documents are from different domains and LLMs. In this case, we investigate the generalizability of the CDC method at both LLM level and Domain level. At domain-level, we employ bias diagnosis on the training set of DL19 to estimate the biased effect ˆβ2 for each retrieval model, and then conduct in-domain and cross-domain evaluation on the test sets 9 Published as conference paper at ICLR 2025 Table 3: Performance (NDCG@3) and bias (Relative (Dai et al., 2024c) on NDCG@3) of the retrievers on mixed SciFact corpus from different LLMs. Bias Diagnosis is conducted on DL19 corpus from Llama-2, where CDC performs generalization at both LLM and data-domain levels. Model BERT RoBERTa ANCE TAS-B Contriever coCondenser Llama-2 (In-Domain) GPT-4 (Out-of-Domain) GPT-3.5 (Out-of-Domain) Mistral (Out-of-Domain) Performance Bias Performance Bias Performance Bias Performance Bias Raw 35.67 38.09 42.13 52.95 55.19 49.53 +CDC 35.08 36.76 42.13 53.94 55.37 49.40 Raw -12.37 -29.54 -8.81 -15.04 -2.87 -12.98 +CDC 6.75 -0.88 4.59 -7.96 1.07 -9. Raw 36.47 38.53 42.67 52.12 55.78 48.57 +CDC 35.75 37.70 42.99 52.44 55.70 48.91 Raw -3.69 -11.98 -5.53 -4.94 -5.32 5.04 +CDC 6.04 4.52 3.28 -0.05 -4.44 6.04 Raw 35.97 39.17 42.76 52.83 56.11 48.59 +CDC 35.27 38.00 42.96 52.90 56.17 48. Raw -5.03 -35.39 -13.59 -5.65 -7.43 -1.00 +CDC 18.08 14.09 6.09 5.57 -2.81 5.30 Raw 35.13 38.29 42.62 52.18 56.13 49.57 +CDC 35.08 37.28 42.71 52.69 56.28 49.92 Raw 0.73 -17.95 -8.59 -8.71 -4.13 -5.90 +CDC 13.07 16.78 1.82 -2.00 -2.39 -0. of DL19, TREC-COVID, SCIDOCS. Note that only 128 samples (i.e., estimation budget = 128) are used for bias diagnosis, this sample size is sufficient for effective results. More detailed settings can be found in Appendix E.1. The averaged results over five different seeds are reported in Table 2. As we can see, using the estimated biased coefficient of in-domain retrieval data, our debiasing CDC successfully mitigates or even reverses the retrieval models preference towards human-written documents without fine-tuning retrievers. Meanwhile, this estimated biased coefficient demonstrates generalizability across out-of-domain datasets. The majority of the retrieval performance degradation was generally less than 2 percentage points, revealing that our debiasing CDC has acceptable impact on ranking performance, see detailed significance test in Appendix Table 7. In addition, the mean and standard deviation of performance and bias after CDC debiasing for the five sampling sessions is provided in Appendix Table 6, indicating the robustness of CDC to training samples. We also find that the debiasing results may vary across different retrievers. Specifically, CDC has more significant effects on vanilla models like BERT while exhabits lower impacts on stronger retrievers such as Contriever. We offer the following analysis to explain such observations. Stronger retrievers are developed using more sophisticated contrastive learning algorithms, which enhance their abilities to differentiate between highly relevant documents and the others. In this way, it may be more challenging for CDC corrections to alter the initial rankings. So more aligned or model-specific approach could potentially enhance the debiasing process. Considering that the web content may be generated by diverse LLMs, we expand our evaluations to assess the generalizability of the CDC method across corpora generated by different LLMs, including Llama (Touvron et al., 2023), GPT-4 (Achiam et al., 2023), GPT-3.5, and Mistral (Jiang et al., 2023). Due to the cost of computing resources, we conducted experiments on smaller dataset SciFact, which is also used in previous works (Dai et al., 2024c;a). In this setup, CDC used Llamas rewritten DL19 documents to estimate β2 and subsequently correct retrieval results on SciFact corpora mixed with each LLM separately. The results displayed in Table 3 confirm that CDC is capable to generalize across various LLMs and maintain high retrieval performance while effectively mitigating bias. In summary, these empirical results validate the feasibility of our proposed debiasing method by effectively reducing the biased impact of document perplexity on model outputs. And this method can be integrated efficiently into dual-encoder architerctures used in ANN search by pre-computing and indexing query-independent document perplexity with embeddings. Moreover, ˆβ2 can be adjusted according to specific requirements, where larger absolute value of ˆβ2 leads to further preference for human-written texts albeit at the potential cost of ranking performance degradation. More discussion about the open question that Should we debias toward human-written contents? is in Appendix A.1."
        },
        {
            "title": "6 CONCLUSION",
            "content": "This paper aims to explain the phenomenon of source bias where PLM-based retrievers overrate lowperplexity documents. Our core conclusion is that PLM-based retrievers use perplexity features for relevance estimation, leading to source bias. To verify this, we conducted two-stage IV regression and found negative causal effect from perplexity to relevance estimation. Theoretic analysis reveals that the gradient correlation between language modeling and retrieval tasks contributes to this causal effect. Based on the analysis, causal-inspired inference-time debiasing method called CDC is proposed. Experimental results verified its effectiveness in terms of debiasing the source bias. 10 Published as conference paper at ICLR"
        },
        {
            "title": "7 ACKNOWLEDGEMENTS",
            "content": "This work was funded by the National Key R&D Program of China (2023YFA1008704), the National Natural Science Foundation of China (62472426, 62276248, 62376275), the Youth Innovation Promotion Association CAS under Grants (2023111), fund for building world-class universities (disciplines) of Renmin University of China, the Fundamental Research Funds for the Central Universities, PCC@RUC, and the Research Funds of Renmin University of China (RUC24QSDL013). Work partially done at Engineering Research Center of Next-Generation Intelligent Search and Recommendation, Ministry of Education."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Joshua Angrist and Jörn-Steffen Pischke. Mostly harmless econometrics: An empiricists companion. Princeton university press, 2009. Guangsheng Bao, Yanbin Zhao, Zhiyang Teng, Linyi Yang, and Yue Zhang. Fast-detectgpt: Efficient zero-shot detection of machine-generated text via conditional probability curvature. arXiv preprint arXiv:2310.05130, 2023. Gordon Burtch, Dokyun Lee, and Zhichen Chen. Generative ai degrades online communities. Communications of the ACM, 67(3):4042, 2024. Yihan Cao, Siyu Li, Yixin Liu, Zhiling Yan, Yutong Dai, Philip Yu, and Lichao Sun. comprehensive survey of ai-generated content (aigc): history of generative ai from gan to chatgpt. arXiv preprint arXiv:2303.04226, 2023. Xiaoyang Chen, Ben He, Hongyu Lin, Xianpei Han, Tianshu Wang, Boxi Cao, Le Sun, and Yingfei Sun. Spiral of silences: How is large language model killing information retrieval?a case study on open domain question answering. Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics, 2024. Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, and Daniel Weld. Specter: Document-level representation learning using citation-informed transformers. arXiv preprint arXiv:2004.07180, 2020. Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen Voorhees. Overview of the trec 2019 deep learning track. arXiv preprint arXiv:2003.07820, 2020. Sunhao Dai, Weihao Liu, Yuqi Zhou, Liang Pang, Rongju Ruan, Gang Wang, Zhenhua Dong, Jun Xu, and Ji-Rong Wen. Cocktail: comprehensive information retrieval benchmark with llm-generated documents integration. Findings of the Association for Computational Linguistics: ACL 2024, 2024a. Sunhao Dai, Chen Xu, Shicheng Xu, Liang Pang, Zhenhua Dong, and Jun Xu. Bias and unfairness in information retrieval systems: New challenges in the llm era. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 64376447, 2024b. Sunhao Dai, Yuqi Zhou, Liang Pang, Weihao Liu, Xiaolin Hu, Yong Liu, Xiao Zhang, Gang Wang, and Jun Xu. Neural retrievers are biased towards llm-generated content. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 526537, 2024c. Sunhao Dai, Chen Xu, Shicheng Xu, Liang Pang, Zhenhua Dong, and Jun Xu. Unifying bias and unfairness in information retrieval: New challenges in the llm era. In Proceedings of the 18th ACM International Conference on Web Search and Data Mining, 2025. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. Published as conference paper at ICLR 2025 Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 41714186, 2019. Shaohua Fan, Xiao Wang, Yanhu Mo, Chuan Shi, and Jian Tang. Debiasing graph neural networks via learning disentangled causal substructure. Advances in Neural Information Processing Systems, 35:2493424946, 2022. Luyu Gao and Jamie Callan. Unsupervised corpus aware language model pre-training for dense passage retrieval. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 28432853, 2022. Tianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple contrastive learning of sentence embeddings. arXiv preprint arXiv:2104.08821, 2021. Charles Goodhart. Problems of monetary management: the uk experience in papers in monetary economics. Monetary Economics, 1, 1975. Josif Grabocka, Randolf Scholz, and Lars Schmidt-Thieme. Learning surrogate losses. arXiv preprint arXiv:1905.10108, 2019. Jiafeng Guo, Yinqiong Cai, Yixing Fan, Fei Sun, Ruqing Zhang, and Xueqi Cheng. Semantic models for the first-stage retrieval: comprehensive review. ACM Transactions on Information Systems (TOIS), 40(4):142, 2022. Jason Hartford, Greg Lewis, Kevin Leyton-Brown, and Matt Taddy. Deep iv: flexible approach for counterfactual prediction. In International Conference on Machine Learning, pages 14141423. PMLR, 2017. Sebastian Hofstätter, Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy Lin, and Allan Hanbury. Efficiently teaching an effective dense retriever with balanced topic aware sampling. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 113122, 2021. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning. Transactions on Machine Learning Research, 2022. Kalervo Järvelin and Jaana Kekäläinen. Cumulated gain-based evaluation of ir techniques. ACM Transactions on Information Systems (TOIS), 20(4):422446, 2002. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Bohan Li, Hao Zhou, Junxian He, Mingxuan Wang, Yiming Yang, and Lei Li. On the sentence embeddings from pre-trained language models. arXiv preprint arXiv:2011.05864, 2020. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing Systems, 36, 2024. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher Manning, and Chelsea Finn. Detectgpt: Zero-shot machine-generated text detection using probability curvature. In International Conference on Machine Learning, pages 2495024962. PMLR, 2023. Michael Mitzenmacher and Eli Upfal. Probability and computing: Randomization and probabilistic techniques in algorithms and data analysis. Cambridge university press, 2017. 12 Published as conference paper at ICLR 2025 Niklas Muennighoff, Nouamane Tazi, Loïc Magne, and Nils Reimers. Mteb: Massive text embedding benchmark. arXiv preprint arXiv:2210.07316, 2022. XuanLong Nguyen, Martin Wainwright, and Michael Jordan. On surrogate loss functions and f-divergences. 2009. Masanari Ohi, Masahiro Kaneko, Ryuto Koike, Mengsay Loem, and Naoaki Okazaki. Likelihoodbased mitigation of evaluation bias in large language models. arXiv preprint arXiv:2402.15987, 2024. Reimers. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084, 2019. Gerard Salton, Anita Wong, and Chung-Shu Yang. vector space model for automatic indexing. Communications of the ACM, 18(11):613620, 1975. Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross Anderson. The curse of recursion: Training on generated data makes models forget. arXiv preprint arXiv:2305.17493, 2023. Rickard Stureborg, Dimitris Alikaniotis, and Yoshi Suhara. Large language models are inconsistent and biased evaluators. arXiv preprint arXiv:2405.01724, 2024. Hexiang Tan, Fei Sun, Wanli Yang, Yuanzhuo Wang, Qi Cao, and Xueqi Cheng. Blinded by generated contexts: How language models merge generated and retrieved contexts for open-domain qa? Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics, 2024. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Ellen Voorhees, Tasmeer Alam, Steven Bedrick, Dina Demner-Fushman, William Hersh, Kyle Lo, Kirk Roberts, Ian Soboroff, and Lucy Lu Wang. Trec-covid: constructing pandemic information retrieval test collection. In ACM SIGIR Forum, volume 54, pages 112. ACM New York, NY, USA, 2021. Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold Overwijk. Approximate nearest neighbor negative contrastive learning for dense text retrieval. arXiv preprint arXiv:2007.00808, 2020. Shicheng Xu, Danyang Hou, Liang Pang, Jingcheng Deng, Jun Xu, Huawei Shen, and Xueqi Cheng. Ai-generated images introduce invisible relevance bias to text-image retrieval. Proceedings of the 47th international ACM SIGIR conference on research and development in information retrieval, 2024. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. survey of large language models. arXiv preprint arXiv:2303.18223, 2023. Wayne Xin Zhao, Jing Liu, Ruiyang Ren, and Ji-Rong Wen. Dense text retrieval based on pretrained language models: survey. ACM Transactions on Information Systems, 42(4):160, 2024. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36, 2024. Yuqi Zhou, Sunhao Dai, Liang Pang, Gang Wang, Zhenhua Dong, Jun Xu, and Ji-Rong Wen. Source echo chamber: Exploring the escalation of source bias in user, data, and recommender system feedback loop. arXiv preprint arXiv:2405.17998, 2024. 13 Published as conference paper at ICLR"
        },
        {
            "title": "A DISCUSSION",
            "content": "A.1 SHOULD WE DEBIAS TOWARD HUMAN-WRITTEN CONTENTS? While we refer to the retrievers preference for LLM-rewritten content as bias, its crucial to recognize that not all biases are harmful. As illustrated in previous works (Dai et al., 2024c; Zhou et al., 2024), from content creators perspective, reducing preference toward LLM-rewritten content helps guarantee sufficient incentives for authors to encourage creativity, and thus sustain healthier content ecosystem. From users perspective, LLM-rewritten documents might possess enhanced quality, such as better coherence, and improved reading experience. In this work, our debiasing approach is primarily methodological application derived from our causal graph analysis, serving to validate the perplexity-trap hypothesis further. At the same time, our framework allows for adjustable preference levels between human-written and LLM-generated documents, catering to specific practical requirements. This flexibility ensures our approach can be tailored to balance between enhancing information quality and maintaining content provider fairness. A.2 SHOULD PERPLEXITY BE CAUSAL FACTOR TO QUERY-DOCUMENT RELEVANCE? Its one of the assumptions of this work that perplexity should not be causal factor to querydocument relevance. It is true that there may be correlation between perplexity and query-document relevance, e.g., the coherence of document may also have an impact on relevance. However, there is an insurmountable gap between the perplexity of LLM-rewritten documents and human work, because people do not intentionally take PPL into account when writing, but LLMs do generation with perplexity as goal. We are currently faced with situation where this perplexity gap has breached the range of human perception of relevance, leading to serious source bias even when the rewritten documents share nearly the same semantics with human works, as verified and discussed in previous literature (Dai et al., 2024c). Its just like what Goodharts Law (Goodhart, 1975) states: When measure becomes target, it ceases to be good measure. So perhaps threshold should be set, and when perplexity is less than the threshold, it should be made independent with relevance."
        },
        {
            "title": "B LIMITATIONS",
            "content": "This study has several limitations that are important to acknowledge. Data and Experiments. Firstly, while our analysis was conducted on three representative datasets, it is recognized that there are numerous other IR datasets that could have been included. Our selection, although limited in scope, was strategic to ensure broad representation across different domains, and we believe that our findings can be generalized to other domains. Secondly, due to the cost associated with human evaluation, we were constrained to perform only 6 20 evaluations for each dataset, corresponding to six different sampling temperatures. This decision, while pragmatic, may limit the extent to which we can generalize our results to other conditions. Thirdly, we have to admit that impacts of LLM rewriting on semantics indeed lack more consideration although they have been designed possibly credible. Since simulated environment construction is not our main contribution, we have adopted the datasets provided by previous works (Dai et al., 2024b) or follow their methodology (Dai et al., 2024c) to evaluate the source bias of retrievers. In Dai et al. (2024c), the document embeddings are compared using cosine similarity, and more detailed human evaluation was conducted to assess the various impacts of LLM rewriting, which indicated no significant changes in document semantics. We will conduct more meticulous semantic checks to pursue more rigorous conclusions if possible. Theoretical Analysis In our theoretical proofs, we made certain assumptions and simplifications. Specifically, we narrow our analysis in PLM-based dual-encoder and mean-pooling scenario. These are necessary to achieve mathematical tractability and are grounded in practical considerations, which have been discussed in the previous sections. We believe these assumptions are reasonable and have validated the reliability of our conclusions through experimental verification. For the other scenarios, such as auto-regressive embedding models and CLS-based retrievers, we will explore and discuss them in the future work. 14 Published as conference paper at ICLR 2025 Despite these limitations, we maintain that our work provides valuable insights into the subject matter and serves as foundation for future research."
        },
        {
            "title": "C NOTES ON THEORETICAL ANALYSIS",
            "content": "In this section, we provide detailed reasons to our assumptions and proof to our proposed theorem in Section 4.2. C.1 EXPLANATION ON ASSUMPTIONS Our theoretical analysis are based on set of assumptions, to which were going to offer the reasons. Encoder-Only Retrievers Encoder-only architectures are generally considered more suitable for textual representation tasks, while encoder-decoder and decoder-only models are typically used for generative tasks. Thus, encoder-only models have been widely employed for retrieval tasks and have demonstrated effective results. In fact, most of the mainstream dense retrievers listed on the MTEB (Muennighoff et al., 2022) leaderboard are based on encoder-only architectures. Mean-Pooling Strategy. We use of mean pooling for query/doc embeddings in the derivation of Theorem 1, while simplification, differs from the practice of using CLS token embeddings in BERT-like models. From practical perspective, (weighted) mean pooling embedding outperform CLS token embedding when ranking, which has been widely confirmed in previous works (Dai et al., 2024b; Reimers, 2019). From theoretical perspective, (weighted) mean pooling is able to retain more local information about documents, which is important for retrieval tasks, as query is regarded related to document when the query is related to particular sentence in the document. Furthermore, there is literature indicating that CLS token embeddings may not always effectively capture sentence representations, which can be limitation in retrieval contexts (Li et al., 2020). Representation Collinearity Hypothesis Representation Collinearity Hypothesis is fundamental assumption long implemented in information retrieval systems (Salton et al., 1975). When measuring relevance scores by calculating dot or cosine similarity, we assume that the best relevant document owns an embedding that is collinear with the query embedding (given that the norm of the document embedding is held constant). In practice, dense retrievers are trained on contrastive learning to maximize the similarity between query and its relevant documents while minimize the similarity of irrelevant documents (Gao et al., 2021; Zhao et al., 2024). Semi-Orthogonal Weight Matrix Hypothesis RN satisfies the Semi-Orthogonal Weight Matrix assumption = IN , which is necessary to achieve mathematical tractability. Since practical PLMs uses 2-layer MLPs rather than the weight matrix , this cant be verified If we ignore the activation function in the MLPs of BERT, let = W1W2, then directly. 2 T 50 1 1 2 diag(W )F , which suggests that the diagonal elements are much larger than the others. One reasonable intuition is conclusion in high-dimension probabilities which states \"for any ϵ > 0, there are = Ω(eN ) vectors in RN such that any pair of them are nearly orthogonal.\"(Mitzenmacher and Upfal, 2017) Since 768 for commonly-used retrievers, the hypothesis holds with high probability. Encoder-decoder Cooperation Hypothesis This assumption has certain practical background. The experiment in Section4 can be viewed as verification of this assumption, where finetuned encoder is used with unfinetuned MLPs to do MLM task. In this setting, the hybrid model recieve relatively low perplexity as PLMs. In practice, the beginning learning rate of finetuning retrievers is usually set at 1 e5, which makes retrievers more likely to the conserve of inversion property. C.2 PROOF OF THEOREM 1 In this section, we provide the proof of Theorem 1. Note that the three conditions made are naturally satisfied: (1) Representation collinearity is fundamental assumption long implemented in information retrieval systems. (2) Matrix orthogonality is common and intuitive property of the decoders weight matrix. (3) Encoder-decoder adheres to the original design principles of auto-encoder networks. Then we give the proof as follows: Proof. Given the following three conditions: 15 Published as conference paper at ICLR 2025 Representation Collinearity: the embedding vectors of relevant query-document pairs are collinear after mean pooling, i.e., 1LLf (q) = λ1LLf (d). Orthogonal Weight Matrix: the weight matrix of the decoder is orthogonal, i.e., = I. Encoder-decoder cooperation: fine-tuning does not disrupt the corresponding function between encoder and decoder, i.e., (d) = g1(d). Our goal is to prove L2/demb = L1/demb. Note that the two losses are both involved with demb, L1 demb ="
        },
        {
            "title": "1\nL",
            "content": "1LL[g(demb) d]W ="
        },
        {
            "title": "1\nL",
            "content": "1LL[σ(dembW ) d]W ."
        },
        {
            "title": "1\nL2 1L×Lqemb.\nReplacing the softmax operation with linear normalization, let ·",
            "content": "L2 demb = denote element-wise division, [σ(x)]l = 1 (cid:80)N xln xl, = 1, . . . , L. Considering the following matrix identity, (AM BN K) (cM 1T K) = (AM (cM 1T )) BN K, it reveals that the gradient of L1 can be rearranged as [σ(dembW ) d]W = ( dembW kL1T d)W = ( demb kL1T d)W , where column vector kL RL satisfies kl = (cid:80)D complex. Meanwhile, using mean inequality (also called QM-AM inequality), we can find that (dembW )ld > 0 because σ(dembW )l is kl (cid:118) (cid:117) (cid:117) (cid:116) 1 (cid:88) (dembW ) ld = (cid:114) 1 (dembW )l(dembW )T = 1 demb 2 = 1 < 1. According to the orthogonal weight matrix assumption, ( demb kL1T W d)W = demb kL1T dW = demb kL1T σ1(d)W = demb kL1T g1(d). From the encoder-decoder cooperation condition, we obtain L1 demb = Considering the positive query-document pair q, d, assume their embedding vectors are collinear, 1L kL kL 1LL[demb( 1T )] = (d)] = 1LLdiag( 1LL[ 1L kL kL demb kL1T 1 1 1 )demb. L2 demb = 1 L2 1LLqemb = λ L2 1LLdemb. we can observe that Let = λ kL 1LkL L2 demb = 1T , then it holds that λ diag( kL 1L kL ) L1 demb . L2 demb = L1 demb . Published as conference paper at ICLR 2025 Figure 4: By leveraging IV regression on Sd, Pd is decomposed into causal and non-causal parts. precise causal effect can be obtained from the coefficient of the second-stage regression, i.e., ˆβ2."
        },
        {
            "title": "D INSTRUMENTAL VARIABLE REGRESSION",
            "content": "In statistics, instrumental variable (IV) is used to estimate causal effects. The changes of IV induces changes of explanatory variable but keeps error term constant. The basic method to estimate causal effect is 2SLS. In the first stage, 2SLS regress explanatory variable on instrumental variable and obtain the predicted values of explanatory variable. In the second stage, 2SLS regress output variable on predicted explanatory variable. Then, the coefficient corresponding to the predicted explanatory variable can be viewed as measure of causal effects. According to our proposed causal graph, the document source Sd has three properties: (1) It is correlated to the document perplexity Pd. (2) It is independent with Document semantics Md because we can instruct LLMs to rewrite human documents for any document semantics. (3) It only affect the estimated relevance score ˆRq,d through document perplexity Pd. Thus, document source Sd can be considered instrumental variable to evaluate the causal effect of document perplexity on estimated relevance scores. As depicted in Figure 4, we estimate document perplexity Pd based on document source Sd in the first stage. The results, coefficient ˆβ1 and predicted document perplexity ˆPd = ˆβ1Sd, are used in the second stage to estimated the predicted relevance score ˆRq,d via linear regression, where the estimated coefficient ˆβ2 is valid measure for the magnitude of the causal effect."
        },
        {
            "title": "E MORE EXPERIMENTS",
            "content": "E.1 EXPERIMENTAL DETAILS Our experiments are all conducted on machines equipped with NVIDIA A6000 GPUs and 52-core Intel(R) Xeon(R) Gold 6230R CPUs at 2.10GHz. For better reproducibility, we employ the following officially released checkpoints: BERT (Devlin et al., 2018; 2019) and RoBERTa (Liu et al., 2019) are used in dense retrieval as PLM encoders. We employ the trained models from the Cocktail benchmark (Dai et al., The models are available at https://huggingface.co/IR-Cocktail/ 2024a). https://huggingface.co/ bert-base-uncased-mean-v3-msmarco IR-Cocktail/roberta-base-mean-v3-msmarco, respectively. and ANCE (Xiong et al., 2020) improves dense retrieval by sampling hard negatives via the Approximate Nearest Neighbor (ANN) index. The model is available at https://huggingface.co/ sentence-transformers/msmarco-roberta-base-ance-firstp. TAS-B (Hofstätter et al., 2021), leverages balanced margin sampling for efficient query selection. The model is available at https://huggingface.co/sentence-transformers/ msmarco-distilbert-base-tas-b. 17 Published as conference paper at ICLR 2025 Table 4: Human evaluation on which document is more relevant to the given query semantically? The numbers in parentheses are the proportion agreed upon by all three human annotators. Temperature 0.00 0.20 0.40 0.60 0.80 1.00 Temperature 0.00 0.20 0.40 0.60 0.80 1.00 Temperature 0.00 0.20 0.40 0.60 0.80 1.00 Human 0.0% (0.0%) 0.0%(0.0%) 0.0% (0.0%) 0.0% (0.0%) 0.0% (0.0%) 0.0%(0.0%) Human 0.0% (0.0%) 0.0% (0.0%) 0.0% (0.0%) 0.0% (0.0%) 0.0% (0.0%) 0.0% (0.0%) Human 0.0% (0.0%) 0.0% (0.0%) 0.0% (0.0%) 0.0% (0.0%) 0.0% (0.0%) 0.0% (0.0%) DL19 LLM 5% (0.0%) 5% (0.0%) 0.0% (0.0%) 0.0% (0.0%) 0.0% (0.0%) 0.0% (0.0%) TREC-COVID LLM 0.0% (0.0%) 0.0% (0.0%) 0.0% (0.0%) 0.0% (0.0%) 0.0% (0.0%) 0.0% (0.0%) SCIDOCS LLM 0.0% (0.0%) 0.0% (0.0%) 0.0% (0.0%) 5.0% (0.0%) 0.0% (0.0%) 5% (0.0%) Equal 95% (83.8%) 95% (94.2%) 100% (79.6%) 100% (84.6%) 100% (94.5%) 100% (94.5%) Equal 100% (84.6%) 100% (94.5%) 100% (74.6%) 100% (94.5%) 100% (79.6%) 100% (84.6%) Equal 100% (84.6%) 100% (84.6%) 100% (79.6%) 95% (83.8%) 100% (79.6%) 95% (89.0%) Contriever (Izacard et al., 2022) employs contrastive learning with positive samples generated through cropping and token sampling. The model is available at https://huggingface.co/ facebook/contriever-msmarco. (Gao and Callan, 2022) coCondenser and supervised fine-tuning. sentence-transformers/msmarco-bert-co-condensor. that conducts both pre-training is available at https://huggingface.co/ is retriever The model We follow the metrics proposed by previous work when measuring ranking performance and source bias. For ranking performance, we use NDCG@k (Järvelin and Kekäläinen, 2002). For source bias, we use Relative NDCG@k (Dai et al., 2024a;c; Xu et al., 2024; Zhou et al., 2024), which is formulated as Relative = etricHuman etricLLM 1 2 (M etricHuman + etricLLM ) 100%. In CDC debiasing, considering the sample size we conduct bias correction for the top 10 candidates in retrival. Rising up the candidates number leads to less preference for LLM-generated documents while ranking performance may drop little. E.2 MORE RESULTS OF GENERATED CORPUS WITH VARYING SAMPLING TEMPERATURE E.2.1 HUMAN EVALUATION Although LLM-generated documents are solely based on their corresponding human documents, it is still necessary to verify that the generated document has the same relevance scores with given query as the original documents. To provide empirical support on the fact that LLM-generated documents are not injected with extraneous information about queries, we conduct human evaluation. We randomly select 20 (query, human-written document, LLM-generated document) triples for each dataset and each sampling temperature. The human annotators who have at least Bachelors degrees are asked to evaluate which document is more relevant without knowing document sources. Their results are transferred into the Human, LLM, or Equal options later. The final labels of each 18 Published as conference paper at ICLR 2025 (a) DL19 (b) TREC-COVID (c) SCIDOCS Figure 5: Perplexity and estimated relevance scores of Contriever on positive query-document pairs in three datasets, where documents are generated by LLM with different sampling temperatures. (a) DL19 (b) TREC-COVID (c) SCIDOCS Figure 6: Perplexity and estimated relevance scores of TAS-B on positive query-document pairs in three datasets, where documents are generated by LLM with different sampling temperatures. triple are determined by the votes of three different annotators. The results in Table 4 illustrate that documents from different sources possess the same relevance to the corresponding queries, which guarantees the correctness of our controlled variables experiments. E.2.2 RESULTS WITH MORE PLM-BASED RETRIEVERS In Section 3.1 we discover the negative correlation between document perplexity and estimated relevance scores by Contriever. In this section, we demonstrate the replicability of the discovery by providing similar results on TAS-B and Contriever. As depicted in Figure 5 and Figure 6, there is significant negative correlation between document perplexity and estimated relevance scores as sampling temperature changes. Documents with lower perplexity obtain prevalent higher estimated relevance scores across different PLM-based retrievers, further affirming the universality of the phenomenon. E.2.3 MORE RESULTS OF β2 ESTIMATION In Section 4.1, we estimated the causal effect of perplexity on estimated relevance scores through 2SLS. Since the estimation needs LLM generation, its natural to explore the hyperparameters related to the generation. According to the causal graph we proposed, the sampling temperature does affect ˆβ1 in the first stage of the regression, but is independent with ˆβ2. We explore whether ˆβ1 changes in turn affects the value of ˆβ2 by using documents generated with different sampling temperature. The ˆβ1 and ˆβ2 obtained from our estimation on the set of rewritten texts with different sampling temperatures are shown in Table 5. It can be found that under the maximum sampling temperature difference, the variation of ˆβ1 is within 15% and the variation of ˆβ2 is within 20%, and such variations are similar to the errors brought by random sampling, so the variation of the sampling temperature is acceptable in the CDC algorithm. 19 Published as conference paper at ICLR Table 5: The influence of generation temperatures on the magnitude of the causal coefficients β1, β2. The coefficients are estimated from all positive query-document pairs. DL19 TREC-COVID SCIDOCS Temperature ˆβ2(BERT) ˆβ2(RoBERTa) ˆβ2(ANCE) ˆβ2(TAS-B) ˆβ2(Contriever) ˆβ2(coCondenser) ˆβ1 0.0 -7.80 -23.57 -0.44 -0.81 -0.01 -0.58 -0. 0.2 -7.78 -23.50 -0.44 -0.80 -0.01 -0.58 -0.44 0.4 -7.77 -23.45 -0.44 -0.80 -0.01 -0.58 -0.44 0.6 -7.94 -23.97 -0.45 -0.82 -0.01 -0.59 -0.43 0.0 -1.21 1.73 0.07 -0.34 -0.03 -0.23 -0.41 0.2 -1.20 1.73 0.07 -0.34 -0.03 -0.23 -0.41 0.4 -1.24 1.77 0.07 -0.35 -0.04 -0.24 -0. 0.6 -1.26 1.80 0.07 -0.35 -0.04 -0.24 -0.39 0.0 -2.29 -6.02 -0.22 -0.37 -0.02 -0.25 -0.41 0.2 -2.29 -6.04 -0.22 -0.37 -0.02 -0.25 -0.40 0.4 -2.33 -6.13 -0.22 -0.37 -0.02 -0.25 -0.40 0.6 -2.46 -6.47 -0.23 -0.39 -0.02 -0.26 -0.38 E.3 MORE RESULTS OF CDC DEBIASING In this section, we report more experimental results to provide more comprehensive analysis of CDC, including robustness analysis with error bar  (Table 6)  and significance test (Tabel 7). Table 6: Mean and standard deviation of Performance (NDCG@3) and bias (Relative (Dai et al., 2024c) on NDCG@3) of different PLM-based retrievers with our proposed CDC debiased method on three datasets in five repetitions. Model BERT RoBERTa ANCE TAS-B Contriever coCondenser DL19 (In-Domain) TREC-COVID (Out-of-Domain) SCIDOCS (Out-of-Domain) Performance Bias Performance Bias Performance Bias Mean 77.65 71.33 67.73 75.63 73.83 75.36 Std Mean 5.90 0.89 4.45 0.48 34.95 0.15 -9.97 0.24 -5.33 0.27 9.60 0.47 Std 4.40 0.80 11.51 5.25 1.93 8.49 Mean 45.88 45.86 69.94 62.84 61.35 71.07 Std Mean -18.40 1.14 -10.51 0.78 -1.94 0.77 -37.42 0.48 -31.33 0.73 -45.39 0. Std 6.72 3.58 4.63 3.99 3.22 8.55 Mean 10.44 8.24 12.31 14.15 15.09 13.79 Std Mean 29.19 0.19 32.13 0.18 26.26 0.33 23.48 0.16 1.63 0.10 1.06 0.21 Std 9.35 7.28 10.61 5.84 1.89 2.79 Table 7: The p-value of significance test conducted on the NDCG@3 and Relative (Dai et al., 2024c) on NDCG@3 with and without CDC debias method, with bold fonts indicating the Performance or Bias can pass significance test with p-value< 0.05. As expected, most Performance DOES NOT pass the significance test while all the Bias DOES pass the significance test. Model BERT RoBERTa ANCE TAS-B Contriever coCondenser DL19 TREC-COVID SCIDOCS Performance 4.56e-03 7.26e-02 1.74e-01 6.16e-01 2.77e-01 8.82e-01 Bias 5.33e-04 2.33e-04 4.48e-03 3.19e-04 3.94e-02 1.16ePerformance 1.87e-03 1.22e-01 4.61e-01 8.58e-01 2.98e-01 5.95e-01 Bias 3.97e-05 5.46e-05 3.09e-04 1.27e-03 5.03e-04 3.81e-04 Performance 1.37e-01 8.48e-01 1.62e-01 6.67e-04 4.44e-02 2.71e-01 Bias 1.47e-03 9.45e-07 2.25e-05 2.16e-04 7.65e-03 1.58e-"
        }
    ],
    "affiliations": [
        "CAS Key Laboratory of AI Safety, Institute of Computing Technology, Beijing, China",
        "Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China",
        "Huawei Noahs Ark Lab, Shenzhen, China"
    ]
}