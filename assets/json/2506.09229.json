{
    "paper_title": "Cross-Frame Representation Alignment for Fine-Tuning Video Diffusion Models",
    "authors": [
        "Sungwon Hwang",
        "Hyojin Jang",
        "Kinam Kim",
        "Minho Park",
        "Jaegul choo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Fine-tuning Video Diffusion Models (VDMs) at the user level to generate videos that reflect specific attributes of training data presents notable challenges, yet remains underexplored despite its practical importance. Meanwhile, recent work such as Representation Alignment (REPA) has shown promise in improving the convergence and quality of DiT-based image diffusion models by aligning, or assimilating, its internal hidden states with external pretrained visual features, suggesting its potential for VDM fine-tuning. In this work, we first propose a straightforward adaptation of REPA for VDMs and empirically show that, while effective for convergence, it is suboptimal in preserving semantic consistency across frames. To address this limitation, we introduce Cross-frame Representation Alignment (CREPA), a novel regularization technique that aligns hidden states of a frame with external features from neighboring frames. Empirical evaluations on large-scale VDMs, including CogVideoX-5B and Hunyuan Video, demonstrate that CREPA improves both visual fidelity and cross-frame semantic coherence when fine-tuned with parameter-efficient methods such as LoRA. We further validate CREPA across diverse datasets with varying attributes, confirming its broad applicability. Project page: https://crepavideo.github.io"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 9 2 2 9 0 . 6 0 5 2 : r Cross-Frame Representation Alignment for Fine-Tuning Video Diffusion Models Sungwon Hwang Hyojin Jang Kinam Kim Minho Park Jaegul Choo KAIST AI https://crepavideo.github.io Figure 1: Videos generated by CogVideoX-5B [48] fine-tuned on the Disney [45] dataset. Each model is fine-tuned with: no regularization (Vanilla), REPA* (our implementation of REPA [50] to video diffusion models), and CREPA (ours). Our model yields beter text reflectivity and semantic consistency across frames compared to the baselines."
        },
        {
            "title": "Abstract",
            "content": "Fine-tuning Video Diffusion Models (VDMs) at the user level to generate videos that reflect specific attributes of training data presents notable challenges, yet remains underexplored despite its practical importance. Meanwhile, recent work such as Representation Alignment (REPA) has shown promise in improving the convergence and quality of DiT-based image diffusion models by aligning, or assimilating, its internal hidden states with external pretrained visual features, suggesting its potential for VDM fine-tuning. In this work, we first propose straightforward adaptation of REPA for VDMs and empirically show that, while effective for convergence, it is suboptimal in preserving semantic consistency across frames. To address this limitation, we introduce Cross-frame Representation Alignment (CREPA), novel regularization technique that aligns hidden states of frame with external features from neighboring frames. Empirical evaluations on large-scale VDMs, including CogVideoX-5B and Hunyuan Video, demonstrate that CREPA improves both visual fidelity and cross-frame semantic coherence when fine-tuned with parameter-efficient methods such as LoRA. We further validate CREPA across diverse datasets with varying attributes, confirming its broad applicability. Equal Contribution 39th Conference on Neural Information Processing Systems (NeurIPS 2025)."
        },
        {
            "title": "Introduction",
            "content": "Fine-tuning Video Diffusion Models (VDMs) to generate videos with specific attributes that are not expressible with text prompts presents unique challenge in the field of generative AI. Especially, one may have set of training videos with desired attributes, such as specific cartoon style, physical attribute, or scene staticity, to fine-tune and to generate videos accordingly. However, it is challenging to fine-tune large-scale VDMs at the user level, where limited training steps are often inevitable. This limitation often leads to suboptimal adaptation to training data, making it difficult for the model to capture nuanced attributes or styles present in the training data. Therefore, there is growing need for effective yet efficient fine-tuning strategies that can bridge the gap between high computational requirements and user-level constraints, while preserving the rich generative capabilities of VDMs. However, this problem remains underexplored with few studies addressing this challenge. Meanwhile, recent advancements such as Representation Alignment [50] (REPA) significantly accelerate the convergence of image diffusion models. Specifically, REPA aligns the internal representations of Diffusion Transformers [34] (DiT) with external visual features from self-supervised, pre-trained encoders such as DINOv2 [33]. REPA builds on the insight that DiT behave like Denoising Autoencoders [47] (DAE), where the earlier transformer blocks behave as an encoder to process the noisy images, while the decoder predicts the noise based on the encoded features. Since DAE can learn less information as its input is inherently noisy, REPA enhances the intermediate hidden states of the DiT encoder by projecting and assimilating them to their corresponding pretrained features via an additional regularization objective. This simple yet effective distillation-based regularization method achieves over 17.5 faster convergence on SiT [31] and improved image generation quality on ImageNet [9]. Inspired by the observation, we first propose REPA*, straightforward method of applying REPA to VDMs, and empirically found out that REPA could also benefit finetuning VDMs in some extent. However, we learned that the inherent nature of DAEs that extract hidden states from noisy inputs lead to suboptimal convergence toward the sequence of pretrained features. For instance, while it may be feasible to align noisy hidden states to each of their frames pretrained feature individually, they can still be projected to arbitrary locations relative to its pretrained feature due to the noise in the input of the diffusion model. Such stochasticity can thus lead to semantic inconsistency of hidden states across frames during alignment. For instance, in Fig. 1, while REPA* shows improved convergence compared to model with no regularization (Vanilla) by better capturing the text prompt and the corresponding visual attributes in the training data compared to the model fine-tuned with scoreprediction objective only, the generated frames often suffer from unnatural temporal context, such as broken transitions and objects fragmenting over time. This observation motivated our hypothesis that per-frame alignment does not ideally consider how the hidden states should semantically relate across frames. In this paper, we argue that further aligning hidden states of frame using the pretrained features of adjacent frames can mitigate the problem and enhance the quality of generated videos. To achieve such alignment, we propose novel distillation-based regularization method, which we refer to as Cross-frame Representation Alignment (CREPA) 2. Through empirical studies, we demonstrate that CREPA enables successful fine-tuning of large-scale video diffusion models such as CogVideoX5B [48] and Hunyuan Video [23] within only 3,000 training iterations, which amount to 9 and 13 hours of training time on single A100 GPU respectively, using parameter-efficient fine-tuning techniques such as LoRA [18]. We compare our method against vanilla fine-tuning with scorematching objective alone, as well as fine-tuning with additional regularization methods such as REPA*, across multiple datasets exhibiting diverse characteristics, including unique visual styles, specific physical interactions, and 3D spatial consistency."
        },
        {
            "title": "2 Related Works",
            "content": "Video Diffusion Models Recent advances in video generation have been driven by extending diffusion models to the video domain. Early works such as Align-Your-Latents [6], Stable Video Diffusion [5], and Open-Sora [52] laid the foundation for video diffusion models by adopting 2We will release the code upon publication. 2 U-Net architecture and incorporating disentangled spatial and temporal attention mechanisms, demonstrating large-scale training on video dataset. Subsequent large-scale models like CogVideoX [48], HunyuanVideo [23], and Wan2.1 [42] leveraged DiT [34] and scaled up to billions of parameters, enabling longer and more complex video sequences. These models introduced joint spatio-temporal attention modules to better capture temporal dependencies and enhance generative expressiveness. While these works primarily focus on architectural design and large-scale training, other lines of work demonstrate the utility of fine-tuning VDMs for task-specific applications. For instance, CustomTTT [3] and JointTuner [8] fine-tune VDMs to enable object-centric video generation with controllable motion and appearance. In the meantime, Long Context Tuning [13] adapts VDMs for long-range temporal generation by additionally modifying attention mechanisms to enhance multi-shot consistency. However, since none of these methods explicitly target the enhancement of semantic consistency through regularization as an auxiliary optimization objective during fine-tuning, we believe our approach can complement prior and future works that fine-tunes VDMs. Fine-Tuning Large-Scale Models The specialization of large-scale models through fine-tuning or adaptation has become widely adopted strategy for tailoring pre-trained models to specific tasks or domains. This paradigm leverages the broad generalization capabilities of foundation models while enabling efficient task-specific refinement. Numerous methods have been introduced to facilitate this process across different modes of data, including large language models [7, 14], vision-language models [28, 27], image diffusion models [36, 38], vision foundation models [19, 51], and even in geometric foundation models [30]. These methods often leverage parameter-efficient fine-tuning methods such as LoRA [18] to maximally leverage prior knowledge of pretrained model and maximize training efficeincy. It is important to recognize that fine-tuning strategies often reflect the intrinsic characteristics of the data modality. For example, fine-tuning text-to-image diffusion models typically focuses on disentangling visual style from object-centric content [38, 44]. In the case of 3D foundation models such as DUSt3R [43], adaptation methods are designed to accurately estimate per-pixel confidence in depth back-projection to ensure reliable geometric reasoning [30]. Analogously, we posit that fine-tuning VDMs should explicitly account for semantic consistency, core property of video data that is not relevant to models designed for other data domains."
        },
        {
            "title": "3 Preliminaries",
            "content": "Diffusion models [39, 16, 40, 41] are class of generative models that synthesize data by inverting gradual noising process applied to clean samples. This procedure consists of two main phases: the forward process and the reverse process. Forward Process. The forward process gradually transforms data sample x0 p(x0) into noisy variable xT through sequence of perturbations, typically by adding Gaussian noise. This transformation can be modeled either in continuous time using stochastic or ordinary differential equation: dx = (x, t)dt + g(t)dw, (1) where (x, t) defines the deterministic drift, g(t) controls the noise magnitude, and dw denotes standard Wiener process (for SDE [41]) or is omitted in the deterministic case (for ODE [40]). As , the sample xt approaches known prior distribution, such as (0, I). Reverse Process. The generative process aims to reconstruct samples from the data distribution by reversing the forward dynamics. Assuming access to the data score function log pt(x), the reverse-time dynamics can be expressed as: dx = (cid:2)f (x, t) g(t)2x log pt(x)(cid:3) dt + g(t)d w, where denotes the reverse-time Wiener process. In practice, the score function is approximated by neural network sθ(x, t) trained to match log pt(x) via denoising score matching. Sampling is then performed by numerically solving the reverse-time SDE or its deterministic counterpart. Denoising Score Matching Loss. To train the score-based model sθ(x, t), one commonly minimizes denoising score matching loss, which encourages the predicted score to match the true gradient of the log-density of xt. widely used objective in diffusion models is the simplified variational bound: (2) Lscore(θ) = Ex0,ϵ,t ϵ ϵθ(xt, t)2 (cid:105) , (3) (cid:104) 3 Figure 2: CKNNA [21] between hidden states of frame to pretrained features, or representation alignment, to (a) preceding, (b) future, and (c) current frames. CREPA promotes alignment to adjacent frames, while maintaining or even slightly improving the alignment to the current frame. where ϵθ denotes neural network trained to predict the noise added at time t, and xt is generated from x0 using the forward noising process. This formulation is equivalent to score matching under certain assumptions and is widely adopted for its stability and empirical performance."
        },
        {
            "title": "4 Methods",
            "content": "We begin by proposing REPA*, an extension of REPA for fine-tuning VDMs, and make empirical verification on its limitations of disregarding the semantics of past and future frames during the alignment of noisy hidden states. To address this issue, we introduce CREPA, which explicitly incorporates temporal context through pretrained features of adjacent frames."
        },
        {
            "title": "4.1 Per-frame Representation Alignment for Video Diffusion Models",
            "content": "Recall that REPA aligns the internal representations of Image Diffusion Transformers with external visual features obtained from self-supervised, pre-trained encoders. Motivated by the observation that DiTs function similarly to Denoising Autoencoders [47], where early transformer blocks act as an encoder to extract relevant information from noisy image and later blocks serve as decoder, REPA enhances noisy hidden states of DiT encoders by projecting and supervising them with corresponding pretrained features that contain richer information for distillation. Correspondingly, better and faster convergence introduced by REPA can also benefit the fine-tuning of DiT on user-level where number of training iterations is often limited. We first propose REPA*, an extension to per-frame application of REPA in VDMs. Since general, widely used pretrained visual encoder for videos does not exist or focus only on specific visual domain such as Minecraft [2], we instead assume pre-trained image encoder for the method. Specifically, given frame xf 0 of clean video x0 p(x0) and pre-trained encoder E, we first extract per-frame pretrained feature yf = E(xf 0 ). Then, the DiT encoder gθ, which is composed of the first few DiT blocks, encodes the noisy video input xt to the hidden state ht = gθ(xt, t), where per-frame hidden is then projected to the pretrained feature space via small MLP hϕ() to be assimilated state hf with its corresponding pretrained feature via the regularization-based distillation: Lalign(ϕ) := Ex0,ϵ,t sim(yf , hϕ(hf )) , (cid:88) where sim(, ) is similarity function. Thus, the final objective for fine-tuning becomes where λ is hyperparameter to control the strength of the alignment. := Lscore + λLalign, 4 (4) (5) Figure 3: Overview of CREPA and comparison to REPA*. By aligning hidden states not only to the current but also to the adjacent pretrained features, CREPA further guides the hidden state representations toward the temporal manifold formed by the sequence of pretrained features."
        },
        {
            "title": "4.2 Cross-frame Representation Alignment",
            "content": "However, due to the inherent nature of DAEs training on noisy inputs and jointly training hθ during fine-tuning, we learned that regularizing VDMs with Eq. (4) alone cannot prevent the projected hidden state for each frame from converging to arbitrary locations with respect to their corresponding pretrained feature. We hypothesize that such ill-posed solutions cause weaker cross-frame representation alignment, which we define as semantic similarity between the hidden states of frame and pretrained features of its adjacent frames. We argue that weaker cross-frame representation alignment causes semantically inconsistent and suboptimal generation in VDMs. Empirical observation We first quantify the degree of cross-frame representation alignment for models fine-tuned with REPA*, and compare with CREPA. Specifically for each model, we compute the similarity between the hidden state of current frame and the pretrained features of both the adjacent and current frames. For similarity, we utilize CKNNA [21], kernel alignment metric that extends CKA [24] by incorporating mutual nearest neighbors to compare representations across different feature spaces. We follow REPA [50] for its implementation, provide further details in Appendix B, and report the results in Fig. 2. Compared to REPA*, CREPA yields higher similarity between the current hidden states and adjacent pretrained features, as shown in Fig. 2-(a),(b), while maintaining or slightly improving the alignment with the pretrained features of the current frame in Fig. 2-(c). This suggests that regularizing hidden states solely based on the current frame is under-constrained: noisy hidden states, which can converge to different locations with respect to its corresponding pretrained features, may exhibit arbitrary distances to the pretrained features of adjacent frames while satisfying similar distance to that of the current frame. Our approach To address such under-constraint problem caused by the nature of DAEs encoding noisy input, we draw inspiration from total variation regularization, well-known technique that penalizes noisy variations using information from neighboring pixels in raster space [35, 49, 32]. Analogously, we encourage to reduce the uncertainty of alignment in temporal dimension by using information of pretrained features from adjacent frames. Note that our objective extends beyond mere temporal smoothness; instead of directly smoothing among the inherently noisy sequence of hidden states, we impose cross-frame distillation from adjacent pretrained features, where semantic is more robustly captured. In other words, our objective is to align hidden states with the semantic evolution of pretrained features, ensuring that hidden states of frame are further regularized toward the temporal semantic context present in training data. To do so, we define simple yet novel regularization objective for alignment: Lalign(ϕ) := Ex0,ϵ,t (cid:88) (cid:32) sim(yf , hϕ(hf )) + (cid:88) kf τ sim(yk, hϕ(hf )) (cid:33) , (6) kK where = {f d, + d}, is the adjacency parameter, and τ is the temperature coefficient. conceptual overview of our method and comparison to REPA* is illustrated in Fig. 3. 5 Models Aesthetic Quality Background Consistency Motion Smoothness Subject Consistency Imaging Quality Vanilla REPA* CREPA (Ours) Vanilla REPA* CREPA (Ours) 0.5013 0.5145 0.5207 0.5243 0.5362 0.5351 0.9293 0.9303 0. 0.9347 0.9362 0.9491 0.9827 0.9828 0.9847 0.9809 0.9849 0.9895 0.8784 0.8798 0.8891 0.9107 0.8984 0.9205 0.5982 0.6284 0. 0.6285 0.6280 0.6451 Table 1: VBench [20] evaluation on Hunyuan Video [23] (Top) and CogVideoX-5B (Bottom). CREPA outperforms baselines on most metrics, especially on metrics related to semantic consistency."
        },
        {
            "title": "5.1 Setup",
            "content": "Models We employ CogVideoX-5B [48], which features an expert transformer with adaptive LayerNorm and 3D full attention for effective fusion of text and video features. We also employ Hunyuan Video [23], dual-to-single stream transformer designed with unified attention mechanism with 13B parameters. We also experiment with the Image-to-Video model of CogVideoX-5B. Baselines Since few works have addressed convergence and semantic consistency in VDM finetuning, we establish and compare against our own baselines. Specifically, we compare CREPA with Vanilla, models fine-tuned with score prediction objective only, and REPA*. Implementation Details We use λ = 0.5 for CogVideoX-5B and λ = 1 for Hunyuan video, where we empirically found to work the best on REPA*. We also use = 1 and τ = 1 for CREPA. As both models define DiT models in the latent space formed by VAE that introduces 4 temporal compression rate, the effective adjacency for = 1 is 4. For pretrained models, we used DINOv2g [33] following REPA [50]. To determine the hidden state layer for alignment in DiT, we first perform linear probing to identify the encoder, followed by layer-wise analysis to select the optimal layer, as detailed in Appendix A. Based on this procedure, we select the 8th layer for CogVideoX and the 10th layer for Hunyuan Video. We train with single A100 NVIDIA GPU with 80G VRAM for 3000 iterations for all models. Dataset We experimented on 7 video datasets that contains various visual and temporal attributes. We list the datasets into 4 categories based on their characteristics: Tom and Jerry [46] and Disney [45] consist of clips of cartoons with unique illustrative styles. Crush [11], Cakeify [10] and Squish [12] consist of curated video clips with specific physical interactions for VDMs to adapt to. Specifically, Crush describes an object flattened under hydraulic press, Cakeify describes cake designed to not look like cake being cut like cake, and Squish describes an object being squished as if it is made out of soft material. DL3DV [26] contains video captures of bounded and unbounded scenes with no dynamic objects included. As so, DL3DV is widely used for Novel View Synthesis and 3D reconstruction tasks. Out of 10K video clips, we randomly sample 200 videos for train and test each. We use CogVLM2-Video [17] to generate captions. Scenes [4] contains clips of movie, which is photorealistic yet contains specific visual nuance. Evaluation We evaluate our method on two representative aspects of video generation: perceptual quality and semantic consistency. For perceptual-level evaluation, we report Fréchet Video Distance (FVD) and Inception Score (IS) using the CogVideoX-5B I2V model finetuned with the DL3DV dataset [26]. FVD is computed using an I3D model , and IS is measured with C3D model. These metrics reflect the distributional alignment and frame-level realism of generated videos. To assess frame-wise semantic consistency and visual coherence, we adopt VBench [20], multiattribute benchmark. Criteria relevant to semantic consistency in VBench are Motion Smoothness, Background Consistency, Subject Consistency. We also measure Aesthetic Quality and Imaging Quality in VBench to further evaluate the perceptual quality."
        },
        {
            "title": "5.2 Results\nQuantitative Results We compare CREPA with Vanilla and REPA on standard benchmarks using\nVBench, as shown in Table 4. CREPA consistently outperforms both baselines across most metrics\nin VBench. It shows strong semantic consistency (e.g., background and subject alignment) and\nsmooth motion, maintaining coherence throughout generated videos. While REPA is on par with a",
            "content": "6 Figure 4: Videos generated by Hunyuan Video [23] fine-tuned on Crush [11] dataset. CREPA enhances convergence relative to Vanilla by better learning the physical attribute of the data. Also, CREPA yields better semantic consistency compared to REPA*. Text prompt reported in Appendix D. few categories related to perceptual quality, CREPA offers more balanced and robust performance including metrics on semantic consistency, highlighting its effectiveness in generating high-quality and semantically consistent video. We report FVD and IS on Table 2. CREPA shows reasonable improvements over the baselines, achieving the best performance in both metrics. This indicates that CREPA excels in both distributionlevel similarity to real videos and frame-level object quality. Qualitative Results3 We report qualitative results on the baselines as well as an example from training data as reference. Notably in Fig. 4 and Fig. 6, both REPA* and CREPA shows better generation quality compared to Vanilla, demonstrating that the distillation-based regularizations do facilitate better convergence. However, REPA* often yields semantically inconsistent videos, (i.e., shape and appearance of statue changes over frames in Fig. 5, the shape of the ball changes in physically implausible ways in Fig. 4, or the appearance of the character changes in Fig. 6). These observations suggest that CREPA is more effective at preserving semantic consistency present in training data. We report additional results in Appendix C. Application to Novel View Synthesis VDMs have recently been adapted into World Fondational Models (WFMs), which are essentially video generative models for simulating 3D consistent scene navigation [1, 25, 15]. They are often fine-tuned on spatially consistent scene navigation dataset such as DL3DV [26]. We hypothesize that CREPA is also suitable for enhancing WFMs by better learning 3D spatial consistency inherent in inter-frame relations within DL3DV. To test this, we first use COLMAP [37] to estimate camera poses of video frames generated from CogVideoX-5B models, each of which are finetuned on DL3DV under the baselines and our method. Then, we reconstruct 3D scenes with 3DGS [22] using all but every 8th frame, and perform novel view synthesis (NVS) on these held-out views. Since NVS relies solely on other frames for rendering, higher similarity between the image rendered by NVS and the original frame at the target view generated by the VDM indicates stronger 3D consistency in the generated video. As Fig. 7 and Table. 3 shows, CREPA yields better NVS performance compared to REPA* and vanilla. 3We strongly recommend to refer to the video results in the project page. 7 Figure 5: Videos generated by CogVideoX-5B [48] fine-tuned on DL3DV [26] dataset. Compared to REPA*, CREPA yields more semantically consistent objects in video across the frames. Meanwhile, Vanilla yields disappearing road in the background. Text prompt reported in Appendix D. Figure 6: Videos generated by CogVideoX-5B [48] fine-tuned on Tom and Jerry [46] dataset. CREPA improves convergence and semantic consistency over Vanilla and REPA*, respectively. Text prompt reported in Appendix D. 8 Figure 7: Novel view synthesis on 3D scenes reconstructed with generated videos. Spatially inconsistent videos cause inaccurate camera pose estimation from COLMAP [37], as well as imprecise supervision for rendering loss while training 3DGS [22]. Models Vanilla REPA* CREPA (Ours) FVD 305.542 291.388 281.192 IS 34.1 35.2 35.8 Models PSNR SSIM LPIPS PSNR SSIM LPIPS T2V I2V Vanilla REPA* CREPA (Ours) 21.83 22.17 22.66 0.741 0.742 0.760 0.255 0.253 0.248 22.10 22.45 22.88 0.755 0.758 0. 0.250 0.252 0.245 Table 2: Quantitative results on FVD and IS. CREPA yields better FVD and IS compared to the baselines, indicating its potential for better visual fidelity. Table 3: Quantitative results on novel view synthesis. Given models finetuned with DL3DV [26] dataset, we generate videos with T2V and I2V models, followed by estimating camera pose for each frame per video using COLMAP [37]. Then, we train and evaluate 3DGS [22] using the camera pose estimations to measure spatial consistency from the generated videos. User Study We additionally conducted user study to evaluate the perceptual quality of the generated videos across six criteria: text-video alignment, video quality, motion quality, semantic consistency, training data reflectivity, and overall preference. Participants consistently favored videos generated with CREPA over those from REPA* and Vanilla across all aspects. Full details of the user study and results are provided in Appendix E."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we proposed CREPA, regularization method for fine-tuning VDMs. We first introduced REPA*, an adaptation of REPA to VDMs, for better and faster convergence for VDM fine-tuning. However, we empirically showed that aligning hidden states only to their corresponding frame via REPA* is insufficient to ensure cross-frame semantic consistency. CREPA addresses this limitation by additionally aligning hidden states to pretrained features from adjacent frames. Through extensive experiments across diverse datasets and models, we demonstrated that CREPA improves convergence compared to model fine-tuned without any regularization, while enhancing semantic consistency compared to the model trained with REPA*. Limitation key limitation of our work is that the proposed regularization method requires searching across DiT layers for different VDMs. However, this layer search does not need to be performed by every end user. single representative or the model distributor can carry out the search once, and share the optimal layer index with downstream users, making the method practical for broader adoption. Future Works Our distillation-based regularization technique presents an interesting opportunity for application during the pre-training phase of VDMs. However, we defer such exploration to future work, particularly by institutions with access to large-scale and high-quality video datasets and sufficient computational resources."
        },
        {
            "title": "References",
            "content": "[1] N. Agarwal, A. Ali, M. Bala, Y. Balaji, E. Barker, T. Cai, P. Chattopadhyay, Y. Chen, Y. Cui, Y. Ding, et al. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025. [2] B. Baker, I. Akkaya, P. Zhokov, J. Huizinga, J. Tang, A. Ecoffet, B. Houghton, R. Sampedro, and J. Clune. Video pretraining (vpt): Learning to act by watching unlabeled online videos. Advances in Neural Information Processing Systems, 35:2463924654, 2022. [3] X. Bi, J. Lu, B. Liu, X. Cun, Y. Zhang, W. Li, and B. Xiao. Customttt: Motion and appearance customized video generation via test-time training. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 18711879, 2025. [4] bigdata pw. Scenes dataset. https://huggingface.co/datasets/bigdata-pw/scenes, 2025. [5] A. Blattmann, T. Dockhorn, S. Kulal, D. Mendelevitch, M. Kilian, D. Lorenz, Y. Levi, Z. English, V. Voleti, A. Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [6] A. Blattmann, R. Rombach, H. Ling, T. Dockhorn, S. W. Kim, S. Fidler, and K. Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2256322575, 2023. [7] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [8] F. Chen, S. Zhao, C. Xu, and L. Lan. Jointtuner: Appearance-motion adaptive joint training for customized video generation. arXiv preprint arXiv:2503.23951, 2025. [9] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. [10] Finetrainers. Cakeify smol dataset. https://huggingface.co/datasets/finetrainers/ cakeify-smol, 2024. [11] Finetrainers. Crush smol dataset. https://huggingface.co/datasets/finetrainers/ crush-smol, 2024. [12] Finetrainers. Squish pika dataset. https://huggingface.co/datasets/finetrainers/ squish-pika, 2024. [13] Y. Guo, C. Yang, Z. Yang, Z. Ma, Z. Lin, Z. Yang, D. Lin, and L. Jiang. Long context tuning for video generation. arXiv preprint arXiv:2503.10589, 2025. [14] S. Gururangan, A. Marasovic, S. Swayamdipta, K. Lo, I. Beltagy, D. Downey, and N. A. Smith. Dont stop pretraining: Adapt language models to domains and tasks. arXiv preprint arXiv:2004.10964, 2020. [15] H. He, Y. Xu, Y. Guo, G. Wetzstein, B. Dai, H. Li, and C. Yang. Cameractrl: Enabling camera control for video diffusion models. In The Thirteenth International Conference on Learning Representations. [16] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [17] W. Hong, W. Wang, M. Ding, W. Yu, Q. Lv, Y. Wang, Y. Cheng, S. Huang, J. Ji, Z. Xue, et al. Cogvlm2: Visual language models for image and video understanding. arXiv preprint arXiv:2408.16500, 2024. [18] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. [19] X. Hu, X. Xu, and Y. Shi. How to efficiently adapt large segmentation model (sam) to medical images. arXiv preprint arXiv:2306.13731, 2023. [20] Z. Huang, Y. He, J. Yu, F. Zhang, C. Si, Y. Jiang, Y. Zhang, T. Wu, Q. Jin, N. Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. [21] M. Huh, B. Cheung, T. Wang, and P. Isola. The platonic representation hypothesis. CoRR, 2024. [22] B. Kerbl, G. Kopanas, T. Leimkühler, and G. Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. [23] W. Kong, Q. Tian, Z. Zhang, R. Min, Z. Dai, J. Zhou, J. Xiong, X. Li, B. Wu, J. Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [24] S. Kornblith, M. Norouzi, H. Lee, and G. Hinton. Similarity of neural network representations revisited. In International conference on machine learning, pages 35193529. PMLR, 2019. [25] H. Liang, J. Cao, V. Goel, G. Qian, S. Korolev, D. Terzopoulos, K. N. Plataniotis, S. Tulyakov, and J. Ren. Wonderland: Navigating 3d scenes from single image. arXiv preprint arXiv:2412.12091, 2024. [26] L. Ling, Y. Sheng, Z. Tu, W. Zhao, C. Xin, K. Wan, L. Yu, Q. Guo, Z. Yu, Y. Lu, et al. Dl3dv-10k: large-scale scene dataset for deep learning-based 3d vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2216022169, 2024. [27] H. Liu, C. Li, Y. Li, and Y. J. Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. [28] H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [29] Y. Liu, X. Cun, X. Liu, X. Wang, Y. Zhang, H. Chen, Y. Liu, T. Zeng, R. Chan, and Y. Shan. Evalcrafter: Benchmarking and evaluating large video generation models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2213922149, 2024. [30] Z. Lu, H. Yang, D. Xu, B. Li, B. Ivanovic, M. Pavone, and Y. Wang. Lora3d: Low-rank self-calibration of 3d geometric foundation models. arXiv preprint arXiv:2412.07746, 2024. [31] N. Ma, M. Goldstein, M. S. Albergo, N. M. Boffi, E. Vanden-Eijnden, and S. Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In European Conference on Computer Vision, pages 2340. Springer, 2024. [32] A. Mahendran and A. Vedaldi. Understanding deep image representations by inverting them. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 51885196, 2015. [33] M. Oquab, T. Darcet, T. Moutakanni, H. V. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. HAZIZA, F. Massa, A. El-Nouby, et al. Dinov2: Learning robust visual features without supervision. Transactions on Machine Learning Research. [34] W. Peebles and S. Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [35] L. I. Rudin, S. Osher, and E. Fatemi. Nonlinear total variation based noise removal algorithms. Physica D: nonlinear phenomena, 60(1-4):259268, 1992. [36] N. Ruiz, Y. Li, V. Jampani, Y. Pritch, M. Rubinstein, and K. Aberman. Dreambooth: Fine tuning text-toimage diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2250022510, 2023. [37] J. L. Schönberger and J.-M. Frahm. Structure-from-motion revisited. In Conference on Computer Vision and Pattern Recognition (CVPR), 2016. [38] V. Shah, N. Ruiz, F. Cole, E. Lu, S. Lazebnik, Y. Li, and V. Jampani. Ziplora: Any subject in any style by effectively merging loras. In European Conference on Computer Vision, pages 422438. Springer, 2024. [39] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 22562265. PMLR, 2015. [40] J. Song, C. Meng, and S. Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. [41] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. [42] A. Wang, B. Ai, B. Wen, C. Mao, C.-W. Xie, D. Chen, F. Yu, H. Zhao, J. Yang, J. Zeng, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 11 [43] S. Wang, V. Leroy, Y. Cabon, B. Chidlovskii, and J. Revaud. Dust3r: Geometric 3d vision made easy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20697 20709, 2024. [44] Z. Wang, L. Zhao, and W. Xing. Stylediffusion: Controllable disentangled style transfer via diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 76777689, 2023. [45] Wild-Heart. Disney video generation dataset. https://huggingface.co/datasets/Wild-Heart/ Disney-VideoGeneration-Dataset, 2024. [46] Wild-Heart. Tom and jerry video generation dataset. https://huggingface.co/datasets/ Wild-Heart/Tom-and-Jerry-VideoGeneration-Dataset, 2024. [47] W. Xiang, H. Yang, D. Huang, and Y. Wang. Denoising diffusion autoencoders are unified self-supervised learners. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15802 15812, 2023. [48] Z. Yang, J. Teng, W. Zheng, M. Ding, S. Huang, J. Xu, Y. Yang, W. Hong, X. Zhang, G. Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. CoRR, 2024. [49] R. A. Yeh, C.-Y. Chen, and A. G. Schwing. Total variation optimization layers for computer vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1234512354, 2022. [50] S. Yu, S. Kwak, H. Jang, J. Jeong, J. Huang, J. Shin, and S. Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. arXiv preprint arXiv:2410.06940, 2024. [51] Y. Yue, A. Das, F. Engelmann, S. Tang, and J. E. Lenssen. Improving 2d feature representations by 3d-aware fine-tuning. In European Conference on Computer Vision, pages 5774. Springer, 2024. [52] Z. Zheng, X. Peng, T. Yang, C. Shen, S. Li, H. Liu, Y. Zhou, T. Li, and Y. You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024."
        },
        {
            "title": "Appendix",
            "content": "A Locating layers for hidden-state retrieval Linear probing for locating denoising encoder We first locate the encoder of the DAEs, as it is important to distill through the intermediate hidden state of the encoder for regularization. Similar to [47, 50], we conduct linear probing experiment, where we train shallow linear classifiers for each hidden state from different layers to predict the class label of the noisy input. The intuition behind this experiment is that the layer with the highest classification accuracy retrieves the most interpretable hidden state, meaning that the layer yields the output of the encoder. To construct dataset for the experiments, we use all the fine-tuning datasets we use for this paper by regarding each dataset as class. We report the linear probing result for CogVideoX-5B and Hunyuan video in Fig. 8. Based on the result, we regard layers behind 14th and 32th as diffusion encoder for CogVideoX-5B and Hunyuan Video, respectively. Figure 8: Linear probing results on CogVideoX-5B [48] and Hunyuan Video [23]. We regard the layers that precede the higher classification accuracy as the layers of the diffusion encoder. Finding optimal hidden state layers for REPA*. Having identified the diffusion encoder for each VDM, we next conduct experiments to determine the optimal intermediate layer of diffusion encoder for hidden state regularization. Specifically, we use VBench [20] to evaluate which layer yields the best performance on models regularized based on REPA*. Block index(REPA*) Aesthetic Quality Background Consistency Motion Smoothness Subject Consistency Imaging Quality 8 10 (selected) 12 14 6 8 (selected) 10 12 0.5640 0.5640 0.5645 0.5643 0.5460 0.5519 0.5478 0.5479 0.9445 0.9459 0.9446 0. 0.9488 0.9497 0.9490 0.9457 0.9874 0.9883 0.9754 0.9765 0.9857 0.9854 0.9861 0.9847 0.9087 0.9143 0.9137 0.9134 0.8963 0.9052 0.9002 0.8994 0.6635 0.6648 0.6640 0. 0.6245 0.6258 0.6265 0.6243 Table 4: VBench [20] results on models trained with REPA under different hidden state layers. We experimented over Hunyuan Video [23] (top) and CogVideoX-5B [48] (bottom). Selected indices are marked as (selected); best scores are highlighted."
        },
        {
            "title": "B Implementation details for empirical analysis via CKNNA",
            "content": "We used Disney [45] dataset for fine-tuning CogVideoX-5B [48] using DINOv2 [33] as pretrained encoder for the distillation-based regularization. Following REPA [50], we evaluate with hidden states for timesteps 0 0.5. For CKA [24], we follow the exact formulation from REPA. Also, we use inner product as kernel, and use = 10 nearest neighbors for CKNNA [21]. When visualizing the results using box plot, each data point is calculated per frame. For instance, video whose number of frames in VAE latent space is 49 yields 48 measurements if it is cross-frame measurement or 49 measurements if it is measurement for each frame. We remove the lower and upper 3% of the measurements."
        },
        {
            "title": "C More qualitative results",
            "content": "Here, we provide additional qualitative results. We strongly recommend to refer to the project page for accurate qualitative comparison. 13 Figure 9: Videos generated by Hunyuan Video [23] fine-tuned on Crush [11] dataset. Figure 10: Videos generated by CogVideoX-5B [48] fine-tuned on Crush [11] dataset. 14 Figure 11: Videos generated by Hunyuan Video [23] fine-tuned on Cakeify [10] dataset. Figure 12: Videos generated by Hunyuan Video [23] fine-tuned on Cakeify [10] dataset. 15 Figure 13: Videos generated by Hunyuan Video [23] fine-tuned on Cakeify [10] dataset. Figure 14: Videos generated by CogVideoX-5B [48] fine-tuned on Cakeify [10] dataset. 16 Figure 15: Videos generated by CogVideoX-5B [48] fine-tuned on Cakeify [10] dataset. Figure 16: Videos generated by Hunyuan Video [23] fine-tuned on Squish [12] dataset. 17 Figure 17: Videos generated by Hunyuan Video [23] fine-tuned on Squish [12] dataset. Figure 18: Videos generated by CogVideoX-5B [48] fine-tuned on Squish [12] dataset. 18 Figure 19: Videos generated by CogVideoX-5B [48] fine-tuned on Disney [45] dataset. Figure 20: Videos generated by CogVideoX-5B [48] fine-tuned on Disney [45] dataset. 19 Figure 21: Videos generated by CogVideoX-5B [48] fine-tuned on Tom and Jerry [46] dataset. Figure 22: Videos generated by CogVideoX-5B [48] fine-tuned on DL3DV [26] dataset. 20 Figure 23: Videos generated by Hunyuan Video [23] fine-tuned on Scenes [10] dataset. Figure 24: Videos generated by Hunyuan Video [23] fine-tuned on Scenes [4] dataset."
        },
        {
            "title": "D Text prompts used for generation",
            "content": "Prompts for figures in main pages  (Fig. 46)  Fig. 4: colorful puzzle ball is being crushed by large metal cylinder, which flattens the objects as if they were under hydraulic press. Fig. 5: The video features series of bronze statues in park, each depicting an adult and child in various intimate and playful interactions. The statues, set against backdrop of lush greenery, wooden bridge, and clear sky, are placed on pathways and bridges, symbolizing nurturing relationship. The scenes are tranquil, with no people present, and the soft lighting suggests its either early morning or late afternoon. The park is well-maintained, with young trees and serene atmosphere, and the text bilibili appears in one of the frames, indicating possible association with media platform. Fig. 6: In dimly lit alleyway, Tom, the mischievous cat, is seen crouching stealthily behind green trash can. His eyes are wide with anticipation, and his ears are perked up, listening for any sign of Jerry, the clever mouse. The alleyway is littered with various discarded items, creating sense of clutter and disarray. The lighting casts dramatic shadows, highlighting the tension between the two characters. In the background, the faint sound of footsteps can be heard, adding to the suspenseful atmosphere. The scene captures the classic rivalry between Tom and Jerry, as they engage in their timeless game of cat and mouse."
        },
        {
            "title": "Prompt for figures in appendix pages",
            "content": "Fig. 9: rubber boot is placed on the platform. As the hydraulic press moves down, the boot compresses and wrinkles before bursting at the seams. Fig. 10: large, cylindrical object is seen pressing down on small orange ball, causing it to flatten as if it were under hydraulic press. The background features green wall with yellow and red warning signs. Fig. 11: hand wearing black glove holds knife, slicing through Coca-Cola can that has been transformed into hyper-realistic prop cake. The cake is cut in half, revealing its cake-like interior. Fig. 12: black leather wallet rests on wooden surface. sharp blade cuts into the material, exposing layers of sponge and frosting beneath the realistic edible leather texture. Fig. 13: bar of soap sits in soap dish, its pastel color catching the light. blade smoothly cuts through the bar, revealing layers of lemon cake and frosting beneath the glossy icing. Fig. 14: stack of pancakes sits on white plate, drizzled with syrup and butter. hand with knife slices through the stack, unveiling that the entire dish is actually cake, complete with pancake-textured fondant and caramel-flavored layers. Fig. 15: hand wearing black glove holds knife, slicing through Coca-Cola can that has been transformed into hyper-realistic prop cake. The cake is cut in half, revealing its cake-like interior. Fig. 16: steaming cup of coffee sits on desk. hand gently presses down on the ceramic mug, which bends and flattens like stress ball, losing all its rigid form. Fig. 17: soccer ball is placed on grassy field. childs foot kicks it, but instead of bouncing away, it deforms completely and remains squashed on the ground. 22 Fig. 18: terracotta pot with visible crack sits centered on white surface, bathed in sunlight. Two hands enter the frame, positioning themselves around the pot. The hands then begin to press and mold the pot, the clay beginning to rise from the opening. The clay is reshaped and compressed, transforming the pot into bulbous, amorphous lump. The final shot displays the morphed clay shape standing upright on the white surface. Fig. 19: black-and-white animated scene unfolds on semi-rural dock, with cow standing on wooden planks, holding piece of paper with FOB written on it. The cow is the central focus, amidst static barrels, crates, and PODUNK LANDING sign in the background. The atmosphere remains calm and still, with the cows presence subtly shifting the narratives tone. sign of pause or anticipation, the scene is frozen in time, inviting the viewer to ponder the storys next development. Fig. 20: black-and-white animated video showcases central character with round body and large ears standing in an indoor setting with plain background. The character is surrounded by smaller figures, displaying various expressions of interest or curiosity. As the video progresses, subtle changes occur among the figures, suggesting movement and reactions. The scene transitions to focus on single bird perched on perch, with its posture and expression changing subtly throughout the frames, showing signs of activity. Fig. 21: Tom, the mischievous cat, is crouched in the corner of dimly lit room, his eyes fixed on Jerry, the clever mouse, who scurries across the wooden floor. The room is adorned with blue wall and golden doorknob, adding touch of elegance to the otherwise mundane setting. Toms fur is mix of gray and white, blending seamlessly with the shadows cast by the flickering light. Jerry, on the other hand, is vibrant mix of brown and white, his nimble movements creating stark contrast against the stillness of the room. The tension between the two characters is palpable, as they engage in silent battle of wit and agility. The scene is classic representation of the timeless rivalry between Tom and Jerry, captured in moment of suspense and anticipation. Fig. 22: The video explores traditional Chinese museum, starting with serene interior featuring glass display case with golden figurines and bamboo wall with calligraphy. As the video continues, various scenes show the museums historical artifacts, including lion statue, model of an ancient Chinese architectural structure, and skeleton with animal bones. The exhibit also includes display case with human skeleton, model of an ancient burial site, and bronze incense burner. The museums ambiance is tranquil, with soft lighting and blend of historical and modern elements, culminating in display of golden figurines and model of an ancient Chinese architectural complex. Fig. 23: person is seen standing in cluttered room, with table covered in various items. The scene then shifts to man holding gun, seemingly in threatening stance. The mans presence creates sense of tension and danger in the room. Fig. 24: In the video, group of individuals are seen entering room, each holding gun. The scene progresses as they move deeper into the room, flattening objects as if they were under hydraulic press. The room is adorned with framed pictures on the walls, adding to the tension of the scene. 23 Figure 25: Results of user study conducted over Vanilla, REPA*, and CREPA. CREPA is preferred by human participants over REPA* and Vanilla on all criteria."
        },
        {
            "title": "E User Study",
            "content": "To evaluate the quality of the generated videos, we conducted human evaluation using 20 samples per criterion. For each sample, participants were shown three videosgenerated by Vanilla, REPA*, and CREPAin randomized order to ensure fairness. Participants were asked to select the best video for given criterion, assigning 1 point to their choice. The final score for each model is the total number of times it was selected, with higher scores indicating stronger preference. The evaluation was conducted across six criteria: (1) TextVideo Alignment, reflecting the frame-level clarity of the video, including sharpness and the absence of artifacts such as tearing or distortion; (2) Visual Quality, reflecting the frame-level clarity of the video, including sharpness and the absence of artifacts such as tearing or distortion;; (3) Motion Quality, evaluating the naturalness and stability of movement across frames, ensuring the motion is smooth and not visually awkward; (4) Overall Likeness, reflecting overall user preference that is, if participant had to select single video for practical use, which one they would choose; (5) Semantic Consistency, measuring whether the video maintains coherent meaning across frames without illogical or abrupt semantic shifts (e.g., object deformation or identity inconsistency); (6) Training Data Reflectivity, assessing how accurately the video reflects the visual and stylistic characteristics of the training distribution; The first five criteria are based on those defined in the EvalCrafter [29] benchmark, with Semantic Consistency adapted from Temporal Consistency to better reflect meaning-level coherence across frames. Training Data Reflectivity was newly introduced to evaluate how well the generated videos capture the visual and stylistic characteristics of the training data. As reported in Fig. 25 , CREPA received the highest scores across all evaluation categories, reflecting strong overall preference by participants."
        },
        {
            "title": "F Societal Impacts and Safeguards",
            "content": "Our method, CREPA, offers several positive societal impacts. By reducing data and compute requirements for fine-tuning video diffusion models, it makes high-quality generative tools more accessible to smaller labs, educators, and creators. This can enhance creativity, personalization, and educational applications. However, it also poses risks, including potential misuse for deepfakes, bias amplification from pretrained models, and disruption in creative job markets. These concerns can be mitigated through watermarking and content traceability, ethical usage guidelines, bias audits, and human-in-the-loop deployment to ensure responsible and socially beneficial use."
        }
    ],
    "affiliations": [
        "KAIST AI"
    ]
}