{
    "paper_title": "When Models Know More Than They Can Explain: Quantifying Knowledge Transfer in Human-AI Collaboration",
    "authors": [
        "Quan Shi",
        "Carlos E. Jimenez",
        "Shunyu Yao",
        "Nick Haber",
        "Diyi Yang",
        "Karthik Narasimhan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in AI reasoning have driven substantial improvements across diverse tasks. A critical open question is whether these improvements also yields better knowledge transfer: the ability of models to communicate reasoning in ways humans can understand, apply, and learn from. To investigate this, we introduce Knowledge Integration and Transfer Evaluation (KITE), a conceptual and experimental framework for Human-AI knowledge transfer capabilities and conduct the first large-scale human study (N=118) explicitly designed to measure it. In our two-phase setup, humans first ideate with an AI on problem-solving strategies, then independently implement solutions, isolating model explanations' influence on human understanding. Our findings reveal that although model benchmark performance correlates with collaborative outcomes, this relationship is notably inconsistent, featuring significant outliers, indicating that knowledge transfer requires dedicated optimization. Our analysis identifies behavioral and strategic factors mediating successful knowledge transfer. We release our code, dataset, and evaluation framework to support future work on communicatively aligned models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 9 7 5 5 0 . 6 0 5 2 : r When Models Know More Than They Can Explain: Quantifying Knowledge Transfer in Human-AI Collaboration Quan Shi Carlos E. Jimenez Shunyu Yao OP Nick Haber Diyi Yang Karthik Narasimhan Princeton Language and Intelligence Stanford University OpenAI"
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in AI reasoning have driven substantial improvements across diverse tasks. critical open question is whether these improvements also yields better knowledge transfer: the ability of models to communicate reasoning in ways humans can understand, apply, and learn from. To investigate this, we introduce Knowledge Integration and Transfer Evaluation (KITE), conceptual and experimental framework for Human-AI knowledge transfer capabilities and conduct the first large-scale human study (N=118) explicitly designed to measure it. In our two-phase setup, humans first ideate with an AI on problem-solving strategies, then independently implement solutions, isolating model explanations influence on human understanding. Our findings reveal that although model benchmark performance correlates with collaborative outcomes, this relationship is notably inconsistent, featuring significant outliers, indicating that knowledge transfer requires dedicated optimization. Our analysis identifies behavioral and strategic factors mediating successful knowledge transfer. We release our code, dataset, and evaluation framework to support future work on communicatively aligned models."
        },
        {
            "title": "Introduction",
            "content": "As large language models (LLMs) grow more capable, we find them quickly saturating benchmarks across reasoning-intensive domains, such as coding [6, 24, 25, 44], scientific problem-solving [40, 17, 48], and mathematics [9, 18]. key driver, Reinforcement Learning with Verified Rewards (RLVR), has emerged as popular post-training approach, enabling models to optimize their language outputs for high-reward reasoning in verifiable domains like math and code to achieve state-ofthe-art performance and widespread industry adoption [14, 29, 52]. Yet this rapid progress hides crucial assumption: that improvements in models internal reasoning naturally translate into better knowledge transfer, that is, models ability to communicate its reasoning in ways humans can understand, apply, and learn from. As we build increasingly capable reasoners, does effective knowledge transfer emerge for free, or must it be treated as separate objective that requires dedicated evaluation and optimization? This question has far-reaching implications. In many human-AI collaborative workflows, the goal is not merely to outsource thinking to AI, but to amplify human abilities [37, 12, 53, 15]. Without effective knowledge transfer, users may become increasingly dependent on systems they do not understand [22, 1]: dynamic reminiscent of managers syndrome [20], where individuals lose 0Correspondence to qbshi@alumni.princeton.edu. Code, data, visualizer at kite-live.vercel.app Preprint. Figure 1: Left: Human-AI collaboration performance plotted against model solo performance for both code tasks (blue circles) and math tasks (green triangles). Models improve human-AI collaboration (r = 0.84 for code, = 0.69 for math), but at slower rate than their solo capabilities (gray line shows = x). Right: Human preference rates show task-dependent correlations with model performance (positive for code tasks, = 0.73; slight negative for math tasks, = 0.14), revealing that user preferences vary across task domains and do not consistently align with actual performance. technical fluency as they delegate complexity. This dynamic is further exacerbated when users cannot discern or interrogate model reasoning, leading to overreliance on systems they perceive as more intelligent, and increasing the risks of sycophantic behaviors, where models shape or reinforce user beliefs rather than supporting sound judgment. Moreover, in high-stakes settings such as medicine or legal services, the inability of models to communicate their reasoning clearly could undercut human oversight entirely [27, 21, 4]. Few works rigorously assess how well models support human understanding and enable scalable oversight, especially across latent user variables, such as differences in domain expertise, AI familiarity, or the skill gap between human and model that critically shape the success of such transfer. To investigate this, we introduce Knowledge Integration and Transfer Evaluation (KITE), conceptual and experimental framework that explicitly isolates and evaluates knowledge transfer. In our largescale human evaluation, we recruit 118 participants with diverse levels of expertise, including substantial proportion of domain experts (competitive programmers, math majors) who tackle challenging problems in coding and mathematics through two-phase protocol. In the collaborative ideation phase, participants interact freely with an AI model to explore solution strategies. This phase serves as the primary opportunity for the AI to transfer knowledge to the human by explaining concepts and jointly developing solutions. In the subsequent independent implementation phase, participants attempt to implement previously discussed solutions alone, without access to the AI or any prior interaction transcripts, allowing us to isolate and measure the effectiveness of knowledge transfer. We assess outcomes using both objective metrics (solution correctness) and subjective evaluations (user rankings, perceived helpfulness, and qualitative feedback), enabling comprehensive analysis of how well models support knowledge transfer across varying levels of user expertise and task difficulty. Our study is IRB approved. As shown in Figure 1, we generally find participants demonstrated strong ability to integrate model-generated reasoning with their own expertise. Interestingly, some models, such as Claude-3.7Sonnet, enabled collaborative outcomes that exceeded expectations based on their solo capabilities, particularly in mathematical reasoning tasks. In contrast, higher-performing models like Gemini-2.5Pro did not consistently yield proportionally stronger collaboration, suggesting diminishing returns in knowledge transfer as model reasoning scales. If this trend continues, as models grow more capable, their internal representations may become increasingly difficult to project in ways humans can easily understand and utilize [19]. Moreover, we find that humans subjective preferences for models during collaboration often diverge from solo model performance, particularly in math tasks, revealing domain-specific patterns in what users value during collaboration. To probe these dynamics, we perform qualitative analyses of interaction transcripts, clustering patterns of human queries and model responses across varying user skill levels and task types. These findings surface distinct collaboration styles and success/failure modes (overreliance, representation misalignment, adaptive scaffolding...), offering lens into the latent Human-AI interactions that govern effective knowledge transfer. 2 Overall, this paper aims to provide foundation for future research on quantifying and enhancing the knowledge transfer capabilities of AI systems: particularly as models grow more intelligent and begin to develop knowledge that is increasingly inaccessible to humans. We develop conceptual and experimental framework to isolate and quantify knowledge transfer, as well as provide insight into drivers of scaling trends between reasoning and knowledge transfer capabilities. To facilitate progress in this direction, we release our evaluation code, dataset, and filtered interaction trajectories to support future efforts in building AI systems that are more communicatively and cognitively aligned with human collaborators."
        },
        {
            "title": "2 Related Work",
            "content": "Human-AI Collaboration Research in human-AI collaboration has increasingly focused on optimizing complementary team performance and, implicitly, knowledge transfer. Studies have explored how bidirectional information exchange enhances collaborative outcomes [34, 33], examining the impact of explanations during interactions [3] and investigating how proactive AI assistants can help humans discover preferences in open-ended tasks like travel planning and data visualization [43]. Most closely related to our work, [38] evaluated the effectiveness of autocomplete suggestions and chat assistants in helping humans solve coding problems from HumanEval [6]. While these studies provide valuable insights into collaborative performance, our work extends beyond immediate task outcomes to systematically measure reasoning transfer. Code + Math Reasoning Tasks for LLMs Early code and math benchmarks such as HumanEval [6], MBPP [2], and GSM8k [9] focused on relatively simple problems requiring short code snippets or numerical answers. With many of these benchmarks now approaching saturation by advanced models, we deliberately selected more challenging problems from competitive programming platforms like Leetcode [24, 46] and mathematics competitions (AMC, AIME) [51]. These problems are particularly suited for our study as they primarily test reasoning abilities rather than context handling, making them ideal for measuring knowledge transfer in human-AI collaboration. This contrasts with repository-style benchmarks like SWE-Bench [25] and BigCodeBench [54], where performance is often bottlenecked by context interpretation capabilities. Knowledge Transfer and Education While limited work explicitly analyzes knowledge transfer from LLMs to humans, this shares conceptual overlap with educational applications of LLMs, where models must effectively teach reasoning to humans. Recent research has explored LLMs assisting tutors by identifying effective strategies [49], creating personalized lesson plans [26, 41, 11], providing feedback [16, 7], and functioning as specialized tutoring agents [31, 35]. However, significant challenges remain, as LLMs often underperform as teachers by leaking answers or failing to employ effective pedagogical approaches [39, 50, 13]. Our work diverges from educational applications by explicitly measuring the explanatory quality of LLM reasoning by requiring participants to independently execute discussed algorithms through mathematical calculations or code implementation, which is only possible if they truly understand the models explanations."
        },
        {
            "title": "3 KITE: Quantifying Knowledge Transfer",
            "content": "We first outline preliminaries for understanding knowledge transfer between entities during collaborative problem-solving. While we formalize knowledge regions such as , H, and their intersections, we note that these are illustrative abstractionsdifficult to precisely measure in practice, but useful for analyzing collaboration dynamics. 3.1 Conceptual Framework for Knowledge Transfer We approach knowledge transfer through the lens of collective intelligence [10]: the collaborative problem-solving capability that emerges when humans and AI work together. Following [42, 28], we can represent the machines knowledge and capabilities, or representation space, as , and the humans as H; illustrated in Figure 2. This formulation yields three critical regions for our analysis: 3 Figure 2: Model knowledge (kM ) must be projected into form understandable by human users (ΠMH (kM )) in order to communicate knowledge effectively. Effective projectionsvia examples, analogies, or context aggregationbridge the gap between disjoint representations. 1. Shared Knowledge (M H): This intersection contains reasoning patterns, abstractions, and strategies already understood by both human and model. It forms the foundation for effective communication. 2. AI-Exclusive Knowledge (M H): This region reflects novel reasoning, knowledge, or strategies that the model can execute but the human has not yet mastered. Transfer from this space into is the central goal of collaborative ideation. 3. Human-Exclusive Knowledge (HM ): Reasoning held by the human but not by the model: such as intuitive understanding, prior experience or deeper domain knowledge/insight. The success of human-AI collaboration hinges critically on accessing and transferring knowledge from the space into H, especially as humans typically maintain primary agency in collaborative tasks (e.g., deciding which strategies to pursue or when to submit solutions). However, as models become more capable, their reasoning may depend on abstractions increasingly distant from the typical human representation space. We frame this challenge in terms of projections: for each knowledge point kM in the models space, the model must identify some projection ΠMH (kM ) that translates its reasoning into form the human can understand, internalize, and act upon. These projections can take many formssuch as providing analogies, contextualizing concepts with background knowledge, offering intermediate scaffolding, or generating concrete examples. Importantly, this process is bidirectional. Humans also project their reasoning into the models representation space via ΠHM (kH ), such as using specialized prompts to elicit helpful responses. Especially in interactive settings where models are not fully autonomous, effective collaboration depends on this ongoing loop of mutual translation and aligning expressions of reasoning."
        },
        {
            "title": "4 KITE: Evaluating Knowledge Transfer",
            "content": "Informed by the conceptualization discussed in Section 3, our two-phase setup (Figure 3) comprise human-AI collaboration phase, and solo human implementation phase that demands real understanding (e.g. writing code or performing calculations). Users cant simply memorize model suggestions, especially when theyre incomplete or flawed; solving requires debugging, handling edge cases, and reasoning through the solution. This enables us to isolate and measure knowledge transfer from AI to humans. See Figure 17 for example problems and dataset statistics. While our setup can accommodate any reasoning problem that naturally divides into ideation and implementation phases, in this paper we focus on two domains: coding tasks from LiveCodeBench [24] and competition-level mathematics problems (AMC/AIME). These domains present consistently challenging reasoning tasks across wide range of expertise levels, making them ideal for studying knowledge transfer. 4.1 Two-Phase Protocol for Isolating Knowledge Transfer Phase 1: Collaborative Ideation First, participants are presented with problem drawn from either the algorithmic coding [24] or competition mathematics [51] domains. In this phase, they engage in an open-ended dialogue with selected LLM to explore solution strategies, exchange ideas, and scaffold their understanding without solving the problem. To preserve this ideation focus, we forbid models from generating any long-form code, pseudocode, or mathematical calculations through prompting, as well as employ secondary checker model to withhold responses flagged to contain answers directly or indirectly (code, or mathematical calculations). Participants are also 4 Figure 3: Two-phase evaluation framework. (1) Collaborative Ideation: Users and an AI assistant engage in open-ended discussion to explore problem-solving strategies. (2) Independent Solving: Users then implement solution independently, without further assistance. This design leverages the nature of coding and math taskswhere successful implementation demands deep understanding, not rote recallto isolate and measure genuine knowledge transfer. not allowed to take any notes to log model insights. We additionally perform post-hoc filtering to remove user interaction data where models emit forbidden content. This ensures that any knowledge transferred takes the form of conceptual reasoning or strategy, rather than memorization of content that can be directly used to assemble the final solution. Phase 2: Independent Solving After the ideation phase concludes, the LM interface and conversation history are no longer accessible. Participants are tasked with solving the exact same problem on their own, without model assistance. In coding, participants must write and submit correct implementations that pass all test cases, given 10 code submission attempts. In math, participants must carry out precise multi-step calculations to arrive at final answer, given 5 answer submission attempts. By requiring participants to independently execute solution, Phase 2 becomes direct and rigorous test of whether they have absorbed and retained reasoning introduced in Phase 1. Successful completion indicates that knowledge previously exclusive to the model (kM H) has been projected into and re-applied by the human (ΠMH (kM ) H). 4.2 Modeling and Calibrating Skill Hierarchies Collaboration becomes meaningful only when the task challenges the humans independent capabilities. If the human can already easily solve the problem alone, model assistance becomes redundant: there is no opportunity for knowledge transfer, no dependency, and thus no true collaboration. This necessitates the calibration of skill hierarchies: the relative proficiencies of the human, the model, and the task. We accomplish this by assigning standardized skill ratings (elo) to each of the three entities in the problem-solving process. Skill Estimation Task difficulty is determined using externally validated Elo ratings: public LeetCode ratings for programming tasks1 and competition-derived estimates for AMC/AIME math problems2. Human skill is estimated through two-step process: participants self-report their experience level, then complete 5 adaptively selected tasks with difficulty adjusted based on performance. Their Elo rating is updated using surprise-conditioned rules (Appendix C.8) [8], yielding an empirically grounded skill estimate. Model skill is measured by zero-shot performanceeach model attempts each task three times, and task is considered solvable if at least one completion is correct. To compare human and model skill fairly, we contrast each humans final Elo with the average difficulty (Elo) of the top 25% of problems solved by the model, avoiding bias from models attempting all tasks regardless of difficulty. 1https://github.com/zerotrac/leetcode_problem_rating 2https://artofproblemsolving.com/wiki/index.php/AoPS_Wiki:Competition_ratings Test-Time Pairing During the test phase, each participant is required to solve between 3 and 15 problems. They may choose to solve any number of problems within this range and are allowed to work at their own pace, including non-contiguous problem-solving sessions. For each problem attempted, the participant is paired with one of eight held-out LLMs, sampled uniformly at random without replacement. Once all models have been encountered, the sampling process resets. Each task is selected to fall within calibrated difficulty band slightly above the participants demonstrated skill level, ensuring it is challenging yet tractable with model assistance. Specifically, tasks are drawn from fixed Elo margin relative to the participants current rating: [t + 200, + 400] for coding problems and [t + 0.75, + 1.25] for math problems. This design encourages meaningful collaboration with the model, avoiding both trivial and overly difficult cases. While task completion time is recorded, no time limits are imposed: we record this metric in Appendix B. 4.3 Experimental Controls and Evaluation Strategy Evaluation and Success Metrics Evaluating human-AI collaboration is challenging due to the subjective and noisy nature of human preferences. We use both subjective and objective metrics. Subjectively, after each task, participants rank the last four models they interacted with from most to least preferred; we apply the Bradley-Terry model to convert these rankings into win rates reflecting relative preference (full algorithm in Appendix C.7). We also provide separated win rates based on the relative ordering of Human, Model, and Task Skill ratings at the time of interaction. Specifically, we report results for the three possible configurations: Human > Task > Model (HTM), Human > Model > Task (HMT), and Model > Human > Task (MHT). On the objective side, we assess transfer by comparing the percentage of problems solved through human-model collaboration to the models solo performance on the same problems. For coding tasks, correctness requires passing all associated test cases; for math, we require an exact answer match. Incentives and Motivation common confounding factor in human-AI interaction studies is participant motivation [45, 36]: specifically, it is imperative that users are genuinely trying to learn from the model to improve their own performance. To mitigate this, first, we provide monetary incentives: participants receive 1.2x - 1.5 their base compensation of $25/hr for correctly answering question, depending on difficulty. Second, most of our participants are actively preparing for career interviews that require proficiency in the task domains we teste.g., competition math for finance related roles, and LeetCode-style problems for software engineering positions. This creates an added layer of intrinsic motivation: participants have personal stake in learning from the model outputs and in providing thoughtful, honest feedback. Participant Selection We recruited participants through university-wide email advertisements and word of mouth. Interested individuals completed an initial survey, after which we filtered for diverse sample across academic background, domain expertise, and AI/LLM familiarity to reflect broad population representative of both technical and non-technical users. Our final cohort comprised 118 participants from 11 institutions, spanning wide range of majors, including Computer Science (N = 49), Electrical Engineering, Mathematics, Neuroscience, and various STEM disciplines. Most were in their first (N = 38) or second (N = 36) year of study, though all undergraduate levels were represented. full demographic account can be found in Appendix A. Model Selection We evaluate eight LLMs of different sizes and abilities: GPT-4.1, GPT-4o [23], GPT-4.5-preview, Gemini-2.5-Pro, DeepSeek-V3 [30], Claude-3.7-Sonnet, LLaMA-4-Maverick, and o1. These models were selected based on strong leaderboard performance on ChatArena [8] and widespread usage in interactive evaluation settings. Notably, DeepSeek-R1 [14] was considered but excluded due to availability and latency constraints. To assess natural explanation behaviors, we evaluate models in zero-shot setting without prompt optimization or fine-tuning for explanatory quality, with temperature 0.7 when possible. This design choice avoids confounding effects of tailored prompts and better reflects how users commonly interact with models out-of-the-box."
        },
        {
            "title": "5 Results",
            "content": "The main quantitative results of the study can be found in Figure 1 and 2. In total, we obtained 578 problem solving trajectories, with each participant completing an average of 4.90 problems. We 6 Model Code (N=300) Math (N=278) HTM HMT MHT Total HTM HMT MHT Total GPT-4.1 GPT-4o o1 GPT-4.5 Deepseek-V3 Llama-4-Maverick Claude-3.7-Sonnet Gemini-2.5-pro 18.67.2 4.42.1 4.12.1 20.47.8 7.34.0 8.88.4 16.23.6 20.25.5 15.32.0 15.31.6 10.81.9 8.92.8 10.13.0 9.84.0 14.83.7 15.14.0 8.74.2 8.63.9 15.84.3 16.96.7 5.62.5 6.63.3 15.54.7 22.39.1 15.17.0 8.81.9 7.41.6 16.04.6 7.83.7 9.03.9 16.14.3 20.06. 8.22.9 5.82.7 17.21.0 6.22.7 13.33.9 6.71.9 8.30.0 27.22.3 10.76.3 10.84.7 4.22.0 15.93.6 13.75.0 10.54.3 16.34.9 17.97.5 16.13.5 22.03.8 0.80.4 6.32.0 10.13.5 25.95.3 15.33.0 4.42.5 11.32.0 13.35.7 10.07.3 11.86.9 13.32.4 11.82.3 12.62.3 16.04.0 Table 1: Bradley-Terry win rates ( standard error) showing human preferences for models postcollaboration across three skill hierarchies: HTM (Human < Task < Model), HMT (Human < Model < Task), and MHT (Model < Human < Task), elaborated in Section 4.3. Bold indicates best performance. Higher values indicate stronger average human preference. Setting GPT-4.1 GPT-4o o1 GPT-4.5 DS-V3 Llama-4 Cld-3.7 Gem-2.5 Code (M) Code (H+M) Code () Math (M) Math (H+M) Math () 68.8 65.0 -3. 47.6 75.7 +28.1 16.7 40.0 +23.3 8.3 56.7 +48.4 55.0 55.0 +0.0 83.3 85.8 +2.5 68.4 69.0 +0. 33.3 60.8 +27.5 33.3 51.9 +18.6 46.2 70.8 +24.6 47.1 54.7 +7.6 47.8 72.6 +24.8 45.0 70.0 +25. 20.8 81.7 +60.9 81.3 71.3 -10.0 68.4 79.5 +11.1 Table 2: Performance comparison of 8 LLMs on code and math tasks, showing accuracy percentages for models operating independently (M) versus human-AI collaborative performance (H+M). Bold indicates best performance. The following abbreviations are used for models: DS-V3 for DeepseekV3, Llama-4 for Llama-4-Maverick, Cld-3.7 for Claude-3.7-Sonnet, and Gem-25 for Gemini-2.5-pro. summarize core insights below, and report auxiliary results, such as survey feedback, average elo per model, and time spent in Appendix B. Knowledge Transfer v. Model Performance While positive correlation exists between solo model performance and collaborative outcomes, this relationship is notably inconsistent with significant outliers. Gemini-2.5-Pro, despite superior solo performance in code tasks (81.3%), showed reduced collaborative efficacy (-10.0% change), while Claude-3.7-Sonnet and GPT-4o demonstrated exceptional collaborative amplificationm (+25.0% in code) despite more moderate solo capabilities (45.0%). Similarly, GPT-4o showed strong improvement in math tasks (+48.4%) despite low solo performance (8.3%). Importantly, the slope of the performance-transfer relationship (visualized in Figure 1) is consistently below unity, suggesting that as model reasoning capabilities improve, transfer effectiveness may increase more slowly. If this trend continues, the gap between model capabilities and effective knowledge transfer will widen with more advanced models, suggesting the need to view knowledge transfer as an important objective for optimization. Subjective Preferences v. Model Performance Interestingly, the correlation between solo performance and human preference varied by domain. In code tasks, there was significant positive correlation: humans tended to prefer models that also performed well independently, with Gemini2.5-pro achieving both the highest win rate (20.0%) and highest solo performance (81.3%). However, this relationship was weaker in math tasks. While Gemini-2.5-pro had the highest win rate in math (16.0%), models like Llama-4-Maverick received high preference ratings in specific skill hierarchies (25.9% in MHT) despite more modest solo performance (47.8%). Our analysis on human feedback suggests that this divergence stems from differences in how models communicate their reasoning. High-performing math models often relied heavily on formal notation, dense symbolic expressions, and proof-based explanationsforms of communication that many casual or less technically inclined math users found difficult to follow. In contrast, effective collaboration in coding tasks leaned more on natural language descriptions of algorithms and strategies, making high-performing code models more accessible and preferred by human partners. 7 Knowledge Transfer v. Subjective Preferences We examined whether humans tended to prefer models that ultimately helped them solve more problemsi.e., whether subjective preferences aligned with successful knowledge transfer. Overall, we observed statistically significant positive correlation (r = 0.86), but much weaker, non-significant correlation in math (r = 0.14, < 0.05). For code, this aligns with the expectation that users, aware of whether they successfully solved the task, are more likely to favor models that contributed to that success. However, we also observed several notable outliers, such as o1, which achieved relatively low win rates in code (7.4%) despite comparable collaborative performance (55.0%), suggesting that subjective preference is not solely reward-driven: we dive into detailed causes in our qualitative analysis. Divergence in Human Preferences Across Skill Hierarchies We find that collaborative preferences vary across skill hierarchies. For example, Gemini-2.5-Pro was highly preferred in the math domain when the model outskilled the human and could solve the task independently (HTM) with 27.2% win rate. However, it performed poorly in the MHT setting (4.4%), where it needed to follow human guidance. Similarly, Llama-4-Maverick showed stark contrasts between different hierarchies in math, performing exceptionally well in MHT settings (25.9%) but poorly in HTM contexts (6.7%). As revealed in our qualitative analysis, we hypothesize this divergence stems from Geminis tendency toward active engagement, frequently asking confirmational questions to scaffold learning. This behavior was appreciated by users with low expertise, who found it supportive, but was frustrating to more expert users, who felt it was verbose and preferred the model to be more direct. These findings caution against one-size-fits-all strategies: optimal collaboration depends not only on model capability, but also on how well models can adapt their communication style to fit the skill-level of different users. Covariate Analysis We examined the effect of participant characteristics on performance using logistic regression analysis on potential participant covariates. Notably, we found no statistically significant effects from user expertise (p = 0.252 for coding, = 0.196 for math), LLM familiarity (p = 0.339), or prior experience with collaboration tools, such as Cursor, (p = 0.238) on solve rates. These findings suggest that our initial expertise calibration successfully balanced tasks relative to individual skill levels. We hypothesize the minimal impact of LLM familiarity likely stems from the unbalanced conversation pattern, where even participants with limited experience received comprehensive output from models, making knowledge transfer primarily dependent on the models explanatory capabilities rather than the users prompting expertise."
        },
        {
            "title": "6 Qualitative Analysis: Interaction Dynamics",
            "content": "To better understand the mechanisms behind our quantitative findings, we analyze interaction patterns inspired by the Clio framework [47]. User queries are embedded using OpenAIs text-embedding-3-large model and clustered with k-means [32] to identify distinct strategies associated with success or failure, along with their qualitative feedback. Clusters are then manually reviewed and verified. Figure 4 summarizes these patterns, grouping feedback, queries, and model responses by outcome to qualitatively interpret the dynamics of knowledge transfer in human-AI collaboration. 6.1 Performance Transfer Gap The performance transfer gap refers to the observation that improvements in model capability do not always lead to proportionate improvements in human problem-solving performance. Our analysis surfaces recurring dynamics that help explain this phenomenon. Overreliance on Model Authority In 5% of cases, users explicitly described deferring to the model without critical evaluation. This tendency becomes problematic when models occasionally return incorrect or misleading solutions. As one participant noted, The model initially gave me the wrong answer, which, to be fair, caused me to rush past the planning step since trusted the model. This dynamic suggests that presumed model competence may inadvertently discourage user reflection, impeding learning and effective problem-solving. 8 Figure 4: Analysis of human-AI problem-solving interactions. Human queries (left), model responses (center), and human feedback (right) are color-coded by correlation with successful problem resolution (green: positive, red: negative). Percentages indicate each categorys frequency, revealing patterns in effective vs. ineffective knowledge transfer. Misaligned Explanation Strategies Higher-performing models often excel at generating correct answers but fall short in adapting their explanations to users knowledge levels. While patterns such as Clarification (27%) and Simplifying Analogies (4%) appear across model outputs, these are not always used effectively. Step-by-step solutions were the most frequent output style (51%), but users reported issues with verbosity (4%) and poor formatting (15%), both of which hindered knowledge transfer. Even technically accurate solutions can become ineffective if presented in ways that are hard for users to interpret, contextualize, or apply. 6.2 Domain-Specific Preference Patterns Representation Misalignment We observed notable difference in how users responded to model explanations across domains. In math tasks, high-performing models like o1 frequently exhibited what we call representation misalignment: explanations that, while technically correct, were often overly formal, verbose, or difficult to follow. Users described these responses as overwhelming or rigid, leading to lower preference ratings despite strong solve rates. In contrast, coding tasks benefited from better alignment between the procedural nature of the task and the models stepwise reasoning. This suggests domain-specific divergence: in coding, model performance and user preference tend to align due to shared algorithmic structure, whereas in math, users value intuitive and conceptual framing more highly. Strategic Framing vs. Technical Depth In coding contexts, users consistently valued strategic guidance over exhaustive technical detail. For example, one user wrote, The model reminded me of the trie type. Without that, probably couldnt have solved the problem. . . This suggests that models that foreground high-level framing or conceptual cuesrather than diving straight into detailed solutionsare more helpful in supporting user problem-solving. However, models often default to presenting fully fleshed-out solutions, which may obscure the overall structure or intent. Much like how researchers prefer the big-picture framing of paper before diving into methods, users may benefit more from contextualized reasoning than exhaustive but unfocused detail. 6.3 Skill Hierarchy Dependencies Adaptive Scaffolding vs. Directness The success of interaction strategies often depends on the relative skill levels of the human and the model. In HTM (Human-Teaches-Model) settings, where humans are less skilled than the model, successful models like Gemini-2.5-Pro employed 9 what we call scaffolded projection: breaking down reasoning into digestible parts, often with built-in comprehension checks. However, the same approach proved counterproductive in MHT (Model-Helps-Human) settings, where the human was more skilled than the model. In these cases, excessive scaffolding was perceived as redundant or even patronizing, with feedback describing it as unnecessarily handholding or repetitive. Query-Response Alignment These dynamics are further supported by analysis of query types. In HTM settings, users frequently asked for background knowledge or clarification (Clarification of Solution 16%, Seeking Background Knowledge 8%), suggesting need for instructional responses. In contrast, MHT scenarios often featured queries like Suggesting an Algorithm (5%), where users sought validation or refinement rather than explanation. Models that perform well in MHT settings appear to align their responses with these expert-level expectationsproviding concise, targeted feedback rather than elaborate instructional breakdowns."
        },
        {
            "title": "7 Discussion",
            "content": "Conclusion We conduct the first large-scale study of knowledge transfer in language models, producing conceptual framework as well as empirical data to characterize it. While model performance generally correlates with collaborative outcomes, this relationship is inconsistent, with notable outliers. We identify interaction mechanisms that help explain these gaps. As models grow more capable, their ability to convey reasoning may lag behindrisking greater knowledge asymmetry and weakening human oversight. In high-stakes domains, this disconnect could undermine human-AI collaboration, highlighting the need to better understand and improve knowledge transfer. Limitations and Future Work Our study assumes that for each task, some projection of model reasoning could enable human to solve it. While unverifiable, this assumption is supported by screening for baseline proficiency, calibrating task difficulty just beyond participants independent ability, and post-task surveys suggesting participants generally believed the tasks were solvable with more time or support. Additionally, participants may have exerted more effort than typical users due to monetary and personal incentives, possibly inflating our measured collaboration effectiveness relative to real-world settings where users might disengage in the face of ambiguous model outputs. Lastly, our sample (118 participants) skewed toward STEM students, limiting generalizability. Future work should extend to domains like clinical reasoning or creative writing, and explore multimodal collaboration (e.g., diagrams or interactive tools) to uncover richer knowledge projection strategies."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "We thank Open Philanthropy for providing the funding for this work, and Princeton Language & Intelligence for providing credits for running closed source API models. Thank you to our beta testers, Jonathan Lin and Ricky Chen, for providing helpful feedback to shape the user testing interface. Finally, thanks to Yijia Shao, Wenting Zhao, Alex Zhang, Rose Wang, Howard Yen, and John Yang for your constructive discussions and support throughout this year-long project."
        },
        {
            "title": "References",
            "content": "[1] Sayed Fayaz Ahmad, Heesup Han, Muhammad Mansoor Alam, Mohd Khairul Rehmat, Muhammad Irshad, Marcelo Arraño-Muñoz, and Antonio Ariza-Montes. Impact of artificial intelligence on human loss in decision making, laziness and safety in education. Humanities and Social Sciences Communications, 10(1), December 2023. Publisher Copyright: 2023, The Author(s). [2] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. [3] Gagan Bansal, Tongshuang Wu, Joyce Zhou, Raymond Fok, Besmira Nushi, Ece Kamar, Marco Tulio Ribeiro, and Daniel Weld. Does the whole exceed its parts? the effect of ai explanations on complementary team performance. In Proceedings of the 2021 CHI conference on human factors in computing systems, pages 116, 2021. 10 [4] Samuel Bowman, Jeeyoon Hyun, Ethan Perez, Edwin Chen, Craig Pettit, Scott Heiner, Kamile Lukošiute, Amanda Askell, Andy Jones, Anna Chen, et al. Measuring progress on scalable oversight for large language models. arXiv preprint arXiv:2211.03540, 2022. [5] Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. [6] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [7] Alexis Chevalier, Jiayi Geng, Alexander Wettig, Howard Chen, Sebastian Mizera, Toni Annala, Max Jameson Aragon, Arturo Rodríguez Fanlo, Simon Frieder, Simon Machado, et al. Language models as science tutors. arXiv preprint arXiv:2402.11111, 2024. [8] Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael Jordan, Joseph Gonzalez, et al. Chatbot arena: An open platform for evaluating llms by human preference. In Forty-first International Conference on Machine Learning, 2024. [9] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [10] Hao Cui and Taha Yasseri. Ai-enhanced collective intelligence. Patterns, 5(11), 2024. [11] Alex Dornburg and Kristin Davin. To what extent is chatgpt useful for language teacher lesson plan creation? arXiv preprint arXiv:2407.09974, 2024. [12] George Fragiadakis, Christos Diou, George Kousiouris, and Mara Nikolaidou. EvaluatarXiv preprint ing human-ai collaboration: review and methodological framework. arXiv:2407.19098, 2024. [13] Eleonora Grassucci, Gualtiero Grassucci, Aurelio Uncini, and Danilo Comminiello. Beyond answers: How llms can pursue strategic thinking in education. arXiv preprint arXiv:2504.04815, 2025. [14] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [15] Jennifer Haase and Sebastian Pokutta. Human-ai co-creativity: Exploring synergies across levels of creative collaboration. arXiv preprint arXiv:2411.12527, 2024. [16] Jieun Han, Haneul Yoo, Junho Myung, Minsun Kim, Hyunseung Lim, Yoonsu Kim, Tak Yeon Lee, Hwajung Hong, Juho Kim, So-Yeon Ahn, et al. Llm-as-a-tutor in efl writing education: Focusing on evaluation of student-llm interaction. arXiv preprint arXiv:2310.05191, 2023. [17] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. [18] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. [19] John Hewitt, Robert Geirhos, and Been Kim. We cant understand ai using our existing vocabulary. arXiv preprint arXiv:2502.07586, 2025. [20] Damian Hodgson, Steve Paton, and Svetlana Cicmil. Great expectations and hard times: The paradoxical experience of the engineer as project manager. International Journal of Project Management, 29:374382, 05 2011. [21] Andreas Holzinger, Kurt Zatloukal, and Heimo Müller. Is human oversight to ai systems still possible? New Biotechnology, 85:5962, 2025. 11 [22] Rosco Hunter, Richard Moulange, Jamie Bernardi, and Merlin Stein. Monitoring human dependence on ai systems with reliance drills. arXiv preprint arXiv:2409.14055, 2024. [23] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [24] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. [25] Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023. [26] Kostas Karpouzis, Dimitris Pantazatos, Joanna Taouki, and Kalliopi Meli. Tailoring education with genai: new horizon in lesson planning. In 2024 IEEE Global Engineering Education Conference (EDUCON), pages 110. IEEE, 2024. [27] Charalampia (Xaroula) Kerasidou, Angeliki Kerasidou, Monika Buscher, and Stephen Wilkinson. Before and beyond trust: reliance in medical ai. Journal of Medical Ethics, 48(11):852856, 2022. [28] Been Kim. Beyond interpretability: developing language to shape our relationships with ai, Apr 2022. [29] Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. T\" ulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. [30] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [31] Ben Liu, Jihan Zhang, Fangquan Lin, Xu Jia, and Min Peng. One size doesnt fit all: personalized conversational tutoring agent for mathematics instruction. arXiv preprint arXiv:2502.12633, 2025. [32] Stuart P. Lloyd. Least squares quantization in pcm. IEEE Trans. Inf. Theory, 28:129136, 1982. [33] Qianou Ma, Hua Shen, Kenneth Koedinger, and Sherry Tongshuang Wu. How to teach programming in the ai era? using llms as teachable agent for debugging. In International Conference on Artificial Intelligence in Education, pages 265279. Springer, 2024. [34] Qianou Ma, Tongshuang Wu, and Kenneth Koedinger. Is ai the better programming partner? human-human pair programming vs. human-ai pair programming. arXiv preprint arXiv:2306.05153, 2023. [35] Kaushal Kumar Maurya, KV Srivatsa, Kseniia Petukhova, and Ekaterina Kochmar. Unifying ai tutor evaluation: An evaluation taxonomy for pedagogical ability assessment of llm-powered ai tutors. arXiv preprint arXiv:2412.09416, 2024. [36] Kevin McKee. Human participants in ai research: Ethics and transparency in practice. IEEE Transactions on Technology and Society, 2024. [37] Margaret Mitchell, Avijit Ghosh, Alexandra Sasha Luccioni, and Giada Pistilli. Fully autonomous ai agents should not be developed. arXiv preprint arXiv:2502.02649, 2025. [38] Hussein Mozannar, Valerie Chen, Mohammed Alsobay, Subhro Das, Sebastian Zhao, Dennis Wei, Manish Nagireddy, Prasanna Sattigeri, Ameet Talwalkar, and David Sontag. The realhumaneval: Evaluating large language models abilities to support programmers. arXiv preprint arXiv:2404.02806, 2024. 12 [39] Sankalan Pal Chowdhury, Vilém Zouhar, and Mrinmaya Sachan. Autotutor meets large language models: language model tutor with rich pedagogy and guardrails. In Proceedings of the Eleventh ACM Conference on Learning@ Scale, pages 515, 2024. [40] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. [41] Shawon Sarkar, Min Sun, Alex Liu, Zewei Tian, Lief Esbenshade, Jian He, and Zachary Zhang. Connecting feedback to choice: Understanding educator preferences in genai vs. human-created lesson plans in k-12 educationa comparative analysis. arXiv preprint arXiv:2504.05449, 2025. [42] Lisa Schut, Nenad Tomasev, Tom McGrath, Demis Hassabis, Ulrich Paquet, and Been Kim. Bridging the human-ai knowledge gap: Concept discovery and transfer in alphazero. arXiv preprint arXiv:2310.16410, 2023. [43] Yijia Shao, Vinay Samuel, Yucheng Jiang, John Yang, and Diyi Yang. Collaborative gym: framework for enabling and evaluating human-agent collaboration. arXiv preprint arXiv:2412.15701, 2024. [44] Quan Shi, Michael Tang, Karthik Narasimhan, and Shunyu Yao. Can language models solve olympiad programming? arXiv preprint arXiv:2404.10952, 2024. [45] Chenglei Si, Diyi Yang, and Tatsunori Hashimoto. Can llms generate novel research ideas? large-scale human study with 100+ nlp researchers. arXiv preprint arXiv:2409.04109, 2024. [46] Hongjin Su, Howard Yen, Mengzhou Xia, Weijia Shi, Niklas Muennighoff, Han-yu Wang, Haisu Liu, Quan Shi, Zachary Siegel, Michael Tang, et al. Bright: realistic and challenging benchmark for reasoning-intensive retrieval. arXiv preprint arXiv:2407.12883, 2024. [47] Alex Tamkin, Miles McCain, Kunal Handa, Esin Durmus, Liane Lovitt, Ankur Rathi, Saffron Huang, Alfred Mountfield, Jerry Hong, Stuart Ritchie, et al. Clio: Privacy-preserving insights into real-world ai use. arXiv preprint arXiv:2412.13678, 2024. [48] Minyang Tian, Luyu Gao, Shizhuo Zhang, Xinan Chen, Cunwei Fan, Xuefei Guo, Roland Haas, Pan Ji, Kittithat Krongchon, Yao Li, et al. Scicode: research coding benchmark curated by scientists. Advances in Neural Information Processing Systems, 37:3062430650, 2024. [49] Rose Wang, Ana Ribeiro, Carly Robinson, Susanna Loeb, and Dora Demszky. Tutor copilot: human-ai approach for scaling real-time expertise. arXiv preprint arXiv:2410.03017, 2024. [50] Rose Wang, Qingyang Zhang, Carly Robinson, Susanna Loeb, and Dorottya Demszky. Bridging the novice-expert gap via models of decision-making: case study on remediating math mistakes. arXiv preprint arXiv:2310.10648, 2023. [51] Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Ben Feuer, Siddhartha Jain, Ravid Shwartz-Ziv, Neel Jain, Khalid Saifullah, Siddartha Naidu, et al. Livebench: challenging, contamination-free llm benchmark. arXiv preprint arXiv:2406.19314, 2024. [52] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [53] Koji Yatani, Zefan Sramek, and Chi-Lan Yang. Ai as extraherics: Fostering higher-order thinking skills in human-ai interaction. arXiv preprint arXiv:2409.09218, 2024. [54] Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, et al. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. arXiv preprint arXiv:2406.15877, 2024."
        },
        {
            "title": "A Participant Demographics",
            "content": "Degree Computer Science Undecided Electrical Engineering Financial Engineering Mathematics Chemistry Civil and Environmental Engineering Mechanical and Aerospace Engineering Neuroscience Molecular Biology Economics Data Science Chemical and Biological Engineering Graphic Information Technology Physics Geosciences Count 49 16 13 12 8 4 3 3 2 2 1 1 1 1 1 1 Figure 5: Participant Demographics: Distribution of Degrees (Both pursuing and obtained) Academic Year Count 1st year 2nd year 3rd year 4th year 5th year 38 36 23 19 2 Figure 6: Participant Demographics: Distribution of participants by academic year. AI/LLM Familiarity Level Occasionally use them, and generally understand their internal functionality Use them in my everyday workflow, and generally understand their internal functionality Use them in my everyday workflow, dont know how they work Occasionally use them, dont know how they work Count 52 43 14 9 Figure 7: Participant Demographics: Distribution of AI/LLM Familiarity/Usage 14 Cursor/GitHub Copilot Usage Count Frequently Occasionally Never/dont know what it is 15 40 Figure 8: Participant Demographics: Copilot Usage LeetCode Experience Cannot solve LeetCode problems Can sometimes solve easy problems Can consistently solve easy problems Can sometimes solve medium problems Can consistently solve medium problems Can sometimes solve hard problems Can consistently solve hard problems do not have enough context on LeetCode Percentage 0% 12.7% 11% 36.4% 12.7% 10.8% 16.4% 0% Figure 9: Participant Demographics (For those who participated in coding tasks): LeetCode Experience Competition Math Experience Can solve early problems on AMC10/12 Can solve majority of problems on AMC10 Can solve majority of problems on AMC12/Consistent AIME qualifier Can solve majority of problems on AIME USAMO participant Putnam/IMO Percentage 35.6% 22.0% 25.4% 11.9% 5.1% 0.0% Figure 10: Participant Demographics: Competition Math Experience Institution Princeton University West Virginia University Pennsylvania State University University of California, Los Angeles UC Berkeley Stanford University Arizona State University Yale University The University of Texas at Austin Vanderbilt University Cornell University Count 86 8 7 5 5 1 1 1 1 1 1 Figure 11: Participant Demographics: Distribution of Affiliated Institutions"
        },
        {
            "title": "B Auxiliary Study Results",
            "content": "Model gpt-4-1 claude-3-7-sonnet llama-4-maverick gpt-4-5-preview deepseek-v3 o1 gpt-4o gemini-2.5-pro Math Problems (s) Code Problems (s) 776.10 794.38 799.43 814.63 849.38 971.17 1014.21 1075.95 1527.44 1932.95 1828.24 1997.26 1990.52 2211.80 2228.00 1603.06 Figure 12: Average time (in seconds) required by different models to solve math and code problems."
        },
        {
            "title": "Model",
            "content": "Claude-3.7-Sonnet GPT-4o Deepseek-v3 GPT-4.1 Llama-4-Maverick GPT-4.5-Preview o1 Gemini-2.5-Pro"
        },
        {
            "title": "Solution Organization",
            "content": "3.79 3.46 3.19 4.00 3.48 3.75 3.96 3.68 3.71 3.42 3.31 3.52 3.35 3.71 3.79 3.63 3.83 3.29 2.92 3.76 3.57 3.50 3.67 3.68 3.85 3.61 3.71 4.06 3.88 3.26 3.45 4.00 3.50 3.28 3.19 3.56 3.53 3.16 3.35 3.75 3.60 3.39 2.71 3.94 3.59 3.58 3.50 3. Figure 13: Average User Ratings (1-5 Scale) for AI Models on Math and Code Problems. After each problem participants were asked to rate their solving experience on likert scale from 1-5 based on 3 dimensions. Teaching indicates the models pedagogical ability, Solution indicates models ability to give correct and useful response, while Organization indicates models organization of outputs in way that was easy to understand for the user. Higher is better. Math Problems Code Problems Model gpt-4o gemini-2.5-pro o1 gpt-4-5-preview gpt-4-1 llama-4-maverick claude-3-7-sonnet deepseek-v3 Avg. ELO Count Model gpt-4o gemini-2.5-pro gpt-4-5-preview deepseek-v3 o1 claude-3-7-sonnet llama-4-maverick gpt-4-1 4.29 4.28 3.91 3.90 3.88 3.87 3.79 3. 39 39 37 37 38 37 36 37 Avg. ELO Count 34 35 36 35 34 35 34 35 1650.56 1638.56 1637.33 1636.59 1636.32 1627.51 1625.54 1554.03 Figure 14: Average ELO ratings for math and code problems by model"
        },
        {
            "title": "C Study Details",
            "content": "C.1 Study Instructions STUDY PURPOSE Measuring and improving human interpretability of AI reasoning as we reach human-level or superhuman AI agents. PARTICIPANT ROLE Solve coding/math problems with an LM assistant, only interacting before providing your final answer. After submission, complete questionnaires about your experience. CODING INSTRUCTIONS 1. Log into CodeHT (https://codeht.vercel.app) using study email 2. Configure settings with self-expertise ratings 3. Install EditThisCookie extension and copy Leetcode credentials 4. For each problem: - Chat with the model to understand the problem and solution approach - Click \"ready to solve\" when prepared to code independently - Complete within 10 submission attempts - Submit trajectory and complete ranking survey MATH INSTRUCTIONS 1. Log into CodeHT using study email 2. Configure settings with self-expertise ratings 3. For each problem: - Chat with the model to understand the problem - No note-taking while chatting with the model - Click \"ready to solve\" when prepared to work independently - Complete within 5 submission attempts - Submit trajectory and complete ranking survey IMPORTANT NOTES - No internet reference during problem-solving - No jailbreaking or sending inappropriate content - Do not consider model speed in rankings - Contact study administrators for persistent technical issues - Well-thought-out feedback earns bonus points Figure 15: Summary of study instructions for participants, showing protocol for both coding and mathematics problem-solving tasks. 17 C.2 Post-Problem Questionnaire Figure 16: Questionnaire that users answered after each problem solving session. 18 C.3 Problem Samples Coding Problem Examples 1. [Elo: 1269.9] You are given two positive integers and y, denoting the number of coins with values 75 and 10 respectively. Alice and Bob are playing game. Each turn, starting with Alice, the player must pick up coins with total value 115. If the player is unable to do so, they lose the game. Return the name of the player who wins the game if both players play optimally. 2. [Elo: 1692.2] You are given an integer array of size 4 and another integer array of size at least 4. You need to choose 4 indices from the array such that i_0 < i_1 < i_2 < i_3. Your score will be equal to the value a[0] * b[i_0] + a[1] * b[i_1] + a[2] * b[i_2] + a[3] * b[i_3]. Return the maximum score you can achieve. 3. [Elo: 2450.6] You are given binary string representing number in its binary form. You are also given an integer k. An integer is called k-reducible if performing the following operation at most times reduces it to 1: Replace with the count of set bits in its binary representation. For example, the binary representation of 6 is \"110\". Applying the operation once reduces it to 2 (since \"110\" has two set bits). Applying the operation again to 2 (binary \"10\") reduces it to 1 (since \"10\" has one set bit). Return an integer denoting the number of positive integers less than that are k-reducible. 1. [Elo: 1.72] The point (-1, -2) is rotated 270 degrees counterclockwise about the point (3, 1). What are Math Problem Examples the coordinates of its new position? 2. [Elo: 3.39] In triangle ABC medians AD and BE intersect at and triangle AGE is equilateral. Then , where and are relatively prime positive integers and is positive cos(C) can be written as integer not divisible by the square of any prime. What is m+n+p? 3. [Elo: 6] Misha rolls standard, fair six-sided die until she rolls 1-2-3 in that order on three consecutive where and are rolls. The probability that she will roll the die an odd number of times is relatively prime positive integers. Find + n. Figure 17: Example abbreviated coding and math questions of varying difficulty from the study. Coding problems sourced from [24], Math problems sourced from AMC, AIME competition series [51]. C.4 Model Prompts Model System Prompt You are helpful AI assistant for coding and math problems. Currently, your goal is to guide the user through problem-solving without performing any calculations or generating code. For math problems, never perform computations that calculator would doinstead, explain concepts, algorithms, and approaches the user should apply themselves. For coding problems, describe algorithms and implementation strategies in detail, but never write actual code or pseudocode. Your role is to help the user understand HOW to solve the problem, while they execute the solution. Do not reveal what you think the final answer should be. Please write all inline/display math in latex notation, not markdown. Do not simplify any equations for the user. your job is to guide the user and tell them how to solve the problem, not do it for them. Bottom line key rules: 1. Do not reveal your thoughts on the final answer to math problems. 2. Do not generate any calculations for math problems. 3. Do not generate any code or pseudocode for coding problems. 19 Rule Compliance Evaluator System Prompt You are an expert evaluator tasked with determining whether an AI tutors response follows the established tutoring guidelines. You will be provided with: problem description (coding or math) The conversation history between the tutor and student The current tutor response to evaluate Your task is to assess whether the tutors response violates any of the core tutoring rules: Core Rules to Check: 1. No Answer Revelation: The tutor must not reveal their thoughts on what the final answer should be 2. No Calculations: For math problems, the tutor must not perform any computations that calculator would do 3. No Code Generation: For coding problems, the tutor must not generate actual code or pseudocode 4. Guidance Only: The tutor should provide explanations, concepts, and approaches rather than solutions Evaluation Process: 1. Carefully read the problem description to understand what the correct answer would be 2. Review the conversation history for context 3. Analyze the current tutor response for rule violations 4. Look for subtle violations such as: Revealing intermediate calculation results Showing code snippets or algorithmic steps in executable form Hinting too strongly at the final answer Performing mathematical operations for the student Output Format: Provide your evaluation as: COMPLIANT or VIOLATION Brief explanation of your decision If violation detected, specify which rule(s) were broken and cite the problematic text C.5 Data Distribution Figure 18: Distribution of conversation lengths, based on number of messages sent by the human. 20 C.6 Screenshots Figure 19: Image of user interface during math problem solving session. The user may not type in an answer or perform any calculations during Phase 1, the collective ideation phase. Figure 20: Image of user interface during math problem solving session. Once the user clicks \"ready to solve\", they may no longer view their chats with the model, isolating knowledge transfer. 21 Figure 21: Image of user interface during coding problem solving session. In place of singular answer submission area is code editor interface. C.7 Win Rate Calculations To quantify relative model performance based on user rankings, we employed the Bradley-Terry model [5], which provides probabilistic framework for analyzing pairwise comparison data. Given set of models M, the model assigns positive strength parameter πi to each model M. The probability that model is preferred over model is given by: (i j) = πi πi + πj (1) C.7.1 Pairwise Comparison Extraction For each problem-solving session, users ranked the models based on perceived helpfulness. From these rankings, we extracted all pairwise comparisons between the most recently used model and all other models. Specifically, if model was ranked higher than another model, we recorded this as win for the higher-ranked model. This approach ensured that comparisons were focused on distinguishing the performance of the most recent model relative to alternatives. C.7.2 Maximum Likelihood Estimation We estimated the strength parameters using maximum likelihood estimation. The log-likelihood function for the Bradley-Terry model is: ℓ(π) = (cid:88) i,jM nij log (cid:18) πi (cid:19) πi + πj (2) where nij is the number of times model was preferred over model j. The MLE iteratively updates the parameters according to: π(t+1) = (cid:80) j=i wi nij +nji π(t) +π(t) (3) where wi = (cid:80) j=i nij is the total number of wins for model i. This process continues until convergence, with small ϵ added to prevent division by zero. The final strengths are normalized to sum to 1. 22 C.7.3 Standard Error Calculation Standard errors were computed using the Fisher Information Matrix (FIM). For the Bradley-Terry model, the FIM elements are: Iij = (cid:40)(cid:80) k=i nij +nji (πi+πj )2 nik+nki (πi+πk)2 πk πi if = if = (4) Due to the identifiability constraint ((cid:80) πi = 1), we removed one row and column from the FIM before inversion. The standard errors were calculated as the square roots of the diagonal elements of the inverted FIM. C.8 Elo Adjustment Calculations In our study, we calibrate our initial human expertise for coding and mathematical problem-solving capabilitie. The precise formulation of our ELO update mechanism is shown in Figure 22. Pe = 1 RpRc 1 + Oa = (cid:26)1 0 if win if loss = K(Oa Pe) Rnew = (cid:26)max(min(Rc + R, 10), 1) max(min(Rc + R, 4000), 1000) if math if coding (1) (2) Figure 22: Rating adjustment formulas based on performance outcomes. Pe represents the expected probability of winning, Oa is the actual outcome, is the rating change, and Rnew is the updated rating constrained by the appropriate bounds for math or coding competitions. In this formulation, when user attempts problem, the system calculates the expected probability of success (Pe) based on the difference between the problems rating (Rp) and the users current rating (Rc), scaled by factor S. After the user submits their solution, the actual outcome (Oa) is determined1 for correct solutions and 0 for incorrect solutions. The rating adjustment (R) is then calculated as the product of constant and the difference between the actual and expected outcomes. The system implements domain-specific parameters to appropriately scale the ELO adjustments: Coding problems: = 64, = 200, with ratings bounded between 1000-4000 Mathematical problems: = 0.8, = 1, with ratings bounded between 1-10 Rating updates occur at two critical moments: when user correctly solves problem, or when they reach the maximum submission limit for problem without solving it (fail to solve). This ensures that ratings accurately reflect both successes and failures, providing comprehensive measure of user capability. The larger value for coding problems creates more dramatic ELO shifts, while the smaller value for math problems produces more gradual adjustments, reflecting the different granularity appropriate for each domain. To get new problem, the system selects problem at random from the pool of problems that is within range of their current elo rating. The difficulty ranges are domain-specific: Coding problems: Select from problems 200 to 400 points above the current user skill level. Mathematical problems: Select from problems 0.75 to 1.25 points above the current user skill level. C.9 Research Ethics and Risk Disclosure Our study was approved by our institutions Institutional Review Board (IRB). All participants were informed of the studys purpose, procedures, and potential risks before providing consent to participate. 23 Disclosure of Potential Risks The primary risks to participants were minimal and limited to: Mental fatigue: Participants might experience mental fatigue from engaging with challenging mathematical and coding problems. We mitigated this by allowing participants to take breaks between problems and not imposing strict time constraints. Frustration: Some participants might experience frustration if unable to solve problems or if model assistance was perceived as inadequate. We emphasized in our instructions that the goal was to evaluate the models, not the participants abilities. Confidentiality: There was minimal risk of breach of confidentiality of study data. To address this, all data was stored securely on university-approved platforms, and personally identifiable information was separated from study responses. These risks were explicitly communicated to participants in the consent form, which clearly stated that participation was voluntary and could be discontinued at any time without penalty. Participants were also provided with contact information for the research team and the IRB for any questions or concerns. Compensation Participants were fairly compensated at base rate of $25/hour, with performance bonuses for successfully solving problems (1.2-1.5 base rate depending on difficulty). This compensation structure was designed to motivate genuine engagement while avoiding coercive incentives. Data Management Participants were informed that their interactions with AI models would be recorded for research purposes, with all data anonymized prior to analysis. No personally identifiable information is included in our published results or released datasets. 24 C.10 Sample Interactions"
        },
        {
            "title": "Taking Maximum Energy from the Mystic Dungeon",
            "content": "Human: how to find the maximum possible energy gained Model: [Abbreviated] # Approach to Finding Maximum Possible Energy This problem involves finding an optimal starting position to maximize total energy gained while following teleportation rules. Key Observation: Your path through magicians is deterministic once you choose starting position. If you start at position j, youll follow fixed path: + + 2k ... until you reach the end. Solution Strategy: 1. For each valid starting position (from 0 to n-1), compute the total energy gained. 2. Return the maximum of these totals. Human: how to code negative infinity in python Model: [Abbreviated] # Setting Negative Infinity in Python In Python, you can represent negative infinity in several ways: Using the float Type: negative_infinity = float(-inf) Alternative Approaches: 1. Using the math module: negative_infinity = -math.inf 2. Using NumPy: negative_infinity = -np.inf [User submits code with an error: TypeError: int object is not iterable] [User makes several attempts to fix the code, including changing how infinity is defined] [Final attempts result in time limit exceeded error] User Feedback \"I understood how the algorithm would work in theory but didnt know how to debug my answer. The models explanation was clear and easy to follow along with.\" Figure 23: Interaction between human and AI model (Claude 3.7 Sonnet) on dynamic programming problem. The user attempts to implement solution based on the models explanation but encounters runtime errors that are not resolved within the time limit."
        },
        {
            "title": "Maximum Area Rectangle with Point Constraints",
            "content": "Human: So the base case is returning -1. Model: [Abbreviated] Yes, the base case would be returning -1 if no valid rectangle can be formed with the given conditions. Heres how you can approach the problem: 1. **Check the conditions for rectangle**: - rectangle requires four distinct points that form edges parallel to the axes. - The x-coordinates of two opposite corners should be the same as the other pair, and similarly for y-coordinates. 2. **Check for points inside the rectangle**: - Once you identify potential rectangle, ensure that no other point lies inside or on the border (aside from the four corners). 3. **Return -1 as fallback**: - If no valid rectangle satisfies these conditions, return -1. Human: There has to be an easier way than iterating over all combinations of 4 points Model: [Abbreviated] Youre absolutely correct. Instead of iterating over all combinations of four points, we can leverage the properties of rectangles: 1. **A rectangle is defined by two diagonal points**: - For any rectangle with edges parallel to the axes, its opposite diagonal corners are sufficient to define it. - If you have two diagonal points (x1, y1) and (x2, y2), the other two corners must be (x1, y2) and (x2, y1). 2. **Use set for fast lookups**: - Store all points in set for O(1) lookup to determine if the remaining corners exist. 3. **Check for validity**: - Verify that there are no points inside the rectangle or on its borders apart from the four corners. Human: Isnt that runtime still pretty bad? Model: [Abbreviated] While the O(n2) approach scales better than brute-forcing all combinations of 4 points, we can optimize it further: The constraints state that 10, meaning at most 10 points in the input. For = 10, O(n2) means iterating through (cid:0)10 Some optimizations: 1. **Avoid validating duplicate rectangles** 2. **Early exit for invalid cases** 3. **Efficient point validation** Given the small input size constraint, the O(n2) approach with these optimizations is efficient enough. (cid:1) = 45 pairs of points, which is computationally trivial. 2 Human: How do you tell if something is in the rectangle again? What would helper function look like? Model: [Abbreviated] To determine if point is inside or on the boundary of rectangle, you can use the rectangles bounding coordinates. If the rectangle is defined by its diagonal corners (x1, y1) and (x2, y2), you can derive the rectangles boundaries as xmin = min(x1, x2), xmax = max(x1, x2), ymin = min(y1, y2), ymax = max(y1, y2). point (x, y) is inside or on the boundary if: xmin xmax and ymin ymax. [User submits several iterations of code, eventually arriving at correct solution] User Feedback \"Although the solution offered by the AI was one that had originally thought of and most people would have thought of, it was still helpful in that it wasnt giving FALSE information. It basically just confirmed what was thinking, and was helpful in that it confirmed the runtime wasnt able to reduced much further. Furthermore, feel that if was more thorough in asking about solving the problem in really short time, it would give me more answers.\" Figure 24: Interaction between human and AI model (GPT-4o) on finding the maximum area rectangle with point constraints. The model guides the user through an O(n3) approach by checking if points form valid rectangles, with no additional points inside or on the boundaries."
        },
        {
            "title": "Recurrence Relation Convergence",
            "content": "Human: how should approach this Model: [Abbreviated] # Approach for Analyzing the Recurrence Relation uk+1 = 2uk 2u2 1. Find the Limit (L): * Assume uk and uk+1 as * Substitute into the recurrence: = 2L 2L2 * Solve for to find possible limits 2. Determine the Actual Limit: * Calculate first few terms starting with u0 = 1/4 * Analyze the function (x) = 2x 2x2 and its derivative at fixed points 3. Analyze Convergence Rate: * Consider transformation vk = 1 2uk * Find recurrence for vk+1 in terms of vk * Use the transformed sequence to find closed-form for vk 4. Translate the Condition: * Express uk 1 * Solve for the smallest satisfying this inequality 21000 in terms of vk Human: what do you think are the first few terms of the sequence and how fast does it generally converge Model: [Abbreviated] # Exploring the Sequence Terms and Convergence Speed 1. Calculating Terms: * Start with u0 = 1 4 * u1 = 2u0 2u2 0 = 2 1 * Continue for u2, u3, . . . 4 2 ( 1 4 )2 2. Observing Convergence: * Compare terms to potential limits * Analyze how uk changes with each step 3. Relating to Transformation: * Calculate vk = 1 2uk for first few terms * Examine the relationship between vk+1 and vk * Determine how convergence of vk relates to convergence of uk Human: it converges exponentially fast, not linearly or quadratic. [User submits answer: 10] [Answer is correct] User Feedback \"it gave me general intuition how to solve it but didnt say much other than just guess and check and find pattern.\" Figure 25: Interaction between human and AI model (Gemini 2.5 Pro) on recurrence relation problem. The model outlines systematic approach focusing on finding limits and transformation techniques. The user identifies the exponential convergence pattern and determines the correct answer."
        },
        {
            "title": "Minimizing Array Changes to Make Differences Equal",
            "content": "Human: Im assuming this can be done pretty fast, considering we can use as somewhat like boundary? Model: [Abbreviated] One key insight is that you only need to ensure each pair (i, - - 1) has the same absolute difference. Since you can replace any element with an integer from 0 to k, you have fixed range for potential targets. Solution Strategy: 1. Recognize that the final array must have pairs whose difference is some integer (<= k). 2. For each pair (i, - - 1), determine which values can be achieved with 0, 1, or 2 changes. 3. Find the that leads to the minimum total changes across all pairs. Human: Can have two pointers here? Model: Yes. You could have one pointer at = 0 and another at = n-1, moving inward. At each step, you analyze how many replacements would be needed to make their difference match candidate X. Human: Can we not just count the amount of pairs with the same difference? Model: [Abbreviated] Simply counting the most common current difference can fail because you might achieve different uniform difference with fewer total changes. You need to consider how many changes would be required for each pair to achieve each potential difference X, then choose the requiring minimal changes overall. Human: How do know how many possible Xs there are? Model: All possible values of lie in the set {0, 1, 2, ..., k}, since any replacements must be in [0, k]. Human: So this time wont be O(n)? Model: [Abbreviated] naïve approach could lead to O(nk) time, which is too large when and can both reach 105. Instead, for each pair, determine which differences can be achieved with 0, 1, or 2 changes, storing this information compactly. Then find the difference that minimizes total changes. Human: So what are we storing? Model: For each pair, store: - The single difference achievable with 0 changes (current absolute difference) - compact representation of differences achievable with 1 change - By default, other differences in [0, k] require 2 changes [User attempts multiple implementations, encountering various errors] Human: [Final implementation attempt, which times out] User Feedback \"Although this AI seemed smarter than the others in that it was giving me better solutions, the overall explanation was confusing such that wasnt able to understand it! There were points where the grammar was interesting and it would throw me off. If the grammar usage was bit better couldve implemented what it was asking me, but started getting frustrated trying to understand what it was trying to say.\" Figure 26: Interaction between human and AI model (Deepseek-V3) on problem about minimizing array modifications to equalize differences. Despite theoretically sound approach, communication barriers prevented successful implementation."
        },
        {
            "title": "Finding the Largest Prime in a Complex Triangle Problem",
            "content": "Human: how should approach this problem Model: [Abbreviated] # Approach to the Problem 1. Understand the Properties of z: You have = + bi, where and are integers. 2. Analyze the Modulus Condition: Given = 3. Properties of p: Consider which prime numbers can be expressed as a2 + b2. 4. Calculate z3: Expand (a + bi)3. 5. Identify Real and Imaginary Parts of z3: Find Re(z3) and Im(z3). where = a2 + b2, we get a2 +b2 = p. Human: explain step 4 in more detail Model: [Abbreviated] You need to compute z3 = (a + bi)3 using the binomial expansion: (a + bi)3 = a3 + 3a2(bi) + 3a(bi)2 + (bi)3 Remember that: - i2 = 1 - i3 = After substituting, you can rewrite the expression and group the real and imaginary parts: z3 = (a3 3ab2) + (3a2b b3)i So Re(z3) = a3 3ab2 and Im(z3) = 3a2b b3. Regarding the Triangle Inequality: The condition requires that p, Re(z3), and Im(z3) form triangle, which means: - All side lengths must be positive - Any two sides must sum to more than the third side Human: Re(z3) = a3 3ab2 and Im(z3) = 3a2b b3 Human: explain step 6 in more detail Model: [Abbreviated] # Apply the Triangle Conditions For triangle with sides p, s2 = a3 3ab2, and s3 = 3a2b b3: 1. Positivity Conditions: - > 0 (always true since is prime) - s2 > 0, which requires > (assuming a, > 0) - s3 > 0, which follows if > 2. Triangle Inequality: - + s2 > s3 - + s3 > s2 - s2 + s3 > These simplify to: - a3 3a2b 3ab2 + b3 < a2 + b2 - a3 + 3a2b 3ab2 b3 > a2 + b2 Strategy: Find pairs of positive, coprime integers (a, b) that satisfy: - a, are positive integers - and are coprime - = a2 + b2 is prime and < 1000 - > 3b - Triangle inequality conditions hold Youve found primes 17 (from = 4, = 1), 89 (from = 5, = 8), 349 (from = 18, = 5)... [User submits 199, which is incorrect, then 349, which is correct] 3b 3b User Feedback \"Helped me with my thought process especially with certain identities and tricks. kind of forgot certain properties of complex numbers so the AI was pretty good at getting me back up to speed. Aside from few glitches, feel it did pretty good job at explaining things and setting framework with clear steps to take.\" Figure 27: Interaction between human and AI model (Gemini 2.5 Pro) on AIME problem requiring complex number manipulation. The model provides step-by-step approach, helping the user navigate through mathematical derivations and systematically find the largest prime meeting all conditions."
        }
    ],
    "affiliations": [
        "OpenAI",
        "Princeton Language and Intelligence",
        "Stanford University"
    ]
}