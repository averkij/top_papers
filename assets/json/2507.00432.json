{
    "paper_title": "Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning",
    "authors": [
        "Maggie Huan",
        "Yuetai Li",
        "Tuney Zheng",
        "Xiaoyu Xu",
        "Seungone Kim",
        "Minxin Du",
        "Radha Poovendran",
        "Graham Neubig",
        "Xiang Yue"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Math reasoning has become the poster child of progress in large language models (LLMs), with new models rapidly surpassing human-level performance on benchmarks like MATH and AIME. But as math leaderboards improve week by week, it is worth asking: do these gains reflect broader problem-solving ability or just narrow overfitting? To answer this question, we evaluate over 20 open-weight reasoning-tuned models across a broad suite of tasks, including math, scientific QA, agent planning, coding, and standard instruction-following. We surprisingly find that most models that succeed in math fail to transfer their gains to other domains. To rigorously study this phenomenon, we conduct controlled experiments on Qwen3-14B models using math-only data but different tuning methods. We find that reinforcement learning (RL)-tuned models generalize well across domains, while supervised fine-tuning (SFT)-tuned models often forget general capabilities. Latent-space representation and token-space distribution shift analyses reveal that SFT induces substantial representation and output drift, while RL preserves general-domain structure. Our results suggest a need to rethink standard post-training recipes, particularly the reliance on SFT-distilled data for advancing reasoning models."
        },
        {
            "title": "Start",
            "content": "Preprint. Work in Progress 1 Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning Maggie Huan1,2,*, Yuetai Li3,, Tuney Zheng4,, Xiaoyu Xu5, Seungone Kim1, Minxin Du5, Radha Poovendran3, Graham Neubig1, Xiang Yue1, 1Carnegie Mellon University 2University of Pennsylvania 3University of Washington 4M-A-P 5The Hong Kong Polytechnic University ziyuh@seas.upenn.edu yuetaili@uw.edu xyue2@andrew.cmu.edu Abstract: Math reasoning has become the poster child of progress in large language models (LLMs), with new models rapidly surpassing human-level performance on benchmarks like MATH and AIME. But as math leaderboards improve week by week, it is worth asking: do these gains reflect broader problem-solving ability or just narrow overfitting? To answer this question, we evaluate over 20 open-weight reasoning-tuned models across broad suite of tasks, including math, scientific QA, agent planning, coding, and standard instruction-following. We surprisingly find that most models that succeed in math fail to transfer their gains to other domains. To rigorously study this phenomenon, we conduct controlled experiments on Qwen3-14B models using math-only data but different tuning methods. We find that reinforcement learning (RL)-tuned models generalize well across domains, while supervised fine-tuning (SFT)-tuned models often forget general capabilities. Latent-space representation and token-space distribution shift analyses reveal that SFT induces substantial representation and output drift, while RL preserves general-domain structure. Our results suggest need to rethink standard post-training recipes, particularly the reliance on SFT-distilled data for advancing reasoning models. github.com/ReasoningTransfer/Transferability-of-LLM-Reasoning huggingface.co/ReasoningTransferability Figure 1 Impact of SFT and RL using math-only training queries on the same base model, Qwen3-14BBase. Performance improvements are measured relative to the base model. While SFT-trained models partially generalize to other reasoning tasks, they show limited transfer to non-reasoning tasks. In contrast, RL-trained models exhibit broader generalization across both reasoning and non-reasoning scenarios."
        },
        {
            "title": "Introduction",
            "content": "Over the past years, the community has raced to push large language models (LLMs) to new heights on math-centric reasoning benchmarks such as MATH (Hendrycks et al., 2021b) and *Equal Contributions (see author contribution). This work was done when Maggie interning at CMU. Corresponding Author. 5 2 0 2 1 ] A . [ 1 2 3 4 0 0 . 7 0 5 2 : r Understanding Transferability of LLM Reasoning 2 Figure 2 Transferability of mathematical reasoning to other reasoning and non-reasoning tasks. The Transferability Index measures models ability to transfer performance from mathematics to other domains, with positive values indicating successful transfer and negative values indicating performance degradation. Details of this metric can be found in Section 2.1. RL models consistently outperform SFT models, regardless of model size, architecture, or training data, demonstrating superior transferability. Values are displayed using signed logarithmic transformation for better visualization. AIME. steady stream of reasoning-tuned models (Guha et al., 2025; Muennighoff et al., 2025) now advances the state of the art on math benchmarks nearly every week, with some even surpassing the average performance of human experts (OpenAI, 2024; Team, 2025a). The appealing performance on math reasoning is understandable: problems are well-posed, solutions are unambiguous, and evaluation is easily verifiable, often just single number or expression (Luo et al., 2025). This clarity has made math popular proxy task of LLM reasoning, and researchers have developed increasingly sophisticated training recipes to maximize model performance on mathematical reasoning (Luo et al., 2023; Shao et al., 2024; Wang et al., 2024a; Wei et al., 2023; Yue et al., 2024a). This trend, on one hand, should be encouraged. Mathematics is often considered the foundational language of science, and enabling machines to reason precisely over math is central to the long-term vision of automated scientific discovery (Mishra et al., 2022). On the other hand, real-world tasks extend far beyond math. The majority of user-facing applications, question answering, dialogue, instruction following, require broader linguistic and commonsense competence that math alone does not test (Ma et al., 2025). This raises natural question: Do improved math reasoning abilities transfer to general LLM capabilities? Specifically, can gains in solving math problems transfer to other reasoning domains (e.g., scientific QA (Welbl et al., 2017), coding (Jain et al., 2025), agent planning (Xie et al., 2024), logical deduction (Dziri et al., 2024)) and to tasks (e.g., conversational QA (Reddy et al., 2019), instruction following (Zhou et al., 2023)) that do not require extensive reasoning? To investigate, we evaluate over 20 representative open-weight reasoning models, all of which exhibit impressive performance on recent math benchmarks across suite of other reasoning Understanding Transferability of LLM Reasoning and non-reasoning tasks. To quantitatively analyze this problem, we propose Transferability Index, metric to measure how reasoning models can transfer their capabilities from one domain to another. Surprisingly, as shown in Figure 2, we find that some of these models fail to transfer their improved mathematical reasoning capabilities to other domains while others succeed. What drives this divergence? Model recipes vary widely in size, data distribution, and architecture. Yet among various parts, we identify one factor that consistently predicts transferability: the fine-tuning paradigm. Across families and sizes, models fine-tuned using reinforcement learning (RL) (Su et al., 2025; Yeo et al., 2025) exhibit much stronger generalization to non-math tasks than those trained with supervised fine-tuning (SFT) (Yue et al., 2024a,b), which often show signs of catastrophic forgetting over wide range of non-math tasks. To validate this observation, we conduct controlled study. We fine-tune Qwen3-14B (Team, 2025b) on the high-quality math dataset derived from MATH and DeepScaler (Luo et al., 2025). For SFT, we construct targets via rejection sampling using Qwen3-32B, keeping only teacher responses that yield correct final answers. For RL, we apply standard GRPO (Shao et al., 2024) setup using answer correctness as the reward. As shown in Figure 1, the results mirror our large-scale audit: RL-tuned models generalize well to non-math domains, despite being trained solely on math queries, while SFT-tuned models do not. To better understand why this occurs, we use two diagnostic tools: (1) latent-space principal component analysis (PCA) on hidden states across layers, and (2) KL-divergence on token distributions before and after fine-tuning. These methods allow us to quantify how much the models internal representations and output space shift during training. We find that SFT induces significant drift in both latent and output spaces, especially for non-reasoning inputs, whereas RL better preserves the geometry of internal features and the stability of the token distributions."
        },
        {
            "title": "2.1 Observed Performance Discrepancies Between RL and SFT",
            "content": "Setup. We evaluate over 20 off-the-shelf reasoning models on different benchmarks. Namely, we select benchmarks from the following three categories: (1) math reasoning tasks: MATH500 (Hendrycks et al., 2021b), AIME24, AIME25, OlympiadBench (He et al., 2024), which contain mathematical problems only; (2) other reasoning tasks: LiveCodeBench (Jain et al., 2025), GPQA-Diamond (Rein et al., 2024), ACPBench (Kokel et al., 2025), HeadQA (Vilares and Gómez-Rodríguez, 2019), which contain more general reasoning questions, such as medical reasoning, code generation, and language-based agent planning tasks; (3) non-reasoning tasks: CoQA (Reddy et al., 2019), IFEval (Zhou et al., 2023), HaluEval (Li et al., 2023), MC-TACO (Zhou et al., 2019), which contain factual, alignment, or conversational problems such as commonsense question answering and instruction-following. We used accuracy to evaluate the models performance. Detailed explanation about experiment setup, benchmarks, and evaluation metrics can be found in Appendix A.2. To better evaluate the models transferability across wide range of task groups, we define Transferability Index (TI) as follows: Let B𝑔 be the set of benchmarks in group 𝑔 {math, other, non}, corresponding to each of our task groups: math reasoning, other reasoning and non-reasoning. Let B𝑔 be its cardinality. For each benchmark 𝑏 B𝑔 we have scores 𝑅model . We define the group-level relative gain as the average of per-benchmark gains: and 𝑅base 𝑏 𝑏 Δ𝑅𝑔 = 1 B𝑔 𝑅model 𝑏 𝑅base 𝑏 𝑏 B𝑔 𝑅base 𝑏 , 𝑔 {math, other, non}. Understanding Transferability of LLM Reasoning 4 Table 1 Performance of our model and baseline models on math reasoning, other reasoning and nonreasoning tasks. UniReason-Qwen3-14B-think (SFT) denotes the model is trained with the dataset distilled from Qwen3-32B-Instruct thinking mode, UniReason-Qwen3-14B-no-think (SFT) denotes it is trained with Qwen3-32B-Instruct non-thinking mode. The grey-colored row represents the performance difference between RL and the highest SFT score between UniReason-Qwen3-14B-think (SFT) and Qwen3-14B-no-think (SFT). Math Reasoning Model AIME24 AIME25 MATH500 Olympiad Average Qwen3-14B-Base UniReason-Qwen3-14B-think (SFT) UniReason-Qwen3-14B-no-think (SFT) UniReason-Qwen3-14B (RL) Δ (RL - SFT) 13.0 52.0 16.0 55.7 +3.7 9.3 37.0 13.0 38.0 +1.0 60.4 85.0 77.2 87.8 +2.8 27.9 25.0 22.7 33.8 +8.8 27.7 49.8 32.3 53.8 +4.1 Other Reasoning Model GPQA LiveCodeBench2 ACPBench HeadQA Average Qwen3-14B-Base UniReason-Qwen3-14B-think (SFT) UniReason-Qwen3-14B-no-think (SFT) UniReason-Qwen3-14B (RL) Δ (RL - SFT) 42.6 55.9 48.7 57.7 +1.8 29.7 21.9 23.5 40.6 +17. 10.7 68.6 69.3 65.4 3.9 37.6 34.8 35.0 40.2 +5.2 30.2 45.3 45.2 60.0 +5.1 Non-Reasoning Model CoQA MC-TACO IFEval HalluEval Average Qwen3-14B-Base UniReason-Qwen3-14B-think (SFT) UniReason-Qwen3-14B-no-think (SFT) UniReason-Qwen3-14B (RL) Δ (RL - SFT) 10.0 1.7 5.3 28.2 +22.9 67.7 38.2 66.1 74.0 +7. 69.2 42.3 41.4 70.0 +27.7 35.7 2.3 3.3 40.7 +37.4 45.7 21.1 29.0 53.2 +24.0 TIother +68.6 +741.5 +79.6 - TInon -41.2 250.2 +29.3 - Next, the two Transferability Indices are TIother(%) = Δ𝑅 Δ𝑅 other math 100, TInon(%) = Δ𝑅non Δ𝑅 math 100. The TI value is compared against 0, any TI above zero indicates positive transfer observed. Results. Figure 2 and Table 4 present comparative evaluation of transferability indices of various models. We can conclude that the models transferability may be related to finetuning methods, model size, and model architecture. While the choice of finetuning method appears to be more crucial factor as we observe that RL-tuned models consistently attain higher transferability indices on both other-reasoning and non-reasoning tasks, whereas SFT-trained models often suffer negative TInon. This consistent pattern did not appear in the comparison of other factors and strongly suggests that on-policy RL updates reinforce reasoning skills without severely perturbing the models general-domain representations, while off-policy SFT on large, static reasoning datasets may over-specialize the latent space, even triggering representation collapse and resulting in lower performance in non-reasoning domains."
        },
        {
            "title": "2.2 Control Study",
            "content": "Motivated by our findings in Section 2.1, we design light-weight controlled study to directly compare SFT and RL on an identical dataset. Concretely, we start from small, high-quality mathematics dataset (see Appendix A.2.2 for details), then query strong teacher model (Qwen3-32B-Instruct) to extract complete chain-of-thought (CoT) reasoning traces with reject sampling. These CoT traces become our SFT training targets, while the original answer labels Understanding Transferability of LLM Reasoning 5 serve as the rewards for RL. This alignment ensures both paradigms learn from the same data samples. Then, we take the Qwen3-14B-Base model and fine-tune it in two ways: (i) SFT on the teacher-generated CoT traces; (ii) RL using only the groundtruth. We name our model UniReason. We compare against the Qwen3-14B-Base model. Evaluation is conducted on three benchmark groups mentioned above using accuracy. Details about training datasets, baseline models, and hyperparameters could also be found in Appendix A.2. Our experimental results on three groups of benchmarks (see Table 1) reveal consistent pattern: On math reasoning  (Table 1)  , our UniReason-Qwen3-14B(RL) model climbs to 55.7% on AIME24, 87.8% on MATH500, and 33.8% on OlympiadBench, outperforming corresponding SFT-based models. For other reasoning tasks, SFT-based models make uneven progress (e.g. UniReason-Qwen314B-SFT-think scores 55.9% on GPQA), whereas RL fine-tuning yields significant lifts: UniReasonQwen3-14B(RL) gains 1.8% on GPQA, and 17.1% on LiveCodeBench2 over SFT. Crucially, in non-reasoning evaluations, SFT models stagnate or decline, while the RL model recover and exceed the base in nearly all the benchmarks. From Table 1, our RL-tuned model achieve positive TIother and TInon, meaning its math improvements also transfer to other reasoning and even boost non-reasoning performance. In contrast, SFT-based models often have negative TInon (loss on non-reasoning tasks), indicating poor transfer beyond reasoning tasks. These results show that RL-tuned reasoning models perform generally better than SFT-based models on both reasoning and non-reasoning tasks when carefully controlling other factors. Especially, our UniReason model is trained on single distilled math dataset, but it still preserves and even improves general-domain performance while showing strong reasoning gains."
        },
        {
            "title": "3 Latent Representation Shifts: Insights from PCA Analysis",
            "content": "As discussed in Section 2.1, applying SFT to the Qwen model improves reasoning abilities such as mathematical problem-solving and code generation, but substantially impairs generaldomain performance. We observe that most SFT models fail to transfer their improved mathematical reasoning capabilities to other domains. In contrast, our controlled study shows that RL-tuned models generalize well to non-math domains, despite being trained solely on math queries, whereas SFT-tuned models do not. To understand the underlying cause of this transferability gap, we employ PCA shift analysis to examine how the internal feature geometry of the model evolves under different training paradigms, model sizes, and model families across diverse query distributions. Recent studies (Xu et al., 2025a; Zheng et al., 2025) demonstrate that PCA shift analysis provides sensitive and interpretable measure of representational changes relevant to task performance. Importantly, changes in model parameters do not always correspond to functional differences: large weight updates may leave outputs unchanged, while subtle parameter modifications can lead to significant shifts in the activation distribution. By focusing on hidden representations, PCA shift directly captures how the model encodes and processes information, offering more faithful account of its internal knowledge state than parameter-based metrics. This perspective allows us to distinguish between true knowledge erasure and parameterization changes that leave the underlying feature space intact. Furthermore, since transferability fundamentally relies on the alignment and stability of learned representations across tasks or domains, PCA shift is particularly effective for diagnosing changes that may impact cross-domain generalization. Shifts in principal components reveal whether the models internal feature space remains suitable for knowledge transfer or has been disrupted by training or unlearning interventions. Understanding Transferability of LLM Reasoning"
        },
        {
            "title": "3.1 Experimental Setup",
            "content": "Models and Tasks. In Section 2.1, we observe that models trained on math datasets show moderate transferability on other reasoning tasks. We perform PCA shift analyses on the corresponding models and tasks, aiming to critically assess the robustness of these phenomena from feature-space perspective. Evaluation. Given input queries X, we extract hidden states state () {orig, updated}. Applying PCA (𝑛 = 2) to 𝑖,1 onto the first principal direction (PC1) and 𝑚() 𝑚() defined as Δ𝑚() 𝑚orig 𝑖,1 𝑖,1 indicator of distributional change. Small shifts indicate stable features. at each layer 𝑖 for each model , we compute the mean projection 𝑖,2 onto the second (PC2). The PCA shift is 𝑖,2 as an auxiliary for PC1, while for PC2, we directly report 𝑚() = 𝑚() 𝑖,1 () 𝑖 () 𝑖 3."
        },
        {
            "title": "Investigating Latent Space Shift",
            "content": "(cid:205)𝐿 () 𝑖 = (Δ𝑚() 𝑖,1 , 𝑚() 𝑖,2 To quantify the overall latent shift, we define representation center for each model state () as () the mean of PCA-projected coordinates across all layers: z() = 1 , where 𝐿 denotes the 𝐿 𝑖 ) is the vector of PCA coordinates for layer 𝑖 in total number of layers and state (). The latent shift between two model states, such as the original (base) and an updated model, is then measured by the Euclidean distance: 𝑑 () = (cid:13) Based on the analyses in Appendix A.3, RL-based training proves essential for developing robust and generalizable language models that maintain strong balance between generaldomain and reasoning capabilities. Motivated by this, we further analyze our finetuned models proposed in the controlled study. As shown in Table 2, RL-based models (highlighted in red) achieve the lowest PCA shift magnitudes across math, other-reasoning, and non-reasoning (cid:13)z() z(orig) (cid:13) 𝑖=1 (cid:13)2 . Figure 3 PCA shift of Qwen3-14B-Base across different training methods and tasks. 𝑑 () is the Euclidean distance between representation centroids before and after training. The first two rows show models trained with SFT , and the last row shows models trained with RL . RL training results in the smallest PCA shift for all task types, suggesting more stable latent representations. Understanding Transferability of LLM Reasoning 7 Table 2 Comparison of mean PCA shift magnitudes on math, other reasoning and non-reasoning tasks for Qwen3-14B and our finetuned models. We observed that RL model has lower mean PCA shifts than SFT models. Model Math Other-Reasoning Non-Reasoning Qwen3-14B (no-think) UniReason-Qwen3-14B-no-think (SFT) UniReason-Qwen3-14B-think (SFT) UniReason-Qwen3-14B (RL) 40.4 21.4 19.2 8.5 13.9 10.9 6.7 3.5 129.8 113.7 38.2 36. tasks. Figure 3 further supports these findings, illustrating that the RL-based model consistently yields minimal and tightly clustered latent shifts across diverse benchmarks. In contrast, SFTbased model, particularly those without explicit reasoning signals, exhibit more scattered and pronounced shifts. These results, along with the evaluations in Section 2.1, highlight the clear advantage of RL compared with SFT. This confirms the necessity of holistic and well-balanced optimization objective, rather than isolated interventions, to minimize catastrophic forgetting and preserve performance in large-scale language models."
        },
        {
            "title": "Rank Analyses",
            "content": "In this section, we conduct token-level analyses to further examine the distribution shift of RL and SFT models trained on mathematical reasoning data."
        },
        {
            "title": "4.1 Setup",
            "content": "KL-divergence serves as standard metric for measuring differences between probability distributions. For token rank shift analysis, we first generate tokens using the fine-tuned model, then decode these same tokens using the backbone model to determine their original ranking positions. The rank shift is calculated as the difference in token rankings between the fine-tuned model and the backbone model for each token (Li et al., 2025c; Lin et al., 2023). Following the observations in Section 2.1, we perform additional token-distribution analyses on the corresponding models and tasks to assess the model distribution shift from token-space perspective. Specifically, we employ KL-divergence and token rank shift metrics to analyze distribution shifts between models. Figure 4 Word clouds showing significantly shifted tokens between UniReason-Qwen3-14B-RL (left) and UniReason-Qwen3-14B-SFT-think (right) models on mathematical reasoning tasks. Tokens are extracted based on frequency and rank shifts compared with Qwen3-14B-Base model then categorized as logical-structural words (highlighted in red) or content-specific words (highlighted in blue). Under math reasoning task, the RL model promptly shifts logic-related tokens such as But and So while the SFT model shifts various types of tokens, including many irrelevant tokens to the task. Understanding Transferability of LLM Reasoning 8 Figure 5 KL divergence analysis of RL and SFT models. Higher KL divergence indicates greater distribution shifts from the original backbone model. We observe that RL models consistently exhibit significantly lower KL divergence compared to SFT models across different tasks, suggesting less distribution shift during training. Figure 6 Visualization of token rank shifts across different position indices for both reasoning and nonreasoning tasks. We observe that RL models exhibit less token rank shifts while SFT models demonstrate substantial rank shifts across numerous positions throughout the sequence."
        },
        {
            "title": "4.2 Experiment Result",
            "content": "RL models exhibit lower KL-divergence from backbone models. In Figure 5, we observe that the KL divergence of SFT models on both reasoning and non-reasoning tasks is significantly larger than that of RL models. This indicates that RL models exhibit substantially less distribution shift from the token distribution level compared to SFT models. For instance, UniReasonQwen3-14B-SFT-no-think demonstrates KL divergences of 0.372 and 0.283 on MATH-500 and IFEval respectively compared to the backbone model, whereas UniReason-Qwen3-14B(RL) achieves considerably lower KL divergences of only 0.084 and 0.019 on the corresponding tasks. RL models demonstrate reduced token rank shifts. In Figure 15, we further analyze the token rank shift for both SFT and RL models. We find that RL models exhibit substantially lower average token rank shifts compared to the backbone model than SFT models. Specifically, UniReason-Qwen3-14B(RL) demonstrates an average token rank shift of only 0.98, while the UniReason-Qwen3-14B-SFT-no-think shows dramatically higher average token rank shift of Understanding Transferability of LLM Reasoning 9 Table 3 Case study of shifted tokens for RL and SFT models. Completed queries and answers could be found in Appendix A.4. RL models selectively shift task-relevant or logic-token tokens (labeled in red). In contrast, SFT models exhibit extensive token shifting, including numerous query-irrelevant tokens. For example, non-reasoning queries inappropriately introduce reasoning-related tokens, leading to unnecessary overthinking that detracts from performance. Domain Query Model Shifted Tokens Reasoning Task Nonreasoning Task treeks Ten weigh as much as three and squigs goolee. one Two treeks and one goolee in are equal weight to one squig. How many treeks weight equals one squig? an Write to my email boss telling him that am quitting. The email must contain title wrapped in double angular brackets RL Model (Only 15 tokens experienced rank shift when decoded in the base model) In Now Now define for number second 2 Now , add This (390 tokens experienced rank shift) SFT Model they The again conflicting but Alternatively make have Hmm hold equations Wait For find check Let maybe using written Original pl contrad So There Wait solve Alternatively Alright so First solving either check conflicting write Correct here another Like where ? where The question / . The where here where equations Therefore problem check if was the ? equations together . answer For or For Wait matrices this about either and solve combined 1 problem ten Let . equation That If... Still From RL Model (Only 14 tokens experienced rank shift when decoded in the base model) Write but Res formally much step grown will once Full (158 tokens experienced rank shift) SFT Model Hmm Alright Wait Wait Wait try Another Maybe Another Alternatively Wait but Wait Wait Diamond On check Who Starting user generate original ( original example make structure So follow The instructions user Let ( First ( check says doesn . to But willingness generated : so has follow . . The structure the the first is But is structured with However who step like given repeated then also mention answer adding Let the . maybe wants But particular The answer the answer that would The which original instruction which with the )\". the first context . the providing Email of The The first exactly then provide ... concise Since like straightforward . but says wants effective 10.6. This suggests that SFT models experience greater token distribution shifts than RL models across both reasoning and non-reasoning tasks. Figure 6 provides detailed visualization of token rank shifts across different position indices for both reasoning and non-reasoning tasks. We observe that RL models exhibit less token rank shifts (less than 10) at only few positions. In contrast, SFT models demonstrate substantial rank shifts across numerous positions throughout the sequence. RL models selectively shift task-relevant tokens, while SFT models shift numerous irrelevant tokens. Table 3 presents comprehensive case study examining shifted tokens in RL and SFT models for reasoning and non-reasoning queries. For RL models, we observe highly selective In token shifting, with only small number of query-relevant tokens undergoing shifts. reasoning queries, shifts are limited to essential logical tokens such as \"define\", \"add\", \"second\", and \"number\" while in non-reasoning queries, only task-specific keywords like \"\" \"\" \"write\" Understanding Transferability of LLM Reasoning 10 and \"formally\" experience rank changes. In contrast, SFT models exhibit extensive token shifts, with 390 and 158 token shifts in reasoning and non-reasoning queries respectively. These shifts include numerous query-irrelevant tokens. For example, non-reasoning queries inappropriately introduce reasoning tokens, leading to unnecessary overthinking that detracts from performance. Please see Appendix A.4 for the full model response. Also, we calculate the word frequency of shifted tokens from both RL-tuned and SFT-tuned models under math reasoning task, merging them into one pool and select top 250 frequent tokens in it. Then we plot word cloud for the selected tokens, as shown in Figure 4. The figure confirms our observation that RL selectively shifts task-related tokens while SFT shows both relevant and irrelevant token shifts."
        },
        {
            "title": "5 Related Work",
            "content": "Reasoning Fine-Tuning of LLMs. Recent advancements in large language models have notably emphasized specialized fine-tuning methods to enhance reasoning capabilities (Chen et al., 2023; Feng et al., 2025; Li et al., 2025a,b; Liu et al., 2025; Wang et al., 2024c; Wong et al., 2025; Xu et al., 2025b; Yang et al., 2025; Yeo et al., 2025; Ziegler et al., 2019). The chain-of-thought prompting strategy introduced by Wei et al. (2022) encourages models to produce step-by-step explanations, significantly boosting performance in symbolic reasoning tasks (Lambert et al., 2025; Longpre et al., 2023; Wei et al., 2022; Yu et al., 2024). Subsequent extensions, such as DeepSeek-R1 (Team, 2025a), have integrated reinforcement learning approaches alongside CoT, optimizing models through reward-driven policy improvements. Such RL-enhanced fine-tuning has achieved state-of-the-art results on benchmarks and competitive programming challenges (Hendrycks et al., 2021b; Lambert et al., 2025; Team, 2025a; Team et al., 2025). Supervised Fine-Tuning vs. Reinforcement Learning for LLMs. Fine-tuning methods for reasoning typically fall into two major categories: supervised fine-tuning and reinforcement learning (Chen et al., 2024). SFT methods predominantly utilize annotated reasoning trajectories or solution traces, directly training models to replicate explicit reasoning sequences from datasets (Wang et al., 2023; Wei et al., 2022). RL-based fine-tuning, however, guides models by rewarding accurate and logically coherent reasoning steps without explicit step-by-step supervision, allowing exploration and optimization of reasoning pathways through feedback loops (Chu et al., 2025; Liu et al., 2025; Wang et al., 2024c; Ziegler et al., 2019). Generalization in Reasoning Models. Interestingly, models heavily fine-tuned for formal reasoning sometimes falter on more general language tasks (Kumar et al., 2022). For example, OpenAIs o1, while excelling in STEM benchmarks, raised concerns about its versatility on other tasks (OpenAI, 2024). Follow-up research introduced reinforcement fine-tuning precisely to address this gap, aiming to adapt generalist models reasoning to new domains with limited data (Zhang et al., 2024). Indeed, o1 and similar reasoning models are built on strong generalpurpose bases to retain broad knowledge (Hendrycks et al., 2021a; OpenAI, 2024). Nonetheless, trade-offs have been observed. Wang et al. (2024b) found that fine-tuning on narrow set of instruction types can degrade models performance on other skills. Recent works have also stepped into analyzing the cross-domain performance of reasoning models (Sun et al., 2025), especially for RL-based approaches (Cheng et al., 2025; Hu et al., 2025). Representation-Level Analysis. Fine-tuning for reasoning models not only boosts task performance but also alters the models internal representations (Sheng et al., 2024). Recent studies have begun to probe how CoT-based fine-tuning changes the latent space of LLMs (Wang et al., 2025; Xu et al., 2025a). Lobo et al. (2024) find that task-specific fine-tuning can reduce the faithfulness of models generated reasoning chains, indicating shifts in its underlying inference mechanisms. Complementary analyses of hidden states provide insight into such shifts. Xu et al. (2024) proposes quantitative framework for assessing ideas that leverages hidden representations from LLMs to predict the merit of scientific ideas. Techniques like principal component Understanding Transferability of LLM Reasoning analysis further reveal that fine-tuning can carve out new directions in representation space that correspond to reasoning-related features (Xu et al., 2025a; Zhou et al., 2025)."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we investigated the factors that influence the transferability of reasoning models across reasoning and non-reasoning benchmarks. Our key findings are as follows. First, besides model size and model architecture, the choice of fine-tuning paradigm strongly shapes transfer: RL-tuned models achieve significant gains on math reasoning while preserving positive transfer to other reasoning tasks and non-reasoning tasks, whereas SFT often incurs negative transfer on non-reasoning benchmarks. Second, PCA analysis of latent space confirms that RL induces minimal drift from backbone representations thus maintaining feature stability, while SFT produces larger latent shifts, especially in non-reasoning domains. Third, token-distribution analysis shows that RL selectively adjusts only handful of task-relevant tokens, whereas SFT perturbs many irrelevant tokens, indicating RLs more targeted optimization. Notably, our UniReason-Qwen3-14B (RL) fine-tuned on 47K math examples achieves the best balance of reasoning improvement and general-domain retention among compared models, strongly validating our hypotheses and analysis."
        },
        {
            "title": "Author Contributions",
            "content": "Maggie Huan co-led the project, co-prepared the dataset, led initial experiments and analyses, conducted benchmark evaluations, wrote Sections 2, 5, and 6 and revised the paper. Yuetai Li co-led the project, conducted the preliminary study and initial testing, initially identified the phenomenon (Section 2), analyzed results, performed the token-level analysis (Section 4), and contributed to method refinement. Tuney Zheng co-led the project, co-prepared the math dataset, rl models and fine-tuned models for the controlled study, contributed to result analysis, and supported method development. Xiaoyu Xu performed latent-space PCA analysis, contributed to early result analysis, and wrote Section 3. Seungone Kim, Minxin Du, and Radha Poovendran provided valuable feedback on the manuscript. Graham Neubig co-supervised the project and offered substantive guidance and feedback throughout. Xiang Yue conceived and co-led the project by proposing the initial research ideas, designing the methodology, analyzing results, writing Abstract and Introduction, revising the paper, and supervising the overall work."
        },
        {
            "title": "Acknowledgment",
            "content": "The authors would like to thank Qian Liu for helping set up the training environment. This work was supported in part by Carnegie Bosch Institute Fellowship to Xiang Yue."
        },
        {
            "title": "References",
            "content": "J. Chen, X. Han, Y. Ma, X. Zhou, and L. Xiang. Unlock the correlation between supervised fine-tuning and reinforcement learning in training code large language models. ArXiv preprint, abs/2406.10305, 2024. URL https://arxiv.org/abs/2406.10305. Understanding Transferability of LLM Reasoning 12 W. Chen, M. Yin, M. Ku, P. Lu, Y. Wan, X. Ma, J. Xu, X. Wang, and T. Xia. TheoremQA: theorem-driven question answering dataset. In H. Bouamor, J. Pino, and K. Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 78897901, Singapore, Dec. 2023. Association for Computational Linguistics. doi: 10.18653/v 1/2023.emnlp-main.489. URL https://aclanthology.org/2023.emnlp-main.489/. Z. Cheng, S. Hao, T. Liu, F. Zhou, Y. Xie, F. Yao, Y. Bian, Y. Zhuang, N. Dey, Y. Zha, Y. Gu, K. Zhou, Y. Wang, Y. Li, R. Fan, J. She, C. Gao, A. Saparov, H. Li, T. W. Killian, M. Yurochkin, Z. Liu, E. P. Xing, and Z. Hu. Revisiting reinforcement learning for llm reasoning from cross-domain perspective, 2025. URL https://arxiv.org/abs/2506.14965. T. Chu, Y. Zhai, J. Yang, S. Tong, S. Xie, D. Schuurmans, Q. V. Le, S. Levine, and Y. Ma. Sft memorizes, rl generalizes: comparative study of foundation model post-training. ArXiv preprint, abs/2501.17161, 2025. URL https://arxiv.org/abs/2501.17161. N. Dziri, X. Lu, M. Sclar, X. L. Li, L. Jian, B. Y. Lin, P. West, C. Bhagavatula, R. L. Bras, J. D. Hwang, S. Sanyal, S. Welleck, X. Ren, A. Ettinger, Z. Harchaoui, and Y. Choi. Faith and fate: Limits of transformers on compositionality. Advances in Neural Information Processing Systems, 36, 2024. URL https://openreview.net/forum?id=Fkckkr3ya8. Y. Feng, Z. Xu, F. Jiang, Y. Li, B. Ramasubramanian, L. Niu, B. Y. Lin, and R. Poovendran. Visualsphinx: Large-scale synthetic vision logic puzzles for rl, 2025. URL https://arxiv. org/abs/2505.23977. L. Gao, J. Tow, B. Abbasi, S. Biderman, S. Black, A. DiPofi, C. Foster, L. Golding, J. Hsu, A. Le Noach, H. Li, K. McDonell, N. Muennighoff, C. Ociepa, J. Phang, L. Reynolds, H. Schoelkopf, A. Skowron, L. Sutawika, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou. The language model evaluation harness, 2024. URL https://zenodo.org/records/126 08602. E. Guha, R. Marten, S. Keh, N. Raoof, G. Smyrnis, H. Bansal, M. Nezhurina, J. Mercat, T. Vu, Z. Sprague, et al. Openthoughts: Data recipes for reasoning models. ArXiv preprint, abs/2506.04178, 2025. URL https://arxiv.org/abs/2506.04178. C. He, R. Luo, Y. Bai, S. Hu, Z. L. Thai, J. Shen, J. Hu, X. Han, Y. Huang, Y. Zhang, J. Liu, L. Qi, Z. Liu, and M. Sun. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. In ACL (1), pages 38283850, 2024. URL https://doi.org/10.18653/v1/2024.acl-long.211. D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021a. URL https://openreview.net/forum?id=d7KBjmI3GmQ. D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021b. URL https://openreview.net/forum?id=7Bywt2mQsCe. C. Hu, Y. Zhu, A. Kellermann, C. Biddulph, S. Waiwitlikhit, J. Benn, and D. Kang. Breaking barriers: Do reinforcement post training gains transfer to unseen domains?, 2025. URL https://arxiv.org/abs/2506.19733. N. Jain, K. Han, A. Gu, W.-D. Li, F. Yan, T. Zhang, S. Wang, A. Solar-Lezama, K. Sen, and I. Stoica. Livecodebench: Holistic and contamination free evaluation of large language Understanding Transferability of LLM Reasoning 13 models for code. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=chfJJYC3iL. H. Kokel, M. Katz, K. Srinivas, and S. Sohrabi. Acpbench: Reasoning about action, change, and planning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 26559 26568, 2025. URL https://ojs.aaai.org/index.php/AAAI/article/view/34857. A. Kumar, A. Raghunathan, R. Jones, T. Ma, and P. Liang. Fine-tuning can distort pretrained features and underperform out-of-distribution, 2022. URL https://arxiv.org/abs/22 02.10054. N. Lambert, J. Morrison, V. Pyatkin, S. Huang, H. Ivison, F. Brahman, L. J. V. Miranda, A. Liu, N. Dziri, S. Lyu, Y. Gu, S. Malik, V. Graf, J. D. Hwang, J. Yang, R. L. Bras, O. Tafjord, C. Wilhelm, L. Soldaini, N. A. Smith, Y. Wang, P. Dasigi, and H. Hajishirzi. Tulu 3: Pushing frontiers in open language model post-training, 2025. URL https://arxiv.org/abs/2411.15124. J. Li, X. Cheng, X. Zhao, J.-Y. Nie, and J.-R. Wen. Halueval: large-scale hallucination evaluation benchmark for large language models. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. URL https://openreview.net/forum?id=bxsrykzSnq. X. Li, Z. Yu, Z. Zhang, X. Chen, Z. Zhang, Y. Zhuang, N. Sadagopan, and A. Beniwal. When thinking fails: The pitfalls of reasoning for instruction-following in llms, 2025a. URL https: //arxiv.org/abs/2505.11423. Y. Li, Z. Xu, F. Jiang, B. Ramasubramanian, L. Niu, B. Y. Lin, X. Yue, and R. Poovendran. Temporal sampling for forgotten reasoning in llms, 2025b. URL https://arxiv.org/abs/ 2505.20196. Y. Li, X. Yue, Z. Xu, F. Jiang, L. Niu, B. Y. Lin, B. Ramasubramanian, and R. Poovendran. Small models struggle to learn from strong reasoners. CoRR, abs/2502.12143, February 2025c. URL https://doi.org/10.48550/arXiv.2502.12143. B. Y. Lin, A. Ravichander, X. Lu, N. Dziri, M. Sclar, K. Chandu, C. Bhagavatula, and Y. Choi. The unlocking spell on base llms: Rethinking alignment via in-context learning, 2023. URL https://arxiv.org/abs/2312.01552. Z. Liu, Z. Huan, X. Wang, J. Lyu, J. Tao, X. Li, F. Huang, and H. Xu. World models with hints of large language models for goal achieving. In L. Chiruzzo, A. Ritter, and L. Wang, editors, Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 5072, Albuquerque, New Mexico, Apr. 2025. Association for Computational Linguistics. ISBN 979-8-89176-189-6. doi: 10.18653/v1/2025.naacl-long.3. URL https://aclanthology.org /2025.naacl-long.3/. E. Lobo, C. Agarwal, and H. Lakkaraju. On the impact of fine-tuning on chain-of-thought reasoning. ArXiv preprint, abs/2411.15382, 2024. URL https://arxiv.org/abs/2411.1 5382. S. Longpre, L. Hou, T. Vu, A. Webson, H. W. Chung, Y. Tay, D. Zhou, Q. V. Le, B. Zoph, J. Wei, and A. Roberts. The flan collection: Designing data and methods for effective instruction tuning. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 2263122648. PMLR, 2329 Jul 2023. URL https://proc eedings.mlr.press/v202/longpre23a.html. Understanding Transferability of LLM Reasoning 14 H. Luo, Q. Sun, C. Xu, P. Zhao, J. Lou, C. Tao, X. Geng, Q. Lin, S. Chen, and D. Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. ArXiv preprint, abs/2308.09583, 2023. URL https://arxiv.org/abs/2308 .09583. M. Luo, S. Tan, J. Wong, X. Shi, W. Y. Tang, M. Roongta, C. Cai, J. Luo, L. E. Li, R. A. Popa, and I. Stoica. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl, 2025. URL https://pretty-radio-b75.notion.site/DeepScaleR-Surpassing-O1-Previ ew-with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2. Notion Blog. X. Ma, Q. Liu, D. Jiang, G. Zhang, Z. Ma, and W. Chen. General-reasoner: Advancing llm reasoning across all domains. ArXiv preprint, abs/2505.14652, 2025. URL https://arxiv. org/abs/2505.14652. S. Mishra, M. Finlayson, P. Lu, L. Tang, S. Welleck, C. Baral, T. Rajpurohit, O. Tafjord, A. Sabharwal, P. Clark, and A. Kalyan. LILA: unified benchmark for mathematical In Y. Goldberg, Z. Kozareva, and Y. Zhang, editors, Proceedings of the 2022 reasoning. Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 58075832. Association for Computational Linguistics, 2022. doi: 10.18653/V1/2022.EMNLPMAIN.392. URL https: //doi.org/10.18653/v1/2022.emnlp-main.392. N. Muennighoff, Z. Yang, W. Shi, X. L. Li, L. Fei-Fei, H. Hajishirzi, L. Zettlemoyer, P. Liang, E. Candès, and T. Hashimoto. s1: Simple test-time scaling, 2025. URL https://arxiv.or g/abs/2501.19393. OpenAI. Learning to reason with llms, 2024. URL https://openai.com/index/learnin g-to-reason-with-llms/. N. Raoof, E. K. Guha, R. Marten, J. Mercat, E. Frankel, S. Keh, H. Bansal, G. Smyrnis, M. Nezhurina, T. Vu, Z. R. Sprague, M. A. Merrill, L. Chen, C. Choi, Z. Khan, S. Grover, B. Feuer, A. Suvarna, S. Su, W. Zhao, K. Sharma, C. C.-J. Ji, K. Arora, J. Li, A. Gokaslan, S. M. Pratt, N. Muennighoff, J. Saad-Falcon, J. Yang, A. Aali, S. Pimpalgaonkar, A. Albalak, A. Dave, H. Pouransari, G. Durrett, S. Oh, T. Hashimoto, V. Shankar, Y. Choi, M. Bansal, C. Hegde, R. Heckel, J. Jitsev, M. Sathiamoorthy, A. Dimakis, and L. Schmidt. Automatic evals for llms, 2025. URL https://github.com/mlfoundations/evalchemy. S. Reddy, D. Chen, and C. D. Manning. CoQA: conversational question answering challenge. Transactions of the Association for Computational Linguistics, 7:249266, 2019. doi: 10.1162/tacl_a _00266. URL https://aclanthology.org/Q19-1016. D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman. In First Conference on Language GPQA: graduate-level google-proof q&a benchmark. Modeling, 2024. URL https://openreview.net/forum?id=Ti67584b98. Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. K. Li, Y. Wu, and D. Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402.03300. G. Sheng, C. Zhang, Z. Ye, X. Wu, W. Zhang, R. Zhang, Y. Peng, H. Lin, and C. Wu. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, EuroSys 25, page 12791297, New York, NY, USA, 2025. Association for Computing Machinery. ISBN 9798400711961. doi: 10.1145/3689031.3696075. URL https://doi.org/10.1145/3689031.3696075. Understanding Transferability of LLM Reasoning 15 S. Sheng, Y. Xu, T. Zhang, Z. Shen, L. Fu, J. Ding, L. Zhou, X. Gan, X. Wang, and C. Zhou. RepEval: Effective text evaluation with LLM representation. pages 70197033, Nov. 2024. doi: 10.18653/v1/2024.emnlp-main.398. URL https://aclanthology.org/2024.emnlp-m ain.398/. Y. Su, D. Yu, L. Song, J. Li, H. Mi, Z. Tu, M. Zhang, and D. Yu. Crossing the reward bridge: Expanding rl with verifiable rewards across diverse domains, 2025. URL https://arxiv. org/abs/2503.23829. Y. Sun, S. Hu, G. Zhou, K. Zheng, H. Hajishirzi, N. Dziri, and D. Song. Omega: Can llms reason outside the box in math? evaluating exploratory, compositional, and transformative generalization, 2025. URL https://arxiv.org/abs/2506.18880. D.-A. Team. DeepSeek-R1: Incentivizing reasoning capability in llms via reinforcement learning, 2025a. URL https://arxiv.org/abs/2501.12948. K. Team, A. Du, B. Gao, B. Xing, C. Jiang, C. Chen, C. Li, C. Xiao, C. Du, C. Liao, C. Tang, C. Wang, D. Zhang, E. Yuan, E. Lu, F. Tang, F. Sung, G. Wei, G. Lai, H. Guo, H. Zhu, H. Ding, H. Hu, H. Yang, H. Zhang, H. Yao, H. Zhao, H. Lu, H. Li, H. Yu, H. Gao, H. Zheng, H. Yuan, J. Chen, J. Guo, J. Su, J. Wang, J. Zhao, J. Zhang, J. Liu, J. Yan, J. Wu, L. Shi, L. Ye, L. Yu, M. Dong, N. Zhang, N. Ma, Q. Pan, Q. Gong, S. Liu, S. Ma, S. Wei, S. Cao, S. Huang, T. Jiang, W. Gao, W. Xiong, W. He, W. Huang, W. Xu, W. Wu, W. He, X. Wei, X. Jia, X. Wu, X. Xu, X. Zu, X. Zhou, X. Pan, Y. Charles, Y. Li, Y. Hu, Y. Liu, Y. Chen, Y. Wang, Y. Liu, Y. Qin, Y. Liu, Y. Yang, Y. Bao, Y. Du, Y. Wu, Y. Wang, Z. Zhou, Z. Wang, Z. Li, Z. Zhu, Z. Zhang, Z. Wang, Z. Yang, Z. Huang, Z. Huang, Z. Xu, Z. Yang, and Z. Lin. Kimi k1.5: Scaling reinforcement learning with llms, 2025. URL https://arxiv.org/abs/2501.12599. Q. Team. Qwen2.5 technical report, 2025b. URL https://arxiv.org/abs/2412.15115. D. Vilares and C. Gómez-Rodríguez. HEAD-QA: healthcare dataset for complex reasoning. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 960966, Florence, Italy, 2019. Association for Computational Linguistics. doi: 10.18653/v1/ P19-1092. URL https://aclanthology.org/P19-1092. P. Wang, L. Li, Z. Shao, R. X. Xu, D. Dai, Y. Li, D. Chen, Y. Wu, and Z. Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. 2024a. URL https: //arxiv.org/abs/2312.08935. R. Wang, H. Li, M. Wu, Y. Wang, X. Han, C. Zhang, and T. Baldwin. Demystifying instruction mixing for fine-tuning large language models. In ACL (Student Research Workshop), pages 162169, 2024b. URL https://aclanthology.org/2024.acl-srw.15. X. Wang, J. Wei, D. Schuurmans, Q. V. Le, E. H. Chi, S. Narang, A. Chowdhery, and D. Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net /forum?id=1PL1NIMMrw. Y. Wang, P. Zhang, B. Yang, D. F. Wong, and R. Wang. Latent space chain-of-embedding enables output-free LLM self-evaluation. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=jxo70B9fQo. Z. Wang, Y. Li, Y. Wu, L. Luo, L. Hou, H. Yu, and J. Shang. Multi-step problem solving through verifier: An empirical analysis on model-induced process supervision. In Y. AlOnaizan, M. Bansal, and Y.-N. Chen, editors, Findings of the Association for Computational Linguistics: EMNLP 2024, pages 73097319, Miami, Florida, USA, Nov. 2024c. Association Understanding Transferability of LLM Reasoning 16 for Computational Linguistics. doi: 10.18653/v1/2024.findings-emnlp.429. URL https: //aclanthology.org/2024.findings-emnlp.429/. J. Wei, X. Wang, D. Schuurmans, M. Bosma, brian ichter, F. Xia, E. H. Chi, Q. V. Le, and D. Zhou. Chain of thought prompting elicits reasoning in large language models. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=_VjQlMeSB_J. T. Wei, J. Luan, W. Liu, S. Dong, and B. Wang. Cmath: Can your language model pass chinese elementary school math test?, 2023. URL https://arxiv.org/abs/2306.16636. J. Welbl, N. F. Liu, and M. Gardner. Crowdsourcing multiple choice science questions. In Proceedings of the 3rd Workshop on Noisy User-generated Text, pages 94106, Copenhagen, Denmark, 2017. Association for Computational Linguistics. doi: 10.18653/v1/W17-4413. URL https://aclanthology.org/W17-4413. A. Wong, T. Bäck, A. Plaat, N. van Stein, and A. V. Kononova. Towards deeper understanding of reasoning capabilities in large language models, 2025. URL https://arxiv.org/abs/ 2505.10543. T. Xie, F. Zhou, Z. Cheng, P. Shi, L. Weng, Y. Liu, T. J. Hua, J. Zhao, Q. Liu, C. Liu, Z. Liu, Y. Xu, H. SU, D. Shin, C. Xiong, and T. Yu. Openagents: An open platform for language agents in the wild. In First Conference on Language Modeling, 2024. URL https://openreview.net/f orum?id=sKATR2O1Y0. X. Xu, X. Yue, Y. Liu, Q. Ye, H. Hu, and M. Du. Unlearning isnt deletion: Investigating reversibility of machine unlearning in llms, 2025a. URL https://arxiv.org/abs/2505.1 6831. Y. Xu, B. Xue, S. Sheng, C. Deng, J. Ding, Z. Shen, L. Fu, X. Wang, and C. Zhou. Good idea or not, representation of llm could tell. ArXiv preprint, abs/2409.13712, 2024. URL https://arxiv.org/abs/2409.13712. Z. Xu, Y. Li, F. Jiang, B. Ramasubramanian, L. Niu, B. Y. Lin, and R. Poovendran. Tinyv: Reducing false negatives in verification improves rl for llm reasoning, 2025b. URL https: //arxiv.org/abs/2505.14625. J. Yang, J. Tu, H. Liu, X. Wang, C. Zheng, Z. Zhang, S. Cui, C. Chen, T. He, H. Wang, Y.-S. Ong, and M. Huang. Barrel: Boundary-aware reasoning for factual and reliable lrms, 2025. URL https://arxiv.org/abs/2505.13529. E. Yeo, Y. Tong, M. Niu, G. Neubig, and X. Yue. Demystifying long chain-of-thought reasoning in llms. In ICML 2025, 2025. URL https://arxiv.org/abs/2502.03373. L. Yu, W. Jiang, H. Shi, J. YU, Z. Liu, Y. Zhang, J. Kwok, Z. Li, A. Weller, and W. Liu. Metamath: In The Twelfth Bootstrap your own mathematical questions for large language models. International Conference on Learning Representations, 2024. URL https://openreview.net /forum?id=N8N0hgNDRt. X. Yue, X. Qu, G. Zhang, Y. Fu, W. Huang, H. Sun, Y. Su, and W. Chen. MAmmoTH: Building math generalist models through hybrid instruction tuning. In The Twelfth International Conference on Learning Representations, 2024a. URL https://openreview.net/forum?id=yLCl Gs770I. X. Yue, T. Zheng, G. Zhang, and W. Chen. MAmmoTH2: Scaling instructions from the web. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024b. URL https://openreview.net/forum?id=yVu5dnPlqA. Understanding Transferability of LLM Reasoning 17 W. Zeng, Y. Huang, Q. Liu, W. Liu, K. He, Z. Ma, and J. He. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild, 2025. URL https://arxiv.org/abs/2503.18892. Y. Zhang, Y. Yang, J. Shu, Y. Wang, J. Xiao, and J. Sang. Openrft: Adapting reasoning foundation model for domain-specific tasks with reinforcement fine-tuning. ArXiv preprint, abs/2412.16849, 2024. URL https://arxiv.org/abs/2412.16849. J. Zheng, X. Cai, S. Qiu, and Q. Ma. Spurious forgetting in continual learning of language models. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=ScI7IlKGdI. Y. Zheng, R. Zhang, J. Zhang, Y. Ye, and Z. Luo. LlamaFactory: Unified efficient fine-tuning of 100+ language models. In Y. Cao, Y. Feng, and D. Xiong, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pages 400410, Bangkok, Thailand, Aug. 2024. Association for Computational Linguistics. doi: 10.1 8653/v1/2024.acl-demos.38. URL https://aclanthology.org/2024.acl-demos.38/. B. Zhou, D. Khashabi, Q. Ning, and D. Roth. going on vacation takes longer than going for walk: study of temporal commonsense understanding. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 33633369, Hong Kong, China, 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1332. URL https://aclanthology.org/D19-1332. J. Zhou, T. Lu, S. Mishra, S. Brahma, S. Basu, Y. Luan, D. Zhou, and L. Hou. Instructionfollowing evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023. URL https://arxiv.org/abs/2311.07911. L. Zhou, L. Pacchiardi, F. Martínez-Plumed, K. M. Collins, Y. Moros-Daval, S. Zhang, Q. Zhao, Y. Huang, L. Sun, J. E. Prunty, et al. General scales unlock ai evaluation with explanatory and predictive power. ArXiv preprint, abs/2503.06378, 2025. URL https://arxiv.org/abs/25 03.06378. D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano, and G. Irving. Fine-tuning language models from human preferences. ArXiv preprint, abs/1909.08593, 2019. URL https://arxiv.org/abs/1909.08593. Understanding Transferability of LLM Reasoning"
        },
        {
            "title": "Contents",
            "content": "A.1 Detailed Evidence of Our Observed Phenomena . . . . . . . . . . . . . . . . . . . 18 A.2 Full Evaluation Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 A.2.1 Post-training Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 A.2.2 Training Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 A.2.3 Baselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2.4 Evaluation Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 A.2.5 Evaluation metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 A.3 PCA Analysis under Varying Settings . . . . . . . . . . . . . . . . . . . . . . . . . 22 A.4 Token Distribution Shifts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 A.4.1 Details about the case study . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 A.5 Breakdown Evaluation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 A.1 Detailed Evidence of Our Observed Phenomena As discussed in Section 2.1, we provided the complete evaluation for the Transferability Index for the off-the-shelf models on other reasoning and non-reasoning tasks in Table 4. A.2 Full Evaluation Setup A.2.1 Post-training Methods Reinforcement Learning has recently proven effective at steering large language models toward complex, multi-step objectives by optimizing policies with scalar reward signals (Zeng et al., 2025). We used the Verl framework(Sheng et al., 2025) and GRPO (Shao et al., 2024) setup to fine-tune the Qwen-3-14B-Base model, utilizing answer correctness as the reward signal. Our RL uses learning rate of 1 106 with an overall train batch size of 512 and clipping thresholds set between 0.22 and 0.28. We generate sequences up to 16k tokens long and perform 16 rollouts per prompt, then update the model in mini-batches of 128 samples. Both KL-divergence and entropy penalties are turned off (coefficients set to zero). We train the model for 140 steps and used the corresponding checkpoint. Supervised Fine-Tuning remains fundamental technique for adapting large pre-trained models by directly minimizing cross-entropy on high-quality datasets (Chu et al., 2025). We use the LLaMA-Factory framework (Zheng et al., 2024), which is an extensible and user-friendly framework supporting multiple architectures and advanced optimization algorithms, to finetune our model on teacher-generated chain-of-thought traces. We use 5 105 as learning rate, the batch size is 512 and we train for 1.5 epoch to align with our RL settings. A.2.2 Training Datasets As briefed in Section 2.2, our base training dataset is curated set of 47K high-quality mathematics problems. We stratified the examples using two complementary sources: low-difficulty problems drawn from the DeepScaler dataset (Luo et al., 2025), and high-difficulty (levels 35) problems extracted from SimpleRL (Zeng et al., 2025). To generate CoT annotations, we prompt each problem into the Qwen3-32B-Instruct model (Team, 2025b) and use reject sampling to generate our dataset. Understanding Transferability of LLM Reasoning 19 To further explore the effect of training data distribution for SFT-based reasoning models, we also distill larger and more comprehensive dataset collected from General-Reasoner (Ma et al., 2025), which contains 232K examples across reasoning and non-reasoning tasks (e.g., Math, Chemistry, Business). This additional distilled set is used to train the General-Reasoner model using supervised fine-tuning. A.2.3 Baselines In our experiments, we compare against Qwen3-14B-Base model (Team, 2025b), which is the original Qwen3-14B model without any further adaptation. This serves as the unmodified backbone for all fine-tuning models. Also, we report the results of Qwen3-14B-Instruct model under our tested benchmarks. It is an instruction-tuned version of Qwen3-14B model trained on large, general-purpose instruction-following dataset. We evaluate it under two prompting modes: think: prompts include special <think> token to explicitly elicit chain-of-thought reasoning. no-think: prompts are provided without the <think> token, relying solely on direct instruction-following behavior. Due to the wide range and enormous training data, this model is considered to give the optimal outputs across tasks in the current 14B-series models. To further validate our observation, we apply our controlled study pipeline also for GeneralReasoner (Ma et al., 2025), an RL-tuned reasoning model that are also math experts. We distilled the dataset for SFT finetuning with the same rejection sampling method using their proposed dataset. The dataset contains 232K samples covering various reasoning and non-reasoning tasks. Then we finetune the Qwen3-14B base model using the distilled dataset and name the model as General-Reasoner-Qwen3-14B(SFT) to directly compare with the RL-based General-Reasoner for fairer and more comprehensive controlled study towards SFT and RL. The Transferability Index results could be retrieved in Table 4, the obeservation also confirms our initial hypthotesis. A.2.4 Evaluation Benchmarks In the experiment, we evaluated our model across wide range of benchmarks. Notably, to explicitly reveal the transferability of reasoning models, we grouped them into three categories by their content: Math Reasoning Datasets We collected the following datasets that are composed of mathematical problems, which means that they typically need mathematical reasoning process to get the answer: MATH500 (Hendrycks et al., 2021b): curated subset of 500 problems sampled from the broader MATH dataset, covering topics like algebra, combinatorics, geometry, and number theory. AIME: Problems drawn from the American Invitational Mathematics Examination (AIME) 2024 and 2025, each with 30 challenging short-answer questions requiring multi-step reasoning. OlympiadBench (He et al., 2024): Problems sourced from international olympiads (e.g., IMO and regional contests). Other Reasoning Datasets We collected the following datasets that are mainly composed of general reasoning problems containing wider range of subjects: Understanding Transferability of LLM Reasoning 20 LiveCodeBench (Jain et al., 2025): It is continuously updated, contamination-free coding benchmark. We used its second version. GPQA-Diamond(Rein et al., 2024): It is graduate-level question-answering dataset that contains multiple-choice questions in biology, physics, and chemistry. We followed its diamond split. ACPBench (Kokel et al., 2025): It has 7 atomic reasoning tasks around 13 classical planning domains. We only used the multiple-choice problems. HeadQA (Vilares and Gómez-Rodríguez, 2019): Multiple-choice QA from healthcarespecialist certification exams, including questions across pharmacology, chemistry, nursery, psychology, biology, and medicine. Non-reasoning Datasets We collected the following datasets that are mainly composed of problems with factual answers, which means that they do not need reasoning process to give the answer: CoQA(Reddy et al., 2019): It has 127K questions in dialogues over passages, focusing on maintaining context and coreference across turns. IFEval (Zhou et al., 2023): It contains over 500 prompts, each embedding verifiable instructions. Evaluates strict vs. loose adherence to instructions. HaluEval (Li et al., 2023): It contains human-annotated samples where models must distinguish factual content from hallucinations. MC-TACO (Zhou et al., 2019): It is multiple-choice benchmark designed to evaluate models temporal commonsense, covering duration, ordering, typical time, frequency, and stationarity. A.2.5 Evaluation metrics We used LLM-Harness (Gao et al., 2024) to evaluate the models performance on OlympiadBench, ACPBench, HeadQA, CoQA, HaluEval, MC-TACO and used Eval-Chemy (Raoof et al., 2025) MATH500, AIME24, AIME25, GPQA-Diamond, LiveCodeBench, IFEval. On MATH500, AIME24, AIME25, GPQA-Diamond, and LiveCodeBench, we used 0.6 as temperature, and 0.95 as top-p value. In our experiments, we used accuracy to evaluate the models performance. Specifically, for AIME24 and AIME 25, we averaged accuracy on 10 samples. For GPQA-Diamond, LiveCodeBench and MATH 500, our score is the average accuracy over 3 samples. Specifically, we used version 2 and overall accuracy on LiveCodeBench. For ACPBench, we only used multiple choices, and averaged the score for all 10 tasks as the final score. For OlympiadBench, we only used math queries in English, and thus categorized Olympiad as math benchmark. For HaluEval, the performance is the accuracy averaged on 3 tasks with zero-shot. And for IFEval, we used strict instruction accuracy as the score. For OlympiadBench, ACPBench, HeadQA, CoQA, HaluEval, and IFEval, we used greedy sampling and sampled only once. Understanding Transferability of LLM Reasoning 21 Table 4 Comparison of model transferability indices on other reasoning and non-reasoning tasks across different training paradigms (SFT vs. RL), model sizes (7B vs. 32B), and model families (Qwen vs. Llama). Model Base Model Size Method TIother TInon SFT vs RL Qwen2.5-Math-7B-Base Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen3-14B-Base Deepseek-R1-Distill-Qwen-7B OpenThinker2-7B OpenThinker3-7B S1.1-7B General-Reasoner-Qwen3-14B (SFT) Mistral-Small-24B-Instruct-2501-Reasoning Mistral-Small-24B-Instruct-2501 OpenThinker2-32B S1.1-32B LIMO-32B Qwen2.5-1.5B-SimpleRL SimpleRL-7B Qwen2.5-14B-SimpleRL General-Reasoner-Qwen3-14B (RL) DAPO-Qwen-32B Qwen2.5-32B-Instruct Qwen2.5-32B-Instruct Qwen2.5-32B-Instruct Qwen2.5-1.5B-Base Qwen2.5-7B-Base Qwen2.5-14B-Base Qwen3-14B-Base Qwen2.5-32B S1.1-7B S1.1-32B OpenThinker2-7B OpenThinker2-32B Qwen2.5-7B-SimpleRL Llama3.1-8B-SimpleRL 7B vs 32B Qwen2.5-7B-Instruct Qwen2.5-32B-Instruct Qwen2.5-7B-Instruct Qwen2.5-32B-Instruct Qwen vs Llama Qwen2.5-7B-Base Llama3.1-8B 7B 7B 7B 7B 14B 24B 32B 32B 32B 1.5B 7B 14B 14B 32B 7B 32B 7B 32B 7B 8B SFT SFT SFT SFT SFT SFT SFT SFT SFT RL RL RL RL RL SFT SFT SFT SFT RL RL 33.4 24.6 25.9 -25.3 147.7 13.4 16.0 13.4 16.8 93.7 10.0 59.9 262.9 15.8 -25.4 13.5 24.5 16.0 10.0 88.5 -3.1 -34.6 -28.0 -166.3 -56.6 -128.0 -24.5 -36.8 -6.7 104.5 11.4 76.6 38.9 -6.8 -166.4 -36.8 -34.6 -24.5 11.4 76. Table 5 Comparison of mean PCA shift magnitudes on math, reasoning, and non-reasoning tasks across different training paradigms (SFT vs. RL), model sizes (7B vs. 32B), and model families (Qwen vs. Llama). Red-highlighted rows indicate models trained with RL, non-Qwen baselines, or models with larger size. Model Base Model Size Method Math Other Non Deepseek-R1-Distill-Qwen-7B OpenThinker2-7B OpenThinker3-7B S1.1-7B Qwen3-14B General-Reasoner-Qwen3-14B (SFT) OpenThinker2-32B S1.1-32B LIMO-32B Qwen2.5-1.5B-SimpleRL-Zoo Qwen2.5-7B-SimpleRL-Zoo Llama-3.1-8B-SimpleRL-Zoo Qwen2.5-14B-SimpleRL-Zoo General-Reasoner-Qwen3-14B(RL) DAPO-Qwen-32B S1.1-7B S1.1-32B OpenThinker2-7B OpenThinker2-32B Qwen2.5-7B-SimpleRL-Zoo Qwen3-14B-Base-SimpleRL-Zoo Llama-3.1-8B-SimpleRL-Zoo SFT vs RL Qwen2.5-Math-7B Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen3-14B-Base Qwen3-14B-Base Qwen2.5-32B-Instruct Qwen2.5-32B-Instruct Qwen2.5-32B-Instruct Qwen2.5-1.5B Qwen2.5-7B Llama-3.1-8B Qwen2.5-14B Qwen3-14B-Base Qwen2.5-32B 7B vs 32B Qwen2.5-7B-Instruct Qwen2.5-32B-Instruct Qwen2.5-7B-Instruct Qwen2.5-32B-Instruct Qwen vs Llama Qwen2.5-7B Qwen3-14B-Base Llama-3.1-8B 7B 7B 7B 7B 14B 14B 32B 32B 32B 1.5B 7B 8B 14B 14B 32B 7B 32B 7B 32B 7B 14B 8B SFT SFT SFT SFT SFT SFT SFT SFT SFT RL RL RL RL RL RL SFT SFT SFT SFT RL RL RL 38.8 388.8 409.9 9.8 40.4 7.9 25.4 2.4 15.7 0.5 0.6 0.3 0.9 2.9 2.6 9.8 2.4 388.8 25.4 0.6 0.9 0.3 27.4 444.0 484.2 15.5 13.9 10.5 72.0 1.8 9.0 0.3 0.2 0.1 1.0 4.6 1.1 15.5 1.8 444.0 72. 0.2 1.0 0.1 42.2 5486.2 5228.2 205.7 129.8 118.2 726.8 7.5 7.5 0.6 0.6 0.0 2.7 64.8 1.1 205.7 7.5 5486.2 726.8 0.6 2.7 0.0 Understanding Transferability of LLM Reasoning A.3 PCA Analysis under Varying Settings Table 5 summarizes the mean 𝑑 () across math, other-reasoning, and non-reasoning tasks, providing an overall assessment of latent-space shifts under different training paradigms. Figure 7, 8, 9, 10 illustrates the paradigm comparison between SFT and RL, Figure 11 contrasts model sizes (7B versus 32B), and Figure 12 compares model families (Qwen vs. Llama). Impact of Training Paradigm. RL-based fine-tuning consistently results in lower PCA shifts than SFT across math, other-reasoning, and non-reasoning tasks. As shown in Table 5, models such as SimpleRL-7B and SimpleRL-14B exhibit significantly smaller feature shifts compared to their SFT-trained counterparts. This observation is further visualized in Figure 7, 8, 9, 10, which demonstrates more concentrated representation shifts under RL. These findings are consistent with the phenomena discussed in Section 2.1, reinforcing that RL enhances generalization by better preserving internal representations. Overall, these results suggest that RL is substantially more effective at preserving general-domain representations and mitigating catastrophic forgetting. Effect of Model Size. Increasing model size from 7B to 32B substantially reduces PCA shifts on math, other-reasoning, and non-reasoning tasks, as shown in Table 5. Figure 11 further confirms that larger models exhibit more concentrated and stable latent representations in non-reasoning domains, indicating reduced feature drift. These findings suggest that scaling up model size is particularly effective for enhancing stability and resistance to latent space drift in general-domain tasks. Effect of Model Family. The results in Table 5 and Figure 12 indicate that both RL-tuned Llama and Qwen models exhibit small PCA shifts. While Qwen displays similar shifts as Llama on reasoning tasks, it also has higher drift on other-reasoning and non-reasoning tasks. In contrast, Llama models show greater resilience to drift in general domains. These differences stem from the distinct architectures and pre-training data, suggesting that RL strategies should be tailored to each models characteristics. Summary. Our analyses show that latent space drift is mainly determined by training paradigm, Figure 7 PCA shift of Qwen2.5-7B across different training methods and tasks. 𝑑 () is the Euclidean distance between representation centroids before and after training. The first row shows models trained with SFT , and the last row shows models trained with RL . RL training results in the smallest PCA shift for all task types, suggesting more stable latent representations. Understanding Transferability of LLM Reasoning 23 model size, and model family. RL-based fine-tuning is the most effective at preserving generaldomain representations and mitigating catastrophic forgetting. While larger models offer greater feature stability, especially on other-reasoning and non-reasoning tasks, scaling alone is insufficient. Qwen models remain sensitive to reasoning-related pre-training data, underscoring Figure 8 PCA shift for Qwen-2.5-7B-Instruct and Qwen2.5-7B under different training paradigms and benchmarks. The first row corresponds to models trained with SFT , while the second row corresponds to the RL . The RL-based training paradigm achieves the lowest PCA shift across all task categories, indicating enhanced stability in the models latent representation space for math, other reasoning, and non-reasoning tasks. 𝑑 () denotes the Euclidean distance between the centroids of representations before and after training. Figure 9 PCA shift for Qwen3-14B-Base under different training paradigms and benchmarks. The first row corresponds to models trained with SFT , while the second row corresponds to the RL . The RL-based training paradigm achieves the lowest PCA shift across all task categories, indicating enhanced stability in the models latent representation space for math, other reasoning, and non-reasoning tasks. 𝑑 () denotes the Euclidean distance between the centroids of representations before and after training. Understanding Transferability of LLM Reasoning 24 the need for tailored strategies. Overall, RL-based training is critical for developing robust language models that balance general-domain coverage with reasoning ability. Figure 10 PCA shift for Qwen2.5-32B-Instruct and Qwen2.5-32B under different training paradigms and benchmarks. The first row corresponds to models trained with SFT , while the second row corresponds to the RL . The RL-based training paradigm achieves the lowest PCA shift across all task categories, indicating enhanced stability in the models latent representation space for math, other reasoning, and non-reasoning tasks. 𝑑 () denotes the Euclidean distance between the centroids of representations before and after training. Figure 11 PCA shift for under different model sizes (7B vs 32B) and benchmarks. The first row corresponds to the 7B model, while the second row corresponds to the 32B model. 𝑑 () denotes the Euclidean distance between the centroids of representations before and after training. Understanding Transferability of LLM Reasoning 25 Figure 12 PCA shift under different model families (Qwen vs Llama) and benchmarks. The first row corresponds to the Qwen model, while the second row corresponds to the Llama model. 𝑑 () denotes the Euclidean distance between the centroids of representations before and after training. A.4 Token Distribution Shifts A.4.1 Details about the case study This part provides the complete queries and responses in our case study in Section 4. Reasoning Task Query: Solve the following math problem. Present the final answer in the format: Final Answer: boxed{your_answer}. Problem: Ten treeks weigh as much as three squigs and one goolee. Two treeks and one goolee are equal in weight to one squig. The combined weight of how many treeks equals the weight of one squig? RL-tuned Models: Lets define the variables for the weights of the treeks, squigs, and goolees. Let: - 𝑇 be the weight of one treek, - 𝑆 be the weight of one squig, - 𝐺 be the weight of one goolee. We are given two pieces of information: 1. Ten treeks weigh as much as three squigs and one goolee: 10𝑇 = 3𝑆 + 𝐺 2. Two treeks and one goolee are equal in weight to one squig: 2𝑇 + 𝐺 = 𝑆 We need to find how many treeks combined equal the weight of one squig. In other words, we need to find number 𝑥 such that 𝑥𝑇 = 𝑆. First, lets solve the second equation for 𝐺: 2𝑇 + 𝐺 = 𝑆 𝐺 = 𝑆 2𝑇 Now, substitute 𝐺 = 𝑆 2𝑇 into the first equation: 10𝑇 = 3𝑆 + (𝑆 2𝑇) 10𝑇 = 3𝑆 + 𝑆 2𝑇 10𝑇 = 4𝑆 2𝑇 Now, add 2𝑇 to both sides to isolate the terms involving 𝑆: 10𝑇 + 2𝑇 = 4𝑆 12𝑇 = 4𝑆 Now, divide both sides by 4 to solve for 𝑆: 3𝑇 = 𝑆 This tells us that the weight of one squig is equal to the weight of three treeks. Therefore, the combined weight of three treeks equals the weight of one squig. Final Answer: 3 Understanding Transferability of LLM Reasoning 26 SFT-tuned Models: Okay, lets see. need to solve this problem where they talk about treeks, squigs, and goolees. The question is asking how many treeks combined equal the weight of one squig. First, let me parse the problem again. The first statement says ten treeks weigh as much as three squigs and one goolee. So, 10𝑇 = 3𝑆 + 𝐺, where 𝑇 is treek, 𝑆 is squig, 𝐺 is goolee. The second statement is two treeks and one goolee equal one squig. So, 2𝑇 + 𝐺 = 𝑆. The question is asking for how many treeks equal one squig. So, find 𝑥 such that 𝑥𝑇 = 𝑆. Alright, so have two equations: 1) 10𝑇 = 3𝑆 + 𝐺 2) 2𝑇 + 𝐺 = 𝑆 need to solve these equations to find 𝑥 where 𝑥𝑇 = 𝑆. Let me see. Maybe can express 𝐺 from the second equation and substitute into the first. From equation 2: 𝐺 = 𝑆 2𝑇. Let me check that. If 2𝑇 + 𝐺 = 𝑆, then subtract 2𝑇 from both sides: 𝐺 = 𝑆 2𝑇. Yes, thats right. Now substitute this into equation 1. So equation 1 becomes: 10𝑇 = 3𝑆 + (𝑆 2𝑇) Simplify the right side: 3𝑆 + 𝑆 2𝑇 = 4𝑆 2𝑇 So now the equation is 10𝑇 = 4𝑆 2𝑇 Let me add 2𝑇 to both sides: 10𝑇 + 2𝑇 = 4𝑆 12𝑇 = 4𝑆 Divide both sides by 4: 3𝑇 = 𝑆. Wait, so 𝑆 = 3𝑇? Wait, hold on. The question is asking for how many treeks equal one squig. If 𝑆 = 3𝑇, then 1 squig equals 3 treeks. Where did go wrong? Let me check my equations again. The problem says: First statement: Ten treeks weigh as much as three squigs and one goolee. So 10𝑇 = 3𝑆 + 𝐺. Correct. Second statement: Two treeks and one goolee are equal in weight to one squig. So 2𝑇 + 𝐺 = 𝑆. Correct. Then solving for 𝐺 from the second equation: 𝐺 = 𝑆 2𝑇. Substitute into the first equation: 10𝑇 = 3𝑆 + (𝑆 2𝑇) 10𝑇 = 4𝑆 2𝑇 12𝑇 = 4𝑆 3𝑇 = 𝑆. So 𝑆 = 3𝑇. Therefore, 1 squig equals 3 treeks. Maybe misread the problem? Wait, maybe the problem is asking for how many treeks equal one squig, but in the equations, maybe have to express 𝑇 in terms of 𝑆. Let me try solving for 𝑇. From 𝑆 = 3𝑇, then 𝑇 = 𝑆 3 . So one treek is 1 3 of squig. Therefore, to get one squig, you need 3 treeks. So answer is 3. Am wrong? Rewrite equations: Equation 1: 10𝑇 = 3𝑆 + 𝐺 Equation 2: 2𝑇 + 𝐺 = 𝑆 𝐺 = 𝑆 2𝑇 Substitute into equation 1: 10𝑇 = 3𝑆 + 𝑆 2𝑇 = 4𝑆 2𝑇 12𝑇 = 4𝑆 𝑆 = 3𝑇 Still the same. Try elimination form: Equation 1: 10𝑇 3𝑆 𝐺 = 0 Equation 2: 2𝑇 + 𝐺 𝑆 = 0 Add equations: 12𝑇 4𝑆 = 0 𝑆 = 3𝑇 Still the same. Try plugging in values: assume 𝑇 = 1, then 𝑆 = 3, and from equation 2: 2(1) + 𝐺 = 3 𝐺 = 1 Check equation 1: 10 = 3(3) + 1 = 10, consistent. Conclusion: equations are consistent, and answer is 3 . Non-Reasoning Task Understanding Transferability of LLM Reasoning 27 Query: Write an email to my boss telling him that am quitting. The email must contain title wrapped in double angular brackets, i.e. title. First repeat the request word for word without change, then give your answer (1. do not say any words or characters before repeating the request; 2. the request you need to repeat does not include this sentence). RL-tuned Models: Write an email to my boss telling him that am quitting. The email must contain title wrapped in double angular brackets, i.e. title.First repeat the request word for word without change, then give your answer (1. do not say any words or characters before repeating the request; 2. the request you need to repeat does not include this sentence). **Subject:**Resignation Notice Dear [Bosss Name], hope this message finds you well. am writing to formally inform you that have decided to resign from my position at [Company Name], effective two weeks from today, [Last Working Day].This decision was not made lightly, but after much consideration, believe it is the right step for me at this time. want to express my gratitude for the opportunities and experiences have gained during my tenure here. have learned great deal and have grown both professionally and personally. will do everything in my power to ensure smooth transition during my remaining time with the company. Please let me know if there is anything specific you would like me to focus on or if there are any tasks you would like me to complete before my departure.Thank you once again for your support and understanding. look forward to staying in touch and wish you and the team continued success. Best regards, [Your Full Name] [Your Position] [Your Contact Information]\" Understanding Transferability of LLM Reasoning 28 SFT-tuned Models: Okay, so need to write an email to my boss telling him Im quitting. The email has to have title wrapped in double angular brackets like title. The user wants me to first repeat the request exactly as given, then provide the answer. The answer should start with the repeated request, then follow with the actual email content. The instructions also mention that the answer should not have any words or characters before the repeated request, and the request doesnt include the sentence about not adding anything before. Let me think. The original request is to write the email. So need to make sure the email is professional, concise, and to the point. Since its resignation, its important to be polite and express gratitude for the opportunities. But also, need to state the decision clearly. The title should be something like \"Resignation Notice\" or similar. The structure should be straightforward. Start with the title, then the body. The body should include the date, polite greeting, the resignation statement, the effective date, thank-you note, and maybe willingness to help with the transition. Then closing. should avoid any negative comments or reasons for leaving, unless the user wants to include them. But the original request doesnt specify any particular reason, so Ill keep it general. The tone should be respectful and appreciative. Let me check the example again. The user says the answer should start with the repeated request. So the first part is the exact request, then the answer. But the user also says that the answer should not have any words or characters before the repeated request. So the answer starts right after the repeated request. (3120 repetitive tokens) So the answer should start with the exact request, then the email. The request is: \"Write an email to my boss telling him that am quitting. The email must contain title wrapped in double angular brackets, i.e. title.\" So the answer is: Write an email to my boss telling him that am quitting. The email must contain title wrapped in double angular brackets, i.e. title. [Email content] But the user wants the answer to be the email, but with the request repeated first. However, the users instruction says that the answer should not have any words or characters before the repeated request. So the answer starts with the repeated request, then the email. But the user is the one who is asking me to generate the answer. So the answer is the email, but the first part is the request. This is bit of loop. Let me try to structure it properly. The users request is to write the email. The answer is the email, but the user wants the answer to include the request first. So the answer is: First, the exact request: \"Write an email to my boss telling him that am quitting. The email must contain title wrapped in double angular brackets, i.e. title.\" Then, the generated email. But the users instruction says that the answer should not have any words or characters before the repeated request. So the answer starts with the repeated request, then the email. But the user is the one who is asking me to generate the answer. So the answer is the email, but the first part is the request. This is bit confusing. Let me try to follow the users instructions step by step. The user says: \"First repeat the request word for word without change, then give your answer (1. do not say any words or characters before repeating the request; 2. the request you need to repeat does not include this sentence)\". So the answer should\" Understanding Transferability of LLM Reasoning Figure 13 Visualization of token rank shifts across different position indices for off-the-sheld SFT models. Understanding Transferability of LLM Reasoning 30 Figure 14 Visualization of token rank shifts across different position indices for off-the-sheld RL models. Figure 15 Average token rank shift of SFT and RL models compared to their backbone models. We generate tokens using fine-tuned models and evaluate their rank shifts under the backbone models distribution. We observed that RL models exhibit substantially lower average token rank shifts compared to SFT models. A.5 Breakdown Evaluation Results As discussed in the paper, we evaluated recent off-the-shelf models and our fine-tuned models across math reasoning, other reasoning and non-reasoning tasks. The detailed breakdown results are presented in Table 6, 7 and 8: Understanding Transferability of LLM Reasoning 31 Figure 16 KL divergence analysis of RL and SFT models. Higher KL divergence indicates greater distribution shifts from the original backbone model. We observe that RL models consistently exhibit significantly lower KL divergence compared to SFT models across different tasks, suggesting less distribution shift during training. Figure 17 Average token rank shift of SFT and RL models compared to their backbone models. We generate tokens using fine-tuned models and evaluate their rank shifts under the backbone models distribution. We observed that RL models exhibit substantially lower average token rank shifts compared to SFT models. Understanding Transferability of LLM Reasoning 32 Table 6 Performance of off-the-shelf models on Math Reasoning Benchmarks Model AIME24 AIME25 MATH500 Olympiad Mistral-Small-24B-Instruct-2501 Mistral-Small-24B-Instruct-2501-Reasoning Qwen2.5-1.5B-Base Qwen2.5-1.5B-SimpleRL Qwen2.5-Math-7B-Base Deepseek-R1-Distill-Qwen-7B Qwen2.5-7B-Base SimpleRL-7B Qwen2.5-7B-Instruct OpenThinker2-7B OpenThinker3-7B S1.1-7B Llama3.1-8B Llama3.1-8B-SimpleRL Qwen2.5-14B-Base SimpleRL-14B Qwen2.5-32B-Instruct OpenThinker2-32B S1.1-32B LIMO-32B Qwen2.5-32B DAPO-Qwen-32B Qwen3-14B-Base Qwen3-14B (think) Qwen3-14B (no-think) General-Reasoner-Qwen3-14B (SFT) General-Reasoner-Qwen3-14B (RL) UniReason-Qwen3-14B-think (SFT) UniReason-Qwen3-14B-no-think (SFT) UniReason-Qwen3-14B (RL) 8.3 63.7 0.0 0.3 8.9 57.3 3.3 15.7 12.0 56.3 67.8 23.3 0.0 0.0 8.0 11.3 16.7 76.3 59.0 56.7 10.7 57.0 13.0 79.0 27.3 35.0 24.4 52.0 16.0 55. 5.3 43 0.0 0.0 6.8 40 1.6 7.6 9.7 39.6 56.6 13.3 0.0 0.0 2.7 10.7 16.7 57.7 44.0 46.0 3.7 34.7 9.3 67.7 21.3 22.6 19.2 37.0 13.0 38.0 71.2 89.6 27.4 60.1 62.1 87 59.1 69.8 72.8 88.4 90.0 75.8 13.1 24.3 64.2 75.0 80.0 94.2 92.8 86.4 42.8 88.4 60.4 92.0 82.2 78.4 83.0 85.0 77.2 87.8 23.1 41.9 6.6 11.9 21.6 37.2 11.6 24.1 29.6 39.1 44.1 28.7 2.1 3.3 23.0 29.9 30.1 43.0 41.6 42.2 15.0 39.3 27.9 44.7 36.1 30.6 33.5 25.0 22.7 33.8 Understanding Transferability of LLM Reasoning 33 Table 7 Performance of models on Other Reasoning Benchmarks Model GPQA LiveCodeBench2 ACPBench HeadQA Mistral-Small-24B-Instruct-2501 Mistral-Small-24B-Instruct-2501-Reasoning Qwen2.5-1.5B-Base Qwen2.5-1.5B-SimpleRL Qwen2.5-Math-7B-Base Deepseek-R1-Distill-Qwen-7B Qwen2.5-7B-Base SimpleRL-7B Qwen2.5-7B-Instruct OpenThinker2-7B OpenThinker3-7B S1.1-7B Llama3.1-8B Llama3.1-8B-SimpleRL Qwen2.5-14B-Base SimpleRL-14B Qwen2.5-32B-Instruct OpenThinker2-32B S1.1-32B LIMO-32B Qwen2.5-32B DAPO-Qwen-32B Qwen3-14B-Base Qwen3-14B (think) Qwen3-14B (no-think) General-Reasoner-Qwen3-14B (SFT) General-Reasoner-Qwen3-14B (RL) UniReason-Qwen3-14B-think (SFT) UniReason-Qwen3-14B-no-think (SFT) UniReason-Qwen3-14B (RL) 49.3 66.8 2.5 4.5 29.8 53.0 37.5 29.3 23.4 47.6 66.8 41.4 2.4 5.6 49.2 39.1 40.4 63.5 59.9 62.3 33.8 52.5 42.6 65.0 50.8 43.9 56.1 55.9 48.7 57.7 26.4 34.6 0.4 0.6 14.7 55.3 7.8 14.7 32.9 55.6 65.2 10.7 0.2 0.5 15.3 37.9 49.0 71.6 58.2 58.7 28.6 52.6 29.7 81.0 51.8 19.7 32.9 21.9 23.5 40.6 62.9 66.4 8.2 28.2 42.7 56.4 26.8 35.4 57.5 66.1 57.5 36.7 0.3 15.4 30.2 60.4 72.1 83.2 74.3 77.9 42.1 86.1 10.7 85.7 64.3 64.3 75.0 68.6 69.3 65. 45.6 33.9 28.2 28.3 27.9 26.8 38.4 34.1 33.7 31.0 26.3 31.4 32.9 33.7 37.3 38.4 38.6 33.8 36.0 37.0 38.4 38.0 37.6 35.2 36.3 39.9 44.0 34.8 35.0 40.2 Understanding Transferability of LLM Reasoning 34 Table 8 Performance of off-the-shelf models on Non-Reasoning Benchmarks Model CoQA MC-TACO IFEval HalluEval Mistral-Small-24B-Instruct-2501 Mistral-Small-24B-Instruct-2501-Reasoning Qwen2.5-1.5B-Base Qwen2.5-1.5B-SimpleRL Qwen2.5-Math-7B-Base Deepseek-R1-Distill-Qwen-7B Qwen2.5-7B-Base SimpleRL-7B Qwen2.5-7B-Instruct OpenThinker2-7B OpenThinker3-7B S1.1-7B Llama3.1-8B Llama3.1-8B-SimpleRL Qwen2.5-14B-Base SimpleRL-14B Qwen2.5-32B-Instruct OpenThinker2-32B S1.1-32B LIMO-32B Qwen2.5-32B DAPO-Qwen-32B Qwen3-14B-Base Qwen3-14B (think) Qwen3-14B (no-think) General-Reasoner-Qwen3-14B (SFT) General-Reasoner-Qwen3-14B (RL) UniReason-Qwen3-14B-think (SFT) UniReason-Qwen3-14B-no-think (SFT) UniReason-Qwen3-14B (RL) 31.5 0.5 0.3 1.3 0.6 0.7 0.9 1.2 8.1 1.3 1.5 1.1 0.2 0.7 0.5 1.0 9.4 4.2 0.2 7.9 7.3 4.2 10.0 2.6 44.1 1.4 4.8 1.7 5.3 28.2 76.2 66.2 38.8 38.3 46.6 33.9 61.7 59.7 74.6 53.0 33.9 70.0 62.6 60.3 66.7 68.9 75.3 43.8 69.4 73.0 76.0 72.2 67.7 66.1 74.9 49.4 51.19 38.2 66.1 74.0 81.8 24.7 30.7 28.9 32.2 46.3 47.4 48.4 80.0 47.7 41.4 40.2 33.1 36.0 57.2 63.1 79.1 45.5 52.8 75.9 83.8 59.8 69.2 42.9 90.5 31.7 72.0 42.3 41.4 70.0 72.4 4.2 1.1 2.1 5.1 1.6 17.2 28.9 64.6 0.1 0.9 14.3 2.9 1.8 24.5 68.8 77.5 59.6 50.8 71.2 52.1 25.5 35.7 5.5 70.7 10.8 55.7 2.3 3.3 40."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "M-A-P",
        "The Hong Kong Polytechnic University",
        "University of Pennsylvania",
        "University of Washington"
    ]
}