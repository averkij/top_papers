{
    "paper_title": "An Empirical Study of GPT-4o Image Generation Capabilities",
    "authors": [
        "Sixiang Chen",
        "Jinbin Bai",
        "Zhuoran Zhao",
        "Tian Ye",
        "Qingyu Shi",
        "Donghao Zhou",
        "Wenhao Chai",
        "Xin Lin",
        "Jianzong Wu",
        "Chao Tang",
        "Shilin Xu",
        "Tao Zhang",
        "Haobo Yuan",
        "Yikang Zhou",
        "Wei Chow",
        "Linfeng Li",
        "Xiangtai Li",
        "Lei Zhu",
        "Lu Qi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The landscape of image generation has rapidly evolved, from early GAN-based approaches to diffusion models and, most recently, to unified generative architectures that seek to bridge understanding and generation tasks. Recent advances, especially the GPT-4o, have demonstrated the feasibility of high-fidelity multimodal generation, their architectural design remains mysterious and unpublished. This prompts the question of whether image and text generation have already been successfully integrated into a unified framework for those methods. In this work, we conduct an empirical study of GPT-4o's image generation capabilities, benchmarking it against leading open-source and commercial models. Our evaluation covers four main categories, including text-to-image, image-to-image, image-to-3D, and image-to-X generation, with more than 20 tasks. Our analysis highlights the strengths and limitations of GPT-4o under various settings, and situates it within the broader evolution of generative modeling. Through this investigation, we identify promising directions for future unified generative models, emphasizing the role of architectural design and data scaling."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 9 7 9 5 0 . 4 0 5 2 : r An Empirical Study of GPT-4o Image Generation Capabilities Sixiang Chen1, Jinbin Bai2, Zhuoran Zhao1, Tian Ye1, Qingyu Shi3, Donghao Zhou4, Wenhao Chai5, Xin Lin6, Jianzong Wu3, Chao Tang3, Shilin Xu3, Tao Zhang6, Haobo Yuan6, Yikang Zhou6, Wei Chow2, Linfeng Li2, Xiangtai Li3 , Lei Zhu1 , Lu Qi6 1The Hong Kong University of Science and Technology (GZ) 2National University of Singapore 3Peking University 4The Chinese University of Hong Kong 5University of Washington 6Wuhan University"
        },
        {
            "title": "Abstract",
            "content": "The landscape of image generation has rapidly evolved, from early GAN-based approaches to diffusion models and, most recently, to unified generative architectures that seek to bridge understanding and generation tasks. Recent advances, especially the GPT-4o, have demonstrated the feasibility of high-fidelity multimodal generation, their architectural design remains mysterious and unpublished. This prompts the question of whether image and text generation have already been successfully integrated into unified framework for those methods. In this work, we conduct an empirical study of GPT-4os image generation capabilities, benchmarking it against leading open-source and commercial models. Our evaluation covers four main categories, including text-to-image, image-to-image, image-to-3D, and image-to-X generation, with more than 20 tasks. Our analysis highlights the strengths and limitations of GPT-4o under various settings, and situates it within the broader evolution of generative modeling. Through this investigation, we identify promising directions for future unified generative models, emphasizing the role of architectural design and data scaling."
        },
        {
            "title": "Introduction",
            "content": "Over the past decade, image generation has undergone remarkable evolutionfrom the early successes of GANs [35] to the dominance of diffusion models [89, 82, 26], which have significantly advanced image fidelity and diversity [37, 7]. In parallel, Large Language Models (LLMs) have achieved exceptional performance across diverse natural language tasks by scaling autoregressive next-token prediction, demonstrating the power of unified modeling principles. These advances naturally raise compelling question: can such principles be extended to image generation? However, fundamental differences between autoregressive and diffusion-based paradigms present non-trivial challenges. Autoregressive models excel in sequential text generation, while diffusion models have become the de facto standard for high-quality image synthesis. Bridging these modalities within unified framework remains an open challenge. Several works [96, 101, 100, 34, 24, 13] attempt to bridge this gap via multimodal connectors or instruction tuning, with LLMs serving as planning modules that produce intermediate representations for image generation. While effective to some extent, these paradigms often exhibit limited interaction between text and image modalities, and struggle with content consistencyparticularly in image-to-image generation and complex instruction-based synthesis. To address these limitations, recent research explores unified generation models that integrate understanding and generation within single architecture, following three main technical paradigms. The first line of work represents both language and vision as discrete token sequences [67, 98, 110, 104, 19, 65, 109], leveraging VQGAN [28] or similar compressors to tokenize images for compatibility with autoregressive models. second direction integrates large language models directly into the diffusion process [128, 126, 112, 72], employing them as denoising backbones for image generation and as unified sequence models for text. While promising, these approaches *Equal contributions. (cid:0): schen691@connect.hkust-gz.edu.cn Corresponding authors. Preprint. Work in progress. typically rely on intermediate compression modules such as VAEs or VQVAEs, which may limit visual fidelity or increase architectural complexity. third and increasingly prominent paradigm investigates discrete diffusion frameworks that natively support both image and text generation within unified modeling space [71, 73, 93]. Building on this insight, recent works [58, 97] propose fully end-to-end diffusion architectures based on shared Transformer backbones, demonstrating competitive performance and seamless modality integration comparable to similarly sized LLMs. Despite these promising directions, such systems still lag behind the sophistication and generalization capabilities of proprietary models like Flux [51] and Midjourney [75], which may lack reasoning capabilities. The recent release of GPT-4o [78] marks significant milestone in multimodal generative modeling. As native multimodal architecture, GPT-4o demonstrates strong capabilities in generating high-fidelity, photorealistic images while seamlessly unifying vision and language generationreportedly in an autoregressive fashion. However, its closed-source natureparticularly the lack of disclosure about its architecture, training regimen, and inference mechanismsposes substantial challenges for scientific scrutiny. This motivates careful empirical assessment of its capabilities relative to open-source state-of-the-art models. Although the visual performance of GPT-4o and Gemini is widely recognized, much of their success likely stems from unprecedented scale in training data, model parameters, and compute resources. Prior studies, including diffusion models and connected-based models, suggest that scaling is key enabler of generative qualitypotentially more so than architectural novelty alone. These trends point to promising trajectory for unified generative models: with sufficient scale, they may rival or even surpass todays best proprietary systems. In this study, we conduct comprehensive evaluation of GPT-4os image generation performance, benchmarking its outputs against leading systems including Gemini 2.0 Flash Experimental [99] and other state-of-the-art models. Building upon our comparative evaluation across text-to-image, image-to-image, image-to-3D, and image-to-X generation tasks, GPT-4o demonstrates several distinctive strengths: Exceptional Text Rendering Capability. GPT-4o demonstrates exceptional capability in rendering textual elements within images, maintaining correct spelling, alignment, and formatting even in documentstyle generation tasks. This level of text fluency is rarely seen in prior models and is crucial for practical applications such as chart generation, document layout synthesis, and instruction-rich visual storytelling. Compositional Generalization and Prompt Following. GPT-4o displays impressive compositional abilities, accurately assembling complex scene elements, styles, or attributes described in prompts. This high prompt following enables it to handle fine-grained multi-attribute conditions in generation tasks with minimal loss of semantic detail. Spatial Reasoning and Multi-View Consistency. In generation tasks involving spatial manipulation, such as 3D view synthesis, camera control, and depth-conditioned rendering, GPT-4o maintains geometric consistency and viewpoint realism. This indicates an inherent capacity for spatial reasoning and structural awareness, even without explicit 3D modeling modules. Comprehensive Image Transformation Capability. GPT-4o shows strong generalization across wide spectrum of image-to-image tasks, ranging from low-level image restoration to high-level perceptual understanding. Without task-specific tuning, it almost handles diverse transformations such as denoising, deblurring, relighting, segmentation, and depth estimation. This suggests the model has learned robust visual priors and spatial semantics, enabling it to perform correction and abstract structural prediction under unified framework. However, limitations remain in inconsistent generation, hallucination, and data bias in underrepresented cultural elements and non-Latin scripts, highlighting current trade-offs in model design and training data coverage. While we do not analyze the internal architecture or implementation details of GPT-4o in this paper*, we believe it plays an important role toward unified multimodal generation. We also emphasize that model architecture is only one part of this progresstraining data, model scale, and optimization strategies are equally important. We hope future work will provide more empirical evidence to better understand such proprietary systems and their position within this evolving research landscape. *There is currently no definitive evidence regarding the specific implementation details or architectural design of GPT-4os image generation capabilities. To ensure the credibility and accuracy of our analysis, we will refrain from making speculative claims in current version."
        },
        {
            "title": "2 Evaluation",
            "content": "As GPT-4os image generation capability has only recently been released and no API is available, we conduct only qualitative comparisons between GPT-4o, Gemini 2.0 Flash [99], and other state-of-the-art models in their respective domains. To systematically compare these models performance across diverse image generation tasks including text-toimage generation, image-to-image generation, text/image to 3D generation, and various image-to-X generation, we conduct detailed case study focused on analyzing the performance of these models. This qualitative analysis provides insight into gpt 4os strengths and limitations in various tasks, as shown in Table 1. Low Visual Quality : The image synthesis model fails to generate fine-grained object details or produces blurry outputs. Typical cases include distorted human bodies or unrealistic hand shapes. Inconsistent Generation : The image synthesis model produces inconsistent output or image details with input image. Lack of Knowledge : The image synthesis model lacks domain-specific knowledge, such as particular artistic styles, and thus generates visually plausible but incorrect results. Failure to Follow Instructions : The image synthesis model misinterprets the input prompt and produces inconsistent results. For example, it may fail to capture specified numbers, colors, or object arrangements. 3 Table 1: GPT-4o vs. Baselines: Qualitative error analysis across image generation tasks. Case Figure Meta-task Sub-task Figure Figure 2 Figure 3 Figure 4 Figure 5 Figure 6 Figure Figure 8 Figure 9 Figure 10 Figure 11 Figure 12 Figure Figure 14 Figure 15 Figure 16 Figure 17 Figure 18 Figure Figure 20 Figure 21 Figure 22 Figure 23 Figure 24 Figure Figure 26 Figure 27 Figure 28 Figure 29 Figure 30 Figure Figure 32 Figure 33 Figure 34 Figure 35 Figure 36 Figure Figure 38 Figure 39 Figure 40 Figure 41 Figure 42 Figure Figure 44 Figure 45 Figure 46 Figure 47 Figure 48 Figure Figure 50 Figure 51 Figure 52 Figure 53 Figure 54 Figure Figure 56 Figure 57 Figure 58 Figure 59 Figure 60 Figure Figure 62 Figure 63 Complex Text Following Text-to-Image Text Rendering Document Generation Panorama Style Transfer Image Editing Single-Concept Customization Multi-Concept Customization Story Image Generation Low-Level Vision-Denoising Low-Level Vision-Deraining Low-Level Vision-Dehazing GPT-4o Success Success Success Success Success Success Success Success Success Success Lack of Knowledge Success Success Gemini-2.0-flash Domain-SOTA Failure to Follow Instructions Failure to Follow Instructions Failure to Follow Instructions Failure to Follow Instructions Success Success Success Low Visual Quality Low Visual Quality Low Visual Quality Low Visual Quality Low Visual Quality Success Lack of Knowledge Lack of Knowledge Success Success Success Low Visual Quality Low Visual Quality Low Visual Quality Low Visual Quality Low Visual Quality Success Lack of Knowledge Lack of Knowledge Low Visual Quality Success Failure to Follow Instructions Failure to Follow Instructions Failure to Follow Instructions Failure to Follow Instructions Failure to Follow Instructions Failure to Follow Instructions Failure to Follow Instructions Failure to Follow Instructions Failure to Follow Instructions Failure to Follow Instructions Inconsistent Generation Failure to Follow Instructions Inconsistent Generation Inconsistent Generation Success Success Success Success Success Success Success Low Visual Quality Success Success Failure to Follow Instructions Failure to Follow Instructions Inconsistent Generation Low Visual Quality Inconsistent Generation Low Visual Quality Low Visual Quality Low Visual Quality Low Visual Quality Inconsistent Generation Success Success Failure to Follow Instructions Success Success Success Success Success Success Success Success Success Success Success Success Success Success Success Success Success Success Success Success Success Success N/A Image-to-Image Low-Level Vision-Low Light Enhancement Low Visual Quality Low-Level Vision-Debluring Low-Level Vision-Super Resolution Success Success Low-Level Vision-Inpainting Low-Level Vision-Outpainting Low-Level Vision-Colorization Low-Level Vision-Shadow Removal Inconsistent Generation Inconsistent Generation Success Success Low-Level Vision-Reflection Removal Inconsistent Generation Failure to Follow Instructions Low-Level Vision-Relighting Success Failure to Follow Instructions Spatial Control-Canny Spatial Control-Depth Spatial Control-Sketch Spatial Control-Pose Spatial Control-Mask Camera Control Inconsistent Generation Failure to Follow Instructions Success Failure to Follow Instructions Inconsistent Generation Success Inconsistent Generation Inconsistent Generation Inconsistent Generation Failure to Follow Instructions Failure to Follow Instructions Failure to Follow Instructions Inconsistent Generation Failure to Follow Instructions Inconsistent Generation In-Context Visual Prompting Failure to Follow Instructions Failure to Follow Instructions Image to 3D Modeling Image-to-3D UV Map to 3D Rendering Novel View Synthesis Image Segmentation Edge Detection Salient Object Depth Estimation Normal Estimation Layout Detection Text Detection Object Tracking Image-to-X Success Success Success Failure to Follow Instructions Failure to Follow Instructions Inconsistent Generation Failure to Follow Instructions Success Failure to Follow Instructions Failure to Follow Instructions Failure to Follow Instructions Success Success Success Success Success Success Success Success Success Success Failure to Follow Instructions Failure to Follow Instructions Success Failure to Follow Instructions Failure to Follow Instructions Failure to Follow Instructions Success Success Failure to Follow Instructions Failure to Follow Instructions Inconsistent Generation Inconsistent Generation Failure to Follow Instructions Failure to Follow Instructions Inconsistent Generation Inconsistent Generation Inconsistent Generation Inconsistent Generation Inconsistent Generation Inconsistent Generation Inconsistent Generation Inconsistent Generation Success Success Success Success Success Success Success Success Success Success Success Success Success Success Success Success Success"
        },
        {
            "title": "2.1.1 Complex Text Following Capability",
            "content": "Recent progress in text-to-image generation has shown impressive abilities in generating diverse and realistic images based on text prompts. However, composing multiple objects with various attributes and relationships accurately into one scene remains significant challenge for current text-to-image generative models [92, 85, 8, 81, 6]. In this section, we assess models ability for compositional text-to-image generation from four perspectives following [41], which include attribute binding, numeracy, object relationship, and complex compositions. Attribute binding evaluates whether the model correctly assigns attributes, such as color, shape, and texture to the appropriate objects. Numeracy evaluates whether the number of generated objects matches the quantities specified in the prompt. Object relationships refer to both spatial (2D/3D) and non-spatial interactions among objects. Complex compositions evaluate the models ability to handle multiple types of constraints simultaneously, especially given long or detailed prompts. As shown in Figure 1 row 1, GPT-4o outperforms both Gemini 2.0 Flash and Midjourney in numeracy tasks. While GPT-4o accurately represents single plate, Gemini 2.0 and Midjourney represent two plates instead. In terms of understanding object relationships, GPT-4o is the only model that correctly infers the action walk towards from the ragdoll to the labrador. However, GPT-4o struggles with more complex terms like pentagonal pyramid, failing to interpret it correctly (see Figure 1 row 4). This suggests that GPT-4o may have difficulty accurately interpreting objects with unusual geometries. When it comes to abstract prompts, GPT-4o also appears to lack imagination (see Figure 2 row 2), whereas Midjourney v6.1 demonstrates better creativity in this case, outperforming both GPT-4o and Gemini 2.0 Flash. For complex text-to-image generation, we evaluate GPT-4os performance with Gemini 2.0 Flash [99] and FLUX.1Pro [51], using the text prompts collected from [124, 106, 115]. As shown in Figure 3, both GPT-4o and FLUX excel at generating realistic and harmonious scenes align with the text prompts. However, we observe that GPT-4o shows limitations in generating culturally related elements. For example, the generated crown for the Chinese general is western-style rather than chinese-style (see Figure 4 row 2). Additionally, in large scene generation, GPT-4o struggles to maintain boundary continuity, whereas FLUX produces more natural composition (see Figure 4 row 3). Overall, we conclude that GPT-4o excels at text-to-image generation in terms of attribute binding, generative numeracy, object relationship, and complex compositions. However, it exhibits limitations in generating uncommon objects, culturally specific elements and in maintaining continuity when composing large scenes. 5 Figure 1: Task: Compositional text-to-image generation. Evaluate the image-text alignment on attribute binding, numeracy, and object relationship. Setup: Each row shows text prompt and the generated outputs from GPT-4o, Gemini 2.0 Flash [99], and Midjourney v6.1 [75]. Observation: GPT-4o outperforms Gemini 2.0 Flash and Midjourney v6.1 across all aspects. However, GPT-4o struggles with uncommon objects with special geometry. 6 Figure 2: Task: Compositional text-to-image generation. Evaluate the image-text alignment on attribute binding and complex compositions. Setup: Each row shows text prompt and the generated outputs from GPT-4o, Gemini 2.0 Flash [99], and Midjourney v6.1 [75]. Observation: GPT-4o outperforms the other two models in generating objects aligned with the text prompts accurately. But for more abstract and creative tasks, Midjourney v6.1 performs the best. 7 Figure 3: Task: Compositional text-to-image generation. Evaluate the image-text alignment on complex compositions. Setup: Each row shows text prompt and the generated outputs from GPT-4o, Gemini 2.0 Flash [99], and FLUX.1-Pro [51]. Observation: GPT-4o and FLUX can generate more harmonious and natural scene than Gemini 2.0 Flash. 8 Figure 4: Task: Compositional text-to-image generation. Evaluate the image-text alignment on complex compositions. Setup: Each row shows text prompt and the generated outputs from GPT-4o, Gemini 2.0 Flash [99], and FLUX.1-Pro [51]. Observation: GPT-4o struggles to generate culturally related elements and maintain boundary continuity (see rows 2 and 3), similar to Gemini 2.0 Flash and FLUX."
        },
        {
            "title": "2.1.2 Text Rendering",
            "content": "Text rendering is task that aims at generating texts (characters, sentences, or even paragraphs) on an image. The text content is usually guided by the input prompt. Previous models [27, 2] show good capability in generating short text (within 10 words, such as signs or short phrases), but their ability to generate long texts remains limited. As shown in Figure 5, GPT-4o demonstrates comparable abilities to existing state-of-the-art (SOTA) baselines when generating short texts. All the methods except FLUX [51] perform well at rendering short text following the prompt. In this section, we primarily focus on long text rendering to examine whether GPT-4o can surpass these baselines for extended textual content. We choose POSTA [12], Gemini 2.0 Flash [99], Ideogram 3.0 [2], and Playground-v3 [64] as the baselines because of their established capabilities in rendering longer texts. The results are shown in Figure 6 and Figure 7. From these examples, we make the following key observations: GPT-4os strength in long text generation: Compared with other baselines, GPT-4o demonstrates superior ability to generate long, coherent text. In example 1 and example 3, GPT-4o produces detailed textual information with fewer than three characters generated incorrectly across more than 100 characters of text. Baseline limitations: When the input prompt becomes extremely long, models such as Gemini 2.0 Flash, Ideogram 3.0, and Playground-v3 often produce significantly more errors or produce vague text patches that are difficult to recognize. POSTAs performance: As model specifically designed for poster-style text generation, POSTA performs closely to, or in some instances slightly more precisely than, GPT-4o. We hypothesize this is due to its multi-step pipeline tailored for long text rendering. Overall, we conclude that GPT-4o excels at long text rendering, offering overwhelming performance compared to most existing commercial models, and delivering results on par with the latest specialized research models. 10 Figure 5: Task: Short text rendering. Generate prompt-aligned, concise textual content (typically within 10 words) on an image. Setup: Each sample is produced based on guiding text prompt. Comparisons are made with prior SOTA models [27, 2] and FLUX [51]. Observations: GPT-4o achieves performance on par with existing SOTA baselines in rendering short texts, consistently following the prompt with minimal errors. All evaluated methodsexcept FLUX [51]deliver high-fidelity results in this setting. 11 Figure 6: Task: Long text rendering. Generate extended, coherent, and prompt-consistent textual content on an image. Setup: Evaluations are conducted against advanced baselines including POSTA [12], Gemini 2.0 Flash [99], Ideogram 3.0 [2], and Playground-v3 [64]. Observations: GPT-4o excels in long text rendering by producing coherent, detailed textual information with very few character errors. In contrast, models like Gemini 2.0 Flash, Ideogram 3.0, and Playground-v3 often exhibit increased errors or generate vague text when faced with lengthy prompts, while POSTAs tailored multi-step pipeline sometimes yields competitive precision. Overall, GPT-4o outperforms most commercial models and rivals specialized research approaches in extended text generation. 12 Figure 7: Task: Long text rendering. The Setup and Observations are the same as Figure 6."
        },
        {
            "title": "2.1.3 Document Generation",
            "content": "We also explore novel task: document image generation with GPT-4o, comparing its performance with Gemini 2.0 Flash [99] and Playground-v3 [64]. As shown in Figure 8 - 10, GPT-4o produces document images with cleaner layouts and more consistent content. Figure 8: Task: Document image generation. Setup: Each row shows text prompt and the generated outputs from GPT-4o, Gemini 2.0 Flash [99], and Playground-v3 [64]. Observation: GPT-4o can generate more consistent and accurate font and format than the other two models. 14 Figure 9: Task: Document image generation. The Setup and Observations are the same as Fig. 8. 15 Figure 10: Task: Document image generation. The Setup and Observations are the same as Fig. 8."
        },
        {
            "title": "2.1.4 Panorama Image Generation",
            "content": "Panorama image generation aims at creating 360-degree view of static scene, enabling immersive and comprehensive visual experiences. In our experiments, we select Pano-SD [119] and Gemini 2.0 Flash [99] as the baselines, with representative results illustrated in Figure 11. The comparisons reveal that while the baseline models can generate coherent panorama-like images with seamlessly connectable left and right sides, GPT-4o struggles to produce true panorama. In most cases, GPT-4o generates images that approximate panoramic view but still fall short in ensuring the necessary continuity across the image boundaries. We attribute this limitation to the insufficient representation of panorama images in its training data, as well as predisposition towards generating images with higher vertical aspect ratio rather than wider one. Consequently, in the realm of panorama image generation, GPT-4o is inferior to the existing baseline models. Figure 11: Task: Panorama image generation, aiming to create immersive 360-degree views of static scenes. Setup: We compare GPT-4o with established baselines such as Pano-SD [119] and Gemini 2.0 Flash [99] to evaluate the generation of coherent panoramic images. Observations: While the baseline models reliably produce panoramas with seamlessly connected left and right sides, GPT-4o tends to only approximate panoramic view and struggles to maintain continuity across image boundaries. This shortfall is likely due to limited panorama image representation in its training data and tendency to generate images with higher vertical aspect ratio rather than wider one, rendering it inferior to the baselines in this task. 17 2.2 Image-to-Image Tasks"
        },
        {
            "title": "2.2.1 Style Transfer",
            "content": "Style transfer is classic yet evolving task in computer vision, aiming to render an image in specific artistic style while preserving the original content. It bridges the domains of vision and art, enabling applications such as digital artwork creation, film post-production, and virtual reality environment design. Early approach [33] used convolutional neural networks to separate and recombine content and style representations from images. This seminal work enabled the artistic stylization of photographs by optimizing pixel values to match desired style. To improve efficiency, Johnson et al. [47] proposed feed-forward networks for real-time style transfer using perceptual losses. Later methods such as AdaIN [43] and WCT [57] enabled arbitrary style transfer without retraining for each new style. Transformer-based models like StyTr2 [23] have been introduced to enhance style transfer quality and better preserve structural details. More recently, with the rapid development of image synthesis techniques, especially diffusion models, style transfer has seen further advancements in both quality and controllability. However, transferring specific artistic styles still typically requires non-trivial amount of training data. To comprehensively evaluate the style transfer capability of GPT-4o, we conduct comparisons against several recent competitive models, including Gemini 2.0 Flash [99] and Midjourney v6.1 [75]. Specifically, Figure 12 illustrates style transfer results for natural scenes, while Figure 13 focuses on human facial images. Across diverse range of styles, such as Monet, Van Gogh, Pixar, Cyberpunk, Snoopy, Disney, Ghibli, and Cubism, GPT-4o demonstrates consistently superior performance in both stylistic fidelity and content preservation. Notably, in the case of Ghibli style transfer, GPT-4o exhibits remarkable fidelity to the original artistic aesthetics, closely resembling the target style with vivid color palettes and soft contours. In contrast, both Gemini and Midjourney often produce inconsistent visual styles and textures. Furthermore, GPT-4o excels at preserving finegrained content details, such as facial structure, earrings, clothing, and hairstyles, which are often misrepresented or lost in the outputs of other models. These results suggest that GPT-4o not only captures high-level style semantics but also maintains strong spatial consistency and semantic alignment. 18 Figure 12: Task: Style transfer, aiming to render an image in specific artistic style while preserving the original content. Setup: We compare GPT-4o with Gemini 2.0 Flash [99] and Midjourney v6.1 [75] on natural scene style transfer across multiple artistic domains. Observations: GPT-4o exhibits significantly better content preservation compared to Midjourney v6.1, maintaining fine-grained content details and structural consistency. In terms of style, it faithfully adheres to the textual description, effectively rendering vivid color palettes and soft contours that characterize the target style. This alignment notably surpasses both Gemini 2.0 Flash and Midjourney v6.1, highlighting GPT-4os strong capabilities in preserving content and faithfully rendering diverse styles. Figure 13: Task: Style transfer, aiming to render an image in specific artistic style while preserving the original content. Setup: We compare GPT-4o with Gemini 2.0 Flash [99] and Midjourney v6.1 [75] on human face style transfer across multiple artistic domains. Observations: GPT-4o exhibits significantly better content preservation compared to Gemini 2.0 Flash and Midjourney v6.1, maintaining fine-grained content details and structural consistency. In terms of style, it faithfully adheres to the textual description, effectively rendering vivid color palettes and soft contours that characterize the target style. This alignment notably surpasses both Gemini 2.0 Flash and Midjourney v6.1 far away, highlighting GPT-4os strong capabilities in preserving content and faithfully rendering diverse styles."
        },
        {
            "title": "2.2.2 Image Editing",
            "content": "Image editing involves modifying the visual elements, composition, or data of an image to achieve desired outcome. This process can range from minor refinements to significant alterations, while maintaining the integrity of the original image. Over time, image editing techniques have evolved from manual, labor-intensive methods to sophisticated AI-driven approaches. Prior works [10, 30, 9, 120, 5, 29, 4, 40] have demonstrated the ability to perform various editing tasks based on textual instructions, such as adding, removing, or replacing objects; altering backgrounds, colors, or styles; and adjusting the number, size, or positions of objects. However, these models still exhibit limitations in certain scenarios, particularly in preserving non-edited regions, maintaining consistent image characteristics, and ensuring seamless blending between edited and non-edited areas. We compare GPT-4o with MGIE [30], LEDITS++ [9], MagicBrush [120], and Gemini 2.0 Flash [99], which are representative of current SOTA methods. These experiments evaluate GPT-4os subject preservation and instruction-following capabilities to determine its effectiveness compared with existing methods. Comparative results are shown in Figure 14 through Figure 19. We find that GPT-4o achieves performance comparable to, and in many cases surpassing, SOTA baselines in image editing tasks. From these examples, GPT-4o exhibits the fewest failure cases, demonstrating strong generalization ability across wide variety of editing tasks. It consistently outperforms baseline models across multiple editing scenarios. We highlight several key observations: Strengths of GPT-4o in image editing: Fine-grained editing: GPT-4o shows superior ability to handle fine-grained editing tasks. For instance, in example 2 of Figure 14 and example 1 of Figure 15, GPT-4o successfully modified small, detailed objects such as toothpick and pink ballerina slippers, outperforming prior methods. Substantial image transformations: GPT-4o excels at large-scale edits, such as background changes or object transformations, while maintaining visual coherence and realism. These complex edits require robust contextual and semantic understanding. Example 1 in Figure 16 illustrates GPT-4os effective handling of major background alteration task. Subject preservation: GPT-4o demonstrates strong subject-preserving capabilities, avoiding common artifacts such as facial distortions or component loss. In example 2 of Figure 14, GPT-4o retains the content of drink that Gemini 2.0 Flash erroneously altered. Similarly, in example 5 of Figure 19, GPT-4o best preserves fuselage patterns and textual markings on an airplane. Instruction and original image adherence: GPT-4o shows notable ability to follow instructions and maintain the structure of the original image, particularly in style editing and tasks involving object quantity, size, or position. This likely stems from its advanced understanding of both the image content and the editing instructions. For example, Figure 18 demonstrates GPT-4os capability in style translation. Example 2 in Figure 17 shows its understanding of the term orange in both textual and visual contexts. similar ability is illustrated in example 4 of Figure 19. Limitations of GPT-4o in image editing: GPT-4o underperforms in scenarios where strict preservation of the original images lighting, shading, and color tones is required. In such cases, the edited images may exhibit noticeable shifts in visual consistency. This is evident in examples 1 and 5 of Figure 14 and example 4 of Figure 15. In some cases, GPT-4o may fail to retain image details outside the intended edit region. For instance, example 4 in Figure 14 shows degradation in image quality in non-targeted areas. In summary, GPT-4o demonstrates substantial advancements in image editing, showing exceptional capabilities in detailed and large-scale edits, subject preservation, and adherence to instructions. While there are limitations in strictly maintaining original image characteristics such as lighting and tonal consistency, GPT-4o significantly reduces failure cases and outperforms existing baselines across wide range of editing tasks, pushing the boundaries of current SOTA performance. 21 Figure 14: Task: Image editing for modifying visual elements and composition. Setup: GPT-4o vs. Gemini 2.0 Flash [99]/MGIE [30]. Observations: GPT-4o achieves higher success rates than MGIE (examples 2/5) but occasionally alters unintended elements (bread in example 4) or lighting/shading structures (example 5). This likely stems from stronger generalization capacity and creative adaptation focus in training, though reduced fidelity suggests insufficient constraints on structural details during fine-tuning. 22 Figure 15: Task: Image editing for modifying visual elements and composition. Setup: GPT-4o vs. Gemini 2.0 Flash [99]/MGIE [30]. Observations: From examples 1-3, GPT-4o shows higher success in fine detail edits and large-scale edits with occlusions. This likely stems from GPT-4os stronger contextual understanding and ability to infer missing or obscured elements, enabling more precise localized edits and coherent large-scale modifications even with partial visibility. However, it sometimes erases non-target elements (e.g., the house in example 5) and significantly alters global lighting (example 4). 23 Figure 16: Task: Image editing for modifying visual elements and composition. Setup: GPT-4o vs. Gemini 2.0 Flash [99]/MGIE [30]. Observations: From Example 1, GPT-4o demonstrates superior performance in style editing, effectively interpreting style instructions and preserving global image structurea capability lacking in baseline models (MGIE, Gemini 2.0 Flash, and MagicBrush, as will be shown later). This likely stems from its stronger cross-modal comprehension and structural awareness during training. 24 Figure 17: Task: Image editing for modifying visual elements and composition. Setup: GPT-4o vs. Gemini 2.0 Flash [99]/LEDITS++ [9]/MagicBrush [120]. Observations: From Examples 2 and 3, GPT-4o demonstrates stronger comprehension of instructions involving the oranges on the shelf and the water the elephants are walking through, translating this understanding into more accurate edits. This suggests better grounding of textual prompts in visual context during generation. 25 Figure 18: Task: Image editing for modifying visual elements and composition. Setup: GPT-4o vs. Gemini 2.0 Flash [99]/MagicBrush [120]. Observations: This set of examples further demonstrates GPT-4os robust capabilities in style editing and background modification, consistent with the findings previously presented in Figure 16. 26 Figure 19: Task: Image editing for modifying visual elements and composition. Setup: GPT-4o vs. Gemini 2.0 Flash [99]/MagicBrush [120]. Observations: Example 4 highlights GPT-4os superior image understandingaccurately distinguishing between hair and scarf (where MagicBrush fails) to execute the edit. In Example 5, its precise retention of the planes logo and text further demonstrates robust object-preservation capabilities."
        },
        {
            "title": "2.2.3 Customization",
            "content": "Customization, also known as subject-driven generation or personalization, aims to enable visual generative models to generate visual concepts from given reference images. Initial methods [31, 91] have achieved this by optimizing text embeddings or model weights. Subsequent approaches [50, 36, 46, 125, 94, 129] expanded on these approaches to handle multiple visual concepts. Customization plays crucial role in making visual generative models more flexible and applicable across diverse domains. By empowering models to adapt to user-provided inputs, it ensures outputs are tailored to specific visual concepts. This is particularly significant in industries such as artistic creation and advertising, where individualization and creativity are paramount. To evaluate the performance of GPT-4o in this challenging task, we collect reference images from previous relevant works [130, 103], and conduct qualitative comparisons as shown in Figure 20 and Figure 21. For single-concept customization, we compare GPT-4o with Gemini 2.0 Flash and DisEnvisioner [130]. The results demonstrate that GPT-4o not only faithfully reproduces the visual concept from the reference image but also accurately adheres to the given textual description. In this task, GPT-4o significantly outperforms Gemini 2.0 Flash and achieves performance on par with the SOTA customization method. However, the images generated by GPT-4o still exhibit some copy-paste artifacts, leaving room for further improvement in the future. For multi-concept customization, we compare GPT-4o with Gemini 2.0 Flash and MS-Diffusion [103]. In this task, GPT-4o can still achieve competitive results for customizing multiple visual concepts in different contexts. Unfortunately, it struggles with certain unique combinations (e.g., making dog wear human dress), which could be attributed to the lack of relevant customization training data. Overall, GPT-4o demonstrates impressive performance in both single-concept and multi-concept customization tasks, showcasing strong concept fidelity and great text alignment. Despite some limitations, GPT-4o achieves remarkable results on par with SOTA customization methods and outperforms Gemini 2.0 Flash. 28 Figure 20: Task: Single-concept customization. The goal is to generate images that faithfully reproduce single visual concept from reference images while aligning with given textual description. Setup: Reference images are collected from prior works [130], and results are compared across GPT-4o, Gemini 2.0 Flash [99], and DisEnvisioner [130]. Each row includes the input reference image, text prompt, and the corresponding outputs. Observations: GPT-4o demonstrates strong performance in faithfully reproducing the single visual concept with high fidelity while adhering closely to the given textual description. It consistently outperforms Gemini 2.0 Flash and achieves results comparable to the SOTA method DisEnvisioner. However, some generated images still exhibit minor copy-paste artifacts, indicating room for further improvement. Figure 21: Task: Multi-concept customization. The goal is to generate images that effectively combine multiple visual concepts from reference images while aligning with given textual description. Setup: Reference images are collected from prior works [103], and results are compared across GPT-4o, Gemini 2.0 Flash [99], and MS-Diffusion [103]. Each row includes the input reference images, text prompt, and the corresponding outputs. Observations: GPT-4o achieves competitive results in combining multiple visual concepts, showing strong fidelity to individual concepts and alignment with text prompts. However, its performance declines with unique or complex combinations. Despite this, GPT-4o outperforms Gemini 2.0 Flash and achieves results on par with SOTA methods."
        },
        {
            "title": "2.2.4 Story Image Generation",
            "content": "Story image generation is task to generate coherent stories based on input text narratives. The conditions may also include the first story frame or character images. We choose Gemini 2.0 Flash [99], StoryDiffusion [38], SEED-Story [111], and DiffSensei [108] as baselines, due to their proven ability to generate coherent and expressive story images and their public availability. The results are shown in Figure 22 and Figure 23. In the first example, GPT-4o and StoryDiffusion successfully generate three-panel short story about fisherman, whereas Gemini 2.0 Flash fails by producing single panel that appears to combine the three story narratives. In the second example, the story narrative is longer, spanning 11 panels. To evaluate this scenario with GPT-4o, we instruct the model to generate story images sequentiallyusing the input image and all previously generated images along with the corresponding text prompts. As shown in the figure, GPT-4o is capable of generating long story with consistency. In the final example, we examine Japanese black-and-white manga style with multiple input character images. GPT-4o is able to generate coherent stories, though it exhibits minor errors in character consistency (notably with the depiction of the woman) and misalignment with the input narrative (the narrative requires 7 panels, but only 6 are generated). The baseline Gemini 2.0 Flash performs worse, failing to preserve character status and the correct number of panels, as it also produces only 6 panels. Conversely, the DiffSensei model demonstrates superior performance, likely due to its specialized design and training for Japanese black-and-white manga generation. In conclusion, while GPT-4o achieves comparable performance to current baselines in story image generation, it shows limitations in specific scenariossuch as Japanese black-and-white manga and precise character status preservationwhen compared to methods specifically tailored for those tasks. 31 Figure 22: Task: Story image generation. The goal is to generate coherent story sequences based on narrative text, optionally conditioned on initial story frames or character images. Setup: Each example combines an input narrative (and, when available, reference character images) with series of generated story panels. We compare outputs from GPT-4o against Gemini 2.0 Flash [99], StoryDiffusion [38], and SEED-Story [111]. Observations: GPT-4o exhibits strong narrative coherence and panel continuity, matching or surpassing general baselines. Figure 23: Task: Story image generation. The goal is to generate coherent story sequences based on narrative text, optionally conditioned on initial story frames or character images. Setup: Each example combines an input narrative (and, when available, reference character images) with series of generated story panels. We compare outputs from GPT-4o against baselines including Gemini 2.0 Flash [99] and DiffSensei [108]. Observations: GPT-4o shows minor shortcomings in precise character consistency and panel count in specialized contexts, such as Japanese black-and-white manga, where dedicated models like DiffSensei deliver superior performance."
        },
        {
            "title": "2.2.5 Low-level Vision",
            "content": "Low-level vision tasks aim to enhance the basic quality or detail of visual content by improving various aspects of an image. Initial methods often focused on optimizing single tasks, such as super-resolution [88, 95], denoising [61, 63, 55], restoration [60, 20, 62, 84, 15, 16, 17], color adjustment [59], and more [22, 66, 116, 1, 122]. As the technology progressed, subsequent approaches expanded these techniques to handle multiple low-level tasks simultaneously, which is called universal image restoration. Low-level tasks play critical role in image generation and editing, allowing visual generative models to provide higher-quality outputs in real-world applications. By enabling models to adapt to diverse inputs, they ensure that the generated images perform well across different visual tasks. This is especially important in areas such as image restoration and video enhancement, where high-precision visual content optimization is crucial, such as in film post-production and autonomous driving. We evaluate the performance of GPT-4o in this challenging task. Firstly, for some image restoration tasks, such as super resolution, denoising, deraining, low-light enhancement, deblurring and dehazing. We collect reference images from previous relevant works Gemini 2.0 Flash and universal image restoration model, InstrucIR [20], as shown in Figures 24, 25, 26, 27, 28, 29, 33, 34. In most scenarios, GPT-4o guarantees high-quality output images, outperforming Gemini 2.0 Flash. However, there are still some degradation issues that are difficult to remove, as seen in the second image of the image denoising task. On the other hand, for low-level image restoration tasks, maintaining pixel consistency between the output and input images is crucial. GPT-4o does not perform well in this regard, as the content of many images changes. In contrast, InstructIR, designed specifically for image restoration, performs better, effectively removing degradation while maintaining pixel consistency throughout. For image inpainting and outpainting in Figure 30, 31. We compared Gemini 2.0 Flash with the latest inpainting and outpainting methods [66, 116, 22, 1]. Only the missing information needs to be completed, but GPT-4o still changes the undesired content of the image. Although the output image quality is higher, this is not ideal for evaluating the task itself. For human face inpainting, compared to the other two methods, the overall artistic style is more natural. For the colorization, we choose the latest colorization model CtrlColor [59]. The overall style is somewhat dark in Figure 32. Compared to Gemini 2.0 Flash, GPT-4os colors are more natural and consistent with the style. However, there are some inaccuracies in color control. For example, in the second image, the cats color is not white as specified in the text. Additionally, GPT-4o still exhibits issues with changes in image content, such as the shape of the humans face in the fourth image. For the image re-lighting task in Figure 35, GPT-4o performs well in applying realistic lighting and shadows, with natural color tones that match the scene. However, it occasionally struggles with maintaining light consistency, particularly in complex lighting scenarios, such as neon or vibrant lights. Compared to Gemini 2.0 Flash, GPT-4o produces more natural and consistent results, but it doesnt always accurately replicate the lighting effects as seen in the second image, where the neon lighting could have been better captured. IC-Light [122] is effective in applying realistic lighting, but tends to lose detail in some complex objects or faces under different light conditions. Overall, GPT-4o is strong contender for the image re-light task, providing good light consistency but leaving room for improvement in some specific scenarios. In summary, GPT-4o demonstrates strong performance in various low-level vision tasks, often surpassing Gemini 2.0 Flash in output quality with more natural and visually appealing results. However, it struggles with maintaining pixel consistency and avoiding undesired changes to image content, which are critical for tasks like restoration and inpainting. While its adaptability and realism are impressive, there is room for improvement in precision and task-specific consistency compared to specialized models like InstructIR and IC-Light. Figure 24: Task: image denoising, aiming to remove the noise information and obtain high-quality clear version. Setup: We compare GPT-4o with InstructIR [20] and Gemini 2.0 Flash [99] to evaluate the denoised images. Observations: GPT-4o can restore high-quality denoised images. Except for the second image, where the noise cannot be completely removed, the other images are free from noise. However, for low-level tasks, GPT-4o does not maintain content consistency well the background colors and object shapes in many images have changed, such as the background color in the first image and the floor in the fourth image. 35 Figure 25: Task: image deraining, aiming to remove the rain streak and get high-quality clear version. Setup: We compare GPT-4o with established baselines such as InstructIR [20] and Gemini 2.0 Flash [99] to evaluate the derained images. Observations: The overall performance of the GPT-4o is well. However, the model struggles with maintaining content consistency in low-level visual details for instance, the polar bears background in the first image becomes unnaturally pink, and the underwater scene loses depth and clarity. The flowers also appear altered in color and arrangement. In contrast, InstructIR demonstrates the most consistent performance across all examples, effectively removing rain while preserving the original scenes structure, color, and composition. Overall, InstructIR is the most balanced and accurate model for image restoration in this comparison. 36 Figure 26: Task: image dehazing, aiming to remove the haze information and get high-quality clear version. Setup: We compare GPT-4o with established baselines such as InstructIR [20] and Gemini 2.0 Flash [99] to evaluate the dehazed images. Observations: GPT-4o performs moderately well in dehazing, managing to restore clearer structures and contrast in most scenes. However, its outputs often have grayish or desaturated tone, especially visible in the second and third rows. Gemini 2.0 Flash produces more colorful results but tends to leave some haze behind, leading to less crisp output. InstructIR outperforms both, offering the most visually natural and sharp dehazing across all examples while preserving original colors and details. Overall, InstructIR demonstrates the strongest capability in removing haze while maintaining realism. Figure 27: Task: low-light image enhancement, aiming to increase the brightness of the image to obtain high brightness image. Setup: We compare GPT-4o with established baselines such as InstructIR [20] and Gemini 2.0 Flash [99] to evaluate the brightness images. Observations: In low-light enhancement tasks, GPT-4o can brighten images and recover basic visibility, but often introduces unnatural lighting and loses detail, especially in the second row, where the image remains overly dark. InstructIR consistently delivers the most balanced results, enhancing visibility while preserving true colors and textures, making it the best performer across all three examples. 38 Figure 28: Task: image deblurring, aiming to remove the blur information to obtain clear image. Setup: We compare GPT-4o with established baselines such as InstructIR [20] and Gemini 2.0 Flash [99] to evaluate the deblurred images. Observations: For motion deblurring, GPT-4o recovers some sharpness, especially in fine details like text or faces, but the content is not matched with the original image. Gemini 2.0 Flash sharpens the image slightly better in some cases but can introduce over-smoothing, making the result look artificial. InstructIR demonstrates the best deblurring performance overall restoring clear edges, facial features, and text while maintaining natural textures. It consistently produces the most stable and visually convincing results across all examples. 39 Figure 29: Task: image super-resolution, aiming to improve the image resolution. Setup: We compare GPT-4o with established baselines such as InstructIR [20] and Gemini 2.0 Flash [99] to evaluate the deblurred images. Observations: In super-resolution, InstructIR delivers the most natural and detailed results across all examplesrestoring fine edges in the card reader, realistic texture on the octopus, and sharp trees in the landscape. GPT-4o enhances clarity but misses details like the octopus surface and tree leaves. Gemini 2.0 Flash produces sharper outputs than GPT-4o but introduces unnatural textures and artifacts, especially in organic regions like the octopus and foliage. Figure 30: Task: Image inpainting, aiming to restore missing or masked regions in an image to appear natural and consistent with the context. Setup: We compare GPT-4o with baselines such as Gemini 2.0 Flash [99] and LatentPaint [22], evaluating their ability to fill in masked regions realistically. Observations: GPT-4o produces plausible completions but often lacks fine structure and texture alignmente.g., the bricks in the first row appear flat and misaligned. Gemini 2.0 Flash generates more visually coherent textures, especially in natural scenes like the second row, but can introduce slight over-smoothing. LatentPaint performs the best, accurately reconstructing facial details and complex textures such as hair and expression in the third row, demonstrating superior semantic understanding and visual consistency. 41 Figure 31: Task: Image outpainting, aiming to extend the visual content of an image beyond its original boundaries coherently and realistically. Setup: We compare GPT-4o with Gemini 2.0 Flash [99], and some Specialized outpainting methods (SGT+ [116], StrDiffusion [66] and Dream360 [1]), evaluating their ability to extend content while maintaining visual consistency in lighting, texture, and semantics. Observations: The Specialized outpainting methods consistently produces the most coherent extensions for example, it accurately maintains the rooms lighting and decor in the first row, continues architectural lines and street perspective in the second, and creates seamless snowy landscapes in the third. GPT-4o offers plausible structure but often lacks fine detail and texture continuity, such as mismatched snow gradients or missing shadows. Gemini 2.0 Flash performs slightly better in semantic extension than GPT-4o but can introduce lighting inconsistencies and abrupt transitions, particularly in wide scenes like the desert in the final row. 42 Figure 32: Task: Image colorization, aiming to add realistic and semantically consistent color to grayscale images based on textual prompts. Setup: We compare GPT-4o with Gemini 2.0 Flash [99] and CtrlColor [59], focusing on their ability to follow instructions and produce visually natural colorized outputs. Observations: CtrlColor performs the best overall, generating vivid and accurate colors that precisely match the promptssuch as green lips and yellow sunglasses in the last row, or the purple grass and kitten hues in the second. GPT-4o provides reasonably faithful colorization but often lacks richness or misinterprets tones (e.g., slightly dull red in the third row or inconsistent purple grass). Gemini 2.0 Flash is more vivid than GPT-4o but tends to oversaturate or produce stylized effects, especially on human features. Figure 33: Task: Shadow removal, aiming to eliminate harsh shadows while preserving the integrity of the scene, textures, and lighting balance. Setup: We compare GPT-4o with Gemini 2.0 Flash [99] and ShadowRefiner [25] to evaluate how well each method removes shadows and retains original object fidelity and lighting consistency. Observations: ShadowRefiner consistently achieves the most natural and effective shadow removal. It produces even, diffuse lighting across all scenese.g., softening shadows without distorting textures in complex scenes like the miniatures and dog portrait. Gemini 2.0 Flash removes shadows reasonably but occasionally leaves faint traces or flattens contrast, as seen in the second and fourth rows. GPT-4o shows stronger shadow reduction than Gemini 2.0 Flash but sometimes alters surface brightness or loses detail fidelity. ShadowRefiner best preserves the original color tones and textures while eliminating harsh shadows. 44 Figure 34: Task: Reflection removal, aiming to eliminate unwanted reflections from transparent or reflective surfaces while preserving original content and realistic lighting. Setup: We compare GPT-4o with Gemini 2.0 Flash [99] and DSIT [39], assessing their ability to remove reflections while maintaining scene realism, texture fidelity, and lighting consistency. Observations: DSIT shows the most effective and natural reflection removal across all examples. It restores interior visibility through windows (e.g., bed and car interior) while preserving lighting and geometry. Gemini 2.0 Flash removes some reflections but often leaves faded traces or dulls textures, especially on glass doors and wet pavement. GPT-4o performs better than Gemini 2.0 Flash in preserving background details but sometimes alters color tones and sharpness. Overall, DSIT provides the cleanest and most photorealistic results, especially for transparent surfaces like glass and reflective wet ground. 45 Figure 35: Task: Image relighting, aiming to modify the lighting of given image based on either reference light map or textual description, while preserving identity, texture, and spatial consistency. Setup: We compare GPT-4o with Gemini 2.0 Flash [99] and IC-Light [122] on two subtasks: reference-based and text-based relighting. Evaluations focus on lighting realism, directionality, shadow accuracy, and semantic preservation. Observations: IC-Light achieves the most realistic and consistent relighting across both tasksaccurately applying neon lighting from reference image and generating sharp shadows and natural light from text prompts. Gemini 2.0 Flash preserves content well but produces softer, less directional lighting. GPT-4o offers more vivid lighting than Gemini 2.0 Flash but sometimes lacks shadow accuracy or background coherence."
        },
        {
            "title": "2.2.6 Spatial Control",
            "content": "Spatial control aims to generate visual outputs that not only reflect the content described in the prompt, but also precisely adhere to additional structural conditions (e.g., canny edge maps, depth maps, sketches, poses, and masks). This task evaluates models ability to faithfully align text guidance with visual constraintsan essential capability for real-world creative applications such as illustration, animation, digital content creation, and visual storytelling. In this section, we examine GPT-4os performance across five representative types of controllable conditions: canny, depth, sketch, pose, and mask. For each setting, we compare its outputs with those from Gemini 2.0 Flash [99] and strong baseline method using ControlNet-based [121] diffusion backbones (FLUX.1-Dev [51], SDXL1.0 [82], SD3 Medium [27] or SD1.5 [90]). The results are illustrated in Figures 36, 37, 38, 39, 40. Overall, GPT-4o achieves performance that is on par with ControlNet-based methods in many cases, especially under common or moderately complex conditions. In particular, GPT-4o is capable of handling semantically rich or contextually complex prompts, where its strong foundation model understanding can help preserve both high-level semantics and visual plausibility. This is especially evident in tasks like pose-to-image or mask-to-image, where the structural signal may be sparse or ambiguous. However, GPT-4os strong generative prior can sometimes lead to overly detailed or hallucinated elements, which compromises structural fidelity. For instance, in canny-to-image or depth-to-image tasks that require fine-grained geometric alignment, GPT-4o may deviate from the input layout more noticeably than traditional diffusion-based methods. In contrast, ControlNet exhibits more stable and accurate control in these low-level structure-guided scenarios, making it better suited for applications where spatial accuracy is critical. That said, ControlNet may struggle in more complex or open-ended cases, such as mask-to-image scenes involving multiple objects or interactions (e.g., aquariums with visitors and fish). In these scenarios, GPT-4os strong cross-modal understanding partially compensates for its weaker control, offering plausible but not fully precise outputs. By comparison, Gemini 2.0 Flash lacks robust controllable generation capabilities across all evaluated control types. Its outputs often fail to match either the control condition or the textual prompt, reflecting limited capacity in multimodal alignment and structural grounding. In summary, GPT-4o demonstrates performance comparable to SOTA methods in most cases, excelling in tasks that require rich semantic understanding and contextual complexity while maintaining balance between high-level semantics and visual plausibility. Although it may exhibit structural deviations in tasks requiring precise geometric alignment, its strong generative prior gives it an advantage in handling complex or open-ended scenarios. 47 Figure 36: Task: Canny-to-Image generation. The goal is to generate prompt-aligned images guided by canny maps. Setup: Each row shows an input canny map and text prompt, with outputs from GPT-4o, Gemini 2.0 Flash [99], and FLUX.1-Dev w. ControlNet [51]. Observations: GPT-4o performs worse than FLUX.1-Dev [51] in structural fidelity, often introducing additional visual details that deviate from the input edge map. However, it produces more semantically aligned and aesthetically pleasing results overall. Compared to Gemini 2.0 Flash, GPT-4o significantly outperforms in both structure preservation and prompt consistency. 48 Figure 37: Task: Depth-to-image generation, aiming to synthesize controllable and visually coherent images based on text prompt and given depth map. Setup: We compare GPT-4o with Gemini 2.0 Flash [99] and FLUX.1-Dev w. ControlNet [51], focusing on controllability, text-prompt alignment, and the visual quality of generated scenes. Observations: GPT-4o generates visually appealing and stylistically consistent images that align reasonably with text and depth cuessuch as the bridge scene and stone ruins with rich lighting and artistic tone. However, its controllability is weaker than FLUX.1-Dev w. ControlNet [51], which shows more precise depth alignment and object placement, as seen in the accurate layout of the bridge and red pillow. GPT-4o leans toward stylized coherence, while FLUX emphasizes photorealism with sharper spatial fidelity. Gemini 2.0 Flash lags behind both, often showing depth misalignment, shape distortion, and weaker semantic grounding. 49 Figure 38: Task: Sketch-to-image generation, which requires translating rough line drawings into realistic and semantically accurate images guided by text prompts. Setup: We evaluate GPT-4o against Gemini 2.0 Flash [99] and SDXL1.0 w. ControlNet [82], focusing on how well each model respects the provided sketch while reflecting the described content. Observations: GPT-4o excels at generating lifelike scenes that match the prompt, often delivering visually pleasing and contextually grounded outputslike the natural posture and setting of the giraffe or the dynamic movement in the parachute example. However, it tends to soften or reinterpret sketch lines, leading to slight mismatches in fine structure. In contrast, SDXL1.0 w. ControlNet [82] offers stronger adherence to the input sketch, capturing geometric details more accurately (e.g., fan blades and figure outlines), albeit with slightly more synthetic textures. Gemini 2.0 Flash shows limited understanding of both sketch and prompt, often producing less realistic or structurally off-target images. 50 Figure 39: Task: Pose-to-image generation, aiming to synthesize realistic images that reflect both the human pose and descriptive prompt. Setup: We benchmark GPT-4o against Gemini 2.0 Flash [99] and SD3 Medium w. ControlNet [27], evaluating their ability to follow pose conditions while generating semantically accurate and coherent images. Observations: GPT-4o performs well in complex scenessuch as the football examplewhere it effectively integrates pose, clothing, and background with strong realism, contextual and pose accuracy. In simpler cases like the pull-up exercise, it shows occasional pose drift, especially in limbs. SD3 Medium w. ControlNet [27] offers better pose fidelity overall, though its visual quality can be inconsistent. Gemini 2.0 Flash underperforms in both structure and coherence, often generating anatomically incorrect or visually weak results. Overall, GPT-4o balances text understanding and generation quality, especially in detailed prompts. 51 Figure 40: Task: Mask-to-image generation, which requires translating semantic segmentation maps and textual prompts into coherent and realistic images. Setup: We compare GPT-4o with Gemini 2.0 Flash [99] and SD1.5 w. ControlNet [90], focusing on their ability to combine spatial layout from the mask with deeper scene understanding from the prompt. Observations: Compared to previous control tasks, this setting demands more from the model in terms of semantic reasoning and compositional understanding. GPT-4o excels in this regard, producing visually consistent scenes that align with the prompts intentsuch as the serene church interior and the immersive aquarium setting with visitors. However, in fine-grained spatial control, especially with small or tightly shaped objects like tropical fish, SD1.5 w. ControlNet [90] performs better in preserving shape and positioning. Gemini 2.0 Flash continues to struggle in both fidelity and adherence to masks, often missing key scene elements or producing oversimplified outputs."
        },
        {
            "title": "2.2.7 Camera Control",
            "content": "Although recent visual generative models demonstrate remarkable capabilities in creating high-quality images, generating images with specific camera settings (e.g., bokeh blur parameters, focal length, shutter speed, color temperature) and making further adjustments remains challenging task. We further explore GPT-4os performance in camera control, evaluating its ability to generate images with desired photographic parameters in text instructions. This task is particularly significant as it bridges the gap between artistic creativity and technical precision, enabling users to simulate professional photography techniques and achieve greater control over the visual output. Such advancements have broad applications in fields like photography, cinematography, and visual design. Specifically, we collect text prompts from [118], and compare GPT-4o and Gemini 2.0 Flash [99] with Generative Photography (GP) [118]. The results are reported in Figures 41, 42. We can observe that GPT-4o achieves decent results in controlling bokeh blur parameters and color temperature, demonstrating its strong generalizability to various photographic settings. However, it still falls short in adjusting focal length and shutter speed, occasionally leading to inconsistent visual semantics or incorrect visual effects. By comparison, Gemini 2.0 Flash struggles significantly across all camera control scenarios, failing to produce coherent or accurate outputs that align with the specified photographic parameters, highlighting its limited capability in this domain. In this task, GPT-4o shows promising potential in camera control, outperforming Gemini 2.0 Flash and achieving competitive results in certain aspects. Nonetheless, there remains room for improvement in handling more complex adjustments, which could further enhance its applicability in professional photography and creative industries. 53 Figure 41: Task: Camera control. The goal is to generate images aligned with specific photographic parameters, such as bokeh blur, focal length, shutter speed, and color temperature. Setup: Results are based on text prompts collected from [118], comparing outputs from GPT-4o, Gemini 2.0 Flash [99], and Generative Photography (GP) [118]. Each row includes the input text instructions and corresponding outputs. Observations: GPT-4o demonstrates strong performance in controlling bokeh blur, producing visually appealing and parameter-aligned results. However, it shows limitations in handling focal length, occasionally generating inconsistent or less accurate outputs. By contrast, Gemini 2.0 Flash struggles significantly in both aspects, often failing to produce coherent results. Overall, GPT-4o achieves better performance in this task but still requires further refinement to enhance focal length control. Figure 42: Task: Camera control. The goal is to generate images aligned with specific photographic parameters, such as bokeh blur, focal length, shutter speed, and color temperature. Setup: Results are based on text prompts collected from [118], comparing outputs from GPT-4o, Gemini 2.0 Flash [99], and Generative Photography (GP) [118]. Each row includes the input text instructions and corresponding outputs. Observations: GPT-4o demonstrates strong performance in controlling color temperature, producing coherent and visually accurate results. However, it struggles with shutter speed, occasionally resulting in inconsistent or unrealistic motion effects. In contrast, Gemini 2.0 Flash fails to consistently handle either parameter, often producing outputs that lack alignment with the desired settings. Overall, GPT-4o outperforms Gemini 2.0 Flash in this task, but further improvements are needed for precise shutter speed control."
        },
        {
            "title": "2.2.8 In-context Visual Prompting",
            "content": "The in-context visual prompting tasks aim at understanding and executing specific tasks on new query images by leveraging pair of task-specific example images and accompanying text instructions. Previous works [105, 18, 52] have explored this capability in the context of diffusion and autoregressive models, demonstrating its potential in enhancing model adaptability. The significance of in-context visual prompting lies in its ability to enable models to generalize to novel tasks. This approach mirrors human-like learning, where new tasks can be understood and performed by observing relevant examples. This capability has broad implications across various domains, and paves the way for more flexible and efficient paradigms capable of adapting to wide range of specific tasks. We curate four representative tasks to evaluate the performance of GPT-4o in in-context visual prompting. These tasks are designed to assess the models ability to understand and adapt to specific visual tasks based on provided examples and guidance, including: Movie-Shot Generation: three-shot image collected from [42] is provided as an example, and the model is instructed to follow this format to generate similar movie shots for the query image. Ray-Tracing Rendering: An example gaming scene is provided with and without ray tracing, and the model is expected to render ray-traced version of the query image. Overlaid Mask Visualization: The model receives an original image accompanied by its corresponding segmented results from [49] and is tasked with outputting the segmented results in the same format for the query image. Maze Solving: maze and its corresponding solution path are provided as examples, and the model is required to draw the solution path for new maze presented in the query image. All the results are illustrated in Figure 43. Compared with Gemini 2.0 Flash [99], GPT-4o demonstrates promising performance in movie-shot generation and ray-tracing rendering tasks, showcasing its ability to follow example formats and generate visually coherent outputs. However, it still struggles with maintaining consistent visual semantics across the generated outputs. For the overlaid mask visualization task, GPT-4o falls short in effectively executing the instructions. The result fails to adhere to the required format, indicating that the models ability to process and generate complex outputs remains limited. For maze solving, task that demands advanced visual reasoning and logical inference, GPT-4o struggles significantly. This highlights the challenges in combining higherlevel reasoning with visual generation capabilities, suggesting that more sophisticated reasoning mechanisms are needed for tasks of this nature. In summary, GPT-4o shows considerable potential in in-context visual prompting, while it still underperforms in certain difficult tasks. These observations suggest that further advancements are necessary to enhance its generation and reasoning capabilities for more complex and diverse visual tasks. Figure 43: Task: In-context visual prompting. The goal is to perform specific visual tasks on new query images based on task-specific example images and text instructions. Setup: Four representative tasks are evaluated: movie-shot generation, ray-tracing rendering, overlaid mask visualization, and maze solving. Each row includes example images, query images, and the corresponding outputs. Observations: GPT-4o excels in movie-shot generation and ray-tracing, producing coherent outputs but lacks consistency in visual semantics. It fails with overlaid mask visualization and maze solving, showing limits in complex task integration. While promising for in-context visual prompting, it needs refinement for more complex and reasoning-intensive tasks. 57 2.3 Image-to-3D Tasks We evaluate the 3D understanding capabilities from 2D images of GPT-4o across three tasks: 2D image-to-3D modeling, 2D UV map-to-3D rendering, and novel view synthesis."
        },
        {
            "title": "2.3.1 Image to 3D modeling",
            "content": "Generating 3D models from monocular images boosts wide range of applications, including augmented reality, virtual reality, and the gaming industry. This capability not only facilitates the content creation process but also mitigates the reliance on specialized 3D artists for creating 3D assets, which is more timeand cost-effective. Therefore, there is growing research interest in generating 3D models from 2D images. Early methods on image-to-3D employ the learning-based approaches for single-view reconstruction [74, 77, 102, 79]. Recent works leverage the diffusion model prior to perform image-conditioned 3D generative modeling [69, 68, 83, 113]. In this section, we investigate the potential of GPT-4o for 3D modeling from 2D images. We begin by prompting GPT-4o to generate Cinema 4D modeling interface to test its ability to produce coherent representations of structure, material, and wireframe based on the input image. As shown in Figure 44, GPT-4o can generate high-quality 3D model renderings within the application interface. Notably, the generated models exhibit clear wireframes and textures consistent with the input images. In contrast, Gemini 2.0 Flash and Midjourney v6.1 fail to achieve comparable results under the same conditions, which produce inconsistent modelings. We then prompt the GPT-4o to generate corresponding 3D object and material files in .obj and .mtl formats to further evaluate its understanding of the underlying structure in the rendered images. However, the output 3D models are coarse and inconsistent with input images, indicating that although GPT-4o can produce visually coherent 3D renderings, its capability to transform these into accurate and usable 3D object files remains limited. Additionally, Gemini 2.0 Flash and Midjourney v6.1 do not support exporting 3D models."
        },
        {
            "title": "2.3.2 UV Map to 3D rendering",
            "content": "UV maps are 2D images that store texture information for 3D models. In 3D modeling, geometric data is represented in 3D space, while texture data is defined in 2D texture space. UV mapping is the process of projecting 2D UV map onto 3D model, accurately aligning texture with geometry. The UV mapping process can evaluate models capability for 3D perception and spatial understanding. Moreover, this task has broad applications in design, helping to reduce the burden on designers to create product renderings from 2D maps manually and provide useful references. As shown in Figure 45, GPT-4o exhibits superior ability to generate consistent 3D renderings from 2D maps compared to Gemini 2.0 Flash and Midjourney v6.1. However, some outputs remain unsatisfactory, displaying inconsistencies in patterns and structure (see row 3 in Figure 45). Gemini 2.0 Flash struggles to correctly wrap the 3D model, though it maintains pattern consistency. Midjourney v6.1 tends to introduce additional, imagined features, which reduce controllability in this task."
        },
        {
            "title": "2.3.3 Novel View Synthesis",
            "content": "From monocular view, humans can imagine an objects 3D shape and appearance since humans have collected enough prior knowledge for different objects throughout their daily lives. This ability to infer novel views of objects is essential for wide range of tasks, from object manipulation to artistic creation such as painting. Early works achieve image-to-3D reconstruction using category-specific priors or large-scale pre-training [45, 80, 87, 32, 131]. Recent studies have shown that large diffusion models contain rich 3D prior information of the visual world, enabling them to perform novel view synthesis [69, 68, 83, 70]. These novel views can then be used for zero-shot 3D reconstruction using different 3D representations such as NeRF [76], mesh, or SDF. In this section, we evaluate the ability of GPT-4o for novel view synthesis on objects with artistic styles and asymmetric geometry. As shown in Figure 46, for artistically styled objects, GPT-4o and Gemini 2.0 Flash largely preserve structural consistency with the input image, although they may change some elements or fine details. For the asymmetric object, GPT-4o can preserve the object scale and size better than Gemini 2.0 Flash. However, Midjourney v6.1 fails to generate consistent novel views, instead producing visually appealing images that do not align with the given prompt of this task. 58 Figure 44: Task: Image-to-3D model rendering. Evaluate the 3D modeling ability given 2D image. Setup: Each row shows an input image and text prompt with outputs from GPT-4o, Gemini 2.0 Flash [99], and Midjourney v6.1 [75]. Observation: GPT-4o can generate better 3D model rendering with consistent shape, texture, and plausible wireframe than Gemini 2.0 Flash and Midjourney v6.1. 59 Figure 45: Task: 2D UV map to 3D rendering. Evaluate the 3D perception and spatial understanding ability. Setup: Each row shows an input image and text prompt with outputs from GPT-4o, Gemini 2.0 Flash [99], and Midjourney v6.1 [75]. Observation: GPT-4o can generate better 3D renderings based on 2D maps than Gemini 2.0 Flash and Midjourney v6.1. However, structure and pattern inconsistencies still exist among these three models. 60 Figure 46: Task: Novel view synthesis. Evaluate the 3D perception and spatial understanding ability. Setup: Each row shows an input image and text prompt with outputs from GPT-4o, Gemini 2.0 Flash [99], and Midjourney v6.1 [75]. Observation: GPT-4o can generate better style and structure-consistent novel views for both artistic painting and asymmetric objects. 61 2.4 Image-to-X Tasks In this section, we further evaluate both GPT-4o and Gemini 2.0 Flash for several dense image understanding tasks, including segmentation-related tasks, depth estimation, normal estimation, matting, salient object detection, edge detection, layout detection, text detection, and object tracking."
        },
        {
            "title": "2.4.1 Image Segmentation",
            "content": "Image segmentation tasks group pixels of the given image or video into semantic regions. It is fundamental problem in computer vision and involves numerous real-world applications, such as robotics, automated surveillance, and image/video editing. With the development of recent deep learning methods, this domain has achieved rapid progress. Early works mainly adopt CNN-based methods with large kernels or respective fields. Recently, transformer-based methods have also worked well and surpassed previous CNN-based methods on various benchmarks. In particular, we test three segmentation tasks, including referring segmentation, semantic segmentation, and panoptic segmentation. Referring Segmentation. This task outputs the corresponding mask according to the input texts, and the goal is to test the pixel-level grounding ability of the model. In Figure 47, we compare GPT-4o, Gemin 2.0 Flash and recent state-of-the art method, Sa2VA [117] (8B model ). We show five open-world test cases. For the first two cases, GPT-4o shows the coarse localization ability on the background region. For example, it can mark the grass region despite the unfavorable boundaries. However, compared to the SOTA method, Sa2VA, GPT-4o mistakenly merges both large regions. In the third row, both GPT-4o and Gemini 2.0 Flash cannot perform grounding with complex text inputs. In the fourth row, all models perform badly. GPT-4o generates an unseen chair in the images while Gemin 2.0 Flash performs image editing functions by replacing the smallest chair with normal chair. Sa2VA also segments the wrong object (the nearest chair). In the last example, GPT-4o also cannot segment smaller objects (bag). For all examples, both GPT-4o and Gemini 2.0 Flash modify the image contents. These examples indicate that GPT-4o has weak pixel grounding ability. Semantic Segmentation. Semantic segmentation assigns each pixel semantic label, which is one basic vision task. In Figure 48, we show several test cases on the semantic segmentation task. In particular, we adopt DeeplabV3+ [14] (ResNet101 as backbone, trained on Pascal-Context) as one expert model for reference. Surprisingly, the mask quality of GPT-4o is good on four examples, even comparable with an expert model, Deeplab-V3+. During the testing, we find the texts may be randomly appended to the masks. This is why the first row differs from the remaining examples. For the second and third examples, GPT-4o misaligns the text and mask regions. Compared to Gemin 2.0 Flash, GPT-4o has much stronger ability in semantic segmentation, particularly for mask shape. However, there is still lot of room for this task, including unified semantic segmentation format, enhanced text and mask alignments, and more correct mask labels. Panoptic Segmentation. This task assigns the foreground region semantic label and assigns one mask label and one instance ID to each instance, which is unified task format of semantic segmentation and instance segmentation. In Figure 49, we compare the panoptic segmentation ability of GPT-4o, Gemini 2.0 Flash, and one expert model, K-Net [123](trained on the COCO panoptic segmentation dataset, with ResNet50 as backbone). Overall, the mask shapes of GPT-4o are good. The model can understand the panoptic segmentation task, while the Gemini 2.0 Flash cannot do this task in the first and third cases. However, the spatial locations have been changed for all cases. The generated masks are in part-whole formats and are even finer-grained than K-Net. For example, in the first example, the jersey number (17) of the person and the hair of the people are also marked. Meanwhile, we also find similar issue: several examples have text, while several do not have text, even though they adopt the same text prompt. In addition, GPT-4o can distinguish different instances with different colors, despite most of them not being good (see the last example). https://huggingface.co/ByteDance/Sa2VA-8B Figure 47: Task: Image to X: Referring expression segmentation. Evaluate the grounding and grouping ability. Setup: Each row shows an input image and text prompt with outputs from GPT-4o, Gemini 2.0 Flash [99], and Sa2VA [117]. Observation: These examples indicate that current GPT-4o has weak pixel-level grounding ability. 63 Figure 48: Task: Image to X: Semantic segmentation. Evaluate the shape and grouping ability. Setup: Each row shows an input image and text prompt with outputs from GPT-4o, Gemini 2.0 Flash [99], and Deeplab-V3+ [14]. Observation: Compared with Gemin-2.0, the mask quality of GPT-4o is good. However, there are still huge gaps in the standard semantic segmentation format. 64 Figure 49: Task: Image to X: Panoptic segmentation. Evaluate the shape and grouping ability. Setup: Each row shows an input image and text prompt with outputs from GPT-4o, Gemini 2.0 Flash [99], and K-Net [123]. Observation: GPT-4o can understand the panoptic segmentation task, while Gemini 2.0 Flash cannot do this task in the first and third cases."
        },
        {
            "title": "2.4.2 Edge Detection",
            "content": "Edge Detection. As classic vision task, edge detection aims to identify the boundaries or edges of objects within an image. These edges represent the locations with significant changes in image intensity, color, or other visual features. Common edge detection operators include the Sobel, Prewitt, and Canny operators. Recent works adopt deep learning-based approaches. In Figure 50, we compare this ability with recent SOTA deep learning based approach, EMDB [56]. For four examples, we find both GPT-4o and Gemini 2.0 Flash can detect object edges for both foreground and background objects. In addition, the details are even good using GPT-4o. We find two critical issues: 1) The spatial localization of GPT-4o is changed as observed by the segmentation tasks. 2) The content of GPT-4o is also changed. For example, in the first example, the road is generated, which does not exist in the input image. Image Matting. Image matting is technique in image processing that aims to separate foreground object from its background and obtain detailed alpha matte, which indicates the transparency or opacity of each pixel in the foreground. It goes beyond simple segmentation by providing more precise information about the boundaries and fine details of the object, especially for complex objects like hair or smoke. In Figure 51, we show three testing examples, with one expert model, Matting Anything [53]. Compared with Gemini, GPT-4o can handle the simple cases, as shown in the third row. Thus, it can understand the task goal. For example, it can even keep the fine-grained details of horse hair. However, considering the strict requirements of image matting (fine-grained and aligned details), the overall quality is bad. Compared with Matting Anything, both GPT-4o and Gemini work poorly. We find nearly the same issues: 1) Wrong spatial localization, 2) Changed contents. 66 Figure 50: Task: Image to X: Edge detection. Evaluate the shape analysis ability. Setup: Each row shows an input image and text prompt with outputs from GPT-4o, Gemini 2.0 Flash [99], and EDMB [56]. Observation: We find both GPT-4o and Gemini 2.0 Flash can detect object edges for both foreground and background objects. 67 Figure 51: Task: Image to X: Image matting. Evaluate the grouping and shape analysis ability. Setup: Each row shows an input image and text prompt with outputs from GPT-4o, Gemini 2.0 Flash [99], and Matting Anything [54]. Observation: Compared with Gemini, GPT-4o can handle the simple cases, as shown in the third row. However, considering the strict requirements of image matting (fine-grained and aligned details), the overall quality is bad."
        },
        {
            "title": "2.4.3 Salient Object",
            "content": "Salient Object Detection. Salient object detection is crucial technique in the field of computer vision and image processing. It aims to identify and locate the most visually prominent objects within an image or video sequence. In Figure 52, we adopt one expert model, BiRefNet [127], as reference. For all examples, compared with Gemini 2.0 Flash, GPT-4o can detect relevant salient objects with the text prompts while Gemini can not achieve this. The second example shows that the GPT-4o can generate the aligned salient masks. However, for other examples, the spatial location is not changed where the results are generated according to the input image and potential classes. In the last examples, GPT-4o cannot generate multiple salient object masks, which is also limitation when dealing with multiple objects. Mirror Detection. Mirror detection is task in computer vision that focuses on identifying mirror surfaces within an image or scene. Previous works explore this direction by adopting visual cues and geometric cues. In Figure 53, we also explore this ability for both GPT-4o and Gemini 2.0 Flash. As for comparison, we adopt recent SOTA expert model, VMD [107]. For simple cases, we find that GPT-4o can carry out mirror detection, as shown in the first example. For the complex scene, it cannot work as well as the expert model, VMD. As shown in the second example, it generates fake mirror and leads to wrong image output with line to mark the boundaries of the fake mirror. As shown in the last row, GPT-4o treats several rectangular objects as mirrors, leading to several false positive examples. Shadow Detection. Shadow detection is significant process in computer vision and image processing that aims to identify and localize shadow regions in an image or video. This technique is crucial, as shadows can otherwise disrupt object detection, recognition, and scene analysis. In Figure 54, we compare and test this ability for GPT-4o. We adopt the SOTA model, SDDNet [21] for reference. For the simple examples (single objects and no objects in the image), both GPT-4o and Gemini can localize the shadow, as shown in the first two rows. For more complex examples, both models detect both objects and their shadows with one mask output, as shown in the last two rows. Thus, GPT-4o cannot handle these inputs. In addition, the spatial misalignments also happen for all the cases. Camouflage Object Detection. Camouflage object detection is challenging task in computer vision. It aims to identify objects that are designed to blend into their backgrounds, making them difficult to distinguish by human eyes or traditional detection methods. This has wide application for the military, security, and wildlife conservation. As shown in Figure 55, we also include one expert model, BiRefNet [127] for reference. For all examples, both GPT-4o and Gemini 2.0 Flash can detect and segment the camouflage animals for simple cases, as shown in the last two rows. GPT-4o can also detect the specific object, given the text prompt, as shown in the first row. However, the same misalignment issues still exist. In addition, it also mixes segmentation maps (in binary masks or color masks), as shown in the last row. 69 Figure 52: Task: Image to X: Salient object detection. Evaluate the grouping and shape analysis ability. Setup: Each row shows an input image and text prompt with outputs from GPT-4o, Gemini 2.0 Flash [99], and BiRefNet [127]. Observation: For all examples, compared with Gemini, GPT-4o can detect related salient objects with the text prompts while Gemini can not achieve this function. 70 Figure 53: Task: Image to X: Mirror detection. Evaluate the grouping and shape analysis ability. Setup: Each row shows an input image and text prompt with outputs from GPT-4o, Gemini 2.0 Flash [99], and VMD [107]. Observation: For simple cases, we find that GPT-4o can carry out mirror detection, as shown in the first example. For the complex scene, it cannot work as well as VMD. 71 Figure 54: Task: Image to X: Shadow detection. Evaluate the grouping and shape analysis ability. Setup: Each row shows an input image and text prompt with outputs from GPT-4o, Gemini 2.0 Flash [99], and SDDNet [21]. Observation: For more complex examples, both models detect both objects and their shadows with one mask output, as shown in the last two rows, leading to false positive predictions. 72 Figure 55: Task: Image to X: Camouflage object detection. Evaluate the grouping and shape analysis ability. Setup: Each row shows an input image and text prompt with outputs from GPT-4o, Gemini 2.0 Flash [99], and BiRefNet [127]. Observation: Both GPT-4o and Gemini 2.0 Flash can detect and segment the camouflage animals for simple cases. However, the spatial misalignments still exist."
        },
        {
            "title": "2.4.4 Depth Estimation",
            "content": "The depth estimation task involves predicting the distance from the camera to objects within scene. In this paper, we focus on monocular depth estimation, which takes single image as input. In Figure 56, we compare GPT-4o, Gemini 2.0 Flash, and recent SOTA method, Depth-Anything [114]. We first notice that Gemini cannot produce reasonable depth estimations. For GPT-4o, although it can output fancy depth map visualization, we want to point out that this output is grayscale visualization of depth estimation and cannot be directly converted to the depth of each pixel. We show mainly five cases. In the first test case, we notice that GPT-4o is good at capturing details in images, which Depth-Anything may not be good at. Although we cannot directly determine the accuracy of the depth value, we can judge from the visualization that the depth relationship between objects is accurate. What GPT-4o cannot do well is the background. Since the background in the image is the sky, we can infer from common sense that these areas are infinitely far away from the camera. However, the depth map output of GPT-4o does not handle these areas correctly. GPT-4o performs similarly in the second, fourth, and fifth examples. Among them, we would like to emphasize the fourth test case, since for buildings farther away, GPT-4o has no way to effectively analyze the distance between each building and the camera. In the third example, although the output of GPT-4o is very confusing, it completely misunderstands the depth relationship of the entire image. Therefore, we believe that the depth estimation performance of GPT-4o is still unstable. 74 Figure 56: Task: Image to X: Depth estimation. Setup: Each row shows an input image and text prompt with outputs from GPT-4o, Gemini 2.0 Flash [99], and Depth-Anything [114]. Observation: We convert the depth map generated by Depth-Anything into visualization map similar to GPT-4o. This evaluation shows that GPT-4o has the capability of distinguishing the depth relationship of different parts in the image, but its understanding of the background is insufficient."
        },
        {
            "title": "2.4.5 Normal Estimation",
            "content": "The surface normal estimation task involves predicting the orientation of surfaces at each pixel in an image, typically represented as 3D vectors. In Figure 57, we compare GPT-4o, Gemini 2.0 Flash, and Marigold normals [48]. The results show that GPT-4o can generate reasonable results. However, since GPT-4os output is an appealing normal map visualization, we want to clarify that this output is color-coded visualization and does not directly provide the exact normal vector for each pixel. Thus, we cannot use lighting or other methods to verify the accuracy of the normal maps, and downstream tasks cannot use the output results. However, we also find some unreasonable details. In the third test case, common sense suggests that the ground should be flat, but GPT-4o predicts normals for these textured areas that differ from the surrounding areas. Figure 57: Task: Image to X: Normal estimation. Setup: Each row shows an input image and text prompt with outputs from GPT-4o, Gemini 2.0 Flash [99], and Marigold [48]. Observation: This evaluation shows that GPT-4o has the capability of generating visualization map of the surface normal, but the understanding of the details is still insufficient."
        },
        {
            "title": "2.4.6 Layout Detection",
            "content": "The layout detection task requires the model to identify structural components (e.g., titles, paragraphs, tables, images) in the given image. In Figure 58, we compare the performance of GPT-4o, Gemini 2.0 Flash, and LayoutLMV3 [44] on the layout detection task. In the test cases, GPT-4o hallucinates layout elements that do not exist, although the final output is another document with layout detection results. If we consider the use in downstream tasks, such results are meaningless. Therefore, we conclude that GPT-4o is not capable of the layout detection task. Figure 58: Task: Image to X: Layout detection. Setup: Each row shows an input image and text prompt with outputs from GPT-4o, Gemini 2.0 Flash [99], and LayoutLMV3 [44]. Observation: The results show that GPT-4o and Gemini frequently generate different document but correct detected layout."
        },
        {
            "title": "2.4.7 Text Detection",
            "content": "The text detection task requires the model to detect the texts in the given image. In Figure 59, we compare the performance of GPT-4o, Gemini 2.0 Flash [99], and CRAFT [3] regarding to text detection. We observe that CRAFT exhibits better performance compared to the other models. In the first test case, GPT-4o demonstrates comparable performance to CRAFT. However, in other cases, GPT-4o continuously generates some nonexistent texts and labels them as text area. This issue becomes particularly evident in cluttered scenes or images with complex backgrounds. These false positives not only reduce detection precision but also make the output less reliable for downstream tasks such as OCR or document understanding. On the other hand, Gemini does not generate nonexistent texts but tends to over-predict some areas as text areas. 78 Figure 59: Task: Image to X: Text detection. Setup: Each row shows an input image and text prompt with outputs from GPT-4o, Gemini 2.0 Flash [99], and CRAFT [3]. Observation: The results show that GPT-4o frequently generates text that does not exist."
        },
        {
            "title": "2.4.8 Object Tracking",
            "content": "The object tracking task requires the model to continuously locate and follow the specific object across the frames in video sequence. We test the multi-object tracking, which requires the model to track several objects concurrently. We test four cases (Figure 60, 61, 62, 63). We compare GPT-4o, Gemini 2.0 Flash, and recent SOTA method SAM-2 [86]. Our first observation is that GPT-4o seems unable to generate images that are consistent with the original image. This may be related to the nature of its generative model. Even if we ignore this, for the tracking task, SAM-2 still performs better, while GPT-4o will have problems such as failing to maintain consistent tracking of the target, frequently drifting, or losing the object entirely. In Figure 60, the output of GPT-4o generally demonstrates the ability to track objects, but there are also some defects. For example, new object is even created out of the existing objects in the last picture generated by GPT-4o. We speculate that this is caused by the influence of the conversation context. In Figure 61, GPT-4o outputs some content that should not be in the output, such as the caf tag. In Figure 62, GPT-4o can track relatively simple object, but it fuses two separate objects. In Figure 63, GPT-4o lacks the capability of tracking in the dense scenario. 80 Figure 60: Task: Image to X: Object tracking, matching, and video analysis (1/4). Setup: Each row shows an input image and text prompt with outputs from GPT-4o, Gemini 2.0 Flash [99], and SAM-2 [86]. Observation: This evaluation shows that GPT-4o has the capability of tracking objects, but it cannot generate consistent image compared to the input image. 81 Figure 61: Task: Image to X: Object tracking, matching, and video analysis (2/4). Setup: Each row shows an input image and text prompt with outputs from GPT-4o, Gemini 2.0 Flash [99], and SAM-2 [86]. Observation: This evaluation shows that GPT-4o has the capability of tracking objects, but it cannot generate consistent image compared to the input image. 82 Figure 62: Task: Image to X: Object tracking, matching, and video analysis (3/4). Setup: Each row shows an input image and text prompt with outputs from GPT-4o, Gemini 2.0 Flash [99], and SAM-2 [86]. Observation: This evaluation shows that GPT-4o has the capability of tracking objects, but it cannot generate consistent image compared to the input image. 83 Figure 63: Task: Image to X: Object tracking, matching, and video analysis (4/4). Setup: Each row shows an input image and text prompt with outputs from GPT-4o, Gemini 2.0 Flash [99], and SAM-2 [86]. Observation: This evaluation shows that GPT-4o has the capability of tracking objects, but it cannot generate consistent image compared to the input image."
        },
        {
            "title": "3 Limitations",
            "content": "Although GPT-4o demonstrates impressive capabilities across wide range of image generation tasks, several limitations remain. These challenges highlight key areas for future improvement in developing unified foundation models for vision-language generation. 3."
        },
        {
            "title": "Inconsistent Generation",
            "content": "While GPT-4o often produces high-quality and semantically relevant images conditioned on textual prompts, it occasionally exhibits inconsistencies. Specifically, the model may generate visually compelling outputs that deviate from precise semantic cues of the input image, such as object count, spatial layout, specific shapes, or designated colors. These inconsistencies are especially problematic in tasks requiring partial image editing or compositional accuracy. Notably, such issues are less common in diffusion-based models or discrete denoising architectures like MaskGIT [11, 6], suggesting that GPT-4o operates under distinct generative paradigm with inherent trade-offs in fidelity and control."
        },
        {
            "title": "3.2 Hallucination",
            "content": "GPT-4o is also susceptible to hallucinationsproducing content that is logically implausible, semantically inconsistent, or factually incorrect. These include fabricating non-existent objects or geographical features (e.g., imaginary islands or landmarks), and misrepresenting relationships between entities. Such errors are particularly prevalent in complex or underspecified prompts, where the model appears to rely on internal priors rather than grounded world knowledge. While hallucination is common challenge across generative models, it poses notable limitations for real-world applications demanding precision, such as education, medical illustration, or scientific visualization."
        },
        {
            "title": "3.3 Data Bias",
            "content": "Despite strong alignment between text and vision modalities, GPT-4o struggles with data bias issue, which fail in generating underrepresented cultural elements and rendering non-Latin scripts such as Chinese, Japanese, and Arabic. The generated characters are often incomplete, distorted, or replaced with Latin-like approximations. These artifacts reflect underlying challenges in multilingual representation, likely due to limited exposure to diverse scripts during training and the inherent difficulty of accurate typographic rendering in pixel space. This phenomenon is emblematic of larger issue in AI systemsdata bias. The training data used to develop models like GPT-4o may disproportionately represent certain languages, cultures, and writing systems, leading to disparities in performance across different linguistic groups. These biases are not only technical limitations but also ethical concerns, as they can contribute to the exclusion of underrepresented languages and cultures from AI applications. As vision-language models are increasingly deployed globally, improving support for multilingual text remains crucial step toward inclusive and culturally competent AI systems."
        },
        {
            "title": "4 Conclusion",
            "content": "In conclusion, this work presents comprehensive study on the development of unified vision-language generative models, with focus on evaluating GPT-4o across wide range of image generation tasks. Our analysis shows that GPT-4o demonstrates strong capabilities in aligning vision and language, achieving competitive results across text-to-image, image-to-image, image-to-3D, and image-to-X tasks. However, limitations remain in inconsistent generation, hallucination, and data bias in underrepresented cultural elements and non-Latin scripts, highlighting current trade-offs in model design and training data coverage. We also emphasize that architecture alone does not determine success; training data, model scale, and optimization strategies are equally critical components of progress. We hope future work will provide deeper empirical insights into such proprietary systems and clarify their position within the broader landscape of unified generative modeling."
        },
        {
            "title": "References",
            "content": "[1] Hao Ai, Zidong Cao, Haonan Lu, Chen Chen, Jian Ma, Pengyuan Zhou, Tae-Kyun Kim, Pan Hui, and Lin Wang. Dream360: Diverse and immersive outdoor virtual scene creation via transformer-based 360 image outpainting. IEEE transactions on visualization and computer graphics, 2024. 34, 42 [2] Ideogram AI. Ideogram. https://ideogram.ai/, 2024. 10, 11, 12 85 [3] Youngmin Baek, Bado Lee, Dongyoon Han, Sangdoo Yun, and Hwalsuk Lee. Character region awareness for text detection. In CVPR, 2019. 78, 79 [4] Jinbin Bai, Wei Chow, Ling Yang, Xiangtai Li, Juncheng Li, Hanwang Zhang, and Shuicheng Yan. Humanedit: high-quality human-rewarded dataset for instruction-based image editing. arXiv preprint arXiv:2412.04280, 2024. [5] Jinbin Bai, Zhen Dong, Aosong Feng, Xiao Zhang, Tian Ye, Kaicheng Zhou, and Mike Zheng Shou. Integrating view conditions for image synthesis. arXiv preprint arXiv:2310.16002, 2023. 21 [6] Jinbin Bai, Tian Ye, Wei Chow, Enxin Song, Qing-Guo Chen, Xiangtai Li, Zhen Dong, Lei Zhu, and Shuicheng Yan. Meissonic: Revitalizing masked generative transformers for efficient high-resolution text-to-image synthesis. arXiv preprint arXiv:2410.08261, 2024. 5, 85 [7] Shane Barratt and Rishi Sharma. note on the inception score. arXiv preprint arXiv:1801.01973, 2018. 1 [8] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Improving image generation with better captions. Computer Science. https://cdn. openai. Lee, Yufei Guo, et al. com/papers/dall-e-3. pdf, 2023. [9] Manuel Brack, Felix Friedrich, Katharina Kornmeier, Linoy Tsaban, Patrick Schramowski, Kristian Kersting, and Apolinrio Passos. Ledits++: Limitless image editing using text-to-image models. 2023. 21, 25 [10] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. arXiv preprint arXiv:2211.09800, 2022. 21 [11] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1131511325, 2022. 85 [12] Haoyu Chen, Xiaojie Xu, Wenbo Li, Jingjing Ren, Tian Ye, Songhua Liu, Ying-Cong Chen, Lei Zhu, and Xinchao Wang. Posta: go-to framework for customized artistic poster generation. arXiv preprint arXiv:2503.14908, 2025. 10, [13] Liang Chen, Shuai Bai, Wenhao Chai, Weichu Xie, Haozhe Zhao, Leon Vinci, Junyang Lin, and Baobao Chang. Multimodal representation alignment for image generation: Text-image interleaved control is easier than you think. arXiv preprint arXiv:2502.20172, 2025. 1 [14] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. arXiv preprint arXiv:1706.05587, 2017. 62, 64 [15] Sixiang Chen, Tian Ye, Jinbin Bai, Erkang Chen, Jun Shi, and Lei Zhu. Sparse sampling transformer with uncertaintyIn Proceedings of the IEEE/CVF International driven ranking for unified removal of raindrops and rain streaks. Conference on Computer Vision, pages 1310613117, 2023. 34 [16] Sixiang Chen, Tian Ye, Yun Liu, and Erkang Chen. Snowformer: Context interaction transformer with scale-awareness for single image desnowing. arXiv preprint arXiv:2208.09703, 2022. 34 [17] Sixiang Chen, Tian Ye, Kai Zhang, Zhaohu Xing, Yunlong Lin, and Lei Zhu. Teaching tailored to talent: Adverse weather restoration via prompt pool and depth-anything constraint. In European Conference on Computer Vision, pages 95115. Springer, 2024. 34 [18] Tianqi Chen, Yongfei Liu, Zhendong Wang, Jianbo Yuan, Quanzeng You, Hongxia Yang, and Mingyuan Zhou. Improving in-context learning in diffusion models with visual context-modulated prompts. arXiv preprint arXiv:2312.01408, 2023. 56 [19] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. 1 [20] Marcos V. Conde, Gregor Geigle, and Radu Timofte. Instructir: High-quality image restoration following human instructions. In ECCV, 2024. 34, 35, 36, 37, 38, 39, [21] Runmin Cong, Yuchen Guan, Jinpeng Chen, Wei Zhang, Yao Zhao, and Sam Kwong. Sddnet: Style-guided dual-layer disentanglement network for shadow detection. In ACM MM, 2023. 69, 72 [22] Ciprian Corneanu, Raghudeep Gadde, and Aleix Martinez. Latentpaint: Image inpainting in latent space with diffusion models. In WACV, 2024. 34, 41 [23] Yingying Deng, Fan Tang, Weiming Dong, Chongyang Ma, Xingjia Pan, Lei Wang, and Changsheng Xu. Stytr2: Image style transfer with transformers. In CVPR, 2022. [24] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, et al. Dreamllm: Synergistic multimodal comprehension and creation. arXiv preprint arXiv:2309.11499, 2023. 1 [25] Wei Dong, Han Zhou, Yuqiong Tian, Jingke Sun, Xiaohong Liu, Guangtao Zhai, and Jun Chen. Shadowrefiner: Towards mask-free shadow removal via fast fourier transformer. arXiv preprint arXiv:2406.02559. 44 [26] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Mller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 1 86 [27] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Mller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 10, 11, 47, [28] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR), pages 1287312883, 2021. 1 [29] Aosong Feng, Weikang Qiu, Jinbin Bai, Kaicheng Zhou, Zhen Dong, Xiao Zhang, Rex Ying, and Leandros Tassiulas. An item is worth prompt: Versatile image editing with disentangled control. arXiv preprint arXiv:2403.04880, 2024. 21 [30] Tsu-Jui Fu, Wenze Hu, Xianzhi Du, William Yang Wang, Yinfei Yang, and Zhe Gan. Guiding instruction-based image editing via multimodal large language models. ICLR, 2024. 21, 22, 23, 24 [31] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. ICLR, 2023. 28 [32] Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen, Kangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, and Sanja Fidler. Get3d: generative model of high quality 3d textured shapes learned from images. NeurIPS, 2022. 58 [33] Leon Gatys, Alexander Ecker, and Matthias Bethge. Image style transfer using convolutional neural networks. CVPR, 2016. 18 [34] Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint arXiv:2404.14396, 2024. 1 [35] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139144, 2020. 1 [36] Yuchao Gu, Xintao Wang, Jay Zhangjie Wu, Yujun Shi, Yunpeng Chen, Zihan Fan, Wuyou Xiao, Rui Zhao, Shuning Chang, Weijia Wu, et al. Mix-of-show: Decentralized low-rank adaptation for multi-concept customization of diffusion models. In NeurIPS, 2024. 28 [37] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. NeurIPS, 2017. 1 [38] Qibin Hou, Yuying Ge, Jing Zhang, Yuchao Dai, and Ming-Ming Cheng. Storydiffusion: Consistent self-attention for long-range image and video generation. In Advances in Neural Information Processing Systems (NeurIPS), 2024. 31, 32 [39] Qiming Hu, Hainuo Wang, and Xiaojie Guo. Single image reflection separation via dual-stream interactive transformers. Advances in Neural Information Processing Systems, 37:5522855248, 2024. 45 [40] Jiancheng Huang, Yi Huang, Jianzhuang Liu, Donghao Zhou, Yifan Liu, and Shifeng Chen. Dual-schedule inversion: Training-and tuning-free inversion for real image editing. arXiv preprint arXiv:2412.11152, 2024. 21 [41] Kaiyi Huang, Chengqi Duan, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench++: An enhanced and comprehensive benchmark for compositional text-to-image generation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. 5 [42] Lianghua Huang, Wei Wang, Zhi-Fan Wu, Yupeng Shi, Huanzhang Dou, Chen Liang, Yutong Feng, Yu Liu, and Jingren Zhou. In-context lora for diffusion transformers. arXiv preprint arXiv:2410.23775, 2024. [43] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In ICCV, 2017. 18 [44] Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu Wei. Layoutlmv3: Pre-training for document ai with unified text and image masking. In ACM MM, 2022. 77 [45] Zixuan Huang, Stefan Stojanov, Anh Thai, Varun Jampani, and James Rehg. Planes vs. chairs: Category-guided 3d shape learning without any 3d cues. In ECCV, 2022. [46] Jiaxiu Jiang, Yabo Zhang, Kailai Feng, Xiaohe Wu, Wenbo Li, Renjing Pei, Fan Li, and Wangmeng Zuo. Mc2: Multi-concept guidance for customized multi-concept generation. arXiv preprint arXiv:2404.05268, 2024. 28 [47] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In ECCV, 2016. 18 [48] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 76 [49] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In ICCV, 2023. 56 [50] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In CVPR, 2023. 28 [51] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. 2, 5, 8, 9, 10, 11, 47, 48, 49 87 [52] Bolin Lai, Felix Juefei-Xu, Miao Liu, Xiaoliang Dai, Nikhil Mehta, Chenguang Zhu, Zeyi Huang, James Rehg, Sangmin Lee, Ning Zhang, et al. Unleashing in-context learning of autoregressive models for few-shot image manipulation. arXiv preprint arXiv:2412.01027, 2024. [53] Jiachen Li, Jitesh Jain, and Humphrey Shi. Matting anything. arXiv: 2306.05399, 2023. 66 [54] Jiachen Li, Jitesh Jain, and Humphrey Shi. Matting anything. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17751785, 2024. 68 [55] Junyi Li, Zhilu Zhang, Xiaoyu Liu, Chaoyu Feng, Xiaotao Wang, Lei Lei, and Wangmeng Zuo. Spatially adaptive self-supervised learning for real-world image denoising. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2023. 34 [56] Yachuan Li, Xavier Soria Poma, Yun Bai, Qian Xiao, Chaozhi Yang, Guanlin Li, and Zongmin Li. Edmb: Edge detector with mamba. arXiv preprint arXiv:2501.04846, 2025. 66, [57] Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu, and Ming-Hsuan Yang. Universal style transfer via feature transforms. In NIPS, 2017. 18 [58] Zijie Li, Henry Li, Yichun Shi, Amir Barati Farimani, Yuval Kluger, Linjie Yang, and Peng Wang. Dual diffusion for unified image generation and understanding. arXiv preprint arXiv:2501.00289, 2024. 2 [59] Zhexin Liang, Zhaochen Li, Shangchen Zhou, Chongyi Li, and Chen Change Loy. Control color: Multimodal diffusionbased interactive image colorization. arXiv preprint arXiv:2402.10855, 2024. 34, [60] Xin Lin, Chao Ren, Kelvin CK Chan, Lu Qi, Jinshan Pan, and Ming-Hsuan Yang. Multi-task image restoration guided by robust dino features. arXiv preprint arXiv:2312.01677, 2023. 34 [61] Xin Lin, Chao Ren, and Xiao Liu. Unsupervised image denoising in real-world scenarios via self-collaboration parallel generative adversarial branches. In ICCV, 2023. 34 [62] Xin Lin, Jingtong Yue, Sixian Ding, Chao Ren, Lu Qi, and Ming-Hsuan Yang. Dual degradation representation for joint deraining and low-light enhancement in the dark. IEEE Transactions on Circuits and Systems for Video Technology, 2024. 34 [63] Xin Lin, Yuyan Zhou, Jingtong Yue, Chao Ren, Kelvin CK Chan, Lu Qi, and Ming-Hsuan Yang. Re-boosting selfcollaboration parallel prompt gan for unsupervised image restoration. arXiv preprint arXiv:2408.09241, 2024. [64] Bingchen Liu, Ehsan Akhgari, Alexander Visheratin, Aleks Kamko, Linmiao Xu, Shivam Shrirao, Chase Lambert, Joao Souza, Suhail Doshi, and Daiqing Li. Playground v3: Improving text-to-image alignment with deep-fusion large language models. arXiv preprint arXiv:2409.10695, 2024. 10, 12, 14 [65] Dongyang Liu, Shitian Zhao, Le Zhuo, Weifeng Lin, Yu Qiao, Hongsheng Li, and Peng Gao. Lumina-mgpt: Illuminate flexible photorealistic text-to-image generation with multimodal generative pretraining, 2024. 1 [66] Haipeng Liu, Yang Wang, Biao Qian, Meng Wang, and Yong Rui. Structure matters: Tackling the semantic discrepancy in diffusion models for image inpainting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 34, 42 [67] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with ringattention. arXiv preprint arXiv:2402.08268, 2024. [68] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Mukund Varma T, Zexiang Xu, and Hao Su. One-2-3-45: Any single image to 3d mesh in 45 seconds without per-shape optimization. Advances in Neural Information Processing Systems, 2023. 58 [69] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object. In Proceedings of the IEEE/CVF international conference on computer vision, 2023. 58 [70] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3d: Single image to 3d using cross-domain diffusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2024. 58 [71] Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion modeling by estimating the ratios of the data distribution. arXiv preprint arXiv:2310.16834, 2023. 2 [72] Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Liang Zhao, et al. Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation. arXiv preprint arXiv:2411.07975, 2024. [73] Chenlin Meng, Kristy Choi, Jiaming Song, and Stefano Ermon. Concrete score matching: Generalized score matching for discrete data. Advances in Neural Information Processing Systems, 35:3453234545, 2022. 2 [74] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Occupancy networks: Learning 3d reconstruction in function space. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019. 58 [75] Midjourney. Midjourney. https://www.midjourney.com, 2024. 2, 6, 7, 18, 19, 20, 59, 60, 61 88 [76] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 2021. 58 [77] Michael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger. Differentiable volumetric rendering: Learning implicit 3d representations without 3d supervision. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020. 58 [78] OpenAI. Addendum to gpt-4o system card: 4o image generation, 2025. Accessed: 2025-04-02. 2 [79] Junyi Pan, Xiaoguang Han, Weikai Chen, Jiapeng Tang, and Kui Jia. Deep mesh reconstruction from single rgb images via topology modification networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019. 58 [80] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and Michael Black. Expressive body capture: 3d hands, face, and body from single image. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019. 58 [81] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Mller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [82] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Mller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. In The Twelfth International Conference on Learning Representations (ICLR), 2024. 1, 47, 50 [83] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren, Aliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Skorokhodov, Peter Wonka, Sergey Tulyakov, et al. Magic123: One image to high-quality 3d object generation using both 2d and 3d diffusion priors. arXiv preprint arXiv:2306.17843, 2023. 58 [84] Chu-Jie Qin, Rui-Qi Wu, Zikun Liu, Xin Lin, Chun-Le Guo, Hyun Hee Park, and Chongyi Li. Restore anything with masks: Leveraging mask image modeling for blind all-in-one image restoration. In ECCV, 2024. 34 [85] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. [86] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rdle, Chloe Rolland, Laura Gustafson, et al. SAM 2: Segment anything in images and videos. ICLR, 2025. 80, 81, 82, 83, 84 [87] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny. Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction. In ICCV, 2021. 58 [88] Bin Ren, Yawei Li, Nancy Mehta, and Radu Timofte. The ninth ntire 2024 efficient super-resolution challenge report. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, 2024. 34 [89] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjrn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR), pages 1068410695, 2022. 1 [90] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjrn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1068410695, June 2022. 47, [91] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In CVPR, 2023. 28 [92] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 2022. 5 [93] Subham Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin Chiu, Alexander Rush, and Volodymyr Kuleshov. Simple and effective masked diffusion language models. Advances in Neural Information Processing Systems, 37:130136130184, 2024. 2 [94] Qingyu Shi, Lu Qi, Jianzong Wu, Jinbin Bai, Jingbo Wang, Yunhai Tong, Xiangtai Li, and Ming-Husan Yang. Relationbooth: Towards relation-aware customized object generation. arXiv preprint arXiv:2410.23280, 2024. [95] Haoze Sun, Wenbo Li, Jianzhuang Liu, Haoyu Chen, Renjing Pei, Xueyi Zou, Youliang Yan, and Yujiu Yang. Coser: Bridging image and language for cognitive super-resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2586825878, 2024. 34 [96] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative pretraining in multimodality. arXiv preprint arXiv:2307.05222, 2023. 1 [97] Alexander Swerdlow, Mihir Prabhudesai, Siddharth Gandhi, Deepak Pathak, and Katerina Fragkiadaki. Unified multimodal discrete diffusion. arXiv preprint arXiv:2503.20853, 2025. 2 [98] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. 1 [99] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 2, 3, 5, 6, 7, 8, 9, 10, 12, 14, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 59, 60, 61, 63, 64, 65, 67, 68, 70, 71, 72, 73, 75, 76, 77, 78, 79, 81, 82, 83, 84 [100] Shengbang Tong, David Fan, Jiachen Zhu, Yunyang Xiong, Xinlei Chen, Koustuv Sinha, Michael Rabbat, Yann LeCun, Saining Xie, and Zhuang Liu. Metamorph: Multimodal understanding and generation via instruction tuning. arXiv preprint arXiv:2412.14164, 2024. 1 [101] Chunwei Wang, Guansong Lu, Junwei Yang, Runhui Huang, Jianhua Han, Lu Hou, Wei Zhang, and Hang Xu. Illume: Illuminating your llms to see, draw, and self-enhance. arXiv preprint arXiv:2412.06673, 2024. 1 [102] Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei Liu, and Yu-Gang Jiang. Pixel2mesh: Generating 3d mesh models from single rgb images. In Proceedings of the European conference on computer vision (ECCV), 2018. 58 [103] Xierui Wang, Siming Fu, Qihan Huang, Wanggui He, and Hao Jiang. Ms-diffusion: Multi-subject zero-shot image personalization with layout guidance. arXiv preprint arXiv:2406.07209, 2024. 28, 30 [104] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. 1 [105] Zhendong Wang, Yifan Jiang, Yadong Lu, Pengcheng He, Weizhu Chen, Zhangyang Wang, Mingyuan Zhou, et al. In-context learning unlocked for diffusion models. NeurIPS, 2023. 56 [106] Zhenyu Wang, Aoxue Li, Zhenguo Li, and Xihui Liu. Genartist: Multimodal llm as an agent for unified image generation and editing. NeurIPS, 2024. 5 [107] Alex Warren, Ke Xu, Jiaying Lin, Gary KL Tam, and Rynson WH Lau. Effective video mirror detection with inconsistent motion cues. In CVPR, 2024. 69, 71 [108] Jianzong Wu, Chao Tang, Jingbo Wang, Yanhong Zeng, Xiangtai Li, and Yunhai Tong. Diffsensei: Bridging multi-modal llms and diffusion models for customized manga generation. CVPR, 2025. 31, 33 [109] Size Wu, Wenwei Zhang, Lumin Xu, Sheng Jin, Zhonghua Wu, Qingyi Tao, Wentao Liu, Wei Li, and Chen Change Loy. Harmonizing visual representations for unified multimodal understanding and generation. arXiv preprint arXiv:2503.21979, 2025. [110] Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, et al. Vila-u: unified foundation model integrating visual understanding and generation. arXiv preprint arXiv:2409.04429, 2024. 1 [111] Yifan Xia, Yuying Ge, Jing Zhang, Yuchao Dai, and Ming-Ming Cheng. Seed-story: Multimodal long story generation with large language model. arXiv preprint arXiv:2407.08683, 2024. 31, 32 [112] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. arXiv preprint arXiv:2409.11340, 2024. 1 [113] Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, and Ying Shan. Instantmesh: Efficient 3d mesh generation from single image with sparse-view large reconstruction models. arXiv preprint arXiv:2404.07191, 2024. [114] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1037110381, 2024. 74, 75 [115] Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, and Bin Cui. Mastering text-to-image diffusion: Recaptioning, planning, and generating with multimodal llms. In ICML, 2024. 5 [116] Hang Yu, Ruilin Li, Shaorong Xie, and Jiayan Qiu. Shadow-enlightened image outpainting. In CVPR, 2024. 34, 42 [117] Haobo Yuan, Xiangtai Li, Tao Zhang, Zilong Huang, Shilin Xu, Shunping Ji, Yunhai Tong, Lu Qi, Jiashi Feng, and Ming-Hsuan Yang. Sa2va: Marrying sam2 with llava for dense grounded understanding of images and videos. arXiv, 2025. 62, 63 [118] Yu Yuan, Xijun Wang, Yichen Sheng, Prateek Chennuri, Xingguang Zhang, and Stanley Chan. Generative photography: Scene-consistent camera control for realistic text-to-image synthesis. arXiv preprint arXiv:2412.02168, 2024. 53, 54, [119] Cheng Zhang, Qianyi Wu, Camilo Cruz Gambardella, Xiaoshui Huang, Dinh Phung, Wanli Ouyang, and Jianfei Cai. Taming stable diffusion for text to 360 panorama image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 17 [120] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instructionguided image editing. In NeurIPS, 2023. 21, 25, 26, 27 [121] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023. 47 [122] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Scaling in-the-wild training for diffusion-based illumination harmonization and editing by imposing consistent light transport. In ICLR, 2025. 34, 46 [123] Wenwei Zhang, Jiangmiao Pang, Kai Chen, and Chen Change Loy. K-net: Towards unified image segmentation. Advances in Neural Information Processing Systems, 34:1032610338, 2021. 62, 65 [124] Xinchen Zhang, Ling Yang, Guohao Li, Yaqi Cai, Jiake Xie, Yong Tang, Yujiu Yang, Mengdi Wang, and Bin Cui. Itercomp: Iterative composition-aware feedback learning from model gallery for text-to-image generation. arXiv preprint arXiv:2410.07171, 2024. 5 [125] Yuxuan Zhang, Yiren Song, Jiaming Liu, Rui Wang, Jinpeng Yu, Hao Tang, Huaxia Li, Xu Tang, Yao Hu, Han Pan, et al. Ssr-encoder: Encoding selective subject representation for subject-driven generation. In CVPR, 2024. 28 [126] Chuyang Zhao, Yuxing Song, Wenhao Wang, Haocheng Feng, Errui Ding, Yifan Sun, Xinyan Xiao, and Jingdong Wang. Monoformer: One transformer for both diffusion and autoregression. arXiv preprint arXiv:2409.16280, 2024. 1 [127] Peng Zheng, Dehong Gao, Deng-Ping Fan, Li Liu, Jorma Laaksonen, Wanli Ouyang, and Nicu Sebe. Bilateral reference for high-resolution dichotomous image segmentation. CCAI, 2024. 69, 70, 73 [128] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. 1 [129] Donghao Zhou, Jiancheng Huang, Jinbin Bai, Jiaze Wang, Hao Chen, Guangyong Chen, Xiaowei Hu, and PhengAnn Heng. MagicTailor: Component-controllable personalization in text-to-image diffusion models. arXiv preprint arXiv:2410.13370, 2024. [130] Zhiyu Zhu, Yingcong Chen, Zhenyu Xie, and Jingyi Yu. Disenvisioner: Disentangled and enriched visual prompt for customized image generation. arXiv preprint arXiv:2410.02067, 2024. 28, 29 [131] Silvia Zuffi, Angjoo Kanazawa, and Michael Black. Lions and tigers and bears: Capturing non-rigid, 3d, articulated shape from images. In CVPR, 2018."
        }
    ],
    "affiliations": [
        "National University of Singapore",
        "Peking University",
        "The Chinese University of Hong Kong",
        "The Hong Kong University of Science and Technology (GZ)",
        "University of Washington",
        "Wuhan University"
    ]
}