{
    "paper_title": "Winning the Pruning Gamble: A Unified Approach to Joint Sample and Token Pruning for Efficient Supervised Fine-Tuning",
    "authors": [
        "Shaobo Wang",
        "Jiaming Wang",
        "Jiajun Zhang",
        "Cong Wang",
        "Yue Min",
        "Zichen Wen",
        "Fei Huang",
        "Huiqiang Jiang",
        "Junyang Lin",
        "Dayiheng Liu",
        "Linfeng Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As supervised fine-tuning (SFT) evolves from a lightweight post-training step into a compute-intensive phase rivaling mid-training in scale, data efficiency has become critical for aligning large language models (LLMs) under tight budgets. Existing data pruning methods suffer from a fragmented design: they operate either at the sample level or the token level in isolation, failing to jointly optimize both dimensions. This disconnect leads to significant inefficiencies--high-value samples may still contain redundant tokens, while token-level pruning often discards crucial instructional or corrective signals embedded in individual examples. To address this bottleneck, we introduce the Error-Uncertainty (EU) Plane, a diagnostic framework that jointly characterizes the heterogeneous utility of training data across samples and tokens. Guided by this insight, we propose Quadrant-based Tuning (Q-Tuning), a unified framework that strategically coordinates sample pruning and token pruning. Q-Tuning employs a two-stage strategy: first, it performs sample-level triage to retain examples rich in informative misconceptions or calibration signals; second, it applies an asymmetric token-pruning policy, using a context-aware scoring mechanism to trim less salient tokens exclusively from misconception samples while preserving calibration samples in their entirety. Our method sets a new state of the art across five diverse benchmarks. Remarkably, on SmolLM2-1.7B, Q-Tuning achieves a +38\\% average improvement over the full-data SFT baseline using only 12.5\\% of the original training data. As the first dynamic pruning approach to consistently outperform full-data training, Q-Tuning provides a practical and scalable blueprint for maximizing data utilization in budget-constrained LLM SFT."
        },
        {
            "title": "Start",
            "content": "Preprint. Under review. WINNING THE PRUNING GAMBLE: UNIFIED APPROACH TO JOINT SAMPLE AND TOKEN PRUNING FOR EFFICIENT SUPERVISED FINE-TUNING Shaobo Wang (cid:66)e,a Jiaming Wang e,n Jiajun Zhang e,b Cong Wang Yue Min e,h Zichen Wen Fei Huang Huiqiang Jiang Junyang Lin Dayiheng Liu Linfeng Zhang (cid:66)e EPIC Lab, SJTU * Equal contribution (cid:66) Corresponding authors (cid:135) Code (cid:109) Project Page Alibaba Group HKUST BJTU NJU 5 2 0 2 8 2 ] . [ 1 3 7 8 3 2 . 9 0 5 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "As supervised fine-tuning (SFT) evolves from lightweight post-training step into compute-intensive phase rivaling mid-training in scale, data efficiency has become critical for aligning large language models (LLMs) under tight budgets. Existing data pruning methods suffer from fragmented design: they operate either at the sample level or the token level in isolation, failing to jointly optimize both dimensions. This disconnect leads to significant inefficiencieshigh-value samples may still contain redundant tokens, while token-level pruning often discards crucial instructional or corrective signals embedded in individual examples. To address this bottleneck, we introduce the Error-Uncertainty (EU) Plane, diagnostic framework that jointly characterizes the heterogeneous utility of training data across samples and tokens. Guided by this insight, we propose Quadrant-based Tuning (Q-Tuning), unified framework that strategically coordinates sample pruning and token pruning. Q-Tuning employs two-stage strategy: first, it performs sample-level triage to retain examples rich in informative misconceptions or calibration signals; second, it applies an asymmetric token-pruning policy, using context-aware scoring mechanism to trim less salient tokens exclusively from misconception samples while preserving calibration samples in their entirety. Our method sets new state of the art across five diverse benchmarks. Remarkably, on SmolLM2-1.7B, Q-Tuning achieves +38% average improvement over the full-data SFT baseline using only 12.5% of the original training data. As the first dynamic pruning approach to consistently outperform full-data training, Q-Tuning provides practical and scalable blueprint for maximizing data utilization in budget-constrained LLM SFT."
        },
        {
            "title": "INTRODUCTION",
            "content": "The explosive growth of alignment datasetsnow routinely spanning billions of tokenshas fundamentally transformed Supervised Fine-Tuning (SFT) from lightweight post-training step into compute-intensive phase rivaling mid-training in scale (Ouyang et al., 2022; Dong et al., 2023; Yang et al., 2025; Achiam et al., 2023; Team et al., 2023). In this new regime, the primary challenge is no longer simply reducing data volume, but maximizing the utility of every retained token and samplea task that demands accurate, on-the-fly estimation of data value. Yet despite the emphasis on data efficiency, recent work reveals troubling paradox: even sophisticated dynamic pruning heuristics often underperform simple random sampling (Xia et al., 2024b). This starkly exposes fundamental disconnect between current strategies and the true utility of alignment data. At the heart of this challenge lies the fragmented design of existing pruning strategies. Samplelevel pruning methods (Qin et al., 2024; Zhou et al., 2023a; Wang et al., 2025a; Yang et al., 2024) identify high-potential examples but treat all tokens within them as equally valuableretaining redundant or even harmful content that dilutes alignment signals. Conversely, token-level pruning approaches (Lin et al., 2024; Xia et al., 2025; Chen et al., 2024b; Zhang et al., 2024c) apply contextagnostic heuristics uniformly across the dataset, blind to the semantic role of each sample. Such 1 Preprint. Under review. Figure 1: (a) Data Pruning. Sample Pruning reduces the training set size by selecting samples based on criteria such as loss, gradient magnitude, importance, diversity, and complexity; Token Pruning further refines individual samples by removing redundant or low-value tokens using metrics like similarity, entropy, and token importance. The pruned dataset is then used to fine-tune base LLM, achieving the objective to match the performance finetuned with the full data. (b) Error-Uncertainty (EU) Plane. two-dimensional analytical framework that categorizes training samples into four quadrants based on perplexity and entropy: Q1 (harmful noise unreliable or mislabeled data), Q2 (valuable misconceptions correctable errors that enhance learning), Q3 (redundant knowledge already mastered content), and Q4 (calibration data challenging yet informative samples that improve model calibration). (c) Q-Tuning. Unlike traditional approaches that apply sample and token pruning independently, our proposed Q-Tuning leverages the EU plane to jointly guide pruning at both levels. First, all harmful noise (Q1) and redundant knowledge (Q3) are pruned. Then, within valuable misconceptions (Q2), Q-Tuning selectively prunes detrimental tokens while preserving beneficial calibration samples (Q4) in full. one-size-fits-all policy fails to differentiate, for instance, between sample containing correctable misconceptionwhere only specific tokens need refinementand one serving as calibration anchor, which must be preserved holistically to maintain model stability. By operating in isolation, neither paradigm captures the interdependent nature of sample and token utility. This raises central question: How can we dynamically coordinate sample selection and token pruning within unified framework to maximize the true learning utility of limited data? To address this, we first formalize the problem as Generalized Dynamic Data Pruning, bilevel optimization framework for jointly optimizing sample and token pruning. We then introduce novel diagnostic lens, the Error-Uncertainty (EU) Plane, which categorizes training instances by mapping model error (perplexity) against model uncertainty (entropy). Specifically, as shown in Figure 1(b), training samples are categorized into four quadrants based on perplexity (low high, indicating uncertainty) and entropy (low high, indicating prediction confidence): (i) Q1 (Harmful Noise) unreliable or mislabeled data that actively harms learning and should therefore be removed via sample-level pruning; (ii) Q2 (Valuable Misconception) confidently wrong responses that, when pruned surgically at the token level, can be transformed into powerful teaching signals, making them ideal candidates for token-level pruning; (iii) Q3 (Redundant Knowledge) mastered content offering diminishing returns, best eliminated through sample-level pruning to improve efficiency without sacrificing performance; and (iv) Q4 (Calibration Data) hard but reliable samples essential for improving model confidence and robustness, which should be preserved in full neither sample nor token pruning should be applied. This insight directly motivates our solution: Quadrant-based Tuning (Q-Tuning), the first principled, integrated method for dynamic data pruning. Guided by EU Plane analysis, Q-Tuning implements two-stage, context-aware strategy, as shown in Figure 1(c). First, at the sample level, it acts as an intelligent triage system: retaining samples that offer clear signals for error correction or calibration (Q2 and Q4), while discarding those classified as harmful noise or redundant knowledge (Q1 and Q3). Crucially, Q-Tuning then applies asymmetric token policy: for confidently wrong samples (valuable misconceptions), it performs token pruning to isolate the core misconception and amplify the learning 2 Preprint. Under review. signal; for uncertain but correct samples (calibration data), it preserves full token sequences to ensure robust uncertainty modeling. Our contributions are as follows: 1. We formalize the joint sample-token pruning problem through the Generalized Dynamic Data Pruning framework bilevel optimization objective for hybrid pruning strategies. 2. We introduce the Error-Uncertainty (EU) Plane, tool that reveals and quantifies the heterogeneous value of data across error and uncertainty dimensions. Based on the EU analysis, we propose Q-Tuning, the first integrated, diagnosis-driven algorithm for dynamic pruning that coordinates sample and token decisions based on EU Plane insights. 3. Experiments demonstrate that Q-Tuning exceeds full-data training and all existing pruning baselines across 4 models, 5 benchmarks, and 6 different kinds of data budgets. Particularly, with LLaMA3-8B on GSM8K, Q-Tuning achieves 48.07 using only 35% of the data (surpassing full training by 6.0 and the best baseline by 9.9)."
        },
        {
            "title": "2 GENERALIZED DYNAMIC DATA PRUNING: A UNIFIED FRAMEWORK",
            "content": "We first propose Generalized Dynamic Data Pruning, framework for accelerating model training where samples and their constituent tokens are selectively and adaptively omitted at each step. Specifically, the framework first considers the coarse-grained sample level pruning, which involves identifying and discarding examples from mini-batch deemed less informative for the models state. Subsequently, token-level pruning operates on the retained samples, performing finer-grained selection to keep critical subset of tokens within each. This two-stage process is inherently dynamic: pruning criteria can be re-evaluated for each new mini-batch, allowing the data distribution used for gradient updates to evolve alongside the models parameters θ. The overarching objective is to focus computation on doubly-pruned data subset to maximize training efficiency while preserving or enhancing the models generalization. To formalize this framework, consider model fθ with parameters θ. At each training step t, mini-batch Bt is drawn from the training distribution D. The hierarchical pruning process can be modeled by two abstract operators: sample-level pruner Ψ and token-level pruner Φ. Stage 1: Sample-Level Pruning. The operator Ψ partitions mini-batch Bt into kept and discarded samples, governed by keep ratio rsample [0, 1]. The retained set, t, is defined as: = Bt Ψ(Bt) Bt, (1) where Ψ(Bt) represents the set of discarded samples, and the size of the retained set is constrained such that t = rsample Bt. Stage 2: Token-Level Pruning. For each sample in the retained set t, the operator Φ determines which tokens to keep, guided by token keep ratio rtoken [0, 1]. This is modeled by generating binary mask m(x) {0, 1}L(x) for each sample t, where L(x) is its sequence length. The final, doubly-pruned mini-batch Bt is constructed by applying these masks: Bt = Φ(B t) = {m(x) t}, (2) where denotes element-wise product, and each mask satisfies m(x)1 = rtokenL(x). Generalized Dynamic Data Pruning. We now put all things together into unified framework. The central problem of this framework is to identify the optimal dynamic pruning operators, (Φ, Ψ), that guide the training process to final model θ with the best possible generalization performance. These operators are applied at each step to transform mini-batch Bt into computationally cheaper, pruned version Bt, while adhering to predefined keep ratios (rsample, rtoken). This problem is naturally captured as bi-level optimization problem. The outer loop seeks optimal pruners, while the inner loop represents the iterative training procedure that produces the final model parameters under the guidance of these pruners. Formally, the objective is as follows: min Φ,Ψ E(x,y)Dtest [Ltest(fθ (x), y)] s.t. θ = arg min θ (cid:88) t=1 EBtDE (x,y) Bt [Ltrain(fθt(x), y)] (3) 3 Preprint. Under review. Figure 2: (a) Constructing the Error-Uncertainty (EU) Plane via Bisect Search. Given dataset of training samples, we first perform inference using the base LLM to compute perplexity (PPL) and entropy for each sample. These metrics are then used to plot the samples on the EU-plane. bisect search is applied to dynamically determine optimal thresholds (α high) that partition the plane into four quadrants: Q1 (Harmful Noise), Q2 (Valuable Misconception), Q3 (Redundant Knowledge), and Q4 (Calibration Data). (b) Sample Pruning for Q1 and Q3. Samples in Q1 (harmful noise) and Q3 (redundant knowledge) are pruned at the sample level, while those in Q2 and Q4 are retained for further processing. (c) Token-Level Pruning for Q2 Samples. For retained Q2 samples, token-level pruning is applied based on both the tokens own perplexity and the average perplexity of its neighboring tokens. Tokens with high local PPL are removed, preserving only the most informative ones, while all tokens in Q4 samples are kept intact to maintain calibration signals. high, β low, α low, β where at each step t: Bt = Φ(Bt Ψ(Bt)), with keep ratios (rsample, rtoken). Specifically, the outer objective defines the quality metric for any pair of pruners (Φ, Ψ): the final test performance of the model they produce. The inner objective defines the training process itself, where the final parameters θ result from cumulatively minimizing the loss over sequence of dynamically pruned mini-batches. All existing method that instantiates these operators Φ and Ψ can be seen as specific solution to this general problem."
        },
        {
            "title": "3 WINNING THE PRUNING GAMBLE",
            "content": "Building on the previous analysis, we now introduce Q-Tuning, dynamic pruning method guided by the ErrorUncertainty (EU) Plane. As shown in Figure 1, Q-Tuning proceeds in two coordinated stages: it first prunes harmful noise and redundant knowledge at the sample level, and then applies targeted token-level pruning to valuable misconceptions while preserving calibration data in full. This integrated strategy avoids the failure modes of one-dimensional heuristics and transforms pruning from gamble into principled, performance-enhancing process."
        },
        {
            "title": "3.1 THE ERROR-UNCERTAINTY PLANE: A DIAGNOSTIC LENS",
            "content": "We now formalize the ErrorUncertainty (EU) Plane introduced in Figure 1(b). Each training sample is positioned by two orthogonal axes: error, quantified by perplexity (PPL), and uncertainty, quantified by predictive entropy (Ent). Error: Measured via Perplexity. For training sample (x, y) with sequence length L(x), let (x) {1, . . . , L(x)} denote the set of trainable token positions. We define sample-level error as the perplexity (PPL), derived from the average token-level negative log-likelihood: PPL(x, y; fθ) = exp ( (cid:88) iT (x) log p(yi x, y<i; fθ) / (x)) High perplexity indicates the model finds the ground-truth continuation highly surprising, which is sign of either genuine difficulty or misconception. Uncertainty: Measured via Predictive Entropy. Independent of correctness, we quantify the models uncertainty using entropy (Ent), computed as the average token-level entropy over trainable 4 Preprint. Under review. positions, where denotes vocabulary token: Ent(x, y; fθ) = (cid:88) (cid:88) ( iT (x) vV p(v x, y<i; fθ) log p(v x, y<i; fθ)) / (x). High entropy signifies the model distributes probability mass broadly, reflecting uncertainty even when the top prediction is correct. Taken together, PPL and Ent map each sample onto the EU Plane, providing principled basis for partitioning data into the four quadrants illustrated in Figure 1(b)."
        },
        {
            "title": "3.2 Q-TUNING: SOLVING GENERALIZED DYNAMIC DATA PRUNING",
            "content": "Q-Tuning operationalizes the insights of the EU Plane into two-stage, per-batch dynamic pruning strategy, as illustrated in Figure 2. In the first stage, bisect search determines quantile-based thresholds that partition samples into four quadrants, enabling sample-level pruning of uninformative data. In the second stage, token-level pruning is selectively applied within retained misconception samples to isolate informative signals, while calibration samples are preserved in full. The pseudocode of our method is illustrated in Algorithm 1 in Appendix D. Stage 1: Constructing the EU Plane via Binary Search and Pruning Samples Accordingly. At each training step, we compute the perplexity (PPL) and entropy (Ent) for every sample Bt using gradient-free forward pass of the current model fθt. These statistics map each sample to point on the EU Plane (Figure 2(a)). Our objective is to determine quantile thresholds (α, β) on the PPL and Entropy axes such that the retained proportion of samples in Q2 Q4 exactly matches the target sample retention ratio rsample. Here Quantileγ(X) denotes the γ-quantile of variable X, i.e., the smallest value such that at least fraction γ of samples satisfy q. To locate (α, β), we perform bisect search on both axes. For each axis, the search interval is initialized as [0, 0.5]. At each iteration we set α = 1 2 (βlow + βhigh), and compute the proportion of samples that would fall into the tentative Q2 Q4 region defined by the current thresholds (α, β). The intervals are then updated as 2 (αlow + αhigh), β = 1 (αlow, αhigh, βlow, βhigh) = (cid:40)(αlow, α, βlow, β), if γ > rsample, (α, αhigh, β, βhigh), otherwise. The search converges within O(log(1/ε)) iterations (typically < 10), incurring negligible overhead. The resulting thresholds (α, β) partition the EU Plane (Figure 2(b)), from which Q2 and Q4 samples are retained and Q1 and Q3 discarded. Stage 2: Token-Level Pruning for Confident Errors. While Stage 1 removes entire samples deemed unhelpful, not all retained examples are equally homogeneous inside. In particular, samples in Q2 (Valuable Misconceptions) often contain mix of informative context and locally harmful tokens that mislead the model. To extract the useful signal, we apply token-level pruning that discards only the most detrimental tokens while preserving the surrounding context. In contrast, samples in Q4 (Calibration Data) are challenging yet reliable, and every token contributes to improving model calibration. Therefore, Q4 sequences are kept intact without any token pruning. For retained sample Q2 with target sequence y, let (x) denote the set of trainable token positions. For each token (x)(Figure 2(c)), we compute its token-level perplexity PPLi, which measures how surprising the ground-truth token yi is to the model. To avoid pruning rare but meaningful tokens based on isolated spikes, we compute smoothed importance score that incorporates local context: si(x, y; fθ) = (1 λ) PPLi(x, y; fθ) + λ[PPLi1(x, y; fθ) + PPLi+1(x, y; fθ)(cid:3), (4) where λ [0, 1] balances the contribution of neighboring tokens (default λ = 0.5). This smoothing reduces the risk of mistakenly removing single high-PPL tokens that may still be semantically critical, and we analyze the sensitivity of this choice in the ablation study (Section 4.4). 5 Preprint. Under review. token is deemed detrimental if both its own PPLi and the average PPL of its immediate neighbors exceed percentile-based threshold (e.g., the median). All tokens in (x) are subsequently ranked by their smoothed scores si, and only the top-rtoken fraction are retained to construct binary mask m(x). This mask selectively removes locally noisy tokens while preserving the informative context essential for learning. By contrast, no token-level pruning is applied to Q4 samples, as each token therein contributes to reliable calibration and must be preserved in full."
        },
        {
            "title": "4.1 EXPERIMENTAL SETTINGS",
            "content": "Models and Datasets. We conducted experiments on language models of different scales, including Mistral-7B (Jiang et al., 2023a), LLaMA2-7B (Touvron et al., 2023), LLaMA3-8B (Dubey et al., 2024), and the smaller SmolLM2-1.7B (Allal et al., 2025). To fine-tune these models, we considered two datasets that target complementary aspects of alignment: WizardLM (Xu et al., 2024a) for general instruction tuning, which provides high-quality instructionresponse pairs for supervised fine-tuning (SFT), and MathInstruct (Yue et al., 2023) for reasoning, which contains mathematically focused instructions designed to assess arithmetic and symbolic problem-solving. Evaluation. For general instruction tuning task, we used five standard benchmarks: ARC-E, ARCC (Clark et al., 2018), GSM8K (Cobbe et al., 2021), SQuAD (Rajpurkar et al., 2016), and TriviaQA (Joshi et al., 2017). These tasks spanned knowledge-intensive question answering, commonsense reasoning, and reading comprehension, offering broad assessment of model capability. For reasoning task, we evaluated models fine-tuned on MathInstruct using GSM8K and MATH (Hendrycks et al., 2021), two widely used benchmarks for arithmetic and mathematical problem solving. Baselines. We constructed baselines by pairing sample-level and token-level pruning strategies. For sample-level pruning, we considered four methods: (i) Random, which dropped samples uniformly at random; (ii) Longest, which removed the longest sequences to maximize computational savings; (iii) Entropy, which retained high-entropy samples assumed to be more informative; and (iv) InfoBatch (Qin et al., 2024), an information-theoretic approach selecting samples expected to yield the largest gradient updates. For token-level pruning, we adopted five methods applied to the retained data: (i) Random, which masked tokens uniformly at random; (ii) PPL, which removed high-perplexity tokens; (iii) Rho-1 (Lin et al., 2024), which preserved tokens with the largest excess loss relative to frozen reference model; (iv) FastV (Chen et al., 2024b), which pruned tokens receiving the least final-layer attention; and (v) SparseVLM (Zhang et al., 2024c), which combined attention and hidden-state features to score token importance and remove redundant image tokens."
        },
        {
            "title": "4.2 RESULTS ON INSTRUCTION DATASETS",
            "content": "Table 1 and table 5 summarize the empirical comparison between Q-Tuning and variety of baselines across different sample and token budgets on general instruction datasets. Outperforming the full data SFT baseline. Q-Tuning often matches or surpasses full-dataset fine-tuning while using only fraction of the training budget. On LLaMA2-7B, with 25% of the samples and 70% of the tokens, Q-Tuning achieves 36.9, closely matching the full-data baseline. On Mistral-7B, the same budget yields 46.2, slightly higher than the full-data result. Superiority under the same budgets with other baselines. When compared with methods such as InfoBatch, PPL, and SparseVLM, Q-Tuning achieves higher accuracy under the same budgets. For example, with 25% samples and 50% tokens, it improves LLaMA2-7B to 36.5, well above the best baseline. On Mistral-7B under the same setting, Q-Tuning exceeds the strongest baseline by 1.63%. Robust generalization across models and ratios. The advantages of Q-Tuning are consistent across model families and keep ratios. With 12.5% samples and 50% tokens, it outperforms the best pruning baselines by 3.3 points on LLaMA2-7B and 2.7 points on Mistral-7B. At larger budgets with 50% 6 Preprint. Under review. Table 1: Evaluation on Wizard dataset under different sample ratios (12.5%, 25%) and token ratios (50%, 70%), reported on ARC-E, ARC-C, GSM8K, SQuAD, TriviaQA, and their average, where and respectively denote improvements or degradations over the Random-Random baseline. Sample Pruner Token Pruner ARC-E ARC-C GSM8K SQuAD TriviaQA LLaMA2-7B Zero-Shot 53.44 38. 5.31 12.18 43.00 Avg. 30.58 ARC-E ARC-C GSM8K SQuAD TriviaQA 66.67 46. 18.35 10.01 43.77 Mistral-7B 12.5% Samples, 50% Tokens Random Longest InfoBatch Entropy 59.25 Random 60.49 1.24 PPL 59.96 0.71 FastV SparseVLM 54.32 4.93 59.96 0.71 Random 61.19 1.94 PPL 59.25 0.00 FastV SparseVLM 54.32 4.93 60.31 1.06 Random 59.43 0.18 PPL 58.90 0.35 FastV SparseVLM 54.67 4. 60.31 1.06 Random 60.49 1.24 PPL 58.91 0.34 FastV SparseVLM 55.20 4.05 41.02 43.39 2.37 42.37 1.35 37.97 3.05 44.41 3.39 43.73 2.71 43.05 2.03 38.31 2.71 41.36 0.34 40.34 0.68 43.39 2.37 40.00 1.02 42.37 1.35 43.73 2.71 43.05 2.03 38.98 2.04 8.11 7.20 0.91 5.76 2.35 7.35 0. 7.51 0.60 6.82 1.29 5.69 2.42 7.13 0.98 5.38 2.73 5.91 2.20 3.34 4.77 7.73 0.38 6.44 1.67 6.90 1.21 6.37 1.74 7.51 0.60 12.75 12.20 0.55 11.31 1.44 12.76 0.01 15.34 2.59 16.33 3.58 13.64 0.89 10.92 1.83 15.71 2.96 13.18 0.43 12.37 0.38 12.41 0. 14.10 1.35 14.53 1.78 13.03 0.28 12.65 0.10 48.75 48.04 0.71 46.42 2.33 44.65 4.10 48.91 0.16 48.16 0.59 46.98 1.77 43.77 4.98 47.74 1.01 48.31 0.44 46.88 1.87 45.07 3.68 48.09 0.66 48.76 0.01 47.05 1.70 46.14 2.61 33.98 34.26 0.28 33.17 0.81 31.41 2. 35.22 1.24 35.24 1.26 33.72 0.26 30.89 3.09 34.10 0.12 33.44 0.54 32.98 1.00 31.98 2.00 34.27 0.29 34.88 0.90 33.68 0.30 32.10 1.88 70.55 70.72 0.17 70.72 0.17 67.02 3.53 74.25 3.70 75.49 4.94 74.43 3.88 69.49 1.06 69.31 1.24 70.72 0.17 69.14 1.41 68.25 2. 72.13 1.58 72.84 2.29 73.90 3.35 68.08 2.47 48.14 48.47 0.33 46.44 1.70 44.75 3.39 48.81 0.67 50.17 2.03 49.15 1.01 46.10 2.04 45.76 2.38 47.12 1.02 45.42 2.72 45.08 3.06 48.81 0.67 47.80 0.34 47.12 1.02 44.07 4.07 22.74 25.78 3.04 18.80 3.94 20.24 2. 28.73 5.99 27.98 5.24 25.70 2.96 28.89 6.15 18.95 3.79 18.12 4.62 14.86 7.88 23.43 0.69 20.09 2.65 24.18 1.44 24.56 1.82 24.87 2.13 19.57 21.36 1.79 19.14 0.43 10.97 8.60 17.66 1.91 21.49 1.92 22.89 3.32 8.62 10.95 21.23 1.66 24.10 4.53 23.19 3.62 10.17 9. 17.55 2.02 22.80 3.23 23.96 4.39 10.72 8.85 52.63 53.92 1.29 51.56 1.07 44.61 8.02 55.73 3.10 56.55 3.92 54.15 1.52 50.30 2.33 50.39 2.24 51.26 1.37 50.58 2.05 45.34 7.29 54.69 2.06 54.69 2.06 54.67 2.04 47.00 5.63 Avg. 36.98 42.73 44.05 1.32 41.33 1.40 37.52 5.21 45.04 2.31 46.33 3.60 45.26 2.53 40.68 2.05 41.13 1.60 42.26 0.47 40.64 2.09 38.46 4.27 42.66 0.07 44.46 1.73 44.84 2.11 38.95 3.78 Q-Tuning (Ours) 64.20 4.95 42.03 1.01 10.54 2.43 18.79 6.04 53.12 4.37 37.74 3. 71.60 1.05 48.14 0.00 29.34 6.60 27.75 8.18 57.78 5.15 46.92 4. Full Dataset 61.55 42.37 8.64 13.80 50. 35.36 71.25 45.76 26.68 31.81 53. 45.84 12.5% Samples, 70% Tokens Random Longest InfoBatch Entropy 59.43 Random 60.14 0.71 PPL 58.20 1.23 FastV SparseVLM 54.67 4.76 59.44 0.01 Random 60.85 1.42 PPL 59.44 0.01 FastV SparseVLM 54.85 4.58 59.26 0.17 Random 60.49 1.06 PPL 58.55 0.88 FastV SparseVLM 56.61 2.82 61.02 1.59 Random 61.02 1.59 PPL 58.73 0.70 FastV SparseVLM 54.85 4.58 41.02 43.39 2.37 41.02 0.00 37.97 3.05 43.39 2.37 43.39 2.37 42.71 1.69 37.97 3. 42.37 1.35 39.32 1.70 43.39 2.37 38.31 2.71 43.05 2.03 43.39 2.37 43.39 2.37 37.29 3.73 6.97 6.22 0.75 6.29 0.68 8.04 1.07 7.35 0.38 7.73 0.76 6.29 0.68 7.05 0.08 6.22 0.75 5.76 1.21 5.53 1.44 5.76 1.21 7.66 0.69 6.97 0.00 6.14 0.83 6.52 0. 13.64 12.18 1.46 13.42 0.22 13.06 0.58 15.59 1.95 16.21 2.57 14.53 0.89 11.20 2.44 16.10 2.46 14.47 0.83 13.13 0.51 12.47 1.17 14.11 0.47 14.94 1.30 14.23 0.59 12.73 0.91 47.97 48.18 0.21 45.32 2.65 44.87 3.10 50.02 2.05 48.57 0.60 47.46 0.51 44.16 3. 47.72 0.25 48.06 0.09 47.64 0.33 44.49 3.48 48.44 0.47 48.94 0.97 47.03 0.94 46.24 1.73 33.81 34.02 0.21 32.85 0.96 31.72 2.09 35.15 1.34 35.35 1.54 34.09 0.28 31.04 2.77 34.33 0.52 33.62 0.19 33.65 0.16 31.53 2.28 34.86 1.05 35.05 1.24 33.90 0.09 31.53 2. 71.08 70.72 0.36 70.72 0.36 67.72 3.36 73.37 2.29 74.96 3.88 74.07 2.99 69.14 1.94 70.19 0.89 70.72 0.36 69.49 1.59 68.25 2.83 73.37 2.29 73.02 1.94 74.07 2.99 68.08 3.00 47.46 47.80 0.34 46.44 1.02 44.75 2.71 48.81 1.35 49.83 2.37 49.83 2.37 44.75 2. 47.80 0.34 46.44 1.02 43.39 4.07 44.41 3.05 49.83 2.37 47.46 0.00 50.85 3.39 44.41 3.05 24.34 25.09 0.75 19.56 4.78 23.65 0.69 27.82 3.48 28.73 4.39 24.18 0.16 31.01 6.67 20.77 3.57 19.03 5.31 16.68 7.66 23.73 0.61 23.05 1.29 24.03 0.68 24.94 0.60 26.38 2. 21.64 21.28 0.36 21.38 0.26 11.76 9.88 21.31 0.33 21.62 0.02 25.74 4.10 6.25 15.39 19.03 2.61 23.20 1.56 25.27 3.63 9.07 12.57 16.52 5.12 22.85 1.21 23.79 2.15 11.06 10.58 53.15 53.83 0.68 53.34 0.19 44.90 8.25 55.77 2.62 56.59 3.44 55.86 2.71 52.94 0. 52.13 1.02 51.75 1.40 51.47 1.68 45.73 7.42 55.18 2.03 54.89 1.74 55.94 2.79 46.68 6.47 43.53 43.74 0.21 42.29 1.24 38.58 4.95 45.42 1.89 46.35 2.82 45.94 2.41 40.82 2.71 41.98 1.55 42.23 1.30 41.26 2.27 38.24 5.29 43.59 0.06 44.45 0.92 45.92 2.39 39.32 4. Q-Tuning (Ours) 64.37 4.94 42.37 1.35 10.84 3.87 17.63 3.99 52.17 4. 37.48 3.67 71.78 0.70 48.14 0.68 30.33 6.00 28.59 6.95 57.93 4. 47.35 3.82 Full Dataset 61.55 42.37 8.64 13. 50.45 35.36 71.25 45.76 26.68 31. 53.67 45.84 Random Longest InfoBatch Entropy 60.32 Random 60.32 0.00 PPL 59.08 1.24 FastV SparseVLM 54.50 5.82 61.20 0.88 Random 60.85 0.53 PPL 59.08 1.24 FastV SparseVLM 56.61 3.71 58.73 1.59 Random 59.96 0.47 PPL 59.08 1.24 FastV SparseVLM 55.73 4.59 60.49 0.17 Random 60.49 0.17 PPL 58.91 1.41 FastV SparseVLM 54.67 5.65 41.69 42.03 0.34 41.69 0.00 38.64 3.05 42.03 0.34 43.39 1.70 42.71 1.02 37.29 4. 40.68 1.01 42.71 1.02 42.37 0.68 39.66 2.03 41.69 0.00 41.02 0.67 41.69 0.00 38.64 3.05 5.76 7.51 1.75 3.56 2.20 6.44 0.68 7.88 2.12 7.20 1.44 5.16 0.60 7.58 1.82 6.67 0.91 6.52 0.76 3.03 2.73 5.31 0.45 7.51 1.75 6.60 0.84 6.07 0.31 6.90 1. 25% Samples, 50% Tokens 13.43 15.94 2.51 12.78 0.65 12.04 1.39 15.40 1.97 13.88 0.45 14.00 0.57 12.09 1.34 9.95 3.48 14.58 1.15 11.13 2.30 11.66 1.77 15.94 2.51 14.92 1.49 12.79 0.64 11.64 1.79 48.41 48.58 0.17 45.60 2.81 44.79 3. 48.29 0.12 48.48 0.07 47.47 0.94 44.76 3.65 48.98 0.57 48.57 0.16 47.50 0.91 43.25 5.16 48.76 0.35 49.33 0.92 46.11 2.30 45.03 3.38 33.92 34.87 0.95 32.54 1.38 31.28 2.64 34.96 1.04 34.76 0.84 33.68 0.24 31.66 2.26 33.00 0.92 34.47 0.55 32.63 1.29 31.12 2. 34.88 0.96 34.47 0.55 33.11 0.81 31.38 2.54 70.19 69.66 0.53 71.78 1.59 67.55 2.64 73.54 3.35 72.31 2.12 72.66 2.47 66.84 3.35 70.55 0.36 71.08 0.89 69.31 0.88 67.20 3.00 70.19 0.00 71.43 1.24 72.31 2.12 68.25 1.94 46.10 47.46 1.36 47.12 1.02 46.44 0. 48.14 2.04 48.14 2.04 46.10 0.00 44.41 1.69 46.44 0.34 47.80 1.70 44.41 1.69 45.76 0.34 47.12 1.02 48.14 2.04 47.46 1.36 44.07 2.03 20.62 19.86 0.76 15.77 4.85 24.41 3.79 23.73 3.11 24.34 3.72 18.88 1.74 29.42 8.80 21.53 0.91 20.62 0.00 14.48 6.14 23.58 2. 22.44 1.82 21.30 0.68 18.04 2.58 26.69 6.07 24.07 19.51 4.56 26.97 2.90 11.80 12.27 26.34 2.27 23.84 -0.23 31.52 7.45 11.22 12.85 23.93 0.14 24.88 0.81 23.63 0.44 9.63 14.44 27.35 3.28 25.52 1.45 25.96 1.89 9.69 14.38 53.74 53.74 0.00 50.84 2.90 48.14 5. 54.06 0.32 55.22 1.48 52.13 1.61 48.47 5.27 52.14 1.60 51.61 2.13 49.16 4.58 46.09 7.65 54.78 1.04 55.60 1.86 52.46 1.28 47.24 6.50 42.95 42.05 0.90 42.50 0.45 39.67 3.28 45.16 2.21 44.77 1.82 44.26 1.31 40.07 2.88 42.92 0.03 43.20 0.25 40.20 2.75 38.45 4. 44.38 1.43 44.40 1.45 43.25 0.30 39.19 3.76 Q-Tuning (Ours) 63.14 2.82 42.03 0.34 8.87 3.11 16.76 3. 51.52 3.11 36.47 2.55 71.78 1.59 47.12 1.02 26.08 5.46 32.79 8. 56.17 2.43 46.79 3.84 Full Dataset 61.55 42.37 8. 13.80 50.45 35.36 71.25 45.76 26. 31.81 53.67 45.84 samples and 70% tokens, Q-Tuning further widens the margin, exceeding the strongest baselines by 2.4 and 3.7 points, respectively, while closely matching full-dataset performance."
        },
        {
            "title": "4.3 RESULTS ON REASONING DATASETS",
            "content": "Table 2 demonstrates the empirical comparison between Q-Tuning and variety of baselines across different sample and token budgets on math reasoning datasets. Matching or surpassing full-data results. Q-Tuning delivers consistent gains across reasoning benchmarks. On GSM8K, it largely improves LLaMA3-8B, Mistral-7B, and SmolLM-1.7B under the 25% 70% budget, all surpassing their full-data counterparts. On the more challenging MATH benchmark, Q-Tuning also exceeds the strongest baselines on both LLaMA3-8B and Mistral-7B. 7 Preprint. Under review. Table 2: Evaluation of pruning strategies on GSM8K and MATH using LLaMA3-8B, Mistral-7B, and SmolLM2-1.7B, comparing Zero-Shot, Full Dataset, multiple SampleToken pruner combinations, and our Q-Tuning under 25% samples with 50% tokens, 25% samples with 70% tokens, and 50% samples with 70% tokens settings. and respectively indicate improvements or degradations over the Random-Random baseline under the same sample and token keep ratio."
        },
        {
            "title": "Token\nPruner",
            "content": "Zero-Shot GSM8K 27.82 LLaMA3-8B MATH 2.26 Avg. 15.04 GSM8K 19.86 Mistral-7B MATH 3.30 Avg. 11.58 GSM8K SmolLM2-1.7B MATH 15.47 2.20 Avg. 8.83 25% Samples, 50% Tokens"
        },
        {
            "title": "Random\nPPL\nFastV\nSparseVLM",
            "content": "23.96 24.18 0.22 12.13 11.83 22.97 0.99 22.14 1.82 24.94 0.98 9.48 14.48 26.91 2.95 26.23 2.27 26.91 2.95 7.58 16.38 14.63 9.33 30.02 6.06 32.98 9.02 17.29 6.67 20.85 3.11 2.56 2.58 0.02 2.32 0.24 4.72 2.16 3.18 0.62 2.78 0.22 2.26 0.30 4.68 2. 2.42 0.14 2.66 0.10 1.88 0.68 3.26 0.70 3.66 1.10 2.92 0.36 2.66 0.10 5.12 2.56 13.26 13.38 0.12 7.23 6.03 13.85 0.59 12.66 0.60 13.86 0.60 5.87 7.39 15.80 2.54 14.33 1.07 14.79 1.53 4.73 8.53 8.95 4.31 16.84 3.58 17.95 4.69 9.97 3.29 12.98 0. 23.35 24.94 1.59 12.36 10.99 19.26 4.09 21.91 1.44 22.90 0.45 7.13 16.22 24.34 0.99 27.14 3.79 27.90 4.55 6.44 16.91 11.90 11.45 26.61 3.26 30.17 6.82 14.56 8.79 19.56 3.79 1.54 2.02 0.48 1.24 0.30 4.58 3.04 2.18 0.64 1.86 0.32 1.46 0.08 4.84 3. 2.24 0.70 2.52 0.98 1.34 0.20 1.94 0.40 2.08 0.54 1.76 0.22 1.12 0.42 4.20 2.66 12.45 13.48 1.03 6.80 5.65 11.92 0.53 12.05 0.40 12.38 0.07 4.29 8.16 14.59 2.14 14.69 2.24 15.21 2.76 3.89 8.56 6.92 5.53 14.35 1.90 15.97 3.52 7.84 4.61 11.88 0. 14.33 14.18 0.15 9.86 4.47 13.19 1.14 12.89 1.44 13.19 1.14 12.36 1.97 12.43 1.90 14.33 0.00 14.71 0.38 7.51 6.82 11.90 2.43 14.18 0.15 16.38 2.05 12.59 1.74 14.18 0.15 2.56 2.08 0.48 1.92 0.64 3.48 0.92 2.06 0.50 1.78 0.78 1.82 0.74 3.60 1. 1.66 0.90 1.90 0.66 1.62 0.94 4.36 1.80 2.24 0.32 2.40 0.16 2.36 0.20 2.90 0.34 8.44 8.13 0.31 5.89 2.55 8.34 0.10 7.47 0.97 7.49 0.95 7.09 1.35 8.02 0.42 7.99 0.45 8.30 0.14 4.56 3.88 8.13 0.31 8.21 0.23 9.39 0.95 7.47 0.97 8.54 0. Q-Tuning (Ours) 36.32 12.36 5.54 2.98 20.93 7.67 41.47 18.12 4.0 2. 22.74 10.29 21.83 7.50 3.90 1.34 12.87 4.43 25% Samples, 70% Tokens"
        },
        {
            "title": "Random\nPPL\nFastV\nSparseVLM",
            "content": "25.09 23.65 1.44 16.91 8.18 22.97 2.12 25.47 0.38 24.87 0.22 18.95 6.14 26.91 1.82 26.91 1.82 Random 25.93 0.84 PPL 16.83 8.26 FastV SparseVLM 14.63 10."
        },
        {
            "title": "Random\nPPL\nFastV\nSparseVLM",
            "content": "31.92 6.83 33.13 8.04 25.25 0.16 20.85 4.24 2.20 2.62 0.42 2.16 0.04 4.72 2.52 3.34 1.14 3.22 1.02 2.84 0.64 4.68 2.48 2.60 0.40 2.48 0.28 2.30 0.10 3.26 1.06 2.50 0.30 2.86 0.66 2.40 0.20 5.12 2.92 13.65 13.14 0.51 9.53 4.12 13.85 0. 14.41 0.76 14.04 0.39 10.90 2.75 15.80 2.15 14.76 1.11 14.20 0.55 9.57 4.08 8.95 4.70 17.21 3.56 18.00 4.35 13.82 0.17 12.98 0.67 24.11 25.32 1.21 16.07 8.04 19.26 4.85 21.83 2.28 23.65 0.46 12.96 11.15 24.34 0.23 28.96 4.85 31.46 7.35 13.87 10.24 11.90 12. 32.37 8.26 30.17 6.06 21.00 3.11 19.56 4.55 1.68 1.54 0.14 1.60 0.08 4.58 2.90 1.76 0.08 2.22 0.54 1.74 0.06 4.84 3.16 2.44 0.76 2.18 0.50 2.06 0.38 1.94 0.26 1.92 0.24 1.96 0.28 1.32 0.36 4.20 2.52 12.89 13.43 0.54 8.84 4.05 11.92 0. 11.80 1.09 12.94 0.05 7.35 5.54 14.59 1.70 15.70 2.81 16.82 3.93 7.97 4.92 6.92 5.97 17.15 4.26 16.07 3.18 11.16 1.73 11.88 1.01 13.80 13.04 0.76 12.89 0.91 13.19 0.61 12.96 0.84 14.33 0.53 12.59 1.21 12.43 1.37 13.72 0.08 14.86 1.06 10.84 2.96 11.90 1. 14.94 1.14 14.94 1.14 14.18 0.38 14.18 0.38 2.22 2.28 0.06 1.94 0.28 3.48 1.26 1.96 0.26 1.60 0.62 1.58 0.64 3.60 1.38 1.82 0.40 1.90 0.32 1.72 0.50 4.36 2.14 1.92 0.30 1.96 0.26 1.98 0.24 2.90 0.68 8.01 7.66 0.35 7.41 0.60 8.34 0. 7.46 0.55 7.96 0.05 7.08 0.93 8.02 0.01 7.77 0.24 8.38 0.37 6.28 1.73 8.13 0.12 8.43 0.42 8.45 0.44 8.08 0.07 8.54 0.53 Q-Tuning (Ours) 37.23 12.14 4.86 2. 21.04 7.39 42.99 18.88 5.08 3.40 24.56 11.67 22.90 9.10 3.64 1. 13.27 5.26 50% Samples, 70% Tokens"
        },
        {
            "title": "Random\nPPL\nFastV\nSparseVLM",
            "content": "26.23 1.67 30.25 2.35 18.42 9.48 18.95 8.95 29.42 1.52 28.73 0.83 19.33 8.57 9.10 18.80 32.60 4.70 Random 33.97 6.07 PPL 24.94 2.96 FastV SparseVLM 10.31 17.59 2.50 2.50 0.00 1.92 0.58 3.58 1.08 2.76 0.26 2.64 0.14 2.28 0.22 4.84 2.34 2.92 0.42 2.78 0.28 1.76 0.74 3.12 0. 2.14 0.36 3.10 0.60 2.26 0.24 4.56 2.06 15.20 14.97 0.23 10.06 5.14 6.95 8.25 14.50 0.70 16.45 1.25 10.35 4.85 11.90 3.30 16.17 0.97 15.76 0.56 10.55 4.65 6.11 9.09 17.37 2.17 18.53 3.33 13.60 1.60 7.44 7.76 32.30 31.99 0.31 17.44 14.86 12.21 20. 27.82 4.48 31.46 0.84 18.65 13.65 19.48 12.82 35.03 2.73 38.82 6.52 18.04 14.26 12.05 20.25 38.67 6.37 40.18 7.88 21.53 10.77 9.40 22.90 2.46 2.04 0.42 1.72 0.74 2.86 0.40 2.20 0.26 2.00 0.46 1.82 0.64 4.74 2.28 2.50 0.04 2.98 0.52 1.94 0.52 2.32 0. 2.10 0.36 2.30 0.16 1.56 0.90 4.40 1.94 17.38 17.02 0.36 9.58 7.80 7.53 9.85 15.01 2.37 16.73 0.65 10.24 7.14 12.11 5.27 18.76 1.38 20.90 3.52 9.99 7.39 7.19 10.19 20.38 3.00 21.24 3.86 11.55 5.83 6.90 10.48 14.94 16.15 1.21 12.13 2.81 11.90 3. 15.69 0.75 16.45 1.51 12.43 2.51 11.07 3.87 16.00 1.06 15.39 0.45 12.05 2.89 10.84 4.10 18.88 3.94 17.21 2.27 16.22 1.28 13.95 0.99 1.76 1.80 0.04 1.60 0.16 3.34 1.58 1.92 0.16 1.94 0.18 1.94 0.18 3.90 2.14 1.78 0.02 2.20 0.44 1.86 0.10 3.96 2. 2.18 0.42 2.06 0.30 1.78 0.02 3.68 1.92 8.35 8.97 0.62 6.87 1.48 7.62 0.73 8.81 0.46 9.20 0.85 7.19 1.16 7.48 0.87 8.89 0.54 8.80 0.45 6.96 1.39 7.40 0.95 10.53 2.18 9.64 1.29 9.00 0.65 8.82 0.47 Q-Tuning (Ours) 38.21 10.31 4.30 1.80 21.26 6.06 48.07 15.77 6.14 3.68 26.57 9. 20.47 5.53 3.20 1.44 11.84 3."
        },
        {
            "title": "Full Dataset",
            "content": "32.90 3.02 17.96 42.08 3.08 22. 16.53 2.10 9.31 Generalization across scales. Q-Tuning scales reliably across model sizes. Averaged over GSM8K and MATH, it reaches 21.5 on LLaMA3-8B, 26.6 on Mistral-7B, and 11.8 on SmolLM-1.7B, all notably higher than their full-dataset counterparts. 8 Preprint. Under review. Figure 3: Effect of varying (a) batch size (8, 16, 32) and (b) neighbor awareness λ (0, 0.3, 0.5, 0.7, 1.0) for Mistral-7B under three datatoken keep ratio configurations (25%50%, 25%70%, 50%70%). Results are reported on GSM8K, SQuAD, and TriviaQA. Dashed lines marked full denote models trained on the full data without pruning. Additional benchmark results (Avg. of five benchmarks, ARC-E, ARC-C) are provided in the Figure 5 in Appendix C.2."
        },
        {
            "title": "4.4 ABLATION STUDY",
            "content": "Sensitivity of batch size in dynamic sample pruning. An important question is how batch size influences the performance of Q-Tuning. To study this, we varied the batch size across {8, 16, 32} and evaluated GSM8K, SQuAD, and TriviaQA. As shown in the upper part of Figure 3, larger batch sizes generally improve performance, especially on GSM8K and SQuAD, where the gains are more pronounced. In contrast, TriviaQA remains relatively stable across all settings, suggesting limited sensitivity to batch size. Overall, these results indicate that Q-Tuning is robust to the choice of batch size, with moderate to larger batches offering additional benefits in certain tasks. Effectiveness of context awareness λ. We also examined the impact of neighbor awareness, controlled by the coefficient λ. As shown in the lower part of Figure 3, moderate values of λ improve performance on GSM8K and SQuAD, whereas extreme values yield diminishing or In contrast, TriviaQA shows little sensitivity to λ, with performance remainunstable gains. ing stable across all settings. These results suggest that Q-Tuning benefits from incorporating neighbor awareness, but only up to moderate level, beyond which gains are marginal. Ablation study on Token Pruning Avg."
        },
        {
            "title": "Method",
            "content": "Table 3: Ablation study on token pruner in Q-tuning. ARC-E ARC-C GSM8K SQuAD TriviaQA Method in Q-Tuning. Table 3 compares token pruning methods under fixed 25% sample and 50% token keep ratio on Mistral-7B. Rho-1 uses reference model obtained by training Mistral-7B on the full dataset for 3 epochs. While baseline methods such as Rho-1 and PPL perform competitively on certain benchmarks, they exhibit inconsistent or imbalanced performance across tasks. In contrast, our Neighbor-aware PPL achieves the best overall balance and attains the highest average score. Notably, reversing the token scoring logic (i.e., pruning low-PPL tokens instead of high-PPL ones) significantly degrades performance, underscoring the importance of our context-aware pruning strategy. Rho-1 PPL (λ = 0) Reversed PPL (λ = 0.5) PPL (λ = 0.5) 55.43 56.54 55.47 56.17 69.66 71.60 73.02 71. 24.03 25.32 16.68 26.08 46.78 46.44 47.12 47.12 29.23 29.71 32.01 32.79 45.03 45.92 44.86 46.79 Can Q-Tuning Outperform Other Methods in Independent Sample and Token Pruning? The core innovation of Q-Tuning is its rejection of the naive, independent application of sample and token pruning. To evaluate whether each pruning strategy is effective on its own, we conducted ablation studies. Figure 4 provides direct comparison under controlled conditions: panel (a) illustrates dynamic sample pruning with all tokens retained, while panel (b) shows dynamic token pruning with all samples retained. Across both scenarios, Q-Tuning consistently surpasses all baseline methods. How samples distribution evolve during training? We conducted experiments on different sample pruners and visualized the average entropy and PPL for samples. Tracking 100 randomly sampled instances, we found that Q-Tuning reduces both perplexity and entropy more rapidly than other strategies, which in turn facilitates better model performance (see Figure 6 in Appendix C.2). 9 Preprint. Under review. Figure 4: Comparison of independent (a) dynamic sample pruning and (b) dynamic token pruning across different keep ratios for LLaMA2-7B and Mistral-7B. Q-Tuning still outperforms all baselines."
        },
        {
            "title": "5 CONCLUSION",
            "content": "This work turns risky dynamic data pruning for LLM fine-tuning into reliable, high-utility strategy. By analyzing sample modes via the novel Error-Uncertainty Plane, we expose datas heterogeneous value and the need for nuance. Our Quadrant-based Tuning (Q-Tuning) uses two-stage framework to smartly coordinate sampleand token-level pruning, preserving valuable signals while removing noiseboosting efficiency without sacrificing (and often improving) performance."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Martín Blázquez, Guilherme Penedo, Lewis Tunstall, Andrés Marafioti, Hynek Kydlíˇcek, Agustín Piqueres Lajarín, Vaibhav Srivastav, et al. Smollm2: When smol goes bigdata-centric training of small language model. arXiv preprint arXiv:2502.02737, 2025. Abdul Hameed Azeemi, Ihsan Ayyub Qazi, and Agha Ali Raza. Language model-driven data pruning enables efficient active learning. arXiv preprint arXiv:2410.04275, 2024. Hao Chen, Yujia Li, Jie Wang, et al. Fastv: Fast visual token pruning for multimodal large language models. In European Conference on Computer Vision (ECCV), 2024a. Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large visionlanguage models. In European Conference on Computer Vision, pp. 1935. Springer, 2024b. Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, et al. Alpagasus: Training better alpaca with fewer data. arXiv preprint arXiv:2307.08701, 2023. Tzu-Chun Chien, Chieh-Kai Lin, Shiang-Feng Tsai, Ruei-Chi Lai, Hung-Jen Chen, and Min Sun. Grounding-aware token pruning: Recovering from drastic performance drops in visual grounding caused by pruning. arXiv preprint arXiv:2506.21873, 2025. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv:1803.05457v1, 2018. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Cody Coleman, Christina Yeh, Stephen Mussmann, Baharan Mirzasoleiman, Peter Bailis, Percy Liang, and Jure Leskovec. Selection via proxy: Efficient data selection for deep learning. In International Conference on Learning Representations (ICLR), 2020. OpenCompass Contributors. Opencompass: universal evaluation platform for foundation models, 2023. 10 Preprint. Under review. Wei Deng, Han Li, Ming Zhou, et al. Less is more: Rethinking preference data selection for alignment. In International Conference on Learning Representations (ICLR), 2025. Guanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayiheng Liu, Wei Wang, Zheng Yuan, Chang Zhou, and Jingren Zhou. How abilities in large language models are affected by supervised fine-tuning data composition. arXiv preprint arXiv:2310.05492, 2023. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pp. arXiv2407, 2024. Qichen Fu, Minsik Cho, Thomas Merth, Sachin Mehta, Mohammad Rastegari, and Mahyar Najibi. Lazyllm: Dynamic token pruning for efficient long context llm inference. arXiv preprint arXiv:2407.14057, 2024. Yangyang Guo and Mohan Kankanhalli. Scan: Bootstrapping contrastive pre-training for data efficiency. arXiv preprint arXiv:2411.09126, 2024. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Xijie Huang, Li Lyna Zhang, Kwang-Ting Cheng, Fan Yang, and Mao Yang. Fewer is more: Boosting llm reasoning with reinforced context pruning. arXiv preprint arXiv:2312.08901, 2023. Ziyu Huang, Xinyu Li, Yicheng Zhao, et al. Mates: Model-aware training example selection. In Advances in Neural Information Processing Systems (NeurIPS), 2024. Hamish Ivison, Noah Smith, Hannaneh Hajishirzi, and Pradeep Dasigi. Data-efficient finetuning using cross-task nearest neighbors. arXiv preprint arXiv:2212.00196, 2022. Hamish Ivison, Muru Zhang, Faeze Brahman, Pang Wei Koh, and Pradeep Dasigi. Large-scale data selection for instruction tuning. arXiv preprint arXiv:2503.01807, 2025. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b, 2023a. Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Llmlingua: Compressing prompts for accelerated inference of large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 1335813376, 2023b. Zhen Jin, Kai Zhang, Jingxuan Liu, et al. D3: Diversity, difficulty, and dependability-aware data selection. In International Conference on Machine Learning (ICML), 2025. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017. Christopher Keith, Michael Robinson, Francis Duncan, Allan Worthington, Joseph Wilson, and Sofia Harris. Optimizing large language models: novel approach through dynamic token pruning. 2024. Chen Li, Rui Wang, and Wei Zhang. Learning to weight data: bi-level optimization framework for dynamic data selection. In International Conference on Machine Learning (ICML), 2025. YUCHENG LI, BO DONG, Frank Guerin, and Chenghua Lin. Compressing context to enhance inference efficiency of large language models. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. Zhenghao Lin, Zhibin Gou, Yeyun Gong, Xiao Liu, Yelong Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, Nan Duan, and Weizhu Chen. Not all tokens are what you need for pretraining. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang (eds.), Advances in Neural Information Processing Systems, volume 37, pp. 2902929063. Curran Associates, Inc., 2024. 11 Preprint. Under review. Lingkun Long, Rubing Yang, Yushi Huang, Desheng Hui, Ao Zhou, and Jianlei Yang. Sliminfer: Accelerating long-context llm inference via dynamic token pruning. arXiv preprint arXiv:2508.06447, 2025. Sören Mindermann, Jan Brauner, Muhammed Razzak, Mrinank Sharma, Andreas Kirsch, Winnie Xu, Benedikt Höltgen, Aidan Gomez, Adrien Morisot, Sebastian Farquhar, et al. Prioritized training on points that are learnable, worth learning, and not yet learnt. In International Conference on Machine Learning, pp. 1563015649. PMLR, 2022. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022. Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue Zhang, Qingwei Lin, Victor Rühle, Yuqing Yang, Chin-Yew Lin, et al. Llmlingua-2: Data distillation for efficient and faithful task-agnostic prompt compression. In ACL (Findings), 2024. Rui Pang, Lei Zhao, Shuo Wang, et al. Token cleaning: Fine-grained data selection for language model fine-tuning. 2025. Ziheng Qin, Kai Wang, Zangwei Zheng, Jianyang Gu, Xiangyu Peng, Zhaopan Xu, Daquan Zhou, Lei Shang, Baigui Sun, Xuansong Xie, et al. Infobatch: Lossless training speed up by unbiased dynamic data pruning. In ICLR, 2024. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016. Ravi Raju, Kyle Daruwalla, and Mikko Lipasti. Accelerating deep learning with dynamic data pruning. arXiv preprint arXiv:2111.12621, 2021. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Shaobo Wang, Xiangqi Jin, Ziming Wang, Jize Wang, Jiajun Zhang, Kaixin Li, Zichen Wen, Zhong Li, Conghui He, Xuming Hu, and Linfeng Zhang. Data whisperer: Efficient data selection for task-specific llm fine-tuning via few-shot in-context learning. Annual Meeting of the Association for Computational Linguistics, 2025a. Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, et al. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939, 2025b. Zichen Wen, Yifeng Gao, Shaobo Wang, Junyuan Zhang, Qintong Zhang, Weijia Li, Conghui He, and Linfeng Zhang. Stop looking for important tokens in multimodal language models: Duplication matters more. arXiv preprint arXiv:2502.11494, 2025. Heming Xia, Chak Tou Leong, Wenjie Wang, Yongqi Li, and Wenjie Li. Tokenskip: Controllable chain-of-thought compression in llms. arXiv preprint arXiv:2502.12067, 2025. Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. Less: Selecting influential data for targeted instruction tuning. arXiv preprint arXiv:2402.04333, 2024a. Tingyu Xia, Bowen Yu, Kai Dang, An Yang, Yuan Wu, Yuan Tian, Yi Chang, and Junyang Lin. Rethinking data selection at scale: Random selection is almost all you need. arXiv preprint arXiv:2410.09335, 2024b. 12 Preprint. Under review. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Qingwei Lin, and Daxin Jiang. Wizardlm: Empowering large pre-trained language models to follow complex instructions. In The Twelfth International Conference on Learning Representations, 2024a. Guo-Hao Xu, Jingzhen Ding, Huping Ding, Zhao Xu, and Kaifu Zhang. Ftp: Efficient prefilling for long-context llm inference via ffn token pruning. 2024b. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Yu Yang, Siddhartha Mishra, Jeffrey Chiang, and Baharan Mirzasoleiman. Smalltolarge (s2l): Scalable data selection for fine-tuning large language models by summarizing training trajectories of small models. Advances in Neural Information Processing Systems, 37:8346583496, 2024. Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning. arXiv preprint arXiv:2502.03387, 2025. Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint arXiv:2309.05653, 2023. Guibin Zhang, Haonan Dong, Zhixun Li, Dingshuo Chen, Kai Wang, Tianlong Chen, Yuxuan Liang, Dawei Cheng, Kun Wang, et al. Gder: Safeguarding efficiency, balancing, and robustness via prototypical graph pruning. Advances in Neural Information Processing Systems, 37:5028550312, 2024a. Lei Zhang, Tong Xu, Ming Gao, et al. Sparsevlm: Training-free sparse visual token pruning for vision-language models. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2024b. Yuan Zhang, Chun-Kai Fan, Junpeng Ma, Wenzhao Zheng, Tao Huang, Kuan Cheng, Denis Gudovskiy, Tomoyuki Okuno, Yohei Nakata, Kurt Keutzer, et al. Sparsevlm: Visual token sparsification for efficient vision-language model inference. arXiv preprint arXiv:2410.04417, 2024c. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. Advances in Neural Information Processing Systems, 36:5500655021, 2023a. Daquan Zhou, Kai Wang, Jianyang Gu, Xiangyu Peng, Dongze Lian, Yifan Zhang, Yang You, and Jiashi Feng. Dataset quantization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1720517216, 2023b. Fan Zhou, Zengzhi Wang, Qian Liu, Junlong Li, and Pengfei Liu. Programming every example: Lifting pre-training data quality like experts at scale. arXiv preprint arXiv:2409.17115, 2024. 13 Preprint. Under review."
        },
        {
            "title": "A RELATED WORK",
            "content": "A.1 SAMPLE PRUNING growing body of work explores sample pruning as means to reduce training cost by selecting smaller yet higher-quality subsets of data without compromising performance (Ivison et al., 2025; Yang et al., 2024; Xia et al., 2024a; Zhou et al., 2024). For example, LIMA (Zhou et al., 2023a) shows that aligning LLMs requires only small collection of high-quality instructionresponse pairs rather than large-scale corpora. Broadly, existing approaches fall into static and dynamic pruning. In the static setting, data subsets are determined in advance using fixed criteria: SVP (Coleman et al., 2020) leverages proxy models to estimate sample importance, D3 (Jin et al., 2025) combines diversity, difficulty, and dependability into weighted coresets, and Less is More (Deng et al., 2025) shows that carefully curated preference subsets can outperform full datasets by filtering noisy or redundant examples. Other static approaches target in-context examples: LIMO (Ye et al., 2025) removes redundant demonstrations while distilling essential reasoning patterns, and methods such as DEFT (Ivison et al., 2022), Alpagasus (Chen et al., 2023), and Data Whisperer (Wang et al., 2025a) automatically select or reweight demonstrations based on influence estimation, contribution to performance, or few-shot evaluation. In contrast, dynamic pruning adapts sample usage throughout training. Early work by (Raju et al., 2021) proposed two methods, ϵ-greedy and UCB, which retain uncertain examples while discarding easier ones. Subsequent approaches explore alternative criteria for adaptive pruning: RHOLOSS (Mindermann et al., 2022) assigns importance using high-quality reference models, InfoBatch (Qin et al., 2024) removes low-loss examples on the fly, and more recent methods such as MATES (Huang et al., 2024) and DWM (Li et al., 2025) reweight or select samples according to the evolving model state.Beyond single-example pruning, several methods consider structured or representation-based criteria: ActivePrune (Azeemi et al., 2024) selects demonstrations per input via similarity metrics, SCAN (Guo & Kankanhalli, 2024) iteratively removes ill-matched and redundant data during contrastive pretraining, GDeR (Zhang et al., 2024a) models data as prototypical graph to prune noisy or redundant samples, and DQ (Zhou et al., 2023b) clusters data in representation space and replaces each cluster with representative subsets. A.2 TOKEN PRUNING Parallel to sample pruning, recent research (Wang et al., 2025b; Pan et al., 2024; Keith et al., 2024) has increasingly focused on token-level pruning to retain only the most informative portions of input sequences. At the pretraining stage, methods such as Rho-1 (Lin et al., 2024) leverage reference models to estimate token importance, discarding uninformative tokens. In fine-tuning and inference, pruning is often performed dynamically: Token Cleaning (Pang et al., 2025) identifies harmful tokens as noisy labels and prunes them based on their estimated influence on parameter updates, while approaches like LazyLLM (Fu et al., 2024) and SlimInfer (Long et al., 2025) accelerate long-context inference by selectively dropping tokens or hidden-state blocks with the help of attention signals and cache mechanisms. Along similar lines, TokenSkip (Xia et al., 2025) removes redundant reasoning steps by skipping semantically less important tokens, and FTP (Xu et al., 2024b) reduces prefilling cost through grouped token processing and optimized attention computation. Another family of approaches instead performs input compression before inference, exemplified by LLMLingua (Jiang et al., 2023b) and Selective Context (LI et al., 2023), which prune low-information tokens, phrases, or sentences through coarse-to-fine filtering with budget control. Beyond text-only models, multimodal pruning exploits cross-modal attention patterns: FastV (Chen et al., 2024a) and SparseVLM (Zhang et al., 2024b) drop redundant visual tokens guided by attention, while LMTL (Huang et al., 2023) dynamically adjusts computation by pruning unnecessary visual features. More recent advances further refine pruning with task-specific objectives, such as GAP (Chien et al., 2025), which preserves spatial grounding by correcting position misalignments during token removal, and DART (Wen et al., 2025), which reduces token-level computation through confidence-based early stopping. Despite their progress, prior work typically treats sample pruning and token pruning as independent lines of research. Sample pruning methods focus on reducing the number of training examples but leave token-level redundancy unaddressed, while token pruning techniques emphasize sequencelevel efficiency without considering redundancy across training samples. Such separation limits the potential for jointly optimizing efficiency and effectiveness. In this work, we bridge these directions 14 Preprint. Under review. by proposing unified framework that integrates sample-level and token-level pruning, enabling models to simultaneously filter uninformative data and compress redundant inputs."
        },
        {
            "title": "B ADDITIONAL EXPERIMENTAL SETTINGS",
            "content": "B."
        },
        {
            "title": "IMPLEMENTATION DETAILS",
            "content": "All experiments were conducted using the LLaMA-Factory framework (Zheng et al., 2024), which provided utilities for training and evaluation across diverse large language models. For benchmark evaluation, we used the OpenCompass framework (Contributors, 2023), which offered standardized interface to wide range of tasks. Unless otherwise specified, models were trained for 3 epochs with learning rate of 1 104 and batch size of 8 per device. In all pruning experiments, we applied pruning exclusively to the question part of each input, leaving answers intact. All experiments were performed on NVIDIA A100 GPUs. For the Random baseline, we repeated each experiment five times and reported the averaged results. B.2 HYPERPARAMETERS Table 4: Complete hyperparameter configurations employed in our experiments."
        },
        {
            "title": "Hyperparameter",
            "content": "λ Train batch size Epoch Learning rate Lora rank Cutoff length Gradient accumulation step Learning rate scheduler type"
        },
        {
            "title": "Value",
            "content": "0.5 8 3 1 104 8 2048 4 cosine We provide the full hyperparameter settings used in our experiments in Table 4. Unless otherwise noted, all other parameters follow the default settings of the LLaMA-Factory framework. 15 Preprint. Under review."
        },
        {
            "title": "C ADDITIONAL EXPERIMENTAL RESULTS",
            "content": "C.1 RESULTS ON INSTRUCTION DATASETS Table 5: Evaluation on Wizard dataset under different sample ratios (25%, 50%) and token ratios (50%, 70%), reported on ARC-E, ARC-C, GSM8K, SQuAD, TriviaQA, and their average, where and respectively denote improvements or degradations over the Random-Random baseline. Sample Pruner Token Pruner ARC-E ARC-C GSM8K SQuAD TriviaQA LLaMA2-7B Zero-Shot 53. 38.98 5.31 12.18 43.00 Avg. 30. ARC-E ARC-C GSM8K SQuAD TriviaQA 66. 46.10 18.35 10.01 43.77 Mistral-7B Random Longest InfoBatch Entropy 60.67 Random 60.32 0.35 PPL 60.32 0.35 FastV SparseVLM 55.73 4.94 61.02 0.35 Random 61.20 0.53 PPL 59.26 1.41 FastV SparseVLM 55.73 4.94 59.79 0.88 Random 60.32 0.35 PPL 59.61 1.06 FastV SparseVLM 55.56 5. 60.67 0.00 Random 61.55 0.88 PPL 59.79 0.88 FastV SparseVLM 54.50 6.17 41.69 42.03 0.34 42.71 1.02 37.97 3.72 41.69 0.00 43.05 1.36 43.05 1.36 36.27 5.42 42.71 1.02 43.39 1.70 42.71 1.02 38.31 3.38 41.02 0.67 41.02 0.67 42.03 0.34 37.97 3.72 6.22 5.91 0.31 6.07 0.15 7.20 0. 7.20 0.98 7.28 1.06 5.53 0.69 8.04 1.82 5.84 0.38 6.67 0.45 3.34 2.88 5.38 0.84 5.99 0.23 5.38 0.84 6.44 0.22 8.87 2.65 25% Samples, 70% Tokens 13.91 15.88 1.97 14.92 1.01 11.83 2.08 15.61 1.70 14.06 0.15 13.98 0.07 11.39 2. 10.42 3.49 13.88 0.55 12.18 1.73 10.81 3.10 14.92 1.01 14.90 0.99 13.21 0.70 12.25 1.66 48.77 48.56 0.21 47.20 1.57 44.96 3.81 49.53 0.76 48.87 0.10 48.03 0.74 44.65 4.12 48.99 0.22 49.03 0.26 47.98 0.79 43.08 5.69 49.34 0.57 49.72 0.95 47.58 1.19 45.05 3. 34.25 34.54 0.29 34.24 0.01 31.54 2.71 35.01 0.76 34.89 0.64 33.97 0.28 31.22 3.03 33.55 0.70 34.66 0.41 33.16 1.09 30.63 3.62 34.39 0.14 34.51 0.26 33.81 0.44 31.73 2.52 70.02 69.49 0.53 71.25 1.23 67.72 2.30 74.43 4.41 72.49 2.47 72.84 2.82 68.08 1. 70.37 0.35 71.08 1.06 69.66 0.36 65.78 4.24 70.37 0.35 70.90 0.88 71.43 1.41 67.20 2.82 46.78 48.14 1.36 47.80 1.02 44.75 2.03 47.80 1.02 47.46 0.68 47.80 1.02 43.73 3.05 46.44 0.34 46.10 0.68 45.76 1.02 45.08 1.70 48.47 1.69 48.14 1.36 47.12 0.34 45.76 1. 19.71 21.08 1.37 18.95 0.76 25.32 5.61 24.56 4.85 25.55 5.84 19.79 0.08 25.78 6.07 21.00 1.29 22.44 2.73 15.39 5.21 20.70 0.99 22.21 2.50 21.15 1.44 20.24 0.53 25.17 5.46 23.50 20.63 2.87 24.29 0.79 12.69 10.81 26.79 3.29 23.92 0.42 31.54 8.04 13.47 10. 24.09 0.59 22.07 1.43 24.75 1.25 10.97 12.53 27.57 4.07 26.22 2.72 26.06 2.56 9.64 13.86 52.93 53.73 0.80 52.07 0.86 47.46 5.47 55.24 2.31 55.26 2.33 53.24 0.31 48.00 4.93 53.75 0.82 52.17 0.76 49.51 3.42 48.66 4.27 56.05 3.12 55.83 2.90 54.40 1.47 48.38 4. Avg. 36.98 42.59 42.61 0.02 42.87 0.28 39.59 3.00 45.76 3.17 44.93 2.34 45.04 2.45 39.81 2.78 43.13 0.54 42.77 0.18 41.01 1.58 38.24 4.35 44.93 2.34 44.45 1.86 43.85 1.26 39.23 3. Q-Tuning (Ours) 62.43 1.76 42.37 0.68 9.25 3.03 19.53 5.62 50.78 2. 36.87 2.62 71.60 1.58 47.12 0.34 26.61 6.90 30.64 7.14 55.13 2. 46.22 3.63 Full Dataset 61.55 42.37 8.64 13. 50.45 35.36 71.25 45.76 26.68 31. 53.67 45.84 Random Longest InfoBatch Entropy 62.08 Random 61.02 1.06 PPL 60.32 1.76 FastV SparseVLM 55.73 6.35 60.85 1.23 Random 62.08 0.00 PPL 60.32 1.76 FastV SparseVLM 56.97 5.11 58.38 3.70 Random 60.67 1.41 PPL 59.44 2.64 FastV SparseVLM 56.08 6.00 59.96 2.12 Random 60.49 1.59 PPL 59.26 2.82 FastV SparseVLM 56.97 5.11 41.36 43.05 1.69 42.03 0.67 38.31 3.05 42.03 0.67 41.69 0.33 40.68 0.68 39.66 1. 42.71 1.35 42.71 1.35 41.36 0.00 37.97 3.39 42.71 1.35 41.02 0.34 42.37 1.01 38.64 2.72 6.75 6.82 0.07 4.40 2.35 7.20 0.45 7.05 0.30 7.51 0.76 4.17 2.58 6.60 0.15 5.76 0.99 5.76 0.99 3.87 2.88 6.52 0.23 7.66 0.91 6.67 0.08 4.02 2.73 5.53 1. 50% Samples, 50% Tokens 12.14 15.08 2.94 11.80 0.34 11.33 0.81 14.90 2.76 14.89 2.75 13.26 1.12 11.93 0.21 13.52 1.38 14.29 2.15 12.39 0.25 10.33 1.81 15.47 3.33 13.51 1.37 13.81 1.67 11.28 0.86 48.86 49.10 0.24 48.35 0.51 44.80 4. 49.46 0.60 48.99 0.13 48.24 0.62 44.52 4.34 48.95 0.09 49.18 0.32 48.25 0.61 43.84 5.02 49.07 0.21 49.75 0.89 48.93 0.07 44.72 4.14 34.24 35.01 0.77 33.38 0.86 31.47 2.77 34.86 0.62 35.03 0.79 33.33 0.91 31.93 2.31 33.86 0.38 34.52 0.28 33.06 1.18 30.95 3. 34.97 0.73 34.29 0.05 33.68 0.56 31.43 2.81 71.25 72.49 1.24 72.13 0.88 67.90 3.35 72.49 1.24 71.60 0.35 72.31 1.06 69.31 1.94 71.08 0.17 71.43 0.18 70.72 0.53 68.25 3.00 72.13 0.88 72.13 0.88 71.60 0.35 67.55 3.70 46.44 46.78 0.34 45.42 1.02 45.42 1. 47.46 1.02 46.78 0.34 45.76 0.68 42.71 3.73 46.44 0.00 47.80 1.36 45.42 1.02 41.69 4.75 47.80 1.36 47.80 1.36 47.46 1.02 43.73 2.71 21.53 22.21 0.68 16.22 5.31 25.78 4.25 21.91 0.38 24.11 2.58 16.83 4.70 23.73 2.20 21.38 0.15 24.11 2.58 13.87 7.66 29.72 8. 22.67 1.14 25.17 3.64 17.97 3.56 25.85 4.32 24.91 33.28 8.37 27.69 2.78 11.85 13.06 29.41 4.50 29.17 4.26 30.37 5.46 12.72 12.19 26.73 1.82 30.22 5.31 27.83 2.92 9.95 14.96 25.19 0.28 30.25 5.34 29.82 4.91 7.09 17.82 54.16 53.83 0.33 50.58 3.58 47.64 6. 55.99 1.83 55.83 1.67 53.99 0.17 49.90 4.26 53.67 0.49 53.97 0.19 51.53 2.63 49.22 4.94 55.70 1.54 56.58 2.42 54.40 0.24 48.24 5.92 43.66 45.72 2.06 42.41 1.25 39.72 3.94 45.45 1.79 45.50 1.84 43.85 0.19 39.68 3.98 43.86 0.20 45.50 1.84 41.88 1.78 39.77 3. 44.70 1.04 46.39 2.73 44.25 0.59 38.49 5.17 Q-Tuning (Ours) 62.79 0.71 42.03 0.67 10.46 3.71 14.53 2. 51.05 2.19 36.17 1.93 73.37 2.12 48.14 1.70 28.81 7.28 36.35 11. 56.30 2.14 48.59 4.93 Full Dataset 61.55 42.37 8. 13.80 50.45 35.36 71.25 45.76 26. 31.81 53.67 45.84 Random Longest InfoBatch Entropy 61.02 Random 60.67 0.35 PPL 59.61 1.41 FastV SparseVLM 54.67 6.35 61.20 0.18 Random 62.26 1.24 PPL 59.61 1.41 FastV SparseVLM 55.38 5.64 61.55 0.53 Random 60.85 0.17 PPL 58.91 2.11 FastV SparseVLM 55.91 5.11 60.49 0.53 Random 60.85 0.17 PPL 58.91 2.11 FastV SparseVLM 56.79 4.23 41.36 42.71 1.35 42.37 1.01 38.98 2. 41.02 0.34 41.36 0.00 42.71 1.35 39.66 1.70 40.34 1.02 42.03 0.67 41.36 0.00 37.29 4.07 41.69 0.33 40.68 0.68 42.37 1.01 37.63 3.73 7.43 6.75 0.68 5.53 1.90 7.28 0.15 6.37 1.06 6.82 0.61 5.53 1.90 6.14 1.29 5.84 1.59 6.75 0.68 4.02 3.41 6.22 1. 7.13 0.30 6.29 1.14 4.85 2.58 5.84 1.59 50% Samples, 70% Tokens 15.56 14.93 0.63 13.36 2.20 10.84 4.72 14.90 0.66 14.83 0.73 14.12 1.44 11.51 4.05 14.99 0.57 14.22 1.34 14.98 0.58 10.40 5.16 15.41 0.15 13.38 2.18 14.27 1.29 11.85 3. 48.91 49.13 0.22 48.48 0.43 44.68 4.23 49.79 0.88 49.04 0.13 48.77 0.14 44.56 4.35 49.45 0.54 48.41 0.50 48.76 0.15 43.75 5.16 48.33 0.58 49.94 1.03 48.84 0.07 43.82 5.09 34.85 34.84 0.01 33.87 0.98 31.29 3.56 34.65 0.20 34.86 0.01 34.15 0.70 31.45 3. 34.43 0.42 34.45 0.40 33.60 1.25 30.71 4.14 34.61 0.24 34.23 0.62 33.85 1.00 31.18 3.67 71.60 72.31 0.71 72.31 0.71 68.78 2.82 72.13 0.53 70.90 0.70 72.66 1.06 69.14 2.46 70.90 0.70 71.08 0.52 70.19 1.41 68.96 2.64 72.13 0.53 72.31 0.71 71.96 0.36 67.55 4. 47.12 47.12 0.00 45.08 2.04 46.10 1.02 48.47 1.35 47.80 0.68 45.08 2.04 45.42 1.70 45.76 1.36 45.76 1.36 45.76 1.36 44.07 3.05 47.46 0.34 46.44 0.68 48.14 1.02 46.44 0.68 22.59 20.70 1.89 18.95 3.64 23.65 1.06 23.05 0.46 24.87 2.28 19.94 2.65 23.43 0. 22.97 0.38 22.82 0.23 19.86 2.73 25.63 3.04 23.43 0.84 23.20 0.61 20.39 2.20 25.17 2.58 27.86 32.02 4.16 28.60 0.74 11.32 16.54 29.60 1.74 27.33 0.53 31.53 3.67 13.42 14.44 28.85 0.99 32.80 4.94 28.95 1.09 9.82 18.04 28.99 1.13 31.44 3.58 30.30 2.44 10.36 17. 53.68 53.56 0.12 51.40 2.28 49.04 4.64 55.94 2.26 55.62 1.94 54.66 0.98 47.86 5.82 54.11 0.43 54.63 0.95 50.92 2.76 49.56 4.12 55.44 1.76 56.67 2.99 54.46 0.78 48.60 5.08 44.57 45.14 0.57 43.27 1.30 39.78 4.79 45.84 1.27 45.30 0.73 44.77 0.20 39.85 4. 44.52 0.05 45.42 0.85 43.14 1.43 39.61 4.96 45.49 0.92 46.01 1.44 45.05 0.48 39.62 4.95 Q-Tuning (Ours) 61.38 0.36 42.03 0.67 9.55 2. 17.69 2.13 51.04 2.13 36.34 1.49 73.90 2.30 47.46 0.34 29.49 6. 37.24 9.38 56.07 2.39 48.83 4.26 Full Dataset 61.55 42. 8.64 13.80 50.45 35.36 71.25 45. 26.68 31.81 53.67 45.84 The detailed results of additional experiments on general instruction tuning are presented in Table 5, providing comprehensive comparison across different pruning strategies. Preprint. Under review. C.2 ABLATION STUDY Sensitivity of hyperparameters We further examined the robustness of our framework with respect to two critical hyperparameters: batch size and neighbor awareness λ. As shown in Figure 5, varying the batch size (8, 16, 32) has only marginal effect across benchmarks and keep-ratio configurations, suggesting that our pruning strategy remains stable even under smaller mini-batches. Meanwhile, the performance under different values of λ (0.01.0) remains consistently close to or above the full-data baseline, indicating that the method is not overly sensitive to the strength of neighbor-aware token scoring. Taken together, these results highlight the robustness of our approach with respect to hyperparameter choices, further underscoring its practicality in diverse training settings. Figure 5: Effect of varying (a) batch size (8, 16, 32) and (b) neighbor awareness λ (0, 0.3, 0.5, 0.7, 1.0) for Mistral-7B under three datatoken keep ratio configurations (25%50%, 25%70%, 50%70%). Results are reported on GSM8K, SQuAD, and TriviaQA. Dashed lines marked full denote models trained on the full data without pruning. The evolution of samples distribution during training In Figure 6, we further show the training dynamics by tracking the average perplexity and entropy of 100 randomly sampled instances. These results provide supporting evidence that Q-Tuning reduces both metrics more rapidly than alternative pruners. Figure 6: Training dynamics of different sample pruners. We tracked the average perplexity (PPL) and entropy on 100 randomly sampled instances across training epochs. Compared to baseline strategies, Q-Tuning consistently reduces both metrics at faster rate, indicating more efficient learning dynamics. 17 Preprint. Under review."
        },
        {
            "title": "D PSEUDOCODE OF THE PROPOSED METHOD",
            "content": "To facilitate clarity and reproducibility, we summarize the proposed Q-Tuning algorithm in Algorithm 1. The procedure unfolds within single training iteration and consists of two tightly coupled stages: (i) sample pruning, where instances are dynamically selected based on their position in the erroruncertainty (EU) plane through an efficient bisection-based search of quantile thresholds, and (ii) token pruning, where retained samples undergo finer-grained filtering to preserve only the most informative subset of tokens. Algorithm 1 Q-Tuning: dynamic data pruning in one iteration 1: Input: Mini-batch Bt, model fθt, retention ratios rsample, rtoken, smoothing λ. 2: Output: Pruned mini-batch Bt. 3: // Stage 1: Sample Pruning via EU Plane 4: Compute (PPL(x, y; fθ), Ent(x, y; fθ)) for each Bt. 5: Initialize ranges αlow = 0, αhigh = 0.49, βlow = 0, βhigh = 0.49. 6: for = 1 to Kmax do 7: 8: α (αlow + αhigh)/2, β (βlow + βhigh)/2. Derive thresholds pplhi = Q1α(PPL), ppllo = Qα(PPL), entlo = Qβ(Ent), enthi = Bisection iterations on both axes Q1β(Ent). Too few kept, relax thresholds Too many kept, tighten thresholds < rsample Bt then Compute supp-score for each Q1 Q3: Ensure target sample ratio supp(x) = max{ ˆPPL(x, y; fθ) ˆEnt(x, y; fθ), ˆEnt(x, y; fθ) ˆPPL(x, y; fθ)}, where ˆ denotes minmax normalization. Select top-scoring samples to augment until target size is met. Partition samples into quadrants Q1Q4. Q2+Q4 Bt if < rsample then . αlow α, βlow β αhigh α, βhigh β else 9: 10: 11: 12: 13: 14: 15: 16: end for 17: 18: if 19: end if Q2 Q4. 20: 21: end if 22: // Stage 2: Token Pruning 23: Bt . 24: for each 25: 26: 27: 28: do if Q4 then else if Q2 then Keep all tokens of x. For each token (x), compute Calibration samples intact si(x, y; fθ) = (1 λ) PPLi(x, y; fθ) + λ [PPLi1(x, y; fθ) + PPLi+1(x, y; fθ)]. Keep top-rtoken fraction of tokens ranked by si. end if Bt Bt {m(x) x}. 29: 30: 31: 32: end for 33: return Bt. 18 Preprint. Under review."
        },
        {
            "title": "E EXAMPLE SAMPLES FROM EACH QUADRANT",
            "content": "We report representative samples from each quadrant (Q1Q4) at the beginning of training, including their perplexity (PPL), entropy, and the corresponding instructionoutput pairs."
        },
        {
            "title": "Entropy",
            "content": "Example (Instruction / Output) Q1 Examples 8.33 2.12 8.04 1.73 91. 1.91 Q2 Examples 8.65 1.54 11.02 1.48 15. 1.50 Q3 Examples 4.80 1.45 2.41 1.09 4. 1.49 Q4 Examples 6.03 1.83 8.04 1.71 5. 1."
        },
        {
            "title": "There are a number of ways to reduce air",
            "content": "How can we reduce air pollution? What are the three primary colors? The three primary colors are red, blue, Instruction: Output: and yellow. Instruction: Output: pollution, such as shifting to renewable energy sources... Instruction: equivalent to 1/4. The fraction 4/16 is equivalent to 1/4 because both numerator and denominator are divisible by 4. Explain why the fraction 4/16 is Output: Output: John was at crossWrite short story in third person Instruction: narration about protagonist making an important career decision. He had just graduate... roads in his life. Instruction: Write short story in third person narration about protagonist making an important career decision. roads in his life. Instruction: Instagram, Telegram He had just graduate... Identify the odd one out.nTwitter, John was at cross-"
        },
        {
            "title": "Telegram",
            "content": "Output: Output: Describe the structure of an atom. An atom is made up of nucleus conInstruction: Output: taining protons and neutrons, surrounded by electrons... Instruction: Output: pollution, such as shifting to renewable energy sources... Instruction: Instagram, Telegram)."
        },
        {
            "title": "There are a number of ways to reduce air",
            "content": "Identify the odd one out (Twitter, How can we reduce air pollution?"
        },
        {
            "title": "Telegram",
            "content": "Output: 2. 1. Eat balanced diet... Give three tips for staying healthy. What are the three primary colors? The three primary colors are red, blue, Instruction: Output: regularly... Instruction: Output: and yellow. Instruction: Write short story in third person narration about protagonist making an important career decision. roads in his life. He had just graduate... John was at cross-"
        },
        {
            "title": "Exercise",
            "content": "Output:"
        }
    ],
    "affiliations": [
        "Alibaba Group",
        "BJTU",
        "EPIC Lab, SJTU",
        "HKUST",
        "NJU"
    ]
}