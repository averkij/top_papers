{
    "paper_title": "SpineBench: A Clinically Salient, Level-Aware Benchmark Powered by the SpineMed-450k Corpus",
    "authors": [
        "Ming Zhao",
        "Wenhui Dong",
        "Yang Zhang",
        "Xiang Zheng",
        "Zhonghao Zhang",
        "Zian Zhou",
        "Yunzhi Guan",
        "Liukun Xu",
        "Wei Peng",
        "Zhaoyang Gong",
        "Zhicheng Zhang",
        "Dachuan Li",
        "Xiaosheng Ma",
        "Yuli Ma",
        "Jianing Ni",
        "Changjiang Jiang",
        "Lixia Tian",
        "Qixin Chen",
        "Kaishun Xia",
        "Pingping Liu",
        "Tongshun Zhang",
        "Zhiqiang Liu",
        "Zhongan Bi",
        "Chenyang Si",
        "Tiansheng Sun",
        "Caifeng Shan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Spine disorders affect 619 million people globally and are a leading cause of disability, yet AI-assisted diagnosis remains limited by the lack of level-aware, multimodal datasets. Clinical decision-making for spine disorders requires sophisticated reasoning across X-ray, CT, and MRI at specific vertebral levels. However, progress has been constrained by the absence of traceable, clinically-grounded instruction data and standardized, spine-specific benchmarks. To address this, we introduce SpineMed, an ecosystem co-designed with practicing spine surgeons. It features SpineMed-450k, the first large-scale dataset explicitly designed for vertebral-level reasoning across imaging modalities with over 450,000 instruction instances, and SpineBench, a clinically-grounded evaluation framework. SpineMed-450k is curated from diverse sources, including textbooks, guidelines, open datasets, and ~1,000 de-identified hospital cases, using a clinician-in-the-loop pipeline with a two-stage LLM generation method (draft and revision) to ensure high-quality, traceable data for question-answering, multi-turn consultations, and report generation. SpineBench evaluates models on clinically salient axes, including level identification, pathology assessment, and surgical planning. Our comprehensive evaluation of several recently advanced large vision-language models (LVLMs) on SpineBench reveals systematic weaknesses in fine-grained, level-specific reasoning. In contrast, our model fine-tuned on SpineMed-450k demonstrates consistent and significant improvements across all tasks. Clinician assessments confirm the diagnostic clarity and practical utility of our model's outputs."
        },
        {
            "title": "Start",
            "content": "SPINEBENCH: CLINICALLY SALIENT, LEVEL-AWARE BENCHMARK POWERED BY THE SPINEMED-450K CORPUS Ming Zhao1,7,*, Wenhui Dong2,7,*, Spine Expert Team10, Xiang Zheng3,7, Zhonghao Zhang4,7, Zian Zhou5,7, Wei Peng6, Jianing Ni7, Changjiang Jiang8,7, Lixia Tian9, Pingping Liu1, Tongshun Zhang1, Zhongan Bi5,7, Chenyang Si2,, and Caifeng Shan2, 1Jilin University, 2Nanjing University, 3Institute of Automation, Chinese Academy of Sciences, 4Ningxia University, 5Zhejiang University, 6Stanford University, 7π3 Lab, 8Wuhan University, 9Beijing Jiaotong University, 10The General Hospital of the Peoples Liberation Army"
        },
        {
            "title": "ABSTRACT",
            "content": "Spine disorders affect 619 million people globally and are leading cause of disability, yet AI-assisted diagnosis remains limited by the lack of level-aware, multimodal datasets. Clinical decision-making for spine disorders requires sophisticated reasoning across X-ray, CT, and MRI at specific vertebral levels. However, progress has been constrained by the absence of traceable, clinically-grounded instruction data and standardized, spine-specific benchmarks. To address this, we introduce SpineMed, an ecosystem co-designed with practicing spine surgeons. It features SpineMed-450k, the first large-scale dataset explicitly designed for vertebral-level reasoning across imaging modalities with over 450,000 instruction instances, and SpineBench, clinically-grounded evaluation framework. SpineMed-450k is curated from diverse sources, including textbooks, guidelines, open datasets, and 1,000 de-identified hospital cases, using clinician-in-the-loop pipeline with two-stage LLM generation method (draft and revision) to ensure high-quality, traceable data for question-answering, multi-turn consultations, and report generation. SpineBench evaluates models on clinically salient axes, including level identification, pathology assessment, and surgical planning. Our comprehensive evaluation of several recently advanced large vision-language models (LVLMs) on SpineBench reveals systematic weaknesses in fine-grained, level-specific reasoning. In contrast, our model fine-tuned on SpineMed-450k demonstrates consistent and significant improvements across all tasks. Clinician assessments confirm the diagnostic clarity and practical utility of our models outputs."
        },
        {
            "title": "INTRODUCTION",
            "content": "5 2 0 2 3 ] . [ 1 0 6 1 3 0 . 0 1 5 2 : r Figure 1: Benchmark performance of SpineGPT *Equal Contributions.Corresponding author: {chenyang.si, cfshan}@nju.edu.cn 1 Figure 2: Overview of SpineMed-450k. Training data was curated from textbooks, public datasets, clinical records, medical guidelines, and hospitals. The process involved data preprocessing, annotation generation, and final clinician review. Our dataset comprises four types: multi-choice QA, open-ended QA, multi-round dialogues, and reports. Spinal disorders (Ferreira et al., 2023), including degenerative diseases (like disc herniation) (Dydyk et al., 2017), deformities (like scoliosis) (Negrini et al., 2018), trauma (fractures) (Vaccaro et al., 2013), and inflammatory conditions (Taurog et al., 2016), are major driver of pain, disability, and surgical care worldwide. key challenge in their management is diagnostic complexity. Unlike many other disorders, spinal conditions typically cannot be precisely diagnosed using single imaging modality. It often requires clinicians to perform level-aware, multimodal reasoning: integrating findings from X-ray, CT, and MRI to pinpoint pathology at specific vertebral levels, grade severity, and plan interventions (Teichner et al., 2025). The precision of this interpretation directly impacts patient outcomes and neurological safety. Although advanced AI holds great promise for augmenting this demanding workflow (Ibrahim et al., 2025b), its potential has been hindered. Fortunately, such clinical tasks can significantly benefit from advanced AI capabilities (Lee et al., 2024b). Yet progress is constrained not by model capacity, but by the absence of traceable instruction data and standardized, clinically validated benchmarks tailored to spine workflows (Lee et al., 2024b). Equally important, prior efforts rarely embed clinicians throughout the pipeline, limiting practical utility. We present SpineMed: comprehensive effort consisting of SpineMed-450k, provenance-rich instruction corpus for spine diagnosis and planning, and SpineBench, targeted evaluation suite that help to evaluate the effectiveness of different AI-based spine diagnosis. To our best knowledge, this is current largest-scale Spinal diagnosis and treatment dataset. Both were co-designed with spine clinicians (radiologists and surgeons) to reflect real decision points. SpineMed-450k aggregates materials from textbooks, surgical guidelines, expert consensuses, question banks, open spine datasets (e.g., Spark, VerSe) (Alibaba Cloud Tianchi, 2020; Sekuboyina et al., 2021), open-access case reports (Europe PMC) (Consortium, 2015), and 1,000 de-identified hospital cases. Throughout curation, clinicians (i) defined inclusion criteria and task taxonomies; (ii) vetted imaging selections from hospital cases to prioritize views most informative for diagnosis and surgical planning; and (iii) specified failure modes that instruction data must surface. To minimize hallucinations and preserve traceability, our pipeline (a) extracts figures and text with PaddleOCR (Du et al., 2020); (b) binds images to their local textual context via caption-pattern regex matching that anchors each figure to its surrounding paragraph; and (c) distills high-quality supervisionmultiple-choice, open-ended QA, multi-turn consultations, and report generationthrough two-stage LLM process (draft revision with explicit prompts and logs). Clinicians review and refine prompt policies and revision criteria to align with reporting standards. SpineBench operationalizes evaluation across clinically relevant axesimaging report, diagnosis, patient guidance, evidence-based treatment, technical feasibility, risk prognosis, coverage, relevance, granularity, and interpretability. Its item design, error taxonomy, and rubrics were developed with clinician input to emphasize fine-grained, anatomy-centric reasoning and the kinds of mistakes that matter in practice. To characterize the state of the field, we evaluate dozen of contemporary large visionlanguage models (LVLMs) (OpenAI, 2025a;b; Hurst et al., 2024; Google, 2025a;b; Sellergren et al., 2025; xAI, 2025; Anthropic, 2025; Bai et al., 2025; Hong et al., 2025; Wang et al., 2025a), both general-purpose and medical. Our evaluation reveals significant weaknesses in fine-grained, level-specific diagnosis and open-ended clinical reasoning, particularly in the handling of complex multi-image tasks. Building on these insights, we introduce fine-tuned spine model SpineGPT trained on SpineMed-450k that delivers consistent improvements on SpineBench as shown in Figure 1. Clinicians assess exemplar outputs for decision relevance, underscoring the practical value of targeted, evidence-linked instruction data. Our contributions are as follows: Clinician-in-the-loop dataset and benchmark. We release SpineMed-450k, more than 450,000 instruction instances spanning multiple-choice, open-ended QA, multi-turn consultations, and report generationcurated via specialist-supported pipeline with anatomical integration and two-stage report refinement, together with SpineBench, level-aware benchmark co-designed with clinicians and enriched with 1,000 real hospital cases. Comprehensive evaluation. We benchmark dozens of open-source LVLMs across closed/open tasks using clinician-shaped taxonomies and rubrics, surfacing systematic failure modes in spine reasoning. practical baseline model. We propose fine-tuned spine LVLM trained on SpineMed450k that achieves consistent gains on SpineBench; exemplar outputs receive clinician feedback on diagnostic clarity and planning utility, establishing high-utility baseline for future research."
        },
        {
            "title": "2 SPINEMED-450K DATASET",
            "content": "Overview. The SpineMed-450k dataset was constructed through meticulous \"clinician-in-theloop\" pipeline designed to ensure clinical accuracy and relevance. This pipeline integrates four core stages: (1) Dataset collection, (2) Structured Information Extraction, (3) Data De-identification and Cleaning, and (4) Dataset Generation. (5) Annotation of the spinal diagnostic report."
        },
        {
            "title": "2.1 DATA COLLECTION",
            "content": "To build complete and comprehensive dataset for spinal diagnosis and treatment, we collected data from variety of sources (Chen et al., 2024a; Wei & Hwei, 2024; Wu et al., 2025; Chen et al., 2024b). Existing general-purpose large vision-language models (Hurst et al., 2024; Google, 2025a;b; Deng et al., 2023; Ullah et al., 2024; AlSaad et al., 2024) and even medical large language models (Li et al., 2023; Wang et al., 2025b; Wu et al., 2024; Lin et al., 2025; Lu et al., 2024; Niu et al., 2025; Nath et al., 2025a; Seyfioglu et al., 2024; Lai et al., 2025; Xu et al., 2025) are trained on generic medical data (Chen et al., 2024a;b; Xie et al., 2024a), which often lacks the high-quality, specialized data needed for orthopedics (Deng et al., 2023; Ullah et al., 2024). To train an effective large model for spinal care, we first compiled high-quality, general orthopedic dataset covering multiple domains, including Spine Surgery, Foot and Ankle Surgery, Orthopedic Trauma, and Hand and Upper Extremity Surgery. As shown in Figure 3, we integrated materials from variety of sources, including textbooks, surgical guidelines, expert consensuses, question banks, open-access case reports from Europe PMC (Consortium, 2015), open single-modality spine datasets (Alibaba Cloud Tianchi, 2020; Sekuboyina et al., 2021) (e.g., Spark, VerSe), and approximately 1,000 de-identified multimodal hospital cases collected from various hospitals. This data covers wide range of modalities, including text, CT, MRI, X-ray, and tables. We track the provenance (dataset IDs/DOIs, case identifiers) for every derived item. Where possible, we adopt upstream datasets with permissive licenses and clear terms of reuse. Clinicians defined the inclusion criteria and, for hospital cases, selected the most decision-informative images (e.g., MRI target sequences, key CT levels) to serve as the foundation for downstream tasks."
        },
        {
            "title": "2.2 DATASET CURATION",
            "content": "Structured Information Extraction To accurately extract comprehensive information from academic sources, we employed PaddleOCR (Du et al., 2020) to parse PDF documents and images from textbooks and literature. The output, containing both recognized text and layout analysis, was 3 Figure 3: Generation pipeline of SpineMed-450k. The pipeline involves data preprocessing (including de-identification, deduplication, and OCR) followed by expert LLM-driven curation. This process generates 450k items for tasks like QA, medical reports, and consultations across various orthopedic subspecialties. exported into Markdown format. This approach effectively preserved the structural integrity of the documents, including tables, figure placements, and overall layout. Furthermore, to ensure precise mapping between figures, their captions, and corresponding contextual descriptions in the text, we developed novel algorithm termed Picture Context Matching. The technical details of this algorithm are elaborated in the Appendix B.4. Data De-identification and Cleaning This stage focused on processing data sourced from collection of clinical records in hospitals. We first performed rigorous de-identification process, removing all sensitive and personally identifiable information (PII), such as patient IDs and physical examination details under HIPAA. We also filtered out irrelevant images, such as post-operative photos and non-diagnostic tables. Subsequently, Expert LLM model was utilized to conduct fine-grained classification of the data, ensuring the datasets purity by excluding non-orthopedic cases. As shown in Figure 2, the orthopedic domain was categorized into 7 classes, with the spine sub-domain further divided into 14 distinct classes. detailed statistical overview of the dataset distribution across these categories is presented in Figure 4. Dataset Generation In close collaboration with medical experts, we designed comprehensive annotation schema to generate high-quality, multi-task training data. The annotation process was tailored to the data source: (1) From External Knowledge Sources (e.g., Textbooks): We generated bilingual (Chinese and English) and multimodal (text and image-based) questions in both multiplechoice and open-ended formats using Expert VLM Model with carefully designed prompts. (2) From Opened-spine Datasets: We processed two open-source spinal datasets, Spark and Verse, to generate multi-turn question-and-answer dialogues that simulate doctor-patient interactions. These datasets consist mainly of unimodal 3D image slices (CT and MRI). To ensure consistency, we standardized the inputs by adaptively sampling 25 slices per case under clinical expert supervision. From this, we created over 300 simulated consultations to train models in their conversational abilities within spinal scenarios. (3) From Real Clinical Records: We created multiple-choice questions, multi-turn conversational datasets for patient interviews, and comprehensive spinal diagnostic reports via Expert VLM Model. For prompt design, please refer to the Appendix B. Annotation of the spinal diagnostic report cornerstone of our dataset is the generation of detailed spinal diagnostic reports. In this process, we utilized real clinical reports from hospitals, incorporating physician recommendations, to design reports that encompass six dimensions, all aimed at simulating complete clinical workflow: (1) Structured Imaging Findings: Analyze the provided medical images and distill key radiological evidence that supports the final diagnosis. (2) AI-Assisted Diagnosis: Formulate diagnostic conclusion and articulate the reasoning process based on the synthesis of clinical data and imaging analysis. (3) Treatment Recommendations: This section is bifurcated to address different audiences. Patient-Centric Advice: Explain the rationale for the 4 Figure 4: Statistics of SpineMed-450k. (a) Distribution of medical records across various hospitals. (b) The prevalence of various orthopedic and spinal diseases. (c) Distribution of different modals and languages. (d) Benchmark token length distribution: blue (non-report tokens), pink (report tokens). recommended surgical procedure in clear, non-technical language. Physician-Centric Rationale: Provide robust, guideline-based decision tree to justify the surgical selection from clinical perspective. (4) Risk and Prognosis Assessment: Conduct an objective evaluation of the potential risks and expected outcomes associated with the proposed surgical plan. (5) Postoperative Issue Management: Predict potential post-surgical complications for specific procedures and develop corresponding management strategies. (6) Diagnostic Rationale and Disclaimer: Provide complete diagnostic and surgical decision-making chain and disclaimer statement. Report examples are provided in the Appendix B.5."
        },
        {
            "title": "3 DATA STATISTICS",
            "content": "SpineMed-450K is large-scale multimodal training dataset for orthopedic spine knowledge in large language models, characterized by strong traceability, comprehensive coverage, diverse question types, and rich modalities."
        },
        {
            "title": "3.1 DISEASE DIVERSITY COVERAGE",
            "content": "As shown in Figure 4(b), SpineMed-450K encompasses seven common orthopedic subspecialties, including Spine Surgery, Foot and Ankle Surgery, and Orthopedic Trauma, with spinal diagnostic data accounting for 47% of the orthopedic data. Furthermore, the spinal diagnostic data includes 14 spine subconditions such as cervical degenerative spine disease and idiopathic scoliosis. We performed sampling on each spinal diagnostic dataset to ensure uniform distribution across all disease categories."
        },
        {
            "title": "3.2 PATIENT SOURCE DIVERSITY",
            "content": "As illustrated in Figure 4(a), our data originates from 1,000 real clinical cases collected from 11 leading expert hospitals. These data span the recent three years and encompass patients of different genders, various age groups, and diverse physical conditions. To protect privacy, personal information has been de-identified. personal information. Given the varying surgical volumes across different hospitals, the largest hospital contributes 33% of the data while the smallest contributes 1%. These 5 valuable real patient data provide crucial evidence for accurately representing the authentic conditions of spine patients."
        },
        {
            "title": "3.3 DATA SOURCE AND QUESTION TYPE DIVERSITY",
            "content": "Table 1: Dataset statistics categorized by data source and split."
        },
        {
            "title": "Total",
            "content": "6,450 17 6,467 377,212 203 377,415 61,453 101 61, 1,087 3 1,090 304 304 9,668 250 456,174 9,918 456,748 Split Multiplechoice Table 2: Dataset distribution across domains and task types. As shown in Table 1, our data derives from six major sources: Literature, Textbooks, Case Reports, hospitals, and others. Textbooks, being the primary knowledge source for physicians, constitute the largest proportion with 377k entries, while hospital data, though valuable, is limited in quantity, with 9,668 data points generated from nearly 1,000 real cases. As presented in Table 2 and Figure 4(c), question types are categorized into pure text QA, multimodal QA, medical consultations, and clinical reports, with multiple-choice questions comprising the largest proportion. For evaluation convenience, our test set includes only multiple-choice and clinical report formats. 197,413 248,789 487 Openended Consultation 1,138 Train Test 197,413 249,276 734 87 Report 1,138 Total"
        },
        {
            "title": "3.4 DATA TYPE DIVERSITY",
            "content": "Our dataset incorporates multiple authentic data types including patient physical examination information, patient consultation records, X-rays, CT scans, and MRI images. Due to variations in hospital facilities and patient conditions, the collected data differs for each case, which introduces modeling challenges but enables our trained models to more closely approximate real clinical scenarios faced by physicians."
        },
        {
            "title": "4.1 BENCHMARK CONSTRUCTION",
            "content": "Data Sampling The SpineBench was constructed by sampling from the SpineMed-450k dataset. Following the original distribution of SpineMed-450k, we sampled 500 multiple-choice questions and 100 medical reports. This subset incorporates 14 spinal sub-diseases and data from multiple sources (see Figure 4(b) for details). Data Validation To ensure the integrity of SpineBench, rigorous review process was implemented involving team of 17 board-certified orthopedic surgeons. To mitigate bias and ensure objectivity, the surgeons were divided into three independent groups. Each group collaboratively validated the quality of the questions. Erroneous question-answer pairs were corrected, and questions deemed unsuitable for the evaluation set were removed. Ultimately, SpineBench comprises 487 high-quality multiple-choice questions and 87 report generation prompts."
        },
        {
            "title": "4.2 EVALUATION METRICS",
            "content": "Under the careful design and guidance of our medical team, We propose comprehensive evaluation framework that integrates three complementary assessment dimensions to measure the overall performance of AI systems in spinal diagnostic tasks: 6 Table 3: Evaluation criteria for AI-generated clinical reports across five key dimensions Report Section Evaluation Criterion Key Assessment Focus I. Structured Imaging Report (SIP) Imaging Report (1-5 pts) Accuracy of findings, clinical significance, quantitative descriptions II. AI-Assisted Diagnosis (AAD) Diagnosis (1-5 pts) Primary diagnosis correctness, differential diagnoses, clinical reasoning III. Treatment Recommendations (TR) Patient Guidance (1-5 pts) Evidence-Based Plan (1-5 pts) Technical Feasibility (1-5 pts) Language clarity, empathy, patient reassurance Rationale, individualization, guideline consistency Surgical details, complication prevention, backup plans IV. Risk & Prognosis Management (RPM) Risk-Prognosis Mgmt (1-5 pts) Perioperative planning, follow-up schedule, safety protocols V. Reasoning & Disclaimer (RD) Coverage (1-5 pts) Relevance (1-5 pts) Granularity (1-5 pts) Explanation (1-5 pts) Completeness of evidence identification and explanation Focus on core diagnosis without irrelevant content Precision and quantitative detail sufficiency Logical coherence and reasoning chain clarity Scoretotal = 3 (cid:88) k=1 wk Pk (1) where P1, P2, and P3 represent the performance scores for text-only multiple-choice questions, multimodal multiple-choice questions, and diagnostic report generation, respectively. The weights wk are dynamically determined based on the sample sizes: wk = Nk i=1 Ni (cid:80)3 (2) where Nk denotes the number of samples in each evaluation category. This data-driven weighting scheme ensures statistical reliability while maintaining balanced representation across all assessment dimensions. The diagnostic report score P3 is computed using our expert-calibrated framework: P3 = 20 5 (cid:88) i=1 1 ni ni(cid:88) j=1 sij (3) where scores are normalized to 0100 scale for consistency across all metrics and sij denotes the score for dimension in section i, ni represents the number of dimensions in section i. This unified scoring system enables direct comparison of model capabilities across diverse clinical tasks, from basic diagnostic reasoning to complex report generation."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "5."
        },
        {
            "title": "IMPLEMENTATION DETAILS",
            "content": "This study employs the Qwen2.5-VL-7B-Instruct model within curriculum learning framework for subsequent training phases, aimed at enhancing the models applicability and proficiency in the field of orthopedic spine care. The training process is divided into three stages, each integrating distinct datasets and training strategies to progressively strengthen the models performance in spinal health. General and Orthopedic Foundational Learning In this initial stage, we utilized several publicly available medical text datasets, including medical-o1-reasoning-SFT (Chen et al., 2024a), MedicalR1-Distill-Data (Chen et al., 2024a), and MedThoughts-8K (hw hwei, 2025). Additionally, we incorporated diverse set of 150,000 multimodal instruction fine-tuning samples uniformly sampled from PubMedVision (Chen et al., 2024b). The primary objective during this phase is to develop the models foundational capabilities in the medical field and to enhance its performance across various contexts. Subsequently, we trained on data from the SpineMed-450k dataset that pertained to non-spinal categories. Our findings indicate that this non-spinal data significantly improved the models performance on the SpineBench benchmark, highlighting the importance of broadening the knowledge base to enhance task-specific performance. 7 Specialized Learning in Spinal Health In this phase, we concentrated on all data pertinent to spinal health. Furthermore, we extracted selection of multiple-choice and open-ended questions to construct long reasoning chains, with the objective of enhancing the models proficiency in the domain of spinal surgery. Enhancement of Report Generation and Conversational Abilities Finally, we conducted further training through multi-turn dialogues, report generation, and datasets comprising long-chain reasoning instructions. The goal of this stage is to develop the models advanced language comprehension and generation abilities, particularly in the contexts of dialogue interaction and report creation. All training details are provided in the Appendix B."
        },
        {
            "title": "5.2 RESULTS ON SPINEBENCH",
            "content": "Table 4: Performance comparison of LVLMs on close-ended QA and medical report generation tasks. Model Size Close-Ended QA Medical Report Generation Text Image Avg. SIP AAD TR RPM RD Sum Proprietary LVLMs GPT5 O3 Gemini-2.5-Pro Claude4 GPT-4o GPT5-mini Gemini-2.5-Flash - - - - - - - 87.41 86.73 88.44 79.59 86.73 89.12 83.67 Open-source LVLMs GLM-4.5V 21B 85.71 Qwen2.5-VL-72B 72B 84.69 32B 81.29 Linshu-32B 75.85 7B HuatuoGPT-7B 75.51 7B Qwen2.5VL-7B 79.97 82.38 88.60 79.79 81.70 80.83 80. 81.35 79.79 76.68 80.83 74.09 Ours 7B 89.46 84.46 84.46 85.01 88.50 79.67 84.74 85.83 82. 83.98 82.75 79.47 77.82 74.95 87.89 4.54 4.39 4.55 3.96 3.16 4.55 4.43 3.85 3.14 3.05 2.42 2.27 4.15 4.51 4.25 4.51 4.08 3.03 4.48 4. 3.78 3.03 3.05 2.42 2.39 4.10 4.53 4.34 4.68 4.04 3.06 4.60 4.57 3.83 3.12 3.23 2.42 2.80 4.41 4.69 4.43 4.79 4.44 3.30 4.98 4. 4.05 3.23 3.49 3.37 3.26 4.54 4.64 4.42 4.80 4.41 3.46 4.78 4.75 4.30 3.42 3.47 2.87 2.92 91.60 87.32 93.32 83.72 64.04 93.56 91.68 79.24 63.80 65.16 54.0 54. 4.62 87.24 87.44 Avg. 85.54 85.36 89.23 80.28 81.60 87.01 83.93 83.26 79.88 77.30 74.21 64. The evaluation results in Table 4 reveal severe limitations of current vision-language models (OpenAI, 2025a; Hurst et al., 2024; Google, 2025a) in medical domain applications. Large-scale open-source models perform particularly poorly: despite having 72B parameters, Qwen2.5-VL-72B (Bai et al., 2025) achieves only 79.88% average performance and mere 63.80 cumulative score on medical report generation, far below practical application requirements. Even the best-performing opensource model GLM-4.5V (Hong et al., 2025) (83.26%) exhibits nearly 6-point gap compared to the leading proprietary model Gemini-2.5-Pro (89.23%). This gap is more pronounced in medical report generation, where proprietary models exceed 85 points while open-source models struggle to reach 80. Additional medical report results are in the Appendix B.3. Pervasive deficiency in cross-modal alignment. Nearly all models exhibit varying degrees of performance degradation on multimodal tasks. Among open-source models, GLM-4.5V shows 4.36-point gap between text (85.71%) and image (81.35%) modalities; Qwen2.5-VL-72B exhibits 4.90-point gap. Even proprietary models suffer from this issue, with GPT5 dropping from 87.41% on text to 79.97% on images, gap of 7.44 percentage points. This cross-modal performance disparity reflects fundamental inadequacies in medical image understanding and vision-language alignment in existing models, limiting their application in clinical scenarios requiring comprehensive analysis of medical images and textual information. Our method achieves breakthrough performance among open-source models. We achieve 87.44% average score, outperforming all open-source models by 4.18+ points and exceeding multiple proprietary models on close-ended QA (87.89% vs Claude4s 79.67%, GPT-4os 84.74%). Our text-only QA (89.46%) surpasses all models including GPT5 (87.41%). 8 Figure 5: Consistency evaluation of large models and scores given by medical experts"
        },
        {
            "title": "5.3 ABLATIONS OF SPINEGPT",
            "content": "Model Training Data General No-Spine Close-Ended QA (%) Table 5: Performance comparison of models on close-ended QA tasks. Limitations of General Medical Data. As shown in Table 5, models trained exclusively on large-scale general medical data (row 2) exhibit significant performance degradation (74.95 vs. 65.31) on SpineBench compared to the baseline model (row 1). This demonstrates that models trained on such data are insufficient for specialized spine diagnostics. The incorporation of our carefully curated general orthopedic non-spine data (row 3) yields substantial performance improvements (82.14 vs. 74.95), validating the importance of domain-aligned training data. We incorporate spine-specific training data (row 5), which further enhances model performance (87.89 vs. 81.11) compared to using only general medical and orthopedic data (row 4). Qwen 2.5 VL-7B SpineGPT SpineGPT SpineGPT SpineGPT 75.51 64.27 82.99 83.67 89.46 74.09 62.69 80.83 77.20 84.46 74.95 65.31 82.14 81.11 87.89 Image Spine Avg. Text"
        },
        {
            "title": "5.4 HUMAN-EXPERT AGREEMENT ANALYSIS",
            "content": "To validate our LLM-based evaluation approach, we conducted human-expert validation study by sampling cases from our dataset for blind expert scoring. Figure 5 shows the correlation analysis between LLM and expert scores across ten evaluation dimensions. The results demonstrate strong alignment with Pearson correlation coefficients ranging from 0.382 to 0.949, with most dimensions showing correlations above 0.7. These findings validate that our automated LLM scoring serves as reliable proxy for expert judgment."
        },
        {
            "title": "6 CONCLUSIONS, LIMITATIONS, AND FUTURE WORK",
            "content": "We introduced SpineMed-450k, provenance-rich instruction corpus for level-aware spine diagnosis and planning, and SpineBench, level-aware benchmark co-designed with clinicians. Experiments on SpineBench reveal consistent weaknesses of contemporary open-source LVLMs. Our fine-tuned model achieves 87.44% performance, substantially outperforming open-source alternatives and demonstrating that specialized instruction data enables clinically relevant AI capabilities for complex anatomical reasoning tasks. Limitations and Future Work. Future work will expand datasets, train larger models beyond 7B parameters, incorporate reinforcement learning techniques, and provide comprehensive direct comparisons with leading proprietary models including GPT-4 and Gemini to establish clear performance benchmarks."
        },
        {
            "title": "7 ETHICS STATEMENT",
            "content": "This work adheres to the ICLR Code of Ethics. In this study, no human subjects or animal experimentation was involved. All datasets used, including SpineMed-450K, were sourced in compliance with relevant usage guidelines, ensuring no violation of privacy. We have taken care to avoid any biases or discriminatory outcomes in our research process. No personally identifiable information was used, and no experiments were conducted that could raise privacy or security concerns. We are committed to maintaining transparency and integrity throughout the research process."
        },
        {
            "title": "8 REPRODUCIBILITY STATEMENT",
            "content": "Our work will be fully reproducible: we will open-source SpineBench, all questions, the code for running the API and open-source models, all model outputs, and the code for scoring the models. In other words, every part of the project will be made available."
        },
        {
            "title": "REFERENCES",
            "content": "Alibaba Cloud Tianchi. SPARk: Spinal disease intelligent diagnosis dataset from Spark \"Digital Human\" AI Challenge. URL: https://tianchi.aliyun.com/competition/ entrance/531796/information, 2020. Dataset provided by Wanli Cloud and AllinMD Orthopaedics for the Spark \"Digital Human\" AI Challenge Intelligent Diagnosis of Spinal Diseases Competition. Rawan AlSaad, Alaa Abd-Alrazaq, Sabri Boughorbel, Arfan Ahmed, Max-Antoine Renault, Rafat Damseh, and Javaid Sheikh. Multimodal large language models in health care: applications, challenges, and future outlook. Journal of medical Internet research, 26:e59505, 2024. Anthropic. Claude Opus 4 and Claude Sonnet 4 System Card. https://www-cdn.anthropic. com/4263b940cabb546aa0e3283f35b686f4f3b2ff47.pdf, May 2025. Accessed: 2025-09-21. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Sami Barrit, Nathan Torcida, Aurélien Mazeraud, Sébastien Boulogne, Jeanne Benoit, Timothée Carette, Thibault Carron, Bertil Delsaut, Eva Diab, Hugo Kermorvant, et al. Neura: specialized large language model solution in neurology. medRxiv, pp. 202402, 2024. Runa Bhaumik, Vineet Srivastava, Arash Jalali, Shanta Ghosh, and Ranganathan Chandrasekharan. Mindwatch: smart cloud-based ai solution for suicide ideation detection leveraging large language models. MedRxiv, pp. 202309, 2023. Junying Chen, Zhenyang Cai, Ke Ji, Xidong Wang, Wanlong Liu, Rongsheng Wang, Jianye Hou, and Benyou Wang. Huatuogpt-o1, towards medical complex reasoning with llms. arXiv preprint arXiv:2412.18925, 2024a. Junying Chen, Chi Gui, Ruyi Ouyang, Anningzhe Gao, Shunian Chen, Guiming Hardy Chen, Xidong Wang, Ruifei Zhang, Zhenyang Cai, Ke Ji, et al. Huatuogpt-vision, towards injecting medical visual knowledge into multimodal llms at scale. arXiv preprint arXiv:2406.19280, 2024b. Europe PMC Consortium. Europe pmc: full-text literature database for the life sciences and platform for innovation. Nucleic acids research, 43(D1):D1042D1048, 2015. Jiawen Deng, Areeba Zubair, and Ye-Jean Park. Limitations of large language models in medical applications. Postgraduate Medical Journal, 99(1178):12981299, 2023. Zhuo Deng, Weihao Gao, Chucheng Chen, Zhiyuan Niu, Zheng Gong, Ruiheng Zhang, Zhenjie Cao, Fang Li, Zhaoyi Ma, Wenbin Wei, et al. Ophglm: An ophthalmology large language-and-vision assistant. Artificial Intelligence in Medicine, 157:103001, 2024. 10 Yuning Du, Chenxia Li, Ruoyu Guo, Xiaoting Yin, Weiwei Liu, Jun Zhou, Yifan Bai, Zilin Yu, Yehua Yang, Qingqing Dang, et al. Pp-ocr: practical ultra lightweight ocr system. arXiv preprint arXiv:2009.09941, 2020. Alexander Dydyk, FB Mesfin, et al. Disc herniation. 2017. Manuela Ferreira, Katie De Luca, Lydia Haile, Jaimie Steinmetz, Garland Culbreth, Marita Cross, Jacek Kopec, Paulo Ferreira, Fiona Blyth, Rachelle Buchbinder, et al. Global, regional, and national burden of low back pain, 19902020, its attributable risk factors, and projections to 2050: systematic analysis of the global burden of disease study 2021. The Lancet Rheumatology, 5(6):e316e329, 2023. Google. https://storage.googleapis.com/ model-cards/documents/gemini-2.5-pro.pdf, June 2025a. Accessed: 2025-09-21. Gemini 2.5 Pro -Model Card. Google. Gemini 2.5 Flash & 2.5 Flash Image - Model Card. https://storage.googleapis. com/deepmind-media/Model-Cards/Gemini-2-5-Flash-Model-Card.pdf, August 2025b. Accessed: 2025-09-21. Yangyang Guo, Airu Huang, Bo Peng, Yufeng Li, and Wei Gu. Mbbo-rpsld: Training multimodal blenderbot for rehabilitation in post-stroke language disorder. IEEE Journal of Biomedical and Health Informatics, 2025. Jing Hao, Yuxuan Fan, Yanpeng Sun, Kaixin Guo, Lizhuo Lin, Jinrong Yang, Qi Yong Ai, Lun Wong, Hao Tang, and Kuo Feng Hung. Towards better dental ai: multimodal benchmark and instruction dataset for panoramic x-ray analysis. arXiv preprint arXiv:2509.09254, 2025. Wenyi Hong, GLM-V Team, and et al. GLM-4.5V and GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning. arXiv preprint arXiv:2507.01006, 2025. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. hw hwei. Medthoughts-8k dataset, 2025. URL https://huggingface.co/datasets/ hw-hwei/MedThoughts-8K. Muhammad Talal Ibrahim, Eric Milliron, and Elizabeth Yu. Artificial intelligence in spinal imaging-a narrative review. Artificial Intelligence Surgery, 5(1):139149, 2025a. Muhammad Talal Ibrahim, Eric Milliron, and Elizabeth Yu. Artificial intelligence in spinal imaging-a narrative review. Artificial Intelligence Surgery, 5(1):139149, 2025b. Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al. Chexpert: large chest radiograph dataset with uncertainty labels and expert comparison. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pp. 590597, 2019. Alistair EW Johnson, Tom Pollard, Nathaniel Greenbaum, Matthew Lungren, Chih-ying Deng, Yifan Peng, Zhiyong Lu, Roger Mark, Seth Berkowitz, and Steven Horng. Mimic-cxr-jpg, large publicly available database of labeled chest radiographs. arXiv preprint arXiv:1901.07042, 2019. Yuxiang Lai, Jike Zhong, Ming Li, Shitian Zhao, and Xiaofeng Yang. Med-r1: Reinforcement learning for generalizable medical reasoning in vision-language models. arXiv preprint arXiv:2503.13939, 2025. Jason Lau, Soumya Gayen, Asma Ben Abacha, and Dina Demner-Fushman. dataset of clinically generated visual questions and answers about radiology images. Scientific data, 5(1):110, 2018. Sungwon Lee, Joon-Yong Jung, Akaworn Mahatthanatrakul, and Jin-Sung Kim. Artificial intelligence in spinal imaging and patient care: review of recent advances. Neurospine, 21(2):474, 2024a. 11 Sungwon Lee, Joon-Yong Jung, Akaworn Mahatthanatrakul, and Jin-Sung Kim. Artificial intelligence in spinal imaging and patient care: review of recent advances. Neurospine, 21(2):474, 2024b. Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training large language-and-vision assistant for biomedicine in one day. Advances in Neural Information Processing Systems, 36: 2854128564, 2023. Tianwei Lin, Wenqiao Zhang, Sijing Li, Yuqian Yuan, Binhe Yu, Haoyuan Li, Wanggui He, Hao Jiang, Mengze Li, Xiaohui Song, et al. Healthgpt: medical large vision-language model for unifying comprehension and generation via heterogeneous knowledge adaptation. arXiv preprint arXiv:2502.09838, 2025. Ming Lu, Bowen Chen, Drew FK Williamson, Richard Chen, Melissa Zhao, Aaron Chow, Kenji Ikemura, Ahrong Kim, Dimitra Pouli, Ankush Patel, et al. multimodal generative ai copilot for human pathology. Nature, 634(8033):466473, 2024. Tingyu Mo, Jacqueline CK Lam, Victor OK Li, and Lawrence YL Cheung. Dect: Harnessing llm-assisted fine-grained linguistic knowledge and label-switched and label-preserved data generation for diagnosis of alzheimers disease. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 2488524892, 2025. Hongbin Na. Cbt-llm: chinese large language model for cognitive behavioral therapy-based mental health question answering. arXiv preprint arXiv:2403.16008, 2024. Vishwesh Nath, Wenqi Li, Dong Yang, Andriy Myronenko, Mingxin Zheng, Yao Lu, Zhijian Liu, Hongxu Yin, Yee Man Law, Yucheng Tang, et al. Vila-m3: Enhancing vision-language models with medical expert knowledge. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1478814798, 2025a. Vishwesh Nath, Wenqi Li, Dong Yang, Andriy Myronenko, Mingxin Zheng, Yao Lu, Zhijian Liu, Hongxu Yin, Yee Man Law, Yucheng Tang, et al. Vila-m3: Enhancing vision-language models with medical expert knowledge. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1478814798, 2025b. Stefano Negrini, Sabrina Donzelli, Angelo Gabriele Aulisa, Dariusz Czaprowski, Sanja Schreiber, Jean Claude de Mauroy, Helmut Diers, Theodoros Grivas, Patrick Knott, Tomasz Kotwicki, et al. 2016 sosort guidelines: orthopaedic and rehabilitation treatment of idiopathic scoliosis during growth. Scoliosis and spinal disorders, 13(1):3, 2018. Chuang Niu, Qing Lyu, Christopher Carothers, Parisa Kaviani, Josh Tan, Pingkun Yan, Mannudeep Kalra, Christopher Whitlow, and Ge Wang. Medical multimodal multitask foundation model for lung cancer screening. Nature Communications, 16(1):1523, 2025. OpenAI. GPT-4V system card. Technical report, OpenAI, 2023. URL https://cdn.openai. com/papers/GPTV_System_Card.pdf. OpenAI. GPT-5 System Card. https://cdn.openai.com/gpt-5-system-card.pdf, August 2025a. Accessed: 2025-09-21. OpenAI. OpenAI o3 and o4-mini System Card. https://cdn.openai.com/pdf/ 2221c875-02dc-4789-800b-e7758f3722c1/o3-and-o4-mini-system-card. pdf, April 2025b. Accessed: 2025-09-21. Jianing Qiu, Jian Wu, Hao Wei, Peilun Shi, Minqing Zhang, Yunyun Sun, Lin Li, Hanruo Liu, Hongyi Liu, Simeng Hou, et al. Visionfm: multi-modal multi-task vision foundation model for generalist ophthalmic artificial intelligence. arXiv preprint arXiv:2310.04992, 2023. Ali Sarabadani, Kheirolah Rahsepar Fard, and Hamid Dalvand. Exkg-llm: Leveraging large language models for automated expansion of cognitive neuroscience knowledge graphs. arXiv preprint arXiv:2503.06479, 2025. 12 Anjany Sekuboyina, Malek Husseini, Amirhossein Bayat, Maximilian Löffler, Hans Liebl, Hongwei Li, Giles Tetteh, Jan Kukaˇcka, Christian Payer, Darko Štern, et al. Verse: vertebrae labelling and segmentation benchmark for multi-detector ct images. Medical image analysis, 73:102166, 2021. Andrew Sellergren, Sahar Kazemzadeh, Tiam Jaroensri, Atilla Kiraly, Madeleine Traverse, Timo Kohlberger, Shawn Xu, Fayaz Jamil, Cían Hughes, Charles Lau, et al. Medgemma technical report. arXiv preprint arXiv:2507.05201, 2025. Mehmet Saygin Seyfioglu, Wisdom Ikezogwo, Fatemeh Ghezloo, Ranjay Krishna, and Linda Shapiro. Quilt-llava: Visual instruction tuning by extracting localized narratives from open-source histopathology videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1318313192, 2024. Joel Taurog, Avneesh Chhabra, and Robert Colbert. Ankylosing spondylitis and axial spondyloarthritis. New England Journal of Medicine, 374(26):25632574, 2016. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Eric Teichner, Robert Subtirelu, Connor Crutchfield, Chitra Parikh, Arjun Ashok, Sahithi Talasila, Victoria Anderson, Milan Patel, Sricharvi Mannam, Andrew Lee, et al. The advancement and utility of multimodal imaging in the diagnosis of degenerative disc disease. Frontiers in Radiology, 5:1298054, 2025. Ehsan Ullah, Anil Parwani, Mirza Mansoor Baig, and Rajendra Singh. Challenges and barriers of using large language models (llm) such as chatgpt for diagnostic medicine with focus on digital pathologya recent scoping review. Diagnostic pathology, 19(1):43, 2024. Alexander Vaccaro, Cumhur Oner, Christopher Kepler, Marcel Dvorak, Klaus Schnake, Carlo Bellabarba, Max Reinhold, Bizhan Aarabi, Frank Kandziora, Jens Chapman, et al. Aospine thoracolumbar spine injury classification system: fracture description, neurological status, and key modifiers. Spine, 38(23):20282037, 2013. Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, et al. Internvl3.5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025a. Ziyue Wang, Junde Wu, Linghan Cai, Chang Han Low, Xihong Yang, Qiaxuan Li, and Yueming Jin. Medagent-pro: Towards evidence-based multi-modal medical diagnosis via reasoning agentic workflow. arXiv preprint arXiv:2503.18968, 2025b. Huan Wei and Wei Hwei. MedThoughts-8K: large-scale medical reasoning dataset. https: //huggingface.co/datasets/hw-hwei/MedThoughts-8K, 2024. Chaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya Zhang, Weidi Xie, and Yanfeng Wang. Pmc-llama: toward building open-source language models for medicine. Journal of the American Medical Informatics Association, 31(9):18331843, 2024. Juncheng Wu, Wenlong Deng, Xingxuan Li, Sheng Liu, Taomian Mi, Yifan Peng, Ziyang Xu, Yi Liu, Hyunjin Cho, Chang-In Choi, et al. Medreason: Eliciting factual medical reasoning steps in llms via knowledge graphs. arXiv preprint arXiv:2504.00993, 2025. xAI. Grok 4 Model Card. https://data.x.ai/2025-08-20-grok-4-model-card. pdf, August 2025. Accessed: 2025-09-21. Yunfei Xie, Ce Zhou, Lang Gao, Juncheng Wu, Xianhang Li, Hong-Yu Zhou, Sheng Liu, Lei Xing, James Zou, Cihang Xie, et al. Medtrinity-25m: large-scale multimodal dataset with multigranular annotations for medicine. arXiv preprint arXiv:2408.02900, 2024a. Yunfei Xie, Ce Zhou, Lang Gao, Juncheng Wu, Xianhang Li, Hong-Yu Zhou, Sheng Liu, Lei Xing, James Zou, Cihang Xie, et al. Medtrinity-25m: large-scale multimodal dataset with multigranular annotations for medicine. arXiv preprint arXiv:2408.02900, 2024b. 13 Weiwen Xu, Hou Pong Chan, Long Li, Mahani Aljunied, Ruifeng Yuan, Jianyu Wang, Chenghao Xiao, Guizhen Chen, Chaoqun Liu, Zhaodonghui Li, et al. Lingshu: generalist foundation model for unified multimodal medical understanding and reasoning. arXiv preprint arXiv:2506.07044, 2025. Xiaojuan Xue, Deshiwei Zhang, Chengyang Sun, Yiqiao Shi, Rongsheng Wang, Tao Tan, Peng Gao, Sujie Fan, Guangtao Zhai, Menghan Hu, et al. Xiaoqing: q&a model for glaucoma based on llms. Computers in Biology and Medicine, 174:108399, 2024. Kailai Yang, Tianlin Zhang, Ziyan Kuang, Qianqian Xie, Jimin Huang, and Sophia Ananiadou. Mentallama: interpretable mental health analysis on social media with large language models. In Proceedings of the ACM Web Conference 2024, pp. 44894500, 2024. Zhejun Yang, Tongtong Tian, Jilie Kong, and Hui Chen. Chatexosome: an artificial intelligence (ai) agent based on deep learning of exosomes spectroscopy for hepatocellular carcinoma (hcc) diagnosis. Analytical Chemistry, 97(8):46434652, 2025. Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint arXiv:2309.17421, 2023."
        },
        {
            "title": "A LLM USAGE",
            "content": "Large Language Models (LLMs) were used to aid in the writing and polishing of the manuscript. Specifically, we used an LLM to assist in refining the language, improving readability, and ensuring clarity in various sections of the paper. The model helped with tasks such as sentence rephrasing, grammar checking, and enhancing the overall flow of the text. It is important to note that the LLM was not involved in the ideation, research methodology, or experimental design. All research concepts, ideas, and analyses were developed and conducted by the authors. The contributions of the LLM were solely focused on improving the linguistic quality of the paper, with no involvement in the scientific content or data analysis. The authors take full responsibility for the content of the manuscript, including any text generated or polished by the LLM. We have ensured that the LLM-generated text adheres to ethical guidelines and does not contribute to plagiarism or scientific misconduct."
        },
        {
            "title": "B APPENDIX",
            "content": "B.1 RELATED WORK The landscape of medical AI is rapidly evolving, moving from broad, general-purpose models to highly specialized systems designed for clinical utility. Our work is situated within this trend, addressing critical gap in the high-stakes field of spine surgery. From Generalist Models to Domain Adaptation. Recent advances in Large Vision-Language Models (LVLMs), such as GPT-4V (OpenAI, 2023) and Gemini2.5-pro (Google, 2025a), have demonstrated significant progress in multimodal tasks (Yang et al., 2023; Team et al., 2023). However, when applied to the medical domain, their generalist nature becomes distinct limitation. Multiple evaluations consistently show that while promising, these models lack the domain-specific expertise required for complex diagnostic tasks, performing below the level of human specialists (AlSaad et al., 2024). This inherent limitation of generalist models has fueled clear and necessary trend toward specialization. In response, specialized medical LVLMs like LLaVA-Med (Li et al., 2023) and PMC-LLaMA (Wu et al., 2024) have been developed, fine-tuned on large biomedical corpora. Nevertheless, this approach still has shortcomings. For instance, in spinal diagnostics, critical task is the synthesis of data from multimodal imagingsuch as X-ray, CT, and MRIto formulate single, \"level-aware\" diagnosis. This integrative reasoning process, which requires localizing findings to specific vertebral levels, is clinical skill that cannot be acquired from static, descriptive datasets alone. This further underscores core principle: for high-stakes clinical applications, deep, narrow expertise is far more valuable than broad, superficial general knowledge. powerful example validating this principle is OralGPT (Hao et al., 2025), model trained on small, highly curated dataset of intraoral photographs, which achieves performance comparable to state-of-the-art generalist models within its niche. This paradigm shift from generalist to specialist models is now clearly evident across numerous medical fields, from oncology to pathology (Qiu et al., 2023; Sarabadani et al., 2025; Yang et al., 2025; 2024; Barrit et al., 2024; Mo et al., 2025; Deng et al., 2024; Xue et al., 2024; Bhaumik et al., 2023; Na, 2024; Guo et al., 2025). Foundational Datasets and the Cognitive Gap. Progress in AI is fundamentally tied to the quality of training data. Foundational datasets like MIMIC-CXR (Johnson et al., 2019) and CheXpert (Irvin et al., 2019) have been instrumental for tasks like chest radiograph classification. Moving up in complexity are datasets for interactive Visual Question Answering (VQA). For instance, VQA-RAD (Lau et al., 2018) was manually constructed by clinicians asking naturally occurring questions about radiology images, representing step toward more dynamic reasoning. More recently, large-scale efforts like MedTrinity-25M (Xie et al., 2024b) have emerged, providing over 25 million images with multi-granular annotations to support wide range of tasks. Within the spine domain itself, public datasets have primarily supported foundational computer vision tasks. The VerSe dataset (Sekuboyina et al., 2021), for example, is critical resource providing CT scans with precise voxel-level annotations for vertebral segmentation and identification. Other datasets (Lee et al., 2024a; Ibrahim et al., 2025a) have followed similar focus, providing valuable benchmarks for segmentation of the lumbar spine from MRI. However, these resources are designed to support lower-level cognitive tasks like perception (\"Where is the L4 vertebra?\") or classification (\"Is 15 fracture present?\"). They do not provide the necessary data to train models for the highest level of clinical cognition: synthesizing multimodal information into comprehensive diagnosis and treatment plan. This reveals crucial gap between existing data and the needs of clinical practice, gap our work aims to fill. AI in Spine Analysis: From Tools to Collaborators. Prior AI applications in spine analysis have focused on discrete tasks, creating valuable \"tools\" rather than \"collaborators.\" These include automated vertebral segmentation and the measurement of spinal parameters (Lee et al., 2024a; Ibrahim et al., 2025a). While useful for improving efficiency, these tools perform isolated tasks, leaving the cognitive burden of synthesis and planning to the human clinician (Nath et al., 2025b). Our work directly addresses these gaps. By creating SpineMed-450k, large-scale dataset derived from clinical workflows, and SpineBench, benchmark focused on level-aware, multimodal reasoning, we provide the infrastructure to build and evaluate AI systems that can function as true clinical collaborators in the complex domain of spine surgery. B.2 TRAINING STRATEGY Table 6: Training configurations across different stages."
        },
        {
            "title": "Configuration",
            "content": "Stage-1 PubMedVision-150k orthopedics-230k MedThoughts-8K Medical-R1-Distill-Data medical-o1-reasoning-SFT Stage-2 Spine-open Spine-choice Stage-3 Spine-chat Spine-chat-reasoning Spine-report Spine-report-reasoning 1e-5 16,284 zero2 1 1e-5 16,284 zero2 1 1e-6 49,152 zero"
        },
        {
            "title": "Learning Rate\nMax Length\nDeepSpeed\nEpochs",
            "content": "B.3 PERFORMANCE COMPARISON ON MEDICAL REPORT GENERATION SUBTASKS Table 7: LVLM performance comparison on medical report generation subtasks: Imaging Report (IR), Diagnosis (DGN), Patient Guidance (PG), Evidence-Based Plan (EBP), Technical Feasibility (TF), Risk Prognosis Management (RPM), Coverage (COV), Relevance (REL), Granularity (GRA), Explanation (EXP). Model IR DGN PG EBP TF RPM COV REL GRA EXP Proprietary LVLMS GPT-5 O3 Gemini-2.5-Pro Claude-4 GPT-4o GPT-5-mini Gemini-2.5-Flash 4.54 4.39 4.55 3.96 3.16 4.55 4.43 Open-source LVLMS (>10B) GLM-4.5V Qwen2.5-VL-72B LinShu-32B 3.85 3.14 3.05 Open-source LVLMS (<10B) HuaTuoGPT-7B Qwen2.5-VL-7B Ours 2.42 2.27 4. 4.51 4.25 4.51 4.08 3.03 4.48 4.29 3.78 3.03 3.05 2.42 2.39 4.10 4.62 4.32 4.79 4.41 3.30 4.62 4.73 4.12 3.25 3. 2.91 2.82 4.71 4.69 4.43 4.79 4.44 3.30 4.98 4.88 4.05 3.23 3.49 3.37 3.26 4. 4.58 4.34 4.69 4.30 3.35 4.66 4.64 4.26 3.27 3.21 2.77 2.77 4.51 4.66 4.45 4.83 4.58 4.30 4.87 4.89 4.63 4.19 4. 3.50 3.66 4.81 4.74 4.50 4.84 4.62 2.92 4.90 4.82 4.23 2.98 2.90 2.57 2.60 4. 4.60 4.39 4.80 4.16 3.25 4.67 4.67 4.09 3.25 3.44 2.63 2.65 4.53 4.41 4.30 4.60 3.76 3.07 4.47 4.51 3.77 3.09 3. 2.76 2.86 4.27 4.56 4.40 4.64 3.94 2.80 4.71 4.48 3.59 3.02 3.04 2.77 2.71 4. 16 B.4 PICTURE CONTEXT MATCHING ALGORITHM The following algorithm processes Markdown files to extract image information and generate structured metadata in JSON format through parallel processing. Figure 6: picture context matching algorithm 17 B.5 QUANTITATIVE COMPARISON OF SPINEGPT WITH GPT-4O Figure 7: Comparative analysis of medical report generation capabilities between SpineGPT (Ours) and ChatGPT-4o (general-purpose AI) for an adolescent idiopathic scoliosis case. The comparison demonstrates significant differences in diagnostic depth, clinical reasoning, and treatment planning specificity. SpineGPT provides 72protocols, while ChatGPT-4o offers basic diagnostic and treatment recommendations suitable for general medical documentation. 18 Figure 8: Our models medical report output for adolescent idiopathic scoliosis, featuring six-section structured format: imaging findings, AI diagnosis, treatment recommendations, risk assessment, post-operative management, and clinical rationale. 19 Figure 9: ChatGPT-4o generated medical report for adolescent idiopathic scoliosis, showing generalpurpose AIs approach to clinical documentation with basic diagnostic and treatment recommendations. B.6 PROMPTS"
        },
        {
            "title": "Criteria for Assessing Dimensional Quality in Reports",
            "content": "I. Role and Core Task You will act as top-tier clinical medical expert and AI evaluator (LLM-Judge). Your core task is to rigorously compare the [LLM Generated Answer] provided below with the [Standard Answer]. Based on comprehensive and detailed scoring rubric, you will systematically evaluate the [LLM Generated Answer]s performance against the [Standard Answer] across multiple dimensions, including accuracy, completeness, logical coherence, readability, and clinical utility. Finally, you will output your evaluation in the specified simple format. II. Inputs for Evaluation 1. [Standard Answer] (Golden Answer) [Please paste the ideal, golden standard answer here] 2. [LLM Generated Answer] [Please paste the AI-generated answer that requires evaluation here] III. Evaluation Instructions & Scoring Rubric Please score the [LLM Generated Answer] for each dimension below, strictly based on its comparison with the [Standard Answer]. Please note: Scores MUST be continuous values (e.g., 3.5, 4.2, 4.7) to more precisely reflect the subtle differences in the evaluation results between the integer standards. The integer scores (1-5 points) in the rubric should serve as the primary anchors for your scoring, but you should use decimal precision to capture nuanced differences. For example: - If performance is slightly above \"Good\" (4 pts) but not quite \"Excellent\" (5 pts), use scores like 4.3 or 4.6. - If performance has minor gaps compared to standard, use scores like 3.8 or 4.2. - Avoid whole numbers unless the performance exactly matches the integer anchor description. 1. Structured Imaging Report 5 pts (Excellent / On Par): On par with the [Standard Answer], accurately describes all key imaging findings, correctly explains their clinical significance, and includes quantitative descriptions. 4 pts (Good / Minor Gaps): The description of major findings is correct, but it lacks some of the quantitative details present in the [Standard Answer]. 3 pts (Fair / Clear Gaps): The description is generally correct, but the explanation of clinical significance is clearly less sufficient or in-depth than the [Standard Answer]. 2 pts (Poor / Serious Deficiencies): Omits or incorrectly describes key findings that are mentioned in the [Standard Answer]. 1 pt (Unacceptable / Completely Wrong): Seriously misinterprets the imaging, with key conclusions that contradict the [Standard Answer]. 2. AI-Assisted Diagnosis 5 pts (Excellent / On Par): On par with the [Standard Answer], the primary diagnosis is completely correct, secondary diagnoses are reasonably listed, and key differential diagnoses are correctly ruled out. 4 pts (Good / Minor Gaps): The primary diagnosis is correct, but the list of secondary diagnoses is less complete than in the [Standard Answer]. 3 pts (Fair / Clear Gaps): The primary diagnosis is correct but omits important differential diagnoses that are mentioned in the [Standard Answer]. 2 pts (Poor / Serious Deficiencies): The primary diagnosis is partially incorrect or omits key components present in the [Standard Answer]. 1 pt (Unacceptable / Completely Wrong): The diagnosis is completely wrong or misses life-threatening condition. 3. Treatment Recommendations 3.1 Patient-Oriented Advice 5 pts (Excellent / On Par): On par with the [Standard Answer], the language is extremely colloquial and easy to understand, the information is completely accurate, the structure is clear, and it is highly effective at reassuring the patient. Figure 10: Criteria for Assessing Dimensional Quality in Reports"
        },
        {
            "title": "Criteria for Assessing Dimensional Quality in Reports",
            "content": "4 pts (Good / Minor Gaps): The language is easy to understand and the core information is accurate, but the level of empathy or nuance is slightly inferior to the [Standard Answer]. 3 pts (Fair / Clear Gaps): The language is generally understandable but contains unexplained jargon, the information is mostly correct but vague, and it clearly lacks the empathy shown in the [Standard Answer]. 2 pts (Poor / Serious Deficiencies): The language is obscure and jargon-heavy, the information contains errors or critical omissions, and it is likely to cause patient anxiety. 1 pt (Unacceptable / Completely Wrong): The communication contains serious errors, is misleading, or provides harmful information, making it completely unacceptable. 3.2 Treatment Plan & Evidence-Based Consistency 5 pts (Excellent / On Par): The plans rationale, individualization, and discussion of evidence-based support are all on par with the depth and breadth of the [Standard Answer]. 4 pts (Good / Minor Gaps): The core plan is reasonable, but the discussion of the evidence base is less detailed than in the [Standard Answer]. 3 pts (Fair / Clear Gaps): The plan is generally reasonable but lacks the individualized adjustments highlighted in the [Standard Answer]. 2 pts (Poor / Serious Deficiencies): Parts of the plan are inconsistent with clinical guidelines, making its rationale far weaker than the [Standard Answer]s. 1 pt (Unacceptable / Completely Wrong): The plan clearly conflicts with evidence-based medicine and is diametrically opposed to the recommendations in the [Standard Answer]. 3.3 Surgical/Technical Details & Feasibility 5 pts (Excellent / On Par): The explanation of surgical goals, technical details, preventive measures, and backup plans is comparable in completeness and professionalism to the [Standard Answer]. 4 pts (Good / Minor Gaps): Covers the main technical details, but its consideration of complication prevention is less thorough than the [Standard Answer]s. 3 pts (Fair / Clear Gaps): The description of details is overly general and lacks the specificity and feasibility assessment present in the [Standard Answer]. 2 pts (Poor / Serious Deficiencies): Omits key technical details mentioned in the [Standard Answer], making its feasibility questionable. 1 pt (Unacceptable / Completely Wrong): The technical details are infeasible or pose safety risk. 4. Risk, Prognosis & Post-Op Management 5 pts (Excellent / On Par): Provides perioperative management plan, follow-up schedule, and strategy for potential issues that is as systematic, complete, and forward-thinking as the [Standard Answer]. 4 pts (Good / Minor Gaps): Covers the main measures but is less systematic or detailed in certain aspects compared to the [Standard Answer]. 3 pts (Fair / Clear Gaps): Mentions basic safety measures but lacks the systematic and structured approach demonstrated in the [Standard Answer]. 2 pts (Poor / Serious Deficiencies): Omits important safety protocols that are emphasized in the [Standard Answer]. 1 pt (Unacceptable / Completely Wrong): Seriously neglects safety, contradicting the patient-centric principles of the [Standard Answer]. 5. Theoretical Basis & Disclaimer (EVA 4D Evaluation) 5.1 Coverage 5 pts (Excellent / On Par): On par with the [Standard Answer], accurately identifies and explains all key pieces of evidence with no omissions. 4 pts (Good / Minor Gaps): Covers most key evidence but may omit one non-critical element that was included in the [Standard Answer]. 3 pts (Fair / Clear Gaps): Covers the main evidence but omits one key element or two minor elements present in the [Standard Answer]. 2 pts (Poor / Serious Deficiencies): Covers only small amount of evidence, and the chain of reasoning is far less complete than the [Standard Answer]s. 1 pt (Unacceptable / Completely Wrong): Fails to cover any key evidence, or the evidence cited contradicts the factual basis of the [Standard Answer]. Figure 11: Criteria for Assessing Dimensional Quality in Reports"
        },
        {
            "title": "Criteria for Assessing Dimensional Quality in Reports",
            "content": "5.2 Relevance 5 pts (Excellent / On Par): On par with the [Standard Answer], all discussion is tightly focused on the core diagnosis and decision, with no irrelevant content. 4 pts (Good / Minor Gaps): The main content is relevant, but it includes minor redundant information not found in the focused [Standard Answer]. 3 pts (Fair / Clear Gaps): The discussion mixes relevant and irrelevant information, diluting the focus compared to the [Standard Answer]. 2 pts (Poor / Serious Deficiencies): The bulk of the discussion is weakly linked to the final decision, and the focus is misplaced. 1 pt (Unacceptable / Completely Wrong): The discussion is entirely irrelevant to the diagnosis or is based on incorrect assumptions. 5.3 Granularity 5 pts (Excellent / On Par): On par with the [Standard Answer], provides precise, quantitative details sufficient to support in-depth clinical judgment. 4 pts (Good / Minor Gaps): Provides key specific information, but the level of detail in some areas is not as deep as in the [Standard Answer]. 3 pts (Fair / Clear Gaps): The information is overly general and lacks the distinguishing details found in the [Standard Answer]. 2 pts (Poor / Serious Deficiencies): Uses highly generalized language, providing far less informational value than the [Standard Answer]. 1 pt (Unacceptable / Completely Wrong): Contains only conclusions with no supporting details, or the details are incorrect. 5.4 Explanation 5 pts (Excellent / On Par): On par with the [Standard Answer], the chain of reasoning is clear, complete, and seamless, with all parts logically supporting the conclusion. 4 pts (Good / Minor Gaps): The overall logic is coherent, but the reasoning for specific step is slightly less clear or direct than in the [Standard Answer]. 3 pts (Fair / Clear Gaps): The chain of reasoning has logical gaps or jumps that are more pronounced than in the [Standard Answer]. 2 pts (Poor / Serious Deficiencies): The reasoning contains clear contradictions, or the conclusion does not match the provided evidence. 1 pt (Unacceptable / Completely Wrong): The reasoning is fatally flawed or directly contradicts the conclusion. IV. Required Output Format Please strictly follow this simple format. Each line should contain exactly one score and justification: IMAGING_REPORT: [Score] [Justification] DIAGNOSIS: [Score] [Justification] PATIENT_GUIDANCE: [Score] [Justification] EVIDENCE_BASED_PLAN: [Score] [Justification] TECHNICAL_FEASIBILITY: [Score] [Justification] RISK_PROGNOSIS: [Score] [Justification] COVERAGE: [Score] [Justification] RELEVANCE: [Score] [Justification] GRANULARITY: [Score] [Justification] EXPLANATION: [Score] [Justification] Example: IMAGING_REPORT: 4.2 The report accurately describes key findings but lacks some quantitative details. DIAGNOSIS: 3.8 Primary diagnosis is correct but secondary diagnoses are incomplete. Figure 12: Criteria for Assessing Dimensional Quality in Reports"
        },
        {
            "title": "Orthopedic Category Classification Prompt",
            "content": "Classify the orthopedic question into ONE category. Answer ONLY the category name. Question: {question} Answer: {answer} Categories: Spine Surgery - Conditions, injuries, and surgeries related to the spine Foot and Ankle Surgery - Conditions, injuries, and surgeries related to the foot and ankle Orthopedic Trauma - Fractures, dislocations, and other acute injuries Hand and Upper Extremity Surgery - Conditions, injuries, and surgeries related to the hand, wrist, elbow, and shoulder Musculoskeletal Oncology - Bone and soft tissue tumors Orthopedic Sports Medicine - Sports-related injuries, arthroscopic surgery Adult Joint Reconstruction - Arthritis, joint replacement surgery (e.g., hip, knee) ANSWER WITH THE EXACT NAME: \"Spine Surgery\", \"Foot and Ankle Surgery\", \"Orthopedic Trauma\", \"Hand and Upper Extremity Surgery\", \"Musculoskeletal Oncology\", \"Orthopedic Sports Medicine\", \"Adult Joint Reconstruction\" Figure 13: Prompt for Orthopedic Category Classification"
        },
        {
            "title": "Spine Category Classification Prompt",
            "content": "Classify the spine surgery question into ONE category. Answer ONLY the category name. Question: {question} Answer: {answer} Categories: General Considerations - Basic spine anatomy, evaluation, imaging Biomechanics - Spine mechanics, forces, stability Anatomic Approaches - Surgical approaches, exposure The Cervical Degenerative Spine - Cervical disc, stenosis, anterior cervical discectomy and fusion (ACDF) The Thoracic and Lumbar Degenerative Spine - Lumbar disc, stenosis, fusion Spondylolisthesis - Spondylolisthesis, pars interarticularis defect Idiopathic Scoliosis - Adolescent idiopathic scoliosis, curves Adult Spinal Deformity - Adult scoliosis, sagittal balance Dysplastic and Congenital Deformities - Dysplastic and congenital deformities Neuromuscular Spine Deformity - Neuromuscular spinal deformity Kyphosis and Postlaminectomy Deformities - Kyphosis, post-laminectomy deformities Trauma - Spine fractures, spinal cord injury Tumor and Osteomyelitis - Spine tumors, infections Complications - Surgical complications, internal fixation failure ANSWER WITH THE EXACT NAME: \"General Considerations\", \"Biomechanics\", \"Anatomic Approaches\", \"The Cervical Degenerative Spine\", \"The Thoracic and Lumbar Degenerative Spine\", \"Spondylolisthesis\", \"Idiopathic Scoliosis\", \"Adult Spinal Deformity\", \"Dysplastic and Congenital Deformities\", \"Neuromuscular Spine Deformity\", \"Kyphosis and Postlaminectomy Deformities\", \"Trauma\", \"Tumor and Osteomyelitis\", \"Complications\" Figure 14: Prompt for Spine Category Classification 24 Generating Medical Q&A for Fine-Tuning Prompt \"You are senior clinical medical educator. Please carefully read the provided medical textbook content below and generate **multiple high-quality open-ended question-answer pairs** based on the core knowledge points, all for fine-tuning large language models. Each Q&A should be self-contained and completely independent.\" \"**Strict Requirements:**\" \"1. **Complete Independence**: Each question and answer must constitute complete knowledge unit that can be understood without any external background materials.\" \"2. **Prohibited Referential Terms**: Strictly prohibit using terms like this guide, the study, the above materials, this article, the reportör any other terms that refer to the original text in questions or answers.\" \"3. **Clinical Depth Requirements**: Questions should reflect real clinical scenarios, testing deep understanding and clinical thinking rather than simple yes/no questions.\" \"4. **Open-Ended Design**: Questions should encourage detailed analysis, requiring comprehensive, structured answers that demonstrate clinical reasoning processes.\" \"5. **Answer Completeness**: Answers must be detailed and comprehensive, including analysis process, reasoning logic, and final conclusions.\" \"6. **Question Type Diversity**: Should cover multiple dimensions including pathological mechanism explanation, diagnostic thinking analysis, treatment plan design, complication prevention strategies, etc.\" \"**Question Quantity Requirements:**\" \"- Generate appropriate number of questions based on text length\" \"- Short text (1-2 paragraphs): Generate 2-3 questions\" \"- Medium text (3-5 paragraphs): Generate 3-5 questions\" \"- Long text (6+ paragraphs): Generate 5-8 questions\" \"**Output Format Requirements:**\" \"Strictly follow the XML format below, each textbook page can generate multiple questions:\" \"xml\" \"<problem>[Open-ended question 1 stem]</problem>\" \"<answer>[Detailed open-ended answer for question 1, including analysis process and conclusions]</answer>\" \"<problem>[Open-ended question 2 stem]</problem>\" \"<answer>[Detailed open-ended answer for question 2, including analysis process and conclusions]</answer>\" \"<problem>[Open-ended question 3 stem]</problem>\" \"<answer>[Detailed open-ended answer for question 3, including analysis process and conclusions]</answer>\" \"\" \"**Important Notes:**\" \"1. Strictly follow the XML format\" \"2. Question stems should be clear and specific, encouraging deep thinking, avoiding simple yes/no questions\" \"3. Answers should be comprehensive and detailed, including analysis process, reasoning logic, and final conclusions\" \"4. Output only XML objects, no additional explanatory text\" \"5. Each question should be independent and complete, not dependent on other questions or external materials\" \"6. Answers should demonstrate the depth of medical professional knowledge and the logic of clinical thinking\" \"**Textbook Content:**content\" \"Please generate high-quality medical open-ended question-answer pairs based on the above textbook content.\" Figure 15: Prompt for Generating Medical Q&A for Fine-Tuning Generating Medical MCQs for Fine-Tuning Prompt \"You are senior clinical medical educator and examination expert. Please carefully read the provided medical textbook content below and generate **multiple high-quality multiple-choice questions with answers** based on the core knowledge points, all for fine-tuning large language models. Each Q&A should be self-contained and completely independent.\" \"**Strict Requirements:**\" \"1. **Complete Independence**: Each question and options must constitute complete knowledge unit that can be understood without any external background materials.\" \"2. **Prohibited Referential Terms**: Strictly prohibit using terms like this guide, the study, the above materials, this article, the reportör any other terms that refer to the original text in questions or options.\" \"3. **Clinical Depth Requirements**: Questions should reflect real clinical scenarios, testing deep understanding and clinical judgment rather than simple memorization.\" \"4. **Option Design**: Each question must include 4 options (A, B, C, D), with 1 correct answer and 3 high-quality distractors. Distractors should be based on common clinical misconceptions or related concepts.\" \"5. **Question Type Diversity**: Should cover multiple dimensions including diagnostic reasoning, treatment selection, mechanism explanation, differential diagnosis, complication prevention, etc.\" \"**Question Quantity Requirements:**\" \"- Generate appropriate number of questions based on text length\" \"- Short text (1-2 paragraphs): Generate 2-3 questions\" \"- Medium text (3-5 paragraphs): Generate 4-6 questions\" \"- Long text (6+ paragraphs): Generate 7-10 questions\" \"**Output Format Requirements:**\" \"Strictly follow the XML format below, each textbook page can generate multiple questions:\" \"xml\" \"<problem>[Question 1 stem] A. [Option A] B. [Option B] C. [Option C] D. [Option D]</problem>\" \"<answer>[Question 1 correct answer option letter]</answer>\" \"<problem>[Question 2 stem] A. [Option A] B. [Option B] C. [Option C] D. [Option D]</problem>\" \"<answer>[Question 2 correct answer option letter]</answer>\" \"<problem>[Question 3 stem] A. [Option A] B. [Option B] C. [Option C] D. [Option D]</problem>\" \"<answer>[Question 3 correct answer option letter]</answer>\" \"\" \"**Important Notes:**\" \"1. Strictly follow the XML format\" \"2. Question stems should be clear and specific, testing deep understanding and clinical judgment\" \"3. Options should be reasonably designed, including correct answers and high-quality distractors\" Figure 16: Prompt for Generating Medical MCQs for Fine-Tuning 26 Generating Context-Localized Multimodal Q&A Prompt \"You are senior clinical medical educator. Based on the provided image information, caption, and context, precisely locate the images position in the context and generate high-quality open-ended questions and answers.\" \"**Core Task: Multimodal Understanding and Precise Localization Open-Ended Q&A**\" \"**Step 1: Multimodal Information Understanding**\" \"1. **Image Understanding**: Analyze the specific medical content shown in the image (anatomical structures, pathological manifestations, surgical procedures, imaging features, instrument usage, etc.)\" Identify figure numbers, positions, operational steps, or key \"2. **Caption Understanding**: information mentioned in the caption\" \"3. **Context Understanding**: Analyze medical knowledge points, operational procedures, and clinical key points in the preceding and following text\" \"4. **Position Localization**: Precisely locate the images specific position and role in the context\" \"**Step 2: Precise Position Localization**\" \"1. **Caption-Context Matching**:\" \" - If the caption contains figure number (e.g., Figure 12.1), find the corresponding figure reference in the context\" \" - If the caption describes operational steps, locate the corresponding operational description in the context\" \" - If the caption describes anatomical structures, find related anatomical descriptions in the context\" \"2. **Context Position Analysis**:\" \" - Analyze preceding text: background information, preparation steps, and related concepts before the image appears\" \" - Analyze following text: operational steps, precautions, and clinical significance after the image is shown\" \" - Determine the images specific role in the entire process\" \"**Step 3: Generate Open-Ended Q&A Based on Precisely Located Content**\" \"Must generate open-ended questions and answers based on precisely located medical knowledge points:\" \"- Deeply analyze the relationship between located content and the image\" \"- Generate open-ended questions based on precisely located content\" \"- Ensure questions are highly relevant to both image content and context\" \"- If unable to precisely locate suitable content, skip this question\" \"**Step 4: High-Quality Open-Ended Q&A Design**\" \"Based on precisely located content, generate open-ended questions and answers with the following characteristics:\" \"**Q&A Design Principles:**\" \"1. **Multimodal Relevance**: Questions must be relevant to image content, caption information, and context content simultaneously\" \"2. **Clinical Orientation**: Questions should be based on real clinical scenarios, testing clinical thinking and decision-making abilities\" \"3. **Open-Ended Design**: Encourage deep thinking, avoid yes/no questions, require detailed analysis\" \"4. **Position Precision**: Questions should be based on the images precise location in the context\" \"5. **Prohibited Referential Terms**: Strictly prohibit using terms like äccording to the context, this guide, the study, the above materials, this article, the reportör any other terms that refer to the original text in questions or answers\" \"6. **Answer Design**:\" Figure 17: Prompt for Generating Context-Localized Multimodal Q&A 27 Generating Context-Localized Multimodal Q&A Prompt \" - Answers must be based on precisely located content\" \" - Cover relevant medical knowledge and clinical considerations\" \" - Reflect clinical thinking and decision-making process\" \"7. **Question Type Priority**:\" \" - Diagnostic analysis questions (in-depth analysis based on imaging findings, clinical symptoms, etc.)\" \" - Treatment decision questions (detailed analysis of surgical indications, treatment plan selection, etc.)\" \" - Mechanism explanation questions (in-depth explanation of anatomical-physiological relationships, pathological mechanisms, etc.)\" \" - Technical operation questions (detailed explanation of surgical steps, instrument usage, etc.)\" \" - Risk assessment questions (comprehensive analysis of complication prevention, management strategies, etc.)\" \"**Output Format Requirements:**\" \"Strictly follow the following XML format, each image can generate multiple different Q&A pairs:\" \"xml\" \"<problem><image> n[First open-ended question stem]</problem>\" \"<answer>[First detailed open-ended answer]</answer>\" \"<problem><image> n[Second open-ended question stem]</problem>\" \"<answer>[Second answer, directly answering the question]</answer>\" \"<problem><image> n[Third open-ended question stem]</problem>\" \"<answer>[Third answer, directly answering the question]</answer>\" \"\" \"**Important Notes:**\" \"1. Strictly follow the XML format\" \"2. Question stems should be clear and specific, encouraging deep thinking, avoiding simple yes/no questions\" \"3. Answers should be comprehensive and detailed, including analysis process, reasoning logic, and final conclusions\" \"4. If unable to precisely locate relevant content, do not generate questions\" \"5. Output only one complete XML object\" \"6. Strictly prohibit using referential terms\" \"7. **Image Reference Standards**: When referencing images in questions, use general terms like äs shown in the image, the image displays, ïmaging findingsëtc., strictly prohibit using specific figure numbers (e.g., Figure 10.8, Figure 12.1, etc.)\" \"**Processing Workflow:**\" \"1. Analyze the image caption to understand the specific medical content shown\" \"2. Precisely locate medical knowledge points in the context related to the caption\" \"3. Determine the images specific position and role in the context\" \"4. If precise localization is successful and content is suitable for questions, generate open-ended Q&A based on located content\" \"5. If unable to precisely locate or content is not suitable for questions, do not generate questions\" \"6. Ensure questions have clinical value and educational significance\" \"**Provided Information:**\" \"Image Caption: caption\" \"Context Information: context\" \"Please precisely locate the images position in the context and generate high-quality medical openended questions and answers. If unable to precisely locate suitable content, do not generate questions. Strictly follow the specified XML format.\" Figure 18: Prompt for Generating Context-Localized Multimodal Q&A Generating Context-Localized Multimodal MCQs Prompt \"You are senior clinical medical educator and examination expert. Your task is to precisely locate the images position in the context based on the provided image information, caption, and context, then generate high-quality multiple-choice questions.\" \"**Core Task: Multimodal Understanding and Precise Localization Question Generation**\" \"**Step 1: Multimodal Information Understanding**\" \"1. **Image Understanding**: Analyze the specific medical content shown in the image (anatomical structures, pathological manifestations, surgical procedures, imaging features, instrument usage, etc.)\" \"2. **Caption Understanding**: Identify figure numbers, positions, operational steps, or key information mentioned in the caption\" \"3. **Context Understanding**: Analyze medical knowledge points, operational procedures, and clinical key points in the preceding and following text\" \"4. **Position Localization**: Precisely locate the images specific position and role in the context\" \"**Step 2: Precise Position Localization**\" \"1. **Caption-Context Matching**:\" \" - If the caption contains figure number (e.g., Figure 12.1), find the corresponding figure reference in the context\" \" - If the caption describes operational steps, locate the corresponding operational description in the context\" \" - If the caption describes anatomical structures, find related anatomical descriptions in the context\" \"2. **Context Position Analysis**:\" \" - Analyze preceding text: background information, preparation steps, and related concepts before the image appears\" \" - Analyze following text: operational steps, precautions, and clinical significance after the image is shown\" \" - Determine the images specific role in the entire process\" \"**Step 3: Generate Questions Based on Precisely Located Content**\" \"Must generate clinical multiple-choice questions based on precisely located medical knowledge points:\" \"- Deeply analyze the relationship between located content and the image\" \"- Generate multiple-choice questions based on precisely located content\" \"- Ensure questions are highly relevant to both image content and context\" \"- If unable to precisely locate suitable content, skip this question\" \"**Step 4: High-Quality Multiple-Choice Question Design**\" \"Based on precisely located content, generate clinical multiple-choice questions with the following characteristics:\" \"**Q&A Design Principles:**\" \"1. **Multimodal Relevance**: Questions must be relevant to image content, caption information, and context content simultaneously\" \"2. **Clinical Orientation**: Questions should be based on real clinical scenarios, testing clinical thinking and decision-making abilities\" \"3. **Multiple-Choice Design**: Provide multiple options to test deep understanding and clinical judgment\" \"4. **Position Precision**: Questions should be based on the images precise location in the context\" \"5. **Prohibited Referential Terms**: Strictly prohibit using terms like äccording to the context, this guide, the study, the above materials, this article, the reportör any other terms that refer to the original text in questions or options\" Figure 19: Prompt for Generating Context-Localized Multimodal MCQs 29 Generating Context-Localized Multimodal MCQs Prompt \"6. **Option Design**:\" \" - Correct answer must be based on precisely located content\" \" - Distractors should be based on common clinical misconceptions or related but inaccurate concepts\" \" - All options should have clinical plausibility\" \"7. **Question Type Priority**:\" \" - Diagnostic analysis questions (in-depth analysis based on imaging findings, clinical symptoms, etc.)\" \" - Treatment decision questions (detailed analysis of surgical indications, treatment plan selection, etc.)\" \" - Mechanism explanation questions (in-depth explanation of anatomical-physiological relationships, pathological mechanisms, etc.)\" \" - Technical operation questions (detailed explanation of surgical steps, instrument usage, etc.)\" \" - Risk assessment questions (comprehensive analysis of complication prevention, management strategies, etc.)\" \"**Output Format Requirements:**\" \"**Strictly follow the following XML format:**\" \"xml\" \"<problem><image> n[Question stem] A. [Option A] B. [Option B] C. [Option C] D. [Option D]</problem>\" \"<answer>[Correct answer option letter]</answer>\" \"\" \"**Important Notes:**\" \"1. Strictly follow the XML format\" \"2. Question stems should be clear and specific, testing deep understanding and clinical judgment\" \"3. Options should be reasonably designed, including correct answers and distractors\" \"4. If unable to precisely locate relevant content, do not generate questions\" \"5. Output only one complete XML object\" \"6. Strictly prohibit using referential terms\" \"7. **Image Reference Standards**: When referencing images in questions, use general terms like äs shown in the image, the image displays, ïmaging findingsëtc., strictly prohibit using specific figure numbers (e.g., Figure 10.8, Figure 12.1, etc.)\" \"**Processing Workflow:**\" \"1. Analyze the image caption to understand the specific medical content shown\" \"2. Precisely locate medical knowledge points in the context related to the caption\" \"3. Determine the images specific position and role in the context\" \"4. If precise localization is successful and content is suitable for questions, generate multiple-choice questions based on located content\" \"5. If unable to precisely locate or content is not suitable for questions, do not generate questions\" \"6. Ensure questions have clinical value and educational significance\" \"**Provided Information:**\" \"Image Caption: caption\" \"Context Information: context\" \"Please precisely locate the images position in the context and generate high-quality medical multiplechoice questions. If unable to precisely locate suitable content, do not generate questions. Strictly follow the specified XML format.\" Figure 20: Prompt for Generating Context-Localized Multimodal MCQs"
        }
    ],
    "affiliations": [
        "Beijing Jiaotong University",
        "Institute of Automation, Chinese Academy of Sciences",
        "Jilin University",
        "Nanjing University",
        "Ningxia University",
        "Stanford University",
        "The General Hospital of the Peoples Liberation Army",
        "Wuhan University",
        "Zhejiang University",
        "π3 Lab"
    ]
}