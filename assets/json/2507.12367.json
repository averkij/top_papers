{
    "paper_title": "GitChameleon: Evaluating AI Code Generation Against Python Library Version Incompatibilities",
    "authors": [
        "Diganta Misra",
        "Nizar Islah",
        "Victor May",
        "Brice Rauby",
        "Zihan Wang",
        "Justine Gehring",
        "Antonio Orvieto",
        "Muawiz Chaudhary",
        "Eilif B. Muller",
        "Irina Rish",
        "Samira Ebrahimi Kahou",
        "Massimo Caccia"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid evolution of software libraries poses a considerable hurdle for code generation, necessitating continuous adaptation to frequent version updates while preserving backward compatibility. While existing code evolution benchmarks provide valuable insights, they typically lack execution-based evaluation for generating code compliant with specific library versions. To address this, we introduce GitChameleon, a novel, meticulously curated dataset comprising 328 Python code completion problems, each conditioned on specific library versions and accompanied by executable unit tests. GitChameleon rigorously evaluates the capacity of contemporary large language models (LLMs), LLM-powered agents, code assistants, and RAG systems to perform version-conditioned code generation that demonstrates functional accuracy through execution. Our extensive evaluations indicate that state-of-the-art systems encounter significant challenges with this task; enterprise models achieving baseline success rates in the 48-51\\% range, underscoring the intricacy of the problem. By offering an execution-based benchmark emphasizing the dynamic nature of code libraries, GitChameleon enables a clearer understanding of this challenge and helps guide the development of more adaptable and dependable AI code generation methods. We make the dataset and evaluation code publicly available at https://github.com/mrcabbage972/GitChameleonBenchmark."
        },
        {
            "title": "Start",
            "content": "GitChameleon: Evaluating AI Code Generation Against Python Library Version Incompatibilities Diganta Misra1,2*, Nizar Islah3,10*, Victor May4, Brice Rauby3, 5, Zihan Wang6, Justine Gehring3,7,8, Antonio Orvieto1,2,9, Muawiz Chaudhary3, Eilif B. Muller3,10, Irina Rish3,10, Samira Ebrahimi Kahou3, Massimo Caccia11 Team Leads, Data and Core Contributors, Senior Advisors 1ELLIS Institute Tübingen, 2MPI-IS Tübingen, 3Mila Quebec AI Institute, 4Google, 5Polytechnique Montréal, 6McGill University, Montréal, 7Moderne, 8Gologic, 9Tübingen AI Center, 10Université de Montréal, 11ServiceNow Research Correspondence: diganta.misra@tue.ellis.eu, nizar.islah@mila.quebec 5 2 0 2 6 1 ] . [ 1 7 6 3 2 1 . 7 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "The rapid evolution of software libraries poses considerable hurdle for code generation, necessitating continuous adaptation to frequent version updates while preserving backward compatibility. While existing code evolution benchmarks provide valuable insights, they typically lack execution-based evaluation for generating code compliant with specific library versions. To address this, we introduce GitChameleon, novel, meticulously curated dataset comprising 328 Python code completion problems, each conditioned on specific library versions and accompanied by executable unit tests. GitChameleon rigorously evaluates the capacity of contemporary large language models (LLMs), LLM-powered agents, code assistants, and RAG systems to perform versionconditioned code generation that demonstrates functional accuracy through execution. Our extensive evaluations indicate that state-of-the-art systems encounter significant challenges with this task; enterprise models achieving baseline success rates in the 48-51% range, underscoring the intricacy of the problem. By offering an execution-based benchmark emphasizing the dynamic nature of code libraries, GitChameleon enables clearer understanding of this challenge and helps guide the development of more adaptable and dependable AI code generation methods. We make the dataset and evaluation code publicly available 1."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) are increasingly integral to software development, being adopted for tasks like code generation and review (Council, 2024; Lambiase et al., 2025). Despite LLM advancements like larger context windows (Su et al., 2023), faster inference (Dao et al., 2022), and high performance on general *Equal Contribution 1https://github.com/mrcabbage972/GitChameleon Benchmark 1 In this GitChameleon problem, Figure 1: the gpt-4o-mini model produced an incorrect solution due for seaborn.violinplot by using the deprecated bw parameter, instead of the appropriate bw_method and bw_adjust required by the specified library version. coding benchmarks (Hendrycks et al., 2021; Chen et al., 2021), critical capability remains underevaluated: generating code that is compliant with specific library version. This task of versionswitching, which is essential for robust development in environments with fixed or legacy dependencies, is not well-verified in contemporary LLMs. Existing benchmarks, while valuable, often focus on migrating codebases to newer versions (i.e., code evolution) or use non-executable evaluation methods. They do not fully address the challenge of generating new, functionally correct code for static version constraint. For instance, PyMigBench (Islam et al., 2023) provides comprehensive datasets of real-world, inter-library migrations, rather than focusing on executable, intra-library tasks conditioned on specific versions. signed to assess the capability of LLMs and AI agents in generating version-aware Python code. GitChameleon features problems centered on documented breaking changes from popular libraries, requiring models to produce solutions for explicitly specified versions (an illustrative example is shown in Figure 1). The development of such benchmark faces challenges in meticulously curating version-specific breaking changes from library changelogs and crafting corresponding testable scenarios. Our comprehensive evaluation of diverse LLM-based tools on GitChameleon reveals critical limitations in existing systems ability to handle library versioning. In summary, our contributions are highlighted as follows: We introduce novel code completion benchmark GitChameleon consisting of 328 Python-based version-conditioned problems, including visible tests for self-debugging and documentation references for RetrievalAugmented Generation (RAG). We present comprehensive empirical study on GitChameleon, evaluating the capabilities of diverse range of contemporary AI code generation systems, including AI agents, IDEintegrated and CLI-based coding assistants, and RAG-based LLM pipelines. We reveal critical limitations in the ability of current AI systems to adhere to specific versioning constraints and highlight factors impacting their performance, thereby providing insights to steer the development of more adaptable and dependable AI code generation methods."
        },
        {
            "title": "2 GitChameleon Benchmark",
            "content": "We introduce GitChameleon, manually authored benchmark that comprises 328 Pythonbased version-conditioned problems focused on popular code libraries. To evaluate performance on GitChameleon, each problem is accompanied by suite of assertion-based unit tests, enabling thorough execution-based assessment of potential solutions. In the following sections, we detail the dataset structure, dataset statistics, evaluation metrics, and sample verification process. Figure 2: An illustration of two evaluation paradigms for code generation models. Code Evolution (right) assesses model capabilities on out-of-distribution (OOD) data, using library versions or new libraries not encountered during training. In contrast, Version-Conditioned Generation (VCG) (left) focuses on the practical ability to generate code for specific, in-distribution (ID) library versions that the model has seen before. CodeUpdateArena (Liu et al., 2025) valuably assesses LLM knowledge editing using synthetically generated API updates for functions in popular libraries, different approach from using documented historical breaking changes. Other relevant studies, such as Wang et al. (2024b), investigate the propensity of LLMs to generate code with deprecated APIs, which does not entirely cover the broader capability of generating software that adheres to precise, user-specified library versions involving various types of API changes. Code Evolution vs. Version Conditioned Generation (VCG). Existing code evaluation benchmarks often focus on assessing the code evolution or migration capabilities of LLMs, where changes occur only in the forward direction and typically involve unseen library versions or entirely new libraries. This framing inherently makes the task out-of-distribution (OOD), as illustrated in Figure 2. In contrast, version-conditioned generation (VCG)the ability of LLMs to produce code aligned with specific, previously seen library versionsis critical for practical deployment. It enables models to function reliably in real-world production environments or constrained settings where the libraries in use may not be the latest stable versions. To better evaluate this capability, benchmark must pose problems that are strictly indistribution (ID) with respect to the relevant library version(s) required to solve them. To bridge this gap, our work introduces GitChameleon, an executable benchmark de-"
        },
        {
            "title": "2.3 Statistics",
            "content": "GitChameleon consists of 328 Python-based version conditioned problems based on 26 libraries spanning scientific computing, data science and web development. The samples were collected from version releases over period from the year 2014 to 2023 and exclude legacy and yanked version releases. Figure 3: Can you predict GitChameleon performance from other code generation benchmarks? Here we present the Spearman (ρ) and Pearson (r) correlations between GitChameleon, SWE-Bench (Jimenez et al., 2024), and LiveCodeBench (Jain et al., 2024). GitChameleon exhibits moderate correlation with SWE-Bench, with ρ of 0.550 and of 0.675; and weak correlation with LiveCodeBench, with ρ of 0.214 and of 0.130."
        },
        {
            "title": "2.1 Dataset Structure",
            "content": "Each dataset sample includes problem related to breaking change in Python library. To validate candidate solution, we provide suite of tests, consisting of comprehensive suite of Hidden Tests to be used for model performance evaluation and ranking and concise Visible Test to provide execution feedback for SelfDebugging (Chen et al., 2023) experiments. The detailed structure of dataset samples is presented in Table 5. For schematic of the workflow for evaluating method against sample from GitChameleon, see Figure 5."
        },
        {
            "title": "2.2 Evaluation Metrics",
            "content": "The benchmark metric is the success rate on hidden tests, which directly penalizes version mismatches that cause runtime errors during our executionbased validation. As secondary metric, we use the API Hit Rate (Wang et al., 2024a): the percentage of generated solutions that correctly call all APIs specified in the ground-truth solution. Note that this hit rate can be lower than the success rate, as functionally correct alternative solutions may use different APIs. Figure 4: (a) Most versions in GitChameleon were released between 20212023, with few in earlier years. (b) The most common type of change between versions was an argument or attribute change, while semantic or functional changes were least common. As demonstrated in Fig. 4(a), most of the samples in GitChameleon are from versions of libraries released in the years 2021-2023. We intentionally use versions that fall within the training window of most evaluated models. The challenge is therefore not one of data contamination, but of control and disambiguation: when model has been exposed to multiple library versions, can it correctly generate code for the specific version required by the prompt? The dataset was constructed through careful manual effort, with over 350 hours invested in identifying historical breaking changes, crafting problem statements, and validating unit tests. Further details about the benchmark and its construction process are presented in Appendix A."
        },
        {
            "title": "3 Empirical Study",
            "content": "We evaluate GitChameleon in comprehensive selection of settings, including Greedy Decod-"
        },
        {
            "title": "3.1.4 Retrieval-Augmented Generation\nWe designed a RAG (Lewis et al., 2020) pipeline\nwhere we first constructed a vectorized database\n(VectorDB) by embedding each sample’s rel-\nevant API documentation with the OpenAI\ntext-embedding-3 large model 3. The corpus\nused for constructing the VectorDB included 536\ndocuments, with 140 samples having 1 associated\ndocument, 168 having 2 associated documents and\n20 having 3 documents.",
            "content": "Subsequently, we used DocPrompting (Zhou et al., 2022) to query the VectorDB to generate solutions."
        },
        {
            "title": "3.1.5 Multi-Step Agent\nWe conducted experiments with a tool-calling\nagent, as implemented by the smolagents (Roucher\net al., 2025) 4 framework. This agent implemen-\ntation mostly follows the ReAct (Yao et al., 2023)\nmethod, but, it alternates between acting and plan-\nning (Li, 2024) steps.",
            "content": "Following the Agentic RAG approach (Singh et al., 2025), we had equipped the agent with grounding tool in order to assess its capability to independently fetch relevant info for solving the benchmark problems. To this end, we had experimented with the following grounding tools: DuckDuckGo Search (DuckDuckGo, 2025), Perplexity (Perplexity AI, 2024), and Gemini with Grounding (Google, 2025). Additionally, we examined agentic multi-step self-debugging (Jin et al., 2024) by including or omitting code execution sandbox tool (Rabin et al., 2025), which provides the needed dependencies for each example. The sandbox takes 3https://openai.com/index/new-embedding-model s-and-api-updates/ Figure 5: An illustration of the workflow for single example within GitChameleon. The inputs, comprising the Problem Statement, Starter Code, and Dependency Info, are processed by an LLM or an AI agent to generate Candidate Solution. This candidate solution then undergoes validation using the Hidden Tests to determine success on the benchmark. Results from the Visible Tests can be fed back into the solution method for self-debugging. ing, Chain-of-Thought (Wei et al., 2023), SelfDebugging (Chen et al., 2023), RAG (Lewis et al., 2020), Multi-Step Agents (Yao et al., 2023) and enterprise Coding Assistant software products, to assess their ability to generate version-specific executable code. This section first presents the experimental setup, then reports the experiment results in each setting, and finally shows breakdown of the observed results along few key dimensions."
        },
        {
            "title": "3.1 Experimental Setup",
            "content": "In this section, we present the experimental setup used for each of our settings. To ensure version compliance, we use dual control mechanism: the target version is explicitly included in the models prompt, and the validation environment is configured with that exact library version. All prompts are shown in Appendix I. For prompt optimization, we used the Anthropic Prompt Improver 2. Further automated prompt optimization efforts did not make significant change, as described in Table 11."
        },
        {
            "title": "3.1.1 Greedy Decoding",
            "content": "We configured the generation parameters with sampling temperature of 0 and top_p value of 0.95. We had specified structured output schema that specifies the fields Answer and Explanation, where both are of type string. 2https://docs.anthropic.com/en/docs/build-wit 4https://huggingface.co/learn/agents-course/ h-claude/prompt-engineering/prompt-improver en/unit2/smolagents/tool_calling_agents Python program as input and outputs the standard output from the program."
        },
        {
            "title": "3.1.6 AI Coding Assistants",
            "content": "In addition to evaluating generic agentic framework endowed with basic tools, we also analyze the performance of specialized AI coding assistant software. For this setting, we examine both CommandLine Interface (CLI), such as Claude Code5 coding assistants and Integrated Development Environment (IDE) coding assistants, such as Cline6. Specifically, in this evaluation we aimed to evaluate the code completion functionality of code assistants in an IDE or terminal environment wherein the goal was to complete the starter code of each GitChameleon problem with the generated solution. The input to the assistants is given as Python file which consists of the required library, version and extra dependencies as in-line comments and subsequently the starter code. NOTE: All assistants had internet and terminal commands execution access."
        },
        {
            "title": "We had furthermore ablated this setting versus",
            "content": "giving the full problem statement as input."
        },
        {
            "title": "3.2 Experiment Results",
            "content": "This section presents the benchmark results in each setting, as described in the Experimental Setup section (3.1). Table 1 contains the results for Greedy Decoding, Self-Debug and Zero-Shot CoT."
        },
        {
            "title": "3.2.1 Greedy Decoding",
            "content": "We observe that the largest Enterprise-grade models, including Claude 3.7 Sonnet, Gemini 2.5 Pro, GPT-4.1, GPT-4o, and o1, exhibit comparable hidden success rates, generally falling within the 4851% range. Among these o1 (51.2% hidden) achieves the highest hidden success rate. The open-weight Llama models are notably behind, even the recently released Llama 4 Maverick FP8 (40.8% hidden success rate). Model size clearly impacts performance: for instance, Gemini 2.5 Flash trails its Pro counterpart by nearly 12% on hidden tests (38.1% vs. 50.0%). Similarly, the mini and nano series within the GPT family (e.g., GPT-4.1-mini, GPT-4.1-nano, GPT-4o-mini) consistently show 5https://docs.anthropic.com/en/docs/claude-c ode/overview 6https://cline.bot/ lower performance than their larger full-size siblings, with differences on hidden tests ranging from approximately 4 to 15 points."
        },
        {
            "title": "3.2.2 Zero-Shot Chain-Of-Thought",
            "content": "This approach does not uniformly improve LLM performance across all models. While some models demonstrate significant gains in hidden success rates, substantial number of enterprise-grade models and their smaller variants experience performance degradation. For instance, notable improvements in hidden success rates are observed in models such as Llama 3.1 Instruct Turbo (from 30.2% to 36.6%, +6.4 point increase) and o3-mini (from 45.1% to 50.9%, +5.8 point increase). Conversely, several models exhibit decrease in performance with CoT. Prominent examples include Gemini 2.0 Flash (from 44.2% to 36.0%) and even the top-performing o1 (from 51.2% to 41.2%)."
        },
        {
            "title": "3.2.3 LLM Self-Debugging",
            "content": "Hidden Success Rate: Across models, SelfDebugging significantly improves the hidden success rates. Observed gains range from approximately 10% to 20%. For instance, Llama 3.1s hidden success rate increases from 30% to 52.1%, and GPT-4.1-mini shows an improvement from 44% to 68%. This demonstrates the strong capability of modern LLMs to diagnose failures and generate corrected code. Visible Success Rate: As expected, the improvement is even more pronounced on visible tests, ranging from 13 to 37 points. For instance, GPT-4.1s success rate improves from 49% to 69%, Claude 3.7 Sonnets success rate improves from 56% to 83% and Gemini 2.0 Flash improves from 50% to 75%. Visible-Hidden Gap Analysis: In Figure 6, we present the effect of self-debugging on the size of the gap between the success rate on visible tests and the success rate on hidden tests."
        },
        {
            "title": "3.2.4 Multi-Step Agent",
            "content": "We report the performance of Multi-Step Agents on GitChameleon in Table 2. clear and significant trend is the substantial increase in success rates for all models and grounding methods when giving the agent sandbox tool. Overall, Claude Sonnet 3.5 demonstrated the highest success rates with sandbox, across all grounding methods, while 5 Model Greedy Decoding Greedy with Self-Debug Zero-shot CoT Success Rate (%) Hidden Visible API Hit Rate (%) Success Rate (%) Hidden Visible API Hit Rate (%) Success Rate (%) Hidden API Hit Rate (%) Open-Weights Models Llama 3.1 Instruct Turbo Llama 3.3 Instruct Turbo 70B Llama 4 Maverick 400B Qwen 2.5-VL Instruct 72B 30.22.5 36.32.7 40.82.7 48.22.8 38.12.7 43.32.7 46.62.8 55.52.7 Enterprise Models Claude 3.7 Sonnet Gemini 1.5 Pro Gemini 2.0 Flash Gemini 2.5 Pro Gemini 2.5 Flash GPT-4.1 GPT-4.1-mini GPT-4.1-nano GPT-4o GPT-4o-mini GPT-4.5 Grok 3 Mistral Medium 3 48.82.8 45.12.7 44.22.7 50.02.8 38.12.6 48.52.8 44.22.7 33.82.6 49.12.8 37.22.6 40.82.7 48.22.8 43.62.7 55.82.7 51.52.8 50.62.8 61.02.8 41.82.7 49.12.8 50.02.8 35.12.6 54.02.8 46.32.7 46.02.7 53.72.8 49.12. 39.72.7 36.42.7 49.52.8 43.82.7 46.02.8 46.82.7 43.82.7 47.72.7 45.42.7 46.82.7 44.52.7 43.12.7 46.52.7 38.42.6 52.82.8 44.82.7 44.22.7 52.12.8 53.02.8 58.52.7 64.62.6 69.22.5 70.12.5 72.32.5 77.42.3 65.92.6 62.52.8 70.42.7 61.32.8 65.92.8 63.42.8 68.02.8 67.72.7 64.92.8 60.42.7 66.22.8 67.12.8 61.32.8 75.92.4 72.62.4 79.02.4 73.82.2 73.22.4 76.82.1 79.32.3 74.42.6 72.32.5 71.62.6 74.42.4 77.12.3 71.32. 41.52.7 37.42.7 46.82.8 45.32.7 47.62.8 48.62.7 49.42.7 49.22.7 45.82.7 48.32.7 46.32.7 45.82.7 48.02.7 40.62.7 54.42.7 46.32.8 45.42.7 36.62.7 37.52.7 46.62.8 45.12.7 45.12.7 43.32.7 36.02.6 49.42.8 30.82.5 47.92.8 24.11.8 11.91.8 50.32.8 36.02.6 39.92.6 49.42.8 44.22.7 35.32.6 37.22.7 41.32.7 43.02.7 43.42.7 44.62.8 41.82.7 49.12.8 49.82.8 44.52.7 41.32.7 32.12.5 42.52.7 37.32.6 48.82.8 44.22.7 44.12. Table 1: Success rate on visible and hidden tests and API hit rate under the Greedy, Self-Debug, and Zero-shot CoT settings, grouped by OSS vs. Enterprise models. Model ranking on the benchmark is determined by Hidden Success Rate. Visible Success Rate figures are for context on Self-Debugging. The best result in each column is in bold. For full model details and citations, please refer to Appendix J. Model Grounding Method Claude Sonnet 3.5 Gemini 1.5 Pro GPT-4o DuckDuckGo Perplexity Grounded Gemini DuckDuckGo Perplexity Grounded Gemini DuckDuckGo Perplexity Grounded Gemini Success Rate (%) API Hit Rate (%) No Sandbox Sandbox No Sandbox Sandbox 41.72.7 44.12.7 40.02.7 46.02.8 46.52.8 44.12.7 23.92.4 33.52.6 25.42.4 55.32.7 51.42.8 53.72.8 49.82.8 44.42.7 49.22.8 33.22.6 41.52.7 50.02. 42.22.7 41.82.7 41.02.7 47.42.8 47.22.8 49.72.8 44.22.7 43.22.7 46.52.8 48.92.8 46.02.8 45.22.7 50.32.8 46.62.8 51.22.8 48.12.8 44.72.7 44.22. Table 2: Multi-Step Agent performance with different models, grounding methods, and sandbox states. The best result in each column is in bold. not given, Cline with GPT-4.1 achieves the best result, with success rate of 38.4%. All assistants besides for Goose on GPT-4o demonstrate significant gains, ranging from 12 to 35 points, from including the problem statement."
        },
        {
            "title": "3.2.6 Retrieval-Augmented Generation",
            "content": "Table 4 presents the performance of various models with RAG. Many models exhibit significant (up to 10%) boost in success rate with RAG compared to greedy decoding alone. Notably, GPT-4.1, the best 7This version of the model is not FP8-quantized, unlike the one presented in Table 1 Figure 6: Analysis of the Visible-Hidden Gap Before and After Self-Debugging. We analyze how selfdebugging affects the gap between the success rate on visible and hidden tests. We can see that for all models, the gap increases after self-debugging. This shows that self-debugging on visible tests has limited ability to improve on the hidden tests. Gemini 1.5 Pro demonstrated the best results without sandbox."
        },
        {
            "title": "3.2.5 AI Coding Assistants",
            "content": "Table 3 presents the success rates of various CLI and IDE assistants on the visible and hidden tests in GitChameleon. When the problem statement is 6 Name Model CLI Assistants Success Rate (%) API Hit Rate (%) various error types. No-prob Prob No-prob Prob Claude Code Claude 3.7 Sonnet 32.02.6 48.82.8 44.22.7 45.52.7 Goose GPT-4o GPT-4.1 IDE Assistants 36.32.7 19.22.2 36.92.7 55.52.7 43.92.7 41.72.7 54.52.7 53.02. Cline Claude 3.7 Sonnet GPT-4.1 GPT-4.1-mini GPT-4.1-nano GPT-4o 32.92.6 38.42.7 27.12.5 38.12.7 41.52.7 44.82.7 54.62.7 42.12.7 54.62.7 Kilocode Claude 3.7 Sonnet 30.22.5 Roocode Claude 3.5 Sonnet 12.51.8 40.52.7 42.42.7 32.92.6 42.42.7 42.72.7 43.32.7 41.22.7 50.22.8 48.82.8 52.42.8 48.82.8 Table 3: Success and API-hit rates for CLI and IDE coding assistants, under the setting where the problem statement is given (Prob) and where it is not (No-prob), in which case we evaluate scenario akin to tab codecompletion. The results show that including the problem statement improves success rate by double-digit margins for 4 out of 5 cases evaluated. Model Success Rate (%) API Hit Rate (%) Precision (%) Recall (%) MRR Open-Weights Models Deepseek V3 Llama 4 Maverick7 Qwen3 Jamba 1.6 Large 48.92.8 45.12.7 41.82.7 41.82.7 Enterprise Models Claude 3.7 Sonnet Claude 4 Sonnet Gemini 2.5 Pro GPT-4.1 Grok3 Mistral Medium 3 Devstral Small Nova Pro 56.12.7 59.42.8 56.72.7 58.52.7 54.32.7 52.42.7 43.32.7 44.22.7 48.52.8 50.52.8 39.62.7 47.12.8 53.02.8 55.82.8 51.12.8 51.82.8 55.22.8 51.22.8 45.12.8 42.42.7 41.62.2 41.22.2 36.32.0 41.92.2 50.42.8 49.82.8 46.92.8 50.72.8 0.620.03 0.610.03 0.560.03 0.620. 41.92.2 41.92.2 41.92.2 41.22.2 41.62.2 41.62.2 41.62.2 40.72.2 50.72.8 50.72.8 50.72.8 50.12.8 50.42.8 50.42.8 50.42.8 49.62.8 0.620.03 0.620.03 0.620.03 0.610.03 0.620.03 0.620.03 0.620.03 0.600.03 Table 4: RAG performance for subset of models when retrieving = 3 most relevant documents. The best success rate and API hit rate results for each model group are in bold. An extended version of the RAG experiment results is presented in Appendix C. performing model achieves success rate of 58.5%, up from 48.5% with greedy decoding. These results demonstrate that the benchmark is still challenging even with access to the library documentation, with over 40% of the problems remaining unsolved in the best case. 3. In-Depth Analysis of Findings This section provides detailed analysis of the experimental results, focusing on model performance across several key dimensions. These dimensions include the impact of different API change types, comparison between success rate and API hit rate, and the effectiveness of self-debugging across 7 Comparison of Success Rate and API Hit Rate API hit rate shows moderate positive Pearson correlation with hidden-test success under Greedy Decoding with the Pearson correlation coefficient (r = 0.392, = 0.097, = 19), indicating that models which invoke the ground truth APIs more often tend to perform better on hidden tests in the Greedy setting, but falls just short of statistical significance at 5% level. Under Zero-Shot CoT, the correlation remains similar in magnitude (r = 0.483) and is statistically significant (p = 0.036, = 19). In the Self-Debug regime, however, the association becomes both stronger and highly significant (r = 0.615, = 0.011, = 16), demonstrating that when models can iteratively refine their outputs, invoking ground truth APIs becomes an especially reliable predictor of hidden-test performance. Analysis of Performance by Type of API Change Figure 7 illustrates the performance of models across various API change types within the GitChameleon benchmark, revealing notable variations in success rates. Semantic changes were the most tractable, with success rates ranging from 6080% with Self-Debug and 5565% without. New-feature additions proved to be the most challenging, with success rates between 2550% for Greedy Decoding and 5065% for Self-Debug. Notably, the Code Assistant Goose exhibited substantial discrepancy in its performance on semantic and function-name changes compared to argument changes and new features. This suggests heightened sensitivity to change category for Goose, characteristic not observed in the enterprise models or the Claude-powered tool-calling agent. Self-Debug Error Categorization Figure 8 shows that self-debugging consistently lowers the rate of every class of traceback error, both in absolute numbers and relative terms: (a) Raw Counts: We observe that for all error categoriesfrom the most common (AssertionError and TypeError) down to the rarest (RuntimeError)applying Self-Debugging significantly lowers the total number of failures. (b) Percentage Reduction: When norDecoding baseline, malized by the Greedy span roughly 50% up to about reductions 90%. The biggest relative improvements appear in the infrequent categoriessuch as RuntimeError and SyntaxErrorwhile the comFigure 7: Success Rate Breakdown by Type of Change: We analyze success rates with and without self-debugging, grouped by the type of change. Light shaded bars represent values obtained from self-debugging. Standard error is drawn as black line. We include DDG-SB, Multi-Step Agent variant where DuckDuckGo is used for grounding and access to sandbox is enabled. and the Coding Assistant Goose. Self-Debug results for these are omitted. gration, this benchmark uses 321 real-world instances, evaluating both individual code transformations and the functional correctness of entire migrated segments via unit tests (Islam et al., 2023). PyMigBench revealed that LLMs often handle individual changes well but struggle with achieving full functional correctness, especially for complex argument transformations. VersiCode (Wu et al., 2024) and the dataset by Wang et al. (Wang et al., 2024b) address library evolution but primarily depend on string matching for evaluation. CodeUpdateArena (Liu et al., 2025) investigates model adaptation to synthetically generated API updates for functions in popular libraries. GitChameleon distinguishes itself by focusing on the real-world scenario where developers are often constrained to specific library versions due to technical debt. Unlike CodeUpdateArenas synthetic changes, GitChameleon evaluates LLMs on their ability to generate code for actual, documented historical breaking changes within library versions they were likely exposed to during training. Furthermore, diverging from the string-matching evaluations of VersiCode and Wang et al. (Wang et al., 2024b), GitChameleon is based on executable tests. This provides more practical and rigorous assessment of functional accuracy in version-specific code generation. For an extended discussion of how GitChameleon is differentiated from existing work, please see Appendix D.2. Figure 8: Total error count for each category under Greedy decoding versus Self-Debug. Self-Debug yields substantial decreases all types of errors. mon AssertionError and TypeError still see decrease in the range of 60-70%."
        },
        {
            "title": "4 Related Work",
            "content": "The continuous evolution of software libraries presents significant challenges for AI-driven code generation. This section reviews existing benchmarks designed to evaluate model performance in this context. Specialized frameworks developed to address the challenge are presented in appendix D.2 The challenge of evaluating large language models (LLMs) in the context of evolving software libraries and their versions has been approached by several benchmarks. These benchmarks, while valuable, often differ in scope, methodology, or evaluation techniques compared to GitChameleon. PyMigBench Focusing on Python library mi-"
        },
        {
            "title": "5 Conclusion",
            "content": "The rapid evolution of software libraries presents critical challenge for LLM-powered AI systems in generating functionally correct, versionconditioned code. To address this, we introduce GitChameleon, novel Python-based benchmark meticulously curated with version-conditioned problems and executable tests. Our extensive evaluation reveals that state-of-the-art LLMs, agents and code assistants currently struggle significantly with this task, achieving modest success rates. facilitating By shedding light on current execution-based limitations and evaluation, GitChameleon aims to foster the development of more robust and adaptable code generation models for evolving software environments."
        },
        {
            "title": "Acknowledgements",
            "content": "The authors thank the International Max Planck Research School for Intelligent Systems (IMPRSIS) for supporting Diganta Misra. This work was partially enabled by compute resources provided by Mila8 and was funded by the Max Planck & Amazon Science Hub."
        },
        {
            "title": "Limitations",
            "content": "While we aim to provide comprehensive and holistic evaluation of LLMs on the task of versionconditioned generation, our benchmark is currently limited to Python and small set of libraries. Moreover, we focus solely on code generation from natural language instructions, and do not evaluate version-to-version translationi.e., converting code from one library version to anothereven when both versions are in-distribution relative to the models training. For instance, if model has been trained on PyTorch versions 1.7, 1.8, and 1.9, it would be valuable to assess whether it performs better when given solution in 1.8 and asked to upgrade to 1.9 or downgrade to 1.7. Finally, we do not include human evaluations, which could provide baseline for estimating average human performance on this task. 8https://mila.quebec"
        },
        {
            "title": "References",
            "content": "Abubakar Abid, Ali Abdalla, Ali Abid, Dawood Khan, Abdulrahman Alfozan, and James Zou. 2019. Gradio: Hassle-free sharing and testing of ML models in the wild. Preprint, arXiv:1906.02569. Meta AI. 2025. Everything we announced at our firstever LlamaCon. https://ai.meta.com/blog/lla macon-llama-news/. Discusses Llama 3.3 Instruct Turbo and Llama 4 Maverick. Mohannad Alhanahnah, Yazan Boshmaf, and Benoit Baudry. 2024. DepsRAG: Towards managing software dependencies using large language models. arXiv preprint arXiv:2405.20455v2. Anthropic. 2025. Claude 3.7 Sonnet and Claude Code. https://www.anthropic.com/news/claude-3-7 -sonnet. Arcee. Model Selection Arcee AI Documentation docs.arcee.ai. https://docs.arcee.ai/arcee-c onductor/arcee-small-language-models/mode l-selection#caller-large-tool-use-and-fun ction-call. [Accessed 15-07-2025]. Farnaz Behrang, Zhizhou Zhang, Georgian-Vlad Saioc, Peng Liu, and Milind Chabbi. 2025. Dr.fix: Automatically fixing data races at industry scale. Preprint, arXiv:2504.15637. Lars Buitinck, Gilles Louppe, Mathieu Blondel, Fabian Pedregosa, Andreas Mueller, Olivier Grisel, Vlad Niculae, Peter Prettenhofer, Alexandre Gramfort, Jaques Grobler, Robert Layton, Jake VanderPlas, Andreas Joly, Bertrand Druillette, Gael Varoquaux, and Marion Gramfort. 2013. API design for machine learning software: experiences from the scikit-learn project. arXiv preprint arXiv:1309.0238. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, and 39 others. 2021. Evaluating large language models trained on code. ArXiv. Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. 2023. Teaching large language models to self-debug. Preprint, arXiv:2304.05128. Keyuan Cheng, Xudong Shen, Yihao Yang, Tengyue Wang, Yang Cao, Muhammad Asif Ali, Hanbin Wang, Lijie Hu, and Di Wang. 2025. Codemenv: Benchmarking large language models on code migration. Preprint, arXiv:2506.00894. Matteo Ciniselli, Alberto Martin-Lopez, and Gabriele Bavota. 2024. On the generalizability of deep learning-based code completion across programming language versions. Preprint, arXiv:2403.15149. Google Cloud. 2025. Gemini 2.5 on Vertex AI: Pro, Flash & Model Optimizer Live. https://cloud. google.com/blog/products/ai-machine-lea rning/gemini-2-5-pro-flash-on-vertex-ai. Discusses Gemini 2.5 Pro and Gemini 2.5 Flash. Team Cohere, :, Aakanksha, Arash Ahmadian, Marwan Ahmed, Jay Alammar, Milad Alizadeh, Yazeed Alnumay, Sophia Althammer, Arkady Arkhangorodsky, Viraat Aryabumi, Dennis Aumiller, Raphaël Avalos, Zahara Aviv, Sammie Bae, Saurabh Baji, Alexandre Barbet, Max Bartolo, Björn Bebensee, and 211 others. 2025. Command a: An enterprise-ready large language model. Preprint, arXiv:2504.00698. Forbes Technology Council. 2024. Revolutionizing software development with large language models. https://www.forbes.com/councils/forbeste chcouncil/2024/03/20/revolutionizing-sof tware-development-with-large-language-mod els/. Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. 2022. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems (NeurIPS). DeepSeek-AI. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Preprint, arXiv:2501.12948. DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, and 181 others. 2025. Deepseek-v3 technical report. Preprint, arXiv:2412.19437. DuckDuckGo. 2025. DuckDuckGo: Privacy, simplified. https://duckduckgo.com/. Lishui Fan, Mouxiang Chen, and Zhongxin Liu. 2024. Self-explained keywords empower large language models for code generation. Preprint, arXiv:2410.15966. Google. 2025. Grounding with Google Search Gemini API. https://ai.google.dev/gemini-api/doc s/grounding. Aric Hagberg, Daniel Schult, and Pieter Swart. 2008. Exploring network structure, dynamics, and function using NetworkX. In Proceedings of the 7th Python in Science Conference, pages 1115. Charles Harris, Jarrod Millman, Stéfan van der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel Smith, Robert Kern, Matti Picus, Changqing Hoyer, Marten van Kerkwijk, Alex Brett, Andrew Wen, Pete Zhang, Joe Igoe, Keith Featherstone, and Travis Oliphant. 2020. Array programming with NumPy. Nature, 585(7825):357 362. Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt. 2021. Measuring coding challenge competence with apps. NeurIPS. J. D. Hunter. 2007. Matplotlib: 2D graphics environment. Computing in Science & Engineering, 9(3):9095. Amazon Artificial General Intelligence. 2024. The amazon nova family of models: Technical report and model card. Amazon Technical Reports. Mohayeminul Islam, Ajay Kumar Jha, Sarah Nadi, and Ildar Akhmetov. 2023. Pymigbench: benchmark for python library migration. In 2023 IEEE/ACM 20th International Conference on Mining Software Repositories (MSR), pages 511515. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. 2024. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. 2024. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations. Haolin Jin, Zechao Sun, and Huaming Chen. 2024. Rgd: Multi-llm based agent debugger via refinement and generation guidance. Preprint, arXiv:2410.01242. Kelsey Jordahl, Joris Van den Bossche, Martin Fleischmann, Jacob Wasserman, James McBride, Jeffrey Gerard, Jeff Tratner, Matthew Perry, Adrian Garcia Badaracco, Carson Farmer, Geir Arne Hjelle, Alan D. Snow, Micah Cochran, Sean Gillies, Lucas Culbertson, Matt Bartos, Nick Eubank, maxalbert, Aleksey Bilogur, and 11 others. 2020. geopandas/geopandas: v0.8.1. Kat Kampf. 2025. Create and edit images with Gemini 2.0 in preview. https://developers.googleblo g.com/en/generate-images-gemini-2-0-flash -preview/. Discusses Gemini 2.0 Flash. Paul Kassianik, Baturay Saglam, Alexander Chen, Blaine Nelson, Anu Vellore, Massimo Aufiero, Fraser Burch, Dhruv Kedia, Avi Zohary, Sajana Weerawardhena, Aman Priyanshu, Adam Swanda, Amy Chang, Hyrum Anderson, Kojin Oshiba, Omar Santos, Yaron Singer, and Amin Karbasi. 2025. Llama3.1-FoundationAI-SecurityLLM-Base-8B Technical Report. arXiv preprint arXiv:2504.21039. Cited for Llama 3.1 Instruct Turbo. Sachit Kuhar, Wasi Uddin Ahmad, Zijian Wang, Nihal Jain, Haifeng Qian, Baishakhi Ray, Murali Krishna Ramanathan, Xiaofei Ma, and Anoop Deoras. 2024. Libevolutioneval: benchmark and study for version-specific code generation. Preprint, arXiv:2412.04478. Stefano Lambiase, Gemma Catolino, Fabio Palomba, Filomena Ferrucci, and Daniel Russo. 2025. Exploring individual factors in the adoption of llms for specific software engineering tasks. Preprint, arXiv:2504.02553. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented Generation for KnowledgeIntensive NLP Tasks. In Advances in Neural Information Processing Systems, volume 33. James Li. 2024. ReAct vs Plan-and-Execute: Practical Comparison of LLM Agent Patterns. https: //dev.to/jamesli. Linxi Liang, Jing Gong, Mingwei Liu, Chong Wang, Guangsheng Ou, Yanlin Wang, Xin Peng, and Zibin Zheng. 2025. Rustevo: An evolving benchmark for api evolution in llm-based rust code generation. Preprint, arXiv:2503.16922. Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Itay Dalmedigos, Erez Safahi, Jhonathan Osin, Shaked Meirom, Yonatan Belinkov, Shai ShalevShwartz, Omri Abend, Raz Alon, Tomer Asida, Amir Bergman, Roman Glozman, Michael Gokhman, Avashalom Manevich, Nir Ratner, Noam Rozen, and 3 others. 2024. Jamba: hybrid transformer-mamba language model. Preprint, arXiv:2403.19887. Yue Liu, Chakkrit Tantithamthavorn, Yonghui Liu, Patanamon Thongtanunam, and Li Li. 2024. Automatically recommend code updates: Are we there yet? Preprint, arXiv:2209.07048. Zeyu Leo Liu, Shrey Pandit, Xi Ye, Eunsol Choi, and Greg Durrett. 2025. Codeupdatearena: Benchmarking knowledge editing on API updates. Edward Loper and Steven Bird. 2002. NLTK: The natural language toolkit. In Proceedings of the ACL-02 Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics, pages 6370, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics. Stephan Lukasczyk and Gordon Fraser. 2022. Pynguin: Automated unit test generation for python. In Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: Companion Proceedings, pages 168172. Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023. WizardCoder: Empowering code large language models with EvolInstruct. 11 Wes McKinney. 2010. Data Structures for Statistical In Proceedings of the 9th Computing in Python. Python in Science Conference, pages 5156. Mistral AI. 2025. Medium is the new large: Introducing mistral medium 3. https://mistral.ai/news/mi stral-medium-3. Accessed: 2025-05-17. OpenAI. 2024a. GPT-4o System Card. arXiv preprint arXiv:2410.21276. Cited for GPT-4o. OpenAI. 2024b. OpenAI o1 System Card. https: //openai.com/index/openai-o1-system-car d/. Discusses the o1 model series, including o1 and mentioning o3-mini. OpenAI. 2025a. Introducing GPT-4.1 in the API. ht tps://openai.com/index/gpt-4-1/. Discusses GPT-4.1, GPT-4.1 mini, and GPT-4.1 nano. OpenAI. 2025b. Introducing GPT-4.5. https://open ai.com/index/introducing-gpt-4-5/. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, and 2 others. 2019. PyTorch: An imperative style, high-performance deep learning library. Preprint, arXiv:1912.01703. Perplexity AI. 2024. Getting started with Perplexity. https://www.perplexity.ai/hub/blog/getti ng-started-with-perplexity. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, and 25 others. 2025. Qwen2.5 technical report. Preprint, arXiv:2412.15115. Rafiqul Rabin, Jesse Hostetler, Sean McGregor, Brett Sandboxeval: ToWeir, and Nick Judd. 2025. wards securing test environment for untrusted code. Preprint, arXiv:2504.00018. Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. 2023. Roformer: Enhanced transformer with rotary position embedding. Preprint, arXiv:2104.09864. Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, Soroosh Mariooryad, Yifan Ding, Xinyang Geng, Fred Alcober, Roy Frostig, Mark Omernick, Lexi Walker, Cosmin Paduraru, Christina Sorokin, and 1118 others. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. Preprint, arXiv:2403.05530. The pandas development dev/pandas: Pandas. team. 2020. pandasPauli Virtanen, Ralf Gommers, Travis Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, and 1 others. 2020. Scipy 1.0: fundamental algorithms for scientific computing in python. Nature methods, 17(3):261272. Chaozheng Wang, Shuzheng Gao, Cuiyun Gao, Wenxuan Wang, Chun Yong Chong, Shan Gao, and Michael R. Lyu. 2024a. systematic evaluation of large code models in api suggestion: When, which, and how. Preprint, arXiv:2409.13178. Chong Wang, Kaifeng Huang, Jian Zhang, Yebo Feng, Lyuye Zhang, Yang Liu, and Xin Peng. 2024b. How and Why LLMs Use Deprecated APIs in Code Completion? an Empirical Study. arXiv preprint arXiv:2312.14617. Chong Wang, Kaifeng Huang, Jian Zhang, Yebo Feng, Lyuye Zhang, Yang Liu, and Xin Peng. 2025a. LLMs Meet Library Evolution: Evaluating Deprecated API Usage in LLM-based Code Completion . In 2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE), pages 781781, Los Alamitos, CA, USA. IEEE Computer Society. Chong Wang, Kaifeng Huang, Jian Zhang, Yebo Feng, Lyuye Zhang, Yang Liu, and Xin Peng. 2025b. Llms meet library evolution: Evaluating deprecated api usage in llm-based code completion. Preprint, arXiv:2406.09834. Reka. RekaAI/reka-flash-3 Hugging Face huggingface.co. https://huggingface.co/RekaAI/reka -flash-3. [Accessed 15-07-2025]. Xingyao Wang. 2025. Introducing openhands lm 32b strong, open coding agent model. All Hands AI Blog. Aymeric Roucher, Albert Villanova del Moral, Thomas Wolf, Leandro von Werra, and Erik Kaunismäki. 2025. smolagents: smol library to build great agentic systems. https://github.com/huggingfa ce/smolagents. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-thought prompting elicits reasoning in large language models. Preprint, arXiv:2201.11903. Aditi Singh, Abul Ehtesham, Saket Kumar, and Tala Talaei Khoei. 2025. Agentic retrieval-augmented generation: survey on agentic rag. Preprint, arXiv:2501.09136. Tongtong Wu, Weigang Wu, Xingyu Wang, Kang Xu, Suyu Ma, Bo Jiang, Ping Yang, Zhenchang Xing, Yuan-Fang Li, and Gholamreza Haffari. 2024. VersiCode: Towards version-controllable code generation. 12 xAI. 2025. Grok-3. Official xAI announcement. Accessed May 17, 2025. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, and 41 others. 2025. Qwen3 technical report. Preprint, arXiv:2505.09388. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations. Sixiang Ye, Zeyu Sun, Guoqing Wang, Liwei Guo, Qingyuan Liang, Zheng Li, and Yong Liu. Prompt alchemy: Automatic prompt re2025. finement for enhancing code generation. Preprint, arXiv:2503.11085. Shuyan Zhou, Uri Alon, Frank Xu, Zhiruo Wang, Zhengbao Jiang, and Graham Neubig. 2022. DocPrompting: Generating code by retrieving the docs."
        },
        {
            "title": "A Benchmark Details",
            "content": "This appendix provides additional details on the GitChameleon benchmark. We provide details on the dataset construction process, the structure of the dataset samples, on the processes for validating the examples and constructing the hidden tests, and finally present additional statistics regarding the dataset. A.1 Dataset Construction Process The examples were created by the authors, which took roughly 350 human hours. To construct that dataset, we compiled list of popular Python libraries, focusing on those that had more than 1000 stars on Github as well as detailed documentation of changes between versions. For each library, we reviewed the change logs to identify breaking changes: deprecated functions, argument changes, alterations in behavior, and newly introduced functions. For each identified change, we wrote concise problem statement, starter code, expected solution and suite of tests, consisting of comprehensive suite of hidden tests to be used for model performance evaluation and ranking and manually written concise visible test to be used for selfdebugging experiments. We also added groundtruth set of relevant documents for RAG experiments. NOTE: Low-level changessuch as backend optimizations that do not alter the surface-level APIare not considered valid changes for our benchmark. For example, if between Torch 1.7 and Torch 1.8 the torch.nn.Softmax() function received CUDA-based numerical stability improvement, this does not modify the API usage of Softmax() and is therefore not labeled as change in our benchmark. Since most changes in mature libraries primarily impact backend functionality, collecting 328 valid samples required significant effort. A.2 Structure of Dataset Samples The main fields of each sample are given in Table 5. Additionally, each problem in GitChameleon is associated with metadata to assist in the analysis of the results, as described in Table 6. Each problem is classified with type of API evolution change among the categories defined in Table 7. Library Library Version Task Description Initial Code Extra Dependencies Hidden Tests Visible Test The software library under test. The exact version of that library. problem centered on particular library change. The Python snippet provided as starting point. Any additional packages required to solve the task. Comprehensive unit tests designed to maximize coverage. The success rate on these is the benchmark metric. concise test that validates the specific target behavior, intended to be used for SelfDebugging experiments. correct, ground-truth implementation. Reference Solution Reference Documents set of version-specific reference documents, to be used for RAG experiments. Table 5: GitChameleon dataset."
        },
        {
            "title": "Problem column definitions",
            "content": "for the Change Category Target Entity Solution Style The type of library-evolution changes, as defined in table 7. The specific function or class under test. Functional if only function body is expected, or Full for general code completion. Web Framework Task Yes if the problem exercises webdevelopment framework, otherwise No. Table 6: Metadata column definitions. A.3 Dataset Validation To ensure the validity of the dataset examples, we followed the following process: First, we created clean Docker container for each problem and installed the required dependencies into it. Then, we executed the visible and hidden validation tests to ensure that all are successful. A.4 Hidden Test Construction This section presents how we generated the hidden tests for each dataset example. These tests were generated by instructing the Zencoder AI Coding Agent 9 to create test files for each example, incorporating the appropriate dependency versions. The Zencoder agent, built on the GPT-4.1 base model, operated with internet search enabled and was granted execution access, allowing it to self-correct outputs that initially failed during runtime. Further errors encountered during verification were resolved by supplying error traces back to Zencoder or through an isolated instance of GPT4o, supplemented with manual intervention where necessary. This process enabled us to construct robust and comprehensive test suite, achieving coverage of 96.5%. The decision to use ZENCODER was motivated by limitations observed in 9https://zencoder.ai 14 Change Category Argument or Attribute change Function Name change Semantics or Function Behavior change New feature or additional dependency-based change Description The API call to function, method, or class has change in arguments (e.g. name, order, new, deprecated argument) between versions. The name of the API call has changed bepandas.append to tween versions (e.g. pandas.concat). The semantic / runtime behavior of the API call changed between versions (e.g. returning different type). feature was introduced in specific version; therefore, to execute the same functionality, model using an older version should make use of an additional dependency (e.g. torch.special was introduced in TORCH 1.10, previously one could use NUMPY for the same). Table 7: Categories of API Evolution Changes alternative unit test generation approaches. Rulebased generators such as Pynguin (Lukasczyk and Fraser, 2022) fail to account for version differences among samples that share the same or similar problem statements. Meanwhile, AI-based unit test generators like Claude Code and EarlyAI10 were not suitable: the former typically generated test classes where each sub-function was populated only with pass() statements, while the latter was restricted to functional-style problems and could not handle the more complex, class-based structures prevalent in GitChameleon. A.5 Additional Dataset Statistics Figure 9 presents the number of unique versions per library and the number of samples per library. Extra Methodologies: Reasoning,"
        },
        {
            "title": "Sampling and Prompting",
            "content": "This section presents results from additional experimental methodologies: Temperature Sampling: Results are shown in Table 9. We evaluate sampling at temperature = 0.8 across 10 seeds using both the OpenAI and Gemini model suites. The performance difference compared to greedy decoding is minimal. Reasoning Models: Performance results for the OpenAI o-series reasoning models are provided in Table 8. Self-Explained Keywords (SEK) Prompting: We evaluate the SEK prompting method proposed by Fan et al. (2024), applied to both OpenAI and Gemini models. SEK involves 10https://www.startearly.ai/ (a) Number of unique versions per library. (b) Number of samples per library. Figure 9: Dataset library statistics. (a) The count of distinct versions identified for each library, presented in decreasing order of uniqueness. (b) The total frequency of samples containing each library, ordered by their occurrence count. two-stage process: (1) Keyword Extraction, where the model generates relevant keywords for the coding task, and (2) Keyword Categorization, where keywords are ranked and classified into (a) Function, (b) General, and (c) Abstract categories. TF-IDF ranking is performed using 50,000-document subset of the EVOL-CODEALPACA-V1 corpus (Luo et al., 2023). As shown in our empirical analysis, SEK does not yield significant improvements over greedy sampling, and in several cases underperforms relative to it. NOTE: Temperature = 0 is used in both stages of SEK prompting."
        },
        {
            "title": "Analysis",
            "content": "This section contains the following additional experimental results: An experiment on Automatic Prompt Optimization of the system prompt for Greedy De15 Model Vanilla Decoding Vanilla with Self-Debug Zero-shot CoT Success Rate (%) Hidden Visible API Hit Rate (%) Success Rate (%) Hidden Visible API Hit Rate (%) Success Rate (%) Hidden API Hit Rate (%) o1 o3-mini o4-mini codex-mini 51.22.8 44.52.7 48.22.8 48.52.8 60.12.7 52.72.8 57.02.7 58.22.7 42.12.7 40.62.7 48.32.8 47.52.8 57.62.7 66.82.6 63.12.7 68.62.6 76.52.3 75.02.4 49.22.8 45.72.8 45.42.7 41.22.7 50.92.8 32.02.6 41.32.7 40.72.7 37.92.7 Table 8: Success rate on visible and hidden tests and API hit rate under the Vanilla, Self-Debug, and Zero-shot CoT settings, for the OpenAI o-series models. Model ranking on the benchmark is determined by Hidden Success Rate. Visible Success Rate figures are for context on Self-Debugging. The best result in each column is in bold. For full model details and citations, please refer to Appendix J. coding is described in Table 11. An experiment on static analysis based generated solutions fixing to ensure model failures are not attributed to confounding factors like indentation problems and unused imports or variable declarations. Refer to Table 13 for further details. Table 12 contains an extended set of RAG results, including both additional models and the setting where only single document is retrieved. We also present the following additional analyses: comparison of success rates between SelfDebug and Greedy Decoding, when broken down by version release year (Figure 10) and by library (Figure 11). comparison of success rates between RAG and Greedy Decoding by library is shown in Figure 12. Figure 13 analyzes the intra-model sample agreement rates in the Greedy Decoding, ZeroShot CoT and RAG settings."
        },
        {
            "title": "Model",
            "content": "o1 o3-mini GPT-4.1 GPT-4.1-mini GPT-4.1-nano GPT-4o GPT-4o-mini Gemini 1.5 Pro Gemini 2.5 Pro Gemini 2.0 Flash Gemini 2.5 Flash Hidden Success Rate (%) API Hit Rate (%) 50.50.8 46.41.6 48.91.4 45.91.3 33.81.1 47.21.2 40.21.2 45.41.2 41.03.4 43.43.1 46.40. 44.00.8 42.50.6 48.11.0 46.90.6 43.80.8 45.10.9 41.01.1 45.50.7 48.31.7 42.50.9 46.81.2 Table 9: Hidden Success Rate using temperature sampling (T = 0.8), averaged over 10 seeds. comparison to the greedy decoding baseline in Table 1 reveals that the changes in performance between greedy decoding and temperature sampling are mixed. For most models, the differences are small, but for few specific models, the changes are big and noteworthy. For the majority of models evaluated (8 out of 11), the performance change is minor, typically within +/- 2 percentage points. For example, Gemini-2.5-pro, shows notable decrease in success rate (-9.0 points). 16 Figure 10: Success Rate Breakdown by Version Release Year. Lighter and darker shaded bars represent values obtained with and without Self-Debugging, respectively. Standard error is drawn as black line. This plot shows that the release year does not significantly impact the results for most evaluated settings."
        },
        {
            "title": "Model",
            "content": "GPT-4o GPT-4o-mini GPT-4.1 GPT-4.1-mini GPT-4.1-nano GPT-4.5 Gemini 1.5 Pro Gemini 2.0 Flash Gemini 2.5 Pro Gemini 2.5 Flash Hidden Success Rate (%) API Hit Rate (%) 29.62.5 27.72.5 43.62.7 41.22.7 32.92.6 33.82.6 44.52.7 41.22.7 47.32.8 48.22. 43.62.7 40.32.7 49.42.8 44.02.7 43.82.7 58.02.7 45.72.8 43.42.7 50.02.8 43.42.7 Table 10: Success and API hit rates under the SEK setting. While SEK, being two-round prompting scheme, is expected to outperform greedy decoding, we observe that it does not yield significant improvements. For example, with GPT-4.1, the success rate actually drops by 4.9% when using SEK compared to greedy decoding."
        },
        {
            "title": "Model",
            "content": "Best Round Success Rate (%) (%) GPT-4.1-mini GPT-4.1-nano GPT-4.1 GPT-4o 1 3 1 0 42.12.7 37.52.7 50.02.8 49.12.8 2.1 +3.7 +1.5 0.0 Table 11: Automatic System Prompt Optimization results. The prompt was optimized for at most 5 rounds using the method described in (Ye et al., 2025), with early stopping if the improvement over previous round is less than 1.5%. We used GPT-4.1 as the mutation model and random fixed 20% subset of the dataset for the optimization process. For the initial prompt, we use the same system prompt that we had used for our Greedy Decoding experiments, as given in Figure 17. We report the delta of the hidden test success rate, in comparison to the Greedy Decoding baseline. The results demonstrate the limited utility of further optimizing the prompts we had used in our experiments."
        },
        {
            "title": "Model",
            "content": "k = 1 = 3 Success Rate (%) API Hit Rate (%) Success Rate (%) API Hit Rate (%) Precision (%) Recall (%)"
        },
        {
            "title": "MRR",
            "content": "Open-Weights Models CommandA CommandR 7B Deepseek R1 Reka Flash-3 Jamba 1.6 Mini OpenHands LM 32B v0.1 Llama 4 Scout"
        },
        {
            "title": "Enterprise Models",
            "content": "Arcee CoderL Claude 3.5 Haiku Claude 3.5 Sonnet Codestral CommandR+ Gemini 2.5 Flash GPT-4.1-mini GPT-4.1-nano GPT-4o-mini GPT-4o Inflection 3 Productivity LFM 40B MoE 43.62.7 23.22.3 50.92.8 8.51.5 18.02.1 34.82.6 38.72.7 46.32.8 43.62.7 8.51.5 44.22.7 32.02.6 54.32.8 46.92.8 38.12.7 41.52.8 48.22.8 24.72.8 30.82.7 43.92.7 36.32.7 44.82.7 34.52.6 35.42.6 41.02.7 45.12.7 47.32.8 47.92.8 18.62.1 47.32.8 43.02.7 50.52.8 50.02.8 45.12.7 45.42.7 47.02.7 42.02.6 38.32.7 48.22.8 23.22.3 51.22.8 11.61.8 29.32.5 28.92.5 39.32. 36.62.7 43.02.7 49.42.8 46.02.8 36.62.7 55.22.8 48.82.8 37.82.7 43.32.8 52.12.8 21.92.7 20.72.7 45.42.7 35.62.6 47.92.8 31.92.6 40.42.7 36.52.7 43.62.7 40.42.7 47.52.8 51.52.8 48.52.8 41.92.7 51.22.8 50.02.8 45.02.7 46.82.8 49.42.8 44.22.7 34.02.7 41.92.7 41.62.7 41.52.7 29.92.5 41.62.7 25.92.4 41.32.7 31.12.6 41.92.7 41.92.7 41.92.7 41.62.7 41.92.7 41.32.7 41.32.7 41.02.7 40.62.7 41.92.7 33.82.7 50.72.8 50.42.8 50.12.8 39.62.8 50.12.8 33.72.7 50.42. 41.02.8 50.72.8 50.72.8 50.72.8 50.42.8 50.72.8 50.42.8 50.42.8 50.12.8 49.52.8 50.72.8 44.82.8 0.630.03 0.620.03 0.620.03 0.470.03 0.620.03 0.420.03 0.620.03 0.490.03 0.620.03 0.620.03 0.620.03 0.620.03 0.620.03 0.620.03 0.620.03 0.620.03 0.610.03 0.620.03 0.530.03 Table 12: RAG performance of additional models when retrieving = 1 and = 3 most relevant documents. Precision is shown only for = 3 as it is equivalent to Recall in the = 1 case. This table shows that retrieving three documents is better in almost all cases than retrieving single document, despite the incurred false positives that arise due to most of the examples having less than three relevant documents. 18 Figure 11: Success Rate Breakdown by Library. This figure shows the differences in success rate between the libraries included in GitChameleon. All evaluated settings do very well on NumPy, which is to be expected given the popularity of the library and the subsequent abundance of code that uses it. The success rates on the web development frameworks are notably lower than on the scientific computing libraries, perhaps due to having more complex abstractions. 19 (a) GPT-4.1 (b) GPT-4.1-mini (c) GPT-4.1-nano Figure 12: Success Rate of RAG over Greedy Decoding, per library. The 10 most frequent libraries in GitChameleon are shown here. The plots demonstrate trend where smaller models are less effective at using RAG, with the full-size GPT-4.1 improving on 7 libraries, the mini version improving on 5 and the nano version improving only on 3. (a) Greedy Decoding (b) Zero-Shot Chain-Of-Thought (c) RAG (k=3) Figure 13: Intra model sample agreement rates. These plots show the rate of samples that have the same pass/fail result among all pairs of models, under the Greedy Decoding, Zero-Shot CoT and RAG settings. Each cell in these plots represents the agreement rate of pair of models, with the rate also being color-coded. The high agreement rates in all three subfigures show that ensembling different models would have limited effect on the success rates. 21 Assistant Model Linter Pylint Score Success Rate (%) Cline (IDE) GPT-4.1 Goose (CLI) GPT-4o Claude Code (CLI) Claude 3.7 Sonnet N/A Black + Isort Ruff N/A Black + Isort Ruff N/A Black + Isort Ruff 1.06 1.69 2. 0.53 1.82 2.92 0.00 1.92 2.60 54.62.8 54.62.8 54.62.8 36.32.7 36.32.7 36.32.7 48.82.8 48.82.8 48.82.8 13: and AutoStatic Analysis Table linting/Formatting. Pylint11 scores are averaged across code samples and are scored out of 10. The success rate numbers presented are the same as in Table 3 wherein Goose has no access to problem statement while Cline and Claude are provided with the same. We observe that the original generated solutions via coding assistants do not meet minimum quality standard requirements, however when improved via auto-linters like Black12, ISort13 and Ruff14, their code quality improves but with no impact to the success rate. This demonstrates that there are no confounding errors like indentation issues, unused imports and other formatting issues influencing our evaluation results observed. NOTE: For Ruff formatting, we used the already formatted/ linted solutions via Black and ISort."
        },
        {
            "title": "D Related Work",
            "content": "D.1 Code Evolution Datasets While the main text provides high-level overview of the most similar benchmarks, this section offers more detailed differentiation between GitChameleon and other relevant works. We categorize these benchmarks based on several key dimensions, including their evaluation method (execution-based vs. non-executable) and, most importantly, their core task format (instructionbased generation vs. completionor repairbased tasks). This distinction is critical as it tests different capabilities of language models. 11https://pylint.pycqa.org/en/latest/index.html 12https://black.readthedocs.io/en/stable/ 13https://pycqa.github.io/isort/ 14https://docs.astral.sh/ruff/ 22 D.1.1 Task Format: Instruction-Based"
        },
        {
            "title": "Generation",
            "content": "GitChameleon is fundamentally an instructionbased benchmark. For each problem, the model is given natural language \"Problem Statement\" and starter code. The core challenge is to comprehend the users intent and generate new, functionally correct solution that adheres to specific version constraints. This tests models ability to translate human requirements into code. D.1.2 Task Format: Code Update, Repair, and"
        },
        {
            "title": "Completion",
            "content": "In contrast, many other benchmarks focus on tasks where the primary input is existing code, not natural language instruction. The models goal is to modify, repair, or complete given code snippet. Code Update and Repair Benchmarks significant body of work evaluates models ability to modify or repair existing code. CodeUpdateEval (Liu et al., 2024) and JavaVersionGenBench (Ciniselli et al., 2024) are code modification benchmarks for Python and Java, respectively. They provide model with working piece of code and require it to be updated to newer library version. RustEvo2 (Liang et al., 2025) is code repair benchmark for Rust. It provides model with code that is broken due to dependency update and asks it to generate fix based on compiler errors. These tasks are distinct from GitChameleons, as they test reactive, corrective capability rather than the proactive generation of new code from specification. Completion-Based and Non-Executable Benchmarks Another category of benchmarks uses non-executable metrics or focuses on code completion. LibEvolutionEval (Kuhar et al., 2024) is non-executable benchmark structured as \"fill-in-the-middle\" completion-based task. Its evaluation is based on textual similarity metrics (e.g., F1 score), not the functional correctness of the code. LLM-Deprecated-APl (Wang et al., 2025b), which we note in our introduction, focuses on replacing deprecated APIs. This is specific Benchmark Language Evaluation Method Core Task Source of Changes GitChameleon Python Execution-Based CodeUpdateEval Python Execution-Based JavaVersionGenBench Java Execution-Based LLM-Deprecated-APl Python Non-Executable LibEvolutionEval Python Non-Executable RustEvo2 Rust Execution-Based CODEMENV Python Execution-Based Generation for static version: Writes new code for specific, often older, library version. Code Updating: Modifies existing code to work with newer library version. Code Updating: Modifies existing Java code to handle version updates. Deprecation Fixing: Identifies and replaces specific deprecated API calls. Real, documented historical breaking changes. Real-world software update commits. Real-world Java projects. curated list of deprecated APIs. Code Completion: Fills in missing part of code snippet based on context. API documentation and release notes. Differentiator Key GitChameleon from (Baseline for comparison) Focuses on migrating code forward to newer version, not generating for static one. Focuses on the Java ecosystem and its specific language/tooling challenges. Uses non-executable evaluation method and has narrow scope focused only on API deprecation. Is completion-based task that does not test functional correctness through execution. Code Repair: Fixes existing code that fails to compile after dependency update. Real breaking changes libraries from (\"crates\"). Rust Focuses on the Rust ecosystem and reactive, compiler-errordriven repair task. Environment Compatibility: Generates code that is compatible with complex environment specification. broad set of environment configurations. Has broader focus on overall environment compatibility, not specifically on historical breaking changes. Table 14: Detailed comparison of GitChameleon with related benchmarks across several key dimensions, highlighting differences in evaluation methodology, core task, and primary programming language. type of repair task that is evaluated using nonexecutable string matching. CODEMENV (Cheng et al., 2025) evaluates models ability to generate code compatible with complex environment specification. While execution-based, its task is primarily driven by satisfying technical constraints rather than implementing distinct, high-level natural language instruction. For detailed breakdown, Table 14 contrasts GitChameleon with these related benchmarks across several key methodological dimensions. D.2 Specialized Frameworks and Repair"
        },
        {
            "title": "Techniques",
            "content": "Recognizing the unique challenges of library evolution, researchers and practitioners are developing specialized frameworks and automated repair techniques that often combine LLMs with other methods. D.2.1 DepsRAG This framework utilizes multi-agent system built around RAG and Knowledge Graphs specifically for reasoning about software dependencies (Alhanahnah et al., 2024). It employs distinct agents managed by an LLM: one to construct and query the dependency KG, another for web searches, and critic agent to review and refine the generated responses, aiming for higher accuracy in complex dependency analysis tasks. D.2.2 Dr.Fix This tool represents family of approaches using LLMs, often combined with program analysis and RAG, for automated program repair. It focuses on fixing API misuse in LLM-generated code based on the taxonomy of misuse types. It employs detectreason-fix pipeline and demonstrates substantial improvements in repair accuracy metrics such as BLEU and Exact Match (Behrang et al., 2025). D.2.3 ReplaceAPI / InsertPrompt targeted techniques deThese are lightweight, signed specifically to mitigate the use of deprecated APIs in LLM-based code completion. ReplaceAPI performs direct, post-generation substitution of known deprecated API calls with their replacements, achieving high fix rates in evaluations (Wang et al., 2025b). InsertPrompt modifies the input prompt to discourage the generation of deprecated APIs in the first place. They serve as valuable baseline approaches for this specific problem (Wang et al., 2025a). 23 D.2.4 Conclusion These works indicate trend towards hybrid and agentic systems, moving beyond single LLM calls to more sophisticated architectures that integrate LLMs with other methods for handling library evolution. GitChameleon serves as an essential resource for evaluating such systems. In the subsequent sections we present qualitative sample analyses and model generation differences. Case Study: Code Assistant Failure"
        },
        {
            "title": "With Search",
            "content": "Here we present an example where coding assistant (Zencoder) fails at solving problem in our benchmark even when it can search the internet for the correct API call. The task required using the torch.special module or equivalent scipy.special to calculate the the modified Bessel function of the first kind of order 0. This can be computed using torch.special.i0 or scipy.special.i0 if the version pre-dates the introduction of torch.special. E."
        },
        {
            "title": "Inputs",
            "content": "Problem Statement: Calculate the modified Bessel function of the first kind, order 0 using pytorchs special functions if available in this version, otherwise you may use another library."
        },
        {
            "title": "Environment",
            "content": "Python Version: 3.7 Pytorch Version: 1."
        },
        {
            "title": "Starting Code",
            "content": "import torch def bessel _ i0 ( input _ tensor : torch . Tensor ) -> torch . Tensor : Listing 1: Initial Python function signature provided. E.2 Model Attempt and Failure The models attempt involved manually implementing the modified Bessel function, handling small and large inputs as separate cases."
        },
        {
            "title": "Assistant Trace",
            "content": "[ truncated ] ** Web search ** Done Let me check if the torch . special module with Bessel functions is specifically available in PyTorch 1.10.0. ** Web search ** Done The search results dont specifically confirm if torch . special . i0 is available in PyTorch 1.10.0. Let me search more specifically . ** Web search ** Done Since Im not getting clear information about the availability of torch . special . i0 in PyTorch 1.10.0 , Ill implement the function manually using the mathematical definition of the modified Bessel function of the first kind of order 0. Now , lets implement the solution : Listing 2: The assistants trace while designing the solution."
        },
        {
            "title": "Proposed Solution",
            "content": "def bessel _ i0 ( input _ tensor : torch . Tensor ) -> torch . Tensor : result = torch . zeros _ like ( input _ tensor ) abs _x = torch . abs ( input _ tensor ) # For small values ( < 15) , use the series expansion small _ mask = abs _x < 15.0 small _x = abs _x[ small _ mask ] if small _x. numel () > 0: small _ result = torch . ones _ like ( small _x) term = torch . ones _ like ( small _x) x_ squared _ over _4 = ( small _x / 2.0) ** # Sum the series until convergence or max iterations for in range (1 , 30) : # 30 terms should be enough for good precision term = term * _ squared _ over _4 / (k ** 2) small _ result = small _ result + term # Check for convergence if torch . all ( term < 1e -10 * small _ result ): break result [ small _ mask ] = small _ result # For large values ( $ geq$ 15) , use the asymptotic approximation large _ mask = small _ mask large _x = abs _x[ large _ mask ] if large _x. numel () > 0: large _ result = torch . exp ( large _x ) / torch . sqrt (2 * math . pi * 24 large _x) result [ large _ mask ] = large _ result return result } Listing 3: The solution proposed by the assistant. Error Traceback: TestBesselI0 . test _ large _ values _________ _______________ def test _ large _ values ( self ): \"\"\" Test bessel_i0 with large positive and negative values . \"\"\" input _ tensor = torch . tensor ([10.0 , -10.0 , 20.0 , -20.0] , dtype = torch . float32 ) result = bessel _ i0 ( input _ tensor ) expected = torch . from _ numpy ( scipy _ ( input _ tensor . numpy () )) > torch . testing . assert _ close ( result , expected ) AssertionError : Tensor - likes are not close ! Listing 4: Error trace for the implementation of bessel_i0. Explanation: The failure in the manual implementation arises because the asymptotic approximation I0(x) ex 2πx has only O(1/x) accuracy and omits higher-order correction terms, so for 20 its relative error exceeds the test tolerance of 105. To fix this, either use the built-in torch.special.i0(x) or include at least the first correction term in the expansion: ex 2πx 1 1 8x I0(x) (cid:16) (cid:17) , which reduces the large-x error below the required threshold. Case Study: Self-Debugging in Batched Matrix Exponential Computation This appendix details an instance where selfdebugging helped rectify an initially incorrect solution from GPT-4.5 for computing the matrix exponential of batched matrices. The task required leveraging the scipy.linalg.expm function while ensuring compatibility with batched inputs. F."
        },
        {
            "title": "Inputs",
            "content": "Problem Statement: Complete the function that computes the matrix exponential of batched matrices, non specified parameters should use the default value 25 Environment: Python Version: 3.10 SciPy Version: 1.8."
        },
        {
            "title": "Starting Code",
            "content": "from scipy import linalg import numpy as np def compute _ matrix _ exponential (A: np . ndarray ) -> np . ndarray : # Model needs to complete this function pass Listing 5: Initial Python function signature provided. F.2 First Model Attempt and Failure The models first attempt involved reshaping the input array and applying scipy.linalg.expm directly."
        },
        {
            "title": "Initial Model Output",
            "content": "from scipy import llinalg import numpy as np def compute _ matrix _ exponential (A: np . ndarray ) -> np . ndarray : return linalg . expm (A ) Listing 6: First attempt by the model compute_matrix_exponential. to define Models Explanation (Initial): The function directly returns linalg.expm(A) because SciPys expm computes the matrix exponential for given matrix. Given that is batch of matrices, SciPy should naturally handle this batch input and apply the exponential operation to each matrix independently, producing the correct batched result. Result: Failure Error"
        },
        {
            "title": "Traceback",
            "content": "Calling linalg.expm(A) on 3D array Rbnn makes SciPy treat it as one matrix, and raises Failure: ValueError: expected square matrix because it only supports 2D inputs. Traceback ( most recent call last ) : File \"/ app / repo / eval _ venvs / gcham _ venv _ 111/ lib / python3 . 10/ site - packages / scipy / sparse / linalg / _ matfuncs . py \", line 601 , in _ expm raise ValueError (' expected square matrix ') ValueError : expected square matrix Listing 7: Error traceback from the initial model output. F.3 Self-Debugging Process and Correction The self-debugging process recognized that simply calling linalg.expm was insufficient. Instead, linalg.expm must be applied individually to each 2D matrix within the batch. Additionally, crucial pre-condition is that each matrix within the batch must be square. Second Model Attempt (Post-Debugging) The model regenerated the solution, incorporating input validation and iterating over the individual matrices in the batch."
        },
        {
            "title": "Regenerated Model Output",
            "content": "from scipy import linalg import numpy as np def compute _ matrix _ exponential (A: np . ndarray ) -> np . ndarray : return np . stack ([ linalg . expm (A[ ]) for in range (A. shape [0]) if A[ ]. shape [0] == A[i ]. shape [1]] , axis =0) Listing 8: Corrected compute_matrix_exponential after self-debugging. Models Explanation (Regenerated): The corrected solution applies linalg.expm separately to each 2D square matrix in the batch using list comprehension, then re-stacks the results into 3D array. This ensures each call to expm receives valid input, resolving the error. Result: Success F.4 Analysis of the Correction The regenerated output effectively addressed the shortcomings of the initial attempt: Fix: Apply expm to each 2D slice Ai Rnn and re-stack, ensuring every call gets proper square matrix. This revised approach correctly computes the matrix exponential for each matrix in the batch and handles the data dimensions appropriately, thus resolving the ValueError and fulfilling the problem requirements."
        },
        {
            "title": "G Qualitative Analysis",
            "content": "This section presents additional qualitative studies of attempted solutions in the Greedy Decoding and the Chain-of-Thought settings. G.1 Greedy Decoding G.1.1 Example 1: (PyTorch) Problem Statement: Calculate the logarithm of the cumulative distribution function of the standard normal distribution using available functions. If not available in PyTorch, use another library. Environment: Python Version: 3.7 Torch Version: 1.9. Extra Dependencies: scipy==1.7.3, numpy==1.21."
        },
        {
            "title": "Starting Code",
            "content": "import torch def log _ ndtr ( input _ tensor : torch . Tensor ) -> torch . Tensor : pass Listing 9: Greedy Example 1 Starting Code"
        },
        {
            "title": "Solution",
            "content": "import numpy as np from scipy . stats import norm output = torch . from _ numpy ( norm . logcdf ( input _ tensor . numpy () )) return output Listing 10: Greedy Example 1 Solution Claude 3.7 Failure Output import torch import math from torch . distributions import Normal def log _ ndtr ( input _ tensor : torch . Tensor ) -> torch . Tensor : normal = Normal (0 , 1) return normal . cdf ( input _ tensor ). log () Listing 11: Claude 3.7 Output Gemini 2.5 Pro Failure Output import torch from torch . distributions import Normal def log _ ndtr ( input _ tensor : torch . Tensor ) -> torch . Tensor : loc = torch . tensor (0.0 , device = input _ tensor . device , dtype = input _ tensor . dtype ) scale = torch . tensor (1.0 , device = input _ tensor . device , dtype = input _ tensor . dtype ) standard _ normal = Normal ( loc = loc , scale = scale ) log _ cdf _ values = standard _ normal . log _ cdf ( input _ tensor ) return log _ cdf _ values Listing 12: Gemini 2.5 Pro Output"
        },
        {
            "title": "Analysis of Model Outputs",
            "content": "Both Claude 3.7 and Gemini 2.5 Pro incorrectly attempt to compute the log-CDF of the standard normal distribution using PyTorchs Normal distribution. Claude 3.7 computes normal.cdf(input_tensor).log(), which is numerically unstable for large negative inputs since the CDF approaches zero and its logarithm can underflow to , leading to invalid results. Gemini 2.5 Pro mistakenly calls non-existent log_cdf method on the PyTorch Normal object, causing an immediate AttributeError in PyTorch 1.9. This reflects confusion between PyTorch and other libraries (such as SciPy or TensorFlow Probability) that do provide this method. In summary, neither model provides robust or correct implementation; the only reliable approach in this environment is to use scipy.stats.norm.logcdf as shown in the reference solution, or to implement numerically stable approximation in PyTorch when SciPy is unavailable. G.1.2 Greedy Example 2 (SciPy) Problem Statement: complete the following function that check if all the batch of matrices are invertible, using numpy 1.25.1. Environment: Python Version: 3.10 Scipy Version: 1.11."
        },
        {
            "title": "Starting Code",
            "content": "import warnings from scipy . linalg import det import numpy as np warnings . filterwarnings ( ' error ') def check _ invertibility ( matrices : np . ndarray ) -> np . bool _: pass Listing 13: Greedy Example 3 Starting Code"
        },
        {
            "title": "Solution",
            "content": "return np . all ( det ( matrices )) Listing 14: Greedy Example 3 Solution Gemini 2.5 Correct Output import warnings from scipy . linalg import det import numpy as np warnings . filterwarnings ( ' error ') def check _ invertibility ( matrices : np . ndarray ) -> np . bool _: try : # Check if matrices is batch of matrices if matrices . ndim < 3: # If it 's single matrix , add batch dimension matrices = np . expand _ dims ( matrices , axis =0) # Check if all matrices are square batch _ size , rows , cols = matrices . shape if rows != cols : return np . bool _( False ) # Calculate determinants for all matrices in the batch determinants = np . array ([ det ( matrix ) for matrix in matrices ]) # matrix is invertible if its determinant is not zero # Check if all determinants are non - zero return np . bool _( np . all ( determinants != 0) ) except Exception : # If any error occurs during computation , return False return np . bool _( False ) Listing 15: Gemini 2.5 Pro Output GPT 4.1 Failure Output import warnings from scipy . linalg import det import numpy as np warnings . filterwarnings ( ' error ') def check _ invertibility ( matrices : np . ndarray ) -> np . bool _: for matrix in matrices : try : = det ( matrix ) if np . isclose (d , 0) : return np . bool _( False ) except Warning : return np . bool _( False ) return np . bool _( True ) Listing 16: GPT 4.1 Output"
        },
        {
            "title": "Analysis of Model Outputs",
            "content": "Gemini 2.5 Pro produces robust solution that handles both single matrices and batches, verifies that matrices are square, computes the determinant for each matrix, and correctly checks if all determinants are non-zero to determine invertibility, returning False on any exception to ensure safe failure. In contrast, GPT 4.1s output contains subtle but critical error in exception handling: the code attempts to catch Warning as an exception, but Python treats warnings and exceptions differently, 27 so this approach is ineffectiveespecially since warnings.filterwarnings(error) escalates warnings to exceptions that should be caught as Exception. As result, GPT 4.1s function may fail or produce unpredictable results in the presence of numerical issues or shape mismatches, despite using the correct logic for checking determinant values. G.1.3 Greedy Example 3 (SymPy) Problem Statement: Write custom jacobi symbols function that compute the Jacobi symbol. Environment: Python Version: 3.9 Sympy Version: 1."
        },
        {
            "title": "Starting Code",
            "content": "import sympy def custom _ jacobi _ symbols (a: int , n: int ) -> int : pass Listing 17: Greedy Example 4 Starting Code"
        },
        {
            "title": "Solution",
            "content": "return sympy . jacobi _ symbol (a , n) Listing 18: Greedy Example 4 Solution GPT 4.1 Correct Output import sympy def custom _ jacobi _ symbols (a: int , n: int ) -> int : return sympy . ntheory . jacobi _ symbol (a , n) Listing 19: GPT 4.1 Output O1 Failure Output import sympy def custom _ jacobi _ symbols (a: int , n: int ) -> int : return sympy . ntheory . residues . jacobi _ symbol (a , ) Listing 20: O1 Output"
        },
        {
            "title": "Analysis of Model Outputs",
            "content": "GPT-4.1 provides the correct solution by sympy.ntheory.jacobi_symbol, the proper and documented API in SymPy In contrast, O1 incorrectly uses calling which is for computing the Jacobi symbol 1.13. sympy.ntheory.residues.jacobi_symbol, 28 the function is nested in the assuming that residues submodule; however, in this version of SymPy, jacobi_symbol is not found there, resulting in an AttributeError on all tests. This error highlights lack of up-to-date API knowledge or reliance on outdated documentation. G.1.4 Greedy Example 4 (Flask) Problem Statement: Complete the app set-up so that, when given batch of matrix, the json encoding compute the determinants of each matrix, before flattening and converting the result to list, we are using scipy 1.11.1. Do not run the app in your code. Environment: Python Version: 3.10 Flask Version: 3.0.0 Extra Dependencoes: scipy==1.11."
        },
        {
            "title": "Starting Code",
            "content": "import flask import numpy as np from scipy import linalg app = flask . Flask ( ' test1 ') @app . route ( '/ data ') def data ( num _ list ): return flask . jsonify ({ ' numbers ': num _ list }) def eval _ app ( app , data _fn , num _ arr ): with app . test _ request _ context () : response = data _ fn ( num _ arr ) return response . get _ data ( as _ text = True ) class MyCustomJSONHandler ( flask . json . provider . DefaultJSONProvider ) : def default ( self , obj ): if isinstance ( obj , np . ndarray ) and len ( obj . shape ) ==3 and obj . shape [ -1]== obj . shape [ -2] : pass Listing 21: Greedy Example 5 Starting Code"
        },
        {
            "title": "Solution",
            "content": "res = linalg . det ( obj ) return res . tolist () return super () . default ( obj ) app . json _ provider _ class = MyCustomJSONHandler app . json = app . json _ provider _ class ( app ) Listing 22: Greedy Example 5 Solution Gemini 2.5 Pro Correct Output import flask import numpy as np from scipy import linalg app = flask . Flask ( ' test1 ') @app . route ( '/ data ') def data ( num _ list ): return flask . jsonify ({ ' numbers ': num _ list }) def eval _ app ( app , data _fn , num _ arr ): with app . test _ request _ context () : response = data _ fn ( num _ arr ) return response . get _ data ( as _ text = True ) class MyCustomJSONHandler ( flask . json . provider . DefaultJSONProvider ): def default ( self , obj ): if isinstance ( obj , np . ndarray ) and len ( obj . shape ) ==3 and obj . shape [ -1]== obj . shape [ -2] : # Calculate determinants for each matrix in the batch determinants = linalg . det ( obj ) # Convert the resulting 1D array of determinants to list return determinants . tolist () # Fallback to the default serialization for other types return super () . default ( obj ) # Apply the custom JSON provider to the app app . json = MyCustomJSONHandler ( app ) Listing 23: Gemini 2.5 Pro Output sosubclasses Listing 24: Claude 3.7 Output"
        },
        {
            "title": "Pro",
            "content": "provides 2.5 that correctly lution flask.json.provider.DefaultJSONProvider and overrides the default method to check for batches of square matrices, compute their determinants using scipy.linalg.det, and serialize the result as list, fully aligning with both the requirements and Flask 3.0.0s JSON provider API. In contrast, Claude 3.7s output incorrectly tries to customize JSON serialization by modifying app.json.encodera property that is not available in Flask 3.0.0and focuses on encoding sets rather than NumPy arrays, neglecting the determinant computation altogether. Consequently, Claudes approach fails with an AttributeError and does not satisfy the task requirements even if the encoder logic were correct, while Geminis implementation is robust and functional for the specified environment. G.2 Zero-Shot Chain-Of-Thought G.2.1 CoT Example 1 (Torch) Problem Statement: Calculate the modified Bessel function of the first kind, order 1 using PyTorchs special functions if available in this version, otherwise you may use another library. Environment: Claude 3.7 Failure Output import flask import json Python Version: 3.7 Torch Version: 1.9.0 app = flask . Flask ( ' test ') @app . route ( '/ data ') def data ( num _ set ): return flask . jsonify ({ ' numbers ': num _ set }) def eval ( app , data _fn , num _ set ): with app . test _ request _ context () : response = data _ fn ( num _ set ) return response . get _ data ( as _ text = False ) def app _ set _ up ( app : flask . Flask ) -> None : def set _ encoder ( obj ): if isinstance ( obj , set ): return sorted ( list ( obj )) raise TypeError (f\" Object of type { type ( obj )} is not JSON serializable \") Extra Dependencies: scipy==1.7.3, numpy==1.21."
        },
        {
            "title": "Starting Code",
            "content": "import torch def bessel _ i1 ( input _ tensor : torch . Tensor ) -> torch . Tensor : pass Listing 25: CoT Example 1 Starting Code"
        },
        {
            "title": "Solution",
            "content": "import numpy as np from scipy . special import i1 as scipy _ i1 output = torch . from _ numpy ( scipy _ i1 ( input _ tensor . numpy () )) return output app . json . encoder = lambda : json . JSONEncoder ( default = set _ encoder ) Listing 26: CoT Example 1 Solution 29 Claude 3.7 Correct Output import torch from scipy import special def bessel _ i1 ( input _ tensor : torch . Tensor ) -> torch . Tensor : # Convert PyTorch tensor to NumPy array input _ numpy = input _ tensor . cpu () . numpy () # Apply the modified Bessel function of the first kind , order 1 result _ numpy = special . i1 ( input _ numpy ) # Convert back to PyTorch tensor result _ tensor = torch . from _ numpy ( result _ numpy ). to ( input _ tensor . device ) # Ensure the output has the same dtype as the input result _ tensor = result _ tensor . to ( dtype = input _ tensor . dtype ) return result _ tensor Listing 27: Claude 3.7 Output Gemini 2.5 Pro Failure Output import torch def bessel _ i1 ( input _ tensor : torch . Tensor ) -> torch . Tensor : \"\"\" Calculates the modified Bessel function of the first kind , order 1. Args : Bessel function, converting the result back to PyTorch tensor and ensuring proper device and dtype handling; this approach is fully compatible with the given environment and passes all tests. In contrast, Gemini 2.5 Pro mistakenly assumes that torch.special.i1 exists in PyTorch 1.9.0, checks for its presence using hasattr, and raises NotImplementedError when unavailable; however, since this function is absent in the specified version, the code fails every test due to the raised exception. G.2.2 CoT Example 2 (Scikit-learn) Problem Statement: Adapt the use of manhattan distances to obtain pairwise distance matrix. Environment: Python Version: 3. Scikit-learn Version: 1.1 Extra Dependencies: numpy==1.23.5 Starting Code from sklearn . metrics . pairwise import manhattan _ distances import numpy as np def get _ pairwise _ dist (X: np . ndarray ,Y : np . ndarray ) -> np . ndarray : distances = manhattan _ distances (X , , sum _ over _ features = False ) pass Listing 29: CoT Example 2 Starting Code input_tensor : The input tensor ."
        },
        {
            "title": "Solution",
            "content": "Returns : tensor containing the element - wise modified Bessel function of the first kind , order 1. \"\"\" # Check if torch . special . i1 is available ( it is in torch 1.9.0) if hasattr ( torch . special , ' i1 '): return torch . special . i1 ( input _ tensor ) else : # Fallback or error handling if needed for other versions # For torch 1.9.0 , this else block should not be reached . raise NotImplementedError (\" torch . special . i1 is not available in this PyTorch version .\" ) Listing 28: Gemini 2.5 Pro Output"
        },
        {
            "title": "Analysis of Model Outputs",
            "content": "Claude 3.7 provides robust solution by correctly identifying that torch.special.i1 is not available in PyTorch 1.9.0 and instead leveraging scipy.special.i1 to compute the modified 30 return np . sum ( distances , axis =1) Listing 30: CoT Example 2 Solution GPT 4.1 Correct Output from sklearn . metrics . pairwise import manhattan _ distances import numpy as np def get _ pairwise _ dist (X: np . ndarray , Y: np . ndarray ) -> np . ndarray : distances = manhattan _ distances (X , ) return distances Listing 31: GPT 4,1 Output Claude 3.7 Failure Output from sklearn . metrics . pairwise import manhattan _ distances import numpy as np def get _ pairwise _ dist (X: np . ndarray ,Y : np . ndarray ) -> np . ndarray : distances = manhattan _ distances (X , , sum _ over _ features = False ) return np . sum ( distances , axis =2) Listing 32: Claude 3.7 Pro Output"
        },
        {
            "title": "Analysis of Model Outputs",
            "content": "def solution () -> None : In this correct implementation example, GPT 4.1 by Y) without provides calling manhattan_distances(X, the sum_over_features argument, which by default returns the desired pairwise distance matrix of shape (nsamplesX , nsamplesY ). In contrast, Claude 3.7 incorrectly sets sum_over_features=False, causing the output to be 3D array of shape, and then applies np.sum(distances, axis=2), which does not match the problems intent and produces incorrect results if further summed incorrectly. The misuse of the sum_over_features parameter and the erroneous axis for reduction demonstrates misunderstanding of the scikitlearn API and leads to failure, while GPT 4.1s output is concise and correct for all valid input shapes. G.2.3 CoT Example 3 (Falcon) Problem Statement: Create class named CustomRouter to manage your applications routes. The class should maintain an internal dictionary named routes for storing the mapping between URI templates and their associated resources. Implement an add_route method that accepts three URI template, resource, and arguments: additional keyword arguments. This method should use Falcons routing utilityspecifically, the map_http_methods function from the falcon.routing moduleto generate mapping of HTTP methods to resource handlers. If fallback parameter is provided in the keyword arguments, it should be passed to map_http_methods. The method should then store tuple consisting of the resource and the generated method mapping in the routes dictionary, using the URI template as the key, and finally return the generated method mapping. You must not modify the provided starting code, which includes the empty CustomRouter class. Instead, complete the solution by dynamically adding the required method in the solution() function. Environment: Python Version: 3.10 Falcon Version: 2."
        },
        {
            "title": "Starting Code",
            "content": "class CustomRouter : def __ init __( self ): self . routes = {} def add _ route ( Listing 33: CoT Example 3 Starting Code"
        },
        {
            "title": "Solution",
            "content": "self , uri _ template , resource , ** kwargs ): from falcon . routing import map _ http _ methods method _ map = map _ http _ methods ( resource , kwargs . get ( ' fallback ' , None )) self . routes [ uri _ template ] = ( resource , method _ map ) return method _ map CustomRouter . add _ route = add _ route Listing 34: CoT Example 3 Solution O1 Correct Output import falcon from falcon . routing import map _ http _ methods class CustomRouter : def __ init __( self ): self . routes = {} def solution () -> None : def add _ route ( self , uri _ template , resource , ** kwargs ): fallback = kwargs . pop ( ' fallback ' , None ) if fallback is not None : method _ map = map _ http _ methods ( resource , fallback = fallback ) else : method _ map = map _ http _ methods ( resource ) self . routes [ uri _ template ] = ( resource , method _ map ) return method _ map CustomRouter . add _ route = add _ route Listing 35: O1 Output GPT 4.1 Failure Output from falcon . routing import map _ http _ methods def solution () -> None : def add _ route ( self , uri _ template , resource , ** kwargs ): fallback = kwargs . pop ( ' fallback ' , None ) if fallback is not None : method _ map = map _ http _ methods ( resource , fallback = fallback ) else : method _ map = map _ http _ methods ( resource ) self . routes [ uri _ template ] = ( resource , method _ map ) return method _ map CustomRouter . add _ route = add _ route Listing 36: GPT 4.1 Output"
        },
        {
            "title": "Analysis of Model Outputs",
            "content": "In this case, the O1 output provides fully correct solution by dynamically attaching an add_route method to the CustomRouter class, properly handling the optional fallback argument, and updating the routes dictionary according to the problem statement. The method is inserted via the solution() function without modifying the provided class definition, and it uses Falcons map_http_methods utility to construct the method mapping as required. In contrast, the GPT 4.1 output omits the explicit definition of the CustomRouter class in its solution, violating the requirement to use the existing starting code. Although the logic within the solution() function is correct, the absence of CustomRouter definition in the completed module would lead to NameError or otherwise prevent the expected dynamic method attachment. The critical distinction is that O1 respects all constraints including not modifying the class definition directly, while GPT 4.1 provides an incomplete module, failing to meet the initialization requirements set by the problem. Logic vs. Knowledge Retention of our goal proposed benchmark, The GitChameleon, is to evaluate models ability to retain version-specific knowledgespecifically, whether it can recall the functionalities associated with particular library versions it has been trained on. Notably, this capability is distinct from the ability to generate logically correct code. While we do not explicitly disentangle whether model failures on our evaluation suite stem from incorrect logic generation or incorrect API version usage, our benchmark is intentionally designed so that most problems primarily test knowledge retention rather than complex logic reasoning. For each problem in our dataset, we compute the number of logic-related nodes in the Abstract Syntax Tree (AST) of the ground-truth solution and present their distribution in Figure 14. As shown, most ground-truth solutions contain fewer than five logic-related AST nodes. This supports our claim that the benchmark is primarily designed to assess version-specific knowledge retention rather than complex logic-based code generation. Table 15: Criteria for classifying AST nodes as logicrelated. Condition Classification Calling user-defined function Calling built-in Python operators (e.g., +) Calling math or utility function with nonobvious purpose Calling torch.from_numpy) Composing multiple calls together method library (e.g., The criteria for classifying AST nodes as logicrelated are provided in Table 15, and we include visualizations of the ASTs for two example groundtruth solutions for further illustration in Figures 15 and 16 respectively. 1. Sample ID: 0, Logic Nodes: 3 import torch def log _ ndtr ( input _ tensor : torch . Tensor ) -> torch . Tensor : import numpy as np from scipy . stats import norm output = torch . from _ numpy ( norm . logcdf ( input _ tensor . numpy () ) ) return output Listing 37: Sample 0 Ground Truth Solution 2. Sample ID: 329, Logic Nodes: import matplotlib . pyplot as plt def use _ seaborn () -> None : plt . style . use (\" seaborn \") Listing 38: Sample 329 Ground Truth Solution Figure 14: Logic Nodes Distribution over samples ground truth solutions ASTs. Most ground truth solutions have less than five logic nodes. 32 Figure 15: AST visualization for the ground-truth solution of Sample ID 0. The three color-coded call nodes (in grey and green) represent the logic-related components, classified under the composing multiple calls together category. The corresponding ground-truth code is shown in Code block 37 for reference. Figure 16: AST visualization for the ground-truth solution of Sample ID 329. No logic nodes are present, as the only call node corresponds to the calling library method category. The ground-truth solution is provided for reference in Code block 38."
        },
        {
            "title": "I Prompt Templates",
            "content": "This appendix contains all the prompts we had used for our experiments: The prompts for greedy sampling are given in Figure 17. The prompts for self-debugging are given in Figure 18. The prompt for the multi-step agent is given in Figure 19. spaCy 18 Django19 SciPy (Virtanen et al., 2020) Flask20 Jinja SymPy22 Seaborn23 The prompt for RAG is given in Figure 20. mitmproxy24 25 The prompt and file format for Coding Assispytest tants are given in Figure 21. The prompt for SEK is given in Figure 22 (for keywords generation) and Figure 23 (for code generation)."
        },
        {
            "title": "J Artifacts and Model Details",
            "content": "This appendix provides citations for various artifacts and models mentioned in the paper. J.1 Libraries This is the full GitChameleon. list of libraries included in PyTorch (Paszke et al., 2019) Geopandas (Jordahl et al., 2020) NLTK (Loper and Bird, 2002) NetworkX (Hagberg et al., 2008) GeoPy15 Gradio (Abid et al., 2019) Scikit-Learn (Buitinck et al., 2013) Matplotlib (Hunter, 2007) PyCaret16 Pandas (The pandas development team, 2020; McKinney, 2010) NumPy (Harris et al., 2020) LightGBM17 15https://pypi.org/project/geopy/ 16https://pycaret.org/ 17https://lightgbm.readthedocs.io/ Falcon web framework27 Tornado web server28 Plotly29 Librosa30 Pillow 31 tqdm Kymatio33 J.2 Models Open-Weights Models The following open-weights models were evaluated: Llama 3.1 Instruct Turbo: (Kassianik et al., 2025) Llama 3.3 Instruct Turbo 70B: (AI, 2025) Llama 4 Maverick 400B: (AI, 2025) 18https://spacy.io/ 19https://www.djangoproject.com/ 20https://flask.palletsprojects.com/ 21https://jinja.palletsprojects.com/ 22https://www.sympy.org/en/index.html 23https://seaborn.pydata.org/ 24https://mitmproxy.org/ 25https://mitmproxy.org/ 26https://pytest.org/ 27https://falconframework.org/ 28https://www.tornadoweb.org/ 29https://plotly.com/python/ 30https://librosa.org/doc/latest/index.html 31https://python-pillow.org/ 32https://github.com/tqdm/tqdm 33https://librosa.org/doc/latest/index.html 34 Figure 17: Prompts for Greedy Sampling (a) System Prompt for Zero-Shot Prompting (b) System Prompt for Chain-Of-Thought Prompting You are skilled Python programmer tasked with solving coding problem . Your goal is to provide You are skilled Python programmer tasked with solving coding problem . Your goal is to provide clear , efficient , and correct solution that meets all the specified requirements . clear , efficient , and correct solution that meets all the specified requirements . Please provide your solution following these guidelines : 1. Use the required library in your solution . First , let 's think step -by - step . Then , please provide your solution following these guidelines : 1. Use the required library in your 2. Incorporate the provided starter solution . code correctly . 3. Write your solution in Python . 4. Format your solution within markdown code block . 5. Ensure your code is clean , efficient , and well - commented . 6. Output only the code block and nothing else . Example output format : ``` python # [ Your code here , incorporating the starter code ] # [ Additional code and comments as needed ] ``` After writing your solution , please review it to ensure all requirements are met and the code is correct and efficient . Here are the key elements for this task : 2. Incorporate the provided starter code correctly . 3. Write your solution in Python . 4. Format your solution within markdown code block . 5. Ensure your code is clean , efficient , and well - commented . 6. Output nothing else after the code block . Example output format : [ Step -by - step thinking ] ``` python # [ Your code here , incorporating the starter code ] # [ Additional code and comments as needed ] ``` After writing your solution , please review it to ensure all requirements are met and the code is correct and efficient . Here are the key elements for this task : (c) User Prompt 1. Required Library : < library > {{ library }} </ library > 2. Python version : < python > {{ python_version }} </ python > 2. Coding Problem : < coding_problem > {{ coding_problem }} </ coding_problem > 3. Starter Code : < starter_code > {{ starter_code }} </ starter_code > 35 Qwen 2.5-VL Instruct 72B: (Qwen et al., codex-mini 2025) Qwen 3 235B:(Yang et al., 2025) Command 111B: (Cohere et al., 2025) DeepSeek R1 685B: (DeepSeek-AI, 2025) DeepSeek v3: (DeepSeek-AI et al., 2025) Openhands LM 32B v0.1: (Wang, 2025) Reka Flash-3: (Reka) Jamba 1.6 Mini, Large: (Lieber et al., 2024) Enterprise Models The following enterprise models were evaluated: Arcee CoderL: (Arcee) Claude 3.5 Haiku Claude 3.5 Sonnet35 Claude 3.7 Sonnet: (Anthropic, 2025) Claude 4 Sonnet36 CommandR+37 Gemini 1.5 Pro: (Team et al., 2024) Gemini 2.0 Flash: (Kampf, 2025) Gemini 2.5 Pro: (Cloud, 2025) Gemini 2.5 Flash: (Cloud, 2025) GPT-4.1: (OpenAI, 2025a) GPT-4.1-mini: (OpenAI, 2025a) GPT-4.1-nano: (OpenAI, 2025a) GPT-4o: (OpenAI, 2024a) GPT-4o-mini: (OpenAI, 2024a) GPT-4.5: (OpenAI, 2025b) o1: (OpenAI, 2024b) o3-mini: (OpenAI, 2024b) Grok 3: (xAI, 2025) Mistral Medium 3: (Mistral AI, 2025) Devstral Small39 Inflection 3 Productivity40 Liquid LFM 40B MoE41 Nova Pro:(Intelligence, 2024) J.3 Coding Assistants (CLI/IDE) The following coding assistants were studied as part of the experimentation pipeline: Claude Code42 (CLI) Goose43 (CLI) Cline44 (IDE-VSCode) RooCode45 (IDE-VSCode) KiloCode46 (IDE-VSCode) 38https://platform.openai.com/docs/models/code x-mini-latest 39https://mistral.ai/news/devstral 40https://openrouter.ai/inflection/inflectio n-3-productivity 41https://www.liquid.ai/blog/liquid-foundatio n-models-our-first-series-of-generative-ai-mod els 42https://docs.anthropic.com/en/docs/claude-c 34https://www.anthropic.com/claude/haiku 35https://www.anthropic.com/news/claude-3-5-sonnet 36https://www.anthropic.com/claude/sonnet 37https://cohere.com/blog/command-r-plus-micro soft-azure ode/overview 43https://block.github.io/goose/ 44https://cline.bot/ 45https://roocode.com/ 46https://kilocode.ai/ 36 Figure 18: Prompts for Self-Debugging (a) System Prompt (b) User Prompt < Problem > { problem } </ Problem > < Python Version > { python_version } </ Python Version > < Library > { library } </ Library > < Version > { version } </ Version > < Extra Dependencies > { additional_dependencies } </ Extra Dependencies > < Starting Code > { starting_code } </ Starting Code > < Generated Solution > { solution } </ Generated Solution > < Trace > { top_level_trace } </ Trace > You are an expert programming assistant . Your task is to fix issues in generated Python solution for given programming problem . You are provided with : - problem statement - Starter code - previously generated incorrect solution - top - level execution trace or error message - Dependencies information ( versions , libraries ). Please generate corrected Python solution by following these strict guidelines : 1. Use the required libraries explicitly in your code . 2. Correctly incorporate the provided starter code - do not remove or alter its structure . 3. Write in standard Python syntax . 4. Wrap your entire solution within single Markdown code block . 5. Do not include any text outside the code block - no explanations , comments , docstrings , or usage examples . 6. Ensure the code is clean , efficient , and syntactically valid . 7. Avoid interactive , stateful , or environment - dependent constructs (e. g., Django projects , web servers ). 8. Your output must be executable in non - interactive environment (e.g., test harness or script runner ). Example output format : ``` python # [ Your corrected code here ] ``` Before submitting , carefully review your code for correctness , completeness , and adherence to all constraints . 37 Figure 19: Tool-Calling Agent Prompt You are to solve coding problem in Python . # Instructions : * The coding problem requires using the library { library }=={ version }. Try using the problem with only this library and the standard Python libraries . * Do thorough research on the web about how to solve the coding problem for the given library version . Repeat multiple times if needed . * BEFORE FINISHING YOUR WORK , YOU MUST check your solution to the coding problem by running the ` docker_problem_sandbox ` tool . * Use the ` final_answer ` tool to return self - contained Python script that solves the problem . DO NOT INCLUDE ANY TEXT BESIDES FOR THE CODE IN THE FINAL ANSWER . * The solution needs to be in markdown code block . * The solution needs to start with the starter code provided below . # Coding Problem : { problem } # Starter Code : ``` python { starting_code } ``` Figure 20: RAG Prompt You are an AI assistant specialized in solving Python programming problems using information derived from documentation . Each query may specify particular libraries and version constraints . Your task is to generate correct , efficient , and minimal Python solution that adheres strictly to these requirements . Please follow these rules when crafting your response : 1. Use only the specified libraries and respect the given version constraints . 2. Incorporate any provided starter code as required . 3. Write only Python code - no in - line comments or usage examples . Do not provide anything in the response but the code . 4. Ensure the code is clean , minimal , and adheres to best practices . 5. The code must be executable in non - interactive environment (e .g . , avoid frameworks like Django or code requiring web server ). Context : { context } Based on the above , respond to the user query below . Query : { query } Here, {context} refers to the context of the top-k retrieved documents from the vectorized database for that query and {query} is the same as the User Prompt given in Figure 17(c). 38 Figure 21: Prompt and File Format for Coding Assistants (a) Prompt (b) Input File Format Solve each sample_ {i }. py in this folder then subsequently save your solutions as py files with the same name in separate subfolder called \"{ assistant name }\" that just completes the starting code provided in the sample and uses the instructions written in the # Complete using the following libraries and / or extra dependencies and their versions : # problem statement : { problem } # library : { library } # version : { version } # extra_dependencies : { extra_dependencies } comments at the start of each file . { starting_code } (a) presents the prompt template we had used for our Coding Assistant experiments. (b) shows the format of the example files referenced in the prompt. Figure 22: Prompts for SEK (Keyword Generation Stage) (a) System Prompt (b) User Prompt < Problem Statement > { problem } </ Problem Statement > < Starting Code > { starting_code } </ Starting Code > You are seasoned Python developer at Fortune 500 company who excels at analyzing complex code . Analyze the given code problem from the problem statement and starter code provided . Try to extract the keywords from the code problem . For each identified keyword : 1. Provide the keyword . 2. Give formalized explanation of the keyword using technical languages . Provided Format : Keywords :[ Keywords ] Explainations :[ Formalized explanations ] Guidelines : - Prioritize keywords that are crucial to understanding the input parameters , return content or supplementary information . - Use precise languages in explanations and provide formalized definitions where appropriate . - Ensure explanations are consistent with the behaviors expected based on the problem description . - Limit to the top 1 -3 important keywords to focus on core concepts . - You are supposed to output structured JSON output containing the extracted keywords and their corresponding formalized explanations in individual lists of strings . The keys for this JSON must be Keywords and Explainations . - Strictly adhere to the provided format , do not output anything else . Figure 23: Prompts for SEK (Code Generation Stage) (a) System Prompt (b) User Prompt You are skilled Python programmer tasked with solving coding problem . Your goal is to provide clear , efficient , and correct solution that < Python Version > { python_version } </ Python Version > < Library > { library } </ Library > < Version > { version } </ Version > < Extra Dependencies > { extra_dependencies } </ Extra Dependencies > < Problem Statement > { problem } </ Problem Statement > < Keywords > Analyze the following key terms and their relationships within the problem context : { General_Keywords } { Abstract_Keywords } </ Keywords > < Starting Code > { starting_code } </ Starting Code > meets all the specified requirements . Please provide your solution following these guidelines : 1. Use the required library in your solution . 2. Incorporate the provided starter code correctly . 3. Write your solution in Python . 4. Format your solution within markdown code block . 5. Ensure your code is clean and efficient . 6. Output only the code block and nothing else . Do not add any in - line comments , documentations , references or usage examples . 7. Make sure your code is executable in non - interactive environment . For example , do not write code which requires building Django project or deploying web - app . Example output format : ``` python # [ Your code here , incorporating the starter code ] ``` After writing your solution , please review it to ensure all requirements are met and the code is correct and efficient . Here are the key elements for this task :"
        }
    ],
    "affiliations": [
        "ELLIS Institute Tübingen",
        "Gologic",
        "Google",
        "MPI-IS Tübingen",
        "McGill University, Montréal",
        "Mila Quebec AI Institute",
        "Moderne",
        "Polytechnique Montréal",
        "ServiceNow Research",
        "Tübingen AI Center",
        "Université de Montréal"
    ]
}