{
    "paper_title": "AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via Reinforcement Learning and Reasoning",
    "authors": [
        "Bo Jiang",
        "Shaoyu Chen",
        "Qian Zhang",
        "Wenyu Liu",
        "Xinggang Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "OpenAI o1 and DeepSeek R1 achieve or even surpass human expert-level performance in complex domains like mathematics and science, with reinforcement learning (RL) and reasoning playing a crucial role. In autonomous driving, recent end-to-end models have greatly improved planning performance but still struggle with long-tailed problems due to limited common sense and reasoning abilities. Some studies integrate vision-language models (VLMs) into autonomous driving, but they typically rely on pre-trained models with simple supervised fine-tuning (SFT) on driving data, without further exploration of training strategies or optimizations specifically tailored for planning. In this paper, we propose AlphaDrive, a RL and reasoning framework for VLMs in autonomous driving. AlphaDrive introduces four GRPO-based RL rewards tailored for planning and employs a two-stage planning reasoning training strategy that combines SFT with RL. As a result, AlphaDrive significantly improves both planning performance and training efficiency compared to using only SFT or without reasoning. Moreover, we are also excited to discover that, following RL training, AlphaDrive exhibits some emergent multimodal planning capabilities, which is critical for improving driving safety and efficiency. To the best of our knowledge, AlphaDrive is the first to integrate GRPO-based RL with planning reasoning into autonomous driving. Code will be released to facilitate future research."
        },
        {
            "title": "Start",
            "content": "AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via Reinforcement Learning and Reasoning Bo Jiang1, Shaoyu Chen1,2 Qian Zhang2 Wenyu Liu1 Xinggang Wang1,(cid:66) 1 Huazhong University of Science and Technology 2 Horizon Robotics https://github.com/hustvl/AlphaDrive 5 2 0 2 0 1 ] . [ 1 8 0 6 7 0 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "OpenAI o1 and DeepSeek R1 achieve or even surpass human expert-level performance in complex domains like mathematics and science, with reinforcement learning (RL) and reasoning playing crucial role. In autonomous driving, recent end-to-end models have greatly improved planning performance but still struggle with long-tailed problems due to limited common sense and reasoning abilities. Some studies integrate vision-language models (VLMs) into autonomous driving, but they typically rely on pre-trained models with simple supervised fine-tuning (SFT) on driving data, without further exploration of training strategies or optimizations specifically tailored for planning. In this paper, we propose AlphaDrive, RL and reasoning framework for VLMs in autonomous driving. AlphaDrive introduces four GRPO-based RL rewards tailored for planning and employs two-stage planning reasoning training strategy that combines SFT with RL. As result, AlphaDrive significantly improves both planning performance and training efficiency compared to using only SFT or without reasoning. Moreover, we are also excited to discover that, following RL training, AlphaDrive exhibits some emergent multimodal planning capabilities, which is critical for improving driving safety and efficiency. To the best of our knowledge, AlphaDrive is the first to integrate GRPO-based RL with planning reasoning into autonomous driving. Code will be released to facilitate future research. 1. Introduction Autonomous driving has witnessed rapid advances in recent years, with end-to-end autonomous driving emerging as one of the most representative models [8, 16, 17, 22, 29]. They take sensor data as input and leverage learnable neural networks to plan the vehicles future trajectory. Benefiting from large-scale driving demonstrations, end-to-end models continuously improving their planning capabilities by Intern of Horizon Robotics. (cid:66) Corresponding author. Figure 1. Our planning-oriented RL and two-stage training strategy significantly boost planning accuracy. With just 20k samples, it outperforms SFT by 35.31%, showing strong performance even with limited data. As data increase, AlphaDrive consistently leads in planning performance. expanding training data and increasing model parameters. However, due to their black-box nature and lack of common sense, end-to-end models still face significant challenges when handling complex and long-tail driving scenarios. For instance, consider situation where the vehicle ahead is carrying traffic cones while driving. An end-to-end model may fail to comprehend the relationship between the leading vehicle and the traffic cones, mistakenly assuming that the road ahead is under construction and thus impassable, leading to an incorrect decision to brake. Therefore, relying solely on end-to-end models to achieve high-level autonomous driving remains challenging. With the success of GPT [6], large language models (LLMs) show remarkable comprehension and reasoning abilities [38, 48]. Furthermore, their capabilities have evolved from unimodal text understanding to multimodal vision-language processing. [3, 10, 24]. The commonsense and reasoning abilities of VLMs hold great potential to mit1 igate the limitations of end-to-end models. Recently, OpenAI o1 [25], which incorporates reasoning techniques, achieves performance comparable to or even surpassing that of human experts in fields such as programming. Additionally, DeepSeek R1 [14], which leverages reinforcement learning, not only demonstrates emergent abilitiesand achieves top-tier performance but also requires significantly lower training costs compared to other models. These advances underscore the immense potential of reasoning techniques and RL in the development of large models. Existing research on applying VLMs to autonomous driving can be broadly categorized into two directions. The first focuses on leveraging VLMs for the understanding of driving scenes [34, 49]. The second explores the use of VLMs for planning, where some studies treat VLMs as end-to-end systems that process driving images and other inputs to directly predict trajectories [7, 47]. However, unlike end-to-end models which are specifically designed for trajectory planning, VLMs operate in language space and are not inherently suited for precise numerical predictions [12, 15]. Consequently, directly employing VLMs for trajectory planning may result in suboptimal performance and even pose safety risks. Some studies leverage VLMs for high-level planning by formulating the ego vehicles future actions in natural language, such as slow down and turn right [18]. Although this approach circumvents the aforementioned drawbacks, existing works still lack further exploration of training methodologies. Most of them primarily rely on SFT, overlooking the impact of different training strategies on planning performance and the associated training costs. In this paper, we explore the following question: How can RL and reasoning which achieves remarkable success in general large models be applied to autonomous driving, particularly in planning, to enhance the performance of VLMs in autonomous driving while reducing training costs? Through preliminary experiments, we find that directly applying existing RL and reasoning techniques to planning results in suboptimal performance. We attribute this to three main factors. First, the reward design in RL for general tasks is not well-suited for planning. For example, in visual object counting, the reward can be simply determined based on whether the model predicts the correct answer. However, in autonomous driving, while high-level planning can be formulated as multi-class classification problem, the varying significance of different driving behaviors makes it inappropriate to assign equal weights to all actions. Second, unlike mathematical or counting, the solution of planning are usually not unique. For instance, on an open, straight road, one may choose to maintain constant speed or accelerate, both of which are valid decisions. Therefore, rigidly assessing whether the models planning output exactly matches the ground truth in the training data may not be the optimal approach. Finally, while domains such as mathematics have abundant reasoning data, including textbooks and solution manuals that can be easily utilized, autonomous driving lacks readily available datasets that capture the reasoning process. Collecting such data is highly costly and requires extensive manual annotation. As result, directly applying existing reasoning techniques to planning remains challenging. To address the aforementioned challenges, this paper introduces AlphaDrive, VLM-based reinforcement learning and reasoning framework specifically designed for autonomous driving planning. In particular, AlphaDrive employs RL strategy based on Group Relative Policy Optimization (GRPO) [33]. Compared to Proximal Policy Optimization (PPO) [32] and Direct Preference Optimization (DPO) [31], GRPO exhibits better training stability and performance. Furthermore, the group relative optimization strategy in GRPO is well-suited for planning, as planning often involves multiple valid solutions, making relative optimization across multiple solutions natural fit. Our experiments show that AlphaDrive exhibits some emergent multimodal planning capabilities, which we think can be attributed to the use of GRPO. AlphaDrive introduces four GRPO rewards tailored for planning. The first is the planning accuracy reward, which evaluates the consistency between the models planning actions and the ground truth actions. The second is the actionweighted reward, which assigns different weights to various actions based on their importance to safety. For instance, actions such as braking and steering are critical for safety, so weighting them accordingly helps the model achieve better performance in planning key actions. The third is the planning diversity reward, which encourages the model to generate multiple diverse solutions. This prevents mode collapse and enhances overall planning performance. The last one is the planning format reward, where we define specific output format and encourage the model to follow it. This ensures more structured outputs and contributes to more stable training. In addition to RL, we propose planning reasoning technique. Our approach employs two-stage training strategy based on knowledge distillation, integrating SFT and RL. In the first stage, we leverage large model, such as GPT-4o, to generate small yet high-quality dataset containing planning reasoning processes derived from real driving actions. This dataset is then used to fine-tune our model via SFT, effectively distilling knowledge from the large model. In the second stage, we further refine the model using RL. Introducing the SFT stage as warm-up step effectively mitigates hallucinations and instability commonly observed in the early stages of reinforcement learning, while also en2 hancing planning performance. Our contributions are summarized as follows: We propose AlphaDrive, VLM tailored for high-level planning in autonomous driving. To the best of our knowledge, AlphaDrive is the first to integrate GRPObased RL with planning reasoning to autonomous driving, significantly boosting both performance and training efficiency. AlphaDrive introduces four GRPO rewards for planning: planning accuracy reward, action-weighted reward, planning diversity reward, and planning format reward. These optimized rewards make GRPO more suitable for autonomous driving. We propose two-stage reasoning training strategy based on knowledge distillation, integrating SFT and RL. Our approach achieves better planning performance compared to training with RL alone or without reasoning. Experiments on large-scale driving dataset validate the superiority of AlphaDrive. Compared to the SFT-trained model, AlphaDrive significantly improves the planning accuracy by 25.52% and, with only 20% of the training data, outperforms the SFT-trained model by 35.31%. We are also excited to discover that, following RL training, AlphaDrive exhibits some emergent multimodal planning capabilities, which is promising for improving driving safety and efficiency. 2. Related Work Vision Language Models. Since the release of GPT [1, 6], the capabilities of large models have gradually expanded from single modality to multi-modalities. Large vision language models [1, 3, 24, 38] now demonstrate superior abilities in visual understanding and reasoning. Early works attempt to integrate visual models with large language models (LLMs), Flamingo [2] uses visual encoder to process visual signals and adds attention layers in the LLM decoder to interact with the visual features. BLIP [20, 21] introduces the Q-Former architecture and cross-modal contrastive learning tasks to bridge the vision encoder with LLMs. LLaVA [23, 24] propose using vanilla MLP as the connector between the visual encoder and LLMs, which achieves impressive visual understanding capabilities with relatively limited data. The QwenVL [3, 41] series continuously improve the visual module, offering better support for high-resolution and dynamic resolution images, while also demonstrate excellent performance in multilingual tasks and spatial perception. Reinforcement Learning and Reasoning. Autoregressive learning [39] is currently the mainstream pre-training strategy for LLMs. Besides, RL and reasoning techniques further enhance the capabilities of large models [26, 31 33, 43]. For instance, GPT [1] employs RL with Human Feedback (RLHF) [26], which incorporates human feedback into the training process. By integrating human intentions and behavioral preferences, RLHF enables LLMs to generate outputs that align more closely with human habits and preferences. Direct Preference Optimization (DPO) [31] enhances the models performance by directly optimizing preference feedback. Building on this, Group Relative Policy Optimization (GRPO) [33] introduces strategy of group relative optimization, which considers the relative superiority or inferiority between multiple output groups, further improving the stability and effectiveness of the training process. The recent DeepSeek R1 [14] experiences an Aha Momentduring training based on GRPO, where, without any explicit guidance, the model autonomously allocates more thinking to the problem and re-evaluates its initial approach. This highlights the potential of RL in enabling large models to evolve from mere imitation to emergent intelligence. In our experients, we are also excited to discover that, after GRPO-based RL training, AlphaDrive demonstrates some emergent multimodal planning capabilities, enabling it to generate multiple reasonable driving plans. We believe it has great potential to improve driving safety and efficiency. In terms of reasoning, Chain-of-thought [43] has demonstrated great performance in solving complex problems by breaking them down and reasoning step by step. OpenAI o1 [25], which is based on Chain-of-thought, introduces inference-time scaling. By increasing the computational cost during inference and combining search strategies such as Monte Carlo Tree Search (MCTS) [35] and Beam Search [46], significant improvements have been achieved in areas such as science and programming that require complex reasoning. This also shows that, beyond scaling model parameters and training data, scaling the inference-time computation is also promising direction for exploration. Autonomous Driving Planning. Planning is the ultimate task of autonomous driving. The earliest planning algorithms are rule-based [27, 36], which have significant limitations in terms of generalizability and efficiency. Recently, end-to-end models [8, 13, 16, 17, 22, 29] has gained popularity, where unified neural network is used to directly output planning trajectories or control signals from sensor data. By leveraging large-scale driving demonstrations, end-toend models are trained in data-driven manner, achieving impressive planning performance. However, since end-toend models are black-box models that lack common-sense and reasoning capabilities, they still struggle to address the long-tailed problems in autonomous driving. VLMs and Autonomous Driving. The common-sense and reasoning abilities of large models can effectively compensate for the limitations of end-to-end models in autonomous 3 Figure 2. Overall training framework of AlphaDrive. AlphaDrive is trained using GRPO-based RL, and we design four planning rewards to help the model understand and learn planning. Besides, we propose two-stage training paradigm, the first stage uses SFT to distill the planning reasoning process from large model and serves as warm-up, while the second stage employs RL to explore planning. driving. In the field of robotics, Vision-Language-Action (VLA) models [5, 19, 44] have made significant progress in understanding language instructions and executing complex actions. common approach is to use VLMs as the planning module to generate planning instructions, which are then translated into control signals through an action model. There have also been some works based on large models in the field of autonomous driving. DriveGPT4 [47] utilizes VLM that takes front-view videos as input, and the model directly predicts control signals. ELM [49] leverages largescale, cross-domain video training for VLMs, showing that using data from various domains can effectively enhance the performance of VLMs in driving-related tasks. OmniDrive [42] proposes the use of sparse 3D tokens to represent driving scenes, which are then input into VLMs for scene understanding and planning. In addition to the above works that directly apply VLMs to driving, DriveVLM [37] combines VLMs with end-toend models for the first time, where VLMs predict lowfrequency trajectories and an end-to-end model generates high-frequency trajectories. Senna [18] proposes framework where VLMs handle high-level planning, while endto-end models are responsible for low-level trajectory prediction. Additionally, several datasets and benchmarks have been proposed [30, 34, 34, 45], which promote the application of VLMs in autonomous driving. However, most of the current works on VLMs in the field of autonomous driving involves directly using pre-trained models and then utilizing SFT on driving data, which lacks in-depth exploration on training strategies specifically designed for planning. Further effort is needed to adapt the impressive RL and reasoning techniques from general tasks to autonomous driving. 3. AlphaDrive 3.1. Overview AlphaDrive is VLM designed for autonomous driving planning. Unlike previous approaches that rely solely on SFT, we explore the incorporation of RL and reasoning techniques to better align with the unique characteristics of driving planning: (1) the varying importance of different driving behaviors; (2) the existence of multiple feasible solutions; and (3) the scarcity of readily available reasoning data for planning decisions. We propose four GRPO-based RL rewards tailored for planning, along with two-stage planning-reasoning training strategy that integrates SFT with RL. Our experiments demonstrate that, compared to using SFT alone or training without reasoning, AlphaDrive achieves significant improvements in both planning performance and training efficiency. In the following sections, we will detail the design of each component. 3.2. Planning-oriented Reinforcement Learning 3.2.1. Reinforcement Learning Algorithm Current commonly used RL algorithms include PPO [32], DPO [31], and GRPO [33]. Given query q, GRPO samples group of outputs {o1, o2, , oG} from the old policy πθold and optimizes the new policy πθ by maximizing: (cid:34) JGRPO(θ) = Eq,{oi}πθold (cid:35) Li βDKL(πθπref ) ,"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 Li = min (wiAi, clip(wi, 1 ϵ, 1 + ϵ)Ai) , (1) (2) where wi = πθ(oiq) πθold (oiq) , ϵ and β are hyper-parameters, and 4 the advantage Ai is computed using the normalized reward within the group. We ultimately choose GRPO as the RL algorithm for AlphaDrive for two key reasons: (1) DeepSeek R1 [14] has demonstrated the effectiveness of GRPO in general domains. Compared to other algorithms, GRPO provides higher training stability and efficiency; (2) Moreover, the group relative optimization strategy introduced by GRPO is particularly well-suited for planning, as planning often involves multiple valid solutions, making relative optimization across multiple solutions is natural fit. Experimental results further confirm that models trained with GRPO exhibit strong planning capabilities. 3.2.2. Planning Reward Modeling Planning Accuracy Reward. In fields such as mathematics or programming, the reward in GRPO can be intuitively determined based on whether the final answer is correct. However, planning is more complex, as it involves both lateral (direction) and longitudinal (speed) components. Furthermore, the set of possible actions is constrained. As result, we use the F1-Score to evaluate the accuracy of both lateral and longitudinal decisions separately, and assign rewards accordingly. Initially, we evaluate accuracy by checking whether the models prediction exactly matches the ground truth. However, due to imperfect format in the models early training phase, such as discrepancies in case sensitivity or the presence of extraneous outputs, this approach results in poor stability during the early stages of training. We then attempt to extract all the words from the prediction and check whether the ground truth is included among the words. This introduces new issue where the model sometimes learns shortcut solutions, such as outputting all possible actions, which causes mode collapse. Ultimately, we adopt the F1-score for evaluation, as it not only prevents the model from learning shortcut solutions (where outputting all decisions could result in high recall but low accuracy) but also improves the stability during the early training phase. Action-Weighted Reward. As mentioned above, the importance of different behaviors in planning varies. For instance, decelerating and stopping are more critical for safety than maintaining speed. Therefore, we assign different importance weights to various actions, incorporating them as weighted components in the final reward. Planning Diversity Reward. Since planning is inherently multimodal, during GRPO-based RL training, the model generates multiple solutions for group relative optimization. In the later stages of training, we observe that the models outputs tend to converge to the same solution. Our goal is to encourage the model to generate variety of feasible solutions, rather than merely aligning with the ground truth Algorithm 1: Planning Reward Modeling. Input: Planning answers A, Ground Truth action Output: Planning Reward 1 Initialization: Planning Reward , Action Weights 2 Speed Action Set S, Path Action Set P, Answer Format 3 # Pytorch-like Code 4 ans counter = Counter() 5 for ans in do 6 action ans = re.search(rF , ans).group(1).strip() ans counter.update(action ans) speed ans = extract ans(action ans, S) path ans = extract ans(action ans, P) # Calculate Planning Accuracy Reward speed acc = cal f1 score(speed ans, e) path acc = cal f1 score(path ans, e) # Calculate Action-Weighted Reward speed weighted = W[speed ans] path weighted = W[path ans] # Calculate Planning Diversity Reward plan div = 0 if sum(ans counter.values()) != 0 then plan div = ans counter[action ans] / sum(ans counter.values()) end # Up to 20% reduction in diversity reward plan div = 1 - min(0.2, plan div R) # Calculate Planning Format Reward format = check format(ans, ) # Final Planning Quality Reward speed = speed acc * speed weighted * plan div path = path acc * path weighted * plan div R.append([speed R, path R, format R]) 7 8 10 11 12 13 14 16 17 18 19 20 22 23 24 25 26 28 29 end 30 Return: extrat ans will extract substrings that match the specified pattern from the given string. cal f1 score will calculate F1 score given the predictions and ground truth. check format will check whether the given string matches the provided pattern based on regular expression matching. actions in the training data. To achieve this, we propose the Planning Diversity Reward. When the models outputs differ, we assign higher reward; otherwise, we reduce the reward. Planning Format Reward. The last reward is used to regularize the output, making it easier to extract both the reasoning process and the final answer. This approach is inspired by R1. The reasoning process is encapsulated within the <think></think> tags, while the planning result is enclosed within the <answer></answer> tags. If the final output does not conform to this format, the format reward will be set to 0. The Planning Accuracy Reward, the Action-Weighted Reward, and the Planning Diversity Reward are multiplied to compute the Planning Quality Reward. We calculate the Planning Quality Reward separately for speed planning and direction planning. Finally, the Planning Quality Reward and the Planning Format Reward are used to calculate the GRPO loss and update the model parameters. For details about Planning Reward Modeling, please refer to Alg. 1. Method Size Acc. (%) InternVL2 [9] Qwen2VL [41] Llama3.2-V [11] Qwen2VL [41] InternVL2 [9] Qwen2VL [41] Llama3.2-V [11] Qwen2VL [41] AlphaDrive 2B 2B 11B 7B 2B 2B 11B 7B 2B 7.23 13.69 11.61 19.28 51.07 55.84 58.21 61.44 77.12 Path (F1) left straight right keep Speed (F1) dec. acc. 33.34 34.05 32.56 45.92 76.13 82.68 85.58 86.45 96.62 9.49 21.46 27.78 33.09 85.16 80.31 84.64 85.84 89.83 4.57 13.46 13.67 19. 64.60 70.04 79.12 87.75 93.25 48.75 52.34 42.71 54.13 74.77 75.97 74.79 84.53 86.80 2.87 11.14 12.77 12.86 21.88 34.92 35.56 43.81 56.33 8.12 13.73 20.81 27. 47.66 55.55 58.99 56.30 71.40 stop 13.87 17.03 18.04 23.48 15.81 72.64 76.20 73.80 86.63 BLEU-4 CIDEr METEOR 8.04 16.41 23.23 30. 27.89 24.46 32.05 41.09 43.54 5.40 10.85 15.87 16.16 19.73 23.14 21.25 30.65 38.97 23.63 27.66 26.30 33.36 28.26 34.26 37.70 47.47 55.23 Table 1. High-level planning and reasoning evaluation results on the MetaAD dataset. Except for AlphaDrive, which utilizes our proposed training strategy, all other models are trained based on SFT. denotes fine-tuned on the MetaAD dataset. ID 1 2 3 4 5 6 Plan. Base Acc. Acc. Weighted Diversity"
        },
        {
            "title": "Action",
            "content": "Plan. Plan. Format Acc. (%) Path (F1) left straight right keep Speed (F1) dec. acc. 42.36 55.71 67.91 72.20 69.38 77.12 69.40 83.19 91.95 95.93 92.10 96.62 64.42 77.34 82.65 85.39 80.48 89.83 59.02 71.65 88.01 88.80 85.59 93. 62.18 67.37 77.74 82.54 84.53 86.80 23.72 34.07 49.79 52.64 49.40 56.33 47.48 59.87 61.38 67.60 64.07 71.40 stop 62.70 76.56 85.75 86.76 83.34 86.63 Table 2. Ablations on the effectiveness of our proposed planning GRPO rewards. 3.3. Reasoning: Distillation from Large Models 3.4. Training: SFT Warm-Up, RL Exploration Unlike fields such as mathematics or science, which have abundant high-quality reasoning data available for training, the planning process in autonomous driving is difficult to record, and the cost of manual annotation is high. As result, there is currently no large-scale, readily available planning reasoning dataset. We initially attempt to incorporate reasoning steps directly into the RL training process, but the final results are suboptimal, mainly due to the following shortcomings: (1) insufficient perception of key elements, such as traffic lights; (2) disorganized reasoning process with weak causal relationships; (3) reasoning outputs that are overly lengthy and ineffective. Therefore, we adopt more capable cloud-based large model, such as GPT-4o, to generate high-quality planning reasoning data from small set of driving clips. Specifically, we provide the model with prompts that include the real driving actions in given scenario, along with the vehicles current state and navigation information, prompting the model to generate concise decision-making process. We find that the quality of the generated reasoning process is pretty good. After conducting manual quality check and filtering out samples with obvious errors, we obtain batch of high-quality planning reasoning data. Subsequently, our model can improve its planning reasoning ability through knowledge distillation based on this data. RL relies on sparse reward signals, whereas SFT is based on dense supervision, making it more suitable for knowledge distillation. Additionally, we find that relying solely on RL can lead to instability in the early stages of training. Therefore, we use small amount of data for warmup phase based on SFT, followed by RL training with the full dataset. We discover that this approach improves stability in the early stages of training and enhances the models planning reasoning performance, ultimately leading to better overall planning capabilities. 4. Experiments 4.1. Experimental Settings Dataset. We adopt MetaAD, large-scale real-world driving dataset, as our training and evaluation benchmark. This dataset consists of total of 120k driving clips, each lasting three seconds. MetaAD is high-quality dataset specifically designed for planning, supporting multi-sensor data and perception annotations. Furthermore, it maintains well-balanced distribution across various driving environments and planning actions. The dataset is divided into 110k clips for training and 10k clips for validation. As for reasoning, we sample 30k data from the training dataset to 6 With Reason. Train. Strategy Acc. (%) Path (F1) left straight right keep Speed (F1) dec. acc. SFT RL SFT+RL SFT RL SFT+RL 56.97 62.16 70.73 65.40 72.41 77.12 77.76 82.32 88. 92.52 93.16 96.62 63.69 72.39 75.75 71.28 84.24 89.83 65.07 71.24 78.79 68.65 89.32 93.25 76.22 75.03 78. 81.91 87.58 86.80 37.11 41.13 45.00 36.48 51.19 56.33 51.99 61.08 65.92 59.31 64.70 71.40 stop 75.72 79.15 83.52 71.55 84.07 86.63 BLEU-4 CIDEr METEOR - - - - - - - - - 37.21 25.14 43.54 34.30 24.58 38.97 47.54 38.10 55.23 Table 3. Ablations on different reasoning training strategies. Train. Data Train. Strategy Acc. (%) Path (F1) left straight right keep Speed (F1) dec. acc. 20k 20k 20k 50k 50k 50k 110k 110k 110k SFT RL SFT+RL SFT RL SFT+RL SFT RL SFT+RL 41.12 45.46 55.64 53.02 59.33 70.83 65.40 72.41 77.12 56.15 69.28 68.25 73.74 77.69 82.30 82.52 93.16 96. 36.72 59.42 64.06 62.45 68.55 78.05 71.28 84.24 89.83 35.59 51.91 56.87 65.43 73.82 82.17 68.65 89.32 93. 40.63 56.93 58.61 70.07 77.05 84.80 81.91 82.58 86.80 17.14 30.82 45.19 33.83 40.72 47.27 36.48 51.19 56. 16.74 37.71 53.68 38.94 45.20 58.29 59.31 64.70 71.40 stop 19.19 30.94 44.09 53.96 57.06 64. 71.55 82.02 86.63 BLEU-4 CIDEr METEOR 27.18 20.33 32.84 34.48 22.37 32.30 37.21 25.14 43.54 15.42 11.01 17. 26.83 16.81 30.38 34.30 24.58 38.97 31.17 23.09 35.93 42.85 25.81 46.38 49.54 38.10 55.23 Table 4. Ablations on the amount of training data. generate the planning reasoning process. All reported results are obtained by training on the training set and evaluating on the validation set. Training. We use Qwen2VL-2B [41] as the base model. Qwen2VL is currently one of the best-performing opensource models, and it offers smaller 2B version that better meets the latency requirements for autonomous driving. Additionally, Qwen2VL provides better support for RL. The models inputs include front-view image and planning prompt, which contains the vehicles current speed and navigation information. The navigation data, consistent with real-world driving, is obtained from sparse navigation points via AMap (similar to Google Maps) and is converted into text form for inclusion in the prompt, such as Go straight for 100m, then turn right. Training is conducted using 16 NVIDIA A800 GPUs. Evaluation. The evaluation metrics consist of two aspects. First, the accuracy of meta-action planning is measured by calculating the F1-Score for all categories of lateral and longitudinal meta-actions, followed by the overall planning accuracy. Additionally, for planning reasoning, we compute the similarity between the generated planning reasoning process and the annotated reasoning process in the dataset using BLEU-4 [28], CIDEr [40], and METEOR [4] scores. 4.2. Main Results Tab. 1 presents the performance of AlphaDrive in high-level planning. The first four rows show the results obtained by directly evaluating the corresponding pretrained models. It can be observed that, while these models demonstrate stronger general capabilities, their performance in planning is suboptimal, highlighting the need for further training with driving data. The subsequent five rows display the results of models fine-tuned on the MetaAD dataset. As shown, AlphaDrive significantly outperforms the other models. Compared to Qwen2VL-7B, the second-best performing model after AlphaDrive, the planning accuracy significantly improves by 25.5%. There is noticeable enhancement in key decisions such as steering and acceleration/deceleration. Additionally, the quality of planning reasoning is the best among all models, demonstrating the effectiveness of our proposed two-stage RL training and reasoning strategies. 4.3. Ablation Study Planning Rewards. In Tab. 2, we validate the effectiveness of the four proposed GRPO planning rewards. The Base Accuracy reward directly determines the reward based on whether the response exactly matches the ground truth, common approach in general domains. As shown, the model using the Base Accuracy reward lags significantly behind across all metrics (ID 1). The combination with the 7 Figure 3. Qualitative results of AlphaDirve. After RL training, AlphaDrive exhibits some emergent multimodal planning capabilities, which has great potential for improving driving safety and efficiency. Planning Format Reward yields slight improvement. (ID 2). significant improvement is seen with the adoption of our proposed Planning Accuracy Reward (ID 3). Further enhancement in acceleration/deceleration decisions is achieved by incorporating the Action-Weighted Reward (ID 4). Finally, by combining the Planning Diversity Reward, the best planning performance is achieved (ID 5-6). Reasoning Training Strategies. The ablation study of the reasoning training strategies is shown in Tab. 3. As observed, introducing planning reasoning under different training strategies effectively enhances model performance. Notably, the improvement is especially significant for complex actions such as acceleration and deceleration, demonstrating that reasoning can greatly enhance decision-making in complex scenarios. Furthermore, the model trained exclusively with RL performs worse in reasoning compared to the model trained with SFT. We attribute this to the limited parameter size of smaller models, which results in insufficient perception and reasoning capabilities. Therefore, incorporating SFT as warm-up phase and using knowledge distillation to learn the reasoning process from larger model can effectively address this issue. By combining SFT and RL, the model achieves the best planning reasoning capabilities. Amount of Training Data. Tab. 4 shows the impact of training data size on different training strategies. As observed, when the training data size decreases, SFT is more affected. With only 20k training samples, the model trained with RL reaches planning accuracy of 46.08%, which is significantly higher than that of the SFT-trained model. When using nearly half of the data, with 50k samples, AlphaDrive already achieves planning accuracy of 70.83%, demonstrating the efficiency of our training strategy. 4.4. Emergence of Multimodal Planning Capability Fig. 3 illustrates the multimodal planning capability of AlphaDrive after RL training. In complex scenarios, it can effectively generate multiple feasible solutions, whereas the SFT-trained model can only produce single planning decision. AlphaDrive can be integrated with downstream action model to dynamically select the optimal solution from multiple options. 5. Conclusions and Limitations In this work, we propose AlphaDrive, VLM for high-level planning in autonomous driving. Compared to previous models that solely employed the SFT, we explore the integration of advanced RL and reasoning in planning. Specifically, AlphaDrive introduces planning-oriented RL strategy based on GRPO and further designs two-stage planning reasoning training paradigm. To the best of our knowledge, AlphaDrive is the first to introduce the RL and reasoning to autonomous driving planning, significantly boosting both performance and training efficiency. Currently, due to lack of rich data annotation, AlphaDrive is still unable to output more complex driving behaviors such as lane changes or nudges. Additionally, the 8 current planning reasoning data come from pseudo-labels generated by large models based on ground-truth driving actions, which still suffer from inaccurate perception and failure to capture key factors. Therefore, further systematic validation is required to improve data quality and verify the performance upper bound of AlphaDrive."
        },
        {
            "title": "Acknowledgments",
            "content": "We sincerely thank Hao Gao, Tianheng Cheng, Bencheng Liao, Haoyi Jiang, and Dongli Hu for their valuable feedback on the draft."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 3 [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 2022. 3 [3] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 1, 3 [4] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, 2005. 7 [5] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023. 4 [6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In NeurIPS, 2020. 1, 3 [7] Long Chen, Oleg Sinavski, Jan Hunermann, Alice Karnsund, Andrew James Willmott, Danny Birch, Daniel Maund, and Jamie Shotton. Driving with llms: Fusing object-level vector modality for explainable autonomous driving. arXiv preprint arXiv:2310.01957, 2023. 2 [8] Shaoyu Chen, Bo Jiang, Hao Gao, Bencheng Liao, Qing Xu, Qian Zhang, Chang Huang, Wenyu Liu, and Xinggang Wang. Vadv2: End-to-end vectorized autonomous driving via probabilistic planning. arXiv preprint arXiv:2402.13243, 2024. 1, [9] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and testtime scaling. arXiv preprint arXiv:2412.05271, 2024. 6 [10] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In CVPR, 2024. 1 [11] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 6 [12] Simon Frieder, Luca Pinchetti, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Petersen, and In Julius Berner. Mathematical capabilities of chatgpt. NeurIPS, 2024. 2 [13] Hao Gao, Shaoyu Chen, Bo Jiang, Bencheng Liao, Yiang Shi, Xiaoyang Guo, Yuechuan Pu, Haoran Yin, Xiangyu Li, Xinbang Zhang, et al. Rad: Training an end-to-end driving policy via large-scale 3dgs-based reinforcement learning. arXiv preprint arXiv:2502.13144, 2025. 3 [14] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 2, 3, [15] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. 2 [16] Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai In Wang, et al. Planning-oriented autonomous driving. CVPR, 2023. 1, 3 [17] Bo Jiang, Shaoyu Chen, Qing Xu, Bencheng Liao, Jiajie Chen, Helong Zhou, Qian Zhang, Wenyu Liu, Chang Huang, and Xinggang Wang. Vad: Vectorized scene representation for efficient autonomous driving. In ICCV, 2023. 1, 3 [18] Bo Jiang, Shaoyu Chen, Bencheng Liao, Xingyu Zhang, Wei Yin, Qian Zhang, Chang Huang, Wenyu Liu, and Xinggang Wang. Senna: Bridging large vision-language modarXiv preprint els and end-to-end autonomous driving. arXiv:2410.22313, 2024. 2, 4 [19] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. 4 [20] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified In ICML, vision-language understanding and generation. 2022. 3 [21] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, 2023. 3 [22] Bencheng Liao, Shaoyu Chen, Haoran Yin, Bo Jiang, Cheng Wang, Sixu Yan, Xinbang Zhang, Xiangyu Li, Ying Zhang, 9 Qian Zhang, et al. Diffusiondrive: Truncated diffusion model for end-to-end autonomous driving. arXiv preprint arXiv:2411.15139, 2024. 1, 3 Hang Zhao. Drivevlm: The convergence of autonomous driving and large vision-language models. arXiv preprint arXiv:2402.12289, 2024. 4 [23] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In CVPR, 2024. 3 [24] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2024. 1, 3 [25] OpenAI. Learning to reason with llms, 2024. 2, 3 [26] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 2022. 3 [27] Brian Paden, Michal ˇCap, Sze Zheng Yong, Dmitry Yershov, and Emilio Frazzoli. survey of motion planning and control techniques for self-driving urban vehicles. IEEE Transactions on intelligent vehicles, 2016. [28] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In ACL, 2002. 7 [29] Aditya Prakash, Kashyap Chitta, and Andreas Geiger. Multimodal fusion transformer for end-to-end autonomous driving. In CVPR, 2021. 1, 3 [30] Tianwen Qian, Jingjing Chen, Linhai Zhuo, Yang Jiao, and Yu-Gang Jiang. Nuscenes-qa: multi-modal visual question answering benchmark for autonomous driving scenario. In AAAI, 2024. 4 [31] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. 2, 3, 4 [32] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 2, 4 [33] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 2, 3, 4 [34] Chonghao Sima, Katrin Renz, Kashyap Chitta, Li Chen, Hanxue Zhang, Chengen Xie, Ping Luo, Andreas Geiger, and Hongyang Li. Drivelm: Driving with graph visual question answering. arXiv preprint arXiv:2312.14150, 2023. 2, [35] Maciej Swiechowski, Konrad Godlewski, Bartosz Sawicki, and Jacek Mandziuk. Monte carlo tree search: review of recent modifications and applications. Artificial Intelligence Review, 56(3):24972562, 2023. 3 [36] Sebastian Thrun, Mike Montemerlo, Hendrik Dahlkamp, David Stavens, Andrei Aron, James Diebel, Philip Fong, John Gale, Morgan Halpenny, Gabriel Hoffmann, et al. Stanley: The robot that won the darpa grand challenge. Journal of field Robotics, 2006. 3 [37] Xiaoyu Tian, Junru Gu, Bailin Li, Yicheng Liu, Chenxu Hu, Yang Wang, Kun Zhan, Peng Jia, Xianpeng Lang, and 10 [38] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 1, 3 [39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. 3 [40] Ramakrishna Vedantam, Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In CVPR, 2015. [41] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 3, 6, 7 [42] Shihao Wang, Zhiding Yu, Xiaohui Jiang, Shiyi Lan, Min Shi, Nadine Chang, Jan Kautz, Ying Li, and Jose Alvarez. Omnidrive: holistic llm-agent framework for autonomous driving with 3d perception, reasoning and planning. arXiv preprint arXiv:2405.01533, 2024. 4 [43] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 3 [44] Junjie Wen, Minjie Zhu, Yichen Zhu, Zhibin Tang, Jinming Li, Zhongyi Zhou, Chengmeng Li, Xiaoyu Liu, Yaxin Peng, Chaomin Shen, et al. Diffusion-vla: Scaling robot foundation models via unified diffusion and autoregression. arXiv preprint arXiv:2412.03293, 2024. 4 [45] Dongming Wu, Wencheng Han, Tiancai Wang, Yingfei Liu, Xiangyu Zhang, and Jianbing Shen. Language prompt for autonomous driving. arXiv preprint arXiv:2309.04379, 2023. 4 [46] Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, James Xu Zhao, Min-Yen Kan, Junxian He, and Michael Xie. Self-evaluation guided beam search for reasoning. Advances in Neural Information Processing Systems, 36:4161841650, 2023. 3 [47] Zhenhua Xu, Yujia Zhang, Enze Xie, Zhen Zhao, Yong Guo, Kenneth KY Wong, Zhenguo Li, and Hengshuang Zhao. Drivegpt4: Interpretable end-to-end autonomous driving via large language model. arXiv preprint arXiv:2310.01412, 2023. 2, [48] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. 1 [49] Yunsong Zhou, Linyan Huang, Qingwen Bu, Jia Zeng, Tianyu Li, Hang Qiu, Hongzi Zhu, Minyi Guo, Yu Qiao, and Hongyang Li. Embodied understanding of driving scenarios. arXiv preprint arXiv:2403.04593, 2024. 2,"
        }
    ],
    "affiliations": [
        "Horizon Robotics",
        "Huazhong University of Science and Technology"
    ]
}