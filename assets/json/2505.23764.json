{
    "paper_title": "MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence",
    "authors": [
        "Sihan Yang",
        "Runsen Xu",
        "Yiman Xie",
        "Sizhe Yang",
        "Mo Li",
        "Jingli Lin",
        "Chenming Zhu",
        "Xiaochen Chen",
        "Haodong Duan",
        "Xiangyu Yue",
        "Dahua Lin",
        "Tai Wang",
        "Jiangmiao Pang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Spatial intelligence is essential for multimodal large language models (MLLMs) operating in the complex physical world. Existing benchmarks, however, probe only single-image relations and thus fail to assess the multi-image spatial reasoning that real-world deployments demand. We introduce MMSI-Bench, a VQA benchmark dedicated to multi-image spatial intelligence. Six 3D-vision researchers spent more than 300 hours meticulously crafting 1,000 challenging, unambiguous multiple-choice questions from over 120,000 images, each paired with carefully designed distractors and a step-by-step reasoning process. We conduct extensive experiments and thoroughly evaluate 34 open-source and proprietary MLLMs, observing a wide gap: the strongest open-source model attains roughly 30% accuracy and OpenAI's o3 reasoning model reaches 40%, while humans score 97%. These results underscore the challenging nature of MMSI-Bench and the substantial headroom for future research. Leveraging the annotated reasoning processes, we also provide an automated error analysis pipeline that diagnoses four dominant failure modes, including (1) grounding errors, (2) overlap-matching and scene-reconstruction errors, (3) situation-transformation reasoning errors, and (4) spatial-logic errors, offering valuable insights for advancing multi-image spatial intelligence. Project page: https://runsenxu.com/projects/MMSI_Bench ."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 4 6 7 3 2 . 5 0 5 2 : r MMSI-Bench: Benchmark for Multi-Image Spatial Intelligence Jingli Lin1,5 Sihan Yang1 Runsen Xu1,2 Yiman Xie1,3 Sizhe Yang1,2 Mo Li1,4 Chenming Zhu1,6 Xiaochen Chen7 Haodong Duan1 Xiangyu Yue1,2 Dahua Lin1,2 Tai Wang1 Jiangmiao Pang1 1Shanghai AI Laboratory University 2The Chinese University of Hong Kong 5Shanghai Jiaotong University 6University of Hong Kong Equal Contribution Project Lead Corresponding Author 3Zhejiang University 4Tsinghua 7Beijing Normal University https://runsenxu.com/projects/MMSI_Bench Figure 1: The MMSI-Bench benchmark. Left and middle: Human experts carefully select sets of images that enable multi-image spatial reasoning to answer unambiguous and challenging spatial questions; human-annotated reasoning processes further facilitate automated error analysis. Right: Comparison of models and human performance on MMSI-Bench, highlighting the benchmarks difficulty and the substantial gap between current MLLMs and human-level spatial intelligence."
        },
        {
            "title": "Abstract",
            "content": "Spatial intelligence is essential for multimodal large language models (MLLMs) operating in the complex physical world. Existing benchmarks, however, probe only single-image relations and thus fail to assess the multi-image spatial reasoning that real-world deployments demand. We introduce MMSI-Bench, VQA benchmark dedicated to multi-image spatial intelligence. Six 3D-vision researchers spent more than 300 hours meticulously crafting 1,000 challenging, unambiguous multiplechoice questions from over 120,000 images, each paired with carefully designed distractors and step-by-step reasoning process. We conduct extensive experiments and thoroughly evaluate 34 open-source and proprietary MLLMs, observing wide gap: the strongest open-source model attains roughly 30% accuracy and OpenAIs o3 reasoning model reaches 40%, while humans score 97%. These results underscore the challenging nature of MMSI-Bench and the substantial headroom for future research. Leveraging the annotated reasoning processes, we also provide an automated error analysis pipeline that diagnoses four dominant failure modes, including (1) grounding errors, (2) overlap-matching and scene-reconstruction errors, (3) situation-transformation reasoning errors, and (4) spatial-logic errors, offering valuable insights for advancing multi-image spatial intelligence. Preprint. Under review."
        },
        {
            "title": "Introduction",
            "content": "Multimodal large language models (MLLMs) have rapidly advanced their ability to ground language and vision in the physical world [35, 12, 47], making them promising stepping-stone toward embodied artificial general intelligence (AGI). core capability of MLLMs deployed in the physical world is spatial intelligencethe skill to understand where things are, how they move, etc. To steer progress toward this goal, the community needs rigorous, diverse, and challenging evaluation benchmarks that can serve as north stars for progress. However, most existing benchmarks probe only single-image spatial understanding and restrict themselves to straightforward spatial relations [34, 56, 27, 17, 15]. Real-world spatial understanding is more complex: models must reason across multiple images to track object and ego motion, and relate entities that never co-occur in single frame [62, 58, 38]. Without benchmarks that demand such multi-image reasoning, we cannot reliably measure or improve an MLLMs spatial competence. In this work, we introduce MMSI-Bench, multiple-choice VQA benchmark dedicated to MultiiMage-based Spatial Intelligence (Figure 1). MMSI-Bench is built on comprehensive taxonomy of ten fundamental spatial reasoning tasks that span the positions, attributes, and motions of three key elementscamera, object, and region in real-world scenes. Beyond these atomic tasks, we construct multi-step reasoning split that chains them into long-horizon questions. Crafting benchmark that is simultaneously diverse, accurate, and challenging is far from trivial. Prior benchmarks [15, 17, 65, 22] populate templated questions from annotated or estimated spatial metadata. These automatic pipelines suffer from limited diversity and incomplete coverage. We therefore adopt fully human-centric design. For each question, one of six 3D-vision researchers scours large image collections, selects set of images that exhibit non-trivial spatial relations, and creates novel, difficult, and unambiguous multiple-choice question whose answer can be derived only by reasoning across all selected images. Every item comes with carefully designed distractors and step-by-step reasoning process that explicitly leads to the correct answer; second annotator then audits each example to ensure clarity and correctness. Over the course of 300+ researcher-hours, we inspected more than 120,000 candidate images and distilled them into 1,000 high-quality Q&A pairs covering broad spectrum of real-world scenesfrom indoor scans [18, 14] and outdoor driving footage [54, 13] to robot manipulation [9] and everyday activities [23]. We evaluate 34 widely used MLLMs on MMSI-Bench. The strongest open-source model attains about 30% accuracy, while the best proprietary system, the OpenAI o3 model [46] with reasoning mode, reaches only 40%, compared with human performance at 97% accuracy. To the best of our knowledge, MMSI-Bench shows the largest performance margin between state-of-the-art models and humans among existing spatial intelligence benchmarks. This pronounced gap underscores both the difficulty of MMSI-Bench and the headroom for progress in spatial intelligence. Our error analysis groups failures into four categories: (1) grounding errors, (2) overlap-matching and scene-reconstruction errors, (3) situation-transformation reasoning errors, and (4) spatial-logic errors. Because every question is paired with reference reasoning process, we also release novel automated analysis pipeline that contrasts models response with the reference to pinpoint error types efficiently across the entire benchmark, providing actionable insights for future work on multi-image spatial intelligence."
        },
        {
            "title": "2 Related Work",
            "content": "Multi-image VQA benchmarks. Driven by the rapid progress of MLLMs, VQA benchmarks have shifted from single-image reasoning [25] to multi-image settings focusing on various skills. Visual Haystacks [60] probes long-context retrieval, asking models to locate relevant content within large image collections. MIBench [36] stresses cross-image reasoning that combines instruction following with knowledge retrieval. ReMI [28] broadens the scope to math, physics, and code across two to six images, blending GPT-generated items with human audits. BLINK [22] zeroes in on inter-image relationships, while MuirBench [57] adds robustness and diversity to multi-image reasoning. Existing benchmarks show that contemporary MLLMs still trail behind humans as either the number of images or the reasoning complexity grows [31]. In this work, instead of building general benchmark, we focus on comprehensively evaluating MLLMs multi-image spatial intelligence. 2 Benchmarks for spatial intelligence of MLLMs. Recent progress in MLLMs has enabled them to potentially serve as the brains of embodied agents, e.g., the decision-making modules of autonomous vehicles [53] or robots [29, 12, 32], where reliable spatial intelligence is indispensable. To measure this capability, variety of benchmarks have been proposed, but the majority assess only single-image spatial understanding [33, 66, 52, 34, 27, 20, 56, 49, 39]. Beyond single view, associating information across multiple images is even more critical in practice, yet existing multi-image benchmarks remain fragmentary. BLINK [22] and MuriBench [57] contain just few spatial sub-splits within otherwise general VQA suites, and thus lack comprehensive coverage. LEGO-Puzzles [55] probes multi-step reasoning in synthetic LEGO-assembly setting, overlooking real-world scenes. Efforts such as UniQA-3D [68], MMIU [40], SAT [50], MultiSPA [63], and VSI-Bench [65] generate template-based or rule-based questions from existing annotations/metadata or simulators, which constrains question diversity. By contrast, our MMSI-Bench is fully humancurated, free from template limitations, contains challenging and diverse questions, and offers rigorous evaluation of MLLMs spatial intelligence in multi-image scenarios."
        },
        {
            "title": "3 MMSI-Bench",
            "content": "In this section, we present MMSI-Bench, comprehensive and challenging benchmark designed to evaluate the multi-image spatial reasoning capability of MLLMs. Specifically, we introduce the concept of multi-image spatial reasoning and outlines its associated sub-tasks in Section 3.1. Subsequently, in Section 3.2, we provide detailed description of the benchmark construction process. 3.1 Overview of MMSI-Bench Humans possess the remarkable ability to infer spatial information about their environment by integrating visual inputs from different viewpoints or moments in time. We refer to this process as multi-image spatial reasoning, in which information from multiple images is combined to reconstruct the underlying scene and enable reasoning to answer complex spatial questions. To evaluate the multiimage spatial reasoning capability of multimodal large language models, we introduce MMSI-Bench. To systematically evaluate multi-image spatial reasoning, we categorize tasks based on three fundamental spatial elements: camera (the agent that captures the scenes), object (entities other than the agent within the scenes), and region (semantic areas within the scenes, such as kitchen). Fundamental spatial understanding requires not only recognizing the positional relationships among these elements but also their attributes and potential motion. Guided by this taxonomy, we define ten spatial reasoning task types and one multi-step spatial reasoning category, which flexibly combines these basic types. Examples for all task categories are provided in Figure 2. Notably, our questions are designed to be human-answerable; thus, we do not include questions about camera attributes (parameters). Additionally, since regions are generally static, we do not include motion-related tasks for regions. The specific categories are as follows: Positional Relationship (camera-camera): Inferring the positional relationship between two camera viewpoints. Positional Relationship (camera-object): Identifying the positional relationship between the camera and an object. Positional Relationship (camera-region): Assessing the positional relationship between the camera and semantic region (e.g., kitchen). Positional Relationship (object-object): Inferring the positional relationship between objects within the scenes. Positional Relationship (object-region): Determining the positional relationship between an object and semantic region. Positional Relationship (region-region): Identifying the positional relationship between different regions within given scene. Attribute Measurement: Inferring geometric properties (e.g., length, size) of elements. Attribute Appearance: Inferring visual characteristics of elements, such as their shape. Motion (camera): Reasoning about the cameras movement, such as its movement direction. 3 Figure 2: Representative MMSI-Bench samples from each category. Please zoom in to inspect image details. Questions and rationales are simplified for brevity; the complete versions appear in the supplementary material. Correct answers are highlighted in green. Motion (object): Assessing the movement of objects within the scene. Multi-Step Reasoning: Solving complex tasks that require multiple sequential steps of the spatial reasoning types described above. In conclusion, MMSI-Bench consists of 1,000 multiple-choice questionanswer pairs based on multiple images, each also accompanied by annotated reasoning processes explaining the answer. Every question is specifically designed such that it cannot be answered using single image alone. Except for the Multi-Step Reasoning category, all other categories contain only two images per question. Detailed statistics on the proportion of questions in each category and other relevant information about MMSI-Bench are presented in Figure 4 and Table 1, respectively. 3.2 Benchmark Construction Process Our detailed benchmark construction process is illustrated in Figure 3. Data collection. To comprehensively evaluate the capability of MLLMs across variety of realworld scenarios, it is essential to curate data from diverse and representative sources. To ensure the complexity and authenticity of the evaluation, we exclusively utilize images captured from real-world scenes. Accordingly, we leverage images from diverse collection of real-world datasets: indoor 3D 4 Figure 3: Illustration of the MMSI-Bench construction pipeline: images are collected from diverse real-world datasets, relevant image sets are carefully selected, complex QA tasks and detailed reasoning processes are manually annotated, and all data undergo rigorous quality control. Statistic Total questions Number of unique images Average question length Average reasoning process length Average image count per question Maximum question length Maximum reasoning process length Maximum image count per question Value 1,000 1,990 130.28 252.83 2.55 449 1,118 10 Data sources: ScanNet, nuScenes, Matterport3D, Ego4D, AgiBot-World, DTU, DAVIS 2017, Waymo. Figure 4: Distribution of categories in MMSI-Bench. Table 1: Dataset statistics. scene datasets (ScanNet [18] and Matterport3D [14]), local scene reconstruction dataset (DTU [26]), autonomous driving datasets (nuScenes [13], Waymo [54]), robotics dataset (AgiBot-World [9]), video object segmentation dataset (DAVIS 2017 [48]), and an egocentric video dataset (Ego4D [23]). These diverse data sources enable the construction of various question-answer pairs, ensuring that our benchmark captures the complexity and diversity inherent in real-world environments. Question-answer design and reasoning process Annotation. Based on the collected datasets, six 3D Vision researchers annotate QA pairs spanning the above eleven distinct task categories (with each category allocated to all six annotators). Annotators are explicitly instructed to create diverse set of questions that are as novel and as challenging as possible for MLLMs, aiming to push the boundaries of current model capabilities. At the same time, annotators are instructed to ensure that all questions are accurate, unambiguous, and answerable by humans. For each question, annotators begin by visually inspecting the image database to choose set of relevant images. They then construct question that cannot be answered based on any single image in isolation; rather, the question requires integrating information from multiple images in order to infer the actual scenario. To ensure diversity in question formats, no templates are used, and all questions are created in free form. Each question is designed as four-option multiple-choice problem, with only one correct answer. The incorrect options are also carefully crafted to serve as plausible distractors. In addition, annotators provide detailed reasoning processes alongside each answer, which are used to facilitate filtering incorrect samples during quality control and to enable automated evaluation and analysis of MLLMs reasoning processes, as described in Section 5.2. 5 Positional Relationship Attribute Motion Cam.Cam.Obj.Obj. Reg.Reg. Cam.Obj. Obj.Reg. Cam.Reg. Meas. Appr. Cam. Obj. MSR Avg. Models Proprietary o3 GPT-4.5 GPT-4.1 GPT-4o Gemini-2.5-Pro Thinking Claude-3.7-Sonnet Thinking Doubao-1.5-pro Seed1.5-VL 45.2 34.4 36.6 34.4 39.7 37.6 32.3 35.5 25.8 32. Open-source 34.4 InternVL3-78B 23.7 InternVL2.5-78B Qwen2.5-VL-72B 25.8 LLaVA-OneVision-72B 43.0 21.5 InternVL3-38B 18.3 InternVL2.5-38B 24.7 Qwen2.5-VL-32B 24.7 InternVL2.5-26B 30.1 NVILA-15B 19.4 InternVL3-14B 25.8 Llama-3.2-11B-Vision 18.3 InternVL3-9B 25.8 InternVL3-8B 32.3 InternVL2.5-8B 17.2 NVILA-8B 24.7 Qwen2.5-VL-7B 20.4 LLaVA-OneVision-7B 31.2 InternVL2.5-4B 26.9 Qwen2.5-VL-3B 26.9 InternVL3-2B 28.0 InternVL2.5-2B 24.7 InternVL3-1B 23.7 InternVL2.5-1B 23.7 DeepSeek-VL2 24.7 DeepSeek-VL2-Small 29.0 DeepSeek-VL2-Tiny Baseline Blind GPT-4o Random Guessing Human Level 20.2 25.0 95.7 39.4 29.8 26.6 24.5 31.9 30.9 26.6 29.8 33.0 30.8 23.4 22.3 34.0 31.9 20.2 22.3 26.6 19.1 39.4 24.5 30.8 25.5 31.9 27.7 29.8 24.5 33.0 23.4 27.7 25.5 27.7 35.1 26.6 31.9 28.7 27.7 17.0 25.0 98. 37.0 39.5 27.2 23.5 39.5 42.0 22.2 29.6 33.3 25.9 32.1 39.5 34.6 33.3 33.3 35.8 29.6 29.6 28.4 24.7 32.0 32.1 37.0 29.6 24.7 24.7 29.6 21.0 30.9 29.6 24.7 22.2 24.7 22.2 18.5 21.0 29.6 25.0 97.5 44.2 51.2 29.1 19.8 45.3 39.5 34.9 31.4 33.7 23.2 12.8 29.1 23.3 30.2 23.3 22.1 22.1 33.7 36.0 23.3 25.6 29.1 25.6 32.6 30.2 25.6 29.1 31.4 29.1 31.4 37.2 30.2 25.6 36.0 33.7 23.3 13.9 25.0 94. 47.1 47.1 36.5 37.6 35.2 40.0 37.6 32.9 49.4 38.8 37.6 31.8 34.1 37.6 35.3 38.8 32.9 31.8 38.8 37.6 21.2 31.8 35.3 24.7 22.4 29.4 25.9 34.1 28.2 28.2 29.4 29.4 31.8 30.6 38.8 17.6 29.4 25.0 98.8 62.6 55.4 27.7 27.7 43.3 42.2 42.2 34.9 39.8 32.5 26.5 42.2 36.1 38.6 25.3 34.9 31.3 37.3 20.5 24.1 25.9 22.9 28.9 32.5 34.9 26.5 30.1 25.3 34.9 27.7 36.1 30.1 25.3 22.9 27.7 31.3 19.2 25.0 96. 54.7 39.1 37.5 32.8 51.5 50.0 25.0 37.5 37.5 39.0 37.5 35.9 45.3 28.1 39.1 37.5 31.2 35.9 29.7 31.2 20.3 29.7 23.4 26.6 34.4 25.0 29.7 23.4 31.2 26.6 43.8 32.8 31.2 28.1 28.1 14.1 21.8 25.0 95.3 28.8 33.3 24.2 31.8 21.2 27.3 22.7 25.8 31.8 21.2 19.7 19.7 27.3 19.7 21.2 25.8 24.2 30.3 31.8 22.7 19.7 24.2 24.2 27.3 25.8 18.2 25.8 24.2 16.7 22.7 15.2 28.8 30.3 15.2 33.3 24.2 12.1 25.0 98. 31.1 41.9 36.5 35.1 36.4 35.1 21.6 23.0 27.0 36.4 28.4 17.6 27.0 13.5 16.2 14.9 18.9 10.8 18.9 24.3 25.6 16.2 16.2 16.2 25.7 20.3 18.9 13.5 17.6 12.2 21.6 17.6 17.6 28.4 24.3 14.9 20.2 25.0 98.6 32.9 40.8 32.9 36.8 30.2 32.9 32.9 31.6 26.3 25.0 31.6 26.3 30.3 32.9 31.6 38.2 35.5 31.6 35.5 31.6 28.9 38.2 32.9 31.6 34.2 39.5 34.2 31.6 27.6 23.7 31.6 19.7 25.0 26.3 25.0 25.0 29.0 25.0 98. 34.9 36.4 28.8 30.8 34.3 34.3 22.7 25.8 29.8 26.2 29.3 27.3 27.3 15.7 25.8 25.3 27.8 26.8 27.8 29.3 19.2 26.8 14.6 30.3 29.8 25.8 11.6 26.8 23.2 23.7 26.8 26.3 26.3 28.3 29.8 27.3 20.2 25.0 97.0 41.0 40.3 30.9 30.3 36.9 37.0 28.7 30.2 33.0 29.7 28.5 28.5 30.7 28.4 26.3 27.9 27.7 28.0 30.5 26.8 25.4 26.7 25.7 28.7 28.1 25.9 24.5 26.3 26.5 25.3 29.0 27.0 26.1 27.1 28.6 24.0 22.7 25.0 97. Table 2: Evaluation results for 34 MLLMs. For each category, the best-performing proprietary model and the best-performing open-source model are both indicated in bold. Quality Control. To ensure the quality of the data, three additional reviewersindependent of the original annotatorssystematically examine all data and remove any instances containing ambiguous questions (due to linguistic ambiguities or insufficient visual information), incorrect answers, or questions that can be answered using single image or common sense alone. This rigorous quality control ensures our benchmark provides reliable, high-quality evaluation."
        },
        {
            "title": "4 Evaluation on MMSI",
            "content": "4.1 Evaluation Setup Benchmark models. We conduct comprehensive evaluation of wide range of MLLMs, encompassing both proprietary and open-source families, diverse model scales, and recent architectural advances. For proprietary models, our benchmark includes o3 [46], GPT-4.5 [45], GPT-4.1 [44], GPT4o[43], Gemini-2.5-Pro [19] (with and without Thinking mode), Claude-3.7-Sonnet [10] (with and without Thinking mode), Doubao-1.5-pro [51], and Seed1.5-VL [24]. On the open-source side, we systematically assess state-of-the-art models such as Qwen2.5-VL [11], LLaVA-OneVision [30], InternVL3 [67], InternVL2.5 [16], DeepSeek-VL2 [61], Llama-3.2-11B-Vision [41], and NVILA [37]. Baselines. For comparison, we include three baselines: Random Guessing, Human Performance, and Blind GPT-4o. Random Guessing reflects the expected accuracy under equal-probability selec6 tion from all options. Human performance is measured as the average accuracy of five independent evaluators not involved in data annotation, representing human-level spatial intelligence on this benchmark. Blind GPT-4o denotes the performance of GPT-4o without access to image inputs. Implementation details. We report accuracy (%) using exact match between answers extracted from model outputs and ground-truth answers for our multiple-choice questions. If model fails to generate an answer in the required format, we adopt the LLM-based fallback strategy from VLMEvalKit [21] to extract the intended response. For consistency, all models are evaluated with temperature of zero and maximum output length of 2,048 tokens. 4.2 Main Results Table 2 presents the primary results of our evaluation, covering both proprietary and open-source models, as well as the baselines of Random Guessing, Human Performance, and Blind GPT-4o. We summarize our main findings as follows: Current MLLMs struggle with multi-image spatial reasoning. Despite recent progress, even the most advanced MLLMs exhibit significant limitations in multi-image spatial reasoning. Most models achieve relatively low average scores, often only marginally outperforming random guessing, and are far from human-level performance. For example, the best-performing proprietary reasoning model, o3, achieves an average accuracy of only 41.0%, while human performance exceeds 97%, highlighting substantial gap in spatial intelligence between current MLLMs and humans. Furthermore, we find that employing the \"thinking\" mode in models such as Gemini-2.5-Pro and Claude-3.7-Sonnet brings only limited improvements, suggesting that these models potentially lack strong spatial reasoning capabilities. Additionally, the accuracy of Blind GPT-4o is close to that of random guessing, indicating that our tasks indeed require genuine visual-spatial reasoning and cannot be solved by language priors or commonsense knowledge alone. Advanced open-source models still trail proprietary counterparts. We observe that leading open-source models generally underperform compared to proprietary models. For instance, the best open-source model, Qwen2.5-VL-72B, achieves an average score of 30.7%, which is notably lower than that of the top proprietary model. This underscores notable performance gap between cutting-edge open-source and proprietary MLLMs on this challenging benchmark. Multi-step reasoning and camera motion comprehension are challenging. For most models, performance on multi-step reasoning (MSR) tasks is lower than on single-step tasks (i.e., the average performance on Positional Relationship, Attribute, and Motion categories), indicating that integrating information across multiple reasoning steps remains challenge for current MLLMs. In addition, most models, particularly open-source ones, perform poorly on camera motion tasks, suggesting that MLLMs, as embodied agents, struggle to understand their own movement. One potential reason is that open-source models generally lack access to annotated first-person motion data during training, while proprietary models may leverage more diverse datasets containing such information. Scaling up model size brings limited gains on MMSI-Bench. We observe that, for models within the same series, increasing the number of parameters leads to only limited performance gains on MMSI-Bench. For example, Qwen2.5-VL-72B achieves only 3% higher accuracy than Qwen2.5VL-32B, and InternVL3-78B outperforms InternVL3-1B by just 1.5%. Moreover, NVILA-15B surpasses most models with over 70 billion parameters, ranking second only to the best-performing open-source model, Qwen2.5-VL-72B. These results suggest that, at present, the bottleneck for advancing MLLM performance on complex spatial reasoning tasks may lie primarily in data quality and diversity, rather than model size. How to best leverage data, model size, and architecture in synergy remains an open and important question for future research. 4.3 Limited Effect of Prompting Techniques on MMSI-Bench Prompting techniques are widely adopted to improve the reasoning and problem-solving capabilities of large models. Building on these advances, we investigate whether such methods can similarly enhance the spatial reasoning abilities of MLLMs on MMSI-Bench. Specifically, we investigate both linguistic and visual prompting approaches (see supplementary material for further details): 7 Linguistic prompting: We adopt the widely used Zero-Shot Chain-of-Thought (CoT) method. Following prior work [59], we prepend Lets think step by step to the prompts to encourage reasoning. Visual prompting: For multi-image spatial understanding, fundamental requirement is the ability to identify overlaps between images in order to establish relationships across them. However, MLLMs may struggle to identify overlaps between images. To explicitly provide this cue to the model, we employ the popular image matching method PATS [42] to establish sparse correspondences between pairs of images, which involves concatenating multiple images and drawing lines between corresponding points to explicitly indicate cross-image relationships. As baseline, we use concatenated images without correspondence lines as visual inputs, and compare their results with those obtained using visual prompt. We conduct experiments using representative models, including GPT-4o, GPT-4.5, LLaVA-OneVision-72B, and Qwen2.5-VL-72B. As shown in Figure 5, the linguistic prompting approach (CoT) leads to modest performance improvement only for GPT-4o, while resulting to performance degradation on the other models. This limited effectiveness of linguistic prompting may be attributed to the models inherent lack of spatial reasoning capabilities; simply encouraging \"thinking\" does not address their fundamental limitations in spatial understandinga finding consistent with the observation that adopting \"thinking\" mode in models like Gemini-2.5-Pro and Claude-3.7Sonnet does not yield performance gains either. The particularly pronounced negative impact for LLaVA-OneVision may be due to its limited exposure to multimodal reasoning data during training, which makes it struggle with complex multimodal reasoning tasks. Figure 5: Impact of linguistic and visual prompting on MMSI-Bench. Similarly, the visual prompting approach yields only slight performance improvements for two models, while resulting in performance degradation for the other two. One possible explanation is that most models still lack the basic spatial intelligence required to benefit from such cues: they may not recognize the need to identify overlaps between images to establish spatial relationships, or, even when overlaps are identified, they struggle to reconstruct the actual scene. These findings indicate that further progress in the spatial reasoning ability of MLLMs may depend on advances in model architectures or training paradigms, such as the incorporation of domain-specific data, rather than relying solely on improved prompting strategies."
        },
        {
            "title": "5 Error Analysis",
            "content": "This section analyzes the errors made by MLLMs on MMSI-Bench. First, Section 5.1 presents manual categorization of error types observed in the reasoning processes of the representative model. Then, in Section 5.2, we introduce an enhanced automated error analysis method for systematically examining the reasoning errors of various MLLMs with the help of our annotated reasoning processes. 5.1 Error Type Categorization To identify the main bottlenecks of MLLMs on our benchmark, we manually analyze the reasoning processes exhibited by representative model, GPT-4o, when prompted with chain-of-thought (CoT) instructions. Specifically, we randomly sample 100 instances from MMSI-Bench and categorize the observed errors into four distinct types (as illustrated in Figure 6): Grounding errors. The model fails to correctly identify or localize relevant objects or details within the image. For instance, it might misclassify an object or incorrectly determine the position of an objecterrors that undermine its ability to anchor visual reasoning in the actual image content. Overlap-matching and scene-reconstruction errors. The model fails to identify and match corresponding points that represent the same locations or objects in the real scene across different images, and struggles to implicitly reconstruct the underlying scene based on these cross-image relationships. Figure 6: Illustration of four error types identified in MLLM spatial reasoning on MMSI-Bench. Figure 7: Distribution of correct and error types across three representative MLLMs. For example, it may be unable to recognize that tree appearing in both images is actually the same tree, leading to an incorrect reconstruction of the scene layout. Situation-transformation reasoning errors. The model makes mistakes when reasoning about spatial directions relative to different reference objects, or when converting between relative directions (such as left and right) and absolute directions (such as north, south, east, and west), leading to incorrect situational inferences. Spatial-logic errors. The model exhibits errors in spatial logical reasoning, such as (1) hallucinating spatial relationships that do not exist in the given context; (2) failing to correctly apply the transitivity of spatial relationsfor example, given that is east of and is east of C, the model incorrectly infers that is west of C; and (3) making mistakes in identifying the correct reference object during motion reasoning, such as using another moving object as the reference point when determining an objects movement status (whereas the correct approach would be to use stationary object like the ground as the reference). 5.2 Automated Error Analysis The manually annotated reasoning processes that we provide, in addition to facilitating quality assurance, have other potential uses, such as serving as training data to improve the spatial reasoning capabilities of MLLMs [15, 64]. However, as our work focuses on evaluation benchmarks, how to scalably collect such data for training remains an open question for future work. Here, we demonstrate another important application: enabling automated error analysis. 9 Specifically, we leverage the reasoning processes generated by GPT-4o on randomly sampled data, as described in Section 5.1. By providing GPT-4o with both the ground-truth answers and the humanannotated reasoning steps, we prompt it to simultaneously assess the correctness of the MLLMs reasoning process and, if incorrect, identify the key error type responsible. For comparison, we assess the MLLMs reasoning processes by identifying the most critical error type responsible for each incorrect case. Ultimately, this automated error analysis closely aligns with human evaluators judgments, with 71.8% of classifications matching when human-annotated reasoning steps are available. In contrast, when these annotated reasoning steps are not provided and only the groundtruth answers are given, only 53.6% of classifications match human evaluators judgments, indicating that correct answers alone are insufficient for effectively assisting the model in evaluating the reasoning process. This highlights the important role of high-quality human annotations in achieving reliable automated error analysis. We then apply this approach to analyze the reasoning processes of representative models across all MMSI-Bench questions, with results presented in Figure 7, which reveals the distribution of error types across different models and highlights specific directions for improving their spatial reasoning abilities. Notably, we observe that even when the final answer is correct, the reasoning process can still contain significant errors. Among the various error types, overlap-matching and scene-reconstruction errors account for the largest proportion of mistakes across all models. This suggests clear direction for future progress as MLLMs strive for human-level spatial intelligence."
        },
        {
            "title": "6 Conclusion",
            "content": "To summarize, we introduce MMSI-Bench, challenging and comprehensive benchmark specifically designed to assess the multi-image spatial reasoning capability of MLLMs. Our evaluation across 34 state-of-the-art models reveals substantial gap between current MLLMs and human-level spatial intelligence. Leveraging expert-annotated reasoning processes, we further propose an automated error analysis pipeline that enables scalable and systematic diagnosis of model failures, highlighting concrete directions for improving spatial reasoning in future models. We hope MMSI-Bench will serve as valuable resource for the community and accelerate progress toward more spatially capable and robust multimodal AI systems. We discuss the limitations in the supplementary material."
        },
        {
            "title": "A Appendix Overview and Organization",
            "content": "This appendix provides supplementary details to support and extend the main paper. The organization of the appendix is as follows: 1. Limitations, Social Impact, and License & Access (Section B): This section outlines not only the limitations of our work, but also discusses its potential social implications, associated risks, and ethical considerations. In addition, it clarifies the licensing terms and access conditions for MMSI-Bench, ensuring responsible and transparent use of the dataset. 2. Comparison with Other Spatial Question-Answering Benchmarks (Section C): In this section, we compare MMSI-Bench with existing spatial question-answering benchmarks, highlighting the unique value of our benchmark. 3. Additional Implementation Details and Experimental Results (Section D): This section provides supplementary technical details regarding the implementation as well as more experimental results. 4. Details of the Data Curation Process (Section E): This section elaborates on the procedures and criteria used in curating the MMSI-Bench dataset, including data sourcing, annotation protocols, and quality control measures. 5. Representative MMSI-Bench Samples from Each Category (Full Version) (Section F): In this section, we present full set of representative samples from each category in MMSIBench, without any truncation or simplification. This is intended to give the reader complete view of the datasets diversity and coverage. 6. Additional Case Studies (Section G): We provide further case studies to illustrate the models reasoning process on MMSI-Bench, supplementing those already discussed in the main text. 10 Each section is intended to provide additional transparency and insight, ensuring the reproducibility and broader contextualization of our research. Limitations, Social Impact, and License & Access B.1 Limitation In developing MMSI-Bench, we opted for manual annotation of question-answer pairs to ensure higher data quality and richer content. While this approach leads to more challenging and diverse benchmarks, it inevitably introduces scalability limitations, as expanding the dataset requires significant human effort. Additionally, during the manual annotation process, we intentionally avoided rigid templates to encourage greater variety and naturalness in the dataset. Although this results in richer and more realistic set of questions, it also makes it more challenging to precisely control the difficulty level across all items. Despite these inherent trade-offs, we believe that our annotation strategy enables MMSI-Bench to better reflect the complexities of real-world spatial reasoning, and will serve as valuable resource for advancing research in vision-language understanding. B.2 Broader Impact The introduction of MMSI-Bench brings both potential benefits and risks to the research community and broader society. On the positive side, MMSI-Bench provides challenging and comprehensive benchmark specifically designed for evaluating the spatial reasoning capabilities of vision-language models. By facilitating more nuanced and rigorous assessment, MMSI-Bench may help drive advances in embodied AI, assistive technologies, spatial robotics, and other domains where spatial understanding is critical. However, we also recognize several risks and challenges associated with the development and release of such datasets. First, improvements in spatial reasoning enabled by MMSI-Bench may have unintended applications, such as enhanced surveillance or automated decision-making systems that could impact privacy or disproportionately affect certain groups. We encourage users of MMSI-Bench to adhere strictly to ethical guidelines and to consider the societal implications of deploying models benchmarked using our dataset. From an environmental perspective, we acknowledge the significant computational resources required to evaluate modern vision-language models. By releasing MMSI-Bench and its associated scripts publicly, we aim to promote reproducibility and reduce redundant efforts, thereby minimizing unnecessary environmental impact. In summary, while MMSI-Bench is intended to advance responsible and transparent research in spatial reasoning for vision-language models, we urge the community to remain vigilant regarding both its positive and negative societal impacts. We are committed to updating and improving MMSI-Bench in response to constructive feedback, and we welcome ongoing dialogue to help identify and mitigate emerging risks. B.3 Ethics Statement As this research exclusively employs publicly accessible pre-trained models, it does not present any ethical concerns. All procedures strictly conform to the guidelines established by the NeurIPS Code of Ethics. B.4 Reproducibility Statement Our experiments follow the baseline configurations established by prior evaluation benchmarks or the original testing protocols of each respective model. Detailed implementation information is provided in the Evaluation Setup and Appendix sections. We will release all data and code, along with thorough documentation, to facilitate faithful reproduction of our main experimental findings. All aspects of this research are fully compliant with the NeurIPS Reproducibility Requirements. 11 B.5 License and Access The MMSI-Bench dataset is distributed under the Creative Commons Attribution 4.0 International License (CC BY 4.0). For all images included in the dataset, their original licensing terms [3, 7, 6, 5, 4, 1, 2, 8] remain fully in effect. These images, as well as their annotations, are available for direct download at https://huggingface.co/datasets/sihany/MMSI-Bench. We release MMSI-Bench under the CC-BY license and Terms of Use, and require that any use of the dataset for model evaluation be properly disclosed. This license supplements but does not override the original licenses of source materials; users must also comply with all relevant legal requirements concerning data subjects. This statement clarifies the obligations and liabilities associated with using this benchmark. While we strive to ensure the accuracy and legality of all samples, we do not guarantee their absolute completeness or correctness. We assume no responsibility for any legal or other issues that may arise from the use of MMSI-Bench, including but not limited to copyright infringement, privacy violations, or the misuse of sensitive information. By accessing, downloading, or using MMSI-Bench, you acknowledge that you accept this statement and agree to comply with the full terms of the CC-BY license. If you do not agree with these terms or the CC-BY license, you are not permitted to use this benchmark. MMSI-Bench will be hosted and maintained on GitHub and the Hugging Face Hub platforms. Comparison with Other Spatial Question-Answering Benchmarks Benchmark Annotation Method Visual Input Modality Number of QAs Scenario Human-AI Gap VSR [34] BLINK [22] UniQA-3D [68] MuriBench [57] SpatialVLM [15] SpatialRGPT [17] MMIU [40] LEGO-Puzzle [55] VSI-Bench [65] Auto & Human Auto & Human Auto & Human Auto & Human Auto & Human Auto Auto Human Auto & Human Single-Image Single/Multi-Image Single/Multi-Image Multi-Image Single-Image Single-Image Multi-Image Single/Multi-Image Video 10K 3,807 (572) 500 2,600 (478) 546 1,406 11K (2904) 1K 5K Indoor & Outdoor Real-world & Synthetic Real-world & Synthetic Real-world & Synthetic Indoor & Outdoor Real-world & Synthetic Real-world & Synthetic Synthetic Indoor MMSI-Bench Human Multi-Image 1,000 Indoor & Outdoor 25.0% 44.4% 14.2% 25.2% <30% <42% <45% 39.5% 33% 56.2% Table 3: Comparison of spatial question-answering benchmarks. Some benchmarks contain QA pairs outside spatial understanding; the number of spatial QA is shown in parentheses. Human-AI performance gap is defined as the accuracy difference between the best-performing model reported in the corresponding paper and human performance; for benchmarks without reported human results, the gap is calculated with respect to 100% accuracy and is marked as <value%. Note: Scenario entries labeled as Indoor & Outdoor or Indoor refer to real-world images. Despite significant advances in multimodal large language models (MLLMs), robust evaluation of their spatial intelligenceespecially in multi-image, real-world scenariosremains limited. As summarized in Table 3, most existing benchmarks focus narrowly on single-image spatial understanding or offer only limited multi-image coverage as minor components within more general VQA suites (e.g., BLINK [22], MuriBench [57]), rather than serving as dedicated spatial reasoning benchmarks. Furthermore, several datasets rely heavily on templateor rule-based question generation [50, 68, 40, 65], which restricts both the diversity and depth of spatial reasoning challenges. Synthetic settings such as LEGO-Puzzle [55] further lack the complexity and contextual richness of real-world environments. In contrast, MMSI-Bench is fully human-curated benchmark, meticulously annotated by six 3D vision researchers without the use of templates. It offers comprehensive coverage of spatial relationships and elementsincluding camera (agent), objects, and regionsacross ten well-defined task types plus one multi-step reasoning category. All scenarios derive from diverse, real-world sources, spanning indoor 3D scene datasets, local scene reconstructions, autonomous driving datasets, and robotics data, encompassing both indoor and outdoor environments. Notably, MMSI-Bench provides detailed, human-authored reasoning chains for each question, enabling rigorous assessment of both answer accuracy and reasoning quality. This benchmark reveals substantial gap between current MLLMs and human-level spatial intelligence, and sets higher standard for multi-image spatial reasoning evaluation in realistic settings."
        },
        {
            "title": "D Additional Implementation Details and Experimental Results",
            "content": "D.1 Implementation Details for Model Evaluation We evaluate models using three prompting strategies: Direct, Linguistic (Zero-Shot Chain-ofThought), and Visual. The exact prompts used for each method are shown in Table 4. Strategy Direct Linguistic (Zero-Shot CoT) Visual Prompt Template {images}{question} Answer with the options letter from the given choices directly. Enclose the options letter within ``. {images}{question} Lets think step by step and then answer with the options letter from the given choices. Enclose the options letter within ``. {image with visual prompt}{question} Answer with the options letter from the given choices directly. Enclose the options letter within ``. Table 4: Prompts used for different evaluation settings. For the visual prompting strategy, {image with visual prompt} refers to the image input combined with visual prompt drawn using PATS [42], as illustrated in Figs. 8, 9, 10, 11. Figure 8: Illustration of the visual prompt utilized in our experiments. Figure 9: Illustration of the visual prompt utilized in our experiments. The specific prompt used for evaluating the reasoning process is shown in Table 5. The prompt used for evaluating reasoning processes with human-annotated reference is shown in Table 6. Figure 10: Illustration of the visual prompt utilized in our experiments. Figure 11: Illustration of the visual prompt utilized in our experiments. Prompt {images}{question} {reasoning process to be evaluated} {the correct answer} You are given images, question about the images, the correct answer, and reasoning process to be evaluated. Your task is to evaluate the reasoning process and determine whether it is entirely correct. If there are any errors, identify the type of error that most critically led to the incorrect answer by selecting the corresponding category number. If the reasoning process is completely correct, select only category 5. Categories: 1. Grounding error: Failure to accurately detect, locate, or reference objects or elements in the image. 2. Overlap identification and scene reconstruction error: Mistakes in identifying overlaps between elements or errors in reconstructing spatial relationships within the scene. 3. Situation transformation reasoning error: Errors in reasoning about directions or spatial relations from the perspective of an entity other than the observer (i.e., allocentric transformation errors). 4. Spatial logic error: Flaws in spatial reasoning logic, such as fabrication, linguistic intelligence error, misusing reference objects in motion reasoning, or other errors not covered by categories 1, 2, or 3. 5. Completely correct reasoning. Please output only the relevant category number. Table 5: Prompt used for evaluating the reasoning process in our study. D.2 Human Evaluation Setup Human performance is measured as the average accuracy of five adult participants who were not involved in the data annotation process. During evaluation, human annotators are provided with both 14 Prompt {images}{question} {reasoning process to be evaluated} {the correct answer} {human-annotated reference reasoning process} You are given images, question about the images, the correct answer, human-annotated reference reasoning process, and reasoning process to be evaluated. Your task is to evaluate the reasoning process and determine whether it is entirely correct. If there are any errors, identify the type of error that most critically led to the incorrect answer by selecting the corresponding category number. If the reasoning process is completely correct, select only category 5. Categories: 1. Grounding error: Failure to accurately detect, locate, or reference objects or elements in the image. 2. Overlap identification and scene reconstruction error: Mistakes in identifying overlaps between elements or errors in reconstructing spatial relationships within the scene. 3. Situation transformation reasoning error: Errors in reasoning about directions or spatial relations from the perspective of an entity other than the observer (i.e., allocentric transformation errors). 4. Spatial logic error: Flaws in spatial reasoning logic, such as fabrication, linguistic intelligence error, misusing reference objects in motion reasoning, or other errors not covered by categories 1, 2, or 3. 5. Completely correct reasoning. Note: The human-annotated reference reasoning process is not the only correct answer and is for reference only. Other reasoning paths may also be correct. You should judge the correctness of the reasoning process and the error category based on the specific images. Please output only the relevant category number. Table 6: Prompt used for evaluating reasoning processes with human-annotated reference in our study. the questions and the corresponding images simultaneously and are allowed unlimited time to answer each question to the best of their ability. They may review the images as many times as needed to gather comprehensive information. The evaluation interface supports two modes of image viewing: all images can be displayed side by side for direct comparison, or the participant can switch between individual images as needed. This flexible setup ensures that evaluators can fully utilize the available visual information for accurate reasoning and decision-making. D.3 Complete Evaluation Results of Prompting Techniques on MMSI-Bench We provide the complete evaluation results of prompting techniques on MMSI-Bench in Table 7, which correspond to the results presented in Figure 5 of the main text."
        },
        {
            "title": "E Details of the Data Curation Process",
            "content": "E.1 Dataset Sources ScanNet [18] : ScanNet is large-scale RGB-D video dataset designed for 3D scene understanding in real-world indoor environments. It consists of 2.5 million RGB-D images across 1,513 scans from 707 unique spaces, each annotated with 3D camera poses, surface reconstructions, and dense objectlevel semantic segmentations. The dataset was collected using scalable pipeline that combines easy data capture with crowdsourced annotations, making it one of the most richly-labeled and comprehensive resources for training and evaluating deep learning models on tasks such as 3D object classification and semantic voxel labeling. nuScenes [13]: nuScenes is multimodal autonomous driving dataset that features complete sensor suite, including 6 cameras, 5 radars, and 1 lidar, all with 360-degree coverage. It contains 1,000 driving scenes, each 20 seconds long, fully annotated with 3D bounding boxes for 23 object classes and 8 attributes. nuScenes stands out for its diversity of sensor data, large scale of annotations, 15 Models Positional Relationship Attribute Motion Cam.Cam.Obj.Obj. Reg.Reg. Cam.Obj. Obj.Reg. Cam.Reg. Meas. Appr. Cam. Obj. MSR Avg. The original MMSI-Bench setting without the use of any prompting techniques. GPT-4.5 GPT-4o Qwen2.5-VL-72B LLaVA-OneVision-72B 39.5 23.5 34.6 33.3 29.8 24.5 34.0 31.9 34.4 34.4 25.8 43. 47.1 37.6 34.1 37.6 51.2 19.8 23.3 30.2 55.4 27.7 36.1 38.6 Linguistic prompting GPT-4.5 GPT-4o Qwen2.5-VL-72B LLaVA-OneVision-72B 44.1 42.1 28.9 31.7 28.7 24.6 30.8 24. 35.8 22.7 34.5 26.5 32.6 25.9 32.5 31.3 40.0 34.5 24.6 22.1 43.4 37.8 30.0 27.3 39.1 32.8 45.3 28.1 57.8 48.0 40.5 25. 33.3 31.8 27.3 19.7 31.8 35.9 28.7 20.1 Concatenating multiple images into single composite image without using any prompting techniques. GPT-4.5 GPT-4o Qwen2.5-VL-72B LLaVA-OneVision-72B 28.9 31.3 28.9 27.7 27.3 19.7 16.7 19.7 34.6 38.3 34.6 30. 43.8 32.8 37.5 32.8 30.8 22.3 22.3 24.5 43.5 41.2 35.3 37.6 35.5 28.0 36.6 32.3 33.7 24.4 18.6 26.7 Visual prompting 36.6 GPT-4.5 28.7 GPT-4o Qwen2.5-VL-72B 30.9 LLaVA-OneVision-Qwen2-72B 29. 31.9 29.5 27.4 33.0 30.9 28.8 30.4 29.6 31.4 25.0 28.7 31.4 42.4 37.0 36.1 38.8 27.7 30.6 32.1 33.7 37.5 40.4 30.5 29. 31.8 27.1 25.0 21.2 41.9 35.1 27.0 13.5 41.9 30.4 22.9 13.4 32.4 21.6 23.0 9.5 24.3 16.4 19.7 14.9 40.8 36.8 30.3 32. 36.8 29.6 31.5 8.0 26.3 30.3 31.6 26.3 28.9 30.5 29.7 26.3 36.4 30.8 27.3 15.7 42.4 28.8 23.6 23.2 28.3 26.3 31.3 29. 30.8 29.2 27.1 28.3 40.3 30.3 30.7 28.4 39.6 32.0 29.0 24.7 32.6 28.5 29.1 27.6 32.1 29.3 28.8 29.0 Table 7: Complete Evaluation Results of Prompting Techniques on MMSI-Bench. and support for advanced tasks such as 3D detection and tracking, making it benchmark for research in perception for autonomous vehicles. Matterport3D [14]: Matterport3D is large-scale RGB-D dataset capturing 10,800 panoramic viewpoints from 194,400 images of 90 building-scale indoor scenes. It provides surface reconstructions, globally aligned camera poses, and detailed 2D and 3D semantic segmentations at both region and object levels. The comprehensive panoramic coverage and precise global alignment enable wide range of computer vision tasks, including keypoint matching, semantic segmentation, and view overlap prediction, particularly in the context of indoor scene understanding. Ego4D [23]: Ego4D is an unprecedented egocentric video dataset, comprising 3,670 hours of firstperson footage collected by 931 participants across 74 locations in 9 countries. The dataset covers wide array of daily-life scenarios and includes supplementary data such as audio, 3D meshes, gaze, and multi-camera views. With millions of annotations supporting various benchmark tasks, Ego4D enables research into first-person perception, episodic memory, and social interactions, significantly advancing the field of egocentric visual understanding. AgiBot-World [9]: AgiBot-World is large-scale robotic manipulation dataset featuring over 1 million trajectories across 217 tasks in five real-world domainsincluding domestic, retail, industrial, restaurant, and office environmentscollected by more than 100 real robots. Data collection is standardized and includes human-in-the-loop verification to ensure high quality and diversity. The dataset supports wide range of hardware, from simple grippers to dexterous hands with visuo-tactile sensors, and provides rich multimodal annotations. AgiBot-World enables the development and evaluation of generalist robot policies in complex, long-horizon, and diverse scenarios, moving towards scalable, general-purpose robotic intelligence. DTU [26]: The DTU dataset is large-scale multi-view stereo (MVS) benchmark designed for 3D reconstruction research. It contains 80 scenes with significant variability in surface properties and geometric complexity. Each scene is captured from 49 or 64 accurate camera positions and includes high-quality reference scans obtained via structured light. This dataset advances the scale and diversity compared to previous MVS datasets and serves as challenging benchmark for evaluating the accuracy and completeness of stereo reconstruction algorithms. DAVIS 2017 [48]: DAVIS 2017 is benchmark dataset for video object segmentation, consisting of 150 video sequences and total of 10,459 annotated frames with pixel-level masks for multiple objects per scene. Compared to earlier versions, DAVIS 2017 introduces greater complexity, including multiple objects, occlusions, small and fine structures, and fast motion. The dataset has driven significant progress in video object segmentation and is widely used for benchmarking and competition in the field. 16 Figure 12: Screenshot of the data annotation interface, where annotators select images, design questions, and provide answers and reasoning. Figure 13: Screenshot of the quality control interface, where reviewers decide whether to retain or discard data and specify the reasons for data issues. Waymo Open Dataset [54]: The Waymo Open Dataset is one of the largest and most diverse multimodal datasets for autonomous driving. It comprises 1,150 scenes, each lasting 20 seconds, captured across multiple cities with extensive geographic coverage. Each scene includes synchronized"
        },
        {
            "title": "Percentage",
            "content": "Matterport3D ScanNet Ego4D AgiBot-World DTU nuScenes DAVIS 2017 Waymo 463 280 67 45 72 39 29 5 46.30% 28.00% 6.70% 4.50% 7.20% 3.90% 2.90% 0.50% Table 8: Sample counts and proportions for each category and calibrated high-resolution camera images and LiDAR data, along with exhaustive 2D and 3D bounding box annotations and consistent object tracking IDs. The dataset supports research in detection, tracking, and sensor fusion, providing valuable foundation for developing robust and generalizable self-driving technologies. The distribution of data sources used for each question in MMSI-Bench is shown in Table 8. E.2 Data Annotation and Quality Control Interfaces The interfaces for data annotation and quality control are illustrated in Figure 12 and Figure 13, respectively. During annotation, annotators select an image, design question, provide the correct answer, and write the corresponding reasoning process. For quality control, reviewers can choose to discard data entry if any issues are identified, providing the reason for rejection; otherwise, they can approve and retain the data. E.3 General Guidelines MMSI-Bench is designed to systematically evaluate the multi-image spatial reasoning capabilities of multimodal large language models (MLLMs) using human-readable questions. The following guidelines govern the data annotation process: Human-Centric Design: All questions and answer choices are formulated in natural, human-readable English, avoiding technical prompts such as direct requests for camera parameters. Image Requirement: Every question is constructed so that it cannot be answered using single image alone; the information required to answer must be synthesized from multiple images. Question Type and Format: All questions are multiple-choice with four options, only one of which is correct. Questions are created in free form (no templates) to ensure diversity and challenge for MLLMs. Category Coverage: Each question is assigned to one of eleven categories, covering positional relationships, attributes, motion, and multi-step reasoning among cameras, objects, and regions. This taxonomy is designed to comprehensively span the space of fundamental spatial reasoning types. Difficulty, Diversity, and Clarity: Questions must be of high difficulty, unambiguous, and answerable by humans. Annotators are encouraged to maximize diversity in question design and avoid repetitive patterns. Annotations must be accurate, consistent, and adhere to high standard of academic rigor. Ethics and Licensing: All images and question sources must comply with copyright and licensing restrictions. Annotators must not use material from sources that prohibit redistribution. Reasoning Process Annotation: For each question, annotators provide detailed step-bystep reasoning process, documenting the logical path from the images to the answer. 18 E.4 Data Format and Structure To ensure consistency and facilitate reliable evaluation and analysis, all benchmark entries conform to standardized data structure: JSON Format: Each question-answer pair is stored as JSON object, with fields for question text, four answer options, the correct answer index, category, difficulty, image paths, and annotated reasoning process. Image Files: All referenced images are stored separately and linked via file paths in the JSON entries. Field Completeness: Each entry must have all fields completed: question, answer, category, image list, and reasoning process. Multi-Image Structure: Except for the Multi-Step Reasoning category, every question is associated with exactly two images. Multi-Step Reasoning samples may include more images according to task complexity. Final Storage Format: For efficient downloading and usage, the entire datasetincluding binary-encoded images and metadatais stored in the Parquet format. E.5 Quality Control and Validation rigorous multi-stage quality control procedure is implemented to ensure the reliability and validity of the benchmark: Independent Review: All annotated samples are independently reviewed by three expert reviewers who were not involved in the original annotation process. Error and Ambiguity Filtering: Reviewers systematically remove or revise questions exhibiting insufficient visual information, ambiguity (linguistic or visual), incorrect answers, or vague/incomplete reasoning processes. Non-Triviality Check: Samples that can be answered using single image or general common sense (rather than spatial reasoning across images) are excluded. Iterative Feedback: Annotators receive feedback on problematic items and are required to revise or replace such entries. Final Audit: Only questions passing all review stages are included in the benchmark, ensuring high data quality and robust evaluation. Representative MMSI-Bench Samples from Each Category (Full Version) For clarity and readability, questions and rationales in the main text are simplified; the complete versions for each category are provided in this section. We present one full example for each of the 11 categories: Position (Camera-Camera), Position (Camera-Object), Position (Camera-Region), Position (Object-Object), Position (Object-Region), Position (Region-Region), Motion (Camera), Motion (Object), Attribute (Measurement), Attribute (Appearance), and Multi-Step Reasoning. These representative samples are illustrated in Figures 1424."
        },
        {
            "title": "G Additional Case Studies",
            "content": "In this section, we present additional complete reasoning processes of current multimodal large language models (MLLMs) to illustrate their spatial reasoning deficiencies more intuitively. As shown in Figure 25, Figure 26, Figure 27, and Figure 28, we highlight four types of errors in different colors: grounding errors (blue), overlap-matching and scene-reconstruction errors (red), situation-transformation reasoning errors (orange), and spatial-logic errors (green). 19 Figure 14: Full example for Position (Camera-Camera) category. Figure 15: Full example for Position (Camera-Object) category. 20 Figure 16: Full example for Position (Camera-Region) category. Figure 17: Full example for Position (Object-Object) category. 21 Figure 18: Full example for Position (Object-Region) category. Figure 19: Full example for Position (Region-Region) category. 22 Figure 20: Full example for Motion (Camera) category. Figure 21: Full example for Motion (Object) category. 23 Figure 22: Full example for Attribute (Measurement) category. Figure 23: Full example for Attribute (Appearance) category. 24 Figure 24: Full example for Multi-Step Reasoning category. 25 Figure 25: Illustration of situation-transformation reasoning errors (highlighted in orange). 26 Figure 26: Illustration of overlap-matching and scene-reconstruction errors (highlighted in red), spatial-logic errors (highlighted in green). 27 Figure 27: Illustration of grounding errors (highlighted in blue), spatial-logic errors (highlighted in green). 28 Figure 28: Illustration of spatial-logic errors (highlighted in green)."
        },
        {
            "title": "References",
            "content": "[1] Agibot-world: The large-scale manipulation platform for scalable and intelligent embodied systems. https://github.com/OpenDriveLab/AgiBot-World. [2] Davis 2017 evaluation license. https://github.com/davisvideochallenge/davis2017evaluation/blob/master/LICENSE. [3] Dtu data terms of use. https://www.bibliotek.dtu.dk/-/media/Andre_Universitet senheder/Bibliotek/pdf/DTU-Data-Terms-of-Use_version_1.pdf. [4] Ego4d licenses draft. https://ego4d-data.org/pdfs/Ego4D-Licenses-Draft.pdf. [5] Habitat matterport3d dataset license. https://github.com/facebookresearch/habitatmatterport3d-dataset/blob/main/LICENSE. [6] nuscenes terms of use. https://www.nuscenes.org/terms-of-use. [7] Scannet license. https://github.com/ScanNet/ScanNet/blob/master/LICENSE. [8] Waymo open dataset license. https://github.com/waymo-research/waymo-opendataset/blob/master/LICENSE. [9] AgiBot-World-Contributors, Qingwen Bu, Jisong Cai, Li Chen, Xiuqi Cui, Yan Ding, Siyuan Feng, Shenyuan Gao, Xindong He, Xuan Hu, Xu Huang, Shu Jiang, Yuxin Jiang, Cheng Jing, Hongyang Li, Jialu Li, Chiming Liu, Yi Liu, Yuxiang Lu, Jianlan Luo, Ping Luo, Yao Mu, Yuehan Niu, Yixuan Pan, Jiangmiao Pang, Yu Qiao, Guanghui Ren, Cheng Ruan, Jiaqi Shan, Yongjian Shen, Chengshi Shi, Mingkang Shi, Modi Shi, Chonghao Sima, Jianheng Song, Huijie Wang, Wenhao Wang, Dafeng Wei, Chengen Xie, Guo Xu, Junchi Yan, Cunbiao Yang, Lei Yang, Shukai Yang, Maoqing Yao, Jia Zeng, Chi Zhang, Qinglin Zhang, Bin Zhao, Chengyue Zhao, Jiaqi Zhao, and Jianchao Zhu. Agibot world colosseo: large-scale manipulation platform for scalable and intelligent embodied systems. arXiv preprint arXiv:2503.06669, 2025. [10] Anthropic. Claude 3.7 sonnet and claude code. https://www.anthropic.com/news/cla ude-3-7-sonnet, 2025. [11] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [12] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Lucy Xiaoyang Shi, James Tanner, Quan Vuong, Anna Walling, Haohuan Wang, and Ury Zhilinsky. Pi0: vision-language-action flow model for general robot control. https://physicalintellig ence.company/blog/pi0, 2024. [13] Holger Caesar, Varun Bankiti, Alex Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: multimodal dataset for autonomous driving. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1162111631, 2020. [14] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-d data in indoor environments. arXiv preprint arXiv:1709.06158, 2017. [15] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In CVPR, 2024. 30 [16] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. [17] An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision-language models. In NeurIPS, 2024. [18] Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Niener. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In CVPR, 2017. [19] Google DeepMind. Gemini 2.5: Our most intelligent ai model. https://blog.google/tech nology/google-deepmind/gemini-model-thinking-updates-march-2025/, 2025. [20] Mengfei Du, Binhao Wu, Zejun Li, Xuanjing Huang, and Zhongyu Wei. Embspatial-bench: Benchmarking spatial understanding for embodied tasks with large vision-language models. arXiv:2406.05756, 2024. [21] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. In Proceedings of the 32nd ACM international conference on multimedia, pages 1119811201, 2024. [22] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. In ECCV, 2024. [23] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1899519012, 2022. [24] Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1. 5-vl technical report. arXiv preprint arXiv:2505.07062, 2025. [25] Drew A. Hudson and Christopher D. Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering, 2019. URL https://arxiv.org/abs/19 02.09506. [26] Rasmus Jensen, Anders Dahl, George Vogiatzis, Engin Tola, and Henrik Aans. Large scale multi-view stereopsis evaluation. In CVPR, 2014. [27] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, and Ross Girshick. Clevr: diagnostic dataset for compositional language and elementary visual reasoning. In CVPR, 2017. [28] Mehran Kazemi, Nishanth Dikkala, Ankit Anand, Petar Devic, Ishita Dasgupta, Fangyu Liu, Bahare Fatemi, Pranjal Awasthi, Dee Guo, Sreenivas Gollapudi, and Ahmed Qureshi. Remi: dataset for reasoning with multiple images, 2024. URL https://arxiv.org/abs/2406.0 9175. [29] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An open-source vision-language-action model. In CoRL, 2024. [30] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [31] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench-2: Benchmarking multimodal large language models, 2023. URL https: //arxiv.org/abs/2311.17092. 31 [32] Xinghang Li, Peiyan Li, Minghuan Liu, Dong Wang, Jirong Liu, Bingyi Kang, Xiao Ma, Tao Kong, Hanbo Zhang, and Huaping Liu. Towards generalist robot policies: What matters in building vision-language-action models. arXiv preprint arXiv:2412.14058, 2024. [33] Yuan-Hong Liao, Rafid Mahmood, Sanja Fidler, and David Acuna. Reasoning paths with reference objects elicit quantitative spatial reasoning in large vision-language models. arXiv:2409.09788, 2024. [34] Fangyu Liu, Guy Emerson, and Nigel Collier. Visual spatial reasoning. TACL, 2023. [35] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In CVPR, 2024. [36] Haowei Liu, Xi Zhang, Haiyang Xu, Yaya Shi, Chaoya Jiang, Ming Yan, Ji Zhang, Fei Huang, Chunfeng Yuan, Bing Li, and Weiming Hu. Mibench: Evaluating multimodal large language models over multiple images, 2024. URL https://arxiv.org/abs/2407.15272. [37] Zhijian Liu, Ligeng Zhu, Baifeng Shi, Zhuoyang Zhang, Yuming Lou, Shang Yang, Haocheng Xi, Shiyi Cao, Yuxian Gu, Dacheng Li, Xiuyu Li, Yunhao Fang, Yukang Chen, Cheng-Yu Hsieh, De-An Huang, An-Chieh Cheng, Vishwesh Nath, Jinyi Hu, Sifei Liu, Ranjay Krishna, Daguang Xu, Xiaolong Wang, Pavlo Molchanov, Jan Kautz, Hongxu Yin, Song Han, and Yao Lu. Nvila: Efficient frontier visual language models, 2024. URL https://arxiv.org/abs/2412.044 68. [38] Ruiyuan Lyu, Tai Wang, Jingli Lin, Shuai Yang, Xiaohan Mao, Yilun Chen, Runsen Xu, Haifeng Huang, Chenming Zhu, Dahua Lin, and Jiangmiao Pang. Mmscan: multi-modal 3d scene dataset with hierarchical grounded language annotations. In NeurIPS, 2024. [39] Wufei Ma, Haoyu Chen, Guofeng Zhang, Celso de Melo, Jieneng Chen, and Alan Yuille. 3dsrbench: comprehensive 3d spatial reasoning benchmark. arXiv preprint arXiv:2412.07825, 2024. [40] Fanqing Meng, Jin Wang, Chuanhao Li, Quanfeng Lu, Hao Tian, Jiaqi Liao, Xizhou Zhu, Jifeng Dai, Yu Qiao, Ping Luo, et al. Mmiu: Multimodal multi-image understanding for evaluating large vision-language models. arXiv preprint arXiv:2408.02718, 2024. [41] Meta. Llama 3.2: Revolutionizing edge ai and vision with open, customizable models. https: //ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/, 2024. [42] Junjie Ni, Yijin Li, Zhaoyang Huang, Hongsheng Li, Hujun Bao, Zhaopeng Cui, and Guofeng Zhang. Pats: Patch area transportation with subdivision for local feature matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1777617786, 2023. [43] OpenAI. Gpt-4o. https://openai.com/index/hello-gpt-4o/, 2024. [44] OpenAI. Introducing gpt-4.1 in the api. https://openai.com/index/gpt-4-1/, 2025. [45] OpenAI. Introducing gpt-4.5. https://openai.com/index/introducing-gpt-4-5/, 2025. [46] OpenAI. Introducing o3 and o4 mini. https://openai.com/index/introducing-o3and-o4-mini/, 2025. [47] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv:2306.14824, 2023. [48] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbelez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint arXiv:1704.00675, 2017. [49] Santhosh Kumar Ramakrishnan, Erik Wijmans, Philipp Kraehenbuehl, and Vladlen Koltun. Does spatial cognition emerge in frontier models? arXiv:2410.06468, 2024. 32 [50] Arijit Ray, Jiafei Duan, Reuben Tan, Dina Bashkirova, Rose Hendrix, Kiana Ehsani, Aniruddha Kembhavi, Bryan Plummer, Ranjay Krishna, Kuo-Hao Zeng, et al. Sat: Spatial aptitude training for multimodal language models. arXiv preprint arXiv:2412.07755, 2024. [51] ByteDance Seed. Doubao-1.5-pro. https://seed.bytedance.com/en/special/doubao _1_5_pro, 2025. [52] Fatemeh Shiri, Xiao-Yu Guo, Mona Far, Xin Yu, Reza Haf, and Yuan-Fang Li. An empirical analysis on spatial reasoning capabilities of large multimodal models. 2024. [53] Chonghao Sima, Katrin Renz, Kashyap Chitta, Li Chen, Hanxue Zhang, Chengen Xie, Jens Beiwenger, Ping Luo, Andreas Geiger, and Hongyang Li. Drivelm: Driving with graph visual question answering. In ECCV, 2024. [54] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, Vijay Vasudevan, Wei Han, Jiquan Ngiam, Hang Zhao, Aleksei Timofeev, Scott Ettinger, Maxim Krivokon, Amy Gao, Aditya Joshi, Yu Zhang, Jonathon Shlens, Zhifeng Chen, and Dragomir Anguelov. Scalability in perception for autonomous driving: Waymo open dataset. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020. [55] Kexian Tang, Junyao Gao, Yanhong Zeng, Haodong Duan, Yanan Sun, Zhening Xing, Wenran Liu, Kaifeng Lyu, and Kai Chen. Lego-puzzles: How good are mllms at multi-step spatial reasoning? arXiv preprint arXiv:2503.19990, 2025. [56] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024. [57] Fei Wang, Xingyu Fu, James Y. Huang, Zekun Li, Qin Liu, Xiaogeng Liu, Mingyu Derek Ma, Nan Xu, Wenxuan Zhou, Kai Zhang, Tianyi Lorena Yan, Wenjie Jacky Mo, Hsiang-Hui Liu, Pan Lu, Chunyuan Li, Chaowei Xiao, Kai-Wei Chang, Dan Roth, Sheng Zhang, Hoifung Poon, and Muhao Chen. Muirbench: comprehensive benchmark for robust multi-image understanding, 2024. URL https://arxiv.org/abs/2406.09411. [58] Tai Wang, Xiaohan Mao, Chenming Zhu, Runsen Xu, Ruiyuan Lyu, Peisen Li, Xiao Chen, Wenwei Zhang, Kai Chen, Tianfan Xue, Xihui Liu, Cewu Lu, Dahua Lin, and Jiangmiao Pang. Embodiedscan: holistic multi-modal 3d perception suite towards embodied ai. In CVPR, 2024. [59] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [60] Tsung-Han Wu, Giscard Biamby, Jerome Quenum, Ritwik Gupta, Joseph E. Gonzalez, Trevor Darrell, and David M. Chan. Visual haystacks: vision-centric needle-in-a-haystack benchmark, 2025. URL https://arxiv.org/abs/2407.13766. [61] Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, Zhenda Xie, Yu Wu, Kai Hu, Jiawei Wang, Yaofeng Sun, Yukun Li, Yishi Piao, Kang Guan, Aixin Liu, Xin Xie, Yuxiang You, Kai Dong, Xingkai Yu, Haowei Zhang, Liang Zhao, Yisong Wang, and Chong Ruan. Deepseek-vl2: Mixture-of-experts vision-language models for advanced multimodal understanding, 2024. URL https://arxiv.org/abs/2412.10302. [62] Runsen Xu, Zhiwei Huang, Tai Wang, Yilun Chen, Jiangmiao Pang, and Dahua Lin. Vlmgrounder: vlm agent for zero-shot 3d visual grounding. 2024. [63] Runsen Xu, Weiyao Wang, Hao Tang, Xingyu Chen, Xiaodong Wang, Fu-Jen Chu, Dahua Lin, Matt Feiszli, and Kevin J. Liang. Multi-spatialmllm: Multi-frame spatial understanding with multi-modal large language models. arXiv preprint arXiv:2505.17015, 2025. 33 [64] Runsen Xu, Weiyao Wang, Hao Tang, Xingyu Chen, Xiaodong Wang, Fu-Jen Chu, Dahua Lin, Matt Feiszli, and Kevin J. Liang. Multi-spatialmllm: Multi-frame spatial understanding with multi-modal large language models, 2025. URL https://arxiv.org/abs/2505.17015. [65] Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. arXiv:2412.14171, 2024. [66] Wenyu Zhang, Wei En Ng, Lixin Ma, Yuwen Wang, Jungqi Zhao, Boyang Li, and Lu Wang. Sphere: hierarchical evaluation on spatial perception and reasoning for vision-language models. arXiv preprint arXiv:2412.12693, 2024. [67] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv:2504.10479, 2025. [68] Yiming Zuo, Karhan Kayan, Maggie Wang, Kevin Jeon, Jia Deng, and Thomas Griffiths. Towards foundation models for 3d vision: How close are we? arXiv preprint arXiv:2410.10799, 2024."
        }
    ],
    "affiliations": [
        "Beijing Normal University",
        "Shanghai AI Laboratory",
        "Shanghai Jiaotong University",
        "The Chinese University of Hong Kong",
        "Tsinghua University",
        "University of Hong Kong",
        "Zhejiang University"
    ]
}