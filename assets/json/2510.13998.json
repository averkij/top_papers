{
    "paper_title": "BitNet Distillation",
    "authors": [
        "Xun Wu",
        "Shaohan Huang",
        "Wenhui Wang",
        "Ting Song",
        "Li Dong",
        "Yan Xia",
        "Furu Wei"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this paper, we present BitNet Distillation (BitDistill), a lightweight pipeline that fine-tunes off-the-shelf full-precision LLMs (e.g., Qwen) into 1.58-bit precision (i.e., ternary weights {-1, 0, 1}) for specific downstream tasks, achieving strong task-specific performance with minimal computational cost. Specifically, BitDistill incorporates three key techniques: the SubLN module, as introduced in BitNet; multi-head attention distillation, based on MiniLM; and continual pre-training, which serves as a crucial warm-up step to mitigate the scalability issue of the performance gap between finetuned full-precision and 1.58-bit LLMs on specific tasks. Experimental results show that BitDistill achieves performance comparable to the full-precision counterpart models across model size, while enabling up to 10x memory savings and 2.65x faster inference on CPUs. Code is available at https://github.com/microsoft/BitNet."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 8 9 9 3 1 . 0 1 5 2 : r a"
        },
        {
            "title": "BitNet Distillation",
            "content": "Xun Wu Shaohan Huang Wenhui Wang Ting Song Li Dong Yan Xia Furu Wei Microsoft Research https://aka.ms/GeneralAI"
        },
        {
            "title": "Abstract",
            "content": "In this paper, we present BitNet Distillation (BitDistill), lightweight pipeline that fine-tunes off-the-shelf full-precision LLMs (e.g., Qwen) into 1.58bit precision (i.e., ternary weights {-1, 0, 1}) for specific downstream tasks, achieving strong task-specific performance with minimal computational cost. Specifically, BitDistill incorporates three key techniques: the SubLN module, as introduced in BitNet [WMD+23]; multi-head attention distillation, based on MiniLM [WBH+20]; and continual pre-training, which serves as crucial warm-up step to mitigate the scalability issue of the performance gap between finetuned full-precision and 1.58-bit LLMs on specific tasks. Experimental results show that BitDistill achieves performance comparable to the fullprecision counterpart models across model size, while enabling up to 10 memory savings and 2.65 faster inference on CPUs. Code is available at github.com/microsoft/BitNet. Figure 1: Performance on downstream tasks across model size, with inference speed and memory efficiency comparison. We observed that directly finetuning full-precision LLMs into 1.58-bit LLMs (denoted as 1.58-bit BitNet-SFT) leads to notable performance gap compared to the FP16 baseline, and this gap remains or even widens as the model size increases. In contrast, BitDistill preserves scalability, resulting in performance comparable to full-precision counterparts across all model size, while reducing 10 memory usage and 2.65 faster inference on CPUs. Corresponding author."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) [AAA+23, GYZ+25] have become indispensable not only in advancing general natural language processing [YLY+25], but more importantly in powering wide range of downstream applications, such as recommendation [WZQ+24, HZL+24, RWX+24], classification [KDSP25, SLL+23], and retrieval [ZSB+24, BMH+22]. Despite their broad applicability, deploying LLMs in downstream applications remains highly challenging. The rapid escalation in model size further amplifies these challenges, especially on resource-constrained devices (e.g., smartphones), where both memory consumption and computational overhead become prohibitive. To address these challenges, recent efforts on extreme low-bit LLMs, such as the 1.58-bit (i.e., ternary values {-1, 0, 1}) BitNet [MWM+24, MWH+25, WMD+23], aim to dramatically reduce memory footprint and accelerate inference, offering promising avenue for efficient deployment in downstream applications. However, achieving competitive accuracy on downstream applications with 1.58-bit BitNet generally requires pretraining from scratch on large-scale corpora [TXL+25, MWH+25] first, resulting in substantial computational and energy overhead. Furthermore, as illustrated in Figure 1, directly applying quantization-aware training (QAT) [DZC+24, CSX+24] to existing full-precision LLMs at 1.58-bit for specific downstream tasks is often unstable, fails to fully preserve the performance of their full-precision counterparts, and exhibit an poor scalability: as model size increases from 0.6B to 4B, the performance gap relative to the full-precision baseline grows from 13.9 to 15.3. This highlights the pressing need for more effective QAT methods specifically designed for 1.58-bit BitNet. In this work, we focus on fine-tuning existing LLMs to 1.58-bit for specific downstream tasks, while achieving performance comparable to their full-precision counterparts. To this end, we propose BitNet Distillation (BitDistill), scaling-friendly QAT framework designed to bridge the gap between extreme 1.58-bit quantization and practical deployment. BitDistill comprises three stages: (i) modeling refinement with SubLN module [WMD+23] for stable optimization, (ii) continued pre-training to mitigate scale-related performance gaps, and (iii) MiniLMbased [WWD+20, WBH+20] multi-head attention distillation to recover full-precision accuracy. Through extensive evaluations across four benchmarks and diverse model scales, we demonstrate that BitDistill scales effectively, achieving downstream task performance on par with fullprecision baselines. At the same time, as shown in Figure 1, it reduces 10 memory savings and 2.65 faster inference on CPUs, offering significant improvements in latency, throughput, memory efficiency, and energy consumption, which makes it particularly well-suited for deployment on resource-constrained hardware. Specifically, this work makes the following contributions: 1. To the best of our knowledge, we are the first to investigate fine-tuning pre-trained full-precision LLMs into 1.58-bit BitNet for specific downstream tasks, and we identify key challenges including: performance degradation, poor scalability, and training instability. 2. To address these challenges, we propose tailored distillation framework named BitDistill, which comprises three key techniques: the SubLN module, as introduced in BitNet [WMD+23]; multi-head attention distillation, based on MiniLM [WBH+20]; and continual pre-training, which serves as crucial warm-up step to mitigate the scalability issue of the performance gap between finetuned full-precision and 1.58-bit LLMs on specific tasks. 3. Extensive experiments across multiple benchmarks and model scales show that BitDistill enables 1.58-bit quantized LLMs to achieve downstream performance comparable to their fullprecision counterparts, while enabling up to 10 memory savings and 2.65 faster inference on CPUs."
        },
        {
            "title": "2 Preliminaries",
            "content": "1.58-bit Quantization. absmean function to map the weights of existing LLMs into ternary values, i.e., {-1, 0, 1}: Following [MWM+24], we adopt per-tensor quantization using the Qw(W) = RoundClip( WFP16 + ϵ , 1, 1), where = mean(W), RoundClip(Y, a, b) = min (max (Y, a) , b) , (1) (2) 2 The notation means the nearest rounding operation. For LLM inputs, we employ 8-bit activation quantization. Specifically, we use per-token absmax and absmean functions to quantize the activations, which can be formulated as: QINT8(X) = γ RoundClip( 127 γ + ϵ XFP16, 128, 127), γ = max(XFP16) (3) Gradient Approximation. Due to the presence of non-differentiable operations in Eq. 2 and Eq. 3 (e.g., RoundClip), the gradient cannot be propagated through the entire model during backpropagation. Following [MWM+24, MWH+25, WMD+23], we employ the Straight-Through Estimator (STE) [BLC13] to approximate gradients for 1.58-bit quantized LLMs."
        },
        {
            "title": "Tasks",
            "content": "In this work, we address the challenge of deploying LLMs on resource-constrained devices for specific downstream tasks. We focus on efficiently compressing existing pre-trained LLMs to 1.58-bit BitNet with minimal performance degradation and training cost. Our proposed BitNet (1) Modeling refinement with Distillation (BitDistill) incorporates three key stages: SubLN [WMD+23] for stable optimization (detailed in 3.1), (2) Continue pre-training as crucial warm-up step to mitigate the performance gap that does not scale well between fine-tuned full-precision models and 1.58-bit BitNet (see in 3.2), and (3) Distillation-based fine-tuning, which leverages both logits distillation and multi-head attention distillation to recover full-precision performance (see 3.3). 3.1 Stage-1: Modeling Refinement Unlike full-precision models, where the variance of hidden states is typically preserved within stable range under standard initialization schemes, low-bit quantized models such as 1.58-bit LLMs often suffer from excessively large activation variance, which results in optimization instability and degraded convergence [MWM+24, WMD+23]. To alleviate this issue, following the design principles of prior 1.58-bit BitNet [MWM+24, MWH+25], we introduce additional normalization layers named SubLN at carefully chosen positions inside each transformer block. Specifically, instead of only applying pre-normalization at the block input, we further insert SubLN right before the output projection of the Multi-Head SelfAttention (MHSA) module as well as before the output projection of the Feed-Forward Network (FFN). Concretely, taking Qwen3 [YLY+25] as reference architecture, the computations at the l-th transformer layer are modified as: Yl = Xl + SubLN(cid:0)Concat(heads)(cid:1)WMHSA out ) σ(YlWFFN Xl+1 = Yl + SubLN(cid:0)( YlWFFN , gate )(cid:1)WFFN down, up (4) (5) where heads = (cid:110) Softmax (cid:18) QiK (cid:19) Vi (cid:12) (cid:12) Qi = XWMHSA (cid:12) Q,i , Ki = XWMHSA K,i , Vi = XWMHSA V,i (cid:111) , (6) where the outer SubLN in each equation corresponds to the newly inserted normalization before the respective output projection. This design ensures that the hidden representations entering quantized projection layers are variance-stabilized, preventing the explosion of activation scale and thereby improving both training stability and task performance. 3.2 Stage-2: Continue Pre-Training As shown in Figure 1, directly fine-tuning 1.58-bit BitNet modified from existing full-precision LLMs on downstream tasks may yield suboptimal results, as the limited number of training tokens is often insufficient to effectively adapt full-precision weights into the constrained 1.58-bit representation, which leads to exhibit poor scalability: as model size increases, the performance gap relative to the full-precision baseline widens. 3 To this end, we propose two-stage training pipeline consisting of continue training stage, which leverages only small amount of pretraining corpus to achieve the desired adaptation, followed by fine-tuning on the downstream task. Specifically, given small set of corpus = {c1, , cN }, we finetuning the modeling-modified pre-trained LLMs attained from 3.1 as: LCT ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) Ti(cid:88) i=1 t=1 log Pθ(ci,t ci,<t) . (7) Here Pθ denotes the probability distribution parameterized by the model. detailed analysis of the effect of continue training, along with an investigation into the underlying mechanisms and supporting hypotheses, can be found in 4.4. 3.3 Stage-3: Distillation-based Fine-tuning To better mitigate the performance degradation introduced by precision reduction, we incorporate two kinds of knowledge distillation technology into the downstream task finetuning phase, where the fine-tuned full-precision LLMs serves as the teacher and its 1.58-bit quantized counterpart acts as the student. Logits Distillation. Logits distillation has recently been widely adopted in the QAT phase of quantized models, demonstrating promising effectiveness [DZC+24, LSK+25, KKCY24]. Given data pairs {(xi, yi)}N i=1 sampled from downstream datasets, the objective of logits distillation is defined as LLD = 1 N (cid:88) i=1 DKL (cid:0)P FP16 θ (yi xi) (cid:13) (cid:13) 1.58-bit θ (yi xi)(cid:1) , () θ (y x) = exp(zy(x; θ)/τ ) exp(zy(x; θ)/τ ) (cid:80) (8) (9) Here zy(x; θ) denotes the unnormalized logit produced by the model for when given the input x. The temperature parameter τ is introduced to control the softening of the output distributions for both the FP16 and 1.58-bit models. DKL( ) represents the KullbackLeibler divergence. Multi-Head Attention Distillation,. Since the attention mechanism plays pivotal role in LLMs and largely determines their overall performance, we further investigate distillation at the attention layer to encourages the 1.58-bit student to capture the fine-grained structural dependencies embedded in the FP16 teachers attention patterns. Following MiniLM series [WBH+20, WWD+20], given training samples drawn from the downstream dataset, we define the attention-relations distillation loss LAD as A() Φ, Φ = {Q, K, V}, Ar(cid:88) (cid:88) Φ (cid:88) Υ (cid:88) LAD = 1 Υ αi 1 Arx i= j=1 a=1 t=1 DKL(RFP16 i,j,a,t R1.58-bit i,j,a,t ). (10) (11) Here Φ correspond to the query, key, and value projections within multi-head attention block, and Υ denotes the set of layers we selected for distillation. αi are coefficients controlling the relative weights of different relational terms. The sequence length is denoted by x, Ar is the number of attention heads. The relational distribution R() i,j,a,t is derived by applying scaled dot-product attention followed by Softmax with hidden dimension dr, while R1.58-bit i,j,a,t is obtained analogously from the quantized student model using hidden dimension r, i.e., (cid:32) RFP i,j,a,t = Softmax (cid:33) AFP16 i,j,a,tAFP16 i,j,a,t dr , R1.58-bit i,j,a,t = Softmax (cid:32) A1.58-bit i,j,a,t A1.58-bit i,j,a,t (cid:112)d (cid:33) . (12) The detailed implement of LAD can be found in Algorithm 1. Following MiniLM [WWD+20, WBH+20], we recommend performing attention distillation at only single layer (i.e., Υ = 1) rather than across all layers, as conferring greater optimization flexibility to the 1.58-bit student BitNet often yields superior downstream performance. 4 Algorithm 1 Pseudo Torch Style Implement of LAD def compute_attention_distillation_loss(student_states, teacher_states, distill_layer, split_heads): # student_states [3, B, num_heads, seq_len, head_dim]: Q, K, states from the 1.58-bit model # teacher_states [3, B, num_heads, seq_len, head_dim]: Q, K, states from the FP16 model # distill_layer [1]: the index of layers used for distillation # split_heads [1]: the number of heads when computing attention relation matrix _, B, heads, L, = student_states.shape = heads * // split_heads # Loop for computing distillation loss across Q, K, for in range(3): s_values, t_values = student_states[i], teacher_states[i] s_values = F.normalize(s_values.transpose(1, 2).reshape(B, L, split_head, D).transpose(1, 2), dim=-1) t_values = F.normalize(t_values.transpose(1, 2).reshape(B, L, split_head, D).transpose(1, 2), dim=-1) # Compute relation martix s_relation = torch.matmul(s_values, s_values.transpose(-2, -1)) t_relation = torch.matmul(t_values, t_values.transpose(-2, -1)) # Reshape: [B, split_heads, L, L] -> [B*split_heads*L, L] s_relation = (s_relation / temperature).reshape(-1, L) t_relation = (t_relation / temperature).reshape(-1, L) s_prob = F.softmax(s_relation, dim=-1).clamp(min=1e-8) t_prob = F.softmax(t_relation, dim=-1).clamp(min=1e-8) distill_loss += F.kl_div(torch.log(s_prob), t_prob, reduction=\"batchmean\", log_target=False) return distill_loss The total loss of the distillation-based finetuning phase comprises three terms that aim to minimize the discrepancy between the student and teacher models and improve downstream task performance, scaled by two distillation coefficients, λ and γ, i.e., = LCE + λLLD + γLAD, where LCE = (cid:88) yi (cid:88) log Pθ (cid:0)yt xi (cid:1) . 1 (13) (14) i=1 Here LCE denotes the cross-entropy loss on the downstream dataset. λ and γ control the trade-off between distillation and model fitting. t="
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setup Datasets. We evaluate the effectiveness of our proposed method, BitDistill, on two reptext classification and text summarization. For classification, we adopt resentative tasks: three widely used datasets from the General Language Understanding Evaluation (GLUE) benchmark [WSM+19]*: the Multi-Genre Natural Language Inference Corpus (MNLI) [WNB18], the Question-answering Natural Language Inference dataset (QNLI) [RZLL16], and the Stanford Sentiment Treebank (SST-2) [SPW+13]. These datasets are employed for both training and evaluation to comprehensively assess the effectiveness of our approach. For summarization, we use the CNN/- DailyMail dataset (CNNDM) [HKG+15] as both the training and evaluation corpus. Baselines for Comparison. Since our objective is to fine-tune pre-trained full-precision LLMs into 1.58-bit BitNet models for specific downstream tasks, we compare the performance of our 1.58-bit models (denoted as BitDistill) with that of FP16 models fine-tuned directly on the corresponding downstream tasks (named FP16-SFT). In addition, we also report the results of directly converting full-precision LLMs into 1.58-bit BitNet models and fine-tuning them on downstream tasks (denoted as BitNet-SFT). Training Settings. We fine-tune the Qwen3 series [YLY+25] as our base models, covering 0.6B, 1.7B, and 4B parameter scales. In addition, we investigate the impact of different base model types by conducting experiments with alternative backbones such as Gemma [TKF+25] and Qwen2.5 [QY+25]. For all baseline methods and our approach, we adopt greedy search strategy to select the optimal learning rate and training epochs. This procedure mitigates overfitting while *https://gluebenchmark.com/ https://huggingface.co/datasets/abisee/cnndailymail 5 Table 1: Results on text classification tasks. All models are initialized from the Qwen3 series [QY+25]. The top scores for each metric and dataset are highlighted in bold. The 1.58-bit BitDistill models achieve performance comparable to the FP16 baseline while providing 2 faster inference and 10 memory reduction across all datasets. denotes the FP16 teacher used in BitDistill. Method MNLI QNLI SST2 Speed Memory 0.6B 1.7B 4B 0.6B 1.7B 4B 0.6B 1.7B 4B (tokens / s) FP16-SFT BitNet-SFT BitDistill (Ours) 88.01 74.09 88. 89.61 75.27 89.53 91.48 76.11 91.40 93.72 78.32 93.66 95.00 79.54 94.82 96.02 79.97 95.93 94.21 79.92 94. 95.43 81.37 95.26 96.57 82.07 96.47 427 1,135 1,135 (G) 1.20 0.11 0.11 Table 2: Results on text summarization tasks (CNNDM dataset). All models are initialized from the Qwen3 series [QY+25]. The top scores for each metric and dataset are highlighted in bold. The 1.58-bit BitDistill models achieve performance comparable to the FP16 baseline while providing 2 faster inference and 10 memory reduction across all datasets. denotes the FP16 teacher used in BitDistill. Method FP16-SFT 13.98 BitNet-SFT 11.47 BitDistill (Ours) 14.41 BLEU ROUGE-1 ROUGE-2 ROUGE-L ROUGE-SUM AVG Speed (tokens / s) Memory (G) 40.62 37.10 40.21 17.77 13.97 17.47 27.72 24.84 27.49 37.80 33.37 37. 27.58 24.15 27.44 427 1,135 1,135 1.20 0.11 0.11 ensuring both strong downstream performance and fair comparisons across methods. We fix the maximum training sequence length to 512 tokens and the batch size to 32. All models are trained on servers equipped with 8AMD Mi300X GPUs. Specifically, we set the temperature for logits distillation (Eq. 9) to 5.0. For the classification task, we use λ = 10 and γ = 1e5 in Eq. 14, while for the summarization task, we set λ = 1 and γ = 1e3. We set αi = 1.0 for all experiments. During the continue pre-training phase described in 3.2, we further train our models using only 10B tokens sampled from the FALCON corpus [PMH+23]. Compared with the cost of pre-training 1.58-bit BitNet from scratch (approximately 4T tokens) [MWH+25], this additional cost is virtually negligible. Evaluation Settings. For both classification and summarization task, we fix the sampling parameters by setting top-p to 1.0 and the temperature to 0. Classification performance is evaluated using accuracy. For the summarization task, we set the maximum generation length to 4096 tokens. Summarization quality is assessed using BLEU [PRWZ02] and ROUGE-1, ROUGE-2, ROUGE-L and ROUGE-SUM [Lin04]. For model runtime efficiency, we report the token throughput (tokens per second) on CPU with 16 threads. 4.2 Main Results Overall Performance. The overall evaluation results on the benchmark datasets are reported in Table 1 and Table 2. Across different model sizes and tasks, the proposed 1.58-bit BitNet models trained with our distillation framework (BitDistill) demonstrate accuracy that is largely comparable to their full-precision counterparts, with only marginal differences observed in most cases. At the same time, the 1.58-bit models deliver substantial gains in system efficiency, including up to 2 inference speedup on CPUs and nearly an order-of-magnitude reduction in memory footprint. These improvements underline the practical utility of our approach for scenarios where computational resources are constrained, while also showing that aggressive quantization can be made viable with carefully designed distillation strategies. Robustness to Different Pretrained Models. To further examine the generality of our framework, we extend the evaluation by replacing the Qwen3 series with alternative base models such as Qwen2.5 [QY+25] and Gemma [TKF+25]. The results, summarized in Table 3, indicate that BitDistill consistently yields downstream performance close to that of full-precision fine-tuning https://huggingface.co/Qwen/Qwen2.5-0.5B https://huggingface.co/google/gemma-3-1b-pt 6 Table 3: Results on the text classification task (MNLI dataset) with different base model initializations. denotes the FP16 teacher used in BitDistill. Table 4: Results on the text classification task with different quantization techniques. B, G, indicates Block Quant, GPTQ and AWQ, respectively. Method FP16-SFT BitNet-SFT BitDistill Gemma3-1B Qwen2.5-0.5B 89.77 78.02 89.61 79.91 60.80 79.98 Method MNLI QNLI BitDistill BitDistill-B [DLSZ21] BitDistill-G [FAHA22] BitDistill-A [LTT+24] 88.17 88.23 88.05 88.25 93.66 93.74 93.63 93.70 across all examined architectures. While minor performance fluctuations are observed between base models, the trend remains stable, suggesting that our method is not tailored to specific pretraining family but can be applied more broadly. This robustness enhances the potential applicability of our approach in diverse deployment environments, where the choice of pretrained backbone may vary depending on availability and task requirements."
        },
        {
            "title": "4.3 Ablation Study",
            "content": "Effect of each individual stages in BitDistill. As outlined in 3, the BitDistill framework consists of three stages. To understand the contribution of each component, we conduct an ablation study by removing one stage at time and re-training the model. The results, reported in Table 5, show that excluding any stage consistently leads to non-trivial drop in downstream performance. This suggests that each stage plays complementary role, and that the full pipeline is necessary to obtain the best trade-off between efficiency and accuracy. Effect of different distillation techniques in Stage-3 3.3. In the final stage of our framework, we introduce two complementary distillation techniques to better optimize 1.58-bit BitNet models for downstream tasks. To disentangle their respective effects, we compare using each technique individually against the joint application of both. As shown in Table 6, while each technique alone provides partial improvements, the combination leads to the most consistent performance across benchmarks. This observation provides evidence that the two techniques address different aspects of the optimization challenge, and their synergy is particularly beneficial under extreme quantization. Compatibility with different quantization techniques. We further examine the compatibility of BitDistill with existing post-training and weight-quantization approaches. In particular, we consider Block-Quant [DLSZ21], GPTQ [FAHA22], AWQ [LTT+24], as well as the simple minmax quantization scheme in Eq. 2. To this end, we integrate BitDistill with each quantization method and evaluate the resulting 1.58-bit models. The results are summarized in Table 4 and lead to two main observations: (1) regardless of the underlying quantization method, models benefit consistently from the proposed framework and generally match the full-precision baseline, and (2) more sophisticated quantization strategies (e.g., GPTQ, AWQ) provide additional gains on top of our distillation pipeline. These findings suggest that BitDistill is complementary to different quantization algorithms, offering unified procedure that can stably enhance low-bit models across diverse range of quantization settings. 4.4 Analysis Effect of SubLN used in Stage-1 3.1. To validate the effect of SubLN, we quantize existing LLMs into 1.58-bit BitNet and fine-tune them on FALCON corpus, comparing the performance with (denoted as BitNet-SFT w/ SubLN) and without the insertion of SubLN (denoted as BitNetSFT w/o SubLN). Specifically, as shown by the training loss curve in Figure 3 (a), we find that the modeling refinement detailed in Stage-1 3.1, which modifies the LLMs architecture by inserting SubLN layers at specific positions, effectively stabilizes the optimization of the 1.58-bit BitNet and leads to improved performance. Why continue-training mitigates the scalability issue. As stated in 1, critical challenge in applying 1.58-bit BitNet to downstream tasks is the poor scalability, i.e., as model size increases, the performance gap between the 1.58-bit BitNet and its FP16 counterpart becomes increasingly pro7 Attention-K Attention-Q Attention-V Attention-O FFN-Gate FFN-Up FFN-Down Figure 2: Visualization of model weights. The top two rows show the quantized weights of BitNet trained from scratch along with their corresponding FP16 distributions. The bottom two rows show the quantized weights of BitNet after loading weights from LLMs and performing continued training (stage-2 in 3.2), together with their corresponding FP16 distributions. (a) (b) (c) Figure 3: Analysis of SubLN, layer selection for Eq. 12 and teacher selection over training steps. (a) Fine-tuning existing LLMs into 1.58-bit BitNet with SubLN yields better performance and faster convergence. (b) Comparison of MNLI accuracies obtained by distilling different layers on Qwen30.6B. (c) Comparison of MNLI accuracies obtained by distilling Qwen3-0.6B with different size of FP16 teachers. nounced. Our experiments reveal that small amount of continue-training can effectively alleviate this issue, and here we investigate the underlying reasons. In Figure 2, we visualize the model weights of 1.58-bit BitNet before and after continue-training, and compare them with those of BitNet trained from scratch. We find that after continue-training, the weight distribution which initially exhibited an approximately Gaussian shape, becomes more similar to that of BitNet trained from scratch. This observation supports our hypothesis in 3.2: continue-training enables BitNet models to rapidly adapt to the feature space that is better suited for 1.58-bit optimization, thereby preventing convergence to suboptimal local minima and ultimately leading to improved downstream performance. Furthermore, we investigate why the BitNet-like weight distribution observed in Figure 2 facilitates improved performance on downstream tasks. In particular, the unique distribution concentrates more weights near the transition boundaries between 0 and -1 as well as between 0 and 1. Such placements allow the quantized values to shift more frequently with small gradient steps, thereby enhancing the 1.58-bit BitNets ability to fit downstream data and reducing the risk of being trapped in suboptimal local minima. Distillation layer selection strategy in Stage-3 3.3. As discussed in 3.3, we hypothesize that performing attention relation distillation on single layer provides the 1.58-bit BitNet with greater optimization flexibility compared to distilling across all layers, thereby yielding better performance. To examine this, we explore strategies for selecting the distillation layer. Figure 3 (b) visualizes the MNLI classification results of Qwen3-0.6B when applying distillation to different layers without 8 Table 5: Effect of different stages in BitDistill. Here Qwen30.6B is used as base model. M.D., C.T., and D.T. denote modeling refinement 3.1, continue pre-training 3.2, and distillation-based finetuning 3.3, respectively. Stage-1 StageStage-3 MNLI CNNDM M.D. C.T. D.F. ACC BLEU ROUGE-1 ROUGE-2 ROUGE-L 74.09 76.30 86.73 88.04 88.17 11.47 11.69 13.96 13.70 14.41 37.10 37.81 39.75 39.92 40.21 13.97 14.13 16.47 16.91 17.47 24.84 25.11 26.96 27.16 27. Table 6: Effect of distillation techniques. Here LD denotes logits distillation in Eq. 9 and AD denotes multi-head attention distillation in Eq. 12. LD AD MNLI 86.73 87.32 87.67 88.17 continue pre-training. Our findings can be summarized as follows: (1) distilling from single layer achieves superior performance compared to using all layers, supporting our hypothesis; (2) the results vary significantly depending on which single layer is chosen, indicating that an appropriate layer selection strategy is crucial; and (3) layers located in the later stages of the model tend to deliver better distillation performance. Better teacher lead to better results. We investigate whether our proposed BitDistill can leverage higher-quality FP16 teacher to provide greater downstream task gains for the 1.58-bit BitNet. To this end, we use Qwen3-1.7B and Qwen3-4B FP16 models as teachers in the distillation process for the Qwen3-0.6B 1.58-bit BitNet. The results are visualized in Figure 3 (c). We find that our algorithm can effectively extract larger gains from higher-quality teacher, even surpassing FP16 models of the same size. This provides performance guarantee for deploying BitNet models tailored to specific tasks."
        },
        {
            "title": "5 Related Work",
            "content": "Quantization for LLMs Quantization [TXL+25, DZC+24, MWM+24] has emerged as widely adopted technique for enhancing the efficiency and scalability of LLMs. Post-training quantization (PTQ) [XLS+23, DLBZ22] like GPTQ [FAHA22] and AWQ [LTT+24] has been extensively studied for weight-only quantization of LLMs. PTQ applies low-bit quantization to full-precision model using small set of calibration data, without requiring access to the end-to-end training loss. However, PTQ always suffer from significant performance degradation, especially when quantization bits are lower than 4 bits [DLBZ22]. To address this limitation, quantization-aware training (QAT) [TXL+25, LOZ+23, CSX+24] has been introduced, which continues training the quantized LLMs with sufficient optimization, thereby raising the performance ceiling achievable by quantized models. Knowledge Distillation for LLMs Knowledge distillation [KKCY24, HVD15, WWD+20, TXL+25] has proven to be an effective technique for compressing large language models (LLMs) while preserving accuracy, by transferring knowledge from high-capacity teacher model to more compact student model. More recently, it has also been shown effective for transferring knowledge from full-precision models to quantized LLMs. For example, TSLD [KLL+23] employs layerto-layer distillation to enhance quantization-aware training (QAT) for ternary quantization, while BitDistiller [DZC+24] leverages self-distillation to improve the performance of LLMs at ultra-low precisions (e.g., 2 or 3 bits). Despite these advances, most existing methods primarily target general language modeling capabilities and still exhibit noticeable performance gaps in downstream applications compared to their full-precision counterparts."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we investigated the problem of adapting pre-trained LLMs to ultra-low precision with only 1.58-bit weights, motivated by the practical need to deploy large-scale models on edge devices under strict memory and latency constraints. To this end, we introduced BitNet Distillation, three-stage framework that first performs model refinement with SubLN, and then continued pretraining to recover critical representation capacity, followed by knowledge distillation at both the hidden-state and attention-relation levels to narrow the accuracy gap between low-precision students 9 and high-precision teachers. Extensive experiments on multiple downstream tasks demonstrate that our method, BitDistill, achieves performance competitive with FP16 models while significantly reducing the computational and memory footprint. Beyond improving efficiency, our approach provides new insights into how low-bit quantization interacts with both pretraining and distillation dynamics, shedding light on scalable strategies for resource-constrained deployment. References [AAA+23] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [BLC13] Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013. [BMH+22] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving language models by retrieving from trillions of tokens, 2022. [CSX+24] Mengzhao Chen, Wenqi Shao, Peng Xu, Jiahao Wang, Peng Gao, Kaipeng Zhang, and Ping Luo. Efficientqat: Efficient quantization-aware training for large language models. arXiv preprint arXiv:2407.11062, 2024. [DLBZ22] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale. Advances in neural information processing systems, 35:3031830332, 2022. [DLSZ21] Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 8-bit optimizers via block-wise quantization. arXiv preprint arXiv:2110.02861, 2021. [DZC+24] Dayou Du, Yijia Zhang, Shijie Cao, Jiaqi Guo, Ting Cao, Xiaowen Chu, and Ningyi Xu. Bitdistiller: Unleashing the potential of sub-4-bit llms via self-distillation. arXiv preprint arXiv:2402.10631, 2024. [FAHA22] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. [GYZ+25] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [HKG+15] Karl Moritz Hermann, Tomás Kociský, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. In NIPS, pages 16931701, 2015. [HVD15] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in neural network, 2015. [HZL+24] Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian McAuley, and Wayne Xin Zhao. Large language models are zero-shot rankers for recommender systems. In European Conference on Information Retrieval, pages 364381. Springer, 2024. [KDSP25] Arina Kostina, Marios Dikaiakos, Dimosthenis Stefanidis, and George Pallis. Large language models for text classification: Case study and comprehensive review. arXiv preprint arXiv:2501.08457, 2025. [KKCY24] Jongwoo Ko, Sungnyun Kim, Tianyi Chen, and Se-Young Yun. Distillm: Towards streamlined distillation for large language models. arXiv preprint arXiv:2402.03898, 2024. 10 [KLL+23] Minsoo Kim, Sihwa Lee, Janghwan Lee, Sukjin Hong, Du-Seong Chang, Wonyong Sung, and Jungwook Choi. Token-scaled logit distillation for ternary weight generative language models. Advances in Neural Information Processing Systems, 36:4209742118, 2023. [Lin04] Chin-Yew Lin. Rouge: package for automatic evaluation of summaries. In Text summarization branches out, pages 7481, 2004. [LOZ+23] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra. Llmqat: Data-free quantization aware training for large language models. arXiv preprint arXiv:2305.17888, 2023. [LSK+25] Jung Hyun Lee, Seungjae Shin, Vinnam Kim, Jaeseong You, and An Chen. Unifying block-wise ptq and distillation-based qat for progressive quantization toward 2-bit instruction-tuned llms. arXiv preprint arXiv:2506.09104, 2025. [LTT+24] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware weight quantization for on-device llm compression and acceleration. Proceedings of machine learning and systems, 6:87100, 2024. [MWH+25] Shuming Ma, Hongyu Wang, Shaohan Huang, Xingxing Zhang, Ying Hu, Ting Song, Yan Xia, and Furu Wei. Bitnet b1. 58 2b4t technical report. arXiv preprint arXiv:2504.12285, 2025. [MWM+24] Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Lifeng Dong, Ruiping Wang, Jilong Xue, and Furu Wei. The era of 1-bit llms: All large language models are in 1.58 bits. arXiv preprint arXiv:2402.17764, 1(4), 2024. [PMH+23] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: Outperforming curated corpora with web data, and web data only, 2023. [PRWZ02] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311318, 2002. [QY+25] Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. [RWX+24] Xubin Ren, Wei Wei, Lianghao Xia, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, and Chao Huang. Representation learning with large language models for recommendation. In Proceedings of the ACM web conference 2024, pages 34643475, 2024. [RZLL16] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100, 000+ questions for machine comprehension of text. CoRR, abs/1606.05250, 2016. [SLL+23] Xiaofei Sun, Xiaoya Li, Jiwei Li, Fei Wu, Shangwei Guo, Tianwei Zhang, and arXiv preprint Text classification via large language models. Guoyin Wang. arXiv:2305.08377, 2023. [SPW+13] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over sentiment treebank. In Proceedings of the 2013 Conference on Empirical 11 Methods in Natural Language Processing, pages 16311642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. [TKF+25] Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. [TXL+25] MiniCPM Team, Chaojun Xiao, Yuxuan Li, Xu Han, Yuzhuo Bai, Jie Cai, Haotian Chen, Wentong Chen, Xin Cong, Ganqu Cui, et al. Minicpm4: Ultra-efficient llms on end devices. arXiv preprint arXiv:2506.07900, 2025. [WBH+20] Wenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong, and Furu Wei. Minilmv2: Multi-head self-attention relation distillation for compressing pretrained transformers. arXiv preprint arXiv:2012.15828, 2020. [WMD+23] Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan Yang, Ruiping Wang, Yi Wu, and Furu Wei. Bitnet: Scaling 1-bit transformers for large language models. arXiv preprint arXiv:2310.11453, 2023. [WNB18] Adina Williams, Nikita Nangia, and Samuel Bowman. broad-coverage challenge In Proceedings of the 2018 corpus for sentence understanding through inference. Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112 1122. Association for Computational Linguistics, 2018. [WSM+19] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: multi-task benchmark and analysis platform for natural language understanding. In International Conference on Learning Representations, 2019. [WWD+20] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers. Advances in neural information processing systems, 33:57765788, 2020. [WZQ+24] Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen, Chuan Qin, Chen Zhu, Hengshu Zhu, Qi Liu, et al. survey on large language models for recommendation. World Wide Web, 27(5):60, 2024. [XLS+23] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In International conference on machine learning, pages 3808738099. PMLR, 2023. [YLY+25] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [ZSB+24] Yiyun Zhao, Prateek Singh, Hanoz Bhathena, Bernardo Ramos, Aviral Joshi, Swaroop Gadiyaram, and Saket Sharma. Optimizing llm based retrieval augmented generation pipelines in the financial domain. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track), pages 279294, 2024."
        }
    ],
    "affiliations": [
        "Microsoft Research"
    ]
}