{
    "paper_title": "UnSAMv2: Self-Supervised Learning Enables Segment Anything at Any Granularity",
    "authors": [
        "Junwei Yu",
        "Trevor Darrell",
        "XuDong Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The Segment Anything Model (SAM) family has become a widely adopted vision foundation model, but its ability to control segmentation granularity remains limited. Users often need to refine results manually - by adding more prompts or selecting from pre-generated masks - to achieve the desired level of detail. This process can be ambiguous, as the same prompt may correspond to several plausible masks, and collecting dense annotations across all granularities is prohibitively expensive, making supervised solutions infeasible. To address this limitation, we introduce UnSAMv2, which enables segment anything at any granularity without human annotations. UnSAMv2 extends the divide-and-conquer strategy of UnSAM by discovering abundant mask-granularity pairs and introducing a novel granularity control embedding that enables precise, continuous control over segmentation scale. Remarkably, with only $6$K unlabeled images and $0.02\\%$ additional parameters, UnSAMv2 substantially enhances SAM-2, achieving segment anything at any granularity across interactive, whole-image, and video segmentation tasks. Evaluated on over $11$ benchmarks, UnSAMv2 improves $\\text{NoC}_{90}$ (5.69 $\\rightarrow$ 4.75), 1-IoU (58.0 $\\rightarrow$ 73.1), and $\\text{AR}_{1000}$ (49.6 $\\rightarrow$ 68.3), showing that small amounts of unlabeled data with a granularity-aware self-supervised learning method can unlock the potential of vision foundation models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 4 1 7 3 1 . 1 1 5 2 : r UNSAMV2: Self-Supervised Learning Enables Segment Anything at Any Granularity"
        },
        {
            "title": "Junwei Yu",
            "content": "Trevor Darrell XuDong Wang* UC Berkeley Project Page: https://yujunwei04.github.io/UnSAMv2-Project-Page/ Figure 1. What is an object? The notion has long been debated: should it follow models learned semantics or an annotators subjective judgment? UNSAMV2 takes third path, granting users full flexibility to define objectness through promptable segmentation with single point and continuous, controllable granularity score. Built on SAM-2 [35], UNSAMV2 introduces granularity-aware, self-supervised training pipeline based on divide-and-conquer pseudo-labels [47]. Trained on just 6000 unlabeled images, it segments anything from finegrained parts to holistic objects, achieving state-of-the-art performance across interactive, whole-image, and video segmentation tasks."
        },
        {
            "title": "Abstract",
            "content": "The Segment Anything Model (SAM) family has become widely adopted vision foundation model, but its ability to control segmentation granularity remains limited. Users often need to refine results manually by adding more prompts or selecting from pre-generated masks to achieve the desired level of detail. This process can be ambiguous, as the same prompt may correspond to several plausible masks, and collecting dense annotations across all granularities is prohibitively expensive, making super- *Corresponding author vised solutions infeasible. To address this limitation, we introduce UNSAMV2 which enables segment anything at any granularity without human annotations. UNSAMV2 extends the divide-and-conquer strategy of UnSAM by discovering abundant mask-granularity pairs and introducing novel granularity control embedding that enables precise, continuous control over segmentation scale. Remarkably, with only 6K unlabeled images and 0.02% additional parameters, UNSAMV2 substantially enhances SAM-2, achieving segment anything at any granularity across interactive, whole-image, and video segmentation tasks. Evaluated on over 11 benchmarks, UNSAMV2 improves NoC90 (5.69 4.75), 1-IoU (58.0 73.1), and AR1000 (49.6 68.3), showing that small amounts of unlabeled data with 1 granularity-aware self-supervised learning method can unlock the potential of vision foundation models. 1. Introduction What is an object? This question has long been debated in both vision and cognition. Should an object be defined by models learned semantics or by an annotators subjective judgment? We propose third perspective: granting users full flexibility to define objectness through single point prompt and continuous granularity score. Imagine clicking on single point in an image and smoothly adjusting slider that controls segmentation granularity. At low granularity values, the model reveals finegrained parts with precise boundaries; as the granularity increases, it gradually merges regions into larger and more semantically coherent entities. In essence, this transforms SAMs [24] three discrete mask hypotheses into continuous granularity axis capable of producing multiple masks per point, each corresponding to user-defined granularity. Such controllable segmentation enables users to flexibly define what constitutes an object for their specific task, whether it involves part-level analysis, instance grouping, or large-scale region editing. The emergence of the Segment Anything family [24, 35] has positioned SAM as vision foundation model, significantly advancing diverse tasks such as video object tracking [51, 56], multi-scale perception [18, 48], and compositional understanding [6, 9, 45]. However, SAM and its successors rely heavily on supervised learning from SA1B dataset [24]. This pipeline directly ties the models notion of an object to human annotation bias and limits its output to three discrete mask hypotheses per prompt. As result, segmentation in SAM is defined by supervision rather than discovered from data. This design assumes shallow hierarchy of objectness, while real-world scenes exhibit complex, nested partwhole structures that cannot be captured through fixed human labels. Although SA-1B covers broad range of object sizes, it lacks explicit correspondences between instanceand part-level masks, making it difficult for supervised models to learn how granularity should vary continuously. We argue that segmentation granularity should be learned through unsupervised learning, which allows models to infer object hierarchies directly from image statistics instead of depending on predefined labels. To this end, we introduce UNSAMV2, self-supervised framework that enables segmentation at any granularity without human supervised labels. UNSAMV2 learns continuous representation of granularity that bridges the gap between parts and wholes, giving users full control over segmentation masks. UNSAMV2 builds upon UnSAM [47], which introduced an unsupervised divide-and-conquer strategy for discoverFigure 2. UNSAMV2 achieves state-of-the-art performance across interactive segmentation benchmarks. Across multiple datasets, UNSAMV2 consistently outperforms SAM-2 and prior methods, turning segmentation into controllable and interpretable process rather than fixed prediction. ing hierarchical masks. While UnSAM focused on unsupervised hierarchy construction, UNSAMV2 extends this idea to granularity-controllable segmentation. In the divide stage, we use the normalized-cut method MaskCut [44] to extract instance-level masks. In the conquer stage, we recursively merge similar pixels within each instance to discover finer parts, forming hierarchical pseudo-labels that encode relative scale. From these hierarchies, we compute continuous granularity scalar for each mask, representing its position along the partwhole continuum. We then augment SAM-2 [35] with lightweight granularity encoder and granularity-aware mask token. Given point prompt and granularity scalar g, UNSAMV2 predicts the mask corresponding to the desired granularity, turning segmentation into controllable function of scale. Training only the lightweight SAM-2 decoder for four hours with 2 A100 GPUs on 6,000 unlabeled images (0.02% additional parameters) enables smooth interpolation between coarse and fine structures and reveals the latent hierarchy within SAMs feature space. Across interactive, whole-image, and video segmentation benchmarks, UNSAMV2 consistently surpasses SAM2 [35] and previous state-of-the-art promptable segmentation methods. Evaluated on more than 11 widely used datasets, including SA-1B, COCO, and PartImageNet, UN2 SAMV2 improves NoC90 from 5.69 to 4.75, 1-IoU from 58.0 to 73.1, and AR1000 from 49.6 to 68.3, all achieved using only unlabeled data. The resulting model empowers users to define their own notion of objectness and to explore segmentation as continuous, controllable process rather than static prediction. Contributions. (i) We propose UNSAMV2, granularitycontrollable segmentation framework that enables continuous control of mask granularity from single point prompt and scalar input. (ii) We develop an unsupervised granularity discovery pipeline that learns hierarchical instance part structures and assigns each mask continuous scale, applicable to various promptable segmentation models. (iii) Trained on only 6,000 unlabeled images, UNSAMV2 achieves state-of-the-art results across interactive, wholeimage, and video segmentation benchmarks. 2. Related Work Multi-Granularity Segmentation. Segment Anything project [24, 35] has greatly advanced segmentation performance by leveraging large-scale human-annotated data and extensive compute. Extensions such as Semantic-SAM [25] improve fine-grained predictions through multiple-choice learning design [12, 26]. However, these approaches constrain point-prompt predictions to fixed number of candidate masks, forcing users to manually select from limited outputs or give additional prompts. This restriction highlights the need for explicit control over mask granularity. Recent work [28, 42] has begun to tackle such ambiguity. GARField [23] and SAMPart3D [54, 55] address scale ambiguity in 3D scene decomposition via absolute scale conditioning, while GraCo [58] achieves granularity-controllable interactive segmentation by extending SimpleClick [29] with discrete granularity inputs. In contrast, our UNSAMV2 tackles mask ambiguity in fully self-supervised manner by treating granularity as continuous, relative concept. We enable granularity-aware segmentation within the widely adopted SAM framework without requiring manual annotation. Self-Supervised Learning and Unsupervised Segmentation. Self-supervised learning (SSL) methods such as MAE [16], JEPA [2], and DINO [5, 32, 40] demonstrate that large-scale pretraining can endow vision transformers with strong semantics-aware representations, benefiting wide range of downstream tasks [10, 19, 20, 50, 52, 53, 57]. In parallel, unsupervised segmentation has gained lots of attention [1, 13, 14, 22, 31, 39, 43, 49, 60]. CutLER [44], as recent foundational work of unsupervised image segmentation, greatly advanced unsupervised instance segmentation by introducing MaskCut, normalized-cutsbased strategy [37] that iteratively extracts multiple objects from images. VideoCutLER [46] extended this framework to video through cutsynthesizelearn pipeline. CutS3D [38] inFigure 3. From ambiguity to control. Without granularity input, SAM-2 yields up to three masks per point, requiring users to manually choose one. UNSAMV2 resolves this ambiguity by introducing continuous granularity variable, allowing users to obtain the intended object at any scale with single prompt. This simple addition turns segmentation from discrete guess into continuous, controllable reasoning process. troduces the concept of projecting 2D image into 3D space via ZoeDepth [3] to enhance unsupervised segmentation performance on overlapping objects. SOHES [4] adopts bottom-up merging scheme, grouping pixels based on cosine similarity to progressively discover objects. More recently, UnSAM [47] introduced divide-and-conquer paradigm to generate hierarchical pseudo labels. Building on these efforts, UnSAMv2 leverages the hierarchical mask discovery perspective in the divide-andconquer [47] pipeline and extends it to assign an explicit granularity scale for each pseudo mask. This enables unsupervised segmentation at arbitrary levels of detail, realizing the goal of segment anything at any granularity. 3. Method We present UNSAMV2, self-supervised framework that enables segmentation at arbitrary levels of granularity without human annotations. Unlike the supervised SAM [24] pipeline, which depends on human-labeled object masks, UNSAMV2 learns granularity directly from image statistics through hierarchy-aware divide-and-conquer process. This enables segmentation granularity to be continuously controlled by single scalar input, rather than restricted to fixed number of discrete mask tokens. We first review prior unsupervised segmentation methods (Sec. 3.1) and the limitations of supervised training paradigms (Sec. 3.2). We then present our granularityaware divide-and-conquer algorithm that automatically constructs maskgranularity pairs from unlabeled data (Sec. 3.3). Next, we describe the architectural design that empowers any promptable segmentation model to interpret and control segmentation granularity (Sec. 3.4). We then briefly discuss the difference with prior supervised learning works (Sec. 3.5). Finally, we present UNSAMV2+, lightly supervised variant that integrates SA-1B annotations to further refine the granularity learning process (Sec. 3.6). 3 3.1. Preliminaries UnSAM, MaskCut, and SOHES. UnSAM [47] introduced divide-and-conquer strategy to generate pseudo masks without supervision. In the divide stage, cut-based segmentation method MaskCut [44] is applied to obtain instance/semantic-level masks. Then, inspired by bottomup hierarchical segmentation, e.g., SOHES [4], the conquer stage iteratively merges similar pixels under sequence of thresholds, constructing partwhole hierarchies. Formally, for local image region Ilocal, patch-level features = DINO(Ilocal) are extracted using DINO [40]. Neighboring patches are merged according to the cosine similarity of their DINO features against thresholds θ1, . . . , θl, yielding part-level masks nested inside instances. This process produces discrete but rich hierarchy of mask granularity. Segment Anything family. Segment Anything models (SAM and SAM-2) [24, 35] have advanced promptable segmentation with scalable encoderdecoder design. (1) Image encoder: ViT that maps an input image to multiscale dense embeddings while preserving spatial structure. (2) Prompt encoder: embeddings for user inputs such as points, boxes, or masks that condition the segmentation process and guide attention to regions of interest. (3) Mask decoder: lightweight transformer that fuses image and prompt features to predict segmentation masks. Despite these strengths, the training pipeline is fully supervised on SA-1B [24], which ties the notion of objectness to human-labeled masks. Moreover, the decoder employs three fixed mask tokens (small, medium, large), producing at most three hypotheses per prompt. This discretization limits controllable granularity and discourages hierarchical reasoning about parts and wholes, motivating an unsupervised formulation that learns granularity from data rather than from fixed labels. 3.2. Limitations of SAMs Supervised Paradigm Lack of granularity control. When single point corresponds to multiple plausible objects (e.g., part versus the whole), SAM generates up to three discrete masks and requires manual selection by the user. Without an explicit granularity variable, the model cannot traverse scales continuouslyfine details and coarse structures remain disconnected. This limitation not only reduces efficiency in interactive segmentation but also prevents smooth, interpretable control over the level of detail. Lack of hierarchical reasoning. Supervised training on human-labeled masks encourages SAM to learn flat object representation, where parts and instance/semantic-level segments are treated as isolated entities rather than components within hierarchy. As consequence, SAM lacks structural awareness and fails to capture relationships across Figure 4. Granularity distribution of discovered masks. Our divide-and-conquer pipeline produces rich, left-tailed hierarchy of pseudo-masks, dominated by fine-grained structures. Despite this imbalance, UNSAMV2 learns stable semantics across all scales. Hierarchical perception can emerge from unlabeled data! scales. It struggles to segment scenes at intermediate levels of detail or to uncover the nested hierarchical structure of visual scenes  (Fig. 3)  . This limitation underscores the necessity of unsupervised learning, which can recover hierarchical dependencies directly from image statistics rather than relying on human annotations. 3.3. Granularity-Aware Divide-and-Conquer We extend UnSAMs divide-and-conquer framework to automatically construct dense maskgranularity pairs from unlabeled images. The process consists of four stages. Stage 1: Instance discovery via N-Cuts. We employ the normalized-cut-based method CutLER [44] to generate segmentation masks = {m1, . . . , mn} at varying levels of granularity. We filter out noisy CutLER mask outputs with confidence threshold τconf, Mhigh = {mi conf(mi) τconf}, [n]. (1) Stage 2: Instancepart relationship discovery. We identify instance-level masks Minst Mhigh based on two criteria: (i) their area-to-image-area ratio exceeds the threshold τarea, and (ii) they dominate overlapping masks, which means for any mi Minst and mj Mhigh, we have IoU(mi, mj) τoverlap Area(mi) Area(mj). (2) The remaining masks form Mrest = Mhigh Minst. Each mr Mrest is assigned as part of mi if IoU(mr, mi) > τoverlap; otherwise, it is discarded. Therefore, for every instance mask mi, we have set of part-level masks Mi,part. Stage 3: Fine-grained mask discovery. To enrich granularity, we further apply patch merging to each mi Minst, decomposing it into finer structures. The resulting finegrained masks Mi,conquer are merged with Mi,part through Non-Maximum Suppression (NMS), denoted as Mi,final. 4 Parameter efficiency. The additional encoding and token modules introduce less than 0.02% extra parameters while providing continuous control over mask detail. 3.5. Distinctions with prior methods. Prior methods such as GraCo [58] and GARField [23] treat granularity as discrete variable or directly associate it with absolute mask size. However, discretizing granularity imposes artificial boundaries between adjacent levels, preventing smooth transitions across scales, while absolutesize definitions fail to account for contextual differences, e.g., small mask could correspond to an entire object in one image but only part in another. In contrast, UNSAMV2 formulates granularity as continuous, relative measure within the instancepart hierarchy, aligning more closely with human perception, where objects and parts are interpreted in relation to one another rather than by their absolute scales. This formulation enables UNSAMV2 to capture fine-grained hierarchical structure and traverse segmentation levels seamlessly. 3.6. UNSAMV2+: Light-Supervised Variant Although the unsupervised pipeline generates abundant maskgranularity pairs, pseudo labels can be noisy. To further stabilize learning, we introduce UNSAMV2+, lightly supervised variant that incorporates SA-1B ground-truth masks into the divide stage: MUNSAMV2+ = MCutLER MSA-1B. The combined masks are then processed through the same conquer and granularity-assignment stages as in UNSAMV2. This hybrid supervision effectively balances human-curated and self-discovered hierarchies, producing cleaner masks while preserving the scalability and flexibility of unsupervised granularity learning. 4. Experiments 4.1. Model Training Settings Pseudo mask-granularity data generation. For the divide stage of our unsupervised data pipeline, we set τconf to 0.3 and τoverlap to 0.8. In the conquer stage, we leverage DINOv3 [40] vision transformer to extract feature embedding and set the iterative merging thresholds as θ = [0.9, 0.8, 0.7, 0.6, 0.5]. We run the unsupervised pipeline to generate mask-granularity pairs on 6,000 images from SA1B. UNSAMV2 training. We adopt SAM-2-small as the base model, and with only 8 A-100 GPU hours, we finetune the base model for 5 epochs with 6,000 unlabeled images. During this process, we freeze the vision encoder and train the granularity encoding module, granularity mask token, and the two-way transformer block in the mask decoder with LoRA [17]. See more experiment details in Sec. A2. Figure 5. Granularity as relative notion. At fixed granularity value, mask sizes vary widely across scenes, showing that UNSAMV2 learns granularity relationally, consistent with human perception of parts and wholes rather than simply associating it with absolute size. This process expands granularity richness in hierarchical perspective while maintaining mask quality. Stage 4: Continuous granularity assignment. After obtaining the instancepart hierarchies, we assign each mask mi Mi,final continuous granularity score gi [0.1, 1.0] according to its relative area within the hierarchy: (cid:18) gi = Ai Amax Amin"
        },
        {
            "title": "Amin",
            "content": "(cid:19) 0.9 + 0.1, (3) where Ai denotes the area of mask mi. As illustrated in Fig. 4, this generates dense hierarchy of masks spanning fine to coarse scales. We observe that most masks have gi < 0.4, indicating an abundance of part-level structures; yet UNSAMV2 learns stable semantics across all scales  (Fig. 5)  , suggesting robust hierarchical understanding. 3.4. UNSAMV2 Architecture Granularity encoding. As shown in Fig. 6, we introduce lightweight granularity encoding module that converts scalar into high-dimensional Fourier embedding [41], denoted as ϕ(g) RdFourier . Then, we project ϕ(g) into the decoder feature space using three-layer MLP: Eg = MLP(ϕ(g)) Rddecoder. This granularity embedding is concatenated with the point prompt features Ep: Eprompt = Concat(Ep, Eg). This design allows the mask decoder to jointly interpret spatial prompts and granularity cues. Granularity-aware mask decoding. We replace SAMs three fixed-size mask tokens with single learnable granularity-aware mask token. This token attends to both prompt embeddings and image features through selfand cross-attention, producing mask that corresponds to the target granularity. The design is model-agnostic and can be integrated with any promptable segmentation framework. 5 Figure 6. Architecture of UNSAMV2. Built on SAM-2, UNSAMV2 introduces Fourier-based granularity encoder and granularityaware mask token to enable segmentation at arbitrary granularity. scalar granularity input [0.1, 1] is mapped to high-dimensional embedding via Fourier transformation and an MLP, then injected into the transformer alongside the sparse point prompt embedding and dense image embedding. The granularity-aware mask token attends to image, point, and granularity embeddings, and is finally decoded by token decoder into mask at the requested granularity. Figure 7. Qualitative comparison with the previous state-of-the-art method [58]. Each scene shows results at different target granularity values. Prior methods often break one object into parts at high granularity or include extra regions at low granularity. In contrast, UNSAMV2 produces clear and consistent masks with smooth transitions across scales. 4.2. Evaluation Datasets and Metrics Interactive Image Segmentation. Following SimpleClick [29] and GraCo [58], we evaluate UNSAMV2s capability to segment objects at various granularity levels with the number of clicks (NoC) required to reach certain Intersection over Union (IoU) threshold to assess the models efficiency in segmenting the target object. Specifically, we choose NoC80 and NoC90 which record the average number of clicks needed to achieve 80% and 90% IoU between predicted mask and ground-truth mask. In addition, we measure the IoU between the predicted mask and the ground-truth mask with just one click, denoted as 1IoU in the table. For object levels, we conduct evaluations on 5 commonly used datasets [15, 24, 30, 33, 36], and for part level, we evaluate models on [7, 8]. Note that for the fairness of comparison, we only compare datasets across models that are not in-distribution with their training data, e.g., SAM-2 [35] is trained on SA-1B, SimpleClick [29] and GraCo [58] are trained with SBD and PascalPart. We provide detailed descriptions of evaluation datasets in Sec. A1. Whole-Image Segmentation. We evaluate our models performance in identifying all possible masks for given images in zero-shot setting across 5 datasets that span broad range of granularity [11, 24, 27, 34, 59]. Importantly, each benchmark annotates only specific semantic classes and typically emphasizes certain levels of the partinstance hierarchy, whereas our method predicts masks at all levels and for any class. Consequently, COCO Average Precision (AP) does not faithfully capture open-world performance. Following prior work [4, 44, 47], we report Average Recall (AR1000) for comparison across methods. 4.3. Experimental Results Interactive Segmentation. Remarkably, UNSAMV2 surpasses SAM-2 across all datasets in zero-shot evaluation settings as summarized in Table 1. Finetuned with only 6,000 images with unsupervised pseudo-labels, UNSAMV2 demonstrates superior performance in segmentIt indicates that ing objects at various granularity levels. 6 GrabCut Berkeley Averaged Datasets NoC80 NoC90 1-IoU NoC80 NoC90 1-IoU NoC80 NoC90 1-IoU NoC80 NoC90 1-IoU NoC80 NoC85 1-IoU NoC80 NoC85 1-IoU Method 6.64 39.5 4.02 SAM-2 [35] UnSAMv2 5.29 57.1 3.41 vs. sup. SAM-2 -0.61 -1.35 +17.6 UnSAMv2+ 4.77 60.7 3.30 Table 1. Comparison with SAM-2 on interactive segmentation benchmarks. Trained on 6,000 images with purely unsupervised pseudo-labels, UNSAMV2 significantly outperforms SAM-2. We report promptable segmentation performance in terms of NoC and 1IoU across five benchmarks. * Following [58], we select the optimal granularity from 0.1 to 1.0 in steps of 0.1 and report averaged results. 9.37 11.80 29.0 9.59 51.5 7.84 -2.21 +22.5 -1.53 9.67 52.0 7.87 6.73 64.6 5.97 74.8 -0.76 +10.2 5.72 77.3 1.72 80.0 1.60 90.8 -0.12 +10.8 1.37 91.1 1.54 76.8 1.30 91.4 -0.24 +14.6 1.26 92. 58.0 73.1 +15.1 74.7 1.48 1.16 -0.32 1.22 2.85 2.75 -0.10 2.55 5.69 4.75 -0.94 4.56 5.09 4.21 -0.88 3.75 1.29 1.10 -0.19 1. PartImageNet PascalPart SBD Datasets Method SimpleClick [29] 3.32 GraCo [58] 2.35 2.44 SAM-2 [35] UnSAMv2 2.28 UnSAMv2+ 2.07 vs. prev. SOTA -0.28 Averaged GrabCut Berkeley DAVIS SA-1B PartImageNet NoC80 NoC90 1-IoU NoC80 NoC90 1-IoU NoC80 NoC90 1-IoU NoC80 NoC90 1-IoU NoC80 NoC90 1-IoU NoC80 NoC85 1-IoU 7.76 29.2 5.03 51.7 6.64 39.5 5.29 57.1 4.77 60.7 -0.26 +9.00 75.6 5.38 74.0 5.09 71.7 4.50 84.7 4.66 4.40 85.7 -0.69 +11. 21.9 7.27 66.2 4.01 77.1 3.76 72.4 4.16 3.69 78.4 -0.32 +12.20 2.44 84.6 1.63 88.7 1.72 80.0 1.60 90.8 1.37 91.1 -0.26 +2.40 1.48 89.6 1.32 91.1 1.54 76.8 1.30 91.4 1.26 92.6 -0.06 +1.50 60.2 74.4 69.0 79.3 81.7 +7.30 4.87 3.42 3.63 3.40 3.10 -0.32 1.32 1.24 1.48 1.16 1.22 -0. 1.26 1.17 1.29 1.10 1.12 -0.05 6.34 4.11 5.09 4.21 3.75 -0.36 4.80 2.41 1.82 2.18 1.87 -0.54 2.88 2.84 2.51 2.75 2.36 -0.48 Table 2. State-of-the-art performance on interactive segmentation at various granularity levels. Trained with only 6,000 images using combination of supervised and unsupervised labels, UNSAMV2+ demonstrates superior segmentation quality and indicates how effectively unsupervised methods can complement supervised data. Results of [29, 58] are reproduced with official code and checkpoints. *: Following [58], we select optimal granularity from 0.1 to 1.0 with step of 0.1 and report average results. Methods 60.8 SAM [24] 41.9 UnSAM [47] 64.8 UnSAM+ [47] 70.6 UnSAMv2 77.9 UnSAMv2+ vs. prev. SOTA +21.5 +26.9 +19.5 +27.4 +20.5 +13. Avg. COCO LVIS ADE Entity SA-1B 49.6 39.2 52.6 68.3 74.1 45.9 39.6 49.8 64.3 70.3 46.1 37.7 50.8 63.8 70.3 49.6 40.5 52.2 74.5 79.1 45.8 35.7 45.3 68.4 72.7 Table 3. State-of-the-art performance on whole image segmentation. UNSAMV2 outperform baseline methods [24, 47] on evaluation datasets that contain instances over wide range of granularity levels. The evaluation metric is AR1000. We copy results of SAM and UnSAM from [47]. For UNSAMV2, we aggregate masks generated at granularity levels ranging from 0.1 to 1.0 in increments of 0.1 and filter out low-confidence masks. our model architecture and unsupervised data pipeline effectively guide the base model to understand the meaning of granularity scaler. On average, UNSAMV2 surpasses SAM-2 by 15.2% in NoC80, 16.5% in NoC90, and 26.0% in 1-IoU. With UNSAMV2, user could segment their desired objects accurately at any granularity level with fewer prompt points, which greatly enhances the flexibility of object segmentation and prepares for downstream tasks. In addition, as shown in Table 2, UNSAMV2+, trained with combination of supervised and unsupervised labels on 6,000 images achieves state-of-the-art performance on all metrics across multiple evaluation datasets. Qualitative comparisons with previous SOTA [58] are shown in Fig. 7. Whole-Image Segmentation. Apart from SOTA results in point-based interactive segmentation, UNSAMV2 achieves superior performance on whole-image segmentation across datasets with instances of abundant granularity levels, out7 Methods SAM-2 UnSAMv2 UnSAMv2 UnSAMv2 # PascalPart PartImageNet Images NoC80 NoC85 1-IoU NoC80 NoC85 1-IoU 39.5 29.0 55.5 49.1 55.7 51.0 57.1 51.5 11.8 9.41 9.55 9.59 9.37 7.82 7.85 7.84 6.64 5.50 5.32 5.29 5.09 4.48 4.27 4. 1K 3K 6K Table 4. Ablations for training data size. UNSAMV2 starts to demonstrate decent understanding of granularity scale even trained with only 1,000 images with unsupervised pseudo-labels. performing SAM [24] by 37.7% and UnSAM [47] by 29.8% in AR1000. The results are shown in Table 3. With granularity scalar as input, UNSAMV2 can surface instances over wide range of detail. As shown in Fig. 8, users simply set the desired granularity to obtain all candidate masks in images at that level, creating new possibilities on how to integrate segmentation models into vision task pipelines. Video Segmentation Results. Despite being trained solely on images, UNSAMV2 demonstrates promising capabilities on interactive video segmentation. Although we keep SAM-2s memory module frozen during training, UNSAMV2 still achieves competitive results on video data, demonstrating that the granularity embeddings and mask token propagate across frames effectively. This further shows that our self-supervised pipeline enables granularity to be assimilated into pretrained models reasoning flow. We show qualitative video segmentation results in Fig. 9. 5. Ablations Efficiency of UNSAMV2 training. In Table 4, we ablate the training data size used for UNSAMV2. We find that Figure 8. Visualizations for whole-image segmentation. Low granularity reveals fine parts, while higher values recover whole objects. UNSAMV2 offers controllable, scalable whole-image segmentation capability, even for scenes with many densely packed entities. Figure 10. Ablation of granularity-aware mask token. Directly finetuning the original SAM-2 mask tokens leads to limited gains, suggesting they already encode strong mask priors. Adding our granularity-aware token enables efficient learning of granularity. Methods Sup. Unsup. Berkeley DAVIS PascalPart PtIn 1-IoU 1-IoU 1-IoU 1-IoU Data Data 39.5 80.0 54.3 89.7 57.1 90.8 60.7 91.1 71.7 80.8 84.7 85.7 29.0 42.5 51.5 52.0 SAM-2 UnSAMv2 UnSAMv2 UnSAMv2+ Figure 9. Granularity generalizes to video. We prompt UNSAMV2 with point and granularity value on the first frame, then propagate the mask to later frames. Even though trained only on images, UNSAMV2 maintains consistent masks over time, showing strong temporal coherence and transferability. granularity training is highly sample-efficient: UNSAMV2 already shows solid grasp of granularity with only 1,000 images. We attribute this efficiency to two factors. First, we derive the granularity scale hierarchically, mirroring how humans perceive scaleby relative size within hierarchy rather than absolute size. Second, during training we update only 0.1% of the parameters and keep the rest frozen, which preserves pretrained models segmentation ability. Overall, our procedure effectively teaches pretrained model the 8 Table 5. Ablation for purely supervised granularity training. Only training with SA-1B ground-truth labels results in mediocre performance compared to data from our unsupervised pipeline. This indicates how we crucial our unsupervised pseudo-labels are to maximize UNSAMV2s capability in learning granularity. concept of mask granularity: with just few example images, UNSAMV2 learns latent representation of granularity rather than merely memorizing instances. Granularity-aware mask token. We observe that training UNSAMV2 directly with the original mask tokens and token decoding module in SAM-2 leads to unsatisfying performance  (Fig. 10)  . This phenomenon has been noticed in HQ-SAM [21]. It indicates SAM-2s original mask tokens have already shown strong-prior knowledge on what constitutes as an object. Teaching these pretrained tokens to understand the meaning of granularity is difficult. Thus, we introduce new mask token and its MLP to do mask decoding that is only trained with mask data accompanied with granrank none 4 8 16 32 3 7 20 30 50 NoC80 4.47 4.41 4.21 4.40 4.38 NoC80 4.21 4.31 4. NoC80 4.41 4.21 4.25 dfourier 32 NoC80 4.44 4.30 4.21 4.37 64 128 256 (a) LoRA rank. (b) # points per mask. (c) # masks per image. (d) Fourier dimension for granularity. Table 6. Ablations for hyperparameter and design choices used for training UNSAMV2. We report UNSAMV2s interactive segmentation performance on validation set of PartImageNet. (a) We vary the rank of LoRA weights in mask decoder. (b) We vary the number of correction points sampled per mask. (c) We vary the number of masks randomly sampled per iteration when training UNSAMV2 models. (d) We study different choices of dimension to encode granularity scaler in Fourier feature space. Default settings are highlighted in teal. ber of correction points sampled per mask in one training step in Table 6b. With the granularity scaler, our model can identify the target mask with few clicks, so we use 3 correction points per mask for efficiency. In Table 6c, we study the number of masks per image in one training iteration. UNSAMV2 benefits from moderate number of masks per step, which best supports learning granularity while maintaining training stability. Finally, we study the effect of the Fourier feature dimension used to encode the granularity input in Table 6d. We observe that moderate dimension best matches the granularity representation and enables UNSAMV2 to distinguish granularity levels smoothly in continuous manner. 6. Conclusion We presented UNSAMV2, self-supervised framework to segment that equips pretrained segmentation model anything at any granularity. By deriving continuous granularity scales from unlabeled data, UNSAMV2 learns to traverse partwhole hierarchies and control segmentation with single scalar. Trained on only 6K unlabeled images, results across interactive, whole-image, and video segmentation. Our results highlight that self-supervised learning can unlock latent hierarchical structure in vision foundation models, transforming segmentation from discrete prediction into continuous, controllable reasoning. state-of-the-art it achieves"
        },
        {
            "title": "References",
            "content": "[1] Shahaf Arica, Or Rubin, Sapir Gershov, and Shlomi Laufer. Cuvler: Enhanced unsupervised object discoveries through exhaustive self-supervised transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2310523114, 2024. 3 [2] Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, and Nicolas Ballas. Self-supervised learning from images with joint-embedding predictive architecture. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1561915629, 2023. 3 [3] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, and Matthias Muller. Zoedepth: Zero-shot transfer by combining relative and metric depth. arXiv preprint arXiv:2302.12288, 2023. 3 9 Figure 11. Fewer prompts, more control. SAM-2 often needs multiple clicks to isolate the target object. With single granularity value, UNSAMV2 finds the correct mask efficiently, and it can also work with multi-point prompts for finer control. ularity scales. By conducting self-attention with the granularity embedding encoded by Fourier module and crossattention with image embeddings, the newly introduced token learn the representation of masks and their corresponding granularity simultaneously. The newly introduced token is crucial component of UNSAMV2 and key component to achieve the goal segment anything at any granularity. Unsupervised pseudo-labels are pivotal for granularity training, as shown in Table 5. We observe that when trained solely on human-labeled SA-1B annotations, UNSAMV2 performs unsatisfactorily compared with training that also incorporates pseudo-labels from our divide-andconquer pipeline. This points to an inherent issue with supervised datasets: they are heavily biased by human labelers notions of what constitutes an object. By contrast, our unsupervised approach focuses on intrinsic relationships among patches, enabling coverage of both instances and parts in coherent hierarchy. Design choices in UnSAMv2 training. In Table 6, we present the ablation studies on design choices for UNSAMV2 model architecture and training procedure. We study the use of LoRA in the mask decoder in Table 6a. The results show that, with LoRA, UNSAMV2 learns the concept of granularity efficiently while retaining the strong segmentation capability of SAM-2. Next, we ablate the num- [4] Shengcao Cao, Jiuxiang Gu, Jason Kuen, Hao Tan, Ruiyi Zhang, Handong Zhao, Ani Nenkova, Liang-Yan Gui, Tong Sun, and Yu-Xiong Wang. Sohes: Self-supervised openarXiv preprint world hierarchical entity segmentation. arXiv:2404.12386, 2024. 3, 4, 6 [5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. 3 [6] Danhui Chen, Ziquan Liu, Chuxi Yang, Dan Wang, Yan Yan, Yi Xu, and Xiangyang Ji. Conformalsam: Unlocking the potential of foundational segmentation models in semisupervised semantic segmentation with conformal predicIn Proceedings of the IEEE/CVF International Contion. ference on Computer Vision, pages 2404524055, 2025. 2 [7] Xianjie Chen, Roozbeh Mottaghi, Xiaobai Liu, Sanja Fidler, Raquel Urtasun, and Alan Yuille. Detect what you can: Detecting and representing objects using holistic models and body parts. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 19711978, 2014. 6, 13 [8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248255, 2009. 6, 13 [9] Carl Doersch, Pauline Luc, Yi Yang, Dilara Gokay, Skanda Koppula, Ankush Gupta, Joseph Heyward, Ignacio Rocco, Ross Goroshin, Joao Carreira, et al. Bootstap: Bootstrapped training for tracking-any-point. In Proceedings of the Asian Conference on Computer Vision, pages 32573274, 2024. 2 [10] Zhirui Gao, Renjiao Yi, Yuhang Huang, Wei Chen, Chenyang Zhu, and Kai Xu. Partgs: Learning part-aware 3d representations by fusing 2d gaussians and superquadrics. arXiv preprint arXiv:2408.10789, 2024. 3 [11] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: dataset for large vocabulary instance segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 53565364, 2019. 6, 13 [12] Abner Guzman-Rivera, Dhruv Batra, and Pushmeet Kohli. Multiple choice learning: Learning to produce multiple structured outputs. Advances in neural information processing systems, 25, 2012. [13] Oliver Hahn, Christoph Reich, Nikita Araslanov, Daniel Cremers, Christian Rupprecht, and Stefan Roth. Scene-centric unsupervised panoptic segmentation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2448524495, 2025. 3 [14] Mark Hamilton, Zhoutong Zhang, Bharath Hariharan, Noah Snavely, and William Freeman. Unsupervised semantic segmentation by distilling feature correspondences. arXiv preprint arXiv:2203.08414, 2022. 3 [15] Bharath Hariharan, Pablo Arbelaez, Lubomir Bourdev, Subhransu Maji, and Jitendra Malik. Semantic contours from inverse detectors. In 2011 international conference on computer vision, pages 991998. IEEE, 2011. 6, 13 [16] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000 16009, 2022. 3 [17] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. [18] Nan Huang, Wenzhao Zheng, Chenfeng Xu, Kurt Keutzer, Shanghang Zhang, Angjoo Kanazawa, and Qianqian Wang. Segment any motion in videos. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 34063416, 2025. 2 [19] Hanwen Jiang, Hao Tan, Peng Wang, Haian Jin, Yue Zhao, Sai Bi, Kai Zhang, Fujun Luan, Kalyan Sunkavalli, Qixing Huang, et al. Rayzer: self-supervised large view synthesis model. arXiv preprint arXiv:2505.00702, 2025. 3 [20] Markus Karmann and Onay Urfalioglu. Repurposing stable diffusion attention for training-free unsupervised interactive In Proceedings of the Computer Vision and segmentation. Pattern Recognition Conference, pages 2451824528, 2025. 3 [21] Lei Ke, Mingqiao Ye, Martin Danelljan, Yu-Wing Tai, ChiKeung Tang, Fisher Yu, et al. Segment anything in high quality. Advances in Neural Information Processing Systems, 36: 2991429934, 2023. 8 [22] Chanyoung Kim, Woojung Han, Dayun Ju, and Seong Jae Hwang. Eagle: Eigen aggregation learning for object-centric In Proceedings of unsupervised semantic segmentation. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 35233533, 2024. 3 [23] Chung Min Kim, Mingxuan Wu, Justin Kerr, Ken Goldberg, Matthew Tancik, and Angjoo Kanazawa. Garfield: In Proceedings of Group anything with radiance fields. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2153021539, 2024. 3, [24] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 40154026, 2023. 2, 3, 4, 6, 7, 13 [25] Feng Li, Hao Zhang, Peize Sun, Xueyan Zou, Shilong Liu, Chunyuan Li, Jianwei Yang, Lei Zhang, and Jianfeng Gao. Segment and recognize anything at any granularity. In European Conference on Computer Vision, pages 467484. Springer, 2024. 3 [26] Zhuwen Li, Qifeng Chen, and Vladlen Koltun. Interactive In Proceedings image segmentation with latent diversity. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 3 [27] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. 6, 13 10 [28] Zheng Lin, Nan Zhou, Chen-Xi Du, Deng-Ping Fan, and ShiMin Hu. Refcut: Interactive segmentation with reference guidance. arXiv preprint arXiv:2503.17820, 2025. 3 [29] Qin Liu, Zhenlin Xu, Gedas Bertasius, and Marc Niethammer. Simpleclick: Interactive image segmentation with simIn Proceedings of the IEEE/CVF ple vision transformers. International Conference on Computer Vision, pages 22290 22300, 2023. 3, 6, [30] Kevin McGuinness and Noel Oconnor. comparative evaluation of interactive segmentation algorithms. Pattern Recognition, 43(2):434444, 2010. 6, 13 [31] Dantong Niu, Xudong Wang, Xinyang Han, Long Lian, Roei Herzig, and Trevor Darrell. Unsupervised universal image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2274422754, 2024. 3 [32] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 3 [33] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus Gross, and Alexander Sorkine-Hornung. benchmark dataset and evaluation methodology for video object segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 724732, 2016. 6, 13 [34] Lu Qi, Jason Kuen, Yi Wang, Jiuxiang Gu, Hengshuang Zhao, Philip Torr, Zhe Lin, and Jiaya Jia. Open world enIEEE Transactions on Pattern Analysis tity segmentation. and Machine Intelligence, 2022. 6, 13 [35] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, et al. Sam 2: arXiv preprint Segment anything in images and videos. arXiv:2408.00714, 2024. 1, 2, 3, 4, 6, 7, [36] Carsten Rother, Vladimir Kolmogorov, and Andrew Blake. grabcut interactive foreground extraction using iterated graph cuts. ACM transactions on graphics (TOG), 23(3): 309314, 2004. 6, 13 [37] Jianbo Shi and Jitendra Malik. Normalized cuts and image IEEE Transactions on pattern analysis and segmentation. machine intelligence, 22(8):888905, 2000. 3 [38] Leon Sick, Dominik Engel, Sebastian Hartwig, Pedro Hermosilla, and Timo Ropinski. Cuts3d: Cutting semantics in 3d for 2d unsupervised instance segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2126521275, 2025. 3 [39] Oriane Simeoni, Gilles Puy, Huy Vo, Simon Roburin, Spyros Gidaris, Andrei Bursuc, Patrick Perez, Renaud Localizing objects with selfMarlet, and Jean Ponce. arXiv preprint supervised transformers and no labels. arXiv:2109.14279, 2021. 3 [40] Oriane Simeoni, Huy Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Michael Ramamonjisoa, et al. Dinov3. arXiv preprint arXiv:2508.10104, 2025. 3, 4, 5, [41] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. Advances in neural information processing systems, 33:75377547, 2020. 5 [42] Bin Wang, Anwesa Choudhuri, Meng Zheng, Zhongpai Gao, Benjamin Planche, Andong Deng, Qin Liu, Terrence Chen, Ulas Bagci, and Ziyan Wu. Order-aware interactive segmentation. arXiv preprint arXiv:2410.12214, 2024. 3 [43] Xinlong Wang, Zhiding Yu, Shalini De Mello, Jan Kautz, Anima Anandkumar, Chunhua Shen, and Jose Alvarez. Freesolo: Learning to segment objects without annotations. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1417614186, 2022. 3 [44] Xudong Wang, Rohit Girdhar, Stella Yu, and Ishan Misra. Cut and learn for unsupervised object detection and instance segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 3124 3134, 2023. 2, 3, 4, 6 [45] Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, and Tiejun Huang. Seggpt: Segmenting everything in context. arXiv preprint arXiv:2304.03284, 2023. 2 [46] Xudong Wang, Ishan Misra, Ziyun Zeng, Rohit Girdhar, and Trevor Darrell. Videocutler: Surprisingly simple unsupervised video instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2275522764, 2024. [47] XuDong Wang, Jingfeng Yang, and Trevor Darrell. Segment anything without supervision. Advances in Neural Information Processing Systems, 37:138731138755, 2024. 1, 2, 3, 4, 6, 7, 13 [48] Xuehao Wang, Zhan Zhuang, Feiyang Ye, and Yu Zhang. Mtsam: Multi-task fine-tuning for segment anything model. In The Thirteenth International Conference on Learning Representations, 2025. 2 [49] Yangtao Wang, Xi Shen, Yuan Yuan, Yuming Du, Maomao Li, Shell Xu Hu, James Crowley, and Dominique Vaufreydaz. Tokencut: Segmenting objects in images and videos with self-supervised transformer and normalized cut. IEEE transactions on pattern analysis and machine intelligence, 45(12):1579015801, 2023. 3 [50] Xiaoyang Wu, Daniel DeTone, Duncan Frost, Tianwei Shen, Chris Xie, Nan Yang, Jakob Engel, Richard Newcombe, Hengshuang Zhao, and Julian Straub. Sonata: Selfsupervised learning of reliable point representations. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2219322204, 2025. 3 [51] Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, and Feng Zheng. Track anything: Segment anything meets videos. arXiv preprint arXiv:2304.11968, 2023. 2 [52] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In CVPR, 2024. 3 [53] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. arXiv:2406.09414, 2024. 11 [54] Yunhan Yang, Yukun Huang, Yuan-Chen Guo, Liangjun Lu, Xiaoyang Wu, Edmund Lam, Yan-Pei Cao, and Xihui Liu. Sampart3d: Segment any part in 3d objects. arXiv preprint arXiv:2411.07184, 2024. 3 [55] Yunhan Yang, Yufan Zhou, Yuan-Chen Guo, Zi-Xin Zou, Yukun Huang, Ying-Tian Liu, Hao Xu, Ding Liang, YanPei Cao, and Xihui Liu. Omnipart: Part-aware 3d generation with semantic decoupling and structural cohesion. arXiv preprint arXiv:2507.06165, 2025. 3 [56] Mingqiao Ye, Seoung Wug Oh, Lei Ke, and Joon-Young Lee. Entitysam: Segment everything in video. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2423424243, 2025. 2 [57] Yujia Zhang, Xiaoyang Wu, Yixing Lao, Chengyao Wang, Zhuotao Tian, Naiyan Wang, and Hengshuang Zhao. Concerto: Joint 2d-3d self-supervised learning emerges spatial representations. arXiv preprint arXiv:2510.23607, 2025. 3 [58] Yian Zhao, Kehan Li, Zesen Cheng, Pengchong Qiao, Xiawu Zheng, Rongrong Ji, Chang Liu, Li Yuan, and Jie Chen. Graco: Granularity-controllable interactive segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 35013510, 2024. 3, 5, 6, 7, [59] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic understanding of scenes through the ade20k dataset. International Journal of Computer Vision, 127:302321, 2019. 6, 13 [60] Tianfei Zhou, Jianwu Li, Shunzhou Wang, Ran Tao, and Jianbing Shen. Matnet: Motion-attentive transition network for zero-shot video object segmentation. IEEE transactions on image processing, 29:83268338, 2020. 3 12 UNSAMV2: Self-Supervised Learning Enables Segment Anything at Any Granularity"
        },
        {
            "title": "Supplementary Material",
            "content": "A1. Evaluation Datasets Interactive Segmentation. Specifically, we adopt the following 7 datasets as benchmarks. GrabCut [36] consists of 50 images, each containing single instance. Berkeley [30] includes 96 images with 100 instances, some of which are more challenging for segmentation. DAVIS [33] contains 50 high-quality videos and we use 345 frames for evaluation. SA-1B [24] contains 11 million high-resolution images (average size 33004950 pixels) and 1.1 billion highquality segmentation masks. For evaluation, we randomly choose 1,000 images that are not included in UNSAMV2 training data as our evaluation set. SBD [15] includes 8,498 training images (with 20,172 instances) and 2,857 validation images (with 6,671 instances). PascalPart [7] provides part-level annotations for 20 classes from PascalVOC, totaling 193 part categories. We evaluate models on the whole validation set. PartImageNet [8] organizes 158 ImageNet classes into 11 super-categories and defines 40 distinct part categories. We utilize its validation set to evaluate models performance on segmenting part-level objects, which consists of 1,206 images and 4,553 annotated parts. Whole-Image Segmentation. We adopt the following 5 datasets to evaluate UNSAMV2s capability on discovering all instances in images. COCO (Common Objects in Context) [27] is widely utilized object detection and segmentation dataset. It consists of 115,000 labeled training images and 5,000 labeled validation images. We evaluate our model on COCO Val2017 with 5000 validation images in zero-shot manner. We use averaged recall (AR1000) as the metrics for the whole-image segmentation task. SA-1B [24] consists of 11 million high-resolution images and 1.1 billion segmentation masks. Following interactive segmentation, we randomly choose 1,000 images that are not included in UNSAMV2 training procedure as evaluation set. LVIS (Large Vocabulary Instance Segmentation) [11] has 164,000 images with over 1,200 categories and 2 million high-quality instance-level segmentation masks. It covers large number of object categories. We evaluate UNSAMV2 using its 5000 validation images. EntitySeg [34] is an open-world, class-agnostic dataset with 33,277 images, averaging 18.1 annotated entities per image. We conduct zero-shot evaluation on 1,314 lowresolution images in the validation set. ADE20K [59] contains 25,574 training and 2,000 testing images covering 365 scenes, emphasizing semantic-level segmentation. It provides labels for 150 semantic categories and 707,868 objects drawn from 3,688 categories. We evaluate zero-shot whole-image segmentation performance on the 2,000-image test split. A2. Training Details. Pseudo mask-granularity data generation. For the divide stage of our unsupervised data pipeline, we set τconf to 0.3 and τoverlap to 0.8. In the conquer stage, we leverage DINOv3 [40] ViT-B/16 backbone to extract feature embedding from the last layer of vision transformer and merge adjacent patches together based on predefined cosine similarity thresholds θ = [0.9, 0.8, 0.7, 0.6, 0.5]. We run the pipeline to generate mask-granularity pairs on 6,000 images from SA-1B in fully unsupervised manner. On average, we have 112 pseudo-labels on each image. Note that unlike UnSAM [47], UNSAMV2 is designed to learn instancepart relationships rather than only instance representations, so our granularity-aware divide-and-conquer pipeline intentionally produces fewer pseudo-labels than UnSAM, which produces 448 pseudo-labels per image. UNSAMV2 Training. We finetune SAM-2-small model for 5 epochs with pseudo-labels on 6,000 images. During finetuning, we freeze the heavy-weight Hiera image encoder and only train the granularity encoding module, granularity mask token, and the two-way transformer block in the mask decoder. For the granularity encoder, we first adopt Fourier transformation to granularity scalar with dimension dFourier = 128 and followed by 3-layer MLP. Following SAM-2 [35], UNSAMV2 adopts combination of focal loss and dice loss with ratio of 20:1. The batch size is set to 4, with the learning rate initialized at 1e-4. We apply LoRA technology to all projection layers of the transformer, setting the LoRA rank to 8. All experiments are conducted on either 2 A-100 or 4 RTX 3090 GPUs. A3. More Visualizations We present more UNSAMV2s qualitative results on interactive image segmentation in Fig. A1, whole image segmentation in Fig. A2, and video segmentation in Fig. A3. 13 Figure A1. Interactive segmentation results on SA-1B. The top row is previous SOTA GraCo [58] and the bottom row is UNSAMV2. Figure A2. Whole image segmentation on SA-1B. From top to bottom are raw images, segmentation by SAM-2 and UNSAMV2. 15 Figure A3. UNSAMV2s interactive video segmentation results on YoutubeVIS dataset at various granularity levels."
        }
    ],
    "affiliations": [
        "UC Berkeley"
    ]
}