{
    "paper_title": "NeuralOS: Towards Simulating Operating Systems via Neural Generative Models",
    "authors": [
        "Luke Rivard",
        "Sun Sun",
        "Hongyu Guo",
        "Wenhu Chen",
        "Yuntian Deng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce NeuralOS, a neural framework that simulates graphical user interfaces (GUIs) of operating systems by directly predicting screen frames in response to user inputs such as mouse movements, clicks, and keyboard events. NeuralOS combines a recurrent neural network (RNN), which tracks computer state, with a diffusion-based neural renderer that generates screen images. The model is trained on a large-scale dataset of Ubuntu XFCE recordings, which include both randomly generated interactions and realistic interactions produced by AI agents. Experiments show that NeuralOS successfully renders realistic GUI sequences, accurately captures mouse interactions, and reliably predicts state transitions like application launches. Although modeling fine-grained keyboard interactions precisely remains challenging, NeuralOS offers a step toward creating fully adaptive, generative neural interfaces for future human-computer interaction systems."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 0 0 8 8 0 . 7 0 5 2 : r NeuralOS: Towards Simulating Operating Systems via Neural Generative Models Luke Rivard1 Sun Sun2 Hongyu Guo2 Wenhu Chen1 Yuntian Deng1 1University of Waterloo 2National Research Council Canada {jlrivard, wenhu.chen, yuntian}@uwaterloo.ca {sun.sun, hongyu.guo}@nrc-cnrc.gc.ca"
        },
        {
            "title": "Abstract",
            "content": "We introduce NeuralOS, neural framework that simulates graphical user interfaces (GUIs) of operating systems by directly predicting screen frames in response to user inputs such as mouse movements, clicks, and keyboard events. NeuralOS combines recurrent neural network (RNN), which tracks computer state, with diffusion-based neural renderer that generates screen images. The model is trained on large-scale dataset of Ubuntu XFCE recordings, which include both randomly generated interactions and realistic interactions produced by AI agents. Experiments show that NeuralOS successfully renders realistic GUI sequences, accurately captures mouse interactions, and reliably predicts state transitions like application launches. Although modeling fine-grained keyboard interactions precisely remains challenging, NeuralOS offers step toward creating fully adaptive, generative neural interfaces for future human-computer interaction systems."
        },
        {
            "title": "Introduction",
            "content": "Chatting with LLM feels like using an 80s computer terminal. The GUI hasnt been invented yet, but some properties of it can start to be predicted. Andrej Karpathy Recent breakthroughs in generative models have transformed human-computer interaction, making it increasingly adaptive, personalized, and intuitive. Historically, computing interfaces were rigid and predefined, such as command-line terminals and static graphical menus [Engelbart, 1968]. The emergence of large language models (LLMs) and multimodal AI systems expanded this paradigm by enabling interactions through natural language [Radford et al., 2019, Brown et al., 2020], images [Ho et al., 2020, Lipman et al., 2022, Radford et al., 2021, Song et al., 2020b], and videos [OpenAI, 2024]. Recently, generative models have even begun simulating dynamic visual environments [Ha and Schmidhuber, 2018a, He et al., 2025], notably interactive video games [Alonso et al., 2024, Feng et al., 2024, Oh et al., 2015, Valevski et al., 2024]. These advancements suggest future where computing interfaces could become fully generative, dynamically adapting in real-time based on user inputs, contexts, and intentions [Deka et al., 2017]. In this paper, we introduce NeuralOS, first step toward realizing this vision. NeuralOS simulates an operating systems graphical interface entirely using deep neural networks. By modeling the OS interface as generative process, it directly predicts graphical frames from user input events, such as mouse movements, clicks, and keyboard interactions, without manually programmed kernels or applications. Figure 1 illustrates an example sequence generated by NeuralOS, demonstrating realistic cursor movements and window interactions predicted solely from user inputs. NeuralOS integrates two complementary neural architectures, analogous to the traditional separation between OS kernels and desktop rendering programs: recurrent neural network (RNN) [Hochreiter Preprint. Under review. (a) (d) (b) (e) (c) (f) Figure 1: Example image sequence predicted by NeuralOS, illustrating the models ability to simulate realistic GUI interactions. The sequence shows key frames as user: (a-b) moves the cursor to the Home icon, (c-d) double-clicks to open the Home folder, (e) moves the cursor toward the windows close button, and (f) clicks to close the window. Cursor positions are highlighted with red circles. Frames are generated autoregressively, conditioned on previous frames and user inputs. and Schmidhuber, 1997] maintains internal computer states (such as open applications, hidden windows, and recent actions), while diffusion-based convolutional neural renderer generates screen images. We train NeuralOS end-to-end on interaction sequences recorded from Ubuntu XFCE environments, combining randomly generated and realistic AI-generated human-like interactions. Developing NeuralOS posed several challenges. (1) Long-term state tracking was essential due to delayed interface responses (e.g., opening Firefox could take up to 30 frames); we addressed this by using an RNN-based state representation. (2) Precise cursor modeling required explicit positional encodings within our diffusion model. (3) Without pretrained encoders for GUI interactions, we developed novel pretraining method in which the RNN outputs were pretrained via regression losses and subsequently integrated into the diffusion model via finetuning. (4) Exposure bias during inference was mitigated using scheduled sampling techniques [Bengio et al., 2015, Deng et al., 2023, Ranzato et al., 2015]. (5) Extensive engineering was necessary for scalable data collection and real-time inference, leveraging parallel Docker environments and AI-generated user interactions. Experiments show that NeuralOS can generate realistic screen sequences, accurately predict mouse interactions, and reliably simulate transitions such as application openings. While computational constraints limit our models ability to capture fine-grained keyboard inputs precisely, NeuralOS offers step toward neural operating systems that adapt interfaces in real time, potentially enabling users to personalize interactions through natural language or gestures instead of fixed menus. More broadly, this work suggests the exciting possibility of blurring boundaries between software applications. For example, in the future, passive media such as movies could be transformed into interactive experiences [Yu et al., 2025]. These results point toward user interfaces that may eventually become fully AI-driven, highly flexible, and tailored to individual needs. Our code, pretrained model, and an interactive demo are available at https://neural-os.com."
        },
        {
            "title": "2 Generative Modeling of Operating System Interfaces",
            "content": "We formalize the task of simulating operating system (OS) graphical interfaces as an autoregressive generative modeling problem. At each discrete timestep t, the model predicts the next graphical frame xt based on the sequence of previously observed frames x<t = x0, x1, . . . , xt1 and user input events at = a1, a2, . . . , at up to and including the current timestep. 2 Time Step 1 Time Step 2 Time Step Frame Frame x2 Frame xt Renderer Renderer Renderer Renderer Context Renderer Context Renderer Context RNN RNN RNN Cursor Position Map Renderer Context + Ut Upper LSTM Frame xt Attention Lower LSTM Ut1 Input Embeddings (a) High-level temporal architecture of NeuralOS. (b) RNN structure at step t. Figure 2: NeuralOS Model Architecture. (a) High-level architecture of NeuralOS. At each timestep, an RNN tracks the operating systems internal state based on user inputs (cursor positions, mouse clicks, keyboard events) and previously generated frames. This state is then passed as context to diffusion-based renderer (UNet) that generates the next graphical frame. (b) Detailed two-level RNN structure at timestep t. The lower-level LSTM encodes user inputs, and then integrates visual information from the previous frame using attention. Its output is passed to the upper-level LSTM, which further processes these attention-informed representations. Feedback from the upper-level LSTM to the lower-level LSTM (Ut1) ensures that the lower-level LSTM maintains awareness of upper-level state context and previous attention results. The combined outputs of both LSTMs, and cursor position encoding, form the renderer context. This hierarchical structure maintains constant computational complexity per timestep and supports continuous state updates during inference, essential for real-time OS interface simulation. Formally, each frame xt is represented as an image tensor xt RHW C, with and denoting image height and width, respectively, and the number of color or feature channels. The input event at at timestep includes cursor coordinates (x, y), binary indicators for mouse clicks (left or right), and binary vector indicating which keyboard keys are pressed or released.1 The joint probability distribution of an OS graphical sequence given user inputs can be expressed as: (x1:T a1:T ; θ) = (cid:89) t= (xt x<t, at; θ), (1) where θ represents the parameters of our neural generative model. Unlike standard video generation, OS simulation must respond instantly to unpredictable user inputs, often causing abrupt changes in the interface, such as when new application is launched. This contrasts with the smooth, predictable transitions typical in video generation. As result, the model must maintain accurate and responsive state tracking. Next, we describe the NeuralOS architecture and training strategies designed for these requirements."
        },
        {
            "title": "3 NeuralOS Architecture",
            "content": "NeuralOS adopts modular architecture inspired by the functional separation in traditional operating systems between kernel-level state management and graphical user interface (GUI) rendering. It comprises two primary components: recurrent neural network (RNN) responsible for maintaining internal system states, and diffusion-based renderer that generates graphical frames based on these states (see Figure 2a). 1We assume user inputs are discretized and associated with each graphical frame, aggregating any events occurring between frames at the subsequent timestep. 3 Latent Diffusion Representation NeuralOS uses latent diffusion framework [Rombach et al., 2022]. We train an autoencoder to compress high-resolution OS screen images into lower-dimensional latent representations, reducing spatial dimensions by scaling factor s. All modeling is performed within this latent space. At inference time, the generated latent frames are decoded back into pixel-level images only for users. Hierarchical RNN for State Tracking NeuralOS employs hierarchical two-level RNN architecture to track the system state (see Figure 2b). Unlike transformers [Vaswani et al., 2017], whose inference complexity increases with context length, the RNN maintains constant complexity per timestep, which is crucial for continuous, long-horizon OS simulation. At each timestep t, user inputs at are encoded into embeddings. Specifically, cursor coordinates are discretized screen positions (ax ), mouse clicks are binary indicators, and keyboard keys are binary press/release states. Each component is embedded separately and then concatenated:2 , ay embed(at) = concat(embed(ax ), embed(ay ), embed(aL click )+embed(aR click (cid:88) )+ key embed(akey )). These embeddings are processed by lower-level LSTM, which also takes its previous hidden state lt1 and feedback from the previous upper-level LSTM state Ut1 as inputs: Lt, lt = LSTMlower(lt1, concat(embed(at), Ut1)), where lt denotes the hidden state and Lt denotes the corresponding output at timestep t. To handle inherent uncertainties in OS behaviors, such as unpredictable application response times, the lower-level LSTM output Lt is used as query vector to attend over the previous graphical frame using multi-headed attention [Vaswani et al., 2017]: ct = MultiHeadedAttention(query = WqLt, keys/values = Wkxt1 + Epos), where Wq, Wk are learnable projections and Epos encodes positional information of the latent frame. The attention output ct is then merged with the original lower-level LSTM output Lt: then processed by the upper-level LSTM: Ct = Lt + Woct, Ut, ut = LSTMupper(ut1, Ct). To ensure that the lower-level LSTM maintains awareness of higher-level contextual information, the upper-level LSTMs output Ut is fed back as an input to the lower-level LSTM in the next timestep. Spatial Encoding of Cursor Positions Precise cursor localization is critical for realistic OS interactions. NeuralOS explicitly encodes cursor positions using Gaussian spatial map Epos = Mt RHW . Instead of using one-hot cursor position (which can lose precision due to latent resolution constraints), we construct Gaussian map centered at the cursors scaled coordinates: (i ax Mt(i, j) = exp( /s)2 + (j ay 2 /s) ). As demonstrated in Section 7, this spatial encoding is vital for accurate cursor rendering. This map Mt, combined with LSTM outputs Lt, Ut, forms the renderer context Rt RHW : Rt = concat(WLLt, WU Ut, Mt). Diffusion-Based Renderer To render the screen image, UNet-based diffusion renderer generates the latent graphical frames conditioned on the renderer context Rt [Ronneberger et al., 2015]. xt Pθ( Rt). We concatenate the noisy image with Rt as input to the UNet, and then predict the clean image.3 2We initialize keyboard embeddings such that the release state corresponds to the zero vector. 3While most diffusion models are trained to predict noise [Ho et al., 2020], we find that predicting the clean image yields better performance in our setting. This may be because our RNN is pretrained to output the clean image, which is then fed into the diffusion UNet. Using matching target for the UNet may better preserve the signal from the RNN output. 4 Stage 1: RNN Pretraining Stage 2: Joint Training xt ˆxt MSE RNN . . . xtN atN +1 RNN xt1 at RNN . . . xtN atN +1 xt Diffusion Loss ˆxt Renderer RNN xt1 at Stage 3: Scheduled Sampling Stage 4: Context Length Extension xt Diffusion Loss ˆxt1 ˆxt xt Diffusion Loss ˆxt Renderer Renderer Renderer RNN . . . RNN RNN RNN . . . RNN . . . RNN xtN atN +1 xt2 at1 ˆxt1 at xtN atN +1 xtN 1 atN xt1 at Extended Context Figure 3: Multi-stage training pipeline for NeuralOS. (1) RNN Pretraining: The RNN is pretrained to predict latent frames using mean squared error (MSE) loss. (2) Joint Training: The pretrained RNN and the diffusion-based renderer are jointly optimized using standard diffusion loss. (3) Scheduled Sampling: To mitigate error accumulation caused by exposure bias, the most recent input frame is occasionally replaced by previously generated frame during training. (4) Context Length Extension: Input context is extended to enable the model to capture long-term dependencies."
        },
        {
            "title": "4 Multi-Stage Training Approach",
            "content": "Training NeuralOS is practically challenging due to ineffective use of RNN outputs, error accumulation during inference, and difficulties in capturing long-term dependencies due to computational constraints. To address these challenges, we take multi-stage training approach (Figure 3). Stage 1: RNN Pretraining Unlike text-to-image diffusion models such as Stable Diffusion [Rombach et al., 2022], which use pretrained textual encoders, NeuralOS uses customized RNN without pretrained checkpoints. Our initial experiments show that direct joint training often leads to the renderer ignoring RNN outputs, as indicated by negligible gradient flow into the RNN. The diffusionbased renderer receives two streams of inputs: noisy latent frames and the RNN output; and without proper initialization of the RNN, it relies solely on the noisy image inputs, resulting in an under-trained RNN. To address this, we first pretrain the RNN. Specifically, we structure the RNN output Rt RHW to match the spatial dimensions of the latent frame xt RHW C, but with more channels (C > C). The RNN is pretrained to predict the latent frames xt using mean squared error (MSE) loss: LMSE = Rt[:, :, : C] xt2 2. After pretraining, the RNN-generated frames alone tend to be blurry due to averaging multiple plausible outcomes, but crucially provide strong initialization for subsequent joint training. 5 Figure 4: Illustration of Search-Tree-Based Data Collection. We construct search tree representing OS states, starting from the initial desktop screen (root node). Each node corresponds to distinct OS state, created by clicking or double-clicking interactable GUI elements identified by computer-use agent. For clarity, only first-level transitions (opening applications) and one deeper exploration within Firefox are shown. This approach enables collecting diverse interaction data efficiently. Stage 2: Joint Training We jointly optimize the pretrained RNN and the diffusion-based renderer with standard diffusion loss [Ho et al., 2020]. The meaningful latent representations learned in RNN pretraining enable the renderer to use the RNN outputs, thus preventing the RNN outputs from being disregarded. Stage 3: Scheduled Sampling During inference, the errors accumulate over time progressively degrade the quality of the generated frames. This issue arises from exposure bias [Ranzato et al., 2015]: model trained exclusively on ground-truth frames becomes overly reliant on perfect inputs and struggles when forced to operate on its own imperfect predictions during inference. To mitigate this, we introduce scheduled sampling training stage [Ranzato et al., 2015, Deng et al., 2023]: during training, we replace the most recent input frame xt1 with the model-generated frame ˆxt1 with small probability p. This method makes the model robust against input noise, thus mitigating error accumulation and improving generation stability over extended interactions. Stage 4: Context Length Extension Although NeuralOS can model sequences of arbitrary length, hardware memory limits require shorter sequences during training, which limits exposure to long-term dependencies. To address this, we introduce final stage that extends training to longer contexts, following initial training on short context windows for efficiency. To help the model distinguish true sequence beginnings from truncated ones, we use two distinct learnable initial states for the RNN: one for genuine starts and one for mid-sequence truncations. This distinction allows the model to manage uncertainties, depending on whether it observes the full input sequence or begins mid-interaction. Curriculum Training on Challenging Transitions In our collected OS interaction dataset, substantial portion of frame transitions involve minor variations, such as slight cursor movements, which exhibit limited learning signals. To prioritize learning of significant OS state changes (e.g., opening menus or launching applications), we first train NeuralOS exclusively on challenging transitions. Specifically, we define challenging transitions as those frames whose pixel differences exceed specified threshold: xt xt12 2 > ϵ. Subsequently, we expand training to the full dataset. We apply this curriculum strategy for Stage 1 (RNN Pretraining) and Stage 2 (Joint Training). Additional Finetuning with Real-User Data After deploying NeuralOS, we collect set of real-user interaction demonstrations and find that finetuning the trained model on these real-world 6 (a) (b) Figure 5: NeuralOS Evaluation Results. (a) Heatmap illustrating predicted vs. ground truth state transitions. Each cell represents the percentage of predictions assigned to particular predicted cluster (x-axis), given ground-truth cluster (y-axis). Only the top 16 clusters are displayed here; refer to Figure 9 for the complete heatmap. (b) Comparison of cursor position errors for NeuralOS (with cursor position map), NeuralOS without the cursor position map, and random baseline. examples improves its performance on frequent user tasks. This adaptive finetuning is conducted continuously through an interactive training framework introduced by Zhang et al. [2025], enabling the model to dynamically incorporate real-time collected data and achieve improved alignment with actual user behavior. Full methodological details and results can be found in that work."
        },
        {
            "title": "5 Data Collection",
            "content": "Agent-Based Demonstrations To collect realistic user interactions, we use Anthropics Claude3.5-Sonnet computer-use agent [Anthropic, 2024], which processes screenshots and invokes provided interaction functions. To maximize interaction diversity, we structure the agents exploration process around state-space search tree representing various OS states (Figure 4). Specifically, we prompt the agent to identify all interactable GUI elements by moving the cursor to each elements center and reporting its bounding box (center, width, height). Each identified GUI element becomes node in search tree rooted at the initial OS state. The agent is then guided through this tree: for each node, it moves the cursor to the corresponding GUI element and performs single or double clicks to transition to new OS state, which then becomes child node in the tree. We iteratively expand the tree until reaching predefined maximum depth. Next, we initiate further interactions from each leaf node, allowing the agent to explore freely from these distinct OS states for fixed duration, thereby capturing diverse interaction sequences. Random Exploration We find that relying exclusively on agent-generated demonstrations introduces spurious correlations. For example, the model incorrectly associates cursor movement toward windows close button with the action of closing, even in the absence of click. To mitigate such unintended associations, we supplement the dataset with random interaction data. In generating these random interactions, we simulate mouse movements, clicks, and keyboard inputs (key presses and releases) stochastically. To improve realism, we introduce several constraints and heuristics iteratively developed through experimentation. Cursor movements are modeled using Bezier curves to emulate natural mouse trajectories. Double-click events, rare under purely random sampling, are explicitly generated. Additionally, we enforce constraints such as limiting simultaneous key presses and ensuring keys are released only if previously pressed. Detailed implementation specifics are provided in our open-source codebase. Large-Scale Data Collection Infrastructure For efficient and scalable data collection, we use Docker containers that support parallel data collection. To simplify the dataset and ensure feasible model training, we customize the desktop environment with minimal set of applications and relatively low screen resolution (512 384). 7 (a) (b) Figure 6: Ablation studies. (a) illustrates the limitations of directly using RNN outputs. (b) shows error accumulation without scheduled sampling, demonstrating its necessity for maintaining stability."
        },
        {
            "title": "6 Experimental Setup",
            "content": "Data We collected data using 64 parallel Docker containers, each configured with Ubuntu 20.04 and XFCE desktops at resolution of 512 384. Data consisted of 2K agent-based and 120K random exploration demonstrations, each 30 seconds long at 15 frames per second (fps), resulting in roughly 12TB of latent data after compression via an autoencoder. The autoencoder reduced the images by factor of 8 to latent resolution of 64 48 with 16 channels per frame. Detailed hyperparameters of the autoencoder are in Appendix D. subset of 40K videos was reserved for evaluation. Model Architecture The hierarchical RNN has two LSTM modules (each with hidden size 4,096) and multi-headed attention module (8 heads, 1,024 total dimension). The RNN output is projected to 32 channels and concatenated with the noisy latent frame (16 channels), resulting in 48-channel input to the UNet renderer. The UNet uses four resolution levels with channel multipliers of [1, 2, 3, 5], two residual blocks per level, and attention layers at resolutions 8, 4, and 2. It has base model dimension of 192 and outputs 16 channels. The final model contains 2.2B parameters for the RNN and 263M parameters for the renderer. Training and Inference NeuralOS was trained using our proposed multi-stage training approach. See Appendix for full details. The total data processing and training took approximately 4 months, requiring 17,000 GPU hours on server with 8 NVIDIA H200 GPUs (141GB memory per GPU) and an additional 6,000 GPU hours on server with 8 NVIDIA H100 GPUs (80GB memory per GPU). At inference time, we used DDIM sampling [Song et al., 2020a] with 32 steps, achieving an inference speed of 1.8 fps on single NVIDIA H100 GPU."
        },
        {
            "title": "7 Experiments",
            "content": "Given the substantial computational resources required to train NeuralOS, our evaluation focused on NeuralOS variants, ablations, and intermediate training phases. For all evaluations, we used subset of 730 examples from the reserved evaluation dataset.4 Cursor Position Accuracy We evaluated cursor-position accuracy by training regression model to predict cursor coordinates from the generated images. With the cursor position map, NeuralOS achieved highly accurate cursor localization, with an average position error of = 1.6 and = 1.4 pixels (Figure 5b). Given the 512 384 resolution of the images, this corresponds to 4This number was chosen because our clustering procedure (detailed later) identified 73 clusters of challenging frame transitions, and we selected 10 examples per cluster, resulting in total of 730 examples. 8 less than 0.5% of the frame width or height, indicating that cursor locations in generated images are very precise. This performance significantly outperformed baseline without cursor position maps (x = 130.0, = 95.8)5 and the random baseline (x = 175.4, = 126.9), confirming the importance of explicit spatial encoding for accurate cursor localization. State-Transition Modeling To evaluate state-transition modeling capability (e.g., opening applications), we clustered challenging frame transitions (those with mean pixel distance greater than 0.1 from the last input frame to the target frame, comprising approximately 2.8% of the dataset) into 73 categories. NeuralOS predictions, given identical past frames and user inputs, were matched against the closest cluster labels. As shown in Figure 5a, NeuralOS achieved an accuracy of 37.7% (diagonal alignment), significantly outperforming majority voting (1.4%). We note that off-diagonal predictions may still represent valid outcomes due to inherent timing variability in OS actions (Appendix A). Ablation Studies We further investigated the effectiveness of key training stages. Without the joint training stage (relying solely on the pretrained RNN), the predictions showed significant blurring (Figure 6a). This is caused by the MSE loss encouraging the RNN to predict averaged representations of multiple plausible outcomes rather than committing to single clear target. Additionally, cursor positions were absent, despite the model correctly capturing state transitions (e.g., opening the home folder), indicating that the RNN still implicitly encoded cursor information. Furthermore, omitting the scheduled sampling stage led to rapid deterioration in generated frame quality due to compounding prediction errors over consecutive steps, as illustrated in Figure 6b. In contrast, incorporating scheduled sampling greatly improved the models robustness (Figure 1)."
        },
        {
            "title": "8 Limitations",
            "content": "Our work represents an initial step toward fully generative OS, but several significant limitations remain. Despite substantial training compute (17,000 H200 GPU hours and 6,000 H100 GPU hours), NeuralOS is still far from replicating the capabilities of real operating system: screen resolution remains very low (512384), fine-grained keyboard interactions such as accurately typing commands in terminal are not reliably supported (see Appendix for detailed failure cases), and inference is limited to approximately 1.8 fps using an NVIDIA H100 GPU. Additionally, many critical open challenges remain, including enabling the generative OS to install new software, interact with external resources (e.g., internet connectivity), and introduce controllability beyond traditional OS boundaries, similar to how language models can be steered by user intent [Hu et al., 2017]. Addressing these limitations offers exciting avenues for future research that could realize the potential advantages of fully generative interfaces."
        },
        {
            "title": "9 Related Work",
            "content": "NeuralOS is closely related to recent generative modeling approaches for simulating interactive environments conditioned on user inputs. World Models[Ha and Schmidhuber, 2018b] introduced latent-variable models for simulating reinforcement learning environments. GameGAN [Kim et al., 2020] used generative adversarial networks (GANs) for interactive game imitation, and Genie [Bruce et al., 2024] generated playable 2D platformer worlds. More recently, diffusion-based models have emerged as powerful real-time simulators: GameNGen [Valevski et al., 2024] simulated the game DOOM, MarioVGG [Protocol, 2024] simulated Super Mario Bros, DIAMOND [Alonso et al., 2024] simulated Atari and Counter-Strike, GameGen-X [Che et al., 2024] simulated open-world games, and Matrix [Feng et al., 2024] simulated AAA games. Beyond video games, UniSim [Yang et al., 2023] developed simulators for real-world scenarios, and Pandora [Xiang et al., 2024] introduced controllable video generation using natural-language prompts. Compared to these prior works, NeuralOS addresses distinct challenges unique to OS simulation: while most GUI frame transitions involve subtle changes, accurately modeling critical discrete events, such as opening applications or menus, is essential. Additionally, precise cursor position prediction is crucial for interactive usability. NeuralOS introduces targeted model and training innovations specifically addressing these challenges, paving the way toward fully generative OS simulations. 5This baseline is an earlier model version trained for 700K steps under slightly different conditions."
        },
        {
            "title": "10 Conclusion and Future Work",
            "content": "In this paper, we introduced NeuralOS, neural framework for simulating operating system graphical interfaces through generative models. NeuralOS combines recurrent neural network (RNN) for tracking persistent computer states with diffusion-based neural renderer that generates screen images conditioned on user inputs. Trained on interaction examples from Ubuntu XFCE desktop environments, NeuralOS produces realistic screen sequences, accurately predicts mouse interactions, and captures state transitions such as opening applications. While precisely capturing fine-grained keyboard inputs remains challenging, NeuralOS provides proof-of-concept demonstration of the feasibility of fully generative operating system interfaces. NeuralOS opens several avenues for future research. First, future work could explore explicitly conditioning NeuralOS generations on natural-language user commands or gestures, enabling highly intuitive and fully customizable user experiences free from fixed interaction patterns. Second, NeuralOSs neural architecture runs natively on parallel hardware such as GPUs, potentially enabling greater efficiency and richer interactions. Finally, fully generative operating systems may fundamentally blur the boundaries between traditionally distinct software categories, such as by converting passive media like movies into immersive video games directly at the OS level. NeuralOS is just an initial step towards new computing paradigm, where user interactions and generative modeling converge to create intuitive, responsive, and adaptive systems. We hope NeuralOS can inspire new generation of research at the intersection of generative modeling and interactive computing."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "This research was supported by Compute Canada through the Resources for Research Groups (RRG) 2025 competition, awarded to Yuntian Deng (RRG No. 5275), and was also partially supported by collaborative research funding from the National Research Council of Canadas Artificial Intelligence for Design Program (AI4D-150). Additionally, Yuntian Deng acknowledges support from an NSERC Discovery Grant (RGPIN-2024-05178) and Starter Grant provided by the University of Waterloo."
        },
        {
            "title": "References",
            "content": "Eloi Alonso, Adam Jelley, Vincent Micheli, Anssi Kanervisto, Amos Storkey, Tim Pearce, and François Fleuret. Diffusion for world modeling: Visual details matter in atari. Advances in Neural Information Processing Systems, 37:5875758791, 2024. Anthropic. Introducing computer use, new claude 3.5 sonnet, and claude 3.5 haiku, October 2024. URL https://www.anthropic.com/news/3-5-models-and-computer-use. Accessed: 2025-05-14. Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks. Advances in neural information processing systems, 28, 2015. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative interactive environments. In Forty-first International Conference on Machine Learning, 2024. Haoxuan Che, Xuanhua He, Quande Liu, Cheng Jin, and Hao Chen. Gamegen-x: Interactive open-world game video generation. arXiv preprint arXiv:2411.00769, 2024. Biplab Deka, Zifeng Huang, Chad Franzen, Joshua Hibschman, Daniel Afergan, Yang Li, Jeffrey Nichols, and Ranjitha Kumar. Rico: mobile app dataset for building data-driven design applications. In Proceedings of the 30th annual ACM symposium on user interface software and technology, pages 845854, 2017. Yuntian Deng, Noriyuki Kojima, and Alexander Rush. Markup-to-image diffusion models with scheduled sampling. In The Eleventh International Conference on Learning Representations, 2023. URL https: //openreview.net/forum?id=81VJDmOE2ol. Douglas C. Engelbart. The mother of all demos. https://www.youtube.com/watch?v=fhEh3tEL1V4, 1968. Demonstration at the Fall Joint Computer Conference, San Francisco, CA. 10 Ruili Feng, Han Zhang, Zhantao Yang, Jie Xiao, Zhilei Shu, Zhiheng Liu, Andy Zheng, Yukun Huang, Yu Liu, and Hongyang Zhang. The matrix: Infinite-horizon world generation with real-time moving control. arXiv preprint arXiv:2412.03568, 2024. David Ha and Jürgen Schmidhuber. Recurrent world models facilitate policy evolution. Advances in neural information processing systems, 31, 2018a. David Ha and Jürgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018b. Haoran He, Yang Zhang, Liang Lin, Zhongwen Xu, and Ling Pan. Pre-trained video generative models as world simulators. arXiv preprint arXiv:2502.07825, 2025. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):17351780, 1997. Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, and Eric Xing. Toward controlled generation of text. In International conference on machine learning, pages 15871596. PMLR, 2017. Seung Wook Kim, Yuhao Zhou, Jonah Philion, Antonio Torralba, and Sanja Fidler. Learning to simulate dynamic environments with gamegan. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12311240, 2020. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard Lewis, and Satinder Singh. Action-conditional video prediction using deep networks in atari games. Advances in neural information processing systems, 28, 2015. OpenAI. Introducing Sora: OpenAIs Text-to-Video Model. https://openai.com/index/sora, February 2024. Accessed: 2025-04-22. Virtuals Protocol. Video game generation: practical study using mario, 2024. URL https://github.com/ Virtual-Protocol/mario-videogamegen/blob/main/static/pdfs/VideoGameGen.pdf. Preprint. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. MarcAurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level training with recurrent neural networks. arXiv preprint arXiv:1511.06732, 2015. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234241. Springer, 2015. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020a. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020b. Dani Valevski, Yaniv Leviathan, Moab Arar, and Shlomi Fruchter. Diffusion models are real-time game engines. arXiv preprint arXiv:2408.14837, 2024. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 11 Jiannan Xiang, Guangyi Liu, Yi Gu, Qiyue Gao, Yuting Ning, Yuheng Zha, Zeyu Feng, Tianhua Tao, Shibo Hao, Yemin Shi, et al. Pandora: Towards general world model with natural language actions and video states. arXiv preprint arXiv:2406.09455, 2024. Mengjiao Yang, Yilun Du, Kamyar Ghasemipour, Jonathan Tompson, Dale Schuurmans, and Pieter Abbeel. Learning interactive real-world simulators. arXiv preprint arXiv:2310.06114, 1(2):6, 2023. Hong-Xing Yu, Haoyi Duan, Charles Herrmann, William T. Freeman, and Jiajun Wu. Wonderworld: Interactive 3d scene generation from single image. In CVPR, 2025. Wentao Zhang, Yang Young Lu, and Yuntian Deng. Interactive training: Feedback-driven neural network optimization, 2025. URL https://interactivetraining.ai/. 12 xt2 xt1 xt ˆxt Figure 7: Correct prediction examples by NeuralOS. Each row shows two past frames (columns 12), ground-truth next frame (column 3), and NeuralOSs prediction (column 4). Cursor positions are marked one frame in advance with circles (red: move-only, blue: left-click, yellow: rightclick). NeuralOS correctly captures various GUI transitions, including opening menus and launching applications."
        },
        {
            "title": "A Qualitative Analysis",
            "content": "We further analyze NeuralOS qualitatively by examining successful and unsuccessful generation examples, shown in Figure 7 and Figure 8, respectively. Each row illustrates two past frames, the ground-truth next frame, and NeuralOSs predicted frame. Cursor positions in past frames are annotated with colored circles to indicate the cursors intended position at the next frame: red represents cursor movement only, blue denotes left-click actions, and yellow signifies right-click actions. Additionally, keys pressed at each frame are displayed in red text. Note that cursor annotations are shifted forward by one frame to clearly depict cursor positions expected in the immediate subsequent frame. In Figure 7, NeuralOS accurately predicts various critical GUI transitions, such as launching applications and opening menus through both mouse clicks and keyboard inputs, demonstrating its ability to capture spatial and functional dynamics. However, as shown in Figure 8, NeuralOS exhibits limitations, particularly for subtle actions like moving the cursor to Close Tab button without clicking. Moreover, NeuralOS currently struggles to accurately represent fine-grained keyboard inputs, such as specific characters typed in terminal. It is worth noting that not all mismatches between predictions and ground truth constitute errors; some discrepancies arise from variable timing in GUI responses, exemplified in Figure 8. 13 xt xt1 xt ˆxt Figure 8: Prediction examples where the generated frame does not match the ground truth frame. Layout follows Figure 7. Note that not all mismatches represent errors. For example, the third row illustrates case where the screenshot tool window is closed in the ground truth frame but remains open in the prediction. This discrepancy arises because the window-closing action (not shown due to the limited context window) can have variable timing. Thus, both the predicted and ground truth frames are valid outcomes in such scenarios."
        },
        {
            "title": "B Full State Transition Heatmap",
            "content": "Due to space constraints, the main text presents only truncated version of the state transition heatmap. In Figure 9, we provide the complete heatmap, showing NeuralOSs predictions across all identified clusters."
        },
        {
            "title": "C Interactive Web Demo",
            "content": "To facilitate user interaction with NeuralOS, we developed web-based frontend using FastAPI, accessible at https://neural-os.com/. Due to input rates (user actions) typically exceeding model inference speeds, we implemented user-input queue. When the model finishes generating frame, the system prioritizes processing recent meaningful inputs (clicks and keyboard events), discarding the redundant cursor movements if necessary. This approach maximizes the responsiveness of interactions."
        },
        {
            "title": "D Autoencoder Details",
            "content": "We trained Variational Autoencoder to compress high-dimensional OS screenshots into lowdimensional latent representations suitable for efficient training of NeuralOS. 14 Figure 9: Complete heatmap of NeuralOS state transitions. Each cell represents the percentage of predictions corresponding to predicted cluster (x-axis) given ground-truth cluster (y-axis). Diagonal entries indicate exact cluster matches. Figure 10: Examples of original images (top row) and their corresponding reconstructions (bottom row) from the trained autoencoder. Despite significant spatial compression (8 downsampling), the autoencoder preserves details. Model Architecture The architecture of the autoencoder is based on the model proposed by [Rombach et al., 2022] with some custom adjustments to improve reconstruction quality. The encoder consisted of four convolutional downsampling blocks with 128 base channels. Each downsampling stage contained two residual blocks and no attention layers. The latent channel dimension was set to 16. Table 1: Detailed hyperparameters and dataset configurations used for each stage of NeuralOSs multi-stage training. Challenging transitions are where the target frame differs from the preceding input frame by mean pixel difference greater than 0.1. These challenging transitions constitute approximately 2.8% of the full dataset. Dataset Stage Sampling Context Batch Steps LR Stage 1: RNN Pretraining Challenging transitions Full dataset 2.8% subset 100% 256 256 8 105 50K 200K 8 105 Stage 2: Joint Training (RNN + Renderer) 2.8% subset 100%"
        },
        {
            "title": "Challenging transitions\nFull dataset",
            "content": "Stage 3: Scheduled Sampling Full dataset Full dataset (lr reduced) 100% 100% Stage 4: Context Length Extension 100%"
        },
        {
            "title": "Full dataset",
            "content": "64 64 256 256 100K 8 105 1M 8 105 500K 8 105 500K 2 105 128 100K 2 10 32 32 32 32 32 32 64 0.05 0.05 0.05 Training The autoencoder was trained using combined reconstruction and adversarial loss function. We trained the autoencoder using the Adam optimizer with learning rate of 1e-6, batch size of 10, and total of 2 million training steps on our dataset. Training was conducted on single NVIDIA H200 GPU. After training, the encoder was able to compress each 512384 RGB frame into latent representation of dimension 166448 (downsampled by factor of 8 in spatial dimensions), significantly reducing memory requirements for subsequent NeuralOS model training. Examples of original and reconstructed images are provided in Figure 10. Multi-Stage Training Hyperparameters This section provides detailed hyperparameters and dataset configurations for each training stage of NeuralOS, which is summarized in Table 1. In Stage 1 (RNN Pretraining), the RNN was trained first on the subset of challenging transitions, defined as examples whose target frame differs from the last input frame by an average pixel difference greater than 0.1. These challenging transitions constitute about 2.8% of the entire dataset. We used batch size of 256 and an initial learning rate of 8 105, training for 50K steps. Afterwards, the model was trained on the full dataset (100% of data) for an additional 200K steps, maintaining the same batch size and learning rate. The context window length was fixed at 32 frames during this stage. In Stage 2 (Joint Training), the pretrained RNN and the renderer were jointly trained end-to-end, first focusing on the challenging transitions (2.8% subset) for 100K steps, then extended to the full dataset for an additional 1M steps. The learning rate remained at 8 105, with reduced batch size of 64 to stabilize diffusion training. The context length remained 32 frames, and no scheduled sampling was applied in this stage. In Stage 3 (Scheduled Sampling), we trained on the full dataset using scheduled sampling with probability = 0.05, where the most recent past frame was occasionally replaced by modelgenerated frame during training. Initially, training was conducted for 500K steps at batch size of 256 and learning rate of 8 105. The learning rate was subsequently reduced to 2 105, followed by an additional 500K training steps. The context window remained 32 frames. Finally, in Stage 4 (Context Length Extension), we increased the context window length from 32 to 64 frames to enable NeuralOS to better capture long-term dependencies. Scheduled sampling probability was maintained at = 0.05. We used lower learning rate of 2 105, reduced the batch size to 128 to fit GPU memory constraints, and finetuned the model for 100K additional steps. need to generate some training data now. Please iteratively map out every application icon/button on the desktop screen by moving your mouse to it. For each icon, move your mouse EXACTLY to its center point. DO NOT click it though. Be thorough and identify every button or icon on the screen from the top to the bottom. {{suffix}} Figure 11: Initial GUI Element Mapping (Root Node). Prompt instructing the agent to identify interactable GUI elements on the initial desktop screen. Now do final check: Look carefully at every part of the screen for any unmapped buttons. If you find any, map their exact centers like before. If you are absolutely certain ALL buttons have been mapped, respond with ONLY the final coordinate list. {{suffix}} Figure 12: Final Verification of GUI Elements (Root Node). Prompt instructing the agent to perform final check for any missed interactable GUI elements on the initial desktop screen. need to generate some training data now. First, please click/open the object at {{x}}, {{y}}. Do not progress until it is open or clicked. Double click to open if necessary. Once you confirmed it is open or clicked, stop here. Figure 13: Transitioning to New UI State (Non-Root Node). Prompt instructing the agent to transition to the current state. Now, iteratively map out all the buttons by moving your cursor to them. Focus on those related to the object you click/open at {{x}}, {{y}} - especially new buttons. For each button, move your mouse EXACTLY to its center point. DO NOT click it though. {{suffix}} Figure 14: Initial Mapping of Newly Revealed GUI Elements (Non-Root Node). Prompt instructing the agent to identify GUI elements newly revealed after transitioning to the current UI state. Please continue mapping out buttons on the screen but DO NOT click anymore buttons. Again, focus on mapping those related to the object you click/open at {{x}}, {{y}} - especially any new ones. As you exhaust the new buttons, move farther out. For each button, move your mouse EXACTLY to its center point. DO NOT click it though. {{suffix}} Figure 15: Continued Mapping of Remaining GUI Elements (Non-Root Node). Prompt instructing the agent to further map any remaining interactable GUI elements in the current UI state. Computer-Use Agent Prompts To build the search tree illustrated in Figure 4, we used structured prompts to guide the computer-use agent. Starting from the initial desktop screen (root node), we sequentially instructed the agent to first map and then verify all interactable GUI elements (Figures 11 and 12). For subsequent GUI states (non-root nodes), we initially prompted the agent to transition to each new state (Figure 13). After transitioning, we issued follow-up prompts to map and verify any newly revealed GUI elements (Figures 14 to 16). Each prompt included standardized suffix, as shown in Figure 17, to ensure that the agent outputs the coordinates and dimensions of mapped GUI elements in consistent, structured format. This allowed efficient parsing and automated processing of collected data. Now do final check: Look carefully around the screen to see if there are any new unmapped buttons related to the object you click/open at {{x}}, {{y}}. If you find any, map their exact centers like before. DO NOT click more buttons though. If you are absolutely certain ALL related buttons have been mapped, respond with ONLY the final coordinate list. {{suffix}} Figure 16: Final Verification of GUI Elements (Non-Root Node). Prompt instructing the agent to perform final check ensuring all interactable GUI elements have been identified at the current UI state. If you find keyboard input box, use the type tool specifying its center coordinates and input box type via text from the following possibilities: terminal, browser, or other, with coordinates appended in the format x:y. At the end of your response, provide structured list of ALL buttons you mapped using this exact format: For each button provide two pairs of numbers: 1. Location (L): xy coordinates where you moved the mouse 2. Size (S): x:y dimensions of the button Example format: [ L100200 S50:30, L300400 S60:40 // Button 1: Located at (100,200) with size 50x30 // Button 2: Located at (300,400) with size 60x40 ] Important: - Use EXACTLY this format: Lxy Sx:y - Separate each button with commas - Include ALL buttons you mapped - Numbers only, no text or descriptions Figure 17: Structured Output Suffix. standardized suffix appended to all prompts, guiding the agent to report mapped GUI elements using consistent coordinate and dimension format."
        },
        {
            "title": "G Cursor Position Prediction Model Training",
            "content": "To quantitatively evaluate cursor position accuracy in the generated frames (Figure 5b), we trained regression model to predict cursor coordinates directly from screen images. The training procedure is detailed below. Model Architecture We used ResNet-50 convolutional backbone pretrained on ImageNet, with modifications for fine-grained spatial localization tasks. Specifically, we adjusted the stride and dilation parameters in the final convolutional layers to reduce downsampling from 32 to 16, preserving more spatial resolution. The feature extractor output is passed through an additional intermediate convolutional layer followed by fully-connected regression head, outputting continuous cursor coordinates (x, y). Training We used the Adam optimizer with an initial learning rate of 6 105 and weight decay set to 1 105. We clipped gradients at maximum norm of 1.0. We optimized an L1 loss between predicted and ground-truth cursor positions. We trained with batch size of 16 for total of 2 epochs. Input images were used directly from collected data at the original resolution (512 384 pixels), normalized and rearranged to match the input format of ResNet-50. The training data consisted of randomly sampled frames from the full dataset, each labeled with the ground truth cursor positions captured during data collection. Training was performed on single NVIDIA A6000 GPU. The test error is 0.5 pixels for both and y. Given that each image is 512 384 pixels, this reflects extremely high localization precision. In other words, the regression model can predict the cursor location from screen image with an average deviation of less than single pixel, making it highly sensitive to small differences and suitable for evaluating fine-grained cursor accuracy in generated frames."
        },
        {
            "title": "H Scheduled Sampling Implementation Details",
            "content": "Scheduled sampling requires generating model-based frames during training, which incurs higher computational costs compared to using only ground-truth frames. In multi-GPU training setting, naively performing scheduled sampling at random intervals could result in synchronization bottlenecks, as some GPUs would have to wait for others to complete computationally expensive sampling steps. To mitigate this issue, we implemented scheduled sampling at regular intervals across all GPUs simultaneously. Specifically, for scheduled sampling probability of = 0.05, each GPU (and all examples within each GPUs batch) performs scheduled sampling exactly once every 20 steps. This synchronization approach ensures consistent training speed and prevents slowdown due to inter-GPU blocking."
        }
    ],
    "affiliations": [
        "National Research Council Canada",
        "University of Waterloo"
    ]
}