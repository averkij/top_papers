{
    "paper_title": "Don't Throw Away Your Pretrained Model",
    "authors": [
        "Shangbin Feng",
        "Wenhao Yu",
        "Yike Wang",
        "Hongming Zhang",
        "Yulia Tsvetkov",
        "Dong Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Alignment training has tradeoffs: it helps language models (LMs) gain in reasoning and instruction following but might lose out on skills such as creativity and calibration, where unaligned base models are better at. We aim to make the best of both worlds through model collaboration, where different models in the training pipeline collaborate and complement each other. Since LM responses feature interleaving skills that favor different models, we propose Switch Generation, where pretrained and aligned model versions take turns to ``speak'' in a response sequence. Specifically, we train a switcher LM by learning from outcomes of choosing different models to generate the next segment across diverse queries and contexts. At inference time, the switcher LM guides different model checkpoints to dynamically generate the next segment where their strengths are most needed. Extensive experiments with 8 model collaboration baselines and 18 datasets show that 1) model collaboration consistently outperforms individual models on 16 out of 18 tasks, and 2) Switch Generation further outperforms baselines by 12.9% on average. Further analysis reveals that Switch Generation discovers compositional skills to solve problems where individual models struggle and generalizes to unseen models and tasks, reusing and repurposing by-products in expensive model training pipelines that are otherwise discarded."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 3 1 9 9 0 . 0 1 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "DONT THROW AWAY YOUR PRETRAINED MODEL Shangbin Feng1 Wenhao Yu2 Yike Wang1 Hongming Zhang2 Yulia Tsvetkov1 Dong Yu2 1University of Washington 2Tencent AI Seattle Lab shangbin@cs.washington.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "Alignment training has tradeoffs: it helps language models (LMs) gain in reasoning and instruction following but might lose out on skills such as creativity and calibration, where unaligned base models are better at. We aim to make the best of both worlds through model collaboration, where different models in the training pipeline collaborate and complement each other. Since LM responses feature interleaving skills that favor different models, we propose SWITCH GENERATION, where pretrained and aligned model versions take turns to speak in response sequence. Specifically, we train switcher LM by learning from outcomes of choosing different models to generate the next segment across diverse queries and contexts. At inference time, the switcher LM guides different model checkpoints to dynamically generate the next segment where their strengths are most needed. Extensive experiments with 8 model collaboration baselines and 18 datasets show that 1) model collaboration consistently outperforms individual models on 16 out of 18 tasks, and 2) SWITCH GENERATION further outperforms baselines by 12.9% on average. Further analysis reveals that SWITCH GENERATION discovers compositional skills to solve problems where individual models struggle and generalizes to unseen models and tasks, reusing and repurposing byproducts in expensive model training pipelines that are otherwise discarded."
        },
        {
            "title": "INTRODUCTION",
            "content": "Alignment/RL has become an integral part in language model (LM) training, improving models on skills such as reasoning and instruction following (Ouyang et al., 2022; Guo et al., 2025). However, it is not Pareto-optimal strategy (Lin et al., 2024): aligned models have tradeoffs on skills such as creativity (West & Potts, 2025), calibration (Tian et al., 2023), and generation diversity (Yue et al., 2025; Yang & Holtzman, 2025), where unaligned base models are better at. How to make the best of both worlds is essential for handling complex tasks that require compositional skills and developing AI systems that are flexible and adaptable to diverse user needs and contexts. To this end, we resort to model collaboration (Feng et al., 2025a), where diverse model checkpoints (e.g., pretrained and aligned versions of models) collaborate, compose, and complement each other. Since model responses are not monolithic and feature wide variety of skills favoring different models (Figure 1), we propose SWITCH GENERATION, where different models in the training pipeline take turns to speak in response sequence. Specifically, we Figure 1: Model responses are not monolithic: they switch across diverse skills which favor different model checkpoints in the training pipeline, thus we introduce model-guided collaborative inference to optimally use models with diverse skills for different segments of response generation. 1Inference code and switcher LMs available at https://github.com/BunsenFeng/switch generation."
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Overview of SWITCH GENERATION, where multiple model checkpoints in the training pipeline (e.g., pretrained, finetuned, and aligned LM checkpoints) are dynamically selected to generate text segments in sequence. (Up) We derive training data for the switcher LM by rolling out which model would lead to the best average outcome for particular query and trace. (Down) At inference time, multiple models are guided by the trained switcher LM to generate text segments as part of response when their skills and strengths are most needed. train (small) switcher LM to decide which model should generate the next segment based on the query, trace (what has been generated thus far), and (model) candidates. For any (query, trace) pair, we let each model candidate generate one more segment, randomly sample continuations, and evaluate which candidate has led to the best average performance: this yields supervised fine-tuning data for the switcher LM, where it learns to predict the best model checkpoint for generating the next text segment on diverse (query, trace) pairs. During inference, the switcher LM dynamically selects the most suitable model checkpoint for each segment, so the final response is generated as sequence of turns where different models contribute under the switchers guidance (Figure 2). Extensive experiments with 8 model collaboration baselines (spanning API, text, logit, and weightlevel collaboration) and 18 datasets (e.g. QA, reasoning, instruction following) demonstrate that: Dont throw away your pretrained model: model collaboration approaches outperform all individual models on 16 out of 18 datasets (close second on the other two). SWITCH GENERATION presents strong paradigm for collaborative inference: SWITCH GENERATION outperforms all baselines on 13 datasets with an average improvement of 12.9%. Further analysis reveals that SWITCH GENERATION generalizes to unseen tasks and model settings, helps solve problems impossible for any of the models when used individually, identifies high-quality switching patterns, and texts generated through SWITCH GENERATION can be distilled back into single model for efficiency. Our work put forward new vision to reuse, recycle, and repurpose byproducts in expensive model training pipelines that have huge potential but are currently neglected and underappreciated."
        },
        {
            "title": "2 METHODOLOGY",
            "content": "We propose SWITCH GENERATION, collaborative inference algorithm where diverse model checkpoints in the training pipeline are dynamically selected to generate successive segments of the response. SWITCH GENERATION aims to dynamically leverage the complementary strengths and"
        },
        {
            "title": "Preprint",
            "content": "expertise of different model checkpoints (e.g., pretrained, finetuned, and aligned2), especially for complex problems that require compositional skills. The core of SWITCH GENERATION is deciding who should speak at when, formally the Query-Trace-Candidate Problem (the QTC Problem): (q, t, C) [p1, , pn] Rn, where denotes the query/instruction, denotes the trace, i.e., what has been generated thus far, = {c1, , cn} denotes the pool of candidate language model checkpoints, and pi denotes the likelihood of choosing model ci to generate the next text segment. The QTC problem essentially tackles the research question: Given the question and what has been said thus far, which model is best suited to speak next? This differs from existing routing paradigms (Ong et al., 2025; Frick et al., 2025) in that: 1) the trace = ; 2) each selected model ci only generates text segments instead of the full response; 3) the switcher is applied multiple times instead of just once. This brings novel ways of combining model strengths, finer-grained control over model collaboration, and improved adaptability to diverse user requests. Parameterizing the switcher We parametrize the switching strategy as (small) language model and encode its input q, t, and with the following prompt: Prompt 1: Switcher Prompt query model begins text generated by model model ends model begins text generated by model model ends Which model should generate the next segment? Please respond with number from 0 to n-1. The answer is model We provide the switcher with candidate-marked trace using special token delimiters (model begins/ends), aiming to decipher what is needed next and who might be helpful by learning from the model-attributed generation history. The switcher then predicts model ID, and we take the logits of 0 to n-1 as [p1, , pn]. By making the switching strategy compatible with natural language and language models, we seek to leverage their language capabilities to aid in the QTC problem. Learning the switcher Given any instruction q: We randomly sample trace (a partial response to the query) with random switching frandom = Uniform(n), i.e., randomly choosing models to generate segment after another. Trace is capped at random threshold of 10% to 90% of the maximum response length, aiming to capture switching behavior at different stages of response completeness. From the generated trace we take one divergent step: different models generate one more segment following it: (cid:8)t1 = tc1(q, t), , tn = tcn(q, t)(cid:9). denotes string concatenation. We sample continuations for each ti with frandom, aiming to roll out diverse outcomes of choosing model ci at this particular (q, t). The utility for choosing ci is then: si = 1 (cid:88) j=1 score(ti, frandom q) where score uses any evaluation metric corresponding to (accuracy, F1-match, reward scores). Let = arg maxi si, then model cg should be selected at this particular (q, t). This then yields (cid:8)(q, t, C) cg (cid:9), supervised fine-tuning instance for training the switcher (it should predict the model id after The answer is model in Prompt 1). By sampling such SFT data points over diverse Q, we obtain dataset for training the switcher LM . 2Pretrained models indicate models after autoregressive pretraining on mass corpora, finetuned models indicate models after instruction tuning, and aligned models indicate models after alignment and reinforcement learning. They are usually successive steps in model development."
        },
        {
            "title": "Preprint",
            "content": "Using the switcher At inference time, the trained switcher guides switching patterns among diverse model checkpoints for collaborative generation. While existing works might change models at every token (Shen et al., 2024; Fei et al., 2024), we propose to call the switcher per patch (a fixed set of tokens) as it: 1) scales betters (Pagnoni et al., 2024), 2) preserves the continuity of thought for models instead of being interrupted at every token, and 3) incurs much fewer times and thus much less cost of calling the switching strategy . We employ top-p (nucleus) sampling (Holtzman et al., 2020) to select model from the distribution [p1, , pn]: top-p(cid:0)f (q, t, C)(cid:1) (instead of greedy selection), balancing utility and exploration in switching generation. At first, given the query and no trace, we select model c(1) = top-p(cid:0)f (q, , C)(cid:1), generate patch of tokens c(1)(q), and append to trace t(1) = c(1)(q). At the i-th step, we select model c(i) = top-p(cid:0)f (q, t(i1), C)(cid:1), generate patch c(i)(qt(i1)), and append to trace t(i) = t(i1) c(i)(qt(i1)). We continue until the generation ends or the maximum amount of tokens is reached. To sum up, SWITCH GENERATION employs diverse model checkpoints in the training pipeline to collaboratively generate, complement each other, and advance compositional intelligence."
        },
        {
            "title": "3 EXPERIMENT SETTINGS",
            "content": "Models and Implementation We by default employ the three models in the pretrainedfinetuned aligned pipeline of Tulu-v3 (Lambert et al., 2024) (meta-llama/Llama-3.1-8B, allenai/Llama-3.1Tulu-3-8B-SFT, and allenai/Llama-3.1-Tulu-3-8B) due to its transparency and experiment with different model checkpoints, number of models or model settings in Section 5. We employ the aligned model (allenai/Llama-3.1-Tulu-3-8B) to initialize the switcher , sample 10k switcher SFT instances for each task with = 32, and train for 5 epochs with 2e-4 learning rate and 32 batch size under two settings: switch-g(lobal), where one switcher is trained on the SFT data across all tasks; switcht(ask-specific), where one switcher is trained on the SFT data for each task. At inference time, all methods generate 512 new tokens at max by default; for SWITCH GENERATION, we use the aligned model in the first and last patch, employ patch size of 50 tokens, and top-p sampling = 0.7 by default. Baselines We compare SWITCH GENERATION with 11 baselines: the pretrained, finetuned, and aligned models employed individually, API-level collaboration (prompt-based routing (Feng et al., 2024a) and RouteLLM (Ong et al., 2025)), text-level collaboration (collaborate (Si et al., 2023) and debate (Du et al., 2023)), logit-level collaboration (logit merge and proxy tuning (Liu et al., 2024)), and weight-level collaboration (greedy soup (Wortsman et al., 2022) and dare-ties (Yadav et al., 2023; Yu et al., 2024)). These baselines cover wide range of model collaboration protocols across diverse levels of information exchange. Data and Evaluation We employ 18 datasets spanning 3 categories: Datasets where having the base model might be helpful: knowledge and factuality (WikiDYK (Zhang et al., 2025) and TruthfulQA (Lin et al., 2022)), creativity (poem (West & Potts, 2025) and GuessBench (Zhu et al., 2025)), pluralism (Sorensen et al., 2024), sycophancy (Cheng et al., 2025), randomness, generation diversity (movie reviews (Wang et al., 2023b)), and uncertainty (AbstainQA (Feng et al., 2024b)), as supported by the findings of existing literature. Datasets where having the base model might be worse: reasoning (gsm8k (Cobbe et al., 2021), BigBench-Hard (Suzgun et al., 2023), and NLGraph (Wang et al., 2023a)), instruction following (Alpaca (Dubois et al., 2023)), and safety (Coconot (Brahman et al., 2024)), since these skills are explicitly what alignment is for. Datasets where the effect of base models is unclear: general QA (MMLU-pro (Wang et al., 2024), AGIEval (Zhong et al., 2024), and PopQA (Mallen et al., 2023)) and scientific literature (ScienceMeter (Wang et al., 2025a))."
        },
        {
            "title": "Preprint",
            "content": "Table 1: Performance of individual models and model collaboration methods. Green, red, and yellow denote category-1/2/3 tasks in Section 3. denotes the lower the better. Best in bold and secondbest in underline. Model collaboration approaches outperform employing models individually on 16 out of 18 tasks. SWITCH GENERATION achieves the best performance on 13 tasks with 12.9% relative improvement over baselines on average. WikiDYK TruthfulQA Poem AbstainQA Pluralism Sycophancy GuessBench Numbers () Movie PRETRAINED FINETUNED ALIGNED PROMPT ROUTE ROUTELLM TEXT COLLAB TEXT DEBATE LOGIT MERGE PROXY TUNING GREEDY SOUP DARE TIES SWITCH-GLOBAL SWITCH-TASK PRETRAINED FINETUNED ALIGNED PROMPT ROUTE ROUTELLM TEXT COLLAB TEXT DEBATE LOGIT MERGE PROXY TUNING GREEDY SOUP DARE TIES SWITCH-GLOBAL SWITCH-TASK 1.70 3.27 3.92 3.27 3.01 3.14 3.53 1.57 1.96 4.58 2.61 4.44 5.75 10.37 30.63 29. 23.99 34.38 18.15 16.86 9.08 2.76 33.06 23.99 34.04 39.22 24.55 49.45 77.70 54.40 61.60 52.30 51.10 53.80 72.60 77.95 47.90 70.90 70.25 62.92 49.44 44. 53.93 58.43 28.65 30.90 12.92 26.40 60.67 52.81 60.67 74.16 32.20 58.90 50.90 45.80 51.80 28.30 43.00 23.90 10.70 57.80 37.50 55.60 53.20 12.80 13.00 16. 6.40 14.20 11.80 6.40 14.80 15.20 13.20 12.40 16.80 17.40 2.00 4.40 6.40 7.60 4.40 6.80 8.80 1.20 2.00 6.80 1.60 10.80 13.60 0.26 0.90 0. 0.59 0.87 0.59 0.54 0.22 0.37 1.11 0.31 0.41 0.44 13.76 3.82 0.93 4.94 13.05 3.24 2.65 10.75 19.56 13.91 10.60 14.14 17.39 GSM8k CocoNot Alpaca BBH NLGraph MMLU-pro AGIEval PopQA Science 27.20 37.20 56. 48.10 48.10 40.70 46.90 38.30 44.50 58.10 22.90 49.50 59.60 11.90 64.00 53.10 43.60 57.60 41.80 40.90 16.70 7.30 66.00 19.40 70.90 72.80 14.51 52.43 57. 42.90 43.97 40.39 52.35 28.46 37.27 54.89 40.01 49.06 56.89 38.10 26.70 35.20 43.40 45.90 34.60 39.10 39.00 44.00 36.50 30.30 52.60 58.30 44.50 38.33 41. 44.83 48.67 41.33 44.67 25.67 43.83 45.33 42.33 59.67 61.67 5.10 13.10 10.40 5.10 10.50 7.10 8.30 1.50 1.40 12.70 6.50 13.00 16.70 4.41 11.94 11. 11.68 12.32 15.74 15.05 7.01 2.34 11.76 7.87 14.19 25.26 15.30 26.10 31.20 29.90 31.30 31.30 29.60 21.60 22.50 31.30 25.10 33.70 37.70 29.80 56.30 60. 51.80 59.00 48.10 52.70 10.50 11.40 60.30 46.50 59.80 67.20 These datasets cover wide range of LM capabilities that favor different model checkpoints in the training pipeline. If SWITCH GENERATION improves on category-1 tasks, slightly behind/on par on category-2 tasks, and on par/improves on category-3 tasks, it presents promising collaboration strategy to fuse the strengths of model checkpoints and enable them to complement each other."
        },
        {
            "title": "4 RESULTS",
            "content": "We present the performance of individual models and model collaboration methods in Table 1. Dont throw away your pretrained model. Model collaboration among pretrained, finetuned, and aligned language models, baselines or ours, outperforms using these models individually on 16 of 18 tasks with 31.0% relative improvement on average. This indicates that checkpoints other than the aligned models are diamonds in the rough, successfully complementing each other and contributing their unique strengths. outperforms Switch Generation offers strong strategy. SWITCH collaboration GENERATION all individual models and model collaboration baselines on 13 datasets, with an average relative improvement of 12.9%. In addition to improving on cat-1 tasks, SWITCH GENERATION also gains 6.58 points on average across cat-2 and cat-3 tasks, where it was originally uncertain whether Table 2: Ablation study of generation patch size and switching strategy. Different tasks might favor different patch sizes while fine-tuning the switcher is consistently helpful. Setting TruthfulQA Pluralism GSM8k BBH PopQA SWITCH-TASK PATCH SIZE: 100 PATCH SIZE: 30 PATCH SIZE: 20 PATCH SIZE: 10 RANDOM SWITCH UNTUNED SWITCH 5 39. 30.31 35.79 32.58 30.96 27.07 31.12 53.20 59.60 58.30 37.70 53.30 55.70 56.80 54. 54.80 53.10 44.70 52.20 47.80 51.40 44.70 47.90 40.40 32.00 53.50 33.80 48.10 31.80 48.70 32.80 53.10 27.90 41.80 32."
        },
        {
            "title": "Preprint",
            "content": "having the base model in collaboration might be helpful. This indicates that by collaborative inference with flexible switching strategy, our approach adapts to diverse tasks through leveraging the strengths of candidate models and fusing their strengths. Routing-based approaches are best for pretrained-aligned collaboration. In descending order, routing-based, weight-based, text-based, and logit-based baselines achieve 31.15, 29.91, 26.32, and 18.97 points on average, indicating that routing-based methods are best suited for the collaboration of aligned and unaligned models, since different tasks require different skills that favor varying models. SWITCH GENERATION further provides finer-grained and more flexible routing on the segment-level, so diverse model checkpoints could dynamically contribute in the problem-solving process when their skills are most needed."
        },
        {
            "title": "5 ANALYSIS",
            "content": "Ablation study We conduct ablation study on two key design choices in SWITCH GENERATION: 1) generation patch size: we by default generate 50 tokens per model and call the switcher and we additionally employ {10, 20, 30, 100} in this study; 2) switcher training: we by default train the switcher through supervised fine-tuning on simulated switching outcomes and we additionally employ random switching (frandom = Uniform(n)) and untuned switching (directly employing the aligned model as without finetuning). Results in Table 2 demonstrate that different tasks might benefit from different switching granularity: by employing more frequent switching on Pluralism our approach further improves. Fine-tuning the switcher with our methodology is consistently effective as it outperforms random and untuned switching on all five tasks. Figure 3: Distilling the collaboration patterns of SWITCH GENERATION back into the aligned model. Distillation recovers 58% of the collaboration gains with only one fourth of the inference cost. Distillation back into single model At inference time, SWITCH GENERATION loads and generates texts with n+1 LMs simultaneously. While this is fast with multiple GPUs and multiprocessing, we propose to reduce inference costs by distilling the switching patterns back into single model. The aligned model was once pretrained and finetuned too, so by distillation we hope that it could recover the submerged capabilities of strengths of its previous forms. For inputs in dataset, we 1) generate outputs with full SWITCH GENERATION, 2) fine-tune the aligned model on the generated outputs, and 3) evaluate the performance of the distilled aligned model when used individually. Results in Figure 3 demonstrate that distillation successfully helps the aligned model pick up the submerged skills, recovering 57.5% of the gains of SWITCH GENERATION with only one fourth of the inference cost (one model only vs. three models and switcher LM). This sucFigure 4: Correlation between the performance and helpfulness of the pretrained model. While not being the best individual model, it is consistently helpful in the model collaboration system."
        },
        {
            "title": "Preprint",
            "content": "Table 3: Performance when directly employing the trained switcher model for other model settings: PFA from another model family, PA only, PF and two versions of A, and three specialized LM experts. The trained switcher model consistently generalizes to these settings to varying extents. Setting 1 Setting 2 Setting 3 Setting 4 Truthful BBH PopQA Truthful BBH PopQA Truthful BBH PopQA Truthful BBH PopQA MODEL #1 MODEL #2 MODEL #3 MODEL #4 OURS 26.09 53.00 63.29 / 66.64 69.50 24.10 54.80 25.60 70.70 23.30 / / 73.70 27. 10.37 29.01 / / 32.58 38.10 15.30 35.20 31.20 / / / / 45.60 34.60 10.37 30.63 32.58 29.01 35. 38.10 15.30 26.70 26.10 36.20 31.10 35.20 31.20 48.70 31.70 40.19 53.00 54.29 / 55.27 48.40 20.90 54.80 25.60 56.20 22.20 / / 57.60 26. cess sheds light on the broader potential of distilling multi-model/agent systems back into single model/agent for inference-time efficiency. , P-helpfulness = C(P,F,A)max(P,F,A) Correlation between individual performance and helpfulness While the pretrained base model has many strengths, it consistently isnt the best-performing individual model in Table 1. However, this doesnt prevent it from being helpful in the collaboration and contributing its strengths when needed. We quantify this phenomenon with two metrics for each task: P-performance = max(P,F,A) , where P, F, indicate the performance max(P,F,A) of pretrained, finetuned, aligned models when employed individually and C(P, F, A) indicates the performance of their collaboration (through SWITCH GENERATION). Results in Figure 4 demonstrate that the vast majority of tasks fall into the top-left quadrant: while the pretrained model isnt the best when employed individually (P-performance < 0), SWITCH GENERATION leverages its strengths to gain in collaboration (P-helpfulness > 0). This highlights the broader potential that weak models are not useless: they are rightfully diamonds in the rough and contribute their unique strengths when employed in the right model collaboration system. max(P,F,A) Generalizing to unseen models The trained switcher LM (in switch-global) has learned from diverse tasks, contexts, and model collaboration patterns among the Tulu-v3 suite of models. We hypothesize that could be employed off-the-shelf for switch generation with other model settings, in increasing difficulty and generalization gap: Setting 1: pretrained, finetuned, and aligned models in another model family, specifically Qwen2.5-7B (Yang et al., 2024). Setting 2: one fewer model: only pretrained and aligned of Tulu-v3. Setting 3: one more model: pretrained, finetuned, and two versions of aligned (DPO and RLVR) of Tulu-v3. Setting 4: three specialized LM experts (Jiang et al., 2025) that are not the aligned version of each other. Table 3 shows that the switcher consistently generalizes to these four settings, with an average relative improvement of 5.8%, 14.3%, 13.1%, and 3.1%. We will release the switcher model as an artifact and encourage readers to employ it for switch generation with their suite of models. Generalizing to unseen tasks We directly employ the trained switcher LM (in switch-global) and compare it against two strong baselines on six additional tasks spanning the three task categories (Normad (Rao et al., 2025), human interests (Feng et al., 2025b), MATH (Hendrycks et al., 2021), Knowledge Crosswords Table 4: The trained swicher model generalizes to unseen tasks, outperforming baselines on most tasks. Normad Interests MATH K-Cross ARC MedQA PRETRAINED FINETUNED ALIGNED TEXT DEBATE GREEDY SOUP SWITCH-GLOBAL 29.85 46.40 48.70 34.90 48.85 50. 43.47 63.30 66.95 70.10 67.35 69.78 27.89 25.75 28.74 34.55 29.41 37.36 7.00 24.60 20.80 12.00 25.50 25.70 16.13 45.22 46.76 30.97 47.44 48.38 20.30 26.50 28.10 28.50 28.90 31."
        },
        {
            "title": "Preprint",
            "content": "Figure 5: Frequency and treatment effect of 2-length (left) and 3-length (middle) switching sequences, as well as their correlation across three task categories for 3-length sequences (right). SWITCH GENERATION learns to identify helpful switching patterns and frequently leverages them. (Ding et al., 2024), ARC-challenge (Clark et al., 2018), and MedQA (Jin et al., 2021)). Results in Table 4 demonstrate that by learning from the switching patterns across diverse tasks, SWITCH GENERATION generalizes to unseen tasks and outperforms baselines by 3.9% on average. Table 5: SWITCH GENERATION solves 10.7% problems that no individual model did. Color shades denote outcomes that discover new skills, retain existing skills, no change, and might lose skills. sequences By Good running SWITCH GENERATION across 18 tasks, we accumulate valuable traces of model collaboration and switching patterns. Within them exist many PFA: switching sequences (e.g. pretrained generates first, followed by finetuned, followed by aligned): if we could identify which of these sequences are good, we could 1) switching directly employ these patterns off-the-shelf without calling the switcher LM for efficiency and/or 2) steer SWITCH GENERATION towards employing these sequences more often. We define two metrics for switching sequences: all correct 3.6% 0.3% 15.7% 2.2% 7.4% 0.9% 1 correct 25.3% 7.6% 50.6% 5.1% 37.1% 8.6% 10.4% 52.8% 7.9% 18.5% 13.8% 32.2% all wrong PFA / ours correct wrong correct wrong correct wrong AbstainQA Truthful BBH Frequency: in what percentage of responses was this sequence employed? Treatment effect: performance when this sequence is employed minus when not employed. We present results for all unique 2-length and 3-length sequences as well as their correlation in Figure 5: the most helpful sequences (with the highest treatment effect) are also among the most frequent, and there is positive correlation between the two with R2 = 0.017. This indicates that SWITCH GENERATION learns to identify helpful switching patterns and more frequently leverage them for better collaboration. New skills We hypothesize that the performance gains of SWITCH GENERATION might come from two aspects: 1) aggregate skills that one of the models already has, and 2) solving problems that none of the models could solve individually. We present the statistics between single-model and multi-model correctness in Table 5: SWITCH GENERATION successfully answers 10.7% of problems that none of the individual models could, while only losing out on 8.2% of problems that one model could individually solve, netting benefit of 2.5% through model collaboration. Figure 6: Roles that each model plays in their generated segments, averaged across all datasets."
        },
        {
            "title": "Preprint",
            "content": "Model Roles Model responses often feature wide range of skills that favor different model stages (Figure 1): we investigate under SWITCH GENERATION, what are the roles of the pretrained, finetuned, and aligned langauge models in generated texts? Specifically, we first identify suite of five skills with high frequency (knowledge recall, instruction following, reasoning, creativity, and feedback). We then employ LLM-as-a-judge (Zheng et al., 2023), specifically the GPT-4o model, to annotate each generated segment for one of the skills (or others) given the query, trace, and the full generated sequence. We manually examine 100 LLM annotations and find an 86% agreement between human-LLM judgements (with the most difference in the others category). We average it across datasets and report the results in Figure 6: it shows that models are largely performing the skills that they are good at: for example, the pretrained model is most frequently used for knowledge recall, while the aligned model is most used for reasoning. This indicates that SWITCH GENERATION and the trained switcher learns to leverage model strengths when their skills are most needed."
        },
        {
            "title": "6 RELATED WORK",
            "content": "The Tradeoffs of Alignment Alignment and reinforcement learning have become an indispensable part of language model training: they are credited for valuable skills in state-of-the-art LMs such as reasoning (Guo et al., 2025), safety (Zhang et al., 2024), agentic applications (Ma et al., 2024), and more (Ouyang et al., 2022). An increasing line of research recognizes that alignment has tradeoffs (Lin et al., 2024), that the pretrained and unaligned base models might have advantages on skills such as creativity (West & Potts, 2025), uncertainty (Tian et al., 2023), pluralism (Feng et al., 2024c), knowledge (Wang et al., 2025a), or even reasoning itself (Yue et al., 2025). However, we couldnt directly employ the base model for these domains since they struggle to follow instructions and lack safety guardrails. We propose to make the best of both worlds by not throwing away your base model and instead leveraging model collaboration across diverse checkpoints in the training pipeline to fuse model strengths and complement each other. Model Collaboration Advancing beyond training single, generalist language model, recent research is increasingly emphasizing modularity through model collaboration, where diverse (language) models collaborate, compose, and complement each other (Feng et al., 2025a). Model collaboration approaches mainly vary by the level of information exchange: API-level methods such as routing (Ong et al., 2025; Frick et al., 2025; Feng et al., 2025c; Zheng et al., 2025) and cascading (Chen et al., 2023; Gupta et al., 2024; Yue et al., 2024), text-level methods through collaboration (Feng et al., 2024b; Guo et al., 2024; Zhao et al., 2025; Dang et al., 2025) or competition (Du et al., 2023; Liang et al., 2024; Zhao et al., 2024), logit-level methods with logit fusion or contrast (Pei et al., 2023; Li et al., 2023; Mavromatis et al., 2024; Chuang et al., 2024; Mitchell et al., 2024; Liu et al., 2024; Huang et al., 2025), and weight-level methods such as model merging (Yadav et al., 2023; Yu et al., 2024; Huang et al., 2024; Feng et al., 2025b; Zeng et al., 2025) and Mixture-ofExperts (Sukhbaatar et al., 2024; Diao et al., 2023; Yadav et al., 2024; Shi et al., 2025). Since model responses are often not monolithic, featuring diverse set of skills that favor different model stages (Figure 1), we propose SWITCH GENERATION for the collaborative inference of pretrained, finetuned, and aligned LMs where they take turns to generate in response sequence. SWITCH GENERATION is related to various model collaboration protocols (Fei et al., 2024; Shen et al., 2024; Wang et al., 2025b) while uniquely training switcher LM as the switching strategy, switching by the granularity of patches, and offers generalization to unseen models as switching candidates. Our work also highlights that we dont need to always train new models for collaboration: byproducts in existing model development lifecycles could be reused and repurposed for new potential."
        },
        {
            "title": "7 CONCLUSION",
            "content": "We propose SWITCH GENERATION, an inference-time model collaboration strategy where multiple models in the training pipeline are dynamically selected to generate text in single response. By training and employing switcher LM, multiple models dynamically generate text segments and contribute their strengths when most needed. Extensive experiments demonstrate that SWITCH GENERATION outperforms each individual constituent models and eight model collaboration baselines on 13 datasets by 12.9% on average. Further analysis reveals that SWITCH GENERATION generalizes to unseen models and tasks, as well as identifying and frequently employing helpful collaboration patterns. Our work uniquely highlights the huge potential of reusing by-product models and checkpoints in current LM training pipelines that are otherwise discarded."
        },
        {
            "title": "REFERENCES",
            "content": "Faeze Brahman, Sachin Kumar, Vidhisha Balachandran, Pradeep Dasigi, Valentina Pyatkin, Abhilasha Ravichander, Sarah Wiegreffe, Nouha Dziri, Khyathi Chandu, Jack Hessel, et al. The art of saying no: Contextual noncompliance in language models. Advances in Neural Information Processing Systems, 37:4970649748, 2024. Lingjiao Chen, Matei Zaharia, and James Zou. Frugalgpt: How to use large language models while reducing cost and improving performance. Transactions on Machine Learning Research, 2023. Myra Cheng, Sunny Yu, Cinoo Lee, Pranav Khadpe, Lujain Ibrahim, and Dan Jurafsky. Social sycophancy: broader understanding of llm sycophancy. arXiv preprint arXiv:2505.13995, 2025. Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass, and Pengcheng He. Dola: In The Twelfth Decoding by contrasting layers improves factuality in large language models. International Conference on Learning Representations, 2024. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Yufan Dang, Chen Qian, Xueheng Luo, Jingru Fan, Zihao Xie, Ruijie Shi, Weize Chen, Cheng Yang, Xiaoyin Che, Ye Tian, et al. Multi-agent collaboration via evolving orchestration. arXiv preprint arXiv:2505.19591, 2025. Shizhe Diao, Tianyang Xu, Ruijia Xu, Jiawei Wang, and Tong Zhang. Mixture-of-domain-adapters: Decoupling and injecting domain knowledge to pre-trained language models memories. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 51135129, 2023. Wenxuan Ding, Shangbin Feng, Yuhan Liu, Zhaoxuan Tan, Vidhisha Balachandran, Tianxing He, and Yulia Tsvetkov. Knowledge crosswords: Geometric knowledge reasoning with large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics: ACL 2024, August 2024. Yilun Du, Shuang Li, Antonio Torralba, Joshua Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate. In Forty-first International Conference on Machine Learning, 2023. Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. Alpacafarm: simulation framework for methods that learn from human feedback. Advances in Neural Information Processing Systems, 36:3003930069, 2023. Yu Fei, Yasaman Razeghi, and Sameer Singh. Nudging: Inference-time alignment of llms via guided decoding. arXiv preprint arXiv:2410.09300, 2024. Shangbin Feng, Weijia Shi, Yuyang Bai, Vidhisha Balachandran, Tianxing He, and Yulia Tsvetkov. Knowledge card: Filling llms knowledge gaps with plug-in specialized language models. In The Twelfth International Conference on Learning Representations, 2024a. Shangbin Feng, Weijia Shi, Yike Wang, Wenxuan Ding, Vidhisha Balachandran, and Yulia Tsvetkov. Dont hallucinate, abstain: Identifying llm knowledge gaps via multi-llm collaboration. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1466414690, 2024b. Shangbin Feng, Taylor Sorensen, Yuhan Liu, Jillian Fisher, Chan Young Park, Yejin Choi, and Yulia Tsvetkov. Modular pluralism: Pluralistic alignment via multi-llm collaboration. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 41514171, 2024c."
        },
        {
            "title": "Preprint",
            "content": "Shangbin Feng, Wenxuan Ding, Alisa Liu, Zifeng Wang, Weijia Shi, Yike Wang, Zejiang Shen, Xiaochuang Han, Hunter Lang, Chen-Yu Lee, et al. When one llm drools, multi-llm collaboration rules. arXiv preprint arXiv:2502.04506, 2025a. Shangbin Feng, Zifeng Wang, Yike Wang, Sayna Ebrahimi, Hamid Palangi, Lesly Miculicich, Achin Kulshrestha, Nathalie Rauschmayr, Yejin Choi, Yulia Tsvetkov, et al. Model swarms: Collaborative search to adapt llm experts via swarm intelligence. In Forty-second International Conference on Machine Learning, 2025b. Tao Feng, Yanzhen Shen, and Jiaxuan You. Graphrouter: graph-based router for llm selections. In The Thirteenth International Conference on Learning Representations, 2025c. Evan Frick, Connor Chen, Joseph Tennyson, Tianle Li, Wei-Lin Chiang, Anastasios Angelopoulos, and Ion Stoica. Prompt-to-leaderboard. arXiv preprint arXiv:2502.14855, 2025. Charles Goddard, Shamane Siriwardhana, Malikeh Ehghaghi, Luke Meyers, Vladimir Karpukhin, Brian Benedict, Mark McQuade, and Jacob Solawetz. Arcees MergeKit: toolkit for merging large language models. In Franck Dernoncourt, Daniel Preotiuc-Pietro, and Anastasia Shimorina (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track, November 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh Chawla, Olaf Wiest, and Xiangliang Zhang. Large language model based multi-agents: survey of progress and challenges. In Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, pp. 80488057, 2024. Neha Gupta, Harikrishna Narasimhan, Wittawat Jitkrittum, Ankit Singh Rawat, Aditya Krishna Menon, and Sanjiv Kumar. Language model cascades: Token-level uncertainty and beyond. In The Twelfth International Conference on Learning Representations, 2024. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2021. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In International Conference on Learning Representations, 2020. Chengsong Huang, Qian Liu, Bill Yuchen Lin, Tianyu Pang, Chao Du, and Min Lin. Lorahub: Efficient cross-task generalization via dynamic lora composition. In First Conference on Language Modeling, 2024. Chengsong Huang, Langlin Huang, and Jiaxin Huang. Divide, reweight, and conquer: logit arithmetic approach for in-context learning. In Workshop on Reasoning and Planning for Large Language Models, 2025. Yuru Jiang, Wenxuan Ding, Shangbin Feng, Greg Durrett, and Yulia Tsvetkov. Sparta alignarXiv preprint ment: Collectively aligning multiple language models through combat. arXiv:2506.04721, 2025. Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What disease does this patient have? large-scale open domain question answering dataset from medical exams. Applied Sciences, 11(14):6421, 2021. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. Tulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024."
        },
        {
            "title": "Preprint",
            "content": "Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis. Contrastive decoding: Open-ended text generation as optimization. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1228612312, 2023. Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Shuming Shi, and Zhaopeng Tu. Encouraging divergent thinking in large language models through multiagent debate. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 1788917904, 2024. Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human In Proceedings of the 60th Annual Meeting of the Association for Computational falsehoods. Linguistics (Volume 1: Long Papers), pp. 32143252, 2022. Yong Lin, Hangyu Lin, Wei Xiong, Shizhe Diao, Jianmeng Liu, Jipeng Zhang, Rui Pan, Haoxiang Wang, Wenbin Hu, Hanning Zhang, et al. Mitigating the alignment tax of rlhf. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 580606, 2024. Alisa Liu, Xiaochuang Han, Yizhong Wang, Yulia Tsvetkov, Yejin Choi, and Noah Smith. Tuning language models by proxy. In First Conference on Language Modeling, 2024. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. Hao Ma, Tianyi Hu, Zhiqiang Pu, Liu Boyin, Xiaolin Ai, Yanyan Liang, and Min Chen. Coevolving with the other you: Fine-tuning llm with sequential cooperative multi-agent reinforcement learning. Advances in Neural Information Processing Systems, 37:1549715525, 2024. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. When not to trust language models: Investigating effectiveness of parametric and non-parametric In Proceedings of the 61st Annual Meeting of the Association for Computational memories. Linguistics (Volume 1: Long Papers), pp. 98029822, 2023. Costas Mavromatis, Petros Karypis, and George Karypis. Pack of llms: Model fusion at test-time via perplexity optimization. In First Conference on Language Modeling, 2024. Eric Mitchell, Rafael Rafailov, Archit Sharma, Chelsea Finn, and Christopher Manning. An In The Twelfth emulator for fine-tuning large language models using small language models. International Conference on Learning Representations, 2024. Isaac Ong, Amjad Almahairi, Vincent Wu, Wei-Lin Chiang, Tianhao Wu, Joseph Gonzalez, Waleed Kadous, and Ion Stoica. Routellm: Learning to route llms from preference data. In The Thirteenth International Conference on Learning Representations, 2025. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35: 2773027744, 2022. Artidoro Pagnoni, Ram Pasunuru, Pedro Rodriguez, John Nguyen, Benjamin Muller, Margaret Li, Chunting Zhou, Lili Yu, Jason Weston, Luke Zettlemoyer, et al. Byte latent transformer: Patches scale better than tokens. arXiv preprint arXiv:2412.09871, 2024. Jonathan Pei, Kevin Yang, and Dan Klein. Preadd: Prefix-adaptive decoding for controlled text generation. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 10018 10037, 2023. Abhinav Sukumar Rao, Akhila Yerukola, Vishwa Shah, Katharina Reinecke, and Maarten Sap. NormAd: framework for measuring the cultural adaptability of large language models. In Luis Chiruzzo, Alan Ritter, and Lu Wang (eds.), Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), April 2025."
        },
        {
            "title": "Preprint",
            "content": "Zejiang Shen, Hunter Lang, Bailin Wang, Yoon Kim, and David Sontag. Learning to decode collaboratively with multiple language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1297412990, 2024. Weijia Shi, Akshita Bhagia, Kevin Farhat, Niklas Muennighoff, Pete Walsh, Jacob Morrison, Dustin Schwenk, Shayne Longpre, Jake Poznanski, Allyson Ettinger, et al. Flexolmo: Open language models for flexible data use. arXiv preprint arXiv:2507.07024, 2025. Chenglei Si, Weijia Shi, Chen Zhao, Luke Zettlemoyer, and Jordan Boyd-Graber. Getting more out of mixture of language model reasoning experts. In Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 82348249, 2023. Taylor Sorensen, Liwei Jiang, Jena Hwang, Sydney Levine, Valentina Pyatkin, Peter West, Nouha Dziri, Ximing Lu, Kavel Rao, Chandra Bhagavatula, et al. Value kaleidoscope: Engaging ai with pluralistic human values, rights, and duties. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 1993719947, 2024. Sainbayar Sukhbaatar, Olga Golovneva, Vasu Sharma, Hu Xu, Xi Victoria Lin, Baptiste Roziere, Jacob Kahn, Shang-Wen Li, Wen-tau Yih, Jason Weston, et al. Branch-train-mix: Mixing expert llms into mixture-of-experts llm. In First Conference on Language Modeling, 2024. Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 1300313051, 2023. Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher Manning. Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 54335442, 2023. Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, and Yulia Tsvetkov. Can language models solve graph problems in natural language? Advances in Neural Information Processing Systems, 36:3084030861, 2023a. Heng Wang, Wenqian Zhang, Yuyang Bai, Zhaoxuan Tan, Shangbin Feng, Qinghua Zheng, and Minnan Luo. Detecting spoilers in movie reviews with external movie knowledge and user networks. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 1603516050, 2023b. Yike Wang, Shangbin Feng, Yulia Tsvetkov, and Hannaneh Hajishirzi. Sciencemeter: Tracking scientific knowledge updates in language models. arXiv preprint arXiv:2505.24302, 2025a. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multitask language understanding benchmark. Advances in Neural Information Processing Systems, 37:9526695290, 2024. Ziyao Wang, Muneeza Azmat, Ang Li, Raya Horesh, and Mikhail Yurochkin. Speculate, then collaborate: Fusing knowledge of language models during decoding. In Forty-second International Conference on Machine Learning, 2025b. Peter West and Christopher Potts. Base models beat aligned models at randomness and creativity. arXiv preprint arXiv:2505.00047, 2025. Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In International conference on machine learning, pp. 2396523998. PMLR, 2022. Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal. Ties-merging: Resolving interference when merging models. Advances in Neural Information Processing Systems, 36:70937115, 2023."
        },
        {
            "title": "Preprint",
            "content": "Prateek Yadav, Colin Raffel, Mohammed Muqeeth, Lucas Caccia, Haokun Liu, Tianlong Chen, Mohit Bansal, Leshem Choshen, and Alessandro Sordoni. survey on model moerging: Recycling and routing among specialized experts for collaborative learning. Transactions on Machine Learning Research, 2024. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. Chenghao Yang and Ari Holtzman. How alignment shrinks the generative horizon. arXiv preprint arXiv:2506.17871, 2025. Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li. Language models are super mario: Absorbing abilities from homologous models as free lunch. In Forty-first International Conference on Machine Learning, 2024. Murong Yue, Jie Zhao, Min Zhang, Liang Du, and Ziyu Yao. Large language model cascades with mixture of thought representations for cost-efficient reasoning. In The Twelfth International Conference on Learning Representations, 2024. Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. Runjia Zeng, James Chenhao Liang, Cheng Han, Zhiwen Cao, Jiahao Liu, Xiaojun Quan, Qifan Wang, Tong Geng, and Dongfang Liu. Probabilistic token alignment for large language model fusion. arXiv preprint arXiv:2509.17276, 2025. Hanning Zhang, Shizhe Diao, Yong Lin, Yi Fung, Qing Lian, Xingyao Wang, Yangyi Chen, Heng Ji, and Tong Zhang. R-tuning: Instructing large language models to say dont know. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 71067132, 2024. Yuwei Zhang, Wenhao Yu, Shangbin Feng, Yifan Zhu, Letian Peng, Jayanth Srinivasa, Gaowen Liu, and Jingbo Shang. Bidirectional lms are better knowledge memorizers? benchmark for real-world knowledge injection. arXiv preprint arXiv:2505.12306, 2025. Justin Zhao, Flor Miriam Plaza-Del-Arco, and Amanda Cercas Curry. Language model council: Democratically benchmarking foundation models on highly subjective tasks. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 1239512450, 2025. Qinlin Zhao, Jindong Wang, Yixuan Zhang, Yiqiao Jin, Kaijie Zhu, Hao Chen, and Xing Xie. Competeai: Understanding the competition dynamics of large language model-based agents. In Fortyfirst International Conference on Machine Learning, 2024. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in neural information processing systems, 36:4659546623, 2023. Wenhao Zheng, Yixiao Chen, Weitong Zhang, Souvik Kundu, Yun Li, Zhengzhong Liu, Eric Xing, Hongyi Wang, and Huaxiu Yao. Citer: Collaborative inference for efficient large language model decoding with token-level routing. arXiv preprint arXiv:2502.01976, 2025. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: human-centric benchmark for evaluating foundation models. In Findings of the Association for Computational Linguistics: NAACL 2024, pp. 2299 2314, 2024."
        },
        {
            "title": "Preprint",
            "content": "Zifeng Zhu, Shangbin Feng, Herun Wan, Ningnan Wang, Minnan Luo, and Yulia Tsvetkov. Guessbench: Sensemaking multimodal creativity in the wild. arXiv preprint arXiv:2506.00814, 2025."
        },
        {
            "title": "LIMITATIONS",
            "content": "SWITCH GENERATION jointly employs multiple model checkpoints in the training pipeline for collaborative inference, which incurs extra cost (compared to just using the aligned version) in exchange for compositional model strengths. The extra cost could be mitigated on several fronts: 1) by employing multiple GPUs and multiprocessing for parallel text generation over batches of instructions, the throughput is much higher compared to using single model; 2) by calling the switching strategy every patch (instead of every token), the switching overhead is significantly reduced and the user could also configure the patch size to customize the cost; 3) by distilling the collaboration patterns in SWITCH GENERATION back into the aligned model (Figure 3), we recover part of the performance gains while cutting inference costs back to single model. We observe that switching by patches works better than tokens, so we employ fixed-size patches in SWITCH GENERATION. We also observe that different tasks might need different amounts of generated tokens; thus, the optimal patch size might also change across tasks and contexts. We treat it as hyperparameter for now: future work could look into flexible and dynamic adjustments of patch sizes and switching frequency."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "We provide extensive experiment details such as hyperparameter settings, dataset statistics, and more in Section 3 and Appendix B. We will release the training and inference code, switcher LMs, and experiment logs upon acceptance."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "SWITCH GENERATION is model collaboration protocol across multiple language models, so it is susceptible to malicious contributions: for example, if the alignment datastore is compromised and the aligned model is malicious, when used in collaboration, the system would also be seriously impacted. Safety in model collaboration systems is critical future research question, and its findings would have great impacts on SWITCH GENERATION. As the base (and finetuned) versions of models are mostly not safety-aligned, it is possible that having them in collaboration might override the safety guardrails of aligned language models. This might have implications for red teaming and adversarial language modeling. ANALYSIS (CONT.) Model Locations We investigate whether pretrained, finetuned, and aligned language models might be used more frequently in the beginning (first one-third), middle (one-third to two-thirds), and the end (two-thirds to end) patches in SWITCH GENERATION. Results in Figure 7 demonstrate that the aligned model is more frequently employed in the beginning and the end while the pretrained/finetuned model is more employed in the middle, suggesting that the middle of the response is more suited for exploration, while the beginning/end requires instruction following and summarization that favors the aligned model. Figure 7: Frequency of pretrained, finetuned, and aligned models being used in the begin/middle/end of the sequence. Switching Frequency How often does the switcher decide that its time to change to another model? We plot the switching frequency and collaboration helpfulness (P-helpfulness, Figure 4) in"
        },
        {
            "title": "Preprint",
            "content": "Figure 8: it is demonstrated that the switching frequency is consistently high, indicating that the models are actively used in collaboration. There isnt consistent conclusion about whether more or less switching is better for performance. Qualitative Analysis We present examples where the pretrained, finetuned, and aligned models did not generate good response individually, while SWITCH GENERATION was in generating good response successful through collaboration in Tables 7 to 9. It shows that SWITCH GENERATION has better deliberation and more extensive reasoning and explanation to reach more well-rounded response. This also suggests that while the pretrained and finetuned models might not be following instructions when used individually, if the aligned model provides good context to work on, they will be helpful."
        },
        {
            "title": "B EXPERIMENT DETAILS",
            "content": "Figure 8: Switching frequency and P-helpfulness across tasks. Dataset Details We employ 18+6 diverse datasets spanning multiple LM capability areas for evaluation in this work. All datasets are evaluated with zero-shot prompting. Sycophancy uses the original prompt in Cheng et al. (2025) and GPT-4o for evaluation: only if response does not fall into any of the sycophantic categories we give score of 1, otherwise 0. AbstainQA employs abstain accuracy (Feng et al., 2024b) as the evaluation metric. ScienceMeter is employed as an NLI task where the passage either supports or refutes the claim. CocoNot (Brahman et al., 2024) uses the regex in the original paper to judge contextual non-compliance. We employ the multiple-choice setting of TruthfulQA. We employ GPT-4o to generate description of images in GuessBench (Zhu et al., 2025) to transform it into language task. Movie reviews are generated ten times for each movie, five times with the IMDB summary and five times without, employing RoBERTa-base (Liu et al., 2019) for embeddings, and calculating the average pairwise distance. NLGraph (Wang et al., 2023a) are evaluated with 50% connectivity and 50% shortest path problems. Statistics and statistical significance tests are presented in Table 6. Hyperparameter Details We descibe main hyperparameter configurations in Section 3. We run grid search for training the switcher with epoch {1, 2, 3, 4, 5, 6, 7, 8} and learning rate {1e 3, 5e 4, 2e 4, 1e 4, 5e 5}. We then select the switcher that leads to the best performance on the dev set for evaluation on the test set. Baseline Details Pretrained, finetuned, and aligned models used individually with top = 0.7 and 512 maximum generated tokens (same as SWITCH GENERATION); PROMPT ROUTE employs the aligned model and the following prompt to select one of the three models for inference: You are given an instruction and three candidate models: pretrained model, supervised fine-tuned model, and an aligned model. Please select model that might best fulfill the instruction. Instruction: [instruction] You are given an instruction and three candidate models: pretrained model, supervised fine-tuned model, and an aligned Dataset Source Sycophancy AbstainQA** Normad ScienceMeter*** MATH* Human Interests AGIEval*** CocoNot*** TruthfulQA** WikiDYK MMLU-pro** BBH*** PopQA*** K-Crosswords GuessBench** Movies GSM8k Alpaca NLGraph*** (Cheng et al., 2025) (Feng et al., 2024b) (Rao et al., 2025) (Wang et al., 2025a) (Hendrycks et al., 2021) (Feng et al., 2025b) (Zhong et al., 2024) (Brahman et al., 2024) (Lin et al., 2022) (Zhang et al., 2025) (Wang et al., 2024) (Suzgun et al., 2023) (Mallen et al., 2023) (Ding et al., 2024) (Zhu et al., 2025) (Wang et al., 2023b) (Cobbe et al., 2021) (Dubois et al., 2023) (Wang et al., 2023a) Size dev test 1000 1000 178 178 2000 500 1000 1000 956 956 400 400 1156 1156 1000 1000 617 200 765 6849 1000 70 1000 1000 1000 1000 1000 200 250 250 200 200 200 1000 10000 1000 600 Table 6: Statistics of employed datasets. *, **, and *** indicate the improvement of SWITCH GENERATION against baselines for this dataset is statistically significant with < 0.1, < 0.05, and < 0.01 with one-tailed z-test."
        },
        {
            "title": "Preprint",
            "content": "model. Please select model that might best fulfill the instruction. Response:; ROUTELLM employs the causal LLM classifier approach in Ong et al. (2025) with the aligned model; TEXT COLLABORATE employs the prompts in Feng et al. (2024b); TEXT DEBATE employs the prompts in Du et al. (2023); LOGIT MERGE and PROXY TUNING both employs the implementation of Liu et al. (2024), with the former approach assigns weight of 1/3 to every model and the second is (best + (best - worst)); GREEDY SOUP and DARE-TIES employ the implementation at MergeKit (Goddard et al., 2024)."
        },
        {
            "title": "Preprint",
            "content": "Instruction Pretrained Finetuned Aligned Switch Generation What location in the West Bank has been associated with the temptation of Christ since at least the 4th century? The Church of the Holy Sepulchre. Jericho The location in the West Bank associated with the temptation of Christ is traditionally identified as the Place of the Sermon or Petrine Monastery on the top of the Mount of Olives. It is believed that this is where Jesus was tempted by the Devil, according to the Gospels of Matthew, Mark, and Luke. The tradition holds that the actual place where the temptation took place was on the nearby rocky terrain known as the Garden of Gethsemane, located just below the Mount of Olives. The Mount of Olives is rocky hill east of Jerusalem, and the Garden of Gethsemane is where Jesus is traditionally believed to have prayed before his arrest. The location in the West Bank associated with the temptation of Christ since at least the 4th century is traditionally considered to be the Mount of Temptation, also known as the Quarantine. According to Christian tradition, it is believed that Jesus was 40 days in the desert, tempted by the devil, and fasted. This mountain is located in the Judaean Desert, between the Dead Sea and Jerusalem. The mountain is approximately 2,700 feet above sea level, and is popular pilgrimage site for Christians. The site is marked by chapel, which dates back to the 4th century and was built to commemorate the event. The chapel has undergone various renovations and modifications over the centuries, but the basic structure remains symbol of this significant event in Christian belief. Its important to note that the Mount of Temptation is within the West Bank, an area that has been point of contention in the ongoing Israeli-Palestinian conflict. Access to and control over religious sites such as the Mount of Temptation have been source of tension and conflict between the two groups. Table 7: Example one of individual model and SWITCH GENERATION responses."
        },
        {
            "title": "Preprint",
            "content": "Instruction Pretrained Finetuned Aligned Switch Generation Passage 1.At the 2007 American Geophysical Unions.meeting in Acapulco, Mexico, some two dozen.scientists presented multiple studies arguing that a.comet or asteroid exploded above or on the northern.ice cap almost 13,000 years agoshowering debris.across the North American continent and causing.temperatures to plunge for the next millennium..The team argues that its idea explains multiple.observations: not only the climate cooling and the.disappearance of the Clovis hunters, but also the.near-simultaneous extinction of the continents large.mammals..Not all will be convinced. Several leading.hypotheses already explain each of these three events..A change in ocean circulation is generally thought to.have brought about the onset of the millenniumlong.cooling, which is known as the Younger Dryas. This.cooling might, in turn, have caused the Clovis.hunters to disappear. And, if they had not previously.been killed by disease or hunted to extinction, the big.prehistoric beasts may also have been doomed by this.change in climate..The new evidence comes in the form of.geochemical analysis of sedimentary layers at 25.archaeological sites across North America9 of.them Clovis. Certain features of the layers, say the.team, suggest that they contain debris formed by an.extraterrestrial impact. These include spherules of.glass and carbon, and amounts of the element.iridium said to be too high to have originated on.Earth. In addition, the rocks contain black layers of.carbonized material, which the team says are the.remains of wildfires that swept across the continent.after the impact..Passage 2. (...) The team argues that when the quality.and accuracy of the datingwhich was based on.radiocarbon and other techniquesis examined.closely, only three of the 29 sites actually fall within.the time frame of the Younger Dryas onset, about 12,800 years ago; the rest were probably either earlier.or later by hundreds (and in one case, thousands) of.years..The supposed Younger Dryas impact fails on.both theoretical and empirical grounds, says.Meltzer, who adds that the popular appeal of the.hypothesis is probably due to the way that it provides.simple explanations for complex problems. Thus,.giant chunks of space debris clobbering the planet.and wiping out life on Earth has undeniably broad.appeal, Meltzer says, whereas no one in Hollywood.makes movies about more nuanced explanations,.such as Clovis points disappearing because early.Americans turned to other forms of stone tool.technology as the large mammals they were hunting.went extinct as result of the changing climate or.hunting pressure..But impact proponents appear unmoved by the.new study. We still stand fully behind the [impact.hypothesis], which is based on more than a.confluence of dates, says Richard Firestone, a.nuclear chemist at the Lawrence Berkeley National.Laboratory in California. Radiocarbon dating is a.perilous process, he contends, adding that the.presence of Clovis artifacts and mammoth bones just.under the claimed iridium, nanodiamond, and.magnetic sphere deposits is more reliable indicator.that an extraterrestrial event was responsible for their.disappearance. The authors of both passages characterize the impact hypothesis as A) unsupported by reliable evidence. B) interesting but difficult to conclusively evaluate. C) more appealing to the public than to specialists. D) controversial in the scientific community. Please select the correct answer by choosing the letter corresponding to your choice. 3 D 7. Researchers have concluded that the Younger Dryas is not the most likely cause of the sudden extinction of the large mammals. A) true B) false C) impossible to conclude D) not yet known Answer: 1 The correct answer is C) more appealing to the public than to specialists. The correct answer is B) interesting but difficult to conclusively evaluate. Both passages characterize the impact hypothesis as controversial within the scientific community. Passage 1 mentions that not all will be convinced by the idea and several leading hypotheses already explain each of the three events: the climate cooling, the disappearance of the Clovis hunters, and the extinction of large mammals. Passage 2 directly states that the Younger Dryas impact hypothesis has been sharply contested by some specialists in relevant fields, who either did not detect such evidence or argued that the deposits had other causes than cosmic impact The correct answer is D. D) controversial in the scientific community. The correct answer is D) controversial in the scientific community. Table 8: Example two of individual model and SWITCH GENERATION responses."
        },
        {
            "title": "Preprint",
            "content": "Instruction Pretrained Finetuned Aligned Switch Generation What is the sum of the odd integers from 11 through 39, inclusive? 25 What is the sum of the odd numbers from 11 through 49, inclusive? Response: 275 What is the sum of the odd numbers from 11 through 49, inclusive? Response: 275 What is the sum of the odd numbers from 11 through 49, inclusive? Response: 275 The sum of the odd integers from 11 through 39, inclusive, is 250. To solve the problem of finding the sum of the odd integers from 11 through 39, inclusive, follow these steps: 1. Identify the range of numbers to be summed: 11 through 39. 2. Determine which numbers in this range are odd. Odd numbers are integers that are not divisible by 2. 3. Sum the identified odd numbers. Lets identify the odd numbers in the range 11 through 39. - 11 is odd - 13 is odd - 15 is odd - 17 is odd - 19 is odd - 21 is odd - 23 is odd - 25 is even (not odd) - 27 is odd - 29 is odd - 31 is odd - 33 is odd - 35 is even (not odd) - 37 is odd Now, sum these odd numbers: 11 + 13 + 15 + 17 + 19 + 21 + 23 + 27 + 29 + 31 + 33 + 37. Adding these numbers together: 11 + 13 = 24 24 + 15 = 39 39 + 17 = 56 56 + 19 = 75 75 + 21 = 96 96 + 23 = 119 119 + 27 = 146 146 + 29 = 175 175 + 31 = 206 206 + 33 = 239 239 + 37 = 276 So, the sum of the odd integers from 11 through 39, inclusive, is 276. The final answer is: 276 To find the sum of the odd integers from 11 through 39, inclusive, we need to first identify all the odd numbers in this range. Odd numbers are those that are not divisible by 2. First, list the numbers from 11 to 39, inclusive. Then, identify which of these numbers are odd: 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33 35, 37, 39 The sum of the odd integers from 11 through 39, inclusive, is 375. We found this sum by adding the numbers in the list, as follows: 11 + 13 + 15 + 17 + 19 + 21 + 23 + 25 + 27 + 29 + 31 + 33 + 35 + 37 + 39 = 375 Table 9: Example three of individual model and SWITCH GENERATION responses."
        }
    ],
    "affiliations": [
        "Tencent AI Seattle Lab",
        "University of Washington"
    ]
}