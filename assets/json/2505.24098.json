{
    "paper_title": "HardTests: Synthesizing High-Quality Test Cases for LLM Coding",
    "authors": [
        "Zhongmou He",
        "Yee Man Choi",
        "Kexun Zhang",
        "Jiabao Ji",
        "Junting Zhou",
        "Dejia Xu",
        "Ivan Bercovich",
        "Aidan Zhang",
        "Lei Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Verifiers play a crucial role in large language model (LLM) reasoning, needed by post-training techniques such as reinforcement learning. However, reliable verifiers are hard to get for difficult coding problems, because a well-disguised wrong solution may only be detected by carefully human-written edge cases that are difficult to synthesize. To address this issue, we propose HARDTESTGEN, a pipeline for high-quality test synthesis using LLMs. With this pipeline, we curate a comprehensive competitive programming dataset HARDTESTS with 47k problems and synthetic high-quality tests. Compared with existing tests, HARDTESTGEN tests demonstrate precision that is 11.3 percentage points higher and recall that is 17.5 percentage points higher when evaluating LLM-generated code. For harder problems, the improvement in precision can be as large as 40 points. HARDTESTS also proves to be more effective for model training, measured by downstream code generation performance. We will open-source our dataset and synthesis pipeline at https://leililab.github.io/HardTests/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 8 9 0 4 2 . 5 0 5 2 : r HARDTESTS: Synthesizing High-Quality Test Cases for LLM Coding Zhongmou He1 Yee Man Choi1 Kexun Zhang1 Jiabao Ji2 Junting Zhou1 Dejia Xu3 Ivan Bercovich Aidan Zhang1 1Carnegie Mellon University 2UC Santa Barbara Lei Li1 3UT Austin"
        },
        {
            "title": "Abstract",
            "content": "Verifiers play crucial role in large language model (LLM) reasoning, needed by post-training techniques such as reinforcement learning. However, reliable verifiers are hard to get for difficult coding problems, because well-disguised wrong solution may only be detected by carefully human-written edge cases that are difficult to synthesize. To address this issue, we propose HARDTESTGEN, pipeline for high-quality test synthesis using LLMs. With this pipeline, we curate comprehensive competitive programming dataset HARDTESTS with 47k problems and synthetic high-quality tests. Compared with existing tests, HARDTESTGEN tests demonstrate precision that is 11.3 percentage points higher and recall that is 17.5 percentage points higher when evaluating LLM-generated code. For harder problems, the improvement in precision can be as large as 40 points. HARDTESTS also proves to be more effective for model training, measured by downstream code generation performance. We will open-source our dataset and synthesis pipeline at https://leililab.github.io/HardTests/."
        },
        {
            "title": "Introduction",
            "content": "Post-training large language models (LLMs) with outcome verifiers2 (Guo et al., 2025; Kimi Team et al., 2025) can greatly improve their reasoning ability. LLMs trained with these techniques are approaching the level of the best humans on challenging problems in math and programming olympiads (OpenAI et al., 2025). To properly assign outcome rewards in post-training, reliable verifiers are needed for both reinforcement learning and (self-) distillation. Verification is non-trivial process. How good are current verifiers? How to get better verifiers? How much does verifier quality matter in LLM post-training? Verification loops become increasingly less tractable as the notion of correctness increases in complexity. For math, it is relatively straightforward to determine correctness by looking at the answer, whereas verifying programs needs execution. An effective approach to verify programs is through test cases (Le et al., 2022; Singh et al., 2023). However, most datasets of coding problems and associated test cases are less than comprehensive. 60% of the programs that pass test cases in APPS (Hendrycks et al., 2021) are in fact, wrong. 46% of the programs that pass test cases in CodeContests (Li et al., 2022) are semantically correct, but too inefficient to pass human-written tests. More importantly, scraping human-written tests is unfeasible according to our study, for 80% of the problems, human-written test cases are proprietary and impossible to scrape, demanding synthesized tests. Previous test synthesis attempts, such as TACO (Li et al., 2023), have limited reliability, with the false positive rate being more than 90% for difficult problems in our experiments. Equal contribution. Project lead. Correspondence to {zhongmou,yeemanc,kexunz}@andrew.cmu.edu 2In this paper, the term verifier refers to rule-based systems that attempt to check the correctness of problem solutions. It is used to differentiate from model-based rewards, such as those in RLHF. Verifiers are not necessarily formal and do not necessarily guarantee correctness. Preprint. Under review. Figure 1: HARDTESTS test cases are significantly better than the baselines. The large improvement in precision indicates that our tests greatly reduce false positives and are indeed harder. The low quality of synthetic tests is due to the challenging nature of coding problems. Coding competitions often require efficient solutions with advanced data structures and algorithms. bad choice of algorithm can lead to well-disguised wrong solution, which may easily pass most random tests but still break on human-written special cases. For example, on random rooted tree with nodes and depth of d, an algorithm with the time complexity of Θ(nd) can be very efficient, as E[d] = Θ(log n) for randomly generated trees (Devroye et al., 2012). For such an algorithm to time out, the test case needs to be valid tree that is large enough (so that is large) and special enough (so that is large). chain (each non-leaf node has exactly one child), whose depth = can cause the algorithm to be as slow as Θ(n2). We need valid, comprehensive tests that cover edge cases. Generating valid and comprehensive tests is hard. Existing test synthesis methods, such as CodeT (Chen et al., 2023) and TACO (Li et al., 2023) rely on LLMs to directly write test inputs. While this works when the test inputs are small, it can barely keep the test inputs valid at larger scale, let alone make them special. To alleviate these issues, we propose HARDTESTGEN, an LLM-based test synthesis pipeline. Our main insights are 1) Test case validity is better preserved when generated from LLM-produced programs rather than directly from the LLMs themselves, and 2) Each test generator has different hypotheses about the programs under test and creates tests from different distribution. With these insights, HARDTESTGEN prompts an LLM with different aspects to consider for test cases, extracts LLM-generated test generator programs, and filters the test cases using human-written oracle programs, which widely exist for all problems in online coding competitions. With HARDTESTGEN, we curate HARDTESTS, comprehensive dataset for coding competitions with 47,136 problems and high-quality test cases. As shown in Figure 1, compared to existing test synthesizers, HARDTESTS tests are more reliable in terms of precision and recall when evaluating programs. The gap in precision can be as large as 40 percentage points for harder problems. Higher reliability of verification makes HARDTESTS the ideal playground for post-training research in the coding domain. To further demonstrate the benefits of high-quality tests, we conduct post-training experiments with HARDTESTS and baseline tests. Our experiments in 3 different scenarios show that test quality matters significantly for self-distillation and reinforcement learning. Higher-quality tests can lead to improvements in downstream performance. However, our results also indicate that test quality is less important for teacher distillation. In summary, this work provides: HARDTESTGEN, an LLM-based test synthesis pipeline that generates high-quality test cases for coding problems, improving precision by 11.3 points and recall by 17.5 points on average. HARDTESTS, comprehensive problem set for competition-level code generation, with 47,136 problems, among which 32.5k have high-quality test cases generated by HARDTESTGEN. Empirical analyses on how test quality affects LLM post-training. We show that test quality is of great importance for reinforcement learning and self-distillation."
        },
        {
            "title": "2 Related work",
            "content": "RLVR. Reinforcement learning has shown great potential in improving LLM reasoning abilities in various domains, such as math (Guo et al., 2025; Zeng et al., 2025b; Ren et al., 2025) and coding (OpenAI, 2025; Liu & Zhang, 2025; Luo et al.). The resulting long-reasoning LLMs, such as OpenAI-o3 (OpenAI, 2024) and DeepSeek-R1 (Guo et al., 2025), largely outperform shortreasoning LLMs through simple RL training to improve outcome-based reward, i.e., whether the model-generated code solution passes all test cases. Although some previous works have explored heuristic rules for selecting training data to improve RL performance (Ye et al., 2025; Wang et al., 2025b; Li et al., 2025) or reward design (Hou et al., 2025; Kimi Team et al., 2025; Costello et al., 2025), the impact of test case quality on coding LLMs during RL training remains underexplored. In this work, we show that high-quality test cases, those better at detecting subtle bugs in code, can largely improve coding LLM performance after RL training. LLM-based test synthesis. Test cases are crucial in evaluating the functional correctness and performance of LLM-generated code. Benchmarks such as HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), and APPS (Hendrycks et al., 2021) provide hand-written test cases that serve as proxy for code correctness. However, such human-authored test cases are often only publicly available for limited set of problems. Early approaches such as EvoSuite (Fraser & Arcuri, 2011) and Pynguin (Lukasczyk & Fraser, 2022) employ search-based heuristics methods. More recently, CodeContests (Li et al., 2022) generates additional test cases by mutating existing crawled inputs. Several efforts leverage LLMs to synthesize test inputs and (pseudo)-oracle programs for test outputs. CodeT (Chen et al., 2023) and ALGO (Zhang et al., 2023) rely on LLMs to generate both tests and reference programs for existing coding problems. EvalPlus (Liu et al., 2023) extends HumanEval with more tests by providing the reference implementation to LLMs to synthesize seed input.Similarly, TACO (Li et al., 2023) also generates test inputs with LLMs and outputs with reference implementation. STGen (Peng et al., 2025) generates stressful test cases for evaluating the time efficiency of code. KodCode (Xu et al., 2025) and AceCoder (Zeng et al., 2025a) push synthetic data even more by generating coding questions, reference solutions, and tests all with LLMs. Although existing LLM test synthesis methods prove to be useful in many scenarios, their quality is far from perfect. We present more thorough discussion on the quality issues in LLM synthetic tests and their implications in Appendix A.1. Concurrently with our work, rStar-Coder (Liu et al., 2025) and HF-Codeforces (Penedo et al., 2025) also study more reliable test synthesis in the competition context. Comparing to them, our work highlights thorough analysis of test quality and unique set of post-training experiments that demonstrate the downstream effects of high-quality tests. Datasets for competition code generation. Existing datasets for competition code generation focus on scaling the number of problems and CoTs. Luo et al. filters high-quality 24k problemset of TACO, LiveCodeBench, and other contest programming problems. CodeForces-CoTs, the dataset of 10k Codeforces problems created by Penedo et al. (2025), contains 100k reasoning traces and solutions generated by DeepSeek R1. OpenCodeReasoning (Ahmad et al., 2025) also compiles dataset of 28k problems, generates 735k reasoning traces, and filters them for syntactic correctness. While these efforts have shown that better models can be trained with more data and more trajectories from teacher models, they are facing code verifiability crisis, as described by Open-R1 (Face, 2025), and programs that pass test cases in these problem sets are not necessarily correct. In our paper, we curate HARDTESTS, the competitive coding problem set with the most number of problems (47k). More importantly, we push the scaling of training data towards higher quality of test cases and evaluate how test quality affects model training."
        },
        {
            "title": "3.1 Problem Setting",
            "content": "Coding problems. We study test generation for coding problems with natural language specifications. We denote the space of problem specifications as , the space of candidate programs as Y, and the space of test suites as V. test suite is set of test cases {t1, t2 , tV }. test case is pair (a, c), where is an input to program, and is checker for the corresponding output 3. candidate 3In most cases, the output checker is simply comparison between golden outputs and program outputs. Others might be equivalence checkers that do not directly compare strings. 3 Figure 2: Comparison of the input generation process between previous test synthesizers (left) and HARDTESTGEN (right). program takes an input and generates an output y(a), which is then sent to the output checker for boolean verdict c(y(a)) {, }. When exceeds pre-defined runtime limit, its verdict is also . Oracle tests and correctness. For every coding problem , we assume the existence of an oracle test suite V, which definitively tells the correctness of program , i.e. Correctness(y, ) := (cid:94) ci(y(ai)). (ai,ci)V (1) In practice, the oracle tests are usually carefully written and proprietary by problem authors. Only very few of them are available for downloading, which makes them infeasible for model training. Oracle programs. Compared to rarely available oracle tests, oracle programs (y such that Correctness(y, ) = ) are available for almost all coding problems in online competitions. Therefore, we assume the existence of oracle programs in our setting. Test synthesis. Given problem x, and an oracle program y, the task of test synthesis is to create test suite that agrees with , i.e., we want Correctness(y, ) = Correctness(y, ) for as many ys as possible. In HARDTESTGEN, we create set of inputs {a1, a2, , aV } and utilize the oracle program to get the outputs, i.e., ci = y(ai)."
        },
        {
            "title": "3.2 Generating Inputs of Test Cases",
            "content": "We synthesize three types of test inputs. One is directly generated by an LLM, while the other two are generated by LLM-generated programs. Before generating inputs, we first prompt an LLM to generate an input validator in Python that checks whether given input satisfies all the constraints in the problem specification. We subsequently prompt the LLM to generate the inputs. In the prompt, we include the input validator and an oracle program, as we find that doing so increases the likelihood of synthesizing valid inputs. Figure 2 illustrates the differences between the input generation processes of previous test synthesiziers and HARDTESTGEN. Type 1. Directly Generated Inputs. We prompt an LLM to directly generate nD = 10 inputs by imitating the sample test cases provided in the problem specification. This type of input is typically small in scale, making it easy to generate and understand, and allowing for quick testing of the candidate programs functional correctness. Type 2. Regular Inputs. Regular inputs are generated randomly according to the constraints specified in the problem specifications. For most problems, we prompt an LLM to generate Python function gR with no parameters that returns random input on each call. We call this function nR = 20 times to get nR random inputs. For some problems, there are some unusual categories of outputs that are rarely triggered by random inputs. For example, when problems expected output is either Yes or No, the correct output for almost all random inputs might be Yes. In such cases, random inputs can potentially lead to false positives. For these problems, we prompt an LLM to generate mR functions, each corresponding to one output category (e.g., Yes and No). We call each function nR = 10 times to obtain total of mR nR inputs with their outputs specified. Type 3. Hacking Inputs. Some well-disguised false positives cannot be easily detected with random inputs. For example, some programs may be functionally correct but inefficient in worst-case scenarios, or some programs may fail to handle certain edge cases that require special treatment. Therefore, we first prompt an LLM to list several candidate programs for the problem in natural language. Then, we prompt it to generate mH input generation functions, each attempting to cause one candidate program to fail. Each function is called nH = 10 times, generating mH nH inputs. After generating the inputs, we filter out all inputs that fail to pass the examination of the input validator."
        },
        {
            "title": "3.3 Generating Outputs of Test Cases",
            "content": "We use human-written oracle programs that exist for all online competitions to test outputs. For each problem, we use at most noracle = 8 oracle programs, prioritizing those from more reliable sources. Each oracle program generates outputs for all synthesized inputs. If the outputs generated by two oracle programs match for more than 90% of the cases, we consider the outputs to be acceptable and adopt the matching portion as the final outputs. For the majority of problems, simple string comparison between two outputs is sufficient to determine whether they match. However, some problems require special judge. For example, problem might require returning set (where element order does not matter) or sequence of operations that achieves certain effect. In that case, we prompt an LLM to implement special judge function. This function takes the input and two outputs as parameters, and returns Boolean value indicating whether the two outputs are equivalent. In our dataset, 25.4% of the problems require special judge function. In subsequent training and testing processes, this function will continue to be used to determine whether the candidate output and the reference output match. In our dataset, we use GPT-4o to generate all of the above content. For all functions that need to be generated, we include two to three carefully crafted examples in the prompts. The implementation details of HARDTESTGEN (e.g., prompts), the number of generated test cases, the failure rate and reasons for failure, as well as concrete example, are provided in Appendix A.2."
        },
        {
            "title": "3.4 HARDTESTS: 47k Problems with High-Quality Test Cases",
            "content": "The HARDTESTS dataset comprises 47,136 competitive programming problems with high-quality test cases, aggregated from 13 major online judges (OJs) for competitive programming. The dataset is constructed from five direct data sources: Codeforces, AtCoder, Luogu, CodeContests (Li et al., 2022), and TACO (Li et al., 2023). We apply HARDTESTGEN to synthesize test cases for 32.5k problems among them. The detailed constitution and description of the data sources are described in Appendix A.3. Cleaning, deduplication, and decontamination. For problems with only non-English descriptions, we translated them into English using GPT-4o. To handle overlapping content among the five direct data sources, we filtered out duplicated problems using problem IDs and n-gram overlaps in description, prioritizing versions from the original platforms rather than mirror sites. For correct programs, we retained all available versions and annotated them with their respective sources. We conduct decontamination by removing the problems that are in LiveCodeBench (Jain et al., 2025b) from our dataset. Since most of its problems are from Codeforces and AtCoder, we directly compare the URLs to the problems. Labelling difficulty. We retained the difficulty labels assigned by all five data sources in our dataset. In the experiments presented in Section 4, we used the difficulty labels from Luogu, as it provides consistent and fine-grained labels for problems from both AtCoder and Codeforces. Luogus difficulty labels are divided into seven levels, with the first level representing beginner-level problems and the seventh level corresponding to problems at the level of national competitions."
        },
        {
            "title": "4.1 Evaluation Criteria",
            "content": "We regard the testing of candidate programs as binary classification process: program is classified as positive if it passes all test cases, and negative otherwise. To directly assess the quality of test cases, we evaluate how good they are as binary classifiers. Given problem x, an oracle test suite , synthesized test suite , and set of candidate programs {y1 yn}, we categorize the programs with their correctness according to and . When and both find candidate program correct, its true positive (TP). When finds program correct while finds it wrong, its false positive (FP). Similarly, we can define true negatives and false negatives. With these categories defined, we use precision and recall to measure test quality: Precision = P + , Recall = P + ."
        },
        {
            "title": "4.2 Baselines",
            "content": "CodeContests. CodeContests (Li et al., 2022) primarily consists of problems from Codeforces. Codeforces only provides test cases within certain length constraints. CodeContests collects these and refers to them as private test cases. Additionally, it generates new test cases by introducing random perturbations to the private test cases; these are referred to as \"generated test cases.\" This gives CodeContests an unfair advantage as it has access to the distribution of oracle tests. In our experiments, we only use generated test cases, which reduces the unfairness but does not eliminate it. TACO. TACO (Li et al., 2023) integrates several existing datasets, such as APPS (Hendrycks et al., 2021) and CodeContests (Li et al., 2022), while retaining their test cases. In addition to this, TACO generates several additional test cases by using GPT-4o to directly generate the inputs and using oracle programs for outputs. Furthermore, we observed that for some problems from AtCoder and Codeforces, the TACO test cases included official test cases. To ensure fair comparisons, we removed these official test cases. Ablative Baselines. We also evaluate HARDTESTGEN with only Type 1 or Type 2 inputs to demonstrate the necessity of all 3 types. Notably, the scenario with only Type 1, LLM directly generated inputs, very much resembles many existing test synthesis methods such as KodCoder (Xu et al., 2025), except that they synthesize not only the inputs but also the oracle programs."
        },
        {
            "title": "4.3 Evaluation Pipeline",
            "content": "To evaluate the accuracy of rewards that our test cases can give to model training, we evaluate the precision and recall over candidate programs generated by LLMs and written by humans on subsets of problems in HARDTESTS. Details about the evaluation protocol can be found in Appendix A.4. Generating candidate problems. To compare our tests with other synthesizers, we choose the problems that exist in both HARDTESTS and the baseline datasets. For problems from AtCoder, we select 653 problems that exist in both HARDTESTS and TACO. For problems from Codeforces, we select 600 problems that exist in HARDTESTS, CodeContests, and TACO. Generating candidate programs. We compare our tests with baseline tests on candidate programs generated by 3 LLMs and also by human programmers. Specifically, we use three LLMs: Qwen2.5Coder-7B-Instruct (Yang et al., 2024), Qwen2.5-Coder-14B-Instruct, and GPT-4o. For each problem, we sample 10 candidate programs from each LLM using temperature of 0.7 and top-p of 0.95. We also randomly select 10 real-world human submissions for each problem. Generating gold labels. We need gold labels to compute precision and recall. For AtCoder, we run candidate programs on official tests that have been previously made available. For Codeforces, we submit candidate programs to the website to obtain ground-truth verdicts. The human-written candidate programs are sampled from MatrixStudio/Codeforces-Python-Submissions, which provides official verdicts. We then use synthetic test cases to classify the correctness of these programs and compare the results against the ground-truth labels, thereby evaluating test case quality."
        },
        {
            "title": "4.4 Results",
            "content": "We evaluate the correctness of programs written by three LLMs and human programmers for problems from AtCoder and Codeforces using test cases from TACO, CodeContests, and HARDTESTS. The results are in Table 1 and 2. We present qualitative analyses of the synthetic tests in Appendix A.5. We find that HARDTESTS significantly outperforms TACO and CodeContests in terms of both precision and recall under most evaluation settings. Moreover, this advantage becomes more pronounced as problem difficulty increases. For example, for the Qwen2.5-Coder-7B-Instruct model on AtCoder problems with difficulty level 4+, TACO achieves precision of 21.67 and recall of 68.42, whereas HARDTESTS achieves precision of 60.00 and recall of 94.74. This implies that using HARDTESTS during RL training would yield more true positive rewards and much fewer false positive rewards. Furthermore, we observe that as the source of programs becomes less intelligent (ranging from human-written to 7B LLM-generated), the precision advantage of HARDTESTS becomes more pronounced. We attribute this to the fact that less skilled programmers are more likely to produce functionally correct but inefficient programs. For instance, among incorrect human-written programs, 14.9% are due to TLE (Time Limit Exceeded), whereas among the incorrect programs written by 6 Table 1: Precision and recall of the test cases of TACO, HARDTESTS, and ablative baseline on AtCoder. HT-TYPE1 refers to the results using only the test cases of Type 1 from HARDTESTS. while TH-TYPE1+2 refers to the results using only the test cases of Type 1 and Type 2 from HARDTESTS. difficulty difficulty 2 difficulty 3 prec. recall prec. recall prec. recall difficulty 4+ recall prec. average prec. recall TACO HT-TYPE1 HT-TYPE1+2 HARDTESTS TACO HT-TYPE1 HT-TYPE1+2 HARDTESTS TACO HT-TYPE1 HT-TYPE1+2 HARDTESTS 99.48 94.63 97.85 98.15 99.82 96.21 97.31 97.99 100.0 99.42 99.53 99. 77.09 99.84 99.35 98.95 78.00 99.72 99.02 99.02 73.06 99.47 99.18 99.18 89.66 74.70 97.58 97.64 93.24 77.22 94.79 96.95 99.75 94.31 99.82 100. Qwen2.5-Coder-7B-Instruct 62.90 100.0 100.0 97.58 69.07 42.20 74.23 86.75 81.71 89.02 87.80 87.80 21.67 10.40 58.06 60.00 Qwen2.5-Coder-14B-Instruct 69.00 100.0 100.0 95.50 67.29 99.32 97.60 97.43 80.23 58.90 87.50 93.33 73.40 96.81 96.81 96.81 GPT-4o 92.74 86.39 96.04 96. 74.08 99.42 98.45 98.45 39.33 18.50 65.71 67.16 62.07 45.56 79.00 84.18 68.42 94.74 94.74 94.74 72.92 97.92 95.83 93.75 71.05 99.67 99.01 98. 69.97 55.48 81.93 85.64 78.16 62.71 86.33 88.86 88.64 81.42 93.60 94.94 72.53 95.90 95.47 94.77 73.33 98.61 97.92 96.27 71.37 99.47 98.56 98. Table 2: Precision and recall of the test cases of TACO, CodeContests, and HARDTESTS evaluated using LLM-generated programs for problems on Codeforces. difficulty 1 difficulty 2 difficulty 3 difficulty 4 average prec. recall prec. recall prec. recall prec. recall prec. recall Qwen2.5-Coder-7B-Instruct"
        },
        {
            "title": "TACO\nCodeContests\nHARDTESTS",
            "content": "89.64 85.74 87.61 86.13 89.24 95.45 71.07 63.73 93.30 92.91 97.64 98.82 31.06 23.80 48.38 39.47 47.54 55. 9.82 6.67 50.00 100.0 100.0 100.0 50.40 44.99 69.82 79.63 83.61 87.47 Qwen2.5-Coder-14B-Instruct"
        },
        {
            "title": "TACO\nCodeContests\nHARDTESTS",
            "content": "80.67 79.70 83.19 87.45 95.59 98.64 83.88 79.29 88.44 81.13 86.16 100.0 53.87 46.49 67.47 73.88 91.84 80. 25.76 18.68 46.58 100.0 100.0 90.80 61.05 56.04 71.42 85.62 93.40 92.46 GPT-4o"
        },
        {
            "title": "TACO\nCodeContests\nHARDTESTS",
            "content": "99.58 99.47 98.80 80.02 94.80 98.20 95.76 95.25 95.66 81.72 89.89 98.71 89.64 86.83 92.73 74.83 87.08 88. 62.64 58.28 79.82 78.17 94.31 94.31 86.91 84.96 92.00 78.69 91.52 94."
        },
        {
            "title": "TACO\nCodeContests\nHARDTESTS",
            "content": "96.28 94.15 93.29 88.89 90.06 94.13 91.48 87.47 85.15 81.59 89.99 95.05 75.90 73.11 73.71 78.84 85.10 93. 62.23 56.80 64.16 73.77 79.88 89.35 81.47 77.88 79.08 64.62 69.01 74.42 the three LLMs, 30.0% are due to TLE. Consequently, the larger and more diverse test cases in HARDTESTS are more likely to catch inefficient programs than the small-scale test cases in TACO and CodeContests. Compared with the ablative baselines in Table 1, HARDTESTS that includes Type2 (Regular) and Type3 (Hacking) test cases consistently leads to precision improvement ranging from 2% to 48%, while the decrease in recall is always within 2.5%. This demonstrates the necessity for having different types of tests."
        },
        {
            "title": "5 Downstream Effects of Test Case Quality in LLM Post-Training",
            "content": "In this section, we aim to answer two questions with HARDTESTS: when does verifier/test quality matter, and how much does it matter in post-training? We run experiments in 3 different post-training scenarios: teacher-distillation, self-distillation, and reinforcement learning. We examine how much verifier quality affects the training results in code generation, if any."
        },
        {
            "title": "5.1 Experiment Setup",
            "content": "Teacher-distillation. Various papers, such as DeepSeek-R1 (Guo et al., 2025) suggest that fine-tuning smaller student model with reasoning trajectories from stronger reasoning model can greatly improve the students performance. In this scenario, verifiers can be used to filter out the incorrect trajectories. We sample one reasoning trajectory with C++ solution program from DeepSeek-R1 for each question in HARDTESTGEN, obtaining 46.6k trajectories in total after deduplication and decontamination against all LiveCodeBench questions. We fine-tune two models from Qwen2.5Coder-Instruct-7B: one with all 46.6k trajectories, the other with 13k trajectories that are correct according to HARDTESTS. As baseline, we also evaluate OlympicCoder-7B (Face, 2025), another Qwen2.5-Coder derivation fine-tuned with 100k trajectories of 10k Codeforces problems. Self-distillation. Fine-tuning model with its own reasoning trajectories can also improve its reasoning ability (Zelikman et al., 2022). Hence, determining which trajectories to use is critical issue. To examine the effects of test quality, we sampled 5 traces of Qwen3-4B and used the tests generated by HARDTESTGEN for filtering. We selected 4989 questions where there is at least one Qwen3-4B generated program that passes the tests and at least one that fails the tests. We create 3 datasets for self-fine-tuning, each containing one trajectory per question. The bad 5k randomly samples one incorrect trajectory for each question. The good 5k randomly samples one correct trajectory. The random 5k randomly samples one trajectory, regardless of its correctness, for each question. We further fine-tune Qwen3-4B with these 3 datasets and compare the performance of the resulting models. All our fine-tuning experiments were done with Llama-factory (Zheng et al., 2024). Reinforcement learning. Verifier feedback is an option for distillation, but it is must for reinforcement learning. To investigate how verifier quality affects RL, we train Qwen3-4B with RL using the same problem set, the identical training setup, and different test cases. We select problem set with 5k problems that exist in both HARDTESTS and TACO for training. We use modified version of veRL (Sheng et al., 2024) inspired by Code-R1 (Liu & Zhang, 2025) for training with GRPO (Shao et al., 2024). When program passes all tests, it gets reward of 1, otherwise, it gets reward of 0. We compare different verifiers by looking at the final performance and the validation curve. Evaluation protocol. We use LiveCodeBench (Jain et al., 2025b) version 5 to evaluate the model performance. Since all the programs we use for tuning are in C++, we build an evaluation pipeline for evaluating C++ programs for LiveCodeBench and select 105-problem subset where all problems have test cases of stdin type. We name this subset of problems we use LiveCodeBench-105. Details about our training and evaluation procedure can be found in Appendix A.6, including the problems and hyperparameters we use for training and the sampling parameters we use for evaluation."
        },
        {
            "title": "5.2 Results",
            "content": "Teacher-distillation benefits more from question scaling than test quality or sample scaling. We evaluate models fine-tuned from Qwen2.5-Coder-7B using different training sets on LiveCodeBench105 and report the results in Table 3. Note that the difficulty labels are obtained from LiveCodeBench. The model trained with HARDTESTS with all 46.6k examples outperforms OlympicCoder-7B (trained with 100k trajectories of 10k questions), suggesting that the quality and diversity of training questions matter more than the number of training samples. Interestingly, the model trained on smaller but more curated subsets (13k filtered trajectories) does not match the performance of using larger, unfiltered data, suggesting that data scaling dominates trajectory correctness in the teacher-distillation setting. This observation aligns with the concurrent findings from OpenCodeReasoning (Ahmad et al., 2025). Self-distillation performance is highly dependent on sample quality and needs good verifier. We evaluated variants of Qwen3-4B models self-distilled with different 5k subsets on LiveCodeBench105 and present the results in Table 4. Model self-distilled from incorrect samples identified by HARDTESTGENs tests drops more significantly in pass@k. Self-distillation with randomly selected 8 Table 3: pass@k (%) of teacher-distilled LLMs based on Qwen2.5-Coder-7B on LiveCodeBench-105. Easy pass@1 Medium pass@1 Hard pass@ pass@10 pass@"
        },
        {
            "title": "All",
            "content": "QC2.5-7B-Ins OlympicCoder-7B (100k trajectories) QC2.5-7B-Ins + HARDTESTS (13k, filtered) QC2.5-7B-Ins + HARDTESTS (46.6k, full) 58.75 65.83 77.08 83.65 9.58 41.25 29.17 44.58 2.46 2.46 1.75 6.49 16.95 25.81 25.24 32.86 27.62 46.67 39.05 53. Table 4: pass@k (%) self-distilled LLMs based on Qwen3-4B on LiveCodeBench-105. Easy pass@1 Medium pass@1 Hard pass@1 pass@1 All pass@ pass@10 Qwen3-4B Qwen3-4B (with bad 5k) Qwen3-4B (with random 5k) Qwen3-4B (with good 5k) 88.75 84.17 84.58 85.42 53.33 45.42 36.25 47.08 11.05 8.07 9.12 10.53 38.48 34.00 32.75 36. 52.04 48.42 50.85 55.15 56.19 54.92 57.14 60.00 Figure 3: RL Validation Rewards Over Time. Reward from HARDTESTS makes the training better. data could harm pass@1 even more, despite the slight improvements in pass@10. In contrast, using 5k subset verified by HARDTESTGENs test cases results in smaller drop in pass@1 and notable gain in pass@5 and pass@10, suggesting that verifiers are important to self-distillation. Test quality matters significantly for reinforcement learning. As shown in Figure 3, the validation reward curve for HARDTESTS during RL training is generally higher than that for TACO. This indicates that for the same problems, HARDTESTS is giving better rewards. To evaluate on LiveCodeBench-105, we run the best checkpoints (according to valid reward) of both training jobs within 100 steps. As reported in Table 5, TACO tests hurt the models overall performance, while HARDTESTS improves the models overall performance. Table 5: pass@k (%) for LLMs RL-trained from Qwen3-4B on LiveCodeBench-105. pass@1 pass@5 pass@10 Qwen3-4B Qwen3-4B (RL with TACO) Qwen3-4B (RL with HARDTESTS) 38.48 36.95 39.42 52.04 51.01 57. 56.19 57.14 64."
        },
        {
            "title": "6 Conclusion",
            "content": "We present HARDTESTGEN, an LLM-based test synthesis pipeline, which is used to create HARDTESTS, competitive coding dataset with 47k problems and significantly higher-quality tests. We examine when and how much test quality matters in LLM post-training, showing that harder tests generated by HARDTESTGEN can indeed help LLM post-training in many scenarios."
        },
        {
            "title": "Limitation",
            "content": "Although HARDTESTS has higher-quality tests than the baselines, they are still not as good as humanwritten ones. Moreover, we assume the existence of oracle solutions to utilize HARDTESTGEN, which may not be true for some coding domains. To address this issue, we briefly discuss an initial idea for synthesizing tests without oracles in Appendix A.7. Another limitation of the HARDTESTGEN is that the code being tested is constrained to single file that uses Standard I/O for input and output. However, many real-world coding problems are more complicated, e.g. coding problems in SWEbench that may involve file I/O or web I/O, and we leave the exploration of applying HARDTESTGEN to these scenarios as future work."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "The OpenAI API credits used in this paper were partially supported by the OpenAI Research Access Program. The training compute used was partially supported by National Center for Supercomputing Applications and ScOp Venture Capital. KZ was partially supported by ChipAgents.ai."
        },
        {
            "title": "References",
            "content": "Wasi Uddin Ahmad, Sean Narenthiran, Somshubra Majumdar, Aleksander Ficek, Siddhartha Jain, Jocelyn Huang, Vahid Noroozi, and Boris Ginsburg. Opencodereasoning: Advancing data distillation for competitive coding, 2025. URL https://arxiv.org/abs/2504.01943. Toufique Ahmed, Martin Hirzel, Rangeet Pan, Avraham Shinnar, and Saurabh Sinha. Tdd-bench verified: Can llms generate tests for issues before they get resolved?, 2024. URL https://arxiv. org/abs/2412.02883. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. Program synthesis with large language models, 2021. URL https://arxiv.org/abs/2108.07732. Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. Codet: Code generation with generated tests. In ICLR, 2023. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021. URL https://arxiv.org/abs/2107.03374. Caia Costello, Simon Guo, Anna Goldie, and Azalia Mirhoseini. Think, prune, train, improve: Scaling reasoning without scaling models. arXiv preprint arXiv: 2504.18116, 2025. Luc Devroye, Omar Fawzi, and Nicolas Fraiman. Depth properties of scaled attachment random recursive trees. Random Structures & Algorithms, 41(1):6698, 2012. Hugging Face. Open r1: fully open reproduction of deepseek-r1, January 2025. URL https: //github.com/huggingface/open-r1. Gordon Fraser and Andrea Arcuri. Evosuite: automatic test suite generation for object-oriented software. In Proceedings of the 19th ACM SIGSOFT Symposium and the 13th European Conference on Foundations of Software Engineering, ESEC/FSE 11, pp. 416419, New York, NY, USA, 2011. Association for Computing Machinery. ISBN 9781450304436. doi: 10.1145/2025113.2025179. URL https://doi.org/10.1145/2025113.2025179. 10 Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et al. Measuring coding challenge competence with apps. arXiv preprint arXiv:2105.09938, 2021. Bairu Hou, Yang Zhang, Jiabao Ji, Yujian Liu, Kaizhi Qian, Jacob Andreas, and Shiyu Chang. Thinkprune: Pruning long chain-of-thought of llms via reinforcement learning. arXiv preprint arXiv: 2504.01296, 2025. Kush Jain, Gabriel Synnaeve, and Baptiste Rozière. Testgeneval: real world unit test generation and test completion benchmark, 2025a. URL https://arxiv.org/abs/2410.00752. Naman Jain, Manish Shetty, Tianjun Zhang, King Han, Koushik Sen, and Ion Stoica. R2E: Turning any github repository into programming agent environment. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (eds.), Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp. 2119621224. PMLR, 2127 Jul 2024. URL https://proceedings.mlr.press/v235/jain24c.html. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. In The Thirteenth International Conference on Learning Representations, 2025b. URL https://openreview.net/forum?id=chfJJYC3iL. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y. Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, and Zonghan Yang. Kimi k1.5: Scaling reinforcement learning with llms, 2025. URL https://arxiv.org/abs/2501.12599. Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Chu Hong Hoi. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. Advances in Neural Information Processing Systems, 35:2131421328, 2022. Rongao Li, Jie Fu, Bo-Wen Zhang, Tao Huang, Zhihong Sun, Chen Lyu, Guang Liu, Zhi Jin, and Ge Li. Taco: Topics in algorithmic code generation dataset. arXiv preprint arXiv:2312.14852, 2023. Xuefeng Li, Haoyang Zou, and Pengfei Liu. Limr: Less is more for rl scaling. arXiv preprint arXiv: 2502.11886, 2025. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson dAutume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level code generation with alphacode. arXiv preprint arXiv:2203.07814, 2022. Jonathan Light, Yue Wu, Yiyou Sun, Wenchao Yu, Yanchi liu, Xujiang Zhao, Ziniu Hu, Haifeng Chen, and Wei Cheng. Scattered forest search: Smarter code space exploration with llms, 2025. URL https://arxiv.org/abs/2411.05010. 11 Jiawei Liu and Lingming Zhang. Code-r1: Reproducing r1 for code with reliable rewards. 2025. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation, 2023. URL https://arxiv.org/abs/2305.01210. Yifei Liu, Li Lyna Zhang, Yi Zhu, Bingcheng Dong, Xudong Zhou, Ning Shang, Fan Yang, and Mao Yang. rstar-coder: Scaling competitive code reasoning with large-scale verified dataset, 2025. URL https://arxiv.org/abs/2505.21297. Stephan Lukasczyk and Gordon Fraser. Pynguin: automated unit test generation for python. In Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: Companion Proceedings, ICSE 22. ACM, May 2022. doi: 10.1145/3510454.3516829. URL http://dx.doi.org/10.1145/3510454.3516829. Michael Luo, Sijun Tan, Roy Huang, Ameen Patel, Alpay Ariyak, Qingyang Wu, Xiaoxiang Shi, Rachel Xin, Colin Cai, Maurice Weber, Ce Zhang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepcoder: fully open-source 14b coder at o3-mini level. Niels Mündler, Mark Niklas Müller, Jingxuan He, and Martin Vechev. Swt-bench: Testing and validating real-world bug-fixes with code agents, 2025. URL https://arxiv.org/abs/2406. 12952. OpenAI. Openai o1 system card. arXiv preprint arXiv: 2412.16720, 2024. OpenAI. Competitive programming with large reasoning models. arXiv preprint arXiv: 2502.06807, 2025. OpenAI, :, Ahmed El-Kishky, Alexander Wei, Andre Saraiva, Borys Minaiev, Daniel Selsam, David Dohan, Francis Song, Hunter Lightman, Ignasi Clavera, Jakub Pachocki, Jerry Tworek, Lorenz Kuhn, Lukasz Kaiser, Mark Chen, Max Schwarzer, Mostafa Rohaninejad, Nat McAleese, o3 contributors, Oleg Mürk, Rhythm Garg, Rui Shu, Szymon Sidor, Vineet Kosaraju, and Wenda Zhou. Competitive programming with large reasoning models, 2025. URL https://arxiv.org/ abs/2502.06807. Guilherme Penedo, Anton Lozhkov, Hynek Kydlíˇcek, Loubna Ben Allal, Edward Beeching, Agustín Piqueres Lajarín, Quentin Gallouédec, Nathan Habib, Lewis Tunstall, and Leandro von Werra. Codeforces. https://huggingface.co/datasets/open-r1/codeforces, 2025. Yun Peng, Jun Wan, Yichen Li, and Xiaoxue Ren. Coffe: code efficiency benchmark for code generation, 2025. URL https://arxiv.org/abs/2502.02827. Z. Z. Ren, Zhihong Shao, Junxiao Song, Huajian Xin, Haocheng Wang, Wanjia Zhao, Liyue Zhang, Zhe Fu, Qihao Zhu, Dejian Yang, Z. F. Wu, Zhibin Gou, Shirong Ma, Hongxuan Tang, Yuxuan Liu, Wenjun Gao, Daya Guo, and Chong Ruan. Deepseek-prover-v2: Advancing formal mathematical reasoning via reinforcement learning for subgoal decomposition. arXiv preprint arXiv: 2504.21801, 2025. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y.K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402.03300. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Avi Singh, John Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Xavier Garcia, Peter Liu, James Harrison, Jaehoon Lee, Kelvin Xu, et al. Beyond human data: Scaling self-training for problem-solving with language models. arXiv preprint arXiv:2312.06585, 2023. Wenhan Wang, Chenyuan Yang, Zhijie Wang, Yuheng Huang, Zhaoyang Chu, Da Song, Lingming Zhang, An Ran Chen, and Lei Ma. Testeval: Benchmarking large language models for test case generation, 2025a. URL https://arxiv.org/abs/2406.04531. Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Lucas Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, et al. Reinforcement learning for reasoning in large language models with one training example. arXiv preprint arXiv:2504.20571, 2025b. Yuxiang Wei, Federico Cassano, Jiawei Liu, Yifeng Ding, Naman Jain, Zachary Mueller, Harm de Vries, Leandro Von Werra, Arjun Guha, and Lingming Zhang. Selfcodealign: Self-alignment for code generation. arXiv preprint arXiv:2410.24198, 2024. Zhangchen Xu, Yang Liu, Yueqin Yin, Mingyuan Zhou, and Radha Poovendran. Kodcode: diverse, challenging, and verifiable synthetic dataset for coding, 2025. URL https://arxiv.org/abs/ 2503.02951. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning. arXiv preprint arXiv: 2502.03387, 2025. Zhiqiang Yuan, Yiling Lou, Mingwei Liu, Shiji Ding, Kaixin Wang, Yixuan Chen, and Xin Peng. No more manual tests? evaluating and improving chatgpt for unit test generation, 2024. URL https://arxiv.org/abs/2305.04207. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:1547615488, 2022. Huaye Zeng, Dongfu Jiang, Haozhe Wang, Ping Nie, Xiaotong Chen, and Wenhu Chen. Acecoder: Acing coder rl via automated test-case synthesis, 2025a. URL https://arxiv.org/abs/2502. 01718. Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerlzoo: Investigating and taming zero reinforcement learning for open base models in the wild. arXiv preprint arXiv: 2503.18892, 2025b. Kexun Zhang, Danqing Wang, Jingtao Xia, William Yang Wang, and Lei Li. Algo: Synthesizing algorithmic programs with llm-generated oracle verifiers, 2023. URL https://arxiv.org/abs/ 2305.14591. Quanjun Zhang, Ye Shang, Chunrong Fang, Siqi Gu, Jianyi Zhou, and Zhenyu Chen. Testbench: Evaluating class-level test case generation capability of large language models, 2024. URL https://arxiv.org/abs/2409.17561. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. URL http://arxiv.org/abs/2403.13372."
        },
        {
            "title": "A Appendix",
            "content": "A.1 More Related Work on Synthetic Test Quality and its Implications Although existing LLM test synthesis methods prove to be useful in many scenarios, such as improving the quality of synthetic data (Wei et al., 2024) and software engineering(Mündler et al., 2025; Jain et al., 2024), their quality is far from perfect (Yuan et al., 2024) and are bounded in complexity, because direct generations of complicated data structures often result in inconsistency (Zhang et al., 2023). Weak verifiers can harm downstream code generation and search performance (Light et al., 2025). The quality of those synthetic tests and their implications are less discussed. Existing benchmarks for LLM test case generation abilities focus on code coverage and/or mutation scores (Wang et al., 2025a; Zhang et al., 2024; Jain et al., 2025a, 2024), the success rate for reproducing issues (Mündler et al., 2025), and the code change coverage for generated code patches (Ahmed et al., 2024; Mündler et al., 2025). A.2 Details of the Test Cases Generation Pipeline HARDTESTGEN As we mentioned in Section 3.2, HARDTESTGEN constructs both the input generator functions and the validator functions for verifying input correctness. In this section, we first introduce the detailed HARDTESTGEN implementation, including the coding problem filtering process, and detailed prompts for input generator/validator synthesis (Section A.2.1), followed by detailed dataset statistics for the final HARDTESTS dataset (Section A.2.2) and some examples in HARDTESTS (Section A.2.3). A.2.1 HARDTESTGEN Implementation Coding problem filtering. Before generating test cases, we first filter out questions not suitable for our test case generation. For example, those without oracle code solutions, and the questions that do not use standard I/O for input and output. More specifically, our question filtering process is as follows: We first remove problems that do not have any oracle programs. Next, we exclude all problems where the starter_code field is non-empty, as they are so-called core logic problems, rather than input-output style problems, and typically originate from online judges like LeetCode and GeeksforGeeks. In such problems, the programmer is not responsible for handling input and output logic, but only for implementing the core function based on given function signature. Since the inputs and outputs in these problems are often not strings, they are difficult to use for test case generation. After the filtering, we are left with 32.5k unique coding problems. Input validator prompt. We use the following LLM prompt to generate an input validator function, and special judge function when necessary. This prompt includes the problem specification and the oracle program to help the LLM have better understanding. have competitive programming problem. To test the correctness of candidate programs, (cid:44) need to create many test cases. Each test case is an input-output pair. The input part will be fully provided as stdin to (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) the candidate program, and then the candidate output will be collected from stdout. In most cases, we determine the correctness of the program by comparing the candidate output with the output part of the test case (i.e., the reference output), while sometimes, we need to use custom function to judge the correctness of the candidate output, instead. (cid:44) Note: Sometimes, problem may require single test case to contain multiple sub-tasks. For example: the first line of the input contains an integer $t (1 leq leq 1000)$, followed by inputs of $t$ independent sub-tasks. The problem statement may sometimes refer to sub-task as \"test case\", but this is merely difference in terminology. (cid:44) (cid:44) (cid:44) # Input Validator 1 2 3 4 5 7 8 9 10 11 13 14 15 16 17 19 20 21 22 23 25 26 27 28 29 31 32 33 34 35 37 (cid:44) (cid:44) Suppose have already written some input generator functions, and used them to generate many test case inputs. However, since they are randomly generated, they may not fully adhere to the constraints specified in the problem. In order to filter out invalid test cases, need you to write function `validate_input(input_str: str) -> bool`. Its input is the complete input string of test case, and it returns boolean indicating whether the input is valid. (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) You should first clearly list all the constraints given in the problem statement, and then generate the function. Your function should check as many of the given constraints as possible, including the complex ones. However, if constraint cannot be verified within reasonable time complexity (e.g., $O(n)$ for $n leq 10^6$, or $O(n^2)$ for $n leq 10^3$), or if it makes the code too complex, then it can be skipped. (cid:44) (cid:44) (cid:44) **Pay close attention**: If the problem says \"It's guaranteed that...\", then what follows (cid:44) is precisely something that must be verified. This is because the so-called \"guarantee\" in the problem is typically enforced through the Input Validator, so you must validate it in `validate_input`. Of course, only if it can be done in reasonable time complexity. (cid:44) (cid:44) (cid:44) **Example 1**: Cicasso has $n$ sticks of lengths $l = (l_0, l_1, dots, l_{n-1})$. But (cid:44) these $n$ sticks cannot form convex polygon with non-zero area. You need to add one stick so that the resulting $n+1$ sticks can form such polygon. The input consists of two lines: the first line is an integer $n$ ($3 leq leq 10^5$). The second line has $n$ integers $l_i$ ($1 leq l_i leq 10^9$). (cid:44) (cid:44) (cid:44) (cid:44) The `validate_input` function should not only check that $n$ and $l_i$ are within the correct range and that there are exactly $n$ numbers in the second line, but also check that the $n$ sticks cannot form convex polygon with non-zero area, i.e., that the longest stick is greater than or equal to the sum of the rest. (cid:44) (cid:44) **Example 2**: Suppose there is permutation $p = (p_0, p_1, dots, p_{n-1})$ of numbers from 1 to $n$ ($1 leq leq 2 times 10^5$). But you do not know the permutation (cid:44) $p$. Instead, you are given an array $s = (s_0, s_1, dots, s_{n-1})$, where $s_i$ is the sum of all $p_j < p_i$ for $j < i$. Your task is to recover $p_i$. (cid:44) (cid:44) In theory, we should verify whether the $s_i$ values correspond to valid permutation (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) $p_i$, but that requires solving for $p_i$, which is too complex. Moreover, when generating inputs, it's quite easy to ensure that the $s_i$ comes from valid permutation, so mistakes are unlikely. (Note: If verifying constraint isn't too complex, you should still check it.) Therefore, we only need to check that $n$ is within the range and that $s$ has exactly $n$ elements. # Output Judging Function In most cases, we can determine whether the candidate program has passed the test case by comparing the `candidate_output` and `reference_output` as strings. The specific function is shown below. (cid:44) (cid:44) ```python def output_judging_function(input_str: str, candidate_output: str, reference_output: (cid:44) str) -> bool: normalized_candidate_output = 'n'.join(line.rstrip() for line in (cid:44) candidate_output.rstrip().splitlines()) normalized_reference_output = 'n'.join(line.rstrip() for line in (cid:44) return normalized_candidate_output == normalized_reference_output reference_output.rstrip().splitlines()) ``` However, for few problems, the above `output_judging_function` does not work. **Example 1**: The problem asks to output list (`List[int]`), but the order of elements (cid:44) in the list does not matter. 15 38 40 41 42 43 44 46 47 48 49 50 52 53 54 55 56 58 59 60 61 62 64 65 66 67 68 70 71 72 73 In this case, we should convert both `candidate_output: str` and `reference_output: str` (cid:44) into `List[int]`, sort them, and then compare them. **Example 2**: Given graph with both directed and undirected edges, you must make all undirected edges directed so that the resulting graph has no cycles. If it is (cid:44) possible, output \"YES\" and the resulting graph (list of directed edges), otherwise output \"NO\". (cid:44) (cid:44) (cid:44) Here, in `output_judging_function`, we should first determine from `reference_output` whether solution is possible. If both `candidate_output` and `reference_output` say \"YES\", then we should also validate whether the graph provided in `candidate_output` is valid: check whether all edges exist in the input and whether the graph is acyclic (e.g., via DFS). (cid:44) (cid:44) (cid:44) **Example 3**: There are total of $T$ sub-tasks. Each sub-task gives pair of integers $l, r$ ($1 leq leq leq 998244353$), and the goal is to find pair of integers (cid:44) $x, y$ such that $l leq x, leq r$, $x ne y$, and $y$ is divisible by $x$. It is guaranteed that every sub-task has valid solution. (cid:44) (cid:44) For each pair $x, y$ provided in the `candidate_output`, simply check whether they (cid:44) (cid:44) (cid:44) satisfy all the conditions mentioned in the problem statement. The `output_judging_function` for this problem does not need to use the `reference_output`; it only requires the `input_str`."
        },
        {
            "title": "You need to first analyze whether this particular problem requires a custom",
            "content": "(cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) `output_judging_function` (different from the one given above). If yes, generate custom `output_judging_function`. If not, don't output it. Sometimes only `input_str` is needed and `reference_output` is not required; other times only `reference_output` is needed and `input_str` is not required; and in some cases, both are needed. However, regardless of which ones are actually used, the function signature must always be: `output_judging_function(input_str: str, candidate_output: str, reference_output: str) -> bool`. (cid:44) Generally speaking, if problem states \"there are multiple possible answers, any one is acceptable,\" this implies that the problem requires custom Output Judging Function. However, even if this is not explicitly mentioned, the problem may still actually require custom Output Judging Function. You need to determine this yourself. (cid:44) (cid:44) --- Also, when generating the above two functions, some known tricks or conclusions may be (cid:44) (cid:44) helpful, and you should derive them yourself if needed. will give you the correct solution to the problem, and you can use it to derive certain conclusions or tricks. Your output format must strictly follow: # Analysis ... (Analyze the problem, constraints, how to generate the Input Validator and Output (cid:44) Judging Function, etc.) # Result ```json { function. No other content.\", \"input_validator\": \"A block of Python code containing the `validate_input` (cid:44) \"needs_custom_output_judging_function\": true or false, \"output_judging_function\": \"A block of Python code containing the `output_judging_function` function. No other content.\" or null (cid:44) } ``` --- 16 75 76 77 78 79 81 82 83 84 85 87 88 89 90 91 93 94 95 96 97 99 100 101 102 103 105 106 107 1 2 4 5 6 7 8 Note: * All your code should be in Python 3. * Do not wrap the Python code in ```python```, just provide it plainly. * The Python code block under each field should be independent. In other words, they (cid:44) should not call or reference each other. If one block imports library, other blocks must re-import it as needed. (cid:44) * In Python block, you should first import the necessary libraries, and then start (cid:44) * Only Python's built-in libraries are permitted for import. defining functions. Important: Do not place import statements inside the functions. For example, block of Python code for Input Validator should look like this: import ... (some modules) def input_validator(input_str: str) -> bool: ... (some code) block of Python code for Output Judging Function (if needed) should look like this: import ... (some modules) def output_judging_function(input_str: str, candidate_output: str, reference_output: str) -> bool: ... (some code) (cid:44) --- # Problem Statement {{ problem_specification }} --- # Correct Program {{ oracle_program }} Input generator prompt. We use the following prompt to have the LLM generate inputs directly (Type 1), regular input generator (Type 2), and hacking input generator (Type 3). This prompt makes use of the problem specification, oracle program, and input validator to help the LLM better understand the problem requirements. have competitive programming problem. To test candidate programs' correctness, need (cid:44) to create many test cases. Each test case is an input-output pair. The input part will be fully provided as stdin to (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) the candidate program, and then the candidate output will be collected from stdout. In most cases, we determine the correctness of the program by comparing the candidate output with the output part of the test case (i.e., the reference output), while sometimes, we need to use custom function to judge the correctness of the candidate output, instead. (cid:44) Note: Sometimes, problem may require single test case to contain multiple sub-tasks. For example: the first line of the input contains an integer $t (1 leq leq 1000)$, followed by inputs of $t$ independent sub-tasks. The problem statement may sometimes refer to sub-task as \"test case\", but this is merely difference in terminology. (cid:44) (cid:44) (cid:44) Since the output part can be obtained by running correct programs, only need you to (cid:44) help me generate the input part. 17 9 10 12 13 14 15 16 18 19 20 21 22 24 25 26 27 28 30 31 32 (cid:44) The input should comply with the constraints given in the problem statement. will give you an Input Validator that checks whether the input meets all the constraints specified in the problem statement. However, some constraints may not be checked by the Input Validator due to the difficulty of verification. Nevertheless, the input you generate should still comply with all of these constraints. (cid:44) (cid:44) (cid:44) # Directly Generated Input Directly generated input (DGI) refers to inputs of small size and scale that can be (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) directly generated, such as the input parts of the sample test cases given in the problem statement. You can get more DGIs by making minor modifications to these inputs. need you to directly generate {{ num_DGI }} DGIs. Note: each DGI's length should be similar to the sample test cases' input, comply with the constraints given in the problem, and must not exceed 300 characters under any circumstances. If it is not possible to generate DGIs under this length limit, give up on generating them. # Regular Input ## Regular Problems Regular Input (RI) refers to ordinary inputs, where all data is fully random and (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) satisfies the constraints specified by the problem statement. You need to write function `gen_regular_input() -> str`, and each time it is called, it should generate one random RI. You should ensure the generated input satisfies the constraints as much as possible, and may even sacrifice some degree of randomness to do so. But if trying to enforce constraint leads to function that cannot run within finite and reasonable time complexity (e.g., $O(n)$ for $n leq 10^6$, or $O(n^2)$ for $n leq 10^3$), then you may ignore that constraint. **Pay close attention**: do not use `while` loops, especially ones that \"keep generating until constraint is satisfied.\" That can cause unlimited running time and make (cid:44) input generation fail. (cid:44) Some problems may require certain test cases to satisfy specific constraints (for (cid:44) (cid:44) (cid:44) example, 10% of test cases satisfy $n leq 100$, 10% of the test cases satisfy $n leq 1000$, etc.). Ignore this requirement. All test cases should be generated according to the most general constraints. Sometimes, generating input that satisfies the constraints requires some trick. You need (cid:44) (cid:44) (cid:44) to deduce it yourself (e.g., the example below about when $n$ sticks cannot form convex polygon). will give you the correct solution for the problem, and you can analyze it to discover some tricks or conclusions. **Example 1**: Cicasso has $n$ sticks ($3 leq leq 10^5$) of lengths $l_i$ ($1 leq l_i leq 10^9$, for $i=0,1,dots,n-1$). But these $n$ sticks cannot form convex polygon (cid:44) of non-zero area. You need to add one more stick, so that the $n+1$ sticks can form convex polygon of non-zero area. Output the minimum length of the additional stick. (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) We can randomly generate $n in [3, 10^5]$, but cannot randomly generate $l_i$, because such $l_i$ will likely not satisfy the constraint that the $n$ sticks cannot form convex polygon of non-zero area. (It's not feasible to randomly generate and then filter, since it's too time-consuming.) We know that this constraint actually requires \"the maximum $l_i$ is greater than or equal to the sum of all the other $l_i$.\" So we can first randomly sample $l_0$ in $[n-1, 10^9]$ as the maximum $l_i$, then sample an integer $s in [n-1, l_0]$ as the total sum of the other $l_1, dots, l_{n-1}$, and finally use partitioning trick to sample $l_1, dots, l_{n-1}$ such that each element is at least 1 and the total sum is $s$. After that, we can shuffle the $l_i$ list. (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) **Example 2**: There is permutation $p = (p_0, p_1, ..., p_{n-1})$ of numbers from 1 to $n$ ($1 leq leq 2cdot 10^5$). You do not know this permutation, but you are (cid:44) given an array $s = (s_0,dots,s_{n-1})$, where $s_i$ is the sum of all $p_j < p_i$ with $j < i$. Find $p_i$. (cid:44) (cid:44) 18 33 34 36 37 38 39 40 42 43 44 45 46 48 49 50 51 52 54 55 56 57 58 60 61 62 (cid:44) We can first randomly generate $n in [1, 2 times 10^5]$. But we cannot directly generate an array $s_i$ randomly, because it is very unlikely to satisfy the constraints. Instead, we should reverse the process: first generate random permutation $p_i$, and then compute the corresponding $s_i$. (cid:44) (cid:44) **Example 3**: This problem has $t in [1, 1000]$ groups of independent sub-tasks. Each sub-task has an integer $n in [1, 10^5]$ and an array $a$ of length $n$, where $a_i (cid:44) in [1,10^5]$. The problem guarantees that the total sum of all $n$ across all $t$ sub-tasks does not exceed $2 times 10^5$. (cid:44) (cid:44) We can first randomly generate $t in [1, 1000]$. But at this point we cannot directly (cid:44) (cid:44) (cid:44) (cid:44) sample $t$ values of $n$ from $[1, 10^5]$, because their sum is likely to exceed $2 times 10^5$. So instead, we randomly sample $s in [t, 2times 10^5]$, and then partition $s$ into $n_0, n_1, dots, n_{t-1}$ such that each value is at least 1 and their sum is $s$. The following Python function demonstrates how, given positive integers $m$ and $s$, with $m leq s$, one can randomly select $m$ positive integers such that their sum equals $s$. This is just for your reference. (cid:44) (cid:44) import random assert <= if >= 2: breaks = random.sample(range(1, s), - 1) breaks.sort() results = [breaks[0]] + [breaks[i] - breaks[i - 1] for in range(1, (cid:44) len(breaks))] + [s - breaks[-1]] else: results = [s] ## Multi-Category Output Problems (cid:44) (cid:44) (cid:44) For most problems, there is only one type of output. But there are some problems where outputs fall into multiple categories. These are called Multi-Category Output Problems. For example, some problems require the output to be \"Yes\" or \"No\", while others ask you to output the solution if it exists, otherwise output -1. In such cases, if we treat it as regular problem and only write single `gen_regular_input` function to generate inputs randomly, the resulting outputs will be very imbalanced. For example, the \"Yes\" outputs may require special construction, so nearly all generated inputs produce \"No\" as the answer. Thus, even candidate program that always prints \"No\" would pass all test cases. (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) For such problems, instead of one `gen_regular_input`, you need to design series of functions `gen_regular_input_suffix`, where each function is responsible for generating inputs corresponding to one category of output. Each time function is called, it should be able to generate--within reasonable time complexity--one random input that satisfies the constraints and whose corresponding output belongs to the corresponding category. If it is difficult to write function that randomly generates some category, you can: (cid:44) (cid:44) (cid:44) (cid:44) 1. Sacrifice randomness and perform special construction, even returning fixed value (cid:44) or 2. Construct completely random data, similar to `gen_regular_input` each time Sometimes, problem may require single test case to contain multiple independent (cid:44) (cid:44) (cid:44) sub-tasks. In this case, each sub-task in each input generated by `gen_regular_input_suffix` should have the corresponding output category, e.g., all corresponding outputs should be \"No\". **Example 1**: Given two $n times m$ binary matrices $A, B$. You can take the following operation: select rectangle in matrix $A$ with height and width both at least 2, (cid:44) and flip the values at the four corner positions. You are to answer whether it's possible to make $A$ equal to $B$ using this operation. If possible, output \"Yes\" and the resulting matrix; otherwise, output \"No\". (cid:44) (cid:44) (cid:44) 19 63 64 65 66 68 69 70 71 72 74 75 76 77 78 80 81 (cid:44) (cid:44) (cid:44) There are two outputs here: \"Yes\" and \"No\", corresponding to two categories of inputs. For the first category, we create `gen_regular_input_yes`, such that $A$ can be transformed into $B$. We can randomly construct matrix $A$, then perform $t$ operations (you can decide $t$ yourself, but it should not be too small or too large to avoid long generation time), where each operation selects rectangle and flips the corners. Then the result becomes matrix $B$. For the second category, we write `gen_regular_input_no`, where $A$ cannot be transformed into $B$. One way is to randomly flip position in matrix $B$ from the previous construction, which makes it impossible. This sacrifices randomness, but is simple and acceptable. (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) **Example 2**: Given two numbers $n, m$ ($1leq leq mleq 5times10^8$), you are to (cid:44) determine whether it is possible to transform $n$ into $m$ by multiplying by 2 and 3, and if so, output the minimum number of operations. Otherwise, output -1. (cid:44) (cid:44) (cid:44) There are two outputs: the minimum operation count, and -1. Correspondingly, we have two input generators. For the first case, where $n$ can be transformed into $m$, we can randomly generate $nin [1, 5times 10^8]$, then perform $t$ operations (multiply by 2 or 3) until $t$ steps are complete or further multiplication would exceed $5times10^8$. The result becomes $m$. For the second case, where $n$ cannot be transformed into $m$, we can firstly randomly generate $m > n$, and then if $n$ can be transformed into $m$, simply set $m = m-1$. (cid:44) (cid:44) (cid:44) (cid:44) **Example 3**: Player and are playing tic-tac-toe. Player goes first. You are given $3 times 3$ board, where each cell is \".\", \"X\", or \"0\". Output the current state, (cid:44) one of: \"first\" (next move is A), \"second\" (next is B), \"illegal\" (not possible in legal game), \"the first player won\", \"the second player won\", or \"draw\". (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) There are 6 output categories, corresponding to 6 input categories. For the first output category, we need to create `gen_regular_input_first` where the next move is A's. We can randomly select $tin[0, 4]$, then randomly place $t$ X's and $t$ 0's. This may lead to win or illegal state, but we should NOT filter those during generation, because doing so would make the code too complex and slow. We only need most of the generated inputs to match this category. For the second category, place $t+1$ X's and $t$ 0's ($tin [1,3]$). For the third category, it must be illegal, e.g. and 0 count difference is too large, or both players have already won. We can create `gen_regular_input_illegal_mark_num` and `gen_regular_input_illegal_both_win`, etc. Do the same for the remaining categories. (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) # Hacking Input (cid:44) (cid:44) Although Regular Input can guarantee large data size (because most of the time, the magnitude of random data is close to the maximum magnitude), for some problems, large-scale random data is not enough. We also need Hacking Input (HI). HI refers to inputs that are very tricky for candidate programs. Specifically, need you to generate series of functions `gen_hacking_input_suffix() -> str`. Each function is responsible for one type of HI. Every time each function is called, it should return one HI (can be random or fixed). (cid:44) (cid:44) (cid:44) (cid:44) Most types of HI are designed to cause brute-force candidate programs with insufficient (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) optimization to run into time limits. Specifically, you should first list what kinds of straightforward or brute-force algorithms candidate programs might use, then construct inputs that would cause them to time out. Note: for most problems, RI data is enough to cause brute-force solutions to TLE, so you don't need to generate more. But for some problems, even though the brute-force algorithm's worst-case complexity is $O(n^2)$, due to rare worst-case inputs, the actual runtime is closer to $O(n)$. In these cases, you need to specially construct the data to repeatedly trigger the worst-case scenario for those brute-force algorithms. For some problems, we also need some types of HI to expose bugs caused by failure to (cid:44) (cid:44) (cid:44) (cid:44) handle edge cases. So you should think about whether there are any special edge cases (e.g., input $n=0$, or tree root is None, etc.). Note that the randomness of the input data itself at this time is not important. The key point is to expose the errors of the candidate programs. 20 83 84 85 86 87 89 90 91 92 93 95 96 97 98 99 101 102 103 104 105 107 108 109 Of course, if the problem doesn't require any HI, then do not generate them. Especially if an HI is simply large-scale data, then you shouldn't bother. HI must be specially constructed--random RI should almost never produce them. (cid:44) (cid:44) **Example 1**: Given two numbers $n$ and $m$ ($1 leq leq leq 5 times 10^8$), the task is to determine whether it is possible to transform $n$ into $m$ by repeatedly (cid:44) multiplying $n$ by 2 or by 3. If possible, output the minimum number of operations required; otherwise, output -1. (cid:44) (cid:44) brute-force approach that candidate program might take is to use DFS, recursively (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) trying to multiply $n$ by 2 or 3 until it becomes greater than or equal to $m$. If we randomly choose $n$ and $m$, the ratio between them is usually small, so this approach might still pass. One kind of effective HI is to set $n in [1, 5]$ and $m in [4 times 10^8, 5 times 10^8]$. This creates large gap between $n$ and $m$, making the brute-force DFS approach inefficient. We can name the corresponding function `gen_hacking_input_small_n_big_m`. You should consider other types of HIs yourself. **Example 2**: Given string $S$ of length $n in [1, 10^5]$, we repeatedly perform the following operation: find two identical adjacent characters and delete them. This (cid:44) continues until there are no more identical adjacent characters in $S$. (cid:44) This problem should be solved using stack to achieve an $O(n)$ time complexity. (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) However, some candidate programs might use brute-force simulation approach -- repeatedly scanning the string and removing adjacent equal characters -- which can result in worst-case time complexity of $O(n^2)$. If we generate $S$ completely at random, it's likely that there will only be few pairs of identical adjacent characters. One kind of HI is to construct string $S$ of long even length (e.g., in $[5 times 10^4, 10^5]$) and set `S[2*k] == S[2*k+1]`, thereby introducing large number of adjacent equal character pairs. However, if the candidate program deletes all adjacent equal pairs in each round, the time complexity remains $O(n)$. Another HI is to construct string $S$ of long even length (e.g., in $[5 times 10^4, 10^5]$) such that `S[:n//2] == S[n//2:][::-1]`, which forces the program to go through $n$ rounds to completely remove all characters, resulting in the true worst-case time complexity of $O(n^2)`. These two functions can be named `gen_hacking_input_pairwise_equal` and `gen_hacking_input_mirrored_halves`, respectively. **Example 3**: Given integer $win[1, 100]$, determine whether it can be written as the (cid:44) sum of two positive even integers. Candidate programs may output \"Yes\" when $w$ is even, and \"No\" when $w$ is odd. But (cid:44) (cid:44) special case is $w=2$, which should be \"No\". So we can create `gen_hacking_input_two`, which always returns the string `\"2\"`. Important: if type of HI is just setting data to their largest scale, then it is unnecessary. (cid:44) ---"
        },
        {
            "title": "Your output format must strictly be",
            "content": "# Analysis ... (generally, you should first analyze the problem and data constraints, and then analyze (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) how to generate Directly Generated Input, how to generate Regular Input, and whether the problem is Multi-Category Output Problem (In that case, generate regular input generation functions for each output category. Make sure you mentioned the corresponding function names in the Analysis part). Then you should list some naive candidate programs and analyze how to generate Hacking Input.) # Result ```json 21 111 112 113 114 115 117 118 119 120 121 123 124 125 126 127 129 130 131 132 133 135 136 137 138 139 141 142 143 144 145 147 148 149 150 151 153 154 155 156 157 159 160 161 162 163 165 166 \"directly_generated_inputs\": [\"DGI1\", \"DGI2\", ...], \"is_multi_category_output_problem\": true or false, \"regular_input_generator\": \"a block of Python code containing function gen_regular_input (for Regular Problem), or multiple functions (cid:44) gen_regular_input_suffix (for Multi-Category Output Problem)\", (cid:44) \"hacking_input_generator\": \"a block of Python code containing multiple (cid:44) gen_hacking_input_suffix functions\" or null (if no Hacking Input is needed) { } ``` --- Note: * All your code should be in Python 3. * Do not wrap the Python code in ```python```, just provide it plainly. * The Python code block under each field should be independent. In other words, they (cid:44) should not call or reference each other. If one block imports library, other blocks must re-import it as needed. (cid:44) * In Python block, you should first import the necessary libraries, and then start (cid:44) * Only Python's built-in libraries are permitted for import. defining functions. Important: Do not place import statements inside the functions. For example, block of Python code for RI of Regular Problems should look like this: import ... (some modules) def gen_regular_input(input_str: str) -> bool: ... (some code) block of Python code for RI of Multi-Category Output Problem may look like this: import ... (some modules) def gen_regular_input_some_suffix(input_str: str) -> bool: ... (some code) def gen_regular_input_some_suffix(input_str: str) -> bool: ... (some code) ... And the Hacking Input block is similar. --- # Problem Statement {{ problem_specification }} --- # Correct Program {{ oracle_program }} --- # Input Validator {{ input_validator }} 22 Figure 4: The distribution of the number of Type1, Type2, and Type3 test cases, as well as the total number of test cases in HARDTESTS. Note that in the prompts above, we provide two to three carefully crafted examples for each function that we ask the LLM to generate, enabling in-context learning. Additionally, we prompt the LLM to perform chain-of-thought reasoning. These two requirements help the LLM understand the task better and improve the data synthesis. A.2.2 HARDTESTS Statistics We generated test cases for all 32.5k valid questions in the HARDTESTS. The status distribution of test case generation is shown in Figure 5. While we carefully designed the test-case generation prompt, we didnt attain 100% coverage. We successfully generated test cases for 81.9% of the questions. The main failure reasons include: no valid oracle programs (i.e., compiles and runs without errors) (6.62%), all output verification failed (5.85%), and input generation failed (3.72%). The distribution of the number of Type1, Type2, and Type3 test cases, as well as the total number of test cases, is shown in Figure 4. A.2.3 HARDTESTS Examples Example 1 This example demonstrates the input validator, Type 1 (Directly Generated) and Type 2 (Regular) test cases, as well as custom judging function. Heres the problem description: Codeforces 1096A: There are total of (1 1000) sub-tasks. Each sub-task gives pair of integers l, (1 998244353), and the goal is to find pair of integers x, such that x, r, = y, and is divisible by x. It is guaranteed that every sub-task has valid solution. Note: It can be mathematically proven that sub-task has solution if and only if 2l < r. 23 Figure 5: The result status distribution of our test case generation pipeline HARDTESTGEN. The input validator is as follows. It checks whether input_str conforms to the required format specified in the problem specification, whether all data falls within the required ranges, and whether other constraints are satisfied (e.g., whether each sub-task has solution). import sys def input_validator(input_str: str) -> bool: lines = input_str.strip().split('n') if not lines: return False try: = int(lines[0]) except: return False if not (1 <= <= 1000): return False if len(lines) != + 1: return False for in range(1, + 1): parts = lines[i].strip().split() if len(parts) != 2: return False try: l, = map(int, parts) except: return False if not (1 <= <= <= 998244353): return False if 2 * > r: return False return True # No valid pair possible Since this problem allows multiple correct solutions, simple string comparison is not sufficient. We need special, customized output judging function. The output judging function is as follows. 24 1 3 4 5 6 7 9 10 11 12 13 15 16 17 18 19 21 22 23 24 25 27 28 29 30 31 33 34 1 2 3 5 6 7 8 9 11 12 13 14 15 17 18 19 20 21 23 24 25 1 2 4 5 6 7 1 3 4 5 6 7 9 10 def output_judging_function(input_str: str, candidate_output: str, reference_output: (cid:44) str) -> bool: try: input_lines = input_str.strip().split('n') = int(input_lines[0]) queries = [tuple(map(int, line.strip().split())) for line in input_lines[1:T+1]] output_lines = candidate_output.strip().split('n') if len(output_lines) != T: return False for (l, r), line in zip(queries, output_lines): parts = line.strip().split() if len(parts) != 2: return False x, = map(int, parts) if not (l <= <= and <= <= r): return False if == y: return False if % != 0: return False return True except: return False The Type1 (Directly Generated) inputs are as follows. [ ] \"3n1 10n2 8n3 10\", \"2n5 20n10 25\", \"3n7 30n1 5n2 6\", \"1n100 300\", \"2n999 2000n1000 3000\" The Type 2 input (Regular) generator is as follows. To ensure solution always exists, the LLM sets 2l. import random def gen_regular_input() -> str: = random.randint(1, 1000) queries = [] for _ in range(T): = random.randint(1, 499122176) = random.randint(2 * l, 998244353) queries.append(f\"{l} {r}\") return f\"{T}n\" + \"n\".join(queries) The LLM believes that there is no need to generate Type 3 (Hacking) input generator for this problem. Example 2 This example demonstrates the input validator, as well as the Type 1 (Directly Generated), Type 2 (Regular), and Type 3 (Hacking) test cases. Heres the problem description: Codeforces 1141 A: Given two numbers n, (1 5 108), you are to determine whether it is possible to transform into by multiplying by 2 and 3, and if so, output the minimum number of operations. Otherwise, output -1. The input validator is as follows. It checks whether input_str conforms to various format requirements and constraints. 25 2 3 4 5 6 8 9 10 11 12 14 15 16 1 2 4 5 6 7 8 10 11 12 1 2 4 5 6 7 8 10 11 12 13 14 16 17 18 19 20 import re try: def validate_input(input_str: str) -> bool: # Split the input string into two parts parts = input_str.strip().split() if len(parts) != 2: return False # Convert parts to integers n, = map(int, parts) # Check the constraints if not (1 <= <= <= 5 * 10**8): return False return True except: return False The Type1 (Directly Generated) inputs are as follows. [ ] \"120 51840\", \"42 42\", \"48 72\", \"1 1\", \"2 6\", \"3 9\", \"5 10\", \"7 21\", \"8 24\", \"10 30\" The Type 2 input (Regular) generator is as follows. The output of this problem has two categories (i.e., possible and impossible), so the LLM generates two regular input generating functions, corresponding to these two categories respectively. import random def gen_regular_input_possible() -> str: = random.randint(1, 10**8) = for _ in range(random.randint(1, 20)): if random.choice([True, False]): *= 2 else: *= 3 if > 5 * 10**8: break return f\"{n} {m}\" def gen_regular_input_impossible() -> str: = random.randint(1, 10**8) = random.randint(n + 1, 5 * 10**8) while % == 0: += 1 return f\"{n} {m}\" The Type 3 input (Hacking) generator is as follows. The LLM generates two hacking input generating functions. The first function sets small and large m. This is because brute-force approach that candidate program might take is to use DFS, recursively trying to multiply by 2 or 3 until it becomes greater than or equal to m. If we randomly choose and m, the ratio between them is usually small, so this approach might still pass. Setting to be small and to be big creates large gap between and m, making the brute-force DFS approach inefficient. The second function sets = n, which serves as an edge case. 26 1 3 4 5 6 7 9 10 import random def gen_hacking_input_small_n_big_m() -> str: = random.randint(1, 5) = random.randint(4 * 10**8, 5 * 10**8) return f\"{n} {m}\" def gen_hacking_input_edge_case() -> str: = random.randint(1, 5 * 10**8) return f\"{n} {n}\" For this problem, the LLM believes that string comparison function would be enough for output judging. A.3 Details of the Collection of Problem Specifications and Oracle Programs in HARDTESTS HARDTESTS consists of 47,136 coding problems collected from 13 OJs. In practice, the dataset obtains problem specifications and oracle programs from five direct data sources: AtCoder, Codeforces, Luogu, CodeContests, and TACO. Data sources. Codeforces (https://codeforces.com/) is one of the largest English OJs. We collected all publicly available problem specifications up to September 2024 from Codeforces. AtCoder. (https://atcoder.jp/) is large OJ offering problems in both Japanese and English. We scraped all problem specifications available up to September 2024, along with three correct user-submitted C++ programs for each problem. We used those directly for problems with official English versions. Luogu (https://www.luogu.com.cn/) is large Chinese OJ consisting of main section (Luogu-Main) and four mirror sections. The main section hosts original problems authored by users and administrators, as well as problems sourced from real-world contests (e.g. USACO). The mirror sections contain problems from other OJs, including AtCoder, SPOJ, Codeforces, and UVa. We collected all available problem specifications and community-authored tutorials, which often include both correct C++ programs and corresponding natural language explanations, from Luogu. CodeContests (Li et al., 2022) is dataset comprising 13,493 problems collected from five OJs. Each entry includes problem specification and several correct programs in C++, Python 2, Python 3, and Java. Only Codeforces problems in CodeContests were used in our dataset, as only their problem IDs were explicitly provided. TACO (Li et al., 2023) is large-scale English dataset containing 25.4k problems sourced from ten OJs. Each entry includes problem specification and multiple correct Python programs. We collect all problems from TACO. The distribution of problem counts across each OJ is shown in Figure 6. The URLs of each OJ, along with the direct data sources of their problem specifications and oracle programs, are listed in Table 6. Note that since some problems have multiple oracle program sources, we prioritize programs from more reliable sources when generating test cases. The reliability, supported languages, and notes regarding each direct source of oracle programs are presented in Table 7. The distribution of the number of oracle programs per problem in HARDTESTS is shown in Figure 7. A.4 Direct Evaluation Details Evaluation details for LLM-generated programs on AtCoder. AtCoder previously made its official test cases publicly available. Although this is no longer the case, we obtained partial archive from the Github repository conlacda/atcoder-testcases. On AtCoder, we use the test cases in TACO as the baselines. We selected problems that have at least one test case in each dataset, resulting in total of 653 problems. Evaluation details for LLM-generated programs on Codeforces. Codeforces does not make its test cases publicly available. Therefore, we manually submit LLM-generated candidate programs to the Codeforces platform to obtain ground-truth verdicts. We use TACO and CodeContests as baselines. For problems where the results of all three datasets agree, we randomly select 5% of them for submission. For problems where the datasets produce conflicting results, we submit 50% of the candidate programs. We compute precision and recall based on the combined submission outcomes. For each difficulty level from 1 to 4, we randomly select 150 problems with at least one test case in each dataset, yielding total of 600 problems. 27 Figure 6: Number of problems from each OJs. the Figure 7: Distribution of number of oracle programs in HARDTESTS. Table 6: Problem specification sources and oracle solution sources of each OJ. OJ"
        },
        {
            "title": "Codeforces",
            "content": "https://codeforces.com/"
        },
        {
            "title": "Luogu\nUVa\nSPOJ\nAizu\nGeeksforGeeks\nCodewars\nKattis\nCodeChef\nHackerEarth\nLeetCode\nHackerRank",
            "content": "https://atcoder.jp/contests/ https://www.luogu.com.cn/ https://onlinejudge.org/ https://www.spoj.com/ https://onlinejudge.u-aizu.ac.jp/ https://www.geeksforgeeks.org/ https://www.codewars.com/ https://open.kattis.com/ https://www.codechef.com/ https://www.hackerearth.com/ https://leetcode.com/ https://www.hackerrank.com/"
        },
        {
            "title": "Oracle Program\nSources",
            "content": "TACO, CodeContests, Luogu AtCoder, TACO, Luogu Luogu Luogu Luogu TACO TACO TACO TACO TACO TACO TACO TACO Table 7: Oracle program sources with reliability, languages, and notes"
        },
        {
            "title": "Notes",
            "content": "User-submitted and accepted programs from AtCoder"
        },
        {
            "title": "High",
            "content": "Python, C++ Code solutions from CodeContests Community-authored editorials from Luogu"
        },
        {
            "title": "Medium",
            "content": "2/3, Python C++, Java C++ Verified programs from TACO, i.e., programs that can pass all TACOs own test cases Other programs from TACO"
        },
        {
            "title": "Python",
            "content": "Some code (either Python or C++) may use AtCoders custom library. Some editorials may lack complete, directly executable code. But if the code has no compilation or runtime errors, it is very likely to be completely correct. Theres some false positives in TACOs test cases. Reliability is not zero due to some false negatives in TACOs test cases. 28 Evaluation details for human-written programs on Codeforces. dataset at Huggingface titled MatrixStudio/Codeforces-Python-Submissions collects 690k human-submitted programs on Codeforces along with their official verdicts. We use the verdicts as the ground-truth labels. All other settings are the same as those of evaluation using LLM-generated programs. A.5 Qualitative Analysis of Generated Tests A.5.1 Example 1: False Positive of TACO and HARDTESTS Type 1 In this example we show how TACO and HARDTESTS Type 1 tests cannot break wrong program and result in false positive, while HARDTESTS Type 2 tests succeeds in making the program fail. Heres the problem description: AtCoder ABC117C: Given an integer (2 2 105) and an integer array of length (0 Ai 109), compute the value of (cid:80)N 1 Since 2 2 105, the solution to the problem needs to be relatively efficient. The correct solu- (cid:19) , tion employs mathematical techniques to simplify the equation into: 1 2 j=i+1 AiAj modulo 109 + 7. (cid:18)(cid:16)(cid:80)N i=1 A2 (cid:80)N i=1 Ai (cid:80)N (cid:17)2 i=1 which yields an O(N ) algorithm. However, candidate program generated by Qwen2.5-Coder-7B-Instruct uses brute-force algorithm with time complexity of O(N 2). The candidate program is as follows: #include <iostream> #include <vector> int main() { std::ios::sync_with_stdio(false); std::cin.tie(nullptr); int N; std::cin >> N; std::vector<long long> A(N); for (int = 0; < N; ++i) { std::cin >> A[i]; } const int MOD = 1000000007; long long sum = 0; for (int = 0; < N; ++i) { for (int = + 1; < N; ++j) { sum = (sum + A[i] * A[j]) % MOD; } } std::cout << sum << std::endl; return 0; } Due to its inefficiency, this candidate program failed to pass the official test cases. Nevertheless, because the test cases in TACO and HARDTESTS Type 1 (Directly Generated) were relatively small (with small ), the candidate program successfully passed these cases. Furthermore, the HARDTESTS Type 2 (Regular) input for this problem is generated using the following Python function: import random def gen_regular_input() -> str: = random.randint(2, 200000) 29 2 3 4 5 6 8 9 10 11 12 14 15 16 17 18 20 21 22 23 24 26 27 28 1 2 4 5 6 = [random.randint(0, 1000000000) for _ in range(n)] return f\"{n}n\" + \" \".join(map(str, a)) Due to the larger scale of HARDTESTS Type 2 (Regular) inputs, the candidate program failed to pass these test cases and we have true negative. A.5.2 Example 2: False Positive of TACO and HARDTESTS Type 1 + In this example we show how TACO and HARDTESTS Type 1 + 2 tests cannot break wrong program and result in false positive, while HARDTESTS Type 3 tests succeeds in making the program fail. Heres the problem description: AtCoder ABC139C: There are (1 105) squares arranged from left to right, with the height of the i-th square from the left being Hi (1 Hi 109). Starting from any square, you can move one step to the right as long as the next squares height is not greater than the current one. Find the maximum number of moves possible. Given that 1 105, the solution needs to be relatively efficient. The correct solution uses an O(N ) greedy algorithm. Specifically, it traverses the array from left to right, counting how many consecutive heights satisfy Hi Hi+1. Each time condition fails, the counter is reset, and the maximum value is maintained throughout the traversal as the answer. candidate program generated by Qwen-Coder-14B-Instruct uses brute-force approach, iterating over each starting position and simulating the moves. Although functionally correct, its complexity is O(N 2) and too inefficient to work. The code is as follows: 1 2 3 5 6 7 8 9 11 12 13 14 15 17 18 19 20 21 23 24 25 26 27 29 30 #include <iostream> #include <vector> using namespace std; int main() { int N; cin >> N; vector<int> H(N); for(int = 0; < N; i++) { cin >> H[i]; } int max_moves = 0; for(int = 0; < N; i++) { int current_moves = 0; for(int = + 1; < N; j++) { if(H[j] <= H[j - 1]) { current_moves++; } else { break; } } if(current_moves > max_moves) { max_moves = current_moves; } } cout << max_moves << endl; return 0; } Because of its inefficiency, this candidate program failed the official test cases. Nevertheless, due to the relatively small scale of the test cases in TACO and HARDTESTS Type 1 (Directly Generated), the candidate program passed these tests. Additionally, the HARDTESTS Type 2 (Regular) input for this problem is generated using the following Python function: 1 2 3 4 5 1 2 3 4 5 7 8 9 10 11 13 14 15 16 17 19 1 2 3 4 6 7 8 9 10 12 13 import random def gen_regular_input() -> str: = random.randint(1, 100000) heights = [random.randint(1, 1000000000) for _ in range(N)] return f'{N}n' + ' '.join(map(str, heights)) We observe that since the Hi sequence is randomly generated, it fluctuates significantly, reducing the complexity of the simulate moving from certain square procedure from O(N ) to approximately O(1). Thus, the tests generated do not lead to the worst case complexity of the inefficient program and its overall time complexity effectively becomes O(N ), enabling the candidate program to pass HARDTESTS Type 2 (Regular) test cases. The HARDTESTS Type 3 (Hacking) inputs for this problem are generated using the following Python functions: import random # Monotonically decreasing sequence def gen_hacking_input_monotonic_decreasing() -> str: = 100000 heights = list(range(1000000000, 1000000000 - N, -1)) return f'{N}n' + ' '.join(map(str, heights)) # Monotonically increasing sequence def gen_hacking_input_monotonic_increasing() -> str: = 100000 heights = list(range(1, + 1)) return f'{N}n' + ' '.join(map(str, heights)) # Alternating heights def gen_hacking_input_alternating() -> str: = 100000 heights = [1000000000 if % 2 == 0 else 1 for in range(N)] return f'{N}n' + ' '.join(map(str, heights)) There are three hacking input generation functions: monotonically decreasing, monotonically increasing, and alternating sequences. The first generated input (monotonically decreasing) successfully increased the actual runtime complexity of the candidate program to O(N 2), causing timeout and consequently failure on this test case. A.5.3 Example 3: False Negative of TACO In this example, we show an example of false negative caused by the lack of special judge function in TACO tests. We also show how HARDTESTS can correctly evaluate the candidate program. Heres the problem description: AtCoder ABC117A: Given an integer and an integer (1 100, 1 100). Compute the value of /X with an error tolerance within 103. candidate program generated by Qwen2.5-Coder-14B-Instruct is: #include <iostream> #include <iomanip> int main() { int T, X; std::cin >> >> X; double timeInWorldA = static_cast<double>(T) / X; std::cout << std::fixed << std::setprecision(10) << timeInWorldA << std::endl; return 0; } 31 1 2 3 5 6 7 8 9 11 12 13 14 15 17 18 19 This is clearly correct and passes all official test cases. It also passes all test cases from HARDTESTS, but it fails on TACOs test cases. This is because using simple string comparison function is insufficient due to potential differences in precision between the candidate output and the reference output. TACO does not provide special output judging function for problems, which leads to false negatives. HARDTESTS provides special output judging function, shown below: def output_judging_function(input_str: str, candidate_output: str, reference_output: (cid:44) str) -> bool: # Parse the input T, = map(int, input_str.split()) # Calculate the expected output expected_output = / # Parse the candidate output try: candidate_value = float(candidate_output.strip()) except ValueError: return False # Check the absolute and relative error absolute_error = abs(candidate_value - expected_output) relative_error = absolute_error / abs(expected_output) if expected_output != 0 else (cid:44) float('inf') # The output is correct if either error is within the tolerance return absolute_error <= 1e-3 or relative_error <= 1e-3 A.6 Downstream Training and Evaluation Details Teacher-distillation training and evaluation details. In the teacher-distillation experiments, our model is trained with the same training parameters used to train OlympicCoder-7B (epochs=10, learning_rate=4e-5, batch_size=128, cosine learning rate schedule with decay to 10% of the peak learning rate and 32,768 max length). The evaluations are sampled with temperature=0.7, top_p=0.95, max_new_tokens=16384. Self-distillation training and evaluation details. In the self-distillation experiments, our model is trained with the following training parameters (epochs=20, learning_rate=4e-5, batch_size=128, cosine learning rate schedule with decay to 10% of the peak learning rate and 32,768 max length). The evaluations are sampled with temperature=0.6, top_p=0.95, top_k=20, min_p=0, max_new_tokens=32768 as recommended by Qwen. RL training and evaluation details. We use verl for RL training and firejail for sandboxing code execution. The rollouts are generated with temperature=1, top_p=0.95, top_k=20, min_p=0, response_length=24000, initial learning rate 5e-7. We use global batch size of 32 and generate 32 samples per rollout. All our experiments are run on 8 NVIDIA H100 GPUs. We do not use KL divergence in our RL loss. A.7 Test Case Generation Without an Oracle Model In the case that an oracle program y, or an oracle test suite does not exist for problem x, such as when problems are synthetically generated, we propose method, based on ALGO (Zhang et al., 2023) that synthesizes both the oracle and tests. To start, we prompt an LLM, such as Anthropic Claude 3.5 Sonnet, to generate brute-force solution ybf to the problem. Specifically, we encourage it to use inefficient methods such as exhaustive search and enumeration of the possible output space. This is founded on the observation that it is relatively easy to generate solution that exhaustively searches the correct output, but more difficult to optimize it within time complexity bound. Then, an LLM is prompted to create validator program and 10 edge test input generators, which are used to generate one test input each, {a1, . . . , a10}. To prevent the ybf from timing out when computing their respective outputs, we explicitly prompt the LLM to keep input values small. Once these test inputs are verified for correctness using the validator, the brute-force solution is used to generate the corresponding outputs ci = ybf (ai) for each input, resulting in total of 10 input-output 32 pairs as test cases. Finally, the LLM is prompted to create one maximum-length test case amax with inputs at the upper bounds of the problems constraints, designed to catch solutions that are functionally correct but inefficient. This test case is considered to be passsed as long as the program produces an output before timing out. Crucially, all 11 of the generated test cases {a1, . . . , a10, amax} are designed to cause seemingly correct programs to fail, and none are generated using random inputs. We compare this method to the baseline method outlined in AceCoder (Zeng et al., 2025a), which uses direct prompt to generate 20 full test cases (inputs and corresponding outputs), also using Claude 3.5. Then, after prompting stronger model such as Qwen2.5Coder-32B-Instruct to generate solution, the test cases that cause the solution to fail are considered hallucinated and are filtered out. Problems with fewer than 5 test cases after filtering are discarded. To evaluate the accuracy of rewards that our test cases can give to model training, we evaluate the precision and recall over candidate programs generated by LLMs and written by humans on subsets of problems in HARDTESTS. The quality of the test cases are verified using 165 Atcoder problems, each with 50 sample solutions. It is clear from these experiments (shown in Table 8) that our method can also work much better than the baseline even when oracle programs are not available. The false positive rate of HARDTESTGEN is only half as high as AceCoder, showing that deliberately crafting high-quality, hard test cases is crucial for effective program verifiers. We will show some examples of the test generation process in the following sections. Table 8: Performance comparison of oracle-free test generation algorithms based on error rates. False Positive Rate (FPR) False Negative Rate (FNR)"
        },
        {
            "title": "HARDTESTGEN",
            "content": "32.49 17.67 2.59 2.19 A.7.1 Example 1: Brute-force oracle solution generated by ALGO In this example, we show brute-force oracle solution ybf that ALGO generates. The problem is challenging, and while the LLM is unable to correctly solve it directly, its brute-force solution works. Heres the problem description: AtCoder ABC301C: single-player card game is popular in AtCoder Inc. Each card in the game has lowercase English letter or the symbol written on it. There is plenty number of cards for each kind. The game goes as follows. - Arrange the same number of cards in two rows. - Replace each card with with one of the following cards: a, t, c, o, d, e, r. - If the two rows of cards coincide, you win. Otherwise, you lose. To win this game, you will do the following cheat. - Freely rearrange the cards within row whenever you want after step 1. You are given two strings and (1 S, 2 105), representing the two rows you have after step 1. Determine whether it is possible to win with cheating allowed. If it is possible to win with cheating allowed, print Yes; otherwise, print No. The solution generated by Claude 3.5 Sonnet is: from itertools import product def solve(): = input().strip() = input().strip() # Find positions of @ in both strings 1 3 4 5 6 7 8 9 10 11 12 14 15 16 17 18 20 21 22 23 24 26 27 28 29 30 32 33 34 35 36 s_at_pos = [i for i, in enumerate(S) if == '@'] t_at_pos = [i for i, in enumerate(T) if == '@'] # Possible characters to replace @ possible = ['a', 't', 'c', 'o', 'd', 'e', 'r'] # Try all possible combinations for s_comb in product(possible, repeat=len(s_at_pos)): # Create new string with replacements s_new = list(S) for pos, char in zip(s_at_pos, s_comb): s_new[pos] = char s_new = ''.join(s_new) for t_comb in product(possible, repeat=len(t_at_pos)): # Create new string with replacements t_new = list(T) for pos, char in zip(t_at_pos, t_comb): t_new[pos] = char t_new = ''.join(t_new) # Sort both strings (since we can rearrange) if sorted(s_new) == sorted(t_new): print(\"Yes\") return print(\"No\") solve() This solution is evidently inefficient. full analysis yields runtime in the bound of O(n log n7n), where = = , but the exponential complexity is clear from the Cartesian product function in the implementation. For inputs > 10, this algorithm quickly becomes intractable. However, for inputs 10 it is able to generate valid test outputs, allowing it to correctly evaluate the validity of submitted solutions. The test outputs it generates achieve 100% accuracy, compared to actual execution results from the online judge. A.7.2 Example 2: Test cases generated by ALGO In this example we show contest coding problem for which ALGO effectively generates testing suite. Heres the problem description: AtCoder cafeteria sells meals consisting of main dish and side dish. There are types of main dishes, called main dish 1, main dish 2, . . . , main dish . Main dish costs ai yen. There are types of side dishes, called side dish 1, side dish 2, . . . , side dish . Side dish costs bi yen. set meal is composed by choosing one main dish and one side dish. The price of set meal is the sum of the prices of the chosen main dish and side dish. However, for distinct pairs (c1, d1), . . . , (cL, dL), the set meal consisting of main dish ci and side dish di is not offered because they do not go well together. That is, set meals are offered. (The constraints guarantee that at least one set meal is offered.) Find the price of the most expensive set meal offered. The input is given from Standard Input in the following format: a1 a2 . . . aN b1 b2 . . . bM c1 d1 c2 d2 ... 34 cL dL Constraints: - 1 N, 105 - 0 min(105, 1) - 1 ai, bi 109 The first 3 edge test input generators created by ALGO are shown below, corresponding to the following test inputs. Note that the values are at the boundaries of the input bounds and follow clearly defined structures. [\"1 1 0n1000000000n1000000000\", \"10 10 1n1000 2000 3000 4000 5000 6000 7000 8000 9000 10000n1000 2000 3000 4000 5000 6000 7000 8000 9000 10000n1 1\", (cid:44) \"50 50 100n1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000n1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000 1000000000n33 36n5 1n18 44n43 12n5 37n50 36n15 14n10 27n34 3n16 40n47 18n28 14n9 10n20 40n41 8n4 41n10 20n7 21n37 1n49 39n9 3n14 26n42 18n41 28n26 9n13 21n39 3n16 17n24 30n17 46n33 42n9 42n34 7n36 13n6 36n33 8n26 2n43 18n4 45n35 23n39 5n33 35n8 45n34 18n43 48n14 12n17 11n26 41n4 47n44 28n9 9n50 19n2 15n20 5n9 39n20 14n24 39n5 13n4 15n10 39n48 23n43 36n21 33n4 35n11 13n46 10n11 31n8 44n2 37n46 37n16 39n39 43n34 38n3 50n7 20n34 13n34 31n28 33n31 4n22 1n38 19n12 9n6 23n27 18n9 4n11 10n28 26n2 7n45 42n31 43n39 1n15 1n11 19n12 48n15 10n4 7n45 26n2 18n35 15n33 27\"] def gen_edge_case_input_1(): # Minimum case: N=M=1, L=0 return \"1 1 0n1000000000n1000000000\" def gen_edge_case_input_2(): # Maximum values case = = 100 # Using smaller values for practicality = N*M - 1 = [10**9] * = [10**9] * pairs = [(i, j) for in range(1, N+1) for in range(1, M+1)] pairs.pop() # Leave one combination possible result = f\"{N} {M} {L}n\" result += \" \".join(map(str, a)) + \"n\" result += \" \".join(map(str, b)) + \"n\" for c, in pairs: result += f\"{c} {d}n\" return result.strip() def gen_edge_case_input_3(): # Only one forbidden pair = = 10 = [i*1000 for in range(1, N+1)] = [i*1000 for in range(1, M+1)] result = f\"{N} {M} 1n\" result += \" \".join(map(str, a)) + \"n\" result += \" \".join(map(str, b)) + \"n\" 35 1 2 3 1 3 4 5 6 7 9 10 11 12 13 15 16 17 18 19 21 22 23 24 25 27 28 29 30 1 3 4 5 6 7 9 10 11 12 13 15 16 17 18 19 21 22 23 24 25 27 28 29 30 31 33 34 35 36 37 2 3 4 5 6 8 result += \"1 1\" return result Also, the generator for the maximum-length test input amax is shown here. It produces test input where = = 105, which is the upper bound of the problem. import random def gen_maximum_edge_case_input(): = 100000 = 100000 = # Generate main dish prices close to max value main_prices = [random.randint(999999000, 1000000000) for _ in range(N)] # Generate side dish prices close to max value side_prices = [random.randint(999999000, 1000000000) for _ in range(M)] # Generate unique forbidden pairs used_pairs = set() forbidden_pairs = [] # Start with some specific high-value combinations for in range(L): while True: = random.randint(1, N) = random.randint(1, M) if (c, d) not in used_pairs: used_pairs.add((c, d)) forbidden_pairs.append((c, d)) break # Build the input string result = [] result.append(f\"{N} {M} {L}\") result.append(\" \".join(map(str, main_prices))) result.append(\" \".join(map(str, side_prices))) for c, in forbidden_pairs: result.append(f\"{c} {d}\") return \"n\".join(result) This test suite effectively achieves 100% accuracy on evaluating submissions, demonstrating that precise test inputs are crucial for oracle-free verifiers. A.7.3 Example 3: Test cases generated by AceCoder For the same Atcoder problem as Example A.7.2, AceCoder generates the following 16 test cases with inputs and outputs after filtering. While the LLM implicitly knows to generate edge test cases, shown in the maximal values of ci, di, all of the test cases have relatively similar and low values of and . [{\"input\": \"2 3 3n2 1n10 30 20n1 2n2 1n2 3\", \"output\": \"31\"}, {\"input\": \"2 1 0n1000000000 1n1000000000\", \"output\": \"2000000000\"}, {\"input\": \"1 1 0n5n7\", \"output\": \"12\"}, {\"input\": \"3 3 4n10 20 30n5 15 25n1 1n2 2n3 1n1 3\", \"output\": \"55\"}, {\"input\": \"5 3 7n100 200 300 400 500n100 200 300n1 1n1 2n1 3n2 1n2 2n3 1n4 1\", (cid:44) {\"input\": \"2 2 1n999999999 999999998n999999997 999999996n1 1\", \"output\": (cid:44) {\"input\": \"3 2 2n5 4 3n2 1n1 1n2 2\", \"output\": \"6\"}, {\"input\": \"4 3 5n10 9 8 7n6 5 4n1 1n2 2n3 3n4 1n4 2\", \"output\": \"15\"}, \"output\": \"800\"}, \"1999999995\"}, 36 9 10 12 13 14 15 16 2 3 4 5 6 8 9 10 11 12 14 15 16 17 18 20 21 22 {\"input\": \"2 4 3n100 200n300 400 500 600n1 1n1 2n2 3\", \"output\": \"800\"}, {\"input\": \"3 3 0n1 2 3n4 5 6\", \"output\": \"9\"}, {\"input\": \"4 2 3n10 20 30 40n50 60n1 1n2 2n3 1\", \"output\": \"100\"}, {\"input\": \"5 2 4n1 2 3 4 5n6 7n1 1n2 1n3 1n4 1\", \"output\": \"12\"}, {\"input\": \"3 4 6n100 200 300n400 500 600 700n1 1n1 2n1 3n2 1n2 2n3 3\", \"output\": (cid:44) {\"input\": \"2 2 0n1000000000 999999999n1000000000 999999999\", \"output\": \"2000000000\"}, {\"input\": \"3 3 3n100 200 300n100 200 300n1 1n2 2n3 3\", \"output\": \"500\"}, {\"input\": \"5 5 12n1 2 3 4 5n1 2 3 4 5n1 1n1 2n1 3n2 1n2 2n2 3n3 1n3 2n3 3n4 (cid:44) 1n4 2n5 1\", \"output\": \"10\"}] \"1000\"}, These test cases fail to correctly categorize solutions that exceed the problems time limit. One such example is shown below, which AceCoder falsely categorizes as positive solution. Compared to Example A.7.2, in which ALGO generated test inputs as large as = = 105, the test cases from AceCoder are no larger than = = 5, making them unable to break inefficient programs. Without brute-force reference oracle, and constrained by the requirement of generating input-output pairs simultaneously, the LLM used by AceCoder sticks to simple test cases that it can be confident are correct. Moreover, longer test cases are likelier to contain hallucinations, and get removed by their filtering process. As result, their test cases are relatively weaker and result in less effective verifiers. import sys def main(): input = sys.stdin.readline N, M, = map(int, input().split()) = list(map(int, input().split())) = list(map(int, input().split())) incompatible_pairs = set() for _ in range(L): c, = map(int, input().split()) incompatible_pairs.add((c - 1, - 1)) # Adjusting indices to be zero-based max_price = 0 for in range(N): for in range(M): if (i, j) not in incompatible_pairs: max_price = max(max_price, a[i] + b[j]) print(max_price) if __name__ == \"__main__\": main()"
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "UC Santa Barbara",
        "UT Austin"
    ]
}