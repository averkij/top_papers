{
    "paper_title": "Bridging Evolutionary Multiobjective Optimization and GPU Acceleration via Tensorization",
    "authors": [
        "Zhenyu Liang",
        "Hao Li",
        "Naiwei Yu",
        "Kebin Sun",
        "Ran Cheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Evolutionary multiobjective optimization (EMO) has made significant strides over the past two decades. However, as problem scales and complexities increase, traditional EMO algorithms face substantial performance limitations due to insufficient parallelism and scalability. While most work has focused on algorithm design to address these challenges, little attention has been given to hardware acceleration, thereby leaving a clear gap between EMO algorithms and advanced computing devices, such as GPUs. To bridge the gap, we propose to parallelize EMO algorithms on GPUs via the tensorization methodology. By employing tensorization, the data structures and operations of EMO algorithms are transformed into concise tensor representations, which seamlessly enables automatic utilization of GPU computing. We demonstrate the effectiveness of our approach by applying it to three representative EMO algorithms: NSGA-III, MOEA/D, and HypE. To comprehensively assess our methodology, we introduce a multiobjective robot control benchmark using a GPU-accelerated physics engine. Our experiments show that the tensorized EMO algorithms achieve speedups of up to 1113x compared to their CPU-based counterparts, while maintaining solution quality and effectively scaling population sizes to hundreds of thousands. Furthermore, the tensorized EMO algorithms efficiently tackle complex multiobjective robot control tasks, producing high-quality solutions with diverse behaviors. Source codes are available at https://github.com/EMI-Group/evomo."
        },
        {
            "title": "Start",
            "content": "IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION,"
        },
        {
            "title": "Bridging Evolutionary Multiobjective Optimization\nand GPU Acceleration via Tensorization",
            "content": "Zhenyu Liang, Hao Li, Naiwei Yu, Kebin Sun, and Ran Cheng 5 2 0 2 7 2 ] . [ 2 6 8 2 0 2 . 3 0 5 2 : r AbstractEvolutionary multiobjective optimization (EMO) has made significant strides over the past two decades. However, as problem scales and complexities increase, traditional EMO algorithms face substantial performance limitations due to insufficient parallelism and scalability. While most work has focused on algorithm design to address these challenges, little attention has been given to hardware acceleration, thereby leaving clear gap between EMO algorithms and advanced computing devices, such as GPUs. To bridge the gap, we propose to parallelize EMO algorithms on GPUs via the tensorization methodology. By employing tensorization, the data structures and operations of EMO algorithms are transformed into concise tensor representations, which seamlessly enables automatic utilization of GPU computing. We demonstrate the effectiveness of our approach by applying it to three representative EMO algorithms: NSGA-III, MOEA/D, and HypE. To comprehensively assess our methodology, we introduce multiobjective robot control benchmark using GPU-accelerated physics engine. Our experiments show that the tensorized EMO algorithms achieve speedups of up to 1113 compared to their CPU-based counterparts, while maintaining solution quality and effectively scaling population sizes to hundreds of thousands. Furthermore, the tensorized EMO algorithms efficiently tackle complex multiobjective robot control tasks, producing high-quality solutions with diverse behaviors. Source codes are available at https://github.com/EMI-Group/evomo. Index TermsEvolutionary Multiobjective Optimization, GPU Acceleration, Tensorization, Robot Control. I. INTRODUCTION In many real-world optimization problems (e.g., material design [1], [2], energy management [3], [4], network optimization [5], and portfolio optimization [6]), decision-makers must consider multiple (and often conflicting) objectives simultaneously. Without loss of generality, such multiobjective optimization problems (MOPs) can be defined as: minimize f (x) = (f1(x), f2(x), . . . , fm(x)), (1) where = (x1, x2, . . . , xd) Rd is the decision vector and is the number of decision variables. : Rm maps the decision vector to an m-dimensional objective space. Each fi : for = 1, . . . , represents an objective function that needs to be minimized (or maximized). The key challenge in solving MOPs is to identify set of tradeoff solutions known as the Pareto set (PS), where no single Zhenyu Liang, Hao Li, Naiwei Yu, and Kebin Sun are with the Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen 518055, China. E-mails: {zhenyuliang97, li7526a, yunaiweiyn, sunkebin.cn}@gmail.com Ran Cheng is with the Department of Data Science and Artificial Intelligence and the Department of Computing, The Hong Kong Polytechnic University, Hong Kong SAR, China. E-mail: ranchengcn@gmail.com. (Corresponding author: Ran Cheng) solution can optimize all objectives simultaneously. This set contains all Pareto-optimal or non-dominated solutions, and their corresponding points in the objective space collectively form the Pareto front (PF). Over the past two decades, the field of evolutionary multiobjective optimization (EMO) [7], [8] has seen rapid advancements, resulting in the development of various effective algorithms for solving MOPs. Broadly, these algorithms can be categorized into three main methods: dominance-based, decomposition-based, and indicator-based. Dominance-based algorithms, such as NSGA-II [9] and NSGA-III [10], select solutions based on dominance relations between individuals. Decomposition-based algorithms, like MOEA/D [11], break down an MOP into multiple simpler subproblems, which are optimized collaboratively. Indicator-based algorithms, such as HypE [12], focus on optimizing specific performance indicators, like hypervolume (HV) [13]. While EMO algorithms have proven effective in solving various MOPs, their performance is significantly constrained by limitations in computing power. First, since the majority of existing EMO algorithms still rely on CPUs for execution, their computational efficiency is inherently limited, particularly when addressing large-scale multiobjective optimization problems (LSMOPs) [14]. Second, the inconsistent implementation of EMO algorithms across different methods has led to fragmentation, making it difficult to standardize solutions and apply them across diverse domains. Without unified framework, efforts to generalize these algorithms and enhance computational efficiency are impeded. Third, much of the current research remains focused on relatively simpler numerical optimization tasks, often neglecting computationally intensive real-world applications. This narrow focus further limits the practical use of EMO algorithms in scenarios where real-time performance and scalability are crucial. To address these limitations, one promising method is to incorporate modern computational accelerators such as GPUs. With their powerful parallel processing capabilities, GPUs have demonstrated significant performance improvements in fields like deep learning [15]. However, to fully leverage the potential of GPUs for EMO algorithms, systematic method of parallelization is necessary, yet little effort has been made in this direction so far. Given the high concurrency enabled by the large number of tensor cores [16], GPUs are particularly well-suited for efficient handling and acceleration of large-scale data processing. Correspondingly, one promising method for parallelization on GPUs is tensorization, i.e., representing data structures and operations as tensors. Building upon this IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, 2025 concept, we introduce concise and general tensorization methodology for accelerating EMO algorithms on GPUs. By leveraging tensor operations and the inherent parallelism of GPUs, this methodology systematically explains how to transform EMO algorithms into concise tensor representations. Using this methodology, we implement tensorized versions of three representative EMO algorithms from each category: the dominance-based NSGA-III [10], the decomposition-based MOEA/D [11], and the indicator-based HypE [12]. Moreover, to evaluate the performance of tensorized EMO algorithms in GPU computing environments, we develop multiobjective robot control benchmark using Brax [17], GPU-accelerated physics engine. The main contributions of this research are as follows: 1) We introduce general tensorization methodology for EMO algorithms that transforms key data structures (i.e., candidate solutions and objective values) and operations (i.e., crossover, mutation, and selection) into tensor representations. This approach establishes concise yet versatile mathematical models for enabling efficient GPU acceleration of EMO algorithms. 2) We apply the proposed tensorization methodology to three representative EMO algorithms: NSGA-III, MOEA/D, and HypE. The tensorized algorithms achieve up to 1113 speedup compared to their CPU-based counterparts, while maintaining solution quality and effectively scaling population sizes to hundreds of thousands. 3) We develop multiobjective robot control benchmark called MoRobtrol. This benchmark represents computationally intensive scenario with complex black-box properties. It demonstrates the ability of the tensorized EMO algorithms to efficiently generate high-quality solutions with diverse behaviors in such computationally expensive environments. The structure of this paper is as follows: Section II reviews the background and related work. Section III introduces the tensorization methodology for GPU acceleration. Section IV details the implementations of core operations in three representative EMO algorithms. Section introduces the multiobjective robot control benchmark. Section VI outlines the experimental setup and results. Section VII summarizes the findings and discusses future work. II. BACKGROUND A. Taxonomy of EMO Algorithms Traditional evolutionary multiobjective optimization (EMO) algorithms can generally be classified into three main categories based on their selection mechanisms: dominance-based, decomposition-based, and indicator-based [7]. Dominance-based EMO algorithms are pivotal in addressing complex optimization tasks through Pareto dominance. As the pioneering algorithm in this category, NSGA-II [9] introduced fast non-dominated sorting approach, which has since become foundation for many subsequent algorithms. SPEA2 [18] introduces fine-grained fitness assignment strategy, density estimation, and enhanced archive truncation to improve performance. GrEA [19] enhances convergence and diversity balance by using grid dominance. NSGA-III [10] further advances diversity management in higher-dimensional spaces with reference points. Recent algorithms build on these foundations with innovative strategies. BiGE [20] focuses on proximity and diversity through bi-goal optimization, while VaEA [21] balances these using vector-angle-based principles. RSEA [22] improves performance by projecting solutions into radial space, and NSGA-II/SDR [23] introduces novel dominance relation with adaptive niching techniques. MSEA [24] divides the optimization process into stages to enhance diversity preservation. PMEA [25] eliminates dominance resistance solutions using an interquartile range method. Decomposition-based EMO algorithms address MOPs by decomposing them into simpler subproblems [26]. These algorithms can be further categorized into two types: weighted aggregation based and reference set based methods. MOEA/D [11] is the most representative weighted aggregation based method, which aggregates objectives using weight vectors and has inspired variants like MOEA/D-DRA [27] and EAG-MOEA/D [28]. Recent developments include MOEA/DAAWNs [29], which adapts weight vectors and neighborhoods to enhance diversity. MOEA/D-GLCM [30] employs bidirectional global search and adaptive neighborhood strategies to improve population distribution. Reference set based methods, on the other hand, divide the objective space using reference points or vectors, guiding the search towards underexplored regions while maintaining diversity. Representative algorithms of this type include MOEA/D-M2M [31], RVEA [32], and θ-DEA [33]. recent work, ECRA-DEA [34], adaptively allocates resources across subspaces using fitness contribution and improvement rates. Indicator-based EMO algorithms rely on performance indicators for selection [35]. As notable example, IBEA [36] uses binary ϵ+ indicators for decision-making. Following this, SMS-EMOA [37] and HypE [12] are representative hypervolume-based algorithms, with HypE using Monte Carlo sampling to approximate HV. BCE-IBEA [38] integrates bicriterion evolution with the IBEA framework, while SRA [39] combines multiple indicators with stochastic ranking based environmental selection. MOMBI-II [40] uses the R2 metric, and AR-MOEA [41] is guided by the IGD-NS metric. MaOEA/IGD [42] employs the inverted generational distance metric. Recently, R2HCA-EMOA [43], HVCTR [44], and IMOEA-ARP [45] extended the SMS-EMOA framework with innovations in HV approximation, reference point management, and diversity handling, respectively. B. GPU Acceleration in EMO Algorithms Most previous efforts in GPU acceleration for EMO have focused on specific algorithms and implementations of certain algorithmic components. An early contribution by Wong et al. [46] introduced GPU-accelerated non-dominated sorting in NSGA-II. Building on this, Sharma et al. [47] proposed GASREA, GPU-accelerated variant of NSGA-II that incorporates an external archive to sort non-dominated solutions on the GPU. Arca et al. [48] applied GPU acceleration to the IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, 2025 3 evaluation phase of NSGA-II, specifically within the context of fuel treatment optimization. Aguilar-Rivera et al. [49] further developed fully vectorized NSGA-II, employing stochastic non-domination sorting and grid-crowding techniques. In addition to dominance-based algorithms, Souza et al. [50] proposed the GPU-accelerated MOEA/D-ACO algorithm, where solution construction and pheromone matrix updates benefit from data-parallel processing. Lopez et al. [51] leveraged GPUs to accelerate hypervolume contribution calculations, improving performance in the SMS-EMOA algorithm. Furthermore, Hussain et al. [52] introduced fast CUDAbased implementation of MOPSO on GPUs. More recently, frameworks based on Googles JAX [53] have opened new possibilities for GPU acceleration in EMO. Examples include EvoJAX [54], evosax [55], and EvoX [56], all of which provide open-source platforms for GPU-accelerated evolutionary algorithms. While EvoJAX and evosax focus on accelerating evolutionary strategies, EvoX is designed as distributed GPU-accelerated framework that supports general evolutionary computation. Building upon EvoX, recent effort has been made to tensorize the RVEA [32] for GPU acceleration [57]. Despite these advances, GPU-accelerated EMO algorithms remain in their infancy. Current research has primarily focused on specific implementations that provide isolated performance improvements. Moreover, many of these implementations rely heavily on CUDA programming [58] and are not open-source, thus making them less accessible, particularly for beginners. III. TENSORIZATION METHODOLOGY In this section, we present how to adopt the general tensorization methodology in EMO algorithms. Specifically, we begin by defining the notation and preliminary concepts used throughout this paper, including the basic definitions of tensor and tensorization. Next, we demonstrate how to transform atomic operations such as basic operations and control flow operations into tensors. Furthermore, we discuss why and how tensorization matters for GPU acceleration. A. Preliminaries tensor is multidimensional array that generalizes scalars, vectors, and matrices to higher dimensions [59]. Formally, k-th order tensor is an element of the tensor product of vector spaces: Rd1d2dk , where di represents the dimension along the i-th mode (axis) of the tensor. Scalars are zero-order tensors, vectors are first-order tensors, and matrices are second-order tensors. Correspondingly, tensorization refers to the process of transforming algorithmic data structures and operations into tensor representations. This transformation facilitates efficient parallel computation, particularly on massively parallel hardware such as GPUs, by leveraging the inherent parallelism in tensor operations to improve computational performance and scalability. In this paper, scalars (0-order tensors) are denoted by lowercase letters (e.g., a), vectors (1st-order tensors) are denoted by italicized bold lowercase letters (e.g., a), matrices (2nd-order tensors) are denoted by italicized bold uppercase letters (e.g., TABLE TENSOR VARIABLES IN EMO ALGORITHMS Notation Description X R, , Population size Number of objectives Problem dimension Solution tensor Objective tensor Reference and weight tensors Upper and lower bound tensors A), and higher-order tensors are denoted by calligraphic letters (e.g., ). The tensor variables related to EMO algorithms are summarized in Table I. Correspondingly, tensorized MOP can be formulated as: minimize (X) = (f1(X), f2(X), . . . , fm(X)), (2) where Rnd is the solution tensor for individuals and dimensions, and Rnm is the corresponding objective tensor. TABLE II BASIC TENSOR OPERATIONS Operation Description H(A) 1A sort argsort min, max argmin vmap Tensor multiplication: Product of two tensors and B. Hadamard product: Element-wise multiplication of and B. Heaviside step function: Returns 1 if Aij 0, 0 otherwise. Indicator function: Returns 1 if Aij is true, 0 otherwise. Arranges elements in ascending order. Returns indices of sorted elements. Returns the smallest or largest element along the specified axis: column-wise if axis = 0, row-wise if axis = 1. Returns the indices of the smallest elements. Vectorization map: Applies function across an array axis. B. Tensorization of Data Structures in EMO Algorithms In EMO algorithms, the candidate solutions and their corresponding objective values are two critical data structures, as expressed in Eq. 2. They can be encoded as tensors and , respectively. Moreover, decomposition-based algorithms use additional structures such as reference vectors or weights, which are similarly encoded as tensors and , respectively. Here, Rrm typically represents reference vectors, each corresponding to different point in the objective space, and Rnm denotes weight tensor with different weights. These tensor representations allow for the parallel and batch processing of operations along the population dimension, capitalizing on the independence of individuals within the population. Consequently, the EMO algorithm can efficiently process one entire population at time. C. Tensorization of Operations in EMO Algorithms After establishing an effective tensorized data structure, the next crucial step is to consider tensorization for core operations in EMO algorithms, such as crossover, mutation, and selection IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, 2025 4 mechanisms (e.g., environmental selection). These operations consist of numerous atomic operations, including both basic tensor operations and control flow operations. 1) Basic Tensor Operations: Basic tensor operations serve as the foundation for transforming more complex operations in EMO algorithms. Table II summarizes these operations, including tensor multiplication, Hadamard product, Heaviside step function, and indicator function, as well as advanced functions like sort, argsort, min, max, and argmin. These functions are essential for implementing selection and ranking strategies, enabling GPU-accelerated non-dominated sorting, and diversity maintenance in EMO algorithms. 2) Control Flow Operations: The tensorization of control flow operations, including loops and branches, is key challenge in transforming traditional EMO algorithms into their tensor-based counterparts. Control flow operations are commonly used to implement iterative procedures and define selection rules based on specific conditions. However, traditional control flow operations such as loops (for and while) and if-else branches introduce sequential dependencies that hinder parallel execution and reduce the efficiency of GPU computations. Tensorizing these operations requires replacing them with tensor-based operations that can execute in parallel. Loop operations are often used in EMO algorithms for tasks like calculating distances or updating solutions. However, with large population size, sequential processing becomes inefficient. These loops can be replaced by either vectorized mapping functions (e.g., vmap), or by using broadcasting combined with basic operations. The vmap function automatically applies specified function across all elements in given tensor dimension, eliminating explicit loops. Mathematically, vmap can be expressed as: (3) vmap(f )(A) = [f (A1), (A2), . . . , (An)], where Rnm is the input tensor and is the function applied to each individual Ai. The function operates independently on each element, with vmap managing parallel computation and concatenation of results. Alternatively, broadcasting can be used to perform the same operations without any loops. Broadcasting works by expanding the dimensions of tensors to align them, enabling element-wise operations to be performed simultaneously across the entire population. Branch, such as if-else operations, are another type of control flow that poses challenges for tensorization. In traditional implementations, if-else operations introduce branching and disrupt parallel execution. To address this issue, tensorization replaces branch with element-wise masking operations such as where. For example, traditional branch assigning values to population matrix based on threshold can be written as: Yij = (cid:40) Aij, Bij, if Mij > τ otherwise , (4) where A, are input tensors, τ is threshold, and is the mask tensor. It can be replaced by tensorized masking operation: = 1M >τ + (1 1M >τ ) B, (5) 1 import torch 2 3 # Conventional implementation 4 def dominance_detection_conventional(P): 5 = P.size(0) dom = torch.zeros(n, n, dtype=torch.bool) for in range(n): for in range(n): if != j: if (P[i] <= P[j]).all() and (P[i] < P[j]).any(): dom[i, j] = True return dom 14 15 # Tensor-based implementation 16 @torch.compile 17 def dominance_detection_tensor(P): P1 = P.unsqueeze(1) P2 = P.unsqueeze(0) return (P1 <= P2).all(dim=2) & 6 7 8 9 11 12 13 19 20 (P1 < P2).any(dim=2) Listing 1: Comparison between conventional and tensor-based implementations of Pareto dominance detection. which can be implemented as where(M > τ, A, B). 3) Advantages over Conventional Operations: Tensorization offers several key advantages over conventional EMO implementations. First, it provides greater flexibility by handling multi-dimensional data, whereas conventional matrix operations are often limited to two dimensions. Second, tensorization enhances computational efficiency by enabling parallel processing and removing the need for explicit loops and conditional branches. Finally, tensorization simplifies the code, making it more concise and easier to maintain. For instance, as shown in Listing 1, conventional Pareto dominance detection relies on nested for loops and if-else statements to compare individuals. In contrast, the tensorized version uses element-wise operations, broadcasting, and masking to perform these comparisons in parallel, significantly reducing code complexity and boosting performance. D. Discussion 1) Why is tensorization crucial for GPU acceleration: With thousands of cores designed to manage multiple tasks simultaneously, GPUs are intrinsically tailored for parallel computing. The architectures of GPUs, such as the SIMT (Single Instruction, Multiple Threads) model, enable efficient execution of tensor operations. Moreover, the specialized features like NVIDIAs Tensor Cores further enhance performance by accelerating matrix multiplication and accumulation tasks. This makes the tensorization methodology ideally suited for GPUs, which inherently involve large-scale parallel computations. 2) What algorithms are suitable for tensorization: Algorithms with independent computations and minimal branching are ideal for tensorization, as they can be easily parallelized. In contrast, algorithms that depend on sequential processes, frequent branching, or recursion pose challenges for parallelization. For example, in traditional MOEA/D, the aggregate function computation relies on results from previous iterations, thus making direct tensorization challenging. Nonetheless, by IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, 2025 5 restructuring and decoupling such algorithms, the tensorization methodology can still be effectively applied. IV. TENSORIZATION IMPLEMENTATION IN REPRESENTATIVE EMO ALGORITHMS In this section, we present the application of tensorization methodology in three representative EMO algorithms: NSGAIII, MOEA/D, and HypE. The genetic operators, including mating selection, crossover, and mutation, are common across most EMO algorithms and follow similar tensorization procedures, which are elaborated in Section S.I of the Supplementary Document. Here, we focus on the tensorized implementation of the environmental selection operators specific to each algorithm. It is important to note that both the environmental selection in NSGA-III and the Monte Carlo-based selection in HypE are inherently highly parallelizable, which allows for straightforward tensorization. In contrast, the MOEA/D algorithm presents unique challenge due to its fundamentally sequential nature. As shown in Algorithm 1, each subproblem in MOEA/D involves four interdependent steps that must be executed in sequence. This sequential dependency prevents tensorization and requires reconfiguration of the direct entire process to enable parallel computation, which will be elaborated in Section IV-B. A. Tensorized Environmental Selection in NSGA-III The key components of environmental selection in NSGAIII include non-dominated sorting, normalization, association, niche count calculation, and niche selection. The tensorization is elaborated as follows. The process of each component pseudocode of both the original and tensorized algorithms is provided in Section S.II of the Supplementary Document. 1) Non-dominated Sorting: Given the combined objective tensor R2nm, representing the objective tensor of both parent and offspring populations, the non-dominated rank is computed iteratively. The primary goal is to assign non-dominated rank to each individual, where lower ranks correspond to better solutions. First, the dominance relation tensor {0, 1}2n2n is computed using vmap or broadcasting for parallel processing. Each element Dij indicates whether the solution Fi dominates Fj: Dij = Fi Fj, i, = 1, 2, . . . , 2n. (6) Next, we calculate the dominance count tensor Z2n, which indicates how many individuals each individual dominates: 2n (cid:88) = Dij. (7) j=1 The rank tensor Z2n is initialized to zeros, and the rank counter is set to zero. The boolean tensor {0, 1}2n, the set of all non-dominated solutions at the current rank, is obtained by = 1c=0. In each iteration, individuals sharing the same dominance rank are identified and processed collectively. This method ensures that even with large population size, the number of iterations remains relatively low. Additionally, the while function is optimized for accelerated computation. The rank tensor is updated as follows: = H(p) + H(1 p) r. (8) After rank assignment, the dominance count is updated by: = 2n (cid:88) i= pi Dij p. (9) The process repeats until all individuals are ranked, which means that all elements in are zero. Once all ranks are assigned, the tensor is sorted to determine the rank of the n-th individual. 2) Normalization: After performing non-dominated sorting, the objective tensor undergoes normalization process, similar to that in the original NSGA-III algorithm. This normalization ensures that the objectives are comparable by mapping them onto hyperplane, enabling the algorithm to maintain diversity across generations. 3) Association: In this step, each individual in the population is associated with the closest reference point. The distance between the normalized objective tensor and the reference tensor is computed using the perpendicular distance: = (cid:113) 1 (F R/(F R))2. (10) Based on the distance tensor D, the index of the closest reference point π is the index of the minimum value in each row of D, and the corresponding distance represents the minimum value in each row. 4) Niche Count Calculation: For each reference point, is computed, which indicates how many the niche count individuals are associated with that reference point. The niche count tensor ρ is calculated as: ρj = 2n (cid:88) i=1 H(l ri) 1πi=j, = 1, . . . , nr, (11) where nr is the number of reference points. The tensor ρl represents the niche count for the last front (i.e., the niche count corresponding to the front when ri = l): ρl,j = 2n (cid:88) i=1 1ri=l 1πi=j, = 1, . . . , nr. (12) The total number of selected individuals ns is then updated as ns = (cid:80) ρ. 5) Niche Selection: In the niche selection process of NSGA-III, the distance tensor represents adjusted by niche counts for each reference tensor. This tensor is used to identify individuals closest to underpopulated niches. The index of the selected individual is determined by minimizing the distance in each row of D, i.e., = arg minj(D). After selecting the individual, the rank tensor is updated by setting r[q] = 1, with further updates in subsequent iterations reflecting the inclusion of individuals in the current front. Once the selection is complete, the indices of the top individuals for the next generation are determined by: inext = sort(H(l r) + (1 H(l r)))[: n], (13) IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, 2025 6 Fig. 1: Overview of the environmental selection in the TensorMOEA/D algorithm. Left: Pseudocode of the algorithm. Right: Tensor dataflow of module (1) and module (2). The upper part of the right figure shows the overall tensor dataflow for modules (1) and (2), while the lower part presents the batch calculation tensor dataflow, with module (1) on the left and module (2) on the right. where = [0, 1, . . . , 1]. The final solution tensors Xnext and Fnext are formed by selecting individuals using the sorted indices: Xnext = X[inext] and Fnext = [inext]. Algorithm 1 Main Framework of Original MOEA/D Input: The maximal number of generations tmax; weight vectors; the neighborhood size of each weight vector; Output: Final population; 1: Initialization; 2: for = 1 to tmax do for = 1 to do 3: Reproduction; 4: Fitness Evaluation; Ideal Point Update; Neighborhood Update; 5: 6: 7: 8: 9: end for end for B. Tensorized Environmental Selection in MOEA/D In the original MOEA/D algorithm, as shown in Algorithm 1, the reproduction, fitness evaluation, ideal point update, and neighborhood update are executed sequentially within single loop. This method requires processing individuals one by one in specific order, which can significantly impede the execution speed of the algorithm. To address this limitation, we apply tensorization methodology to decouple these four steps in the inner loop (i.e., environmental selection), treating them as independent operations. This adjustment enables parallel processing of all individuals in the tensorized version, referred to as TensorMOEA/D. In TensorMOEA/D, reproduction generates individuals simultaneously based on the neighborhood, contrasting with the original MOEA/D, which produces one individual at time. The environmental selection process is further divided into two main steps: comparison and population update, and elite selection. These two steps are primarily implemented using two vmap operations, with the tensorization process detailed as follows. The pseudocode of both the original and tensorized algorithms is provided in Section S.III of the Supplementary Document. 1) Comparison and Population Update: The primary objective of this step is to determine the indices for the updated population by comparing the aggregated function values of the old and new populations. This process ultimately generates an updated index tensor Inew, where each row corresponds to subpopulation that mirrors the structure of the original population, containing indices of individuals. Positions that require updates are indicated by 1. Given the solution tensor X, objective tensor F1, offspring tensor O, the objective of offspring F2, the ideal points z, the weights , and the neighbors indices Inb, the process begins by calculating the minimal objective values to update the reference points, zmin, by finding the minimum objective values between the current population and the offspring. Next, the subpopulation indices Isub are created to track population updates. The vmap function is utilized to calculate the update indices Inew: Inew = vmap(fop1)(Inb, F2), (14) where fop1(inb, f2) = Isub. This process is conducted in parallel by batching the rows of Inb and F2, with inb and f2 representing single batch of these tensors. Each batch undergoes the fop1 operation, and all batches are processed simultaneously to yield results for all updates. visual representation of this operation can be found in Fig. 1, with an example for the first batch illustrated in the lower left corner of the tensor dataflow on the right. IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, 2025 7 In the fop1 function, inb is used as an index to extract the corresponding rows from F1 and , and the entries of f2 are replicated to obtain the inputs needed for the aggregation function. The aggregation function employs the penalty-based boundary intersection (PBI) function: fPBI(f , w, z) = d1 + θ d2, (15) (f z)w , d2 = (f z) (d1 w), and θ is where d1 = preset penalty parameter. The old and new aggregated values gold and gnew are computed by applying the PBI function. The mask stores the comparison results and is initially zero tensor, updated as follows: [inb] = H(gold gnew). (16) Finally, the subpopulation indices Isub are updated based on : 1) Sampling Bound Determination: The lower and upper bounds for sampling are determined by calculating the minimum objective values, fl = mini(F ), and using the reference point, fu = vref. These bounds define the hyperrectangle for sampling. Next, uniformly sample points from the hyperrectangle defined by fl and fu to generate sample tensor of dimension m. The initial distance score vds is then initialized to zeros with dimensions 1 s. 2) Sampling Weights Calculation: The sampling weights α are calculated as follows: αj = (cid:89) i= λi/j, = 1, 2, . . . , k, (19) where λ is defined as λ = [1, (k l)/(n1 l)]. Here, = [i N, 1 < n1], and n1 is the number of rows in . 3) Dominance Score and Distance Updating: Calculate the dominance scores for each sample using the function fpds: Isub = (1) + (1 ) Isub, (17) fpds(f ) = 1(cid:80)m j=1 H(Sij )=m, (20) where positions that need updates are assigned value of 1. 2) Elite Selection: The purpose of this step is to select the best individuals along distinct weighted directions based on the aggregated function values. Each direction yields single elite individual, resulting in total of individuals that form the population of next generation. To efficiently update the solution and objective tensor for the entire population, the function vmap is applied to parallelize the computation over all rows of the input: Xnext, Fnext = vmap(fop2)(I new, X, F1, ), (18) where vmap maps the function fop2(inew, x, f1, w) to each row of the provided inputs in parallel. Fig. 1 shows this process in the bottom-right corner, highlighting the computation for batch 3 (i.e., the 4th row). The function fop2 is defined to update the population and objective tensor based on the new indices inew, f1, x, and w. the objective values are updated as = Specifically, 1inew=1 F2 + (1 1inew=1) f1. And the individual is updated as = 1inew=1 + (1 1inew=1) x. The index = argmin(fPBI(f , w, zmin)) is used to select the best solution in x. Finally, the ideal points are updated to the minimum objective values zmin for the next iteration. C. Tensorized Environmental Selection in HypE The environmental selection in HypE primarily involves non-dominated sorting and HV calculation. In this paper, we utilize the Monte Carlo estimation method for HV calculation that is well-suited for tensorization. This Monte Carlo-based HV calculation consists of four key steps: sampling bound determination, sampling weights calculation, dominance score and distance update, and hypervolum calculation. The implementation details for these steps are as follows. The pseudocode of both the original and tensorized algorithms are provided in Section S.IV of the Supplementary Document. where = 1, 2, . . . , s. The dominance scores Tpds are then computed by applying vmap in parallel to the function fpds across all rows of the objective tensor . The distance score vds is then updated based on temporary matrix Ttemp, which is computed as Tpds combined with vds and tensor of ones 1n1: vds = maximum (cid:33) (Ttemp)i 1, , (21) (cid:32) n1(cid:88) i=1 where Ttemp is calculated as Tpds(1n1vds+1)+(1Tpds) (1n1 vds), and maximum(, 0) is an element-wise operation that compares each element with 0, returning the element itself if it is greater than or equal to 0, and 0 otherwise. 4) Hypervolume Calculation: The HV contributions are calculated using the function fhv, which sums the contributions from each sample: fhv(tpds) = (cid:88) i=1 (cid:0)α[δ] 1tpds=1 (cid:1) , (22) where δ = tpds vds (1 tpds). This function is then applied in parallel to each row of the point dominance score tensor Tpds using vmap, which efficiently computes the HV contributions across all samples. Finally, the total HV vhv is obtained by aggregating these contributions and normalizing: vhv = vhv (cid:89) i=1 (vref,i fl,i) /s. (23) For other indicator-based EMO algorithms, the indicator is typically calculated using mathematical expressions, which facilitates straightforward tensorization. In the case of more complex indices, such as HV, Monte Carlo sampling can be employed to enhance algorithm efficiency by approximating these values. Additionally, to improve efficiency and scalability, the niche selection phase can use one-shot method [12] for parallel selection. IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, 8 Fig. 2: The seamless transformation of the tensorized non-dominated sorting from pseudocode (Left) to Python code (Right). D. Discussion The tensorization methodology offers unique transformative benefits in EMO algorithm design and implementation. One of the key advantages is the seamless transformation of EMO algorithms from mathematical formulations into efficient code implementations. This bridge between algorithm design and programming is particularly valuable in GPU computing, where tensor operations can be leveraged for significant performance gains. For instance, Fig. 2 illustrates how pseudocode for tensorized non-dominated sorting can be directly translated into Python code. This straightforward translation reduces the gap between high-level algorithmic design and practical implementation, thereby enabling practitioners to focus more on the theoretical aspects without the burden of intricate code optimization. Tensorization also provides high level of conciseness, significantly reducing code complexity compared to traditional iterative pseudocode. By representing population-wide operations as single tensor expressions, tensorization minimizes the need for loops and conditional statements, making the codebase more compact and readable. This conciseness not only eases code maintenance but also reduces the risk of programming errors by limiting procedural complexity. Moreover, tensorization facilitates reproducibility in the field of EMO. As tensorized code relies on structured mathematical expressions, it becomes easier for researchers and developers to replicate results and benchmark different methods. This standardization paves the way for creating robust and high-performance libraries that can be shared and reused across various applications, thereby ultimately advancing research and industrial applications in EMO. V. MULTIOBJECTIVE ROBOT CONTROL BENCHMARK Traditional EMO benchmarks, such as ZDT [60], DTLZ [61], WFG [62], LSMOP [63], and MaF [64], primarily focus on numerical optimization problems. While these benchmarks are effective for evaluating the basic mechanisms of EMO algorithms, they are limited in their ability to leverage hardware acceleration, thereby reducing their relevance in GPU computing environments. In contrast, multiobjective robot control tasks present more realistic and computationally intensive challenges that better reflect real-world applications, which is particularly significant in the emerging area of embodied artificial intelligence (Embodied AI) [65]. These tasks provide complex and dynamic environments where multiple objectives must be balanced, such as energy efficiency and stability, making them well-suited for testing the adaptability and robustness of EMO algorithms. However, these tasks have been underexplored in the EMO community due to lack of suitable benchmarks and the significant computational cost of running these environments. To address this gap, we introduce the multiobjective robot control benchmark test suite, dubbed MoRobtrol, which reformulates nine tasks from the Brax environment [17] into MOPs. As GPU-accelerated physics simulation engine, Brax provides substantial performance improvement over CPU-based platforms such as OpenAI Gym [66] and Mo-Gymnasium [67]. By leveraging Braxs GPU computing capabilities, MoRobtrol enables scalable and rapid evaluations, making it an ideal benchmark for testing EMO algorithms in computationally demanding settings in practice. As illustrated in Fig. S.1, the MoRobtrol benchmark includes nine robot control tasks: MoHalfcheetah, MoHopper, MoSwimmer, MoInvertedDoublePendulum (MoIDP), MoWalker2d, MoPusher, MoReacher, MoHumanoid, and MoHumanoidStandup (MoHumanoid-s). These tasks involve optimizing multiple conflicting objectives, such as speed, energy consumption, and distance to target, reflecting tradeoffs commonly encountered in robotics applications. Specifically, in MoRobtrol, the parameters being optimized are the weights of multilayer perceptron (MLP), common design in control policy modeling for evolutionary reinforcement learning (EvoRL) [68]. These parameters are optimized by EMO algorithms to enable agents to maximize performance IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, 2025 9 TABLE III OVERVIEW OF MULTIOBJECTIVE ROBOT CONTROL PROBLEMS IN THE PROPOSED MOROBTROL BENCHMARK TEST SUITE Problem MLP architecture m 17 16 6 MoHalfcheetah 11 16 3 MoHopper 8 16 2 MoSwimmer 8 16 1 MoIDP 17 16 6 MoWalker2d 23 16 7 MoPusher 11 16 2 MoReacher 244 16 17 MoHumanoid 244 16 17 MoHumanoid-s All MLP networks use the tanh activation function. fv, fc fv, fh, fc fv, fc fdp, fsp fv, fc fn, fd, fc fd, fc fv, fc fv, fc 390 243 178 161 390 503 226 4209 4209 across conflicting objectives. Table III provides an overview of the nine tasks, including the MLP structure, number of parameters (d), and specific objectives for each task. The key objectives across tasks include forward reward (fv), control cost (fc), height (fh), distance penalty (fdp), speed penalty (fsp), distance reward (fd), and near reward (fn). The number of objectives varies depending on the task, allowing for detailed evaluations of algorithmic performance in diverse, real-world-inspired control scenarios. Detailed mathematical definitions are provided in Section S.VI of the Supplementary Document. VI. EXPERIMENTAL STUDY In this section, we conduct experiments to evaluate the including performance of the tensorized EMO algorithms, TensorNSGA-III, TensorMOEA/D, and TensorHypE. The experiments are categorized into three main aspects: acceleration performance, benchmarking on standard EMO test problems, and evaluation in multiobjective robot control tasks. All experiments are conducted on an RTX 4090 GPU server with AMD EPYC 7543 CPUs using the EvoX [56] framework. A. Acceleration Performance To verify the acceleration performance of the three proposed algorithms, we have conducted two sub-experiments. The first sub-experiment doubles the population size and observes the average runtime per generation. The second sub-experiment doubles the problem dimension and observes the average runtime per generation. We compare the performance of NSGAIII, MOEA/D, and HypE before and after tensorization on both CPU and GPU platforms using the DTLZ1 [61] problem. Additionally, further experiments have been conducted to compare the performance of the tensorized algorithms in comparison with CUDA-accelerated algorithms in EvoTorch [69], types of as well as to investigate the impact of different GPUs on performance. Detailed results of these experiments can be found in Section S.VII-B and Section S.VII-C of Supplementary Document, respectively. 1) Experimental Settings: In the two sub-experiments, the tensorized and non-tensorized1 algorithms are independently repeated 10 times on both CPU and GPU devices, with each algorithm evolving for 100 generations. The average runtime 1The non-tensorized algorithms are partially tensorized for efficiency, as the original versions are time-consuming for large populations. per generation is then calculated. In the first sub-experiment, the DTLZ1 problem has dimension = 500, with = 3, and the population size is doubled incrementally from 128 to 32 768. In the second sub-experiment, the DTLZ1 problem is configured with = 3, = 100, and is incrementally doubled from 1024 to 1 048 576. 2) Comparison Results: Fig. 3 shows that TensorNSGAIII, TensorMOEA/D, and TensorHypE consistently achieve faster runtimes on GPU compared to their non-tensorized versions on CPUs. When the population size reaches 32 768, TensorNSGA-III, TensorMOEA/D, and TensorHypE attain speedups of approximately 191, 1113, and 186, respectively, compared to their CPU-based counterparts. As the problem dimension increases to 1 048 576, these speedups rise to 304, 228, and 263. Although the runtime of tensorized algorithms may increase with n, they consistently outperform the non-tensorized versions. Additionally, as problem dimensions scale up, the runtime of tensorized algorithms remains relatively stable, maintaining significant performance advantage over the original algorithms. (a) (c) (e) (b) (d) (f) Fig. 3: Comparative acceleration performance of NSGA-III, MOEA/D, and HypE with their tensorized counterparts on CPU and GPU platforms across varying population sizes and problem dimension. Notably, when exceeds 16 384, the runtime for NSGA-III on GPU surpasses the preset threshold of 5 hours, resulting in missing data points for larger populations. For the HypE algorithm, once exceeds 1024, its average runtime per generation on GPU begins to exceed that of TensorHypE on IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, 2025 10 TABLE IV STATISTICAL RESULTS (MEAN AND STANDARD DEVIATION) OF THE IGD AND RUNTIME (S) FOR NON-TENSORIZED AND TENSORIZED EMO ALGORITHMS IN LSMOP1LSMOP9. ALL EXPERIMENTS ARE ON AN RTX 4090 GPU AND THE BEST RESULTS ARE HIGHLIGHTED. Algorithm Problem IGD (Non-Tensorized) IGD (Tensorized) Time (Non-Tensorized) Time (Tensorized) NSGA-III MOEA/D HypE LSMOP1 LSMOP2 LSMOP3 LSMOP4 LSMOP5 LSMOP6 LSMOP7 LSMOP8 LSMOP LSMOP1 LSMOP2 LSMOP3 LSMOP4 LSMOP5 LSMOP6 LSMOP7 LSMOP8 LSMOP9 LSMOP1 LSMOP2 LSMOP3 LSMOP4 LSMOP5 LSMOP6 LSMOP7 LSMOP8 LSMOP9 8.2378e01 (5.2173e03) 3.5883e01 (5.9506e06) 1.4106e+00 (7.5580e02) 5.9798e01 (1.5968e04) 5.6862e01 (2.3841e03) 2.8277e+00 (5.9075e01) 1.8425e+00 (7.5222e03) 3.0317e01 (2.9719e02) 7.4488e01 (9.4755e03) 7.3475e01 (3.8276e04) 3.5885e01 (3.4131e05) 1.0815e+00 (1.0631e01) 5.9570e01 (1.5407e03) 4.0554e01 (8.8890e03) 1.6366e+00 (7.7256e02) 1.5534e+00 (2.6469e01) 2.1207e01 (6.4136e03) 5.1964e01 (1.7163e02) 8.2757e01 (1.2610e03) 3.5912e01 (5.5267e+00) 4.6906e+00 (3.2754e01) 5.9841e01 (2.3223e04) 5.2160e01 (1.3777e03) 3.0407e+00 (1.2805e01) 1.8223e+00 (2.5992e03) 3.6062e01 (9.5863e03) 6.5873e01 (3.0162e03) 1.0399e+00 (1.2327e01) 3.5883e01 (2.0165e05) 7.3244e+00 (5.3558e01) 5.9815e01 (2.0699e04) 6.4269e01 (6.6591e03) 2.3737e+01 (2.9479e+00) 1.8474e+00 (3.7155e03) 3.5186e01 (1.3261e02) 7.6837e01 (2.9024e03) 7.4365e01 (2.1169e03) 3.5888e01 (5.9703e06) 8.2722e01 (5.3689e02) 5.9420e01 (1.1813e03) 3.7065e01 (8.5792e03) 1.8622e+00 (1.2612e01) 1.7345e+00 (3.6746e03) 2.0107e01 (1.8164e02) 5.0269e01 (1.0548e02) 8.2747e01 (1.0132e03) 3.5912e01 (4.9532e05) 4.6906e+00 (3.2754e01) 5.9847e01 (2.5082e04) 5.2153e01 (1.6406e03) 3.0267e+00 (1.1215e01) 1.8220e+00 (2.5472e03) 3.6060e01 (9.5862e03) 6.5797e01 (3.4461e03) 3.5048e+03 (2.7933e+00) 6.0457e+01 (1.2916e+00) 6.1996e+01 (1.6457e+00) 6.5350e+01 (1.8958e+00) 6.0026e+01 (1.0193e+00) 6.3764e+01 (1.8220e+00) 6.2175e+01 (1.2735e+00) 6.6262e+01 (1.4221e+00) 6.1194e+01 (1.6353e+00) 9.4239e+01 (3.0883e+00) 9.9946e+01 (2.7744e+00) 1.0148e+02 (2.6072e+00) 1.0919e+02 (2.2606e+00) 9.7131e+01 (2.9509e+00) 9.6220e+01 (2.9977e+00) 1.0456e+02 (2.4456e+00) 1.0154e+02 (2.0132e+00) 1.0443e+02 (2.9910e+00) 6.3918e+01 (6.1344e01) 6.4140e+01 (4.4810e01) 6.4090e+01 (4.1599e01) 6.3900e+01 (2.4586e01) 6.4216e+01 (2.9358e01) 6.5536e+01 (3.2336e01) 6.4099e+01 (2.5939e01) 6.4537e+01 (3.3889e01) 6.3643e+01 (2.6093e01) 7.8573e+00 (2.0971e01) 1.6127e+00 (1.8569e02) 2.4987e+00 (3.4992e02) 1.7237e+00 (1.5717e02) 7.0027e+00 (8.2602e02) 3.5406e+00 (5.6013e02) 5.0992e+00 (1.3959e01) 5.6878e+00 (4.9742e02) 3.4327e+00 (2.2911e02) 5.4412e+00 (2.8179e01) 5.7516e+00 (6.0279e01) 7.0391e+00 (4.7843e01) 7.1559e+00 (3.8251e01) 7.1893e+00 (4.6098e01) 6.8919e+00 (2.4831e01) 7.0900e+00 (4.0993e01) 7.1273e+00 (4.2536e01) 7.2184e+00 (7.7100e01) 1.8814e+00 (9.9671e02) 1.5211e+00 (2.8512e02) 1.7588e+00 (4.3342e02) 1.6785e+00 (4.7110e02) 2.0728e+00 (1.6839e02) 3.4693e+00 (7.3326e02) 1.7594e+00 (2.9685e02) 2.3856e+00 (2.8679e02) 1.5741e+00 (1.2765e02) CPU, and for > 8192, the GPU version of HypE becomes even slower than the original CPU-based implementation. These performance drops can be attributed to two main factors. First, the original implementations of NSGA-III and HypE do not fully leverage the multicore parallel processing capabilities of GPUs, leading to underutilization of GPU cores. Second, the data transfer overhead between CPU and GPU further reduces efficiency, particularly for large population sizes where the NSGA-III and HypE algorithms incur higher computational costs on GPU than on CPU. Additionally, the acceleration performance varies across tensorized algorithms. MOEA/D and HypE, due to their simpler operations and fewer conditional branches, achieve greater acceleration after tensorization compared to their original versions. Conversely, NSGA-III shows more limited acceleration, as its more complex operations involve intricate loops and branches, which are less amenable to GPU parallelization. B. Performance in Numerical Optimization To verify the precision before and after tensorization, the three proposed algorithms and their original versions are comprehensively tested on the LSMOP [63] and DTLZ [61] test suites. The detailed results for the DTLZ are provided in Section S.VII-D of Supplementary Document. 1) Experimental Settings: In this experiment, all algorithms are independently repeated 31 times on 9 LSMOP problems and 7 DTLZ problems. Each algorithm is run for 100 generations with population size of 10 000. Each problem has dimension of 5000. Performance is measured using the average inverted generational distance (IGD) [70] and average runtime over 31 runs. Wilcoxon rank-sum test is used to compare tensorized and non-tensorized algorithms. If the test shows no significant difference (i.e., > 0.05), both performance indicators are highlighted in bold. 2) Comparison Results: Table IV demonstrates that the tensorized algorithms maintain comparable precision to their nontensorized counterparts while achieving significantly faster runtimes. The average IGD between the tensorized and nontensorized algorithms consistently remain within the same order of magnitude, indicating similar levels of solution quality. In some instances, the tensorized algorithms even exhibit superior indicator performance. Additionally, the average runtime for tensorized algorithms is consistently lower than that for non-tensorized algorithms, underscoring the efficiency gains enabled by tensorization. Notably, when comparing performance within equivalent time frames, tensorized algorithms consistently achieve better indicator performance, highlighting their effectiveness in optimizing both time and solution quality metrics. However, some performance degradation is observed in certain cases. In TensorNSGA-III, batch random operations simulate the niche selection process of the original algorithm, which differs from the original methods precise operation on individual solutions. This discrepancy can lead to performance degradation, as the batch approach may not capture niche selection as effectively. Similarly, TensorMOEA/D adopts batch offspring generation and updates, which accelerate computation per generation but may result in lower performance for the same number of generations. Despite this, TensorMOEA/D consistently achieves or sometimes surpasses the performance of the original algorithm when comparing performance over IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, 11 (a) HV on MoHalfcheetah (b) EU on MoHalfcheetah (c) Final results on MoHalfcheetah (d) HV on MoHopper (e) EU on MoHopper (f) Final results on MoHopper (g) HV on MoWalker2d (h) EU on MoWalker2d (i) Final results on MoWalker2d Fig. 4: Comparative performance (HV, EU, and visualization of final results) of TensorNSGA-III, TensorMOEA/D, TensorHypE, TensorRVEA, and random search (RS) across varying problems: MoHalfcheetah (390D), MoHopper (243D), and MoWalker2d (390D). Note: Higher values for all metrics indicate better performance. equivalent time periods. As for HypE, since its method of calculating the HV is largely similar to that of the original algorithm, the primary difference lies in computation speed. C. Performance in Multiobjective Robot Control Benchmark In this experiment, we evaluate the advantages of tensorization and the effectiveness of the three proposed tensorized algorithms in solving the multiobjective rotbot control tasks using the proposed MoRobtrol benchmark test suite. Following the paradigm of EvoRL, the EMO algorithms evolve population of MLP neural networks, with each MLP serving as policy model within the simulation environment of each task. 1) Experimental Settings: In this experiment, we apply TensorNSGA-III, TensorMOEA/D, TensorHypE, TensorRVEA [57], and random search (RS) algorithms to solve 9 multiobjective robot control problems in MoRobtrol. Each algorithm is repeated 10 times with population size of 10 000. Performance is evaluated using hypervolume (HV) [13], expected utility (EU) [71], and visualization of the final nondominated solutions. These indicators are calculated based on the non-dominated solutions of each generation. Details on the TensorRVEA algorithm and the reference points for HV calculation are provided in Sections S.V and S.VII-E of the Supplementary Document, respectively. 2) Comparison Results: As shown in Fig. 4, TensorRVEA achieves the highest HV on the MoHalfcheetah, MoHopper, and MoWalker2d problems, followed closely by TensorHypE and TensorNSGA-III, which show similar performance. TensorHypE outperforms TensorNSGA-III on MoWalker2d, while TensorNSGA-III performs better on MoHalfcheetah. For EU and final results, TensorRVEA demonstrates high EU scores and strong diversity in MoHalfcheetah and MoHopper. On MoWalker2d, TensorRVEA achieves better HV, while TensorMOEA/D scores higher in EU, indicating better preference under uniform weights. TensorNSGA-III performs best on MoWalker2d for the first objective, achieving very high speeds. Although TensorMOEA/D does not perform as well as the other algorithms, it still significantly surpasses random search. As shown in Fig. 5 and Fig. 6, TensorRVEA achieves higher HV on MoPusher and MoReacher, although its EU is lower IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, 2025 12 (a) HV on MoPusher (b) EU on MoPusher (c) Final results on MoPusher (d) HV on MoHumanoid (e) EU on MoHumanoid (f) Final results on MoHumanoid (g) HV on MoHumanoid-s (h) EU on MoHumanoid-s (i) Final results on MoHumanoid-s Fig. 5: Comparative performance (HV, EU, and visualization of final results) of TensorNSGA-III, TensorMOEA/D, TensorHypE, TensorRVEA, and random search (RS) across varying problems: MoPusher (503D), MoHumanoid (4209D), and MoHumanoids (4209D). Note: Higher values for all metrics indicate better performance. than that of the other algorithms. On MoReacher, solutions from TensorNSGA-III, TensorMOEA/D, and TensorHypE dominate parts of TensorRVEAs solutions, though TensorRVEA still maintains superior HV. TensorMOEA/D performs significantly better on the MoHumanoid and MoHumanoid-s problems, highlighting its effectiveness in handling large-scale problems. Another important observation is that TensorRVEA and TensorMOEA/D can exhibit an initial rise in HV, followed by decline before stabilizing, as seen in problems like MoSwimmer in Fig. 6. This phenomenon can be attributed to the weight or reference tensors guiding the population to optimize in specific directions early in the process, leading to premature convergence and subsequent decline in HV as diversity decreases. By contrast, on the MoIDP problem, TensorRVEA achieves the highest HV, although its EU and visualization results remain comparable to those of the other algorithms. Overall, decomposition-based algorithms such as TensorRVEA and TensorMOEA/D exhibit superior performance in large-scale multiobjective robot control tasks, particularly in handling high-dimensional decision spaces and maintaining solution diversity. VII. CONCLUSION This paper limitations of the computational introduces tensorization approach to address traditional CPUbased evolutionary multiobjective optimization (EMO) algorithms, enhancing both speed and scalability. We applied this approach across three representative EMO algorithm classes: dominance-based (NSGA-III), decomposition-based (MOEA/D), and indicator-based (HypE), which demonstrated substantial performance improvements on GPU platforms. Our results confirm that tensorized algorithms can significantly accelerate computations while maintaining solution quality comparable to their original CPU-based counterparts. To demonstrate the applicability of tensorized EMO algorithms in GPU computing environments, we also developed MoRobtrol, comprehensive benchmark test suite that reformulates complex multiobjective robot control tasks from the physics simulation environments into multiobjective optimization problems. MoRobtrol underscores the potential of tensorized EMO algorithms to efficiently address the high IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, 13 (a) HV on MoSwimmer (b) EU on MoSwimmer (c) Final results on MoSwimmer (d) HV on MoIDP (e) EU on MoIDP (f) Final results on MoIDP (g) HV on MoReacher (h) EU on MoReacher (i) Final results on MoReacher Fig. 6: Comparative performance (HV, EU, and visualization of final results) of TensorNSGA-III, TensorMOEA/D, TensorHypE, TensorRVEA, and random search (RS) across varying problems: MoSwimmer (178D), MoIDP (161D), and MoReacher (226D). Note: Higher values for all metrics indicate better performance. computational demands of Embodied AI, relevance to dynamic real-world applications. illustrating their While tensorization has substantially improved algorithmic efficiency, opportunities remain to further optimize speed and memory use. Future work will focus on refining key operators such as non-dominated sorting and exploring new tensorized operators optimized for multi-GPU environments to maximize performance. Additionally, leveraging large population data to strengthen search strategies and integrating deep learning techniques may further extend the capabilities of EMO algorithms in tackling large-scale challenges. REFERENCES [1] P. R. Wiecha, A. Arbouet, C. Girard, A. Lecestre, G. Larrieu, and V. Paillard, Evolutionary multi-objective optimization of colour pixels based on dielectric nanoantennas, Nature Nanotechnology, vol. 12, no. 2, pp. 163169, 2017. [2] B. Peng, Y. Wei, Y. Qin, J. Dai, Y. Li, A. Liu, Y. Tian, L. Han, Y. Zheng, and P. Wen, Machine learning-enabled constrained multi-objective design of architected materials, Nature Communications, vol. 14, no. 1, p. 6630, 2023. [3] Y. Cui, Z. Geng, Q. Zhu, and Y. Han, Multi-objective optimization methods and application in energy saving, Energy, vol. 125, pp. 681 704, 2017. [4] X. Wang, X. Mao, and H. Khodaei, multi-objective home energy management system based on internet of things and optimization algorithms, Journal of Building Engineering, vol. 33, p. 101603, 2021. [5] Z. Fei, B. Li, S. Yang, C. Xing, H. Chen, and L. Hanzo, survey of multi-objective optimization in wireless sensor networks: Metrics, algorithms, and open problems, IEEE Communications Surveys & Tutorials, vol. 19, no. 1, pp. 550586, 2016. [6] A. Ponsich, A. L. Jaimes, and C. A. C. Coello, survey on multiobjective evolutionary algorithms for the solution of the portfolio optimization problem and other finance and economics applications, IEEE Transactions on Evolutionary Computation, vol. 17, no. 3, pp. 321344, 2012. [7] K. Deb, K. Sindhya, and J. Hakanen, Multi-objective optimization, in Decision Sciences. CRC Press, 2016, pp. 161200. [8] B. Li, J. Li, K. Tang, and X. Yao, Many-objective evolutionary algorithms: survey, ACM Computing Surveys (CSUR), vol. 48, no. 1, sep 2015. [9] K. Deb, A. Pratap, S. Agarwal, and T. Meyarivan, fast and elitist multiobjective genetic algorithm: NSGA-II, IEEE Transactions on Evolutionary Computation, vol. 6, no. 2, pp. 182197, 2002. [10] K. Deb and H. Jain, An evolutionary many-objective optimization algorithm using reference-point-based nondominated sorting approach, part i: Solving problems with box constraints, IEEE Transactions on Evolutionary Computation, vol. 18, no. 4, pp. 577601, 2014. [11] Qingfu Zhang and Hui Li, MOEA/D: multiobjective evolutionary algorithm based on decomposition, IEEE Transactions on Evolutionary Computation, vol. 11, no. 6, pp. 712731, Dec. 2007. [12] J. Bader and E. Zitzler, HypE: An algorithm for fast hypervolume-based IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, 2025 14 many-objective optimization, Evolutionary Computation, vol. 19, no. 1, pp. 4576, 03 2011. Transactions on Evolutionary Computation, vol. 25, no. 4, pp. 710723, 2021. [13] E. Zitzler and L. Thiele, Multiobjective evolutionary algorithms: comparative case study and the strength pareto approach, IEEE Transactions on Evolutionary Computation, vol. 3, no. 4, pp. 257271, 1999. [14] Y. Tian, L. Si, X. Zhang, R. Cheng, C. He, K. C. Tan, and Y. Jin, Evolutionary large-scale multi-objective optimization: survey, ACM Computing Surveys (CSUR), vol. 54, no. 8, oct 2021. [15] Y. LeCun, Y. Bengio, and G. Hinton, Deep learning, Nature, vol. 521, no. 7553, pp. 436444, 2015. [16] S. Markidis, S. W. D. Chien, E. Laure, I. B. Peng, and J. S. Vetter, NVIDIA Tensor Core programmability, performance & precision, in 2018 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW), 2018, pp. 522531. [17] C. D. Freeman, E. Frey, A. Raichuk, S. Girgin, I. Mordatch, and O. Bachem, Brax - differentiable physics engine for large [Online]. Available: http://github.com/google/brax rigid body simulation, 2021. scale [18] E. Zitzler, M. Laumanns, and L. Thiele, SPEA2: Improving the strength pareto evolutionary algorithm, Technical Report Gloriastrasse, 2001. [19] S. Yang, M. Li, X. Liu, and J. Zheng, grid-based evolutionary algorithm for many-objective optimization, IEEE Transactions on Evolutionary Computation, vol. 17, no. 5, pp. 721736, 2013. [20] M. Li, S. Yang, and X. Liu, Bi-goal evolution for many-objective optimization problems, Artificial Intelligence, vol. 228, pp. 4565, 2015. [21] Y. Xiang, Y. Zhou, M. Li, and Z. Chen, vector angle-based evolutionary algorithm for unconstrained many-objective optimization, IEEE Transactions on Evolutionary Computation, vol. 21, no. 1, pp. 131152, 2016. [22] C. He, Y. Tian, Y. Jin, X. Zhang, and L. Pan, radial space division based evolutionary algorithm for many-objective optimization, Applied Soft Computing, vol. 61, pp. 603621, 2017. [23] Y. Tian, R. Cheng, X. Zhang, Y. Su, and Y. Jin, strengthened dominance relation considering convergence and diversity for evolutionary many-objective optimization, IEEE Transactions on Evolutionary Computation, vol. 23, no. 2, pp. 331345, 2018. [24] Y. Tian, C. He, R. Cheng, and X. Zhang, multistage evolutionary algorithm for better diversity preservation in multiobjective optimization, IEEE Transactions on Systems, Man, and Cybernetics: Systems, vol. 51, no. 9, pp. 58805894, 2019. [25] Y. Liu, N. Zhu, and M. Li, Solving many-objective optimization problems by Pareto-based evolutionary algorithm with preprocessing and penalty mechanism, IEEE Transactions on Cybernetics, vol. 51, no. 11, pp. 55855594, 2021. [26] K. Li, survey of multi-objective evolutionary algorithm based on decomposition: Past and future, IEEE Transactions on Evolutionary Computation, pp. 11, 2024. [27] Q. Zhang, W. Liu, and H. Li, The performance of new version of MOEA/D on CEC09 unconstrained MOP test instances, in 2009 IEEE Congress on Evolutionary Computation, 2009, pp. 203208. [28] X. Cai, Y. Li, Z. Fan, and Q. Zhang, An external archive guided multiobjective evolutionary algorithm based on decomposition for combinatorial optimization, IEEE Transactions on Evolutionary Computation, vol. 19, no. 4, pp. 508523, 2015. [29] Q. Zhao, Y. Guo, X. Yao, and D. Gong, Decomposition-based multiobjective optimization algorithms with adaptively adjusting weight vectors and neighborhoods, IEEE Transactions on Evolutionary Computation, vol. 27, no. 5, pp. 14851497, 2023. [30] Q. Wang, Q. Gu, L. Chen, Y. Guo, and N. Xiong, MOEA/D with global and local cooperative optimization for complicated bi-objective optimization problems, Applied Soft Computing, vol. 137, p. 110162, 2023. [31] H.-L. Liu, F. Gu, and Q. Zhang, Decomposition of multiobjective optimization problem into number of simple multiobjective subproblems, IEEE Transactions on Evolutionary Computation, vol. 18, no. 3, pp. 450455, 2014. [32] R. Cheng, Y. Jin, M. Olhofer, and B. Sendhoff, reference vector guided evolutionary algorithm for many-objective optimization, IEEE Transactions on Evolutionary Computation, vol. 20, no. 5, pp. 773791, 2016. [33] Y. Yuan, H. Xu, B. Wang, and X. Yao, new dominance relationbased evolutionary algorithm for many-objective optimization, IEEE Transactions on Evolutionary Computation, vol. 20, no. 1, pp. 1637, 2016. [34] J. Zhou, L. Gao, and X. Li, Ensemble of dynamic resource allocation strategies for decomposition-based multiobjective optimization, IEEE [35] J. G. Falcon-Cardona and C. A. C. Coello, Indicator-based multiobjective evolutionary algorithms: comprehensive survey, ACM Computing Surveys (CSUR), vol. 53, no. 2, Mar. 2020. [36] E. Zitzler and S. Kunzli, Indicator-based selection in multiobjective search, in Parallel Problem Solving from Nature - PPSN VIII. Berlin, Heidelberg: Springer Berlin Heidelberg, 2004, pp. 832842. [37] M. Emmerich, N. Beume, and B. Naujoks, An EMO algorithm using the hypervolume measure as selection criterion, in Evolutionary MultiCriterion Optimization. Berlin, Heidelberg: Springer Berlin Heidelberg, 2005, pp. 6276. [38] M. Li, S. Yang, and X. Liu, Pareto or non-Pareto: Bi-criterion evolution in multiobjective optimization, IEEE Transactions on Evolutionary Computation, vol. 20, no. 5, pp. 645665, 2016. [39] B. Li, K. Tang, J. Li, and X. Yao, Stochastic ranking algorithm for many-objective optimization based on multiple indicators, IEEE Transactions on Evolutionary Computation, vol. 20, no. 6, pp. 924938, 2016. [40] R. Hernandez Gomez and C. A. Coello Coello, Improved metaheuristic based on the R2 indicator for many-objective optimization, in Proceedings of the 2015 Annual Conference on Genetic and Evolutionary Computation, ser. GECCO 15. ACM, 2015, p. 679686. [41] Y. Tian, R. Cheng, X. Zhang, F. Cheng, and Y. Jin, An indicator-based multiobjective evolutionary algorithm with reference point adaptation for better versatility, IEEE Transactions on Evolutionary Computation, vol. 22, no. 4, pp. 609622, 2018. [42] Y. Sun, G. G. Yen, and Z. Yi, IGD indicator-based evolutionary algorithm for many-objective optimization problems, IEEE Transactions on Evolutionary Computation, vol. 23, no. 2, pp. 173187, 2019. [43] K. Shang and H. Ishibuchi, new hypervolume-based evolutionary algorithm for many-objective optimization, IEEE Transactions on Evolutionary Computation, vol. 24, no. 5, pp. 839852, 2020. [44] L. M. Pang, H. Ishibuchi, L. He, K. Shang, and L. Chen, Hypervolumebased cooperative coevolution with two reference points for multiobjective optimization, IEEE Transactions on Evolutionary Computation, vol. 28, no. 4, pp. 10541068, 2024. [45] Z. Wang, K. Lin, G. Li, and W. Gao, Multi-objective optimization problem with hardly dominated boundaries: Benchmark, analysis, and indicator-based algorithm, IEEE Transactions on Evolutionary Computation, pp. 11, 2024. [46] M. L. Wong, Parallel multi-objective evolutionary algorithms on graphics processing units, in Proceedings of the 11th Annual Conference Companion on Genetic and Evolutionary Computation Conference: Late Breaking Papers, ser. GECCO 09. ACM, 2009, pp. 25152522. [47] D. Sharma and P. Collet, GPGPU-compatible archive based stochastic ranking evolutionary algorithm (G-ASREA) for multi-objective optimization, in Parallel Problem Solving from Nature, PPSN XI. Berlin, Heidelberg: Springer Berlin Heidelberg, 2010, pp. 111120. [48] B. Arca, T. Ghisu, and G. A. Trunfio, GPU-accelerated multi-objective optimization of fuel treatments for mitigating wildfire hazard, Journal of Computational Science, vol. 11, pp. 258268, 2015. [49] A. Aguilar-Rivera, GPU fully vectorized approach to accelerate performance of NSGA-2 based on stochastic non-domination sorting and grid-crowding, Applied Soft Computing, vol. 88, p. 106047, 2020. [50] M. Z. De Souza and A. T. R. Pozo, GPU implementation of MOEA/D-ACO for the multiobjective traveling salesman problem, in 2014 Brazilian conference on intelligent systems. IEEE, 2014, pp. 324329. [51] E. M. Lopez, L. M. Antonio, and C. A. Coello Coello, GPU-based algorithm for faster hypervolume contribution computation, in Evolutionary Multi-Criterion Optimization. Cham: Springer International Publishing, 2015, pp. 8094. [52] M. M. Hussain and N. Fujimoto, GPU-based parallel multi-objective particle swarm optimization for large swarms and high dimensional problems, Parallel Computing, vol. 92, p. 102589, 2020. [53] J. R. Bradbury, P. Hawkins, M. Frostig, C. Leary, D. Maclaurin, G. Necula, A. Paszke, S. Wanderman-Milne, of Python+NumPy transformations Available: http://github.com/google/jax Zhang, programs, and Q. JAX: J. Johnson, J. VanderPlas, composable [Online]. 2018. [54] Y. Tang, Y. Tian, and D. Ha, EvoJAX: Hardware-accelerated neuroevolution, in Proceedings of the Genetic and Evolutionary Computation Conference Companion, ser. GECCO 22. ACM, 2022, p. 308311. [55] R. T. Lange, evosax: JAX-based evolution strategies, in Proceedings of the Companion Conference on Genetic and Evolutionary Computation, ser. GECCO 23 Companion. ACM, 2023, p. 659662. IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, 2025 15 [56] B. Huang, R. Cheng, Z. Li, Y. Jin, and K. C. Tan, EvoX: distributed GPU-accelerated framework for scalable evolutionary computation, IEEE Transactions on Evolutionary Computation, 2024. [57] Z. Liang, T. Jiang, K. Sun, and R. Cheng, GPU-accelerated evolutionary multiobjective optimization using tensorized RVEA, in Proceedings of the Genetic and Evolutionary Computation Conference, ser. GECCO 24, 2024, pp. 566575. [58] D. Kirk, NVIDIA CUDA software and GPU parallel computing architecture, ser. ISMM 07. ACM, 2007, p. 103104. [59] T. G. Kolda and B. W. Bader, Tensor decompositions and applications, SIAM Review, vol. 51, no. 3, pp. 455500, 2009. [60] E. Zitzler, K. Deb, and L. Thiele, Comparison of multiobjective evolutionary algorithms: Empirical results, Evolutionary Computation, vol. 8, no. 2, pp. 173195, 2000. [61] K. Deb, L. Thiele, M. Laumanns, and E. Zitzler, Scalable test problems for evolutionary multiobjective optimization. London: Springer London, 2005, pp. 105145. [62] S. Huband, P. Hingston, L. Barone, and L. While, review of multiobjective test problems and scalable test problem toolkit, IEEE Transactions on Evolutionary Computation, vol. 10, no. 5, pp. 477506, 2006. [63] R. Cheng, Y. Jin, M. Olhofer, and B. Sendhoff, Test problems for largescale multiobjective and many-objective optimization, IEEE Transactions on Cybernetics, vol. 47, no. 12, pp. 41084121, 2017. [64] R. Cheng, M. Li, Y. Tian, X. Zhang, S. Yang, Y. Jin, and X. Yao, benchmark test suite for evolutionary many-objective optimization, Complex & Intelligent Systems, vol. 3, no. 1, pp. 6781, Mar. 2017. [65] M. Savva, A. Kadian, O. Maksymets, Y. Zhao, E. Wijmans, B. Jain, J. Straub, J. Liu, V. Koltun, J. Malik, D. Parikh, and D. Batra, Habitat: platform for embodied AI research, in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2019. [66] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba, OpenAI Gym, arXiv preprint arXiv:1606.01540, 2016. [67] F. Felten, L. N. Alegre, A. Nowe, A. L. C. Bazzan, E. G. Talbi, G. Danoy, and B. C. da. Silva, toolkit for reliable benchmarking and research in multi-objective reinforcement learning, in Proceedings of the 37th Conference on Neural Information Processing Systems (NeurIPS 2023), 2023. [68] H. Bai, R. Cheng, and Y. Jin, Evolutionary reinforcement learning: survey, Intelligent Computing, vol. 2, p. 0025, 2023. [69] N. E. Toklu, T. Atkinson, V. Micka, P. Liskowski, and R. K. Srivastava, EvoTorch: Scalable evolutionary computation in Python, arXiv preprint, 2023, https://arxiv.org/abs/2302.12600. [70] C. A. C. Coello and N. C. Cortes, Solving multiobjective optimization problems using an artificial immune system, Genetic Programming and Evolvable Machines, vol. 6, pp. 163190, 2005. [71] L. M. Zintgraf, T. V. Kanters, D. M. Roijers, F. Oliehoek, and P. Beau, Quality assessment of MORL algorithms: utility-based approach, in Benelearn 2015: Proceedings of the 24th Annual Machine Learning Conference of Belgium and the Netherlands, 2015. [72] Y. Tian, R. Cheng, X. Zhang, and Y. Jin, PlatEMO: MATLAB platform for evolutionary multi-objective optimization, IEEE Computational Intelligence Magazine, vol. 12, no. 4, pp. 7387, 2017. IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, 2025 Bridging Evolutionary Multiobjective Optimization and GPU Acceleration via Tensorization (Supplementary Document) S.I. GENETIC OPERATORS IN EMO ALGORITHMS This section provides comprehensive overview of the genetic operators used in evolutionary multiobjective optimization (EMO) algorithms, including mating selection, original crossover and mutation, and the tensorization of these crossover and mutation operations. A. Mating Selection Mating selection is critical component of EMO algorithms, whereby individuals are selected from the existing population to act as parents for the offspring generation. Classical methods, including roulette wheel selection, tournament selection, and random selection, are commonly utilized in this context. These methods, which leverage probability and statistical principles to varying degrees for solution selection, are wellsuited for tensorization. B. Original Crossover and Mutation In EMO algorithms, genetic operators play pivotal role in both the exploration and exploitation of the search space. Among the most commonly used operators in genetic algorithms tailored for real-coded variables are simulated binary crossover (SBX) and polynomial mutation. These operators are specifically engineered to maintain balance between exploration and exploitation, proving particularly effective for problems characterized by continuous decision variables. The conventional implementation is as follows. 1) Simulated Binary Crossover: SBX operates under the principle of creating offspring that are close to the parent solutions, analogous to the single-point crossover used in binary-coded genetic algorithms. Given two parent solutions x1 = (x1 2), SBX generates two offspring c1 and c2 using the following equations: 1) and x2 = (x1 1, . . . , xd 2, . . . , xd 2, 1, x2 1 = 0.5 (cid:2)(1 + β)xi ci 2 = 0.5 (cid:2)(1 β)xi ci 1 + (1 β)xi 2 1 + (1 + β)xi 2 (cid:3) , (cid:3) , (S.1) (S.2) where is the index of each decision variable, β is the spread factor, calculated based on probability distribution and user-defined parameter ηc, typically known as the distribution index. The spread factor β is calculated as follows: 1 ηc+1 , β = (2µ) (cid:16) 1 2(1µ) (cid:17) 1 ηc+1 µ 0.5 , µ > 0. , (S.3) where µ is random number with values between 0 and 1. 2) Polynomial Mutation: Polynomial Mutation is designed to introduce minor perturbations in the offspring, promoting diversity in the population. For given parent solution x, the mutated solution is generated as follows: yi = xi + δ(ui li), (S.4) where ui and li are the upper and lower bounds of the i-the decision variable, respectively, and δ is the mutation step size. The value of δ is determined by: (cid:40) δ = [2µ + (1 2µ)(1 δ1)η] 1 [2 2µ + (2µ 1)(1 δ2)η] 1 η 1, µ 0. 1 η , µ > 0.5 , (S.5) where η = ηm + 1, with ηm being the distribution index for mutation. The term µ represents uniformly distributed random number within the range [0,1]. Additionally, δ1 = xili uili and δ2 = uixi uili define the normalized differences relevant to the mutation process. C. Tensorization of Crossover and Mutation The tensorized methods implemented in TensorRVEA [57] and PlatEMO [72] facilitate the concurrent processing of genetic information across population, significantly enhancing the speed and scalability of genetic operations within EMO frameworks. This demonstrates the efficacy of tensorization in contemporary computational environments. The tensorized implementation of crossover and mutation operators is as follows. 1) Simulated Binary Crossover: In tensorized form, the crossover operation can be performed in parallel across the population. For instance, in the SBX, the tensorized population Rnd is divided into two parent tensors, X1 and X2, (cid:5) d. The tensorized formulations for each with dimension (cid:4) SBX are as follows: 2 Xc = (cid:21) (cid:20)[(1 + B) X1 + (1 B) X2] /2 [(1 B) X1 + (1 + B) X2] /2 , where is tensor of spread factors computed as: =(2M ) ηc+1 H(0.5 )+ 1 (1/(2 2M )) 1 , ηc+1 (1 H(0.5 )) (S.6) (S.7) where ηc is the distribution parameter of SBX, is tensor with the same dimension as X1 and X2, containing uniformly distributed random numbers in [0,1]. The operator denotes IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, 2025 17 the Hadamard product, which performs element-wise multiplication. The H() function is the Heaviside step function, returning 1 for non-negative inputs and 0 otherwise. 2) Polynomial Mutation: Mutation operations introduce small random changes to solutions. In tensorized form, mutation can be performed in parallel. For polynomial mutation: Algorithm S.1 Original Environmental Selection in NSGA-III Input: Combined population Pt with size 2n, population size n, set of reference points Z. Output: Next generation population Pt+1. 1: Perform Non-dominated Sorting on Pt to identify the fronts F1, F2, . . . ; Xm = Xc + (U L), (S.8) where the tensors and represent the upper and lower bounds of the population, respectively. Xc is the population after crossover. is size tensor of mutation steps, computed as: 1 = [2M + (1 2M ) (1 1)η] 1 η , (S.9) 2 = 1 [2 2M + (2M 1) (1 2)η] 1 η , (S.10) = 1 H(0.5 ) + 2 (1 H(0.5 )), (S.11) where η = ηm + 1, and ηm is the distribution index, consists of uniformly distributed random numbers within [0,1], and is the Heaviside step function. Additionally, 1 = (Xc L)/(U L) and 2 = (U Xc)/(U L) are transformation tensors for normalizing crossover transformations, similar to mutation processes. S.II. TENSORIZATION OF NSGA-III This section compares the original NSGA-III algorithm with its tensorized version, focusing on the environmental selection process. The pseudocode for both implementations is provided to demonstrate the key differences and the efficiency gains achieved through tensorization methodology. A. Original Environmental Selection In the original NSGA-III algorithm, environmental selection consists of several key steps. The process begins with non-dominated sorting, which classifies the population into different Pareto fronts by comparing individuals. After sorting, the population undergoes normalization step to scale the objective values. Following this, an association operation assigns individuals to predefined reference points. Finally, niche-preservation method is applied to maintain diversity among the selected individuals by filling underrepresented niches. The non-dominated sorting and subsequent operations, particularly the iterative loops, can become computationally expensive as the population size increases. The pseudocode for the original environmental selection process, including non-dominated sorting, normalization, association, and nichepreservation, is provided in Algorithm S.1 and Algorithm S.2. B. Tensorized Environmental Selection The tensorized version of NSGA-III leverages GPU parallelism to accelerate the entire environmental selection process, which includes non-dominated sorting, normalization, association, niche count calculation, and niche selection. By transforming these operations into tensor form, batch processing can be performed across the population, significantly reducing computational complexity and execution time. 2: Set Pt+1 = ; 3: 1; 4: while Pt+1 + Fi do Pt+1 Pt+1 Fi; 5: + 1; 6: 7: end while 8: St Pt+1 Fi; 9: if Pt+1 < then 10: 11: 12: Normalize the objective values of solutions in St; Calculate the distance between St and Z; Associate all solutions in St to reference point in based on the minimum distance; Compute the niche counts for each reference point in Pt+1 and St; while Pt+1 < do Identify the least crowded reference point Z; if There is no solution associated with in Fi then {j}; else if There is no solution associated with in Pt+1 then Select the solution Fi with the minimum distance to j; else Randomly select solution Fi associated with j; that is end if Pt+1 Pt+1 {a}; Update the niche count for in both Pt+1 and St; 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: end if end while 26: 27: 28: end if 29: return Pt+1 In the tensorized process, non-dominated sorting is conducted in parallel, efficiently classifying individuals into Pareto fronts. This is followed by normalization step to scale objective values, an association step to assign individuals to reference points, niche count calculation to track the distribution of individuals across niches, and finally, niche selection to maintain diversity. All these operations are performed in parallel, taking full advantage of the tensorization methodology. The pseudocode for the tensorized environmental selection, including all these steps, is provided in Algorithm S.4. S.III. TENSORIZATION OF MOEA/D This section contrasts the original MOEA/D algorithm with its tensorized version, highlighting how tensorization methodology enhances computational efficiency. The provided pseudocode offers insights into the original and tensorized implementations of environmental selection. IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, 2025 18 Algorithm S.2 Non-dominated Sorting in Original NSGA-III Input: Population of size n, with each individual pi having objective values (pi) Output: Set of non-dominated fronts F1, F2, . . . 1: Initialize an empty list of fronts: = ; 2: Initialize an empty list of domination counts: ni = 0 for all pi ; 3: Initialize an empty list of dominated sets: Si = for all pi ; 4: for pi in do 5: 6: for pj in do if = then 7: 8: 9: 10: 11: 12: if pi dominates pj then Si Si {pj}; else if pj dominates pi then ni ni + 1; end if Fi Fi {pi}; end if end for if ni = 0 then 13: 14: 15: end if 16: 17: end for 18: 1; 19: while Fk = do Fk+1 = ; 20: for pi in Fk do 21: 22: 23: for pj in Si do nj nj 1; if nj = 0 then end if end for 24: 25: 26: 27: 28: 29: 30: end while end for + 1; Fk+1 Fk+1 {pj}; Algorithm S.3 Tensorized Non-dominated Sorting Input: Objective tensor R2nm and population size n. Output: Nondomination rank tensor and the last rank N. 1: [Fi Fj]i,j, 2: (cid:80)2n 3: 02n1; 4: 0; 5: 1c=0; 6: while any(p) do 7: i, = 1, 2, . . . , 2n; = 1, 2, . . . , 2n; j=1 Dij, = 1, 2, . . . , 2n; H(p) + H(1 p) r; dj (cid:80)2n i=1(pi Dij), p; + 1; 1c=0; 8: 9: 10: 11: 12: end while 13: sort(r)[n]; Algorithm S.4 Environmental Selection in TensorNSGA-III Input: Shuffled solution tensor and corresponding objective tensor , reference tensors with nr vectors, and population size n. Output: Next solution tensor Xnext and corresponding objective tensor Fnext. ; i=1 (cid:17)2 1 = 1, . . . , nr; = 1, . . . , nr; (cid:16) F // refer to Algorithm S.5 1: r, GPU-accelerated Non-dominated Sorting(F , n); 2: + NaN H(r l); 3: Normalize(F ); 4: // Association: (cid:114) 5: 6: π arg minj(D); 7: minj(D); 8: // Niche count calculation of reference tensor: 9: ρj (cid:80)2n i=1 H(l ri) 1πi=j, 10: ρl,j (cid:80)2n 1ri=l 1πi=j, 11: ns (cid:80) ρ; 12: // Niche selection: 13: ρ 1ρl=0; 14: π π + 1r=l; 15: [1, 2, . . . , nr]; 16: ρs 1ρ=0 + 1ρ=0; 17: Mij 1ρs,i=πj ; 18: + (1 ); 19: arg minj(D); 20: 1ρselected= + (1 1ρselected=); 21: Update(q); 22: r[q] 1; 23: ns ns + (cid:80) 1ρ=0; 24: ndif ns; 25: Update-Rank(r, q, ndif, l); refer // to Algorithm S.6 26: inext sort(H(l r) + (1 H(l r)))[: n], = [0, 1, . . . , 1]; 27: Xnext, Fnext X[inext], [inext]; Algorithm S.5 Normalization in TensorNSGA-III Input: Objective Tensor ; Output: Normalized objective tensor ; argmin(maxj(F /w)); return 1: mini(F ); 2: z; 3: Im; 4: Function (w) 5: 6: 7: vmap(f )(W ); 8: [e]; 9: if rank(E) = then 10: 11: else 12: 13: end if 14: /a; maxi(F ); compute intercepts based on E; IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, 19 Algorithm S.6 Update-Rank in TensorNSGA-III Input: Rank tensor r, selected indices ssel, difference ndif, last rank l. Output: Updated rank tensor r. 1: {i ri = l}; 2: sort(s); 3: H(ndif i) + ssel[0] H(i ndif), = [1, 2, . . . , len(s)]; 4: µ 1ssel=ssel[0] + ssel (1 1ssel=ssel[0]); 5: idrop sort(µ)[0]; 6: µadj µ H(ndif i) + idrop H(i + ndif), = [1, 2, . . . , len(µ)]; 7: radd, rcut r; 8: radd[s] 1; 9: rcut[µadj] l; 10: H(ndif) radd + H(ndif) rcut; A. Original MOEA/D MOEA/D decomposes MOP into several single-objective subproblems. Each subproblem is optimized individually, and the solutions are combined to approximate the Pareto front. The original MOEA/D uses weight vectors to guide the search process, and solutions are iteratively updated based on their performance relative to these vectors. The decomposition process in the original MOEA/D involves calculating the aggregation function for each subproblem and updating the solutions sequentially. This step is computationally intensive, especially as the number of subproblems and the population size increase. The pseudocode for the original MOEA/D is provided in Algorithm S.7. B. Tensorized MOEA/D The implementation of TensorMOEA/D retains the core principles of the original algorithm but leverages the parallel processing capabilities of GPUs. The pseudocode for the environmental selection of TensorMOEA/D is shown in Algorithm S.8. S.IV. TENSORIZATION OF HYPE In this section, we discuss the original Monte Carlo hypervolume (HV) estimation method employed in HypE and present its tensorized adaptation, which enhances computational efficiency by utilizing GPU parallelization. The tensorized version, TensorHypE, is specifically designed to handle larger populations and more complex optimization tasks by performing HV contribution calculations in parallel, significantly speeding up the environmental selection process. Algorithm S.7 Original MOEA/D Algorithm Input: The maximal number of generations tmax; weight vectors λ1, . . . , λn; the number of the weight vectors in the neighborhood of each weight vector; maximum number of iterations tmax; Output: EP (efficient set of solutions); 1: EP ; 2: for each weight vector λi do 3: Compute Euclidean distances to all other weight vectors; Determine the closest weight vectors B(i) = {i1, . . . , iT }; 4: 5: end for 6: Generate initial population x1, . . . , xn: 7: for = 1 to do fi (xi); 8: 9: end for 10: Initialize = (z1, . . . , zm); 11: for = 1 to tmax do for = 1 to do 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: Randomly select two indices p, from B(i); Generate new solution from xp and xq using genetic operators; Apply problem-specific improvement heuristic on to produce y; for = 1 to do if fj(y) < zj then zj fj(y); end if end for for each B(i) do if g(yλj, z) g(xjλj, z) then xj y; fj (y); end if end for Remove from EP all vectors dominated by (y); if no vectors in EP dominate (y) then Add (y) to EP; 29: 30: 31: 32: end for end if end for sequentially sampling points within defined HV region and updating fitness values iteratively. Algorithm S.9 illustrates the pseudocode for the original Monte Carlo HV estimation method in HypE. This method can be computationally expensive when applied to large populations and high-dimensional objective spaces, as it relies heavily on sequential processing. A. Original Environmental Selection The original HypE uses Monte Carlo sampling to estimate the HV contributions of solutions in population. The algorithm ranks solutions based on their contributions to the overall HV, favoring those that contribute more to the Pareto fronts volume. The original implementation of HypE involves B. Tensorized Environmental Selection While preserving the core principles of HypE, crucial steps such as non-dominated sorting, HV estimation, and selection are accelerated using GPUs. Algorithm S.10 illustrates the tensorized environmental selection process, and Algorithm S. IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, 2025 20 Algorithm S.8 Environmental Selection of TensorMOEA/D Input: Solution tensor X, Objective tensor F1, Offspring tensor O, the objective of offspring F2, the population size n, the ideal points z, the weights , the neighbors indices Inb, and the PBI function fPBI; Output: Next solution tensor Xnext and next objective tensor Fnext; gold fPBI(F1[inb], w[inb], zmin); gnew fPBI(f2, w[inb], zmin); 1: zmin mini(z F2); 2: Isub [i N, 0 < n]; 3: 0n; 4: Function fop1(inb, f2) 5: 6: 7: [inb] H(gold gnew); 8: 9: 10: // Comparison and Population Update: 11: Inew vmap(fop1)(Inb, F2); 12: Function fop2(inew, x, f1, w) 13: 14: 15: Isub (1) + (1 ) Isub; return Isub 1inew=1 F2 + (1 1inew=1) f1; 1inew=1 + (1 1inew=1) x; argmin(fPBI(f , w, zmin)); return x[i], [i] 16: 17: // Elite Selection: 18: Xnext, Fnext vmap(fop2)(I 19: // Update the ideal point: 20: zmin; new, X, F1, w); provides details on the GPU-accelerated HV estimation via Monte Carlo sampling. S.V. INTRODUCTION TO TENSORRVEA TensorRVEA [57] is tensorized extension of the original RVEA [32] designed to leverage the computational advantages of modern hardware, such as GPUs. By representing key data structures and operations in tensor form, TensorRVEA efficiently processes large populations and high-dimensional objectives, making it particularly suitable for solving complex MOPs. The key contribution in TensorRVEA lies in its ability to parallelize the selection process using tensor operations. The selection operation, crucial component of the algorithm, is responsible for maintaining diverse set of solutions that are well-distributed along the Pareto front. This operation uses angular distances between objective tensors and reference tensors to guide the search towards unexplored regions of the objective space. The TensorRVEA environmental selection operation illustrated in Algorithm S.12 demonstrates the efficiency of tensorization. By performing operations on entire populations simultaneously, TensorRVEA significantly accelerates the selection process compared to its traditional counterpart. This enhancement makes it particularly effective for large-scale multiobjective optimization tasks, where maintaining diverse set of solutions along the Pareto front is critical. Algorithm S.9 Original Monte Carlo Hypervolume Estimation Input: Population P, reference set R, fitness parameter N, number of sampling points li minaP fi(a); ui max(r1,...,rm)R ri; Output: Estimated fitness values 1: // determine sampling box 2: for 1 to do 3: 4: 5: end for 6: [l1, u1] [lm, um]; 7: (cid:81)m i=1 max{0, (ui li)}; 8: // reset fitness assignment 9: (cid:83) 10: // sampling 11: for 1 to do 12: 13: 14: aP{(a, 0)}; aP,f (a)s{f (a)}; UP (cid:83) if UP then choose uniformly at random; if : then 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: l=1 kl Pl ; // hit in relevant partition α (cid:81)UP1 // update HV estimates ; for all (a, v) do if (a) then F {(a, + α UP )}; else F {(a, v)}; end if end for F; 26: 27: 28: end if 29: 30: end for end if Algorithm S.10 Environmental Selection of TensorHypE Input: Merged solution tensor X, merged objective tensor , reference point vref, the population size n, and the number of sample points s; Output: Next solution tensor Xnext and next objective tensor Fnext; i=1 1: r, GPU-based Non-dominated Sorting(F ); 2: rmask H(l r); 3: (cid:80)len(rmask) rmask,i n; 4: vhv Hypervolume Estimation(F , vref, k, s); 5: rmask vhv + (1 rmask) (); 6: lexsort(r, d)[: n]; 7: Xnext X[I]; 8: Fnext [I]; IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, 21 Algorithm S.11 Hypervolume Estimation via Monte Carlo Sampling Input: Objective tensor , reference point vref, the parameter and the number of sample points s; Output: Hypervolume values vhv; i=1 λi/j, = 1, 2, . . . , k; j=1 H(Sij )=m, where = 1, . . . , s; 1: n1, shape(F ); 2: [i N, 1 < n1]; 3: λ [1, (k l)/(n1 l)]; 4: αj (cid:81)j 5: fl mini(F ); 6: fu vref; 7: Uniform sampling from [fl, fu] in dimension sm; 8: vds 01s; 9: Function fpds(f ) tpds 1(cid:80)m 10: return tpds; 11: 12: Function fhv(tpds) 13: 14: 15: 16: // Compute point dominance scores: 17: Tpds vmap(fpds)(F ); 18: Ttemp Tpds (1n1 vds + 1) + (1 Tpds) (1n1 vds); 19: vds maximum((cid:80)n1 20: // Compute hypervolume values: 21: vhv vmap(fhv)(Tpds); 22: vhv vhv (cid:81)m vtemp tpds vds (1 tpds); vhv,i (cid:80)s i=1 return vhv,i; (cid:0)α[vtemp] 1vtemp=1 i=1 Ttemp,i 1, 0); i=1(vref,i fl,i)/s; i; (cid:1) Algorithm S.12 Environmental Selection of TensorRVEA Input: Solution tensor with individual, objective tensor , reference tensors with vectors, maximum number of generations tmax, current generation t, and rate of change of penalty α; Output: Elite solution tensor Xelite; 1: zmin mini(F ); 2: zmin; 3: Θ arccos (cid:0)F /(F )(cid:1) ; 4: repeat(argminj(Θ), r); 5: Tpart repeat((cid:2)0, 1, . . . , 1(cid:3) 6: repeat((cid:2)0, 1, . . . , 1(cid:3) , n); 7: Tpart (1 sgn(A I)) Tpart sgn(A I); arccos (cid:0)V /(V )(cid:1) (cid:17) 8: Γ argminj ; 9: Function fAPD(tpart, γ, θ) (cid:16) , r); (cid:17)α (cid:16) (cid:17) θ[tpart] γ [tpart]; (cid:16) tmax 10: 1 + tAPD = return tAPD; 11: 12: TAPD vmap(fAPD)(Tpart, Γ, Θ); 13: Replace elements in TAPD with inf where Tpart = 1; 14: Inext argmini(TAPD); 15: Xelite X[Inext]; S.VI. MULTIOBJECTIVE ROBOT CONTROL BENCHMARK In this section, we introduce the multiobjective robot control problems (MoRobtrol) designed for this study. These problems are specifically crafted to evaluate the performance of the proposed tensorized EMO algorithms in realistic and challenging scenarios. The MoRobtrol consists of 9 distinct multiobjective problems: MoHalfcheetah, MoHopper, MoSwimmer, MoInvertedDoublePendulum (MoIDP), MoWalker2d, MoPusher, MoReacher, MoHumanoid, and MoHumanoidstandup (MoHumanoid-s). Each problem is variant task, reformulated to of classic single-objective control include multiple objectives that often conflict with one another, increasing both complexity and relevance to real-world applications. The specific mathematical definitions of these problems are detailed below, offering precise description of the objectives and constraints associated with each task. (a) MoHalfcheetah (b) MoHopper (c) MoSwimmer (d) MoIDP (e) MoWalker2d (f) MoPusher (g) MoReacher (h) MoHumanoid (i) MoHumanoid-s Fig. S.1: Visual illustration of the robot control tasks in the proposed MoRobtrol benchmark. MoHalfcheetah: The observation space is represented as R17, indicating 17-dimensional vector for each observation. the action space is defined as R6, Similarly, indicating 6-dimensional vector for each action. Each episode is constrained to maximum of 1000 time steps. The first objective, termed the forward reward, is defined as: fv = w1 vx, (S.12) where vx denotes the velocity in the x-direction, and w1 is the weight associated with this velocity component. The second objective, known as the control cost, is expressed as: fc = w2 (cid:88) a2 , (S.13) IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, 2025 22 where ai represents the action taken by the i-th actuator, and w2 is the weight applied to the control cost. MoHopper: The observation space is represented as R11, indicating that each observation is an 11-dimensional vector. The action space is defined as R3, meaning that each action is 3-dimensional vector. Each episode is constrained to maximum of 1000 time steps. The first objective, termed the forward reward, is given by: fv = w1 vx + C, (S.14) where vx denotes the velocity in the x-direction, w1 is the weight associated with this velocity component, and = 1 is constant survival reward indicating that the agent remains active. The second objective, referred to as height, is expressed as: fh = 10 (hcurr hinit) + C, (S.15) where hcurr represents the current height of the hopper, and hinit denotes the initial height. The third objective, known as the control cost, is defined as: fc = a2 + C, (cid:88) (S.16) where ai represents the action taken by the i-th actuator. MoSwimmer: The observation space is represented as R8, indicating that each observation is an 8-dimensional vector. The action space is defined as R2, meaning that each action is 2-dimensional vector. Each episode is constrained to maximum of 1000 time steps. The first objective, referred to as the forward reward, is defined as: fv = w1 vx, (S.17) where vx denotes the velocity in the x-direction, and w1 is the weight associated with this velocity component. The second objective, known as the control cost, is expressed as: fc = w2 (cid:88) a2 , (S.18) where ai represents the action taken by the i-th actuator, and w2 is the weight associated with the control cost. MoInvertedDoublePendulum (MoIDP): The observation space is represented as R11, indicating that each observation is an 11-dimensional vector. The action space is defined as R1, meaning that each action is scalar. Each episode is constrained to maximum of 1000 time steps. The first objective, referred to as the distance penalty, is defined as: MoWalker2d: The observation space is represented as R17, indicating that each observation is 17-dimensional vector. The action space is defined as R6, meaning that each action is 6-dimensional vector. Each episode is limited to maximum of 1000 steps. The first objective, referred to as the forward reward, is defined as: fv = w1 vx + C, (S.21) where vx denotes the velocity in the x-direction, w1 is the weight associated with this velocity component, and = 1 represents constant survival reward, indicating that the agent remains active. The second objective, known as the control cost, is expressed as: fc = w2 a2 + C, (cid:88) (S.22) where ai represents the action taken by the i-th actuator, and w2 is the weight associated with the control cost. MoPusher: The observation space is represented as R23, indicating that each observation is 23-dimensional vector. The action space is defined as R7, meaning that each action is 7-dimensional vector. Each episode is constrained to maximum of 1000 time steps. The first objective, referred to as the near reward, is defined as: (cid:113) fn = (xfin xobj)2 + (yfin yobj)2, (S.23) where xfin and yfin denote the coordinates of the fingertip, while xobj and yobj denote the coordinates of the object. The second objective, known as the distance reward, is expressed as: fd = (xobj xtar)2 + (yobj ytar)2, (S.24) (cid:113) where xobj and yobj are the coordinates of the object, and xtar and ytar are the coordinates of the target. The third objective, termed the control cost, is defined as: fc = (cid:88) a2 , (S.25) where ai represents the action taken by the i-th actuator. MoReacher: The observation space is represented as R11, indicating that each observation is an 11-dimensional vector. The action space is defined as R2, meaning that each action is 2-dimensional vector. Each episode is constrained to maximum of 1000 time steps. The first objective, referred to as the distance reward, is defined as: fdp = w1 (x x0)2 w2 (y y0)2 + C, (S.19) fd = (cid:112)(xfin xtar)2 + (yfin ytar)2, (S.26) where and are the current positions of the model, x0 = 0 and y0 = 2 are the initial positions, w1 and w2 are the weights for the distance penalty in the and directions, respectively, and = 10 is constant survival reward indicating that the agent remains operational. The second objective, known as the speed penalty, is expressed as: fsp = w3 v2 w4 y + C, (S.20) where vx and vy are the velocities in the and directions, respectively, and w3 and w4 are the weights associated with these velocity components. where xfin and yfin denote the coordinates of the fingertip, and xtar and ytar denote the coordinates of the target. The second objective, known as the control cost, is expressed as: fc = (cid:88) a2 , (S.27) where ai represents the action taken by the i-th actuator. MoHumanoid: The observation space is represented as R244, indicating that each observation is 244-dimensional vector. The action space is defined as R17, meaning that each action is 17-dimensional vector. Each episode is constrained IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, 23 to maximum of 1000 time steps. The first objective, referred to as the forward reward, is defined as: fv = w1 vx + C, (S.28) where vx denotes the velocity in the x-direction, w1 is the weight associated with this velocity component, and = 5 represents constant survival reward, indicating that the agent remains operational. The second objective, known as the control cost, is given by: fc = w2 a2 + C, (cid:88) (S.29) where ai represents the action taken by the i-th actuator, and w2 is the weight applied to the control cost. MoHumanoidstandup (MoHumanoid-s): The observation space is represented as R244, indicating that each observation is 244-dimensional vector. The action space is defined as R17, meaning that each action is 17-dimensional vector. Each episode is constrained to maximum of 1000 time steps. The first objective, referred to as the forward reward, is defined as: fv = w1 vy + C, (S.30) where vy denotes the velocity in the y-direction, w1 is the weight associated with this velocity component, and = 5 represents constant survival reward, indicating that the agent remains operational. The second objective, known as the control cost, is given by: fc = w2 a2 + C, (cid:88) (S.31) where ai represents the action taken by the i-th actuator, and w2 is the weight applied to the control cost. S.VII. EXPERIMENTS This section presents the performance indicators and reference points employed in the MoRobtrol benchmark, along with supplementary experiments covering comparisons with CUDA-based acceleration algorithms, analysis of GPU performance impact, and detailed evaluations on the DTLZ test suite. A. Performance Indicators For comprehensive analysis of the proposed tensorized EMO algorithms, we employ three key performance indicators: inverted generational distance (IGD), hypervolume (HV), and expected utility (EU) metric. 1) IGD: The IGD measures how closely the set of final solutions approximates the reference PF . It is defined as: IGD(F , ) = (cid:80) minf f , (S.32) where denotes the Euclidean distance. lower IGD value indicates better approximation to the true Pareto Front. 2) HV: The HV metric evaluates the volume of the objective space dominated by the obtained solutions and bounded by reference point vref. The HV is calculated as: HV(F , vref) = (cid:91) volume(vref, ), (S.33) where volume(vref, ) represents the volume of the hypercube defined by the vector and the reference point vref. The reference point vref is predetermined as vector of ones, and represents the set of normalized objective values. higher HV value indicates better spread and convergence of the solutions. 3) EU: The EU for set of objectives Rmn is calculated as: EU(F , ) = 1 n (cid:88) j=1 max i{1,...,m} (cid:0)W fi(Fj)(cid:1) , (S.34) where Rm is the weight vector, Fj is the j-th solution in the objective space, and fi(Fj) represents the utility function applied to the i-th objective of the j-th solution. The expression fi(Fj) calculates the weighted utility of the objectives, and the max operation selects the maximum utility across all objectives for each solution. The mean value across all solutions provides the expected utility. B. Comparison with CUDA-based Acceleration Algorithms In this experiment, we compare the TensorNSGA-II algorithm with the NSGA-II implementation in EvoTorch [69]. EvoTorch is PyTorch-based evolutionary algorithm library that enables CUDA acceleration. Since EvoTorch only provides the NSGA-II algorithm, we conduct comparison between TensorNSGA-II and the EvoTorch NSGA-II on the DTLZ test suite. 1) Experimental Settings: The population size is set to 10 000, with dimensionality of 5000 and 3 objectives. Both algorithms are independently run 31 times, with 100 iterations per run. The comparison is based on the IGD and computation time. We employ the Wilcoxon rank-sum test to assess whether there are significant differences between the two algorithms, with the performance metrics of the superior algorithm highlighted. shown 2) Comparison Results: As in Table S.I, TensorNSGA-II outperforms the NSGA-II implementation in EvoTorch across all DTLZ problems, both in terms of IGD and computation time. Furthermore, on the RTX 4090 GPU, the speedup ranged from maximum of 5.61 to minimum of 1.59, demonstrating the advantages of the tensorized algorithm. C. Impact of Different GPUs on Performance In this experiment, we compare the performance of three proposed algorithms: TensorNSGA-III, TensorMOEA/D, and TensorHypE. The comparison is conducted across multiple hardware configurations, including CPU and the GPUs RTX 2080Ti, RTX 3090, and RTX 4090. The evaluation is conducted using the DTLZ1 problem. The goal is to analyze IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, 2025 24 D. Detailed Results for the DTLZ Test Suite Table S.III presents the results for tensorized and nontensorized algorithms on the DTLZ test suite. Tensorized algorithms achieve faster runtimes while maintaining similar IGD values. The performance differences are small and consistent across most problems. Some variations are due to the batch operations in tensorized algorithms, which simplify computations compared to the original methods. E. Experiment in Multiobjective Robot Control Benchmark For the hypervolume (HV) calculation in the multiobjective robot control benchmark experiments, reference points were established based on the minimum values of non-dominated solutions obtained by all algorithms for each problem. For objectives related to forward movement and height, if the minimum value was less than 0, the reference value was set to 0. In the MoPusher and MoReacher problems, the reference points were derived by taking the minimum values across all algorithms for each objective. Table S.IV provides the reference points used for the 9 multiobjective robot control problems. TABLE S.IV THE REFERENCE POINTS OF 9 MOROBTROL PROBLEMS FOR HV CALCULATION Problem Reference Point MoHalfcheetah MoHopper MoSwimmer MoIDP MoWalker2d MoPusher MoReacher MoHumanoid MoHumanoid-s (0, 530.97) (283.52, 994.39, 1498.70) (0, 0.20) (3246.31, 153.32) (0, 11.99) (1505.66, 984.00, 13645.28) (389.79, 1668.30) (340.74, 321.41) (4756.48, 6.06) TABLE S.I STATISTICAL RESULTS (MEAN AND STANDARD DEVIATION) OF THE IGD AND RUNTIME (S) OBTAINED BY NSGA-II AND TENSORNSGA-II IN DTLZ1DTLZ7. THE BEST RESULTS ARE HIGHLIGHTED. Indicator Problem NSGA-II TensorNSGA-II IGD Time DTLZ1 DTLZ2 DTLZ3 DTLZ4 DTLZ5 DTLZ6 DTLZ7 DTLZ1 DTLZ2 DTLZ3 DTLZ4 DTLZ5 DTLZ6 DTLZ7 1.4861e+05 (4.9045e+02) 3.7692e+02 (2.7958e+00) 5.0293e+05 (6.4328e+02) 3.6834e+02 (6.4508e+00) 3.7780e+02 (2.7668e+00) 4.4523e+03 (2.9406e+00) 9.2249e+00 (3.1569e01) 9.2198e+00 (2.4736e02) 9.0389e+00 (2.4060e02) 9.3291e+00 (1.9043e02) 1.3494e+01 (2.1555e+00) 9.4792e+00 (6.2154e02) 8.8182e+00 (9.3220e03) 9.7167e+00 (2.7945e02) 1.3866e+05 (5.4954e+02) 2.3238e+02 (8.9165e01) 4.6302e+05 (1.2591e+03) 2.3839e+02 (1.0780e+00) 2.4280e+02 (1.0162e+00) 4.3366e+03 (3.7426e+00) 7.1779e+00 (4.0208e02) 1.8127e+00 (4.0681e02) 2.1148e+00 (1.4466e02) 1.9310e+00 (1.5188e02) 8.5085e+00 (2.0471e01) 3.3992e+00 (5.1943e02) 1.5714e+00 (1.1792e02) 2.3176e+00 (1.1198e02) how different hardware configurations affect the performance of the algorithms, with focus on speedup achieved on GPUs relative to the CPU. 1) Experimental Settings: For each algorithm, the population size is set to 10 000, with dimensionality of 1000 and 3 objectives. Each algorithm is independently executed 10 times on each device, with 100 iterations per run. The average execution time across the 10 runs is calculated for each configuration, and the speedup achieved on the GPU relative to the CPU is determined, where speedup is defined as the ratio of execution time on the CPU to that on the GPU. TABLE S.II STATISTICAL RESULTS (MEAN AND STANDARD DEVIATION) OF THE RUNTIME (S) AND SPEEDUP FOR EMO ALGORITHMS ON DIFFERENT DEVICES IN DTLZ1 Device Algorithm Time Speedup CPU RTX 2080Ti RTX RTX 4090 NSGA-III MOEA/D HypE 8.7775e+02 (8.3293e+01) 9.6668e+02 (4.9265e+01) 4.7882e+02 (2.2464e+01) TensorNSGA-III TensorMOEA/D TensorHypE 3.8679e+00 (2.5282e02) 1.5599e+00 (1.2737e02) 5.6312e+00 (1.2946e01) TensorNSGA-III TensorMOEA/D TensorHypE 2.7511e+00 (1.7527e02) 8.6320e01 (5.6514e03) 2.7511e+00 (1.7530e02) TensorNSGA-III TensorMOEA/D TensorHypE 1.6648e+00 (1.0636e02) 6.0490e01 (5.0061e03) 2.4712e+00 (6.0123e02) 1.0 1.0 1.0 226.9 619.7 85.0 319.1 1119.9 174. 527.2 1598.1 193.8 Speedup = CPU Time / GPU Time. 2) Comparison Results: As shown in Table S.II, the type of GPU indeed affects the execution time. However, compared to the CPU, even the less powerful RTX 2080Ti GPU achieves minimum speedup of 85, illustrating the scalability of tensorized algorithms across different hardware configurations. The maximum speedup reaches 226.9. These results highlight the efficiency of tensorized algorithms in utilizing GPU resources, even on relatively low-end consumer hardware. The acceleration effect is more pronounced with higher-end GPUs, reaching maximum speedup of 1598.1 on the RTX 4090 GPU. IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, 2025 25 TABLE S.III STATISTICAL RESULTS (MEAN AND STANDARD DEVIATION) OF THE IGD AND RUNTIME (S) FOR NON-TENSORIZED AND TENSORIZED EMO ALGORITHMS IN DTLZ1DTLZ7. ALL EXPERIMENTS ARE CONDUCTED ON AN RTX 4090 GPU AND THE BEST RESULTS ARE HIGHLIGHTED. Algorithm Problem IGD (Non-Tensorized) IGD (Tensorized) Time (Non-Tensorized) Time (Tensorized) NSGA-III MOEA/D HypE DTLZ1 DTLZ2 DTLZ3 DTLZ4 DTLZ5 DTLZ6 DTLZ7 DTLZ1 DTLZ2 DTLZ3 DTLZ4 DTLZ5 DTLZ6 DTLZ7 DTLZ1 DTLZ2 DTLZ3 DTLZ4 DTLZ5 DTLZ6 DTLZ7 1.3619e+05 (3.6592e+02) 2.3069e+02 (1.2189e+00) 4.6228e+05 (1.1636e+03) 2.3785e+02 (1.0874e+00) 2.3463e+02 (1.1257e+00) 4.2845e+03 (3.4493e+00) 6.8165e+00 (4.9090e02) 1.2951e+05 (1.9198e+03) 1.2359e+02 (2.0404e+01) 4.3746e+05 (4.3569e+03) 3.4160e+02 (1.8663e+01) 1.2483e+02 (2.4523e+01) 4.3760e+03 (5.5493e+00) 3.0036e+00 (1.2811e01) 1.3705e+05 (5.2939e+02) 1.9759e+02 (1.0991e+00) 4.5667e+05 (8.3459e+02) 2.0196e+02 (1.0117e+00) 2.1053e+02 (1.1076e+00) 4.3122e+03 (3.8700e+00) 6.1372e+00 (2.9802e02) 1.3693e+05 (4.3756e+02) 2.2882e+02 (9.4040e01) 4.6225e+05 (1.2913e+03) 2.3572e+02 (8.5302e01) 2.3864e+02 (1.1237e+00) 4.3192e+03 (2.4402e+00) 7.0588e+00 (4.4870e02) 1.3819e+05 (5.6393e+02) 2.0785e+02 (1.3811e+00) 4.6991e+05 (1.7973e+03) 2.1645e+02 (2.2032e+00) 2.0857e+02 (1.7378e+00) 4.3915e+03 (3.0552e+00) 5.0944e+00 (1.9928e01) 1.3705e+05 (5.2939e+02) 1.9759e+02 (1.0991e+00) 4.5667e+05 (8.3459e+02) 2.0196e+02 (1.0712e+00) 2.1053e+02 (1.1076e+00) 4.3122e+03 (3.8700e+00) 6.1372e+00 (2.9802e02) 7.9940e+01 (2.0346e+00) 4.2566e+01 (1.0087e+00) 4.7783e+01 (6.8060e01) 5.9497e+01 (9.8333e01) 5.0183e+01 (1.0445e+00) 8.3887e+01 (9.5350e01) 6.5831e+01 (9.2720e01) 8.9395e+01 (2.8132e+00) 8.8740e+01 (2.6398e+00) 9.0152e+01 (3.0706e+00) 7.2891e+01 (2.9190e+00) 8.6675e+01 (2.9418e+00) 8.7844e+01 (3.0132e+00) 8.6198e+01 (2.9972e+00) 5.7589e+01 (1.5513e+00) 5.8689e+01 (1.4897e+00) 5.7680e+01 (1.5285e+00) 6.7169e+01 (1.4286e+00) 6.5312e+01 (1.4286e+00) 5.7556e+01 (1.5598e+00) 5.6269e+01 (1.7775e+00) 1.8314e+00 (1.5300e02) 2.3500e+00 (1.3600e02) 2.1542e+00 (1.2700e02) 8.5898e+00 (2.4899e01) 4.1632e+00 (5.5200e02) 1.8032e+00 (1.2700e02) 2.6303e+00 (5.1900e02) 1.0660e+00 (6.6339e03) 1.0694e+00 (5.3202e03) 1.0701e+00 (3.3552e03) 1.4081e+00 (4.2637e03) 1.0661e+00 (3.9010e03) 1.0704e+00 (2.9017e03) 1.0743e+00 (3.8539e03) 2.3500e+00 (3.9000e02) 3.8541e+00 (4.2200e02) 2.8743e+00 (2.8500e02) 1.1103e+01 (3.8963e01) 1.0254e+01 (1.0990e01) 3.0027e+00 (5.2700e02) 2.9777e+00 (1.2100e02)"
        }
    ],
    "affiliations": [
        "Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen 518055, China",
        "Department of Data Science and Artificial Intelligence and the Department of Computing, The Hong Kong Polytechnic University, Hong Kong SAR, China"
    ]
}