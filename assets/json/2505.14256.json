{
    "paper_title": "FuxiMT: Sparsifying Large Language Models for Chinese-Centric Multilingual Machine Translation",
    "authors": [
        "Shaolin Zhu",
        "Tianyu Dong",
        "Bo Li",
        "Deyi Xiong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this paper, we present FuxiMT, a novel Chinese-centric multilingual machine translation model powered by a sparsified large language model (LLM). We adopt a two-stage strategy to train FuxiMT. We first pre-train the model on a massive Chinese corpus and then conduct multilingual fine-tuning on a large parallel dataset encompassing 65 languages. FuxiMT incorporates Mixture-of-Experts (MoEs) and employs a curriculum learning strategy for robust performance across various resource levels. Experimental results demonstrate that FuxiMT significantly outperforms strong baselines, including state-of-the-art LLMs and machine translation models, particularly under low-resource scenarios. Furthermore, FuxiMT exhibits remarkable zero-shot translation capabilities for unseen language pairs, indicating its potential to bridge communication gaps where parallel data are scarce or unavailable."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 6 5 2 4 1 . 5 0 5 2 : r FuxiMT: Sparsifying Large Language Models for Chinese-Centric Multilingual Machine Translation Shaolin Zhu1, Tianyu Dong1, Bo Li2, Deyi Xiong1* 1College of Intelligence and Computing, Tianjin University, Tianjin, China 2School of Software, Tsinghua University, Beijing, China {zhushaolin, tydong, dyxiong}@tju.edu.cn {li-b19}@mails.tsinghua.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "In this paper, we present FuxiMT, novel Chinese-centric multilingual machine translation model powered by sparsified large language model (LLM). We adopt two-stage strategy to train FuxiMT. We first pre-train the model on massive Chinese corpus and then conduct multilingual fine-tuning on large parallel dataset encompassing 65 languages. FuxiMT incorporates Mixture-of-Experts (MoEs) and employs curriculum learning strategy for robust performance across various resource levels. Experimental results demonstrate that FuxiMT significantly outperforms strong baselines, including state-of-the-art LLMs and machine translation models, particularly under low-resource scenarios. Furthermore, FuxiMT exhibits remarkable zero-shot translation capabilities for unseen language pairs, indicating its potential to bridge communication gaps where parallel data are scarce or unavailable."
        },
        {
            "title": "Introduction",
            "content": "The rapid advancement of large language models (LLMs) has ushered in new era for machine translation (MT) (Zhu et al., 2024c). Despite this, the huge multilingual translation demands for Chinese have not been technologically satisfied, especially in comparison to those for English. While LLMs like BLOOM and LLaMA demonstrate multilingual capabilities, their coverage is severely limited. LLaMA supports 20 languages, and even the more extensive BLOOM covers only 46. Considering there are over 7,000 languages worldwide, existing LLMs only serve tiny fraction of the global population, leaving vast number of people unable to benefit from their multilingual features. In order to bridge this gap, we propose FuxiMT, novel Chinese-centric multilingual translation model powered by sparsified LLM. Particularly, FuxiMT, with approximately 13 billion parameters, *Corresponding author. builds upon the BLOOMz model (Workshop et al., 2022) and incorporates Sparse Mixture-of-Experts (MoEs) (Zhu et al., 2025). This modular structure enables efficient computation and scalability, crucial for handling large models and diverse language data (Shazeer et al., 2017). Our training strategy for FuxiMT leverages the strengths of both monolingual and parallel data (1) Chinese Prethrough two distinct phases: training: We first pre-train the sparse BLOOM model on massive dataset of 5 billion Chinese tokens. This pre-training stage aims to imbue the model with deep understanding of the Chinese language, ensuring that FuxiMT is inherently Chinese-centric and capable of high-quality Chinese language processing. (2) Multilingual Translation Training: We then utilize comprehensive parallel corpus of over 10 billion sentences, covering 65 languages. Specifically, we freeze the parameters of the pre-trained BLOOM model and introduce MoE modules at regular intervals within the decoder stack. These MoE modules are initialized using mixed strategy, leveraging both random initialization and weights from corresponding layers of the pre-trained BLOOM model to facilitate knowledge transfer. We then fine-tune the MoE parameters on the multilingual parallel data, employing curriculum learning strategy to gradually increase the language coverage and data complexity. To summarize, the key contributions of this paper are threefold: We introduce FuxiMT, LLM combining BLOOMz with MoE. By dynamically routing inputs to specialized experts, FuxiMT efficiently handles multilingual translation while reducing computational costs, addressing the limitations of English-dominated models. We develop Chinese-first pre-training strategy, initializing FuxiMT on 5B Chinese sentences to embed deep Chinese understanding, followed by multilingual fine-tuning on 100B+ parallel sentences across 65 languages. Experimental results demonstrate FuxiMT outperforms strong baselines (e.g., NLLB, GPT-3.5). These results highlight its robustness in real-world multilingual translation challenges."
        },
        {
            "title": "2 Datasets Construction",
            "content": "The training process for FuxiMT utilizes monolingual and parallel data. In this section, we present the construction pipeline from Chinese datasets and the multilingual datasets used by FuxiMT. 2.1 Chinese Datasets Collection. We use 3 sources for high quality post-cleaning pre-training corpus, totaling more than 5 billion sentences with data size of more than 1 TB. Intern WanJuan (260G)1 (He et al., 2023) is Chinese corpus constructed by the Shanghai AI Lab. It consists of cleaned pre-training corpus from various sources such as websites, encyclopedias, books, patents, textbooks, exam questions, etc. and is subjected to fine-grained cleaning, deduplication and value alignment. WuDaoCorpora (200G)2 is large, high-quality Chinese dataset constructed by the BAAI. It uses more than 20 rules to clean the final dataset from 100 TB of original website data and remove the private data information, and it contains data from more than 50 domains such as education, science and technology. ChineseWebText (530G)3 (Chen et al., 2023) is monolingual Chinese quality dataset extracted from CommonCrawl data and filtered based on manually formulated rules (including data length, sensitive words, proportion of Chinese characters, etc.). The quality of the filtered data is evaluated using BERT-based evaluation model. Filtering. We prepare the above raw datasets to ensure their quality for the training of machine translation models. First, sentences are extracted from paragraphs using predefined rules. Sentences 1https://opendatalab.org.cn/OpenDataLab/WanJuan1_dot_0 2https://data.baai.ac.cn/details/WuDaoCorporaText 3https://huggingface.co/datasets/CASIALM/ChineseWebText Figure 1: Language and data distribution in the pretraining data of FuxiMT. shorter than 50 characters or longer than 250 characters are filtered out, leaving sentences of appropriate length for effective training and semantic completeness. Next, language identification is performed to exclude sentences without Chinese characters, resulting in dataset consisting solely of Chinese text. This step mitigates the effects of noise on the model training. Finally, characters are normalized, leaving only Chinese characters, English letters, common punctuation marks and spaces. This process removes superfluous special characters and formatting symbols, ensuring the purity of the data and improving the effectiveness of model training."
        },
        {
            "title": "2.2 Multilingual Datasets\nCollection. We use OpusTools4 (Aulamo et al.,\n2020) to extract resources from the OPUS project\n(Nygaard and Tiedemann, 2003), a renowned plat-\nform for parallel corpora, and create a multilingual\ndataset. Specifically, we collect the parallel corpora\nfrom prominent projects within OPUS, including\nNLLB (Fan et al., 2021), CCMatrix (Schwenk et al.,\n2019) and OpenSubtitles (Lison and Tiedemann,\n2016). This comprehensive data collection process\nresults in a corpus of more than 2T, covering 65\nlanguages and over 1900 language pairs (As in Ap-\npendix Table 6). The dataset comprises over 80\ndatasets spread across 24,000 subset. Within this\nextensive collection, there is 650 subset dedicated\nto Chinese, totaling 60G.",
            "content": "4https://github.com/Helsinki-NLP/OpusTools Preprocessing. High-quality parallel corpora are essential for training powerful NMT models. However, raw data, such as data crawled from the web, often contains significant amount of noise, including length inconsistencies, irrelevant information, and sensitive content, which can negatively impact model performance. To address this issue, we refer to the preprocessing pipeline proposed by Fan et al. (2021), and use six-stage cleaning pipeline to create high-quality multilingual parallel corpora. Text extraction and preprocessing. We obtain compressed files with the target language pair from the OPUS corpus. After decompression, we remove all files that are not in Moses format and keep only the customized plain text files for the target language pair. For example, for English-Chinese translation, we only keep files with the suffixes .en and .zh. Proportion of characters. In this step, sentences containing excessive noise or non-target language content are identified and removed. Three strategies are used: (1) Punctuation ratio filtering: sentences with punctuation ratio of more than 50% are removed as they often contain no meaningful information. (2) Rule-based filtering: We use the simple_cleaning script from the preprocess5 toolkit to remove sentences that contain only spaces, excessive non-printable characters, invalid UTF8-encoded characters or excessively long tokens (e.g. DNA sequences, nonsensical repetitions). (3) Filtering the character ratio of the target language: The clean_histogram script from the fairseq6 toolkit identifies and removes sentences with high proportion of characters outside the target language. This ensures that the sentences consist mainly of the target language and that irrelevant information is minimized. Data length. Excessively long or short sentence pairs can interfere with the training of the NMT model. Therefore, we employ several measures to control sentence length: (1) We use the SentencePiece Model (SPM) (Kudo and Richardson, 2018) from the fairseq toolkit to tokenize both the source and the target language. (2) Sentence length ratio filtering: The clean-corpus-n.perl script from the moses-smt7 is used to eliminate sentence pairs with significant length differences, e.g. those where the target sentence is more than three times as long as the source sentence. (3) Filtering short text: Docu5https://github.com/kpu/preprocess 6https://github.com/facebookresearch/fairseq/ 7https://github.com/moses-smt/ ments with an average line length of less than 10 words or total length of more than 250 characters are discarded as they do not provide enough context for effective model learning. Sensitive words. To prevent the model from generating harmful or offensive language, we define list of sensitive words and analyze their frequency in the text. If sentence contains sensitive words whose frequency exceeds predefined threshold (e.g. 0.5), it is removed from the training dataset. Duplication. Duplicate data can lead to overfitting of the model and reduce the generalization ability. We use the dedup script from the preprocessing toolkit to identify and remove duplicate record pairs in the parallel data, retaining only the first instance. Normalization. Finally, we normalize the text formatting to make sure that the same symbols, such as punctuation marks, numbers, and spaces, with the same UTF8 codes, are used in all the datasets. We use the normalize-punctuation.perl script of the sacremoses toolkit8. In addition, to reduce the vocabulary and increase the efficiency of the model, we normalize these different quote marks to the same one. Significantly, all filtering steps should be applied to both source and target languages, e.g., if the source segment is empty and should be deleted, the target segment should also be deleted to keep the data parallel. The distribution of language and dataset after preprocessing is shown in Figure 1."
        },
        {
            "title": "3.1 Model Details",
            "content": "FuxiMT builds upon the BLOOMz model (specifically BLOOMz-7B according to the diagram), decoder-only LLM. As illustrated in Figure 2, the left part is the Original Model which is the pre-trained BLOOMz model. Crucially, this base models components (self-attention, Add & Normalize, and FFN layer) are all marked as Frozen. This design choice is fundamental to FuxiMT, preventing catastrophic forgetting of the pre-trained linguistic knowledge during subsequent fine-tuning. The frozen BLOOM model serves as robust backbone, providing general language understanding capabilities. The right part of Figure 2 depicts the integrated Sparsified Mode with the MoE layer. The diagram shows the MoE layer being inserted between the Add & Normalize and the FFN layer 8https://github.com/alvations/sacremoses Figure 2: Diagram of FuxiMT. FuxiMT is built upon BLOOMz-7B and fine-tuned on translation and general tasks. of the original BLOOM model. Within the MoE layer, we see multiple FFN blocks labeled FFN1, FFN2 etc. These represent the expert networks within the MoE, each specializing in different aspect of the translation task. The Router component is also depicted within the MoE layer. This router is the gating network, responsible for dynamically selecting the most relevant expert for each input token based on its context. The diagram shows the router receiving input and then directing the flow of information to the selected expert(s). the parameters of the selected experts parameters θei. Chinese pre-training stage: First, we train the model using Causal Language Modeling (CLM) on 5B monolingual Chinese sentences to improve its understanding of the syntactic and semantic structures of the Chinese language. This task is about predicting the next token in sequence based on the previous tokens. Formally, the model learns to maximize the probability of sequence of tokens = x1, x2, . . . , xT :"
        },
        {
            "title": "3.2 Post-training",
            "content": "To effectively integrate the MoE architecture into the BLOOM for FuxiMT, we adjust the MoErelated parameters in the initial configuration as Fedus et al. (2022). Specifically, we set sparse_step to 8, which denotes that only one expert is active for every 8 layers in the model. This sparsity strategy significantly improves computational efficiency while preserving the expressiveness of the model. moe_expert_count is set to 8, which means that eight experts are used in each MoE layer, allowing the model to specialize and distribute the processing of different linguistic data. For details on other training hyperparameters, see Section 4.1. The MoE model is formally defined as: (ytX, θ) = (cid:88) i=1 gi(X; θg) (ytX, θei), (1) where gi(X; θg) represents the gating network responsible for selecting the i-th expert ei, and (ytX, θei) denotes the probability distribution over the next token yt given the initial set and (x) = (cid:89) t=1 (xtx<t), (2) where (xtx<t) represents the conditional probability of token xt given all preceding tokens x<t = x1, x2, . . . , xt1. Compared to Masked Language Modeling (MLM), CLM is more suitable for generation-based tasks, which aligns with the primary focus of this stage. This advantage is particularly relevant for Chinese, where tokenization plays significant role in capturing sequential dependencies. During training, we use the crossentropy loss as the optimization objective: LCLM(θ) = (cid:88) t=1 log Pθ(xtx<t), (3) where Pθ(xtx<t) is the models predicted probability of token xt given the preceding tokens, parameterized by θ. The training process aims to minimize this loss across the entire training corpus. Multilingual translation training stage: In this stage, we fine-tune the BLOOM MoE model using an extensive multilingual parallel corpus for translation. The training data includes over Ltotal = (cid:88) t=1 wt L(X src , tgt ), (4)"
        },
        {
            "title": "4.2 Main Results",
            "content": "100 billion sentence pairs in 65 languages, mostly from the OPUS project, covering both high and low resource languages. We use instruction-tuning strategy to reformulate each translation example into an instruction-driven query. We create 40 structured instruction templates that guide the model to produce more accurate translations by clearly indicating the context of the source language, the target language, and the nature of the task. Detailed instruction templates refer to Appendix Table 9. We use curriculum learning to avoid catastrophic forgetting and ensure robust translation in all languages. Training starts with high-resource language pairs and gradually incorporates lowresource languages to prevent over-matching and promote balanced performance. We achieve this by weighting the contribution of each language pair to the overall training loss: where wt represents the weighting assigned to the language pair t, whereby the importance of the low-resource languages is gradually increased as training progresses. To further increase the robustness and coverage of our model, especially for low-resource languages, we employ data augmentation technique known as back-translation. In this method, the target sentence tgt is translated back into the source language, resulting in synthetic source sentence. This technique is particularly effective for augmenting data in low-resource language pairs. The augmented dataset is generated as follows: =D {(Y tgt, back-translated(Y tgt)) (X src, tgt) D}, (5) where represents the original dataset of sentence pairs and back-translated(Y tgt) is the synthetic source sentence generated by translating the target sentence tgt into the source language."
        },
        {
            "title": "4.1 Settings",
            "content": "We conducted our experiments on 8 NVIDIA TESLA A100-80G GPUs with Pytorch. To mitigate memory consumption and further improve training efficiency, we leverage ZeRO-2 (Rajbhandari et al., 2020) and Flash-Attention V2 (Dao, 2024) technologies. For optimization, the standard AdamW optimizer (Loshchilov and Hutter, 2017) was utilized with hyper-parameters set to β1 = 0.9, β2 = 0.99, and ϵ = 108. Additionally, we employ the cosine learning rate scheduler, adopt warm-up strategy for the learning rate, gradually increasing it to the peak value over the first 1000 steps and then linearly decaying it to zero throughout the remaining training steps. Detailed training parameters and configurations are provided in Appendix Table 3. We compare FuxiMT against several strong baselines: BLOOMz-7B (Workshop et al., 2022), GPT3.5 (Brown et al., 2020), NLLB (CostaJussà et al., 2022), LLaMAX3-8B (Lu et al., 2024), LLaMA-3.1-8B(Grattafiori et al., 2024), Mistral-7B-v0.3(Jiang et al., 2023), Qwen 2.5-7B (Team, 2024b), Aya-23-8B (Aryabumi et al., 2024), Gemma-2-9B (Team, 2024a) and InternLM 2.5-7Bchat (Cai et al., 2024). Table 1 presents the performance of FuxiMT and the baselines across different resource levels of languages, categorized as High, Medium, Low, and Very Low resource in terms of the amount of parallel data available (as described in Appendix Table 5). key observation is the consistent improvement of FuxiMT over the original BLOOMz-7B model. This improvement underscores the effectiveness of our proposed methodology, combining MoEs with the two-stage training strategy focusing first on Chinese pre-training and then multilingual fine-tuning. The significant gains under the low-resource setting highlight FuxiMTs ability to leverage crosslingual knowledge transfer, effectively addressing the data scarcity challenges prominent in many languages. Compared to other LLMs, FuxiMT exhibits competitive performance. While some models, like InternLM2.5 and Qwen2.5-7B, show higher scores in high-resource scenarios, FuxiMT maintains strong advantage under the low and very low resource settings. The specialized Chinese pre-training of FuxiMT likely contributes to its superior performance with Chinese-involved translation pairs, even surpassing strong baselines like GPT-3.5 in certain cases. While NLLB, specifically trained for lowresource translation, performs well in those categories, FuxiMT often achieves comparable or even better results, demonstrating its effectiveness in leveraging monolingual data and the benefits of Model High Resource Medium Resource Low Resource Very Low Resource FuxiMT GPT-3.5 BLOOMz NLLB LLaMAX3-8B LLaMA-3.1-8B Mistral-7B-v0.3 Qwen2.5-7B Aya-23-8B Gemma-2-9B InternLM2.5-7B-chat 37.0257 27.2460 13.6740 18.3420 28.0430 2.5763 3.5789 31.6810 28.9705 20.4730 29.6108 25.2682 20.0110 12.4014 19.4779 26.4063 1.9268 6.0708 26.7201 20.7915 16.0337 21.1705 20.6446 10.4000 14.0044 19.3294 21.9199 3.0245 2.4043 13.4558 6.8899 8.8800 7. 20.6649 9.1491 12.5400 20.1124 15.1937 2.9714 1.2762 9.8152 6.3070 6.5642 7.7571 Table 1: BLEU scores for different resource languages xx-zh. Method BLOOMz FuxiMT-Random-Init FuxiMT-Reuse-Init FuxiMT-Random-Train FuxiMT-Order-Train FuxiMT Average BLEU 13.08 24.46 25.37 19.27 21.93 26.15 Table 2: Ablation Results (Average BLEU across various language pairs) the MoE architecture. The strong performance of LLaMa models, particularly LLaMA 3-8B, highlights the potential of decoder-only models for multilingual translation. However, FuxiMTs tailored training strategy and incorporation of MoEs appear to provide further edge, especially in data-sparse scenarios."
        },
        {
            "title": "4.3 Ablation Study",
            "content": "To validate the necessity of the sparse MoE architecture, different FFN initialization methods, and the impact of curriculum learning in FuxiMT, we conducted comprehensive ablation experiments by comparing four variants: FuxiMT-RandomInit, FuxiMT-Reuse-Init, FuxiMT-Random-Train, FuxiMT-Order-Train. These settings are detailed in Appendix A.5. Results are presented in Table 2. We can find that the FuxiMT configuration, incorporating all designed elements, achieves superior performance. The performance difference between FuxiMT and FuxiMT-Random-Init suggests that the pre-training on the Chinese dataset provide much knowledge for the later training. Specifically, the sparse MoE architecture proves crucial for dynamically routing inputs to specialized experts, enabling effective partitioning of the complex multilingual knowledge space. We also show that initialization methods has large influence on the performance of the model. We then examine the training strategy variants further illuminating the benefits of our curriculum learning approach. FuxiMT-Random-Train, which dispenses with curriculum learning in favor of uniform language pair mixing, suffers significant performance degradation. This directly underscores the importance of the gradual introduction of languages, particularly the low-resource ones, in mitigating catastrophic forgetting. We also find that training the models with FuxiMT-Order-Train can also bring some improvement, which is better than the models with FuxiMT-Random-Train, but still worst than the complete model. These findings validate the designed process of MoE initialization, the Chinese-centric pre-training, and curriculum learning."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we present FuxiMT, multilingual Chinese-centric translation LLM. We design twostage training approach that explicitly combines Chinese pre-training followed by multilingual finetuning with curriculum learning and strategically incorporates MoEs, enabling FuxiMT to excel particularly in low-resource scenarios. Our results demonstrate substantially superior performance compared to strong baselines. Moreover, FuxiMTs remarkable zero-shot translation capabilities further cement its potential as valuable MT tool for bridging communication gaps across diverse linguistic landscapes where parallel data are scarce."
        },
        {
            "title": "References",
            "content": "Viraat Aryabumi, John Dang, Dwarak Talupuru, Saurabh Dash, David Cairuz, Hangyu Lin, Bharat Venkitesh, Madeline Smith, Kelly Marchisio, Sebastian Ruder, Acyr Locatelli, Julia Kreutzer, Nick Frosst, Phil Blunsom, Marzieh Fadaee, Ahmet Üstün, and Sara Hooker. 2024. Aya 23: Open weight releases to further multilingual progress. Preprint, arXiv:2405.15032. Mikko Aulamo, Umut Sulubacak, Sami Virpioja, and Jörg Tiedemann. 2020. OpusTools and parallel corpus diagnostics. In Proceedings of The 12th Language Resources and Evaluation Conference, pages 37823789. European Language Resources Association. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901. Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, Xiaoyi Dong, Haodong Duan, Qi Fan, Zhaoye Fei, Yang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao Gui, Aijia Guo, Qipeng Guo, Conghui He, Yingfan Hu, Ting Huang, Tao Jiang, Penglong Jiao, Zhenjiang Jin, Zhikai Lei, Jiaxing Li, Jingwen Li, Linyang Li, Shuaibin Li, Wei Li, Yining Li, Hongwei Liu, Jiangning Liu, Jiawei Hong, Kaiwen Liu, Kuikun Liu, Xiaoran Liu, Chengqi Lv, Haijun Lv, Kai Lv, Li Ma, Runyuan Ma, Zerun Ma, Wenchang Ning, Linke Ouyang, Jiantao Qiu, Yuan Qu, Fukai Shang, Yunfan Shao, Demin Song, Zifan Song, Zhihao Sui, Peng Sun, Yu Sun, Huanze Tang, Bin Wang, Guoteng Wang, Jiaqi Wang, Jiayu Wang, Rui Wang, Yudong Wang, Ziyi Wang, Xingjian Wei, Qizhen Weng, Fan Wu, Yingtong Xiong, Chao Xu, Ruiliang Xu, Hang Yan, Yirong Yan, Xiaogui Yang, Haochen Ye, Huaiyuan Ying, Jia Yu, Jing Yu, Yuhang Zang, Chuyu Zhang, Li Zhang, Pan Zhang, Peng Zhang, Ruijie Zhang, Shuo Zhang, Songyang Zhang, Wenjian Zhang, Wenwei Zhang, Xingcheng Zhang, Xinyue Zhang, Hui Zhao, Qian Zhao, Xiaomeng Zhao, Fengzhe Zhou, Zaida Zhou, Jingming Zhuo, Yicheng Zou, Xipeng Qiu, Yu Qiao, and Dahua Lin. 2024. Internlm2 technical report. Preprint, arXiv:2403.17297. Jianghao Chen, Pu Jian, Tengxiao Xi, Dongyi Yi, Qianlong Du, Chenglin Ding, Guibo Zhu, Chengqing Zong, Jinqiao Wang, and Jiajun Zhang. 2023. Chinesewebtext: Large-scale high-quality chinese web text extracted with effective evaluation model. Preprint, arXiv:2311.01149. Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 40924102. Association for Computational Linguistics. Marta Costa-Jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, et al. 2022. No language left behind: Scaling human-centered machine translation. arXiv preprint arXiv:2207.04672. Tri Dao. 2024. FlashAttention-2: Faster attention with better parallelism and work partitioning. In International Conference on Learning Representations (ICLR). Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten P. Bosma, Zongwei Zhou, Tao Wang, Yu Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen S. MeierHellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc V. Le, Yonghui Wu, Zhifeng Chen, and Claire Cui. 2022. Glam: Efficient scaling of language models with mixture-of-experts. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 55475569. PMLR. Nadir Durrani, Helmut Schmid, Alexander M. Fraser, Philipp Koehn, and Hinrich Schütze. 2015. The operation sequence model - combining n-gram-based and phrase-based statistical machine translation. Comput. Linguistics, 41(2):185214. Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, et al. 2021. Beyond english-centric multilingual machine translation. Journal of Machine Learning Research, 22(107):148. William Fedus, Barret Zoph, and Noam Shazeer. 2022. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):139. Orhan Firat, Kyunghyun Cho, and Yoshua Bengio. 2016. Multi-way, multilingual neural machine translation with shared attention mechanism. In NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, San Diego California, USA, June 12-17, 2016, pages 866875. The Association for Computational Linguistics. Yong Cheng, Ankur Bapna, Orhan Firat, Yuan Cao, Pidong Wang, and Wolfgang Macherey. 2022. Multilingual mix: Example interpolation improves multilingual neural machine translation. In Proceedings of the 60th Annual Meeting of the Association for Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Conghui He, Zhenjiang Jin, Chao Xu, Jiantao Qiu, Bin Wang, Wei Li, Hang Yan, Jiaqi Wang, and Dahua Lin. 2023. Wanjuan: comprehensive multimodal dataset for advancing english and chinese large models. Preprint, arXiv:2308.10755. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for NLP. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 27902799. PMLR. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7b. Preprint, arXiv:2310.06825. Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda B. Viégas, Martin Wattenberg, Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2017. Googles multilingual neural machine translation system: Enabling zero-shot translation. Trans. Assoc. Comput. Linguistics, 5:339351. Alexander Jones, Isaac Caswell, Ishank Saxena, and Orhan Firat. 2023. Bilex rx: Lexical data augmentation for massively multilingual machine translation. CoRR, abs/2303.15265. Taku Kudo and John Richardson. 2018. SentencePiece: simple and language independent subword tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 6671, Brussels, Belgium. Association for Computational Linguistics. Zehui Lin, Liwei Wu, Mingxuan Wang, and Lei Li. 2021. Learning language specific sub-network for In Proceedings multilingual machine translation. of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 293305. Association for Computational Linguistics. Pierre Lison and Jörg Tiedemann. 2016. Opensubtitles2016: Extracting large parallel corpora from movie and tv subtitles. Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101. Yinquan Lu, Wenhao Zhu, Lei Li, Yu Qiao, and Fei Yuan. 2024. Llamax: Scaling linguistic horizons of llm by enhancing translation capabilities beyond 100 languages. arXiv preprint arXiv:2407.05975. Lars Nygaard and Jörg Tiedemann. 2003. Opusan open source parallel corpus. In Proceedings of the 13th Nordic Conference on Computational Linguistics. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1 16. IEEE. Xiaozhe Ren, Pingyi Zhou, Xinfan Meng, Xinjing Huang, Yadao Wang, Weichao Wang, Pengfei Li, Xiaoda Zhang, Alexander Podolskiy, Grigory Arshinov, Andrey Bout, Irina Piontkovskaya, Jiansheng Wei, Xin Jiang, Teng Su, Qun Liu, and Jun Yao. 2023. Pangu-Σ: Towards trillion parameter language model with sparse heterogeneous computing. CoRR, abs/2303.10845. Holger Schwenk, Guillaume Wenzek, Sergey Edunov, Edouard Grave, and Armand Joulin. 2019. Ccmatrix: Mining billions of high-quality parallel sentences on the web. arXiv preprint arXiv:1911.04944. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le, Geoffrey E. Hinton, and Jeff Dean. 2017. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net. Haoran Sun, Renren Jin, Shaoyang Xu, Leiyu Pan, Supryadi, Menglong Cui, Jiangcun Du, Yikun Lei, Lei Yang, Ling Shi, Juesi Xiao, Shaolin Zhu, and Deyi Xiong. 2024. Fuxitranyu: multilingual large language model trained with balanced data. CoRR, abs/2408.06273. Gemma Team. 2024a. Gemma. Qwen Team. 2024b. Qwen2.5: party of foundation models. BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, et al. 2022. Bloom: 176bparameter open-access multilingual language model. arXiv preprint arXiv:2211.05100. Haoran Xu, Young Jin Kim, Amr Sharaf, and Hany Hassan Awadalla. 2024. paradigm shift in machine translation: Boosting translation performance of large language models. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net."
        },
        {
            "title": "A Appendix",
            "content": "Wen Yang, Chong Li, Jiajun Zhang, and Chengqing Zong. 2023. Bigtranslate: Augmenting large language models with multilingual translation caarXiv preprint pability over 100 languages. arXiv:2305.18098. Shaolei Zhang, Qingkai Fang, Zhuocheng Zhang, Zhengrui Ma, Yan Zhou, Langlin Huang, Mengyu Bu, Shangtong Gui, Yunji Chen, Xilin Chen, and Yang Feng. 2023. Bayling: Bridging cross-lingual alignment and instruction following through interactive translation for large language models. CoRR, abs/2306.10968. Jiawei Zheng, Hanghai Hong, Xiaoli Wang, Jingsong Su, Yonggui Liang, and Shikai Wu. 2024. Finetuning large language models for domain-specific machine translation. CoRR, abs/2402.15061. Fan Zhou and Chengtai Cao. 2021. Overcoming catastrophic forgetting in graph neural networks with experience replay. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, pages 47144722. AAAI Press. Shaolin Zhu, Menglong Cui, and Deyi Xiong. 2024a. Towards robust in-context learning for machine translation with large language models. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC/COLING 2024, 20-25 May, 2024, Torino, Italy, pages 1661916629. ELRA and ICCL. Shaolin Zhu, Leiyu Pan, Dong Jian, and Deyi Xiong. 2025. Overcoming language barriers via machine translation with sparse mixture-of-experts fusion of large language models. Information Processing & Management, 62(3):104078. ShaoLin Zhu, Leiyu Pan, and Deyi Xiong. 2024b. FEDS-ICL: enhancing translation ability and efficiency of large language model by optimizing Inf. Process. Manag., demonstration selection. 61(5):103825. Shaolin Zhu, Shaoyang Xu, Haoran Sun, Leiyu Pan, Menglong Cui, Jiangcun Du, Renren Jin, António Branco, Deyi Xiong, et al. 2024c. Multilingual large language models: systematic survey. arXiv preprint arXiv:2411.11072. Yaoming Zhu, Jiangtao Feng, Chengqi Zhao, Mingxuan Wang, and Lei Li. 2021. Counter-interference adapter for multilingual machine translation. In Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November, 2021, pages 2812 2823. Association for Computational Linguistics. Hidden Size Intermediate Size Heads Layers Position Embed Vocab Size MoE Expert Count Sparse Step Learning Rate Batch Size Context Length Weight Decay FlashAttn V2 ZeRO-2 4,096 16,384 32 30 4,096 250,752 8 8 5e-5 8 4,096 3e-7 Table 3: Hyper-parameters of training. A.1 Language Pairs Classifications Languages can be categorized into high-resource, medium-resource, low-resource, and very lowresource levels, focusing on xx zh translation pairs. FuxiMT supports total of 65 languages, including Chinese, which results in 64 language pairs. High-resource languages (e.g., English, Spanish, French) have abundant data, mediumresource languages (e.g., Turkish, Lithuanian, Estonian) have moderate data, low-resource languages (e.g., Amharic, Zulu, Nepali) have limited data, and very low-resource languages (e.g., many indigenous languages) have scarce data. The FuxiMT model has achieved competitive translation quality across all these language pairs, demonstrating its effectiveness at different resource levels. A.2 Post-training data In the multilingual translation training phase, FuxiMT prioritizes Chinese-centric data by allocating over 50% of its parallel corpus to xxzh language pairs. This deliberate imbalance ensures the model retains strong Chinese capabilities while acquiring multilingual expertise through curriculum learning (progressively integrating low-resource languages), parameter-efficient MoE architecture (isolating Chinese knowledge in the frozen BLOOM backbone), and back-translation augmentation (increasing Chinese data via synthetic source texts). A.3 Baselines A.5 Detail Setting of Ablation Study We compare FuxiMT against several strong baselines reflecting the state-of-the-art in multilingual machine translation. FuxiMT-Random-Init: All FFN experts are randomly initialized, with curriculum learning applied. BLOOMz-7B: The original BLOOMz-7B model serves as crucial baseline, allowing us to assess the impact of incorporating MoEs and our specialized training regime. This comparison helps isolate the contributions of our proposed modifications. GPT-3.5: widely-used large language model known for its strong translation capabilities. NLLB: prominent multilingual machine translation model developed by Meta AI, specifically designed for low-resource languages. This comparison highlights FuxiMTs performance relative to model explicitly trained for translation between diverse languages.We use the 54.5B parameter version of NLLB-200. LLaMA-3.1-8B: Representative large language models developed by Meta AI, offering point of comparison against other decoderonly models with different scales and architectures. LLaMAX3-8B/Mistral-7B-v0.3/Qwen2.57B/Aya-23-8B/Gemma-2-9B/InternLM2.57B-chat: Other state-of-the-art large language models, providing diverse range of architectural choices and training methodologies to benchmark against. A.4 Results In Tables 7 and 8, we provide detailed comparison of FuxiMTs performance with other large language models (LLMs) on 64 language pairs, showcasing both BLEU and CHRF scores. As observed, FuxiMT consistently outperforms other models in translating majority of these language pairs, with particularly remarkable advantage in translating from low-resource and very low-resource languages into Chinese. This demonstrates FuxiMTs strong capacity for cross-lingual knowledge transfer, especially in scenarios where parallel data is scarce. FuxiMT-Reuse-Init: All FFN experts reuse the parameters of the corresponding FFN layers in the pre-trained BLOOMz model, along with curriculum learning. FuxiMT-Random-Train: Uses the mixed FFN initialization method but with random training, where all languages are mixed uniformly without weighting. FuxiMT-Order-Train: Applies the mixed FFN initialization method and fixed-order training. Languages are ordered by resource level (high low) but with equal weights. A.6 Zero-Shot Translation FuxiMTs generalization ability was further evaluated through zero-shot translation on four unseen language pairs: Tigrinyan (ti), Tibetan (bo), Turkmen (tk), and Pijin (pis). As shown in Figure 3, FuxiMT consistently outperforms all baseline models across these pairs, demonstrating its remarkable capacity to transfer knowledge to unseen languages without explicit training data. This success is attributed to several key factors: (1) the multilingual training with curriculum learning, which exposes the model to diverse language pairs and translation patterns, promoting robust cross-lingual representations (2) and the Mixture-of-Experts architecture, which facilitates efficient knowledge sharing and transfer between different language pairs, even in zero-shot scenarios. The incorporation of MoEs, coupled with the data augmentation via back-translation (applied to seen pairs), further enhances FuxiMTs generalization capabilities, enabling it to handle unseen language pairs more effectively. These combined strategies demonstrate FuxiMTs potential as powerful tool for cross-lingual communication, especially for low-resource scenarios. The strong zero-shot performance highlights FuxiMTs ability to bridge communication gaps where parallel data is scarce or non-existent, crucial advantage for fostering seamless communication and collaboration across the diverse linguistic landscape. (a) Tigranian (b) Tibetan (c) Turkmen (d) Pijin Figure 3: Results on zero-shot language pairs A.7 Comparison to Other MT model based on Lang FuxiMT BigTranslate"
        },
        {
            "title": "LLM",
            "content": "BigTranslate is another MT model based on LLM which demonstrates strong performance on the FLORES-200 dataset. In this section, we compare FuxiMT with BigTranslate on FLORES-200. As shown in Table 4, FuxiMT consistently achieves higher BLEU scores across range of language pairs, especially those involving Chinese. This advantage stems from FuxiMTs Chinese-centric pre-training, which provides strong foundation for Chinese translation, and its multilingual training with curriculum learning, enabling effective cross-lingual knowledge transfer. The MoE architecture further enhances FuxiMTs ability to handle diverse linguistic nuances and low-resource scenarios, leading to overall improved translation quality compared to BigTranslates more general LLM approach. A."
        },
        {
            "title": "Instructions for Implement Machine\nTranslation",
            "content": "Multilingual Neural Machine Translation (MNMT) continues to evolve but faces significant challenges in balancing performance across multiple en es fr pt id hi ko ne ka my bo 53.12 46.78 47.73 39.47 41.58 28.93 30.27 29.92 12.22 11.32 25.24 31.1 23.5 24.2 23.6 18.4 15.1 19.6 10.8 0.9 5.9 36.3 Table 4: Comparison with BigTranslate languages. Translation approaches based on Large Language Models (LLMs) each have their advantages and limitations. FuxiMT introduces an innovative solution by combining two-stage training process with mixture of experts (MoEs) model, effectively addressing existing issues. In practical applications, comprehensive set of detailed translation instructions has been developed to ensure translation quality and consistency (see Table 9). These instructions cover variety of expressions designed to guide the model in accurately understanding the translation task, specifying the source and target languages, and addressing different text units such as sentences and phrases. For instance, instructions like Translate from {src_lang} to {tgt_lang}: {src_text} help the model better grasp the translation intent. By leveraging these directives, FuxiMT produces more accurate and fluent translations across different language pairs. It excels particularly in handling low-resource language pairs and complex linguistic structures, significantly improving both accuracy and robustness. This provides an effective solution for multilingual translation challenges. A.9 Related Work This section delves into the existing landscape of multilingual machine translation and the recent advancements in leveraging large language models for this task, highlighting the limitations of current approaches and positioning FuxiMT as novel solution. A.9.1 Multilingual Neural Machine"
        },
        {
            "title": "Translation",
            "content": "Multilingual Neural Machine Translation (MNMT) has made significant strides in recent years, aiming to build single system capable of translating between multiple language pairs (Cheng et al., 2022). This approach offers significant advantages over training individual bilingual systems, particularly in low-resource scenarios where parallel data is limited. MNMT leverages the power of crosslingual transfer learning, allowing knowledge acquired from high-resource language pairs to enhance the translation quality for low-resource pairs (Johnson et al., 2017). Early MNMT research focused on extending traditional statistical machine translation methods to handle multiple languages (Durrani et al., 2015). However, the emergence of neural machine translation (NMT) revolutionized the field, leading to significant improvements in translation quality. Neural MNMT models typically employ encoder-decoder architectures, where the encoder maps the source sentence into context vector and the decoder generates the target sentence based on this context. Numerous techniques have been proposed to further enhance the performance of neural MNMT, including parameter sharing (Firat et al., 2016), language-specific components(Lin et al., 2021), and data augmentation (Jones et al., 2023). These techniques aim to encourage crosslingual transfer learning, capture language-specific nuances, and address the data scarcity issue, respectively. Despite these advancements, achieving balanced performance across multiple language pairs, especially for those with limited resources, remains significant challenge in MNMT. A.9.2 Large Language Models for Machine Translation The emergence of LLMs has opened up new possibilities for multilingual translation. These models, trained on vast amounts of text data, exhibit remarkable capabilities in understanding and generating natural language, even in zero-shot or fewshot settings. Several research directions have explored adapting LLMs for MT (Zhang et al., 2023; Yang et al., 2023). Direct fine-tuning, straightforward approach, involves training pretrained LLMs on parallel data for the specific task of translation. However, this method can suffer from catastrophic forgetting, where the model loses previously acquired linguistic knowledge during fine-tuning (Zhu et al., 2021; Zhou and Cao, 2021). Parameter-efficient fine-tuning methods like Adapters (Houlsby et al., 2019) and (Hu et al., 2022) aim to reduce computational cost and mitigate catastrophic forgetting by fine-tuning only small subset of the LLMs parameters. While offering efficiency and stability, these techniques may not fully exploit the vast linguistic knowledge already encoded within the LLM, potentially limiting its cross-lingual transfer learning capabilities (Xu et al., 2024). Prompt engineering and in-context learning leverage the models ability to learn from few examples provided during inference, eliminating the need for explicit fine-tuning (Zhu et al., 2024a,b). However, achieving consistent and highquality translation across diverse language pairs using these methods remains challenging, as the quality heavily relies on the design of prompts and the selection of in-context examples, making it difficult to generalize across different language pairs and domains (Zheng et al., 2024). Finally, Mixtureof-Experts (MoEs) enhance the capacity and efficiency of LLMs by allowing specialized experts to handle different aspects of the input space. Models like GLaM (Du et al., 2022) and PanGu-Σ (Ren et al., 2023) demonstrate the effectiveness of MoEs in scaling LLMs and achieving state-of-the-art results on various NLP tasks, including MT. However, these models are typically trained from scratch with MoEs and do not explicitly address the challenge of adapting existing pre-trained multilingual LLMs for MT. This approach may miss out on leveraging Resource Level Languages High Resource Medium Resource Low Resource Very Low Resource en, es, fr, de, pt, it, ru, pl, cs, hu, el, ro, sk, bg, ar tr, sl, lt, zh, et, id, lv, vi, hr, ja, uk, fa, hi, ko, bn, sq, mk, ms, th, bs, sw, kk sr, ta, be, ur, ne, si, az, ka, hy, am, km, my, ky, mg, mn, so ps, ha, lo, rw, mi, ug, prs, ti, bo, tk, pis Table 5: Resource level of different languages. ISO-639 Language Language Family ISO-639 Language Language Family ISO-639 Language Language Family am ar hy az be bn bo bs bg my km cs prs en et fa pis pl pt mn ne sr Afro-Asiatic Afro-Asiatic Indo-European Amharic Arabic Armenian Azerbaijani Turkic Indo-European Belarusian Indo-European Bengali Sino-Tibetan Tibetan Indo-European Bosnian Indo-European Bulgarian Burmese Sino-Tibetan Cambodian Austroasiatic Czech Dari English Estonian Persian Pijin Polish Portuguese Mongolian Mongolic Nepali Serbian Indo-European Indo-European Indo-European Uralic Indo-European English Creole Indo-European Indo-European Indo-European Indo-European fr ka de el ha hi hr hu id it ja kk ko ky lo lt mg mi mk ms ps ro Indo-European Kartvelian Indo-European Indo-European Afro-Asiatic Indo-European Indo-European Uralic Austronesian Indo-European Japonic Turkic Koreanic Turkic Tai-Kadai Indo-European French Georgian German Greek Hausa Hindi Croatian Hungarian Indonesian Italian Japanese Kazakh Korean Kyrgyz Laotian Lithuanian Madagascar Austronesian Austronesian Maori Indo-European Macedonian Austronesian Malay Indo-European Pashto Indo-European Moldovan ro ru rw si sk sl so es sw ta th ti tr tk uk ur ug vi zh sq lv Indo-European Indo-European Niger-Congo Indo-European Indo-European Indo-European Afro-Asiatic Indo-European Niger-Congo Dravidian Tai-Kadai Afro-Asiatic Turkic Turkic Indo-European Indo-European Turkic Romanian Russian Rwandan Sinhalese Slovak Slovenian Somali Spanish Swahili Tamil Thai Tigranian Turkish Turkmen Ukrainian Urdu Uyghur Vietnamese Austroasiatic Sino-Tibetan Chinese Indo-European Albanian Indo-European Latvian Table 6: The list of 65 natural languages supported by FuxiMT. This novel combination of techniques positions FuxiMT as promising solution for achieving highquality, Chinese-centric multilingual translation, particularly for under-resourced languages. existing linguistic knowledge and requires significant computational resources for training from scratch. A.9.3 Bridging the Gap: FuxiMT Our work, FuxiMT, builds upon the advancements in both MNMT and LLM-based translation, drawing inspiration from the FuxiTranyu paper (Sun et al., 2024) and addressing the limitations of existing approaches. Unlike previous work that focuses on either direct fine-tuning, parameter-efficient methods, or prompting, FuxiMT leverages MoEs for efficient knowledge transfer while preserving the pre-trained BLOOM models core linguistic capabilities. This approach mitigates the risk of catastrophic forgetting, allowing FuxiMT to retain the benefits of pre-training while acquiring translationspecific expertise. Similar to FuxiTranyu, FuxiMT adopts two-stage training strategy, first establishing Chinese-centric model through pre-training on massive Chinese corpus and then enhancing its multilingual capabilities using large-scale parallel corpus. However, FuxiMT distinguishes itself by uniquely incorporating MoEs to enhance the models ability to handle diverse language pairs. Lang FuxiMT GPT-3.5 BLOOMz NLLB LLaMAX3 LLaMA3 Mistral Qwen Aya Gemma InternLM en es fr de pt it ru pl cs hu el ro sk bg ar tr sl lt et id lv vi hr ja uk fa hi ko bn sq mk ms th bs sw kk sr ta be ur ne si az ka hy am km my ky mg mn so ps ha lo rw mi ug prs ti bo tk pis ro 53.12 46.78 47.73 43.05 39.47 38.59 36.63 36.30 35.69 29.95 28.92 30.05 24.68 26.41 38.01 34.72 24.93 15.45 23.14 41.58 12.74 23.85 28.47 33.05 23.14 23.19 28.93 30.27 26.68 21.17 12.84 36.03 22.74 27.82 25.01 14.88 28.04 19.53 27.73 25.27 29.92 21.31 27.88 12.22 25.16 18.02 11.91 11.32 15.55 17.36 24.66 14.42 18.24 17.79 12.33 12.39 24.90 21.20 19.89 21.67 25.24 24.55 29.10 30.05 47.76 35.84 28.44 28.48 34.11 32.64 25.89 27.20 22.26 19.29 17.47 27.06 19.18 20.82 22.25 25.18 23.19 14.18 17.35 37.45 10.85 21.84 27.87 23.79 21.13 16.60 19.49 25.47 14.07 17.21 10.48 33.60 17.75 18.71 16.48 7.54 16.94 13.07 7.95 16.49 14.84 11.16 13.00 9.66 6.15 5.70 8.52 5.84 8.20 8.54 12.37 7.97 5.16 3.59 6.72 6.42 11.15 9.78 11.96 9.60 8.84 8.67 18.75 27.06 21.29 18.81 20.64 12.31 19.29 11.09 15.75 5.82 6.13 9.07 11.91 9.63 8.62 21.29 13.46 7.46 10.23 9.42 9.50 18.66 10.95 17.05 7.98 13.59 18.22 12.14 14.72 5.39 14.47 9.93 16.39 16.58 11.65 12.95 10.11 13.04 10.97 10.81 19.05 9.24 15.72 11.10 8.51 19.27 20.86 12.63 18.74 14.09 17.18 14.91 15.54 5.45 10.23 6.45 11.30 7.36 12.59 10.47 19.27 18.87 16.18 9.51 15.71 9. 25.93 17.74 2.97 19.83 19.01 18.08 20.55 15.68 18.26 18.50 19.58 22.09 17.12 18.88 20.92 18.97 17.60 17.12 17.71 19.65 16.81 21.13 17.04 17.15 21.06 21.90 20.81 22.16 19.89 19.44 19.59 22.39 20.13 18.76 18.25 21.48 23.34 20.92 18.64 22.20 21.80 19.99 13.95 20.74 22.11 22.93 22.29 16.09 15.98 14.09 17.23 16.98 22.90 19.59 23.78 19.51 19.88 21.71 26.22 18.35 11.29 22.40 15.61 22.09 30.33 26.17 29.49 29.36 28.77 26.93 28.03 25.91 28.14 27.15 26.64 29.76 27.99 28.63 27.34 26.12 26.73 26.33 27.28 28.46 28.09 27.05 27.59 22.69 25.93 24.66 25.86 24.27 24.56 27.03 28.98 27.61 25.34 29.14 24.60 26.18 29.45 22.70 22.92 23.61 25.39 20.97 11.68 24.45 28.25 19.08 22.66 20.36 21.38 17.37 23.01 17.42 21.94 19.92 19.84 14.88 16.54 16.26 22.41 5.86 2.25 15.16 12.08 29.76 7.69 2.85 1.47 1.40 2.30 2.30 1.48 4.68 2.91 2.31 1.52 2.79 2.16 1.37 1.41 1.73 2.30 3.82 1.27 1.40 2.18 6.22 2.25 1.32 1.31 1.43 1.28 1.46 1.35 2.52 1.33 1.29 1.45 1.69 1.37 1.49 1.34 1.31 1.49 1.41 1.29 1.40 1.33 5.17 5.30 5.03 12.63 5.35 1.48 1.25 1.31 1.28 1.44 1.32 4.19 1.70 1.30 12.30 1.43 3.22 1.08 2.68 2.02 2.79 8.80 1.95 2.06 1.80 1.43 1.95 3.76 1.31 1.91 1.83 14.54 4.48 2.07 3.59 2.21 4.63 1.37 1.29 1.13 1.43 9.68 4.47 1.47 17.99 4.69 7.96 12.65 13.91 10.35 4.43 4.95 4.39 12.49 1.84 1.27 5.10 2.07 3.94 1.99 9.25 1.42 0.71 3.08 1.79 4.88 1.03 1.51 1.81 1.69 0.70 1.84 0.74 2.71 0.75 0.72 0.92 1.07 2.88 1.42 0.58 0.59 1.27 1.13 4.48 40.21 30.48 34.75 34.10 34.44 31.83 32.45 29.23 31.91 26.18 25.80 32.23 29.30 31.10 31.20 28.90 26.89 24.08 24.12 33.83 25.79 32.19 29.44 28.97 30.33 28.12 26.97 29.59 24.35 18.44 29.34 31.05 29.09 30.35 12.07 17.21 29.98 13.37 20.97 21.40 20.22 8.30 9.13 15.94 16.41 4.26 13.30 7.46 11.57 6.12 10.93 5.92 11.27 6.00 9.35 5.52 9.62 10.87 26.31 2.86 1.71 12.14 12.32 32.23 34.78 28.37 31.37 31.01 31.78 29.59 29.29 27.84 30.87 19.15 28.23 31.18 27.15 24.29 29.65 26.88 19.80 20.58 13.53 30.24 15.18 29.35 24.61 26.89 29.53 28.24 25.64 27.05 8.43 13.10 18.97 26.14 13.86 24.19 6.91 7.50 19.39 10.74 13.84 10.28 12.22 1.42 8.63 5.84 4.51 0.78 0.94 0.74 8.62 3.99 3.49 4.81 4.96 3.67 2.55 3.90 4.89 3.14 26.35 0.45 0.24 10.15 9.08 31. 39.99 29.25 12.67 18.86 5.37 4.10 30.49 16.81 18.44 12.32 4.09 32.41 29.81 31.81 20.68 4.99 3.43 15.11 13.11 31.46 30.06 27.91 28.27 16.86 14.41 6.59 4.05 7.73 26.56 28.12 6.09 11.05 23.76 30.73 3.07 3.34 2.66 4.50 15.43 2.99 8.83 8.26 2.15 17.84 3.51 16.97 17.64 17.93 4.18 7.94 1.79 9.46 11.38 18.19 1.78 14.60 2.52 1.20 1.45 3.01 5.22 6.52 6.35 32.41 42.51 29.68 34.28 33.58 34.49 30.39 30.96 26.53 30.13 23.58 20.22 32.17 25.99 28.89 20.78 27.41 23.67 16.54 17.10 29.57 17.93 24.37 27.21 27.75 28.46 19.97 19.13 24.99 12.40 14.20 24.73 26.32 16.48 28.36 8.88 9.13 24.21 4.47 16.91 13.23 11.22 2.05 5.30 3.81 3.06 1.49 4.48 2.01 7.28 5.80 4.98 5.20 5.54 5.68 5.08 4.91 7.48 12.77 19.01 1.48 2.96 9.07 11.35 32.17 Table 7: The detail BLEU scores in different language pairs xx-zh Lang FuxiMT GPT-3.5 BLOOMz LLaMAX LLaMA3 Mistral Qwen Aya Gemma InternLM en es fr de pt it ru pl cs hu el ro sk bg ar tr sl lt et id lv vi hr ja uk fa hi ko bn sq mk ms th bs sw kk sr ta be ur ne si az ka hy am km my ky mg mn so ps ha lo rw mi ug prs ti bo tk pis ro 49.6 45.68 46.49 41.99 37.52 36.08 34.34 34.34 34.67 31.42 29.12 29.46 23.54 24.69 35.38 32.78 23.67 14.02 22.35 39.89 13.73 19.85 27.05 38.28 22.29 22.44 26.2 32.39 25.09 20.92 12.29 35.71 24.57 25.46 23.75 15.82 25.49 20.21 28.54 23.58 30.08 23.06 28.66 13.68 26.22 19.08 13.3 12.89 16.29 18.39 25.35 15.47 18.52 18.33 14.19 14.12 26.47 22.27 20.09 23.11 26.56 26.05 25.48 29. 39.88 29.57 24.21 25.68 27.95 27.53 19.78 22.48 17.86 16.58 13.51 23.14 15.39 16.34 15.53 20.53 19.32 13.47 15.04 30.59 9.91 19.08 22.9 22.92 16.35 11.74 16.05 20.05 10.93 16.91 9.38 28.85 13.75 17.07 13.67 6.42 14 9.3 7.86 12.63 12.85 8.1 10 7.06 5.12 5.45 6.8 5.47 5.76 9.29 10.63 7.55 3.73 3.12 5.77 5.8 10.1 8.47 8.86 7.02 5.8 5.66 17.22 23.14 17.38 15.82 16.87 11.37 15.8 10.6 13.42 6.48 6.58 8.68 10.92 9.06 8.44 17.31 12.13 7.5 9.66 8.87 9.32 15.75 10.34 14.2 7.82 11.99 15.2 11.19 12.86 6.3 12.62 9.16 14.18 14.51 11.26 11.59 9.89 11.78 10.13 10.29 15.95 9.3 13.54 10.12 8.1 15.99 16.93 11.63 15.87 13.04 14.45 13.12 13.28 6.49 10 7.4 10.62 7.46 11.41 9.9 16.02 15.93 13.86 9 13.38 9.06 28.96 25.71 28.26 28.44 27.87 26.36 26.73 25.7 27.41 26.56 25.83 28.7 27.47 27.16 26.07 26.06 26.27 25.58 26.75 27.62 27.1 26.68 26.97 22.41 25.13 24.48 25.14 24.06 23.99 26.55 27.59 27.09 25.12 28.13 24.43 25.32 28.02 22.36 22.95 23.27 24.75 21.01 12.92 23.95 26.78 19.33 23.52 20.63 21.47 18.39 22.75 18.35 22.41 20.58 20.8 16.25 18.01 17.04 22.36 7.32 3.79 16.1 13.57 28.7 8.05 3.62 2.38 2.21 3.12 3.14 2.21 5.49 3.83 3.07 2.41 3.67 2.97 2.15 2.28 2.49 3.09 4.71 2.05 2.25 3.1 6.96 3.12 2.08 2.04 2.29 2.04 2.16 2.19 3.42 2.07 2 2.07 2.56 2.23 2.24 2.12 2.11 2.42 2.21 2.05 2.24 2.1 5.98 5.97 6.3 14.3 6.47 2.26 2.09 2.09 2.01 2.3 2.16 5.89 2.49 2.11 13.01 2.28 4.79 1.98 3.63 3.08 3.67 9.55 2.83 3.05 2.63 2.44 2.98 4.85 2.26 2.85 2.86 15.87 5.59 3.06 4.65 3.3 6.14 2.35 2.25 1.98 2.28 11.54 5.79 2.33 18.88 5.61 9.61 14.3 15.07 12.17 6.04 6.21 5.46 14.46 2.8 2.16 6.97 2.92 5.94 3.21 11.17 2.48 1.31 4.5 2.92 6.57 2.08 2.91 2.99 2.71 1.21 2.81 1.29 4.21 1.24 1.34 1.59 1.92 4.49 2.41 1.24 1.17 2.19 2.06 5.59 37.59 29.44 32.74 32.14 32.56 30.48 30.76 28.42 30.43 26.01 25.22 30.73 28.37 29.52 29.55 28.26 26.28 23.81 24 32 25.24 30.87 28.38 27.91 28.92 27.41 26.09 28.54 23.98 19.12 28.02 29.81 28.19 29.06 13.69 18.04 28.4 14.65 21.65 21.46 20.51 10.05 10.82 16.88 17.07 6.09 15.92 9.64 13.35 8.1 12.45 7.79 13.03 7.82 11.97 7.44 11.7 12.51 25.65 4.53 3.33 13.65 14.28 30. 32.49 27.24 29.42 29.16 29.97 28.2 27.71 27.01 29.38 19.71 26.93 29.33 26.36 23.76 28.12 26.37 20.3 20.73 14.95 28.77 16.29 28.14 24.16 26.01 27.94 27.16 24.73 26.44 10.41 14.43 19.3 25.41 15.52 23.65 8.53 9.03 19.6 12.3 15.33 11.83 13.44 2.83 10.08 7.96 6.52 1.48 1.88 1.33 10.38 5.44 4.71 6.47 6.48 5.04 4.21 5.26 6.46 4.55 25.4 1.04 0.74 11.71 10.72 29.33 37.03 28.53 13.35 18.91 6.59 5.09 28.95 17.46 18.8 13.09 4.9 31.26 29.25 30.21 20.11 6.27 4.75 15.57 13.93 30.41 28.97 27.55 27.67 16.95 14.79 7.51 4.7 8.71 25.66 27.78 7.13 12.1 23.5 29.78 4.14 4.31 3.64 5.77 15.99 4.02 9.71 9.24 2.88 17.81 4.34 17.59 19.49 18.76 5.15 9.62 2.67 10.95 12.71 19.46 2.92 16.47 3.54 2.01 2.25 4.03 6.87 7.5 7.83 31.26 39.14 28.58 32 31.54 32.17 29.04 29.3 25.7 28.64 23.36 20.43 30.44 25.38 27.47 20.71 26.56 23.19 17.24 17.87 28.15 18.55 24.2 26.11 26.65 27.2 20.31 19.46 24.61 13.76 15.36 23.83 25.5 17.34 27.12 10.3 10.64 23.54 6.45 18.06 14.41 12.85 3.78 7.05 5.92 4.91 3.19 6.94 3.46 8.99 7.59 6.68 6.72 7.38 7.18 7.19 6.5 9.31 14.18 19.36 2.92 5.19 10.69 13.17 30.44 Table 8: The detail chrf scores in different language pairs xx-zh Instructions How do you say {src_text} in {tgt_lang}? {src_text} How do you say this sentence in {tgt_lang}? {src_text} Say this using {tgt_lang} Translate from {src_lang} to {tgt_lang}: {src_text} Translate {src_text} from {src_lang} to {tgt_lang}. Translate {src_text} to {tgt_lang}. Translate the following. {src_lang}: {src_text} {tgt_lang}: Translate the sentence from {src_lang} to {tgt_lang}. {src_lang}: {src_text} Corresponding {tgt_lang} translation: Translate the sentence from {src_lang} to {tgt_lang}. {src_lang}: {src_text} {tgt_lang}: Translate from {src_lang} to {tgt_lang}. {src_lang}: {src_text} {tgt_lang}: Render the {src_lang} sentence {src_text} into {tgt_lang}. Produce the {tgt_lang} equivalent for the {src_lang} phrase {src_text}. Present {src_text} in {tgt_lang}, originally written in {src_lang}. Translate the {src_lang} text {src_text} to {tgt_lang}. Provide {tgt_lang} version of this {src_lang} statement: {src_text} Reword {src_text} from {src_lang} into {tgt_lang}. Convert this {src_lang} expression {src_text} to {tgt_lang}. Express the {src_lang} phrase {src_text} in {tgt_lang}. How would you phrase {src_text} in {tgt_lang}? Transcribe {src_text} from {src_lang} to {tgt_lang}. Can you adapt {src_text}, which is in {src_lang}, to {tgt_lang}? Rephrase {src_text} in {tgt_lang}, its original language is {src_lang}. Show me {src_text} written in {tgt_lang} instead of {src_lang}. Please rewrite {src_text} in {tgt_lang}, it was originally in {src_lang}. Represent {src_text} in {tgt_lang}, its source language is {src_lang}. Translate {src_text} which is in {src_lang}, to {tgt_lang}. How can {src_text} be expressed in {tgt_lang}? Give me the {tgt_lang} translation for {src_text}. need {src_text} written in {tgt_lang}, please help. Please provide the {tgt_lang} translation for the following {src_lang} sentence: {src_text} Convert the {src_lang} sentence {src_text} into {tgt_lang}. What is the {tgt_lang} version of the {src_lang} sentence {src_text}? need the {tgt_lang} rendition of this {src_lang} phrase: {src_text} Transform {src_text} from {src_lang} to {tgt_lang}. Translate {src_text} from {src_lang} to {tgt_lang} for me. Can you change {src_text} which is in {src_lang} to {tgt_lang}? Give me the {tgt_lang} equivalent of this {src_lang} sentence: {src_text} Rewrite the {src_lang} phrase {src_text} in {tgt_lang}. would like the {tgt_lang} translation of the following {src_lang} sentence, please: {src_text} Table 9: Examples of translation instruction"
        }
    ],
    "affiliations": [
        "College of Intelligence and Computing, Tianjin University, Tianjin, China",
        "School of Software, Tsinghua University, Beijing, China"
    ]
}