{
    "paper_title": "Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs",
    "authors": [
        "Bo Bai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have demonstrated remarkable capabilities in numerous real-world applications. While the vast majority of research conducted from an experimental perspective is progressing rapidly, it demands substantial computational power, data, and other resources. Therefore, how to open the black-box of LLMs from a theoretical standpoint has become a critical challenge. This paper takes the theory of rate-distortion function, directed information, and Granger causality as its starting point to investigate the information-theoretic principles behind LLMs, leading to the development of semantic information theory for LLMs, where the fundamental unit is token, rather than bits that lacks any semantic meaning. By defining the probabilistic model of LLMs, we discuss structure-agnostic information-theoretic measures, such as the directed rate-distortion function in pre-training, the directed rate-reward function in post-training, and the semantic information flow in inference phase. This paper also delves deeply into the theory of token-level semantic embedding and the information-theoretically optimal vectorization method. Thereafter, we propose a general definition of autoregression LLM, where the Transformer architecture and its performance such as ELBO, generalization error bound, memory capacity, and semantic information measures can be derived theoretically. Other architectures, such as Mamba/Mamba2 and LLaDA, are also discussed in our framework. Consequently, this paper provides a theoretical framework for understanding LLMs from the perspective of semantic information theory, which also offers the necessary theoretical tools for further in-depth research."
        },
        {
            "title": "Start",
            "content": "TECHNICAL REPORT 1 Forget BIT, It is All about TOKEN: Towards"
        },
        {
            "title": "Bo Bai",
            "content": "Abstract 5 2 0 2 3 ] . [ 1 2 0 2 1 0 . 1 1 5 2 : r Large language models (LLMs) have demonstrated remarkable capabilities in numerous realworld applications. While the vast majority of research conducted from an experimental perspective is progressing rapidly, it demands substantial computational power, data, and other resources. Therefore, how to open the black-box of LLMs from theoretical standpoint has become critical challenge. This paper takes the theory of rate-distortion function, directed information, and Granger causality as its starting point to investigate the information-theoretic principles behind LLMs, leading to the development of semantic information theory for LLMs, where the fundamental unit is token, rather than bits that lacks any semantic meaning. By defining the probabilistic model of LLMs, we discuss structure-agnostic information-theoretic measures, such as the directed ratedistortion function in pre-training, the directed rate-reward function in post-training, and the semantic information flow in inference phase. This paper also delves deeply into the theory of token-level semantic embedding and the information-theoretically optimal vectorization method. Thereafter, we propose general definition of autoregression LLM, where the Transformer architecture and its performance such as ELBO, generalization error bound, memory capacity, and semantic information measures can be derived theoretically. Other architectures, such as Mamba/Mamba2 and LLaDA, are also discussed in our framework. Consequently, this paper provides theoretical framework for understanding LLMs from the perspective of semantic information theory, which also offers the necessary theoretical tools for further in-depth research. I. INTRODUCTION At the end of 2022, ChatGPT emerged and its capabilities stunned the entire world! few month later, we fortunately invited Prof. Arikan, the inventor of Polar codes, for panel discussion.1 My colleague, Dr. Wu, hosted the event, his first question was brilliant: Prof. Arikan, what do you consider the greatest invention of the information age? After Bo Bai, Lab Director and Chief Scientist of Information Theory, is with Theory Lab - Leibniz, Central Research Institute, 2012 Labs, Huawei Technology Co., Ltd., Hong Kong. Email: baibo8@hauwei.com 1The event was broadcast live through the Chaspark website and became the best live event of that year. 2 TECHNICAL REPORT moment of thought, the professor gave decisive answer: The BIT! believe that the bit is the greatest invention of the information age. This answer deeply shook me and has since inspired me to think about question: What is the most important concept with the same fundamental importance as the bit in AI age, especially after ChatGPT emerged? After deeply involved into the research of LLMs, finally realized that: the concept am seeking is none other than the TOKEN. Inspired by Shannons seminal 1948 paper [1], tried to approach the explanation theory of LLMs from inference perspective. Shannon started with the goal of achieving reliable information transmission in communication system. From that starting point, he laid out complete set of mathematical concepts and theorems, which is known as information theory. In 1949, Weaver and Shannon co-authored paper in which they clearly identified three levels of communication problems [2]. They are: Level-A: Technical problem. How accurately can the symbols of communication be transmitted? Level-B: Semantic problem. How precisely do the transmitted symbols convey the desired meaning? Level-C: Effectiveness problem. How effectively does the received meaning affect conduct in the desired way? Shannon humbly suggested that his theory only solved the problem of reliable communication, i.e., Level-A technical problem. This is because, in Shannons theory, information is equivalent to uncertainty. He was not concerned with the meaning or significance of the transmitted message, but only with whether its binary representation was received without error. However, it is shown in our work that by extending Shannons theory to center on tokens, the underlying principles of LLMs can be explained from information-theoretic perspective, which will be referred to as semantic information theory. Early research on semantics can be traced back to the work of Carnap, who had series of brilliant discussions on this issue from the perspectives of empiricism, ontology, linguistics, and logic [3][5]. In the classic book [6], Carnap provides comprehensive and systematic exposition of semantics and modal logic. The modern developments of these approaches are well summarized in [7], [8]. Deeply influenced by Carnap, Solomonoff proposed the concept of algorithmic probability and integrated it into Bayesian inference framework, thereby providing formal theory of inductive inference [9][11]. In Solomonoffs theory, the prior probability of sequence is determined by its complexity. Therefore, the shortest program that can generate the sequence has the highest prior probability, which is referred BAI: FORGET BIT, IT IS ALL ABOUT TOKEN: TOWARDS THE SEMANTIC INFORMATION THEORY OF LLMS 3 to as the universal prior. The length of this shortest program defined on Turing machine is known as the Kolmogorov complexity of the sequence. In [12], [13], Kolmogorov complexity is introduced as new logical basis for Shannons information theory based on computing complexity on Turing machine. It can be seen that this is exactly about viewing sequence from generative perspective based on Turing machine. Based on the Solomonoff prior and Kolmogorov complexity, universal reinforcement learning is proposed for sequence decision and AI agent [14]. However, calculating the Kolmogorov complexity of sequence is Turing-undecidable problem, which in turn makes the theories of Kolmogorov and Solomonoff difficult to apply in practice. When we apply Kolmogorov complexity to the sample sequences of random variable, the expected value is exactly the Shannon entropy [15], [16]. Therefore, it is believed that Shannons information theory is probabilistic special case of Kolmogorov complexity theory. However, the probabilistic approach of information theory is more valuable for modern neural networks and LLMs, the core reason may lie in the computability of information-theoretic measures such as entropy, mutual information, and Kullback-Leibler (KL) divergence (or cross-entropy), and also the fact that they are easy to approximate from data in practice using other more easily computable quantities [17]. This concept is precisely took away from Suttons famous short essay [18], specifically the first sentence: The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by large margin. key question of extending Shannons theory to center on tokens is how to represent semantics of token in computable form. Unfortunately, source coding in Shannons theory only concerns how to represent the original message with the minimum number of binary symbols, but not with the semantics of the source. The idea of representing and retrieving information with vectors can be traced back to the work in [19]. The vector representation became the semantic basis of information-retrieval system [20]. In [21], Bengio et. al was the first to propose simultaneously learning low-dimensional, distributed representation for words, i.e., word vector, as part of training language model. This marked the first time the concept of word vectors was combined with neural networks. In [22], Mikolov et. al introduced two model architectures: CBOW and Skip-gram, which demonstrated that highquality word vectors can be trained with great efficiency on massive text corpora using simple neural network. In their following work [23], they showed that the learned word vectors exhibit linear substructures that capture meaningful semantic relationships between words. This finding was groundbreaking and sparked wave of research on word embeddings, 4 TECHNICAL REPORT leading to the development of various models such as GloVe [24], FastText [25], and ELMo [26]. The vector representation of semantics has become the foundation of modern NLP and LLMs [27]. The vector representation, however, is only token-level semantics. How to extend the semantic representation and generation to sentence, paragraph, or even an article in computing efficient way has long been challenging problem. The advent of the Transformer [28], an architecture founded on the attention mechanism, represented critical breakthrough, delivering extraordinary potential on NLP tasks. Subsequently, OpenAI introduced series of GPT models built upon the Transformer architecture, which have exhibited remarkable capabilities in diverse applications [29][32]. Based on the classic Transformer architecture, DeepSeek has proposed suite of enhancements aimed at substantially enhancing training efficiency. Consequently, the published LLMs exhibit remarkable inferential power [33], [34]. However, there still lacks deep theoretical understanding of the principles behind the Transformer architecture. Therefore, improving the architecture and further enhancing LLM capabilities relies heavily on large-scale experiments on GPUs, which in turn requires an immense investment of resources. Numerous studies have found that information-theoretic methods have been applied to many aspects of machine learning and have played significant role [35]. The information bottleneck method, employed to analyze the mechanics of deep learning, has gained significant attention within academia and industry [36]. In [37], the rate-distortion function and information bottleneck method are applied to explain the semantic embedding for LLMs. The language model based textual transform coding is proposed for sharply improving the compression performance of multimedia [38]. To capture both the fidelity and the reality at the same time, the rate-distortion-perception function is surveyed for generative models in our work [39]. The Transformer is modeled as an interacting particle system, with particular emphasis on long-time clustering behavior [40]. The centrality of data to LLM training underscores the significance of information-theoretic methods in data science, which is comprehensively reviewed in [41]. However, the autoregression LLM (AR-LLM), such as Transformer architecture, have not to be systematically studied from an information-theoretic perspective. This paper leverages semantic information theory to construct theoretical framework for understanding LLMs. We first propose probabilistic model for LLM as next-token predictor, which reveals it as discrete-time channel with feedback and state. significant modification to Shannons theory is to treat the channel as generative model instead BAI: FORGET BIT, IT IS ALL ABOUT TOKEN: TOWARDS THE SEMANTIC INFORMATION THEORY OF LLMS 5 of media for information transmission. The objective shifts from exactly recovering the original information to ensuring the generated sequence meets specific requirements. This perspective leads us to propose the directed rate-distortion function as universal measure for LLMs in the pre-training phase [42], [43]. The directed rate-reward function is also introduced for the reinforcement learning based post-training phase [44], which shows that the LLM is approximating Granger causality at human level for next-token prediction [45]. The semantic information flow is defined and analyzed from the perspective of submartingale for the inference phase. Focusing on the foundations of LLMs, we then delve into the token-level semantic space and its vectorization. The semantic vector compression and the Gromov-Wasserstein distance based semantic distortion metric are discussed [46], [47]. Based on this groundwork, an information-theoretically optimal semantic vectorization method is introduced for next-token prediction. Its connection to contrastive predictive coding (CPC) is also examined [48], [49]. Thereafter, premised on the theory of time-varying vector autoregression (TV-VAR) processes, we formally establish general mathematical definition for AR-LLMs [50]. It is demonstrated that the Transformer architecture constitutes specialized case of this general AR-LLM formulation [28]. Based on the variational inference principle, the evidence lower bound (ELBO) of Transformer is derived for both training phase and inference phase [51]. The generalization error bound for Transformer is analyzed by using Rademacher complexity and Talagrand inequality [52]. The memory capacity, referred to as Gardner capacity for Hopfield network, is discussed for Transformer [53][55]. The semantic information theoretical measure for LLMs, is discussed from the perspective of directed information estimation. The connection between AR-LLM and other novel architectures, such as Mamba/Mamba2 and LLaDA, are also discussed [56][58]. The rest of this paper is organized as follows. Section II presents the key concepts. In Section III, the LLM is studied as next-token predictor. Section IV discusses the vector representation of token-level semantics. The general definition of AR-LLMs is proposed in Section V, where the Transformer architecture is thoroughly studied. Other LLM architectures are also discussed in this section. Finally, Section VI concludes this paper. II. PRELIMINARIES In this section, we will introduce the rate-distortion function, the directed information, and Granger causality, which will play key roles for understanding LLMs in subsequent discussions. TECHNICAL REPORT A. Rate-distortion Function Rate-distortion theory, proposed by Shannon [1] and systematically discussed in [43], addresses the problem of determining the minimum rate bits/symbol, so that the source symbol can be approximately reconstructed at the receiver without exceeding an expected distortion D. Definition 1: The rate-distortion function for source sequence X1:n with non-negative distortion measure is defined as R(D) = lim 1 inf ( ˆX1:nX1:n):E{d(X1:n, ˆX1:n)}D I(X1:n; ˆX1:n), (1) where ˆX1:n is the output of the lossy source codec. The rate-distortion function is in general very difficult to compute, where the classical Blahut-Arimoto algorithm is proposed in [59], [60]. Recently, we proposed communication optimal transport approach and constrained Blahut-Arimoto algorithm to compute the ratedistortion function and the rate-distortion-perception function [61][63]. B. Directed Information In information theory, the directed information is first defined by Massey in his pioneer work [42] for discussing the channel with feedback. This idea was systematically developed for extensive channels with feedback in [64]. Let X1:n and Y1:n be two random sequences with N, we then have the following definition. Definition 2: The directed information from X1:n to Y1:n is defined as I(X1:n Y1:n) = (cid:88) I(X1:t; YtY1:t1). (2) t=1 Following this idea, we introduce the backward directed information from Xn:1 to Y1:n as follows: Definition 3: The backward directed information from Xn:1 to Y1:n is defined as I(Xn:1 Y1:n) = (cid:88) t=1 I(Xt+1:n; YtY1:t1). (3) The information density, first proposed by Dobrushin in [65], has been widely used in finite blocklength information theory and machine learning [35]. Similarly, we introduce the directed information density. Definition 4: The directed information density from X1:n to Y1:n is defined as ı(X1:n Y1:n) = (cid:88) t= ı(X1:n; YtY1:t1), (4) BAI: FORGET BIT, IT IS ALL ABOUT TOKEN: TOWARDS THE SEMANTIC INFORMATION THEORY OF LLMS 7 where ı(X1:n; YtY1:t1) = log (YtY1:t1, X1:n) (YtY1:t1) . (5) Similar to the rate-distortion function, it is also very difficult to compute directed information in practice. The classical Blahut-Arimoto algorithm has been extended to maximize directed information in [66]. Inspired by the idea of mutual information neural estimator (MINE) [67], the directed information neural estimator (DINE) is proposed in [68]. seminal work of computing information density is proposed by Strassen in [69]. C. Granger Causality Granger, the Nobel prize winner of 2003, proposed general definition of causality in [45], which is referred to as Granger causality afterwards. Definition 5: Let Ut be all the knowledge in the universe available at time with 1 n, be the knowledge in the modified universe in which X1:n is excluded, Xt is said to cause Yt+1 if (Yt+1 AUt) = (Yt+1 AU ). (6) This definition is general but not operational. In [70], several version of operational definition have been discussed, where the directed information or transfer entropy are proposed as strength measure of Granger causality. As finite length version of directed information, the transfer entropy is first introduced in [71]. In many following works, Granger causality is shown to be equivalent to directed information or transfer entropy for Gaussian vector autoregression (VAR) processes [72]. In fact, Massey also discussed the causality for communication system with feedback in his seminal work [42]. The directed information, transfer entropy, and Granger causality are widely used in physics, neuroscience, social networks, and finance [73]. From the perspective of [74], however, Granger causality is classified as statistical rather than causal. III. LLM AS NEXT-TOKEN PREDICTOR Inspired from information theory, this section will introduce the probabilistic model and architecture irrelevant properties for LLMs. A. Probabilistic Model of LLMs The probabilistic model of LLMs is illustrated in Fig. 1. The input token sequence is X1:n with 1 < and N, which will be mapped to semantic vector sequence S1:n 8 TECHNICAL REPORT Embedding X1:n S1:n LLM (UtS1:n, Un+1:t1; Φ) Ut φ Yt Ut Fig. 1. The probabilistic model of an LLM at time N, where X1:n is the token sequence with 1 < whose semantic vector embedding is S1:n, Yn+1:T is the output token sequence, whose semantic vector embedding is Un+1:T . Φ represents the parameters after training. by semantic embedding module . The LLM is modeled as transition probability with parameter Φ, which represents the parameters of the LLM after training. The LLM generates the embedding of next token Ut based on S1:n and the previously generated Un+1:t1, that is (UtUn+1:t1, S1:n; Φ). (7) φ is an inverse module of embedding, which maps Ut to the output token Yt. It should be noticed that the probabilistic model in Fig. 1 is general and architecture irrelevant. Remark 1 (Kolmogorov Complexity Formulation of LLMs): The Kolmogorov complexity K(y) is defined as the length of the shortest program that generates the output y, formally written as K(y) = min {l(p) : (p) = y}, (8) where is universal Turing machine, is the program, and l(p) is the length of p. According to [75], the Kolmogorov complexity can be rewritten as K(y) = min i,p {K(i) + l(p) : Ti(p) = y} (9) where is the index of sequence of Turing machines. It can be seen that K(y) is decomposed into two parts: the first part is Turing machine Ti, i.e., the meaningful information or model in the data, and the second part is the irregular aspects of y, i.e., program to be interpreted by Ti. Following this idea, the LLM is equivalent to Ti, is the input x, i.e., the prompt. B. Directed Rate-distortion Function in Pre-training Phase From the perspective of information theory, Eq. (7) is discrete-time channel with feedback and state [42], [64]. The input is S1:n, the output is Un+1:T , the feedback at time is Ut, and the channel state is the parameter Φ. In contrast to the reliable communication problem, the BAI: FORGET BIT, IT IS ALL ABOUT TOKEN: TOWARDS THE SEMANTIC INFORMATION THEORY OF LLMS goal here is to ensure the output token sequence aligns with our expectations, rather than flawless recovery of the input. As discrete-time channel with feedback and state, the directed information is natural choice to measure the information transferred from S1:n to Un+1:T with parameter Φ [42], [64]. According to Definition 2, we have I(S1:n Un+1:T ; Φ) = (cid:88) t=n+1 I(S1:n; UtUn+1:t1; Φ). (10) Let ℏ the KL divergence. Denote ℏ n+1:T be the labeled sequence by human being with the input S1:n, and DKL() be = (UtUn+1:t1, S1:n; Φ) for n+1:t1, S1:n) and QΦ = (U ℏ ℏ = + 1, . . . , , we then have the following definition. Definition 6: The directed rate-distortion function for LLMs in the pre-training phase is defined as Rpre(D) = 1 Φ: 1 inf t=n+1 DKL(P ℏ (cid:80)T QΦ )<D I(S1:n Un+1:T ; Φ). (11) Similar to Shannon capacity, Rpre(D) is defined as universal measure connecting the input sequence S1:n and output sequence Un+1:T . Furthermore, Rpre(D) is independent of any implementation methods, such as Transformer or novel architectures yet to be conceived. In contrast to the classical rate-distortion function in Definition 1, which governs lossy source codecs, the output sequence Un+1:T in this context is instead constrained by condition defined in terms of KL divergence. Therefore, Rpre(D) is the minimum information needed from S1:n to generate the expected Un+1:T with an average distortion D. The curve of Rpre(D) versus the optimization process of Φ will reveal key properties of the pre-training in practice. Simple derivation will give us the following theorem. Theorem 1: In the pre-training phase with the cross-entropy loss, we have Rpre(0) = 1 I(S1:n ℏ n+1:T ), (12) when convergence. Proof: The cross-entropy between ℏ and QΦ is given by H(P ℏ , QΦ ) = H(P ℏ ) + DKL(P ℏ QΦ ), Thus, the objective of pre-training can be written as min Φ H(P ℏ , QΦ ) min Φ DKL(P ℏ QΦ ), = + 1, . . . , T. (13) = + 1, . . . , T. (14) The minimization is achieved by adjusting Φ such that QΦℏ = (UtUn+1:t1, S1:n; Φℏ) = (U ℏ ℏ n+1:t1, S1:n) = ℏ , = + 1, . . . , T, (15) TECHNICAL REPORT where Φℏ is the optimal solution of Eq. (14). It implies that = DKL(P ℏ QΦℏ ) = 0, when convergence. Recalling Definition 2, we have I(S1:n Un+1:T ; Φℏ) = (cid:88) t=n+1 I(S1:n; UtUn+1:t1; Φℏ) = = (cid:88) t=n+1 (cid:88) t=n+1 H(UtUn+1:t1; Φℏ) (cid:88) t=n+1 H(UtUn+1:t1, S1:n; Φℏ) H(U ℏ ℏ n+1:t1) (cid:88) t=n+1 H(U ℏ ℏ n+1:t1, S1:n) (16) (17) =I(S1:n ℏ n+1:T ). This theorem has been established. The aforementioned definition and theorem show that minimizing the directed information by adjusting Φ filters out information irrelevant to generate the output, which may effectively prevent hallucinations caused by the propagation of extraneous information by LLMs. Therefore, we suggest to use the following loss function for LLM pre-training: L(Φ) = I(S1:n; UtUn+1:t1; Φ) + λH(P ℏ , QΦ ), = + 1, . . . , T, (18) where λ is the Lagrangian multiplier. Remark 2 (Information Geometry and Pre-training): Consider the pre-training phase, the distribution before and after one training step is denoted by Pt(Φ) = (UtUn+1:t1, S1:n; Φ) and Pt(Φ) = (UtUn+1:t1, S1:n; Φ), respectively. According to [76], the entry of the Fisher information matrix at the i-th row and j-th column is given by (cid:12) (cid:12) H(Pt(Φ), Pt(Φ)) (cid:12) (cid:12)Φ=Φ 2 iΦ [I(Φ)]ij = Φ . (19) Thus, the Fisher information matrix represents the curvature of the cross-entropy loss with respect to the parameters Φ. By modifying the gradient with Fisher information matrix, the natural gradient method is then proposed for neural network training. Due to the high computation complexity and storage cost, the Kronecker-factored approximate curvature method is used in practice [77]. C. Directed Rate-reward Function in Post-training Phase The objective of pre-training is to accurately predict the next-token. The generated token sequence, however, may not follow the human preference. The post-training shifts the focus to BAI: FORGET BIT, IT IS ALL ABOUT TOKEN: TOWARDS THE SEMANTIC INFORMATION THEORY OF LLMS 11 evaluate whether the entire generated sequence aligns with human preferences by fine-tuning with reinforcement learning from human feedback (RLHF) [32]. An evaluation function w(S1:n, Un+1:T ), the reward function in RLHF, is introduced to assign score to the generated sequence Un+1:T for the input sequence S1:n. We then have the following definition. Definition 7: The directed rate-reward function for LLMs in the post-training phase is defined as Rpost(W ) = 1 inf Φℏ:w(S1:n,Un+1:T )>W I(S1:n Un+1:T ; Φℏ). Therefore, we suggest to use the following loss function for LLM post-training: L(Φℏ) = I(S1:n Un+1:T ; Φℏ) λw(S1:n, Un+1:T ), (20) (21) where λ is the Lagrangian multiplier. The optimization solution will be denoted as Φℏ+. Recalling the proof of Theorem 1, L(Φℏ) is equivalent to the loss function of RL fine-tuning phase in [78]. Theorem 1 shows that the LLM approaches I(S1:n ℏ n+1:T ) during pre-training, which measures the information transferred from S1:n to ℏ n+1:T by human being. The post-training further adjusts the parameter from Φℏ to Φℏ+ such that the generated sequence Un+1:T meets human preferences. Recalling the discussion in Section II-C, we have the following conclusion. Corollary 1: The LLM approaches the human-level Granger causality for next-token prediction with human preference after training. D. Semantic Information Flow in Inference Phase During the inference phase, the LLM with parameter Φℏ+ is employed to generate the output token sequence Un+1:T based on the input token sequence S1:n. In contrast to the post-training phase, where the focus is on the average performance across all possible output sequences, the inference phase considers the specific output sequence for the given input sequence. Therefore, it is natural to use the directed information density in Definition 4 to analyze the inference process. The semantic information flow can then be defined as follows. Definition 8: The semantic information flow for LLMs is defined as the directed information density from S1:n to Un+1:t as follows: ı(S1:n Un+1:t; Φℏ+) = (cid:88) τ =n+ ı(S1:n; Uτ Un+1:τ 1; Φℏ+), = + 1, . . . , T. (22) 12 TECHNICAL REPORT In the inference phase, the generation will stop when special token, denoted by , is generated. Thus, is the stopping time with respect to the event {UT = s()}, where the vector representation of is s(). We then have the following theorem. Theorem 2: The semantic information flow ı(S1:n Un+1:t; Φℏ+) is Markovian submartingale for = + 1, . . . , . Proof: According to the Definition 4, we have ı(S1:n Un+1:t; Φℏ+) = ı(S1:n Un+1:t1; Φℏ+) + ı(S1:n; UtUn+1:t1; Φℏ+), (23) and ı(S1:n; UtUn+1:t1; Φℏ+) = log (UtUn+1:t1, S1:n; Φℏ+) (UtUn+1:t1; Φℏ+) . Thus, we consider the conditional expectation as follows: E{ı(S1:n Un+1:t; Φℏ+)ı(S1:n Un+1:t1; Φℏ+), . . . , ı(S1:n Un+1; Φℏ+)} =E{ı(S1:n Un+1:t; Φℏ+)ı(S1:n Un+1:t1; Φℏ+)} =ı(S1:n Un+1:t1; Φℏ+) + E{ı(S1:n; UtUn+1:t1; Φℏ+)} (24) (25) =ı(S1:n Un+1:t1; Φℏ+) + DKL(P (UtUn+1:t1, S1:n; Φℏ+)P (UtUn+1:t1; Φℏ+)) ı(S1:n Un+1:t1; Φℏ+). The last inequality holds because the KL divergence is non-negative, which establishes this theorem. In the following, we will discuss the properties of semantic information flow as submartingale. According to Doob decomposition, we have ı(S1:n Un+1:t; Φℏ+) = Mt + At, (26) where At is predictable and non-decreasing process At = (cid:88) j=n+1 E{ı(S1:n Un+1:j; Φℏ+) ı(S1:n Un+1:j1; Φℏ+)ı(S1:n Un+1:j1; Φℏ+)} and Mt is martingale Mt =ı(S1:n Un+1; Φℏ+) (cid:88) + (ı(S1:n Un+1:j; Φℏ+) ı(S1:n Un+1:j1; Φℏ+) Aj). j=n+"
        },
        {
            "title": "Define the sum of the conditional variances of the differences as",
            "content": "Vt = (cid:88) j=n+1 E{(Mj Mj1)2Mj1, . . . , Mn+1}. (27) (28) (29) BAI: FORGET BIT, IT IS ALL ABOUT TOKEN: TOWARDS THE SEMANTIC INFORMATION THEORY OF LLMS 13 The following corollary can be directly established according to Freedmans inequality [79]. Corollary 2: For all α, β > 0, we have Pr{Mt > α, Vt < β} exp (cid:18) α2 2(α + β) (cid:19) . (30) According to Doobs optional stopping time theorem [80] for sub-martingale, we have the following corollary directly. Corollary 3: I(S1:n Un+1:T ; Φℏ+) I(S1:n Un+1; Φℏ+). (31) Sharing the same spirit of Shannon capacity, i.e., the maximum mutual information over all input distributions, this corollary inspired us to give the following definition. Definition 9: The semantic information capacity for LLMs is defined as max (S1:n):w(S1:n,Un+1:T )>W I(S1:n Un+1:T ; Φℏ+). (32) Eq. (32) can be seen as theoretical foundation for prompt engineering. IV. VECTOR REPRESENTATION OF TOKEN-LEVEL SEMANTICS prerequisite for the efficient training of LLMs is the effective representation of tokenlevel semantics. This section will first define the token-level semantic space, and then elaborate on the vector representation of semantics, semantic compression/de-dimensionality, and the information-theoretic optimal semantic embedding/vectorization. A. Token-level Semantic Space While grammatical and logical rules are central to how human being communicate and think, they are of indirect utility for the automated and computationally efficient processing of natural language by machines. As starting point, we will disregard the use of intrinsic grammatical and logical structure of natural language, considering it solely from probabilistic standpoint. Definition 10: The token-level semantic space of language is probabilistic space (Ω, , ), where Ω = 1 is set of all tokens, each of which is the atomic unit with specific semantics in this language, 2Ω is the σ-algebra, is the probability measure defined on . The probability measure , which can be learned from large corpus, encodes semantics of every token in the language with intrinsic grammatical and logical structures. token sequence generated from may not be an understandable sentence for human being, because 14 TECHNICAL REPORT it may not follow grammatical and logical structures with certain probability. However, computing based directly on the probability measure is very costly and not practical. Therefore, we need to find computation efficient representation of token-level semantics. B. Token-level Semantic Vector Space It took decades of effort to finally discover that the crucial step was to transition from token-level probabilistic models to semantic models based on vector representations. The shift is favored for its computational efficiency and its remarkable effectiveness in NLP tasks [27]. However, this conclusion is drawn mainly from extensive experiments and lacks solid theoretical foundation. In this subsection, we will attempt to establish the mathematical foundations of semantic vector spaces. Definition 11: The token-level semantic vector space of language is probabilistic inner product space = (SN 1, , µ, , ), where SN 1 is (N 1)-dimensional unit sphere, each SN 1 represents semantic vector, is σ-algebra on SN 1, µ is probability measure defined on , , is an inner product. 1 s2. The squared Euclidean distance is defined as d2 If we use s1 and s2 to denote two column vectors on SN 1, the inner product can be written e(s1, s2) = s1 s22 = as s1, s2 = sT (s1 s2)T (s1 s2). The cosine similarity is defined as cos(s1, s2) = sT 1 s2. It is noticed that Ω in Definition 10 can only be mapped to points in SN 1. Let the set of semantic vector of tokens in be S(A) SN 1 with Ω. Thus, µ is an extension from such that µ(S(A)) = (A) if FS, otherwise µ(S(A)) = 0. Many works suggest that the semantic vector space should be more complex low dimensional manifold. In practice, however, the Euclidean distance and cosine similarity remain the most widely used metrics, because of its simplicity in computation and adequate performance. Therefore, we argue that defining the semantic vector space directly on SN 1 strikes an effective trade-off between accuracy and computational efficiency. The essential purpose of representing tokens as vectors is to use the cosine similarity between these high dimension vectors to represent semantic differences. The simple algebraic operations on vectors may not always work, because they do not necessarily reflect semantic relationships. For example, the conceptual illustration in the following may work for some tokens, but not apply to every token [23]: s(King) s(Men) + s(Woman) s(Queen). (33) However, this example effectively demonstrates projection do exist between the vector representations of King and Men. Consequently, scalars alone are insufficient to fully BAI: FORGET BIT, IT IS ALL ABOUT TOKEN: TOWARDS THE SEMANTIC INFORMATION THEORY OF LLMS 15 characterize the semantic relations. Moreover, the cosine similarity is invariant to rotation and scaling, and much more robust than Euclidean distance in high dimension space. Thus, the cosine similarity and probability measure in are of fundamental importance. Following the idea of Gromov-Wasserstein distance [46], [47], we define the distance of two semantic vector spaces as follows: Definition 12: Let and be two semantic vector spaces with probability measures µ and ν, respectively. The squared distance between and is defined as: s(S, S) = min d2 πΠ(µ,ν) (cid:90) (cid:90) SS SS (cid:12) (cid:12)sT 1 1 sT 2 2 2 (cid:12) (cid:12) dπ(s1, s2)dπ(s 1, 2), (34) where Π(µ, ν) is the set of all transportation plans between µ and ν. The definition seeks to find an optimal transport plan π that minimizes the weighted average of the internal cosine similarity difference for all pairs of points, measured before and after the transport. The distance difference imposes high cost on pairings that distort the intrinsic geometry of two semantic vector spaces. Therefore, if ds(S, S) = 0, and are equivalent in the sense of token-level semantics, which results in an easy translation between these two languages. In fact, the Gromov-Wasserstein distance has already been successfully applied to the alignment of two word embeddings [81]. Remark 3 (Vectorization in Information Theory): The relationship between semantic space and semantic vector space is similar to the relationship between information theory and signal processing. Information theory, based on probability theory, is framework for understanding the nature and limits of information compression, transmission, and storage. However, it is not particularly concerned with the specific methods of implementation in practice [16]. Signal processing, on the other hand, represents information as vectors in Rn or Cn, making it suitable for sensing, transmission, and storage in physical media. This representation enables vast body of mathematical theory to be applied to the design of efficient algorithms for practical sensing, communication, and storage systems [82]. C. Semantic Compression/De-dimensionality In information theory, the objective of source coding is to use as few bits as possible to represent source symbol, such that the source message can be exactly recovered for lossless compression or recovered within given distortion for lossy compression [43]. According to Definition 11, however, Ω = implies SN 1 is very high dimension sphere such that the direct computation on is still not practical. Extensive experimental results suggest that the choice of dimensionality for semantic vector space involves crucial trade-off, implying the 16 TECHNICAL REPORT existence of an optimal range or sweet spot [83]. In this case, the semantic compression is the compression of the entire semantic space, i.e., dimension reduction that preserves cosine similarity. In practice, the random projection is widely used to reduce the dimensionally of vectors. The distance conservation property is guaranteed by Johnson-Lindenstrauss (JL) lemma [84]. In the following, we introduce the cosine similarity based JL lemma without proof [85]. Lemma 1: Let ϵ (0, 1) and {s1, . . . , sM } SN 1, if ϵ2 log , there exists matrix RmN such that: where = AT A. sT sj sT Psj ϵ, i, {1, . . . , }, (35) According to JL lemma, the dimensionality of the semantic vector space can be reduced from to ϵ2 log . As aforementioned, each semantic vector can be seen as real signal vector which should be very sparse in SN 1. Inspired by compressive sensing, the cosine similarity based JL lemma can be improved by applying restricted isometry property (RIP). Let be matrix satisfying (k, δ)-RIP, that is 1 δ As2 1 + δ, (36) for all k-sparse SN 1, i.e., s0 k. The following result is established in [86]. Theorem 3: Let η, ϵ (0, 1), {s1, . . . , sM } SN 1, and RmN be (k, δ)-RIP with δ ϵ/4 and 40 log 4M η . Let σ Rademacher sequence, i.e., uniformly distributed on {1, 1}N . Then, with probability exceeding 1 η, sT sj sT DσPDσsj ϵ, i, {1, . . . , }, (37) where Dσ is diagonal matrix whose diagonal entries are the elements of the vector σ and = AT A. According to the theory of compressive sensing, the partial Gaussian matrix can be used with ϵ2 log η log N, (38) but the complexity of the matrix-vector multiplication is very high. However, can also be obtained by randomly selecting rows from the discrete Fourier transform (DFT) matrix, discrete cosine transform (DCT) matrix, or Hadamard matrix. In this case, will be larger than using partial Gaussian matrix, but the complexity is greatly reduced. BAI: FORGET BIT, IT IS ALL ABOUT TOKEN: TOWARDS THE SEMANTIC INFORMATION THEORY OF LLMS Recalling Definition 12, the distortion of semantic compression can be evaluated by the distance of two semantic vector spaces. Let be the original semantic vector space on SN 1 and on Sm with 1 < 1, the distortion of semantic compression can be written as s(S, S) = min d2 πΠ(µ,µ) (cid:90) (cid:90) SS SS (cid:12)sT sT Ps(cid:12) (cid:12) (cid:12) 2 dπ(s, As)dπ(s, As), (39) where is projection matrix and = AT A. The following theorem can be established by applying Lemma 1 or Theorem 3 directly. Theorem 4: The distortion of semantic compression can be bounded by ϵ, i.e., d2 s(S, S) ϵ, with high probability. The semantic compression/de-dimensionality discussed in this subsection does not consider the distribution on semantic vector space. Therefore, the bound in Theorem 4 is not tight, yet far from optimal in the sense of information theory. Similar to rate-distortion theory, the dimension-distortion theory can be further developed for semantic compression, especially for the case of smaller than the threshold in Lemma 1 or Theorem 3. Remark 4 (Approximate Nearest Neighbor Search): Vector databases are regarded as critical piece of infrastructure for helping LLMs mitigate hallucinations. They can also store vast amounts of private and proprietary data, enhancing the capabilities of LLMs in vertical domains. Consequently, approximate nearest neighbor (ANN) vector search algorithms stand out as key technology that integrates vector databases with LLMs. From the perspective of information theory, the nearest ANN vector searching is an extension to decoding algorithm, which is to search the nearest codeword for the received symbols. Since 2023, the ANN vector search algorithms proposed by the experts from our lab have been ranked TOP-1 on ANN-Benchmarks leader-board.2 Interested researchers can access our code repository. D. Semantic Embedding/Vectorization for Next-token Prediction In practice, we typically select proper dimension to directly perform the semantic embedding or vectorization. In the following, we will discuss information-theoretically optimal approach. It is natural to understand that the semantics of an utterance highly depend on the speakers intended goal, i.e., the downstream task in machine learning. Therefore, for token sequence with length n, the semantic embedding is mapping : Ωn (Sm)n, such that loss functional L(f ), defined by the downstream task, is minimized. 2https://ann-benchmarks.com. 3https://github.com/WPJiang/HWTL_SDU-ANNS. 18 TECHNICAL REPORT From the perspective of LLMs, the objective is to predict the next token based on the prompt and the parameterized memory. Therefore, L(f ) should be designed to best facilitate of achieving this goal. Let X1:n be token sequence, S1:n be the corresponding semantic vector representation of X1:n. For the task of the next token prediction, St should contain all the information in X1:t which is useful to predict Xt+1:n. From the perspective of information theory, the optimal semantic encoder for next token prediction should be the solution of the following problem: max St=f (X1:t) I(Xt+1:n; StS1:t1), 1 N. (40) The condition means St only contains new information for predicting Xt+1:n which is not contained in S1:t1. The solution of Eq. (40) maximizes the backward directed information I(Xn:1 S1:n) as follows: (Xn:1 S1:n) = (cid:88) t=1 max St=f (X1:t) I(Xt+1:n; StS1:t1). (41) Following the inequalities of directed information in [64], we have (Xn:1 S1:n) (cid:88) t=1 max St=f (X1:t) I(Xt+1:n; St) (cid:88) nt (cid:88) t=1 k=1 max St=f (X1:t) I(Xt+k; St). (42) Inspired by the idea of predictive coding in information theory [87], [88], the CPC is proposed for semantic embedding in [48], which is also adopt in OpenAI [49]. Let Z1:n be the latent representation of X1:n with Zt = gENC(Xt), S1:n be the semantic vector obtained by CPC, which is defined as St = gAR(Z1:t1). The training process of CPC is to solve the following optimization problem: nt (cid:88) k=1 max St=f (X1:t) I(Xt+k; St). (43) Therefore, the CPC maximizes the upper-bound of (Xn:1 S1:n), which is sub-optimal semantic encoder from the perspective of information theory. In this context, the information theoretical optimal semantic embedding can be achieved, if we can optimize the backward directed information Eq. (41) or its tighter upper bound. V. AUTOREGRESSION LLMS In this section, we focus on LLMs with special architecture, i.e., AR-LLMs. The Transformer architecture and its performance can be derived from our general definition. Other LLM architectures, such as Mamba/Mamba2 and LLaDA, are also discussed. BAI: FORGET BIT, IT IS ALL ABOUT TOKEN: TOWARDS THE SEMANTIC INFORMATION THEORY OF LLMS 19 A. TV-VAR based AR-LLMs Let st with = 1, . . . , and ut with = + 1, . . . , be sample vectors of random variables St and Ut. To simplify the notation, we let ut = st for = 1, . . . , n. We then have the following definition. Definition 13: The TV-VAR based AR-LLM is defined as ut = arg softmax (cid:32) 1 Ξ uT 1:N (cid:32) t1 (cid:88) j=1 (cid:33)(cid:33) Atjuj , = + 1, . . . , T, (44) where Atj is the coefficient matrix, u1:N are all possible token vectors in S(Ω), and Ξ is the sampling temperature. In contrast to the standard VAR model [50], Atj is time-variant, which is very difficult to estimate in practice. B. Transformer Architecture Consider decomposition of Atj as follows: Atj = πtjA, (45) where is time-invariant parameter matrix, and πtj is the only time-variant scalar weight satisfying (cid:80)t j=1 πtj = 1 and πtj 0. Simple derivation yields the following theorem. Theorem 5: The Transformer is an AR-LLM with the following form ut = arg softmax (cid:32) 1 Ξ uT 1:N (cid:32) t1 (cid:88) j=1 (cid:33)(cid:33) πtjAuj , = + 1, . . . , T, (46) where πtj is the output of the softmax, that is πtj = t1Buj) exp(uT i=1 exp(uT (cid:80)t1 t1Bui) , = 1, . . . , 1. (47) Proof: Let qt, kt, and vt be sample vectors of random variables Qt, Kt, and Vt. The attention scheme in [28] implies qt = Wqut, kt = Wkut, vt = Wvut, (48) for = 1, . . . , . The output of the Transformer is ut = arg softmax (cid:32) 1 Ξ uT 1:N (cid:32) t1 (cid:88) j=1 (cid:33)(cid:33) πtjvj , = + 1, . . . , T, (49) 20 TECHNICAL REPORT where exp(qT i=1 exp(qT is the attention score. This theorem is established by letting = Wv and = 1, . . . , 1 t1kj) t1ki) πtj = (cid:80)t1 , = WT Wk. (50) (51) This theorem shows that the Transformer is equivalent to decomposition of Atj as follows: Atj = πtjA, (52) where πtj measures the semantic relevance from uj with = 1, . . . , 1 for predicting ut. In an utterance, the semantic relevance is asymmetric between different tokens. Recalling Section IV, the inner product is used to measure the correlations of token-level semantic. For the asymmetric semantic relevance in an utterance, the inner-product based bilinear form for predicting ut is introduced as follows: B(ut1, uj) = uT t1Buj, = 1, . . . , 1, and = + 1, . . . , T, (53) where = BT in general. πtj can then be assigned by using softmax as Eq. (47). According to Jaynes maximum entropy principle [89], the softmax is probability assignment on discrete sample space that maximize the entropy with the constraint on the first order moment. Therefore, the obtained estimation of the semantic relevance is the one with the maximum uncertainty, i.e., the best achievable estimation in the worst case. C. ELBO of the Transformer The performance of AR-LLM can be analyzed from variational inference perspective. Similar to [90], is introduced as latent variable defined on {1, . . . , }. πtj can then be seen as the probability that choosing the position = j. Thus, the prediction of Ut in Eq. (46) is the expectation over as follows: ut = arg softmax (cid:18) 1 Ξ uT 1:N EJQ(Un+1:t1,S1:n;{A,B}){AuJ } (cid:19) , = + 1, . . . , T, (54) where Q(jUn+1:t1, S1:n; {A, B}) = πtj, = 1, . . . , 1. (55) By applying the principle of variational inference [51], we then have the following theorems. Theorem 6: The pre-training phase of Transformer is equivalent to max A,B ELBO(Q(JU ℏ n+1:t1, S1:n; {A, B})), = + 1, . . . , T. (56) BAI: FORGET BIT, IT IS ALL ABOUT TOKEN: TOWARDS THE SEMANTIC INFORMATION THEORY OF LLMS Proof: In the pre-training phase, we will maximize the following cross-entropy loss: max Φ H(P ℏ , QΦ ) = min Φ P ℏ {log QΦ }, = + 1, . . . , T. In the optimum, we have QΦℏ = (UtUn+1:t1, S1:n; Φℏ) = (U ℏ ℏ n+1:t1, S1:n) = ℏ . (57) (58) Therefore, the pre-training phase is equivalent to solve the following optimization problem: max Φ log (U U ℏ n+1:t1, S1:n; Φ). According to the principle of variational inference, we have log (U t ℏ n+1:t1, S1:n; Φ) = log = log t1 (cid:88) j= t1 (cid:88) j=1 (U , jU ℏ n+1:t1, S1:n; Φ) (U t , jU ℏ n+1:t1, S1:n; Φ) = log JQ(U ℏ n+1:t1,S1:n;{A,B}) JQ(U ℏ n+1:t1,S1:n;{A,B}) (cid:26) log Q(jU ℏ Q(jU ℏ n+1:t1, S1:n; {A, B}) n+1:t1, S1:n; {A, B}) (cid:27) (cid:26) (U ℏ Q(JU ℏ (U ℏ Q(JU ℏ n+1:t1, S1:n) , JU ℏ n+1:t1, S1:n; {A, B}) , JU ℏ n+1:t1, S1:n; {A, B}) n+1:t1, S1:n) (cid:27) . The last term is exactly the ELBO, which can be rewritten as ELBO(Q(JU ℏ n+1:t1, S1:n; {A, B})) n+1:t1,S1:n;{A,B}){log (U ℏ =E JQ(U ℏ , JU ℏ n+1:T , S1:n)} (59) (60) (61) DKL(Q(JU ℏ n+1:t1, S1:n; {A, B})P (JU ℏ n+1:t1, S1:n)). As result, the training phase is equivalent to max A,B ELBO(Q(JU ℏ n+1:t1, S1:n; {A, B})), = + 1, . . . , T. (62) Theorem 7: The inference phase of Transformer is equivalent to max UtS(Ω) ELBO(Qt(JUn+1:t1, S1:n; {Aℏ+, Bℏ+})), = + 1, . . . , T, (63) where Aℏ+ and Bℏ+ are the parameter matrices after training. Proof: In the inference phase, Ut is chosen from S(Ω) such that log (UtUn+1:t1, S1:n; Φℏ+) (64) 22 TECHNICAL REPORT is maximized. According to the principle of variational inference, we have log (UtUn+1:t1, S1:n; Φℏ+) = log = log t1 (cid:88) j= t1 (cid:88) j=1 (Ut, jUn+1:t1, S1:n; Φℏ+) (Ut, jUn+1:t1, S1:n; Φℏ+) Q(jUn+1:t1, S1:n; {Aℏ+, Bℏ+}) Q(jUn+1:t1, S1:n; {Aℏ+, Bℏ+}) (65) = log EJQ(Un+1:t1,S1:n;{Aℏ+,Bℏ+}) EJQ(Un+1:t1,S1:n;{Aℏ+,Bℏ+}) (cid:26) log (cid:26) (Ut, JUn+1:t1, S1:n; Φℏ+) (cid:27) Q(JUn+1:t1, S1:n; {Aℏ+, Bℏ+}) (Ut, JUn+1:t1, S1:n; Φℏ+) Q(JUn+1:t1, S1:n; {Aℏ+, Bℏ+}) (cid:27) . The last term is exactly the ELBO, which can be rewritten as ELBO(Q(JUn+1:t1, S1:n; {Aℏ+, Bℏ+})) =EJQ(Un+1:t1,S1:n;{Aℏ+,Bℏ+}){log (UtJ, Un+1:t1, S1:n; Φℏ+)} (66) DKL(Q(JUn+1:t1, S1:n; {Aℏ+, Bℏ+})P (JUn+1:t1, S1:n; Φℏ+)). As result, the inference phase is equivalent to max UtS(Ω) ELBO(Q(JUn+1:t1, S1:n; {Aℏ+, Bℏ+})), = + 1, . . . , T. (67) D. Generalization Error Bound of the Transformer Rademacher complexity and Talagrands concentration inequalities are fundamental tools in statistical learning theory for analyzing the generalization error bounds of machine learning algorithms [52]. This section applies these tools to study the generalization error bound of the Transformer. Let uℏ be the ground-truth output vector at time for = + 1, . . . , , where the corresponding random variable is ℏ . Therefore, the generalization error is given By H(P (U ℏ ), Q(Ut)), (68) where (U ℏ ) is the one-shot coding, Q(Ut) is the output of the softmax function. Given t, we take samples from the Transformer output Ut, each of which is denoted as umt for = 1, . . . , . Recalling Theorem 5, the i-th entry of the logits zm is defined by zi = 1 Ξ uT (cid:32) t1 (cid:88) j=1 (cid:33) πtjAumj , = 1, . . . , N. (69) BAI: FORGET BIT, IT IS ALL ABOUT TOKEN: TOWARDS THE SEMANTIC INFORMATION THEORY OF LLMS 23 The empirical generalization error over sample set with size is given by ˆL(A, B) = 1 (cid:88) m=1 1T (uℏ mt) log 1 q(zm) = 1 (cid:88) m=1 log 1 q(zℏ m) , where q(zm) is the output of the softmax function, and q(zℏ m) = 1T (uℏ mt)q(zm). (70) (71) Theorem 8: For any δ > 0, the generalization error of the Transformer is upper bounded by H(P (U ℏ ), Q(Ut)) ˆL(A, B) + 2 2 M (cid:88) m=1 zℏ + 3 (cid:115) log 2 δ 2M , with probability at least 1 δ over the choice of samples. = + 1, . . . , T. (72) Proof: The empirical Rademacher complexity of the Transformer is given by ˆR(A, B) = Eσ (cid:40) sup A,B 1 (cid:88) m=1 σm log (cid:41) , 1 q(zℏ m) where σ is Rademacher sequence. According to Theorem 3.3 in [52], we have H(P (U ℏ ), Q(Ut)) ˆL(A, B) + 2 ˆR(A, B) + (cid:115) log 2 δ 2M . (73) (74) Because q(zm) is the output of the softmax function, ˆL(A, B) is l2-norm. According to Talagrands Lemma in [52], we have 2-Lipschitz over zℏ for ˆR(A, B) Eσ (cid:40) sup A,B 1 (cid:88) m=1 (cid:41) σmzℏ 2 (cid:88) m=1 zℏ m. (75) This theorem has been established. This result shows that the logits determines the accuracy during the inference phase. Therefore, when using quantization for inference acceleration, it is crucial to ensure that the quantization algorithm has minimal impact on the logits. E. Memory Capacity of the Transformer The statistical physics approaches, such as spin glass model and replica method, have been widely used to analyze the performance of signal processing, coding, and satisfiability (SAT) problems [91]. In series of landmark papers [53][55], Gardner investigated the memory capacity of the classical Hopfield network [92] by applying the replica method, which is referred to as Gardner capacity afterwards. 24 TECHNICAL REPORT Definition 14: Let NP be the maximum number of random patterns which can be memorized in classical Hopfield network with neurons. The generalized Gardner capacity is defined as CG = α(NP ) , (76) where α() is chosen to scale with n. It is an identity function in the original definition. As matter of fact, generalized Gardner capacity has deep connection with Shannon capacity. If the pattern here is not binary n-sequence but binary n-sphere, the Gardner capacity is equivalent to Shannon capacity, where α() is chosen as logarithm function. The transformation from n-sequence to n-sphere is critical, which explains the error correction capability of modern neural networks. Recent work in [93] focused on the modern continuous Hopfield network, which is shown to be equivalent to the attention scheme. It is also proved that the memory capacity is exponential in the dimension of the space of the query and key-value patterns. Therefore, it is not surprising that large amount of patterns can be memorized by small LLM. Following this idea, we model the behavior of Transformers with associative memories using modern continuous Hopfield networks, which is used to explain the scaling law from theoretic perspective [94]. F. Semantic Information Theoretic Measure for the Transformer In Section III, we introduce semantic information theoretic measures for LLMs, such as the directed rate-distortion function in the pre-training phase, the directed rate-reward function in the post-training phase, and the semantic information flow in inference phase, where the key is to estimate the directed information. The directed information I(S1:n Un+1:t; Φ) can be represented by KL divergence as follows I(S1:n Un+1:t; Φ) = DKL (S1:n, Un+1,t)P (S1:n) (cid:32) (cid:33) (UjUn+1:j) . (77) (cid:89) j=n+1 Therefore, the Donsker-Varadhan representation can be used for directed information estimation [95]. This idea is proposed and thoroughly analyzed in [96] for transfer entropy estimation, where the transformer itself is used as the estimator. BAI: FORGET BIT, IT IS ALL ABOUT TOKEN: TOWARDS THE SEMANTIC INFORMATION THEORY OF LLMS 25 G. Other Architectures To simplify the computation complexity in both training and inference phases. Various LLM architectures, such as Mamba/Mamba2 [56], [57] and LLaDA [58], have been proposed. We will discuss the relation between these new architectures and Definition 13. 1) Mamba/Mamba2: To save the computation of softmax in attention scheme, Mamba/ Mamba2 architectures are proposed and thoroughly analyzed in [56], [57]. Inspired by control theory, the discrete state space model (SSM) used in Mamba/Mamba2 is ut = Atut1 + Btst; yt = Cut. (78) Clearly, the SSM is special case of the AR-LLM in Definition 13, which exactly belongs to linear TV-VAR models [50]. The linear TV-VAR model is widely used in time series analysis for economics and finance [97], [98]. Therefore, the developed parameter estimation method may be applicable to improve the performance of Mamba/Mamba2. Because there lacks the bilinear model of semantic relevance, it is not difficult to understand that the performance of Mamba/Mamba2 could be worse than Transformer. However, the Mamba/Mamba2 architectures inspire us to consider other forms of AR-LLM which may have similar performance as Transformer but much lower computation complexity. Based on the improved Mamba2 [99], Qwen3-Next is the first LLM which implements the hybrid attention scheme.4 The Transformer, however, is different from linear TV-VAR model because πtj introduces nonlinear relation, i.e., the softmax function over bilinear form of ut and uj. 2) LLaDA: As diffusion LLM, LLaDA constitutes groundbreaking attempt to transcend the Transformer paradigm [58]. In LLaDA, it assumes many tokens in an utterance are masked, which will be predicted based on the unmasked ones. The loss function for training LLaDA is cross-entropy computed only on the masked tokens: L(Φ) = τ,U τ 1:T ,U 0 1:T (cid:40) 1 τ (cid:88) t=1 1(U τ = ) log (U 0 τ 1:T ; Φ) , (79) (cid:41) where denote the masked token. The transformer without causal mask is used as the core component to predict the masked tokens. Evidently, while LLaDA is fundamentally built upon diffusion framework, the AR-LLM remains central to the task of masked token prediction in LLaDA. 4https://qwen.ai/blog?from=research.latest-advancements-list&id=4074cca80393150c248e508aa62983f9cb7d27cd& TECHNICAL REPORT VI. CONCLUSIONS Drawing from the theory of rate-distortion function, directed information, and Granger causality, this paper aims to uncover the semantic information-theoretic principles underlying LLMs. We discussed the structure-agnostic information-theoretic measures, the token-level semantic embedding, and the general definition of AR-LLM, from which the Transformer architecture and its performance have been derived theoretically. Our theory indicates that the capabilities of current LLMs remain within the scope of Granger causality. How to achieve the counterfactual reasoning and system 2 reasoning abilities [100], [101], remains formidable challenge. Consequently, our semantic information theory framework provides lens through which many experimentally observations can be explained, which also paves the way for unlocking the full potential of LLMs. ACKNOWLEDGMENT am grateful to T. Wu, X. Niu, K. Zhang, C. Zhang, Y. Lan, Z. Zhong, B. Chen, and Q. Zhang for productive discussions. REFERENCES [1] C. Shannon, mathematical theory of communication, Bell System Technical Journal, vol. 27, no. 7, pp. 379-423, Oct. 1948. [2] W. Weaver, Recent contributions to the mathematical theory of communications, The Rockefeller Foundation, Sep. 1949. [3] R. Carnap, Empiricism, semantics, and ontology, Revue Internationale de Philosophie, no. 4, pp. 20-40, Apr. 1950. [4] R. Carnap and Y. Bar-Hillel, An outline of theory of semantic information, Massachusetts Institute of Technology, Cambridge, MA, USA, Research Laboratory of Electronics Technical Report No. 247, Oct. 1952. [5] Y. Bar-Hillel and R. Carnap, Semantic information, The British Journal for the Philosophy of Science, vol. 4, no. 14, pp. 147-157, Aug. 1953. [6] R. Carnap, Meaning and Necessity: Study in Semantics and Modal Logic, 2nd ed. Chicago, IL, USA: University of Chicago Press, 1988. [7] M. Burgin, Theory of Information: Fundamentality, Diversity and Unification. Singapore: World Scientific Publishing, 2009. [8] L. Floridi, Ed., The Routledge Handbook of Philosophy of Information. London, UK: Routledge, 2016. [9] R. Solomonoff, formal theory of inductive inference - Part 1, Information and Control, vol. 7, no. 1, pp. 1-22, Mar. 1964. [10] R. Solomonoff, formal theory of inductive inference - Part 2, Information and Control, vol. 7, no. 2, pp. 224-254, Jun. 1964. [11] R. Solomonoff, The discovery of algorithmic probability, Journal of Computer and System Sciences, vol. 55, no. 1, pp. 73-88, Aug. 1997. [12] A. Kolmogorov, Three approaches to the quantitative definition of information, International Journal of Computer Mathematics, vol. 2, no. 1-4, pp. 157-168, Jan. 1968. BAI: FORGET BIT, IT IS ALL ABOUT TOKEN: TOWARDS THE SEMANTIC INFORMATION THEORY OF LLMS 27 [13] A. Kolmogorov, Logical basis for information theory and probability theory, IEEE Trans. Inf. Theory, vol. 14, no. 5, pp. 662-664, Sep. 1968. [14] M. Hutter, Universal Artificial Intelligence: Sequential Decisions Based on Algorithmic Probability. Berlin, Germany: Springer, 2004. [15] A. Shen, V. Uspensky, and N. Vereshchagin, Kolmogorov Complexity and Algorithmic Randomness. Providence, RI, USA: American Mathematical Society, 2022. [16] T. Cover and J. Thomas, Elements of Information Theory, 2nd ed. Hoboken, NJ, USA: John Wiley & Sons, 2006. [17] B. Poole, S. Ozair, A. Oord, A. Alemi, and G. Tucker, On variational bounds of mutual information, in Proc. 36th ICML 19, Long Beach, CA, USA: ICML, Jun. 2019. [18] R. Sutton, The bitter lesson, University of Alberta, Edmonton, Canada, Mar. 2019. [19] H. Luhn, new method of recording and searching information, American Documentation, vol. 4, no. 1, pp. 14-16, Jan. 1953. [20] G. Salton, A. Wong, and C. Yang, vector space model for automatic indexing, Commun. ACM, vol. 18, no. 11, pp. 613-620, Nov. 1975. [21] Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin, neural probabilistic language model, J. Machine Learn. Res., vol. 3, pp. 1137-1155, 2003. [22] T. Mikolov, K. Chen, G. Corrado, and J. Dean, Efficient estimation of word representations in vector space, arXiv: 1301.3781, Sep. 2013. [23] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean, Distributed representations of words and phrases and their compositionality, in Proc. 27th NIPS 13, Lake Tahoe, NV, USA, Dec. 2013. [24] J. Pennington, R. Socher, and C. Manning, GloVe: Global vectors for word representation, in Proc. ACL EMNLP 14, Doha, Qatar, Oct. 2014. [25] P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov, Enriching word vectors with subword information, Transactions of the Association for Computational Linguistics, vol. 5, pp. 135-146, 2017. [26] M. Peters et al., Deep contextualized word representations, in Proc. ACL NAACL-HLT 18, New Orleans, LA, USA, Jun. 2018. [27] D. Jurafsky and J. Martin, Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition with Language Models, 3rd ed. Draft, 2025. [28] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. Gomez, Ł. Kaiser, and I. Polosukhin, Attention is all you need, in Proc. 31st NIPS 17, Long Beach, CA, USA, 4-9 Dec. 2017. [29] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, Improving language understanding by generative pretraining, OpenAI, Jun. 2018. [30] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, Language models are unsupervised multitask learners, OpenAI, Feb. 2019. [31] T. Brown et al., Language models are few-shot learners, in Proc. 34th NeurIPS 20, Virtual Conference, 6-12 Dec. 2020. [32] L. Ouyang et al., Training language models to follow instructions with human feedback, arXiv: 2203.02155, Mar. 2022. [33] D. Guo et al., DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning, Nature, vol. 645, no. 8081, pp. 633-638, Sep. 2025. [34] DeepSeek-V3.2-Exp: Boosting long-context efficiency with DeepSeek sparse attention, DeepSeek, Hangzhou, China, Sep. 2025. [35] Y. Polyanskiy and Y. Wu, Information Theory: From Coding to Learning. Cambridge, UK: Cambridge University Press, 2025. 28 TECHNICAL REPORT [36] R. Shwartz-Ziv and N. Tishby, Opening the black box of deep neural networks via information, arXiv: 1703.00810, Apr. 2017. [37] C. Shani, D. Jurafsky, Y. LeCun, and R. Shwartz-Ziv, From tokens to thoughts: How LLMs and humans trade compression for meaning, arXiv: 2505.17117, Jun. 2025. [38] T. Weissman, Toward textual transform coding, IEEE BITS Inform. Theory Mag., vol. 3, no. 2, pp. 32-40, Jun. 2023. [39] X. Niu, B. Bai, N. Guo, W. Zhang, and W. Han, Rate-distortion-perception trade-off in information theory, generative models, and intelligent communications, Entropy, vol. 27, no. 4, Apr. 2025. [40] B. Geshkovski, C. Letrouit, Y. Polyanskiy, and P. Rigollet, mathematical perspective on transformers, arXiv: 2312.10794, Aug. 2025. [41] M. Rodrigues and Y. Eldar, Information-Theoretic Methods in Data Science. Cambridge, UK: Cambridge University Press, 2021. [42] J. Massey, Causality, feedback and directed information, in Proc. IEEE ISIT 90, Waikiki, HI, USA, Nov. 1990. [43] T. Berger, Rate Distortion Theory: Mathematical Basis for Data Compression. Englewood Cliffs, NJ, USA: Prentice Hall PTR, 1971. [44] R. Sutton and A. Barto, Reinforcement Learning: An Introduction, 2nd ed. Cambridge, MA, USA: The MIT Press, 2018. [45] C. Granger, Testing for causality: personal viewpoint, Journal of Economic Dynamics and Control, vol. 2, no. 1, pp. 329-352, Jan. 1980. [46] M. Gromov, Metric Structures for Riemannian and Non-Riemannian Spaces. Boston, MA, USA: Birkhäuser, 2007. [47] C. Villani, Optimal Transport: Old and New. New York, NY, USA: Springer, 2009. [48] A. Oord, Y. Li, and O. Vinyals, Representation learning with contrastive predictive coding, arXiv: 1807.03748, Jan. 2019. [49] A. Neelakantan et al., Text and code embeddings by contrastive pre-training, arXiv: 2201.10005, Jan. 2022. [50] H. Lütkepohl, New Introduction to Multiple Time Series Analysis. Berlin, Germany: Springer, 2007. [51] M. Wainwright and M. Jordan, Graphical models, exponential families, and variational inference, Foundation and Trends in Machine Learning, vol. 1, no. 1-2, pp. 1-305, Nov. 2008. [52] M. Mohri, A. Rostamizadeh, and A. Talwalkar, Foundations of Machine Learning, 2nd ed. Cambridge, MA, USA: The MIT Press, 2018. [53] E. Gardner, The space of interactions in neural network models, J. Phys. A: Math. Gen., vol. 21, no. 1, pp. 257-270, Jan. 1988. [54] E. Gardner and B. Derrida, Optimal storage properties of neural network models, J. Phys. A: Math. Gen., vol. 21, no. 1, pp. 271-284, Jan. 1988. [55] E. Gardner and B. Derrida, Three unfinished works on the optimal storage capacity of networks, J. Phys. A: Math. Gen., vol. 22, no. 12, pp. 1983-1994, Jun. 1989. [56] A. Gu and T. Dao, Mamba: Linear-time sequence modeling with selective state spaces, arXiv: 2312.00752, May 2024. [57] T. Dao and A. Gu, Transformers are SSMs: Generalized models and efficient algorithms through structured state space duality, arXiv: 2405.21060, May 2024. [58] S. Nie et al., Large language diffusion models, arXiv: 2502.09992, Feb. 2025. [59] R. Blahut, Computation of channel capacity and rate-distortion functions, IEEE Trans. Inf. Theory, vol. 18, no. 4, pp. 460-473, Jul. 1972. [60] S. Arimoto, An algorithm for computing the capacity of arbitrary discrete memoryless channels, IEEE Trans. Inf. Theory, vol. 18, no. 1, pp. 14-20, Jan. 1972. BAI: FORGET BIT, IT IS ALL ABOUT TOKEN: TOWARDS THE SEMANTIC INFORMATION THEORY OF LLMS 29 [61] S. Wu, W. Ye, H. Wu, H. Wu, W. Zhang, and B. Bai, communication optimal transport approach to the computation of rate distortion functions, arXiv: 2212.10098, Dec. 2022. [62] L. Chen et al., constrained BA algorithm for rate-distortion and distortion-rate functions, arXiv: 2305.02650, Jan. 2024. [63] C. Chen et al., Computation of rate-distortion-perception functions with Wasserstein barycenter, in Proc. IEEE ISIT 23, Taipei, Taiwan, Jun. 2023. [64] G. Kramer, Directed information for channels with feedback, Ph. Dissertation, ETH Zurich, Zurich, Switzerland, 1998. [65] R. Dobrushin, General formulation of Shannons main theorem in information theory, American Mathematical Society Translations: Series 2, vol. 33, no. 2, pp. 323-438, 1963. [66] I. Naiss and H. Permuter, Extension of the Blahut-Arimoto algorithm for maximizing directed information, IEEE Trans. Inf. Theory, vol. 59, no. 1, pp. 204-222, Jan. 2013. [67] M. Belghazi et al., MINE: Mutual information neural estimation, arXiv: 1801.04062, Aug. 2021. [68] D. Tsur, Z. Aharoni, Z. Goldfeld, and H. Permuter, Neural estimation and optimization of directed information over continuous spaces, IEEE Trans. on Inf. Theory, vol. 69, no. 8, pp. 4777-4798, Aug. 2023. [69] V. Strassen, Asymptotische abschätzungen in Shannons informationstheorie, in Proc. Trans. 3rd Prague Conf. Inf. Theory 62, Prague, Czech Republic, 1962. [70] P. Amblard and O. Michel, The relation between Granger causality and directed information theory: review, Entropy, vol. 15, no. 1, pp. 113-143, Jan. 2013. [71] T. Schreiber, Measuring information transfer, Phys. Rev. Lett., vol. 85, no. 2, pp. 461-464, Jul. 2000. [72] L. Barnett, A. Barrett, and A. Seth, Granger causality and transfer entropy are equivalent for Gaussian variables, Phys. Rev. Lett., vol. 103, no. 23, p. 238701, Dec. 2009. [73] D. Gençaga, Ed., Transfer entropy, Entropy, vol. 20, no. 4, p. 288, Apr. 2018. [74] J. Pearl, Causality: Models, Reasoning, and Inference, 2nd ed. New York, NY, USA: Cambridge University Press, 2009. [75] P. Grünwald and P. Vitányi, Shannon information and Kolmogorov complexity, arXiv: cs/0410002, Jul. 2010. [76] S. Amari, Information Geometry and Its Applications, Tokyo, Japan: Springer, 2016. [77] J. Martens and R. Grosse, Optimizing neural networks with Kronecker-factored approximate curvature, in Proc. 32nd ICML 15, Lille, France: ICML, Jul. 2015. [78] R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. Manning, and C. Finn, Direct preference optimization: Your language model is secretly reward model, arXiv: 2305.18290, Jul. 2024. [79] D. Freedman, On tail probabilities for martingales, The Annals of Probability, vol. 3, no. 1, pp. 100-118, Feb. 1975. [80] D. Williams, Probability with Martingales. Cambridge, UK: Cambridge University Press, 1991. [81] D. Alvarez-Melis and T. Jaakkola, Gromov-Wasserstein alignment of word embedding spaces, in Proc. ACM EMNLP 18, Brussels, Belgium, Oct. 2018. [82] A. Lapidoth, Foundation in Digital Communication. New York, NY, USA: Cambridge University Press, 2009. [83] T. Landauer, P. Foltz, and D. Laham, An introduction to latent semantic analysis, Discourse Processes, vol. 25, no. 2-3, pp. 259-284, Jan. 1998. [84] W. Johnson, J. Lindenstrauss, and G. Schechtman, Extensions of Lipschitz maps into Banach spaces, Israel J. Math., vol. 54, no. 2, pp. 129-138, Jun. 1986. [85] S. Foucart and H. Rauhut, Mathematical Introduction to Compressive Sensing. New York, NY, USA: Birkhäuser, 2013. [86] F. Krahmer and R. Ward, New and improved Johnson-Lindenstrauss embeddings via the restricted isometry property, SIAM J. Math. Anal., vol. 43, no. 3, pp. 1269-1281, Jan. 2011. 30 TECHNICAL REPORT [87] P. Elias, Predictive coding - Part 1, IRE Trans. Inf. Theory, vol. 1, no. 1, pp. 16-24, Mar. 1955. [88] P. Elias, Predictive coding - Part 2, IRE Trans. Inf. Theory, vol. 1, no. 1, pp. 24-33, Mar. 1955. [89] E. Jaynes, Probability Theory: The Logic of Science. New York, NY, USA: Cambridge University Press, 2003. [90] Y. Kim, C. Denton, L. Hoang, and A. Rush, Structured attention networks, arXiv: 1702.00887, Feb. 2017. [91] N. Macris and R. Urbanke, Statistical Physics for Communications, Signal Processing, and Computer Science. Lausanne, Swiss: École Polytechnique Fédérale de Lausanne, 2017. [92] J. Hopfield, Neural networks and physical systems with emergent collective computational abilities, Proceedings of the National Academy of Sciences, vol. 79, no. 8, pp. 2554-2558, Apr. 1982. [93] H. Ramsauer et al., Hopfield networks is all you need, arXiv: 2008.02217, Apr. 2021. [94] X. Niu, B. Bai, L. Deng, and W. Han, Beyond scaling laws: Understanding transformer performance with associative memory, arXiv: 2405.08707, 14 May 2024. [95] M. Donsker and S. Varadhan, Asymptotic evaluation of certain markov process expectations for large time, IV, Comm. Pure Appl. Math., vol. 36, no. 2, pp. 183-212, Mar. 1983. [96] O. Luxembourg, D. Tsur, and H. Permuter, TREET: Transfer entropy estimation via transformers, arXiv:2402.06919, Jul. 2025. [97] T. Lubik and C. Matthes, Time-varying parameter vector autoregressions: Specification, estimation, and an application, Economic Quarterly, vol. 101, no. 4, pp. 323-352, Q4 2015. [98] J. Haslbeck, L. Bringmann, and L. Waldorp, tutorial on estimating time-varying vector autoregressive models, Multivariate Behavioral Research, vol. 56, no. 1, pp. 120-149, Jan. 2021. [99] S. Yang, J. Kautz, and A. Hatamizadeh, Gated delta networks: Improving Mamba2 with delta rule, arXiv: 2412.06464, Mar. 2025. [100] J. Pearl and D. Mackenzie, The Book of Why: The New Science of Cause and Effect. New York, NY, USA: Basic Books, 2018. [101] D. Kahneman, Thinking, Fast and Slow. New York, NY, USA: Farrar, Straus and Giroux, 2013."
        }
    ],
    "affiliations": [
        "Theory Lab - Leibniz, Central Research Institute, 2012 Labs, Huawei Technology Co., Ltd., Hong Kong"
    ]
}