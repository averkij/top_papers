{
    "paper_title": "Rectified Point Flow: Generic Point Cloud Pose Estimation",
    "authors": [
        "Tao Sun",
        "Liyuan Zhu",
        "Shengyu Huang",
        "Shuran Song",
        "Iro Armeni"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Rectified Point Flow, a unified parameterization that formulates pairwise point cloud registration and multi-part shape assembly as a single conditional generative problem. Given unposed point clouds, our method learns a continuous point-wise velocity field that transports noisy points toward their target positions, from which part poses are recovered. In contrast to prior work that regresses part-wise poses with ad-hoc symmetry handling, our method intrinsically learns assembly symmetries without symmetry labels. Together with a self-supervised encoder focused on overlapping points, our method achieves a new state-of-the-art performance on six benchmarks spanning pairwise registration and shape assembly. Notably, our unified formulation enables effective joint training on diverse datasets, facilitating the learning of shared geometric priors and consequently boosting accuracy. Project page: https://rectified-pointflow.github.io/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 2 8 2 5 0 . 6 0 5 2 : r Rectified Point Flow: Generic Point Cloud Pose Estimation Tao Sun Stanford University Liyuan Zhu Stanford University Shengyu Huang NVIDIA Research Shuran Song Stanford University Iro Armeni Stanford University"
        },
        {
            "title": "Abstract",
            "content": "We present Rectified Point Flow, unified parameterization that formulates pairwise point cloud registration and multi-part shape assembly as single conditional generative problem. Given unposed point clouds, our method learns continuous point-wise velocity field that transports noisy points toward their target positions, from which part poses are recovered. In contrast to prior work that regresses partwise poses with ad-hoc symmetry handling, our method intrinsically learns assembly symmetries without symmetry labels. Together with self-supervised encoder focused on overlapping points, Rectified Point Flow achieves new state-of-the-art performance on six benchmarks spanning pairwise registration and shape assembly. Notably, our unified formulation enables effective joint training on diverse datasets, facilitating the learning of shared geometric priors and consequently boosting accuracy. Our code and models are available at https:// rectified-pointflow.github.io/ ."
        },
        {
            "title": "Introduction",
            "content": "Estimating the relative poses of rigid parts from 3D point clouds for alignment is core task in computer vision and robotics, with applications spanning pairwise registration [1] and complex multi-part shape assembly [2]. In many settings, the input consists of an unordered set of partlevel point cloudswithout known correspondences, categories, or semantic labelsand the goal is to infer globally consistent configuration of poses, essentially solving multi-part (two or more) point cloud pose estimation problem. While conceptually simple, this problem is technically challenging due to the combinatorial space of valid assemblies and the prevalence of symmetry and part interchangeability in real-world shapes [3, 4, 5]. Despite sharing the goal of recovering 6-DoF transformations, different 3D reasoning taskssuch as object pose estimation, part registration, and shape assemblyhave historically evolved in silos, treating each part independently and relying on task-specific assumptions and architectures. For instance, object pose estimators often assume known categories or textured markers [6, 7], while part assembly algorithms may require access to canonical target shape or manual part correspondences [8]. This fragmentation has yielded solutions that perform well in narrow domains but fail to generalize across tasks, object categories, or real-world ambiguities. Among these tasks, multi-part shape assembly presents especially unique challenges. The problem is inherently under constrained: parts are often symmetric [9], interchangeable [10], or geometrically ambiguous, leading to multiple plausible local configurations. As result, conventional part-wise registration can produce flipped or misaligned configurations that are locally valid but globally inconsistent with the intended assembly. Overcoming such ambiguities requires model that can Equal contribution. Preprint. Under review. reason jointly about part identity, relative placement, and overall shape coherencewithout relying on strong supervision or hand-engineered heuristics. In this work, we revisit 3D pose regression and propose conditional generative model for generic point cloud pose estimation that casts the problem as learning continuous point-wise flow over the input geometry, effectively capturing priors over assembled shapes. More specifically, our method, Rectified Point Flow, models the motion of points from random Gaussian noise in Euclidean space toward the point clouds of assembled objects, conditioned on the unposed part point clouds. This learned flow implicitly encodes part-level transformations, enabling both discriminative pose estimation and generative shape assembly within single framework. To further instill geometric awareness of inter-part relationships, we pretrain the encoder of the conditional point clouds on large-scale 3D shape datasets using self-supervised task: predicting point-wise overlap across parts, formulated as binary classification task. While GARF [11] also highlights the value of encoder pretraining for flow model, it relies on mesh-based physical simulation [12] to generate fracture-based supervision signals. In contrast, we introduce lightweight and scalable alternative that constructs pretraining data by computing geometric overlap between parts. Our data generation is agnostic to data sources tailored for different tasksincluding part segmentation [13, 14, 15], shape assembly [12, 16, 17], and registration [18, 19]without requiring watertight mesh or simulation, an important step towards scalable pretraining for pose estimation. Our flow-based pose estimation departs from traditional pose-vector regression in three ways: (i) Geometric grounding: Rather than regressing poses directly in SE(3), we operate in Euclidean space over dense point clouds. This makes the model inherently robust to symmetries, part interchangeability, and spatial ambiguities that often challenge conventional methods; (ii) Joint shape-pose reasoning: By training to predict the final assembled point cloud, our model learns to reconstruct the shape of the object; and (iii) Scalable shape prior: We cast registration and assembly task as reconstruct the completed assembly. This unified objective lets single network learn from heterogeneous datasets, yielding scalable training and transferable geometric knowledge across standard pairwise registration, fracture reassembly, and complex furniture assembly tasks. Our main contributions are summarized as follows: We propose Rectified Point Flow, conditional generative model for generic point cloud pose estimation that addresses both pairwise registration and multi-part assembly tasks and achieves state-of-the-art performances on all the tasks. We propose generalizable pretraining strategy for learning geometric awareness over inter-part relationships across several 3D shape datasets, and formulate it as point-wise overlap prediction across parts. We show that our parameterization supports joint training across different registration tasks, boosting the performance on each individual task."
        },
        {
            "title": "2 Related Work",
            "content": "Parametrization for Pose Estimation. Euler angles and quaternions are the predominant parametrization of rotation in various pose regression tasks [20, 21, 22, 23, 24, 25, 11, 26] due to their simplicity and usability. As Euler angles and quaternions are discontinuous representations, Zhou et al. [27] proposed to represent 3D rotation with continuous representation for neural networks using 5D and 6D vectors. In contrast to directly regressing pose vectors, other methods train networks to find sparse correspondences between image pairs or point cloud pairs and extract pose vectors using Singular Value Decomposition (SVD) [28, 29, 8, 30, 31, 32]. More recently, RayDiffusion [33] proposed to represent camera poses as ray bundles, naturally suited for coupling image features and transformer architectures. DUSt3R [34] directly regresses the pointmap of each camera in global reference frame and then extracts the camera pose using RANSAC-PnP [35, 36]. Our proposed rectified point flow, extends the pointmap representation for learning generalizable pose estimation on point cloud registration and assembly. Learning-based 3D Registration. 3D registration aims to align point cloud pairs in the same reference frame by solving the relative transformation from source to target. The first line of work focuses on correspondence-based methods [37, 1, 38, 39] that first extract correspondences between 2 Figure 1: Pose Estimation via Rectified Point Flow. Our formulation supports shape assembly and Xi}i pairwise registration tasks in single framework. Given set of unposed part point clouds Ω, { ˆXi(0) Rectified Point Flow predicts each parts point cloud at the target assembled state Ω. }i { Subsequently, we solve Procrustes problem via SVD between the condition point cloud Xi and the estimated point cloud ˆXi(0) to recover the rigid transformation ˆTi for each non-anchored part. point clouds, followed by robust estimators to recover the transformation. Subsequent works [29, 40, 41, 32] advance the performance by learning more powerful features with improved architecture and loss design. The second line of work comprises direct registration methods [31, 42, 30, 22] that directly compute score matrix and apply differentiable weighted SVD to solve for the transformation. Correspondence-based methods can fail in extremely low-overlap scenarios in shape assembly and direct methods fall short in terms of pose accuracy. Our method, which directly regresses the coordinates of each point in the source point cloud, is agnostic and more generalizable to varying overlap ratios compared to direct methods. Multi-Part Registration and Assembly. Multi-part registration and shape assembly generalize pairwise relative pose estimation to multiple parts, with applications in furniture assembly [17] and shape reassembly [12]. Methods [43, 44, 26, 45, 46] tackle the multi-part registration problem by estimating the transformation for each rigid part in the scene (multi-source and multi-target). Multi-part shape assembly differs as task from registration because it has multi-source input and canonical target, and each part has almost zero overlap with each other. Chen et al. [47] adopt an adversarial learning scheme to examine the plausibility for different shape configurations. Wu et al. [48] leverage SE(3)-equivariant representation to handle pose variations in shape assembly. DiffAssembly [49] and PuzzleFusion [24, 25] leverage diffusion models to predict the transformation for each part. GARF [11] combines fracture-aware pretraining with flow matching model to predict per-part transformation. However, these approaches do not explicitly address part symmetry or interchangeability as effectively as our method. concurrent work [50] tackles multi-part assembly via equivariant flow matching, relying on an SE(3)-equivariant network and carefully designed flow trajectories to account for part symmetry; in contrast, we handle the part symmetry with simpler straight-line flow formulation in Euclidean space. Moreover, Rectified Point Flow is the first class-agnostic solution for furniture assembly on the PartNet [15] and IKEA-Manual [17] datasets."
        },
        {
            "title": "3 Pose Estimation via Rectified Point Flow",
            "content": "Rectified Point Flow addresses the multi-part point cloud pose estimation problem, defined in Sec. 3.1. The overall pipeline consists of two consecutive stages: self-supervised overlap-aware point encoding (Sec. 3.2) and conditional Rectified Point Flow (Sec. 3.3). Finally, we explain how our formulation inherently addresses the challenges posed by symmetric and interchangeable parts in Sec. E. 3 Figure 2: Encoder pre-training via overlap points prediction. Given unposed multi-part point clouds, our encoder with point-wise overlap prediction head performs binary classification to identify overlapping points. Predicted overlap points are shown in blue. For comparison, the groundtruth overlap points are visualized on the assembled object for clarity (target overlap). 3.1 Problem Definition Ω, where Ω is the Consider set of unposed point clouds of multiple object parts, }i is the number of parts, and Ni is the number of points in part i. The goal part index set, := Ω is to solve for set of rigid transformations Ω that align each part in the unposed Ti multi-part point cloud to form single, assembled object in global coordinate frame, where Xi { SE(3) }i { R3 Ni := R3 , := Xi TiXi R3 , and := Ni. (1) Ω (cid:91)i Ω (cid:91)i Ω (cid:88)i To eliminate global translation and rotation ambiguity, we set the first part (i = 0) as the anchor and define its coordinate frame as the global frame. All other parts are registered to this anchor. 3.2 Overlap-aware Point Encoding Pose estimation relies on geometric cues from mutually overlapping regions among connected parts [29, 32, 11]. In our work, we address this challenge through self-supervised pretraining approach that develops task-agnostic, overlap-aware encoder capable of producing pose-invariant point features. As illustrated in Fig. 2, we train an encoder to identify overlapping points in different SE(3) and parts. Given set of unposed parts compose transformed point clouds Xi = TiXi as input to the encoder. These data augmentations enable the encoder to learn more robust pose-invariant features. The encoder then computes per-point Rd for the j-th point on part i, after which an MLP overlap prediction head estimates features Ci,j the overlap probability ˆpi,j. The binary ground-truth label pi,j is 1 if point xi,j falls within radius ϵ of at least one point in other parts. Ω, we first apply random rigid transforms Ti Xi}i { We train both the encoder and the overlap head using binary cross-entropy loss. For objects without predefined part segmentation, we employ off-the-shelf 3D part segmentation methods to generate the necessary labels. The features extracted by our trained encoder subsequently serve as conditioning input for our Rectified Point Flow model. 3.3 Generative Modeling for Pose Estimation The overlap-aware encoder identifies potential overlap regions between parts but cannot determine their final alignment, particularly in symmetric objects that allow multiple valid assembly configurations. To address this limitation, we formulate the point cloud pose estimation as conditional generation task. With this approach, Rectified Point Flow leverages the extracted point features to sample from the conditional distribution of all feasible assembled states across multi-part point clouds, generating estimates that maximize the likelihood of the conditional input point cloud. By recasting pose estimation as generative problem, we naturally accommodate the inherent ambiguities arising from symmetry and part interchangeability in the data. Preliminaries. Rectified Flow (RF) [51, 52] is score-free generative modeling framework that learns to transform sample X(0) from source distribution, into X(1) from target distribution. The forward process is defined as linear interpolation between them with timestep as X(t) = (1 t)X(0) + tX(1), [0, 1]. (2) 4 Xi}i Figure 3: Learning Rectified Point Flow. The input to Rectified Point Flow are the condition point clouds Ω at timestep t. They are first encoded by the }i pre-trained encoder and the positional encoding, respectively. The encoded features are concatenated Ω and and passed through the flow model, which predicts per-point velocity vectors defines the flow used to predict the part point cloud in its assembled state. Ω and noised point clouds dXi(t)/dt { Xi(t) { }i { The reverse process is modeled as velocity field (t, X(t) X) conditioned on and trained using conditional flow matching (CFM) loss [53], tX(t), which is parameterized as network LCFM(V ) = Et,X (t, X(t) (cid:104) X) 2 tX(t) . (cid:105) (3) R3 Rectified Point Flow. In our method, we directly apply RF to the 3D Euclidean coordinates of the Mi denote the time-dependent point cloud for part i, where multi-part point clouds. Let Xi(t) Mi is number of sampled points. At = 0, Ω is uniformly sampled from the assembled Xi(0) { object , while at = 1, points on each part are independently sampled from Gaussian, i.e., (0, I). Then, we define the continuous flow for each part as straight-line interpolation in Xi(1) 3D Euclidean space between the points in noised and assembled states. Specifically, for each part i, (4) t)Xi(0) + tXi(1), Xi(t) = (1 [0, 1]. }i The velocity field of Rectified Point Flow is therefore, dXi(t) dt = Xi(1) Xi(0). (5) We fix the anchored part (i = 0) by setting X0(t) = X0(0) for all [0, 1], implemented via mask that zeros out the velocity for its points. Once the model predicts the assembled point cloud of each part ˆXi(0), we recover its pose Ti in Procrustes problem, ˆXi(0) (6) ˆTi = arg min ˆTi SE(3) ˆTiXi . Solving poses ˆTi for all non-anchored parts via SVD completes the pose estimation task in Eq. 1. Xi(t) { Learning Objective. We train flow model to recover the velocity field in Eq. 5, taking the noised point clouds Ω and conditioning on unposed multi-part point cloud X, as shown in Fig. 3. First, we encode using the pre-trained encoder . For each noised point cloud, we apply positional encoding to its 3D coordinates and part index, concatenate these embeddings with the point features, and feed the result into the flow model. We denote its predicted velocity field by . We optimize the flow model by the flow model for all points by (t, Ω; X) minimizing the conditional flow matching loss in Eq. 3. Xi(t) { }i }i R3 3.4 Invariance Under Rotational Symmetry and Interchangeability In our method, the straight-line point flow and point-cloud sampling, while simple, guarantee that every flow realization and its loss in Eq. (3) remain invariant under an assembly symmetry group : , we have the learning Theorem 1 ( objective in Eq. 3 following -invariance of the learning objective). For every element The formal definition of result, the flow model learns all the symmetries in hand-made data augmentation or heuristics on symmetry and interchangeability. LCFM(V ) = and the proof of Theorem 1 appear in the supplementary material. As during training, without the need for additional LCFM(g(V (t, Ω; g(X)))). Xi(t) { }i"
        },
        {
            "title": "4 Experiments",
            "content": "Implementation Details. We use PointTransformerV3 (PTv3) [54] as the backbone for point cloud encoder, and use Diffusion Transformer (DiT) [55] as our flow model. Each DiT layer applies two self-attention stages: (i) part-wise attention to consolidate part-awareness, and (ii) global attention over all part tokens to fuse information. We stabilize the attention computation by applying RMS Normalization [56, 57] to the query and key vectors per head before attention operations. We sample the time steps from U-shaped distribution following [58]. We pre-train the PTv3 encoder on all datasets with an additional subset of Objaverse [14] meshes, where we apply PartField [13] to obtain annotations. After pretraining, we freeze the weights of the encoder. We train our flow model on 8 NVIDIA A100 80GB GPUs for 400k iterations with an effective batch size of 256. We use the 4 which is halved every 25k iterations AdamW [59] optimizer with an initial learning rate 5 after the first 275k iterations. 10 Table 1: Dataset Statistics. We train our model on seven datasets with varying sizes and complexity. Dataset Task Train & Val Test # Samples # Parts # Samples # Parts IKEA-Manual [17] TwoByTwo [16] PartNet-Assembly BreakingBad [12] TUD-L [18] ModelNet 40 [19] Assembly Assembly Assembly Assembly Registration Registration Objaverse [14] Pre-training Only 84 308 23755 35114 19138 19680 [2, 19] [2, 2] [2, 64] [2, 49] [2, 2] [2, 2] [3, 12] 18 144 261 265 300 260 6794 [2, 19] [2, 2] [2, 64] [2, 49] [2, 2] [2, 2] [3, 12] 4.1 Experimental Setting Datasets. For the multi-part shape assembly task, we experiment on the BreakingBad [12], TwoByTwo [16], PartNet [15], and IKEA-Manual [17] datasets. The PartNet dataset has been processed for the shape assembly task following the same procedure as [17] but includes all object categories; we refer to this version as PartNet-Assembly. Evaluation of the pairwise registration is performed on the TUDL [18] and ModelNet 40 [19] datasets. We split all datasets into train/val/test sets following existing literature for fair comparisons. The statistics of all datasets are in Tab. 1. Evaluation Metrics. We evaluate the pose accuracy following the convention of each benchmark, with Rotation Error (RE), Translation Error (TE), Rotation Recall at 5 (Recall @ 5), and Translation Recall at 1 cm (Recall @ 1 cm). For the shape assembly task, we measure Part Accuracy (Part Acc) by computing per object the fraction of parts with Chamfer Distance under 1 cm, and then averaging those per-object scores across the dataset, following [25, 11, 30, 17]. Baseline Methods. We evaluate our method against state-of-the-art methods for pairwise registration and shape assembly. For pairwise registration, we compare against DCPNet [31], RPMNet [30], GeoTransformer [32], and Diff-RPMNet [22]. For shape assembly, we compare against MSN [47], SE(3)-Assembly [48], Jigsaw [60], PuzzleFussion++ [25], and GARF [11]. We did not include the results of [50] because it is evaluated only on samples up to 8 parts (in BreakingBad-Everyday) and their models and codes have not yet been released. We report our performance in two training configurations: dataset-specific training in which models are trained independently for each dataset (Ours (Single)), and joint training in which single model is trained on all datasets (Ours (Joint)). 4.2 Evaluation We report pose accuracy for shape assembly and pairwise registration in Tab. 2 2 and Tab. 3, respectively. Our model outperforms all existing approaches by substantial margin. For multi-part 2We found that the BreakingBad benchmark [12, 11, 25] originally computed rotation error (RE) using the RMSE of Euler angles, which is not proper metric on SO(3). To ensure consistency, we re-evaluate GARF using the geodesic distance between rotation matrices via the Rodrigues formula [61, 31, 29, 32]. 6 Figure 4: Qualitative Results on PartNet-Assembly. Columns show objects with increasing number of parts (left to right). Rows display (1) colored input point clouds of each part, (2) GARF outputs (dashed boxes indicate samples limited to 20 by GARFs design, selecting the top 20 parts by volume), (3) Rectified Point Flow outputs, and (4) ground-truth assemblies. Compared to GARF, our method produces more accurate pose estimation on most parts, especially as the number of parts increases. Table 2: Multi-part Assembly Results. Rectified Point Flow (Ours) achieves the best performance across all metrics on BreakingBad-Everyday, TwoByTwo, and PartNet-Assembly datasets. Methods BreakingBad-Everyday [12] Part Acc RE [%] [deg] TE [cm] TwoByTwo [16] TE RE [cm] [deg] MSN [47] SE(3)-Assembly [48] Jigsaw [60] PuzzleFusion++ [25] GARF [11] Ours (Single) Ours (Joint) 85.6 73.3 42.3 38.1 9.9 9.6 7. 15.7 14.8 10.7 8.0 2.0 1.8 2.0 16.0 27.5 68.9 76.2 93.0 93.5 91.1 70.3 52.3 53.3 58.2 22.1 18.7 13. 28.4 23.3 36.0 34.2 7.1 4.1 3.0 PartNet-Assembly RE [deg] 66.9 24.8 21. TE [cm] 21.9 15.4 14.8 Part Acc [%] 25.7 50.2 53. assembly, the closest competitor is GARF [11], which formulates per-part pose estimation as 6-DoF pose regression; see Figs. 4 and 5. We attribute our superior results to two key advantages of Rectified Point Flow: (i) in contrast to our closest competitor GARF [11] which performs 6-DoF pose regression, our dense shape-and-pose parametrization helps the model learn better global shape prior and fine-grained geometric details more effectively; and (ii) our generative formulation natively handles part symmetries and interchangeability. For pairwise registration, GARFdespite being retrained on target datasetsfails to generalize beyond the original task. In contrast, our method achieves new state-of-the-art performance on registration benchmarks, outperforming methods designed solely for registration (e.g., GeoTransformer [32] and Diff-RPMNet [22]) and demonstrating strong generalization across different datasets (Fig 5). We also achieve the strongest shape assembly performance on IKEA-Manual [17]; for more details on evaluation and visualizations, see supplementary. Joint Training. By recasting pairwise registration as two-part assembly task, our unified formulation enables joint training on all six datasetsincluding very small sets like TwoByTwo (308 samples) and IKEA-Manual (84 samples). Ours (Joint) consistently matches or outperforms the 7 Figure 5: Qualitative Results Across Registration and Assembly Tasks. From left to right: pairwise registration on ModelNet 40 and TUD-L, shape assembly on BreakingBad-Everyday. From top to bottom: Colored input point clouds, GARF results, ours, and ground truth (GT). Our single model performs the best across registration and assembly tasks. Table 3: Pairwise Registration Results. Rectified Point Flow (Ours) outperforms all baselines on both TUD-L and ModelNet 40, achieving the highest accuracy and lowest errors across all metrics. Methods DCPNet [31] RPMNet [30] GeoTransformer [32] GARF [11] Diff-RPMNet [22] Ours (Single) Ours (Joint) TUD-L [18] Recall @5 [%] Recall @1cm [%] ModelNet 40 [19] TE RE [unit] [deg] 23.0 73.0 88.0 53.1 90.0 97.0 97.7 4.0 89.0 97.5 52.5 98.0 98.7 99.0 11.98 1.71 1.58 42.5 1.37 0. 0.171 0.018 0.018 0.063 0.003 0.002 30%), and on BreakingBad from 9.6 to 7.4 ( dataset-specific (Ours (Single)) models. For example, on TwoByTwo the rotation error drops from 18.7 to 13.2 ( 23%), while on ModelNet40, the rotation error is reduced from 1.37 to 0.93. These results demonstrate that joint training enables the model to learn shared geometric priorssuch as part symmetries, common pose distributions, and cross-dataset correlationswhich substantially boosts performance particularly on datasets with limited training samples. Symmetry Handling. We demonstrate our models ability to handle symmetry (Sec. E) on IKEAManual [17], dataset with symmetric assembly configurations. As shown in Fig. 6, while being only trained on single configuration (left), Rectified Point Flow samples various reasonable assembly configurations (right), conditioned on the same input unposed point clouds. Note how our model permutes the 12 repetitive vertical columns and swaps the two middle baskets, yet always retains the non-interchangeable top and footed bottom baskets in their unique positions. Ablation on Self-supervised Pretraining. Tab. 4 compares four pretraining strategies for our flow-based assembly model on BreakingBadEveryday [12]. The first two encoders (MLP and 8 Figure 6: Learning from symmetric assembly. Left (1) single training sample from IKEAto right: Manual [17], and (24) three independent samples generated, conditioned on the same inputs. Parts are colorcoded consistently across plots. (Best viewed in color.) Figure 7: Two common failure types. First column: Assemblies that are geometrically plausible but mechanically nonfunctional. Second column: Objects with high geometric complexity. Table 4: Ablation on Encoder Pre-training. We ablate the impact of different pre-training tasks on the shape assembly performance. Our overlap detection pre-training yields the best results. Encoder Pre-training Task BreakingBad-Everyday [12] Part Acc RE [%] [deg] TE [cm] MLP PTv3 PTv3 PTv3 No Pre-training No Pre-training Instance Segmentation Overlap Detection (ours) 41.7 18.5 16.7 9.6 12.3 4.9 4.4 1.8 68.3 79.5 80.9 93.5 PTv3 without pre-training) are trained jointly with the flow model. The last two encoders are PTv3 pretrained on instance segmentation and our overlap-aware prediction tasks, respectively. Their pretrained weights are frozen during flow model training. We find that PTv3 is more powerful feature encoder compared to the MLP, and pretraining on instance segmentation can already extract useful features for pose estimation, while our proposed overlap-aware pretraining leads to the best accuracy. We hypothesize that, although the segmentation backbone provides strong semantic features, only our overlap prediction task explicitly encourages the encoder to learn fine-grained part interactions and pre-assembly cuescritical for precise assembly and registration."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduce Rectified Point Flow, unified flow-based framework for point cloud pose estimation across registration and assembly tasks. By modeling part poses as velocity fields, it captures fine geometry, handles symmetries and part interchangeability, and scales to varied part counts via joint training on 100K shapes. Our two-stage pipelineoverlap-aware encoding and rectified flow trainingachieves state-of-the-art results on six benchmarks. Our work opens up new directions for robotic manipulation and assembly by enabling precise, symmetry-aware motion planning. Limitations and Future Work. While our experiments focus on object-centric point clouds, realworld scenarios often involve cluttered scenes and partial observations. Moreover, while our model can generate multiple plausible assemblies, some of these may not be mechanically functional; see Fig. 7 (first column). Also, our model cannot handle objects that exceed certain geometric complexity; see Fig. 7 (second column). Another limitation arises from the number of points our model can handle, which may restrict its usage on large-scale objects. Future work will extend Rectified Point Flow to robustly handle occlusion, support scene-level and multi-body registration, incorporate object-function reasoning, and scale to objects with larger point clouds. Broader Impact. Rectified Point Flow makes it easier to build reliable 3D alignment and assembly systems directly from raw scansbenefiting robotics, digital manufacturing, AR, and heritage 9 reconstruction. However, the model can still produce incorrect, hallucinated, or nonfunctional assemblies. For safety, further work on assembly verification and error detection will be essential."
        },
        {
            "title": "Acknowledgments",
            "content": "Tao Sun is supported by the Stanford Graduate Fellowship (SGF). Liyuan Zhu is partially supported by SPIRE Stanford Student Impact Fund Grant. Shuran Song is supported by the NSF Award #2037101. The authors also thank Stanford Marlowe Cluster [62] for providing GPU resources."
        },
        {
            "title": "Supplementary Material",
            "content": "In this supplementary material, we provide the following: Model Details (Sec. A): Description of the DiT architecture and positional encoding scheme. Additional Evaluation (Sec. B): Comparison against category-specific assembly models on PartNet and IKEA-Manual, evaluation on the preservation of rigidity at the part level, and analysis of different generative formulations. Randomness in Assembly Generation (Sec. C): Investigation of the assemblies generated through noise sampling and linear interpolation in the noise space. Generalization Ability (Sec. D): Qualitative results on unseen assemblies with sameor crossand complete category parts to test the models generalization ability. Proof of Theorem 1 (Sec. E): Formal definition of the assembly symmetry group proof of the flow invariance under the group . Generalization Bounds (Sec. F): Derivation of the generalization risk guarantees and comparison with that of existing 6-DoF methods."
        },
        {
            "title": "A Model Details",
            "content": "Figure 8: Details of the DiT Block. Our flow model consists of an Encoder and position embedding (Pos. Emb.), and sequential DiT blocks (N = 6). Each block comprises Part-wise Attention, Global Attention, MLP, and AdaLayerNorm layers. DiT Architecture. Our flow model consists of 6 sequential DiT [55] blocks, each with hidden dimension of 512. For the multi-head self-attention in the DiT block, we set the number of attention heads to 8, resulting in head dimension of 64. As illustrated in Figure 8, inspired by [63], we apply separated Part-wise Attention and Global Attention operations in each DiT block to capture both intra-part and inter-part context: Part-wise Attention: Points within each part independently undergo self-attention operation, improving the models ability to capture local geometric structures. Global Attention: Subsequently, global self-attention operation is applied to all points across parts, facilitating inter-part information exchange. 10 As discussed in Sec. 4, we apply RMS normalization individually to the query and key features in each attention head before both attention operations to enhance numerical stability during training. Additionally, every DiT block employs AdaLayerNorm, layer normalization whose scaling and shifting parameters are modulated by the time step t, following [55]. Positional Encoding. We adopt multi-frequency Fourier feature mapping [64], to encode spatial information in both the condition and noised point clouds. For the j-th point in the i-th part in the condition point cloud, xi,j The 3D absolute coordinates of xi,j. The 3D surface normal ni,j at that point xi,j in the condition point cloud. The 3D absolute coordinates of the noised point cloud X(t) at the index (i, j). X, we construct 10-dimensional vector, which comprises: The scalar part index i. Each of these vectors is mapped through sinusoidal embeddings at multiple frequencies and then concatenated with the point-wise feature output of encoder . Inference. At inference time, we recover the assembled point cloud of each part by numerically integrating the predicted velocity fields (t, X) from = 1 to = 0. In practice, we Xi(t) { perform uniform Euler steps as, }i Ω ˆX(t t) = ˆX(t) }i After iterations, the resulting ˆX(0) approximates the point clouds of all parts in the assembled state. For all evaluations, we set = 20. Ω X)t, where = 1/K. (t, Xi(t) {"
        },
        {
            "title": "B Additional Evaluation",
            "content": "Comparison with Category-specific Models. We compare against category-specific point-cloud assembly methods in Table 5. All baselines are trained separately for each category, and the category labels are assumed to be known at inference time. RGL-Net [65] additionally assumes top-to-bottom ordering of the input parts. In contrast, Rectified Point Flow is class-agnostic and performs inference without any class label or part ordering. We evaluated both shape Chamfer Distance (CD) and Part Accuracy (PA) in PartNet-Assembly and IKEA-Manual, following the protocol of Huang et al. [46]. Without category or ordering assumptions like the baseline methods, our joint model still achieves the lowest CD and matches or exceeds the PA of category-specific baselines optimized for each category (chair, table, lamp). In particular, we observe relative improvement of 110.2% on Lamps PA over the strongest baseline. In IKEA-Manual, we observe that all category-specific models collapse to PA 6.9% for the Chair category. We hypothesize that the baselines architecture and hyperparameter are largely tailored to PartNet. In contrast, our joint model achieves 29.9% PA for the Chair category and 33.2% PA for all categories, over 4 times higher than any baselines. Those observations confirm that our category-agnostic cross-dataset training improves the learning of shared geometric priors far beyond any single category or dataset. Part-level Rigidity Preservation. As dense point map prediction framework, Rectified Point Flow is not explicitly trained to preserve the rigidity of each part. To quantify how well it preserves the rigidity of the parts, we first align each predicted part ˆXi(0) with the part in assembled state Xi(0) using the Kabsch algorithm. We then measure two rigidity preservation errors using (1) the Root Mean Square Error (RMSE) over all points and (2) the Overlap Ratio (OR) over all points at cm. Specifically, for part with Mi points, we compute varying thresholds τ 0.1, 0.2, 0.5, 1, 2 { RMSE = OR (τ ) = 1 Mi (cid:118) (cid:117) (cid:117) (cid:116) 1 Mi } Mi j=1 (cid:88) { (cid:12) (cid:12) ˆxi,j(0) xi,j(0) 2 and (cid:13) (cid:13) ˆxi,j(0) (cid:13) (cid:13) xi,j(0) < τ . } (cid:12) (cid:12) 11 Table 5: Comparison with Category-specific Models. We report Shape Chamfer Distance (CD) and Part Accuracy (PA) on the PartNet-Assembly and IKEA-Manual. All baselines are trained per category, whereas Rectified Point Flow is trained over all categories. (RGL-Net additionally requires top-to-bottom part ordering.) Method Known Category PartNet-Assembly IKEAManual [17] Chair Table Lamp All Chair All CD PA CD PA CD PA CD PA CD PA CD PA [%] [cm] [cm] [cm] [cm] [cm] [cm] [%] [%] [%] [%] [%] B-LSTM [66] B-Global [66] RGL-Net* [65] Huang et al. [66] Ours (Joint) 1.31 1.46 0.87 0.91 0.71 21.8 15.7 49.2 39.0 44.1 1.25 1.12 0.48 0. 0.36 28.6 15.4 54.2 49.5 49.4 0.77 0.79 0.72 0.93 0.49 20.8 22.6 37.6 33. 70.0 0.48 53.9 1.81 1.95 5.08 1. 1.49 3.5 0.9 4.0 6.9 29.9 0. 33.2 Table 6: Part-level Rigidity Preservation Evaluation. Rectified Point Flow demonstrates low shape discrepancy between condition and predicted part point clouds, measured by the Root Mean Square Error (RMSE), Relative RMSE, and Overlap Ratios (ORs) across datasets. represents the average object scale of each dataset. (Abbr: BreakingBad-E = BreakingBad-Everyday; PartNet-A = PartNet-Assembly; IKEA-M = IKEA-Manual.) Metric Object Scale [cm] [cm] RMSE [%] Relative RMSE [%] [%] [%] [%] [%] OR (τ = 0.1 cm) OR (τ = 0.2 cm) OR (τ = 0.5 cm) OR (τ = 1 cm) OR (τ = 2 cm) Shape Assembly Pairwise Registration BreakingBad-E TwoByTwo PartNet-A IKEA-M TUD-L ModelNet-40 52.1 0.76 1.5 52.3 61.7 74.9 81.4 89.5 107.7 2.46 2.3 63.8 70.8 76.8 78.7 81. 89.0 1.04 1.2 33.1 48.6 66.8 77.9 87.4 61.4 0.66 1.1 46.7 57.7 69.8 81.0 92.0 40.8 0.16 0.4 96.9 97.1 97.4 97.7 98. 70.0 0.30 0.4 95.0 96.0 96.3 96.6 97.1 SE(3) denotes the optimal rigid transform returned by Kabsch; xi,j and ˆxi,j denote the Here, j-th point on Xi(0) and on ˆXi(0), respectively. Because each part is first rigidly aligned to the ground-truth assembled state, these metrics intentionally ignore pose errors, and only measure the shape difference between the predicted and ground truth point parts. To factor in the variations in object size across datasets, we compute the average scale of an object, denoted by D, as twice the average distance from the objects center of gravity to all its points. Then, we define the Relative RMSE as RMSE / D, i.e., the RMSE normalized by the average object scale. We report these metrics averaged for all parts in each dataset in Table 6. For the pairwise registration task, Rectified Point Flow demonstrates strong rigidity preservation. On TUD-L, we obtain Relative RMSE of 0.4% and ORs above 96.9% even at the strictest τ = 0.1 cm threshold; on ModelNet-40, we achieve the same Relative RMSE of 0.4% with similar high ORs above 95.0%. Specifically, on TUD-L we record ORs of 96.9% (τ = 0.1 cm), 97.1% (τ = 0.2 cm), 97.4% (τ = 0.5 cm), 97.7% (τ = 1 cm) and 98.2% (τ = 2 cm); on ModelNet-40 the corresponding ORs are 95.0%, 96.0%, 96.3%, 96.6% and 97.1%, demonstrating consistently strong rigidity preservation. In the more challenging shape assembly task, rigidity errors remain low. Across the four datasets, the Relative RMSE ranges from 1.1% to 2.3%. At strict threshold of τ = 0.1 cm, overlap ratios (ORs) span 33.1 % (PartNet-Assembly) up to 63.8 % (TwoByTwo); By τ = 1 cm, the ORs exceed 77.9% in the four datasets (77.9%-81.4%), increasing further to 81.9%-92.0% in the more relaxed τ = 2 cm. The highest Relative RMSE and lower averaged ORs are observed in TwoByTwo, probably due to its limited training samples and lower shape similarity to other datasets, and the fact that TwoByTwo has the largest overall object scale of 107.7 cm among all datasets. In contrast, IKEA-Manual, despite having fewer training samples, benefits from shared priors in furniture objects in joint training, Figure 9: Sampling in Noise Space. For each fixed condition input point clouds, we sample four independent Gaussian noise vectors to generate distinct assembly outputs (shown in columns 25). While all samples preserve the objects structure, they show meaningful variation in part placement, orientation, and overall geometry, particularly for symmetric parts (e.g., armrests and chair bases). For comparison, the first column shows the ground-truth assemblies. delivering the lowest RMSE and high ORs at all thresholds. These results demonstrate robust rigidity preservation of Rectified Point Flow even in complex shape assembly scenarios. Note that the subsequent pose recovery stage in Rectified Point Flow further refines part poses via an SVD-based global optimization, which fits optimal poses under noises. In Sec. F, we demonstrate that this SVD-based optimization is crucial to reducing the risk limits of generalization to match the rates achieved by the existing 6-DoF methods. Overall, we empirically confirm that Rectified Point Flow generates point clouds that reliably respect the rigid structure of the conditioning parts. Ablation on Generative Formulation. As an alternative to the generative formulation of Rectified Flow (RF) in our method, we also evaluate Denoising Diffusion Probabilistic Model (DDPM) [67] using an identical DiT architecture and the pre-trained encoder. In this setup, the forward noising 4 to 0.02 over = 1000 process employs constant variances (β) that increase linearly from 10 timesteps. As shown in Table 7, the RF-based model consistently outperforms the DDPM variant on both shape assembly and pairwise registration tasks, with 35.3% lower rotation error (RE) and 11.63% lower translation error (TE). This result is in line with the findings of GARF [11]. We hypothesize that the straight-line flow in RF reduces the learning difficulty in our tasks. DDPMs frequency-based generationwhich works well for imagesmay not be as effective as RF for 3D point cloud synthesis in Euclidean space. Table 7: Generative Formulation Comparison. We compare Rectified Flow (RF) with Denoising Diffusion Probabilistic Model (DDPM) in our method, with both using the same DiT architecture and pretrained encoder. RF achieves superior performance on Rotation Error (RE) and Translation Error (TE) across all datasets. (Abbr: BreakingBad-E = BreakingBad-Everyday; PartNet-A = PartNetAssembly; IKEA-M = IKEA-Manual.) Metric Generative Formulation RE [deg] TE [cm] DDPM RF DDPM RF Shape Assembly Pairwise Registration BreakingBad-E TwoByTwo PartNet-A IKEA-M TUD-L ModelNet-40 29.5 21.8 21.3 14.8 21.4 10.8 19.2 17. 2.6 1.4 0.5 0.3 3.4 0.9 0.7 0.2 13.0 7.4 3.5 2. 17.2 13.2 10.1 3.0 13 Figure 10: Linear Interpolation in Noise Space. For different objects in each row, we fix the same conditional input and decode two independently sampled Gaussian noise vectors, Z0 (leftmost) and Z1 (rightmost), into plausible part configurations. The three center columns show outputs from the linearly interpolated noises between Z0 and Z1. We observe continuous, semantically meaningful mapping from Gaussian noise to valid assemblies."
        },
        {
            "title": "C Randomness in Assembly Generation",
            "content": "Diversity via Noise Sampling. To evaluate the diversity of assembly configurations generated by Rectified Point Flow, we sample the Gaussian noise vector multiple times for the same conditional (unposed) point cloud inputs. At inference time, we set X(1) = and run the model to obtain prediction ˆX(0). In Figure 9, each row corresponds to single final assembly: the first column shows the ground-truth assembly, and the next four columns display outputs produced by four different Gaussian noises. All generated assemblies preserve the part structure, yet exhibit meaningful variations in the parts placement and orientation, and overall geometry of the object. As expected, the model produces diverse configurations for symmetric or interchangeable parts, such as the armrests and the chair base. This shows that Rectified Point Flow effectively captures diverse conditional distribution of valid assemblies. Linear Interpolation in Noise Space. We illustrate Rectified Point Flows learned mapping from random Gaussian noise to plausible assembly configurations. In Figure 10, each row uses the same condition (unposed) point cloud, with the left and right columns showing the outputs of two randomly sampled noise vectors Z0 and Z1, respectively. The three columns in between display results generated by Z(s) which linearly interpolates between Z0 and Z1 in noise space, i.e., Z(s) := (1 s)Z0 + sZ1, where 0.25, 0.5, 0.75 (7) { . } At each interpolation step s, we run inference with X(1) = Z(s). As increases, the predicted shapes smoothly morph from the configuration induced by Z0 toward that of Z1. As shown in the first 2 rows in Figure 10, we observe smooth transitions among interchangeable parts in both examples. The 2 bottom rows in Figure 10 visualize the transitions in the overall structure of objects. In the table example, we observe gradual reduction in overall height, lowering of the horizontal beams, and more centralized positioning where the four legs meet. In the shelf example, the transformation 14 Figure 11: Generalization to Unseen Assemblies Within the Same Category: We select parts from two objects of the same category in the PartNet-Assembly test set. Parts from Object 1 are shown in blue, and parts from Object 2 in red; unselected parts are shown in gray. The results demonstrate that the model comprehends the underlying geometric structure of the category and can re-target parts to construct the final shape of the same category. is more drastic: two vertical boards become horizontal and two diagonal cables are rearranged to new vertical configuration. The above transitions across various assemblies confirm that Rectified Point Flow learns continuous mapping from Gaussian noise to semantically meaningful geometry space. Note that most of the interpolated configurations are physically plausible assemblies, creating functional objects that can stand in real-world."
        },
        {
            "title": "D Generalization Ability",
            "content": "We test the generalization ability of our model for novel assemblies under two different settings: between objects from the same (in-category) and different (cross-category) categories. Given two objects in PartNet-Assembly, we select certain parts from each of them as the input to Rectified Point Flow to test if the model can generate novel and plausible assemblies. In-category Test. As shown in Figure 11, parts selected from Object 1 are rendered in blue and those from Object 2 in red. Our model then synthesizes novel assemblies that blend and reconfigure these parts in coherent and category-consistent manner. For example, in the chair category (first two rows), the model successfully retains functional and plausible seat-back-leg structure while creatively mixing parts. In the lamp category (third row), even though the base and shade style differ significantly between objects, generated results exhibit sensible combinations that maintain structural integrity. Similarly, in the table category (last row), our method combines parts from flat-top table and lattice-style base to produce hybrid yet coherent table designs. Cross-category Test. Figure 12 highlights Rectified Point Flows ability to generalize to unseen part combinations across categories. This is particularly challenging test, since such part combination 15 Figure 12: Generalization to Unseen Assemblies Across Categories: We select parts from two objects of different categories in the PartNet-Assembly test set. Parts from Object 1 are shown in blue, and parts from Object 2 in red; unselected parts are shown in gray. The results demonstrate that the model can reason about part compositionality and re-target parts to construct plausible final shape even if some of them originate in completely different objects. may not even be possible to be assembled into meaningful object. Nevertheless, our method still demonstrates certain degree of generalization. We show two input objects from different categories, for example, monitor and chair, chair and lamp, or wall sconce and spray bottle. The results on the right demonstrate that our model can reconfigure these parts into plausible new assemblies, preserving geometric coherence. This suggests that the model has learned strong understanding of part relationship, allowing it to reason about compositionality even across category boundaries. Proof of Theorem 1 key advantage of Rectified Point Flow is that it learns both rotational symmetries of individual parts and the interchangeability of set of identical parts, without any labels of symmetry parts. Below, we first formally define an assembly symmetry group that characterizes the symmetry and interchangeability of the parts in the multi-part point cloud. Definition 1 (Assembly symmetry group). For each part SO(3) be the (finite) be the set of stabilizer of its assembled shape, i.e., RXi(0) = Xi(0) for all permutations that only permute indices of identical parts. We define the assembly symmetry group as the semidirect product Ω, let Gi Gi. Let Ω acts on every realization of the Rectified Point group element = (R1, . . . , Flow by g(Xi(t)) := Ri Xσ1(i)(t), and on network outputs of the i-th part (denoted as Vi) by g(Vi(t, g(X))) := Ri Vσ1(i)(t, g(X)). (cid:0) , σ) Ω (cid:1) = G1 Ω S. (8) Now, we show the following result that single points flow distribution is invariant under any Lemma 1 ( point cloud X, we sample flow realization: -invariance of the flow distribution). For every element . and given multi-part x(t) = tx(1) + ( t)x(0), where x(1) (0, I), x(0) X. 16 then, we have { x(t) }t [0,1] x(t) = p( { }t [0,1]). (cid:0) (cid:1) Proof. Recall that, in Rectified Point Flow, flow of single point is x(t) := (1 t)x(0) + tx(1), (0, I). Because the where x(0) endpoints of the linear interpolation are sampled independently, the PDF of the path distribution factorizes as is drawn uniformly from the assembled shape and x(1) x(t) { [0,1] }t = x(1) x(0) , (9) which indicates the randomness resides by the states = 0 and = 1 only. Because the perturbation x(1) SO(3). For p(x(0)) we distinguish two cases: (0, I) is isotropic, p(x(1)) is invariant under every rotation (cid:1) (cid:0) (cid:1) (cid:1) (cid:0) (cid:0) Rotational symmetry: If Gi, then RXi(0) = Xi(0) point-wise, so p(x(0)) = p(Rx(0)). Interchangeability. If parts and are identical, sampling first part index with probability Xj(0)). p(i) = Ni/N and then point uniformly inside it implies p(x(0) Therefore exchanging the indices (σ(i) = j, σ(j) = i) leaves p(x(0)) unchanged. Xi(0)) = p(x(0) By composing the above two properties for all parts, we complete the proof. Lemma 1 can directly lift from single points to the full multipart flow us to the Theorem 1: For every element LCFM(g(V (t, LCFM(V ) = Ω; g(X)))). Ω. This leads , we have the learning objective in Eq. 3 following Xi(t) { Xi(t) }i }i {"
        },
        {
            "title": "F Generalization Bounds",
            "content": "While the Rectified Point Flow predicts much higher-dimensional space (3Mi coordinates per part), we find that its Rademacher complexity scales exactly the same rate as the 6-DoF methods, O(1/m), where is the number of samples in the training set. Below, we compute their Rademacher complexities and empirical risks, respectively. Without loss of generality, we use the reconstruction error for the evaluation of poses, i.e., ℓ( ˆR, ˆt; R, t) = ( ˆR . First, we define hypothesis classes for both methods: R)X + ˆt (cid:13) Our Rectified Point Flow: (cid:13) (cid:13) (cid:13) Fi = Ci (cid:55) { ˆXi(0; θ) θ Θ , where ˆXi(0; θ) := Xi(1) } 1 0 (cid:90) Vi(t; C, θ)dt. Pose vector-based flow: Gi = Rademacher Complexity of Rectified Point Flow. With i.i.d. (C (k), R(k), t(k)) { cial risk k=1, we write the population risk } (h) = Ci (cid:55) . } ( ˆRi, ˆti)ϕ Φ ϕ { ℓ ˆ RD(h) = 1 (cid:88)k=1 (cid:0) ℓ h(C (k)), R(k), t(k) . (cid:2) (cid:0) (cid:1) training objects = and emprih(C), R, (cid:1)(cid:3) ˆX(0; θ) , where Pr : R3N Since our Rectified Point Flow method estimates the part pose by the Procrustes operator, i.e., ( ˆR, ˆt) = Pr SE(3) is the Procrustes operator, we have following Lipschitz contracting property. (cid:1) Property 1 (Lipschitz Contracting). Let single part, and denote σmin = σmin((X )X ). If solution ( ˆR, ˆt) = ( ˆX(0)) satisfies R3N be the centralized ground-truth point set of ε, the optimal Procrustes ˆX(0) (cid:0) ( ˆR R, ˆt t) ε σmin . (10) 17 This property directly follows from DavisKahan perturbation bounds [68] for the Top-3 singular vectors. Crucially, σmin = Ω(N )3 for well-spread point clouds, so Pr is 1 -Lipschitz map. Let Rm( L-Lipschitz map contracts Rademacher complexity ) denote the empirical Rademacher complexity on S. Because composition with Rm(Pr ) 1 Rm( ) LΘ3N 1 = (cid:16) LΘ . (cid:17) (11) Rademacher Complexity of 6DoF-based Methods. For the baseline we need only regress = 6 numbers, hence Rm( ) LΦd = (cid:16) LΦ . (cid:17) (12) Comparison of Generalization Bounds. Applying Bartlett Theorem and using (10), we obtain, with probability at least 1 δ over the samples from D, ˆf ˆ RD ˆf + 2 Rm(P ) + 3 (cid:0) (cid:0) (cid:1) (ˆg) (cid:1) ˆ RD(ˆg) + 2 Rm( are the empirical-risk minimizers on S. ) + 3 (cid:114) log(2/δ) 2m , log(2/δ) 2m , (cid:114) (FLOW) (6DOF) and ˆg where ˆf In conclusion, while Rectified Point Flow predicts much higher-dimensional space, the contraction of the SVD stage cancels this apparent over-parameterization, producing complexity term that scales at the same rate of O(1/m) as the 6-DoF baseline; (FLOW)(6DOF). As result, our method enjoys at least same generalization risk guarantees despite operating in an over-parameterized prediction space, while retaining the -invariance benefits proven in Sec. E. G"
        },
        {
            "title": "References",
            "content": "[1] A. Zeng, S. Song, M. Nießner, M. Fisher, J. Xiao, and T. Funkhouser, 3dmatch: Learning local geometric descriptors from rgb-d reconstructions, in CVPR, 2017. [2] Y. Li, A. Zeng, and S. Song, Rearrangement planning for general part assembly, in Conference on Robot Learning, 2023. [3] Y. Li, L. Jiang, Y. Liu, Y. Nie, H. Zhu, and D. Lin, Gapartnet: Graph-structured assembly from part segments, in ECCV, 2022. [4] J. Zhang, M. Wu, and H. Dong, Generative category-level object pose estimation via diffusion models, NeurIPS, vol. 36, 2024. [5] H. Zhao, S. Wei, D. Shi, W. Tan, Z. Li, Y. Ren, X. Wei, Y. Yang, and S. Pu, Learning symmetry-aware geometry correspondences for 6d object pose estimation, in ICCV, 2023. [6] T. Hodan et al., Bop challenge 2020 on 6d object localization, in ECCV Workshops, 2020. [7] B. Tekin, S. Sinha, and P. Fua, Real-time seamless single shot 6d object pose prediction, in CVPR, 2018. [8] H. Wang, S. Sridhar, J. Huang, J. Valentin, S. Song, and L. J. Guibas, Normalized object coordinate space for category-level 6d object pose and size estimation, in CVPR, 2019. [9] K. A. Murphy, C. Esteves, V. Jampani, S. Ramalingam, and A. Makadia, Implicit-pdf: Non-parametric representation of probability distributions on the rotation manifold, in ICML, 2021. [10] Y. Li, K. Mo, Y. Duan, H. Wang, J. Zhang, and L. Shao, Category-level multi-part multi-joint 3d shape assembly, in CVPR, pp. 32813291, 2024. [11] S. Li, Z. Jiang, G. Chen, C. Xu, S. Tan, X. Wang, I. Fang, K. Zyskowski, S. P. McPherron, R. Iovita, C. Feng, and J. Zhang, Garf: Learning generalizable 3d reassembly for real-world fractures, arXiv preprint arXiv:2504.05400, 2025. [12] S. Sellán, Y.-C. Chen, Z. Wu, A. Garg, and A. Jacobson, Breaking bad: dataset for geometric fracture and reassembly, NeurIPS, 2022. 3Here, Ω() denote the asymptotic rate, instead of part index set. 18 [13] M. Liu, M. A. Uy, D. Xiang, H. Su, S. Fidler, N. Sharp, and J. Gao, Partfield: Learning 3d feature fields for part segmentation and beyond, in arxiv, 2025. [14] M. Deitke, D. Schwenk, J. Salvador, L. Weihs, O. Michel, E. VanderBilt, L. Schmidt, K. Ehsani, A. Kembhavi, and A. Farhadi, Objaverse: universe of annotated 3d objects, arXiv preprint arXiv:2212.08051, 2022. [15] K. Mo, S. Zhu, A. X. Chang, L. Yi, S. Tripathi, L. J. Guibas, and H. Su, Partnet: large-scale benchmark for fine-grained and hierarchical part-level 3d object understanding, in CVPR, 2019. [16] Y. Qi, Y. Ju, T. Wei, C. Chu, L. L. Wong, and H. Xu, Two by two: Learning multi-task pairwise objects assembly for generalizable robot manipulation, CVPR, 2025. [17] R. Wang, Y. Zhang, J. Mao, R. Zhang, C.-Y. Cheng, and J. Wu, Ikea-manual: Seeing shape assembly step by step, in NeurIPS Datasets and Benchmarks Track, 2022. [18] T. Hodan, F. Michel, E. Brachmann, W. Kehl, A. GlentBuch, D. Kraft, B. Drost, J. Vidal, S. Ihrke, X. Zabulis, et al., Bop: Benchmark for 6d object pose estimation, in ECCV, 2018. [19] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao, 3d shapenets: deep representation for volumetric shapes, in CVPR, 2015. [20] J. L. Schönberger and J.-M. Frahm, Structure-from-motion revisited, in CVPR, 2016. [21] Z. Zhu, S. Peng, V. Larsson, W. Xu, H. Bao, Z. Cui, M. R. Oswald, and M. Pollefeys, Nice-slam: Neural implicit scalable encoding for slam, in CVPR, 2022. [22] H. Jiang, M. Salzmann, Z. Dang, J. Xie, and J. Yang, Se (3) diffusion model-based point cloud registration for robust 6d object pose estimation, in NeurIPS, 2023. [23] L. Zhu, Y. Li, E. Sandström, S. Huang, K. Schindler, and I. Armeni, Loopsplat: Loop closure by registering 3d gaussian splats, in 3DV, 2025. [24] S. Hosseini, M. A. Shabani, S. Irandoust, and Y. Furukawa, Puzzlefusion: Unleashing the power of diffusion models for spatial puzzle solving, in NeurIPS, 2023. [25] Z. Wang, J. Chen, and Y. Furukawa, Puzzlefusion++: Auto-agglomerative 3d fracture assembly by denoise and verify, in ICLR, 2025. [26] L. Zhu, S. Huang, and I. A. Konrad Schindler, Living scenes: Multi-object relocalization and reconstruction in changing 3d environments, in CVPR, 2024. [27] Y. Zhou, C. Barnes, J. Lu, J. Yang, and H. Li, On the continuity of rotation representations in neural networks, in CVPR, 2019. [28] P.-E. Sarlin, D. DeTone, T. Malisiewicz, and A. Rabinovich, Superglue: Learning feature matching with graph neural networks, in CVPR, 2020. [29] S. Huang, Z. Gojcic, M. Usvyatsov, and K. S. Andreas Wieser, Predator: Registration of 3d point clouds with low overlap, in CVPR, 2021. [30] Z. J. Yew and G. H. Lee, Rpm-net: Robust point matching using learned features, in CVPR, 2020. [31] Y. Wang and J. M. Solomon, Deep closest point: Learning representations for point cloud registration, in ICCV, 2019. [32] Z. Qin, H. Yu, C. Wang, Y. Guo, Y. Peng, and K. Xu, Geometric transformer for fast and robust point cloud registration, in CVPR, 2022. [33] J. Y. Zhang, A. Lin, M. Kumar, T.-H. Yang, D. Ramanan, and S. Tulsiani, Cameras as rays: Pose estimation via ray diffusion, in ICLR, 2024. [34] S. Wang, V. Leroy, Y. Cabon, B. Chidlovskii, and J. Revaud, Dust3r: Geometric 3d vision made easy, in CVPR, 2024. [35] M. A. Fischler and R. C. Bolles, Random sample consensus: paradigm for model fitting with applications to image analysis and automated cartography, Communications of the ACM, 1981. [36] V. Lepetit, F. Moreno-Noguer, and P. Fua, Ep p: An accurate (n) solution to the problem, IJCV, 2009. [37] C. Choy, J. Park, and V. Koltun, Fully convolutional geometric features, in ICCV, pp. 89588966, 2019. [38] H. Deng, T. Birdal, and S. Ilic, Ppfnet: Global context aware local features for robust 3d point matching, in CVPR, 2018. [39] Z. Gojcic, C. Zhou, J. D. Wegner, and A. Wieser, The perfect match: 3d point cloud matching with smoothed densities, in CVPR, pp. 55455554, 2019. [40] X. Bai, Z. Luo, L. Zhou, H. Fu, L. Quan, and C.-L. Tai, D3feat: Joint learning of dense detection and description of 3d local features, arXiv:2003.03164 [cs.CV], 2020. 19 [41] H. Wang, Y. Liu, Z. Dong, and W. Wang, You only hypothesize once: Point cloud registration with rotation-equivariant descriptors, in ACM International Conference on Multimedia, 2022. [42] Y. Wang and J. M. Solomon, Prnet: Self-supervised learning for partial-to-partial registration, NeurIPS, 2019. [43] J. Huang, H. Wang, T. Birdal, M. Sung, F. Arrigoni, S. Hu, and L. J. Guibas, Multibodysync: Multi-body segmentation and motion estimation via 3d scan synchronization, in CVPR, 2021. [44] C. Deng, J. Lei, B. Shen, K. Daniilidis, and L. Guibas, Banana: banach fixed-point network for pointcloud segmentation with inter-part equivariance, in NeurIPS, 2023. [45] M. Atzmon, J. Huang, F. Williams, and O. Litany, Approximately piecewise e(3) equivariant point networks, in ICLR, 2024. [46] S. Huang, Z. Gojcic, J. Huang, A. Wieser, and K. Schindler, Dynamic 3d scene analysis by point cloud accumulation, in ECCV, 2022. [47] Y.-C. Chen, H. Li, D. Turpin, A. Jacobson, and A. Garg, Neural shape mating: Self-supervised object assembly with adversarial shape priors, in CVPR, 2022. [48] R. Wu, C. Tie, Y. Du, Y. Zhao, and H. Dong, Leveraging se (3) equivariance for learning 3d geometric shape assembly, in ICCV, 2023. [49] G. Scarpellini, S. Fiorini, F. Giuliari, P. Morerio, and A. Del Bue, Diffassemble: unified graph-diffusion model for 2d and 3d reassembly, arXiv preprint arXiv:2402.19302, 2024. [50] Z. Wang, N. Xue, and R. Jörnsten, Equivariant flow matching for point cloud assembly, arXiv preprint arXiv:2505.21539, 2025. [51] X. Liu, C. Gong, and Q. Liu, Flow straight and fast: Learning to generate and transfer data with rectified flow, arXiv preprint arXiv:2209.03003, 2022. [52] Q. Liu, Rectified flow: marginal preserving approach to optimal transport, 2022, URL https://arxiv. org/abs/2209.14577. [53] Y. Lipman, R. T. Chen, H. Ben-Hamu, M. Nickel, and M. Le, Flow matching for generative modeling, arXiv preprint arXiv:2210.02747, 2022. [54] X. Wu, L. Jiang, P.-S. Wang, Z. Liu, X. Liu, Y. Qiao, W. Ouyang, T. He, and H. Zhao, Point transformer v3: Simpler, faster, stronger, in CVPR, 2024. [55] W. Peebles and S. Xie, Scalable diffusion models with transformers, arXiv preprint arXiv:2212.09748, 2022. [56] B. Zhang and R. Sennrich, Root mean square layer normalization, NeurIPS, vol. 32, 2019. [57] P. Esser, S. Kulal, A. Blattmann, R. Entezari, J. Müller, H. Saini, Y. Levi, D. Lorenz, A. Sauer, F. Boesel, et al., Scaling rectified flow transformers for high-resolution image synthesis, in ICML, 2024. [58] S. Lee, Z. Lin, and G. Fanti, Improving the training of rectified flows, vol. 37, pp. 6308263109, 2024. [59] I. Loshchilov and F. Hutter, Decoupled weight decay regularization, arXiv preprint arXiv:1711.05101, 2017. [60] J. Lu, Y. Sun, and Q. Huang, Jigsaw: Learning to assemble multiple fractured objects, NeurIPS, 2023. [61] Wikipedia contributors, Rodrigues rotation formulaWikipedia, The Free Encyclopedia. https: //en.wikipedia.org/wiki/Rodrigues%27_rotation_formula, 2025. [Online; accessed 11 May 2025]. [62] C. Kapfer, K. Stine, B. Narasimhan, C. Mentzel, and E. Candes, Marlowe: Stanfords gpu-based computational instrument, Jan. 2025. [63] J. Wang, M. Chen, N. Karaev, A. Vedaldi, C. Rupprecht, and D. Novotny, Vggt: Visual geometry grounded transformer, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. [64] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng, Nerf: Representing scenes as neural radiance fields for view synthesis, Communications of the ACM, vol. 65, no. 1, pp. 99106, 2021. [65] A. Narayan, R. Nagar, and S. Raman, Rgl-net: recurrent graph learning framework for progressive part assembly, in WACV, 2022. [66] G. Zhan, Q. Fan, K. Mo, L. Shao, B. Chen, L. J. Guibas, H. Dong, et al., Generative 3d part assembly via dynamic graph learning, NeurIPS, 2020. [67] J. Ho, A. Jain, and P. Abbeel, Denoising diffusion probabilistic models, NeurIPS, 2020. [68] C. Davis and W. M. Kahan, The rotation of eigenvectors by perturbation. iii, SIAM Journal on Numerical Analysis, vol. 7, no. 1, pp. 146, 1970."
        }
    ],
    "affiliations": [
        "NVIDIA Research",
        "Stanford University"
    ]
}