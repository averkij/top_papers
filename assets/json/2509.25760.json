{
    "paper_title": "TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning",
    "authors": [
        "Zhepei Wei",
        "Xiao Yang",
        "Kai Sun",
        "Jiaqi Wang",
        "Rulin Shao",
        "Sean Chen",
        "Mohammad Kachuee",
        "Teja Gollapudi",
        "Tony Liao",
        "Nicolas Scheffer",
        "Rakesh Wanga",
        "Anuj Kumar",
        "Yu Meng",
        "Wen-tau Yih",
        "Xin Luna Dong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While large language models (LLMs) have demonstrated strong performance on factoid question answering, they are still prone to hallucination and untruthful responses, particularly when tasks demand information outside their parametric knowledge. Indeed, truthfulness requires more than accuracy -- models must also recognize uncertainty and abstain when unsure to avoid hallucinations. This presents a fundamental challenge for existing methods: approaches that optimize for accuracy often amplify hallucinations, while those that encourage abstention can become overly conservative, sacrificing correct answers. Both extremes ultimately compromise truthfulness. In this work, we present TruthRL, a general reinforcement learning (RL) framework that directly optimizes the truthfulness of LLMs. Specifically, we implement TruthRL using GRPO with a simple yet effective ternary reward that distinguishes correct answers, hallucinations, and abstentions. It incentivizes models to reduce hallucinations not only by providing correct responses, but also by enabling abstention when uncertain, thereby improving truthfulness. Extensive experiments across four knowledge-intensive benchmarks show that, compared to vanilla RL, TruthRL significantly reduces hallucinations by 28.9% and improves truthfulness by 21.1%, with consistent gains across various backbone models (e.g., Qwen, Llama) under both retrieval and non-retrieval setups. In-depth ablation study demonstrates that vanilla accuracy-driven methods, such as supervised fine-tuning or RL with a binary reward, struggle to balance factual correctness and uncertainty. In contrast, our proposed truthfulness-driven TruthRL achieves strong performance in both accuracy and truthfulness, underscoring the importance of learning objective design for developing truthful LLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 0 6 7 5 2 . 9 0 5 2 : r TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning Zhepei Wei1,2,, Xiao Yang2, Kai Sun2, Jiaqi Wang2, Rulin Shao3,4, Sean Chen2, Mohammad Kachuee2, Teja Gollapudi2, Tony Liao2, Nicolas Scheffer2, Rakesh Wanga2, Anuj Kumar2, Yu Meng1, Wen-tau Yih4, Xin Luna Dong2 1University of Virginia, 2Meta Reality Labs, 3University of Washington, 4FAIR at Meta Work done at Meta While large language models (LLMs) have demonstrated strong performance on factoid question answering, they are still prone to hallucination and untruthful responses, particularly when tasks demand information outside their parametric knowledge. Indeed, truthfulness requires more than accuracymodels must also recognize uncertainty and abstain when unsure to avoid hallucinations. This presents fundamental challenge for existing methods: approaches that optimize for accuracy often amplify hallucinations, while those that encourage abstention can become overly conservative, sacrificing correct answers. Both extremes ultimately compromise truthfulness. In this work, we present TruthRL, general reinforcement learning (RL) framework that directly optimizes the truthfulness of LLMs. Specifically, we implement TruthRL using GRPO with simple yet effective ternary reward that distinguishes correct answers, hallucinations, and abstentions. It incentivizes models to reduce hallucinations not only by providing correct responses, but also by enabling abstention when uncertain, thereby improving truthfulness. Extensive experiments across four knowledge-intensive benchmarks show that, compared to vanilla RL, TruthRL significantly reduces hallucinations by 28.9% and improves truthfulness by 21.1%, with consistent gains across various backbone models (e.g., Qwen, Llama) under both retrieval and non-retrieval setups. In-depth ablation study demonstrates that vanilla accuracy-driven methods, such as supervised fine-tuning or RL with binary reward, struggle to balance factual correctness and uncertainty. In contrast, our proposed truthfulness-driven TruthRL achieves strong performance in both accuracy and truthfulness, underscoring the importance of learning objective design for developing truthful LLMs. We also experiment with more complicated reward designs, such as knowledge-enhanced and reasoning-enhanced variants, and show that simple ternary reward scheme generally performs better. Moreover, we find the improvement of TruthRL arises from enhancing the capability of LLMs to recognize their knowledge boundary, hence avoiding being overly conservative as the baselines are. Further analysis confirms that TruthRL is robust to hallucination-baiting questions and more confident in producing accurate responses. Date: October 1, 2025 Correspondence: zhepei.wei@viginia.edu"
        },
        {
            "title": "1 Introduction",
            "content": "While large language models (LLMs) have demonstrated remarkable abilities in generating factual responses (Brown et al., 2020; Touvron et al., 2023; Gemini et al., 2023), they tend to produce plausible but factually incorrect statements rather than acknowledge uncertainty when encountering questions beyond their knowledge (Xu et al., 2024b; Zhang et al., 2023). This hallucination behavior is especially concerning in high-stakes domains (e.g., law, medicine) where inaccurate outputs can cause severe consequences (Singhal et al., 2023; Xiao et al., 2021; Xiong et al., 2024)In such scenarios, the models capability to admit dont know can be just as critical as providing correct information, and truthful LLM should avoid hallucinations as much as possible. From this perspective, model that answers fewer questions correctly while reliably abstaining when uncertain is far more trustworthy than higher-accuracy model that frequently fabricates plausible but incorrect answers. In high-stakes domains, such misleading answers risk doing far more harm than abstention. This underscores that factual accuracy alone does not necessarily guarantee truthfulness. 1 Figure 1 Comparison between vanilla supervised fine-tuning (SFT), reinforcement learning (RL), and TruthRL. In vanilla SFT/RL, the model is optimized solely for accuracy, implicitly rewarding hallucinations over abstentions and thus always attempting to answer or guess, which ultimately compromises truthfulness. In contrast, TruthRL not only rewards correct answers, but explicitly penalizes hallucinations, and treats abstentions neutrally, thereby leading to greater truthfulness. Recent works have explored teaching LLMs to admit uncertainty (Cheng et al., 2024; Yang et al., 2024b). Notably, R-Tuning (Zhang et al., 2024) trains the model on unanswerable questions with dont know as the ground-truth label (Song et al., 2025). However, such methods require non-trivial annotation on model-specific datasets, leading to limited generalization or overly conservative behavior (e.g., abstaining even when the model has sufficient knowledge). On the other hand, lots of research efforts have sought to mitigate hallucinations by expanding the models knowledge scope, either by updating its parametric knowledge through fine-tuning or by incorporating external information via retrieval-augmented generation (RAG) (Kasai et al., 2024; Yang et al., 2024a). However, the retrieved documents in RAG can be noisy or even contain factually incorrect content, potentially misleading the model and posing additional challenges (Wei et al., 2025). Meanwhile, fine-tuning methods typically improve accuracy but can also reinforce hallucinations, particularly when the model is uncertain (Kang et al., 2025). In fact, such accuracy-driven methods inherently motivate LLMs to guess rather than abstain from answering when unsure, since the expected incentive for guessing an answer is always higher than that from abstention by design (Kalai et al., 2025). As result, existing approaches remain deficient in training truthful LLMs that can both provide accurate answers and acknowledge uncertainty. In light of this, we argue that more aligned learning objective is needed for developing truthful LLMsone that explicitly incentivizes models not only to maximize correct responses, but also to appropriately abstain from answering when being uncertain. In this work, we introduce TruthRL, general reinforcement learning (RL) framework designed to directly optimize truthfulness rather than accuracy alone. As illustrated in Figure 1, unlike accuracy-driven methods such as vanilla SFT or RL, which implicitly favor guessing over abstentions by encouraging the model to always provide an answer to maximize accuracy, our method introduces truthfulness-driven ternary reward design that explicitly rewards correct answers, penalizes hallucinations, and treats abstentions as neutral. This design encourages the model to generate correct responses when possible, but more importantly, to properly abstain rather than wildly guessing. Specifically, we implement TruthRL with GRPO (Shao et al., 2024), and our findings show that this simple yet principled reward formulation yields substantial gains in truthfulness. Experiments demonstrate that our method improves the truthfulness of LLMs not only by converting hallucinations into abstentions, but also by promoting more accurate responses, particularly in retrieval-augmented settings where the model has access to additional information. Notably, the increase in abstentions arises not from over-conservatism but from genuine recognition of the knowledge boundary: TruthRL abstains most often, whereas the baseline tends to hallucinate when knowledge is insufficient. In summary, our findings advocate shift from accuracy-driven to truthfulness-driven methods for developing LLMs. The main contributions of this work are as follows: (1) We propose TruthRL, general RL framework that 2 (a) Prompting (b) Vanilla SFT (c) Vanilla RL Figure 2 Scaling curve of prompting and vanilla SFT/RL methods on the CRAG benchmark (Yang et al., 2024a), using Llama3.1-8B-Instruct as the backbone. Before training, the model shows strong potential in majority@k scaling, with reduced hallucination and improved accuracy and abstentions as the number of responses increases. However, despite their slightly improved accuracy, vanilla SFT and RL diminish this potential and lead to much higher hallucinations, underscoring their limitations and the need for more truthful training paradigm. directly optimizes truthfulness through simple yet principled ternary reward design. (2) We demonstrate that, compared to vanilla RL, TruthRL consistently reduces hallucination and improves truthfulness across multiple knowledge-intensive benchmarks in both retrieval and non-retrieval settings, with up to 28.9% reduction in hallucination and 21.1% improvement in truthfulness. (3) Extensive ablation studies demonstrate that simple ternary reward scheme generally outperforms both binary and more complicated counterparts, including knowledge-enhanced and reasoning-enhanced variants. Further analyses confirm that LLMs trained with TruthRL are effective at recognizing their knowledge boundaries, robust to hallucination-inducing questions, and more confident in providing correct answers, while maintaining significantly lower hallucination rate."
        },
        {
            "title": "2.1 Problem Formulation",
            "content": "In contrast to the traditional method that optimizes for accuracy only (Kalai et al., 2025), we propose to optimize for truthfulness with multi-dimensional objective. Truthfulness. Let = {(xi, yi)}N denote the problem set. For model fθ, we evaluate its predictions ˆyi = fθ(xi) and compute (i) accuracy (Acc), the fraction of questions answered correctly; (ii) uncertainty rate (Unc), the fraction of questions where the model abstains (e.g., answers dont know); and (iii) hallucination rate (Hall), the fraction of responses that are factually incorrect. Following standard practices (Yang et al., 2024a; Kachuee et al., 2025; Huang et al., 2025b), we define the truthfulness score as weighted combination: Truthfulness = w1 Acc + w2 Unc w3 Hall, where w1, w2, w3 0 control the desired behavior among the three dimensions. i=1 Objective. Our objective is to design training methods that maximize the expected truthfulness score (i.e., maxθ ED[Truthfulness(fθ)]). This formulation captures the core idea of truthfulness: unlike an accuracyfocused setup that only cares about correctness, our problem formulation favors models that maximize correct answers, appropriately abstain when uncertain, and minimize hallucinations."
        },
        {
            "title": "2.2 Vanilla Fine-tuning Methods\nSupervised fine-tuning (SFT). We train the LLM using the standard SFT objective, which aims to maximize\n(cid:80) log p(y|x; θ),\nthe likelihood of producing the ground-truth response given an input: LSFT(θ) = −E(x,y)∼D\nwhere (x, y) is the input-output pair and θ denotes the parameters. While typically effective for improving\naccuracy, SFT tends to memorize the training data and has limited generalizability (Chu et al., 2025).\nMoreover, the model is trained to always provide an answer, even when unsure, which inevitably encourages\nhallucinations (Kalai et al., 2025).",
            "content": "Reinforcement learning (RL). Traditional RL methods optimize the LLM using accuracy-based reward signals, provided by verifier to determine whether prediction is correct (Guo et al., 2025). Although RL 3 typically achieves better generalization than SFT by eliminating direct supervision with ground-truth answers, vanilla RL is not explicitly designed to recognize uncertainty or abstain when appropriate. As result, it may substantially increase correctness but still fails to prevent hallucinations (Kang et al., 2025), as also observed in our preliminary findings."
        },
        {
            "title": "2.3 Preliminary Findings\nAs shown in Figure 2, we report the majority@k results on the CRAG benchmark for prompting, vanilla SFT\nand RL methods, using Llama3.1-8B-Instruct as the backbone model. The prompting baseline demonstrates\nthat increasing the number of sampled responses consistently reduces hallucination with improved accuracy\nand the abstention rate. This suggests that even without fine-tuning, the base model already has strong\npotential in achieving higher truthfulness.",
            "content": "In contrast, despite their improvements in accuracy, vanilla SFT and RL methods almost completely suppress abstention behavior (i.e., maintaining near-zero uncertainty rate) and provide only limited reduction in hallucinationsor even an increased hallucination rate compared to the baseline when is large. These results reveal the limitations of vanilla fine-tuning methods that focus solely on accuracy: they not only fail to address the truthfulness problem but also diminish the models inherent capacity to express uncertainty, underscoring the need for truthful training approaches."
        },
        {
            "title": "3 Methodology",
            "content": "To address the problem, we first establish strong fine-tuning baselines that can express uncertainty while maintaining accuracy using knowledge boundary probing mechanism, and then elaborate on the design of our proposed solution TruthRL."
        },
        {
            "title": "3.1 Knowledge Boundary Probing",
            "content": "To enable the model to express uncertainty, we first construct appropriate training data for baselines by probing an LLMs knowledge boundary on the training set to identify out-of-knowledge (OOK) questions. For each training question, we sample 256 responses, and the question is marked as OOK if none of the responses is correct. These questions are then relabeled with dont know as the ground-truth answer and used to train the model with the standard SFT objective. Similar approaches have been explored in prior works (Zhang et al., 2024; Yang et al., 2024b; Song et al., 2025), where samples with uncertain labels are incorporated into SFT training using various data construction strategies. We refer to this baseline as R-Tuning (Zhang et al., 2024). Further, we extend the idea of rejection sampling fine-tuning (RFT) (Yuan et al., 2023) with uncertain responses. Rather than directly learning the ground-truth answers, RFT trains the model on reasoning traces generated by the model itself. In this baseline, we prompt the model to generate multiple reasoning traces for each training question and select the trace that concludes with dont know as the target response for OOK questions, whereas for non-OOK questions, we select the trace that leads to the correct answer. In the same spirit, we also leverage the knowledge boundary probing results for knowledge-enhanced reward design in our proposed method, which will be detailed in the following section."
        },
        {
            "title": "3.2 TruthRL: Incentivizing Truthfulness via RL",
            "content": "We implement TruthRL using GRPO (Shao et al., 2024), an online RL method that optimizes the following objective: LGRPO(θ) = xD,{yi}G i=1πθold (x) 1 (cid:88) i=1 1 yi yi (cid:88) t= (cid:16) min wi,t(θ) ˆAi, clip(wi,t(θ), 1 ϵ, 1 + ϵ) ˆAi (cid:17) 4 βDKL (πθπref) , where ϵ and β are hyperparameters, is the group size (i.e., the number of sampled responses from the old for each question x), πref denotes the reference policy, wi,t(θ) denotes the importance ratio, ˆAi is policy πθold the estimated advantage for response yi, computed using group of rewards {r1, r2, . . . , rG} corresponding to the outputs within each group: ˆAi = r(x, yi) mean (cid:0){r(x, yj)}G std (cid:0){r(x, yj)}G (cid:1) j=1 j=1 (cid:1) . Reward design. We consider two main types of reward designs: (1) Binary reward: and (2) Ternary reward: rbinary(x, y) = (cid:40) +1, 1, if is correct, otherwise. rternary(x, y) = +1, 0, 1, if is correct, if is uncertain, if is incorrect. As introduced above, in GRPO training, the advantage of response is computed by comparing its reward against the mean reward within sampled group. For example, consider group consisting of two responses y1 and y2 where y1 expresses abstention and y2 contains hallucination. Under the binary reward scheme, both responses receive 1 reward (i.e., rbinary(x, y1) = rbinary(x, y2) = 1), yielding zero relative advantage (i.e., ˆAbinary(x, y1) = ˆAbinary(x, y2))thus the policy update conflates hallucination with abstention. Under the ternary reward scheme, y1 receives 0 reward while y2 receives 1 reward (i.e., rternary(x, y1) = 0, rternary(x, y2) = 1), resulting in larger advantage for abstention than for hallucination (i.e., ˆAternary(x, y1) > ˆAternary(x, y2)). This encourages the models to abstain rather than hallucinate when they lack the knowledge to make accurate predictions. This exemplifies how the ternary reward inherently better distinguishes abstention from hallucination with the relative advantage estimation of GRPO. We further consider two enhancements on top of these rewards. First, knowledge-enhanced variant treats abstention as positive when the model genuinely lacks knowledge. For out-of-knowledge (OOK) questions, it assigns +1 to uncertain responses and 1 to other responses; for non-OOK questions, it assigns +1 to correct answers, 1 for incorrect answers, and 0 to abstentions if applied with the ternary reward. Second, reasoning-enhanced variant builds on the above outcome-based reward by incorporating additional reward signals that evaluate the quality of the models reasoning process. Different reward designs induce distinct model behaviors, and detailed analysis is provided in Section 4.4 and Section 4.6, which suggests that simple ternary reward scheme generally works better than the binary scheme and more complicated designs."
        },
        {
            "title": "4.1 Experimental Setting\nDatasets and evaluation metrics. We conduct experiments on four knowledge-intensive benchmarks,\nunder with and without retrieval setups: CRAG (Yang et al., 2024a), NaturalQuestions (NQ) (Kwiatkowski\net al., 2019), HotpotQA (Yang et al., 2018), and MuSiQue (Trivedi et al., 2022). Models are trained on\nCRAG and evaluated across all four datasets. The primary evaluation metrics are truthfulness score and\nhallucination rate, with accuracy reported as an auxiliary metric. Following Yang et al. (2024a), we set\nw1 = 1, w2 = 0, w3 = 1 for truthfulness score calculation (defined in Section 2.1). The correctness of predicted\nanswers are judged by Llama3.3-70B-Instruct against the reference answers.",
            "content": "Models and baselines. We compare our method with prompting baseline, vanilla SFT, and two knowledgeenhanced SFT baselines, RFT and R-Tuning, as introduced in Section 3. By default, TruthRL is implemented 5 Table 1 Comparison between TruthRL and baselines across four knowledge-intensive benchmarks under with and without retrieval settings. We report the truthfulness score (T), hallucination rate (H), and accuracy (A) as evaluation metrics. The best scores are highlighted in bold. Method () () () () () () () () () () () () () () () CRAG NQ HotpotQA MuSiQue Average Qwen2.5-7B-Inst Prompting SFT RFT R-Tuning TruthRLBinary TruthRL Llama3.1-8B-Inst Prompting SFT RFT R-Tuning TruthRLBinary TruthRL Qwen2.5-7B-Inst Prompting SFT RFT R-Tuning TruthRLBinary TruthRL Llama3.1-8B-Inst Prompting SFT RFT R-Tuning TruthRLBinary TruthRL -17.4 -51.5 -16.8 -7.5 -29.2 16.2 -4.4 -42.1 -7.6 -13.7 -14.5 22.4 10.6 -2.3 22.6 13.4 8.4 33.1 5.3 1.4 -3.7 15.2 20.8 37.2 48.1 75.7 46.7 21.9 64.5 8.7 44.5 71.1 48.1 39.5 57.2 16. 38.4 51.2 31.4 35.0 45.3 17.3 43.5 49.3 48.8 33.1 39.5 19.4 30.6 24.3 29.9 14.5 35.3 24.9 40.1 28.9 40.4 25.8 42.8 38.7 49.0 48.8 54.0 48.4 53.7 50.4 48.8 50.7 45.1 48.4 60.3 56. -32.4 -49.4 -20.8 -0.9 -35.9 -1.6 -5.2 -38.4 -11.4 -16.6 -5.3 12.9 9.0 0.3 18.4 4.3 11.8 26.4 -5.8 1.6 -4.7 2.1 19.0 28.8 60.1 74.7 49.6 12.6 67.8 25.0 49.2 69.2 51.8 42.5 52.6 30. 41.1 49.9 32.1 44.5 43.9 21.2 50.7 49.2 50.4 47.5 40.5 24.9 Without Retrieval 27.7 25.3 28.9 11.7 31.9 23.5 43.9 30.8 40.4 25.9 47.4 43.8 -36.0 -46.7 -19.1 3.3 -31.2 9. -19.9 -38.9 -23.2 -3.5 -19.6 14.3 With Retrieval 50.1 50.1 50.5 48.8 55.7 47.6 44.9 50.8 45.7 49.6 59.5 53.7 0.2 -2.4 23.4 13.8 20.1 33.3 -4.4 -4.3 1.1 1.7 25.9 37. 60.7 73.4 46.5 8.4 65.3 12.7 53.9 69.5 57.9 26.7 59.8 18.9 43.6 51.2 23.3 30.3 39.1 10.7 49.0 52.1 45.8 46.2 37.0 14.9 24.6 26.6 27.4 11.7 34.1 22.5 34.0 30.5 34.7 23.2 40.2 33. 43.8 48.8 46.6 44.1 59.2 43.9 44.6 47.9 46.9 47.9 62.9 52.3 -68.8 -81.8 -41.8 -0.7 -71.7 -1.7 -54.2 -81.9 -58.0 -20.7 -67.2 -7.7 -51.3 -68.2 -20.6 -23.0 -49.4 -0.6 -60.5 -69.8 -55.7 -53.9 -47.6 -0. 76.7 90.9 50.7 2.1 84.8 5.3 64.7 90.9 69.2 25.2 83.6 16.0 62.8 84.1 33.8 32.5 72.2 9.0 73.0 84.9 68.8 68.3 73.7 15.9 7.9 9.1 8.9 1.4 13.2 3.6 10.5 9.1 11.2 4.5 16.4 8. 11.5 15.9 13.2 9.4 22.8 8.4 12.5 15.1 13.1 14.4 26.1 15.0 -38.7 -57.4 -24.6 -1.5 -42.0 5.7 -20.9 -50.3 -25.1 -13.6 -26.7 10.5 -7.9 -18.2 11.0 2.1 -2.3 23.1 -16.4 -17.8 -15.8 -8.7 4.5 25. 61.4 78.7 48.4 11.3 70.6 12.9 53.1 75.2 56.8 33.5 63.3 20.5 46.5 59.1 30.2 35.6 50.1 14.6 54.1 58.9 53.5 48.8 47.7 18.8 22.7 21.3 23.8 9.8 28.6 18.6 32.1 24.8 31.7 19.9 36.7 31. 38.6 40.9 41.1 37.7 47.9 37.6 37.7 41.1 37.7 40.1 52.2 44.4 with the ternary reward, and when implemented with the binary reward, it recovers the vanilla RL. We instantiate the above methods using Llama3.1-8B-Instruct (Dubey et al., 2024) and Qwen2.5-7B-Instruct (Qwen et al., 2025) as backbone models. More implementation details, including retrieval setup, training and inference hyperparameters, and prompt design are provided in Appendix and B."
        },
        {
            "title": "4.2 Main Result",
            "content": "Table 1 shows the overall experimental results, providing comprehensive comparison between TruthRL and baseline methods in both retrieval and non-retrieval settings. Vanilla SFT increases both accuracy and hallucination, while knowledge-enhanced SFTs effectively reduce hallucination with little loss or even meaningful gain in accuracy. Compared to the prompting baseline, vanilla SFT substantially increases hallucination rates, showing that simply optimizing for accuracy can inadvertently encourage incorrect answers. This effect is particularly pronounced in the non-retrieval setting: while retrieval provides access to external information and can improve accuracy, in the absence of retrieval, the model may even lose accuracy. This is likely because using SFT for ground-truth answers that the model does not know encourages the model to generate content beyond its knowledge, thereby promoting hallucinations. In contrast, knowledge-enhanced SFT methods (i.e., RFT, R-Tuning) achieve much lower hallucination with little to no compromise in accuracy, and can even improve accuracy when sufficient information is provided through retrieval, demonstrating the benefit of explicitly modeling uncertainty in knowledge-intensive tasks. TruthRL consistently outperforms baselines in terms of truthfulness, with significantly reduced hallucination and increased accuracy, particularly in the retrieval setup. Our TruthRL achieves the lowest hallucination rates and highest truthfulness scores across different setups and backbone models. Access to external information consistently improves performance for all methods, highlighting the importance of 6 (a) Performance on all CRAG questions. (b) Performance on difficult CRAG questions. Figure 3 Performance decomposed to accuracy (blue), hallucination (red), and uncertainty (gray). Compared to baselines, TruthRL achieves the highest overall accuracy and the lowest hallucination. On difficult questions where almost no method can provide correct answers, TruthRL produces minimal hallucinations while other methods hallucinate heavily, demonstrating its improved capability in recognizing knowledge boundaries. retrieval in mitigating hallucinations. On CRAG, TruthRL reduces hallucination for Llama3.1-8B-Instruct by 24.1% and achieves an absolute improvement of 31.9% in truthfulness compared to the prompting baseline under the retrieval setup. While knowledge-enhanced SFT methods such as R-Tuning substantially improve truthfulness over vanilla SFT and prompting baselines, they still struggle to balance between hallucination reduction and maintaining accuracy. TruthRLBinary is variant of our method that uses binary reward, achieving the highest accuracy but also exhibiting high hallucination rate and losing the ability to abstain from answering, similar to vanilla SFT, which ultimately limits its truthfulness. In contrast, TruthRL not only reduces hallucinations but also encourages more accurate responses and appropriate abstentions, yielding the highest overall truthfulness."
        },
        {
            "title": "4.3 TruthRL Improves LLMs in Recognizing Their Knowledge Boundaries",
            "content": "TruthRL enables LLMs to abstain from answering mostly when they genuinely lack knowledge. To better understand the behaviors of LLMs trained with different methods, Figure 3 breaks down performance on the CRAG benchmark under the retrieval setup, evaluating both the full test set and challenging subset, using Llama3.1-8B-Instruct as the backbone model. On the full set (Figure 3a), compared to the prompting method, fine-tuning baselines either achieve improved accuracy with almost zero uncertainty rate (e.g., SFT, TruthRLBinary) or sacrifice accuracy to allow abstention (e.g., RFT, R-Tuning). In contrast, TruthRL achieves the lowest hallucination rate while maintaining competitive accuracy and the highest uncertainty rate among all baselines, confirming its effectiveness. When evaluating on the difficult questions (Figure 3b), where almost no method provides correct answers, all baselines hallucinate heavilymodels that achieve high overall accuracy can even hallucinate nearly 100% on these challenging questions (e.g., SFT, TruthRLBinary). In contrast, TruthRL produces minimal hallucinations (15.5%) while generating uncertain responses for most cases (84.5%), demonstrating an improved ability to recognize its knowledge boundaries. Table 2 Comparison of different methods on hallucination-baiting questions. TruthRL is robust to hallucination-baiting questions. We evaluate all methods on the comparison-type questions from CRAG, where candidate answers are explicitly provided in the input (e.g., Which is larger, or B?). Such multiple-choice style questions are known to be prone to inducing hallucinations (Kang et al., 2025), and as shown in Table 2, all baseline methods exhibit high hallucination rates and limited truthfulness scores. Notably, the knowledge-enhanced baselines (RFT, R-Tuning) demonstrate promising overall performance in Table 1, but still suffer substantial hallucinations on these hallucination-baiting questions, highlighting their vulnerability to Prompting SFT RFT R-Tuning Method () () () 39.8 48.5 38.8 43. 9.7 3 12.7 6.8 10.7 0 9.7 5.8 TruthRL 52.4 16.5 14. 7 Table 3 Ablation study on the reward design of TruthRL, demonstrating the effectiveness of simple ternary scheme. CRAG NQ HotpotQA MuSiQue Average TruthRL with binary reward + knowledge-enhanced with ternary reward + knowledge-enhanced () () 39.5 20.8 30.6 27.4 19.4 37.2 21.9 32.7 () () 40.5 19.0 38.2 19.2 24.9 28.8 25.3 27. () () 37.0 25.9 32.0 28.9 14.9 37.4 35.1 13.4 () () 73.7 -47.6 52.3 -28.9 15.9 -0.9 -2.3 15.0 () () 47.7 4.5 38.3 11.7 18.8 25.6 18.9 23.2 (a) Hallucination Rate. (b) Uncertainty Rate. (c) Accuracy. Figure 4 Learning dynamics of TruthRL under different reward designs. The dashed lines labeled as Enhanced represent the knowledge-enhanced reward schemes (defined in Section 3.2). Table 4 Comparison between Offline RL (DPO), Semi-Online RL (Iterative DPO), and Online RL (TruthRL) across four knowledge-intensive benchmarks. Method DPO Iterative DPO Iter 1 Iter 2 Iter 3 Iter 4 TruthRL CRAG NQ HotpotQA MuSiQue Average () () 43.7 6.8 () () 49.8 -2.8 () () 43.8 6.3 () () 67.0 -50. () () 51.1 -10.1 12.9 16.3 28.0 19.0 37.2 40.9 39.8 29.5 33.9 19.4 6.7 9.3 15.0 4.3 28.8 45.5 43.4 37.1 44.5 24.9 12.7 18.5 26.5 8.1 37. 41.0 37.5 26.5 40.1 14.9 -49.7 -44.4 -19.0 -39.5 -0.9 67.4 61.9 33.7 52.7 15.9 -4.4 -0.1 12.6 -2.0 25.6 48.7 45.7 31.7 42.8 18.8 such cases. In contrast, our method achieves the highest truthfulness while maintaining the lowest hallucination, further confirming the effectiveness of TruthRL."
        },
        {
            "title": "4.4 Ablation Study",
            "content": "Binary reward design excels in accuracy but is limited in truthfulness, while ternary reward achieves the best truthfulness score with competitive accuracy. As shown in Table 3, we next ablate the reward design with Llama3.1-8B-Instruct as the backbone. binary reward, which only distinguishes incorrect answers, strongly increases accuracy but drives the model towards the elimination correct vs. of abstentions. Augmenting binary reward with knowledge-enhanced signals partially alleviates this issue, improving abstention rates while at the cost of compromised accuracy. However, the best performance comes from our ternary reward, which explicitly recognizes three outcomes: correct, hallucinated, and abstained. This formulation rewards correctness while tolerating abstention, thereby striking balance between informativeness and reliability. Empirically, the ternary reward achieves the lowest hallucination and highest truthfulness, validating that nuanced feedback is critical for uncertainty-aware optimization. Online RL outperforms offline and semi-online counterparts. Table 4 compares different reinforcement learning paradigms using the same backbone model of Llama3.1-8B-Instruct. We observe that purely offline RL via DPO leads to limited gains: although slightly better than promoting baseline, the truthfulness score remains low, as the fixed dataset constrains the models ability to adaptively refine its behavior. Semi-online training through iterative DPO provides some remedy by refreshing preference data after each iteration, but 8 the performance is inconsistent: early iterations bring steady improvements, yet excessive iterations (e.g., Iter 4) show regressions, suggesting that repeated offline fine-tuning cannot effectively balance exploration and exploitation. In contrast, our TruthRL with online GRPO achieves the best results across all benchmarks, consistently lowering hallucination while improving truthfulness. This highlights the advantage of learning from online interactions, which enables timely policy updates without drifting toward overfitting or degeneration. See Appendix for DPO implementation details."
        },
        {
            "title": "4.5 Analysis",
            "content": "In this section, we conduct an in-depth analysis of TruthRL using Llama3.1-8B-Instruct as the backbone under the retrieval setup. TruthRL is more confident in giving correct answers and abstaining, while the hallucination rate is significantly lower. As shown in Figure 5, we group the model outputs based on their confidence intervals. Even before fine-tuning, Llama3.18B-Instruct already exhibits high confidence in its predictions. However, large portion of outputs in each confidence interval are hallucinations. Moreover, the uncertainty rate decreases as confidence increases, indicating that the model tends to provide an answer rather than abstain when its confidence is high. In contrast, TruthRL further increases the confidence of model outputs, it not only improves accuracy but also significantly reduces overconfident hallucinations. This indicates that our method produces responses that are not only more accurate but also better aligned with their confidence, leading to more trustworthy uncertainty estimates. Figure 5 Study of model behaviors under different output confidence on CRAG. Table 5 Training with rule-based verifier vs. training with model-based verifier. LLM-based verifier provides more reliable training signals than rule-based verifier. During the training of TruthRL, when replacing the LLM-based verifier with rule-based verifier (i.e., string matching), the model collapses into overly conservative behavior, abstaining on the vast majority of queries, as shown in Table 5. Although this results in an extremely low hallucination rate, the truthfulness score becomes negative, reflecting the lack of meaningful answers. This occurs because the predicted answer rarely matches the reference answer in exact string form, causing rule-based verifiers to misclassify many correct responses and thus fail to provide reliable reward signals. By contrast, the LLM-based verifier can handle semantic equivalence in answer verification and better capture partial correctness and nuanced errors, which stabilizes RL training and leads to higher overall performance. This demonstrates that high-quality verifier is as important as the reward design itself in reinforcement learning for truthfulness. with rule-based verifier with model-based verifier () () TruthRL 3.6 19.4 -3.6 37.2 TruthRL is robust across different LLM judges. Since our training pipeline relies on an LLM to provide rewards, it is important to ensure that performance is not due to hacking specific model. Table 6 reports results on CRAG under three distinct high-capacity evaluators: Llama3.3-70B-Instruct (Dubey et al., 2024), Qwen2.5-72B-Instruct (Qwen et al., 2025), and Gemma3-27B-Instruct (Gemma et al., 2025). While absolute scores vary slightly across judges, the relative improvements of TruthRL are consistent: it achieves the lowest hallucination and the highest truthfulness under all evaluators. This robustness suggests that TruthRL learns generalizable behaviors rather than overfitting to the idiosyncrasies of single judge. TruthRL consistently improves across model scales. We further examine the scalability of our method across spectrum of model sizes, ranging from compact backbones (e.g., Llama3.2-3B-Instruct, Qwen2.5-3BInstruct) to mid/large-scale models (e.g., Qwen2.5-7B-Instruct, Llama3.1-8B-Instruct, Qwen2.5-32B-Instruct). As summarized in Table 7, TruthRL consistently reduces hallucination and boosts truthfulness regardless of the base model size. Interestingly, the relative gain is more pronounced for smaller models, which suffer from higher hallucination rates under prompting. This suggests that our approach not only strengthens already strong models but also helps weaker models develop more reliable uncertainty-awareness. At the large-model end, the improvements on 32B backbones highlight that even highly capable LLMs benefit from TruthRL with uncertainty-aware rewards, underscoring the scalability of our method to state-of-the-art models. 9 Table 6 Evaluation on CRAG under different LLM judges, confirming the consistent improvements of TruthRL. Llama3.3-70B-Inst Qwen2.5-72B-Inst Gemma3-27B-Inst Average Method Prompting SFT RFT R-Tuning TruthRL () 5.3 1.4 -3.7 15.2 37. () 43.5 49.3 48.8 33.1 19.4 () 1.9 1.7 -5.0 14.9 35.6 () 45.3 49.1 49.5 33.3 20.2 () 6.5 6.7 -3.1 18.0 39.7 () 42.9 46.7 48.5 31.7 18.2 () () 43.9 4.6 48.4 3.3 48.9 -3.9 32.7 16.0 19.3 37. Table 7 The performance of TruthRL with different backbones on the CRAG benchmark, demonstrating its scalability. Llama3.2-3B-Inst Qwen2.5-3B-Inst Qwen2.5-7B-Inst Llama3.1-8B-Inst Qwen2.5-32B-Inst Method Prompting TruthRL () 1.9 27.4 () 45.1 21. () -0.3 21.9 () 45.4 16.2 () 10.6 33.1 () 38.4 17.3 () 5.3 37.2 () 43.5 19. () 29.1 40.0 () 27.1 18."
        },
        {
            "title": "4.6 Beyond Outcome Reward",
            "content": "We conduct reasoning-quality analysis on CRAG using the prompting method with Llama3.1-8B-Instruct, evaluating model responses for both outcome and reasoning quality, resulting in an overall truthfulness score of 5.3% and reasoning score of 50.2%. Specifically, results show that accurate responses are typically associated with high reasoning quality score of 92% and uncertain responses exhibit reasoning score of 0%, while hallucinated responses have reasoning sore of 12.1%. The findings suggest strong correlation between response accuracy and reasoning quality. The high reasoning score of accurate responses indicates that the model excels in generating accurate outcomes with promising reasoning. However, the low reasoning scores for uncertain and hallucinated responses highlight the need for quality reasoning. Introducing reasoning rewards could potentially help mitigate these issues, enabling more accurate outcomes with better reasoning. Table 8 Effect of incorporating reasoning reward into TruthRL on CRAG. We explore three heuristic strategies for incorporating the reasoning reward rreason on top of the outcome reward routcome: (1) multiplicative strategy scales the outcome reward by reasoning quality, i.e., rfinal = routcome (1+rreason), which particularly encourages better reasoning when the outcome is correct. (2) An additive strategy treats reasoning as complementary signal with scaling factor λ, giving rfinal = routcome + λ rreason, so that good reasoning can get rewarded even when the outcome reward is moderate. (3) conditional strategy applies reasoning rewards only if the outcome is correct: rfinal = routcome rreason when routcome = 1, and rfinal = routcome otherwise, enforcing stricter alignment where reasoning quality matters primarily in successful completions. The results in Table 8 indicate that outcome-only rewards implicitly improve reasoning ability, while explicitly optimizing reasoning quality requires non-trivial design to balance multiple objectives. For instance, heuristic designs like additive reasoning rewards can boost reasoning scores but may compromise the outcome, underscoring the need for thoughtful design. We leave this exploration for future work. Prompting TruthRL (routcome only) + multiplicative rreason + additive rreason + conditional rreason 43.5 5.3 37.2 19.4 37.0 19.4 36.1 19.1 19.3 35.6 50.2 56.6 54.7 59.1 55. () () Score () Reasoning Outcome Method"
        },
        {
            "title": "5.1 LLM Hallucination and Mitigation",
            "content": "A recurring challenge in LLMs is their tendency to generate fluent but factually incorrect statements, commonly termed hallucinations (Ji et al., 2023; Zhang et al., 2023; Xu et al., 2024b). Hallucinations arise from several factors, including limited grounding in external knowledge sources (Shuster et al., 2021) and over-reliance on parametric recall (Petroni et al., 2019). This gap between surface-level accuracy and deeper factual 10 correctness becomes especially problematic in high-stakes domains such as law (Xiao et al., 2021) and medicine (Singhal et al., 2023; Xiong et al., 2024), where confidently wrong outputs can mislead users. Several lines of mitigation strategies have been explored. Retrieval-augmented methods ground LLMs in external knowledge bases or search engines to reduce reliance on memory alone (Lewis et al., 2020; Borgeaud et al., 2022; Izacard et al., 2023). Decoding strategies encourage self-correction and uncertainty expression, including self-consistency sampling (Wang et al., 2023), calibrated decoding (Kadavath et al., 2022), and contrastive decoding (Chuang et al., 2024). Fine-tuning approaches seek to instill more truthful behavior directly into the models parameters. Common methods include SFT (Zhou et al., 2023) and reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022) on curated datasets of high-quality, factual question-answer pairs. While these methods can enhance accuracy on in-distribution topics, their generalization might degrade significantly on out-of-distribution questions (Kirk et al., 2024). Our work addresses key limitation of many existing approaches: they do not explicitly train models to recognize when to abstain. Among prior efforts, the most closely related is R-Tuning (Zhang et al., 2024), which likewise aims to reduce hallucinations. However, as we show in Section 4, its reduction of hallucination comes at the cost of substantially reduced coverage. This trade-off underscores the need for training frameworks that directly optimize for truthfulnessstriking balance between factual accuracy and calibrated abstention, thereby minimizing the risk of misleading outputs."
        },
        {
            "title": "5.2 Reinforcement Learning for LLMs",
            "content": "Reinforcement learning (RL) has become central paradigm for post-training LLMs, enabling alignment beyond supervised fine-tuning. The most prominent example, RLHF (Ouyang et al., 2022; Christiano et al., 2017; Rafailov et al., 2023), encodes user preferences into reward models and has produced systems that are generally more helpful, safer, and better aligned. More recently, Reinforcement learning with verifiable rewards (RLVR) (Shao et al., 2024; Zhu et al., 2025; Shao et al., 2025; Lambert et al., 2024; Guo et al., 2025; Huang et al., 2025a; Li et al., 2025b; Zheng et al., 2025) has shown that binary reward signals (correct vs. incorrect) can elicit sophisticated chain-of-thought reasoning. However, this formulation conflates abstention with error, thereby discouraging models from producing calibrated dont know responses (Song et al., 2025). To alleviate such limitations, several extensions introduce richer reward structures, including uncertainty-aware RL (Xu et al., 2024a; Xue et al., 2024; Lin et al., 2024; Wang et al., 2024c; Li et al., 2025a) and multi-objective optimization for factual faithfulness (Wang et al., 2024a). Despite these advances, designing scalable reward signals that reliably capture truthfulness while balancing accuracy and uncertainty remains an open challenge. In this work, we demonstrate that reward structurebinary vs. ternary, whether and how uncertainty is incorporatedcan fundamentally influence whether models learn to balance factual accuracy with abstention."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we presented TruthRL, general reinforcement learning framework that directly optimizes the truthfulness of large language models. By leveraging simple yet effective ternary reward design, TruthRL incentivizes models to provide accurate responses, abstain when uncertain, and avoid hallucinations. Experiments on four knowledge-intensive benchmarks demonstrate TruthRL consistently improves truthfulness and reduces hallucinations, achieving significant gains across various backbone models under both with and without retrieval setups. Compared to the vanilla RL method, TruthRL achieves an average reduction of up to 28.9% in hallucinations and an average improvement of up to 21.1% in truthfulness, demonstrating its effectiveness in enhancing the reliability and trustworthiness of large language models."
        },
        {
            "title": "References",
            "content": "Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving language models by retrieving from trillions of tokens. In International conference on machine learning, pages 22062240. PMLR, 2022. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Qinyuan Cheng, Tianxiang Sun, Xiangyang Liu, Wenwei Zhang, Zhangyue Yin, Shimin Li, Linyang Li, Zhengfu He, Kai Chen, and Xipeng Qiu. Can AI assistants know what they dont know? In International Conference on Machine Learning, pages 81848202. PMLR, 2024. Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. SFT memorizes, RL generalizes: comparative study of foundation model post-training. In Forty-second International Conference on Machine Learning, 2025. Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass, and Pengcheng He. DoLa: Decoding by contrasting layers improves factuality in large language models. In The Twelfth International Conference on Learning Representations, 2024. Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In The Twelfth International Conference on Learning Representations, 2024. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The Llama 3 herd of models. arXiv e-prints, pages arXiv2407, 2024. Team Gemini, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Team Gemma, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning. Nature, 2025. Chengsong Huang, Wenhao Yu, Xiaoyang Wang, Hongming Zhang, Zongxia Li, Ruosen Li, Jiaxin Huang, Haitao Mi, and Dong Yu. R-Zero: Self-evolving reasoning LLM from zero data. arXiv preprint arXiv:2508.05004, 2025a. Yin Huang, Yifan Ethan Xu, Kai Sun, Vera Yan, Alicia Sun, Haidar Khan, Jimmy Nguyen, Mohammad Kachuee, Zhaojiang Lin, Yue Liu, et al. ConfQA: Answer only if you are confident. arXiv preprint arXiv:2506.07309, 2025b. Hugging Face. Open R1: fully open reproduction of DeepSeek-R1, January 2025. https://github.com/huggingface/ open-r1. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Atlas: Few-shot learning with retrieval augmented language models. Journal of Machine Learning Research, 24(251):143, 2023. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):138, 2023. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-R1: Training LLMs to reason and leverage search engines with reinforcement learning. In Second Conference on Language Modeling, 2025. Mohammad Kachuee, Teja Gollapudi, Minseok Kim, Yin Huang, Kai Sun, Xiao Yang, Jiaqi Wang, Nirav Shah, Yue Liu, Aaron Colak, et al. PrismRAG: Boosting RAG factuality with distractor resilience and strategized reasoning. arXiv preprint arXiv:2507.18857, 2025. Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221, 2022. Adam Tauman Kalai, Ofir Nachum, Santosh S. Vempala, and Edwin Zhang. Why language models hallucinate, 2025. https://openai.com/index/why-language-models-hallucinate/. Katie Kang, Eric Wallace, Claire Tomlin, Aviral Kumar, and Sergey Levine. Unfamiliar finetuning examples control how language models hallucinate. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 36003612, 2025. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick SH Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In EMNLP (1), pages 67696781, 2020. Jungo Kasai, Keisuke Sakaguchi, Ronan Le Bras, Akari Asai, Xinyan Yu, Dragomir Radev, Noah Smith, Yejin Choi, Kentaro Inui, et al. RealTime QA: Whats the answer right now? Advances in Neural Information Processing Systems, 36, 2024. Robert Kirk, Ishita Mediratta, Christoforos Nalmpantis, Jelena Luketina, Eric Hambro, Edward Grefenstette, and In The Twelfth Roberta Raileanu. Understanding the effects of RLHF on LLM generalisation and diversity. International Conference on Learning Representations, 2024. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural Questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452466, 2019. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with PagedAttention. In Proceedings of the 29th Symposium on Operating Systems Principles, pages 611626, 2023. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. Tulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive NLP tasks. Advances in neural information processing systems, 33:94599474, 2020. Lei Li, Hehuan Liu, Yaxin Zhou, ZhaoYang Gui, Xudong Weng, Yi Yuan, Zheng Wei, and Zang Li. Uncertainty-aware iterative preference optimization for enhanced LLM reasoning. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2399624012, 2025a. Zongxia Li, Wenhao Yu, Chengsong Huang, Rui Liu, Zhenwen Liang, Fuxiao Liu, Jingxi Che, Dian Yu, Jordan Boyd-Graber, Haitao Mi, et al. Self-rewarding vision-language model via reasoning decomposition. arXiv preprint arXiv:2508.19652, 2025b. Sheng-Chieh Lin, Luyu Gao, Barlas Oguz, Wenhan Xiong, Jimmy Lin, Wen-tau Yih, and Xilun Chen. FLAME: Factuality-aware alignment for large language models. Advances in Neural Information Processing Systems, 37: 115588115614, 2024. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 24632473, 2019. Team Qwen, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. 13 Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 116. IEEE, 2020. Rulin Shao, Shuyue Stella Li, Rui Xin, Scott Geng, Yiping Wang, Sewoong Oh, Simon Shaolei Du, Nathan Lambert, Sewon Min, Ranjay Krishna, et al. Spurious rewards: Rethinking training signals in RLVR. arXiv preprint arXiv:2506.10947, 2025. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. DeepSeekMath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. HybridFlow: flexible and efficient RLHF framework. arXiv preprint arXiv: 2409.19256, 2024. Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. Retrieval augmentation reduces hallucination in conversation. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 37843803, 2021. Karan Singhal, Shekoofeh Azizi, Tao Tu, Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language models encode clinical knowledge. Nature, 620 (7972):172180, 2023. Linxin Song, Taiwei Shi, and Jieyu Zhao. The hallucination tax of reinforcement finetuning. arXiv preprint arXiv:2505.13988, 2025. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. MuSiQue: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539554, 2022. Haiyang Wang, Yuchen Pan, Xin Song, Xuechen Zhao, Minghao Hu, and Bin Zhou. F2RL: Factuality and faithfulness reinforcement learning framework for claim-guided evidence-supported counterspeech generation. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 44574470, 2024a. Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. Improving text embeddings with large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1189711916, Bangkok, Thailand, August 2024b. Association for Computational Linguistics. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023. Yikun Wang, Rui Zheng, Liang Ding, Qi Zhang, Dahua Lin, and Dacheng Tao. Uncertainty aware learning for language model alignment. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1108711099, 2024c. Zhepei Wei, Wei-Lin Chen, and Yu Meng. InstructRAG: Instructing retrieval-augmented generation via self-synthesized rationales. In The Thirteenth International Conference on Learning Representations, 2025. Chaojun Xiao, Xueyu Hu, Zhiyuan Liu, Cunchao Tu, and Maosong Sun. Lawformer: pre-trained language model for Chinese legal long documents. AI Open, 2:7984, 2021. Guangzhi Xiong, Qiao Jin, Zhiyong Lu, and Aidong Zhang. Benchmarking retrieval-augmented generation for medicine. arXiv preprint arXiv:2402.13178, 2024. Hongshen Xu, Zichen Zhu, Situo Zhang, Da Ma, Shuai Fan, Lu Chen, and Kai Yu. Rejection improves reliability: Training LLMs to refuse unknown questions using RL from knowledge feedback. In First Conference on Language Modeling, 2024a. 14 Ziwei Xu, Sanjay Jain, and Mohan Kankanhalli. Hallucination is inevitable: An innate limitation of large language models. arXiv preprint arXiv:2401.11817, 2024b. Boyang Xue, Fei Mi, Qi Zhu, Hongru Wang, Rui Wang, Sheng Wang, Erxin Yu, Xuming Hu, and Kam-Fai Wong. UAlign: Leveraging uncertainty estimations for factuality alignment on large language models. arXiv preprint arXiv:2412.11803, 2024. Xiao Yang, Kai Sun, Hao Xin, Yushi Sun, Nikita Bhalla, Xiangsen Chen, Sajal Choudhary, Rongze Daniel Gui, Ziran Will Jiang, Ziyu Jiang, Lingkun Kong, Brian Moran, Jiaqi Wang, Yifan Ethan Xu, An Yan, Chenyu Yang, Eting Yuan, Hanwen Zha, Nan Tang, Lei Chen, Nicolas Scheffer, Yue Liu, Nirav Shah, Rakesh Wanga, Anuj Kumar, Wen tau Yih, and Xin Luna Dong. CRAG comprehensive RAG benchmark. arXiv preprint arXiv:2406.04744, 2024a. Yuqing Yang, Ethan Chern, Xipeng Qiu, Graham Neubig, and Pengfei Liu. Alignment for honesty. Advances in Neural Information Processing Systems, 37:6356563598, 2024b. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. HotpotQA: dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018. Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, and Jingren Zhou. Scaling relationship on learning mathematical reasoning with large language models. arXiv preprint arXiv:2308.01825, 2023. Hanning Zhang, Shizhe Diao, Yong Lin, Yi Fung, Qing Lian, Xingyao Wang, Yangyi Chen, Heng Ji, and Tong Zhang. R-Tuning: Instructing large language models to say dont know. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 71067132, 2024. Muru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah Smith. How language model hallucinations can snowball. arXiv preprint arXiv:2305.13534, 2023. Tong Zheng, Hongming Zhang, Wenhao Yu, Xiaoyang Wang, Xinyu Yang, Runpeng Dai, Rui Liu, Huiwen Bao, Chengsong Huang, Heng Huang, et al. Parallel-R1: Towards parallel thinking via reinforcement learning. arXiv preprint arXiv:2509.07980, 2025. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. LIMA: Less is more for alignment. Advances in Neural Information Processing Systems, 36:5500655021, 2023. Xinyu Zhu, Mengzhou Xia, Zhepei Wei, Wei-Lin Chen, Danqi Chen, and Yu Meng. The surprising effectiveness of negative reinforcement in LLM reasoning. arXiv preprint arXiv:2506.01347, 2025."
        },
        {
            "title": "A Implementation Details",
            "content": "Retrieval setup We follow the retrieval setup from CRAG (Yang et al., 2024a), using up to 50 web pages as retrieval documents per question. For each question, the question text is used as the search query, and up to 50 HTML pages are stored from the search API. For NaturalQuestions (NQ), HotpotQA, and MuSiQue, we use the 2018 Wikipedia dump (Karpukhin et al., 2020) as the knowledge source and employ the E5 retriever (Wang et al., 2024b), as in line with the Search-R1 setup (Jin et al., 2025). DPO DPO (Rafailov et al., 2023) is an offline RL method that trains the model to prefer certain responses over others. Preference pairs are constructed differently for OOK and non-OOK questions. For OOK questions, the preferred response is dont know, and the dispreferred response is randomly chosen incorrect answer. For non-OOK questions, correct and incorrect responses are paired. DPO expresses the probability of preference data with the policy model rather than the reward model, yielding the following objective: LDPO(θ) = E(x,yw,yl)D (cid:20) (cid:18) log σ β log πθ(yw x) πref(yw x) β log πθ(yl x) πref(yl x) (cid:19)(cid:21) , where (x, yw, yl) are preference pairs consisting of the prompt, the winning response, and the losing response from the preference dataset D. Iterative DPO This variant builds on DPO-trained checkpoint and iteratively constructs preference pairs in the same way over the training set. Training details. Our models are trained on 8 NVIDIA H100 GPUs with 80GB memory using full-parameter fine-tuning. By default, we use the Open-R1 library (Hugging Face, 2025) as the training framework. To optimize GPU utilization, we adopt DeepSpeed (Rajbhandari et al., 2020) with ZeRO-3 offload, along with gradient checkpointing, FlashAttention-2 (Dao, 2024), and bf16 mixed-precision training enabled. To optimize model performance, we conduct an extensive hyperparameter search with batch sizes in [16, 32, 64], learning rates in [5e-7, 1e-6, 2e-6, 3e-6, 5e-6, 1e-5], and training epochs in [1, 2, 3]. For SFT, RFT, and R-Tuning, we use learning rate of 5e-6 and batch size of 16, with cosine learning rate scheduler and 3% warmup steps, trained for 1 epoch. For DPO and iterative DPO, we use learning rate of 3e-6 and batch size of 32, trained for 1 epoch. For RL training, we use the VeRL framework (Sheng et al., 2024) with constant learning rate of 1e-6, and batch size of 64. The KL divergence regularization coefficient β and clip ratio ϵ are set to 0.001 and 0.2, respectively. The maximum context length and number of generated tokens are set to 16,384 and 2,048. For efficient LLM rollouts, we use vLLM (Kwon et al., 2023) with tensor parallel size of 2 and GPU memory utilization ratio of 0.8. Rollout sampling is performed with temperature = 1.0 and top-p = 1.0. The maximum token length for all models is fixed at 16k. The scaling factor λ defined in Section 4.6 is set to 0.5. Inference details. We use vLLM for efficient inference and adopt greedy decoding (i.e., temp = 0) for evaluation to ensure reproducible results. For data construction in RFT, we sample 64 responses with temperature of 0.6 and top-p of 0.9. The maximum token length at inference is set to 32k."
        },
        {
            "title": "B Prompt Template",
            "content": "Inference prompts. Below we present the inference prompts for both without and with retrieval setups in Table 9 and Table 10, respectively. LLM-as-a-judge prompts. Below we present the judge prompts for outcome and reasoning quality in Table 11 and Table 12, respectively. 16 Table 9 Inference prompt under without retrieval setup. Inference Prompt (Without Retrieval) Input: You are given Question and the time when it was asked in the Pacific Time Zone (PT), referred to as Query Time\". The query time is formatted as mm/dd/yyyy, hh:mm:ss PT\". Your task is to answer the question based on factual information in your own knowledge. Please adhere to the following guidelines when formulating the answer: 1. If the question contains false premise or assumption, answer invalid question\". 2. If you are uncertain or dont know the answer, answer dont know\". Please reason step by step and then provide the final answer. The reasoning process must be enclosed within <think> </think> tags. The final answer MUST be put in boxed{}. For example, boxed{I dont know}, boxed{invalid question}, boxed{3 times}, boxed{New York}, etc. Question: {question} Query Time: {query time} Output: {answer} Table 10 Inference prompt under with retrieval setup. Inference Prompt (With Retrieval) Input: You are given Question, References and the time when it was asked in the Pacific Time Zone (PT), referred to as Query Time\". The query time is formatted as mm/dd/yyyy, hh:mm:ss PT\". The references may or may not help answer the question. Your task is to answer the question based on factual information in the references or your own knowledge. Please adhere to the following guidelines when formulating the answer: 1. If the question contains false premise or assumption, answer invalid question\". 2. If you are uncertain or dont know the answer, answer dont know\". Please reason step by step and then provide the final answer. The reasoning process must be enclosed within <think> </think> tags. The final answer MUST be put in boxed{}. For example, boxed{I dont know}, boxed{invalid question}, boxed{3 times}, boxed{New York}, etc. Question: {question} Query Time: {query time} References: {documents} Output: {answer} Table 11 LLM-as-a-judge prompt for evaluating outcome. LLM-as-a-judge Prompt (Outcome) Input: Assume you are human expert in grading predictions given by model. You are given question and model prediction. Judge if the prediction matches the ground truth answer by following these steps: 1: Take it as granted that the Ground Truth is always correct. 2: If the Prediction exactly matches the Ground Truth, score\" is 1. 3: If the Prediction does not exactly match the Ground Truth, go through the following steps and likely give score as 0. 4: If the Ground Truth is number, score\" is 1 if and only if the Prediction gives number that almost exactly matches the ground truth. 5: If the Prediction is self-contradictory, score\" must be 0. 6: If the prediction is not answering the question, score\" must be 0. 7: If the prediction is concise and correct summary of the ground truth, score\" is 1. 8: If ground truth contains set of items, prediction must contain exactly same items for the score to be 1. 9: Otherwise, score\" is 0. Output JSON blob with an explanation\" field explaining your answer as short as possible and an score\" field with value 1 or 0. You should make the judgment based on provided examples. Examples: {examples} Question: {question} Ground Truth: {ground truth} Prediction: {predicted answer} Output: {judgment} Table 12 LLM-as-a-judge prompt for evaluating reasoning quality. LLM-as-a-judge Prompt (Reasoning Quality) Input: Assume you are human expert in evaluating the usefulness of model-generated reasoning. You are given question and model-generated reasoning. Judge if the reasoning provides precise information to correctly answer the question by following these steps: 1: Evaluate if the reasoning directly addresses the question. 2: Check if the key points in the reasoning are relevant to the query. 3: If the reasoning provides precise and relevant information, score\" is 1. 4: If the reasoning is vague, unrelated, or does not address the question, score\" is 0. Output JSON blob with an explanation\" field explaining your answer as short as possible and an score\" field with value 1 or 0. You should make the judgment based on provided examples. Examples: {examples} Question: {question} Ground Truth: {ground truth} Reasoning: {predicted reasoning} Output: {judgment}"
        }
    ],
    "affiliations": [
        "FAIR at Meta",
        "Meta Reality Labs",
        "University of Virginia",
        "University of Washington"
    ]
}