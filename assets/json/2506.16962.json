{
    "paper_title": "Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs",
    "authors": [
        "Haoran Sun",
        "Yankai Jiang",
        "Wenjie Lou",
        "Yujie Zhang",
        "Wenjie Li",
        "Lilong Wang",
        "Mianxin Liu",
        "Lei Liu",
        "Xiaosong Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal large language models (MLLMs) have begun to demonstrate robust reasoning capabilities on general tasks, yet their application in the medical domain remains in its early stages. Constructing chain-of-thought (CoT) training data is essential for bolstering the reasoning abilities of medical MLLMs. However, existing approaches exhibit a deficiency in offering a comprehensive framework for searching and evaluating effective reasoning paths towards critical diagnosis. To address this challenge, we propose Mentor-Intern Collaborative Search (MICS), a novel reasoning-path searching scheme to generate rigorous and effective medical CoT data. MICS first leverages mentor models to initialize the reasoning, one step at a time, then prompts each intern model to continue the thinking along those initiated paths, and finally selects the optimal reasoning path according to the overall reasoning performance of multiple intern models. The reasoning performance is determined by an MICS-Score, which assesses the quality of generated reasoning paths. Eventually, we construct MMRP, a multi-task medical reasoning dataset with ranked difficulty, and Chiron-o1, a new medical MLLM devised via a curriculum learning strategy, with robust visual question-answering and generalizable reasoning capabilities. Extensive experiments demonstrate that Chiron-o1, trained on our CoT dataset constructed using MICS, achieves state-of-the-art performance across a list of medical visual question answering and reasoning benchmarks. Codes are available at GitHub - manglu097/Chiron-o1: Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 2 6 9 6 1 . 6 0 5 2 : r Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs Haoran Sun1,2*, Yankai Jiang1*(cid:66), Wenjie Lou1, Yujie Zhang2 Wenjie Li1,3, Lilong Wang1, Mianxin Liu1, Lei Liu2, Xiaosong Wang1(cid:66) 1Shanghai Artificial Intelligence Laboratory 2Fudan University 3Shanghai Jiao Tong University jiangyankai@pjlab.org.cn, wangxiaosong@pjlab.org.cn"
        },
        {
            "title": "Abstract",
            "content": "Multimodal large language models (MLLMs) have begun to demonstrate robust reasoning capabilities on general tasks, yet their application in the medical domain remains in its early stages. Constructing chain-of-thought (CoT) training data is essential for bolstering the reasoning abilities of medical MLLMs. However, existing approaches exhibit deficiency in offering comprehensive framework for searching and evaluating effective reasoning paths towards critical diagnosis. To address this challenge, we propose Mentor-Intern Collaborative Search (MICS), novel reasoning-path searching scheme to generate rigorous and effective medical CoT data. MICS first leverages mentor models to initialize the reasoning, one step at time, then prompts each intern model to continue the thinking along those initiated paths, and finally selects the optimal reasoning path according to the overall reasoning performance of multiple intern models. The reasoning performance is determined by an MICS-Score, which assesses the quality of generated reasoning paths. Eventually, we construct MMRP, multi-task medical reasoning dataset with ranked difficulty, and Chiron-o1, new medical MLLM devised via curriculum learning strategy, with robust visual question-answering and generalizable reasoning capabilities. Extensive experiments demonstrate that Chiron-o1, trained on our CoT dataset constructed using MICS, achieves state-ofthe-art performance across list of medical visual question answering and reasoning benchmarks. Code will be available at https://github.com/manglu097/Chiron-o"
        },
        {
            "title": "Introduction",
            "content": "Multimodal Large Language Models (MLLMs) [7, 20, 23, 31, 45, 66] have demonstrated prominent performance in wide range of tasks, including image captioning, visual question answering, and video analysis. Recent breakthroughs in MLLMs have also shed new light on the development of general-purpose medical AI. Remarkable efforts have been made to adapt existing MLLMs to address complex clinical tasks through supervised fine-tuning (SFT) on carefully curated multimodal medical instruction fine-tuning datasets [6, 25, 27, 61, 64]. However, most current medical MLLMs rely on direct-prediction paradigm, which produces brief, immediate answers to problems and often overlooks the interleaved imagetext reasoning processes essential for real-world clinical scenarios. This critical shortcomingthe inability to perform deep, multimodal reasoning analysis of medical datahas given rise to an urgent need for the development of more sophisticated medical MLLMs capable of tackling complex clinical challenges and offering improved diagnostic support. *Equal contribution. (cid:66) Corresponding authors. This Work is under support from the Shanghai Artificial Intelligence Laboratory. Preprint. Under review. Building on recent advancements in reinforcement learning (RL) optimization, several promising approaches [21, 36, 43] have leveraged Group Relative Policy Optimization (GRPO) [39] to elicit reasoning capabilities in models, introducing multiple R1-series medical MLLMs. Nevertheless, these models continue to exhibit limited performance in answering real-world clinical questions. As pointed out by recent studies [69], although RL training improves performance by biasing the models output distribution toward reward-yielding trajectories, it fails to generate novel reasoning paradigms. Consequently, the performance ceiling of the R1 models has long been constrained by their underlying base MLLM. One potential solution is to incorporate high-quality chain-ofthought (CoT) annotations into SFT to bolster the models reasoning capabilities and foster novel reasoning paradigms. However, unlike general-domain tasks in which large-scale CoT datasets can be crowdsourced, medical reasoning demands domain-specific logical structures and clinical expertise. Curating such medical CoT datasets is prohibitively expensive and time-consuming. Furthermore, the absence of standardized evaluation metrics to ensure the validity of generated CoT processes also remains critical barrier to step-by-step visual reasoning annotation. Therefore, it is crucial to develop an effective and sophisticated method that can generate intermediate reasoning steps toward the final answer and evaluate the step-by-step visual reasoning quality. Motivated by the aforementioned challenges, we propose Mentor-Intern Collaborative Search (MICS), new multi-model collaborative searching strategy designed to generate effective step-by-step CoT data. The core idea of MICS is leveraging multiple knowledgeable mentor models to collaboratively search for reasoning paths, while evaluating the searched paths based on feedback from intern models. The entire search process aligns with the logic of how mentor guides interns, where the effectiveness of the mentors guidance is validated by whether interns can correctly solve problems based on the provided instructions, i.e., initialized reasoning paths. We integrate the valuable insights of multiple mentor models, retaining effective steps and discarding low-quality or hallucinated ones to identify the correct reasoning paths. Along the searching, we propose an MICS-score to enable the evaluation of CoT data quality, to further facilitate the efficient construction of multimodal reasoning data. Building on the proposed methods, we develop MMRP, multimodal medical reasoning dataset comprising three subsets: simple questionanswer (QA) pairs, imagetext alignment annotations, and MICS-generated multimodal CoT data for complex clinical scenarios. Thus, we employ novel curriculum learning paradigm that progressively infuses medical knowledge, from fundamental concepts to complex cases in MMRP, thereby enhancing the models reasoning capabilities. Finally, we achieve Chiron-o1, general-purpose multimodal medical model with multimodal CoT reasoning capabilities through stage-wise SFT with the composed curriculum. We conduct comprehensive evaluation on seven benchmarks, containing both in-domain and out-of-domain scenarios, to rigorously assess the performance of Chiron-o1. The results indicate that Chiron-o1 exhibits robust multimodal reasoning capabilities, outperforming the SOTA medical reasoning models across all benchmarks. Our contributions can be summarized as follows: We present MICS, multi-model collaborative search strategy that facilitates the generation of effective step-by-step CoT data. We construct MMRP, high-quality multimodal medical dataset comprising QA pairs, image-text alignment data, and effective reasoning paths for complex medical VQA problems, spanning 12 imaging modalities and 20 body systems. We develop Chiron-o1, new multimodal medical model that demonstrates outstanding reasoning abilities in handling both in-domain and out-of-domain complex clinical problems. We demonstrate that our approach achieves competitive performance compared to previous SOTA medical MLLMs through extensive experiments across multiple benchmarks."
        },
        {
            "title": "2 Related Works",
            "content": "Reasoning in Medical MLLMs. As multimodal reasoning demand grows, frameworks and techniques have evolved, extending the reasoning capabilities of Large Language Models (LLMs) [15, 47, 48] to tackle more complex general-purpose vision-language tasks [50, 66, 8, 3]. Techniques such as prompt tuning [70], SFT [40], and RL [10] have emerged as critical methods for boosting the reasoning capabilities of MLLMs. Given its potential to enhance models performance on complex tasks, reasoning ability has also garnered significant attention in medical domains [6, 51]. However, equipping MLLMs with reasoning capabilities by integrating multimodal medical information remains an 2 Figure 1: Overview of the MMRP Dataset and Chiron-o1 Performance. (a) The MMRP dataset encompasses 12 imaging modalities and 20 body systems. (b) Chiron-o1 achieves SOTA performance across various benchmarks compared to existing multimodal medical models. open and challenging problem. Recent studies [32, 60] have explored designing well-crafted prompts to simulate the reasoning process from medical problems to correct answers, often involving multiple doctor roles or functional modules. However, the applicability of the aforementioned methods is limited. Meanwhile, as DeepSeek-R1 significantly enhances model reasoning capabilities through GRPO [15], an increasing number of studies follow this paradigm to adapt RL to the medical domain. Med-R1 [21], utilizing the GRPO strategy, demonstrates superior performance across different image modalities, enhancing the generalizability of MLLMs in medical reasoning. Similarly, MedVLM-R1 [36] adopts an RL framework to encourage the discovery of human-interpretable reasoning paths. However, these RL-based models rely solely on fixed reward functions (e.g., format and accuracy), neglecting the evaluation of reasoning paths, which can easily lead to superficial or hallucinated reasoning processes. In contrast, our proposed method leverages collaborative search to identify high-quality reasoning paths with minimal hallucinations, thereby enhancing reasoning abilities. Construction of CoT Datasets. The development of CoT datasets for SFT is crucial in multimodal reasoning, setting it apart from traditional models [1, 26, 75, 30] by emphasizing the reasoning process rather than merely providing final answers [57, 59]. Research in this field can be categorized into prompt-based, plan-based, and learning-based methods [58], each contributing uniquely to the CoT generation in multimodal contexts. Prompt-based methods utilize carefully designed prompts to guide models in generating CoT during inference. Simple prompts, such as \"think step-by-step\" [18], initiate creation of reasoning paths for multimodal tasks, while advanced approaches establish clear reasoning workflow by specifying detailed objectives and procedures for each step [9, 12, 34]. Planbased methods enable dynamic exploration of reasoning paths, enhancing adaptability. For example, MM-ToT [14] combines GPT-4 [20] and Stable Diffusion [38], using depth-first and breadth-first searches to select optimal solutions. Additionally, learning-based methods embed CoT construction within model training or fine-tuning. PCoT [52] optimizes this approach for CoT generation, whereas MC-CoT [44] improves reasoning capabilities in smaller-scale models via majority voting during training. However, as constructing medical CoT requires specific logic and domain expertise, the aforementioned methods cannot be effectively transferred to the medical field. In other words, the construction of multimodal CoT in the medical domain remains an underexplored stage. Existing studies [60] primarily rely on manual annotations or model-generated reasoning paths without any evaluation. Our proposed method takes step further by addressing these challenges by leveraging an automatic and collaborative search strategy to identify high-quality reasoning paths."
        },
        {
            "title": "3 Methods",
            "content": "In this section, we elaborate on our method for eliciting effective reasoning capabilities in MLLMs to tackle complex clinical problems by constructing multimodal medical CoT datasets and applying SFT. Specifically, we propose novel curriculum learning scheme tailored for training medical MLLMs. It involves leveraging the constructed dataset MMRP to progressively inject medical knowledge into MLLMs, advancing from foundational principles to complex problems. Concretely, we develop QA and image-text alignment datasets to strengthen the models foundational capabilities in the medical domain. Subsequently, we introduce the MICS strategy to generate high-quality CoT data 3 for complex medical scenarios. MICS also incorporates novel evaluation metric designed to assess the effectiveness of reasoning paths. Ultimately, using the aforementioned datasets, we perform stage-wise SFT to train our model, Chiron-o1, which demonstrates robust reasoning performance. 3.1 MMRP Dataset Currently, numerous websites and publications provide complex clinical cases for educational purposes, such as Radiopaedia [13], BMJ Case Reports [11], and World Journal of Clinical Cases [16]. These platforms aim to enhance learners knowledge and, more crucially, translate it into tangible improvements in individual competence for daily clinical practice. Utilizing the data released in [74] (mainly from Radiopaedia [13]), we additionally aim to recompose training data for robust reasoning capabilities. Specifically, we construct the MMRP dataset with three subsets. Part 1 of the MMRP is designed to create QA dataset at the level of clinical trainees. Initially, we collect over 60K+ cases containing pure text information, encompassing neurological disorders, cardiovascular abnormalities, skeletal system diseases, and more. Subsequently, QA pairs are synthesized in segmented manner based on the information richness of the cases. Furthermore, we compose Part 2 of the MMRP with aligned image-text pairs. Unlike other methods that generate synthetic image captions [6], texts in this subset are entirely sourced from authentic medical imaging analyses in the educational site, covering various imaging findings such as masses and lesions, anatomical abnormalities, inflammatory changes, and more. Further details about Parts 1 and 2 of MMRP are provided in the Appendix A.1 and A.2. Next, we construct multimodal CoT data in Part 3 of the MMRP using MICS to enhance the reasoning capabilities of MLLMs. 3.2 Mentor-Intern Collaborative Search for Effective Reasoning Besides the data in Parts 1 and 2, the MMRP dataset also aims to construct subset of multimodal reasoning paths by leveraging valuable clinical case information. To explore effective reasoning paths, we propose the MICS, multi-model collaborative reasoning path search strategy. The strategy emulates the mentor-intern dynamic, where effective guidance depends on interns performance in correctly solving problems using the prompts. The core idea is to leverage powerful mentor models to iteratively search for and identify valuable reasoning steps, shown in Figure 2. The effectiveness of these steps is determined by feedback from intern models. Inspired by CoT [49, 63] and existing process reward models [53, 55, 73], we believe that models can address problems by thinking in step-by-step manner, thereby deriving answers in reasonable and understandable way. To evaluate the quality of each step, we define its value as the potential of leading to the correct answer. We denote the mentor models as {θ1, . . . , θn} and the intern models as {β1, . . . , βm}. The former are role-played with generalist models, while the latter typically is composed of open-sourced models, often much smaller in size. During searching, at the step k, an intermediate reasoning step generated by θn is represented as sk,n accordingly. The searching begins at the <start point> and subsequently explores effective reasoning paths through iterative search. It involves three key operations. 1) Collaborative search for reasoning paths by multiple mentors The objective of this operation is to combine the knowledge acquired by multiple mentor models to collaboratively search for effective reasoning paths. This approach not only enhances the diversity of reasoning paths but also mitigates the risk of single model unilaterally solving problem, particularly when the models understanding of the problem is biased. Specifically, starting from the <start point> or the optimal path selected in operation 3), each mentor model θ generates complete solution, thereby deriving the correct answer. In practice, this process involves θ acting as completer, extending the reasoning path prefix. The entire process can be formalized as follows: {s1,x, s2,y, ..., sk,z} MICSk( θ1, . . . , θn, β1, . . . , βm) (1) sk+1,v θv( P, Q, A, {s1,x, s2,y, ..., sk,z}, I) (2) where MICSk denotes the search process up to the step k, and {s1,x, s2,y, ..., sk,z} represents the all reasoning steps searched thus far, serving as the reasoning path prefix for the subsequent search. , Q, A, and represent patient information, questions, ground truth, and corresponding images. 2) Evaluation of reasoning paths based on MICS-Score We argue that relying on closed-source MLLMs to evaluate the steps in reasoning path is arbitrary [65]. Instead, the value of reasoning path explored by mentor model θ should be assessed by intern models β. Fundamentally, this 4 Figure 2: Framework of the MICS Strategy. MICS enables search for effective reasoning paths through collaboration between mentor and intern models until the maximum search depth is reached or early-stopping conditions are met. θ denotes the mentor model, and β denotes the intern model. The example of CoT construction using MICS is provided in the Figure 6. operation involves the intern models β completing the entire reasoning process based on the prompt (reasoning path prefix) provided by the mentor model θ. The effectiveness of the reasoning path prefix is then measured by comparing the answer decoded by the intern model β against the ground truth. This validation logic aligns with the principle that mentors guidance is effective only if interns can correctly solve problems based on the provided prompts. Specifically, we employ lightweight models from diverse families with varying temperatures as intern models β to improve sampling diversity and randomness. Ultimately, the quality of reasoning path is defined as the frequency with which it achieves the correct answer. The entire evaluation process is formalized as follows: s>k+1,w, aw βw( Q, {s1,x, s2,y, ..., sk,z, sk+1,v}, I), ranging from 1 to MICS-Score(sk+1) = number of LLM(aw, A) number of aw (3) (4) where s>k+1,w and aw denote the reasoning path completed by the intern model βw and the final answer derived, respectively. Subsequently, we employ an LLM [28] to compare the generated answer with the ground truth to determine its correctness. The proportion of interns answering correctly is calculated as the value score (MICS-Score) of the reasoning path prefix. 3) Selection of the optimal reasoning path Following the previous operations, we obtain the reasoning prefixes generated by different mentor models in the current search iteration, along with their corresponding value scores. In each search iteration, we select the reasoning path with the highest score as the reasoning prefix for the next search, continuing until the maximum search depth is reached. MICS strategy is not solely guided by high-value outcomes but also incorporates exploratory attributes [5]. If multiple reasoning path prefixes in iteration yield the same scores, i.e., score({s1,x, s2,y, ..., sk,z, sk+1,v}) equals score({s1,x, s2,y, ..., sk,z, sk+1,r}). We prioritize the reasoning step generated by mentor model θr that was not selected in previous iterations as the current step. Additionally, during the search process, we implement two early stopping mechanisms. First, if all mentor models in search iteration reach score of zero, the data is marked as search failure, triggering subsequent re-search. Second, if the current reasoning path prefix enables all intern models to derive the correct answer, the search is terminated. Particularly, if multiple full-score reasoning paths exist, we calculate the competitiveness of the corresponding mentor models, as follows: Competitiveness(θv, n) = MICS-Score({s<i, si,v}) (5) i=1 where s<i denotes the reasoning path searched up to the i-th iteration, and Competitiveness(θv, n) represents the competitiveness score of the mentor model at the (n+1)-th search, calculated by multiplying the value scores obtained in previous search iterations. higher competitiveness score indicates that the mentor model possesses clearer insights into the problem compared to other models. 5 (cid:89) In summary, the MICS strategy leverages three key operations to iteratively search until the maximum depth is reached or full-score reasoning path is identified. By applying MICS to complex clinical problems, we can construct set of high-quality, step-by-step reasoning triplets, comprising questions, reasoning paths, and answers, thereby enabling MLLMs to learn to reason. 3.3 Chiron-o1: Multi-Stage SFT for Reasoning Emergence Inspired by curriculum learning [4, 41, 56], we design three-stage model training strategy based on different task types. This strategy emulates the human learning process, progressing from simple to complex knowledge acquisition. Specifically, we begin by training the model on pure text medical QA tasks, gradually enabling it to answer simple medical questions and provide concise explanations. Subsequently, the model is trained to become familiar with the characteristics of medical images, achieving effective image-text alignment through authentic clinical images and their analyses. Unlike methods that generate image findings from images and captions [6], we utilize the original image analyses from cases as the supervision signal during this stage to minimize the introduction of fabricated information. Building on these stages, the final training stage leverages high-quality CoT data, generated by the MICS strategy in complex medical scenarios, to stimulate the models reasoning capabilities. However, the MMRP dataset alone is insufficient to endow the model with comprehensive multimodal medical knowledge, which could hinder the emergence of reasoning abilities. Therefore, we incorporated several commonly-used Visual Question Answering (VQA) datasets [6, 17, 22, 29, 72] to bolster the models foundational capabilities. In practice, all curated datasets are mixed in specific proportions during the training process to train the model effectively. Dl = (VQA_Set, (cid:88) MMRP_Part_l), ranging from 1 to i=1 L(Ω, l) = (cid:88) log Ω(Y, Q) (Q,Y,A)Dl (6) (7) where () denotes the fusion of the utilized datasets in specific proportions, as detailed in the Appendix C. We iterate through all SFT data (consisting of triplets of questions, reasoning paths, and answers, like (Q, Y, A)) from the predefined dataset Dl to train the MLLM Ω."
        },
        {
            "title": "4 Experiments and Results",
            "content": "4.1 Experiment Settings Benchmarks To evaluate our model, we utilize two types of medical benchmarks: VQA benchmarks and reasoning benchmarks. (1) The former focuses on testing the models foundational capabilities, such as visual understanding. We utilized the VQA-RAD [22], SLAKE [29], PathVQA [17], PMC-VQA [72], and the Health & Medicine track subset of the MMMU dataset [68]. (2) The latter is designed to evaluate the models reasoning capabilities when addressing complex problems. This benchmark includes MedXpertQA_MM (multimodal subset) [76] and the MMRP test set. MedXpertQA_MM is divided into two subsets emphasizing reasoning and understanding, respectively. The MMRP test set encompasses two pre-divided subsets of pure text and multimodal reasoning data. Implementation Details In our experiments, the MICS strategy employs three mentor models: ChatGPT-4o [20], Gemini 2.5 Pro Preview [46], and Qwen2.5-VL-72B-Instruct [3]. After careful selection, we designate three open-source models as intern models, each with two distinct temperature (0.3 and 1.2): Qwen2-VL-7B [54], Qwen2.5-VL-7B [3], and InternVL3-8B [8]. We utilize DeepSeekV3 [28] as the \"judge\" to evaluate the alignment between the answers generated by intern models and the ground truth. To balance search accuracy and efficiency, the maximum search depth is set to 4. During the training phase, we adopt InternVL3-8B as the base model for Chiron-o1, employing LoRA for fine-tuning [19], with AdamW as the optimizer, learning rate of 4e-5 following cosine decay schedule, batch size of 16, and bfloat16 mixed precision. The training data comprise the MMRP dataset and commonly-used VQA datasets, with details provided in the Appendix C. Baseline Methods & Evaluation Metrics We compare Chiron-o1 with the following three types of baseline models: (1) General MLLMs, including high-performance general vision models, both closed-source (GPT series [20], Gemini series [46]) and open-source (LLaVA series [30], Qwen 6 Table 1: Main Results on Medical VQA Benchmarks. Our model achieves SOTA performance across various benchmarks. Bold denotes the highest score, and underline denotes the second-highest. Methods Close-Source SOTA Gemini Pro [45] GPT-4o-mini [20] GTP-4o [20] Open-Source SOTA LLaVA-v1.5-7B [30] LLaVA-v1.6-13B [30] LLaVA-v1.6-34B [30] Yi-VL-34B [67] Qwen-VL-Chat [2] Medical MLLM LLaVA-Med [25] Med-Flamingo [35] RadFM [62] GMAI-VL [27] HuatuoGPT-Vision-7B [6] HuatuoGPT-Vision-34B [6] Reasoning Medical Model Med-R1 MedVLM-R1 InternVL3-2B Chiron-o1-2B InternVL3-8B Chiron-o1-8B VQA-RAD SLAKE PathVQA PMC-VQA MMMU(H&M) AVG 60.3 55.8 54.2 54.2 55.8 58.6 53.0 47.0 51.4 45.4 50.6 66.3 63.8 68.1 55.9 61. 68.3 75.4 73.1 76.8 72.6 50.4 50.1 59.4 58.9 67.3 58.9 56.0 48.6 43.5 34.6 72.9 74.5 76.9 55.1 65.9 65.9 85.3 71.1 83. 70.3 48.7 59.2 54.1 51.9 59.1 47.3 55.1 56.8 54.7 38.7 59.9 63.5 53.3 55.2 65.2 70.3 67.9 74.0 39.6 40. 36.4 36.6 44.4 39.5 36.6 24.7 23.3 25.9 54.3 52.7 58.2 45.8 44.8 49.1 54.3 53.2 57.5 47.9 38.2 39.3 48.8 41.5 32. 36.9 28.3 27.0 51.3 49.1 54.4 32.7 35.5 38.4 42.1 52.1 54.6 62.8 48.6 52.1 48.5 48.5 55.6 48.1 45.5 43.7 39.1 35.4 61.2 60.0 64. 48.6 52.6 57.4 65.5+8.1 63.5 69.2+5.7 series [3]). (2) Medical MLLMs, comprising vision models pretrained on specific medical corpora, such as LLaVA-Med [25], Med-Flamingo [35], GMAI-VL [27], and HuatuoGPT-Vision [6]. (3) Medical Reasoning MLLMs, including Med-R1 [21] and MedVLM-R1[36], which are fine-tuned using GRPO [39]. For evaluation metrics, we use choice accuracy to measure model performance on VQA benchmarks. For the open-ended questions requiring reasoning in Part 3 of the MMRP dataset, BERT-Score [71] is employed to assess the semantic similarity between the models final answer and the ground truth, while MICS-Score is used to evaluate the effectiveness of the reasoning path. 4.2 Main Results Performance on the Medical VQA Benchmarks Medical models should strive to further enhance their performance on VQA tasks while improving reasoning capabilities. Therefore, to evaluate the foundational performance of Chiron-o1, we first test it on several commonly-used VQA benchmarks. The results, compared against other mainstream models, are summarized in Table 1. Compared to the baseline models, Chiron-o1 achieves significant performance improvements across five benchmarks. Specifically, Chiron-o1-8B and Chiron-o1-2B outperform their respective baseline models by an average of 5.7% and 8.1%. This demonstrates that our reasoning-focused model further enhances visual understanding and question-answering capabilities. Subsequently, Chiron-o1 outperforms most medical MLLMs across all benchmarks. It even achieves comparable performance to HuatuoGPTVision-34B, which has four times the parameters, on the PMC-VQA and MMMU (Health & Medicine) benchmarks, while significantly surpassing it on the remaining benchmarks, with an overall average improvement of 5%. Finally, we observe that existing medical reasoning models, such as Med-R1 and MedVLM-R1, tend to lose basic VQA capabilities while focusing on reasoning, as shown in Table 1. In contrast, Chiron-o1-8B outperforms them on VQA benchmarks by an average of 20.6% and 16.6%, respectively. Overall, Chiron-o1 demonstrates competitive performance across multiple benchmarks, highlighting its versatility in medical image understanding and question answering. Performance on Medical Reasoning Benchmarks We further evaluate the performance of Chirono1 on reasoning benchmarks, including in-domain (MMRP) and out-of-domain (MedXpertQA_MM) datasets. [37] posits that training multimodal reasoning models may compromise their textual reasoning capabilities. Therefore, we first assess Chiron-o1 on the pure text reasoning data of the MMRP dataset. The results in Table 2 demonstrate that our model significantly outperforms the 7 Table 2: Results on Medical Reasoning Benchmarks. ACC represents accuracy, and MICS-Score refers to the evaluation metric described in Equation 4. indicates pure text reasoning models. Model MedReason* HuatuoGPT-o1* Med-R1 MedVLM-R1 Chiron-o1-2B Chiron-o1-8B MMRP (Pure Text) ACC MedXpertQA_MM (Reasoning) ACC MedXpertQA_MM (Understanding) ACC MMRP (Reasoning) ACC Bert-Score MICS-Score 79.2 85.1 72.7 77.5 90.6 92.1 20.1 21. 19.8 23.3 20.8 20.0 23.1 25.1 28.1 31.2 43.8 58.4 83.4 83. 88.2 90.4 22.5 23.5 32.2 49.4 Figure 3: Case Study on the MMRP Test Set. Compared to other multimodal medical reasoning models, Chiron-o1-8B demonstrates the ability to generate deep and reasonable reasoning paths, leading to correct answers. Due to page limitations, details are provided in the Appendix G. visual reasoning models Med-R1 and MedVLM-R1. Even compared to pure text reasoning models, Chiron-o1 surpasses MedReason and HuatuoGPT-o1 by 12.9% and 7%, respectively. These results strongly validate that our proposed MICS and stage-wise training strategy effectively enhance the models text reasoning capabilities. In the multimodal reasoning domain, Chiron-o1 excels in both the accuracy of final answers and the effectiveness of reasoning paths compared to other reasoning models. The results in Table 2 indicate that Chiron-o1-8B consistently outperforms MedVLM-R1 (the best among other reasoning models) by 27.2% in accuracy and 6.9% in semantic similarity. The effectiveness of reasoning paths is another key focus. Compared to all reasoning models, Chiron-o18B achieves the highest score of 49.4% on the MICS-Score. Our model not only excels on in-domain datasets but also demonstrates robust performance on out-of-domain benchmarks. Chiron-o1-8B outperforms Med-R1 and MedVLM-R1 on MedXpertQA_MM by an average of 3.75% and 3.35%, respectively. Figure 3 illustrates that Chiron-o1 can engage in deep and reasonable reasoning for real-world complex clinical problems, ultimately providing correct answers. 4.3 Ablation Studies Effect of MICS Strategy We apply the MICS strategy to build the MMRP multi-task dataset to improve the models multimodal reasoning. An ablation study on Chiron-o1-8B evaluates its impact. Unlike MICS, the vanilla method enables the mentor model to generate reasoning directly, bypassing the evaluation of intern models. Table 3 shows that excluding reasoning data (first row) or using reasoning paths constructed by vanilla method(second and third rows) degrades performance. Compared to Chiron-o1-8B, the former reduces average performance by 3% across four benchmarks, and the latter by 2.7%. Furthermore, Figure 4(a) illustrates the differences in 8 effectiveness between reasoning paths constructed using MICS and the vanilla method across three distinct medical scenarios. We compute the MICS-Score step-by-step for an entire reasoning path to evaluate the value of reasoning prefixes and analyze the trend of MICS-Score changes (categorized into four types, with monotonically increasing score indicating an effective trend and the others deemed low-value, details provided in the Appendix B). As depicted in Figure 4(a), the proportion of effective reasoning paths identified by MICS significantly surpasses that of the vanilla method. These results underscore the critical role of the MICS strategy in searching for effective reasoning paths. Table 3: Ablation Studies on Training Set. We examine how different dataset combinations affect performance. MMRP() indicates direct use of mentor models for reasoning path search, while MMRP() denotes MICS-based search. VQA shows whether medical VQA datasets are included in training. QC indicates whether quality control is applied to MICS-searched reasoning paths. MMRP VQA QC VQA-RAD SLAKE MMMU(H&M) MedXpertQA_MM 73.6 71.3 75.7 72.4 74.3 76. 80.3 76.9 81.2 75.0 82.6 83.2 49.7 52.3 51.6 51.1 49.3 54.6 23.2 23.4 23.6 24.0 22.6 24.2 Figure 4: Ablation Studies on MICS and Model Training. (a) Contribution of the MICS strategy to reasoning path score trends, with a, b, and denoting three clinical scenarios (Appendix A.3). \"vanilla\" refers to directly generating reasoning paths using the mentor model without evaluation. (b) Investigation of the advantages of our training strategy compared to other training methods. Effect of Training Set Settings During training, we add VQA datasets to enhance the models visual understanding. To assess their impact, we compare model trained without VQA datasets to Chiron-o1-8B. Table 3 (second and fourth rows) shows VQA data improves performance on simpler benchmarks, with VQA-RAD and SLAKE increasing by 5% and 7.3%, respectively, but has limited effect on complex reasoning benchmarks. We also examine whether quality control of MMRP reasoning data boosts reasoning performance. Table 3 (fifth row) indicates quality control raises accuracy on MMMU (H&M) from 49.3% to 54.6% (+5.3%) and on MedXpertQA_MM from 22.6% to 24.2% (+1.6%). These results confirm the effectiveness of our train set configuration. Effect of Training Strategy In Section 3.3, we introduce stage-wise training strategy for model fine-tuning. To assess its impact, we traine models using stage subsets and compare their performance against Chiron-o1-8B on multiple benchmarks. For the complex VQA benchmark, such as MMMU (H&M), Figure 4(b) shows that models trained with only Stage 1, Stage 1+2, or Stage 3 exhibit performance drops of 6.9%, 6.2%, and 3.5%, respectively, compared to Chiron-o1-8B. Omitting Stage 3 notably weakens reasoning capabilities, with models trained on Stage 1 or Stage 1+2 scoring 3.1% and 0.25% lower on average than Stage 3 alone on MMMU (H&M) and MedXpertQA_MM. Similarly, training only with Stage 3, without enhancing visual question-answering, significantly reduces performance on VQA-RAD and SLAKE compared to other ablation models."
        },
        {
            "title": "5 Conclusion",
            "content": "This paper introduces MICS, multi-model collaborative search strategy that generates high-quality multimodal medical reasoning data by preserving valid reasoning steps and eliminating incorrect ones, enhancing medical CoT construction efficiency. With stage-wise fine-tuning approach, we use MICS to create MMRP, multi-task medical reasoning dataset with varying difficulty. This enables Chiron-o1, robust multimodal model, to achieve SOTA performance across benchmarks. We believe this work advances medical CoT data construction and improves reasoning in medical MLLMs."
        },
        {
            "title": "References",
            "content": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. [2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [4] Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pages 4148, 2009. [5] Cameron Browne, Edward Powley, Daniel Whitehouse, Simon Lucas, Peter Cowling, Philipp Rohlfshagen, Stephen Tavener, Diego Perez, Spyridon Samothrakis, and Simon Colton. survey of monte carlo tree search methods. IEEE Transactions on Computational Intelligence and AI in games, 4(1):143, 2012. [6] Junying Chen, Chi Gui, Ruyi Ouyang, Anningzhe Gao, Shunian Chen, Guiming Hardy Chen, Xidong Wang, Ruifei Zhang, Zhenyang Cai, Ke Ji, et al. Huatuogpt-vision, towards injecting medical visual knowledge into multimodal llms at scale. arXiv preprint arXiv:2406.19280, 2024. [7] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. Science China Information Sciences, 67(12):220101, 2024. [8] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2418524198, 2024. [9] Zhenfang Chen, Qinhong Zhou, Yikang Shen, Yining Hong, Hao Zhang, and Chuang Gan. See, think, confirm: Interactive prompting between vision and language models for knowledge-based visual reasoning. arXiv preprint arXiv:2301.05226, 2023. [10] Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: comparative study of foundation model post-training. arXiv preprint arXiv:2501.17161, 2025. [11] Nathan Douthit and Seema Biswas. Global health education and advocacy: using bmj case reports to tackle the social determinants of health. Frontiers in public health, 6:114, 2018. [12] Hao Fei, Shengqiong Wu, Wei Ji, Hanwang Zhang, Meishan Zhang, Mong-Li Lee, and Wynne Hsu. Video-of-thought: Step-by-step video reasoning from perception to cognition. arXiv preprint arXiv:2501.03230, 2024. [13] Gaillard et al. Radiopaedia: building an online radiology resource. European Congress of Radiology-RANZCR ASM 2011, 2011. [14] Kye Gomez. Multimodal-tot, 2023. https://github.com/kyegomez/MultiModal-ToT. 10 [15] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [16] Mayank Gupta and Aditya Sharma. Fear of missing out: brief overview of origin, theoretical underpinnings and relationship with mental health. World journal of clinical cases, 9(19):4881, 2021. [17] Xuehai He, Yichen Zhang, Luntian Mou, Eric Xing, and Pengtao Xie. Pathvqa: 30000+ questions for medical visual question answering. arXiv preprint arXiv:2003.10286, 2020. [18] Vaishnavi Himakunthala, Andy Ouyang, Daniel Rose, Ryan He, Alex Mei, Yujie Lu, Chinmay Sonar, Michael Saxon, and William Yang Wang. Lets think frame by frame with vip: video infilling and prediction dataset for evaluating video chain-of-thought. arXiv preprint arXiv:2305.13903, 2025. [19] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. [20] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [21] Yuxiang Lai, Jike Zhong, Ming Li, Shitian Zhao, and Xiaofeng Yang. Med-r1: Reinforcement learning for generalizable medical reasoning in vision-language models. arXiv preprint arXiv:2503.13939, 2025. [22] Jason Lau, Soumya Gayen, Asma Ben Abacha, and Dina Demner-Fushman. dataset of clinically generated visual questions and answers about radiology images. Scientific data, 5(1):110, 2018. [23] Hugo Laurençon, Andrés Marafioti, Victor Sanh, and Léo Tronchon. Building and better understanding vision-language models: insights and future directions. In Workshop on Responsibly Building the Next Generation of Multimodal Foundational Models, 2024. [24] Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, and Shixiang Shane Gu. Aligning text-to-image models using human feedback. arXiv preprint arXiv:2302.12192, 2023. [25] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training large language-and-vision assistant for biomedicine in one day. Advances in Neural Information Processing Systems, 36:2854128564, 2023. [26] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR, 2023. [27] Tianbin Li, Yanzhou Su, Wei Li, Bin Fu, Zhe Chen, Ziyan Huang, Guoan Wang, Chenglong Ma, Ying Chen, Ming Hu, et al. Gmai-vl & gmai-vl-5.5 m: large vision-language model and comprehensive multimodal dataset towards general medical ai. arXiv preprint arXiv:2411.14522, 2024. [28] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [29] Bo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, and Xiao-Ming Wu. Slake: semanticallylabeled knowledge-enhanced dataset for medical visual question answering. In 2021 IEEE 18th international symposium on biomedical imaging (ISBI), pages 16501654. IEEE, 2021. [30] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. [31] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [32] Jiaxiang Liu, Yuan Wang, Jiawei Du, Joey Tianyi Zhou, and Zuozhu Liu. Medcot: Medical chain of thought via hierarchical expert. arXiv preprint arXiv:2412.13736, 2024. 11 [33] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. [34] Fanxu Meng, Haotong Yang, Yiding Wang, and Muhan Zhang. Chain of images for intuitively reasoning. arXiv preprint arXiv:2311.09241, 2023. [35] Michael Moor, Qian Huang, Shirley Wu, Michihiro Yasunaga, Yash Dalmia, Jure Leskovec, Cyril Zakka, Eduardo Pontes Reis, and Pranav Rajpurkar. Med-flamingo: multimodal medical few-shot learner. In Machine Learning for Health (ML4H), pages 353367. PMLR, 2023. [36] Jiazhen Pan, Che Liu, Junde Wu, Fenglin Liu, Jiayuan Zhu, Hongwei Bran Li, Chen Chen, Cheng Ouyang, and Daniel Rueckert. Medvlm-r1: Incentivizing medical reasoning capability of vision-language models (vlms) via reinforcement learning. arXiv preprint arXiv:2502.19634, 2025. [37] Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, and Xu Yang. Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl. arXiv preprint arXiv:2503.07536, 2025. [38] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models, 2021. [39] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [40] Ming Shen. Rethinking data selection for supervised fine-tuning. arXiv preprint arXiv:2402.06094, 2024. [41] Petru Soviany, Radu Tudor Ionescu, Paolo Rota, and Nicu Sebe. Curriculum learning: survey. International Journal of Computer Vision, 130(6):15261565, 2022. [42] Kuan-Wu Su, Jenq-Shiou Leu, Min-Chieh Yu, Yong-Ting Wu, Eau-Chung Lee, and Tian Song. Design and implementation of various file deduplication schemes on storage devices. Mobile Networks and Applications, 22:4050, 2017. [43] Yanzhou Su, Tianbin Li, Jiyao Liu, Chenglong Ma, Junzhi Ning, Cheng Tang, Sibo Ju, Jin Ye, Pengcheng Chen, Ming Hu, et al. Gmai-vl-r1: Harnessing reinforcement learning for multimodal medical reasoning. arXiv preprint arXiv:2504.01886, 2025. [44] Cheng Tan, Jingxuan Wei, Zhangyang Gao, Linzhuang Sun, Siyuan Li, Ruifeng Guo, Bihui Yu, and Stan Li. Boosting the power of small multimodal reasoning models to match larger models with self-consistency training. In European Conference on Computer Vision, pages 305322. Springer, 2024. [45] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [46] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [47] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. [48] Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. [49] Omkar Thawakar, Dinura Dissanayake, Ketan More, Ritesh Thawkar, Ahmed Heakl, Noor Ahsan, Yuhao Li, Mohammed Zumri, Jean Lahoud, Rao Muhammad Anwer, et al. Llamav-o1: Rethinking step-by-step visual reasoning in llms. arXiv preprint arXiv:2501.06186, 2025. [50] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [51] Bingning Wang, Haizhou Zhao, Huozhi Zhou, Liang Song, Mingyu Xu, Wei Cheng, Xiangrong Zeng, Yupeng Zhang, Yuqi Huo, Zecheng Wang, et al. Baichuan-m1: Pushing the medical capability of large language models. arXiv preprint arXiv:2502.12671, 2025. 12 [52] Lei Wang, Yi Hu, Jiabang He, Xing Xu, Ning Liu, Hui Liu, and Heng Tao Shen. T-sciq: Teaching multimodal chain-of-thought reasoning via large language model signals for science question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1916219170, 2024. [53] Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. arXiv preprint arXiv:2312.08935, 2023. [54] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [55] Weiyun Wang, Zhangwei Gao, Lianjie Chen, Zhe Chen, Jinguo Zhu, Xiangyu Zhao, Yangzhou Liu, Yue Cao, Shenglong Ye, Xizhou Zhu, et al. Visualprm: An effective process reward model for multimodal reasoning. arXiv preprint arXiv:2503.10291, 2025. [56] Xin Wang, Yudong Chen, and Wenwu Zhu. survey on curriculum learning. IEEE transactions on pattern analysis and machine intelligence, 44(9):45554576, 2021. [57] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. [58] Yaoting Wang, Shengqiong Wu, Yuecheng Zhang, William Wang, Ziwei Liu, Jiebo Luo, and Hao Fei. Multimodal chain-of-thought reasoning: comprehensive survey. arXiv preprint arXiv:2503.12605, 2025. [59] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [60] Lai Wei, Wenkai Wang, Xiaoyu Shen, Yu Xie, Zhihao Fan, Xiaojin Zhang, Zhongyu Wei, and Wei Chen. Mc-cot: modular collaborative cot framework for zero-shot medical-vqa with llm and mllm integration. arXiv preprint arXiv:2410.04521, 2024. [61] Chaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya Zhang, Weidi Xie, and Yanfeng Wang. Pmcllama: toward building open-source language models for medicine. Journal of the American Medical Informatics Association, 31(9):18331843, 2024. [62] Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Towards generalist foundation model for radiology by leveraging web-scale 2d&3d medical data. arXiv preprint arXiv:2308.02463, 2023. [63] Guowei Xu, Peng Jin, Li Hao, Yibing Song, Lichao Sun, and Li Yuan. Llava-o1: Let vision language models reason step-by-step. arXiv preprint arXiv:2411.10440, 2024. [64] Lin Yang, Shawn Xu, Andrew Sellergren, Timo Kohlberger, Yuchen Zhou, Ira Ktena, Atilla Kiraly, Faruk Ahmed, Farhad Hormozdiari, Tiam Jaroensri, et al. Advancing multimodal medical capabilities of gemini. arXiv preprint arXiv:2405.03162, 2024. [65] Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin Song, Haocheng Feng, Li Shen, et al. Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search. arXiv preprint arXiv:2412.18319, 2024. [66] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. [67] Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Guoyin Wang, Heng Li, Jiangcheng Zhu, Jianqun Chen, et al. Yi: Open foundation models by 01. ai. arXiv preprint arXiv:2403.04652, 2024. [68] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal In Proceedings of the IEEE/CVF understanding and reasoning benchmark for expert agi. Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. 13 [69] Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. [70] Diego Zamfirescu-Pereira, Richmond Wong, Bjoern Hartmann, and Qian Yang. Why johnny cant prompt: how non-ai experts try (and fail) to design llm prompts. In Proceedings of the 2023 CHI conference on human factors in computing systems, pages 121, 2023. [71] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675, 2019. [72] Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-vqa: Visual instruction tuning for medical visual question answering. arXiv preprint arXiv:2305.10415, 2023. [73] Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. The lessons of developing process reward models in mathematical reasoning. arXiv preprint arXiv:2501.07301, 2025. [74] Qiaoyu Zheng, Weike Zhao, Chaoyi Wu, Xiaoman Zhang, Lisong Dai, Hengyu Guan, Yuehua Li, Ya Zhang, Yanfeng Wang, and Weidi Xie. Large-scale long-tailed disease diagnosis on radiology images. Nature Communications, 15(1):10147, 2024. [75] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. [76] Yuxin Zuo, Shang Qu, Yifei Li, Zhangren Chen, Xuekai Zhu, Ermo Hua, Kaiyan Zhang, Ning Ding, and Bowen Zhou. Medxpertqa: Benchmarking expert-level medical reasoning and understanding. arXiv preprint arXiv:2501.18362, 2025."
        },
        {
            "title": "A Deatails of MMRP",
            "content": "Figure 5: Distribution of MMRP across various systems and modalities. A.1 Part 1: Starting with Simple Knowledge Injection! The \"simple\" refers not only to task difficulty but also to the number of modalities involved. It is widely acknowledged that models robust reasoning capabilities are built upon extensive pretraining with vast datasets. Accordingly, Part 1 of the MMRP dataset is designed to create text-only QA dataset at the level of medical interns or clinical interns. Initially, we collected 60,789 cases containing textual information. Due to missing critical information in the medical imaging analysis or case summary and discussion D, incomplete cases were filtered out. Subsequently, each case was categorized into four levels based on the token count of and D, as calculated by tokenizer [3], to assess the information richness of the case. Cases with greater information content were deemed suitable for generating more QA pairs, while those with limited information were used sparingly to avoid redundancy in QA pairs. We employed DeepSeek-V3 [28] as the LLM to generate QA pairs in segmented manner, including the question q, question options o, the correct answer a, and the rationale r, as outlined below: token_number = tokenizer(C, D) (8) (q, o, a, r) LLM( promptx(P, C, D) ) , promptx depends on token_number (9) where refers to basic information of the patients. The promptx is provided in the Figure 7. Among the generated QA pairs, some were of low quality, irrelevant to medicine, or failed to parse correctly. These inappropriate QA pairs were removed. Additionally, we filtered out data that could cause ambiguity or hallucinations (e.g., content containing phrases like this case or this image) to ensure that the generated questions could be answered independently. Ultimately, Part 1 of the MMRP dataset comprises 57,630 QA quadruplets. A.2 Part 2: Understanding the Content Conveyed by Images! To enable the model to perform reasoning in multimodal medical scenarios, it is essential to align multimodal information using image-text pairs derived from real medical images and their corresponding analyses. Unlike Part 1, this subset of data includes only annotated cases (i.e., cases with explicitly identified key information, such as lesion locations, on specific slices), totaling approximately 3K cases and covering 12 distinct imaging modalities. As the imaging analysis may reference multiple key findings associated with the same image, we employed file-level MD5 hashing to deduplicate images [42] and mitigate hallucination risks. Additionally, low-resolution images were filtered out, ensuring minimum dataset resolution of 196196. During training, we treat each modalitys image-text pairs within case as single data unit. However, if modality contains an excessive number of keyframe images, it may lead to misalignment between images and text [24] or cause out-of-memory issues during training and inference. Consequently, we excluded image-text pairs with more than 10 keyframes. In constructing the training data, we designed two distinct alignment rules: coarse alignment and precise alignment. The former emphasizes holistic understanding of multiple images, ensuring that 15 the sequence of image descriptions corresponds to the order of the images. The latter focuses on mapping specific keywords in the text to their corresponding images (e.g., like . . . heart failure (image x) . . . ). Notably, the image description information in this subset is directly sourced from authentic medical imaging analyses in Radiopaedia [13], rather than being synthetically generated. Given that even SOTA MLLMs struggle to accurately interpret medical images, Part 2 of the MMRP dataset is designed to minimize hallucination risks. Consequently, we constructed dataset of 5,878 triplets, each comprising keyframe image, question about the image, and the imaging analysis, to serve as image-text alignment data. A.3 Part 3: Learning to Reason for Complex Problems via MICS! Data Collection Unlike Parts 1 and 2 described earlier, this subset aims to synthesize multimodal reasoning processes by leveraging valuable clinical case information. Specifically, during CoT synthesis, we utilize broader range of authentic clinical information to minimize unfounded model hallucinations, as opposed to relying solely on QA pairs. For the collected cases, we designed three typical clinical QA scenarios using DeepSeek-V3 [28], resulting in total of 8,328 complex visual QA pairs, as outlined below (see Figure 9, 10 and 11 for prompts): Patient-to-Doctor: Questions posed from the patients perspective, reflecting confusion about doctors explanation or concerns about their condition. These are typically colloquial, lacking medical knowledge, and may carry emotional or binary (yes/no) undertones. Doctor-to-Doctor: Questions framed from physicians perspective, emulating professional exchanges between doctors regarding specific aspects of cases condition, diagnosis, or treatment. These focus on details from the chief complaint, imaging analysis, and clinical summary, and are presented in an open-ended discussion format. Intern-to-Senior: Simulating an intern consulting senior physician on complex or challenging clinical issues within case, often requiring reasoning. Questions primarily focus on the analysis of imaging results and are posed in an open-ended format. Implentation Details in MICS The MICS strategy leverages three key operations (Section 3.2) to iteratively search until the maximum depth is reached or full-score reasoning path is identified. By applying MICS to complex clinical problems, we construct set of high-quality, step-by-step reasoning data, thereby enabling MLLMs to learn reasoning progressively. Notably, to reduce data construction costs, we configure three mentor models [3, 20, 46] and six distinct intern models [3, 8] (three models, each with two different temperatures) to execute the search. Additionally, if the (n + 1)-th step is generated by mentor model θ, θ can directly adopt the (n + 2)-th step from the complete solution generated in the previous search round, thereby avoiding redundant reasoning during the search process. Furthermore, we set the maximum search depth to 4. Unlike mathematical problem-solving, which may require dozens of steps, resolving complex medical problems typically involves approximately 4 to 7 steps, encompassing medical history analysis, image interpretation, differential diagnosis, and more. Given the limited number of steps, the basic reasoning logic is generally established by the time the maximum depth is reached. If the path still fails to enable interns to derive the correct answer, it indicates very low-quality reasoning path, rendering further exploration unproductive. Finally, we perform quality control on the data from completed searches. Reasoning paths exhibiting characteristics such as \"no upward trend\", \"consistently zero\", or \"rising then falling\" are flagged as low-quality data (see Appendix for details)."
        },
        {
            "title": "B Metrics",
            "content": "As the reasoning dataset in MMRP consists of open-ended questions, we not only employ DeepSeekV3 [28] as the judge to determine the correctness of generated answers but also use the text embedding model \"roberta-large\" [33] to compute semantic similarity between the models final answer and the ground truth. MICS-Score is used to evaluate the effectiveness of the reasoning path. Trend of Path Scores When using MICS to search for effective reasoning paths, we obtain path score that records the highest MICS-Score of the reasoning step selected in each search iteration. Evidently, as the reasoning process deepens, this score should exhibit gradually increasing trend. To evaluate the effectiveness of the MICS strategy, we analyze the trend of path score changes and 16 Figure 6: Qualitative illustration of effective medical reasoning paths search using MICS. compare it with the vanilla method that does not employ MICS, with results presented in Figure 4 (a). We define four distinct trends: (1) Monotonically increasing, where effective reasoning paths are expected to show steadily increasing or at least stable scores. (2) Non-increasing, characterized by an overall downward trend. (3) Constant, where the path score remains unchanged. (4) Fluctuating, indicating unstable search with scores that vary unpredictably."
        },
        {
            "title": "C Training Sets",
            "content": "During the training process, we combine commonly-used medical VQA datasets with MMRP to train Chiron-o1. This approach aims to further enhance the models visual understanding and questionanswering capabilities, laying the groundwork for improved reasoning abilities. Results from ablation studies  (Table 3)  indicate that our configuration of the training set is reasonable and effective. Table 4: The distribution of datasets used in each training stage. \"HuatuoV_A\" and \"HuatuoV_I\" refer to the Huatuo_PubMedVision_Alignment and Huatuo_PubMedVision_InstructionTuning VQA datasets, respectively. Dataset MMRP Part 1 MMRP Part 2 MMRP Part 3 HuatuoV_A HuatuoV_I PMC_VQA VQA_RAD SLAKE PATH_VQA Stage 1 111260 129351 129351 30520 1794 3951 3934 Stage 2 55630 58780 129351 129351 30520 1794 3951 3934 Stage 3 55630 29390 183150 646759 646759 152603 8970"
        },
        {
            "title": "D Experiments",
            "content": "Since the MMMU(H&M) benchmark encompasses five distinct categories of medical questions, we compare Chiron-o1 with existing medical reasoning models, Med-R1 and MedVLM-R1, to analyze their performance across these subdomains. The results in Table 5 demonstrate that our model achieves substantial performance improvements across all categories of the test set. Notably, Chiron-o1 exhibits significant performance gains in \"Clinical Medicine\", \"Diagnostics and Laboratory Medicine\", and \"Pharmacy\", which focus heavily on clinical problems. This is attributed to the MMRP dataset, constructed based on complex clinical cases, enabling our model to demonstrate superior performance in addressing clinical problems. Table 5: Results on the test set of MMMU(H&M). The subset is divided into five categories: BMS for Basic Medical Science, CM for Clinical Medicine, DLM for Diagnostics and Laboratory Medicine, for Pharmacy, and PH for Public Health. Model Med-R1 MedVLM-R1 Chiron-o1-2B Chiron-o1-8B"
        },
        {
            "title": "E Limitations",
            "content": "BMS CM DLM 38.1 39.6 42.9 55.7 27.8 29.0 42.0 48.1 32.7 36.4 42.5 54.9 29.1 33.9 44.1 56.2 PH AVG 32.7 33.7 35.5 35.6 42.1 39.1 54.6 54.5 Our work introduces novel reasoning path search strategy, MICS, to efficiently construct multimodal medical CoT data. Using the reasoning dataset established with MICS, we develop Chiron-o1, which exhibits robust visual understanding and reasoning capabilities. This may offer new insights and perspectives for multimodal reasoning in the medical domain. However, our approach has certain limitations to consider: (1) The MICS strategy requires collaboration between mentor models and intern models to search for reasoning paths, necessitating numerous costly API requests and valuable computational resources. (2) The multimodal reasoning dataset MMRP we proposed requires further expansion in scale, which will be focus of our future work. Meanwhile, as medical CoT construction methods like MICS continue to advance, we hope the emergence of more reasoning datasets in the future."
        },
        {
            "title": "F Prompts",
            "content": "Figure 7: System prompt for generating QA pairs in Part 1 of MMRP. Figure 8: Input prompt for generating QA pairs in Part 1 of MMRP. 19 Figure 9: System prompt for constructing \"Patient-to-Doctor\" VQA data in Part 3 of MMRP. Figure 10: System prompt for constructing \"Doctor-to-Doctor\" VQA data in Part 3 of MMRP. Figure 11: System prompt for constructing \"Intern-to-Senior\" VQA data in Part 3 of MMRP. Figure 12: Input prompt for constructing VQA data in Part 3 of MMRP. Figure 13: Prompt for the intern model to perform reasoning based on the mentor models guidance. Figure 14: Prompt for comparing the final answer produced by the intern model with the ground truth. 21 Figure 15: Prompt for the mentor model to exploring reasoning paths based on prefix."
        },
        {
            "title": "G Qualitative Analysis of Medical Reasoning Models",
            "content": "Figure 16: Reasoning result of Med-R1. 22 Figure 17: Reasoning result of MedVLM-R1. Figure 18: Reasoning result of Chiron-o1."
        },
        {
            "title": "H Case Study",
            "content": "Figure 19: Chiron-o1 Reasoning on Example 1. 24 Figure 20: Chiron-o1 Reasoning on Example 2. 25 Figure 21: Chiron-o1 Reasoning on Example 3. Figure 22: Chiron-o1 Reasoning on Example 4. 27 Figure 23: Chiron-o1 Reasoning on Example 5. 28 Figure 24: Chiron-o1 Reasoning on Example 6."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Shanghai Artificial Intelligence Laboratory",
        "Shanghai Jiao Tong University"
    ]
}