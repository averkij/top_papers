{
    "paper_title": "Gaussian Splatting with Discretized SDF for Relightable Assets",
    "authors": [
        "Zuo-Liang Zhu",
        "Jian Yang",
        "Beibei Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "3D Gaussian splatting (3DGS) has shown its detailed expressive ability and highly efficient rendering speed in the novel view synthesis (NVS) task. The application to inverse rendering still faces several challenges, as the discrete nature of Gaussian primitives makes it difficult to apply geometry constraints. Recent works introduce the signed distance field (SDF) as an extra continuous representation to regularize the geometry defined by Gaussian primitives. It improves the decomposition quality, at the cost of increasing memory usage and complicating training. Unlike these works, we introduce a discretized SDF to represent the continuous SDF in a discrete manner by encoding it within each Gaussian using a sampled value. This approach allows us to link the SDF with the Gaussian opacity through an SDF-to-opacity transformation, enabling rendering the SDF via splatting and avoiding the computational cost of ray marching.The key challenge is to regularize the discrete samples to be consistent with the underlying SDF, as the discrete representation can hardly apply the gradient-based constraints (\\eg Eikonal loss). For this, we project Gaussians onto the zero-level set of SDF and enforce alignment with the surface from splatting, namely a projection-based consistency loss. Thanks to the discretized SDF, our method achieves higher relighting quality, while requiring no extra memory beyond GS and avoiding complex manually designed optimization. The experiments reveal that our method outperforms existing Gaussian-based inverse rendering methods. Our code is available at https://github.com/NK-CS-ZZL/DiscretizedSDF."
        },
        {
            "title": "Start",
            "content": "Zuo-Liang Zhu1 1Nankai University Jian Yang1 Beibei Wang2 2Nanjing University 5 2 0 2 1 ] . [ 1 9 2 6 5 1 . 7 0 5 2 : r Figure 1. We present relightable Gaussian splatting framework that introduces discretized SDF to promote decomposition quality. We show five relighting results (left) and their normal estimation (right), including Tbell and Horse from NeRO [17], Helmet and Car from Ref-NeRF [26], and Qilin from NeILF++ [37], where Qilin is real scene. Our method demonstrates robust decomposition of geometry and material for various objects, leading to photo-realistic object relighting."
        },
        {
            "title": "Abstract",
            "content": "3D Gaussian splatting (3DGS) has shown its detailed expressive ability and highly efficient rendering speed in the novel view synthesis (NVS) task. The application to inverse rendering still faces several challenges, as the discrete nature of Gaussian primitives makes it difficult to apply geometry constraints. Recent works introduce the signed distance field (SDF) as an extra continuous representation to regularize the geometry defined by Gaussian primitives. It improves the decomposition quality, at the cost of increasing memory usage and complicating training. Unlike these works, we introduce discretized SDF to represent the continuous SDF in discrete manner by encoding it within each Gaussian using sampled value. This approach allows us to link the SDF with the Gaussian opacity through an SDF-to-opacity transformation, enabling rendering the SDF via splatting and avoiding the computational cost of ray marching. The key challenge is to regularize the discrete samples to be consistent with the underlying SDF, as the discrete representation can hardly apply the gradientbased constraints (e.g., Eikonal loss). For this, we project Corresponding author. 1PCA Lab, VCIP, College of Computer Science, Nankai University 2School of Intelligence Science and Technology, Nanjing University, Suzhou, China. Gaussians onto the zero-level set of SDF and enforce alignment with the surface from splatting, namely projectionbased consistency loss. Thanks to the discretized SDF, our method achieves higher relighting quality, while requiring no extra memory beyond GS and avoiding complex manually designed optimization. The experiments reveal that our method outperforms existing Gaussian-based inverse rendering methods. Our code is available at https: //github.com/NK-CS-ZZL/DiscretizedSDF. 1. Introduction While creating relightable 3D assets from multi-view images is valuable for many downstream tasks (e.g., augmented reality, virtual reality), it is challenging due to the ambiguity from decomposing geometry, material, and light. key point for decomposing these factors is the regularization of geometry, as high-quality geometry is the prerequisite to estimate reasonable material and light. The signed distance field (SDF) has been introduced into neural radiance field (NeRF) [21] as an effective geometric prior, as NeuS [28]. Due to SDF constraints, several methods [14, 25, 42] have shown robust decomposition of geometry and material. Unfortunately, these methods require long training and rendering time, due to the expensive ray1 marching operation. More recently, 3D Gaussian splatting (3DGS) [12] achieves more detailed appearance modeling while enabling real-time rendering and fast training. It has also been introduced into inverse rendering [6, 9, 16, 44]. However, the decomposition in the GS framework becomes more problematic, due to insufficient geometric constraints. Several works [32, 35, 41, 44] optimize an extra SDF network with Gaussians jointly for geometry regularization. While these designs improve the geometry quality, they increase the memory usage and require complex optimization strategies (e.g., warm-up [35], or multi-stage training [44]) to balance two representations. In this paper, we introduce discretized SDF to represent the continuous SDF in discrete manner by encoding it within each Gaussian using sampled value. This design enables effective geometry regularization in Gaussianbased relighting frameworks while maintaining simplicity and memory efficiency. We link the SDF with the Gaussian opacity through an SDF-to-opacity transformation, allowing rendering the SDF via splatting and avoiding the computational cost of ray marching. The key problem is to regularize SDF with the discrete samples, as the gradientbased losses used to regularize SDF (e.g., Eikonal loss [7]) are unfeasible for the discrete samples. For this, we project Gaussians onto the zero-level set of SDF and enforce alignment with the surface from splatting, namely projectionbased consistency loss, proven to be an approximation of the Eikonal loss. This way, we unify the representation between SDF and Gaussian, benefiting from the flexibility of the Gaussian primitive and the robustness of SDF while maintaining simplicity, without introducing extra representation. Besides, inspired by previous SDF works [28, 33], we propose spherical initialization tailored for Gaussian splatting, which avoids falling into local minima and improves the robustness of the model. As result, our method provides high-quality and robust decomposition for objects across various materials, enables realistic relighting with only 20% of memory usage compared to previous methods [44], and simplifies the optimization process, thanks to the unified representation. In summary, our main contributions include novel regularization in the GS framework with an discretized SDF to improve decomposition quality, projection-based loss to regularize discretized SDF, proven as an approximation of the Eikonal loss, and extensive experiments show our method outperforms existing Gaussian-based methods with diverse metrics. 2. Related works 2.1. Inverse rendering Inverse rendering aims to decompose geometry, material, and light from multi-view RGB images. One group of methods adopts the geometry formulation from NeRF [21] in density [3, 4, 10, 24] or SDF manner [2, 25, 27, 38, 39, 42]. Due to the well-constrained geometry of the SDF, SDFbased methods can decompose the material and geometry robustly. Especially, NeRO [17] can handle challenging reflective objects by explicitly integrating the rendering equation. TensoSDF [14] uses roughness-aware integration of radiance and reflectance fields to manage diverse materials. However, these methods rely on expensive ray marching and suffer from slow training speed with high memory usage in training. Unlike prior works, our discretized SDF ensures robust decomposition across various materials, while maintaining fast training speed and low memory costs. More recently, Gaussian-based inverse rendering methods have emerged. These methods regularize geometry from Gaussians variously. GShader [9] and GIR [23] link the shortest axis of Gaussian with the normal. GShader, R3DG [6], and GS-IR [16] introduce loss to align the Gaussian normal with the depth-derived one. Later, SDF is introduced into this field to regularize geometry. DeferredGS [32] distills the normal from the pretrained SDF to refine the normal from Gaussians. GS-ROR [44] utilizes the normal and depth as bridge to optimize the Gaussian and SDF jointly. However, introducing an extra SDF network makes the model larger and harder to optimize. Instead of using additional continuous representation for SDF, our work aims to directly encode discrete values of SDF samples on Gaussian primitives and regularize the geometry via this discretized SDF. In this way, our framework benefits from both GS and SDF within unified representation, leading to more lightweight and efficient model. 2.2. Learning SDF from multi-view images SDF is fundamental surface representation [19] adopted by various tasks (e.g., surface reconstruction, inverse rendering). Particularly, NeuS [28] that links the SDF with the density in NeRF [21] became mainstream for learning SDF In their framework, the SDF is from multi-view images. stored by multi-layer perceptrons [33, 36] or grid-like representations [14, 15, 22, 29], which is continuous and convenient to obtain the gradient to regularize the SDF [7, 31, 36]. Recently, 3DGS [12] became new framework for the multi-view stereo, providing impressive real-time novel view synthesis (NVS) results. However, due to its discrete nature, reconstructing an accurate and continuous surface is challenging. To address this, some works optimize the SDF with 3DGS jointly. NeuSG [5] align the SDF gradient with the Gaussian normal. 3DGSR [18] builds an SDF-toopacity transformation and regularizes the depth along with normal from Gaussians by the ones from SDF. GSDF [35] utilizes the depth from Gaussians to accelerate the sampling and manipulate densifying/pruning via SDF. GS-Pull [41] introduces the pulling operation to align the zero-level set Figure 2. Overview of our framework. Our model discretizes the underlying continuous SDF into discrete samples and encodes the SDF values of samples on Gaussian primitives. We propose the median loss to facilitate convergence and projection-based consistency loss to regularize the discrete values of SDF samples. Besides, we design spherical initialization for foreground objects to avoid local minima. and the Gaussian position for an accurate surface. All previous works utilize an extra representation while our work employs single representation that uses Gaussian primitives as discrete samples from SDF to regularize geometry. 3. Method In this section, we briefly review Gaussian splatting (Sec. 3.1) and propose discretized SDF within the Gaussian splatting framework for inverse rendering (Sec. 3.2). Then, we present projection-based loss to constrain the geometry when common gradient-based regularization becomes unfeasible for the discrete representation (Sec. 3.3). Finally, we propose spherical initialization to avoid the geometry trapping into local minima (Sec. 3.4). 3.1. Preliminaries 2DGS. 2DGS [8] flattens the Gaussian primitive into 2D disk defined by two tangent vectors (tu, tv) and their cortv. responding scaling (su, sv), whose normal is = tu They compute the ray-Gaussian response αi by ray-splat intersection and follow the rendering pipeline in 3DGS as: 3.2. Relightable GS with discretized SDF Previous works [34, 35, 41] introduced an extra SDF representation (e.g., network) into the GS framework as geometry constraints to improve decomposition quality, leading to higher memory costs and more complex optimization strategies. Hence, the main question is how to unify the Gaussian primitive and the SDF to avoid these drawbacks. To this end, we propose the discretized SDF into the Gaussian splatting as geometry prior, as shown in Fig. 2. We first describe our baseline solution and introduce our discretized SDF. Our framework is built on 2DGS [8] with the deferred shading, as the 2D Gaussian is robust for geometry [8] and deferred shading is more stable across diverse materials [32, 44]. Besides, we choose the Principled Bidirectional Reflectance Distribution Function (BRDF) [20] with split-sum approximation [11] as our shading model. Next, we propose the discretized SDF into our framework. Instead of using an extra representation for SDF, we encode an SDF value on the Gaussian as an extra attribute, besides PBR attributes, scaling, and quaternion. Then, the opacity in each Gaussian is derived from the SDF value via an SDF-to-opacity transformation: = ciαi i=0 (cid:88) 1 ( j=1 (cid:89) αj), (1) oi = Tγ(si) = 4 (1 + γsi γsi )2 , (2) where ci, αi is the color and the ray-Gaussian response. Relightable Gaussian splatting. To enable relighting, the color of Gaussian is replaced by physically based rendering (PBR) attributes (e.g., albedo, roughness). Two existing pipelines differ in the order of blending and evaluating the rendering equation, namely forward and deferred shading. The forward one renders the color for each Gaussian and blends them, while the deferred one blends the PBR attributes first and renders each pixel. where oi is the opacity, si is the SDF value of i-th Gaussian, and γ is global learnable parameter to control the variance of the transformation. Note that the opacity of each Gaussian is computed from the SDF, rather than directly learned. With the transformation, Gaussians can be splatted the same as in previous works [8, 18], and the Gaussian attributes together with the parameter of the transformation can be optimized. However, we observe slow convergence, as the joint optimization of the transformation parameter γ and the SDF value si spans large search space. Figure 3. The SDF-to-opacity transformation γ for splatting is bell-shaped function ranging from zero to one. The proposed median loss γ (red line) to narrower γm (blue line). γ drags L Figure 4. The difference between continuous and discretized SDF in regularization. For discrete samples, we can only obtain the gradient direction of SDF,1so the Eikonal loss is infeasible. Besides, since discretized SDF does not support the query of SDF value at the projected point, we cannot constrain projected points on the surface directly and thus use depth as bridge to achieve that. To mitigate the challenges in the joint learning of γ and si, we found the median of unsigned distance across all Gaussian primitives is an effective indicator for convergence. Specifically, high opacity om = m) at the median distance indicates the transformation should be narrowed, as shown in Fig. 3. Hence, we link the transformas tion parameter to the median and empirically encourage the opacity om below 0.5. Accordingly, we derive γm m) = 0.5 to meet this condition: from log(3 Tγm ( Tγm ( 22) (3) γm = , which is used to guide the transformation parameter γ with our proposed SDF-to-opacity transformation median loss: Lγ = max(γm γ, 0). (4) Note that, γm is used as guide of γ to quickly narrow the Tγm defined by γm, we do transformation. After it reaches Tγm and allow not force the transformation 1An ideal SDF has gradient magnitude of one, which does not hold during optimization, and discrete-sample magnitudes cannot be directly measured. Thus, only the direction on Gaussian is accessible. Tγ identical to Figure 5. The projection-based consistency loss minimizes the difference between the zero-level set of SDF and the alpha-blended surface defined by Gaussians. The exceptional Gaussians whose difference is larger than ε are discarded. it to be sharper. In this way, the Gaussians can quickly approach the surface and converge into sharp distribution. 3.3. Projection-based consistency loss signed distance function meets the Eikonal condition = 1, indicating the gradient magnitude to position should be constantly one, which is usually used to constrain can be computed easily an implicit SDF, as the gradient from continuous SDF. However, it is unclear how to obtain it for SDF samples encoded on Gaussians. As shown in Fig. 4, only the gradient direction on Gaussian is available, which is the normal direction = tv. Inspired by Neural-Pull [1], we design new constraint for the SDF samples tailored with Gaussians by projecting Gaussians to the surface defined by the zero-level set, enforcing the consistency between the projected point and alpha-blended surface defined by Gaussians, namely projection-based consistency loss, proven to be an approximation of the Eikonal condition. Note that although our design shares similar theory with Neural-Pull, we do not use an extra network for SDF and have no ground-truth point cloud as supervision. Specifically, we project Gaussian gi onto the zero-level set, leading to projected point µproj: = tu , (5) si µproj = µi fi fi where µi, si, fi are the position, SDF value and SDF gradient of gi, respectively. The projected point should align with the alpha-blended surface from Gaussians. For this, we ensure this property by minimizing their difference. Here, we use depth rather than position, due to simplicity, leading to the following difference ϵproj = where Dagg is the alpha-blended or aggregated depth, and Dproj is the depth of projected point µproj under the same view. Dagg Dproj This difference can not be used as the loss directly, as some exceptional Gaussians should be discarded. One case is that some Gaussians belong to occluded surfaces or objects in the current view. The other case is that nondifferentiable regions (such as self-intersection [19]) might 4 GShader PSNR / SSIM / LPIPS 17.49 / 0.8336 / 0.1901 19.01 / 0.8804 / 0.5203 16.00 / 0.8642 / 0.1591 22.49 / 0.9262 / 0.1606 15.62 / 0.8254 / 0.1391 12.33 / 0.7575 / 0.2142 14.42 / 0.8007 / 0.3368 18.21 / 0.8560 / 0.1716 16.95 / 0.8430 / 0.2365 0.5h 50 4G GS-IR PSNR / SSIM / LPIPS 15.64 / 0.6126 / 0.1428 12.61 / 0.2807 / 0.1431 18.04 / 0.7907 / 0.1171 17.40 / 0.7270 / 0.0866 19.00 / 0.7727 / 0.1094 16.37 / 0.7051 / 0.1614 14.35 / 0.5419 / 0.1970 16.63 / 0.7646 / 0.1312 16.26 / 0.6494 / 0.1361 0.5h 214 8G R3DG PSNR / SSIM / LPIPS 16.65 / 0.8013 / 0.1181 16.15 / 0.8391 / 0.1329 17.49 / 0.8503 / 0.1114 20.63 / 0.8832 / 0.0498 17.47 / 0.8168 / 0.1057 14.99 / 0.7799 / 0.1832 15.99 / 0.7965 / 0.1877 17.36 / 0.8389 / 0.1194 17.09 / 0.8258 / 0.1260 1h 50 20G GS-ROR PSNR / SSIM / LPIPS 20.81 / 0.8775 / 0.0858 24.49 / 0.9267 / 0.0795 26.28 / 0.9421 / 0.0596 23.31 / 0.9376 / 0.0356 22.61 / 0.8995 / 0.0672 25.67 / 0.9175 / 0.0937 22.80 / 0.9180 / 0.0953 21.17 / 0.8932 / 0.0982 23.39 / 0.9140 / 0.0769 1.5h 208 22G Ours PSNR / SSIM / LPIPS 22.03 / 0.8919 / 0.0819 24.67 / 0.9280 / 0.0842 26.48 / 0.9374 / 0.0661 24.01 / 0.9481 / 0.0351 23.80 / 0.9017 / 0.0699 27.31 / 0.9280 / 0.0982 23.66 / 0.9191 / 0.0981 24.19 / 0.9293 / 0.0760 24.52 / 0.9229 / 0.0762 1h 143 4G Angel Bell Cat Horse Luyu Potion Tbell Teapot Mean Training Ren. FPS Memory Table 1. The quantitative comparison with Gaussian-based methods on the Glossy Blender dataset in terms of PSNR Numbers in red indicate the best performance, and numbers in orange indicate the second best. , SSIM , and LPIPS . GShader PSNR / SSIM / LPIPS 22.86 / 0.9280 / 0.0821 24.61 / 0.9491 / 0.0956 17.45 / 0.8838 / 0.1408 13.41 / 0.7904 / 0.1193 19.58 / 0.8878 / 0.1095 GS-IR PSNR / SSIM / LPIPS 28.31 / 0.9121 / 0.0591 24.39 / 0.8762 / 0.0360 22.36 / 0.8931 / 0.0875 23.79 / 0.8750 / 0.0879 24.71 / 0.8891 / 0.0676 R3DG PSNR / SSIM / LPIPS 30.76 / 0.9526 / 0.0649 27.23 / 0.9637 / 0.0437 24.59 / 0.9162 / 0.1275 22.49 / 0.8682 / 0.1423 26.27 / 0.9252 / 0.0946 GS-ROR PSNR / SSIM / LPIPS 31.33 / 0.9593 / 0.0482 26.28 / 0.9542 / 0.0439 25.21 / 0.9307 / 0.0771 25.46 / 0.9083 / 0.0724 27.07 / 0.9381 / 0. Ours PSNR / SSIM / LPIPS 31.05 / 0.9621 / 0.0536 27.85 / 0.9639 / 0.0390 26.23 / 0.9360 / 0.0746 25.81 / 0.9087 / 0.0791 27.78 / 0.9427 / 0.0626 Armad. Ficus Hotdog Lego Mean Table 2. The quantitative comparison with Gaussian-based methods on the TensoIR Synthetic dataset in terms of PSNR LPIPS . Numbers in red indicate the best performance and numbers in orange indicate the second best. , SSIM , and lead to huge difference ϵproj. These two cases interrupt consistency significantly. Unfortunately, it is difficult to recognize them. Therefore, we use simple and practical solution, by assuming the difference between the two surfaces should be already close and thresholding the difference with hyper-parameter ε, as shown in Fig. 5. Lp = 1 N (cid:40) (cid:88)i ϵi proj, 0, ε, ϵi proj ϵi proj > ε. (6) Note that we apply this loss after 1K iterations when the coarse geometry is stable so that the assumption holds. The theory behind the loss is an approximation of the Eikonal condition, proven in the supplementary. This loss encourages smooth reconstructed surfaces while maintaining the flexibility from Gaussian for geometry details. 3.4. Spherical initialization We found that some erroneous geometry emerges at the early stage of training and can hardly be recovered. This indicates that the initialization makes the geometry prone to trapping into local minima. Since sphere-like initializations revealed their effect in volumetric rendering of SDF [28, 33], we design spherical initialization of foreground objects tailored for Gaussian splatting to avoid local 5 minima and to encourage optimal geometry. Specifically, we employ the points uniformly sampled on unit sphere to initialize Gaussians. The design mitigates broken surfaces and overfitting under the training appearance, further improving the geometry robustness. 4. Experiments In this section, we present our training details (Sec. 4.1) and evaluation setup (Sec. 4.2). Then, we compare our method with the representative methods (Sec. 4.3) and conduct ablation studies on our proposed designs (Sec. 4.4). 4.1. Implementation details We optimized our model using Adam optimizer [13] for 30,000 iterations with the loss: = Lc+λnLn+λdLd+λγLγ +λpLp+λsmLsm+λmLm, (7) where λ[ Lc is the ] is the corresponding weight of loss rendering loss between the rendered image and the ground truth. Ld are the normal consistency and the distorLsm is smoothness regtion loss, inheriting from 2DGS. ularization for PBR attributes, and Lm is the optional mask loss widely adopted by existing relighting methods [6, 38]. Ln and ]. L[ Figure 6. The comparison of relighting results and normal on the Glossy Blender (the 1st-4th rows) and the TensoIR synthetic dataset (the 5th, 6th rows). Our method provides reasonable normal robustly across materials, thus rendering realistic relighting results that include specular highlights. The PSNR/SSIM of relighting results under the current view are below images. More results are in the supplementary. Lγ and details are in the supplementary. Lp are our median and projection-based loss. More 4.2. Evaluation setup Dataset. We evaluate our method on three synthetic datasets, including TensoIR [10] synthetic dataset, Glossy Blender dataset [17] and Shiny Blender dataset. For real scenes, we select some objects from NeILF++ [37]. Methods for comparison. We select 4 representative Gaussian-based inverse rendering methods for comparison, including GShader [9], GS-IR [16], R3DG [6] and GSROR [44]. We trained these models based on their public codes and configurations with an extra mask loss for stabilizing training and fair comparison. Metrics. We use the peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) [30], the learned perceptual image patch similarity (LPIPS) [40] to measure the relighting quality. Following NeRO and TensoIR, we rescale the relighting images for quantitative evaluation. We compare the mean angular error (MAE) and the Chamber distance (CD) for geometry quality. We record the mean training time, the frame per second (FPS), and the memory usage during training on RTX 4090. 4.3. Comparison with previous methods Relighting of synthetic objects. We evaluate the relighting performance of synthetic objects with various materials on the Glossy Blender and TensoIR synthetic datasets. The quantitative measurements are shown in Tab. 1 and Tab. 2. The relighting results with normal visualizations are in Fig. 6. Due to the robustness of decomposition, our method provides high-quality relighting results and smooth normal. In contrast, GS-IR, R3DG, and GShader overfit the training appearance, leading to erroneous normal. Regarding GS-ROR, it shows higher geometry quality, as it introduces continuous SDF for regularization. However, the Figure 7. The qualitative comparison of relighting results and normal on the NeILF++ dataset. Our method can provide realistic relighting results and detailed normal for real data. More results are in the supplementary. Figure 8. The qualitative comparison of normal on the Shiny Blender dataset. Our method provides globally smooth normal while preserving the sharp details. The MAE under the current view is below the visualization. More results are in the supplementary. Ball Car Coffee Helmet Teapot Toaster Mean GS-IR R3DG GShader GS-ROR Ours 1.20 7.03 25.79 6.82 14.05 28.31 11.42 14.93 15.38 3.90 9.33 25.58 5.02 7.17 15.35 10.51 13.08 33.51 6.48 10.93 23.99 22.44 26.02 13.39 19.63 9.21 28.17 19.81 0.92 11.98 12.24 4.10 5.88 8.24 7. Table 3. Normal quality with Gaussian-based methods on Shiny Blender dataset in terms of MAE . Numbers in red indicate the best performance and numbers in orange indicate the second best. geometry details are lost in some regions, due to the difficulty in balancing two representations during training. Relighting of real objects. The results of real scenes from NeILF++ are shown in Fig. 7. As ground truth is unavailable, we show reference training views and the results under novel light conditions.2 Our method produces reasonable relighting results with sharp geometry details preserved. Geometry quality. We compare reconstructed geometry quality in terms of normal MAE in Tab. 3 with the visualization in Fig. 8 and CD in Tab. 4.3Numerically, our model outperforms all Gaussian-based inverse rendering 2R3DG, GS-IR do not converge on the scenes, thus no results provided. GS-IR 0.0177 0.1153 0.0588 0.0196 0.0225 0.0623 0.1021 0.0719 0.0588 R3DG GShader GS-ROR 0.0063 0.0085 0.0098 0.0096 0.0110 0.0418 0.0222 0.0256 0.0339 0.0061 0.0073 0.0135 0.0107 0.0168 0.0098 0.0146 0.0474 0.0380 0.0270 0.0574 0.0500 0.0166 0.0340 0.0479 0.0140 0.0253 0.0315 Ours 0.0053 0.0094 0.0251 0.0052 0.0106 0.0081 0.0101 0.0115 0.0107 Angel Bell Cat Horse Luyu Potion Tbell Teapot Mean Table 4. Surface quality with Gaussian-based methods on Glossy Blender dataset regarding CD . Numbers in red indicate the best performance, and numbers in orange indicate the second best. methods on most scenes and average metrics. However, our method fails in scenes with complex inter-reflection (Toaster, Luyu, Cat) but still achieves sub-SOTA performance. Only GS-ROR succeeds in the scenes, at the cost of 5 of GPU memory usage than ours, due to their extra SDF network. Visually, our model provides globally smooth normal (teapot in Fig. 6) and preserves local details (horse tail in Fig. 6), while GS-ROR discards sharp details and others fail to reconstruct smooth surfaces. 3The choice of quantitative metrics depends on the available ground truth, and we use TSDF fusion [43] for surface extraction to evaluate CD. Figure 9. Ablation of the key components in our method on the Glossy Blender dataset, including the discretized SDF framework with the median loss, the projection-based consistency loss (Proj.), and spherical initialization (Sph.). (The red arrow points out the artifacts.) Components SDF Sph. Proj. Angel Teapot Scene Horse PSNR/SSIM PSNR/SSIM PSNR/SSIM 19.22/0.8674 20.72/0.8998 20.23/0.8533 23.26/0.9223 22.13/0.9120 21.09/0.8815 22.97/0.9176 21.96/0.9102 21.43/0.8881 23.45/0.9222 22.87/0.9284 21.59/0.8910 24.05/0.9289 23.21/0.9395 21.65/0.8917 24.19/0.9293 24.01/0.9481 22.03/0.8919 Table 5. Ablation study of three key components on the Glossy Blender dataset. SDF means incorporating the discretized SDF with the median loss, Proj. means the projection-based consistency loss, and Sph. means the spherical initialization. Spherical initialization. The spherical initialization leads to performance increase with or without SDF designs (the 3rd, 4th, 6th columns of Fig. 9). However, applying it individually cannot ensure artifact-free results. 4.5. Discussion and limitations In this work, we focus on improving the relighting quality by introducing SDF prior and leave mesh extraction with BRDF parameters for future work. Besides, we consider the direct lighting only for efficiency, so our method may fail for objects with complex occlusion. An extra indirect illumination term [6, 17] will further benefit our approach. 4.4. Ablation study 5. Conclusion We conduct ablation studies of our main components on the Glossy Blender dataset, and the ablations of detailed choices (e.g., losses, threshold) are in the supplementary. The quantitative results are in the Tab. 5. The performance of the model improves consistently when employing new components. We start from the baseline described in Sec. 3.2. As shown in the 1st column of Fig. 9, this baseline overfits the training appearance and fails to decompose geometry and material for relighting. Discretized SDF with the median loss. Then, we incorporate the SDF-to-opacity transformation and the median loss that promotes the Gaussians converging to the opaque surface. As shown in the 2nd column of Fig. 9. The variant can decompose the material and geometry more robustly, thus improving the relighting quality. However, due to insufficient constraints, we still observe erroneous normal. Projection-based consistency loss. Due to this loss is coupled with SDF, it cannot be ablated individually. Applying the projection-based consistency loss leads to more reasonable normal. However, erroneous geometry caused by misleading initialization still exists, as in the 5th row of Fig. 9. In this paper, we presented our Gaussian splatting framework for real-time object relighting, which regularizes the geometry from Gaussian primitives with discretized SDF. In this framework, we proposed the median loss to facilitate the convergence and the projection-based consistency loss to constrain the discrete samples of SDF, along with the spherical initialization to avoid local minima. Therefore, our method benefits from the flexibility of GS and the robustness of SDF, thus outperforming the existing Gaussianbased inverse rendering methods in terms of relighting and decomposition quality, while maintaining simplicity and efficiency. In future work, extending our framework to unbounded scenes is promising direction. Acknowledgment: We thank the reviewers for the valuable comments. This work has been partially supported by the National Science and Technology Major Project under grant No. 2022ZD0116305, National Natural Science Foundation of China (NSFC) under grant No. 62172220, and the NSFC under 62361166670 and U24A20330. Computation is Grant Nos. supported by the Supercomputing Center of Nankai University."
        },
        {
            "title": "References",
            "content": "[1] Ma Baorui, Han Zhizhong, Liu Yu-Shen, and Zwicker Matthias. Neural-Pull: Learning signed distance functions from point clouds by learning to pull space onto surfaces. In ICML, 2021. 4 [2] Sai Bi, Zexiang Xu, Pratul Srinivasan, Ben Mildenhall, Kalyan Sunkavalli, Miloˇs Haˇsan, Yannick Hold-Geoffroy, David Kriegman, and Ravi Ramamoorthi. Neural reflectance fields for appearance acquisition. arXiv preprint arXiv:2008.03824, 2020. 2 [3] Mark Boss, Raphael Braun, Varun Jampani, Jonathan T. Barron, Ce Liu, and Hendrik P.A. Lensch. NeRD: Neural reflectance decomposition from image collections. In ICCV, pages 1268412694, New York, NY, USA, 2021. IEEE. 2 [4] Mark Boss, Varun Jampani, Raphael Braun, Ce Liu, Jonathan Barron, and Hendrik PA Lensch. Neural-PIL: Neural pre-integrated lighting for reflectance decomposition. In NeurIPS, pages 1069110704, Red Hook, NY, USA, 2021. Curran Associates, Inc. 2 [5] Hanlin Chen, Chen Li, and Gim Hee Lee. NeuSG: Neural implicit surface reconstruction with 3D Gaussian splatting guidance. arXiv preprint arXiv:2312.00846, 2023. 2 [6] Jian Gao, Chun Gu, Youtian Lin, Zhihao Li, Hao Zhu, Xun Cao, Li Zhang, and Yao Yao. Relightable 3D Gaussians: Realistic point cloud relighting with BRDF decomposition and ray tracing. In ECCV, 2024. 2, 5, 6, 8 [7] Amos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, and Yaron Lipman. Implicit geometric regularization for learning shapes. In ICML. JMLR.org, 2020. [8] Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, and Shenghua Gao. 2D Gaussian splatting for geometrically accurate radiance fields. In SIGGRAPH, New York, NY, USA, 2024. Association for Computing Machinery. 3 [9] Yingwenqi Jiang, Jiadong Tu, Yuan Liu, Xifeng Gao, Xiaoxiao Long, Wenping Wang, and Yuexin Ma. GaussianShader: 3D Gaussian splatting with shading functions for reflective surfaces. In CVPR, pages 53225332, New York, NY, USA, 2024. IEEE. 2, 6 [10] Haian Jin, Isabella Liu, Peijia Xu, Xiaoshuai Zhang, Songfang Han, Sai Bi, Xiaowei Zhou, Zexiang Xu, and Hao Su. TensoIR: Tensorial inverse rendering. In CVPR, pages 165 174, New York, NY, USA, 2023. IEEE. 2, 6 [11] Brian Karis. Real shading in unreal engine 4. Proc. Physically Based Shading Theory Practice, 2013. 3 [12] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3D Gaussian splatting for real-time radiance field rendering. ACM TOG, 42(4):1391, 2023. [13] Diederik P. Kingma and Jimmy Ba. Adam: method for arXiv preprint arXiv:1412.6980, stochastic optimization. 2017. 5 [14] Jia Li, Lu Wang, Lei Zhang, and Beibei Wang. TensoSDF: Roughness-aware tensorial representation for robust geometry and material reconstruction. ACM TOG, 43(4), 2024. 1, 2 [15] Zhaoshuo Li, Thomas Muller, Alex Evans, Russell H. Taylor, Mathias Unberath, Ming-Yu Liu, and Chen-Hsuan Lin. Neuralangelo: High-fidelity neural surface reconstruction. In CVPR, pages 84568465, New York, NY, USA, 2023. IEEE. 2 [16] Zhihao Liang, Qi Zhang, Ying Feng, Ying Shan, and Kui Jia. GS-IR: 3D Gaussian splatting for inverse rendering. In CVPR, pages 2164421653, New York, NY, USA, 2024. IEEE. 2, [17] Yuan Liu, Peng Wang, Cheng Lin, Xiaoxiao Long, Jiepeng Wang, Lingjie Liu, Taku Komura, and Wenping Wang. NeRO: Neural geometry and BRDF reconstruction of reflective objects from multiview images. ACM TOG, 42(4), 2023. 1, 2, 6, 8 [18] Xiaoyang Lyu, Yang-Tian Sun, Yi-Hua Huang, Xiuzhe Wu, Ziyi Yang, Yilun Chen, Jiangmiao Pang, and Xiaojuan Qi. 3DGSR: Implicit surface reconstruction with 3D Gaussian splatting. ACM TOG, 43(6), 2024. 2, 3 [19] Daniel Mayost. Applications of the signed distance function to surface geometry. University of Toronto, 2014. 2, 4 [20] Stephen McAuley, Stephen Hill, Adam Martinez, Ryusuke Villemin, Matt Pettineo, Dimitar Lazarov, David Neubelt, Brian Karis, Christophe Hery, Naty Hoffman, and Hakan Zap Andersson. Physically based shading in theory and pracIn ACM SIGGRAPH 2013 Courses, New York, NY, tice. USA, 2013. Association for Computing Machinery. 3 [21] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. In ECCV, pages 405421, Berlin, Heidelberg, 2020. Springer. 1, 2 [22] Radu Alexandru Rosu and Sven Behnke. PermutoSDF: Fast multi-view reconstruction with implicit surfaces using permutohedral lattices. In CVPR, pages 84668475, New York, NY, USA, 2023. IEEE. 2 [23] Yahao Shi, Yanmin Wu, Chenming Wu, Xing Liu, Chen Zhao, Haocheng Feng, Jingtuo Liu, Liangjun Zhang, Jian Zhang, Bin Zhou, Errui Ding, and Jingdong Wang. GIR: 3D Gaussian inverse rendering for relightable scene factorization. arXiv preprint arXiv:2312.05133, 2023. 2 [24] Pratul P. Srinivasan, Boyang Deng, Xiuming Zhang, Matthew Tancik, Ben Mildenhall, and Jonathan T. Barron. NeRV: Neural reflectance and visibility fields for relighting and view synthesis. In CVPR, pages 74957504, New York, NY, USA, 2021. IEEE. [25] Cheng Sun, Guangyan Cai, Zhengqin Li, Kai Yan, Cheng Zhang, Carl Marshall, Jia-Bin Huang, Shuang Zhao, and Zhao Dong. Neural-PBIR reconstruction of shape, material, and illumination. In ICCV, pages 1804618056, New York, NY, USA, 2023. IEEE. 1, 2 [26] Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler, Jonathan T. Barron, and Pratul P. Srinivasan. Ref-NeRF: Structured view-dependent appearance for neural radiance In CVPR, pages 54915500, New York, NY, USA, fields. 2022. IEEE. 1 [27] Haoyuan Wang, Wenbo Hu, Lei Zhu, and Rynson W.H. Lau. Inverse rendering of glossy objects via the neural plenoptic function and radiance fields. In CVPR, pages 1999920008, New York, NY, USA, 2024. IEEE. 2 9 [42] Yuanqing Zhang, Jiaming Sun, Xingyi He, Huan Fu, Rongfei Jia, and Xiaowei Zhou. Modeling indirect illumination for inverse rendering. In CVPR, pages 1864318652, New York, NY, USA, 2022. IEEE. 1, 2 [43] Qian-Yi Zhou, Jaesik Park, and Vladlen Koltun. Open3D: modern library for 3d data processing. arXiv preprint arXiv:1801.09847, 2018. [44] Zuo-Liang Zhu, Beibei Wang, and Jian Yang. GS-ROR: 3D Gaussian splatting for reflective object relighting via sdf priors. arXiv preprint arXiv:2406.18544, 2024. 2, 3, 6 [28] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. NeuS: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. In NeurIPS, pages 2717127183, Red Hook, NY, USA, 2021. Curran Associates, Inc. 1, 2, 5 [29] Yiming Wang, Qin Han, Marc Habermann, Kostas Daniilidis, Christian Theobalt, and Lingjie Liu. NeuS2: Fast learning of neural implicit surfaces for multi-view reconstruction. In ICCV, pages 32953306, New York, NY, USA, 2023. IEEE. 2 [30] Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE TIP, 13(4):600612, 2004. 6 [31] Zixiong Wang, Yunxiao Zhang, Rui Xu, Fan Zhang, PengShuai Wang, Shuangmin Chen, Shiqing Xin, Wenping Wang, and Changhe Tu. Neural-singular-hessian: Implicit neural representation of unoriented point clouds by enforcing singular hessian. ACM TOG, 42(6), 2023. 2 [32] Tong Wu, Jia-Mu Sun, Yu-Kun Lai, Yuewen Ma, Leif Kobbelt, and Lin Gao. DeferredGS: Decoupled and editable Gaussian splatting with deferred shading. arXiv preprint arXiv:2404.09412, 2024. 2, [33] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Volume rendering of neural implicit surfaces. In NeurIPS, pages 48054815, Red Hook, NY, USA, 2021. Curran Associates, Inc. 2, 5 [34] Keyang Ye, Qiming Hou, and Kun Zhou. 3D Gaussian splatting with deferred reflection. In SIGGRAPH, New York, NY, USA, 2024. Association for Computing Machinery. 3 [35] Mulin Yu, Tao Lu, Linning Xu, Lihan Jiang, Yuanbo Xiangli, and Bo Dai. GSDF: 3DGS meets SDF for improved rendering and reconstruction. In NeurIPS, 2024. 2, 3 [36] Jingyang Zhang, Yao Yao, Shiwei Li, Tian Fang, David McKinnon, Yanghai Tsin, and Long Quan. Critical regularizations for neural surface reconstruction in the wild. In CVPR, pages 62706279, New York, NY, USA, 2022. IEEE. 2 [37] Jingyang Zhang, Yao Yao, Shiwei Li, Jingbo Liu, Tian Fang, David McKinnon, Yanghai Tsin, and Long Quan. NeILF++: Inter-reflectable light fields for geometry and material estimation. In ICCV, pages 36013610, New York, NY, USA, 2023. IEEE. 1, 6 [38] Kai Zhang, Fujun Luan, Qianqian Wang, Kavita Bala, and Noah Snavely. PhySG: Inverse rendering with spherical Gaussians for physics-based material editing and relighting. In CVPR, pages 54535462, New York, NY, USA, 2021. IEEE. 2, 5 [39] Kai Zhang, Fujun Luan, Zhengqi Li, and Noah Snavely. IRON: Inverse rendering by optimizing neural sdfs and materials from photometric images. In CVPR, pages 55655574, New York, NY, USA, 2022. IEEE. [40] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, New York, NY, USA, 2018. IEEE. 6 [41] Wenyuan Zhang, Yu-Shen Liu, and Zhizhong Han. Neural signed distance function inference through splatting 3D Gaussians pulled on zero-level set. In NeurIPS, 2024. 2,"
        }
    ],
    "affiliations": [
        "Nanjing University",
        "Nankai University"
    ]
}