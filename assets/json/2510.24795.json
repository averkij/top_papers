{
    "paper_title": "A Survey on Efficient Vision-Language-Action Models",
    "authors": [
        "Zhaoshu Yu",
        "Bo Wang",
        "Pengpeng Zeng",
        "Haonan Zhang",
        "Ji Zhang",
        "Lianli Gao",
        "Jingkuan Song",
        "Nicu Sebe",
        "Heng Tao Shen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-Language-Action models (VLAs) represent a significant frontier in embodied intelligence, aiming to bridge digital knowledge with physical-world interaction. While these models have demonstrated remarkable generalist capabilities, their deployment is severely hampered by the substantial computational and data requirements inherent to their underlying large-scale foundation models. Motivated by the urgent need to address these challenges, this survey presents the first comprehensive review of Efficient Vision-Language-Action models (Efficient VLAs) across the entire data-model-training process. Specifically, we introduce a unified taxonomy to systematically organize the disparate efforts in this domain, categorizing current techniques into three core pillars: (1) Efficient Model Design, focusing on efficient architectures and model compression; (2) Efficient Training, which reduces computational burdens during model learning; and (3) Efficient Data Collection, which addresses the bottlenecks in acquiring and utilizing robotic data. Through a critical review of state-of-the-art methods within this framework, this survey not only establishes a foundational reference for the community but also summarizes representative applications, delineates key challenges, and charts a roadmap for future research. We maintain a continuously updated project page to track our latest developments: https://evla-survey.github.io/"
        },
        {
            "title": "Start",
            "content": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1 Survey on Efficient Vision-Language-Action Models Zhaoshu Yu, Bo Wang, Pengpeng Zeng, Haonan Zhang, Ji Zhang, Lianli Gao, Jingkuan Song, Nicu Sebe, and Heng Tao Shen, IEEE Fellow AbstractVision-Language-Action models (VLAs) represent significant frontier in embodied intelligence, aiming to bridge digital knowledge with physical-world interaction. While these models have demonstrated remarkable generalist capabilities, their deployment is severely hampered by the substantial computational and data requirements inherent to their underlying large-scale foundation models. Motivated by the urgent need to address these challenges, this survey presents the first comprehensive review of Efficient VisionLanguage-Action models (Efficient VLAs) across the entire data-model-training process. Specifically, we introduce unified taxonomy to systematically organize the disparate efforts in this domain, categorizing current techniques into three core pillars: (1) Efficient Model Design, focusing on efficient architectures and model compression; (2) Efficient Training, which reduces computational burdens during model learning; and (3) Efficient Data Collection, which addresses the bottlenecks in acquiring and utilizing robotic data. Through critical review of state-of-the-art methods within this framework, this survey not only establishes foundational reference for the community but also summarizes representative applications, delineates key challenges, and charts roadmap for future research. We maintain continuously updated project page to track our latest developments: https://evla-survey.github.io/ Index TermsVision-Language-Action Models, Efficient VLAs, Embodied AI, Robotic Manipulation"
        },
        {
            "title": "1 INTRODUCTION",
            "content": "V ISION-LANGUAGE-ACTION Models (VLAs) herald breakthrough in embodied intelligence, forging direct pathway from digital cognition to physical-world actuation. Building upon large-scale pretrained vision-language models (VLMs), representative VLAs such as OpenVLA [1], π0 [2], and others [3], [4] achieve multimodal perception and understanding by fusing visual and linguistic modalities, thereby enabling effective action generation for real-world interactions. Their applications are expansive and transformative, spanning autonomous driving [5], [6], industrial manufacturing [7], medical robotics [8], laboratory automation [9], and beyond, with the potential to revolutionize industries, enhance human-robot collaboration, and tackle foundational challenges in robotics. At the core of VLAs lies modular architecture that inputs through vision encoder and fuses multimodal language encoder, which process raw cross-modal data into high-dimensional embeddings; large language model backbone, serving as the reasoning hub to contextualize instructions and perceptions; and an action decoder, which maps fused representations to low-level control signals for embodied execution. Current research on VLAs centers on unifying perception, language understanding, and action generation in end-to-end frameworks, with strong emphasis on architectural innovations, large-scale multimodal pretraining, and strategies to enhance generalization across Zhaoshu Yu, Bo Wang, Pengpeng Zeng, Haonan Zhang, Jingkuan Song, and Heng Tao Shen are with the School of Computer Science and Technology, Tongji University, China. Ji Zhang is with the School of Computing and Artificial Intelligence, Southwest Jiaotong University, China. Lianli Gao is with the School of Computer Science and Engineering, University of Electronic Science and Technology of China, China. Nicu Sebe is with the Department of Information Engineering and Computer Science, University of Trento, Italy. Fig. 1: Necessity of Efficient VLAs. This figure highlights the disparity between powerful but resource-intensive foundation VLAs and the practical deployment requirements of diverse edge robotic platforms. Bridging this gap by developing more compact, economical, and applicable solutions is the primary motivation for pursuing efficient VLAs. diverse tasks, embodiments, and environments. Despite propelling embodied intelligence to unprecedented heights by seamlessly integrating perception, reasoning, and control, their efficacy remains critically hindered by profound efficiency bottlenecks. These scalingdependent paradigms inherit the resource-intensive nature of their LLM and VLM foundations, manifesting in: (1) high inference latency and insufficient frequency, incompatible with the sub-second control cycles required for responsive manipulation; (2) prohibitive pretraining demands, with models like π0 [2] requiring 10,000 hours of robotic tra5 2 0 O 7 2 ] . [ 1 5 9 7 4 2 . 0 1 5 2 : r JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 Fig. 2: The Organization of Our Survey. We systematically categorize efficient VLAs into three core pillars: (1) Efficient Model Design , encompassing efficient architectures and model compression techniques; (2) Efficient Training , covering efficient pre-training and post-training strategies; and (3) Efficient Data Collection, including efficient data collection and augmentation methods. The framework also reviews VLA foundations, key applications, challenges, and future directions, establishing the groundwork for advancing scalable embodied intelligence. jectories and OpenVLA [1] consuming 21,500 A100-GPU hours across 64-GPU cluster; and (3) time-consuming and labor-intensive large datasets, rendering training infeasible in resource-constrained environments. As illustrated in Fig. 1, foundational VLA models grapple with significant efficiency challenges, typically characterized by the need for computing power support from cloud servers, and cannot be efficiently developed and deployed on edge devices. Addressing these through efficient VLA designs [10], [11], [12], [13], [14], [15] promises broader accessibility for resource-constrained researchers and enables deployment in diverse scenarios, while enhancing inference speed to yield smoother control and improved task performance. In this survey, we are dedicated to efficient VLAs in the embodied AI domain, aiming to systematize the rapidly evolving field by presenting detailed taxonomy. We define efficient VLAs as systems aiming for high performance while maintaining low overhead across the development and deployment lifecycle. As illustrated in Fig. 2, we organize existing approaches into structured taxonomy comprising three core technical directions: (1) Efficient Model Design encompasses innovative strategies to optimize the architectural and inference efficiency of VLAs. This includes Efficient Architectures, which offers innovative structural optimizations alongside scalable frameworks, and Model Compression, which involves techniques to compress model parameters and refine token handling. (2) Efficient Training incorporates advanced approaches to lessen the computational and data burdens during the VLA training process. This includes Efficient Pre-training, which utilizes optimized data techniques and refined action representations, and Efficient Post-training that applies customized supervised fine-tuning and reinforced learning strategies. (3) Efficient Data Collection integrates cuttingedge methods to efficiently enhance data gathering and enrichment for VLA development, which employs interactive, simulated, reutilize-oriented, and self-driven techniques to acquire datasets in more scalable and efficient way. This pioneering taxonomy marks the seminal comprehensive effort to synthesize cutting-edge advancements and technical trajectories in efficient VLAs, and is the first review of efficient VLAs that covers VLA development and deployment around the full data-model-training process, setting foundation that galvanizes future research and steers the field toward cohesive frontier. Although several excellent surveys [16], [17], [18], [19], [20], [21] have charted the landscape of general Vision-Language-Action Models or the broader field of embodied AI, systematic review dedicated to the crucial aspect of efficiency in VLAs has been conspicuously absent. This work fills this critical void, uniquely consolidating the disparate research efforts focused on optimizing VLAs for real-world deployment in resource-constrained scenarios, making it an indispensable reference for researchers and practitioners. In doing so, this survey not only complements existing literature but also formally defines and structures the emerging subfield of efficient VLAs. The primary contributions of this survey are summarized as follows: JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 3 Pioneering Survey: To the best of our knowledge, this work presents the first comprehensive survey specifically dedicated to the realm of efficient VLAs that covers the entire data-model-training process. It fills critical gap in the literature and aims to establish foundational reference for the research community. Novel Taxonomy: We introduce novel and systematically structured taxonomy that organizes the core technical landscape for building efficient VLAs into three interconnected pillars: Efficient Model Design, Efficient Training, and Efficient Data Collection. Future Roadmap: We critically distill the key challenges and current limitations plaguing the field, thereby outlining promising and forward-looking research directions to inspire and guide future endeavors in scalable embodied intelligence. The remaining sections of this survey are organized as illustrated in Fig. 2. We begin in section 2 by introducing the foundational concepts of Vision-Language-Action models, delineating the computational challenges that necessitate the pursuit of efficiency. Section 3 delves into efficient model design, systematically categorizing techniques that span efficient architectural innovations and model compression. Section 4 investigates efficient training paradigms, encompassing strategies to reduce the computational overhead during both pre-training and post-training stages. Section 5 examines the crucial role of efficient data collection, exploring methodologies for scalable data collection and effective augmentation to maximize data utility. We then survey key applications in section 6 where efficient VLAs have demonstrated significant potential, highlighting their practical utility in real-world scenarios. Section 7 discusses current challenges and outlines promising future research directions. Finally, section 8 summarizes the survey. Fig. 3: An Overview Of VLAs. VLAs integrate vision encoders to extract visual features, LLM backbones to fuse multimodal inputs, and action decoders (MLP-based, autoregressive, or generative) to produce robotic control signals, enabling end-to-end vision-language-action reasoning for embodied manipulation tasks."
        },
        {
            "title": "2 VISION-LANGUAGE-ACTION MODELS",
            "content": "The emergence of Vision-Language-Action (VLA) models represents paradigm-shifting frontier in embodied AI, redefining the synergy between human-like perception and autonomous action. Classic VLA frameworks extend the prowess of pre-trained Vision-Language Models (VLMs) into actionable robotic control, seamlessly integrating multimodal perception and language to drive precise motor responses. However, their reliance on extensive computational clusters and vast, diverse training datasets often hinders development and deployment in resource-constrained settings, underscoring the urgent need for efficient VLA solutions. In response, researchers committed to Efficient VLAs are actively pursuing innovative architectures and optimization techniques to enhance efficiency in real-world applications. 2.1 An Overview of VLAs VLAs mark critical milestone in embodied AI, extending the capabilities of pre-trained VLMs to enable precise robotic action execution. At their core, these models tackle the fundamental task of translating multimodal sensory and linguistic inputs into coherent physical responses, addressing the gap between abstract understanding and practical motor control in dynamic environments. 2.1.1 Foundational Pipeline As depicted in Fig. 3, VLAs foundational pipeline divides multimodal inference into three synergistic modules: visual encoder that distills scene images into patch embeddings; large language model (LLM) backbone, augmented for vision-language fusion, that orchestrates high-level reasoning; and an action decoder that yields precise control trajectories. By harnessing pre-trained models, VLAs transcend task silos, fostering scalable agents adept at long-horizon manipulation. Vision Encoder. At the ingress, the vision encoder Eimg() ingests RGB observations and extracts their hierarchical features. Prevailing choices include vision transformers like ViT [22], SigLIP [23], DINOv2 [24], and CLIP [25], whose contrastive or self-supervised pre-training on vast corpora imparts zero-shot versatility. Abstractly, this module maps raw imagery to semantic tokens: = Eimg(I; θimg), (1) JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 4 Fig. 4: Timeline of Foundational VLA Models and Efficient VLAs. The timeline illustrates the progression of foundational VLA models and efficient VLAs from 2023 to 2025, highlighting the explosive growth in enhancing the efficiency of VLA to bridge computational demands with real-world robotic deployment. where RHW 3 denotes the input images, RNvDv represents the vision tokens, and θimg indicates the encoder parameters. LLM Backbone. After projector () that ensures seamless alignment and mitigates modality gaps, the multimodal sequence then feeds into the LLM backbone LLM ()the center for semantic reasoning. Spanning LLaVA [26], Qwen [27], PaLM [28], Gemma [29], Llama [30], Mamba [31], VILAU [32], and Vicuna [33], pre-trained LLMs process the fused embeddings for task planning, which can be formulated as follows: = LLM (P (v, l); θLLM ), (2) where RNlDl is language tokens, RNhD denotes the hidden states and θLLM represents the LLM parameters. Action Decoder. Culminating the flow, the action decoder transmutes latents into robot-ready outputs, such as endeffector poses and gripper commands. Common instantiations include diffusion/flow matching [34], [35], [36], [37] for stochastic refinement of trajectories, autoregressive decoding for sequential action prediction, and MLP-based architectures for lightweight design. They all generate action sequences conditioned on reasoning: a1:T = Dact(h; θact), (3) where a1:T RT Da denotes the action chunk over timesteps and θact indicates the decoder parameters. In essence, this tripartite architecture, including visual encoding, linguistic reasoning, and action generation, unifies perception and execution in an end-to-end framework, propelling VLAs toward generalizable embodied intelligence. 2.1.2 Datasets and Benchmarks The advancement of VLAs is intrinsically linked to the availability of large-scale, diverse, and well-structured training data and evaluation benchmarks. The field has seen the curation of substantial datasets, which can be broadly categorized into real-world and simulation-sourced collections. Real-world Datasets. cornerstone of VLA training is the Open X-Embodiment (OXE [38]) dataset. This collaborative effort aggregates data from multiple robotics labs, encompassing vast array of tasks, robot morphologies, and environments. Its scale and diversity are instrumental for training models with strong generalization and transfer learning capabilities. Complementing OXE are other significant datasets such as BridgeData and BridgeData V2 [39], [40], which focuses on cross-domain tasks, and DROID [41], known for its large-scale dexterous teleoperated data in the wild. These datasets provide the crucial embodied experience necessary for VLAs to learn the dynamics and constraints of the physical world. Simulation Datasets. Due to the cost and scalability limitations of physical data collection, simulation platforms play an indispensable role in generating the large-scale training data required for VLAs. RLBench [42] offers vast library of predefined manipulation tasks with standardized evaluation protocols, serving as popular testbed for algorithm development and providing substantial pre-training data. Further expanding the scope of simulation, RoboCasa [43] provides large-scale simulation environment focusing on everyday tasks for generalist robots, significantly enriching JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 the diversity and practicality of training scenarios. Meanwhile, RoboGen [44] introduces novel paradigm by leveraging generative simulation to automatically produce vast array of tasks, scenes, and training demonstrations, thereby pushing towards unleashing infinite data for automated robot learning. These simulated environments collectively enable rapid prototyping, extensive data augmentation, and the safe testing of policies under controlled conditions prior to real-world deployment. Benchmarks. Rigorous evaluation is paramount for tracking progress. suite of benchmarks has been established to assess different aspects of VLA performance. Meta-World [45] provides set of challenging robotic manipulation tasks, while LIBERO [46] focuses on lifelong learning and knowledge retention in sequential task settings. CALVIN [47] and SIMPLER [48] are designed to benchmark generalization to novel instructions, objects, and scenes, and long-horizon reasoning. More recently, VLABench [49] has emerged as benchmark specifically tailored for evaluating the visual grounding and action reasoning capabilities of VLAs in embodied scenarios. The collective use of these benchmarks ensures comprehensive assessment of models robustness, generalization, and practical utility. 2.2 The Need for Efficient VLAs Despite the formidable capabilities demonstrated by VLA foundational models, their adoption beyond research prototypes faces significant barriers due to profound inefficiencies. These challenges stem from the two attributes that underpin their success: the model architecture and the data pipeline. TABLE 1: Efficiency-Related Metrics of Representative VLA Models. The table compares the number of parameters, inference latency, and operating frequency of various representative VLAs, where indicates that lower values are better, and indicates that higher values are better. Models Params () Infer. Latency (ms) () Freq. (Hz) () RT-2-PaLI-X [3] RT-2-PaLI-X [3] Open-VLA [1] π0 [2] HiRobot [50] GR00T N1 [51] 55B 5B 7B 3.3B 3B 2.2B 330-1000 200 166 73 73 63. 1-3 5 6 20/50 10/50 - The primary source of inefficiency lies in the computational footprint of the large VLM backbones themselves. VLA foundational models, characterized by their complex transformer-based architectures and enormous parameter counts (e.g., billions to tens of billions [52], [53], [54], [55]), demand substantial compute power for inference. As shown in Tab. 1, this results in high latency and insufficient control frequencies, which are often incompatible with the realtime demands of robotic control in dynamic environments. Furthermore, the high dimensionality of visual inputs exacerbates this issue, as processing lengthy sequences of visual tokens through the large model is computationally intensive. second critical challenge is the immense cost associated with the large-scale datasets required for VLA pretraining. The collection of real-world robotic demonstration 5 data, as seen in OXE [38] or BridgeData [40], is inherently time-consuming, expensive, and complex. Similarly, generating massive simulation datasets [44] requires significant computational resources and expert knowledge for scenario design. The subsequent pre-training of models on these large-scale datasets incurs exorbitant computational costs, consuming substantial energy and making the process accessible only to well-resourced institutions. These profound inefficiencies collectively culminate in critical deployment bottleneck for applications, particularly in resource-constrained settings such as autonomous vehicles and affordable consumer robotics. Specifically, high inference latency, excessive energy consumption, and prohibitive development cost obstruct widespread deployment and the attainment of real-time performance for responsive robotic control. Therefore, the pursuit of efficiency is not merely an optional optimization but fundamental prerequisite for unlocking the transformative potential of VLAs across the broader robotics landscape. Addressing this imperative, the field of Efficient VLAs has emerged. As illustrated in Fig. 4, the developmental trajectories of foundational VLAs and efficient VLAs are progressing in parallel. Notably, the timeline reveals distinct acceleration in the emergence of efficient VLAs, particularly from late 2024 onwards. This proliferation underscores rapidly intensifying research focus on resource-efficient models, likely driven by the demands of real-world robotic deployment. 2.3 Relevant Surveys The burgeoning development of VLAs is establishing this framework as cornerstone paradigm within embodied intelligence, catalyzing surge of scholarly surveys [16], [17], [18], [19], [20], [21] dedicated to its systematic exposition. These surveys predominantly focus on the definitions, compositional constructs, model architectures, and training paradigms, as well as datasets and benchmarks pertinent to VLAs. For instance, Ma et al. [16] provides comprehensive overview of VLA components, encompassing both low-level control policies and high-level task planners, along with wide array of datasets and benchmarks, thus offering holistic examination of various aspects of VLA models in embodied intelligence. Shao et al. [17], in addition to summarizing mainstream VLA architectures, introduces perspective driven by Large Vision-Language Models (VLMs), elucidating how large-scale pre-trained VLMs have influenced the evolution of VLAs. Xiang et al. [18] draws an analogy between VLA model post-training and human motor learning, proposing structured taxonomy aligned with human learning mechanisms. Zhong et al. [19] identifies the design of action tokenizers as the core of VLA architectural design and systematically categorizes the design principles of mainstream VLA action tokenizers. Although [16] and [17] briefly touch upon works related to efficient VLAs in certain fields, their coverage is neither thorough nor has it culminated in universally accepted taxonomy. Consequently, the field still lacks dedicated survey focusing on efficient VLAs. This survey aims to fill this critical gap by serving as the first comprehensive review of efficient VLAs, concentrating on the entire lifecycle across data, model, and training. It seeks to systematically dissect and synthesize the JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 architectural, algorithmic, and optimization strategies that enable efficient VLA development and deployment, thereby laying foundational cornerstone for future research aimed at creating scalable, resource-conscious, and practically deployable embodied AI systems."
        },
        {
            "title": "3 EFFICIENT MODEL DESIGN\nAdvancing VLAs toward ubiquitous deployment demands\nbalancing their superior multimodal representational capa-\nbilities with resource constraints in real-world applications,\nwhere computational efficiency remains a critical bottleneck.\nWhile VLAs have revolutionized embodied AI through\nseamless fusion of perception and action reasoning, their\nreliance on billion-parameter architectures imposes exces-\nsive inference latencies and resource demands, thwarting\nintegration into constrained robotic systems. Efficient model\ndesign bridges this gap through lightweight yet powerful\narchitectures and advanced compression techniques that\nuphold semantic integrity while sharply reducing parame-\nter counts and computational costs. This chapter introduces\na unified taxonomy comprising two complementary facets:\n(1) Efficient Architectures that optimizes structures via\nmodular designs, inference accelerations, and sparse pro-\ncessing to enhance efficiency without compromising model\npotency; and (2) Model Compression strategies that min-\nimize representational redundancy through layer pruning,\nquantization, and token optimization to yield compact, per-\nformant models.",
            "content": "3.1 Efficient Architectures Efficient architectures underpin scalable VLAs, curtailing computational overheads through targeted innovations that sustain multimodal synergy and action acuity amid resource scarcity. As illustrated in Fig. 5, this subsection thoroughly examines seminal contributions, from efficient attention and transformer alternatives to decoding accelerations via parallelization and generative paradigms, lightweight components, mixture-of-experts, and hierarchical processing. All representative works on efficient architectures are summarized in Tab. 2. 3.1.1 Efficient Attention Transformer [56] is the foundational model in modern neural architectures [22], [57], with attention mechanism at its core, ubiquitously facilitating feature alignment and multimodal fusion across domains, including VLAs where they orchestrate perceptual-linguistic synergies for embodied reasoning. However, attentions inherent quadratic complexity in sequence length engenders prohibitive computational burdens, particularly for protracted action horizons in real-time robotics. To address these, optimizations emerge along three axes: linear-time architectures [58] that subvert quadratic scaling, efficient masking strategies [59], [60] that prune redundant interactions, and KV-cache optimizations [61], [62] that streamline memory-bound inference. In linear-time paradigms, SARA-RT [58] introduces an up-training regimen to seamlessly transmute Transformers into linear-attention counterparts, preserving representational fidelity to enable real-time control under constrained 6 budgets. For efficient masking, Long-VLA [59] deploys phase-aware input masking, directing focus to static camera tokens during movement phases and gripper tokens during interactions, forging robust mechanism for extended operations; meanwhile, dVLA [60] pioneers unified diffusion scaffold with prefix attention masks, synergizing with KV caching to curtail inference compute and memory footprints. KV-cache refinements manifest in RetoVLAs [61] strategic injection of discarded register tokens as auxiliary key-value pairs into action experts, augmenting crossattention with global spatial context for efficient decisionmaking without core complexity escalation. Furthermore, KV-Efficient VLA [62] compresses historical KV caches into informative chunked representations via lightweight recurrent gating, adaptively retaining salient contexts for streamlined autoregressive flows. Collectively, these refinements distill attentions essence into scalable VLA pipelines, balancing expressiveness with economical operation. 3.1.2 Transformer Alternatives Beyond attention refinements, emergent paradigms supplant Transformer backbones with equally potent yet more efficient architectures in VLAs, exemplified by Mamba [31]- led state-space models (SSMs) that deliver formidable sequence modeling with linear computational scaling. RoboMamba [12] inaugurates Mambas adoption in VLAs as the language backbone, radically obviating Transformers quadratic bottlenecks to foster streamlined embodied reasoning. FlowRAM [63] builds upon this foundation by intimately coupling Mamba with conditional flow matching and dynamic radius scheduling, amplifying efficiency and precision in high-precision manipulation scenarios. These alternatives herald shift toward leaner VLA architecture, synchronously developing architectural thrift with uncompromised multimodal performance. 3.1.3 Efficient Action Decoding In VLAs, the canonical action decoding paradigm casts images and language instructions as prompts, discretizing continuous actions into binned tokens that the visionlanguage model autoregressively generates as responses [1], thereby unifying motor outputs within cohesive token stream. Yet, this autoregressive approach harbors inherent vulnerabilities, chiefly cumulative inference latency from token-by-token generation, which hampers real-time embodied control. To counter these, parallel decoding paradigmsexemplified by Jacobi decodings iterative synchronization and speculative decodings draft-verification cascadesaccelerate autoregressive streams by predicting multiple tokens in parallel, all while curbing fidelity loss. Furthermore, generative modeling-based modules displace autoregressive chains with holistic trajectory synthesis, leveraging latent distributions to evade sequential bottlenecks and deliver deterministic, low-latency action sequences for nimble robotic agents. Parallel Decoding. Parallel decoding paradigms alleviate autoregressive latencies in VLAs by orchestrating concurrent token predictions, thereby catalyzing real-time embodied actuation. OpenVLA-OFT [10], an extension of OpenVLA [1], pioneers bidirectional attention masks to supJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 7 Fig. 5: Key strategies for Efficient Architectures (section 3.1) in VLAs. We illustrate six primary approaches: (a) Efficient Attention (section 3.1.1), mitigating the O(n2) complexity of standard self-attention; (b) Transformer Alternatives (section 3.1.2), such as Mamba; (c) Efficient Action Decoding (section 3.1.3), advancing from autoregressive generation to parallel and generative methods; (d) Lightweight Components (section 3.1.4), adopting smaller model backbones; (e) Mixture-of-Experts (section 3.1.5), employing sparse activation via input routing; and (f) Hierarchical Systems (section 3.1.6), which decouple high-level VLM planning from low-level VLA execution. plant causal ones, enabling single-pass forward propagation to forecast action chunks of length in parallel, tactic echoed in EdgeVLAs [79] analogous framework for streamlined decoding. PD-VLA [65] reframes autoregressive sequences as nonlinear fixed-point equations solvable via parallel Jacobi iterations, converging in far fewer steps than sequence length to yield holistic action trajectories with minimal overhead. CEED-VLA [68] refines this by relaxing convergence thresholds for early-exit decoding, mitigating PD-VLAs stringent criteria-induced inefficiencies, while enforcing output fidelity through consistency distillation to preserve performance. Spec-VLA [71] inaugurates speculative decoding in VLAs, augmented by relaxed acceptance mechanism that amplifies draft token acceptance rates and mean lengths, precipitating marked inference accelerations. Generative Decoding. Pioneering generative paradigms for action sequences in VLAs, TinyVLA [64] inaugurates Diffusion Policy [89] as dedicated decoder, directly synthesizing continuous robotic actions and circumventing discrete tokenizations rigidity. HybridVLA [66] elevates this synergy by co-locating diffusion and autoregressive modeling within singular Transformer scaffold, compressing DDIM [35] sampling to parsimonious four steps sans efficacy erosion, thus catalyzing inference alacrity. FreqPolicy [67] augments flow-based policies with frequency-domain consistency constraints, exploiting action sequences temporal coherence to linearize probability flows and expedite single-step inference while safeguarding sequential integrity. FlowRAM [63] embeds Conditional Flow Matching into robotic policy synthesis, regressing deterministic vector fields to deftly evade diffusions iterative denoising cascades, yielding swift inference across minimal timesteps. MinD [69] forges an efficient generative decoding strategy by conditioning its diffusion policy on emergent single-step latents from predictive world model, eschewing costly full-frame video generation and affirming that compact, information-dense future representations suffice for high-fidelity control signals. VQ-VLA [70] pioneers VQ-VAE [90] for action trajectory discretization, cascading pretrained VQ decoder atop OpenVLA to distill high-fidelity sequences with unyielding efficiency. AMS [72] refines diffusion decoding through cached latent vectors and vetted success trajectories, judiciously pruning denoising iterations for accelerated convergence. NinA [73] supplants diffusion with normalizing flow decoders, harnessing invertible one-shot sampling to exorcise latency specters inherent in generative chains. Culminating these advances, Discrete Diffusion VLA [74] fuses diffusions progressive refinement with discretetoken interface, enabling adaptive easy-first, hard-later decoding and re-masking for error correctionpreserving VLM priors, sidestepping autoregressive bottlenecks, and thereby aligning action decoding with VLM transformers to harness unified scaling for expansive VLAs. 3.1.4 Lightweight Component Lightweight components furnish the most straightforward conduit to efficient VLAs, distilling parametric essence while upholding multimodal prowess and action acuity. RoboMamba [12] exemplifies this ethos with mere 3.7M-parameter MLP policy headconstituting just 0.1% of total parametersto deftly forecast 6-DoF end-effector poses, slashing overhead without curtailing precision. TinyVLA [64] pioneers compact scaffold, pairing pretrained lightweight VLM (<1.4B parameters) on universal vision-language corpora with diffusion-based policy decoder, yielding unprecedented inference velocity and data thriftiness alongside intact operational fidelity and generalization. Likewise, Edge-VLA [79] and MiniVLA [80] (a lightweight variant of OpenVLA) assemble 1B-parameter model via Qwen2-0.5B SLM backbone integrated with SigLIP and DINOv2 visual encoders, supporting edge deployment with commendable compactness. CLIP-RT [75] repurposes frozen pretrained CLIP as unified encoder, paring parameters to one-seventh of OpenVLAs [1] 7B (down to 1B) yet surpassing its average success rate by 24%, thus inverting scale-performance axioms. Among Diffusionthe minimalist DiVLA-2B leverages VLA [91] variants, Qwen2-VL-2B [92] as VLM backbone, clocking 82 Hz on solitary A6000 GPU to epitomize throughput supremacy. SVLR [76] innovates by amalgamating sundry lightweight pretrained modulesencompassing Mini-InternVL [93] for vision-language, CLIPSeg [94] for zero-shot segmentation, JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 8 TABLE 2: Representative Works on Efficient Architectures. Method/Model Year Key Innovation in Efficient Architectures SARA-RT [58] Long-VLA [59] RetoVLA [1] KV-Efficient VLA [1] dVLA [1] RoboMamba [12] FlowRAM [63] TinyVLA [64] PD-VLA [65] OpenVLA-OFT [10] HybridVLA [66] FreqPolicy [67] CEED-VLA [68] FlowRAM [63] MinD [69] VQ-VLA [70] Spec-VLA [71] AMS [72] NinA [73] Discrete Diffusion VLA [74] RoboMamba [12] TinyVLA [64] CLIP-RT [75] SVLR [76] NORA [77] SmolVLA [13] SP-VLA [78] EdgeVLA [79] MiniVLA [80] GeRM [81] FedVLA [82] TAVP [83] HiRT [84] RoboDual [85] DP-VLA [86] HAMSTER [87] FiS [88] MinD [69] 2024 2025 2025 2025 2025 2024 2025 2024 2025 2025 2025 2025 2025 2025 2025 2025 2025 2025 2025 2024 2024 2024 2025 2025 2025 2025 2025 2025 2024 2025 2025 2024 2024 2024 2025 2025 2025 (A) Efficient Attention Introduces up-training to transform quadratic transformers into linear-attention models for efficient VLAs. Uses phase-aware input masking to optimize attention focus in long-horizon efficient VLAs. Reuses discarded register tokens to enhance spatial reasoning in VLAs. Employes RNN-gated chunked KV cache to accelerate and compress attention in VLAs. Integrates prefix attention masking and KV caching for inference speedup in diffusion VLAs. (B) Transformer Alternatives Replaces Transformers with Mamba for linear-complexity reasoning in efficient VLAs. Integrates region-aware Mamba fusion with flow matching for fast robotic policies. (C) Efficient Action Decoding Distills large VLAs for fast, data-efficient generative action decoding. Reformulates autoregressive decoding into parallel fixed-point iterations for efficient VLA action chunking. Enables parallel action decoding using bidirectional attention in fine-tuned continuous VLAs. Merges diffusion and autoregression in unified models for hybrid generative decoding. Uses frequency consistency to optimize flow-based generative action policies. Enhances Jacobi decoding via consistency distillation and early-exit for parallel VLA efficiency. Combines Mamba fusion with flow matching for rapid generative manipulation. Develops dual-system world models for generative real-time VLA planning. Scales quantized tokenizers for smoother discrete generative action outputs. Introduces speculative decoding with relaxed acceptance for parallel token verification in VLAs. Applied OS-level primitives to enhance generative action efficiency in VLAs. Trained VLA decoders using normalizing flows for continuous action generation. Embeds discrete diffusion for scalable unified action decoding. (D) Lightweight Component Integrates lightweight action decoder for parameter-efficient robotic policy fine-tuning Distills knowledge from large models into compact VLAs with reduced parameters. Adapts pre-trained CLIP backbone, achieving 7x fewer parameters for efficient policies. Combines small, pre-trained models in modular framework for consumer-grade GPU deployment. Employs 3B-parameter Qwen backbone with action token compression for size reduction. Enables single-GPU training via lightweight architecture for affordable robotics. Applies action-aware scheduling with lightweight action generator to eliminate temporal redundancies. Utilizes small language models, eliminating autoregression for edge device efficiency. Reduces parameter count 7x using smaller LLM while maintaining model performance. (E) Mixture-of-Experts Integrates MoE in VLA for faster inference and higher model capacity. Introduces dual gating MoE for adaptive expert activation in federated VLAs. Employs MoE visual encoder to disentangle features across tasks in VLAs. (F) Hierarchical System Implements hierarchical transformers that enable low-frequency VLM to guide high-frequency policy for efficiency. Synergizes generalist VLA with specialist diffusion for precise multi-step action generation. Adopts dual-process framework with large reasoning model directing small sensory executor. Deploys high-level VLM producing 2D paths to direct low-level manipulation policies. Unifies dual systems by embedding execution in reasoning via parameter sharing. Utilizes asynchronous diffusions for low-frequency prediction and high-frequency action coordination. Phi-3 [95] for language modeling, and all-MiniLM [96] for sentence embeddingsinto retraining-free VLA edifice, enabling scalable task generalization on consumer-grade hardware. NORA [77] harnesses Qwen-2.5-VL-3B [97] as core, synergizing with the FAST+ [14] action tokenizer to rival or eclipse far bulkier VLAs in efficacy. SP-VLA [78] unveils an action-aware scheduler that partitions sequences into deliberative and intuitive segments, dynamically invoking heavyweight VLAs or lightweight predictive generators for frequency-adaptive acceleration with negligible performance decrement. These innovations unlock the promise of ever-lighter components in VLAs, collectively catalyzing scalable embodied intelligence. 3.1.5 Mixture-of-Experts Mixture-of-Experts (MoE) architectures engender efficiency in VLAs by routing tokens to specialized subnetworks, activating only fraction of parameters to amplify capacity sans commensurate inference costs. GeRM [81] pioneers sparse MoE integration into quadruped reinforcement learning, demonstrating that selective parameter activation scales model expressiveness for multitask generalization while upholding inference thrift, thereby inaugurating paradigm for high-fidelity, computationally viable VLA policies. FedVLA [82] advances this with Dual Gating Mixture-of-Experts (DGMoE) mechanism, transcending conventional top-K routings unidirectional token-expert selection via self-aware experts endowed with JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 9 Fig. 6: Key strategies for Model Compression (section 3.2) in VLAs. We illustrate three primary approaches: (a) Layer Pruning (section 3.2.1), which removes redundant layers to reduce model depth and computational cost; (b) Quantization (section 3.2.2), which reduces the numerical precision of model parameters to decrease memory footprint and accelerate inference; and (c) Token Optimization (section 3.2.3), which minimizes the number of processed tokens via token compression (merging tokens), token pruning (dropping non-essential tokens), and token caching (reusing static tokens). bidirectional affinities, dynamically sparsifying computational graphs to sustain task efficacy amid resource austerity. TAVP [83] further refines the motif through TaskAware MoE (TaskMoE), which conditionally engages taskspecific experts on amalgamated linguistic-visual signals, disentangling representations across heterogeneous manipulation domains while fostering parameter efficiency via semantically clustered routing gates. Collectively, these MoE instantiations herald trajectory toward modular, adaptive VLAs that equilibrate expansive generalization with pragmatic deployment. 3.1.6 Hierarchical Systems Hierarchical systems in VLAs draw seminal inspiration from psychological dual-process theories, such as [98], [99], bifurcating cognition into deliberate, semantics-rich deliberation and intuitive, rapid execution to emulate humanlike embodied agency. These paradigms decouple computeintensive vision-language model (VLM) inference (System 2) from latency-sensitive action generation (System 1), orchestrating asynchronous execution to harmonize profound semantic comprehension with instantaneous control imperatives. HiRT [84], DP-VLA [86], and SmolVLA [13] inaugurate such hierarchical scaffolds, wherein languid yet semantically dense VLM representations asynchronously steer lightweight, high-cadence policies, striking an optimal balance between generalization capacity and real-time actuation fidelity. RoboDual [85] operationalizes this duality with OpenVLA as high-level planner that provides discretized actions and latent encodings, augmented by lightweight Diffusion Transformer specialist for rapid enactment; latency-aware training therein equips the specialist to redress temporal misalignments from asynchronous interplay, enhancing synergistic robustness. HAMSTER [87] harnesses upper-echelon VLMs to engender executable 2D trajectory sketches that guide specialized low-level policies, melding VLMs expansive generalization with compact policy thrift. FiS [88] advances inter-system cohesion via partial parameter sharing, embedding the System 1 executor within the VLM-centric System 2 to foster seamless inference-execution orchestration. Fast ECoT [100] extends this asynchronous inference ethos to the Embodied Chainof-Thought (ECoT) reasoning framework, strategically decoupling latent deliberation from action streams to enable parallel cognitive refinement and output generation, thereby amplifying reasoning depth without temporal encumbrances. MinD [69] consummates this lineage by coadjudicating low-frequency generative world model for protracted scene foresight with high-frequency diffusion policy for on-the-fly control, conditioning the latter on semantically potent single-step latents to enable effective realtime decision-making without exorbitant computational toll. Collectively, these resilient hierarchical VLA pipelines scale cognitive depth with operational alacrity. 3.2 Model Compression At the forefront of resource-constrained deployment, model compression stands as the cornerstone for efficient VLAs, masterfully distilling voluminous parameter ensembles into streamlined architectures while skillfully enhancing the inference efficiency of VLAs. As illustrated in Fig. 6, this subsection systematically surveys pivotal techniques, from layer pruning and quantization to token optimization, elucidating their general schemes and how they benefit efficiency to cultivate robust, edge-compatible architectures. Tab. 3 summarizes the representative model compression methods. 3.2.1 Layer Pruning Layer pruning, the most straightforward paradigm in model compression, precisely excises redundant layers via dynamic mechanisms such as early exits or selective layer skipping, yielding pronounced reductions in parameter counts and inference latency for VLAs. Motivated by the massive interlayer redundancy in large language modelswhere adjacent layers exhibit high interlayer cosine similarity [11], [15]this approach unlocks efficiency without eroding the nuanced multimodal synergies essential for embodied tasks. We delineate two principal categories: training-free and training-based strategies. Training-free methods expedite deployment by leveraging post-hoc analysis. Pioneering this vein, DeeR-VLA [101] JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 10 TABLE 3: Representative Works on Model Compression. Method/Model Year Key Innovation in Model Compression DeeR-VLA [101] MoLe-VLA [11] SmolVLA [13] EfficientVLA [15] RLRC [102] LightDP [103] FLOWER [104] OpenVLA [1] QAIL [105] FAST [14] SQIL [106] BitVLA [107] RLRC [102] SQAP-VLA [108] FAST [14] VLA-Cache [109] HybridVLA [66] FlashVLA [110] SmolVLA [13] Fast ECoT [100] EfficientVLA [15] SP-VLA [78] CronusVLA [111] VOTE [112] AMS [72] CogVLA [113] SpecPrune-VLA [114] SQAP-VLA [108] LightVLA [115] KV-Efficient VLA [62] ADP [116] Oat-VLA [117] 2024 2025 2025 2025 2025 2025 2025 2024 2024 2025 2025 2025 2025 2025 2025 2025 2025 2025 2025 2025 2025 2025 2025 2025 2025 2025 2025 2025 2025 2025 2025 (A) Layer Pruning Deploys multi-exit architecture for dynamic early termination of redundant layers in VLAs. Routes spatial-temporal states to selectively skip non-essential LLM layers in VLAs. Prunes redundant = L/2 layers in the VLM for compact VLA deployment. Analyzes inter-layer redundancies to prune inconsequential language layers training-free. Applies structured layer pruning followed by RL-based recovery in compressed VLAs. Optimizes pruning recoverability in denoising modules via unified retraining pipeline. Fuses intermediate modalities to prune 50% of LLM layers for diffusion head capacity. (B) Quantization Demonstrates quantization for serving 7B-parameter VLAs without degrading downstream task success rates. Integrates quantization-aware fine-tuning in imitation learning to boost robustness against low-bit precision errors. Applies DCT-based tokenization to enable efficient quantization of high-frequency action sequences. Employs saliency-based loss weighting during quantization-aware training to prioritize mission-critical states. Achieves 1-bit ternary quantization across VLAs with distillation for vision encoder alignment. Employs RL to recover performance after pruning, enhancing robustness to subsequent 4-bit quantization. Co-designs quantization with token pruning via awareness criteria for incompatibility resolution. (C) Token Optimization Compresses action tokens effectively via DCT and BPE for enhanced high-frequency VLA efficiency. Caches static visual tokens dynamically with task-aware eviction for improved sequential VLA speedup. Caches KV states before diffusion tokens for better iterative denoising efficiency in VLAs. Prunes visual tokens precisely via ICS and reuses actions based on stable tokens. Compresses visual tokens down to 64 per frame using advanced pixel shuffle. Caches high-level reasoning tokens efficiently for reuse across multiple ECoT timesteps in VLAs. Prunes visual tokens dynamically with task-relevance guidance and caches intermediates for efficient action decoding. Prunes spatio-semantic tokens adaptively through velocity-correlated retention ratios for better performance. Caches motion features securely in FIFO queues for efficient multi-frame VLA inference. Compresses action chunks into single <ACT> tokens precisely for VLA decoding acceleration. Compresses action contexts through hashing-based indexing and evicts low-priority KV caches using LRU-priority. Aggregates and prunes visual tokens instruction-driven precisely with FiLM routing and percentile thresholds. Prunes tokens dynamically using global-local attention mechanisms and velocity control strategies. Prunes tokens in quantized model effectively with robot-aware rings and spatial sampling criteria. Prunes visual tokens differentiably via query-based Gumbel-softmax selection for optimization. Prunes KV chunks selectively with RNN gating for recurrent context summarization efficiency. Prunes vision tokens dynamically with trajectory-gated retention and detailed text guidance. Compresses to object-centric and agent-centric tokens via average pooling and gripper-guided selection. introduces dynamic early-exit framework that circumvents subsequent layers upon ensuring action prediction consistency, thereby slashing computational overhead while preserving competitive task performance. SmolVLA [13], inspired by foundational pruning heuristics, adopts pragmatic, naive strategyskipping fraction = 2/L of layersfor real-world applicability in resource-scarce settings. RLRC [102] uses Taylor importance criteria to gauge and eliminate low-contribution layers, achieving an overall aggressive sparsity of 90%. FLOWER [104] tailors strategic pruning to architecture: for encoder-decoder VLMs like Florence-2 [118], it discards the entire decoder, retaining only the encoder to halve layer counts; for decoder-only variants like SmolFlow2-Video [119], it prunes the terminal 30% of layers. In contrast, training-based schemes infuse adaptability through optimization. MoLe-VLA [11] reconceptualizes LLM layers as distinct experts, deploying lightweight Spatial-Temporal Aware Router (STAR) to parse visualspatial and linguistic-temporal cues, dynamically electing and bypassing superfluous layers for tailored execution. LightDP [103] harmonizes learnable layer pruning with consistency distillation via SVD-based importance estimation and the Gumbel-Softmax trick [120], [121], orchestrating unified framework that dynamically prunes Diffusion Transformer layers during training to amplify compression efficacy. 3.2.2 Quantization Quantization, pivotal pillar in model compression, discretizes continuous weights and activations into lower-bit representations, curtailing memory footprints and accelerating inference for VLAs while preserving the fidelity of multimodal action synthesis. Pioneering empirical validation, OpenVLA [1] systematically probes quantizations efficacy in large-scale VLAs, demonstrating that aggressive 4-bit post-training quantization halves GPU memory demands while sustaining real-world robotic task proficiency comparable to fullprecision baselines. QAIL [105] advances this frontier with Quantization-Aware Imitation Learning framework, incorporating Quantization-Robust Behavior Cloning (QBC) loss that explicitly aligns quantized policy action distributions with their full-precision counterparts, reducing error accumulation in sequential decision-making and facilitating performant edge-device orchestration. SQIL [106] introduces saliency-aware quantization paradigm, reclaiming near-full-precision performance under 4-bit austerity and JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 yielding up to 2.5 inference speedup in authentic robotic benchmarks. BitVLA [107] innovates with distillationaware training paradigm to pioneer 1-bit quantization in VLAs, seamlessly embedding LLM backbones and visual encoders into ternary parameter spaces {1, 0, 1}, vindicating extreme post-training quantizations viability with 3.36 memory compression and competitive prowess in intricate manipulation tasks. RLRC [102] augments this toolkit via performance recovery pipeline, strategically deploying reinforcement learning fine-tuning to empower subsequent 4-bit quantization with maximal memory thrift without performance degradation. Extending quantization to ancillary efficiencies, FAST [14] repurposes it for token compression, forging systematic action quantization scaffold that transmutes continuous action into informationrich discrete latent spaces via spectral decomposition. SQAP-VLA [108] integrates quantization and token pruning via Pruning-Targeted Quantizer Enhancement, applying Hadamard transforms to the weights and activations of the query and key layers prior to low-bit discretization, thereby mitigating quantization-induced distortions in attention maps to foster their interpretability and resilience for salient token selection. Collectively, these advancements underscore quantizations transformative role in sculpting VLAs for ubiquitous, resource-frugal embodiment. 3.2.3 Token Optimization Token optimization, an elegant facet of model compression, strategically refines the sequence of representational tokens in VLAs, thereby mitigating the quadratic escalation of transformer-based computations in inference while safeguarding the performance of integrated visual-linguisticactional reasoning. Through targeted mechanisms like compression, pruning, and caching, this paradigm curtails token redundancy, alleviating memory pressures and expediting the temporal dynamics of decision-making in embodied systems, thus enabling agile deployment on resource-limited robotic platforms. This subsection methodically reviews representative methods, illuminating their seamless integration with decoder frameworks to yield lean, high-fidelity multimodal inference pipelines. Token Compression. Token compression distills voluminous token streams into succinct representations by algorithmically aggregating informational essence, thereby curtailing the computational swell in VLAs and bolstering inference velocity without forfeiting multimodal fidelity. For vision tokens, SmolVLA [13] enforces spatial thrift via pixel shuffle operations, confining visual tokens to mere 64 per frame to temper the deluge of perceptual inputs. CogVLA [113] advances aggregation routing with Encoder-FiLM modules [122], coalescing myriad patch tokens into sparse, instruction-driven aggregates that yield substantial computational economy, while preservingor even augmentingcross-modal coherence for action synthesis. Oat-VLA [117], on the other hand, pioneers an objectagent-centric tokenization scheme that strategically injects structural inductive biases into visual processing, resulting in an order-of-magnitude compression of vision tokens. Shifting to action tokens, FAST [14] pioneers spectralit applies discrete cosine transform domain parsimony: 11 (DCT) to normalized action sequences, transmuting signals into frequency components, then refines them via bytepair encoding (BPE [123]) into compact, information-dense token cascades that streamline sequential policy generation. VOTE [112], in turn, pioneers extreme condensation by emitting singular <ACT> token to encapsulate prospective action trajectories, decoded post hoc through lightweight MLP action head into precise continuous maneuversthus dramatically contracting the output streams. Token Pruning. Token pruning algorithmically excises redundant tokens to preserve only indispensable ones, thereby streamlining computational demands in VLAs and enhancing inference efficiency without impairing multimodal coherence. This technique harnesses salience metrics and adaptive heuristics to distill token sequences, fostering agile embodied reasoning amid resource constraints. Inspired by FastV [124], which is classic representative of token pruning schemes in the field of VLMs, FlashVLA [110] proposes an information contribution score (ICS)-guided pruning mechanism, devising training-free, Flash Attention [125]-compatible framework that sets plug-and-play paradigm for VLA inference acceleration. Building thereon, EfficientVLA [15] deploys multi-step Task-Relevance and Diversity-Driven Visual Token Pruning scheme, harmonizing task saliency with feature heterogeneity to forge concise, richly informative token ensemble. SP-VLA [78] refines spatial-semantic perceptual capability through Dual-Aware Token Pruning, jointly evaluating semantic and positional importance while dynamically tuning pruning ratios guided by velocity scales for nuanced adaptation. Advancing cognitive integration, CogVLA [113] embeds instruction sensitivity within the LLM backbones pruning pipeline, engendering lean representations that amplify semantic potency. SpecPrune-VLA [114] experimentally points out that EfficientVLAs [15] strategy of relying only on local information for pruning is unreliable and pioneers self-speculative pruning by exploiting temporal continuity from prior inferences to inform current token curation, yielding marked speedups in VLA dynamics. SQAPVLA [108] fortifies resilience with Quantization-Aware Token Pruning, adeptly pinpointing vital tokens despite quantizations skewing of statistical distributions in features such as attention scores, and ultimately outperformed the EfficientVLA [15] in experiments. Culminating this progression, LightVLA [115] introduces an adaptive, performanceoriented visual token pruning framework, generating dynamic queries for importance gauging and Gumbel softmax for seamless, differentiable token selection, solving the problem of EfficientVLAs [15] reliance on fixed pruning ratios. Furthermore, KV-Efficient VLA [62] introduces two-stage token pruning mechanism, which first splits the historical KV Cache into fixed-size chunks, aggregates them into single compressed representation, and then applies lightweight RNN to prune these chunks through threshold. Like SP-VLA [78] and SpecPrune-VLA [114], ADP [116] introduces an action-aware gating token pruning mechanism. The difference is that SP-VLA [78] and SpecPruneVLA [114] set the token pruning ratio based on the motion speed of the end effector, while ADP [116] dynamically decides whether to prune or not based on the displacement JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 of the end effector trajectory within small period of time. In addition, ADP introduces text-driven pruning mechanism, which is able to compute the similarity between vision tokens and task instructions based on cross-modal attention and retains the most relevant Top-K tokens, enriching the semantic information of token pruning. Token Caching. Token caching, the capstone of token optimization, stockpiles reusable tokens for iterative reuse in subsequent processing stages, thereby eliminating redundant computations in VLAs and enhancing inference efficiency while maintaining multimodal temporal coherence. This strategy exploits temporal invariances across frames or steps, transforming static or stable representations into persistent assets that undergird fluid embodied trajectories. VLA-Cache [109] forges reusable token repertoires by pinpointing static tokens with minimal inter-frame variance and evicting task-relevant ones, leveraging KV-cache for seamless redeployment in ensuing iterations. HybridVLA [66] ingeniously extrapolates KV-cachingthe bedrock of autoregressive decodingto diffusion models iterative denoising, hoarding invariant conditional token key-value pairs to excise inter-step redundancies, thus amplifying inference velocity without compromising diffusion-driven continuous action fidelity. FlashVLA [110] introduces judiciously recycling token-aware action reuse protocol, prior actions via metrics of action and token stability to harness sequential consistencies. Fast ECoT [100] infuses caching into Embodied Chain-of-Thought (ECoT) reasoning, reusing swaths of inference chains rather than regenerating them, compacting computational graphs for precipitous latency reductions. EfficientVLA [15] critically points out that the VLA-Caches [109] approach is limited by LLMs memory bottleneck and deploys static caching of intermediate self-attention and MLP features across denoising steps, sidestepping iterative redundancies in generation loops. CronusVLA [111] pioneers feature-level token caching via FIFO queue that stockpiles and recycles compact motion features, decoupling compute-intensive singleframe perception from lightweight multi-frame inference. AMS [72] sustains GPU-resident Context Pool archiving prior inference intermediates, inaugurating hardwareaware, holistic caching paradigm that transcends conventional key-value bounds to encompass latent vectors and output embeddings across the whole VLA pipeline. In essence, token caching increases VLA efficiency by perpetuating representational continuity across temporal horizons."
        },
        {
            "title": "4 EFFICIENT TRAINING",
            "content": "Vision-Language-Action (VLA) models derive their profound capabilities from large-scale, pre-trained VLM backbones, inheriting robust visual understanding and commonsense reasoning. This foundation provides significant advantage, enabling VLAs to interpret complex multimodal scenes and natural language instructions from the outset. However, this inheritance itself brings substantial burdens, rendering their training computationally intensive [51], time-consuming [1], and highly dependent on vast high-quality datasets [38]. Consequently, advancing efficient training methodologies has emerged as paramount 12 research objective, aiming to alleviate these resource demands without sacrificing model performance. As illustrated in Fig. 7, this chapter systematically explores the spectrum of these techniques, focusing on two pivotal stages: (1) Efficient Pre-training, instilling foundational action capabilities into pre-trained VLMs or training the entire VLA from scratch with minimal overhead, and (2) Efficient Posttraining, enabling the swift and effective deployment of VLAs to specific downstream tasks. VLA-centric Taxonomy of Training. Our taxonomy is defined from functional, VLA-centric perspective. We define Pre-training as the entire process of migrating generalpurpose VLM into the embodied domain to create an initial, action-aware policy. This foundational stage is concerned with instilling the fundamental capability to act. In contrast, Post-training focuses on the subsequent specialization of this generalist policy, adapting it to excel in specific tasks, environments, or under particular resource constraints. This perspective clarifies why our pre-training VLA stage encompasses broad spectrum of methods, including some techniques often labeled as post-training in other fields. The key distinction is not the method itself, but its objective: if the goal is to create the first version of the action-capable model from VLM, we classify it as pre-training. This approach offers more fundamental and logically consistent framework that aligns with the practical development lifecycle of VLAs. 4.1 Efficient Pre-Training Efficient pre-training is critical research direction aimed at decoupling VLA model performance gains from the prohibitive computational footprint incurred by large-scale foundation backbones and massive multimodal datasets. As illustrated in Fig. 7 left, this section systematically examines advancements categorized into data-efficient pretraining, efficient action representation, and other efficient pre-training strategies. By highlighting these innovations, we outline the critical pathways for developing robust, general VLA policies without incurring the traditional burden of extensive, resource-intensive pre-training cycles. We list representative works in Tab. 4. 4.1.1 Data-Efficient Pre-training For the data scarcity problem faced in training VLA models, some works address it by efficient data collection, which will be presented in section 5, and other works focus on dataefficient pre-training. Data-efficient pre-training addresses the VLA data dependency by prioritizing the judicious use of both scarce robotic trajectories and readily available largescale non-robotic data, such as human ego-centric videos, to overcome the prohibitive cost and redundancy of vast multimodal corpora. This subdomain bifurcates into two principal strategies: leveraging unlabeled data via sophisticated self-supervised training objectives and bridging domain gaps through mixed data co-training frameworks. Self-Supervised Training. Self-supervised training represents crucial paradigm for enhancing data efficiency in VLA models by synthesizing effective supervisory signals from unlabeled or readily available datasets, thereby mitigating the substantial data scarcity inherent in embodied learning. This methodology bifurcates into two main JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 13 Fig. 7: Key strategies for Efficient Training (section 4) in VLAs, divided into two main stages. (a) Efficient PreTraining (section 4.1) migrates general-purpose VLMs into the embodied domain to create an initial, action-aware policy, encompassing Data-Efficient Pre-training (section 4.1.1), Efficient Action Representation (section 4.1.2) , and Other Pretraining Strategies (section 4.1.3). (b) Efficient Post-Training (section 4.2) subsequently specializes this policy for specific tasks, leveraging Supervised Fine-tuning (section 4.2.1) and RL-Based Methods (section 4.2.2). strategies: (1) augmenting limited expert trajectories and (2) harnessing internet-scale ego-centric video. The first approach focuses on maximizing the utility of existing robotic data. DTP [126] strategically employs diffusion-based trajectory prediction model as an efficient and scalable pre-training objective. By learning the generated future end-effector trajectories in the RGB domain, DTP effectively bridges the modality gap between highlevel visual-language input and the continuous physical action space, consequently improving sample efficiency and long-horizon generalization in imitation learning. The second, and highly prevalent, strategy centers on utilizing massive, unlabeled internet-scale ego-centric videos to alleviate the severe data dependency of VLA models. Early works, such as LAPA [127], innovated by entirely circumventing the need for expensive real robot action labels. It achieves this by learning discrete latent action space from vast amounts of unlabeled video data. However, Bu et al. [128] critically noted that LAPAs [127] raw pixel-based reconstruction objective inadvertently encodes task-irrelevant dynamics (e.g., camera shake, background motion), which severely interferes with subsequent policy training. To address this, they proposed Task-centric Latent Action Learning, which utilizes two-stage VQ-VAE to decouple and quantize purely task-centric latent actions from the video stream, further increasing data efficiency. LAWM [129] later extended LAPAs [127] latent action learning principle, applying it to more compact architectures like BAKU [130] and RSSM [131]. EgoVLA [132] abandoned latent action spaces and introduced shared action space based on MANO parameters, effectively translating human self-centric video data into actionable VLA model training. For dexterous manipulation, Being-H0 [133] introduced Physical Instruction Tuning by leveraging part-level motion tokenization (discretizing MANO parameters) on the largescale UniHand dataset, allowing the VLA model to acquire high-fidelity dexterous manipulation priors with superior sample efficiency. Finally, methods focused on explicit dynamics learning, such as RynnVLA-001s [134] three-stage generative pretraining (I2V prediction of future frames) and Wang et al.s [135] World Model objective, demonstrate how learning environment dynamics from action-less videos can form robust foundation, which is then fine-tuned with action tokens to acquire the final policy. Mixed Data Co-training. Mixed Data Co-training is potent strategy for boosting VLA model efficiency and generalization by jointly training on heterogeneous datasets across varying quality and modality. GeRM [81] establishes generalist robotic model using Conservative Q-Learning (CQL) offline RL framework, which robustly leverages both expert and suboptimal data by strategically mitigating policy overestimation on out-of-distribution actions. HAMSTER [87] employs hierarchical VLA structure: highlevel VLM predicts coarse 2D end-effector trajectory, which conditions low-level 3D control policy. This decoupling efficiently assimilates massive cross-domain data. Similarly, GraspVLA [138] unifies autoregressive perception and flow-matching action generation within Chain-ofThought (CoT) objective, enabling seamless joint training across synthetic and internet semantic data. Furthermore, AnyPos [139] addresses task-specific data dependency by introducing Arm-Decoupled Estimation and DirectionAware Decoder. This mechanism extracts generalizable motion primitives from large-scale task-agnostic datasets, substantially enhancing zero-shot transferability. 4.1.2 Efficient Action Representation Action, as the distinct embodied modality, is frequently high-dimensional, continuous, and noisy, significantly impeding VLA training efficiency and generalization. To address this, several works explore more compact and semantic action representations. This effort can be categorized into action space compression and innovative action modeling. primary direction employs compression to transform continuous, high-dimensional actions into more succinct representation space. This reduces the search space for policy learning. LAPA [136], Bu et al. [128], RynnVLA001 [134], and LAWM [129] all utilize the autoencoder [143] JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 14 TABLE 4: Representative Works on Efficient Pre-training. Method/Model Year Key Innovation in Efficient Pre-training GeRM [81] LAPA [136] HAMSTER [87] DTP [126] Humanoid-VLA [137] GraspVLA [138] UniVLA (Bu et al. ) [128] UniVLA (Wang et al.) [135] EgoVLA [132] AnyPos [139] Being-H0 [133] RynnVLA-001 [134] LAWM [129] LAPA [136] FAST [14] UniVLA (Bu et al. ) [128] cVLA [140] EgoVLA [132] VLA-Adapter [141] RynnVLA-001 [134] ReSET [142] LAWM [129] RoboMamba [12] TinyVLA [64] TAVP [83] (A) Data-Efficient Pre-training Uses mixture-of-experts architecture and offline reinforcement learning to optimize data utilization. Through unsupervised action quantization, latent skills can be extracted from unlabeled videos. Hierarchical action modeling minimizes data redundancy for open-world manipulation pretraining. Diffusion-based synthesis optimizes trajectory data usage for long-horizon task pretraining. Egocentric visual priors reduce data requirements for universal humanoid VLA pretraining. Synthetic data scaling enables robust grasping pretraining with minimal real-world samples. Cross-embodiment analysis extracts task-centric latents from sparse videos for adaptive pretraining. Multimodal tokenization unifies sparse data streams for efficient VLA pretraining. Transfers expertise from egocentric human videos with minimal data through unified action space. Enables training from task-agnostic data for bimanual pretraining by an inverse dynamics model. Hand motion priors from human videos minimize dexterous pretraining data via physical instruction tuning. Human demonstrations encoded via generative models streamline robot manipulation pretraining. Self-supervised world models forecast latent dynamics, minimizing imitation pretraining data. (B) Efficient Action Representation VQ-VAE quantizes unsupervised latent actions from videos for label-free VLA pretraining. Uses DCT, BPE, and quantization to compress action sequences, reducing VLA pre-training time. Language-conditioned decoupling derives latent actions from cross-embodiment videos for compact pretraining. Discrete keypose prediction in image space enables lightweight simulation-based VLA pretraining. Proposes shared action space based on MANO parameters and retargets robot demo to human action. Bridge Attention injects optimal vision-language features into actions for tiny-scale model pretraining. ActionVAE compresses chunks into continuous latents for multi-stage pretraining transfer. Restructures initial states with human-derived policies, and significantly reduces reliance on expert presentations. Extract latent actions via joint imitation-world modeling from unlabeled videos. (C) Other Pre-training Strategies Frozen CLIP integrates with Mamba, followed by lightweight projectors, enabling compute-efficient pre-training. Compact VLMs initialize VLAs via LoRA, bypassing extensive robotic datasets for data-efficient training. Accelerated an efficient strategy by novel pseudo-environment to proactively capture informational perspectives. 2024 2024 2025 2025 2025 2025 2025 2025 2025 2025 2025 2025 2025 2024 2025 2025 2025 2025 2025 2025 2025 2025 2024 2024 2025 principle to distill actions into latent space. Specifically, LAPA and Bu et al. leverage VQ-VAE [90] to capitalize on both quantization and strong compression capabilities. In contrast, RynnVLA-001 proposes ActionVAE on standard VAE [144] architecture, while LAWM derives latent action representation from the powerful DreamerV3 [131] world model. Beyond AE, FAST [14] achieves up to 5 reduction in pre-training time by directly compressing action sequences via algorithms like Discrete Cosine Transform (DCT) and Byte-Pair Encoding (BPE), effectively achieving dimensionality reduction in the sequence domain. Another set of strategies innovates on how actions are modeled relative to other modalities and defined. Both EgoVLA [132] and VLA-Adapter [141] employ crossmodal feature linkage strategy to enhance training efficiency. Specifically, EgoVLA bridges human demonstrations to robot action representations through shared action space based on MANO parameters, while VLA-Adapter utilizes Bridge Attention module to connect vision-language representations to actions. Furthermore, optimizing the coordinate system choice can drastically lower complexity. cVLA [140] eschews the traditional robot base frame, instead representing actions in the image coordinate system before mapping them to the end-effector pose space, facilitating lower-dimensional action encoding. Finally, Resimpler, SET [142] focuses on compressing the dense robotic action state distribution into manageable set of anchor states, granting policies enhanced generalization capabilities in data-scarce settings. In essence, efficient action representation shifts VLA learning from modeling noisy, high-dimensional control to mastering compact, semantic, and transferable motion primitives. 4.1.3 Other Pre-training Strategies Beyond data-efficient and action-representation techniques, innovative strategies further enhance VLA preseveral training efficiency by leveraging specialized paradigms. Multi-stage training emerges as prominent approach, adopted in works such as [12], [126], [134], [137], [138], [161]. This method decomposes the training pipeline into sequential phases, decoupling modality alignment, cognitive reasoning, and action fine-tuning. By isolating these components, models acquire sophisticated inference capabilities and robust real-world interaction skills with reduced computational overhead, enabling progressive knowledge distillation across stages. Reinforcement learning (RL) offers another avenue for efficient VLA pre-training, as demonstrated in [81], [83]. Specifically, [81] employs Conservative Q-Learning to maximize data utilization, mitigating sample inefficiency in exploratory settings. Meanwhile, [83] introduces pseudoenvironments to simulate interactions, accelerating convergence by bypassing costly real-world data collection and trial-and-error loops. Additionally, low-rank adaptation (LoRA [162]) techniques facilitate resource-aware pre-training in [64], [84]. These methods inject lightweight adapters into the VLM backbone, preserving its potent visual perception and semantic comprehension while enabling targeted updates for JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 15 TABLE 5: Representative Works on Efficient Post-training. Method/Model Year Key Innovation in Efficient Post-training OpenVLA [1] Atomic Skill Library [145] OpenVLA-OFT [10] MoManipVLA [146] OpenHelix [147] ControlVLA [148] CronusVLA [111] InstructVLA [149] RICL [150] ATE [151] ConRFT [152] RPD [153] RIPT-VLA [154] VLA-RL [155] CO-RFT [156] ARFM [157] SimpleVLA-RL [158] Dual-Actor Fine-Tuning [159] World-Env [160] 2024 2025 2025 2025 2025 2025 2025 2025 2025 2025 2025 2025 2025 2025 2025 2025 2025 2025 2025 (A) Supervised Fine-tuning Explores the pros and cons systematically of different parameter-efficient fine-tuning paradigms. Few-shot supervised fine-tuning adapts pre-trained VLAs to atomic skills with minimal data. Parallel decoding and action chunking accelerate LoRA fine-tuning of VLAs for high-frequency bimanual tasks. Adapts pre-trained VLAs to mobile manipulation efficiently using waypoint-guided supervised fine-tuning. Refines only the <ACT> token during fine-tuning to boost efficiency with minimal parameter updates. Injects object-centric representations into pretrained VLAs with ControlNet-style efficient adaptations. Finetunes single-frame pre-trained VLAs into multi-frame decoding using compact historical frame aggregation. Enhance pre-trained VLAs from understanding to manipulation with LoRA and MoE-adaptation. Inject in-context adaptability into pre-trained VLAs with distance-weighted action interpolation. Align and steer pre-trained VLAs for task adaptation using unified latent guidance through latent space fine-tuning. (B) RL-Based Method Unifies consistency-based RL objective for offline policy extraction from few demonstrations. Distills VLAs into compact RL experts using MSE-guided on-policy exploration for enhanced sample efficiency. Enpowers RL post-training with dynamic rollout sampling and leave-one-out advantage estimation. Applies trajectory-level RL to auto-regressive VLAs using dense rewards from pseudo-labeled task segments. Extends TD learning to action chunks in offline RL for VLA fine-tuning. Introduces scaling factor in flow matching loss to balance RL advantage preservation and gradient variance. Enhance scalable RL with VLA-specific trajectory sampling and parallelization under data scarcity. Deploy dual actors for multi-task actions and latent refinements guided by human language interventions. Leverage video-based world model as resettable virtual environment for safe RL exploration with minimal demos. action integration. This modular adaptation minimizes parameter updates, yielding efficient VLAs without compromising foundational multimodal prowess. 4.2 Efficient Post-Training Following the foundational pre-training phase, efficient post-training techniques refine VLAs to align with downstream tasks while minimizing computational demands. As illustrated in Fig. 7 right, these methods encompass supervised fine-tuning for targeted parameter updates and reinforcement learning-based approaches for policy optimization. Notably, efficient post-training is the crucial adaptation phase that ensures VLAs are not only specialized but also computationally viable for robust deployment in wide array of practical applications. Tab. 5 summarize representative works. 4.2.1 Supervised Fine-tuning Supervised fine-tuning refines pre-trained VLAs by further training on labeled datasets for specific downstream tasks, thereby enhancing task-specific capability while preserving multimodal generalization. In parameter-efficient fine-tuning, OpenVLA [1] pioneers systematic exploration of five strategiesfull finetuning, last-layer-only, frozen vision, sandwich fine-tuning, and LoRAdemonstrating that LoRA fine-tuning strikes an optimal performance-compute trade-off. Subsequent work, OpenVLA-OFT [10], advances this paradigm with an integrated approach combining parallel decoding, action chunking, continuous action representation, and simple L1regression-based learning objective, significantly boosting efficiency for edge deployment of VLAs. InstructVLA [149] further innovates by merging LoRA adapters with scaled MoE-adaptation heads, achieving robust parameter-efficient tuning. Beyond these, the Atomic Skill Library [145] constructs dynamic, reusable repository of atomic skills via data-driven three-wheeled approach, enabling low-datacost post-training adaptation and potent compositional generalization. MoManipVLA [146] introduces novel bi-level trajectory optimization framework, leveraging merely 50 real-world samples to seamlessly transfer pre-trained VLAs to mobile manipulation tasks. OpenHelix [147] augments MLLM inputs with learnable <ACT> token, freezing all MLLM parameters while training only the <ACT> token embedding for cost-effective task adaptation. ControlVLA [148] fuses pre-trained VLAs with object-centric representations via ControlNet-style architecture, employing zero-initialized projection layers to enable efficient finetuning with 10-20 demonstration samples while retaining prior knowledge. CronusVLA [111] enhances singleframe VLAs with multi-frame capabilities by freezing the backbones historical frame perception and appending lightweight cross-frame decoder through minimum computational overhead. RICL [150] integrates In-Context Learning (ICL [163]) into VLA post-training, mimicking RAG processes by concatenating retrieved (image, state, action) sequences with query sequences, training the model to predict actions from contextual cues for few-shot tuning. Lastly, ATE [151] employs reverse KL divergence for structured alignment in latent space, guided by energy-modeldefined gradients to steer the sampling of diffusion or flowmatching VLAs toward target distributions with remarkable efficiency. 4.2.2 RL-Based Method Although supervised fine-tuning excels in leveraging highquality task-specific data, its efficacy hinges precariously on data abundance and quality, rendering reinforcement learning (RL)-based post-training potent antidote to scarcity and variability in robotic datasets. These methods bifurcate into online paradigms, which harness real-time environmental interactions to iteratively refine policies, and offline JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 paradigms, which distill expertise from static trajectories without further data acquisition. Online RL fosters adaptive exploration, as exemplified by RIPT-VLA [154], which integrates sparse binary rewards with rejection-sampled PPO variant to elevate success rates from 4% (SFT) to 97% in mere 15 iterations using single demonstration. Also based on the PPO [164] algorithm, VLA-RL [155] reframes trajectories as multi-turn conversations, deploying PPO with VLM-derived dense rewards and curriculum optimizations to yield 4.5% success gains over OpenVLA (SFT) on 4 LIBERO task suites. SimpleVLARL [158] extends this efficiency via GRPO [165] and interactive sampling on OpenVLA-OFT, reducing data needs by boosting rates from 17.3% to 91.7% with one trajectory per task while enhancing sim-to-real transfer. Complementarily, RPD [153] distills teacher VLAs into compact policies using MSE-aligned PPO, surpassing originals in sparsereward ManiSkill3 [166] tasks with accelerated convergence. human-in-the-loop dual-actor framework [159] further refines diffusion-based VLAs with Talk-and-Tweak scheme through latent tweaks from language-mapped corrections, attaining 100% success in 101 minutes across tasks with 2 multi-robot efficiency. World-Env [160] innovates by simulating futures in video-based virtual environments with VLM-guided rewards, achieving 79.6% LIBERO success from five demonstrations post-training without realworld costs. In contrast, offline RL maximizes the utility of archived data. Both ConRFT [152] and CO-RFT [156] use CalQL [167], which effectively mitigates the value overestimation problem in offline RL by penalizing the Q-value of out-of-distribution actions and compensating for in-dataset actions. Specifically, ConRFTs [152] initial Q-learning with consistency objectives initializes stable policies from 2030 demos, enabling 96.3% real-world success post-online handover with 144% baseline gains, while CO-RFT [156] chunks actions for transformer-critic Q-prediction, lifting success by 57% and trimming cycles 22.3% on 3060 samples. Furthermore, ARFM [157] caps this lineage with adaptive scaling in flow-matching losses to curb variance, delivering 4.5% multi-task uplift and 11.4% perturbation robustness over π0 [2] baseline. Collectively, these RL pipelines empower VLAs with resilient, data-thrifty adaptation, transcending supervised limits toward autonomous prowess."
        },
        {
            "title": "5 EFFICIENT DATA COLLECTION",
            "content": "The performance of VLAs depends critically on the scale, quality, and diversity of demonstration datasets across embodiments and task variations. Unlike LLMs and VLMs, which benefit from Internet-scale training data, VLAs cannot directly leverage such resources. Their prevailing data collection paradigmhuman teleoperation and expert demonstrations in real-world settingsis inherently laborintensive, prohibitively costly, and severely limited in scalability. As illustrated in Fig. 8, Recent efforts to overcome these challenges have pursued several strategies, including human-in-the-loop data collection (section 5.1), simulation data collection(section 5.2), Internet-scale and crossdomain data utilization(section 5.3), self-exploration data 16 Fig. 8: Taxonomy of Efficient Data Collection Strategies in VLAs. This figure illustrates the primary approaches under section 5, encompassing human-in-the-loop, simulated, reusability-oriented, self-driven, and augmentative techniques for scalable acquisition of high-quality robotic datasets while minimizing resource overhead. collection(section 5.4), and data augmentation(section 5.5). We systematically review these strategies, analyzing their core principles and representative methods, with concise summary provided in Tab. 6. 5.1 Human-in-the-Loop Data Collection Traditional human-in-the-loop data collection proves costly, labor-intensive, time-consuming, and fundamentally inefficient. Firstly, this approach exhibits strong dependence on expert human operators, specialized hardware, manual annotations, and physical robot deployments in real-world settings with carefully designed scenarios, requiring substantial financial and resource investments. Moreover, the 1:1 ratio between human demonstration time and collected data, further degraded by environment setup, task resets, and human errors, results in efficiency far below expectations. These compounding limitations lead to severe data scarcity, impeding efforts to scale dataset sizes and thereby restricting the generalizability and robustness of VLAs. To acquire novel robotic data in faster, more costeffective, and scalable manner, the most direct approach is to optimize the role and efficiency of human involvement in the data collection loop. Along this approach, recent research has explored methods where humans are repositioned as supervisors or high-level instruction providers who collect data through more efficient interfaces or intervene only at critical junctures. CLIP-RT [75] collects robot demonstrations via natural language interfaces. Users conversationally interact with an LLM, which translates linguistic commands into low-level end-effector actions. The camera captures observations, the robot executes actions, and the system records complete trajectories. While CLIP-RT eliminates the need for expert knowledge, it still requires continuous human engagement throughout data collection. In contrast, GCENT [168] addresses the constraint by positioning the human operator JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 17 TABLE 6: Representative Works on Efficient Data Collection. Method/Model Year Key Innovation in Efficient Data Collection CLIP-RT [75] GCENT [168] GeRM [81] GraspVLA [138] cVLA [140] RoboTwin 2.0 [169] ReBot [170] R2R2R [171] RealMirror [172] SmolVLA [13] EgoVLA [132] RynnVLA [134] Egoscaler [173] Being-H0 [133] MimicDreamer [174] EMMA [175] Humanoid-VLA [137] Anypos [139] SimpleVLA-RL [158] DiffusionRLVLA [176] World-Env [160] VLA-RFT [177] LLaRA [178] InstructVLA [149] RoboChemist [9] ReconVLA [179] CLIP-RT [75] ERMV [180] (A) Human-in-the-loop Data Collection Enables non-expert data collection using language teleoperation and stochastic augmentation. Integrates human rewind-and-refine guidance for efficient, failure-centric data collection. (B) Simulation Data Collection Leverages pre-trained policies to autonomously collect mixed-quality demonstration data. Develops an efficient, parallelized pipeline for massive-scale synthetic grasping data generation. Leverages simulation with domain randomization to enable scalable VLA training without costly real-world data. Combines automated code synthesis with multimodal feedback loops for scalable expert data generation. Employs real-to-sim-to-real pipeline combining trajectory replay with background inpainting. Synthesizes large-scale data from smartphone scans and single demonstrations without physics simulation. Introduces simulated teleoperation system to acquire high-quality training data without physical robots. (C) Internet-Scale and Cross-Domain Data Utilization Standardizes heterogeneous community datasets for scalable VLA pretraining. Leverages large-scale egocentric human videos for pretraining VLA models to overcome robot data scarcity. Introduces automated ego-centric video curation pipeline transferring human manipulation skills to robot learning. Extracts trajectories from egocentric videos without auxiliary annotations or specialized hardware. Introduces physical instruction tuning paradigm that pretrains on human hand videos. Aligns human videos to robot domain via joint vision-viewpoint-action transformation. Enables text-controlled visual transfer of robot demonstrations. Language-motion alignment via compositional quantization and pseudo-annotation. (D) Self-Exploration Data Collection Automates task-agnostic action collection via RL-based workspace coverage. Employs online RL to self-generate diverse trajectories from minimal human demonstrations. Generates synthetic demonstrations via diffusion RL replacing costly human data. Combines small, pre-trained models in modular framework for consumer-grade GPU deployment. Employs world model simulator for efficient reinforcement fine-tuning with trajectory-level rewards. (E) Data Augmentation Reformats datasets into conversational instruction-response pairs with templates. Uses GPT-4o annotations to mitigate catastrophic forgetting. Fusing multi-modal prompts and failures enhances self-correction. Segments gaze regions for visual reconstruction pre-training. Stochastically drives robot to novel states beyond demonstrations. Propagates frame edits across multi-view timesteps consistently. 2024 2024 2025 2025 2025 2025 2025 2025 2025 2025 2025 2025 2025 2025 2025 2025 2025 2025 2025 2025 2025 2025 2025 2025 2025 2025 2025 as guardian who intervenes only when failed or nearfailure step is detected. Interventions are executed through an interactive rewind and correction mechanism, which allows operators to restore the robot to previous state and provide corrective demonstrations. And by selectively soliciting human corrections and iteratively refining the policy online, GCENT progressively reduces intervention frequency and increases task success, ultimately enabling one-operatormultiple-robots setup. 5.2 Simulation Data Collection While these human-in-the-loop optimization strategies substantially improve efficiency over traditional teleoperation methods, the data achievable through such approaches remains insufficient and monolithic. Alternatively, simulation environments allow data generation to be scaled across wide range of settings, object types, lighting conditions, and agent embodiments through automated, parallel processes, thus enabling the curation of large-scale datasets with greater diversity at fraction of the time and expense. GraspVLA [138] introduces SynGrasp-1B, billion-frame synthetic grasping dataset generated through photorealistic simulation in parallel. They employ modular expert policy to produce collision-free trajectories autonomously, validated in MuJoCo and rendered via Isaac Sim. GeRM [81] introduces QUARD-Auto, training dataset for quadruped manipulation, also generated through massively parallel simulation in Isaac Gym. pre-trained policy is adopted to eliminate the need for manual teleoperation. cVLA [140] presents models trained on datasets generated via the ManiSkill simulator. The data generation pipeline employs analytical grasp models and privileged pose information to produce verified action trajectories. RoboTwin 2.0 [169] generated bimanual manipulation data through an expertsimulated pipeline. The automated pipeline employs closed-loop architecture where code-generation agent synthesizes task programs from natural language instructions, executes them in simulation, and iteratively refines them based on dual feedback until achieving target success rates. To ensure task feasibility across heterogeneous dual-arm platforms, it also annotates objects with diverse candidate grasp poses and applies robot-specific motion planning. However, oversimplified simulation environments may fail to capture real-world complexity, necessitating bridging the sim-to-real gap. Its typically addressed through domain randomization of visual and physical parameters, photorealistic rendering with ray tracing, systematic augmentations encompassing lighting conditions, camera viewpoints, JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 18 background textures, and object appearances, as well as hybrid training that combines large-scale simulation data with minimal real demonstrations. however, remains human videos, more commonly, egocentric videos. In this pathway, the core obstacle lies in bridging the human-robot embodiment gap. Additional efforts have been made to minimize the sim-to-real gap: ReBot [170] proposes real-to-sim-to-real pipeline that grounds data generation in authentic robot trajectories rather than relying solely on simulator-based policies. The method replays identical real robot trajectories across diverse simulated scenarios with varying objects, then composites the acquired simulation robot movements onto task-agnostic real-world backgrounds obtained through inpainting techniques. Similarly, R2R2R [171] synthesizes large-scale, photorealistic robot demonstrations from real-world input: smartphone scan of objects and single human manipulation video. The pipeline extracts 3D assets and segments them into semantic parts, tracks 6-DoF object trajectories through 4D Differentiable Part Modeling. Differently, RealMirror [172] introduces teleoperationsimulation combined collection framework. The system uses motion-control pipeline in simulation to control robots in the real world, with multi-level filtering mechanisms to ensure physically plausible trajectories, and lightweight WebXR-based communication protocol that significantly reduces end-to-end latency compared to conventional frameworks. This design enables efficient real-world data acquisition through simulated interfaces. Either way, they collected large-scale, high-fidelity simulation datasets that exhibit minimal sim-to-real gap. 5.3 Internet-Scale and Cross-Domain Data Utilization Simulation data collection effectively addresses the scalability limitations in human teleoperation demonstrations. Despite the advancement, the approach needs to build datasets from scratch and is constrained by the sim-to-real gap. This has catalyzed shift in focus toward exploiting internetscale and other existing data sources. This emerging approach seeks to harness the vast, diverse, yet unstructured, unannotated data repositories available online, including egocentric human videos and community-contributed robot datasets. The central challenge lies in reconciling the inherent heterogeneity of these dataspanning embodiment disparities, viewpoint discrepancies, and action space misalignmentsand transforming them into formats amenable to VLA training. Towards this, researchers have developed several distinct yet complementary strategies. One strategy focuses on curating and standardizing the noisy, heterogeneous data that already exists within the robotics community. The SmolVLA [13] paper exemplifies this community-driven approach by aggregating numerous small-scale datasets from platforms like Hugging Face. To tackle the inherent inconsistencies, it employs VLM to automatically generate clean, consistent task descriptions from noisy original labels and manually maps diverse camera viewpoints into standardized format. This curation-heavy strategy demonstrates that even with dataset an order of magnitude smaller than state-of-the-art, high performance can be achieved by maximizing the quality and diversity of real-world, albeit messy, data. While curating community-contributed robot datasets partially enhances efficiency, the most abundant data source, EgoVLA [132] pioneers this egocentric paradigm: it introduces the foundational concept of treating humans as form of robot, establishing egocentric human videos as viable training modality for VLAs.The work constructs large-scale Ego-Centric Human Manipulation Dataset aggregating skill-rich sequences from multiple sources. Identically, EgoVLA [132] is pretrained on this heterogeneous human dataset.It reveals the potential of leveraging abundant, unstructured human egocentric videos to achieve superior generalization. Building upon EgoVLAs conceptual foundation, RynnVLA-001 [134] addresses the practical challenge of massive egocentric manipulation video acquisition by establishing an automated, multi-stage data curation pipeline. The pipeline leverages pose estimation to identify egocentric perspectives through the absence of facial landmarks and the presence of hand keypoints. Following the acquisition of web-scale egocentric videos, EgoScaler [173]narrows the human-robot embodiment gap by transforming raw, unstructured visual data into structured robotic action representations. The work introduces an automated pipeline that extracts 6-DoF object trajectories directly from egocentric videos without manual annotation, thereby converting passive visual observations into actionable manipulation sequences suitable for robot policy learning. Progressively, Being-H0 [133] elevates action representation from coarse object trajectories to fine-grained hand poses, addressing the precision and standardization requirements for dexterous manipulation tasks. In more advanced manner, MimicDreamer [174] confronts the human-robot embodiment gap by translating human demonstration videos into synthetic sequences that conform to robotic appearance and dynamics through video diffusion models, directly resolving visual and kinematic discrepancies. The framework canonicalizes egocentric videos through stabilization and inpainting, then employs constrained inverse kinematics to map human wrist trajectories into robot joint configurations, and critically, deploys video diffusion model to synthesize photorealistic robot demonstration videos that conform to robotic embodiment constraints. Consequently, the training data observed by VLA models becomes visually identical to what they encounter during real-world task execution, thereby resolving the embodiment gap between human demonstrations and robotic operation.DreamTransfer [175] also introduces diffusion Transformer framework for generating photorealistic, multi-view consistent robot manipulation videos. The method leverages pretrained diffusion models to jointly encode synchronized multi-view depths and text prompts, enabling text-controlled visual transfer of real or simulated demonstrations. However, Internet-scale human videos often lack language annotations and egocentric perspectives. To address this, HumanoidVLA [137] uses generalizable methodology that leverages third-person human-motion videos. By decomposing body poses into part-specific tokens and applying temporal and spatial perturbations with corresponding instructional prompts, the approach transforms raw third-person videos into structured training signals without JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 manual annotation. This framework expands the scope of usable human data beyond first-person videos and further alleviates data scarcity through scalable self-supervision. 5.4 Self-Exploration Data Collection While the aforementioned paradigms enhance data collection efficiency through diverse mechanisms(e.g., active collection and cross-domain transformation), they fundamentally remain constrained by passive dependence on generated trajectory data. To transcend this limitation, an emerging research paradigm shifts from passive data reception toward autonomous exploration, wherein agents actively generate valuable training data through environment interaction. This paradigm fundamentally reframes data collection from human-intensive bottleneck into an agentdriven, self-improving process, positioning self-exploration as pivotal enabler for scalable and efficient data collection. Before addressing task-specific learning, fundamental question emerges: Does the robot possess comprehensive awareness of its physical capability boundaries? Without systematic exploration of its kinematic reachable space, downstream task learning risks operating on incomplete behavioral priors. AnyPos [139] tackles this foundational challenge through ATARA, self-supervised framework that decouples data generation from task semantics by leveraging RL-driven policies to orchestrate efficient and uniform coverage of the robots end-effector workspace, autonomously synthesizing large-scale task-agnostic image, action datasets. This approach mitigates limitations in naive random explorationsparse coverage, motion redundancy, and frequent self-collisionsthereby establishing reusable, unbiased, and physically grounded kinematic foundation for all subsequent task learning, answering what the robot can do before prescribing what it should do. Task-agnostic exploration [139] equips VLAs with kinematic prior that characterizes their physical capabilities. However, executing specific user instructions demands task-relevant behavioral sequences with purposeful intent. Upon this, online reinforcement learning (RL) emerging as the predominant pathway that enables agents to learn while collecting through direct environment interaction. SimpleVLA-RL [158] pioneers the demonstration of selfdriven data collection through online RL with only minimal human demonstrations as seeds. The framework transforms the VLA itself into high-quality trajectory data generator through generate-evaluate-optimize cycle: generating diverse trajectories via interactive rollouts with stochastic action sampling, filtering them with binary success rewards, and retaining successful executions as training data while simultaneously optimizing the policy. Nevertheless, this direct RL fine-tuning approach remains constrained by the policy models representational capacityfor complex, multimodal behaviors, standard policies may converge to local optima or produce suboptimal averaging actions, limiting exploration breadth and trajectory quality. To address this challenge, Yang [176] proposes more sophisticated solution wherein the policy architecture is replaced with the highly expressive diffusion model. The diffusion models powerful generative capacity enables 19 superior capture of multi-modal distributions inherent in human demonstrations while generating smoother, more consistent, and higher-quality near-optimal trajectories during RL exploration. Crucially, this methodology produces synthetic dataset that surpasses the quality of original human demonstrations. While online RL demonstrates notable efficacy, it needs extensive interaction with either physical environments or high-fidelity physics simulators. Consequently, the next evolutionary trajectory naturally migrates exploration from costly physical domains to low-cost virtual worlds. WorldEnv [160] provides direct solution by leveraging existing high-fidelity simulators (e.g., LIBERO): starting from minimal expert demonstrations, it deploys VLA policies to explore within the simulator through controlled stochasticity, thereby economically augmenting diverse interaction data. However, this approach remains constrained by preconstructed simulators that may fail to generalize across novel environments. In such cases, VLA-RFT [177] advances this methodology to its extreme by learning data-driven world model directly from offline interaction datasets, eliminating reliance on high-fidelity simulators. This learned world model serves as controllable simulator that captures the diversity of real-world interactions, enabling the VLA policy to undergo reinforcement fine-tuning entirely within synthetic environment through massively parallel rollouts of predicted visual trajectories. The approach fundamentally transitions data collection from passive accumulation to active generation. 5.5 Data Augmentation Data augmentation can also be positioned as strategies of efficient data collection, which is achieved by maximizing the utility and diversity of existing data, transforming collected trajectories into richer, more varied training signals. prominent approach involves enriching linguistic and semantic annotations. LLaRA [178] pioneered the automated reformatting of existing behavior cloning datasets into conversational instruction-response pairs using templates, while also defining auxiliary tasks for self-supervised enhancement. Building significantly on this, InstructVLA [149] utilizes advanced LLMs (GPT-4o) to curate diverse, hierarchical annotationsincluding scene captions, QA pairs, and command rewritingfrom large manipulation datasets, specifically to mitigate the catastrophic forgetting of pretrained VLM capabilities during fine-tuning. This strategy of using models for annotation is also seen in RoboChemist [9], which employs LLMs to diversify language instructions and VLMs to generate automated visual prompts (e.g., labeling grasp points) to ensure safety compliance. Other methods generate novel training objectives from existing data; ReconVLA [179], for instance, fine-tunes Grounding DINO on robotic datasets to automatically segment interactionrelevant gaze regions, thereby constructing large-scale pre-training dataset dedicated to visual reconstruction. Augmentation can also target the trajectory, state, and temporal dimensions. CLIP-RT [75] introduced Stochastic Trajectory Augmentation (STA), which stochastically drives the robot into novel states beyond the expert demonstrations and applies simple heuristics for automatic labeling. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 RoboChemist [9] enhances robustness differently, by explicitly injecting teleoperated failure scenarios and retry attempts into the training data, improving the models capacity for self-correction. Finally, several methods achieve augmentation by directly manipulating the visual modality. Addressing existing 4D data, ERMV [180] proposes framework that applies targeted edits (e.g., inpainting) to an initial frame and propagates these changes consistently across all views and timesteps using Epipolar Motion-Aware Attention, generating new visual sequences paired with the original, unmodified actions."
        },
        {
            "title": "6 APPLICATIONS",
            "content": "The pursuit of efficiency in Vision-Language-Action (VLA) models is not an end in itself, but critical enabler for their deployment in the physical world. The techniques outlined in preceding chaptersfrom streamlined architectures to efficient training paradigms and data curation strategiesculminate in their practical utility across spectrum of real-world robotic applications. This chapter delineates some representative applications, demonstrating how efficient VLAs are transforming domains that demand realtime response, operational robustness, and computational frugality. 6.1 Intelligent Vehicles and Autonomous Driving Intelligent vehicles operate under one of the most stringent sets of constraints for any embodied AI system. They must process high-dimensional sensor data in real-time, understand complex traffic scenarios, and execute safe control commandsall within the severe computational and power budgets of mobile platform. Bulky models are fundamentally unsuitable for this domain. Efficient VLAs address this by enabling direct, end-to-end mapping from sensory inputs to driving actions with minimal latency. Compressed and optimized models can be deployed on automotive-grade hardware, facilitating nuanced driving behavior based on multimodal inputs, such as interpreting traffic officers gesture or responding to complex verbal navigational command. Recent research efforts [181], [182], [183], [184] have echoed this technical imperative. The computational leanness of these models is not merely convenience; it is prerequisite for safety and reliability, allowing for rapid inference that keeps pace with the dynamic driving environment. 6.2 Family Robotics and Smart Home The home environment presents unique set of challenges centered on privacy, persistent operation, and natural human-robot interaction. Cloud-dependent architectures raise significant privacy concerns and introduce latency that breaks the flow of interaction. Efficient VLAs are pivotal in shifting the intelligence to the edge, directly onto the robot. Lightweight models empower service robots to comprehend and execute open-ended commands like tidy up the living room entirely offline, ensuring user data never leaves the device. This on-board processing capability, coupled with low power consumption, enables robots to offer assistance over extended periods without frequent recharging. Furthermore, the reduced inference latency is crucial for sustaining engaging and safe conversational interactions, making the robot responsive and seamless part of the domestic fabric. 6.3 Industrial Manufacturing and Logistics Industrial settings demand precision, high throughput, and scalability. The vision of large-scale, collaborative robot fleets hinges on the cost-effectiveness and computational efficiency of the underlying AI models. Efficient VLAs are the cornerstone of this vision. By dramatically reducing the parameter counts and computational overhead, these models make it economically viable to deploy advanced intelligence across hundreds of manipulators and autonomous guided vehicles (AGVs). They enable real-time visual recognition for precise part selection and assembly, while their low latency ensures operational cycles meet demanding production line tempos. Beyond single-task execution, efficient VLAs facilitate quick redeployment through natural language instructions, allowing single robot to perform multiple functionsfrom pick and place component to inspect the final product for defectsthereby enhancing overall manufacturing flexibility and agility. For example, CIPHER [7] efficiently switches between tasks in the 3D printing industryfrom tuning extrusion parameters for layer precision to conducting visual defect scansenabling seamless, multi-role adaptation through natural language without hardware changes. 6.4 Medical Assistive Robotics In medical and assistive contexts, the imperatives of precision, data security, and personalized adaptation converge. Surgical and rehabilitation robots require exquisitely finegrained control, which in turn demands real-time sensorimotor processing. The sensitive nature of health information mandates that patient data be processed locally, precluding the use of cloud-based models. Efficient VLAs are uniquely suited to meet these dual challenges. Their optimized architecture allows for the low-latency, high-precision control loops essential for assisting in delicate procedures. By operating entirely on-premise, they guarantee the confidentiality and privacy of patient data. Moreover, the dataefficient nature of these models, often leveraging powerful pre-trained backbones, permits effective fine-tuning with limited patient-specific datasets. This enables new level of personalization, where assistive devices can quickly adapt to the unique physiology and needs of an individual user, paving the way for more accessible and effective personalized care."
        },
        {
            "title": "7 CHALLENGES AND FUTURE WORKS\nIn this chapter, we delineate the principal challenges imped-\ning the maturation of efficient VLAs and propose forward-\nlooking research trajectories. Anchored in the taxonomy\nelucidated earlier, encompassing efficient model design,\nefficient training and efficient data collection, we dissect\nthese facets across Model, Training, and Data dimensions.",
            "content": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 This structured exposition aims to catalyze advancements toward scalable embodied AI, mitigating computational exigencies while amplifying performance in robotic manipulation and beyond. 7.1 Challenges Notwithstanding strides in efficient VLAs, enduring impediments curtail their scalability and robustness. These arise from intrinsic trade-offs in multimodal integration, where vision, language, and action modalities demand harmonious yet parsimonious orchestration, often yielding suboptimal generalization in dynamic settings. 7.1.1 Model: Balancing Compactness and Expressivity Efficient VLA design confronts fundamental tension between architectural parsimony and representational richness, where aggressive compression sacrifices fidelity in capturing fine-grained spatiotemporal dynamics. Inference acceleration in efficient VLA design inevitably trades model capability for speed, undermining cross-embodiment robustness. Hierarchical or modular paradigms, while promising scalability, introduce routing overheads that undermine real-time deployability, particularly on resource-constrained hardware. Ultimately, the pursuit of sub-billion-parameter efficiency must contend with emergent degradation in longhorizon reasoning and adaptation to unseen tasks, which may hinder seamless transition from simulation to physical worlds. 7.1.2 Training: Scalability versus Stability Trade-offs Training efficient VLAs is beset by the dual imperatives of computational frugality and convergence reliability. Pretraining efficient VLAs, despite leveraging frozen visionlanguage backbones, still incurs substantial compute for action head alignment and risks brittle generalization under embodiment shifts in downstream tasks. Post-training adaptation, especially via reinforcement learning, grapples with high-variance gradients and reward sparsity, exacerbating instability in policy refinement. Action representation compression, though expedient, distorts continuous kinematics, impeding transfer across embodiments. These tradeoffs fundamentally hinder scalable, reproducible training pipelines, curtailing widespread adoption across heterogeneous robotic platforms. 7.1.3 Data: Quality, Diversity, and Accessibility Barriers Data remains the bedrock and bottleneck of VLA efficacy, plagued by scarcity of high-fidelity, task-diverse trajectories. Human-sourced collections suffer from prohibitive costs, while synthetic alternatives falter in physical realism, yielding persistent sim-to-real gaps. Augmentation strategies, though volume-enhancing, risk injecting distributional biases that erode generalization. Self-supervised or exploration-driven paradigms generate voluminous but noisy signals, necessitating costly curation. The absence of standardized, ethically sourced, cross-domain repositories stifles reproducible progress and equitable access, intensifying disparities in embodied AI development. 21 7.2 Future Works To surmount these obstacles, emergent directions should transcend incremental gains, embracing paradigm shifts redefine efficiency as holistic system-level optithat mization. We advocate integrative, interdisciplinary approachesspanning architecture, training theory, and data ecosystemsto forge VLAs that are not merely lightweight but fundamentally scalable, adaptive, and deployable across the spectrum of embodied intelligence. 7.2.1 Model: Toward Adaptive, Embodiment-Agnostic Architectures Future VLA designs must evolve toward intrinsic adaptability, dynamically modulating complexity in response to task and hardware contexts. Dynamic token pruning with context-aware routing could modulate computational paths on-the-fly, preserving critical spatiotemporal details while achieving sub-linear scaling across embodiments. Modality-agnostic backbones, coupled with token orchestration, promise unified efficiency across vision, language, and action streams. Hardware-software co-design, leveraging architectural optimization of computing platforms, may shatter current latency barriers, enabling edge-native VLAs that seamlessly span consumer devices to industrial manipulators. 7.2.2 Training: Scalable, Resilient Learning Paradigms Training regimes should pivot toward decentralized, continual, and theoretically grounded protocols. Federated paradigms, augmented with differential privacy, could harness distributed robotic agents for lifelong learning, amortizing costs while enriching data diversity. Physics-informed objectives, integrated into pre-training, may enforce kinematic consistency, bridging simulation and reality at the optimization level. Meta-learning and curriculum strategies could instill rapid adaptation, minimizing fine-tuning overhead. Ultimately, training must be reimagined as closedloop, self-improving processleveraging online interaction to refine models, rendering efficiency an emergent property of deployment. 7.2.3 Data: Self-sustaining Generative Ecosystems Data infrastructures demand transformation into generative, self-sustaining ecosystems. Diffusion-guided synthesis, conditioned on physical priors and linguistic intent, could produce infinite, verifiable trajectories from minimal seeds. Further reduction of the sim-to-real gap, e.g., through the embedding of the laws of physics, can improve the reliability of simulation data and reduce the dependence on highcost real data. Multi-agent, curiosity-driven exploration in shared virtual worlds may yield emergent task diversity, supplanting human teleoperation. Such ecosystems would not merely feed models but co-evolve with them, establishing virtuous cycle where data quality, model capability, and real-world impact recursively amplify."
        },
        {
            "title": "8 CONCLUSION",
            "content": "This survey systematically charts the emergent field of Efficient VLAs, providing the first unified taxonomy focusing JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 on the data-model-training loop to consolidate the fragmented efforts addressing their prohibitive computational and data bottlenecks. We demonstrate that current solutions converge upon three interdependent pillars, including Efficient Model Design, Efficient Training, and Efficient Data Collection. Our analysis establishes foundational reference that illuminates the critical interplay and persistent trade-offs, such as compactness versus expressivity, defining this frontier. The challenges and future directions delineated are direct consequence of this synthesis, charting necessary roadmap that moves beyond isolated optimizations toward adaptive, co-designed systems. This work thus serves to catalyze this transition, accelerating the vital shift from resource-bound prototypes to truly ubiquitous physical-world intelligence."
        },
        {
            "title": "REFERENCES",
            "content": "[4] [3] [2] [1] M. J. Kim, K. Pertsch, S. Karamcheti, T. Xiao, A. Balakrishna, S. Nair, R. Rafailov, E. Foster, G. Lam, P. Sanketi et al., Openvla: An open-source vision-language-action model, arXiv preprint arXiv:2406.09246, 2024. K. Black, N. Brown, D. Driess, A. Esmail, M. Equi, C. Finn, N. Fusai, L. Groom, K. Hausman, B. Ichter et al., π0: visionlanguage-action flow model for general robot control, arXiv preprint arXiv:2410.24164, 2024. B. Zitkovich, T. Yu, S. Xu, P. Xu, T. Xiao, F. Xia, J. Wu, P. Wohlhart, S. Welker, A. Wahid et al., Rt-2: Vision-language-action models transfer web knowledge to robotic control, in Conference on Robot Learning. PMLR, 2023, pp. 21652183. Q. Li, Y. Liang, Z. Wang, L. Luo, X. Chen, M. Liao, F. Wei, Y. Deng, S. Xu, Y. Zhang et al., Cogact: foundational visionlanguage-action model for synergizing cognition and action in robotic manipulation, arXiv preprint arXiv:2411.19650, 2024. J. Cao, Q. Zhang, P. Jia, X. Zhao, B. Lan, X. Zhang, X. Wei, S. Chen, Z. Li, Y. Wang et al., Fastdrivevla: Efficient end-to-end driving via plug-and-play reconstruction-based token pruning, arXiv preprint arXiv:2507.23318, 2025. X. Zhou, X. Han, F. Yang, Y. Ma, and A. C. Knoll, Opendrivevla: Towards end-to-end autonomous driving with large vision language action model, arXiv preprint arXiv:2503.23463, 2025. C. Margadji and S. W. Pattinson, Hybrid reasoning for perception, explanation, and autonomous action in manufacturing, arXiv preprint arXiv:2506.08462, 2025. S. Li, J. Wang, R. Dai, W. Ma, W. Y. Ng, Y. Hu, and Z. Li, Robonurse-vla: Robotic scrub nurse system based on visionlanguage-action model, arXiv preprint arXiv:2409.19590, 2024. Z. Zhang, C. Yue, H. Xu, M. Liao, X. Qi, H.-a. Gao, Z. Wang, and H. Zhao, Robochemist: Long-horizon and safetycompliant robotic chemical experimentation, arXiv preprint arXiv:2509.08820, 2025. [9] [5] [7] [6] [8] [10] M. J. Kim, C. Finn, and P. Liang, Fine-tuning vision-languageaction models: Optimizing speed and success, arXiv preprint arXiv:2502.19645, 2025. [11] R. Zhang, M. Dong, Y. Zhang, L. Heng, X. Chi, G. Dai, L. Du, Y. Du, and S. Zhang, Mole-vla: Dynamic layer-skipping vision language action model via mixture-of-layers for efficient robot manipulation, arXiv preprint arXiv:2503.20384, 2025. J. Liu, M. Liu, Z. Wang, P. An, X. Li, K. Zhou, S. Yang, R. Zhang, Y. Guo, and S. Zhang, Robomamba: Efficient vision-languageaction model for robotic reasoning and manipulation, Advances in Neural Information Processing Systems, vol. 37, pp. 40 08540 110, 2024. [12] [13] M. Shukor, D. Aubakirova, F. Capuano, P. Kooijmans, S. Palma, A. Zouitine, M. Aractingi, C. Pascal, M. Russi, A. Marafioti et al., Smolvla: vision-language-action model for affordable and efficient robotics, arXiv preprint arXiv:2506.01844, 2025. [14] K. Pertsch, K. Stachowicz, B. Ichter, D. Driess, S. Nair, Q. Vuong, O. Mees, C. Finn, and S. Levine, Fast: Efficient action tokenization for vision-language-action models, arXiv preprint arXiv:2501.09747, 2025. 22 [15] Y. Yang, Y. Wang, Z. Wen, L. Zhongwei, C. Zou, Z. Zhang, C. Wen, and L. Zhang, Efficientvla: Training-free acceleration and compression for vision-language-action models, arXiv preprint arXiv:2506.10100, 2025. [16] Y. Ma, Z. Song, Y. Zhuang, J. Hao, and I. King, survey on vision-language-action models for embodied ai, arXiv preprint arXiv:2405.14093, 2024. [17] R. Shao, W. Li, L. Zhang, R. Zhang, Z. Liu, R. Chen, and L. Nie, Large vlm-based vision-language-action models for robotic manipulation: survey, arXiv preprint arXiv:2508.13073, 2025. [18] T.-Y. Xiang, A.-Q. Jin, X.-H. Zhou, M.-J. Gui, X.-L. Xie, S.-Q. Liu, S.-Y. Wang, S.-B. Duan, F.-C. Xie, W.-K. Wang et al., Parallels between vla model post-training and human motor learning: Progress, challenges, and trends, arXiv preprint arXiv:2506.20966, 2025. [19] Y. Zhong, F. Bai, S. Cai, X. Huang, Z. Chen, X. Zhang, Y. Wang, S. Guo, T. Guan, K. N. Lui et al., survey on vision-languageaction models: An action tokenization perspective, arXiv preprint arXiv:2507.01925, 2025. [20] M. U. Din, W. Akram, L. S. Saoud, J. Rosell, and I. Hussain, Vision language action models in robotic manipulation: systematic review, arXiv preprint arXiv:2507.10672, 2025. [21] D. Zhang, J. Sun, C. Hu, X. Wu, Z. Yuan, R. Zhou, F. Shen, and Q. Zhou, Pure vision language action (vla) models: comprehensive survey, arXiv preprint arXiv:2509.19012, 2025. [22] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al., An image is worth 16x16 words: Transformers for image recognition at scale, in International Conference on Learning Representations, 2020. [23] X. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer, Sigmoid loss for language image pre-training, in Proceedings of the IEEE/CVF international conference on computer vision, 2023, pp. 11 97511 986. [24] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby et al., Dinov2: Learning robust visual features without supervision, arXiv preprint arXiv:2304.07193, 2023. [25] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., Learning transferable visual models from natural language supervision, in International conference on machine learning. PmLR, 2021, pp. 87488763. [26] H. Liu, C. Li, Q. Wu, and Y. J. Lee, Visual instruction tuning, information processing systems, vol. 36, pp. Advances in neural 34 89234 916, 2023. J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang et al., Qwen technical report, arXiv preprint arXiv:2309.16609, 2023. [27] [28] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann et al., Palm: Scaling language modeling with pathways, Journal of Machine Learning Research, vol. 24, no. 240, pp. 1113, 2023. [29] G. Team, T. Mesnard, C. Hardin, R. Dadashi, S. Bhupatiraju, S. Pathak, L. Sifre, M. Rivière, M. S. Kale, J. Love et al., Gemma: Open models based on gemini research and technology, arXiv preprint arXiv:2403.08295, 2024. [30] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., Llama 2: Open foundation and fine-tuned chat models, arXiv preprint arXiv:2307.09288, 2023. [31] A. Gu and T. Dao, Mamba: Linear-time sequence modeling with selective state spaces, in First Conference on Language Modeling, 2024. [32] Y. Wu, Z. Zhang, J. Chen, H. Tang, D. Li, Y. Fang, L. Zhu, E. Xie, H. Yin, L. Yi et al., Vila-u: unified foundation model integrating visual understanding and generation, arXiv preprint arXiv:2409.04429, 2024. [33] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez et al., Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality, See https://vicuna. lmsys. org (accessed 14 April 2023), vol. 2, no. 3, p. 6, 2023. J. Ho, A. Jain, and P. Abbeel, Denoising diffusion probabilistic models, Advances in neural information processing systems, vol. 33, pp. 68406851, 2020. J. Song, C. Meng, and S. Ermon, Denoising diffusion implicit models, arXiv preprint arXiv:2010.02502, 2020. [35] [34] JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 [36] W. Peebles and S. Xie, Scalable diffusion models with transformers, in Proceedings of the IEEE/CVF international conference on computer vision, 2023, pp. 41954205. [37] Y. Lipman, R. T. Chen, H. Ben-Hamu, M. Nickel, and M. Le, Flow matching for generative modeling, arXiv preprint arXiv:2210.02747, 2022. [38] A. ONeill, A. Rehman, A. Maddukuri, A. Gupta, A. Padalkar, A. Lee, A. Pooley, A. Gupta, A. Mandlekar, A. Jain et al., Open x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0, in 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024, pp. 6892 6903. [39] F. Ebert, Y. Yang, K. Schmeckpeper, B. Bucher, G. Georgakis, K. Daniilidis, C. Finn, and S. Levine, Bridge data: Boosting generalization of robotic skills with cross-domain datasets, arXiv preprint arXiv:2109.13396, 2021. [40] H. R. Walke, K. Black, T. Z. Zhao, Q. Vuong, C. Zheng, P. HansenEstruch, A. W. He, V. Myers, M. J. Kim, M. Du et al., Bridgedata v2: dataset for robot learning at scale, in Conference on Robot Learning. PMLR, 2023, pp. 17231736. [42] [41] A. Khazatsky, K. Pertsch, S. Nair, A. Balakrishna, S. Dasari, S. Karamcheti, S. Nasiriany, M. K. Srirama, L. Y. Chen, K. Ellis et al., Droid: large-scale in-the-wild robot manipulation dataset, arXiv preprint arXiv:2403.12945, 2024. S. James, Z. Ma, D. R. Arrojo, and A. J. Davison, Rlbench: The robot learning benchmark & learning environment, IEEE Robotics and Automation Letters, vol. 5, no. 2, pp. 30193026, 2020. S. Nasiriany, A. Maddukuri, L. Zhang, A. Parikh, A. Lo, A. Joshi, A. Mandlekar, and Y. Zhu, Robocasa: Large-scale simulation of everyday tasks for generalist robots, arXiv preprint arXiv:2406.02523, 2024. [43] [44] Y. Wang, Z. Xian, F. Chen, T.-H. Wang, Y. Wang, K. Fragkiadaki, Z. Erickson, D. Held, and C. Gan, Robogen: Towards unleashing infinite data for automated robot learning via generative simulation, arXiv preprint arXiv:2311.01455, 2023. [45] T. Yu, D. Quillen, Z. He, R. Julian, K. Hausman, C. Finn, and S. Levine, Meta-world: benchmark and evaluation for multitask and meta reinforcement learning, in Conference on robot learning. PMLR, 2020, pp. 10941100. [46] B. Liu, Y. Zhu, C. Gao, Y. Feng, Q. Liu, Y. Zhu, and P. Stone, Libero: Benchmarking knowledge transfer for lifelong robot learning, Advances in Neural Information Processing Systems, vol. 36, pp. 44 77644 791, 2023. [47] O. Mees, L. Hermann, E. Rosete-Beas, and W. Burgard, Calvin: benchmark for language-conditioned policy learning for longhorizon robot manipulation tasks, IEEE Robotics and Automation Letters, vol. 7, no. 3, pp. 73277334, 2022. [48] X. Li, K. Hsu, J. Gu, K. Pertsch, O. Mees, H. R. Walke, C. Fu, I. Lunawat, I. Sieh, S. Kirmani et al., Evaluating realworld robot manipulation policies in simulation, arXiv preprint arXiv:2405.05941, 2024. S. Zhang, Z. Xu, P. Liu, X. Yu, Y. Li, Q. Gao, Z. Fei, Z. Yin, Z. Wu, Y.-G. Jiang et al., Vlabench: large-scale benchmark for language-conditioned robotics manipulation with long-horizon reasoning tasks, arXiv preprint arXiv:2412.18194, 2024. [49] [50] L. X. Shi, M. R. Equi, L. Ke, K. Pertsch, Q. Vuong, J. Tanner, A. Walling, H. Wang, N. Fusai, A. Li-Bell et al., Hi robot: Openended instruction following with hierarchical vision-languageaction models, in Forty-second International Conference on Machine Learning, 2025. J. Bjorck, F. Castañeda, N. Cherniadev, X. Da, R. Ding, L. Fan, Y. Fang, D. Fox, F. Hu, S. Huang et al., Gr00t n1: An open foundation model for generalist humanoid robots, arXiv preprint arXiv:2503.14734, 2025. [51] [52] L. Beyer, A. Steiner, A. S. Pinto, A. Kolesnikov, X. Wang, D. Salz, M. Neumann, I. Alabdulmohsin, M. Tschannen, E. Bugliarello et al., Paligemma: versatile 3b vlm for transfer, arXiv preprint arXiv:2407.07726, 2024. S. Karamcheti, S. Nair, A. Balakrishna, P. Liang, T. Kollar, and D. Sadigh, Prismatic vlms: Investigating the design space of visually-conditioned language models, in Forty-first International Conference on Machine Learning, 2024. [53] [54] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q. Vuong, T. Yu et al., Palm-e: An embodied multimodal language model, in International Conference on Machine Learning. PMLR, 2023, pp. 84698488. [55] X. Chen, J. Djolonga, P. Padlewski, B. Mustafa, S. Changpinyo, J. Wu, C. R. Ruiz, S. Goodman, X. Wang, Y. Tay et al., Pali-x: On scaling up multilingual vision and language model, arXiv preprint arXiv:2305.18565, 2023. [56] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, Attention is all you need, Advances in neural information processing systems, vol. 30, 2017. [57] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, Swin transformer: Hierarchical vision transformer using shifted windows, in Proceedings of the IEEE/CVF international conference on computer vision, 2021, pp. 10 01210 022. I. Leal, K. Choromanski, D. Jain, A. Dubey, J. Varley, M. Ryoo, Y. Lu, F. Liu, V. Sindhwani, Q. Vuong et al., Sara-rt: Scaling up robotics transformers with self-adaptive robust attention, in 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024, pp. 69206927. [58] [60] [59] Y. Fan, S. Bai, X. Tong, P. Ding, Y. Zhu, H. Lu, F. Dai, W. Zhao, Y. Liu, S. Huang et al., Long-vla: Unleashing long-horizon capability of vision language action model for robot manipulation, in 9th Annual Conference on Robot Learning, 2025. J. Wen, M. Zhu, J. Liu, Z. Liu, Y. Yang, L. Zhang, S. Zhang, Y. Zhu, and Y. Xu, dvla: Diffusion vision-language-action model with multimodal chain-of-thought, arXiv preprint arXiv:2509.25681, 2025. J. Koo, T. Cho, H. Kang, E. Pyo, T. G. Oh, T. Kim, and A. J. Choi, Retovla: Reusing register tokens for spatial reasoning in visionlanguage-action models, arXiv preprint arXiv:2509.21243, 2025. [61] [63] [62] W. Xu and L. Zhuang, Kv-efficient vla: method of speed up vision language model with rnn-gated chunked kv cache, arXiv preprint arXiv:2509.21354, 2025. S. Wang, L. Wang, S. Zhou, J. Tian, J. Li, H. Sun, and W. Tang, Flowram: Grounding flow matching policy with region-aware mamba framework for robotic manipulation, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 12 17612 186. J. Wen, Y. Zhu, J. Li, M. Zhu, Z. Tang, K. Wu, Z. Xu, N. Liu, R. Cheng, C. Shen et al., Tinyvla: Towards fast, data-efficient vision-language-action models for robotic manipulation, IEEE Robotics and Automation Letters, 2025. [64] [65] W. Song, J. Chen, P. Ding, H. Zhao, W. Zhao, Z. Zhong, Z. Ge, J. Ma, and H. Li, Accelerating vision-language-action model integrated with action chunking via parallel decoding, arXiv preprint arXiv:2503.02310, 2025. J. Liu, H. Chen, P. An, Z. Liu, R. Zhang, C. Gu, X. Li, Z. Guo, S. Chen, M. Liu et al., Hybridvla: Collaborative diffusion and autoregression in unified vision-language-action model, arXiv preprint arXiv:2503.10631, 2025. [66] [67] Y. Su, N. Liu, D. Chen, Z. Zhao, K. Wu, M. Li, Z. Xu, Z. Che, and J. Tang, Freqpolicy: Efficient flow-based visuomotor policy via frequency consistency, arXiv preprint arXiv:2506.08822, 2025. [68] W. Song, J. Chen, P. Ding, Y. Huang, H. Zhao, D. Wang, and H. Li, Ceed-vla: Consistency vision-language-action model with earlyexit decoding, arXiv preprint arXiv:2506.13725, 2025. [69] X. Chi, K. Ge, J. Liu, S. Zhou, P. Jia, Z. He, Y. Liu, T. Li, L. Han, S. Han et al., Mind: Learning dual-system world model for real-time planning and implicit risk analysis, arXiv preprint arXiv:2506.18897, 2025. [70] Y. Wang, H. Zhu, M. Liu, J. Yang, H.-S. Fang, and T. He, Vqvla: Improving vision-language-action models via scaling vectorquantized action tokenizers, arXiv preprint arXiv:2507.01016, 2025. S. Wang, R. Yu, Z. Yuan, C. Yu, F. Gao, Y. Wang, and D. F. Wong, Spec-vla: speculative decoding for vision-language-action models with relaxed acceptance, arXiv preprint arXiv:2507.22424, 2025. [71] [72] W. Zheng, B. Li, B. Xu, E. Feng, J. Gu, and H. Chen, Leveraging os-level primitives for robotic action management, arXiv preprint arXiv:2508.10259, 2025. [73] D. Tarasov, A. Nikulin, I. Zisman, A. Klepach, L. Nikita, A. Polubarov, A. Derevyagin, and V. Kurenkov, Nina: Normalizing flows in action. training vla models with normalizing flows, in NeurIPS 2025 Workshop on Embodied World Models for Decision Making, 2025. [74] Z. Liang, Y. Li, T. Yang, C. Wu, S. Mao, L. Pei, X. Yang, J. Pang, Y. Mu, and P. Luo, Discrete diffusion vla: Bringing discrete diffusion to action decoding in vision-language-action policies, arXiv preprint arXiv:2508.20072, 2025. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 24 [75] G.-C. Kang, J. Kim, K. Shim, J. K. Lee, and B.-T. Zhang, Cliprt: Learning language-conditioned robotic policies from natural language supervision, arXiv preprint arXiv:2411.00508, 2024. [76] M. Samson, B. Muraccioli, and F. Kanehiro, Scalable, trainingfree visual language robotics: modular multi-model framework for consumer-grade gpus, in 2025 IEEE/SICE International Symposium on System Integration (SII). IEEE, 2025, pp. 193198. [77] C.-Y. Hung, Q. Sun, P. Hong, A. Zadeh, C. Li, U. Tan, N. Majumder, S. Poria et al., Nora: small open-sourced generalist vision language action model for embodied tasks, arXiv preprint arXiv:2504.19854, 2025. [78] Y. Li, Y. Meng, Z. Sun, K. Ji, C. Tang, J. Fan, X. Ma, S. Xia, Z. Wang, and W. Zhu, Sp-vla: joint model scheduling and token pruning approach for vla model acceleration, arXiv preprint arXiv:2506.12723, 2025. [79] P. Budzianowski, W. Maa, M. Freed, J. Mo, W. Hsiao, A. Xie, T. Młoduchowski, V. Tipnis, and B. Bolte, Edgevla: Efficient vision-language-action models, arXiv preprint arXiv:2507.14049, 2025. S. Belkhale and D. Sadigh, Minivla: better vla with smaller footprint, 2024. [Online]. Available: https://github. com/Stanford-ILIAD/openvla-mini [80] [81] W. Song, H. Zhao, P. Ding, C. Cui, S. Lyu, Y. Fan, and D. Wang, Germ: generalist robotic model with mixture-of-experts for quadruped robot, in 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2024, pp. 11 879 11 886. [82] C. Miao, T. Chang, M. Wu, H. Xu, C. Li, M. Li, and X. Wang, Fedvla: Federated vision-language-action learning with dual gating mixture-of-experts for robotic manipulation, arXiv preprint arXiv:2508.02190, 2025. [83] Y. Bai, Z. Wang, Y. Liu, W. Chen, Z. Chen, M. Dai, Y. Zheng, L. Liu, G. Li, and L. Lin, Learning to see and act: Task-aware view planning for robotic manipulation, arXiv preprint arXiv:2508.05186, 2025. J. Zhang, Y. Guo, X. Chen, Y.-J. Wang, Y. Hu, C. Shi, and J. Chen, Hirt: Enhancing robotic control with hierarchical robot transformers, in Conference on Robot Learning. PMLR, 2025, pp. 933946. [84] [85] Q. Bu, H. Li, L. Chen, J. Cai, J. Zeng, H. Cui, M. Yao, and Y. Qiao, Towards synergistic, generalized, and efficient dual-system for robotic manipulation, arXiv preprint arXiv:2410.08001, 2024. [86] B. Han, J. Kim, and J. Jang, dual process vla: Efficient robotic manipulation leveraging vlm, arXiv preprint arXiv:2410.15549, 2024. [87] Y. Li, Y. Deng, J. Zhang, J. Jang, M. Memmel, C. R. Garrett, F. Ramos, D. Fox, A. Li, A. Gupta et al., Hamster: Hierarchical action models for open-world robot manipulation, in The Thirteenth International Conference on Learning Representations, 2025. [88] H. Chen, J. Liu, C. Gu, Z. Liu, R. Zhang, X. Li, X. He, Y. Guo, C.- W. Fu, S. Zhang et al., Fast-in-slow: dual-system foundation model unifying fast manipulation within slow reasoning, arXiv preprint arXiv:2506.01953, 2025. [89] C. Chi, Z. Xu, S. Feng, E. Cousineau, Y. Du, B. Burchfiel, R. Tedrake, and S. Song, Diffusion policy: Visuomotor policy learning via action diffusion, The International Journal of Robotics Research, p. 02783649241273668, 2023. [90] A. Van Den Oord, O. Vinyals et al., Neural discrete representation learning, Advances in neural information processing systems, vol. 30, 2017. J. Wen, M. Zhu, Y. Zhu, Z. Tang, J. Li, Z. Zhou, C. Li, X. Liu, Y. Peng, C. Shen et al., Diffusion-vla: Generalizable and interpretable robot foundation model via self-generated reasoning, arXiv preprint arXiv:2412.03293, 2024. [91] [92] P. Wang, S. Bai, S. Tan, S. Wang, Z. Fan, J. Bai, K. Chen, X. Liu, J. Wang, W. Ge et al., Qwen2-vl: Enhancing vision-language models perception of the world at any resolution, arXiv preprint arXiv:2409.12191, 2024. [93] Z. Chen, J. Wu, W. Wang, W. Su, G. Chen, S. Xing, M. Zhong, Q. Zhang, X. Zhu, L. Lu et al., Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2024, pp. 24 18524 198. [94] T. Lüddecke and A. Ecker, Image segmentation using text and image prompts, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 70867096. [95] M. I. Abdin, S. Ade Jacobs, A. A. Awan, J. Aneja, A. Awadallah, H. Hassan Awadalla, N. Bach, A. Bahree, A. Bakhtiari, H. Behl, A. Benhaim, M. Bilenko, J. Bjorck, S. Bubeck, M. Cai, C. C. T. Mendes, W. Chen, V. Chaudhary, P. Chopra, A. D. Giorno, G. de Rosa, M. Dixon, R. Eldan, D. Iter, A. Goswami, S. Gunasekar, E. Haider, J. Hao, R. J. Hewett, J. Huynh, M. Javaheripi, X. Jin, P. Kauffmann, N. Karampatziakis, D. Kim, M. Khademi, L. Kurilenko, J. R. Lee, Y. T. Lee, Y. Li, C. Liang, W. Liu, X. E. Lin, Z. Lin, P. Madan, A. Mitra, H. Modi, A. Nguyen, B. Norick, B. Patra, D. Perez-Becker, T. Portet, R. Pryzant, H. Qin, M. Radmilac, C. Rosset, S. Roy, O. Saarikivi, A. Saied, A. Salim, M. Santacroce, S. Shah, N. Shang, H. Sharma, X. Song, O. Ruwase, X. Wang, R. Ward, G. Wang, P. Witte, M. Wyatt, C. Xu, J. Xu, W. Xu, S. Yadav, F. Yang, Z. Yang, D. Yu, C. Zhang, C. Zhang, J. Zhang, L. L. Zhang, Y. Zhang, Y. Zhang, and X. Zhou, Phi-3 technical report: highly capable language model locally on your phone, Microsoft, Tech. Rep. MSR-TR-2024-12, August 2024. [96] W. Wang, H. Bao, S. Huang, L. Dong, and F. Wei, Minilmv2: Multi-head self-attention relation distillation for compressing pretrained transformers, arXiv preprint arXiv:2012.15828, 2020. S. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang et al., Qwen2. 5-vl technical report, arXiv preprint arXiv:2502.13923, 2025. [97] [98] P. C. Wason and J. S. B. Evans, Dual processes in reasoning? Cognition, vol. 3, no. 2, pp. 141154, 1974. [99] D. Kahneman, Thinking, fast and slow. macmillan, 2011. [100] Z. Duan, Y. Zhang, S. Geng, G. Liu, J. Boedecker, and C. X. Lu, Fast ecot: Efficient embodied chain-of-thought via thoughts reuse, arXiv preprint arXiv:2506.07639, 2025. [101] Y. Yue, Y. Wang, B. Kang, Y. Han, S. Wang, S. Song, J. Feng, and G. Huang, Deer-vla: Dynamic inference of multimodal large language models for efficient robot execution, Advances in Neural Information Processing Systems, vol. 37, pp. 56 61956 643, 2024. [102] Y. Chen and X. Li, Rlrc: Reinforcement learning-based recovery for compressed vision-language-action models, arXiv preprint arXiv:2506.17639, 2025. [103] Y. Wu, H. Wang, Z. Chen, J. Pang, and D. Xu, On-device diffusion transformer policy for efficient robot manipulation, arXiv preprint arXiv:2508.00697, 2025. [104] M. Reuss, H. Zhou, M. Rühle, Ö. E. Ya gmurlu, F. Otto, and R. Lioutikov, Flower: Democratizing generalist robot policies with efficient vision-language-action flow policies, arXiv preprint arXiv:2509.04996, 2025. [105] S. Park, H. Kim, W. Jeon, Y. Oh, and J. Choi, Quantization-aware imitation-learning for resourceefficient robotic control, arXiv preprint arXiv:2412.01034, 2024. J. Yang, B. Jeon, [106] S. Park, H. Kim, S. Kim, W. Jeon, J. Yang, B. Jeon, Y. Oh, and J. Choi, Saliency-aware quantized imitation learning for efficient robotic control, arXiv preprint arXiv:2505.15304, 2025. [107] H. Wang, C. Xiong, R. Wang, and X. Chen, Bitvla: 1-bit vision-language-action models for robotics manipulation, arXiv preprint arXiv:2506.07530, 2025. [108] H. Fang, Y. Liu, Y. Du, L. Du, and H. Yang, Sqap-vla: synergistic quantization-aware pruning framework for high-performance vision-language-action models, arXiv preprint arXiv:2509.09090, 2025. [109] S. Xu, Y. Wang, C. Xia, D. Zhu, T. Huang, and C. Xu, Vlacache: Towards efficient vision-language-action model via adaptive token caching in robotic manipulation, arXiv preprint arXiv:2502.02175, 2025. [110] X. Tan, Y. Yang, P. Ye, J. Zheng, B. Bai, X. Wang, J. Hao, and T. Chen, Think twice, act once: Token-aware compression and action reuse for efficient inference in vision-language-action models, arXiv preprint arXiv:2505.21200, 2025. [111] H. Li, S. Yang, Y. Chen, Y. Tian, X. Yang, X. Chen, H. Wang, T. Wang, F. Zhao, D. Lin et al., Cronusvla: Transferring latent motion across time for multi-frame prediction in manipulation, arXiv preprint arXiv:2506.19816, 2025. [112] J. Lin, A. Taherin, A. Akbari, A. Akbari, L. Lu, G. Chen, T. Padir, X. Yang, W. Chen, Y. Li et al., Vote: Vision-language-action optimization with trajectory ensemble voting, arXiv preprint arXiv:2507.05116, 2025. [113] W. Li, R. Zhang, R. Shao, J. He, and L. Nie, Cogvla: Cognitionaligned vision-language-action model via instruction-driven routing & sparsification, arXiv preprint arXiv:2508.21046, 2025. [114] H. Wang, J. Xu, J. Pan, Y. Zhou, and G. Dai, Specprune-vla: AcJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 25 celerating vision-language-action models via action-aware selfspeculative pruning, arXiv preprint arXiv:2509.05614, 2025. [115] T. Jiang, X. Jiang, Y. Ma, X. Wen, B. Li, K. Zhan, P. Jia, Y. Liu, S. Sun, and X. Lang, The better you learn, the smarter you prune: Towards efficient vision-language-action models via differentiable token pruning, arXiv preprint arXiv:2509.12594, 2025. [116] X. Pei, Y. Chen, S. Xu, Y. Wang, Y. Shi, and C. Xu, Action-aware dynamic pruning for efficient vision-language-action manipulation, arXiv preprint arXiv:2509.22093, 2025. [117] R. Bendikas, D. Dijkman, M. Peschl, S. Haresh, and P. Mazzaglia, Focusing on what matters: Object-agent-centric tokenization for vision language action models, in 9th Annual Conference on Robot Learning, 2025. [118] B. Xiao, H. Wu, W. Xu, X. Dai, H. Hu, Y. Lu, M. Zeng, C. Liu, and L. Yuan, Florence-2: Advancing unified representation for variety of vision tasks, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 48184829. [119] A. Marafioti, O. Zohar, M. Farré, M. Noyan, E. Bakouch, P. Cuenca, C. Zakka, L. B. Allal, A. Lozhkov, N. Tazi et al., Smolvlm: Redefining small and efficient multimodal models, arXiv preprint arXiv:2504.05299, 2025. [120] G. Fang, H. Yin, S. Muralidharan, G. Heinrich, J. Pool, J. Kautz, P. Molchanov, and X. Wang, Maskllm: Learnable semistructured sparsity for large language models, Advances in Neural Information Processing Systems, vol. 37, pp. 77367758, 2024. [121] E. Jang, S. Gu, and B. Poole, Categorical reparametrization with gumble-softmax, in International Conference on Learning Representations (ICLR 2017). OpenReview. net, 2017. [122] E. Perez, F. Strub, H. De Vries, V. Dumoulin, and A. Courville, Film: Visual reasoning with general conditioning layer, in Proceedings of the AAAI conference on artificial intelligence, vol. 32, no. 1, 2018. [123] P. Gage, new algorithm for data compression, Users Journal, vol. 12, no. 2, pp. 2338, 1994. [124] L. Chen, H. Zhao, T. Liu, S. Bai, J. Lin, C. Zhou, and B. Chang, An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models, in European Conference on Computer Vision. Springer, 2024, pp. 1935. [125] T. Dao, D. Fu, S. Ermon, A. Rudra, and C. Ré, Flashattention: Fast and memory-efficient exact attention with io-awareness, Advances in neural information processing systems, vol. 35, pp. 16 34416 359, 2022. [126] S. Fan, Q. Yang, Y. Liu, K. Wu, Z. Che, Q. Liu, and M. Wan, Diffusion trajectory-guided policy for long-horizon robot manipulation, arXiv preprint arXiv:2502.10040, 2025. [127] S. Ye, J. Jang, B. Jeon, S. J. Joo, J. Yang, B. Peng, A. Mandlekar, R. Tan, Y.-W. Chao, B. Y. Lin et al., Latent action pretraining from videos, in The Thirteenth International Conference on Learning Representations, 2025. [128] Q. Bu, Y. Yang, J. Cai, S. Gao, G. Ren, M. Yao, P. Luo, and H. Li, Learning to act anywhere with task-centric latent actions, arXiv preprint arXiv:2502.14420, 2025. [129] B. Tharwat, Y. Nasser, A. Abouzeid, and I. Reid, Latent action pretraining through world modeling, arXiv preprint arXiv:2509.18428, 2025. [130] S. Haldar, Z. Peng, and L. Pinto, Baku: An efficient transformer for multi-task policy learning, Advances in Neural Information Processing Systems, vol. 37, pp. 141 208141 239, 2024. [131] D. Hafner, J. Pasukonis, J. Ba, and T. Lillicrap, Mastering diverse domains through world models, arXiv preprint arXiv:2301.04104, 2023. [132] R. Yang, Q. Yu, Y. Wu, R. Yan, B. Li, A.-C. Cheng, X. Zou, Y. Fang, X. Cheng, R.-Z. Qiu et al., Egovla: Learning vision-languageaction models from egocentric human videos, arXiv preprint arXiv:2507.12440, 2025. [133] H. Luo, Y. Feng, W. Zhang, S. Zheng, Y. Wang, H. Yuan, J. Liu, C. Xu, Q. Jin, and Z. Lu, Being-h0: vision-languageaction pretraining from large-scale human videos, arXiv preprint arXiv:2507.15597, 2025. [134] Y. Jiang, S. Huang, S. Xue, Y. Zhao, J. Cen, S. Leng, K. Li, J. Guo, K. Wang, M. Chen et al., Rynnvla-001: Using human demonstrations to improve robot manipulation, arXiv preprint arXiv:2509.15212, 2025. [136] S. Ye, J. Jang, B. Jeon, S. Joo, J. Yang, B. Peng, A. Mandlekar, R. Tan, Y.-W. Chao, B. Y. Lin et al., Latent action pretraining from videos, arXiv preprint arXiv:2410.11758, 2024. [137] P. Ding, J. Ma, X. Tong, B. Zou, X. Luo, Y. Fan, T. Wang, H. Lu, P. Mo, J. Liu et al., Humanoid-vla: Towards universal humanoid control with visual integration, arXiv preprint arXiv:2502.14795, 2025. [138] S. Deng, M. Yan, S. Wei, H. Ma, Y. Yang, J. Chen, Z. Zhang, T. Yang, X. Zhang, W. Zhang et al., Graspvla: grasping foundation model pre-trained on billion-scale synthetic action data, arXiv preprint arXiv:2505.03233, 2025. [139] H. Tan, Y. Feng, X. Mao, S. Huang, G. Liu, Z. Hao, H. Su, and J. Zhu, Anypos: Automated task-agnostic actions for bimanual manipulation, arXiv preprint arXiv:2507.12768, 2025. [140] M. Argus, J. Bratulic, H. Masnavi, M. Velikanov, N. Heppert, A. Valada, and T. Brox, cvla: Towards efficient camera-space vlas, arXiv preprint arXiv:2507.02190, 2025. [141] Y. Wang, P. Ding, L. Li, C. Cui, Z. Ge, X. Tong, W. Song, H. Zhao, W. Zhao, P. Hou et al., Vla-adapter: An effective paradigm for tiny-scale vision-language-action model, arXiv preprint arXiv:2509.09372, 2025. [142] Y. Dai, A. Keyser, and D. P. Losey, Prepare before you act: Learning from humans to rearrange initial states, arXiv preprint arXiv:2509.18043, 2025. [143] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, Learning internal representations by error propagation, Tech. Rep., 1985. [144] D. P. Kingma and M. Welling, Auto-encoding variational bayes, arXiv preprint arXiv:1312.6114, 2013. [145] D. Li, B. Peng, C. Li, N. Qiao, Q. Zheng, L. Sun, Y. Qin, B. Li, Y. Luan, B. Wu et al., An atomic skill library construction method for data-efficient embodied manipulation, arXiv preprint arXiv:2501.15068, 2025. [146] Z. Wu, Y. Zhou, X. Xu, Z. Wang, and H. Yan, Momanipvla: Transferring vision-language-action models for general mobile manipulation, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 17141723. [147] C. Cui, P. Ding, W. Song, S. Bai, X. Tong, Z. Ge, R. Suo, W. Zhou, Y. Liu, B. Jia et al., Openhelix: short survey, empirical analysis, and open-source dual-system vla model for robotic manipulation, arXiv preprint arXiv:2505.03912, 2025. [148] P. Li, Y. Wu, Z. Xi, W. Li, Y. Huang, Z. Zhang, Y. Chen, J. Wang, S.-C. Zhu, T. Liu et al., Controlvla: Few-shot object-centric adaptation for pre-trained vision-language-action models, arXiv preprint arXiv:2506.16211, 2025. [149] S. Yang, H. Li, Y. Chen, B. Wang, Y. Tian, T. Wang, H. Wang, F. Zhao, Y. Liao, and J. Pang, Instructvla: Vision-language-action instruction tuning from understanding to manipulation, arXiv preprint arXiv:2507.17520, 2025. [150] K. Sridhar, S. Dutta, D. Jayaraman, and I. Lee, Ricl: Adding incontext adaptability to pre-trained vision-language-action models, in 9th Annual Conference on Robot Learning, 2025. [151] Y. Zhang, C. Wang, O. Lu, Y. Zhao, Y. Ge, Z. Sun, X. Li, C. Zhang, C. Bai, and X. Li, Align-then-steer: Adapting the vision-language action models through unified latent guidance, arXiv preprint arXiv:2509.02055, 2025. [152] Y. Chen, S. Tian, S. Liu, Y. Zhou, H. Li, and D. Zhao, Conrft: reinforced fine-tuning method for vla models via consistency policy, arXiv preprint arXiv:2502.05450, 2025. [153] T. Jülg, W. Burgard, and F. Walter, Refined policy distillation: From vla generalists to rl experts, arXiv preprint arXiv:2503.05833, 2025. [154] S. Tan, K. Dou, Y. Zhao, and P. Kraehenbuehl, Interactive posttraining for vision-language-action models, in Workshop on Foundation Models Meet Embodied Agents at CVPR 2025, 2025. [155] G. Lu, W. Guo, C. Zhang, Y. Zhou, H. Jiang, Z. Gao, Y. Tang, and Z. Wang, Vla-rl: Towards masterful and general robotic manipulation with scalable reinforcement learning, arXiv preprint arXiv:2505.18719, 2025. [156] D. Huang, Z. Fang, T. Zhang, Y. Li, L. Zhao, and C. Xia, Co-rft: Efficient fine-tuning of vision-language-action models through chunked offline reinforcement learning, arXiv preprint arXiv:2508.02219, 2025. [135] Y. Wang, X. Li, W. Wang, J. Zhang, Y. Li, Y. Chen, X. Wang, and Z. Zhang, Unified vision-language-action model, arXiv preprint arXiv:2506.19850, 2025. [157] H. Zhang, S. Zhang, J. Jin, Q. Zeng, Y. Qiao, H. Lu, and D. Wang, Balancing signal and variance: Adaptive offline rl post-training for vla flow models, arXiv preprint arXiv:2509.04063, 2025. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 The Thirteenth International Conference on Learning Representations, 2025. [179] W. Song, Z. Zhou, H. Zhao, J. Chen, P. Ding, H. Yan, Y. Huang, F. Tang, D. Wang, and H. Li, Reconvla: Reconstructive vision-language-action model as effective robot perceiver, arXiv preprint arXiv:2508.10333, 2025. [180] C. Nie, G. Wang, Z. Lie, and H. Wang, Ermv: Editing 4d robotic multi-view images to enhance embodied agents, arXiv preprint arXiv:2507.17462, 2025. [181] Y. Luo, F. Li, S. Xu, Z. Lai, L. Yang, Q. Chen, Z. Luo, Z. Xie, S. Jiang, J. Liu et al., Adathinkdrive: Adaptive thinking via reinforcement learning for autonomous driving, arXiv preprint arXiv:2509.13769, 2025. [182] A. Jiang, Y. Gao, Y. Wang, Z. Sun, S. Wang, Y. Heng, H. Sun, S. Tang, L. Zhu, J. Chai et al., Irl-vla: Training an visionlanguage-action policy via reward world model, arXiv preprint arXiv:2508.06571, 2025. [183] Z. Zhou, T. Cai, S. Z. Zhao, Y. Zhang, Z. Huang, B. Zhou, and J. Ma, Autovla: vision-language-action model for end-to-end autonomous driving with adaptive reasoning and reinforcement fine-tuning, arXiv preprint arXiv:2506.13757, 2025. [184] A. Jiang, Y. Gao, Z. Sun, Y. Wang, J. Wang, J. Chai, Q. Cao, Y. Heng, H. Jiang, Y. Dong et al., Diffvla: Vision-language guided diffusion planning for autonomous driving, arXiv preprint arXiv:2505.19381, 2025. [158] H. Li, Y. Zuo, J. Yu, Y. Zhang, Z. Yang, K. Zhang, X. Zhu, Y. Zhang, T. Chen, G. Cui et al., Simplevla-rl: Scaling vla training via reinforcement learning, arXiv preprint arXiv:2509.09674, 2025. [159] P. Jin, Q. Wang, G. Sun, Z. Cai, P. He, and Y. You, Dual-actor fine-tuning of vla models: talk-and-tweak human-in-the-loop approach, arXiv preprint arXiv:2509.13774, 2025. [160] J. Xiao, Y. Yang, X. Chang, R. Chen, F. Xiong, M. Xu, W.-S. Zheng, and Q. Zhang, World-env: Leveraging world model as virtual environment for vla post-training, arXiv preprint arXiv:2509.24948, 2025. [161] J. Wen, Y. Zhu, J. Li, Z. Tang, C. Shen, and F. Feng, Dexvla: Vision-language model with plug-in diffusion expert for general robot control, arXiv preprint arXiv:2502.05855, 2025. [162] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen et al., Lora: Low-rank adaptation of large language models. ICLR, vol. 1, no. 2, p. 3, 2022. [163] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., Language models are few-shot learners, Advances in neural information processing systems, vol. 33, pp. 18771901, 2020. [164] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, Proximal policy optimization algorithms, arXiv preprint arXiv:1707.06347, 2017. [165] Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. Li, Y. Wu et al., Deepseekmath: Pushing the limits of mathematical reasoning in open language models, arXiv preprint arXiv:2402.03300, 2024. [166] S. Tao, F. Xiang, A. Shukla, Y. Qin, X. Hinrichsen, X. Yuan, C. Bao, X. Lin, Y. Liu, T.-k. Chan et al., Maniskill3: Gpu parallelized robotics simulation and rendering for generalizable embodied ai, arXiv preprint arXiv:2410.00425, 2024. [167] M. Nakamoto, S. Zhai, A. Singh, M. Sobol Mark, Y. Ma, C. Finn, A. Kumar, and S. Levine, Cal-ql: Calibrated offline rl pretraining for efficient online fine-tuning, Advances in Neural Information Processing Systems, vol. 36, pp. 62 24462 269, 2023. [168] W. Wang, J. Song, C. Liu, J. Ma, S. Feng, J. Wang, Y. Jiang, K. Chen, S. Zhan, Y. Wang et al., Genie centurion: Accelerating scalable real-world robot training with human rewind-and-refine guidance, arXiv preprint arXiv:2505.18793, 2025. [169] T. Chen, Z. Chen, B. Chen, Z. Cai, Y. Liu, Z. Li, Q. Liang, X. Lin, Y. Ge, Z. Gu et al., Robotwin 2.0: scalable data generator and benchmark with strong domain randomization for robust bimanual robotic manipulation, arXiv preprint arXiv:2506.18088, 2025. [170] Y. Fang, Y. Yang, X. Zhu, K. Zheng, G. Bertasius, D. Szafir, and M. Ding, Rebot: Scaling robot learning with real-to-sim-to-real robotic video synthesis, arXiv preprint arXiv:2503.14526, 2025. [171] J. Yu, L. Fu, H. Huang, K. El-Refai, R. A. Ambrus, R. Cheng, M. Z. Irshad, and K. Goldberg, Real2render2real: Scaling robot data without dynamics simulation or robot hardware, arXiv preprint arXiv:2505.09601, 2025. [172] C. Tai, Z. Zheng, H. Long, H. Wu, H. Xiang, Z. Long, J. Xiong, R. Shi, S. Zhang, G. Qiu et al., Realmirror: comprehensive, open-source vision-language-action platform for embodied ai, arXiv preprint arXiv:2509.14687, 2025. [173] T. Yoshida, S. Kurita, T. Nishimura, and S. Mori, Developing vision-language-action model from egocentric videos, arXiv preprint arXiv:2509.21986, 2025. [174] H. Li, I. Zhang, R. Ouyang, X. Wang, Z. Zhu, Z. Yang, Z. Zhang, B. Wang, C. Ni, W. Qin et al., Mimicdreamer: Aligning human and robot demonstrations for scalable vla training, arXiv preprint arXiv:2509.22199, 2025. [175] Z. Dong, X. Wang, Z. Zhu, Y. Wang, Y. Wang, Y. Zhou, B. Wang, C. Ni, R. Ouyang, W. Qin et al., Emma: Generalizing real-world robot manipulation via generative visual transfer, arXiv preprint arXiv:2509.22407, 2025. [176] R. Yang, H. Wei, R. Zhang, Z. Feng, X. Chen, T. Li, C. Zhang, L. Zhao, J. Bian, X. Su et al., Beyond human demonstrations: Diffusion-based reinforcement learning to generate data for vla training, arXiv preprint arXiv:2509.19752, 2025. [177] H. Li, P. Ding, R. Suo, Y. Wang, Z. Ge, D. Zang, K. Yu, M. Sun, H. Zhang, D. Wang et al., Vla-rft: Vision-language-action reinforcement fine-tuning with verified rewards in world simulators, arXiv preprint arXiv:2510.00406, 2025. [178] X. Li, C. Mata, J. Park, K. Kahatapitiya, Y. S. Jang, J. Shang, K. Ranasinghe, R. D. Burgert, M. Cai, Y. J. Lee et al., Llara: Supercharging robot learning data for vision-language policy, in"
        }
    ],
    "affiliations": [
        "Department of Information Engineering and Computer Science, University of Trento, Italy",
        "School of Computer Science and Engineering, University of Electronic Science and Technology of China, China",
        "School of Computer Science and Technology, Tongji University, China",
        "School of Computing and Artificial Intelligence, Southwest Jiaotong University, China"
    ]
}