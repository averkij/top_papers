{
    "paper_title": "Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction",
    "authors": [
        "Jing He",
        "Haodong Li",
        "Wei Yin",
        "Yixun Liang",
        "Leheng Li",
        "Kaiqiang Zhou",
        "Hongbo Zhang",
        "Bingbing Liu",
        "Ying-Cong Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Leveraging the visual priors of pre-trained text-to-image diffusion models offers a promising solution to enhance zero-shot generalization in dense prediction tasks. However, existing methods often uncritically use the original diffusion formulation, which may not be optimal due to the fundamental differences between dense prediction and image generation. In this paper, we provide a systemic analysis of the diffusion formulation for the dense prediction, focusing on both quality and efficiency. And we find that the original parameterization type for image generation, which learns to predict noise, is harmful for dense prediction; the multi-step noising/denoising diffusion process is also unnecessary and challenging to optimize. Based on these insights, we introduce Lotus, a diffusion-based visual foundation model with a simple yet effective adaptation protocol for dense prediction. Specifically, Lotus is trained to directly predict annotations instead of noise, thereby avoiding harmful variance. We also reformulate the diffusion process into a single-step procedure, simplifying optimization and significantly boosting inference speed. Additionally, we introduce a novel tuning strategy called detail preserver, which achieves more accurate and fine-grained predictions. Without scaling up the training data or model capacity, Lotus achieves SoTA performance in zero-shot depth and normal estimation across various datasets. It also enhances efficiency, being significantly faster than most existing diffusion-based methods. Lotus' superior quality and efficiency also enable a wide range of practical applications, such as joint estimation, single/multi-view 3D reconstruction, etc. Project page: https://lotus3d.github.io/."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 8 2 ] . [ 4 4 2 1 8 1 . 9 0 4 2 : r Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction LOTUS: DIFFUSION-BASED VISUAL FOUNDATION MODEL FOR HIGH-QUALITY DENSE PREDICTION Jing He1 Haodong Li1 Wei Yin2 Yixun Liang1 Leheng Li1 Kaiqiang Zhou3 Hongbo Zhang3 Bingbing Liu3 Yingcong Chen1,4 (cid:0) 1HKUST(GZ) 2University of Adelaide 3Noahs Ark Lab 4HKUST {jhe812, hli736}@connect.hkust-gz.edu.cn; yingcongchen@ust.hk Figure 1: We present Lotus, diffusion-based visual foundation model for dense geometry prediction. With minimal training data, Lotus achieves SoTA performance in two key geometry perception tasks, i.e., zero-shot depth and normal estimation. Avg. Rank indicates the average ranking across all metrics, where lower values are better. Bar length represents the amount of training data used. Both authors contributed equally (order randomized). (cid:0) Corresponding author. 1 Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction"
        },
        {
            "title": "ABSTRACT",
            "content": "Leveraging the visual priors of pre-trained text-to-image diffusion models offers promising solution to enhance zero-shot generalization in dense prediction tasks. However, existing methods often uncritically use the original diffusion formulation, which may not be optimal due to the fundamental differences between dense prediction and image generation. In this paper, we provide systemic analysis of the diffusion formulation for the dense prediction, focusing on both quality and efficiency. And we find that the original parameterization type for image generation, which learns to predict noise, is harmful for dense prediction; the multistep noising/denoising diffusion process is also unnecessary and challenging to optimize. Based on these insights, we introduce Lotus, diffusion-based visual foundation model with simple yet effective adaptation protocol for dense prediction. Specifically, Lotus is trained to directly predict annotations instead of noise, thereby avoiding harmful variance. We also reformulate the diffusion process into single-step procedure, simplifying optimization and significantly boosting inference speed. Additionally, we introduce novel tuning strategy called detail preserver, which achieves more accurate and fine-grained predictions. Without scaling up the training data or model capacity, Lotus achieves SoTA performance in zero-shot depth and normal estimation across various datasets. It also enhances efficiency, being significantly faster than most existing diffusion-based methods. Lotus superior quality and efficiency also enable wide range of practical applications, such as joint estimation, single/multi-view 3D reconstruction, etc. Project page: lotus3d.github.io."
        },
        {
            "title": "INTRODUCTION",
            "content": "Dense prediction is fundamental task in computer vision, benefiting wide range of applications, such as 3D/4D reconstruction (Huang et al., 2024; Long et al., 2024; Wang et al., 2024; Lei et al., 2024), tracking (Xiao et al., 2024; Song et al., 2024), and autonomous driving (Yurtsever et al., 2020; Hu et al., 2023). Estimating pixel-level geometric attributes from single image requires comprehensive scene understanding. Although deep learning has advanced dense prediction, progress is limited by the quality, diversity, and scale of training data, leading to poor zero-shot generalization. Instead of merely scaling data and model size, recent works (Lee et al., 2024; Ke et al., 2024; Fu et al., 2024; Xu et al., 2024) leverage diffusion priors for zero-shot dense prediction. These studies demonstrate that text-to-image diffusion models like Stable Diffusion (Rombach et al., 2022), pretrained on billions of images, possess powerful and comprehensive visual priors to elevate dense prediction performance. However, most of these methods directly inherit the pre-trained diffusion models for dense prediction tasks, without exploring more suitable diffusion formulations. This oversight often leads to challenging issues. For example, Marigold (Ke et al., 2024) directly finetunes Stable Diffusion for image-conditioned depth generation. While it significantly improves depth estimation, its performance is still constrained by overlooking the fundamental differences between dense prediction and image generation. Especially, its efficiency is also severely limited by standard iterative denoising processes and ensemble inferences. Motivated by these concerns, we systematically analyze the diffusion formulation, trying to find better formulation to fit the pre-trained diffusion model into dense prediction. Our analysis yields several important findings: ① The widely used parameterization, i.e., noise prediction, for diffusionbased image generation is ill-suited for dense prediction. It results in large prediction errors due to harmful prediction variance at initial denoising steps, which are subsequently propagated and magnified throughout the entire denoising process (Sec. 4.1). ② Multi-step diffusion formulation is computation-intensive and is prone to sub-optimal with limited data and resources. These factors significantly hinder the adaptation of diffusion priors to dense prediction tasks, leading to decreased accuracy and efficiency (Sec. 4.2). ③ Though remarkable performance achieved, we observed that the model usually outputs vague predictions in highly-detailed areas  (Fig. 8)  . This vagueness is attributed to catastrophic forgetting: the pre-trained diffusion models gradually lose their ability to generate detailed regions during fine-tuning (Sec. 4.3). 2 Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction Figure 2: Adaptation protocol of Lotus. After the pre-trained VAE encoder encodes the image and annotation to the latent space: ① the denoiser U-Net model fθ is fine-tuned using x0-prediction; ② we employ single-step diffusion formulation at time-step = for better convergence; ③ we propose novel detail preserver, to switch the model either to reconstruct the image or generate the dense prediction via switcher s, ensuring more fine-grained prediction. The noise zy in bracket is used for our generative Lotus-G and is omitted for the discriminative Lotus-D. Following our analysis, we propose Lotus, diffusion-based visual foundation model for dense prediction, featuring simple yet effective fine-tuning protocol (see Fig. 2). First, Lotus is trained to directly predict annotations, thereby avoiding the harmful variance associated with standard noise prediction. Next, we introduce one-step formulation, i.e., one step between pure noise and clean output, to facilitate model convergence and achieve better optimization performance with limited high-quality data. It also considerably boosts both training and inference efficiency. Moreover, we implement novel detail preserver through task switcher, allowing the model either to generate annotations or reconstruct the input images. It can better preserve the fine-grained details in input image during dense annotation generation, achieving higher performance without compromising efficiency, requiring additional parameters, or being affected by surface textures. Figure 3: Inference time comparison in depth estimation between Lotus and SoTA methods. Lotus is hundreds of times faster than Marigold and slightly faster than DepthAnything V2 at high resolutions. DepthAnything V2s inference time at 2048 2048 is not plotted because it requires > 80GB graphic memory. To validate Lotus, we conduct extensive experiments on two primary geometric dense prediction tasks: zero-shot monocular depth and normal estimation. The results demonstrate that Lotus achieves SoTA performance on these tasks across wide range of evaluation datasets. Compared to traditional discriminative methods, Lotus delivers remarkable results with only 59K training samples by effectively leveraging the powerful diffusion priors. Among generative approaches, Lotus also outperforms previous methods in both accuracy and efficiency, being significantly faster than methods like Marigold (Ke et al., 2024)  (Fig. 3)  . Beyond these improvements, Lotus seamlessly supports various applications, such as joint estimation, single/multi-view 3D reconstruction, etc. In conclusion, our key contributions are as follows: We systematically analyze the diffusion formulation and find their parameterization type, designed for image generation, is unsuitable for dense prediction and the computationintensive multi-step diffusion process is also unnecessary and challenging to optimize. We propose novel detail preserver that ensures more accurate dense predictions especially in detail-rich areas, without compromising efficiency, introducing additional network parameters, or being affected by surface textures. Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction Based on our insights, we introduce Lotus, diffusion-based visual foundation model for dense prediction with simple yet effective fine-tuning protocol. Lotus achieves SoTA performance on both zero-shot monocular depth and surface normal estimation. It also enables wide range of applications."
        },
        {
            "title": "2.1 TEXT-TO-IMAGE GENERATIVE MODELS",
            "content": "In the field of text-to-image generation, the evolution of methodologies has transitioned from generative adversarial networks (GANs) (Goodfellow et al., 2014; Zhang et al., 2017; 2018; 2021; He et al., 2022; Karras et al., 2019; 2020; 2021; Zhang et al., 2017; 2018; Xu et al., 2018; Zhang et al., 2021) to advanced diffusion models (Ho et al., 2020; Ramesh et al., 2022; Saharia et al., 2022; Ramesh et al., 2021; Nichol et al., 2021; Chen et al., 2023; Rombach et al., 2022; Ramesh et al., 2021). series of diffusion-based methods such as GLIDE (Nichol et al., 2021), DALLE2 (Ramesh et al., 2022), and Imagen (Saharia et al., 2022) have been introduced, offering enhanced image quality and textual coherence. The Stable Diffusion (SD) (Rombach et al., 2022), trained on large-scale LAION-5B dataset (Schuhmann et al., 2022), further enhances the generative quality, becoming the community standard. In our paper, we aim to leverage the comprehensive and encyclopedic visual priors of SD to facilitate zero-shot generalization for dense prediction tasks. 2.2 GENERATIVE MODELS FOR DENSE PERCEPTION Currently, notable trend involves adopting pre-trained generative models, particularly diffusion models, into dense prediction tasks. Marigold (Ke et al., 2024) and GeoWizard (Fu et al., 2024) directly apply the standard diffusion formulation and the pre-trained parameters, without addressing the inherent differences between image generation and dense prediction, leading to constrained performance. Their efficiency is also severely limited by standard iterative denoising processes and ensemble inferences. In this paper, we propose novel diffusion formulation tailored to the characteristics of dense prediction. Aiming to fully leveraging the pre-trained diffusions powerful visual priors, Lotus enables more accurate and efficient predictions, finally achieving SoTA performance. More recent works, GenPercept (Xu et al., 2024) and StableNormal (Ye et al., 2024), also adopted single-step diffusion. However, GenPercept (Xu et al., 2024) first removes noise input for deterministic characteristic based on DMP (Lee et al., 2024), and then adopts one-step strategy to avoid surface texture interference. It lacks systematic analysis of the diffusion formulation, only treats the U-Net as deterministic backbone and still falls short in performance. In contrast, Lotus systematically analyzes the standard stochastic diffusion formulation for dense prediction and proposes innovations such as the detail preserver to improve accuracy especially in detailed area, finally delivering much better results (Tab. 1). Additionally, Lotus is stochastic model. In contrast to GenPercepts deterministic nature, Lotus enables uncertainty predictions. StableNormal (Ye et al., 2024) predicts normal maps through two-stage process. While the first stage produces coarse normal maps with single-step diffusion, the second stage performs refinement still with iterative diffusion which is computation-intensive. In comparison, Lotus not only achieves fine-grained predictions thanks to our novel detail preserver without extra stages or parameters, but also delivers much superior results (Tab. 2) thanks to our designed diffusion formulation that better fits the pre-trained diffusion for dense prediction. 2.3 MONOCULAR DEPTH AND NORMAL PREDICTION Monocular depth and normal prediction are two crucial dense prediction tasks. Solving them typically demands comprehensive scene understanding capability. Starting from (Eigen et al., 2014), early CNN-based methods for depth prediction, such as (Fu et al., 2018; Lee et al., 2019; Yuan et al., 2022), focus only on specific domains. Subsequently, in pursuit of generalizable depth estimator, many methods expand model capacity and train on larger and more diverse datasets, such as DiverseDepth (Yin et al., 2021a) and MiDaS (Ranftl et al., 2020). DPT (Ranftl et al., 2021) and Omnidata (Eftekhar et al., 2021) are further proposed based on vision transformer (Ranftl et al., 2021), significantly enhancing performance. LeRes (Yin et al., 2021b) and HDN (Zhang et al., 4 Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction 2022) further introduce novel training strategies and multi-scale depth normalization to improve predictions in detailed areas. More recently, the DepthAnything series (Yang et al., 2024a;b) and Metric3D series (Yin et al., 2023; Hu et al., 2024) collect and leverage millions of training data to develop more powerful estimators. Normal prediction follows the same trend. Starting with the early CNN-based methods like OASIS (Chen et al., 2020), EESNU (Bae & Davison, 2021) and Omnidata series (Eftekhar et al., 2021; Kar et al., 2022) expand the model capacity and scale up the training data. Recently, DSINE (Bae & Davison, 2024) achieves SoTA performance by rethinking inductive biases for surface normal estimation. In our paper, we focus on leveraging pre-trained diffusion priors to enhance zero-shot dense predictions, rather than expanding model capacity or relying on large training data, which avoids the need for intensive resources and computation."
        },
        {
            "title": "3 PRELIMINARIES",
            "content": "Diffusion Formulation for Dense Prediction. Following (Ke et al., 2024) and (Fu et al., 2024), we also formulate dense prediction as an image-conditioned annotation generation task based on Stable Diffusion (Rombach et al., 2022), which performs the diffusion process in low-dimensional latent space for computational efficiency. First, there is pair of auto-encoders {E(), D()} trained to map between RGB space and latent space, i.e., E(x) = zx, D(zx) x. The auto-encoder also maps between dense annotations and latent space effectively, i.e., E(y) = zy, D(zy) (Ke et al., 2024; Fu et al., 2024; Xu et al., 2024; Ye et al., 2024). Following (Ho et al., 2020), Stable Diffusion establishes pair of forward nosing and reversal denoising processes in latent space. In forward process, Gaussian noise is gradually added at levels [1, ] into sample zy to obtain the noisy sample zy : (1) where ϵ (0, I), αt := (cid:81)t s=1(1 βs), and {β1, β2, . . . , βT } is the noise schedule with steps. At time-step , the sample zy is degraded to pure Gaussian noise. In the reversal process, neural network fθ, usually U-Net model (Ronneberger et al., 2015), is trained to iteratively remove noise from zy to predict the clean sample zy. The network is trained by sampling random [1, ] and minimizing the loss function Lt. 1 αtϵ, αtzy + zy = Parameterization Types. To enable gradient computation for network training, there are two basic parameterizations of the loss function Lt. ① ϵ-prediction (Ho et al., 2020): the model fθ learns to predict the added noise ϵ; ② x0-prediction (Ho et al., 2020): the model fθ learns to directly predict the clean sample zy. The loss functions for these parameterizations are formulated as: ϵ-prediction: Lϵ x0-prediction: Lz = ϵ ϵ = zy θ (zy θ(zy , zx, t)2, , zx, t)2. (2) where θ is the denoiser model to be learnt, {ϵ, z}. ϵ-prediction is commonly chosen as the standard for parameterizing the denoising model, as it empirically achieves high-quality image generation with fine details and realism. Denoising Process. DDIM (Song et al., 2020) is key technique for multi-step diffusion models to achieve fast sampling, which implements an implicit probabilistic model that can significantly reduce the number of denoising steps while maintaining output quality. Formally, the denoising process from zy τ to zy τ 1 is: τ 1 = (cid:112)ατ 1ˆzy zy τ is the predicted clean sample at the denoising step τ , direction(zy (3) where ˆzy τ ) represents the direction pointing to zy τ and στ can be set to 0 if deterministic denoising is needed. And τ {τ1, τ2, . . . , τS}, an increasing sub-sequence of the time-step set [1, ], is used for fast sampling. During inference, DDIM iteratively denoises the sample from τS to τ1 to obtain the clean one. τ ) + στ ϵτ , τ + direction(zy"
        },
        {
            "title": "4 METHODOLOGY",
            "content": "We start our analysis by directly adapting the original diffusion formulation with minimal modifications as illustrated in Fig. 4. We call this starting point as Direct Adaptation1. Direct Adaptation 1Details of Direct Adaptation will be provided in Sec. of the supplementary materials. 5 Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction Figure 4: Adaptation protocol of Direct Adaptation. Starting with pre-trained Stable Diffusion model, image and annotation are encoded using the pre-trained VAE. The noisy annotation zy is obtained by adding noise at level [1, ]. The U-Net input layer is coupled to accommodate the concatenated inputs and then fine-tuned using the standard diffusion objective, ϵ-prediction, under the original multi-step formulation. Figure 5: Comparisons among different parameterizations using various seeds. All models are trained on Hypersim (Roberts et al., 2021) and tested on the input image for depth estimation. The standard DDIM sampler is used with 50 denoising steps. Four steps are selected for clear illustration. From left (larger τ ) to right (smaller τ ) is the iterative denoising process. is optimized using the standard diffusion objective as formulated in Eq. 2 (first row) and inferred by standard multi-step DDIM sampler. As shown in Tab. 3, Direct Adaptation fails to achieve satisfactory performance. In following sections, we will systematically analyze the key factors that affect adaptation performance step by step: parameterization types (Sec. 4.1); number of time-steps (Sec. 4.2); and the novel detail preserver (Sec. 4.3). 4.1 PARAMETERIZATION TYPES The type of parameterization is vital configuration, it not only determines the loss function discussed in Sec. 3, but also influences the inference process (Eq. 3). During inference, the predicted clean sample ˆzy τ , key component in Eq. 3, is calculated according to different parameterizations 2. ϵ-prediction: ˆzy τ = (zy τ x0-prediction: ˆzy τ = τ , zx, τ ). 1 ατ θ(zy 1 ατ ϵ θ (zy τ , zx, τ )), (4) In the community, ϵ-prediction is chosen as the standard for image generation. However, it is not effective for dense prediction task. In the following, we will discuss the impact of different parameterization types in denoising inference process for dense prediction task. Insights from the literature (Benny & Wolf, 2022; Salimans & Ho, 2022) reveal that ϵ-prediction introduces larger pixel variance compared to x0-prediction, especially at the initial denoising steps (large τ ). This variance mainly originates from the noise input. Specifically, for ϵ-prediction in +. Thus, the prediction variance Eq. 4, at initial denoising step, τ , the value 1 ατ 2The latest parameterization, v-prediction, combines ϵ-prediction and x0-prediction, producing results that are intermediate between the two. Please see Sec. of the supplementary materials for more details. 6 Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction θ (zy τ , zx, τ ) will be amplified significantly, resulting in large variance of predicted ˆzy from ϵ τ . In contrast, there is no coefficient for x0-prediction to re-scale the model output, achieving more stable predictions of ˆzy τ is used in Eq. 3, where its ατ 1 are same across the two parameterizations, and other terms are of the same order coefficient of magnitude. Therefore, the ˆzy τ predicted by ϵ-prediction, which has larger variance, exerts more significant influence on denoising process. Since the process is iterative, this influence is continually preserved and maybe amplified. τ at initial denoising steps. Subsequently, the predicted ˆzy We take the depth estimation as an example. During the inference process, we compute the predicted depth map ˆzy τ at each denoising step τ . As illustrated in Fig. 5, the depth maps predicted by ϵ-prediction significantly vary under different seeds while those predicted by x0-prediction are more consistent. Although the large variance enhances diversity for image generation, it lead to unstable predictions in dense prediction tasks, potentially resulting in significant errors. For example in Fig. 5, the dark gray cabinet (highlighted in red circles) maybe wrongly considered as an opened door with significantly larger depth. While the predicted depth map looks more and more plausible, the error gradually propagates to the final prediction (τ = 1) along the denoising process, indicating the persistent influence of the large variance. We further quantitatively measure the predicted depth maps by the absolute mean relative error (AbsRel) on NYUv2 dataset (Silberman et al., 2012). As shown in Fig. 6, ϵ-prediction exhibits higher error with much larger variance compared to x0-prediction at the initial denoising steps (τ ), and the prediction error propagates with higher slope. In contrast, x0-prediction, directly predicting ˆzy τ without any coefficients to amplify the prediction variance, yields more stable and correct dense predictions than ϵ-prediction. In conclusion, to mitigate the errors from large variance that adversely affect the performance of dense prediction, we replace the standard ϵ-prediction with the more tailored x0-prediction. Figure 6: Quantitative evaluation of the predicted depth maps ˆzy τ along the denoising process. The experimental settings are same as Fig. 5. Six steps are selected for illustration. The banded regions around each line indicate the variance, wider areas representing larger variance. 4.2 NUMBER OF TIME-STEPS Although x0-prediction can improve the prediction quality, the multi-step diffusion formulation still leads to the propagation of predicted errors during the denoising process (Fig. 5, 6). Furthermore, utilizing multiple time-steps enhances the models capacity, typically requiring large-scale training data to optimize and is beneficial for complex tasks such as image generation. However, for simpler tasks like dense prediction, where large-scale, high-quality training data is also scarce, employing multiple time-steps can make the model difficult to optimize. Additionally, training/inferring multi-step diffusion model is slow and computation-intensive, hindering its practical application. Therefore, to address these challenges, we propose fine-tuning the pre-trained diffusion model with fewer training time steps. Specifically, the original set of training time-steps is defined as [1, ] = {1, 2, 3, . . . , }, where denotes the total number of original training time-steps. We fine-tune the pre-trained diffusion model using sub-sequence derived from this set. We define the length of this sub-sequence as , where and is divisible by . This sub-sequence is obtained by evenly sampling the original set at intervals, defined as: {ti = = 1, 2, . . . , }, (5) where = /T is the sampling interval. During inference, the DDIM denoises the sample from noise to annotation using the same sub-sequence if 50, otherwise we use 50 denoising steps. As illustrated in Fig. 7, we conduct experiments by varying the number of time-steps under x0-prediction. The results clearly show that the performance gradually improves as the number of time-steps is reduced, no matter the training data scales, culminating in the best result when reduced to only single step. We further consider more strict scenarios with more limited training Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction Figure 8: Depth maps w/ and w/o the detail preserver and reconstruction outputs. Fine-tuning the diffusion model for dense prediction tasks can potentially degrade its ability to generate highly detailed images, resulting in blurred predictions in regions with rich detail. To preserve these finegrained details, we introduce detail preserver that incorporates an additional reconstruction task, enhancing the models capacity to produce more accurate dense annotations. data to assess its impact on model optimization. As depicted in Fig. 7, these experiments reveal that the multi-step formulation is more sensitive to increases in training data scales compared with single-step. Notably, the single-step formulation consistently yields lower prediction errors and demonstrates greater stability. Although it is conceivable that multi-step and single-step formulations might achieve comparable performance with unlimited high-quality data, its expensive and sometimes impractical in dense prediction. Decreasing the number of denoising steps can reduce the optimization space of the diffusion model, leading to more effective and efficient adaption, as suggested by the above phenomenon. Therefore, for better adaptation performance under limited resource, we reduce the number of training time-steps of diffusion formulation to only one, and fixing the only timestep to . Additionally, the single-step formulation is much more computationally efficient. It also naturally prevents the harmful error propagation as discussed in Sec. 4.1, further enhancing the diffusions adaptation performance in dense prediction. 4.3 DETAIL PRESERVER Figure 7: Comparisons among various training time-steps and data scales evaluated on NYUv2 in depth estimation. All models are fine-tuned on Hypersim using x0-prediction. During inference, if > 50, the DDIM sampler is used with 50 denoising steps; otherwise, the number of denoising steps is equal to . The results demonstrate improved performance with decreased training time-steps. The single-step diffusion formulation (T = 1) exhibits best performance across different data volumes. Despite the effectiveness of the above designs, the model still struggles with processing detailed areas (Fig. 8, w/o Preserver). The original diffusion model excels at generating detailed images. However, when adapted to predict dense annotations, it can lose such detailed generation ability, due to unexpected catastrophic forgetting (Zhai et al., 2023; Du et al., 2024). This leads to challenges in predicting dense annotations in intricate regions. To preserve the rich details of the input images, we introduce novel regularization strategy called Detail Preserver. Inspired by previous works (Long et al., 2024; Fu et al., 2024), we utilize task switcher {sx, sy}, enabling the denoiser model fθ to either generate annotation or reconstruct the input image. When activated by sy, the model focuses on predicting annotation. Conversely, when sx is selected, it reconstructs the input image. The switcher is one-dimensional vector 8 Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction Figure 9: Depth maps of multiple inferences and uncertainty maps. Areas like the sky, object edges, and intricate details (e.g., cat whiskers) typically exhibit high uncertainty. encoded by the positional encoder and then added with the time embeddings of diffusion model, ensuring seamless domain switching without mutual interference. This dual capability enables the diffusion model to make detailed predictions and thus leading to better performance. Overall, the loss function Lt is: Lt = zx fθ(zy , zx, t, sx)2 + zy fθ(zy , zx, t, sy)2, (6) where = and thus zy is pure Gaussian noise. 4.4 STOCHASTIC NATURE OF DIFFUSION MODEL One major characteristic of generative models is their stochastic nature, which, in image generation, enables the production of diverse outputs. In perception tasks like dense prediction, this stochasticity has the potential to allow the model generating predictions with uncertainty maps. Specifically, for any input image, we can conduct multiple inferences using different initialization noises and aggregate these predictions to calculate its uncertainty map. Thanks to our systematic analysis and tailored fine-tuning protocol, our method effectively reduces excessive flickering (large variance), only allowing for more accurate uncertainty calculations in naturally uncertain areas, such as the sky, object edges, and fine details (e.g. cat whiskers), as shown in Fig. 9. Most existing perception models are deterministic. To align with these, we can remove the noise input zy and only input the encoded image features zx to the U-Net denoiser. The model still performs well. In this paper, we finally present two versions of Lotus: Lotus-G (generative) with noise input and Lotus-D (discriminative) without noise input, catering to different needs. 4.5 INFERENCE The inference pipeline is illustrated in Fig. 10. We initialize the annotation map with standard Gaussian noise zy T, and encode the input image into its latent code zx. The noise zy and the image zx are concatenated and fed into the denoiser U-Net model. In our single-step formulation, we set = and the switcher to sy. The denoiser U-Net model then predicts the latent code of the annotation map. The final annotation map is decoded from the predicted latent code via the VAE decoder. For deterministic prediction, we eliminate the Gaussian noise zy and only feed the latent code of the input image into U-Net. 9 Figure 10: Inference Pipeline of Lotus. The noise zy in bracket is used for Lotus-G and omitted for Lotus-D. Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction"
        },
        {
            "title": "5.1 EXPERIMENTAL SETTINGS",
            "content": "Implementation details. We implement Lotus based on Stable Diffusion V2 (Rombach et al., 2022), with text conditioning disabled. During training, we fix the time-step = 1000. For more details, please see Sec. A.1 of the supplementary materials. Training Datasets. Both depth and normal estimation are trained on two synthetic dataset covering indoor and outdoor scenes: ① Hypersim (Roberts et al., 2021) is photorealistic synthetic dataset featuring 461 indoor scenes. We use the official training split, which contains approximately 54K samples. After filtering out incomplete samples, around 39K samples remain, all resized to 576768 for training. ② Virtual KITTI (Cabon et al., 2020) is synthetic street-scene dataset with five urban scenes under various imaging and weather conditions. We utilize four of these scenes for training, comprising about 20K samples. All samples are cropped to 352 1216, with the far plane at 80m. Following Marigold (Ke et al., 2024), we probabilistically choose one of the two datasets and then draw samples from it for each batch (Hypersim 90% and Virtual KITTI 10%). Evaluation Datasets and Metrics. ① For zero-shot affine-invariant depth estimation, we evaluate Lotus on NYUv2 (Silberman et al., 2012), ScanNet (Dai et al., 2017), KITTI (Geiger et al., 2013) and ETH3D (Schops et al., 2017) using absolute mean relative error (AbsRel), and also report δ1 and δ2 values. ② For surface normal prediction, we employ NYUv2, ScanNet, iBims-1 (Koch et al., 2018) and Sintel (Butler et al., 2012) datasets, reporting mean angular error (m.) as well as the percentage of pixels with an angular error below 11.25 and 30. Please see Sec. A.2 of the supplementary materials for further details on the evaluation datasets and metrics. 5.2 QUANTITATIVE COMPARISONS ① For depth estimation (Tab. 1), Lotus-G achieves the best performance compared to all generative baselines. Notice that we only require single step denoising process, also significantly boosting the inference speed as shown in Fig. 3. Lotus-D also performs well, only slightly inferior to DepthAnything. However, it is worthy to notice that Lotus is trained on only 0.059M images compared to DepthAnythings 62.6M images. ② For normal estimation (Tab. 2), both Lotus-G and Lotus-D outperform all other generative and discriminative methods on zero-shot surface normal estimation with significant margins. Please see Sec. of the supplementary materials for Qualitative Comparisons. 5.3 ABLATION STUDY As shown in Tab. 3, we conduct ablation studies to validate our designs. Starting with Direct Adaptation, we incrementally test the effects of different components, such as parameterization types, the single-step diffusion process, and the detail preserver. Initially, we train the model using only the Hypersim dataset to establish baseline. We then expand the training dataset using mixture dataset strategy by including Virtual KITTI, aiming to enhance the models generalization ability across different domains. The findings from these ablations validate the effectiveness of our proposed adaptation protocol, demonstrating that each design plays vital role in optimizing the diffusion models for dense prediction tasks."
        },
        {
            "title": "6 CONCLUSION AND FUTURE WORK",
            "content": "In this paper, we introduce Lotus, diffusion-based visual foundation model for dense prediction. Through systematic analysis and tailored diffusion formulation, Lotus finds way to better fit the rich visual prior from pre-trained diffusion models into dense prediction. Extensive experiments demonstrate that Lotus achieves SoTA performance on zero-shot depth and normal estimation with minimal training data, paving the way of various practical applications. Please see the supplementary materials for our discussion about Applications (Sec. F) and Future Work (Sec. G). 10 Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction Table 1: Quantitative comparison on zero-shot affine-invariant depth estimation between Lotus and SoTA methods. The upper section lists discriminative methods, the lower lists generative ones. The best and second best performances are highlighted. Lotus-G outperforms all others methods while Lotus-D is only slightly inferior to DepthAnything. Please note that DepthAnything is trained indicates results revised by on 62.6M images while Lotus is only trained on 0.059M images. ourselves. denotes the method relies on pre-trained Stable Diffusion. Method DiverseDepth MiDaS LeRes Omnidata DPT HDN GenPercept DepthAnything V2 Lotus-D (Ours) DepthAnything GeoWizard Marigold(LCM) Marigold Lotus-G (Ours) Training NYUv2 (Indoor) ScanNet (Indoor) Avg. Data AbsRel δ1 δ2 AbsRel δ1 δ2 AbsRel δ1 δ2 AbsRel δ1 δ2 Rank ETH3D (Various) KITTI (Outdoor) 320K 2M 354K 12.2M 1.4M 300K 74K 62.6M 59K 62.6M 280K 74K 74K 59K 11.7 11.1 9.0 7.4 9.8 6.9 5.6 4.5 5.3 4.3 5.6 6.1 5.5 5. 87.5 88.5 91.6 94.5 90.3 94.8 - - - - - - 96.0 99.2 97.9 99.3 96.7 99.2 98.1 99.6 96.3 99.1 95.8 99.0 96.4 99.1 96.6 99.2 19.0 23.6 14.9 14.9 10.0 11.5 13.0 7.4 9.3 7. 14.4 9.8 9.9 11.3 70.4 63.0 78.4 83.5 90.1 86.7 - - - - - - - 84.2 94.6 98.6 92.8 98.8 94.7 99.2 82.0 96.6 91.8 98.7 91.6 98.7 87.7 97. 22.8 18.4 17.1 16.6 7.8 12.1 7.0 13.1 6.8 12.7 6.6 6.8 6.5 6.2 69.4 75.2 77.7 77.8 94.6 83.3 - - - - - - - 95.6 98.8 86.5 95.3 98.9 88.2 - 95.8 98.4 95.6 99.0 95.9 99.0 96.1 99.0 10.9 12.1 9.1 7.5 8.2 8.0 6.2 4.2 6.0 4.3 6.4 6.9 6.4 6. 88.2 84.6 91.7 93.6 93.4 93.9 - - - - - - 96.1 99.1 97.8 99.3 96.3 99.1 98.1 99.6 95.0 98.4 94.6 98.6 95.2 98.8 96.0 99.0 9.5 9.5 7.6 6.4 5.5 5.0 3.8 2.9 2.5 2. 3.3 2.9 1.8 1.5 Table 2: Quantitative comparison on zero-shot surface normal estimation between Lotus and SoTA methods. Discriminative methods are shown in the upper section, generative methods in the lower. Both Lotus-D and Lotus-G outperform all other methods. refers the Marigold normal model as detailed in this link. denotes the method relies on pre-trained Stable Diffusion. Method Training Avg. Data m. 11.25 30 m. 11.25 30 m. 11.25 30 m. 11.25 30 Rank ScanNet (Indoor) iBims-1 (Indoor) NYUv2 (Indoor) Sintel (Outdoor) OASIS Omnidata EESNU GenPercept Omnidata V2 DSINE Lotus-D (Ours) Marigold GeoWizard StableNormal Lotus-G (Ours) 110K 29.2 12.2M 23.1 2.5M 16.2 74K 18.2 12.2M 17.2 160K 16.4 59K 16. 20.9 74K 280K 18.9 250K 18.6 59K 16.9 23.8 45.8 58.6 56.3 55.5 59.6 58.2 50.5 50.7 53.5 59.1 - 60.7 32.8 73.6 22.9 83.5 81.4 17.7 83.0 16.2 83.5 16.2 83.6 15.3 - 21.3 81.5 17.4 81.7 17.1 83.2 15.3 15.4 47.4 - 58.3 60.2 61.0 62.9 45.6 53.8 57.4 64.0 - 52.6 32.6 73.2 19.0 20.0 82.7 18.2 84.7 18.2 84.4 17.1 85.7 17.7 - 18.5 83.5 19.3 84.1 18.2 85.2 17.5 23.5 62.1 58.5 64.0 63.9 67.4 64.9 64.7 63.0 65.0 66.1 57.4 43.1 80.1 41.5 78.2 42.1 82.0 37.6 81.1 40.5 82.3 34.9 82.5 34.6 - - 80.3 40.3 82.4 36.7 82.7 35.2 7.0 11.4 11.5 16.2 14.7 21.5 20.5 - 12.3 14.1 19.9 35.7 42.0 41.2 51.0 43.5 52.7 55.8 - 43.5 50.7 54.8 7.0 5.3 4.4 3.7 3.6 1.8 1. 4.2 3.2 2.0 1.0 Table 3: Ablation studies on the step-by-step design of our adaptation protocol for fitting pre-trained diffusion models into dense prediction. Here we show the results in monocular depth estimation. Method Direct Adaptation + x0-prediction + Single Time-step + Detail Preserver + Mixture Dataset (Lotus-G) Noise Input (Lotus-D) Training NYUv2 (Indoor) KITTI (Outdoor) ETH3D (Various) ScanNet (Indoor) Data AbsRel δ1 δ2 AbsRel δ1 δ2 AbsRel δ1 δ2 AbsRel δ1 δ2 39K 39K 39K 39K 59K 59K 11.551 87.692 96.122 20.164 70.403 90.996 19.894 76.464 87.960 15.726 78.885 93.651 8.332 92.769 97.941 17.008 74.969 93.611 11.075 87.952 94.978 10.212 89.130 97.181 6.262 95.394 98.791 5.587 96.272 99.113 13.262 83.210 97.237 6.201 95.470 98.814 5.555 96.303 99.118 13.170 83.657 97. 7.586 94.143 97.678 7.147 95.000 98.058 5.425 96.597 99.156 11.324 87.692 97.780 9.334 92.813 98.795 5.334 96.729 99.198 6.172 96.077 98.980 6.846 95.290 98.899 6.024 96.026 99.730 5.982 96.287 99.087 11 Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction SUPPLEMENTARY MATERIALS OF LOTUS: DIFFUSION-BASED VISUAL FOUNDATION MODEL FOR HIGH-QUALITY DENSE PREDICTION"
        },
        {
            "title": "A EXPERIMENTAL SETTINGS",
            "content": "A."
        },
        {
            "title": "IMPLEMENTATION DETAILS",
            "content": "We implement Lotus based on Stable Diffusion V2 (Rombach et al., 2022), with text conditioning disabled. During training, we fix the time-step = 1000. To optimize the model, we utilize the standard Adam optimizer with the learning rate 3 105. All experiments are conducted on 8 NVIDIA A800 GPUs and the total batch size is 128. For our discriminative variant, we train for 4,000 steps, which takes 8.1 hours, while for the generative variant, we extend training to 10,000 steps, requiring 20.3 hours. A.2 EVALUATION DATASETS AND METRICS Evaluation Datasets. ① For affine-invariant depth estimation, we evaluate on 4 real-world datasets that are not seen during training: NYUv2 (Silberman et al., 2012) and ScanNet (Dai et al., 2017) all contain images of indoor scenes; KITTI (Geiger et al., 2013) contains various outdoor scenes; ETH3D (Schops et al., 2017), high-resolution dataset, containing both indoor and outdoor scenes. ② For surface normal prediction, we employ 4 datasets for evaluation: NYUv2 (Silberman et al., 2012), ScanNet (Dai et al., 2017), and iBims-1 (Koch et al., 2018) contain real indoor scenes; Sintel (Butler et al., 2012) contains highly dynamic outdoor scenes. Metrics. ① For affine-invariant depth, we follow the evaluation protocol from (Ranftl et al., 2020; Ke et al., 2024; Yang et al., 2024a;b), aligning the estimated depth predictions with available ground truths using least-squares fitting. The accuracy of the aligned predictions is assessed using the (cid:80)M absolute mean relative error (AbsRel), i.e., 1 i=1 ai di/di, where is the total number of pixels, ai is the predicted depth map and di represents the ground truth. We also report δ1 and δ2, the proportion of pixels satisfying Max(ai/di, di/ai) < 1.25 and < 1.252 respectively. ② For surface normal, following (Bae & Davison, 2024; Ye et al., 2024), we evaluate the predictions of Lotus by measuring the mean angular error for pixels with available ground truth. Additionally, we report the percentage of pixels with an angular error below 11.25 and 30. For all tasks, we report the Avg. Rank, which indicates the average ranking of each method across various datasets and evaluation metrics. lower value signifies better overall performance."
        },
        {
            "title": "B DETAILS OF DIRECT ADAPTION",
            "content": "As illustrated in Fig. 4 of the main paper, our Direct Adaption means directly adapting the standard diffusion formulation for dense prediction task with minimal modifications. Specifically, starting with the pre-trained Stable Diffusion model, image and annotation are encoded using the pretrained VAE encoder. Noise is added to the encoded annotation to obtain the noisy annotation zy at noise level [1, ]. The encoded image zx is then concatenated with the noisy annotation zy to form the input of the denoiser U-Net model. To handle this concatenated input, the U-Net input layer is duplicated (from 4 channels to 8 channels) and its original weights are halved as initialization, which prevents activation inflation (Ke et al., 2024). Direct Adaptation is optimized using the standard multi-step formulation the standard diffusion objective, ϵ-prediction, as described in Eq. 2 (first row) of the main paper. To analyze the original diffusion formulation more effectively, we avoid specialized techniques introduced in prior methods (Ke et al., 2024; Fu et al., 2024; Xu et al., 2024; Ye et al., 2024), such as annealed multi-resolution noise and test-time ensembling. 12 Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction ANALYSIS OF DIRECTION(zy τ ) IN DDIM PROCESS (EQ. 4) In addition to the predicted clean sample ˆzy direction(zy τ ). It is calculated according to different parameterization types: τ , Eq. 4 of the main paper includes another term, ϵ-prediction: = wτ ϵ θ x0-prediction: = wτ [ 1 1 ατ (zy τ ατ θ)] (7) θ and τ ), wτ = where represents the term direction(zy 1 ατ 1 is the weight at denoising step τ . And ϵ θ denote the model outputs for different parameterizations. For clarity, the input of the model fθ is omitted. As shown in Eq. 7, for x0-prediction, when τ 1, i.e., at the end of the denoising process, the factor θ. However, its influence is limited. The reasons are as follows: ① The rate of change of 1 ατ from to 1 is initially slow and then accelerates. As result, the factor remains close to 1 for most of the denoising process, only close to 0 in the final steps. ② In x0-prediction, compared to the initial denoising steps, the gap between network output τ in the final steps is much weaker and ατ θ 0, which may gradually approaching zero. With also indicate the limited influence of factor 1 ατ 0, which may amplify variance from ατ 1 as τ 1, we can get zy θ and zy τ 1 ατ . PERFORMANCE OF v-PREDICTION ατ In sec. 4.1, we discussed two basic parameterization types: ϵ-prediction and x0-prediction. The latest parameterization, v-prediction (Salimans & Ho, 2022), combines these two basic parameterizations to avoid the invalid prediction values of ϵ-prediction at some timesteps for progressive distillation. Specifically, the U-Net denoiser model fθ learns to predict the combination of added noise ϵ and the clean 1 ατ zy, where sample zy: = ατ ϵ 2 = 1. During inference, 2 + according to the Eq. 4 of main paper, the prediction ˆzy τ θ , where θ represents the predicted combination, striking balance between ϵ (ϵ-prediction) and zy (x0-prediction). As shown in Fig. 11, we conduct experiments based on the settings in Fig. 5 and 6 of the main paper. The results indicate that the performance of v-prediction falls between that of x0-prediction and ϵ-prediction, with moderate variance. However, for dense prediction tasks, minimizing variance is crucial to avoid unstable prediction. Therefore, v-prediction may not be the optimal choice. In contrast, x0-prediction achieves the best performance with the lowest variance, which is why we replace the standard ϵprediction with the more suitable x0-prediction. Figure 11: Quantitative evaluation of the predicted depth maps ˆzy τ along the denoising process. The experimental settings are same as Fig. 5 and 6. Six steps are selected for illustration. The banded regions around each line indicate the variance, wider areas representing larger variance. 1 ατ 1 ατ ατ zy τ ="
        },
        {
            "title": "E QUALITATIVE COMPARISONS",
            "content": "In Fig. 12, we further compare the performance of our Lotus with other methods in detailed areas. The quantitative results obviously demonstrate that our method can produce much finer and more accurate depth predictions, particularly in complex regions with intricate structures, which sometimes cannot be reflected by the metrics. Also, as illustrated in Fig. 13, Lotus consistently provides accurate surface normal predictions, effectively handling complex geometries and diverse environments, highlighting its robustness on fine-grained prediction. 13 Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction Figure 12: Qualitative comparison on zero-shot affine-invariant depth estimation. Lotus demonstrates higher accuracy especially in detailed areas."
        },
        {
            "title": "F APPLICATIONS OF LOTUS",
            "content": "Thanks to its superiority, Lotus can seamlessly support variety of applications. Fig. 14 illustrates four key applications: ① Depth to Point Cloud. The depth maps estimated by Lotus are projected into 3D point clouds; ② Joint Estimation. By incorporating task switcher, Lotus can perform multiple tasks simultaneously, such as joint depth and normal map estimation with 100% shared network parameters; ③ Single-View Reconstruction. Using Lotuss normal predictions, high-quality meshes can be reconstructed through through Bilateral Normal Integration (Cao et al., 2022); ④ Multi-View Reconstruction. Leveraging per-view depth and normal predictions from Lotus, highquality meshes can be reconstructed with MonoSDF (Yu et al., 2022), without RGB supervision, showcasing Lotuss robustness and accurate spatial understanding. These applications emphasize the importance of Lotus in the field of computer vision. Its accuracy and efficiency will help in addressing increasingly complex problems. 14 Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction Figure 13: Qualitative comparison on zero-shot surface normal estimation. Lotus offers improved accuracy particularly in complex regions."
        },
        {
            "title": "G FUTURE WORK",
            "content": "While we have applied Lotus to two geometric dense prediction tasks, it can be seamlessly adapted to other dense prediction tasks requiring per-pixel alignment with great potential, such as panoramic segmentation and image matting. Additionally, our performance is slightly behind DepthAnything (Yang et al., 2024a) which utilizes large-scale training data. In the future, scaling up the training data, as reveal in Fig. 7 and Tab. 3 (Mixture Dataset) of the main paper, has great potential to further enhance Lotuss performance. 15 Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction Figure 14: Applications of Lotus. ① Depth to 3D Point Clouds. ② Joint Estimation: Simultaneous depth and normal estimation with 100% shared parameters. ③ Single-View Reconstruction: Reconstructing 3D meshes from normal predictions. ④ Multi-View Reconstruction: Reconstructing high-quality meshes using depth/normal predictions without RGB supervision."
        },
        {
            "title": "REFERENCES",
            "content": "Gilwon Bae and Andrew Davison. Aleatoric uncertainty in monocular surface normal estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), pp. 14721485, 2021. Gilwon Bae and Andrew Davison. Rethinking inductive biases for surface normal estimation. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. Yaniv Benny and Lior Wolf. Dynamic dual-output diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1148211491, 2022. Daniel Butler, Jonas Wulff, Garrett Stanley, and Michael Black. naturalistic open source movie for optical flow evaluation. In Computer VisionECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part VI 12, pp. 611625. Springer, 2012. Yohann Cabon, Naila Murray, and Martin Humenberger. Virtual kitti 2. arXiv preprint arXiv:2001.10773, 2020. Xu Cao, Hiroaki Santo, Boxin Shi, Fumio Okura, and Yasuyuki Matsushita. Bilateral normal integration. In ECCV, 2022. Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. Weifeng Chen, Shengyi Qian, David Fan, Noriyuki Kojima, Max Hamilton, and Jia Deng. Oasis: large-scale dataset for single image 3d in the wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 679688, 2020. Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 58285839, 2017. Wenyu Du, Shuang Cheng, Tongxu Luo, Zihan Qiu, Zeyu Huang, Ka Chun Cheung, Reynold Cheng, and Jie Fu. Unlocking continual learning abilities in language models. arXiv preprint arXiv:2406.17245, 2024. 16 Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction Ainaz Eftekhar, Alexander Sax, Jitendra Malik, and Amir Zamir. Omnidata: scalable pipeline for making multi-task mid-level vision datasets from 3d scans. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1078610796, 2021. David Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from single image using multi-scale deep network. Advances in neural information processing systems, 27, 2014. Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, and Dacheng Tao. Deep ordinal regression network for monocular depth estimation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 20022011, 2018. Xiao Fu, Wei Yin, Mu Hu, Kaixuan Wang, Yuexin Ma, Ping Tan, Shaojie Shen, Dahua Lin, and Xiaoxiao Long. Geowizard: Unleashing the diffusion priors for 3d geometry estimation from single image. arXiv preprint arXiv:2403.12013, 2024. Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. The International Journal of Robotics Research, 32(11):12311237, 2013. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. Jing He, Yiyi Zhou, Qi Zhang, Jun Peng, Yunhang Shen, Xiaoshuai Sun, Chao Chen, and Rongrong Ji. Pixelfolder: An efficient progressive pixel synthesis network for image generation. arXiv preprint arXiv:2204.00833, 2022. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Mu Hu, Wei Yin, Chi Zhang, Zhipeng Cai, Xiaoxiao Long, Hao Chen, Kaixuan Wang, Gang Yu, Chunhua Shen, and Shaojie Shen. Metric3d v2: versatile monocular geometric foundation model for zero-shot metric depth and surface normal estimation. arXiv preprint arXiv:2404.15506, 2024. Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, et al. Planning-oriented autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1785317862, 2023. Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, and Shenghua Gao. 2d gaussian splatting for geometrically accurate radiance fields. In SIGGRAPH 2024 Conference Papers. Association for Computing Machinery, 2024. doi: 10.1145/3641519.3657428. Oguzhan Fatih Kar, Teresa Yeo, Andrei Atanov, and Amir Zamir. 3d common corruptions and data In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern augmentation. Recognition, pp. 1896318974, 2022. Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 44014410, 2019. Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 81108119, 2020. Tero Karras, Miika Aittala, Samuli Laine, Erik Harkonen, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free generative adversarial networks. Advances in Neural Information Processing Systems, 34:852863, 2021. Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 94929502, 2024. 17 Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction Tobias Koch, Lukas Liebel, Friedrich Fraundorfer, and Marco Korner. Evaluation of cnn-based single-image depth estimation methods. In Proceedings of the European Conference on Computer Vision (ECCV) Workshops, pp. 00, 2018. Hsin-Ying Lee, Hung-Yu Tseng, and Ming-Hsuan Yang. Exploiting diffusion prior for generalizable dense prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 78617871, 2024. Jin Han Lee, Myung-Kyu Han, Dong Wook Ko, and Il Hong Suh. From big to small: Multi-scale local planar guidance for monocular depth estimation. arXiv preprint arXiv:1907.10326, 2019. Jiahui Lei, Yijia Weng, Adam Harley, Leonidas Guibas, and Kostas Daniilidis. Mosca: Dynamic gaussian fusion from casual videos via 4d motion scaffolds. arXiv preprint arXiv:2405.17421, 2024. Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3d: Single image to 3d using cross-domain diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 99709980, 2024. Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pp. 88218831. PMLR, 2021. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical textconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. Rene Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE transactions on pattern analysis and machine intelligence, 44(3):16231637, 2020. Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1217912188, 2021. Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb, and Joshua Susskind. Hypersim: photorealistic synthetic dataset for holistic indoor scene understanding. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1091210922, 2021. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedIn Medical image computing and computer-assisted intervention ical image segmentation. MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pp. 234241. Springer, 2015. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:3647936494, 2022. Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512, 2022. 18 Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction Thomas Schops, Johannes Schonberger, Silvano Galliani, Torsten Sattler, Konrad Schindler, Marc Pollefeys, and Andreas Geiger. multi-view stereo benchmark with high-resolution images and In Proceedings of the IEEE conference on computer vision and pattern multi-camera videos. recognition, pp. 32603269, 2017. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294, 2022. Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support inference from rgbd images. In Computer VisionECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part 12, pp. 746760. Springer, 2012. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. Yunzhou Song, Jiahui Lei, Ziyun Wang, Lingjie Liu, and Kostas Daniilidis. Track everything everywhere fast and robustly, 2024. Qianqian Wang, Vickie Ye, Hang Gao, Jake Austin, Zhengqi Li, and Angjoo Kanazawa. Shape of motion: 4d reconstruction from single video. arXiv preprint arXiv:2407.13764, 2024. Yuxi Xiao, Qianqian Wang, Shangzhan Zhang, Nan Xue, Sida Peng, Yujun Shen, and Xiaowei Zhou. Spatialtracker: Tracking any 2d pixels in 3d space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. Guangkai Xu, Yongtao Ge, Mingyu Liu, Chengxiang Fan, Kangyang Xie, Zhiyue Zhao, Hao Chen, and Chunhua Shen. Diffusion models trained with large data are transferable visual models. arXiv preprint arXiv:2403.06090, 2024. Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: Fine-grained text to image generation with attentional generative adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 13161324, 2018. Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1037110381, 2024a. Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. arXiv preprint arXiv:2406.09414, 2024b. Chongjie Ye, Lingteng Qiu, Xiaodong Gu, Qi Zuo, Yushuang Wu, Zilong Dong, Liefeng Bo, Yuliang Xiu, and Xiaoguang Han. Stablenormal: Reducing diffusion variance for stable and sharp normal. arXiv preprint arXiv:2406.16864, 2024. Wei Yin, Yifan Liu, and Chunhua Shen. Virtual normal: Enforcing geometric constraints for accurate and robust depth prediction. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(10):72827295, 2021a. Wei Yin, Jianming Zhang, Oliver Wang, Simon Niklaus, Long Mai, Simon Chen, and Chunhua Shen. Learning to recover 3d scene shape from single image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 204213, 2021b. Wei Yin, Chi Zhang, Hao Chen, Zhipeng Cai, Gang Yu, Kaixuan Wang, Xiaozhi Chen, and Chunhua Shen. Metric3d: Towards zero-shot metric 3d prediction from single image. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 90439053, 2023. Zehao Yu, Songyou Peng, Michael Niemeyer, Torsten Sattler, and Andreas Geiger. Monosdf: Exploring monocular geometric cues for neural implicit surface reconstruction. Advances in Neural Information Processing Systems (NeurIPS), 2022. 19 Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction Weihao Yuan, Xiaodong Gu, Zuozhuo Dai, Siyu Zhu, and Ping Tan. Neural window fully-connected crfs for monocular depth estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 39163925, 2022. Ekim Yurtsever, Jacob Lambert, Alexander Carballo, and Kazuya Takeda. survey of autonomous driving: Common practices and emerging technologies. IEEE access, 8:5844358469, 2020. Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu Cai, Qing Qu, Yong Jae Lee, and Yi Ma. Investigating the catastrophic forgetting in multimodal large language models. arXiv preprint arXiv:2309.10313, 2023. Chi Zhang, Wei Yin, Billzb Wang, Gang Yu, Bin Fu, and Chunhua Shen. Hierarchical normalization for robust monocular depth estimation. Advances in Neural Information Processing Systems, 35: 1412814139, 2022. Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dimitris Metaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks. In Proceedings of the IEEE international conference on computer vision, pp. 59075915, 2017. Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dimitris Metaxas. Stackgan++: Realistic image synthesis with stacked generative adversarial networks. IEEE transactions on pattern analysis and machine intelligence, 41(8):19471962, 2018. Han Zhang, Jing Yu Koh, Jason Baldridge, Honglak Lee, and Yinfei Yang. Cross-modal contrastive learning for text-to-image generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 833842, 2021."
        }
    ],
    "affiliations": [
        "HKUST",
        "HKUST(GZ)",
        "Noahs Ark Lab",
        "University of Adelaide"
    ]
}