{
    "paper_title": "AV-Odyssey Bench: Can Your Multimodal LLMs Really Understand Audio-Visual Information?",
    "authors": [
        "Kaixiong Gong",
        "Kaituo Feng",
        "Bohao Li",
        "Yibing Wang",
        "Mofan Cheng",
        "Shijia Yang",
        "Jiaming Han",
        "Benyou Wang",
        "Yutong Bai",
        "Zhuoran Yang",
        "Xiangyu Yue"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recently, multimodal large language models (MLLMs), such as GPT-4o, Gemini 1.5 Pro, and Reka Core, have expanded their capabilities to include vision and audio modalities. While these models demonstrate impressive performance across a wide range of audio-visual applications, our proposed DeafTest reveals that MLLMs often struggle with simple tasks humans find trivial: 1) determining which of two sounds is louder, and 2) determining which of two sounds has a higher pitch. Motivated by these observations, we introduce AV-Odyssey Bench, a comprehensive audio-visual benchmark designed to assess whether those MLLMs can truly understand the audio-visual information. This benchmark encompasses 4,555 carefully crafted problems, each incorporating text, visual, and audio components. To successfully infer answers, models must effectively leverage clues from both visual and audio inputs. To ensure precise and objective evaluation of MLLM responses, we have structured the questions as multiple-choice, eliminating the need for human evaluation or LLM-assisted assessment. We benchmark a series of closed-source and open-source models and summarize the observations. By revealing the limitations of current models, we aim to provide useful insight for future dataset collection and model development."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 3 ] . [ 1 1 1 6 2 0 . 2 1 4 2 : r AV-Odyssey Bench: Can Your Multimodal LLMs Really Understand Audio-Visual Information? Kaixiong Gong1*, Kaituo Feng1*, Bohao Li2*, Yibing Wang, Mofan Cheng, Shijia Yang3, Jiaming Han1, Benyou Wang2, Yutong Bai4, Zhuoran Yang5, Xiangyu Yue1 1CUHK MMLab, 2CUHK (SZ), 3Stanford University, 4UC Berkeley, 5Yale University https://av-odyssey.github.io/"
        },
        {
            "title": "Abstract",
            "content": "Recently, multimodal large language models (MLLMs), such as GPT-4o, Gemini 1.5 Pro, and Reka Core, have expanded their capabilities to include vision and audio modalities. While these models demonstrate impressive performance across wide range of audio-visual applications, our proposed DeafTest reveals that MLLMs often struggle with simple tasks humans find trivial: 1) determining which of two sounds is louder, and 2) determining which of two sounds has higher pitch. Motivated by these observations, we introduce AV-Odyssey Bench, comprehensive audiovisual benchmark designed to assess whether those MLLMs can truly understand the audio-visual information. This benchmark encompasses 4,555 carefully crafted problems, each incorporating text, visual, and audio components. To successfully infer answers, models must effectively leverage clues from both visual and audio inputs. To ensure precise and objective evaluation of MLLM responses, we have structured the questions as multiple-choice, eliminating the need for human evaluation or LLM-assisted assessment. We benchmark series of closed-source and open-source models and summarize the observations. By revealing the limitations of current models, we aim to provide useful insight for future dataset collection and model development. 1. Introduction Multimodal Large Language Models have evolved progressively, beginning with vision language models. Vision Language Models (VLMs), exemplified by GPT-4V(ision) [55], have endowed language models with visual perception, en- *Equal Contribution Project Leader Corresponding Authors: wangbenyou@cuhk.edu.cn and xyyue@ie.cuhk.edu.hk Figure 1. Illustration of two out of four DeafTest tasks. Loudness comparison is used to determine the louder sound of two given sounds. Pitch comparison is to determine which sound has the higher pitch. abling them to tackle wide array of vision-language tasks [80]. These models demonstrate remarkable capabilities, including counting objects in images [78], performing numerical calculations on tabular data [82], and solving geometric problems with provided figures [93]. Building upon this foundation, Multimodal Large Language Models (MLLMs)1 have further expanded their capabilities by incorporating audio modality, e.g., GPT-4o [27] and Gemini 1.5 [70]. These advancements push the boundaries of multimodal reasoning, particularly in areas including automatic speech recognition (ASR) [27], automatic speech translation (AST) [70], audio-visual captioning [23, 89], and general audio-visual processing [23]. Most previous multimodal benchmarks focus on visual problem-solving [30, 42, 86], e.g., general vision comprehension [21, 32, 42] and multimodal mathematical reasoning [810, 48, 49, 93]. To evaluate audio-visual comprehension, dedicated benchmarks such as AVQA [81] and Mu1In this paper, MLLMs only refer to the audio-vision LLMs. We use VLM to refer to the vision-language LLMs. 1 Figure 2. Overview of AV-Odyssey Benchmark. AV-Odyssey Bench demonstrates three major features: 1. Comprehensive Audio Attributes; 2. Extensive Domains; 3. Interleaved Text, Audio, and Images. sicAVQA [33] have been introduced, assessing models in several audio-visual tasks [33, 37, 68, 81, 88]. However, we primarily identify two issues of previous audio-visual benchmarks: 1) They overlook assessing the basic listening ability (no reasoning) of MLLMs. 2) They are limited in evaluation dimensions (e.g., restricted audio attributes and narrow application domains), as illustrated in Table 2. This leads to an insufficient evaluation of the audio-visual information understanding capability of MLLMs. These findings raise the central question we aim to investigate in this paper: Can Multimodal LLMs Truly Understand Audio-Visual Information? To investigate this, we propose DeafTest to assess the basic listening ability of MLLMs, along with the comprehensive AVOdyssey Benchmark to further assess their audio-visual understanding capabilities. DeafTest, suite of four low-level auditory tasks (examples are illustrated in Figure 1), is introduced to examine the fundamental ability of MLLMs to listen, inspired by the Schwabach test in audiology [26] and BlindTest [61] in vision domain. We evaluate Gemini 1.5 [70], Reka [71], and GPT-4o [27] on our four simple tasks that only involve basic elements of sound (e.g., loudness, pitch, and duration) as shown in Table 1. The key findings are: 1. Despite their ability to recognize complex speech content, MLLMs do not perform as well as expected on sound counting tasks. The best-performing model, Gemini 1.5 Pro, achieves only 81%, while humans can easily score 100%. The sounds in these tasks are monotonous and are clearly separated by silent intervals within the audio clip. 2. MLLMs appear to be insensitive to sound volume. When two sounds, one louder than the other, are fed into models, they often fail to distinguish the louder one. All models perform below 65%, significantly lower than the expected 100%. Table 1. Results on four basic auditory tasks (DeafTest). The questions are designed as two-choice questions. The random baseline performance is 50%. Method Sound Counting Loudness Comparison Pitch Comparison Duration Comparison Random Gemini 1.5 Flash [70] Gemini 1.5 Flash-8B [70] Gemini 1.5 Pro [70] Reka Core [71] Reka Flash [71] Reka Edge [71] GPT-4o audio-preview [27] 50.0 55.0 49.0 81.0 54.0 48.0 47.0 50. 50.0 62.0 55.0 60.0 43.0 58.0 56.0 58.0 50.0 54.0 51.0 52.0 42.0 51.0 50.0 58.0 50.0 89.0 51.0 84.0 40.0 44.0 44.0 57.0 3. MLLMs also struggle to discriminate the higher pitch between two given sounds. None of the models can score above 60%. 4. The duration comparison task presents models with two sounds and asks them to determine which has the longer duration. Two Gemini models perform significantly better than the others that are merely on par with the random baseline. The results suggest that while MLLMs excel at some high-level multimodal reasoning tasks, they still have limitations in basic listening abilities, which may further hinder their integration of audio-visual information (hypothesis ). DeafTest allows us to critically assess whether MLLMs truly understand audio-visual information or if their apparent capabilities are limited to surface-level pattern recognition. By doing so, we aim to contribute to deeper understanding of multimodal comprehension in current MLLMs and identify areas for future improvementissues that have been overlooked by previous audiovisual benchmarks [33, 37, 81]. To further investigate question , we introduce AVOdyssey, comprehensive audio-visual benchmark designed to challenge MLLMs by requiring them to lever2 Table 2. Comparisons between MLLM benchmarks / datasets. Benchmark / Dataset Modality Questions Answer Type Customized Question Audio Attributes Timbre Tone Melody Space Time Hallucination Intricacy Multiple Domains Interleaved MME Bench [21] MMBench [42] SEED-Bench-2 [32] AVQA Dataset [81] Pano-AVQA Dataset [88] Music-AVQA Dataset [33] Image Image(s) Image(s) & Video Video & Audio Video & Audio Video & Audio SAVE Bench [68] OmniBench [37] Image & Video & Audio Image & Audio 2194 2974 24371 57335 51700 45867 4350 1142 AV-Odyssey Bench (ours) Image(s) & Video & Audio(s) 4555 Y/N A/B/C/D A/B/C/D A/B/C/D defined words & bbox defined words free-form A/B/C/D A/B/C/D - - - - - - - - - - - - - - - - - - - - - age information from all input modalities to derive accurate answers. This meticulously crafted dataset encompasses 4,555 carefully selected questions spanning 26 distinct tasks, with each question strategically constructed to include three critical components: text, image/video, and audio clip. To ensure the benchmarks rigor, we employ vision and audio language models to filter out questions that could be easily resolved by single-modal approaches. This guarantees that only complex, multi-dimensional questions remain, truly testing the models ability to integrate information across modalities. AV-Odyssey is designed with broad scope, covering wide range of sound attributes, including timbre, tone, spatial characteristics, and temporal dynamics, while simultaneously exploring diverse application domains such as music, daily life, and transportation, as illustrated in Figure 2. By structuring the evaluation as multiple-choice selections, weve streamlined the assessment process, eliminating the need for manual verification or LLM-assisted evaluation. We conduct extensive experiments on closed-source model [27, 70, 71] and open-source models [15, 22, 23, 47, 67, 77, 89] on the proposed AV-Odyssey Bench. Considering the results of DeafTest and AV-Odyssey, we have the following findings: Overall, current MLLMs still fall short in processing complex audio-visual information integration tasks. The audio-caption-vision training paradigm fails to effectively combine audio and visual modalities, limiting the integration of audio-visual information (see Sec. 4.2). Through error analysis (see Sec. 4.3), we find out that the major cause of error in audio-visual inference is the perceptual error of audio input, which is in line with the hypothesis . In nutshell, this paper explores whether MLLMs really understand audio-visual information from two aspects. To begin with, we propose DeafTest to evaluate MLLMs and find out that these MLLMs still have obvious limitations in basic listening ability. This could lead to bottleneck in subsequent audio-visual information integration for complex audio-visual reasoning, which is then validated by the results and analysis of our proposed AVOdyssey benchmark. 2. Related Work Multimodal Large Language Models. Large language models (LLMs) have demonstrated remarkable performance across diverse textual domains [6, 7, 55, 60, 73]. The success of these models has catalyzed significant advancements in vision language models and multimodal large language models. Inspired by the textual prowess of LLMs, vision language models have emerged to extend computational capabilities into visual comprehension. These models enable LLMs to perform sophisticated visual tasks, including visual question answering [5, 18, 34, 39, 40, 85, 91, 96], visual grounding [11, 12, 58, 76], document understanding [25, 50, 85, 94], long video understanding [36, 41, 62, 64, 92]. Building upon vision-language achievements, researchers have further expanded multimodal horizons by integrating the audio modality [15, 22, 23, 47, 67, 77, 89]. These advanced models now accommodate audio inputs, further expanding the landscape of multimodal artificial intelligence. Benchmarking Multimodal Large Language Models. The rapid development of vision language models has been accompanied by the emergence of specialized benchmarks to assess their performance across various domains [9, 21, 32, 49, 87]. significant subset of these benchmarks focuses on vision comprehension [21, 31, 32, 87] and mathematical reasoning capabilities [8, 9, 49, 63, 87, 93]. However, current audio-visual benchmarks [33, 37, 68, 81, 88] face significant limitations in comprehensively assessing multimodal large language models (MLLMs). Firstly, they predominantly focus on high-level visual tasks and neglect to explore the basic auditory perception limitations. Secondly, they are limited in application domains. For example, Music-AVQA [33] limits evaluation to the music domain, and AVQA [81] primarily tests daily life applications. Thirdly, they do not comprehensively evaluate all In contrast, this paper introduces attributes of the audio. DeafTest tasks to evaluate fundamental capabilities and the AV-Odyssey benchmark, which spans wide spectrum of 3 audio attributes and application domains, enabling comprehensive assessment of the audio-visual comprehension performance of MLLMs. 3. Method 3.1. DeafTest Tasks Drawing inspiration from the Schwabach test [26], we introduce DeafTest, suite of four simple auditory tasks that critically examine the fundamental audio perception capabilities of Multimodal Large Language Models (MLLMs). DeafTest includes the determination of the number of sounds, identification of the louder sound, recognition of the sound with higher pitch, and detection of the sound with longer duration. We hypothesize that MLLMs may not perform as well as expected on these basic tasks. This potential shortcoming arises from the training objectives of these models, which primarily focus on achieving high-level semantic alignment between different modalities. Consequently, this approach tends to overlook the effective utilization of low-level auditory information, which is crucial for accurately processing and understanding basic sound characteristics. 1. Count the Number of Sounds. Given that Multimodal LLMs achieve impressive performance on ASR (GPT-4os 3% word error rate on ASR Western Europe) [54], we expect that counting the number of sounds is not difficult for MLLMs. In this task, we give an audio clip that contains several sounds ranging from 3 to 8 and ask MLLMs for the number of sounds. In an audio clip, the sounds are monotonous and clearly separated by silent clip. We formulate these queries as two-choice questions. The MLLMs only need to predict the correct option. The question number for this task, as well as for all remaining tasks in DeafTest, is set to 100. 2. Discriminate the Louder Sound. In this task, we test the basic ability of MLLMs to distinguish between the loudness of sounds. The goal of MLLMs is to discriminate which sound is louder out of two given audio clips. Specifically, the decibel for quieter audio ranges from 30 dB to 60 dB, while the decibel for louder audio ranges from 70 dB to 100 dB. We randomly sample decibels from these two ranges to create two audio clips. In addition, we randomly switch the input order of the two audio clips; that is, for some questions, the quiet audio comes first, and for the rest, the loud audio comes first. Similarly, the question format is also two-choice question. 3. Discriminate the Higher Pitch. This task is similar to task 2 in 3.1. We also create two audio clips. The key difference between the two audios is pitch. Pitch is the basic element of sound, which is helpful in discriminating tone, emotion, environment, etc. For the lower pitch audio, we randomly sample its pitch from 100Hz to 500Hz, while Table 3. Detailed statistics of AV-Odyssey Benchmark. Statistics Total Questions Total Tasks Domains Questions with Multiple Images, Singe Audio Questions with Single Image, Multiple Audios Questions with Singe Image, Singe Audio Questions with Singe Video, Singe Audio Questions with Single Video, Multiple Audios Number 4555 26 10 2610 891 434 220 400 Correct Option Distribution (A:B:C:D) 1167:1153:1119:1116 Average Audio Time Average Image Resolution Average Video Resolution Average Video Time 16.32 seconds 1267.72 891.40 1678.69 948.56 15.58 seconds we randomly sample pitch from 1000Hz to 2000Hz for the higher pitch audio. We manually check these sounds to ensure that humans can easily discriminate between different pitches. In task 2, we randomly switch the input order of the two audio clips. 4. Recognize the Duration of Sound. We also test MLLMs with the duration of sound. In this task, we simplify the question by giving two audio clips of different durations. We sample the duration from 1s to 3s for the short audio, while we sample from 4s to 6s for the long audio. Similar to task 2, we provide the MLLMs with two audio clips, asking them to identify the longer one. The results on DeafTest are shown in Table 1. Among the four tasks, sound counting and duration separation seem to be simpler than the other two for MLLMs, since Gemini 1.5 Pro achieves more than 80% accuracy on the two tasks. Nonetheless, all the results are far behind the expected 100%. Especially on loudness comparison and pitch comparison tasks, none of these MLLMs achieve score over 65%. All these results suggest that MLLMs fall short in basic listening instinct, which might hinder further audio-visual information integration for solving sophisticated audio-visual comprehension tasks. To verify this, we further introduce holistic AV-Odyssey benchmark to comprehensively evaluate the audio-visual performance of MLLMs as depicted in the following. 3.2. Overview of AV-Odyssey Bench Our AV-Odyssey Bench is meticulously curated benchmark designed to comprehensively assess the audio-visual capabilities of MLLMs. To ensure robust and unbiased assessment, all questions in AV-Odyssey are structured as multiple-choice, with four options per question, and options can be presented in various formats, including text, images, or audio clips. To mitigate format-specific biases, we have curated five distinct multi-choice question types. Additionally, all inputs, including text, image/video, and audio clips, are fed into MLLMs in an interleaved manner. vanced tasks that demand internal expert-level knowledge learned from the large-scale pretraining data to solve, such as singer recognition and bird species identification. Tone Tasks. These tasks target evaluating MLLMs with speech sentiment analysis and meme understanding. For example, meme understanding requires MLLMs to infer humorous reasons simultaneously from the voice tone and visual context. Melody Tasks. For evaluating melody understanding abilities, we propose melody tasks. For example, the dance and music matching task requires the MLLM to understand the melody of the music and identify the one that aligns with the dance in video. Space Tasks. To test the spatial inference ability with audio and visual information, space tasks require MLLMs to infer the distance of certain object producing sound or to determine the 3D angle. Time Tasks. These tasks test the cross-modal matching and temporal correlation abilities at the same time. For example, audio time estimation requires MLLMs to determine the start and end time of an action. Hallucination Tasks. Inspired by POPE [35] that indicates severe object hallucination existing in vision language models, we designed this task to assess the hallucination issue in audio-visual reasoning. Intricacy Tasks. These tasks challenge MLLMs to perform integrated analysis or reasoning through both visual and audio inputs, leveraging multiple attributes. For example, action prediction requires models to infer actions based on visual elements alongside various audio attributes, such as timbre and timing. These diverse tasks provide rigorous and multifaceted assessment of MLLMs audio-visual information integration capabilities, systematically probing the depth, nuance, and complexity of cross-modal perception and reasoning. 3.3. Data Curation Process Data Collection. AV-Odyssey Bench is an audio-visual benchmark to evaluate whether MLLMs truly have audiovisual reasoning capability. Since the audio is the newly added modality by these omni-modal models and there is already an array of visual benchmarks, we put our attention on the attributes of sound in the benchmark construction. We first go through all the attributes of sound, such as timbre, tone, time, space, etc. Then, we start from specific attribute of sound and span the domains to cover wide range of application domains, such as music, daily life, and transportation. We primarily use two strategies to construct questions: 1) For most conceptmatching questions (e.g., bird recognition, material recognition), we gather audio clips from public datasets and crawl corresponding visual data from the internet to automatically generate questions and options. Human experts conFigure 3. Overview of 26 evaluation tasks of AV-Odyssey Benchmark. We mainly categorize these tasks with the sound attributed into 7 classes. We compare our AV-Odyssey benchmark with previIt can ous MLLM benchmarks and datasets in Table 2. be found that previous works suffer from certain limitations, such as restricted audio attributes, which fail to capture the full spectrum of auditory complexity; narrow domain focus, limiting the generalizability of findings; and the absence of interleaved settings, crucial for assessing realworld multimodal integration capabilities. For instance, Music-AVQA [33] limits audio and visual data to the music domain, while OmniBench [37] lacks multiple audio attributes, making it difficult to comprehensively assess the capabilities of MLLMs in audio-visual tasks. In contrast, our AV-Odyssey encompasses 26 tasks across 10 diverse domains and includes 7 audio attributes, with interleaved and customized questions. The detailed statistics are shown in Table 3. This design enables an exhaustive evaluation of MLLMs, providing nuanced and thorough assessment of their performance in complex, real-world audio-visual scenarios. Next, we will briefly introduce the task categories that span broad spectrum of audio attributes, including Timbre, Tone, Melody, Spatial characteristics, Temporal dynamics, and Hallucination detection. The detailed task distribution and task examples are shown in Figure 3 and Figure 4, respectively. Timbre Tasks. In order to test the concept of matching across vision and audio modalities, MLLMs are required to match audio-visual pairs (e.g., lions roar sound with lion images) in timbre tasks. In addition, we have designed ad5 Figure 4. Sampled examples from our AV-Odyssey Benchmark. duct post-evaluation and filter out low-quality questions. 2) For other questions (e.g., meme understanding, audio 3D angle estimation), we manually collect audio and visual data from the internet or public datasets, relying on human experts to craft the questions and options. The datasets we used are listed as follows: [14, 19, 20, 24, 28, 43 46, 51, 53, 56, 57, 59, 65, 66, 72, 74, 84, 95]. Quality Control. Duplicated text information that describes visual inputs will induce MLLMs to bypass the visual input to directly derive the answer by memorizing is as simple as possible. the answer from the internet-scale training dataset [13]. Inspired by this, we first ensure that our text questions context Then we filter out those questions that have redundant images or audio clips by leveraging VLMs and audio LLMs. Specifically, we test all the curated questions with VLM: InternVL2 [14], Qwen2-VL [75], MiniCPM-V 2.5 [83], BLIP3 [79], and VILA1.5 [38] and audio LLM Qwen-Audio [16], Qwen2Audio [17], SALMONN [69], and Typhoon-Audio [52], and filter out those questions that can be solved by either of 6 Table 4. Evaluation results of various MLLMs in different parts of AV-Odyssey Bench. The highest performance is highlighted in bold, while the second highest is underlined. is the averaged accuracy across corresponding dimensions, and is the rank based on the the averaged accuracy. All Avg. represents the averaged accuracy over all questions in our AV-Odyssey Bench. Model Random Unified-IO-2 [47] Unified-IO-2 XL [47] Unified-IO-2 XXL [47] OneLLM [23] PandaGPT [67] Video-llama [90] VideoLLaMA2 [15] AnyGPT [89] NExT-GPT [77] VITA [22] LLM Size Timbre T - 25. 1B 3B 7B 7B 7B 7B 7B 7B 7B 23.8 24.3 26.3 25.0 23.5 25.5 24.1 24.6 23.2 8 7B 24.1 Gemini 1.5 Flash [70] Gemini 1.5 Flash-8B [70] Gemini 1.5 Pro [70] Reka Core [71] Reka Flash [71] Reka Edge [71] GPT-4o visual caption [27] GPT-4o audio caption [27] - - - 67B 21B 7B - - 27.2 25.1 30.8 26.7 25.5 23.8 37.4 38.6 Tone 25.0 24.1 23.2 22.7 25.5 23.2 22.3 25.5 25.0 20.9 26.4 25.0 24.5 31.4 27.7 24.1 20.5 28.6 31.8 - 11 13 15 6 13 16 6 8 17 5 8 10 2 4 11 18 3 1 Melody T Space T 25.0 28.8 27.8 26.4 21.5 27.6 24.4 26.4 26.4 27.8 27.8 28.8 28.9 31.3 26.4 27.2 26.3 32.3 33.6 - 6 7 12 18 10 17 14 15 9 7 5 4 3 13 11 16 2 25.0 15.0 22.5 32.5 37.5 45.0 30.0 30.0 27.5 30.0 22.5 30.0 27.5 37.5 22.5 30.0 22.5 27.5 32.5 - 18 14 4 2 1 6 6 11 6 14 6 11 2 14 6 14 11 Time 25.0 26.8 25.3 26.8 29.3 23.8 26.2 27.2 29.2 28.8 26.3 25.3 27.5 27.7 26.5 27.5 25.5 25.5 27.5 - 9 16 9 1 18 13 8 2 3 12 16 5 4 11 5 14 14 5 Hallucination T Intricacy R All Avg. T 25.0 30.0 31.5 24.5 25.5 28.0 25.0 33.0 29.0 28.5 31.0 28.5 29.0 20.5 24.0 31.5 22.5 23.0 25. - 5 2 14 11 10 12 1 6 8 4 8 6 18 15 2 17 16 12 25.0 30.4 34.8 33.8 38.4 23.9 30.7 34.5 25.7 23.6 36.8 31.2 30.2 33.0 34.3 24.1 36.8 28.9 26. - 11 4 7 1 17 10 5 15 18 2 9 12 8 6 16 3 13 14 25.0 26.0 26.3 27.2 27.4 26.7 26.1 26.8 26.1 25.5 26.4 27.8 26.8 30.8 26.9 26.3 25.0 32.3 34. - 16 12 6 5 10 14 9 15 17 11 4 8 3 7 13 18 2 1 - 16 12 6 10 17 7 13 11 18 14 4 9 3 5 8 15 2 r n e o s these models. In experiment, 2.54% questions are filtered out because they are solved by all audio LLMs or VLMs 4. Experiment We test various closed-source and open-source MLLMs that accommodate the inputs of text, image/video, and audio. Experiments are conducted in the zero-shot setting to evaluate the performance of MLLMs without finetuning and fewshot prompting. The text prompts are designed as concise as possible to remove redundant information. 4.1. Models We evaluate 18 models in total, 8 closed-source models, including Gemini 1.5 Flash, Gemini 1.5 Flash-8B, Gemini 1.5 Pro [70], Reka Core, Reka Flash, Reka Edge [71], GPT-4o [27] and 10 open-source models including Unifed-IO-2 [47], Unified-IO-2 XL, Unified-IO2 XXL, OneLLM [23], PandaGPT [67], Video-llama [90], VideoLLaMA2 [15], AnyGPT [89], NExT-GPT [77], VITA [22]. We test those open-source models based on their source code and the latest checkpoint and test the closed-source models with available APIs. Since we currently cannot access the GPT-4o API that supports simultaneous image, video, and audio inputs, we have adopted an alternative approach to evaluating GPT4o models. The GPT-4o series includes two types of APIs: GPT-4o, which processes image and text inputs, and GPT4o-audio, which processes audio and text inputs. Based on these two APIs, we develop two methods to evaluate GPT4o: (1) We use GPT-4o-audio to generate captions for audio clips, then feed the text, image/video, and audio captions into GPT-4o. We refer to this approach as the GPT-4o audio caption method. (2) Similarly, we use GPT-4o to generate captions for images or videos, then input the text, audio, and visual captions into GPT-4o-audio. We refer to this approach as the GPT-4o visual caption method. We set random baseline which is 25% for AV-Odyssey Bench with four-choice questions. When task performance is below the random baseline, it indicates that the model is unable to handle the task effectively. Consequently, if two models both perform worse than this random baseline, the performance gap between them becomes meaningless. 4.2. Main Result Analysis In this section, we analyze the performance of MLLMs in our AV-Odyssey benchmark, as presented in Table 4. Due to the space limit, detailed results and data distribution are provided in the Appendix. Our key findings are as follows: Challenging Nature of AV-Odyssey. As presented in Table 4, the average performance of most existing MLLMs is only marginally higher than 25%comparable to the expected accuracy of random guessing on four-choice questions. Notably, even the top-performing model in our AVOdyssey, GPT-4o audio caption, only achieves 34.5% accuracy. This result underscores the high level of challenge posed by our benchmark, which significantly goes beyond the distribution of training data of current models. By setting rigorous standards, our benchmark serves as crucial tool for evaluating the robustness and versatility of MLLMs in audio-visual tasks. It highlights the limitations of existing models and provides directions for future improvements. 7 Figure 5. Distribution of 104 human-annotated errors in the Gemini 1.5 Pro. Discrepancies Between Closed-Source and OpenSource Models. The gap between open-source and closedsource models is relatively small. For example, the leading open-source models PandaGPT and OneLLM, with accuracies of 27.4% and 27.2% respectively, are only marginally behind GPT-4o audio caption and Gemini 1.5, which achieve accuracies of 34.5% and 30.8%. Besides, it can be found that PandaGPT and OneLLM deliver performance comparable to certain closed-source models, such as Gemini 1.5 Flash 8B and Reka Core. This indicates that our AV-Odyssey is challenging for both open-source and closed-source models. Comparison Between Audio Captions and Visual Captions. It can be observed that GPT-4o audio caption achieves higher performance than GPT-4o visual caption as shown in Table 4. This demonstrates that audio captions enable GPT-4o to process audio-visual information more effectively than visual captions. This advantage may be due to the greater information loss in visual captions compared to audio captions when handling audio-visual content. Limitations of Open-Source MLLMs in Connecting Audio and Visual Information. The Audiocaps dataset [29] introduces audio captions, i.e., audio-text pairs (including animal sound audios), which are used to train MLLMs for learning audio recognition abilities by OneLLM [23], Unified-IO-2 [47], VideoLLaMA2 [15], and NExT-GPT [77]. In addition, they adopt the image-text paired data for training vision comprehension ability. The results on our AV-Odyssey Bench showcase that the audiotext-vision training pipeline is insufficient to bridge audio and vision modalities and truly learn the audio-visual information integration capability. Figure 6. An example of audio understanding error. More examples are provided in the Appendix. select 4 error instances for human experts to annotate the error reasons, resulting in total of 104 instances with humanannotated error reasons. The distribution of these errors is illustrated in Figure 5. Detailed analyses for each case are provided in the Appendix. Perception Understanding Errors (81%). Perception understanding errors, which include audio understanding errors (63%), vision understanding errors (10%), and text understanding errors (8%), form the majority of the errors. Audio Understanding Errors stand out among all the error types with an error rate of 63%. An example is illustrated in Figure 6, where the content of the audio clip is misidentified, leading to an incorrect answer. This result indicates that the major bottleneck of audio-visual information integration is still in the perception ability in audio. This is in line with the hypothesis induced by the DeafTest results that if an MLLM has shortage in fundamental listening ability, it will struggle in audio-visual information integration. Reasoning Errors (13%). In these cases, Gemini 1.5 Pro successfully extracts information from both audio and visual inputs but still produced an incorrect answer due to incorrect reasoning. Other Errors (6%). Other errors, primarily due to rejected answers, arise from various factors. For instance, content may be mistakenly flagged for security reasons, preventing the model from providing an answer. 4.3. Error Analysis 5. Conclusion In this section, we focus on the errors of Gemini 1.5 Pro to analyze the underlying causes. For each task, we randomly In this work, we introduce AV-Odyssey Bench, comprehensive audio-visual benchmark designed to evaluate the 8 capabilities of MLLMs in understanding audio-visual information. Our AV-Odyssey Bench includes 4,555 meticulously crafted multiple-choice problems, each designed to challenge models in integrating information from both visual and audio cues. Through benchmarking range of closed-source and open-source models, we uncover the current limitations of MLLMs in effectively understanding audio-visual inputs. We hope that AV-Odyssey Bench will serve as valuable resource for the community, facilitating the advancement of MLLMs and ultimately leading to more powerful and human-like audio-visual understanding."
        },
        {
            "title": "References",
            "content": "[1] Abdulvahap. Music instrument sounds for classificahttps : / / www . kaggle . com / datasets / tion. abdulvahap / music - instrunment - sounds - for-classification. 6 [2] Erhan AKBAL, Turker TUNCER, and Sengul. Vehicle interior sound dataset, 2021. Gtzan dataset - music genre classifica- [3] Andrada. https : / / www . kaggle . com / datasets / tion. andradaolteanu / gtzan - dataset - music - genre-classification. [4] Emrah AYDEMIR. Gunshot audio dataset. https : / / www . kaggle . com / datasets / emrahaydemr / gunshot-audio-dataset. 6 [5] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 3 [6] Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, et al. Deepseek llm: Scaling opensource language models with longtermism. arXiv preprint arXiv:2401.02954, 2024. 3 [7] Tom Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. 3 [8] Jie Cao and Jing Xiao. An augmented benchmark dataset for geometric question answering through dual parallel text encoding. In Proceedings of the 29th International Conference on Computational Linguistics, pages 15111520, 2022. 1, 3 [9] Jiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang, Lingbo Liu, Eric Xing, and Liang Lin. Geoqa: geometric question answering benchmark towards multimodal numerical reasoning. arXiv preprint arXiv:2105.14517, 2021. 3 [10] Jiaqi Chen, Tong Li, Jinghui Qin, Pan Lu, Liang Lin, Chongyu Chen, and Xiaodan Liang. Unigeo: Unifying geometry logical reasoning via reformulating mathematical expression. arXiv preprint arXiv:2212.02746, 2022. 1 [11] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. large language model as unified interface Minigpt-v2: arXiv preprint for vision-language multi-task learning. arXiv:2310.09478, 2023. [12] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Shikra: Unleashing multiarXiv preprint Feng Zhu, and Rui Zhao. modal llms referential dialogue magic. arXiv:2306.15195, 2023. 3 [13] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024. 6 [14] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. 6 [15] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatialtemporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. 3, 7, 8, 2 [16] Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and Jingren Zhou. Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models. arXiv preprint arXiv:2311.07919, 2023. [17] Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, et al. Qwen2-audio technical report. arXiv preprint arXiv:2407.10759, 2024. 6 [18] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards generalpurpose vision-language models with instruction tuning. arXiv preprint arXiv:2305.06500, 2023. 3 [19] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, et al. In Scaling egocentric vision: The epic-kitchens dataset. Proceedings of the European conference on computer vision (ECCV), pages 720736, 2018. 6 [20] Michael Defferrard, Kirell Benzi, Pierre Vandergheynst, and Xavier Bresson. Fma: dataset for music analysis. arXiv preprint arXiv:1612.01840, 2016. 6 [21] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Rongrong Ji. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. 1, 3 [22] Chaoyou Fu, Haojia Lin, Zuwei Long, Yunhang Shen, Meng Zhao, Yifan Zhang, Xiong Wang, Di Yin, Long Ma, Xiawu Zheng, et al. Vita: Towards open-source interactive omni multimodal llm. arXiv preprint arXiv:2408.05211, 2024. 3, 7, [23] Jiaming Han, Kaixiong Gong, Yiyuan Zhang, Jiaqi Wang, Kaipeng Zhang, Dahua Lin, Yu Qiao, Peng Gao, and Xiangyu Yue. Onellm: One framework to align all modalities with language. In Proceedings of the IEEE/CVF Conference 9 on Computer Vision and Pattern Recognition, pages 26584 26595, 2024. 1, 3, 7, 8, 2 [24] Toni Heittola, Annamaria Mesaros, and Tuomas Virtanen. TAU Urban Acoustic Scenes 2019, Development dataset, 2019. 6 [25] Anwen Hu, Yaya Shi, Haiyang Xu, Jiabo Ye, Qinghao Ye, Ming Yan, Chenliang Li, Qi Qian, Ji Zhang, and Fei Huang. mplug-paperowl: Scientific diagram analysis with the multimodal large language model. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 6929 6938, 2024. 3 [26] EH Huizing. The early descriptions of the so-called tuningfork tests of weber, rinne, schwabach, and bing: Iii. the development of the schwabach and bing tests. ORL, 37(2):92 96, 1975. 2, [27] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 1, 2, 3, 7 [28] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017. 6 [29] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim. Audiocaps: Generating captions for audios in the wild. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 119132, 2019. 8 [30] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. 1 [31] Bohao Li, Yuying Ge, Yi Chen, Yixiao Ge, Ruimao Zhang, and Ying Shan. Seed-bench-2-plus: Benchmarking multimodal large language models with text-rich visual comprehension. arXiv preprint arXiv:2404.16790, 2024. 3 [32] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench: Benchmarking multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1329913308, 2024. 1, [33] Guangyao Li, Yake Wei, Yapeng Tian, Chenliang Xu, JiLearning to answer questions Rong Wen, and Di Hu. In Proceedings of the in dynamic audio-visual scenarios. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1910819118, 2022. 2, 3, 5 [34] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with ICML, frozen image encoders and large language models. 2023. 3 [35] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Evaluating object hallucinaarXiv preprint Zhao, and Ji-Rong Wen. tion in large vision-language models. arXiv:2305.10355, 2023. 5 [36] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. In European Conference on Computer Vision, 2024. 3 [37] Yizhi Li, Ge Zhang, Yinghao Ma, Ruibin Yuan, Kang Zhu, Hangyu Guo, Yiming Liang, Jiaheng Liu, Jian Yang, Siwei Wu, et al. Omnibench: Towards the future of universal omnilanguage models. arXiv preprint arXiv:2409.15272, 2024. 2, 3, [38] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2668926699, 2024. 6 [39] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023. 3 [40] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023. 3 [41] Jiajun Liu, Yibing Wang, Hanghang Ma, Xiaoping Wu, Xiaoqi Ma, xiaoming Wei, Jianbin Jiao, Enhua Wu, and Jie Hu. Kangaroo: powerful video-language model supporting long-context video input. arXiv preprint arXiv:2408.15542, 2024. 3 [42] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023. 1, 3 [43] Steven R. Livingstone. Ravdess emotional speech auhttps : / / www . kaggle . com / datasets / dio. uwrfkaggler / ravdess - emotional - speech - audio. 6 [44] Eu Jin Lok. Crowd sourced emotional multimodal actors dataset. https://www.kaggle.com/datasets/ ejlok1/cremad, . [45] Eu Jin Lok. Surrey audio-visual expressed emotion (savee). https://www.kaggle.com/datasets/ejlok1/ surrey - audiovisual - expressed - emotion - savee, . [46] Eu Jin Lok. Toronto emotional speech set (tess). https:// www.kaggle.com/datasets/ejlok1/torontoemotional-speech-set-tess, . 6 [47] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. Unified-io: unified model for vision, language, and multi-modal tasks. In The Eleventh International Conference on Learning Representations, 2022. 3, 7, 8, 2 [48] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning. arXiv preprint arXiv:2105.04165, 2021. 1 [49] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. 1, 10 [50] Tengchao Lv, Yupan Huang, Jingye Chen, Lei Cui, Shuming Ma, Yaoyao Chang, Shaohan Huang, Wenhui Wang, Li Dong, Weiyao Luo, et al. Kosmos-2.5: multimodal literate model. arXiv preprint arXiv:2309.11419, 2023. 3 [51] Jeannette Shijie Ma. Eating sound collection. https:// www.kaggle.com/datasets/mashijie/eatingsound-collection. 6 [52] Potsawee Manakul, Guangzhi Sun, Warit Sirichotedumrong, Kasima Tharnpipitchai, and Kunat Pipatanakul. Enhancing low-resource language and instruction following capabilities of audio language models. arXiv preprint arXiv:2409.10999, 2024. 6 [53] Microsoft. Birdclef 2020. https://www.imageclef. org/BirdCLEF2020. 6 [54] OpenAI. Hello gpt-4o. https : / / openai . com / index/hello-gpt-4o/. 4 [55] OpenAI. Gpt-4 technical report. arxiv 2303.08774. View in Article, 2(5), 2023. 1, 3 [56] Fabian Ostermann, Igor Vatolkin, and Martin Ebeling. Aam: dataset of artificial audio multitracks for diverse music information retrieval tasks. EURASIP Journal on Audio, Speech, and Music Processing, 2023(1):13, 2023. 6 [57] Andrew Owens, Phillip Isola, Josh McDermott, Antonio Torralba, Edward Adelson, and William Freeman. Visually indicated sounds. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 24052413, 2016. [58] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023. 3 [59] Rushi Balaji Putthewad. Sound classification of https : / / www . kaggle . com / animal voice. datasets / rushibalajiputthewad / sound - classification-of-animal-voice. 6 [60] Alec Radford. Improving language understanding by generative pre-training. 2018. 3 [61] Pooyan Rahmanzadehgervi, Logan Bolton, Mohammad Reza Taesiri, and Anh Totti Nguyen. Vision language models are blind. arXiv preprint arXiv:2407.06581, 2024. 2 [62] Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. Timechat: time-sensitive multimodal large language model for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1431314323, 2024. [63] Minjoon Seo, Hannaneh Hajishirzi, Ali Farhadi, Oren Etzioni, and Clint Malcolm. Solving geometry problems: Combining text and diagram interpretation. In Proceedings of the 2015 conference on empirical methods in natural language processing, pages 14661476, 2015. 3 [64] Xiaoqian Shen, Yunyang Xiong, Changsheng Zhao, Lemeng Wu, Jun Chen, Chenchen Zhu, Zechun Liu, Fanyi Xiao, Balakrishnan Varadarajan, Florian Bordes, Zhuang Liu, Hu Xu, Hyunwoo J. Kim, Bilge Soran, Raghuraman Krishnamoorthi, Mohamed Elhoseiny, and Vikas Chandra. Longvu: Spatiotemporal adaptive compression for long video-language understanding. arXiv preprint arXiv:2410.17434, 2024. 3 [65] Kazuki Shimada, Archontis Politis, Parthasaarathy Sudarsanam, Daniel Krause, Kengo Uchida, Sharath Adavanne, Aapo Hakala, Yuichiro Koyama, Naoya Takahashi, Shusuke Takahashi, et al. Starss23: An audio-visual dataset of spatial recordings of real scenes with spatiotemporal annotations of sound events. Advances in Neural Information Processing Systems, 36, 2024. 6 [66] Auston Sterling, Justin Wilson, Sam Lowe, and Ming Lin. Isnn: Impact sound neural network for audio-visual object In Proceedings of the European Conference classification. on Computer Vision (ECCV), pages 555572, 2018. 6 [67] Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. Pandagpt: One model to instruction-follow them all. arXiv preprint arXiv:2305.16355, 2023. 3, 7, 2 [68] Guangzhi Sun, Wenyi Yu, Changli Tang, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, Yuxuan Wang, and Chao Zhang. video-salmonn: Speech-enhanced audio-visual large language models. arXiv preprint arXiv:2406.15704, 2024. 2, 3 [69] Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, and Chao Zhang. Salmonn: Towards generic hearing abilities for large language models. arXiv preprint arXiv:2310.13289, 2023. [70] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 1, 2, 3, 7 [71] Reka Team, Aitor Ormazabal, Che Zheng, Cyprien de Masson dAutume, Dani Yogatama, Deyu Fu, Donovan Ong, Eric Chen, Eugenie Lamprecht, Hai Pham, et al. Reka core, flash, and edge: series of powerful multimodal language models. arXiv preprint arXiv:2404.12387, 2024. 2, 3, 7 [72] Yapeng Tian, Jing Shi, Bochen Li, Zhiyao Duan, and Chenliang Xu. Audio-visual event localization in unconstrained videos. In Proceedings of the European conference on computer vision (ECCV), pages 247263, 2018. 6 [73] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 3 [74] Shuhei Tsuchida, Satoru Fukayama, Masahiro Hamasaki, and Masataka Goto. Aist dance video database: Multi-genre, multi-dancer, and multi-camera database for dance informaIn Proceedings of the 20th International tion processing. Society for Music Information Retrieval Conference, ISMIR 2019, Delft, Netherlands, 2019. 6 [75] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 6 [76] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et al. Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079, 2023. 3 crete sequence modeling. arXiv preprint arXiv:2402.12226, 2024. 1, 3, 7, 2 [90] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. 7, 2 [91] Pan Zhang, Xiaoyi Dong Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang Zhang, Haodong Duan, Hang Yan, et al. Internlmxcomposer: vision-language large model for advanced text-image comprehension and composition. arXiv preprint arXiv:2309.15112, 2023. 3 [92] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. 3 [93] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pages 169186. Springer, 2025. 1, 3 [94] Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun. Llavar: Enhanced visual instruction tuning for text-rich image understanding, 2023. [95] Yu Zhang, Changhao Pan, Wenxiang Guo, Ruiqi Li, Zhiyuan Zhu, Jialei Wang, Wenhao Xu, Jingyu Lu, Zhiqing Hong, Chuxin Wang, et al. Gtsinger: global multi-technique singing corpus with realistic music scores for all singing tasks. arXiv preprint arXiv:2409.13832, 2024. 6 [96] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 3 [77] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal llm. arXiv preprint arXiv:2309.05519, 2023. 3, 7, 8, 2 [78] Jingyi Xu, Hieu Le, Vu Nguyen, Viresh Ranjan, and Dimitris Samaras. Zero-shot object counting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1554815557, 2023. 1 [79] Le Xue, Manli Shu, Anas Awadalla, Jun Wang, An Yan, Senthil Purushwalkam, Honglu Zhou, Viraj Prabhu, Yutong Dai, Michael Ryoo, et al. xgen-mm (blip-3): family of open large multimodal models. arXiv preprint arXiv:2408.08872, 2024. 6 [80] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441, 2023. [81] Pinci Yang, Xin Wang, Xuguang Duan, Hong Chen, Runze Hou, Cong Jin, and Wenwu Zhu. Avqa: dataset for audiovisual question answering on videos. In Proceedings of the 30th ACM international conference on multimedia, pages 34803491, 2022. 1, 2, 3 [82] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint arXiv:2309.17421, 9(1):1, 2023. 1 [83] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. 6 [84] yash songs. Emotify bhaskar. classificaiton emotion https : / / www . kaggle . com / in datasets / yash9439 / emotify - emotion - classificaiton-in-songs. 6 - [85] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023. 3 [86] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. 1 [87] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for exIn Proceedings of the IEEE/CVF Conference pert agi. on Computer Vision and Pattern Recognition, pages 9556 9567, 2024. 3 [88] Heeseung Yun, Youngjae Yu, Wonsuk Yang, Kangil Lee, and Gunhee Kim. Pano-avqa: Grounded audio-visual quesIn Proceedings of the tion answering on 360deg videos. IEEE/CVF International Conference on Computer Vision, pages 20312041, 2021. 2, [89] Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan, Ge Zhang, Linyang Li, et al. Anygpt: Unified multimodal llm with dis12 AV-Odyssey Bench: Can Your Multimodal LLMs Really Understand Audio-Visual Information?"
        },
        {
            "title": "Contents in Appendix",
            "content": "A. Data Distribution B. Breakdown Results C. Case Study A. Data Distribution 1 3 In this section, we present the detailed data distribution of our AV-Odyssey Bench in Table 5. Our AV-Odyssey bench consists of 26 tasks covering wide range of task categories. We will make all the data and evaluation codes public. Table 5. Detailed task statistics in AV-Odyssey Bench. Task ID Task Name Task Category Class Number 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 Instrument Recognition Singer Recognition Gunshot Recognition Bird Recognition Animal Recognition Transportation Recognition Material Recognition Scene Recognition Hazard Recognition Action Recognition Eating Sound Recognition Speech Sentiment Analysis Meme Understanding Music Sentiment Analysis Music Genre Classification Dance and Music Matching Film and Music Matching Music Score Matching Audio 3D Angle Estimation Audio Distance Estimation Audio Time Estimation Audio-Visual Synchronization Action Sequencing Hallucination Evaluation Action Prediction Action Tracing Timbre Timbre Timbre Timbre Timbre Timbre Timbre Timbre Timbre Timbre Timbre Tone Tone Melody Melody Melody Melody Melody Space Space Time Time Time Hallucination Intricacy Intricacy 28 20 13 39 13 8 10 8 8 20 20 7 N/A 7 8 10 5 N/A N/A N/A N/A N/A N/A 19 N/A N/A 200 200 200 200 200 200 200 200 108 196 200 200 20 197 200 200 200 200 20 20 200 200 200 200 199 1 B. Breakdown Results In this section, we provide detailed results of evaluated methods on our proposed AV-Odyssey Bench, as demonstrated in Table 6 and Table 7. Table 6. Evaluation results of various MLLMs in Timbre part of AV-Odyssey Bench. The best (second best) is in bold (underline). The corresponding brackets for each task indicate the number of associated questions. Model Unified-IO-2 [47] Unified-IO-2 XL [47] Unified-IO-2 XXL [47] OneLLM [23] PandaGPT [67] Video-llama [90] VideoLLaMA2 [15] AnyGPT [89] NExT-GPT [77] VITA [22] Gemini 1.5 Flash [70] Gemini 1.5 Flash-8B [70] Gemini 1.5 Pro [70] Reka Core [71] Reka Flash [71] Reka Edge [71] GPT-4o visual caption [27] GPT-4o audio caption [27] u e c S o LLM Size 1B 3B 7B 7B 7B 7B 7B 7B 7B 8 7B - - - 67B 21B 7B - - Instrument Recognition Singer Recognition Gunshot Recognition Bird Recognition Animal Recognition Transportation Recognition Material Recognition Scene Recognition Hazard Recognition Action Recognition Eating Sound Recognition (200) (200) (200) (200) (200) (200) (200) (200) (108) (196) (200) 20.5 20.0 29.5 26.0 20.0 22.5 22.5 22.5 21.0 22.0 24.5 16.5 33.0 32.5 20.0 21.5 33.0 40. 22.5 23.5 24.0 21.5 21.5 24.5 24.0 28.5 23.5 20.5 24.0 22.5 26.0 20.0 22.5 24.0 30.5 38.0 25.5 24.0 23.5 27.0 23.0 27.0 27.0 28.0 25.5 24.5 23.5 24.0 29.0 26.5 26.5 30.5 24.0 27.5 18.5 20.5 29.0 26.0 17.5 26.5 17.0 17.5 21.5 21.5 17.0 19.0 25.0 25.0 26.0 20.0 26.5 26. 27.0 27.5 23.5 22.0 26.0 27.0 23.5 24.0 25.5 27.5 32.5 28.0 25.5 24.0 28.5 19.5 43.0 45.0 26.5 26.0 25.5 20.0 26.5 23.5 27.5 25.5 25.5 25.0 26.0 26.5 26.0 27.0 26.5 22.5 42.0 42.0 23.0 27.5 30.5 29.5 28.0 28.0 26.5 23.0 21.0 23.5 22.5 27.0 29.5 30.0 26.5 20.5 32.5 27. 28.0 30.0 26.5 24.5 27.0 25.0 26.5 28.0 24.0 28.5 29.5 29.0 30.0 27.0 29.0 25.5 39.0 41.0 21.3 19.4 23.1 26.9 23.1 25.0 19.4 25.9 19.4 21.3 34.3 26.9 38.0 25.0 28.7 25.9 49.1 42.6 20.9 19.9 27.0 23.0 21.4 26.0 23.0 20.4 23.0 19.4 48.0 32.7 57.7 34.2 22.4 23.5 67.3 62. 26.5 26.5 25.5 29.5 24.5 25.5 25.5 27.5 24.0 29.5 21.5 24.5 22.5 21.5 25.0 29.0 30.5 35.5 Table 7. Evaluation results of various MLLMs in Time, Melody, Space. Time, Hallucination, and Intricacy parts of AV-Odyssey Bench. The best (second best) is in bold (underline). The corresponding brackets for each task indicate the number of associated questions. Model u e Unified-IO-2 [47] Unified-IO-2 XL [47] Unified-IO-2 XXL [47] OneLLM [23] PandaGPT [67] Video-llama [90] VideoLLaMA2 [15] AnyGPT [89] NExT-GPT [77] VITA [22] LLM Size 1B 3B 7B 7B 7B 7B 7B 7B 7B 8 7B u e C Gemini 1.5 Flash [70] Gemini 1.5 Flash-8B [70] Gemini 1.5 Pro [70] Reka Core [71] Reka Flash [71] Reka Edge [71] GPT-4o visual caption [27] GPT-4o audio caption [27] - - - 67B 21B 7B - - Tone Melody Space Time Hallucination Intricacy Speech Sentiment Analysis Meme Understanding Music Sentiment Analysis Music Genre Classification Dance and Music Matching Film and Music Matching Music Score Matching Audio 3D Angle Estimation Audio Distance Estimation Audio Time Estimation Audio-Visual Synchronization Action Sequencing Hallucination Evaluation Action Prediction Action Tracing (20) 15.0 30.0 50.0 45.0 45.0 45.0 45.0 40.0 45.0 25.0 30.0 40.0 35.0 25.0 45.0 30.0 25.0 30.0 (20) 15.0 15.0 15.0 30.0 45.0 15.0 15.0 15.0 15.0 20.0 30.0 15.0 40.0 20.0 15.0 15.0 30.0 35.0 (200) (200) (200) (200) (199) (195) 28.0 26.5 28.0 31.5 18.5 28.5 28.5 30.5 31.5 26.5 27.5 31.0 30.0 30.0 30.0 30.0 21.5 23.5 25.5 25.5 25.0 29.5 26.0 23.5 26.5 28.0 24.0 25.5 23.5 25.5 24.5 25.5 25.5 25.5 22.5 25. 27.0 24.0 27.5 27.0 27.0 26.5 26.5 29.0 31.0 27.0 25.0 26.0 28.5 24.0 27.0 21.0 32.5 33.5 30.0 31.5 24.5 25.5 28.0 25.0 33.0 29.0 28.5 31.0 28.5 29.0 20.5 24.0 31.5 22.5 23.0 25.0 27.1 35.7 33.2 41.7 19.6 28.6 28.6 21.1 20.6 34.2 27.6 25.6 32.2 33.7 19.1 38.2 32.2 30. 33.8 33.8 34.4 34.9 28.2 32.8 40.5 30.3 26.7 39.5 34.9 34.9 33.8 34.9 29.2 35.4 25.6 22.0 (200) 24.5 23.0 23.0 26.0 23.5 23.0 26.0 25.5 21.5 24.5 23.5 24.5 29.5 28.5 24.5 20.5 26.0 28.0 (20) 20.0 25.0 20.0 20.0 20.0 15.0 20.0 20.0 15.0 45.0 40.0 25.0 50.0 20.0 20.0 20.0 55.0 70.0 (97) 27.9 26.9 23.9 20.8 21.6 25.8 26.8 23.4 23.7 26.8 21.3 25.9 25.4 22.8 30.5 24.9 24.4 24.4 (200) (200) (200) (200) 31.0 30.5 31.5 23.5 28.0 24.0 29.0 29.5 26.0 26.0 31.0 33.0 42.5 24.5 29.5 24.5 48.0 56.5 27.5 27.0 27.5 26.5 27.0 20.0 25.5 25.5 28.0 27. 27.5 27.5 28.0 27.5 27.5 27.5 27.0 27.5 24.5 22.5 23.5 18.0 26.0 28.0 20.5 26.0 28.0 24.5 28.0 24.5 29.0 25.5 24.5 24.0 23.5 22.5 32.5 31.5 24.5 18.5 32.5 25.0 30.5 26.0 31.0 33.5 32.5 32.0 28.5 30.0 25.5 30.0 34.5 32.5 C. Case Study"
        },
        {
            "title": "List of Case Study Figures",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "7 Timbre, Instrument Recognition: Audio Understanding Error . . . . . . . . . . . . . . . . . . . . . . . . . . .\n.\n8 Timbre, Singer Recognition: Audio Understanding Error\n9 Timbre, Gunshot Recognition: Audio Understanding Error\n.\n10 Timbre, Bird Recognition: Audio Understanding Error, Vision Understanding Error . . . . . . . . . . . . . . .\n.\n11 Timbre, Animal Recognition: Audio Understanding Error . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12 Timbre, Transportation Recognition: Audio Understanding Error . . . . . . . . . . . . . . . . . . . . . . . . .\n.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13 Timbre, Material Recognition: Text Understanding Error\n.\n14 Timbre, Scene Recognition: Audio Understanding Error\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n.\n15 Timbre, Hazard Recognition: Audio Understanding Error . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16 Timbre, Action Recognition: Audio Understanding Error . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n.\n. . . . . . . . . . . . . . . . . . . . . . . . .\n17 Timbre, Eating Sound Recognition: Audio Understanding Error\n18 Tone, Speech Recognition: Audio Understanding Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n.\n19 Tone, Meme Recognition: Reject to Answer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n.\n20 Melody, Music Sentiment Recognition: Audio Understanding Error\n. . . . . . . . . . . . . . . . . . . . . .\n.\n21 Melody, Music Genre Classification: Audio Understanding Error . . . . . . . . . . . . . . . . . . . . . . . .\n22 Melody, Dance and Music Matching: Reject to Answer . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23 Melody, Film and Music Matching: Reasoning Error\n24 Melody, Music Score Matching: Audio Understanding Error, Reasoning Error . . . . . . . . . . . . . . . . .\n.\n25 Space, Audio 3D Angle Estimation: Vision Understanding Error, Audio Understanding Error . . . . . . . . . .\n26 Space, Audio Distance Estimation: Audio Understanding Error . . . . . . . . . . . . . . . . . . . . . . . . .\n.\n27 Time, Audio Time Estimation: Audio Understanding Error\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28 Time, Audio-Visual Synchronization: Audio Understanding Error, Vision Understanding Error . . . . . . . . .\n29 Time, Action Sequencing: Audio Understanding Error, Reasoning Error . . . . . . . . . . . . . . . . . . . . .\n30 Hallucination, Hallucination Evaluation: Audio Understanding Error . . . . . . . . . . . . . . . . . . . . . .\n.\n31 Intricacy, Action Prediction: Audio Understanding Error, Reasoning Error . . . . . . . . . . . . . . . . . . . .\n.\n32 Intricacy, Action Tracing: Text Understanding Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . 4 . 5 . 6 . 7 . 8 . . 9 . 10 . 11 . 12 . 13 . 14 . 15 . 16 . 17 . 18 . 19 . 20 . 21 . 22 . 23 . 24 . 25 . 26 . 27 . 28 . 29 3 Figure 7. sampled error case in the instrument recognition task. 4 Figure 8. sampled error case in the singer recognition task. 5 Figure 9. sampled error case in the gunshot recognition task. 6 Figure 10. sampled error case in the bird recognition task. 7 Figure 11. sampled error case in the animal recognition task. 8 Figure 12. sampled error case in the transportation recognition task. 9 Figure 13. sampled error case in the material recognition task. 10 Figure 14. sampled error case in the scene recognition task. 11 Figure 15. sampled error case in the hazard recognition task. 12 Figure 16. sampled error case in the action recognition task. 13 Figure 17. sampled error case in the eating sound recognition task. 14 Figure 18. sampled error case in the speech sentiment analysis task. 15 Figure 19. sampled error case in the meme understanding task. 16 Figure 20. sampled error case in the music sentiment analysis task. 17 Figure 21. sampled error case in the music genre classification task. 18 Figure 22. sampled error case in the dance and music matching task. 19 Figure 23. sampled error case in the film and music matching task. 20 Figure 24. sampled error case in the music score matching task. 21 Figure 25. sampled error case in the audio 3D angle estimation task. 22 Figure 26. sampled error case in the audio distance estimation task. 23 Figure 27. sampled error case in the audio time estimation task. 24 Figure 28. sampled error case in the audio-visual synchronization task. 25 Figure 29. sampled error case in the action sequencing task. 26 Figure 30. sampled error case in the hallucination evaluation task. 27 Figure 31. sampled error case in the action prediction task. 28 Figure 32. sampled error case in the action tracing task."
        }
    ],
    "affiliations": [
        "CUHK (SZ)",
        "CUHK MMLab",
        "Stanford University",
        "UC Berkeley",
        "Yale University"
    ]
}