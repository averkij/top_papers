{
    "paper_title": "Free$^2$Guide: Gradient-Free Path Integral Control for Enhancing Text-to-Video Generation with Large Vision-Language Models",
    "authors": [
        "Jaemin Kim",
        "Bryan S Kim",
        "Jong Chul Ye"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion models have achieved impressive results in generative tasks like text-to-image (T2I) and text-to-video (T2V) synthesis. However, achieving accurate text alignment in T2V generation remains challenging due to the complex temporal dependency across frames. Existing reinforcement learning (RL)-based approaches to enhance text alignment often require differentiable reward functions or are constrained to limited prompts, hindering their scalability and applicability. In this paper, we propose Free$^2$Guide, a novel gradient-free framework for aligning generated videos with text prompts without requiring additional model training. Leveraging principles from path integral control, Free$^2$Guide approximates guidance for diffusion models using non-differentiable reward functions, thereby enabling the integration of powerful black-box Large Vision-Language Models (LVLMs) as reward model. Additionally, our framework supports the flexible ensembling of multiple reward models, including large-scale image-based models, to synergistically enhance alignment without incurring substantial computational overhead. We demonstrate that Free$^2$Guide significantly improves text alignment across various dimensions and enhances the overall quality of generated videos."
        },
        {
            "title": "Start",
            "content": "Free2Guide: Gradient-Free Path Integral Control for Enhancing Text-to-Video Generation with Large Vision-Language Models Jaemin Kim, Bryan Kim, Kim Jaechul Graduate School of AI, KAIST {kjm981995, bryanswkim, jong.ye}@kaist.ac.kr"
        },
        {
            "title": "Jong Chul Ye",
            "content": "4 2 0 2 6 2 ] . [ 1 1 4 0 7 1 . 1 1 4 2 : r Baseline Free2Guide Baseline Free2Guide \"A person is strumming guitar\" \"A dog and horse\" Baseline Free2Guide Baseline Free2Guide \"A happy fuzzy panda playing guitar nearby campfire, snow mountain in the background\" \"The bund Shanghai, vibrant color\" Figure 1. Representative video results using Free2Guide, novel framework that enables train-Free, gradient-Free video Guidance leveraging Large Vision-Language Model. Best viewed with Acrobat Reader. Click each image to play the video clip."
        },
        {
            "title": "Abstract",
            "content": "Diffusion models have achieved impressive results in generative tasks like text-to-image (T2I) and text-to-video (T2V) synthesis. However, achieving accurate text alignment in T2V generation remains challenging due to the complex temporal dependency across frames. Existing reinforcement learning (RL)-based approaches to enhance text alignment often require differentiable reward functions or are constrained to limited prompts, hindering their scalability and applicability. In this paper, we propose Free2Guide, novel gradient-free framework for aligning generated videos with text prompts without requiring additional model training. Leveraging principles from path integral control, Free2Guide approximates guidance for diffusion models using non-differentiable reward functions, thereby enabling the integration of powerful black-box Large Vision-Language Models (LVLMs) as reward model. Additionally, our framework supports the flexible ensembling of multiple reward models, including largescale image-based models, to synergistically enhance alignment without incurring substantial computational overhead. We demonstrate that Free2Guide significantly improves text alignment across various dimensions and enhances the overall quality of generated videos. Our results and code are available at our project page 1. 1. Introduction Diffusion models [21, 30, 31, 33] have emerged as powerful and versatile tools for generative modeling, achieving state-of-the-art results in tasks that require fine-grained control over content generation, such as text-to-image (T2I) [30] and text-to-video (T2V) generation [7, 15]. However, achieving perfect alignment with text conditions remains significant challenge [12]. This issue becomes even more challeng1https://kjm981995.github.io/free2guide/ ing in the video domain, where maintaining text-relevant content across frames requires handling complex temporal dependencies, often resulting in misalignment between generated frames and the given text prompt. In the image domain, reinforcement learning (RL)-based methods have been introduced to address challenges in textguided T2I generation by using reward models to estimate human preferences within diffusion models [2, 10, 43, 44]. Previous works mainly focus on either directly fine-tuning the diffusion model with gradients derived from reward function [6, 27, 28] or employing an RL-based policy gradient approach [2, 10]. While these fine-tuning methods can effectively improve sample alignment, they have notable limitations: the former requires differentiable reward function, while the latter is typically limited to only few prompts. Directly adapting these text alignment approaches for the video domain presents two main challenges. First, they often require dedicated video-specific reward function or additional training on curated video datasets. Collecting large-scale, aligned text-video datasets is far more complex than gathering image data, and developing reward functions tailored to video tasks is similarly difficult. Second, even with trained reward models for the video domain, additional challenges such as substantial memory demands for backpropagation emerge, which grow proportionally as model scale increases (i.e., scaling laws) [19]. An alternative approach involves using differential reward models during inference time to guide diffusion models without fine-tuning model parameters [38]. However, guidance-based methods still require differentiable reward function, which excludes non-differentiable options like state-of-the-art visual-language model APIs or human preference-based metrics. To address this, recent studies have explored stochastic optimization to guide diffusion models during the sampling process using non-differentiable objective functions [17], and concurrent research extends this idea within the image domain [46, 47]. However, such methods cannot be directly applied to video diffusion models due to the complex temporal dependencies involved. Thus, we propose method to extend stochastic optimization to the video domain, leveraging the temporal understanding capabilities of Large Vision-Language Models (LVLMs). Despite the advantages of using powerful black-box models, their application in the context of non-differentiable reward functions remains underexplored in previous work. Specifically, we introduce Free2Guide, novel framework for aligning text prompts in video generation that does not require gradients from the reward function. Drawing on principles from path integral control, Free2Guide approximates guidance to align generated videos with text prompts, regardless of the reward functions differentiability. As such, Free2Guide enables usage of powerful black-box visionlanguage model as reward models, improving text-video alignment illustrated in Fig. 1. In addition, our framework enables flexible combinations of reward models by eliminating the need for computationally intensive fine-tuning and backpropagation. We explore several combinatorial approaches to collaborate LVLMs with existing large-scale image-based models. Extensive experiments show that our methods improve text alignment and the quality of the generated videos. Our contributions are summarized as follows: We introduce Free2Guide, novel framework for aligning generated videos with text prompts without requiring gradients from the reward function. To the best of our knowledge, Free2Guide is the first gradient-free guidance approach for text-to-video generation that requires no additional training. We adapt non-differentiable LVLM APIs to enhance textvideo alignment and develop an effective ensemble approach to leverage large-scale image-based models for guiding video generation. 2. Related Work Text-to-Video diffusion model Text-to-Video diffusion models (e.g., LaVie [39], VideoCrafter [3, 4]) employ diffusion processes to generate coherent video sequences from textual prompts [13, 16, 24]. However, notable limitation is that video diffusion models often struggle to generate videos that align accurately with the given text prompts, specifically in terms of spatial relationships (e.g., on B) and the representation of temporal style (e.g., zooming in). Diffusion model with LVLM feedback While several approaches have been proposed to improve the diffusion generation process with Large Language Models (LLMs) [11, 22, 42, 48], there has been limited exploration of methods leveraging Large Vision Language Models (LVLMs) that can also handle image domains. Recent works explore the integration of LVLMs as feedback mechanism to diffusion models to enhance control and guide diffusion processes. For instance, RPG [45] utilize an LVLM as planner to manipulate cross-attention layers in the diffusion model, while Demon [46] demonstrates that LVLMs can guide diffusion in alignment with given persona. In contrast, our approach takes advantage of LVLMs capability to understand frame-to-frame dynamics, applying this strength in the video domain to improve text-video alignment. Human Preference Alignment via Reward Models Aligning with human preferences has improved generative quality in diffusion models through fine-tuning diffusion model using reward model gradients (DRaFT [6], AlignProp [27]) or policy gradients (DDPO [2], DPOK [10]). On the other hand, DOODL [38] and Demon [46] guide the 2 Figure 2. Overall pipeline of Free2Guide, leveraging path integral control to enhance text-video alignment without requiring reward gradient. During the sampling process, Free2Guide generates multiple denoised video samples and evaluate text alignment using non-differentiable Large Vision-Language Models (LVLMs). denoising process to achieve text alignment without training diffusion models. Note, however, that the previously mentioned methods all focus on the image domain. Recent work VADER [28] fine-tunes pre-trained video diffusion model using gradients of reward models for aesthetic and text-aligned generation. While this approach shows promising results using video reward model, it demands substantial memory and does not utilize LVLMs. We address these limitations by proposing text-video alignment method that approximates image reward gradients without fine-tuning. Zeroth order gradient approximation Zeroth-order gradients, or gradient-free approaches, approximate gradients of non-differentiable functions by evaluating multiple points [23, 25]. In diffusion-based inverse problems, methods like EnKF [47] and SCG [17] leverage gradient-free approximations to guide sampling based on non-differentiable or black-box forward models. However, there is lack of research specifically focused on gradient-free approaches to guide sampling for video diffusion models. In video diffusion models, approximating black-box reward model with zeroth-order gradient is advantageous, as gradients of reward are unavailable and the high-dimensional space of video data imposes memory limitations. 3. Preliminaries 3.1. Video Latent diffusion model Video Latent diffusion model (VLDM) learns stochastic process by iteratively denoising random noise generated by the forward diffusion process [7] q(ztz0) = (zt; 1 αt z0, αtI), (1) where z0 = E(x) is the latent encoding of the clean video with encoder and αt is noise scheduling coefficient at timestep t. The VLDM estimates the noise in zt by minimizing the following objective: Ez0,ϵ,t,c (cid:2)ϵ ϵθ(zt, t, c)2(cid:3) , (2) where ϵ (0, I) and represents the conditioning input. To retrieve clean latent representation, we use reversetime Stochastic Differential Equation (SDE) sampling process: dzt = (zt)dt + g(zt) = (cid:2)f (zt) g(zt)2zt log p(zt)(cid:3) dt + g(zt) w, where and are the drift term for the forward SDE and reverse SDE, respectively, is the diffusion coefficient, and represents reverse time Wiener process. The initial point for reverse SDE is sampled from normal Gaussian distribution. By discretizing the reverse SDE with an appropriate noise schedule, the VLDM retrieves clean latent representation based on the DDIM [32] trajectory, (3) σt := η z0t = (cid:115)(cid:18) 1 αt1 1 αt (cid:0)zt 1 αt zt1 = αt1z0t + (cid:19) (cid:18) 1 (cid:19) αt αt1 1 αtϵθ(zt, t, c)(cid:1) (cid:113) 1 αt1 σ2 ϵθ(zt, t, c) + σtϵ, (4) where σt controls the stochasticity of sampling, ϵ (0, I) and z0t = E[z0zt] denotes the posterior mean or denoised version of zt, computed by Tweedies formula [8]. To transform the latent representation back to the video domain, decoder is used to decode the latent. 3.2. Guidance in Diffusion Model Given the reverse SDE in Eq. (3), our goal is to obtain the optimal control u(zt) : dzt = (cid:2) (zt) + u(zt)(cid:3) dt + g(zt) w, (5) which directs the sampling process toward target distribution p(zty), where represent desired condition, such as label, class or text prompt [41]. In classifier guidance [26], if an auxiliary classifier is available to estimate the likelihood p(yzt), the control term can be defined as u(zt) = g(zt)2wzt log p(yzt), (6) where is scaling factor that adjusts the strength of the guidance. This control term follows from applying the Bayes rule to express p(zty) p(zty)p(yzt)w. One might consider adapting classifier guidance by treating the reward model as classifier. However, this approach presents two challenges: the reward model is not trained on noisy latent representations zt and requires differentiability. To alleviate these limitations, we utilize path integral control approach with zeroth order gradient approximation, as described in the following Section 3.3. 3.3. Path Integral Control Consider the diffusion model as the entropy regularized Markov Decision Process (MDP), we can conceptualize reverse SDE as the Reinforcement Leargning (RL) framework [2, 10, 36] with the state st and the action at correspond to the input zt. In this formula, the optimal policy maximize the following objective: Ep[r(z0) α 1 (cid:88) τ =T DKL(p(zτ 1zτ )pθ(zτ 1zτ ))], (7) where α is coefficient of KL divergence with original policy pθ defined by diffusion model. Let pθ(zt1zt) = (µt, σ2 I) be reverse transition distribution in the SDE for the diffusion model and pθ(z0:t) := pθ(zt)Πt τ =1p(zτ 1zτ ). We can define value function as exp (cid:19) (cid:18) v(zt) α (cid:90) = exp (cid:18) v(zt1) α (cid:19) pθ(zt1zt)dzt1 = Epθ(z0:t) (cid:20) exp (cid:18) r(z0) α (cid:19) (cid:21) zt , (8) satisfying v(z0) = r(z0) is reward function [36]. The optimal control which address entropy-regularized MDP system can be obtain by solving Hamilton-JacobiBellman (HJB) equation as following [17, 37]: u(zt) = σ2 ztv(zt) α . (9) However, this term requires the gradient of the value function. To bypass the gradient requirements, one can use path integral control, which is an approach to estimate the optimal control (or guidance) based on the principles of stochastic optimal control [20, 35, 37]. In [17], the optimal control can be approximated as u(zt) (cid:104) exp (cid:17) (cid:16) r(z0) α (zt1 µt)zt (cid:104) exp (cid:17) (cid:16) r(z0) α (cid:105) zt (cid:105) . (10) While SCG [17] utilize this optimal control with diffusion model to solve inverse problem in image domain, we aim to use LVLMs to guide the video to improve text alignment. 4. Method: Free2Guide In this section, we introduce Free2Guide, framework that uses non-differentiable reward model to guide video generation during the sampling process. In Sec. 4.1, we discuss how to apply image-based reward models, including LVLM, for text-video alignment. Sec. 4.2 outlines methods for ensembling multiple reward models to achieve synergistic effects. Finally, we interpret the diffusion model as an entropy-regularized MDP and describe its practical implementation (Sec. 4.3). 4.1. Adapting Image-based Rewards for Video By leveraging the path integral control approach discussed in Sec. 3.3, we can guide the reverse process without relying on the gradient of the reward function. If the reward model in Eq. (10) assesses the alignment of the generated video with the text prompt, it can help steer the video output to enhance fidelity to the prompt. However, due to the complexity of video compared to static images, there are limited large-scale models specifically trained for video and text alignment. Consequently, we rely on models trained on large-scale text-image paired datasets. Applying these image-based reward models directly for video guidance, however, presents challenges. Image-based models are not designed to process time-dependent features, such as motion, flow, and dynamics, so specific adaptations are required for these models to assess text-video alignment. As shown in Algorithm 1, we calculate the reward for video by summing frame-by-frame rewards from the imagebased model. This approach enables alignment with spatial information within individual video frames but still lacks guidance on temporal dynamics. Since our framework does not require differentiability in the reward model, we can fully leverage powerful black-box LVLMs capable of handling temporal information in video alignment. Although LVLMs are trained on static imagetext data, it can effectively handle temporal information since their broad pre-training on diverse visual contexts enables them to capture elements of motion. Here, we employ LVLMs as reward models to assess text-video alignment by leveraging temporal awareness of LVLM. To adapt LVLMs to evaluate multiple frames simultaneously, we combine key 4 Algorithm 1 Reward Model r(D(z0t), c) Require: Reward function r, condition c, prompt p, decoded frames x0t := D(z0t), and key frames [1, ] 1: if is CLIP then reward (cid:80) ik sim(r(xi 2: 3: else if is ImageReward then reward (cid:80) 0t, c) 4: 5: else if is LVLM then 6: 7: end if 8: return reward reward r(concatik(xi ik r(xi 0t), r(c)) 0t), c, p) Algorithm 2 Free2Guide Require: Video diffusion model ϵθ, reward function r, decoder D, noise scheduling parameter {αt}T t=1, {σt}T t=1 1: for = to 1 do 2: (cid:0)zt 1 αtϵθ(zt)(cid:1) αtz0t + (cid:112)1 αt1 σ2 ϵθ(zt) z0t 1 αt1 ˆzt1 ϵ1, , ϵn (0, I) t1 ˆzt1 + σtϵi zi (cid:0)zi 0t1 1 zi r1 LVLM if Ensemble then αt1 t1 1 αt1ϵθ(zi t1)(cid:1) r2 {CLIP, ImageReward} argmaxi Rewardens(D(zi 0t1), r1, r2) From Sec. 4.2. else argmaxi r1(D(zi 0t1), c) 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: end if zt1 zj t1 14: 15: end for 16: return z0 frames into single composite image, enabling simultaneous temporal processing, as shown in Fig. 2. We then provide explicit instructions regarding the frame order within the composite image through system prompt, allowing the model to understand the sequence. This efficient adaptation enables LVLMs to recognize frame order by referencing frame numbers rather than listing them linearly. We then request an alignment score between the composite image and the text prompt. The LVLM evaluates the alignment of the key frames with the prompt on scale of 1 to 9, which serves as the reward function. The system instructions and query templates are detailed in Appendix A. 4.2. Ensembling Reward Functions Unlike gradient-based guidance, our method significantly reduces memory requirements by avoiding the computationally intensive backpropagation process. This enables us to concurrently employ multiple rewards for sampling guidance, potentially leading to synergistic benefits with largescale image model. We explore ensemble methods that allow LVLMs to incorporate temporal information, thereby supporting more effective guidance for video alignment when combined with large-scale image models. Note that Demon [46], concurrent work that also proposed ensemble rewards, failed to show the synergy effect of ensemble and did not have to handle temporal information. Given the videos {Vi}n i=1, we propose three ensembling methods to combine multiple reward models: Weighted Sum, Normalized Sum, and Consensus. Weighted Sum: This method combines the outputs by computing fixed weighted sum, allowing us to control the influence of each reward model. Rewardens(Vi, r1, r2) = βr1(Vi) + (1 β)r2(Vi), (11) where β [0, 1] is constant weight factor that balances the contributions of reward models r1 and r2. Normalized Sum: In this method, we first normalize each rewards output to the range [0, 1], then sum these normalized values to get the final ensemble reward. This normalization ensures that each reward models scores are comparably scaled, allowing balanced contribution. (cid:88) Rewardens(Vi, r1, r2) = r(Vi) min(r(Vi)) max(r(Vi)) min(r(Vi)) (12) where max(r), min(r) represents the maximum and minimum score from reward outputs. , Consensus: In the consensus method, or Borda count [9], each reward model ranks the videos from best to worst, assigning points based on their rank. The top-ranked video receives the maximum points, down to 1 point for the lowest rank. The total reward for each video Vi is the sum of points from both reward model Rewardens(Vi, r1, r2) = pointsr2 (Vi) + pointsr (Vi), (13) where pointsr assigns points to each rank (e.g., 5 for best video, 4 for second, etc.). 4.3. Guidance using Path Integral Control To guide the reverse sampling process without computing the gradient of the reward function, we utilize the framework outlined in Eq. (10). However, the expectation of the reward function in Eq. (10) demands extensive network function evaluations (NFE) by solving complex differential equations, 5 such as PF-ODE [33]. Inspired by [17], we instead apply DPS [5] approach to approximate the Eq. (8) by using the posterior mean of zt, as define in Eq. (4). Following DPS, we can set p(z0:t) = δ(z [z0zt]) using Direc delta distribution δ and then Eq. (10) becomes u(zt) Epθ(zt1zt) (cid:104) exp (cid:16) r(z0t1) α (cid:16) r(z0t) α (cid:17) exp (cid:17) (cid:105) (zt1 µt) . (14) To approximate this expectation using Monte Carlo method, we sample different zt1 through the reverse SDE as outlined in Eq. (4). Then we assume α 0 to obtain optimal control. Under this assumption, Eq. (3) becomes equivalent to selecting the zt1 that maximizes the reward of z0t1 [17]. While [17] arbitrarily weighted the reward function and assumed the weight to be zero, we interpret this as relaxing the entropy-regularization term in Eq. (7) by defining the diffusion process as an entropy-regularized MDP. In practical terms, this approach eliminates careful parameter exploration by selecting zt1 with the largest reward. By following this adjusted sampling strategy as described in Algorithm 2, Free2Guide can efficiently steer video generation towards better alignment with the reward signals. 5. Experiments Baselines and Sampling Strategy. We use open-source textto-video diffusion models, LaVie [39] and VideoCrafter2 [4], as baseline models. The generated videos contain 16 frames with resolution of 320 512. We employ LVLM as GPT-4o-2024-08-06 [1] using OpenAI APIs. We employ two large-scale models as CLIP [29] and ImageReward [44], to validate that LVLMs capability to account for temporal dynamics can enhance text-video alignment when used alongside large-scale image reward models. In CLIP, we can assess alignment by measuring cosine similarity between text and image embeddings. On the other hand, we can use ImageReward output as an reward since it predict human preference for image-text pairs. For adaptation to the video domain, we extract key frames from each denoised video and sum the reward for each frame to evaluate overall alignment, as outlined in Algorithm 1. We employ stochastic DDIM sampling with η = 1 in Eq. (4), total of = 50 steps and apply classifier-free guidance [14] using guidance scale of = 7.5 for LaVie and = 12 for VideoCrafter2. The number of samples at each guidance step is set to = 5 for LaVie and = 10 for VideoCrafter2. Guidance is applied during the early sampling steps, specifically within [T, 5]. In the weighted sum ensemble, we assign weight of β = 0.75 to the LVLM reward. Text Alignment Evaluation. We conduct quantitative evalu6 ation using VBench [18], benchmark designed to evaluate the alignment of text-to-video (T2V) models with respect to text prompt. Our evaluation protocol measures text alignment across six dimensions: Appearance Style, Temporal Style, Human Action, Multiple Objects, Spatial Relationships and Overall Consistency. For fair comparison, we use standardized prompts for each metric, ensuring consistent conditions across different models. These dimensions can be grouped into three core aspects of evaluation: Style, Semantics, and Condition Consistency. The Style group assesses the stylistic quality such as color, texture, and camera movement. The Semantics group evaluates the models ability to generate semantic content, such as human-centric motion, object interactions, and adherence to spatial relationships specified by the text prompt. Condition Consistency measures the coherence of the generated video in terms of semantic and stylistic stability. General Video Quality Evaluation. In addition to text alignment, we evaluate the general quality of generated videos independently of text prompts using six metrics: Subject Consistency, Background Consistency, Motion Smoothness, Dynamic Degree, Aesthetic Quality, and Imaging Quality. These are categorized into three groups: Temporal Consistency (subject and background stability), Dynamics (movement smoothness and flow), and Quality (realism, naturalness, and artistic quality at the frame level). 5.1. Results In this section, we present both qualitative and quantitative results to demonstrate the effectiveness of our method. The top four rows of Fig. 3 shows visual comparisons between the baseline and reward models. We observe that leveraging the GPT-4o model to assess text-video alignment improves alignment with respect to temporal dynamics (e.g. \"tilt down\") and semantic representation (e.g. \"A and B\"). These results indicate that LVLM can account for temporal information by processing multiple sub-frames of video simultaneously, with strong performance in spatial understanding. Building on LVLMs capability to account for temporal dynamics, we validate the feasibility of ensembling techniques that integrate guidance from large-scale image models to improve text-video alignment. This approach enables LVLM to process temporal information, enhancing the quality of guidance. In Table 1, we explore the most effective ensemble method by comparing average scores on text alignMethod LaVie + CLIP + GPTWeighted Sum + GPTNormalized Sum + GPTConsensus Avg. 0.5712 0.5738 0.5734 0.5679 Method LaVie + ImageReward + GPTWeighted Sum + GPTNormalized Sum + GPTConsensus Avg. 0.5676 0.5726 0.5715 0.5692 Table 1. Qualitative comparison between ensemble methods. Figure 3. Qualitative results of our method. Baseline with LaVie on the left and VideoCrafter2 on the right. Method LaVie [39] +GPTOurs + CLIP ++ GPTOurs + ImageReward ++ GPTOurs VideoCrafter2 [4] +GPTOurs + CLIP ++ GPTOurs + ImageReward ++ GPTOurs Style Semantics Condition Consistency Appearance Style Temporal Style Human Action Multiple Objects Spatial Relationship Overall Consistency 0.2312 0.2366 (+2.3%) 0.2370 (+2.5%) 0.2350 (+1.6%) 0.2360 (+2.1%) 0.2373 (+2.6%) 0.2490 0.2504 (+0.6%) 0.2542 (+2.1%) 0.2490 (+0.0%) 0.2513 (+0.9%) 0.2533 (+1.7%) 0.2502 0.2508 (+0.2%) 0.9300 0.9300 (-0.0%) 0.2027 0.2546 (+25.6%) 0.2490 (-0.5%) 0.2487 (-0.6%) 0.9400 (+1.1%) 1.000 (+7.5%) 0.2607 (+28.6%) 0.2447 (+20.7%) 0.2483 (-0.8%) 0.2497 (-0.2%) 0.9300 (-0.0%) 0.9400 (+1.1%) 0.2637 (+30.1%) 0.2462 (+21.4%) 0.2567 0.2568 (+0.0%) 0.9300 0.9500 (+2.2%) 0.3880 0.4878 (+25.7%) 0.2621 (+2.1%) 0.2612 (+1.8%) 0.9300 (-0.0%) 0.9600 (+3.2%) 0.4261 (+9.8%) 0.4474 (+15.3%) 0.2574 (+0.3%) 0.2607 (+1.6%) 0.9700 (+4.3%) 0.9400 (+1.1%) 0.4733 (+22.0%) 0.5160 (+33.0%) 0.3496 0.3531 (+1.0%) 0.3074 (-12.1%) 0.3180 (-9.0%) 0.2614 (-25.2%) 0.3014 (-13.8%) 0.3760 0.4225 (+12.4%) 0.2923 (-22.3%) 0.3361 (-10.6%) 0.4264 (+13.4%) 0.4371 (+16.3%) 0.2694 0.2709 (+0.6%) 0.2738 (+1.6%) 0.2742 (+1.7%) 0.2728 (+1.2%) 0.2772 (+2.9%) 0.2778 0.2872 (+3.4%) 0.2802 (+0.9%) 0.2837 (+2.1%) 0.2826 (+1.7%) 0.2828 (+1.8%) Avg. 0.3722 0. 0.3780 0.3868 0.3687 0.3753 0.4129 0.4425 0.4075 0.4229 0.4435 0.4483 Table 2. Quantitative evaluation on text alignment. Higher numbers indicate better alignment with the text prompt. The numbers in parentheses denote the performance difference from the baseline. ment and general video quality evaluation from VBench. We find that assigning more weight to LVLM outperformed the alternative of balancing model contributions equally in the ensemble, indicating that the role of LVLM is significant. Thus, we adopt the weighted sum ensemble as the default setting. The bottom four rows of Fig. 3 also illustrates qualitative results for the ensemble, showing that combining GPT-4o with other image reward models accurately resolves 7 Method LaVie [39] +GPTOurs + CLIP ++ GPTOurs + ImageReward ++ GPTOurs VideoCrafter2 [4] +GPTOurs + CLIP ++ GPTOurs + ImageReward ++ GPTOurs Temporal Consistency Dynamics Frame-wise Quality Subject Consistency Background Consistency Motion Smoothness Dynamic Degree Aesthetic Quality Imaging Quality 0.9450 0.9470 (+0.2%) 0.9495 (+0.5%) 0.9622 (+1.8%) 0.9443 (-0.1%) 0.9758 (+1.0%) 0.9658 0.9746 (+0.9%) 0.9762 (+1.1%) 0.9770 (+1.2%) 0.9739 (+0.8%) 0.9458 (+1.0%) 0.9689 0.9693 (+0.0%) 0.9712 (+0.2%) 0.9781 (+0.9%) 0.9681 (-0.1%) 0.9813 (+0.7%) 0.9748 0.9800 (+0.5%) 0.9816 (+0.7%) 0.9823 (+0.8%) 0.9801 (+0.5%) 0.9813 (+0.7%) 0.9718 0.9742 (+0.2%) 0.9735 (+0.2%) 0.9804 (+0.9%) 0.9732 (+0.1%) 0.9832 (+0.1%) 0.9818 0.9827 (+0.1%) 0.9839 (+0.2%) 0.9838 (+0.2%) 0.9828 (+0.1%) 0.9832 (+0.1%) 0.4799 0.4725 (-1.5%) 0.4560 (-5.0%) 0.3703 (-22.8%) 0.4872 (+1.5%) 0.5165 (+7.6%) 0.3846 0.2949 (-23.3%) 0.2491 (-35.2%) 0.2399 (-37.6%) 0.2711 (-29.5%) 0.2564 (-33.3%) 0.5687 0.5726 (+0.7%) 0.5727 (0.7%) 0.5951 (+4.6%) 0.5664 (-0.4%) 0.5662 (-0.4%) 0.5860 0.5977 (+2.0%) 0.6037 (+3.0%) 0.6042 (+3.1%) 0.5994 (+2.3%) 0.6039 (+3.1%) 0.6611 0.6615 (+0.1%) 0.6637 (+0.4%) 0.6795 (+2.8%) 0.6605 (-0.1%) 0.6530 (-1.2%) 0.6772 0.6924 (+2.3%) 0.6886 (+1.7%) 0.6878 (+1.6%) 0.6857 (+1.3%) 0.6877 (+1.6%) Avg. 0.7659 0.7662 0.7644 0.7609 0.7666 0.7699 0.7617 0.7537 0.7472 0.7458 0.7488 0. Table 3. Comparison of the general quality of the generated video independent of the text prompt. Higher numbers indicate better video quality. The numbers in parentheses denote the performance difference from the baselines. issues related to dynamics or multiple objects that standalone reward models struggle to properly identify, while maintaining overall structure. For more detailed evaluations, we compare the quantitative results in Table 2 to assess text-video alignment. Analysis of the average evaluation scores reveals that incorporating LVLM consistently outperforms configurations that exclude it. Specifically, we observe the most significant improvement in handling Spatial Relationship across baselines. Since CLIP has limited zero-shot spatial reasoning capability [34], the text alignment performance decrease in Spatial Relationship when using CLIP alone. However, ensembling with LVLM offers additional cues that help CLIP to better account for spatial semantics, leading to performance improvements. Furthermore, incorporating LVLM enhances Temporal Style except for cases where CLIP is used as the reward model. Since LVLM can understand temporal nuances by processing multiple frames at once, it improves performance by supporting the alignment of temporal movement. Additionally, we compare general video quality in Table 3. We confirm that even without explicit guidance for consistency or motion, alignment with text prompts improves most quality metrics except for Dynamic Degree. This metric often trades off with consistency but can be improved by ensembling GPT-4o with ImageReward in the LaVie model. This suggests that ImageReward compensates for the performance drop in Dynamic Degree that GPT-4o alone does not address, resulting in the best performance. 5.2. Ablation Study Assessment policy using LVLM. We evaluate the impact of the assessment protocol in LVLM by analyzing the average scores generated with the VideoCrafter2 model. Specifically, we modify the system prompt to instruct LVLM to answer only with yes or no when assessing text-video alignment. Method Text Alignment General Quailty Avg. VideoCrafter2 +GPTYes or No +GPTFrom 1 to 9 0.4129 0.4358 0.4425 0.7617 0.7550 0. 0.5873 0.5954 0.5981 Table 4. Average results by assessment policy using LVLM. The alignment score is then derived by calculating the percentage of the top 5 logits that correspond to yes. Table 4 reveals that scoring alignment on scale from 1 to 9 achieves better performance in terms of text alignment. This is likely because broader scale allows for more nuanced distinctions in fidelity, enabling LVLM to capture subtle differences in text-video alignment more effectively. 6. Conclusion and Limitation Conclusion In this paper, we introduced Free2Guide, novel gradient-free framework to enhance text-video alignment in diffusion-based generative models without relying on reward gradients. By approximating the gradient of the reward function, Free2Guide effectively integrates nondifferentiable reward models, including powerful black-box LVLMs, to steer the video generation process towards better alignment. Our experiments demonstrate that Free2Guide consistently improve alignment with text prompts and general video quality. By enabling ensembling with LVLM, our method benefits from synergistic effects, further enhancing performance. Limitation Sampling in our approach requires additional processing time to approximate the gradient. While our approach may slightly extend sampling time compared to baseline, it uniquely enables guidance with non-differentiable reward models such as LVLM APIs. Additionally, the effectiveness of our framework is influenced by the accuracy of the reward function, which opens avenues for further improvements as reward models continue to advance."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 6 [2] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023. 2, 4 [3] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter1: Open diffusion models for high-quality video generation, 2023. 2 [4] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models, 2024. 2, 6, 7, 8 [5] Hyungjin Chung, Jeongsol Kim, Michael Thompson Mccann, Marc Louis Klasky, and Jong Chul Ye. Diffusion posterior sampling for general noisy inverse problems. In International Conference on Learning Representations, 2023. 6 [6] Kevin Clark, Paul Vicol, Kevin Swersky, and David Fleet. Directly fine-tuning diffusion models on differentiable rewards. arXiv preprint arXiv:2309.17400, 2023. 2 [7] Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat GANs on image synthesis. In Advances in Neural Information Processing Systems, 2021. 1, 3 [8] Bradley Efron. Tweedies formula and selection bias. Journal of the American Statistical Association, 106(496):16021614, 2011. [9] Peter Emerson. The original borda count and partial voting. Social Choice and Welfare, 40(2):353358, 2013. 5 [10] Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Reinforcement learning for fine-tuning text-to-image diffusion models. Advances in Neural Information Processing Systems, 36, 2024. 2, 4 [11] Weixi Feng, Wanrong Zhu, Tsu-jui Fu, Varun Jampani, Arjun Akula, Xuehai He, Sugato Basu, Xin Eric Wang, and William Yang Wang. Layoutgpt: Compositional visual planning and generation with large language models. Advances in Neural Information Processing Systems, 36, 2024. 2 [12] Tejas Gokhale, Hamid Palangi, Besmira Nushi, Vibhav Vineet, Eric Horvitz, Ece Kamar, Chitta Baral, and Yezhou Yang. Benchmarking spatial relationships in text-to-image generation. arXiv preprint arXiv:2212.10015, 2022. 1 [13] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation. arXiv preprint arXiv:2211.13221, 2022. 2 [14] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 6 [15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:68406851, 2020. [16] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:86338646, 2022. 2 [17] Yujia Huang, Adishree Ghatare, Yuanzhe Liu, Ziniu Hu, Qinsheng Zhang, Chandramouli Sastry, Siddharth Gururani, Sageev Oore, and Yisong Yue. Symbolic music generation with non-differentiable rule guided diffusion. arXiv preprint arXiv:2402.14285, 2024. 2, 3, 4, 6, 13 [18] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 6 [19] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. 2 [20] Hilbert Kappen. Path integrals and symmetry breaking for optimal control theory. Journal of statistical mechanics: theory and experiment, 2005(11):P11011, 2005. 4 [21] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In Proc. NeurIPS, 2022. [22] Long Lian, Baifeng Shi, Adam Yala, Trevor Darrell, and Boyi Li. Llm-grounded video diffusion models. arXiv preprint arXiv:2309.17444, 2023. 2 [23] Sijia Liu, Pin-Yu Chen, Bhavya Kailkhura, Gaoyuan Zhang, Alfred Hero III, and Pramod Varshney. primer on zeroth-order optimization in signal processing and machine learning: Principals, recent advances, and applications. IEEE Signal Processing Magazine, 37(5):4354, 2020. 3 [24] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Latent consistency models: Synthesizing highZhao. resolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023. 2 [25] Yurii Nesterov and Vladimir Spokoiny. Random gradient-free minimization of convex functions. Foundations of Computational Mathematics, 17(2):527566, 2017. 3 [26] Weili Nie, Brandon Guo, Yujia Huang, Chaowei Xiao, Arash Vahdat, and Anima Anandkumar. Diffusion models for adversarial purification. arXiv preprint arXiv:2205.07460, 2022. 4 [27] Mihir Prabhudesai, Anirudh Goyal, Deepak Pathak, and Katerina Fragkiadaki. Aligning text-to-image diffusion arXiv preprint models with reward backpropagation. arXiv:2310.03739, 2023. [28] Mihir Prabhudesai, Russell Mendonca, Zheyang Qin, Katerina Fragkiadaki, and Deepak Pathak. Video diffusion alignment via reward gradients. arXiv preprint arXiv:2407.08737, 2024. 2, 3 [29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning 9 Computer Vision and Pattern Recognition, pages 63276336, 2024. 2 [43] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. 2 [44] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36, 2024. 2, [45] Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, and CUI Bin. Mastering text-to-image diffusion: Recaptioning, planning, and generating with multimodal llms. In Forty-first International Conference on Machine Learning, 2024. 2 [46] Po-Hung Yeh, Kuang-Huei Lee, and Jun-Cheng Chen. Training-free diffusion model alignment with sampling demons. arXiv preprint arXiv:2410.05760, 2024. 2, 5 [47] Hongkai Zheng, Wenda Chu, Austin Wang, Nikola Kovachki, Ricardo Baptista, and Yisong Yue. Ensemble kalman diffusion guidance: derivative-free method for inverse problems. arXiv preprint arXiv:2409.20175, 2024. 2, 3, 13 [48] Shanshan Zhong, Zhongzhan Huang, Weushao Wen, Jinghui Qin, and Liang Lin. Sur-adapter: Enhancing text-to-image pre-trained diffusion models with large language models. In Proceedings of the 31st ACM International Conference on Multimedia, pages 567578, 2023. 2 transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 6 [30] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1068410695, 2022. 1 [31] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pages 22562265. PMLR, 2015. [32] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In 9th International Conference on Learning Representations, ICLR, 2021. 3 [33] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021. 1, 6 [34] Sanjay Subramanian, William Merrill, Trevor Darrell, Matt Gardner, Sameer Singh, and Anna Rohrbach. Reclip: strong zero-shot baseline for referring expression comprehension. arXiv preprint arXiv:2204.05991, 2022. 8 [35] Evangelos Theodorou, Jonas Buchli, and Stefan Schaal. generalized path integral control approach to reinforcement learning. The Journal of Machine Learning Research, 11: 31373181, 2010. 4 [36] Masatoshi Uehara, Yulai Zhao, Tommaso Biancalani, and Sergey Levine. Understanding reinforcement learning-based fine-tuning of diffusion models: tutorial and review. arXiv preprint arXiv:2407.13734, 2024. 4 [37] Masatoshi Uehara, Yulai Zhao, Kevin Black, Ehsan Hajiramezanali, Gabriele Scalia, Nathaniel Lee Diamant, Alex Tseng, Tommaso Biancalani, and Sergey Levine. Fine-tuning of continuous-time diffusion models as entropy-regularized control. arXiv preprint arXiv:2402.15194, 2024. [38] Bram Wallace, Akash Gokul, Stefano Ermon, and Nikhil Naik. End-to-end diffusion latent optimization improves classifier In Proceedings of the IEEE/CVF International guidance. Conference on Computer Vision, pages 72807290, 2023. 2 [39] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. arXiv preprint arXiv:2309.15103, 2023. 2, 6, 7, 8, 13 [40] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, et al. Internvid: large-scale video-text dataset for multimodal understanding and generation. In The Twelfth International Conference on Learning Representations, 2023. 12 [41] David Williams and Chris Rogers. Diffusions, Markov processes, and martingales. John Wiley & Sons, 1979. 4 [42] Tsung-Han Wu, Long Lian, Joseph Gonzalez, Boyi Li, and Trevor Darrell. Self-correcting llm-controlled diffusion In Proceedings of the IEEE/CVF Conference on models. 10 Free2Guide: Gradient-Free Path Integral Control for Enhancing Text-to-Video Generation with Large Vision-Language Models"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Implementation Details A.1. Model Checkpoints We use the pre-trained T2V diffusion model LaVie and VideoCrafter2, available at https://github.com/Vchitect/LaVie and https://github.com/AILab-CVC/VideoCrafter, respectively. For LaVie, the Stable Diffusion v1.4 model is employed to encode and decode latent. We also utilize CLIP from https://huggingface.co/openai/clip-vit-base-patch32 and the ImageReward model from https://github.com/THUDM/ImageReward. A.2. Evaluation Details During the video guidance process, we extract key frames from the videospecifically, the first, sixth, eleventh, and sixteenth framesand assess the reward. When using an LVLM as the reward model, we concatenate the key frames using the following scripts: 1 fig , axes = plt . subplots (2 , 2, figsize =(12 , 8) ) 2 key_frames = [0 , 5, 10 , 15] 3 4 for idx , frame in enumerate ( key_frames ): 5 6 7 8 ax = axes [ idx // 2, idx % 2] ax . imshow ( video [0 , :, frame , :, :]. permute (1 , 2, 0) . cpu () . numpy () ) ax . axis ( off ) ax . set_title (f Frame { frame + 1} ) 9 10 # Adjust the layout and show the plot 11 plt . tight_layout () 12 plt . savefig (f frame_ {i}_{j }. png ) Listing 1. Pseudo-code for stiching key frames at once. Next, we provide system instruction that allows the LVLM to understand the sequence order and explicitly describes the task it should perform. the bottom left that responds to video quality assessments . is the third frame, and finally the bottom right 1 You are useful helper 2 The given image is grid of four key frames of video: 3 frame, 4 Answer the reason first and the final answer later . Start 5 and review your reasoning logically . 6 After reviewing your reasoning , give the final answer with Answer: . 7 You should check all frame and comparing them, and ensure your reasoning leads to sound final answer. 8 Your final 9 Lets think step by step . answer should one score only and the score must be from 1 to 9 without decimals . is the fourth frame. the top right the top left is the first frame, the reason first with Reasoning: in front of the reason part is the second Listing 2. System instruction for GPT-4o For given video, we input the system prompt to the LVLM as follows: 1 For given image as keyframes of video , Rate the following questions : 2 Considering all 3 Review your reasoning thoroughly and then respond with your final decision prefixed by Answer: . four images, does the prompt, prompt, describe the video well enough? where prompt is the given text prompt (e.g. bird and cat) Listing 3. System prompt for GPT-4o 11 B. Additional Ablation Study Number of Samples We analyze the effect of the sampling quantity on text alignment performance, evaluating the average text alignment score using the LaVie model with CLIP reward model. As shown in Table 5, we find an optimal sampling size at = 5. Increasing the number of samples increases the likelihood of selecting denoised video that aligns with the desired control. However, excessive sampling introduces risk: errors predicted by Tweedies formula in initial sampling steps may result in irreversible changes, affecting video quality negatively. Guidance Range We also evaluate the effect of the guidance range with the same baseline. Table 6 reveals that applying guidance in the early stages is more effective than in later stages, as these initial steps establish the overall spatial structure of the video. However, extending the guidance range too far allows errors in the approximated optimal control to accumulate, ultimately degrading the quality of the final output video. C. Additional Analysis C.1. Reward Robustness 1 3 5 10 Avg. 0.3722 0.3749 0.3780 0.3705 Table 5. Quantitative results on text alignment by sample size."
        },
        {
            "title": "Guidance Step",
            "content": "None [T, 5] [T 5, 10] [T, 10] Avg. 0.3722 0.3780 0.3769 0.3635 Table 6. Quantitative results on text alignment by range of guidance step. We verify that our method achieves robust performance without overfitting to any particular reward. Table 7 compares the reward functions described in Algorithm 1 for the final video output produced by each method. Video guidance with rewards ensembled with LVLM generally achieves higher metrics, showing trend similar to the results for text alignment in Table 2. These findings indicate that the ensemble approach is not over-optimized for specific reward, contributing to improved robustness across diverse rewards. Method LaVie +GPT +CLIP ++GPT +ImageReward ++GPT CLIP () ImageReward () GPT () Method CLIP () ImageReward () GPT () 29.60 29.60 29.76 30.28 29.57 29.73 -0.49 -0. -0.44 -0.33 -0.51 -0.46 6.79 6.86 6.78 6.62 6.87 6.92 VideoCrafter2 +GPT +CLIP ++GPT +ImageReward ++GPT 30.39 30.90 30.96 30.95 30.92 30.96 -0.10 0. 0.14 0.20 0.22 0.28 7.09 7.28 7.11 7.07 7.28 7.33 Table 7. Comparison on reward function of generated video. Bold: best, underline: second best. C.2. Video Reward Guidance While using video-based reward model to guide videos is more natural approach, we claim that video reward models fail to capture the representation needed for guidance because the dataset of video-text pairs is relatively limited compared to images. To support this, we compare the results of using video-based reward model for guidance with video-based reward model for text alignment. We adopt ViCLIP [40], pre-trained video-text representation learning model available at https://huggingface.co/OpenGVLab/ViCLIP, as the video reward model. Using LaVie as the baseline, we compute the reward based on eight video frames, measuring the similarity between the video and text embeddings. Table 8 shows that the video-based reward model does not significantly outperform the image-based reward model. However, it specifically enhances the Overall Consistency and Dynamic Degree metrics. Notably, the Overall Consistency metric is evaluated using ViCLIP itself, which could introduce bias favoring the video reward model. In addition, we observe that ViCLIP struggles with spatial information processing compared to CLIP, leading to lower performance on the Multiple Objects and Spatial Relationship metrics. These results highlight the challenges of video reward models to fully capture the relationship between video and text due to the lack of training datasets. 12 Method LaVie [39] + CLIP + ViCLIP Method LaVie [39] + CLIP + ViCLIP Style Semantics Condition Consistency Appearance Style Temporal Style Human Action Multiple Objects Spatial Relationship Overall Consistency 0.2312 0.2370 (+2.5%) 0.2348 (+1.6%) 0.2502 0.2490 (-0.5%) 0.2485 (-0.7%) 0.9300 0.9400 (+1.1%) 0.9600 (+3.2%) 0.2027 0.2607 (+28.6%) 0.2149 (+6.0%) 0.3496 0.3074 (-12.1%) 0.2872 (-17.9%) 0.2694 0.2738 (+1.6%) 0.2752 (+2.1%) Temporal Consistency Dynamics Frame-wise Quality Subject Consistency Background Consistency Motion Smoothness Dynamic Degree Aesthetic Quality Imaging Quality 0.9450 0.9495 (+0.5%) 0.9443 (-0.1%) 0.9689 0.9712 (+0.2%) 0.9694 (+0.0%) 0.9718 0.9735 (+0.2%) 0.9741 (+0.2%) 0.4799 0.4560 (-5.0%) 0.4707 (-1.9%) 0.5687 0.5727 (0.7%) 0.5746 (1.0%) 0.6611 0.6637 (+0.4%) 0.6487 (-1.9%) Avg. 0.3722 0.3780 0. Avg. 0.7659 0.7644 0.7636 Table 8. Comparison with video-based reward model. Higher numbers indicate better video quality. The numbers in parentheses denote the performance difference from the baselines. C.3. Video Inverse Problem Our framework can readily extend to inverse problems in the video domain, building on approaches from previous work [17, 47]. In Figure 4, we show video reconstructed by our method using 16 average pooling on spatial resolution. For the reward function, we use the L2 distance between the corrupted denoised video and the corrupted video, applying sampling size of 10 at each step with DDIM over 500 steps, using VideoCrafter2. Our results demonstrate that, compared to unguided sampling, our method generates realistic videos that remain faithful to the input. We leave the video inverse problem as future work. Figure 4. The result of applying our method to the inverse problem. Baseline represents that no guidance is applied during sampling. 13 D. Additional Visual Results Figure 5. More qualitative comparison of different reward models. The red text highlights the difference between the models. 14 Figure 6. More qualitative results of ensembling with LVLMs. The red text highlights the difference between the models."
        }
    ],
    "affiliations": [
        "Graduate School of AI, KAIST"
    ]
}