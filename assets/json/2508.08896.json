{
    "paper_title": "Towards Affordance-Aware Robotic Dexterous Grasping with Human-like Priors",
    "authors": [
        "Haoyu Zhao",
        "Linghao Zhuang",
        "Xingyue Zhao",
        "Cheng Zeng",
        "Haoran Xu",
        "Yuming Jiang",
        "Jun Cen",
        "Kexiang Wang",
        "Jiayan Guo",
        "Siteng Huang",
        "Xin Li",
        "Deli Zhao",
        "Hua Zou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "A dexterous hand capable of generalizable grasping objects is fundamental for the development of general-purpose embodied AI. However, previous methods focus narrowly on low-level grasp stability metrics, neglecting affordance-aware positioning and human-like poses which are crucial for downstream manipulation. To address these limitations, we propose AffordDex, a novel framework with two-stage training that learns a universal grasping policy with an inherent understanding of both motion priors and object affordances. In the first stage, a trajectory imitator is pre-trained on a large corpus of human hand motions to instill a strong prior for natural movement. In the second stage, a residual module is trained to adapt these general human-like motions to specific object instances. This refinement is critically guided by two components: our Negative Affordance-aware Segmentation (NAA) module, which identifies functionally inappropriate contact regions, and a privileged teacher-student distillation process that ensures the final vision-based policy is highly successful. Extensive experiments demonstrate that AffordDex not only achieves universal dexterous grasping but also remains remarkably human-like in posture and functionally appropriate in contact location. As a result, AffordDex significantly outperforms state-of-the-art baselines across seen objects, unseen instances, and even entirely novel categories."
        },
        {
            "title": "Start",
            "content": "Towards Affordance-Aware Robotic Dexterous Grasping with Human-like Priors Haoyu Zhao *1, 2, Linghao Zhuang *1, Xingyue Zhao *2, Cheng Zeng5, Haoran Xu4, Yuming Jiang2, Jun Cen2, 3, 4, Kexiang Wang2, Jiayan Guo2, Siteng Huang2, 3, 4, Xin Li2, 3, Deli Zhao2, 3, Hua Zou1 1 Wuhan University 2 DAMO Academy, Alibaba Group 3 Hupan Lab 4 Zhejiang University 5 Tsinghua University 5 2 0 2 2 1 ] . [ 1 6 9 8 8 0 . 8 0 5 2 : r Abstract dexterous hand capable of generalizable grasping objects is fundamental for the development of general-purpose embodied AI. However, previous methods focus narrowly on low-level grasp stability metrics, neglecting affordanceaware positioning and human-like poses which are crucial for downstream manipulation. To address these limitations, we propose AffordDex, novel framework with two-stage training that learns universal grasping policy with an inherent understanding of both motion priors and object affordances. In the first stage, trajectory imitator is pretrained on large corpus of human hand motions to instill strong prior for natural movement. In the second stage, residual module is trained to adapt these general human-like motions to specific object instances. This refinement is critically guided by two components: our Negative Affordance-aware Segmentation (NAA) module, which identifies functionally inappropriate contact regions, and privileged teacher-student distillation process that ensures the final vision-based policy is highly successful. Extensive experiments demonstrate that AffordDex not only achieves universal dexterous grasping but also remains remarkably humanlike in posture and functionally appropriate in contact location. As result, AffordDex significantly outperforms stateof-the-art baselines across seen objects, unseen instances, and even entirely novel categories. Source code is available at: https://github.com/Maxwell-Zhao/AffordDex/."
        },
        {
            "title": "Introduction",
            "content": "Dexterous grasping, as foundational capability for robotic manipulation, has garnered significant attention from both academia and industry (Zhao et al. 2024b). Compared to simpler end-effectors (e.g., parallel jaws, vacuum grippers), five-fingered dexterous hands closely resemble human hand structure, providing substantially enhanced flexibility, precision, and task adaptability (Zhong et al. 2025). Furthermore, anthropomorphic robots expedite the collection of rich human demonstration data via teleoperation (Li et al. 2025a). Consequently, this synergy has fueled rapid progress, with recent algorithms achieving high success rates in generalizing grasps to novel objects (Fang et al. 2022, 2020; Gou et al. 2021; Wang et al. 2021; Xu et al. 2023; Wan et al. 2023). * Equal contributions. Corresponding Author Figure 1: Performance comparision among UniDexGrasp (Xu et al. 2023), UniDexGrasp++ (Wan et al. 2023), and our AffordDex, on the vision-based setting. we report human-likeness score (HLS) and affordance score (AS) across seen objects, unseen objects, and unseen categories. We also present qualitative comparison, where AffordDex performs natural and safe grasping by avoiding the blade. Due to the high degrees of freedom (DOFs) of dexterous hands, traditional motion planning-based methods (Andrews and Kry 2013; Bai and Liu 2014) struggle to handle such complex hand joint movements. Recent advancements in reinforcement learning (RL) (Wan et al. 2023; Mandikal and Grauman 2022; Christen et al. 2022; Nagabandi et al. 2020; Mandikal and Grauman 2021) have shown promising results in complex dexterous manipulation. However, the goal of grasping is not merely to lift an object. It involves alignment with human intent and preparation for subsequent manipulation tasks, such as avoiding the blade of knife or preparing to open bottle cap. Existing methods, while focused on low-level grasp stability metrics, largely overlook this crucial synthesis of affordance-aware positioning and human-like kinematics, limiting their utility in real-world, multi-step manipulation scenarios. In this work, we focus on the critical aspect of safety and functional correctness by modeling negative affordancesregions to be avoided, which provide clear, unambiguous negative constraints and thus simplify the learning problem. We propose AffordDex, novel framework that learns universal grasping policy that is both human-like in its motion and functionally aware of object affordances. We achieve this through structured, two-stage training paradigm. In the first stage, we pre-train base policy on large corpus of human hand motions to instill strong prior for natural movement. In the second stage, residual module is trained to adapts the general human-like motions from the pre-trained policy to specific objects. This refinement is critically guided by our proposed Negative Affordance-aware Segmentation (NAA) module, which provides explicit visual-geometric constraints on functionally inappropriate contact regions. Moreover, the training is enhanced with teacher-student distillation framework, which leverages ground-truth state information to ensure the final vision-based policy is highly effective and robust. As illustrated in Fig. 1, AffordDex produces grasps that are not only successful but also remarkably human-like and functionally correct, such as safely grasping knife by its handle. Extensive experiments validate that our method significantly outperforms existing approaches across benchmarks of seen objects, unseen objects, and even objects across datasets. In summary, AffordDex makes the following contributions: We propose AffordDex, two-stage framework that synergistically and effectively integrates human motion priors with functional affordance constraints to achieve generalizable and anthropomorphic dexterous grasping. We introduce Negative Affordance-aware Segmentation (NAA) module that, by reformulating segmentation as VLM-guided classification problem, provides explicit geometric constraints to prevent functionally improper grasps. Extensive experiments demonstrate AffordDex achieves SOTA success rates across multiple levels of generalization while producing grasps that are qualitatively superior in human-likeness and functional appropriateness."
        },
        {
            "title": "2.1 Dexterous Grasping\nRobotic grasping (Fang et al. 2022, 2020; Gou et al. 2021;\nWang et al. 2021) has been a longstanding research, aiming\nto enable robots to interact with objects reliably and adap-\ntively. While significant advances have been made with sim-\nple parallel-jaw grippers (Fang et al. 2020; Mahler et al.\n2019), their limited dexterity restricts adaptability to ob-\njects with intricate geometries. Dexterous, multi-fingered\nhands (Xu et al. 2023; Wan et al. 2023) offer a solution\nbut pose a severe control challenge for traditional analyti-\ncal methods (Bai and Liu 2014; Liu et al. 2021), motivating\nthe shift towards learning-based approaches.",
            "content": "One paradigm decouples the grasping process into static grasp pose generation followed by dynamic grasping through trajectory planning or goal-conditioned reinforcement learning (RL) (Wan et al. 2023; Christen et al. 2022; Wang et al. 2025). For example, UniDexGrasp++ proposes geometry-aware curriculum learning and leverages the geometry feature for RL. However, these RL-based methods may produce physically unrealistic joint configurations. An alternative paradigm directly learns the entire grasping trajectory through expert demonstrations from humans or reinforcement learning agents (Xu et al. 2023; Liu et al. 2024; Huang et al. 2023; Lu et al. 2024; Zhang et al. 2024b,a). These approaches tend to achieve more natural motions but suffer from poor generalization to novel objects due to the limited diversity of demonstrations and inherent policy constraints. To address these failures, our AffordDex combines strong, human-derived motion priors to ensure natural movement with affordance-based guidance to achieve robust generalization, resulting in policy that is both natural and functionally effective across wide range of objects."
        },
        {
            "title": "3 Methodology\nTo generate grasps with affordance-aware positioning and\nhuman-like kinematics, crucial for facilitating downstream\nmanipulation, we propose a novel two-stage framework. The\nfirst stage establishes a strong human motion prior by pre-\ntraining a base policy πH , on a large-scale human motion\ndataset (Zhan et al. 2024) via imitation learning. This con-\nstrains the policy to a manifold of natural, human-like move-\nments. In the second stage, we freeze the weights of πH and\ntrain a lightweight residual module via reinforcement learn-\ning (RL) to adapt these general motions to specific object in-\nteractions. This RL refinement stage is critically guided by\ntwo components: our Negative Affordance-aware Segmen-\ntation (NAA) module, which provides explicit constraints",
            "content": "Figure 2: Pipeline of AffordDex. To generate grasps with affordance-aware positioning and human-like kinematics, crucial for facilitating downstream manipulation, we propose novel two-stage framework. The first stage establishes strong human motion prior by training base policy πH , on human motion dataset via imitation learning. This constrains the policy to space of natural, human-like movements. Subsequently, the second stage employs reinforcement learning (RL) to refine this coarse policy πH for precise, functional interaction. We fine-tune πH with residual module that is guided by our Negative Affordance-aware Segmentation (NAA) module, which provides explicit constraints on where not to touch the object. The entire learning pipeline is further enhanced by teacher-student distillation framework, leveraging privileged inputs to significantly boost the final grasping performance. on where not to touch an object, and teacher-student distillation framework that leverages privileged state information to significantly boost the final policys performance. An overview of our method is illustrated in Fig. 2."
        },
        {
            "title": "3.1 Human Hand Trajectory Imitating",
            "content": "In this stage, our objective is to learn base policy πH , that captures the kinematic priors of natural human hand motions. We formulate this task as reinforcement learning (RL) problem where the policy πH (atSH ) learns to generate dexterous hand action at based on the current state SH at time t. To facilitate the following fine-tuning stage, the state consists of robot state Rt, object state Ot, and point cloud representation of object Pt, i.e., SH = {Rt, Ot, Pt}. Reward function. We design reward function rH to promote both precise imitation of human hand trajectories and the motion stability. It is composed of two terms: finger imitation reward rH finger and smoothness reward rH smooth. The finger imitation reward rH finger encourages the dexterous hand to closely track the reference finger poses from human hand dataset. Following (Li et al. 2025b), we define this reward based on the distance between the corresponding keypoints on the robot dexterous hand and the MANO hand. The reward at time is formulated as: rH finger = (cid:88) =1 (cid:16) wf exp λf jd,f jh,f 2 2 (cid:17) , (1) The smoothness reward rH where jd,f is the position of the -th keypoint on the dexterous hand, jh,f is its corresponding target position from the reference trajectory, wf is weight and λf is the decay rate. smooth encourages energyefficient movements by penalizing excessive power consumption. This is computed as the element-wise product of joint velocities and applied torques. detailed formulation of our reward function is available in Supp. Mat."
        },
        {
            "title": "3.2 Negative Affordance-aware Segmentation",
            "content": "A significant limitation of prior work in grasp synthesis (Xu et al. 2023; Wan et al. 2023; Zhong et al. 2025), is its neglect of the semantic and functional context of the interaction. classic example is knife: while its blade is geometrically stable for grasping, any such grasp is functionally incorrect and unsafe. To address limitation, we introduce the Negative Affordance-aware Segmentation (NAA) module to incorporate negative affordancesreasoning about which parts of an object should not be touched. The proposed NAA has the ability to operate in an open-vocabulary manner by harnessing the rich world knowledge embedded in Vision-Language Models (VLMs) (Radford et al. 2021; Achiam et al. 2023), automatically benefiting from future progress in foundation models. This ensures that the generated grasps are not only geometrically stable but also semantically coherent and taskaware. VLMs struggle to interpret non-textured 3D meshes, as these models primarily rely on rich visual cues learned from images. To bridge this gap, we first apply procedural texturing to the raw meshes by (Zhang et al. 2024c), which generates semantically plausible textures based on geometric analysis, ensuring robustness across different object shapes. Next, we render the textured object from six cardinal directions to create multi-view image set as holistic visual representation. While this may not capture all concavities in highly complex objects, we found it provides sufficient basis for affordance prediction for objects in the benchmark datasets, representing practical trade-off between coverage and computational cost. We then query GPT-4V (Achiam et al. 2023) to elicit detailed description of the objects negative affordances. VLMs (Radford et al. 2021) and Multimodal Large Language Models (MLLMs) (Achiam et al. 2023) excel at image-level understanding but struggle with the fine-grained spatial localization required for segmentation. To solve this, instead of asking CLIP (Radford et al. 2021) to find blade part from the image, we turn the segmentation task into much simpler classification task. We generate set of precise object-part masks Mi and use them as visual prompt to let CLIP identify which mask in Mi has the highest semantic similarity to the textual description blade part. Specifically, for each image Ii I, we prompt Segment Anything Model (SAM) (Kirillov et al. 2023) with dense grid of points overlaid on Ii, which prompts SAM to perform an exhaustive segmentation, identifying all potential objects and parts. The resulting collection of masks is then refined using Non-Maximum Suppression (NMS) to eliminate duplicates, yielding clean candidate masks set Mi: Mi = NMS(SAM(Ii, Gi)). (2) For each mask Mi, we generate visually prompted image by blurring regions outside the mask with Gaussian filter following (Yang et al. 2023). The prompted image set {I } is then passed to CLIP along with the text query to compute similarity score for each imagetext pair. The mask with the highest similarity score is selected as the final segmentation mask. The mask is then projected into 3D space to segment the corresponding regions of the objects point cloud to get the negative affordance Nt, as shown in Fig. 3."
        },
        {
            "title": "3.3 Affordance-aware Residual Learning\nThe negative affordance predicted from proposed NAA, we\nuse a residual module R to refine the pre-trained policy πH .\nSince visual pose estimation is inherently less precise than\nusing privileged state information, directly training an effec-\ntive vision-based policy can be challenging. Therefore, we",
            "content": "Figure 3: Visualization of Negative Affordances Predicted by our NAA. The point cloud, highlighted in red, represents the negative affordances identified on various objects. These points denote regions that are functionally unsafe or inappropriate for grasping, such as knifes blade. first train state-based teacher policy πT which can access the ground-truth states of the environment, such as object states, to learn residual actions to refine the initial actions predicted by πH . Once the teacher policy πT finishes training, we use an imitation learning algorithm, DAgger (Ross, Gordon, and Bagnell 2011), to distill πT to vision-based student policy πS that can access oracle information and let policy help and ease the vision-based policy learning. State-based teacher policy. In this stage, the inputs are robot state Rt, object state Ot, the scene point clouds Pt, and predicted negative affordance Nt. Here the scene point cloud is fused by multi-view depth cameras. Our goal is to learn residual actions at = πT (ST ) with predicted negative affordance by PPO (Schulman et al. 2017). The final action is computed with an element-wise addition: at = πH (ST ) + πT (ST ). Reward function. The reward function rT is defined as: rT rT = rT rT + rT (3) (4) where the grasp reward rT penalizes the distance between the dexterous hand and the object, encouraging the hand to maintain contact with the object surface for secure grasp. The goal reward rT penalizes the distance between the object and the target goal, and the success reward rT provides bonus when the object successfully reaches the goal. Also the negative affordance reward rT penalizes the dexterous hand to approach the predicted negative affordance. The formal definitions of all rewards are available in our Supp. Mat. Vision-based student policy. For vision-based policy, we only allow it to access information available in the real world, including robot state Rt, the scene points clouds Pt, Figure 4: Qualitative Comparison on UniDexGrasp (Xu et al. 2023) and OakiInk2 (Zhan et al. 2024). comparison of grasps generated by our AffordDex with several baselines, including UniDexGrasp (Xu et al. 2023), UniDexGrasp++ (Wan et al. 2023), and DexGrasp Anything (Zhong et al. 2025). and predicted negative affordance Nt. Then, we distill the teacher policy πT into vision-based student policy πS using DAgger (Ross, Gordon, and Bagnell 2011), i.e., πS = arg min πS πT (ST ) πS(SS ), (5) the state where {Rt, Ot, Pt, Nt}, and the state for SS = {Rt, Pt, Nt}. the for teacher policy ST = the student policy"
        },
        {
            "title": "4.1 Datasets",
            "content": "UniDexGrasp (Xu et al. 2023). This dataset contains 3165 different object instances spanning 133 categories. Evaluation is conducted on these 3,200 seen objects, as well as on 140 unseen objects from seen categories and 100 unseen objects from unseen categories. Each environment is randomly initialized with one object and its initial pose, and the environment consists of panoramic 3D point cloud Pt captured from the fixed cameras for vision-based policy learning. OakiInk2 (Zhan et al. 2024). This dataset record the manipularion processes with pose and shape of the human upper-body and objects. We pre-train our πH using about 2,200 right hand manipulation sequences in this dataset. Also we employ objects in OakiInk2 to evaluate the generalization capabilities for grasping."
        },
        {
            "title": "4.2 Metrics\nFollowing previous works (Xu et al. 2023; Wan et al. 2023;\nWang et al. 2025), each object is randomly rotated and\ndropped onto the table to enhance the diversity of its initial",
            "content": "poses. We report the success rate of grasp Succ, Humanlikeness Score HLS, and Affordance Score AS across all objects and grasp attempts. grasp is considered successful if the object reaches the target goal within 200 steps in simulator. The Human-likeness Score HLS assesses the anthropomorphic quality of the grasp, which is obtained by prompting the Gemini 2.5 Pro (Comanici et al. 2025) to analyze visual sequence of the grasp execution. This metric is specifically to rate the resemblance of the dexterous hands motion to that of typical human, yielding quantitative measure of naturalness. The Affordance Score AS, in contrast, evaluates the functional correctness of the grasp by penalizing contact with inappropriate object parts. This metric is calculated using point cloud of 100 negative affordance points sampled from our NAA. Specifically, the score is incremented by one for each fingertip that maintains distance greater than 2cm from any point in this negative set, thus rewarding functionally sound grasps."
        },
        {
            "title": "4.3\nWe conduct our experiments in IssacGym (Makoviychuk\net al. 2021) simulator. During training, 4096 environments\nare simulated in parallel on an NVIDIA RTX 4090 GPU.\nFor the network architecture, we use MLP with 4 hidden\nlayers (1024,1024,512,512) for the policy network and value\nnetwork in the state-based setting, and an additional Point-\nNet+Transformer (Mu et al. 2021) to encode the 3D scene\npoint cloud input in the vision-based setting. Other detailed\nhyperparameters are shown in our Supp. Mat.",
            "content": "Dexterous hand configuration. We use the Shadow Hand, which features 24 active degrees of freedom (DOFs). The wrist has 6 DOFs controlled by force and torque, while the fingers have 18 active DOFs controlled by joint angles. Table 1: Quantitative comparisons on UniDexGrasp (Xu et al. 2023) and OakiInk2 (Zhan et al. 2024). HLS denotes Humanlikeness Score, while AS is Affordance Score. Method Seen Obj. Unseen Obj. Seen Cat. Unseen Obj. Unseen Cat. OakiInk2 Succ HLS AS Succ HLS AS Succ HLS AS Succ HLS AS PPO (Schulman et al. 2017) DAPG (Rajeswaran et al. 2017) GSL (Jia et al. 2022) ILAD (Wu, Wang, and Wang 2023) UniDexGrasp (Xu et al. 2023) UniDexGrasp++ (Wan et al. 2023) DexGrasp Anything (Zhong et al. 2025) AffordDex PPO (Schulman et al. 2017) DAPG (Rajeswaran et al. 2017) GSL (Jia et al. 2022) ILAD (Wu, Wang, and Wang 2023) UniDexGrasp (Xu et al. 2023) UniDexGrasp++ (Wan et al. 2023) AffordDex 24.3 20.8 57.3 31.9 79.4 87.9 71.2 89.2 20.6 20.8 54.1 27.6 73.7 85.4 87. - - - - 6.9 5.4 - 8.6 - - - - 6.2 5.4 8.3 - - - - 12 28 20 4 State-Based Setting - 20.9 - 15.3 - 54.1 - 26.4 6.4 74.3 5.2 84.3 - 69.1 87.7 8.5 Vision-Based Setting - 17.2 - 15.3 - 50.2 - 23.2 6.1 68.6 5.1 79.6 7.8 82.8 - - - - 16 29 10 - - - - 15 26 18 - - - - 18 25 14 17.2 11.1 50.9 23.1 70.8 83.1 67.3 85.2 15.0 11.1 44.8 20.0 65.1 76.7 79.2 - - - - 6.3 5.0 - 8.1 - - - - 6.0 4.8 8.0 - - - - 18 27 22 - - - - 17 28 15 - - - - 68.4 79.6 65.9 82.2 - - - - 62.8 74.4 77.3 - - - - 5.9 4.9 - 8.2 - - - - 5.6 4.7 7.8 - - - - 18 28 24 - - - - 20 29 13 Table 2: Ablation Study on UniDexGrasp (Xu et al. 2023) in Seen Object. HTI NAA Distillation Succ HLS AS State-Based Setting 85.4 87.9 89.2 Vision-Based Setting 70.1 84.9 85.8 86.9 87.0 5.2 8.2 8. 5.0 5.6 7.2 8.1 8.3 27 22 4 27 28 13 20 10 Table 3: Results on UniDexGrasp (Xu et al. 2023) in Seen Object in state-based setting. Method UniDexGrasp++ (Wan et al. 2023) UniDexGrasp++ + HTI UniDexGrasp++ + NAA UniDexGrasp++ + HTI + NAA Succ HLS AS 5.4 7.8 5.9 8. 87.9 88.2 88.0 88.8 28 23 19 12 Specifically, the thumb has 5 DOFs, the little finger has 4, and the remaining three fingers each have 3. Additionally, each finger, excluding the thumb, includes passive, noncontrolled DOF."
        },
        {
            "title": "4.4 Comparison with SOTA Methods",
            "content": "We evaluate AffordDex by first training state-based policy and then distilling it into vision-based one. For comparison, we compare AffordDex with several state-of-theart methods. These include RL and imitation learning algorithms like PPO (Schulman et al. 2017), DAPG (Rajeswaran et al. 2017), and ILAD (Wu, Wang, and Wang 2023). We also compare against methods with advanced learning paradigms such as GSL (Jia et al. 2022) (generalistspecialist), UniDexGrasp++ (Wan et al. 2023) (geometryaware curriculum learning), UniDexGrasp++ (Wan et al. 2023) which proposes geometry-aware curriculumn learning and generalist-specialist learning, and DexGrasp Anything (Zhong et al. 2025), diffusion-based dexterous grasp generation models. Since DexGrasp Anything (Zhong et al. 2025) generates only the final static grasp pose rather than grasping motion, the HLS is not applicable in this setting. To evaluate grasp robustness, we apply random external force ranging from 0 to 200N to the object to simulate objects gravity. Tab. 1 compares our AffordDex with these SOTA methods using universal model for dexterous robotic grasping across both state-based, vision-based and even across dataset settings. Our method achieves highest scores in grasping success rate, outperforming other state-of-the-art methods. This improvement stems from our proposed Human Hand Trajectory Imitating, where the policy learns to generate correct and stable grasp poses from human hand motion. This not only greatly enhances the grasp success rate but also leads to significant improvement in our methods human-likeness score (HLS). The significantly lower Affordance Score AS validates the effectiveness of our Negative Affordance-aware Segmentation module. This result indicates that the module successfully guides the policy away from functionally inappropriate regions, leading to grasps at the most suitable locations. As illustrated in Fig. 4, our method generates diverse set of grasps. Crucially, it consistently identifies functionally appropriate grasp locations and forms natural hand postures. This combination of functional awareness and natuFigure 5: Ablation Study on Human Hand Trajectory Imitating (HTI). Without the human motion prior, the policy converges to solution that, while potentially successful, is kinematically awkward and non-humanlike. Figure 6: Ablation Study on proposed NAA, which guides the policy to correct and safte position. The higher Affordance Score (AS) for the NAA-guided grasp confirms its superior functional quality. ralness makes the generated poses highly effective for direct application in downstream manipulation tasks."
        },
        {
            "title": "4.5 Ablation study\nUnless otherwise specified, the ablation studies are con-\nducted on the seen objects under the state-based setting.",
            "content": "Human Hand Trajectory Imitating. The results in Tab. 2 and Fig. 8 show the critical role of pre-training on human trajectories (HTI). When this imitation stage is omitted, the policy, while still capable of finding geometrically stable grasps, produces motions that are kinematically unnatural. This is quantitatively reflected in sharp increase in the Human-Likeness Score (HLS). Such configurations are not merely an aesthetic issue, they can be inefficient, unpredictable, and detrimental to downstream tasks that require fluid, human-centric interaction. NAA. As shown in Fig. 7, naive approach combining an MLLM (Achiam et al. 2023) with SAM (Kirillov et al. 2023) (denoted as GPT+SAM) proves ineffective for this task. This baseline first uses the MLLMs coarse localization ability to provide prompts to SAM. However, because MLLMs like GPT-4V (Achiam et al. 2023) excel at image-level understanding but struggle with the fine-grained spatial localization required for segmentation, this process often results in the segmentation of the entire object. In contrast, our NAA module solves this by converting the segmentation task into Figure 7: Ablation Study on proposed NAA, which has capability to segment fine-grained negative affordance. simpler classification problem. By first using SAM to generate accurate mask proposals and then using CLIP to select the one with the highest semantic similarity to the negative affordance description, NAA achieves precise segmentation. As shown in Fig. 6 and Tab. 2, the guidance from NAA results in significant decrease in the Affordance Score AS, which indicates that the policy successfully learns to make contact at more rational and safer locations on the object. By generating functionally sound grasps, AffordDex greatly improves the feasibility of performing downstream tasks. Teacher-student distillion. Without the teacher-student distillion (Distillation) grasping accuracy decreases significantly. This is primarily due to the lack of privileged information guidance, which makes it challenging for the single-stage RL policy to learn the position to grasp. As shown in Tab. 2, the policy without teacher-student distillation demonstrates lower grasping success rate."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we present AffordDex, novel framework for generating dexterous grasps that are not only successful but also human-like and functionally correct. Our key insight is that the challenges of naturalness and functional correctness can be effectively decoupled and then synergized: strong motion prior learned from human data constrains the policy to manifold of natural poses, while visual understanding of negative affordances guides the policy to safe and appropriate contact regions. Extensive experiments validate that this approach significantly outperforms stateof-the-art baselines in success rate, pose naturalness, and contact appropriateness. We believe this work lays crucial foundation for more general-purpose embodied agents and opens new avenues for research in dexterous manipulation. Limitation. limitation of our approach stems from its reliance on fixed set of six rendered views for negative affordance prediction, which can fail to capture all functionally relevant parts on geometrically complex or concave objects. This can lead to imprecise negative affordance segmentation due to occlusion. Future work could overcome this by adopting volumetric-based affordance learning on implicit 3D representations, which are inherently robust to viewpoint-specific occlusions. References Achiam, J.; Adler, S.; Agarwal, S.; Ahmad, L.; Akkaya, I.; Aleman, F. L.; Almeida, D.; Altenschmidt, J.; Altman, S.; Anadkat, S.; et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Andrews, S.; and Kry, P. G. 2013. Goal directed multi-finger manipulation: Control policies and analysis. Computers & Graphics, 37(7): 830839. Bai, Y.; and Liu, C. K. 2014. Dexterous manipulation using both palm and fingers. In International Conference on Robotics and Automation (ICRA), 15601565. IEEE. Cao, Z.; Gao, H.; Mangalam, K.; Cai, Q.-Z.; Vo, M.; and Malik, J. 2020. Long-term human motion prediction with scene context. In Proc. of European Conf. on Computer Vision, 387404. Springer. Christen, S.; Kocabas, M.; Aksan, E.; Hwangbo, J.; Song, J.; and Hilliges, O. 2022. D-grasp: Physically plausible dynamic grasp synthesis for hand-object interactions. In Proc. of IEEE Conf. on Computer Vision and Pattern Recognition, 2057720586. Comanici, G.; Bieber, E.; Schaekermann, M.; Pasupat, I.; Sachdeva, N.; Dhillon, I.; Blistein, M.; Ram, O.; Zhang, D.; Rosen, E.; et al. 2025. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, arXiv preprint and next generation agentic capabilities. arXiv:2507.06261. Corona, E.; Pumarola, A.; Alenya, G.; Moreno-Noguer, F.; and Rogez, G. 2020. Ganhand: Predicting human grasp affordances in multi-object scenes. In Proc. of IEEE Conf. on Computer Vision and Pattern Recognition, 50315041. Fang, H.; Fang, H.-S.; Xu, S.; and Lu, C. 2022. Transcg: large-scale real-world dataset for transparent object depth completion and grasping baseline. IEEE Robotics and Automation Letters, 7(3): 73837390. Fang, H.-S.; Wang, C.; Gou, M.; and Lu, C. 2020. Graspnet1billion: large-scale benchmark for general object grasping. In Proc. of IEEE Conf. on Computer Vision and Pattern Recognition, 1144411453. Gibson, J. J. 2014. The theory of affordances:(1979). In The people, place, and space reader, 5660. Routledge. Gou, M.; Fang, H.-S.; Zhu, Z.; Xu, S.; Wang, C.; and Lu, C. 2021. Rgb matters: Learning 7-dof grasp poses on monocular rgbd images. In International Conference on Robotics and Automation (ICRA), 1345913466. IEEE. Huang, S.; Wang, Z.; Li, P.; Jia, B.; Liu, T.; Zhu, Y.; Liang, W.; and Zhu, S.-C. 2023. Diffusion-based generation, optimization, and planning in 3d scenes. In Proc. of IEEE Conf. on Computer Vision and Pattern Recognition, 1675016761. Jia, Z.; Li, X.; Ling, Z.; Liu, S.; Wu, Y.; and Su, H. 2022. Improving policy optimization with generalist-specialist learnIn Proc. of Intl. Conf. on Machine Learning, 10104 ing. 10119. PMLR. Jiang, H.; Liu, S.; Wang, J.; and Wang, X. 2021. Handobject contact consistency reasoning for human grasps genIn Proc. of IEEE Conf. on Computer Vision and eration. Pattern Recognition, 1110711116. Kirillov, A.; Mintun, E.; Ravi, N.; Mao, H.; Rolland, C.; Gustafson, L.; Xiao, T.; Whitehead, S.; Berg, A. C.; Lo, W.- Y.; et al. 2023. Segment anything. In Proc. of IEEE Intl. Conf. on Computer Vision, 40154026. Li, H.; Zhao, Q.; Xu, H.; Jiang, X.; Ben, Q.; Jia, F.; Zhao, H.; Xu, L.; Zeng, J.; Wang, H.; et al. 2025a. TeleOpBench: Simulator-Centric Benchmark for Dual-Arm Dexterous Teleoperation. arXiv preprint arXiv:2505.12748. Li, K.; Li, P.; Liu, T.; Li, Y.; and Huang, S. 2025b. Maniptrans: Efficient dexterous bimanual manipulation transfer via residual learning. In Proc. of IEEE Conf. on Computer Vision and Pattern Recognition, 69917003. Li, X.; Liu, S.; Kim, K.; Wang, X.; Yang, M.-H.; and Kautz, J. 2019. Putting humans in scene: Learning affordance in 3d indoor environments. In Proc. of IEEE Conf. on Computer Vision and Pattern Recognition, 1236812376. Liu, T.; Liu, Z.; Jiao, Z.; Zhu, Y.; and Zhu, S.-C. 2021. Synthesizing diverse and physically stable grasps with arbitrary hand structures using differentiable force closure estimator. IEEE Robotics and Automation Letters, 7(1): 470477. Liu, Y.; Yang, Y.; Wang, Y.; Wu, X.; Wang, J.; Yao, Y.; Schwertfeger, S.; Yang, S.; Wang, W.; Yu, J.; et al. 2024. Realdex: Towards human-like grasping for robotic dexterous hand. arXiv preprint arXiv:2402.13853. Lu, D.; Kong, L.; Huang, T.; and Lee, G. H. 2025. Geal: Generalizable 3d affordance learning with cross-modal conIn Proc. of IEEE Conf. on Computer Vision and sistency. Pattern Recognition, 16801690. Lu, J.; Kang, H.; Li, H.; Liu, B.; Yang, Y.; Huang, Q.; and Hua, G. 2024. Ugg: Unified generative grasping. In Proc. of European Conf. on Computer Vision, 414433. Springer. Mahler, J.; Matl, M.; Satish, V.; Danielczuk, M.; DeRose, B.; McKinley, S.; and Goldberg, K. 2019. Learning ambidextrous robot grasping policies. Science Robotics, 4(26): eaau4984. Makoviychuk, V.; Wawrzyniak, L.; Guo, Y.; Lu, M.; Storey, K.; Macklin, M.; Hoeller, D.; Rudin, N.; Allshire, A.; Handa, A.; et al. 2021. Isaac gym: High performance gpubased physics simulation for robot learning. arXiv preprint arXiv:2108.10470. Mandikal, P.; and Grauman, K. 2021. Learning dexterous grasping with object-centric visual affordances. In international conference on robotics and automation (ICRA), 61696176. IEEE. Yang, L.; Wang, Y.; Li, X.; Wang, X.; and Yang, J. 2023. Fine-grained visual prompting. Proc. of Advances in Neural Information Processing Systems, 36: 2499325006. Zhan, X.; Yang, L.; Zhao, Y.; Mao, K.; Xu, H.; Lin, Z.; Li, K.; and Lu, C. 2024. Oakink2: dataset of bimanual handsobject manipulation in complex task completion. In Proc. of IEEE Conf. on Computer Vision and Pattern Recognition, 445456. Zhang, H.; Christen, S.; Fan, Z.; Hilliges, O.; and Song, J. 2024a. Graspxl: Generating grasping motions for diverse objects at scale. In Proc. of European Conf. on Computer Vision, 386403. Springer. Zhang, H.; Christen, S.; Fan, Z.; Zheng, L.; Hwangbo, J.; Song, J.; and Hilliges, O. 2024b. ArtiGrasp: Physically plausible synthesis of bi-manual dexterous grasping and articulation. In 2024 International Conference on 3D Vision (3DV), 235246. IEEE. Zhang, H.; Pan, Z.; Zhang, C.; Zhu, L.; and Gao, X. 2024c. Texpainter: Generative mesh texturing with multi-view consistency. In Acm siggraph 2024 conference papers, 111. Zhao, H.; Wang, H.; Zhao, X.; Wang, H.; Wu, Z.; Long, C.; and Zou, H. 2024a. Automated 3D Physical Simulation of Open-world Scene with Gaussian Splatting. arXiv preprint arXiv:2411.12789. Zhao, H.; Yang, C.; Wang, H.; Zhao, X.; and Shen, W. 2024b. Sg-gs: Photo-realistic animatable human avatars with semantically-guided gaussian splatting. arXiv e-prints, arXiv2408. Zhong, Y.; Jiang, Q.; Yu, J.; and Ma, Y. 2025. Dexgrasp anything: Towards universal robotic dexterous grasping with In Proc. of IEEE Conf. on Computer physics awareness. Vision and Pattern Recognition, 2258422594. Mandikal, P.; and Grauman, K. 2022. Dexvip: Learning dexterous grasping with human hand pose priors from video. In Conference on Robot Learning, 651661. PMLR. Mu, T.; Ling, Z.; Xiang, F.; Yang, D.; Li, X.; Tao, S.; Huang, Z.; Jia, Z.; and Su, H. 2021. Maniskill: Generalizable manipulation skill benchmark with large-scale demonstrations. arXiv preprint arXiv:2107.14483. Nagabandi, A.; Konolige, K.; Levine, S.; and Kumar, V. 2020. Deep dynamics models for learning dexterous maIn Conference on robot learning, 11011112. nipulation. PMLR. Radford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; et al. 2021. Learning transferable visual models from natural In Proc. of Intl. Conf. on Machine language supervision. Learning, 87488763. Rajeswaran, A.; Kumar, V.; Gupta, A.; Vezzani, G.; Schulman, J.; Todorov, E.; and Levine, S. 2017. Learning complex dexterous manipulation with deep reinforcement learning and demonstrations. arXiv preprint arXiv:1709.10087. Ross, S.; Gordon, G.; and Bagnell, D. 2011. reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, 627635. Schulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and Klimov, O. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347. Shao, Y.; Zhai, W.; Yang, Y.; Luo, H.; Cao, Y.; and Zha, Z.-J. 2025. Great: Geometry-intention collaborative inference for open-vocabulary 3d object affordance grounding. In Proc. of IEEE Conf. on Computer Vision and Pattern Recognition, 1732617336. Wan, W.; Geng, H.; Liu, Y.; Shan, Z.; Yang, Y.; Yi, L.; and Wang, H. 2023. Unidexgrasp++: Improving dexterous grasping policy learning via geometry-aware curriculum and iterative generalist-specialist learning. In Proc. of IEEE Intl. Conf. on Computer Vision, 38913902. Wang, C.; Fang, H.-S.; Gou, M.; Fang, H.; Gao, J.; and Lu, C. 2021. Graspness discovery in clutters for fast and accurate grasp detection. In Proc. of IEEE Intl. Conf. on Computer Vision, 1596415973. Wang, W.; Wei, F.; Zhou, L.; Chen, X.; Luo, L.; Yi, X.; Zhang, Y.; Liang, Y.; Xu, C.; Lu, Y.; et al. 2025. Unigrasptransformer: Simplified policy distillation for scalable dexterous robotic grasping. In Proc. of IEEE Conf. on Computer Vision and Pattern Recognition, 1219912208. Wu, Y.-H.; Wang, J.; and Wang, X. 2023. Learning generalizable dexterous manipulation from human grasp affordance. In Conference on Robot Learning, 618629. PMLR. Xu, Y.; Wan, W.; Zhang, J.; Liu, H.; Shan, Z.; Shen, H.; Wang, R.; Geng, H.; Weng, Y.; Chen, J.; et al. 2023. Unidexgrasp: Universal robotic dexterous grasping via learning diIn verse proposal generation and goal-conditioned policy. Proc. of IEEE Conf. on Computer Vision and Pattern Recognition, 47374746."
        },
        {
            "title": "6 Details about Our Method",
            "content": "Algorithm 1: Overall AffordDex Framework Require: Human hand motion dataset DH , object dataset DO Ensure: Final vision-based grasping policy πS 1: Stage 1: Human Hand Trajectory Imitating 2: Pre-train base policy πH on DH via imitation learning to establish human motion prior. 3: Stage 2: Affordance-aware Residual Learning and Distillation 4: Generate negative affordance point clouds {nO}ODO for all objects using the NAA module. 5: Learn state-based teacher policy πT by fine-tuning πH with residual module, guided by the negative affordances {nO}. 6: Distill the teacher policy πT into vision-based student policy πS via teacher-student framework (e.g., DAgger). 7: return Final vision-based policy πS."
        },
        {
            "title": "7 Details about Baselines",
            "content": "PPO. We use Proximal Policy Optimization (PPO) (Schulman et al. 2017), standard on-policy reinforcement learning (RL) algorithm, as the foundation for our RL-based training stages. DAPG. To leverage expert data, we employ DemoAugmented Policy Gradient (DAPG) (Rajeswaran et al. 2017). This imitation learning (IL) method accelerates policy training by combining policy gradient loss with behavior cloning loss on expert demonstrations. These demonstrations are generated via motion planning. GSL. We also include Generalist-Specialist Learning (GSL) (Jia et al. 2022), three-stage training paradigm. It first trains generalist policy, then fine-tunes specialized policies on difficult task subsets, and finally uses demonstrations from these specialists to train final, more capable generalist via IL. For fair comparison, our GSL implementation uses PPO for the RL components and DAPG for the final IL stage. ILAD. To further improve generalization, we compare against ILAD (Wu, Wang, and Wang 2023), an IL method that builds upon DAPG. It introduces an auxiliary objective that forces the policy to learn geometric object representation from the same motion-planned demonstrations, enhancing its adaptability. UniDexGrasp and UniDexGrasp++. UniDexGrasp (Xu et al. 2023) is two-stage learning method for dexterous grasping. In the first stage, it trains state-based teacher policy using Reinforcement Learning. To manage training across numerous objects, it introduces Object Curriculum Learning (OCL), which starts with single object and gradually incorporates more objects from similar semantic categories. In the second stage, this proficient teacher policy Figure 8: Illustration of the simulation environment. is distilled into vision-based student policy using DAgger (Ross, Gordon, and Bagnell 2011), enabling it to operate from point cloud inputs. Building upon this foundation, UniDexGrasp++ aims to significantly enhance the policys generalizability across thousands of object instances with diverse geometries. It argues that curriculum based on semantic categories can be suboptimal and instead proposes two novel, geometrycentric techniques: Geometry-aware Curriculum Learning (GeoCurriculum) and Geometry-aware iterative GeneralistSpecialist Learning (GiGSL). GeoCurriculum organizes the training progression based on object geometric features rather than categories, creating more effective learning path for grasping. GiGSL further refines the policy by iteratively training generalist model on all objects and specialist models on geometrically challenging subsets. These innovations lead to more robust universal grasping policy that substantially outperforms its predecessor. DexGrasp Anything. DexGrasp Anything (Zhong et al. 2025) is recent method for generating high-quality, static dexterous grasp poses. It utilizes diffusion-based generative model to address object diversity and hand complexity. Its core innovation is the direct integration of physical constraints (e.g., collision-freeness, stability) into both the training and sampling phases of the diffusion process. Since it generates only the final static grasp pose rather than grasping motion, we apply random external force ranging from 0 to 200N to the object to simulate objects gravity. For PPO, DAPG, GSL, ILAD, UniDexGrasp, and UniDexGrasp++, we maintain the same experimental settings as reported in the UniDexGrasp++ paper (Wan et al. 2023) to ensure fair comparison. For the DexGrasp Anything baseline, we directly utilized the officially released, pretrained model weights provided by the authors."
        },
        {
            "title": "8 Experiment Details",
            "content": "learning. We use We use PPO to train Reinforcement DAgger-based policy distillation to distill the state-based policy into vision-based one. Table 4: Ablation Study on UniDexGrasp dataset (Xu et al. 2023) in Seen Object in state-based setting. Configuration λsmooth = 0.02 λsmooth = 0.05 λsmooth = 0.1 λf inger = 0.5 λf inger = 0.8 λf inger = 1.0 Succ 89.0 89.2 88.2 87.8 89.2 88.5 HLS 7.9 8.6 8.5 8.5 8.6 8.2 AS 4 4 6 4"
        },
        {
            "title": "8.1 Experiment Setup",
            "content": "State Definition. The full state of the teacher policy ST = {Rt, Ot, Pt, Nt}. The complete object point clouds are assumed to be perfectly accurate. Object states Ot, including positions, rotations, and velocities, are directly accessible. To accelerate the training process, we sample 1024 points from the object and the hand in the scene point cloud Pt. The full state of the student policy SS = {Rt, Pt, Nt}. Partial object point clouds are reconstructed and segmented from depth data captured by five cameras around the table. The hand-object distance is computed using the partial object point cloud in the vision-based setting. Action Space. The action space consists of the motor commands for the 24 actuators of the dexterous hand. The first 6 actuators control the global position and orientation, while the remaining 18 control the finger joints. We normalize the action range to (1, 1). Camera Setup. Following similar approach to UniDexGrasp++ (Wan et al. 2023), five RGBD cameras are mounted around the table, as shown in Fig. 8. The cameras are positioned relative to the table center at coordinates (0.0, 0.0, 0.55), (0.5, 0.0, 0.15), (0.5, 0.0, 0.15), (0.0, 0.5, 0.15), (0.0, 0.5, 0.15), with their focal points aligned at (0, 0, 0.15). In the vision-based setting, the depth images captured by these cameras are fused to generate scene point cloud, from which the partial point cloud of the object is segmented. Reward Function for Human Hand Trajectory Imitating. The goal of our human hand trajectory imitation reward, rH is to encourage the agent to mirror the articulation and motion smoothness of reference human trajectory. It is formulated as weighted sum of two key components: finger imitation reward and smoothness reward. The finger imitation reward rH finger encourages the agents hand to accurately track the reference finger poses. We define this reward based on the squared Euclidean distance between corresponding keypoints on the agents hand and the human reference hand: rH finger = (cid:88) =1 (cid:16) wf exp λf jd,f jh,f 2 2 (cid:17) , (6) where jd,f is the position of the -th keypoint on the dexterous hand, and jh,f is its corresponding target position from the reference trajectory. The parameters wf and λf are set according to the anatomical group to which keypoint belongs. Specifically, we group keypoints into two levels: For keypoints in Level-1 (base joints), we use stricter decay rate of λf = 50. For keypoints in Level-2 (middle joints), we use λf = 40. The weights wf are configured to combine these rewards appropriately. This hierarchical parameterization allows the model to prioritize accurate positioning of the more critical base joints."
        },
        {
            "title": "The smoothness reward rH",
            "content": "smooth is designed to alleviate jerky motions, penalizing the power exerted on each joint, defined as the element-wise product of joint velocities and torques rsmooth = wsmooth (cid:88) i=1 τt,i qt,i , (7) where wsmooth is positive weighting coefficient, is the total number of actuated joints, and τt,i and qt,i are the torque and angular velocity of the i-th joint at timestep t, respectively. The product τt,i qt,i represents the instantaneous power exerted by the actuator of joint i. By taking the absolute value, we penalize any high-power action, including both strong acceleration and aggressive braking, thus encouraging the policy to learn energy-efficient and hardwarefriendly motions. The final reward in this stage is: rH = λsmoothrsmooth + λfingerrH finger, (8) where we set λsmooth = 0.05 and λfinger = 0.8. We also conduct ablation studies on these weights in Tab. 4. Reward Function for Affordance-aware Residual Learning. The reward function described in Eq.(4) of the main paper comprises four components: grasp reward rT , goalreaching reward rT , and negative affordance penalty rT , success bonus rT . Each component is detailed below. The grasp reward rT encourages the hand to approach and stay close to the object. It is defined as penalty proportional to the distance between the hand and the objects center: = λT rT pdex pobj, (9) where pdex and pobj are the Cartesian position of the dexterous hand and the object, respectively. We set λT to -1. The goal reward rT guides the object towards the target goal. It is structured as penalty on the distance between the object and the goal: = λT rT pobj pgoal, (10) where pgoal is the position of the target goal. We set λT to -1. bonus is provided upon task completion. This is triggered when the object enters small threshold radius around the goal: = λT rT I(pobj pgoal < αs), (11) Table 5: Ablation Study on UniDexGrasp dataset (Xu et al. 2023) in Seen Object in state-based setting. Configuration λT = 0.5 λT = 1 λT = 2 λT = 0.5 λT = 1 λT = 2 λT = 0.5 λT = 1 λT = 2 λT = 5 λT = 10 λT = Succ 84.2 89.2 88.4 87.9 89.2 88.9 83.1 89.2 89.0 89.1 89.2 87.1 HLS 7.2 8.6 8.4 8.1 8.6 8.3 7.0 8.6 8.5 8.5 8.6 8.0 AS 5 4 4 4 4 6 6 4 5 16 4 0 to 1. where I() is the indicator function, which evaluates to 1 if the condition is true and 0 otherwise. αs is set to 0.05. We set λT The negative affordance reward rT penalizes the dexterous hand fingertips to approach the predicted negative affordance: = λT rT (cid:18) min pnPn (cid:88) f Kc pf tip pn < αn (cid:19) , (12) where αn is set to 0.03, Kc is the set of the hands fingertips, pf tip is the position of fingertip , and Pn is the set of points representing the negative affordance. We set λT = 10. Ablation studies are conducted on these weights to evaluate their contribution. The results are presented in Hyperparameters part. Human-likeness Score (HLS). The Human-likeness Score (HLS) is designed to quantify the anthropomorphic quality of the generated grasping motion. We leverage the advanced multi-modal capabilities of Gemini 2.5 Pro (Comanici et al. 2025) to serve as an expert evaluator. For each grasp, video sequence of the complete grasp execution is provided as input to the model. The model is then prompted with specific set of instructions to assess the motion based on key kinematic criteria. The exact prompt provided to the model is as follows: Expert Role: You are an expert in hand kinematics evaluation. Task: Evaluate the similarity between the robotic hand motion in the simulation video and that of real human hand, based on the following three criteria: Motion trajectory Velocity smoothness Joint coordination Output Format: Return only valid JSON, following this format: { \"score\": <1-10> }. VLM-based Negative Affordance Identification VLMs struggle to interpret non-textured 3D meshes, as these models primarily rely on rich visual cues learned from images. To bridge this gap, we first apply procedural texturing to the raw meshes using Tex-Painter (Zhang et al. 2024c), which generates semantically plausible textures based on geometric analysis. Next, we render the textured object from six cardinal directions to create multi-view image set, I, as holistic visual representation. While this may not capture all concavities in highly complex objects, we found it provides sufficient basis for affordance prediction for the objects in our benchmark datasets. We then query GPT-4V (Achiam et al. 2023) with the multi-view images to elicit description of the objects negative affordances. The exact prompt is as follows: Expert Role: You are an expert in identifying which part of physical object should not be touched, especially in robotic grasping tasks. Task: You will be given 6-view images of an object. Your task is: Identify one non-touchable part. Infer the objects identity from the image name. Output Format: Return one sentence only, following this format: \"This is not be touched.\" should of . Points selected in Negative Affordance-aware Segmentation The core idea is to systematically prompt SAM with dense, regular grid of points covering the entire image. This process ensures that objects of various sizes and locations are likely to be hit by at least one point prompt, triggering SAM to segment them. The procedure consists of the following steps: 1. Grid Definition: We define regular grid of points to be overlaid on the input image. The density of this grid is controlled by single hyperparameter, = 16, which represents the number of points along each of the images dimensions. This results in total of g2 keypoints. 2. Keypoint Generation: For an input image with width and height H, the set of grid points is generated. The coordinates (xi, yj) for each point are calculated by uniformly spacing them across the image dimensions. The formula for point at grid position (i, j) is: (cid:26) = (xi, yj) xi = + 1 W, yj = (cid:27) + 1 (13) for all i, {1, 2, . . . , g}. Using + 1 as the denominator ensures that the points are placed within the image boundaries and not on the absolute edges. 3. Prompting SAM: Each of the g2 points in the set is used as an individual positive point prompt for SAM. This yields large collection of raw segmentation masks, Mraw = SAM(I, G), where many masks may be redundant or highly overlapping. 4. Filtering with NMS: To produce refined set of unique proposals, we apply Non-Maximum Suppression (NMS) to the raw masks. The NMS algorithm filters out duplicate masks based on the Intersection over Union (IoU) of their corresponding bounding boxes, retaining only the most confident and distinct object proposals. Hyperparameters. As shown in Tab. 5, we conduct series of ablation studies to validate our design choices and analyze the models sensitivity to key hyperparameters. Our investigation focuses on the weights of our proposed reward function. The goal is to find robust set of parameters that balances efficient learning with task performance. The results indicate that our chosen default configuration is robust and well-justified. For the grasp reward weight λT and the goal reward weight λT , we observe an optimal value of -1.0. Deviating from this value, by either decreasing the penalty to -0.5 or increasing it to -2.0, leads to degradation in both Succ and HLS. This suggests critical balance: penalty that is too weak fails to prevent undesirable actions, while an overly strong penalty can cause the policy to focus myopically on avoiding penalties rather than achieving stable grasp, ultimately degrading task success. Similarly, for the bonus reward weight λT , the optimal value is 1.0. lower weight of 0.5 fails to sufficiently incentivize the target behavior, resulting in significant drop in performance. Conversely, increasing the weight to 2.0 offers no additional benefit and slightly hinders performance, indicating that an excessive bonus can also be detrimental. Similarly, the negative affordance weight λT requires careful tuning. An overly strong penalty can make the agent too conservative, causing it to avoid the target object altogether in its effort to steer clear of negative affordances. Conversely, an insufficient penalty fails to effectively deter the agent from frequently approaching these undesirable regions."
        }
    ],
    "affiliations": [
        "DAMO Academy, Alibaba Group",
        "Hupan Lab",
        "Tsinghua University",
        "Wuhan University",
        "Zhejiang University"
    ]
}