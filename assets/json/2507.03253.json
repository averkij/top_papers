{
    "paper_title": "RefineX: Learning to Refine Pre-training Data at Scale from Expert-Guided Programs",
    "authors": [
        "Baolong Bi",
        "Shenghua Liu",
        "Xingzhang Ren",
        "Dayiheng Liu",
        "Junyang Lin",
        "Yiwei Wang",
        "Lingrui Mei",
        "Junfeng Fang",
        "Jiafeng Guo",
        "Xueqi Cheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The foundational capabilities of large language models (LLMs) are deeply influenced by the quality of their pre-training corpora. However, enhancing data quality at scale remains a significant challenge, primarily due to the trade-off between refinement effectiveness and processing efficiency. While rule-based filtering remains the dominant paradigm, it typically operates at the document level and lacks the granularity needed to refine specific content within documents. Inspired by emerging work such as ProX, we propose $\\textbf{RefineX}$, a novel framework for large-scale, surgical refinement of pre-training data through programmatic editing tasks. RefineX enables efficient and fine-grained data refinement while reliably preserving the diversity and naturalness of raw text. The core strength of RefineX lies in distilling high-quality, expert-guided end-to-end refinement results into minimal edit-based deletion programs. This high-precision distillation pipeline is used to train an efficient and reliable refine model that can systematically improve every instance in the corpus at scale. We evaluate RefineX across from-scratch pre-training at multiple model scales and find that it consistently outperforms models trained on raw, filtered, or alternatively refined data across diverse downstream tasks. On the 750M model, RefineX yields 2.6%-7.2% average gains on lighteval tasks, and achieves comparable performance using significantly fewer training tokens. Further analysis shows that RefineX reliably enhances text quality with both high efficiency and precision, outperforming prior approaches such as end-to-end generation and Prox-C. These results position RefineX as a scalable, effective, and reliable solution for optimizing pre-training data in modern LLM pipelines."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 3 5 2 3 0 . 7 0 5 2 : r a"
        },
        {
            "title": "Under review as a conference paper",
            "content": "REFINEX: LEARNING TO REFINE PRE-TRAINING DATA AT SCALE FROM EXPERT-GUIDED PROGRAMS Baolong Bi1 Shenghua Liu1 Xingzhang Ren2 Dayiheng Liu2 Junyang Lin2 Yiwei Wang3 Lingrui Mei1 Jiafeng Guo1 Xueqi Cheng1 Junfeng Fang4 1Institute of Computing Technology, Chinese Academy of Sciences 2Alibaba Group 3University of California, Merced 4National University of Singapore {bibaolong23z, liushenghua}@ict.ac.cn, liudayiheng.ldyh@alibaba-inc.com"
        },
        {
            "title": "ABSTRACT",
            "content": "The foundational capabilities of large language models (LLMs) are deeply influenced by the quality of their pretraining corpora. However, enhancing data quality at scale remains significant challenge, primarily due to the trade-off between refinement effectiveness and processing efficiency. While rule-based filtering remains the dominant paradigm, it typically operates at the document level and lacks the granularity needed to refine specific content within documents. Inspired by emerging work such as ProX, we propose REFINEX, novel framework for large-scale, surgical refinement of pretraining data through programmatic editing tasks. REFINEX enables efficient and fine-grained data refinement while reliably preserving the diversity and naturalness of raw text. The core strength of REFINEX lies in distilling high-quality, expert-guided end-to-end refinement results into minimal edit-based deletion programs. This high-precision distillation pipeline is used to train an efficient and reliable refine model that can systematically improve every instance in the corpus at scale. We evaluate REFINEX across from-scratch pretraining at multiple model scales and find that it consistently outperforms models trained on raw, filtered, or alternatively refined data across diverse downstream tasks. On the 750M model, REFINEX yields 2.6%-7.2% average gains on lighteval tasks, and achieves comparable performance using significantly fewer training tokens. Further analysis shows that REFINEX reliably enhances text quality with both high efficiency and precision, outperforming prior approaches such as end-to-end generation and Prox-C. These results position REFINEX as scalable, effective, and reliable solution for optimizing pretraining data in modern LLM pipelines."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large language models (LLMs) (Meta, 2024; Achiam et al., 2023; Anthropic, 2024; Yang et al., 2025) represent major milestone in the development of artificial intelligence, demonstrating impressive capabilities across wide range of tasks, including natural language understanding (Ni et al., 2025; Mei et al., 2024), question answering (Zhuang et al., 2023), complex reasoning(Wei et al., 2022; Liu et al., 2025), and agentic task planning and execution (Fan et al., 2022; Park et al., 2023). Underpinning these capabilities is the quality of the pretraining corpus, which serves as the fundamental source of both knowledge and reasoning logic (Together, 2023; Penedo et al., 2024a). The internet offers vast supply of pretraining data for LLMs (Ravinder et al., 2024), but much of it is noisy and unrefined, including spam, meaningless advertisements, and corrupted or incoherent text. Such low-quality content degrades data utility and increases hallucination risk (Huang et al., 2023; Tonmoy et al., 2024). Thus, scalable refinement of pretraining data is increasingly viewed as critical step to push the performance limits of LLMs. In particular, refinement must meet two key criteria: (1) Efficiency: given the massive data volume for pretraining corpus, refinement must be Authors from affiliation1 are also affiliated with: Key Laboratory of Network Data Science and Technology, ICT, CAS; State Key Laboratory of AI Safety; University of Chinese Academy of Sciences. Corresponding authors."
        },
        {
            "title": "Under review as a conference paper",
            "content": "scalable and low-cost; (2) Reliability: it must preserve valuable information and avoid introducing additional bias from models or human preferences that could distort the essence of raw data. Meeting both of these objectives simultaneously is highly challenging. To ensure efficiency, heuristic methods often sacrifice granularity by operating at the document level rather than refining specific content within documents. Rule-based pipelines using document filters (Rae et al., 2021; Penedo et al., 2024a; Soldaini et al., 2024) or perplexity scores (Together, 2023) retain only data meeting rigid criteria, resulting in limited coverage and missed opportunities. Recent work (Xie et al., 2023; Wettig et al., 2024; Yu et al., 2024; Dubey et al., 2024) incorporates LLMs for filtering, but these approaches remain coarse-grained or incur high computational costs due to their generative nature. As result, they often discard potentially valuable data and fail to improve the quality of what is retained. The emergence of ProX (Zhou et al., 2024) offered promising direction by showing that small distilled LMs can efficiently predict refinement programs with very few tokens, which are then executed to yield refined text at low cost. However, the ProX pipeline suffers from reliability issues due to the complexity of its distillation process. The refinement programs generated from expert models often diverge from the intended behavior, making the resulting refine model unreliable. Inspired by ProX, we introduce REFINEX, novel large-scale refinement framework that builds on efficient program-based data refinement while significantly improving reliability through carefully constructed distillation data. The key limitation of ProX lies in training directly on noisy refinement programs derived from expert outputs, which often fail to reflect high-quality refinement operations. In contrast, REFINEX first leverages expert models to generate reliable end-to-end refined texts. Quality evaluation experiments show that these end-to-end outputs achieve the greatest improvements in text quality. However, while these generations offer strong quality gains, they are expensive to produce and carry over-editing risks, as model-specific preferences may be imposed during generation, compromising the original data. To mitigate this, REFINEX employs minimum edit distance algorithm to identify the minimal sequence of operations required to transform the original text into the expert-refined output. It filters out potentially over-aggressive edits such as insertions and replacements that may introduce model-specific preferences, and retains only high-quality deletion operations, which are encapsulated into predefined functions. Overall, REFINEX structures the construction of distillation data into two explicit stages: first performing end-to-end refinement, then generating supervision programs by comparing the refined text with the original. This separation enables the creation of cleaner and more reliable distillation data, which is then used to train refine model capable of generating efficient and trustworthy refinement programs at scale. To evaluate the effectiveness of REFINEX, we conduct from-scratch pretraining at multiple model scales (350M and 750M) on corpora refined by range of baselines. These include various documentlevel filtering strategies (e.g., C4, Gopher, FineWeb, and their combination), as well as Prox-C, the strongest prior fine-grained, program-based refinement method. All corpora are constructed from RedPajama (Together, 2023) with fixed 20B-token budget and are evaluated across 10 LightEval (Fourrier et al., 2023) tasks. On the 750M model, REFINEX consistently achieves the best average performance across different data settings, yielding 2.6%7.2% gains over other baselines. Moreover, it matches or exceeds the performance of other methods using significantly fewer training tokens, demonstrating improved data efficiency. In addition to end-task performance, we conduct detailed evaluation of refined text using DataMan (Peng et al., 2025), which shows that REFINEX delivers the highest quality improvement aside from E2E generation-while incurring lower token overhead, introducing no new content, and avoiding the risks of over-editing. These results demonstrate that REFINEX achieves both high efficiency and strong reliability, establishing it as practical and scalable solution for pretraining data refinement."
        },
        {
            "title": "2 BACKGROUND",
            "content": "This section summarizes the main technical paradigms for pretraining data refinement, including quality-based filtering, end-to-end refinement, and program-based refinement. We analyze their respective strengths and limitations in terms of efficiency and reliability as follows. Quality-Based Filtering Filtering-limited refinement selects pretraining data based on estimated document quality, typically using heuristic rules or LLM-assigned scores. Heuristic filtering (Rae et al., 2021; Penedo et al., 2024a; Soldaini et al., 2024) applies predefined rules-such as thresholds on URL ratio, garbled character proportion, document length, or repetition-to efficiently remove"
        },
        {
            "title": "Under review as a conference paper",
            "content": "Figure 1: Overview of the program-based refinement pipeline and comparison of training data construction for the distilled refine model in ProX and REFINEX. While ProX generates refine programs using complex prompts prone to hallucination, REFINEX first produces high-quality endto-end refined texts and derives reliable refine programs by comparing them with the original input. low-quality documents. However, these rules lack flexibility for instance-level variation and often discard useful content while retaining text that still needs refinement. LLM-based methods (Xie et al., 2023; Wettig et al., 2024; Yu et al., 2024) use perplexity or model-assigned quality scores but can be unstable, biased, and computationally expensive. Importantly, most filtering operates at the document or sentence level, lacking the fine-grained resolution needed to improve content at the character or sub-span level. As result, the reliability of the refined text remains limited. End-to-End Refinement End-to-end refinement directly prompts powerful LLM to rewrite given text under refinement-specific instructions. While carefully designed prompts can yield high-quality outputs, this approach faces two key limitations in practice: (1) It is prohibitively expensive. Since the model generates text at the same scale as the input, end-to-end refinement incurs high inference costs, making it infeasible for large-scale deployment. (2) It may compromise data reliability. Despite explicitly constrained instructions, end-to-end models tend to over-edit by modifying sentence structure, over-correcting spelling, or applying stylistic preferences. These behaviors can introduce model bias and reduce the diversity of the raw data. Further implementation and performance details of this approach are discussed in Section 4.3. Program-Based Refinement Program-based refinement, introduced by ProX (Zhou et al., 2024), offers promising pipeline that jointly achieves efficiency and reliability for refinement. It prompts expert models to generate programmatic editing functions on seed examples, and trains distilled small model to predict such refinement programs for large-scale inference and execution. The effectiveness of this approach heavily depends on the quality of the distillation data. However, prompting expert models to directly generate editing programs often fails to yield reliable supervision. This is because it requires the model not only to determine how to refine, but also to articulate the corresponding transformation logic, which introduces both modeling and supervision complexity. This often leads the trained refine model to generate hallucinated or malformed programs, resulting in execution failures or incorrect edits that compromise data quality. In this work, we follow ProXs core pipeline by distilling small model capable of efficiently inferring and executing refinement programs. Figure 1 illustrates the key conceptual and procedural differences between our approach and ProX. Unlike ProX, which directly prompts expert models to generate programs, REFINEX first performs end-to-end refinement and then constructs supervision programs by comparing the refined outputs with the original text. This two-stage process yields significantly more reliable supervision and effectively eliminates over-editing risks introduced during generation, ultimately leading to more effective and robust refine model."
        },
        {
            "title": "3 REFINEX: SCALABLE REFINEMENT WITH EFFICIENCY AND RELIABILITY",
            "content": "Figure 2 illustrates the core workflow of REFINEX. The goal of REFINEX is to reduce the difficulty for expert models to directly generate refinement programs for distillation, while preserving as many valid refinement operations from end-to-end outputs as possible. To achieve both objectives, REFINEX first prompts an expert model to generate high-quality refined text under carefully designed"
        },
        {
            "title": "Under review as a conference paper",
            "content": "Figure 2: An overview of the REFINEX framework. (1) During training, REFINEX prompts an expert model to generate high-quality refined text under instructions, then extracts valid deletions via minimal edit distance. These are converted into program functions to supervise reliable refine model. (2) At inference time, the trained model generates fine-grained refinement programs for each document, which are executed by Python interpreter to produce the final refined corpus. instructions. It then compares the refined text with the raw input to extract reliable sequence of deletion operations based on minimal edit distance. These operations are converted into predefined set of program functions (see Section 1), serving as trusted supervision to train compact refine model. Once trained, the model generates reliable refinement programs through inference, which are then executed to efficiently perform fine-grained refinement across the corpus."
        },
        {
            "title": "3.1 PRELIMINARIES",
            "content": "Refinement Task Definition As discussed in the comparison in Section 2, our focus is on finegrained refinement at the character level for individual text instances, rather than documentor sentence-level quality-based filtering. Formally, given an input text t, we apply an executor to transform it into higher-quality version ˆt. In our REFINEX, we constrain to deletion-only operations, allowing us to remove advertisements, meaningless URL links, random code gibberish, and other low-value content occurring anywhere in the text. Moreover, this restriction effectively prevents the introduction of model-driven stylistic preferences (Bi et al., 2025) that could compromise the authenticity of the original text-as observed in end-to-end refinement  (Table 14)  . This also means that minor imperfections, such as spelling errors, are allowed to remain, as they are typically neutralized during large-scale pretraining through distributional memorization. Concretely, we define deletion operation sequence as Odel = {d1, d2, ..., dOdel}, where each dj denotes contiguous span of character indices to be removed. We define the following execution process to refine the original text t: E(Odel, t) = (c i)t i=1, where = if ci in (dj)Odel j=1 else ci (1) Here, = (c1, c2, ..., ct) denotes the original input represented as sequence of characters. By constraining the refinement to deletion-only operations, the task formulation preserves the authenticity of the raw text while effectively removing irrelevant or low-value content. Program Function Design Based on the deletion-only refinement task, we design specific set of program functions for REFINEX. This design is crucial for achieving efficient and reliable refinement, and it must satisfy two key objectives: (1) minimal and simple function set to reduce redundancy and mitigate the risk of hallucination from overly complex or ambiguous instructions, and (2) efficient program generation to avoid efficiency degeneration during inference. straightforward idea is to specify exact character indices for deletion within the program. However, predicting precise character spans is extremely challenging for LLMs. Moreover, directly providing the target strings to be deleted may result in overly long deletion content, which in turn causes the generated function instructions to become excessively long in token length, significantly degrading generation efficiency."
        },
        {
            "title": "Description",
            "content": "remove_lines(start_line, end_line) Deletes all content between start_line<int> and end_line<int> remove_str(line, del_str) Deletes del_str<int> from line<int> only if it occurs exactly once keep_all() <str> Return the original text. Table 1: Program function definitions in REFINEX, designed to compactly cover refinement operations. Given an input text, the trained refine model outputs these functions to perform optimization. Inspired by ProX (Zhou et al., 2024), we design three program functions in REFINEX, as summarized in Table 3.1. The remove_lines() function addresses long-span deletions by allowing multiple consecutive lines to be removed using compact token representation. The remove_str() function supports fine-grained character-level deletion to ensure high precision. While remove_str() poses risk when multiple matching substrings appear on the same line, we mitigate this through careful data selection in Section 3.2 and execution checks in Section 3.3. Additionally, the keep_all() function is introduced to indicate that no refinement is needed, avoiding empty or ambiguous outputs. These program functions are designed to be both comprehensive and minimal, reducing the risk of hallucination and maximizing refinement coverage under strict token budget constraints."
        },
        {
            "title": "3.2 TRAINING A REFINER WITH RELIABLE PROGRAM SUPERVISION",
            "content": "The key to training small-scale language model to generate refinement programs lies in obtaining high-quality distillation data. However, directly prompting expert LLMs to generate standardconforming programmatic data is challenging. As noted by Zhuo et al. (2024), generating customized API calls is difficult even for state-of-the-art models, as it requires both reasoning about solution steps and formatting them into precise function calls. Although ProX improves this process using few-shot prompts and post-inference filtering, reliance on heuristics and fixed thresholds introduces significant reliability issues into the distilled data. To address this, REFINEX adopts two-stage approach: it first generates end-to-end refined text using expert models, then derives reliable supervision programs by comparing the refined outputs with the original input. This design improves the consistency and trustworthiness of the data used to train the refine model. End-to-End Refinement Rather than directly guiding the expert model to generate refinement programs, we first instruct it to produce end-to-end refined text. Specifically, we design instruction templates aligned with the REFINEX task, which prompt the model to first reason about why given text should be modified, and then output refined version tE. These prompts incorporate carefully constructed rules and examples, as detailed in Appendix A.2. As shown in Table 3, end-to-end refinement achieves the highest quality gains among all strategies, largely due to its token-level granularity and unconstrained rewriting flexibility. The substantial improvements over ProX are attributed to the fact that end-to-end refinement eliminates the burden of function generation, allowing the model to focus solely on improving the text. Function Conversion via Minimal Edit Distance Despite its high quality, end-to-end refinement suffers from expensive inference costs, making it impractical for large-scale deployment. Additionally, even with explicit instructions and examples, LLMs may over-edit the original text-introducing unintended stylistic changes or semantic drift (see Table 14). To reconcile reliability and efficiency, REFINEX converts the valid portions of end-to-end outputs into executable refinement programs. To achieve this, we use minimal edit distance (Levenshtein distance) (Yujian & Bo, 2007) to calculate the fewest number of edit operations required to transform the original text into the end-to-end refined version, along with the corresponding minimal operations. These edit operations include three basic types: insertion, replacement, and deletion. The principles and implementation details of the minimal edit distance algorithm can be found in Appendix A.1. Since our refinement task is limited to deletions, we capture only the deletion operations from the minimal edit distance, discarding insertions and substitutions (which typically correspond to excessive modifications in the end-to-end refinement process). The deletion operations can be formally expressed as follows: Odel Omin, where Omin = {Odel, Oins, Orep} and E(Omin, t) = tE (2) Here, Oins and Orep denote the insertion and replacement operations under the minimal edit distance. These program representations preserve the core effects of end-to-end refinement while avoiding overcorrections, and they can be applied efficiently and reliably through token-light function calls."
        },
        {
            "title": "Under review as a conference paper",
            "content": "Distilling and Training the Refine Model To further improve the quality of supervision, we apply additional post-processing to the converted program data. We split long texts into multiple overlapping chunks to balance long-range context understanding with the limited capacity of the small-scale model. Each chunk is capped at 12k tokens, and to maximize coverage, content repetition averagely across chunks is permitted to ensure full utilization of the models attention window. We also enforce strict filtering criteria: transformations involving overly long insertions or substitutions, or very short deletions, are discarded to ensure reliability. Only high-confidence deletion programs are retained for distillation. To construct the training corpus, we sample large-scale seed data aligned with the distribution of real-world web-crawled corpora. We use Qwen2.5-72B-Instruct (Yang et al., 2025) as the expert model to generate end-to-end refined text, consuming approximately 12,480 GPU hours on H800-80G GPUs to process nearly 5 million raw documents. Following the conversion and filtering steps described above, we obtain final corpus of approximately 2 million high-quality distillation examples, which are used to train 0.6B Qwen-3-Base model as our REFINEX refiner."
        },
        {
            "title": "3.3 PROGRAM EXECUTION AT SCALE",
            "content": "The distilled refine model can efficiently generates refinement programs for large-scale pretraining corpora using high-quality distillation data. The final refined text is obtained by sequentially executing the predefined functions according to the generated program. In rare cases where the same string appears multiple times on single line, we skip the corresponding remove_str() operation to avoid unintended deletions. For long texts that are refined in multiple chunks, we apply an offset to the line indices in each chunks program to align them with the original text, ensuring proper integration into unified refinement. We believe that the refine model produced by the REFINEX pipeline enables scalable and reliable refinement of arbitrary text inputs."
        },
        {
            "title": "4.1 EXPERIMENTAL SETUP",
            "content": "To assess the impact of refined data on model performance, we pretrain LLMs of different sizes from scratch on corpora refined by each method and evaluate them across downstream tasks (Section 4.2). We also conduct detailed analysis of individual text instances to further investigate refinement effects (Section 4.3). The pretraining setup is detailed below. Pretraining Corpus and Base Models We use RedPajama-V2 (Together, 2023), large-scale corpus of 300 trillion tokens sourced from diverse web content, as our pretraining dataset. Starting from its publicly released 400B-token subset, we apply various refinement baselines to construct corresponding 20B-token corpora for each method. These are used to pretrain two base models with 350M and 750M parameters, respectively, both following the LLAMA-2 architecture (Touvron et al., 2023b), enabling consistent evaluation across model scales. Evaluation Tasks and Baselines We evaluate each pretrained model after one epoch of training on ten downstream tasks using the official implementation from LightEval (Fourrier et al., 2023). To ensure fair comparison across approaches, we consider three settings for data preparation: filtering only, fine-grained refinement only, and their combination. Fine-grained refinement is applied on top of various filtered datasets, and the full set of filtering and refinement baselines used in our experiments is described below. Quality-Based Filtering: We include several rule-based filtering baselines commonly used for corpus cleaning, including C4 (Raffel et al., 2020), Gopher (Rae et al., 2021), and FineWeb (Penedo et al., 2024a), as well as their combined variant, COMB (Go + C4 + Fw). In addition, we consider Prox-D, state-of-the-art LLM-based filtering method that distills scoring instructions from expert LLMs to decide whether to retain each document. Prox-C Refine Prox-C directly distills the program-based refinement ability of expert LLMs and performs chunk-level refinement. We follow their setting of splitting text into 1.5K-token chunks. Further details on the pretraining and evaluation setups can be found in Appendix and C. In our experiments, the refine model trained with REFINEX using the Qwen3 0.6B Base (Yang et al., 2025) is applied to both raw and quality-filtered corpora. We compare its downstream performance with that of Prox C, using the respective refined datasets for pretraining."
        },
        {
            "title": "Method",
            "content": "ARC-C ARC-E CSQA HellaS MMLU OBQA PIQA SIQA WinoG SciQ Avg #Win"
        },
        {
            "title": "24.5\nRaw\n+ Prox-C\n25.3\n+ REFINEX 25.2",
            "content": "45.4 46.5 45.9 30.5 30.2 32.0 38.0 38.2 39.4 27.5 27.8 27.5 28.2 31.0 31.2 63.8 66.9 66. 39.8 39.9 41.0 51.0 51.9 52.5 67.0 41.6 0 / 10 66.4 42.4 4 / 10 68.3 42.9 6 / 10 Rule-based filtering: GO = Gopher rules, C4 = C4 rules, FW = FineWeb rules, COMB = Go + C4 + Fw."
        },
        {
            "title": "25.1\nProx-D\n+ Prox-C\n27.2\n+ REFINEX 28.7",
            "content": "45.1 46.0 48.2 44.2 45.5 44.5 45.6 44.7 47.4 44.7 45.4 46.6 45.6 51.0 53.2 28.6 30.9 30. 30.9 31.7 31.8 31.6 30.8 31.4 31.0 31.7 31.4 31.6 30.1 30.8 39.7 41.0 41.4 39.3 41.1 41. 39.9 39.0 40.1 40.4 41.1 42.1 27.1 27.8 28.1 27.4 27.9 28.1 27.2 27.9 27.6 27.1 27.8 27. 28.4 30.4 29.0 29.4 29.4 30.4 27.6 29.2 30.8 28.6 29.4 28.0 66.7 66.7 66.7 66.5 66.6 68. 66.6 67.4 66.6 66.2 66.7 68.0 LLM-based filtering: Prox-D 39.9 41.2 41.7 27.2 28.9 29.6 27.6 30.4 31. 66.6 65.9 67.8 39.4 38.4 40.7 39.5 40.1 41.2 39.3 39.5 39.6 39.0 39.5 39.6 39.3 39.4 39. 50.9 51.5 52.8 51.3 51.4 50.5 49.9 51.3 51.5 53.3 51.7 53.2 49.9 50.8 51.6 66.9 41.7 0 / 10 68.4 42.6 3 / 10 69.3 43.2 7 / 67.3 42.1 0 / 10 66.6 42.5 4 / 10 68.2 42.9 6 / 10 66.2 41.9 1 / 10 67.5 42.2 3 / 10 65.4 42.7 6 / 10 66.5 42.1 2 / 10 65.8 42.4 3 / 10 65.9 42.8 5 / 10 67.3 42.0 1 / 10 70.2 43.5 0 / 10 70.9 44.7 9 / 10 Table 2: Performance of 750M pretrained models on 10 selected downstream tasks. All models are trained on 20B-token corpora of equal size, derived from different sources: Raw (unfiltered corpus), various filtering strategies, and further fine-grained refinement applied to the filtered corpora. Underlined results indicate the best performance within the Raw group or within each specific filtering group. #Win represents the number of tasks for which the method achieves the best performance within its group. Bolded results indicate the best overall performance across all settings."
        },
        {
            "title": "4.2 EVALUATION ON PRETRAINED LANGUAGE MODELS",
            "content": "We evaluate the effectiveness of data refinement by examining the performance of language models trained from scratch. Specifically, we begin by applying several document-level filtering methods, including Gopher, C4, FW, COMB, and Prox-D, to the raw RedPajama-V2 corpus. The filtered datasets, along with the original raw data, are then used to pretrain models from scratch, serving as our baseline setups. On top of these filtered datasets, we further apply fine-grained refinement using Prox-C and our proposed method, REFINEX, enabling fair and comprehensive comparison across refinement strategies. For each baseline, we ensure that the post-filtering or post-refinement data contains significantly more than 20B tokens. To control for total compute, we fix the number of training steps at 10,000, with each step consisting of 1,024 global batches, and each batch containing 2048 training tokens-effectively capping the total training token budget at approximately 20B tokens. Evaluating each baseline requires training model on its corresponding corpus, consuming approximately 1,728 GPU hours on H800-80G GPUs. Figure 3: Downstream average performance (%) of model checkpoints with different numbers of training tokens during pretraining. REFINEX Excels Across Model Sizes and Downstream Tasks We conduct pretraining using the LLAMA-2 architecture (Touvron et al., 2023b) at two model scales: 350M and 750M parameters. Evaluation is carried out using lighteval (Fourrier et al., 2023) across 10 widely-used downstream tasks covering diverse domains. As shown in Table 2, models trained with REFINEX consistently achieve the highest average scores and win the most tasks, regardless of whether refinement is applied"
        },
        {
            "title": "Under review as a conference paper",
            "content": "Figure 4: Token count distributions before and after refinement on raw data, rule-based filtered data (Comb), and LLM-based filtered data (Prox-C). We randomly sample 5M document instances from each corresponding baseline to ensure representative analysis. to raw data or to previously filtered datasets. REFINEX achieves the best result on every individual task, although the top-performing variant may be derived from different data sources across tasks. When applied to data filtered by the LLM-based Prox-D method, REFINEX improves over raw data by +7.2%, over Comb by +5.9%, and even outperforms the strongest prior fine-grained refinement method, Prox-C, by +2.6%. Additional results for the 350M model setting are presented in Table 8. REFINEX Achieves Better Performance with Fewer Training Tokens As illustrated in Figure 3, REFINEX shows clear advantages under varying training token budgets. While models trained on raw or filtered data exhibit diminishing returns as token counts increase, character-level refinement methods like REFINEX continue to yield consistent gains. Notably, models trained on just 10B tokens of REFINEX-refined data match or exceed the performance of those trained on 20B tokens of Comb-filtered data, indicating that REFINEX achieves stronger performance with fewer training tokens. Figure 4 further reports the token count distribution after fine-grained refinement. Both rule-based filtering methods and LLM-based methods help normalize the initially noisy distribution of the raw data, bringing it closer to bell-shaped curve while still exhibiting long-tail trend at the high-token end. Notably, our REFINEX achieves the largest overall reduction in token count, while still maintaining the desirable statistical structure of the data. This demonstrates that REFINEX effectively reduces the per-document token cost by eliminating noisy content, thereby enabling the model to access more diverse set of documents under the same training token budget. 4.3 IN-DEPTH ANALYSIS OF REFINED TEXT INSTANCES We conduct comprehensive set of experiments to investigate how different fine-grained refinement methods affect individual text instances. To enable more nuanced analysis across varying quality levels, we begin by pre-classifying raw text data collected from RedPajama-V2 using DataMan (Peng et al., 2025), state-ofthe-art data quality scoring tool. DataMan assigns holistic quality score ranging from 1 to 5 to each instance, based on 14 quality dimensions and 15 application-specific signals, to estimate its pretraining utility. Specifically, we divide the data into five groups according to their scores, and randomly sample 100k instances from each group for evaluation. Based on these grouped datasets, we evaluate how each refinement method affects individual instances in terms of quality improvement, aggressive editing, refinement speed and no-change ratio."
        },
        {
            "title": "Method",
            "content": "Rat.(%) Rat.(%) Avg Score = 2 Score = 3 Score = 4 E2E E2Edel Prox-C REFINEX E2E E2Edel Prox-C REFINEX E2E E2Edel Prox-C REFINEX 68.8 38.3 17.2 42.2 59.0 33.2 21.5 41.2 21.3 9.8 8.3 12.7 0.38 1.05 0.96 0. 3.39 7.95 5.47 4.58 3.59 8.80 5.64 4.86 3.24 2.59 2.23 2.78 3.62 3.28 3.17 3.45 4.15 4.01 4.02 4.09 Table 3: Quality scores and changes by group after refinement. E2Edel refers to end-to-end refinement outputs restricted to deletion-only edits. REFINEX Effectively Improves Text Quality Table 3 presents the quality shift of selected groups after refinement. As expected, end-to-end (E2E) refinement achieves the highest post-refinement quality due to its costly inference and aggressive rewriting, but this also makes it impractical and risky for large-scale use. Our REFINEX ranks second in performance but operates under far more efficient"
        },
        {
            "title": "Under review as a conference paper",
            "content": "and controlled setting. It consistently improves data quality across all input groups. Compared to Prox-C, REFINEX yields greater quality improvements with lower risk of degrading the original text. particularly encouraging result is that REFINEX significantly outperforms its own distillation source E2Edel, used as supervision during training. This suggests that the refine model not only learned the deletion-based programs effectively but also generalized beyond the examples it was distilled from, which highlights its potential for further development. Method Score=1 Score=2 Score=3 REFINEX Achieves Both Efficiency and Reliability To better observe the behavior of different methods, we conduct detailed comparison along multiple dimensions. Figure 5 presents two complementary views. The top plot reports the ratio of output tokens to input tokens for each method, indicating their relative expansion cost. The bottom plot shows the percentage of instances that remain completely unchanged after refinement, revealing the methods tendency to preserve input content. To estimate the risk of over-editing, we compute the number of words introduced during refinement that are absent from the original text. The results are shown in Table 4. Table 4: Number of newly introduced words per 1,000 refined tokens not in the original text. E2E Prox-C RefineX 11.25 0.01 0.00 16.72 0.08 0.00 15.06 0.17 0.00 19.26 0.03 0. 5.54 0.32 0.00 Score=4 Score=5 From these results, we observe that while E2E delivers the largest quality gains, it is also prohibitively slow, particularly when inference depends on large expert models. As shown in Figure 5 (top), the number of generated tokens is comparable to that of the original text, making the approach impractical at scale. Moreover, E2E suffers from excessive rewriting. This is evident from its low no-change rate in Figure 5 (bottom) and the high number of hallucinated words in Table 4, despite being explicitly prompted to avoid substantive edits and perform deletions only. In contrast, Prox-C and REFINEX are program-based methods that achieve significantly faster refinement. This efficiency stems from the concise nature of programmatic instructions and the fast inference enabled by distilled models. According to the comparison in Figure 5, Prox-C leaves significantly higher proportion of text untouched. However, as shown in Table 3, it still leads to more cases of quality degradation. This highlights concerning outcome: despite making minimal changes, Prox-C worsens the quality of the text more, suggesting that its limited interventions are not only insufficient but also potentially harmful. Figure 5: The top plot reflects the refinement efficiency, while the bottom plot shows the preference for leaving text untouched. REFINEX applies moderate level of edits that enhance text quality, while ensuring reliability by relying solely on deletion operations. As shown in Table 4, it introduces no additional content (i.e., zero new words), thereby avoiding risks of hallucination or over-modification. This demonstrates that REFINEX stands out by achieving both high efficiency and strong reliability. Compared to E2E, which is effective but prohibitively slow and risky, or Prox-C, which is efficient but often unreliable, REFINEX offers principled alternative that efficiently realizes the reliability of E2E-style refinement. It achieves this through lightweight models that execute minimal and interpretable deletion-based operations. This dual advantage enables REFINEX to consistently produce refined data that is trustworthy and scalable, highlighting its potential utility in large-scale pretraining workflows."
        },
        {
            "title": "4.4 CASE STUDY",
            "content": "Despite the strong results observed in large-scale evaluations, we also perform detailed case studies on data from multiple domains to verify the effectiveness and reliability of REFINEX refinements. Selected real-world cases are reported in Appendix E."
        },
        {
            "title": "5 RELATED WORK",
            "content": "Pretraining Data Crawl and Processing The quality of pretraining data is critical for shaping large language models (LLMs). Since raw web dataoften from Common Crawlis noisy and inconsistent, most LLM pipelines apply extensive preprocessing (Touvron et al., 2023a; Penedo et al., 2024a; Together, 2023). Recent work like Craw4LLM (Yu et al., 2025) and Nemotron-CC (Su et al., 2024) improves these snapshots via automated filtering, deduplication, and metadata-aware extraction. Baack (2024) critically assess Common Crawls suitability, citing spam, duplication, and content imbalance. Pipelines typically start with document-level filtering (e.g., URL removal, boilerplate stripping, language detection)(Smith et al., 2022), followed by heuristics on length, symbol ratios, or abnormal characters(Zhang et al., 2024a; Dou et al., 2024). Deduplication uses MinHash (Broder, 1997) or n-gram hashing (Penedo et al., 2024c). While effective for filtering low-quality content, such rule-based methods are coarse and miss local noiseleading to over-filtering or incomplete cleanup. Filtering and Selection Strategies In addition to preprocessing, data filtering and selection techniques have been widely adopted to improve corpus quality. While traditional filtering approaches rely on heuristics, recent work has explored using perplexity-based scoring or LLM-judged quality signals to rank and retain high-quality documents (Xie et al., 2023; Wettig et al., 2024; Yu et al., 2024). These strategies have been extended from supervised fine-tuning into the pretraining stage, as seen in SlimPajama (Soboleva et al., 2023) and MATES (Yu et al., 2024), where learned models or proxy scoring functions resample or dynamically filter the training data. However, these methods primarily focus on selecting which documents to include or exclude, rather than improving the internal quality of documents that are partially useful but noisy. The limitations are especially evident at the sentence or token level, where heuristic filters fail to adapt and LLM scorers introduce inconsistency and bias. LLMs for Code Generation Recent advances show that large language models (LLMs) possess strong code generation and structured reasoning capabilities, making them well-suited for tasks involving executable logic (Chen et al., 2021; Austin et al., 2021; Fried et al., 2022; Wang et al., 2023). These abilities have inspired methods where LLMs produce interpretable instruction programs instead of directly generating final outputs. For example, Schick et al. (2024) and Yao et al. (2023) design prompting strategies in which LLMs emit tool-use plans or code-like sequences that are later executed. This approach reduces hallucination risks, supports modular reasoning, and enables easier error detection, as incorrect programs can be filtered or fail safely. Model-based Programmatic Refinement Motivated by these benefits, recent work explores using LLMs to generate editing programs for data refinement (Zhou et al., 2024; Zhang et al., 2023). Rather than rewriting text directly, the model produces interpretable edit operations that are applied to the original input, allowing for better quality control. However, methods like ProX (Zhou et al., 2024) prompt expert models to generate full editing programs and then distill them into smaller models. This single-step approach often yields noisy supervision and lacks fine-grained editing precision. In contrast, REFINEX follows more structured strategy. It first prompts an expert model to produce high-quality end-to-end refined text, then derives minimal sequence of deletion-only operations to convert the original input into the refined version. These operations are filtered for precision and distilled into compact model, enabling interpretable, low-risk, and scalable refinement."
        },
        {
            "title": "6 CONCLUSION AND DISCUSSION",
            "content": "In this paper, we propose REFINEX, novel approach for large-scale refinement of pretraining data. REFINEX first generates high-quality refined text in an end-to-end manner, then computes the minimal set of edit operationsbased on minimum edit distancethat transform the original text into its improved version. We extract valid operations and convert them into predefined set of programmatic functions, creating high-quality supervision data for training compact refine model. This model is capable of inferring and executing refinement programs, enabling efficient and reliable enhancement of pretraining corpora. Extensive experiments demonstrate that REFINEX substantially improves both the quality of individual text instances and the downstream performance of models trained on the refined data. Our results highlight promising direction for scalable and trustworthy data refinement that combines both efficiency and reliability."
        },
        {
            "title": "Under review as a conference paper",
            "content": "Limitations While our experimental design comprehensively evaluates the effectiveness of REFINEX, there are two main limitations to note. First, due to resource constraints, our pretraining experiments are conducted under limited token budget and do not cover larger-scale training regimes. Nevertheless, the consistent improvements and clear performance trends observed across model sizes and filtering setups demonstrate the strong potential of REFINEX. In particular, we simulate high-quality pretraining scenarios by applying REFINEX on top of several widely adopted filtering strategies. Second, the quality of our distillation data heavily relies on the capabilities of the expert model used during refinement. In our setup, we employ Qwen2.5-72B-Instruct, requiring approximately 12,480 GPU hours on H800s to refine 5 million raw documents. Although this model delivers competitive results, it still lags behind state-of-the-art proprietary models. In preliminary comparisons, models like GPT-4o exhibit stronger end-to-end refinement quality with fewer over-edits. As such, we believe that REFINEX has the potential to achieve even higher quality and generalization when paired with stronger expert models, assuming sufficient computational resources are available. Acknowledgments We sincerely thank the Qwen team for their generous support in providing computational resources and sharing valuable expertise on pretraining, which enabled us to conduct comprehensive large-scale experiments to evaluate the practical effectiveness of REFINEX. We are also grateful to Zengzhi Wang from the ProX team for his kind assistance in aligning the experimental setup with that of ProX. This work is supported in part by the National Key R&D Program of China under Grant Nos. 2023YFA1011602 and 2023YFC3305303, and the National Natural Science Foundation of China under Grant Nos. 62472408, 62372431, 62441229, and 62377043."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Lightning AI. Litgpt. https://github.com/Lightning-AI/litgpt, 2023. ClaudeAI Anthropic. https://www-cdn.anthropic.com/ 3 Model de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf. The claude 3 model family: sonnet, haiku. Card, Opus, 2024."
        },
        {
            "title": "URL",
            "content": "Jacob Austin, Augustus Odena, Max Nye, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Stefan Baack. critical analysis of the largest source for generative ai training data: Common crawl. In Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency, pp. 21992208, 2024. Baolong Bi, Shenghua Liu, Yiwei Wang, Yilong Xu, Junfeng Fang, Lingrui Mei, and Xueqi Cheng. Parameters vs. context: Fine-grained control of knowledge reliance in language models. arXiv preprint arXiv:2503.15888, 2025. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 74327439, 2020. Andrei Broder. On the resemblance and containment of documents. In Proceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No. 97TB100171), pp. 2129. IEEE, 1997. Mark Chen, Jerry Tworek, Heewoo Jun, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In International Conference on Learning Representations (ICLR), 2024."
        },
        {
            "title": "Under review as a conference paper",
            "content": "Longxu Dou, Qian Liu, Guangtao Zeng, Jia Guo, Jiahui Zhou, Wei Lu, and Min Lin. Sailor: Open language models for south-east asia. CoRR, abs/2404.03608, 2024. doi: 10.48550/ARXIV.2404. 03608. URL https://doi.org/10.48550/arXiv.2404.03608. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale knowledge. Advances in Neural Information Processing Systems, 35: 1834318362, 2022. Clémentine Fourrier, Nathan Habib, Thomas Wolf, and Lewis Tunstall. Lighteval: lightweight framework for llm evaluation, 2023. URL https://github.com/huggingface/ lighteval. Daniel Fried, Sewon Chen, Eric Wallace, et al. Incoder: generative model for code infilling and synthesis. In arXiv preprint arXiv:2204.05999, 2022. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions, 2023. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Zhenghao Lin, Zhibin Gou, Yeyun Gong, Xiao Liu, Yelong Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, Nan Duan, et al. Rho-1: Not all tokens are what you need. arXiv preprint arXiv:2404.07965, 2024. Yue Liu, Jiaying Wu, Yufei He, Hongcheng Gao, Hongyu Chen, Baolong Bi, Jiaheng Zhang, Zhiqi Huang, and Bryan Hooi. Efficient inference for large reasoning models: survey. arXiv preprint arXiv:2503.23077, 2025. Sachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, et al. Openelm: An efficient language model family with open-source training and inference framework. arXiv preprint arXiv:2404.14619, 2024. Lingrui Mei, Shenghua Liu, Yiwei Wang, Baolong Bi, and Xueqi Cheng. Slang: New concept comprehension of large language models. arXiv preprint arXiv:2401.12585, 2024. Meta. Introducing meta llama 3: The most capable openly available llm to date, 2024. URL https://ai.meta.com/blog/meta-llama-3. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Junichi Tsujii (eds.), Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 23812391, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1260. URL https: //aclanthology.org/D18-1260. Shiyu Ni, Keping Bi, Jiafeng Guo, Lulu Yu, Baolong Bi, and Xueqi Cheng. Towards fully exploiting llm internal states to enhance knowledge boundary perception. arXiv preprint arXiv:2502.11677, 2025."
        },
        {
            "title": "Under review as a conference paper",
            "content": "Joon Sung Park, Joseph OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, pp. 122, 2023. Guilherme Penedo, Hynek Kydlíˇcek, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, Thomas Wolf, et al. The fineweb datasets: Decanting the web for the finest text data at scale. arXiv preprint arXiv:2406.17557, 2024a. Guilherme Penedo, Hynek Kydlíˇcek, Alessandro Cappelli, Mario Sasko, and Thomas Wolf. Datatrove: large scale data processing, 2024b. URL https://github.com/huggingface/ datatrove. Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Hamza Alobeidli, Alessandro Cappelli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: Outperforming curated corpora with web data only. Advances in Neural Information Processing Systems, 36, 2024c. Ru Peng, Kexin Yang, Yawen Zeng, Junyang Lin, Dayiheng Liu, and Junbo Zhao. Dataman: Data manager for pre-training large language models. arXiv preprint arXiv:2502.19363, 2025. Jack Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. Bathini Ravinder, Senthil Kumar Seeni, VS Prabhu, Asha, SP Maniraj, and Srinivasan. Web data mining with organized contents using naive bayes algorithm. In 2024 2nd International Conference on Computer, Communication and Control (IC4), pp. 16. IEEE, 2024. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense reasoning about social interactions. arXiv preprint arXiv:1904.09728, 2019. Timo Schick, Ananya Dwivedi-Yu, Yeganeh Kordi Hou, et al. Toolformer: Language models can teach themselves to use tools. In International Conference on Learning Representations (ICLR), 2024. Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. Using deepspeed and megatron to train megatron-turing nlg 530b, large-scale generative language model. arXiv preprint arXiv:2201.11990, 2022. Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob Steeves, Joel Hestness, and Nolan Dey. SlimPajama: 627B token cleaned and deduplicated version of RedPajama, June 2023. URL https://huggingface.co/datasets/cerebras/SlimPajama-627B. Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Evan Walsh, Luke Zettlemoyer, Noah Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo. Dolma: an open corpus of three trillion tokens for language model pretraining research. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1572515788, Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.acl-long.840."
        },
        {
            "title": "Under review as a conference paper",
            "content": "Dan Su, Kezhi Kong, Ying Lin, Joseph Jennings, Brandon Norick, Markus Kliegl, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. Nemotron-cc: Transforming common crawl into refined long-horizon pretraining dataset. arXiv preprint arXiv:2412.02595, 2024. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: question answering challenge targeting commonsense knowledge. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 41494158, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1421. URL https://aclanthology.org/N19-1421. Together. Redpajama: an open dataset for training large language models, October 2023. URL https://github.com/togethercomputer/RedPajama-Data. S. Towhidul Islam Tonmoy, Mehedi Zaman, Vinija Jain, Anku Rani, Vipula Rawte, Aman Chadha, and Amitava Das. comprehensive survey of hallucination mitigation techniques in large language models, 2024. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Albert Wang, Baptiste Roziere, Gautier Izacard, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Johannes Welbl, Nelson Liu, and Matt Gardner. Crowdsourcing multiple choice science questions. arXiv preprint arXiv:1707.06209, 2017. Alexander Wettig, Aatmik Gupta, Saumya Malik, and Danqi Chen. QuRating: Selecting high-quality data for training language models. In International Conference on Machine Learning (ICML), 2024. Sang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy Liang. Data selection for language models via importance resampling. Advances in Neural Information Processing Systems, 36: 3420134227, 2023. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Shinn Yao, Jiong Zhao, Dian Yu, and et al. React: Synergizing reasoning and acting in language models. In NeurIPS, 2023. Shi Yu, Zhiyuan Liu, and Chenyan Xiong. Craw4llm: Efficient web crawling for llm pretraining. arXiv preprint arXiv:2502.13347, 2025. Zichun Yu, Spandan Das, and Chenyan Xiong. Mates: Model-aware data selection for efficient pretraining with data influence models. arXiv preprint arXiv:2406.06046, 2024. Li Yujian and Liu Bo. normalized levenshtein distance metric. IEEE transactions on pattern analysis and machine intelligence, 29(6):10911095, 2007. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019."
        },
        {
            "title": "Under review as a conference paper",
            "content": "Ge Zhang, Scott Qu, Jiaheng Liu, Chenchen Zhang, Chenghua Lin, Chou Leuang Yu, Danny Pan, Esther Cheng, Jie Liu, Qunshu Lin, et al. Map-neo: Highly capable and transparent bilingual large language model series. arXiv preprint arXiv:2405.19327, 2024a. Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: An open-source small language model. arXiv preprint arXiv:2401.02385, 2024b. Yuxian Zhang, Canwen Xu, Zhiyuan Liu, and Maosong Sun. Editavalanche: multi-granularity benchmark for edit-based language model evaluation. arXiv preprint arXiv:2310.11603, 2023. Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, Alban Desmaison, Can Balioglu, Pritam Damania, Bernard Nguyen, Geeta Chauhan, Yuchen Hao, Ajit Mathews, and Shen Li. Pytorch fsdp: Experiences on scaling fully sharded data parallel. Proc. VLDB Endow., 16(12):38483860, aug 2023. ISSN 21508097. doi: 10.14778/3611540.3611569. URL https://doi.org/10.14778/3611540. 3611569. Bohan Zhou, Zixuan Zhang, Yuxian Zhang, et al. Programming every example: Efficient and interpretable refinement of pretraining data. arXiv preprint arXiv:2402.14602, 2024. Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and Chao Zhang. Toolqa: dataset for llm question answering with external tools. Advances in Neural Information Processing Systems, 36: 5011750143, 2023. Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, et al. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. arXiv preprint arXiv:2406.15877, 2024."
        },
        {
            "title": "A REFINE MODEL IMPLEMENTATION IN REFINEX",
            "content": "A.1 DISTILLATION DATA CONSTRUCTION AND SELECTION Seed Data Selection and Preprocessing To construct the seed dataset, we begin by scoring the raw collected corpus using DataMan (Peng et al., 2025), state-of-the-art quality scoring model. To ensure that the seed data reflect the true distribution of the full corpus, we sample approximately 5 million documents in accordance with the score distribution. For long documents, we apply chunking strategy by splitting them into multiple 12k-length chunks with overlapping windows. Adjacent chunks are allowed to partially overlap to ensure each chunk maintains full 12k token span, maximizing the utility of available text. End-to-End Refinement with Expert LLM We use Qwen2.5-72B-Instruct (Yang et al., 2025) as the expert model to perform end-to-end refinement over the seed data. The decoding parameters are set to top-p = 0.8 and top-k = 20. We carefully design following end-to-end refinement prompts to guide the expert model in generating high-quality outputs. While over-editing cannot be entirely avoided, our prompts explicitly discourage unnecessary modifications and instead emphasize preserving valuable information. Unlike ProX, which tends to indiscriminately remove URLs or metadata, our prompts instruct the model to retain useful content, such as informative links or relevant metadata, when appropriate. Our prompt design avoids complex few-shot constructions and enables more fine-grained and reliable refinement, facilitating the creation of higher-quality distillation data. Filtering and Program Supervision Extraction We further apply data selection on the end-to-end refined text to ensure the reliability of supervision. Specifically, we discard transformations that involve overly long insertions or replacements, as well as deletions that affect only minimal portion of the original text. To identify these operations, we compute the minimal edit operations between the original and refined text using standard minimum edit distance algorithm, as described in Algorithm 2. In practice, we implement this using Pythons difflib.get_opcodes() function, which returns sequence of span-level operations with edit indices. The structure of these operations is summarized in Table 5. We discard any document containing insertion or replacement spans of 20 characters or more, and further filter out examples with fewer than 10 characters affected by deletions. After filtering, we obtain set of 2 million high-quality supervision pairs. Finally, we align the resulting edit operations to our predefined set of refinement functions  (Table 1)  through post-processing procedure outlined in Algorithm 1. A.2 MODEL TRAINING DETAILS We fine-tune the Qwen3-0.6B-base model using the constructed high-quality supervision pairs of text and corresponding refinement programs, employing the LLaMA-Factory framework. In practice, we initially train multiple variants of the refine model across different model sizes, including Qwen3-Base 0.6B, 1.7B, 4B, and 8B. Table 6 reports their performance in terms of text quality improvement. We observe that while performance differences across sizes are marginal, the 0.6B model offers significant advantage in inference speed, making it the most suitable choice for large-scale data refinement."
        },
        {
            "title": "Description",
            "content": "equal(i1, i2, j1, j2) Span a[i1 : i2] equals b[j1 : j2]; no change needed delete(i1, i2) Remove span a[i1 : i2]; corresponds to deletion insert(i1, j1, j2) Insert b[j1 : j2] at position i1 in replace(i1, i2, j1, j2) Replace a[i1 : i2] with b[j1 : j2] Table 5: Explanation of difflib.get_opcodes() edit operations used for program extraction."
        },
        {
            "title": "Under review as a conference paper",
            "content": "End-to-End Refinement Prompts You are careful content refiner for text relevance. Your task is to analyze the input containing text field, and return refined version of the text with all inter-irrelevant content removed, along with explanations of what was kept or deleted. Format: Analyze the provided input of [doc]<text>[/doc] and return an output in the following format: modification_reason: [doc]<brief explanation of which parts were deleted or kept, and why>[/doc] refined_text: [doc]<the final refined text>[/doc] Refinement Rules: Delete content clearly irrelevant to the main theme or informational value, such as: Website headers, navigation bars, or menu items (e.g., Home About Contact) Unrelated HTTP links (e.g., ads, trackers) Generic footers with contact info, privacy policies, or unsubscribe links Empty lines or visual dividers (e.g., ) Preserve content that is informative, on-topic, or provides contextual value. If sentence mixes relevant and irrelevant fragments: Delete only the irrelevant fragment if the remainder stays coherent. Otherwise delete the whole sentence. Only deletions are permitted; do not rewrite, correct, or rephrase. Preserve all original spelling, punctuation, and formatting exactly. If deleting leaves content incoherent or devoid of value, delete the entire sentence/section. Examples: Example 1: Input: [doc]The development of renewable energy technologies, particularly solar photovoltaic systems, has reduced carbon emissions in urban areas. Recent studies show 22% decrease in CO2 levels.[/doc] Output: modification_reason: [doc]The text is highly relevant and coherent. All content discusses the impact of renewable energy technologies on carbon emissions. Nothing was removed.[/doc] refined_text: [doc]The development of renewable energy technologies, particularly solar photovoltaic systems, has reduced carbon emissions in urban areas. Recent studies show 22% decrease in CO2 levels.[/doc] Example 2: Input: [doc]Home Products About Us ContactnWelcome to our websitenHeres our main content...[/doc] Output: modification_reason: [doc]The first line is standard website navigation header and was removed. The other lines contain core content and are preserved.[/doc] refined_text: [doc]Welcome to our websitenHeres our main content...[/doc] Example 3: Input: [doc]Join the conversation here: http://random-forum-link.com[/doc] Output: modification_reason: [doc]The only sentence consists of an unrelated URL. Removing the link would leave no meaningful content, so the entire line was deleted.[/doc] refined_text: [doc][/doc] Example 4: Input: [doc]For more information on AI governance, visit https://www.credible-ai.org/resources[/doc] Output: modification_reason: [doc]Although the sentence contains link, it is directly related to the topic of AI governance and provides supplementary value. Therefore, it was retained in full.[/doc] refined_text: [doc]For more information on AI governance, visit https://www.credible-ai.org/resources[/doc] [Omitting several meaningful examples ...] ACTUAL TASK EXECUTION: Input: [doc]{input_text_task}[/doc] Output:"
        },
        {
            "title": "Under review as a conference paper",
            "content": "Initialize affected_lines [ ] for each line Lℓ with span [sℓ, eℓ] do if [sℓ, eℓ] overlaps [i1, i2] then if i1 sℓ and i2 eℓ then Algorithm 1 Mapping Valid Delete Operations to Program Functions Require: Original text as list of lines; Valid delete opcodes = {(i1, i2)} Ensure: List of program functions 1: Split into lines {L1, L2, . . . , Ln} 2: Compute character span (si, ei) for each line Li 3: Initialize [ ], group [ ] 4: for each (i1, i2) in do 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: end for 21: return end for Group consecutive full-line deletions into ranges for each range [ℓs, ℓe] do Extract substring [i1 : i2] as del_str remove_str(line=ℓ, del_str) remove_lines(ℓs, ℓe) Mark Lℓ as full-line end for end if end if else"
        },
        {
            "title": "Model",
            "content": "Avg. Rat.(%) Rat.(%) Untouched(%) Empty(%) Score = 1 Score = Score = 3 Score = 4 Score = 5 Qwen3-0.6B 2.009 Qwen3-1.7B 2.075 2.209 Qwen3-4B 2.282 Qwen3-8B Qwen3-0.6B 2.718 Qwen3-1.7B 2.711 2.744 Qwen3-4B 2.749 Qwen3-8B Qwen3-0.6B 3.413 Qwen3-1.7B 3.410 3.420 Qwen3-4B 3.423 Qwen3-8B Qwen3-0.6B 4.075 Qwen3-1.7B 4.079 4.088 Qwen3-4B 4.085 Qwen3-8B Qwen3-0.6B 4.917 Qwen3-1.7B 4.917 4.923 Qwen3-4B 4.924 Qwen3-8B 22.23 24.14 22.28 20.82 42.14 44.26 43.62 43.61 41.14 41.40 41.75 41.88 12.70 12.86 13.32 13. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.66 0.61 0.58 0.59 4.58 4.39 4.39 4.15 4.86 4.63 4.30 4.41 8.20 8.13 7.62 7. 7.87 6.28 4.71 3.45 7.59 8.06 7.65 7.27 9.85 10.10 10.73 10.41 13.52 13.16 14.77 14.18 16.50 15.85 17.41 16.70 68.17 66.91 71.31 74. 19.97 15.35 18.48 19.27 6.18 4.32 5.25 5.48 0.93 0.54 0.60 0.67 0.18 0.09 0.10 0.11 Table 6: Refinement statistics by group documents with different size refine models. Avg., Rat., and Rat. denote the average quality scores and the percentage of documents with improved or degraded scores after refinement, respectively. Untouched indicates the proportion of documents that remain unchanged, while Empty refers to those reduced to empty content."
        },
        {
            "title": "Under review as a conference paper",
            "content": "else for = 1 to do if S[i] = [j] then dp[i][0] i; op[i][0] Delete dp[0][j] j; op[0][j] Insert dp[i][j] dp[i 1][j 1] op[i][j] Match Try Replace if dp[i 1][j 1] + 1 < dp[i][j] then dp[i][j] dp[i 1][j 1] + 1 op[i][j] Replace Algorithm 2 Minimum Edit Distance with Operation Extraction Require: Source text of length m, Target text of length Ensure: List of edit operations 1: Initialize dp[0 . . . m][0 . . . n] 2: Initialize op[0 . . . m][0 . . . n] to store backtrack operations 3: dp[0][0] 0 4: for = 0 to do 5: 6: end for 7: for = 0 to do 8: 9: end for 10: for = 1 to do 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: end for 32: Backtrack to extract operations 33: m, n, [ ] 34: while > 0 or > 0 do 35: 36: 37: 38: 39: 40: 41: 42: end if 43: 44: end whilereturn end if if dp[i 1][j] + 1 < dp[i][j] then dp[i][j] dp[i 1][j] + 1 op[i][j] Delete end if if dp[i][j 1] + 1 < dp[i][j] then dp[i][j] dp[i][j 1] + 1 op[i][j] Insert action op[i][j] Append (action, i, j) to front of if action = Match or Replace then 1, 1 else if action = Delete then else if action = Insert then 1 1 end for end if end if"
        },
        {
            "title": "Under review as a conference paper",
            "content": "A.3 REFINEX INFERENCE AT SCALE We follow the general processing setup introduced in ProX (Zhou et al., 2024), where each document is split into manageable chunks before refinement. To efficiently parallelize large-scale inference, we leverage the infrastructure provided by the Datatrove project (Penedo et al., 2024b), which allows us to distribute the corpus across multiple workers-typically one per GPU, as small models do not require tensor parallelism. All refinement operations are performed using the vLLM engine (Kwon et al., 2023), which offers high-throughput execution. For chunk-level refinement, each document is segmented into multiple overlapping spans to fit within the models context length. While ProX restricts each chunk to roughly 1,500 tokens, our REFINEX increases the chunking window to approximately 12,000 characters, enabling the model to leverage broader semantic context across longer documents. To reduce preprocessing overhead, we approximate token-based length constraints using word counts. The overall procedure for chunk generation is outlined below: else + if = then {c} if TokenCount(c + l) then Algorithm 3 Document Chunk Splitting Algorithm Require: Document D, context window size Ensure: Set of chunks 1: , 2: for each line in do 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: end for 17: if = then 18: 19: end if 20: return {FlagAsSkipped(l)} end if if TokenCount(l) then {c} end if end if else Add line to current chunk Save current chunk Start new chunk Flag long line Add the final chunk"
        },
        {
            "title": "B PRETRAINING DETAILS",
            "content": "To ensure consistency with prior work, we align our pretraining pipeline with the settings adopted in ProX (Zhou et al., 2024). All models are trained using the open-source frameworks LIT-GPT(AI, 2023) and TINYLLAMA(Zhang et al., 2024b), which offer high flexibility for architecture customization and distributed training. In particular, we incorporate fused CUDA kernels from FlashAttention2 (Dao, 2024)-including optimized rotary positional embeddings (RoPE), fused layer normalization, and efficient cross-entropy computation-to reduce memory usage during training. For scalability, we employ Fully Sharded Data Parallelism (FSDP) (Zhao et al., 2023), enabling efficient multi-node training across different model scales. Given limited computational resources and the goal of fair comparison, we do not pretrain on the entire corpus exhaustively. Instead, we construct our training pools by randomly sampling subsets from large datasets. Specifically, for the RedPajama dataset (Together, 2023), we randomly select 70 data shards following ProX, totaling approximately 500B tokens. These are further divided into 8 equal partitions (62.5B tokens each). Since both quality-based filtering and fine-grained refinement substantially reduce the usable token count-and to varying degrees across methods-we apply refinement sequentially over the shards until the final effective token count reaches 20B."
        },
        {
            "title": "Under review as a conference paper",
            "content": "Table 7: Pretrained model architecture and training hyper-parameters. Model Hidden Size Intermediate Size Context Len Heads Layers Vocab Size # Params (w/o embed) 350M 750M 1,280 1, 2,048 4,864 2,048 2,048 16 24 24 24 32,000 32,000 354,284,800 (313,324,800) 758,982,144 (709,830,144) Model Context Length Batch Size Max Steps Warmup Steps Weight Decay Optimizer LR Scheduler LR 350M 750M 1,024 1,024 2,048 2, 12,500 12,500 500 500 0.1 0.1 AdamW AdamW cosine cosine 5e-4 5e-5 5e-4 5eFor all from-scratch pretraining experiments, we follow uniform setup across model sizes. We use learning rate of 5e-4 for both 350M and 750M models, maximum sequence length of 2048 tokens, and global batch size of 2 million tokens. Learning rates are scheduled using cosine decay, and additional training hyperparameters follow the configurations introduced in Zhang et al. (2024b) and Lin et al. (2024). Detailed model architecture and optimizer settings are summarized in Table 7."
        },
        {
            "title": "C EXPERIMENTAL EVALUATION DETAILS",
            "content": "Lighteval Benchmarks and Setup To assess the downstream capabilities of our pretrained models, we follow ProX to adopt diverse set of evaluation tasks drawn primarily from the nine early signal benchmarks introduced in FineWeb (Penedo et al., 2024a). Evaluations are conducted using the official implementation of LIGHTEVAL(Fourrier et al., 2023), ensuring consistent and reproducible results. In addition to these nine tasks, we also include SciQ(Welbl et al., 2017) as tenth benchmark, which has been widely adopted in recent works (Mehta et al., 2024; Wettig et al., 2024) and shown to be an informative proxy for broader model capabilities. The complete list of evaluated datasets includes ARC-Easy and ARC-Challenge (Clark et al., 2018), CommonSenseQA (Talmor et al., 2019), HellaSwag (Zellers et al., 2019), MMLU (Hendrycks et al., 2021), OpenBookQA (Mihaylov et al., 2018), PIQA (Bisk et al., 2020), SocialIQA (Sap et al., 2019), WinoGrande (Sakaguchi et al., 2021), and SciQ. We report normalized zero-shot accuracy as the default evaluation metric across all benchmarks. Sampling and Aggregation Strategy Following the default behavior of LIGHTEVAL, we randomly sample 1,000 examples from each dataset. For MMLU, which contains 57 sub-tasks, we sample 1,000 examples from each sub-task independently and aggregate the scores into single overall MMLU result. The final reported average is computed over all nine benchmarks, treating ARC-E and ARC-C as separate tasks while considering MMLU as single benchmark entry. Unlike the aggregation strategy in FineWeb, which averages across all individual subcomponents of MMLU, we adopt an equal-weighted aggregation across the nine benchmarks. This decision is motivated by the relatively high variance observed in MMLU scores and our desire to avoid overemphasizing single benchmark in the overall evaluation. For complete view of benchmark-specific performance and score dynamics, we provide the full set of LIGHTEVAL results in Appendix D, along with visualizations that trace model performance as function of training tokens. Refined Text Evaluation Setup To enable more granular analysis across varying data quality levels, we additionally evaluate the impact of refinement at the level of individual text instances. We begin by pre-classifying raw documents from the RedPajama-V2 corpus using DataMan (Peng et al., 2025), state-of-the-art data quality scoring framework. DataMan assigns each document holistic score from 1 to 5 by integrating signals from 14 quality dimensions and 15 applicationspecific heuristics, aiming to estimate its utility for pretraining. Based on these quality scores, we divide the data into five discrete groups and randomly sample 100,000 instances from each group for evaluation. For each method, we assess its refinement impact across multiple dimensions, including the post-refinement quality shift, the ratio of output to input tokens, the percentage of instances left untouched, and the number of newly introduced words that do not appear in the original text. These metrics collectively capture both the effectiveness and reliability of the refinement process."
        },
        {
            "title": "Method",
            "content": "ARC-C ARC-E CSQA HellaS MMLU OBQA PIQA SIQA WinoG SciQ Avg #Win"
        },
        {
            "title": "24.2\nRaw\n+ Prox-C\n24.6\n+ REFINEX 24.5",
            "content": "39.8 43.9 44.1 28.7 29.2 29.4 34.6 35.5 35.1 26.6 27.2 27.0 27.2 27.2 30.4 64.3 64.5 64. 38.6 38.8 39.1 49.5 50.1 50.8 62.4 39.6 0 / 10 65.4 40.6 5 / 10 65.2 41.0 5 / 10 Rule-based filtering: GO = Gopher rules, C4 = C4 rules, FW = FineWeb rules, COMB = Go + C4 + Fw."
        },
        {
            "title": "25.1\nProx-D\n+ Prox-C\n24.8\n+ REFINEX 26.8",
            "content": "40.9 43.1 43.6 42.2 43.1 43.6 39.6 43.2 43.5 42.6 42.6 44.1 44.3 46.4 48.1 28.7 28.5 30. 29.8 28.5 29.4 28.6 28.2 29.4 29.6 30.0 29.6 28.5 29.1 28.6 34.6 36.5 35.4 35.4 36.5 35. 35.6 36.0 35.8 35.8 36.7 37.2 26.7 26.7 27.0 26.3 26.7 27.3 26.3 27.1 26.6 26.4 27.1 26. 28.6 26.4 28.4 25.6 26.4 30.1 27.8 28.2 31.0 28.0 27.4 29.8 64.2 64.8 66.0 64.5 64.8 63. 63.3 64.8 65.9 65.9 65.1 64.4 LLM-based filtering: Prox-D 35.6 35.3 36.8 27.8 28.1 29.3 28.8 30.4 29. 64.1 65.3 64.4 39.5 38.4 39.5 39.3 38.4 40.5 38.2 38.8 39.5 39.6 38.7 39.2 38.5 38.5 39. 49.7 51.6 50.7 49.1 51.6 52.2 50.3 51.8 52.5 51.5 51.0 51.1 51.3 52.4 52.8 66.1 40.2 3 / 10 65.7 40.5 2 / 10 65.9 41.1 5 / 63.7 39.9 1 / 10 65.7 40.5 2 / 10 66.2 41.3 7 / 10 64.5 39.8 0 / 10 64.9 40.7 3 / 10 66.0 41.4 7 / 10 61.3 40.5 3 / 10 64.9 40.8 3 / 10 65.4 41.2 4 / 10 67.2 41.1 0 / 10 69.3 41.9 3 / 10 69.5 42.5 7 / 10 Table 8: Performance of 350M pretrained models on 10 selected downstream tasks. All models are trained on 20B-token corpora of equal size, derived from different sources: Raw (unfiltered corpus), various filtering strategies, and further fine-grained refinement applied to the filtered corpora. Underlined results indicate the best performance within the Raw group or within each specific filtering group. #Win represents the number of tasks for which the method achieves the best performance within its group. Bolded results indicate the best overall performance across all settings."
        },
        {
            "title": "D FULL EVALUATION RESULTS",
            "content": "We conduct comprehensive set of experiments to validate the effectiveness of our proposed refinement approach under the evaluation setups introduced earlier. Our evaluation covers two main axes: (1) from-scratch pretraining at different model scales, and (2) detailed analysis of refinement effects on individual text instances. For from-scratch pretraining, we evaluate models at both 350M and 750M parameter scales. The performance of the 350M models on 10 selected downstream tasks is summarized in Table 8. Full results across all downstream benchmarks for the 350M models are reported in Tables 10 and 11, and for the 750M models in Tables 12 and 13. These tables provide complete breakdown of performance across different refinement and filtering strategies. In addition, we present thorough evaluation of the quality impact of refinement on individual documents. The full set of results for this analysis is provided in Table 9, which includes multiple metrics to capture the effectiveness, efficiency, and reliability of each refinement method."
        },
        {
            "title": "E CASE STUDY",
            "content": "To better understand the qualitative effects of different refinement methods, we present several realworld examples drawn from the RedPajama-V2 corpus. These case studies highlight key behavioral differences between end-to-end refinement, ProX, and our proposed REFINEX. In Table 14, we showcase examples of end-to-end refinement outputs. Despite carefully designed prompts intended to restrict unnecessary modifications, the expert LLM often introduces overly aggressive edits, altering the original structure or semantics of the input. In contrast, REFINEX avoids such risks by design, as it only permits deletion operations, thereby preserving the integrity of the original text and minimizing the introduction of model-specific biases."
        },
        {
            "title": "Method",
            "content": "Avg. Score Rat.(%) Rat.(%) Untouched(%) Empty(%) #Toks Score = 1 Score = Score = 3 Score = 4 Score = 5 E2E E2Edel Prox REFINEX E2E E2Edel Prox REFINEX E2E E2Edel Prox REFINEX E2E E2Edel Prox REFINEX E2E E2Edel Prox REFINEX 2.704 1.972 1.287 2.009 3.266 2.590 2.233 2.718 3.631 3.281 3.192 3.413 4.175 4.003 4.026 4. 4.914 4.833 4.905 4.917 38.86 27.31 17.05 22.23 68.83 38.30 17.20 42.14 59.01 33.29 21.54 41.14 21.30 9.85 8.38 12.70 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.38 1.05 0.96 0.66 3.39 7.95 5.47 4.58 3.59 8.80 5.64 4.86 8.52 16.18 9.17 8.20 1.11 3.75 17.12 7. 1.39 6.24 19.98 7.59 1.93 7.85 21.66 9.85 2.41 9.92 23.10 13.52 4.19 13.86 24.24 16.50 56.41 56.41 33.67 68.17 12.27 12.27 14.97 19. 3.72 3.72 12.80 6.18 0.26 0.26 3.58 0.93 0.03 0.03 0.88 0.18 19.36 4.91 4.46 39.02 4.73 6.95 48.50 3.96 5. 55.50 3.16 4.92 70.05 2.64 2.67 Table 9: Full evaluation results on refined documents. Avg., Rat., and Rat. represent the average quality score and the proportion of documents whose scores increased or decreased after refinement, respectively. Untouched indicates the percentage of documents left unchanged, and Empty denotes those reduced to empty content. #Toks indicates the ratio of output tokens to input tokens. Beyond these examples, we present additional cases in subsequent tables to compare the refinement behaviors of REFINEX and ProX. These examples demonstrate that REFINEX is better at identifying and reliably removing low-value content-such as boilerplate text, irrelevant metadata, or malformed strings-while preserving meaningful information. The case studies further confirm that REFINEX produces more precise and trustworthy refinements, making it more robust solution for large-scale pretraining data optimization."
        },
        {
            "title": "Under review as a conference paper",
            "content": "Table 10: Full evaluation results (1/2) on 350M pretrained model across different downstream tasks, with varying numbers of training tokens used during pretraining. #token ARC-C ARC-E CSQA HellaSwag MMLU OBQA PiQA SIQA WinoG SciQ AVG 4 8 12 16 20 4 8 12 16 20 4 8 12 16 20 4 8 12 16 4 8 12 16 20 4 8 12 16 20 4 8 12 16 20 4 8 12 16 20 4 8 12 16 20 22.1 23.0 23.8 23.8 24. 22.2 23.5 24.1 24.0 24.6 23.9 24.2 24.3 24.2 24.5 23.3 22.4 22.6 22.4 23.2 23.3 23.4 23.9 25.1 23.4 20.4 22.0 23.0 23.2 24.5 22.2 22.6 23.0 24.5 23. 22.7 23.5 23.1 23.2 23.4 22.7 22.9 23.6 23.9 24.5 37.3 38.6 38.3 41.0 39.8 37.9 40.2 43.3 43.6 43.9 37.6 40.3 42.6 42.9 44.1 36.4 38.8 39.9 39.7 40. 37.6 39.2 41.3 42.4 43.1 37.4 42.0 41.7 42.1 43.6 37.5 39.7 43.5 42.1 42.2 37.4 39.9 41.3 42.6 43.1 36.9 40.9 41.4 42.6 43.6 25.5 27.8 27.6 28.8 28. 25.9 27.9 28.7 29.3 29.2 25.6 25.7 27.9 28.8 28.4 26.1 26.6 27.9 29.8 28.7 26.8 28.3 27.8 28.5 28.5 27.1 29.7 28.9 29.2 30.4 24.5 28.1 28.5 28.7 30. 25.4 27.9 28.5 28.0 28.5 25.7 28.6 28.4 29.2 29.4 61.2 62.4 62.0 64.0 64.3 60.1 61.4 63.1 63.4 64.5 60.0 62.4 62.9 62.8 64.2 60.5 61.7 63.4 63.3 64. 60.6 63.9 64.7 64.9 64.8 60.7 62.1 64.4 65.8 66.0 59.8 62.4 63.9 64.5 64.5 59.4 61.1 63.4 63.6 64.8 60.8 62.5 62.6 62.8 63.9 38.1 37.4 38.9 39.4 38. 38.6 38.5 39.9 39.5 38.8 38.1 39.3 39.5 38.8 38.9 38.0 38.9 38.4 39.1 39.5 39.7 39.2 39.8 38.8 38.4 39.0 39.6 39.5 38.4 39.5 38.1 37.5 37.6 37.9 39. 37.2 38.5 38.9 40.2 38.4 37.1 38.8 39.0 39.8 40.5 50.6 49.4 49.3 49.2 49.5 51.0 51.2 52.6 52.0 50.1 49.9 50.8 51.7 51.6 51.5 49.8 52.2 50.6 50.4 49. 50.7 51.0 51.1 50.7 51.6 50.0 51.4 51.1 50.1 50.7 51.2 51.3 49.1 50.3 49.1 50.2 52.0 51.4 51.6 51.6 51.7 54.0 52.4 52.0 52.2 60.3 62.7 63.8 64.3 62. 58.1 60.8 64.2 64.1 65.4 58.8 61.3 63.9 64.7 65.2 59.5 63.8 64.3 65.9 66.1 59.1 61.8 64.8 63.9 65.7 59.6 63.2 65.3 65.9 65.9 57.1 61.3 63.7 61.6 63. 58.2 65.5 65.9 65.4 65.7 60.0 62.6 64.6 65.0 66.2 37.7 38.5 39.0 39.8 39.6 37.4 38.8 40.1 40.5 40.6 37.7 39.0 40.3 40.6 40.9 37.2 38.9 39.5 40.0 40. 37.6 39.2 40.2 40.3 40.5 37.7 39.6 40.3 40.6 41.1 37.2 38.7 39.5 39.7 39.9 37.2 39.4 40.1 40.3 40.5 37.8 39.7 40.2 40.6 41.3 30.4 31.0 31.7 33.8 34. 30.1 32.6 32.6 34.2 35.5 30.1 32.4 34.7 35.3 35.7 27.5 30.7 33.1 33.4 34.6 27.6 32.3 34.0 35.1 36.5 29.6 32.3 33.6 35.2 35.4 29.7 31.3 31.7 35.1 35. 28.9 32.8 34.6 34.7 36.5 31.2 32.7 34.3 34.8 35.7 25.8 26.8 27.6 27.4 27.2 25.6 25.6 25.8 27.6 27.2 26.8 27.8 29.2 29.8 30.4 25.2 28.0 28.8 29.8 28. 24.6 26.8 28.0 27.2 26.4 27.2 28.2 28.4 29.0 28.4 25.8 26.2 27.4 26.4 25.6 27.0 26.4 27.6 27.4 26.4 26.2 27.6 28.9 29.3 30.1 Raw 25.7 26.2 26.8 26.5 26.6 Raw + ProX 25.5 26.2 26.9 26.9 27.2 Raw + RefineX 25.8 26.2 26.6 26.7 27.0 Go 25.6 26.0 25.7 26.6 26.7 Go + ProX 26.1 26.3 26.8 26.7 26.7 Go + RefineX 25.6 25.8 26.7 26.8 27.0 25.8 26.1 27.1 26.3 26.3 C4 + ProX 25.9 26.2 26.4 26.7 26.7 C4 + RefineX 26.0 26.2 26.7 27.0 27."
        },
        {
            "title": "Under review as a conference paper",
            "content": "Table 11: Full evaluation results (2/2) on 350M pretrained model across different downstream tasks, with varying numbers of training tokens used during pretraining. #token ARC-C ARC-E CSQA HellaSwag MMLU OBQA PiQA SIQA WinoG SciQ AVG 4 8 12 16 20 4 8 12 16 20 4 8 12 16 20 4 8 12 16 4 8 12 16 20 4 8 12 16 20 4 8 12 16 20 4 8 12 16 20 4 8 12 16 20 21.5 22.8 23.2 23.8 23. 22.4 22.3 24.2 23.9 24.4 23.4 24.2 24.0 22.4 24.2 22.1 22.8 23.5 23.1 24.5 22.2 23.5 23.7 24.6 24.9 24.2 23.9 24.2 23.9 24.3 23.9 24.8 24.7 24.9 25. 23.5 22.8 24.0 25.1 24.8 23.1 23.8 25.1 25.9 26.8 34.8 39.0 39.2 41.8 39.6 37.1 39.1 42.4 42.1 43.2 37.1 40.5 40.9 42.5 43.5 36.5 40.2 40.6 41.4 42. 36.8 38.9 41.1 42.7 42.6 39.0 40.8 41.8 43.8 44.1 41.6 43.2 44.5 45.4 44.3 42.6 43.2 44.7 46.4 46.4 39.7 43.8 46.6 47.5 48.1 25.1 25.5 28.1 28.7 28. 26.4 28.0 28.5 28.9 28.2 26.5 29.0 28.7 29.2 29.4 25.7 28.6 29.2 28.8 29.6 26.1 28.0 28.6 30.7 30.0 26.2 27.8 28.6 29.4 29.6 25.3 27.6 28.2 28.6 28. 26.3 28.6 28.7 29.5 29.1 25.5 28.1 27.5 27.8 28.6 59.4 63.1 64.3 64.3 64.5 60.4 63.0 62.3 64.1 64.8 59.9 61.4 63.5 63.6 65.9 62.5 62.1 64.9 64.0 65. 60.8 61.2 63.3 63.7 65.1 60.2 63.3 63.5 65.2 64.4 59.5 62.0 61.2 64.1 64.1 59.5 62.1 65.9 64.5 65.3 59.3 62.2 63.6 64.5 64.4 38.9 38.8 39.0 39.8 38. 38.3 39.7 39.8 39.1 38.8 38.3 38.2 39.4 38.5 39.5 38.7 39.2 38.6 40.2 39.6 37.9 38.1 37.9 38.4 38.7 38.3 39.7 38.0 39.1 39.2 37.7 37.8 38.4 38.0 38. 37.8 38.1 37.6 38.4 38.5 38.4 37.8 37.9 38.8 39.3 50.8 50.7 51.6 50.3 50.8 51.0 50.3 52.5 51.2 51.8 50.5 52.7 52.9 51.9 52.5 48.7 48.8 51.0 52.0 51. 50.5 50.5 51.6 52.6 51.0 50.4 51.8 49.7 52.4 51.1 49.9 51.7 51.5 51.2 51.3 49.3 49.7 51.3 51.9 52.4 50.5 49.9 51.1 51.9 52.8 58.2 60.6 63.1 65.1 64. 57.8 63.3 64.1 65.2 64.9 60.4 61.8 64.1 66.2 66.0 56.0 60.5 64.5 62.1 61.3 55.1 60.8 61.1 62.6 64.9 56.9 62.4 63.0 63.5 65.4 62.2 68.4 66.5 66.5 67. 63.3 64.5 65.7 67.8 69.3 58.5 67.9 65.2 67.7 69.5 36.7 38.3 39.4 40.3 40.1 37.7 39.1 40.1 40.4 40.7 37.8 39.2 40.2 40.5 41.4 37.2 38.7 39.9 40.0 40. 37.1 38.6 39.5 40.6 40.8 38.0 39.4 39.7 41.0 41.2 38.2 40.2 40.7 41.1 41.1 38.5 40.0 41.0 41.7 42.0 38.1 40.4 41.0 41.8 42.5 Fw 25.4 26.0 26.0 26.3 26.3 Fw + ProX 25.9 26.4 26.4 27.0 27.1 Fw + RefineX 25.8 26.4 26.3 26.3 26.6 Comb 25.5 25.9 26.2 26.5 26.4 Comb + ProX 25.7 26.7 26.7 26.6 27.1 Comb + RefineX 26.0 26.1 26.3 26.7 26.8 ProX-D 25.8 26.9 27.5 27.8 27.8 ProX-D + ProX 25.9 26.2 26.9 27.8 28.1 24.0 26.4 26.4 27.6 27.8 27.0 27.6 28.0 28.8 28.2 26.6 26.8 28.8 28.8 31. 26.2 26.4 27.6 26.8 28.0 25.4 26.4 25.6 28.0 27.4 26.0 26.6 27.0 30.0 29.8 26.2 27.8 30.2 28.8 28.8 27.3 28.9 29.2 30.2 30.4 ProX-D + RefineX 26.4 27.4 27.6 27.9 29.3 28.4 29.4 30.1 30.5 29.6 29.3 30.0 32.7 35.1 35.6 30.8 31.8 33.0 33.9 36.0 29.2 31.4 33.0 35.5 35.8 29.9 32.2 33.2 35.1 35. 30.4 31.8 35.3 36.3 36.7 28.8 31.7 34.8 36.2 37.2 30.0 31.5 34.1 35.3 35.6 30.0 35.7 36.3 35.2 35.3 30.8 33.4 34.8 35.3 36."
        },
        {
            "title": "Under review as a conference paper",
            "content": "Table 12: Full evaluation results (1/2) on 750M pretrained model across different downstream tasks, with varying numbers of training tokens used during pretraining. #token ARC-C ARC-E CSQA HellaSwag MMLU OBQA PiQA SIQA WinoG SciQ AVG 4 8 12 16 20 4 8 12 16 20 4 8 12 16 20 4 8 12 16 4 8 12 16 20 4 8 12 16 20 4 8 12 16 20 4 8 12 16 20 4 8 12 16 20 22.9 24.3 24.2 24.7 24. 24.1 23.5 24.4 24.5 25.3 24.1 23.9 23.1 23.3 25.2 23.2 23.5 23.7 24.7 24.3 24.9 23.8 24.8 23.7 25.0 23.6 23.5 24.6 25.2 25.0 21.2 23.1 24.7 24.1 25. 23.3 24.8 25.0 24.8 25.1 21.8 23.9 23.9 24.3 24.8 40.1 42.6 43.6 45.2 45.4 38.5 40.1 43.6 44.9 46.5 40.1 41.4 43.0 45.0 45.9 40.4 41.4 43.5 43.9 45. 39.1 42.9 43.6 46.6 46.0 39.2 43.0 46.2 46.8 48.2 37.1 40.4 44.8 44.2 44.2 40.1 42.2 42.6 45.3 45.5 39.3 41.6 43.3 44.7 44.5 26.4 27.9 28.5 30.5 30. 26.8 28.4 29.4 30.5 30.2 26.5 29.3 29.5 31.1 32.0 27.6 27.6 27.7 28.2 28.6 26.2 28.6 29.6 30.5 30.9 26.9 28.7 29.5 29.5 30.4 27.1 28.2 30.4 29.1 30. 26.6 29.2 30.2 30.3 31.7 27.2 30.0 31.7 31.4 31.6 60.4 64.4 65.8 66.8 63.8 61.6 62.4 62.4 63.8 66.9 60.4 63.2 65.5 65.7 66.3 60.1 63.6 65.3 66.7 66. 61.5 64.3 66.2 65.9 66.7 61.6 63.4 66.4 66.9 66.7 59.5 62.8 65.0 66.2 66.5 62.2 65.1 63.9 66.7 66.6 62.4 62.8 65.9 67.3 68.0 38.7 40.2 38.8 39.5 39. 38.6 37.7 39.0 39.3 39.9 38.3 38.9 40.4 40.1 41.0 39.9 39.4 39.0 40.4 39.4 38.6 38.0 39.6 38.8 38.4 39.4 39.4 38.5 40.3 40.7 38.7 37.4 39.1 38.7 39. 37.0 38.5 38.5 38.4 40.1 39.1 39.4 39.2 38.0 39.9 49.8 48.0 48.6 50.6 51.0 50.7 52.1 49.6 52.1 51.9 50.7 51.3 52.2 50.4 52.5 51.5 51.2 52.1 51.5 50. 51.0 50.6 51.5 52.5 51.5 51.4 53.0 52.3 51.8 52.8 51.2 50.5 51.3 51.4 51.3 49.7 52.4 51.0 52.0 51.4 50.5 50.2 50.3 50.4 50.5 55.5 63.1 64.2 66.1 67. 56.4 57.9 64.1 66.6 66.4 56.5 63.5 64.5 65.5 68.3 55.9 62.0 64.2 65.9 66.9 55.2 62.0 65.1 67.4 68.4 58.2 61.4 66.6 67.2 69.3 56.8 62.5 67.1 66.9 67. 55.0 62.1 65.1 65.0 66.6 59.3 60.5 67.2 67.5 68.2 37.5 39.9 40.5 41.8 41.6 37.9 38.9 40.4 41.5 42.4 38.0 40.1 41.1 41.6 42.9 38.3 39.8 40.7 41.6 41. 37.9 40.1 41.5 42.2 42.6 38.3 40.7 42.0 42.5 43.2 37.7 40.5 40.8 41.9 42.1 37.7 40.5 40.8 41.9 42.5 38.1 39.7 41.5 42.1 42.7 30.5 34.8 36.7 38.1 38. 30.6 33.6 35.7 37.5 38.2 30.5 34.8 37.0 37.9 39.4 32.2 35.4 38.3 39.1 39.7 30.5 35.2 38.8 40.1 41.0 31.1 36.6 40.2 40.8 41.4 30.1 35.5 37.2 39.1 39. 29.9 34.6 35.9 39.2 41.1 30.7 34.2 37.9 40.1 41.2 24.2 27.2 27.8 29.6 28.2 25.2 27.0 29.4 28.8 31.0 27.0 27.8 28.6 29.8 31.2 26.6 27.8 26.4 28.7 28. 25.6 28.4 28.6 29.0 30.4 26.2 31.4 28.2 28.4 29.0 28.6 28.0 28.2 28.8 29.4 27.4 28.6 28.4 29.6 29.4 25.2 28.2 28.6 29.4 30.4 Raw 26.1 26.7 27.2 27.1 27.5 Raw + ProX 26.2 26.5 26.9 26.9 27.8 Raw + REFINEX 26.2 26.6 26.9 27.1 27.5 Go 25.9 26.6 26.6 27.0 27.1 Go + ProX 26.0 26.9 27.2 27.5 27.9 Go + REFINEX 25.7 26.9 27.8 27.7 28.1 25.8 27.2 27.0 27.5 27.4 C4 + ProX 26.2 27.0 27.0 27.9 27.9 C4 + REFINEX 25.7 26.7 26.8 27.7 28."
        },
        {
            "title": "Under review as a conference paper",
            "content": "Table 13: Full evaluation results (2/2) on 750M pretrained model across different downstream tasks, with varying numbers of training tokens used during pretraining. #token ARC-C ARC-E CSQA HellaSwag MMLU OBQA PiQA SIQA WinoG SciQ AVG 4 8 12 16 20 4 8 12 16 20 4 8 12 16 20 4 8 12 16 4 8 12 16 20 4 8 12 16 20 4 8 12 16 20 4 8 12 16 20 4 8 12 16 20 21.4 23.3 25.1 24.6 25. 23.7 24.9 26.8 26.4 25.5 22.3 24.6 25.6 25.7 26.3 22.3 23.6 23.4 23.4 24.3 24.2 24.1 24.0 25.0 24.8 23.1 25.3 25.4 25.5 25.5 21.4 23.3 25.1 24.6 25. 23.3 26.1 26.1 27.1 27.2 24.3 26.5 26.0 27.7 28.7 38.1 41.9 44.5 44.3 45.6 37.5 42.0 42.7 44.5 44.7 39.0 40.4 44.9 46.5 47.4 40.9 41.8 43.2 44.2 44. 39.9 42.6 42.4 43.1 45.4 39.1 42.5 44.6 45.9 46.6 38.1 41.9 44.5 44.3 45.6 44.4 46.7 50.9 51.7 51.0 44.2 47.9 50.3 51.7 53.2 27.8 28.5 29.9 30.0 31. 26.2 28.9 29.9 30.0 30.8 27.3 30.2 31.8 29.7 31.4 27.0 28.5 30.2 31.4 31.0 27.7 28.7 31.1 31.5 31.7 27.5 28.6 29.8 30.6 31.4 27.8 28.5 29.9 30.0 31. 27.5 27.3 28.8 30.2 30.1 26.1 29.0 29.4 29.8 30.8 61.0 62.8 65.0 65.3 66.6 61.3 65.4 65.7 65.9 67.4 61.1 64.3 65.1 65.8 66.6 62.0 63.8 65.9 66.4 66. 60.2 64.1 66.4 67.3 66.7 62.1 65.1 66.1 66.2 68.0 61.0 62.8 65.0 65.3 66.6 61.4 63.3 64.1 66.0 65.9 60.4 64.0 64.2 66.2 67.8 38.7 39.3 38.8 40.5 39. 38.9 39.9 39.8 39.3 39.5 38.3 38.7 39.4 39.8 39.6 38.6 38.3 39.3 39.5 39.0 37.6 38.9 39.8 40.1 39.5 39.3 39.3 39.8 38.1 39.6 38.7 39.3 38.8 40.5 39. 37.8 37.4 38.3 38.6 39.4 38.4 38.8 39.3 39.7 39.9 50.2 49.9 51.1 49.8 49.9 49.4 49.7 49.8 51.8 51.3 51.6 51.4 50.1 49.6 51.5 50.7 50.4 52.4 52.8 53. 51.9 52.4 50.4 52.3 51.7 50.0 52.2 52.8 54.5 53.2 50.2 49.9 51.1 49.8 49.9 49.9 50.7 49.9 50.1 50.8 51.6 50.4 51.0 52.8 51.6 57.1 62.9 64.9 67.9 66. 57.7 62.6 65.0 66.8 67.5 57.3 62.0 64.3 64.9 65.4 57.6 62.9 65.2 66.5 66.5 55.9 63.0 63.4 66.2 65.8 57.8 62.4 65.0 66.0 65.9 57.1 62.9 64.9 67.9 67. 61.7 64.8 69.0 69.3 70.2 61.4 64.4 66.8 68.8 70.9 37.4 39.4 40.9 41.6 41.9 38.0 40.0 41.3 42.0 42.3 37.8 39.8 41.2 41.8 42.7 38.4 39.8 41.3 41.9 42. 38.1 40.2 40.9 42.2 42.4 38.0 40.6 41.7 42.3 42.8 37.4 39.4 40.9 41.6 42.0 39.4 40.7 42.4 43.6 43.5 39.0 41.4 42.4 43.8 44.7 Fw 25.3 26.6 27.2 27.1 27.2 Fw + ProX 26.2 26.6 26.8 27.5 27.9 Fw + REFINEX 26.2 26.3 26.7 26.8 27.6 Comb 26.3 25.7 26.6 27.1 27.1 Comb + ProX 26.4 26.7 26.9 27.4 27.8 25.4 27.0 27.2 28.6 27.6 27.8 26.0 29.2 29.0 29.2 25.0 26.4 27.4 30.7 30. 28.2 28.2 29.0 27.8 28.6 24.2 26.2 27.0 30.0 29.4 Comb + REFINEX 26.1 26.8 26.9 27.6 27.5 ProX-D 25.3 26.6 27.2 27.1 27. ProX-D + ProX 27.1 28.0 28.2 28.5 28.9 24.2 27.8 29.0 29.0 28.0 25.4 27.0 27.2 28.6 27.6 28.8 27.2 30.6 33.8 30.4 29.3 32.2 35.6 37.9 39. 30.9 33.8 37.1 38.3 39.0 30.3 34.0 37.2 38.9 40.1 30.7 34.4 37.5 40.4 40.4 32.7 35.1 38.0 39.3 41.1 30.6 35.5 38.1 40.1 42.1 29.3 32.2 35.6 37.9 39. 32.2 35.3 38.2 40.7 41.2 ProX-D + REFINEX 31.5 36.3 38.0 40.9 41.7 26.8 27.7 28.7 28.9 29.6 25.6 29.2 30.4 31.2 31."
        },
        {
            "title": "Under review as a conference paper",
            "content": "Table 14: Cases study illustrating the deletion-only constraint in REFINEX. E2E modifies stylistic or lexical content, while REFINEX performs only removals, preserving imperfections. Case 1: Deletion-Only Constraint Raw Text: Climate change has become one of the most urgent issues facing our planet. Many scientists agree that human activities are significantly affecting the enviroment in negative way. Click here to BUY NOW: www.greendeals.fake/discount In recent years, more people have taken interest in renewable energy sources such as solar and wind power. $$$$###!!! <broken tag> // sdlfkj243== This transition could greatly reduce our dependence on fossil fuels. Visit ENVYBOOST.SITE to earn $$$ from home! ## Refined by E2E: Climate change has become one of the most urgent issues facing our planet. Many scientists agree that human activities are significantly affecting the environment in negative way. In recent years, more people have taken interest in renewable energy sources such as solar and wind power. This transition could greatly reduce our dependence on fossil fuels. Refined by REFINEX: Climate change has become one of the most urgent issues facing our planet. Many scientists agree that human activities are significantly affecting the enviroment in negative way. In recent years, more people have taken interest in renewable energy sources such as solar and wind power. This transition could greatly reduce our dependence on fossil fuels. Case 2: Deletion vs. Stylistic Rewriting Raw Text: The study results show that people dont sleep well when exposed to high levels of blue light at night. Buy blue-light blockers now! Limited-time offer: www.sleepmagic.io This can lead to issues with memory and focus. sldkfj234##!! <meta-junk> watch till the end Its better to use dim lights or avoid screens before going to bed. Refined by E2E: The study results indicate that people experience impaired sleep quality when exposed to high levels of blue light at night. This can lead to issues with memory and focus. Its better to use dim lights or avoid screens before going to bed. Refined by REFINEX: The study results show that people dont sleep well when exposed to high levels of blue light at night. This can lead to issues with memory and focus. Its better to use dim lights or avoid screens before going to bed. Case 3: Unintended Stylistic Change Raw Text: Many students didnt really understand the topic after the lecture. Watch our sponsors at www.ads-university.biz This suggests that the teaching method might need improvement. asd8127&# noise fragment Learn Crypto in 2 weeks! Refined by E2E: Many students struggled to grasp the topic after the lecture. This suggests that the teaching method might need improvement. Refined by REFINEX: Many students didnt really understand the topic after the lecture. This suggests that the teaching method might need improvement."
        },
        {
            "title": "Under review as a conference paper",
            "content": "Table 15: Cases (1/5) after applying ProX and REFINEX. Text in red indicates low-value content to be removed. ... denotes omitted content due to limited space. Case 1 Raw Text: ... Lerners engaging prose and robust idealism cast laboratory methods in positive light, yet he does not overlook the dark side of experimentation towards what works (12). At the end of chapter eight, he acknowledges that experimentation entails failure. What am urging here is for reformers . . . to have the courage to failbut then learn from those failures and mount new experiment rather than revert to the status quo (187). Lerner makes this bold call, though he is well aware that the structure of higher education is not conducive to it. Failure is an anathema in higher education, particularly in tight budgetary times. After observing that literacy activities in the early grades often illustrate laboratory methods of learning, he remarks, The challenge, certainly, is how to retain the play that marks these early efforts at language learning in system that often does not believe in playing around (196). The call to pursue laboratory methods may be less appealing when it is understood as call to embrace failure, even if only in the short-term. Return to Composition Forum 21 table of contents. </s> desktop is booked in for repair it goes into queue to await the services of technician. ($cookies, $Dallas) </s> DBrandeis-Bardin campusresh, reflect and enjoy each others company! We offer multiple venues for your Family Reunion, Wedding Reception or Rehearsal Dinner. Contact us. ... Refined by Prox: ... Lerners engaging prose and robust idealism cast laboratory methods in positive light, yet he does not overlook the dark side of experimentation towards what works (12). At the end of chapter eight, he acknowledges that experimentation entails failure. What am urging here is for reformers . . . to have the courage to failbut then learn from those failures and mount new experiment rather than revert to the status quo (187). Lerner makes this bold call, though he is well aware that the structure of higher education is not conducive to it. Failure is an anathema in higher education, particularly in tight budgetary times. After observing that literacy activities in the early grades often illustrate laboratory methods of learning, he remarks, The challenge, certainly, is how to retain the play that marks these early efforts at language learning in system that often does not believe in playing around (196). The call to pursue laboratory methods may be less appealing when it is understood as call to embrace failure, even if only in the short-term. Return to Composition Forum 21 table of contents. </s> desktop is booked in for repair it goes into queue to await the services of technician. ($cookies, $Dallas) </s> DBrandeis-Bardin campusresh, reflect and enjoy each others company! ... Refined by REFINEX: ... Lerners engaging prose and robust idealism cast laboratory methods in positive light, yet he does not overlook the dark side of experimentation towards what works (12). At the end of chapter eight, he acknowledges that experimentation entails failure. What am urging here is for reformers . . . to have the courage to failbut then learn from those failures and mount new experiment rather than revert to the status quo (187). Lerner makes this bold call, though he is well aware that the structure of higher education is not conducive to it. Failure is an anathema in higher education, particularly in tight budgetary times. After observing that literacy activities in the early grades often illustrate laboratory methods of learning, he remarks, The challenge, certainly, is how to retain the play that marks these early efforts at language learning in system that often does not believe in playing around (196). The call to pursue laboratory methods may be less appealing when it is understood as call to embrace failure, even if only in the short-term. ..."
        },
        {
            "title": "Under review as a conference paper",
            "content": "Table 16: Cases (2/5) after applying ProX and REFINEX. Text in red indicates low-value content to be removed. ... denotes omitted content due to limited space. Case 2 Raw Text: ... June 24, 2016 STORE CHECKOUT/CART EXCLUSIVE // Listen to Stage Kids new album Intra Mental in full Five years after their full-length release, Killer Tofu, the San Diego-based instrumental band Stage Kids have reemerged with their latest masterpiece, Intra Mental. Two new members (keyboard and electronics) have joined the quintet since 2011, resulting in more complex and dynamic soundscapes. But the majority of us are summing this one up in one word: definitive. Stage Kids waste no time introducing their new sound with the opening track Delaylay. The atmospheric synth and guitar give way to barrage of start/stop rhythms that jump from idea to idea, frequently straying from the tonic of the song, but never too far from comfort. In Face First the rapid staccato of the guitar and stabbing piano keys create an almost glitchy feel that evolves into frenetically tapped phrases typical of bands like Rooftops, Invalids, and Floral. In Pulsewave, The Noise After, and Connections, Stage Kids re-direct their song writing towards electronic sensibilities. Intra Mental comes out this weekend. You can get all the info on their Bandcamp page. You can also keep in touch with Stage Kids on their Facebook page. Related exclusive fecking bahamas full intra mental stage kids stream Share On Tweet Previous ArticleEXCLUSIVE // Dumb Waiter premier new track Cheesevader Stephen Kemp Contributor Related Posts ... Refined by Prox: ... June 24, 2016 Five years after their full-length release, Killer Tofu, the San Diego-based instrumental band Stage Kids have reemerged with their latest masterpiece, Intra Mental. Two new members (keyboard and electronics) have joined the quintet since 2011, resulting in more complex and dynamic soundscapes. But the majority of us are summing this one up in one word: definitive. Stage Kids waste no time introducing their new sound with the opening track Delaylay. The atmospheric synth and guitar give way to barrage of start/stop rhythms that jump from idea to idea, frequently straying from the tonic of the song, but never too far from comfort. In Face First the rapid staccato of the guitar and stabbing piano keys create an almost glitchy feel that evolves into frenetically tapped phrases typical of bands like Rooftops, Invalids, and Floral. In Pulsewave, The Noise After, and Connections, Stage Kids re-direct their song writing towards electronic sensibilities. Intra Mental comes out this weekend. You can get all the info on their Bandcamp page. You can also keep in touch with Stage Kids on their Facebook page. ... Refined by REFINEX: ... Five years after their full-length release, Killer Tofu, the San Diego-based instrumental band Stage Kids have reemerged with their latest masterpiece, Intra Mental. Two new members (keyboard and electronics) have joined the quintet since 2011, resulting in more complex and dynamic soundscapes. But the majority of us are summing this one up in one word: definitive. Stage Kids waste no time introducing their new sound with the opening track Delaylay. The atmospheric synth and guitar give way to barrage of start/stop rhythms that jump from idea to idea, frequently straying from the tonic of the song, but never too far from comfort. In Face First the rapid staccato of the guitar and stabbing piano keys create an almost glitchy feel that evolves into frenetically tapped phrases typical of bands like Rooftops, Invalids, and Floral. In Pulsewave, The Noise After, and Connections, Stage Kids re-direct their song writing towards electronic sensibilities. ..."
        },
        {
            "title": "Under review as a conference paper",
            "content": "Table 17: Cases (3/5) after applying ProX and REFINEX. Text in red indicates low-value content to be removed. ... denotes omitted content due to limited space. Case 3 Raw Text: Atlanta Palm Beach Tampa / St. Pete My Name is Maryan Docent-Led Exhibition Tour TAGS: Art Museum Tour Sun 08/14/2022 Register for tour led by MOCA docent Dr. Helen Sachs Chaset. Dr. Chaset is an educator with more than 45 years of experience in administration, professional development, and program development. She is also board member of Miami-Dade Holocaust Survivors, the Jewish Community Services of South Florida and the Center for the Advancement of Jewish Education. Dr. Chaset is the daughter of two Holocaust Survivors and was born in displaced persons camp in Hannover, Germany. Time 11:30 a.m. Venue Museum of Contemporary Art North Miami Address 770 N.E. 125 Street North Miami, FL 33161 GET DIRECTIONS The Palm Miami BOURBON STEAK Nature Photography at Deering Estate - March Art on the Plaza: Clint and April Bird Walk at Deering Estate OUTshine LGBTQ+ Film Festival Fort Lauderdale Edition Refined by Prox: My Name is Maryan Docent-Led Exhibition Tour TAGS: Art Museum Tour Sun 08/14/2022 Register for tour led by MOCA docent Dr. Helen Sachs Chaset. Dr. Chaset is an educator with more than 45 years of experience in administration, professional development, and program development. She is also board member of Miami-Dade Holocaust Survivors, the Jewish Community Services of South Florida and the Center for the Advancement of Jewish Education. Dr. Chaset is the daughter of two Holocaust Survivors and was born in displaced persons camp in Hannover, Germany. Time 11:30 a.m. Venue Museum of Contemporary Art North Miami Address 770 N.E. 125 Street North Miami, FL 33161 GET DIRECTIONS The Palm Miami BOURBON STEAK Nature Photography at Deering Estate - March Art on the Plaza: Clint and April Bird Walk at Deering Estate Refined by REFINEX: My Name is Maryan Docent-Led Exhibition Tour TAGS: Art Museum Tour Sun 08/14/2022 Register for tour led by MOCA docent Dr. Helen Sachs Chaset. Dr. Chaset is an educator with more than 45 years of experience in administration, professional development, and program development. She is also board member of Miami-Dade Holocaust Survivors, the Jewish Community Services of South Florida and the Center for the Advancement of Jewish Education. Dr. Chaset is the daughter of two Holocaust Survivors and was born in displaced persons camp in Hannover, Germany. Time 11:30 a.m. Venue Museum of Contemporary Art North Miami Address 770 N.E. 125 Street North Miami, FL"
        },
        {
            "title": "Under review as a conference paper",
            "content": "Table 18: Cases (4/5) after applying ProX and REFINEX. Text in red indicates low-value content to be removed. ... denotes omitted content due to limited space. Case 4 Raw Text: Justin Iveland and Claude Weisbuch. Direct measurement of Auger electrons emitted from semiconductor lightemitting diode under electrical injection: identification of the dominant mechanism for efficiency droop. Phys. Rev. Lett., 110, 177406, 2013. Fred Jendrzejewski, Alain Bernard, Killian Muller, Patrick Cheinet, Vincent Josse, Marie Piraud, Luca Pezzé, Laurent Sanchez-Palencia, Alain Aspect, and Philippe Bouyer. Threeture Physics 8, 398, 2012. Fred Jendrzejewski, Killian Muller, Jérémie Richard, Aditya Date, Thomas Plisson, Philippe Bouyer, Alain Aspect, and Vincent Josse. Coherent Backscattering of Ultracold Atoms. Physical Review Letters 109 (19), 2012. Juliette Billy, Vincent Josse, Zhanchun C. Zuo, Alain Bernard, Ben Hambrecht, Pierre Lugan, David Clément, Laurent Sanchez-Palencia, Philippe Bouyer, and Alaine 453, 891, 2008. S. Félix, M. Asch, M. Filoche, and B. Sapoval. Localization and increased damping in irregular acoustic cavities. Journal of Science and Vibration 299, 965-976, 2007. Luis A. CaffarYves Meyer and Ronald R. Coifman. Wavelets: CalderónZygmund and Multilinear Operators. Number 48 in7.fi. </s> Another woman who worked at the ABCs Toowong office in Brisbane has been diagnosed with breast cancer. LEIGH SALES, PRESENTER: Another woman who worked at the ABCTests carried out on the site failed to uncover cause of the cluster. </s> How do insert the citation? According to Smith (2016) sup.14). Sustainability can be defined as socio-ecological process (Smith, 2016). Cite both names everytime you refer to their work. You must cite all the authors names the first time you refer to that work. The work you submit for grading must be your own. If you use work from other sources, you must acknowledge it properly. This also applies to online sources such as the web. Your work will be assession tools. Visit the Academic integrity and copyright website for further information.. </s> Whats the change the fans on laptop (approximately)? 2m experience with how was plugged into an outlet. It isnt causing any with to recover my files with chips called? After few minutes update (64) + about 5 3020 brands than Intel? Refined by Prox: Justin Iveland and Claude Weisbuch. Direct measurement of Auger electrons emitted from semiconductor lightemitting diode under electrical injection: identification of the dominant mechanism for efficiency droop. Phys. Rev. Lett., 110, 177406, 2013. Fred Jendrzejewski, Alain Bernard, Killian Muller, Patrick Cheinet, Vincent Josse, Marie Piraud, Luca Pezzé, Laurent Sanchez-Palencia, Alain Aspect, and Philippe Bouyer. Threeture Physics 8, 398, 2012. Fred Jendrzejewski, Killian Muller, Jérémie Richard, Aditya Date, Thomas Plisson, Philippe Bouyer, Alain Aspect, and Vincent Josse. Coherent Backscattering of Ultracold Atoms. Physical Review Letters 109 (19), 2012. Juliette Billy, Vincent Josse, Zhanchun C. Zuo, Alain Bernard, Ben Hambrecht, Pierre Lugan, David Clément, Laurent Sanchez-Palencia, Philippe Bouyer, and Alaine 453, 891, 2008. S. Félix, M. Asch, M. Filoche, and B. Sapoval. Localization and increased damping in irregular acoustic cavities. Journal of Science and Vibration 299, 965-976, 2007. Luis A. CaffarYves Meyer and Ronald R. Coifman. Wavelets: CalderónZygmund and Multilinear Operators. Number 48 in7.fi. </s> Another woman who worked at the ABCs Toowong office in Brisbane has been diagnosed with breast cancer. LEIGH SALES, PRESENTER: Another woman who worked at the ABCTests carried out on the site failed to uncover cause of the cluster. </s> How do insert the citation? According to Smith (2016) sup.14). Sustainability can be defined as socio-ecological process (Smith, 2016). Cite both names everytime you refer to their work. Refined by REFINEX: Justin Iveland and Claude Weisbuch. Direct measurement of Auger electrons emitted from semiconductor lightemitting diode under electrical injection: identification of the dominant mechanism for efficiency droop. Phys. Rev. Lett., 110, 177406, 2013. Fred Jendrzejewski, Alain Bernard, Killian Muller, Patrick Cheinet, Vincent Josse, Marie Piraud, Luca Pezzé, Laurent Sanchez-Palencia, Alain Aspect, and Philippe Bouyer. Threeture Physics 8, 398, 2012. Fred Jendrzejewski, Killian Muller, Jérémie Richard, Aditya Date, Thomas Plisson, Philippe Bouyer, Alain Aspect, and Vincent Josse. Coherent Backscattering of Ultracold Atoms. Physical Review Letters 109 (19), 2012. Juliette Billy, Vincent Josse, Zhanchun C. Zuo, Alain Bernard, Ben Hambrecht, Pierre Lugan, David Clément, Laurent Sanchez-Palencia, Philippe Bouyer, and Alaine 453, 891, 2008. S. Félix, M. Asch, M. Filoche, and B. Sapoval. Localization and increased damping in irregular acoustic cavities. Journal of Science and Vibration 299, 965-976, 2007."
        },
        {
            "title": "Under review as a conference paper",
            "content": "Table 19: Cases (5/5) after applying ProX and REFINEX. Text in red indicates low-value content to be removed. ... denotes omitted content due to limited space. Case 5 Raw Text: ... Comments are turned off. Learn more Description Subscribe for more movie scenes! Shrek 2 - Livin la Vida Loca scene Show less Show more Watch on YouTube Animation 2004 1 hr 32 min English audio BUY OR RENT Happily ever after never seemed so far far away when trip to meet the in-laws turns into hilariously twisted adventure for Shrek (Mike Myers) and Fiona (Cameron Diaz). With the help of his faithful Donkey (Eddie Murphy), Shrek takes on potion-brewing Fairy Godmother, the pompous Prince Charming (Rupert Everett), and the ogre-killer, Puss In Boots (Antonio Banderas) whos pussycat at heart. Save the Enemy Spookiz Cartoons for Kids WildBrain Kids WildBrain Kids ... Refined by Prox: ... Comments are turned off. Learn more Description Subscribe for more movie scenes! Shrek 2 - Livin la Vida Loca scene Show less Show more Watch on YouTube Animation 2004 1 hr 32 min English audio BUY OR RENT Happily ever after never seemed so far far away when trip to meet the in-laws turns into hilariously twisted adventure for Shrek (Mike Myers) and Fiona (Cameron Diaz). With the help of his faithful Donkey (Eddie Murphy), Shrek takes on potion-brewing Fairy Godmother, the pompous Prince Charming (Rupert Everett), and the ogre-killer, Puss In Boots (Antonio Banderas) whos pussycat at heart. Save the Enemy Spookiz Cartoons for Kids WildBrain Kids WildBrain Kids ... Refined by REFINEX: ... Happily ever after never seemed so far far away when trip to meet the in-laws turns into hilariously twisted adventure for Shrek (Mike Myers) and Fiona (Cameron Diaz). With the help of his faithful Donkey (Eddie Murphy), Shrek takes on potion-brewing Fairy Godmother, the pompous Prince Charming (Rupert Everett), and the ogre-killer, Puss In Boots (Antonio Banderas) whos pussycat at heart. ..."
        }
    ],
    "affiliations": [
        "Alibaba Group",
        "Institute of Computing Technology, Chinese Academy of Sciences",
        "National University of Singapore",
        "University of California, Merced"
    ]
}