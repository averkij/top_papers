{
    "paper_title": "STream3R: Scalable Sequential 3D Reconstruction with Causal Transformer",
    "authors": [
        "Yushi Lan",
        "Yihang Luo",
        "Fangzhou Hong",
        "Shangchen Zhou",
        "Honghua Chen",
        "Zhaoyang Lyu",
        "Shuai Yang",
        "Bo Dai",
        "Chen Change Loy",
        "Xingang Pan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present STream3R, a novel approach to 3D reconstruction that reformulates pointmap prediction as a decoder-only Transformer problem. Existing state-of-the-art methods for multi-view reconstruction either depend on expensive global optimization or rely on simplistic memory mechanisms that scale poorly with sequence length. In contrast, STream3R introduces an streaming framework that processes image sequences efficiently using causal attention, inspired by advances in modern language modeling. By learning geometric priors from large-scale 3D datasets, STream3R generalizes well to diverse and challenging scenarios, including dynamic scenes where traditional methods often fail. Extensive experiments show that our method consistently outperforms prior work across both static and dynamic scene benchmarks. Moreover, STream3R is inherently compatible with LLM-style training infrastructure, enabling efficient large-scale pretraining and fine-tuning for various downstream 3D tasks. Our results underscore the potential of causal Transformer models for online 3D perception, paving the way for real-time 3D understanding in streaming environments. More details can be found in our project page: https://nirvanalan.github.io/projects/stream3r."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 3 9 8 0 1 . 8 0 5 2 : r STREAM3R: Scalable Sequential 3D Reconstruction with Causal Transformer Yushi Lan1, Yihang Luo1, Fangzhou Hong1, Shangchen Zhou1, Honghua Chen1, Zhaoyang Lyu2, Shuai Yang3, Bo Dai4, Chen Change Loy1, Xingang Pan1 1S-Lab, Nanyang Technological University, Singapore 2Shanghai Artificial Intelligence Laboratory 3WICT, Peking University 4The University of Hong Kong https://nirvanalan.github.io/projects/stream3r Figure 1: STREAM3R. Given stream of input images, our method estimates dense 3D geometry for each incoming frame using causal Transformer. Features from previously observed frames are cached as context for future inference."
        },
        {
            "title": "Abstract",
            "content": "We present STREAM3R, novel approach to 3D reconstruction that reformulates pointmap prediction as decoder-only Transformer problem. Existing state-ofthe-art methods for multi-view reconstruction either depend on expensive global optimization or rely on simplistic memory mechanisms that scale poorly with sequence length. In contrast, STREAM3R introduces an streaming framework that processes image sequences efficiently using causal attention, inspired by advances in modern language modeling. By learning geometric priors from large-scale 3D datasets, STREAM3R generalizes well to diverse and challenging scenarios, including dynamic scenes where traditional methods often fail. Extensive experiments show that our method consistently outperforms prior work across both static and dynamic scene benchmarks. Moreover, STREAM3R is inherently compatible with LLM-style training infrastructure, enabling efficient large-scale pretraining and fine-tuning for various downstream 3D tasks. Our results underscore the potential of causal Transformer models for online 3D perception, paving the way for realtime 3D understanding in streaming environments. More details can be found in our project page. *Equal contribution. Preprint. Under review."
        },
        {
            "title": "Introduction",
            "content": "Reconstructing detailed 3D geometry from images is the crux in computer vision [1, 2, 3] and serves as the pre-requisite for series of downstream applications, like autonomous driving [4], virtual reality [5, 6], robotics [7], and more. While traditional visual-geometry methods like SfM [1] and Multi-view Stereo [8, 9] tackle this problem by solving series of sub-problems through handcrafted designs, recent trend led by DUSt3R [10] has demonstrated promising new way of directly regressing point clouds using powerful transformers. This paradigm, along with its follow-up works including MASt3R [11], Fast3R [12], and VGG-T [13], enables the reconstruction of 3D geometry from number of input imagesranging from single image to hundredsoffering more unified solution to 3D reconstruction. While these works focus on processing fixed set of images, real-world applications often require continuously processing streaming visual input and updating the reconstruction on-the-fly [14], such as when an autonomous agent explores new environment or when processing long video sequence. Handling streaming input poses significant new challenges. For example, naively running Fast3R or VGG-T every time new image arrives would incur significant redundant computation, as they have to reconstruct from scratch without inheriting previous results. These methods also struggle with long videos due to the expensive full-attention operation. Spann3R [15] extends DUSt3R with memory design [16] to support incremental reconstruction, but it still suffers from significant accumulated drift and fails over dynamic scenes. The most relevant concurrent work is CUT3R [17], which proposes RNN paradigm [18] to handle unstructured or streaming inputs. However, the RNN-based design does not scale well with modern network architectures [19] and struggles with long-range dependency due to its limited memory size. In light of the streaming nature of this task, in this work, we are interested in investigating the use of transformer with uni-directional causal attention to achieve online, incremental 3D reconstruction. In an LLM-style transformer with causal attention, the prediction at each step reuses previous computations through KVCache, which is proved successful in many language and audio tasks [20, 21]. We observe that this property is also highly desirable for addressing online 3D reconstruciton from streaming data, as each step should build upon the previous reconstruction while integrating new content from the incoming frame. Motivated by this, we propose STREAM3R, comprehensive framework that performs 3D reconstruction from unstructured or streaming input images, and predicts the corresponding point maps in both world and local coordinates [12]. Unlike concurrent works [12, 13] that resolve this issue by replacing DUSt3Rs asymmetric decoders with bi-directional attention blocks [22, 23], STREAM3R follows the modern decoder-only [24] transformer design, where incoming frames are sequentially processed and registered with causal attention [25]. In this way, STREAM3R is naturally compatible with modern Large Language Models (LLMs) [20] training and inference techniques such as window attention [26] and KVCache [24], i.e., the tokens of processed observations will be saved as reference for registering incoming frames. We train our method end-to-end on large collection of 3D data, and benchmark the proposed method on series of downstream applications. In summary, our key contributions are as follows: 1. We propose STREAM3R, decoder-only transformer framework that reformulates dense 3D reconstruction into sequential registration task with causal attention, enabling scalability to unstructured and streaming inputs. 2. STREAM3R is inherently compatible with modern LLM-style training and inference techniques, allowing efficient and scalable context accumulation across frames. 3. Our architecture supports both worldand local-coordinate pointmap prediction, and naturally generalizes to large-scale novel view synthesis scenarios via splatting-based rendering. 4. We train the model end-to-end on diverse 3D data and demonstrate competitive or superior performance on standard benchmarks, with strong generalization and fast inference speed."
        },
        {
            "title": "2 Related Work",
            "content": "Classic 3D Reconstruction. Early 3D reconstruction pipelines such as Structure-from-Motion (SfM) [27, 1, 28] and SLAM [14, 29, 30] estimate sparse geometry and camera poses from 2 image collections via geometric reasoning. More recent approaches such as NeRF [31, 32, 33] and Gaussian Splatting [34, 35] shift the focus to high-fidelity novel view synthesis using continuous volumetric representations. However, these methods are typically trained per-scene with no learned priors, leading to slow convergence and poor generalization to sparse or occluded inputsa limitation sometimes referred to as the tabula rasa assumption [17]. In contrast, we adopt data-driven approach that learns geometric priors from large-scale 3D datasets [36, 37], enabling fast and generalizable reconstruction from unstructured or streaming inputs. Learning 3D Priors from Data. Recent works leverage large-scale data to learn priors for depth estimation [38, 39, 40], pose+depth estimation [41, 42], and bundle adjustment [43]. While these methods improve generalization, most focus on monocular depth or two-view setups, limiting their ability to reconstruct full geometry in the absence of known intrinsics [44]. VGGSfM [43] introduces differentiable bundle adjustment by integrating neural feature matching with classic optimization, but remains iterative and computationally heavy, impeding scalability. In the multi-view stereo domain, approaches such as MVSNeRF [3, 45] and MVSNet [8] integrate neural networks into the MVS pipeline but typically require known camera poses and still heavily rely on hand-crafted components to effectively incorporate 3D geometry. Pointmap-based Representations. Pointmap-based representations [10, 11, 46, 47, 48, 49, 50] have recently emerged as unifying format for dense 3D geometry prediction, aligning well with the output structure of neural networks. Compared to voxels [51], meshes [52], or implicit fields [53, 31], pointmaps enable feedforward inference and real-time rendering, and can directly support applications such as rasterization-based rendering [34], SLAM [54, 55], and few-shot synthesis [56]. DUSt3R [10] and follow-ups like MASt3R [11] recast stereo 3D reconstruction as dense pointmap regression, jointly estimating depth, pose, and intrinsics from image pairs. However, their pairwise design fundamentally limits scalability requiring quadratic fusion operations and complex global alignment procedures when handling multi-view scenarios. Our approach maintains the advantages of pointmap representations while overcoming these scalability limitations. 4D Reconstruction from Monocular Videos. Reconstructing dense geometry of dynamic scenes from monocular video is significant but challenging for conventional methods. Recent methods [57, 58, 41, 59] leverages depth priors to resolve this challenge. Specifically, Robust-CVD [59] and MegaSAM [41] requires time-consuming per-video optimization. MonST3R [49] builds on DUSt3R to output pointmaps for dynamic scenes by fine-tuning DUSt3R on the dynamic datasets. However, it still requires sliding-window based per-video global alignment as post-processing. In contrast, our method enables feedforward 4D reconstruction directly from monocular videos, supporting online prediction without costly per-video optimization or post-processing alignment. Reconstruction Methods from Streaming Inputs. Streaming approaches offer more scalable alternative solution for the 3D reconstruction problem, represented by the monocular SLAM pipelines [14, 55, 60]. Inspired by the existing learning-based online 3D reconstruction methods [61, 62, 63], recently Spann3R [15] introduces memory-based extension to DUSt3R, while Fast3R [12] and VGG-T [13] replace asymmetric decoders with Transformer-based attention stacks to directly enable multi-view fusion. Despite these advances, these approaches still predominantly rely on global full-attention mechanisms, limiting their real-time scalability with increasing sequence length. CUT3R [17] adopts an RNN-style architecture to process unstructured inputs incrementally, but suffers from limited memory capacity and poor compatibility with modern hardware acceleration techniques [19]. Our method fundamentally reconceptualizes pointmap prediction as decoder-only Transformer task, enabling efficient causal inference through techniques like KVCache and windowed attention [26, 24]. This architectural design allows us to scale effectively to long sequences while maintaining full compatibility with modern LLM-style training infrastructure and optimization techniques, overcoming the limitations of previous approaches."
        },
        {
            "title": "3 Preliminaries: DUSt3R",
            "content": "We reformulate DUSt3R [10] to accept stream of images as input. In DUSt3R, each incoming image It is initially patchified into set of tokens, Ft = Encoder(It), where Ft RKC and Encoder is weight-sharing ViT [64]. Specifically, DUSt3R is designed to ingest two input images at time, i.e., {1, 2}. The encoded images yield two sets of tokens: F1 = Encoder(I1), F2 = Encoder(I2). (1) 3 Figure 2: Method Overview. Built on causal transformer, STREAM3R processes streaming images sequentially for 3D reconstruction. Each input image is first tokenized using shared-weight ViT encoder, and the resulting tokens are passed to our causal decoder. Each decoder layer begins with frame-wise self-attention. For subsequent views, the model applies causal attention to the memory tokens cached from previous observations. The outputs include point maps and confidence maps in both world and camera coordinate systems, as long as the camera pose as shown on the right. Note that we visualize the point cloud of the Headlocal with its depth map. Afterwards, the decoder networks Decodert reason over both of them through series of transformer blocks with cross attention layer: 1 = DecoderBlocki (2) 1 := F1 2 := F2. Finally, the corresponding regression head of each branch predicts pointmap with with ranging from 1 to B, representing the block index in decoder of blocks in total. G0 and G0 an associated confidence map: 2 = DecoderBlocki 1(Gi1 1 2(Gi1 2 , Gi1 , Gi1 2 ), Gi Gi ), ˆX1,1, ˆC1,1 = Head1(G0 1, . . . , GB 1 ), ˆX2,1, ˆC2,1 = Head2(G0 2, . . . , GB 2 ). (3) Note that DUSt3R is designed for two-view inputs and requires an expensive and unscalable global alignment process to incorporate more input views."
        },
        {
            "title": "4 Method",
            "content": "We introduce STREAM3R, transformer that ingests uncalibrated streaming images as inputs and yields series of 3D attributes as output. The input can be either unstructured image collections or video. Unlike existing approaches [13, 12] that address this issue by adopting costly bi-directional attention over the entire input sequence or using fixed-size memory buffers [15, 17], STREAM3R instead caches the features from the past frames as context and sequentially processes the incoming frame by performing causal attention over the accumulated observations. This design not only enables faster training and quicker convergence but also aligns with the architectural principles of modern LLMs, allowing us to leverage the advancement of that domain. We first introduce the problem formulation in Sec. 4.1, the architecture in Sec. 4.2, and the training objectives in Sec. 4.3, and the implementation details in Sec. 5. An overview of the proposed method is shown in Fig. 2."
        },
        {
            "title": "4.1 Problem Definition and Notation",
            "content": "STREAM3R is regression model that sequentially takes streaming of RGB images (I)N , where each image R3HW belongs to the same 3D scene. The streaming inputs are successively transformed into set of 3D annotations corresponding to each frame: fθ((I)N ) = ( ˆX local , ˆX global , ˆPt)N . (4) 4 Technically, STREAM3R is implemented as causal transformer that maps each image It into its corresponding pointmap of the local coordinate ˆX local R3HW and its pointmap in global coordinate ˆX global R3HW , which is indicated by the first input frame I0, and its relative camera pose ˆPt R9 including both intrinsics and extrinsics. We devise later how these 3D attributes are predicted. t"
        },
        {
            "title": "4.2 Causal Transformer for 3D Regression",
            "content": "Causal Attention for Long-context 3D Reasoning. As mentioned in Sec. 3, given the streaming inputs, for each current image, It, our method first tokenizes it into the features Ft = Encoder(It). The main difference lies in the decoder side: rather than performing bi-directional attention over the whole sequence [12] or interacting with learnable state as in RNN [17], we draw inspiration from the LLMs [20, 24, 65] and perform causal attention efficiently with previous observations. Specifically, after performing frame-wise self-attention in each decoder block, the current feature Gi1 will cross-attend to the features of previously observed frames corresponding to the same layer: , Gi1 = DecoderBlocki (cid:0)Gi1 Gi 1 Gi1 t1 0 Gi1 (cid:1) . (5) This interaction ensures efficient information transfer to handle long-context dependencies. Note that this operation is easy to implement and well optimized with KV cache during inference for efficient computation [24, 20]. Simplified Decoder Design. To achieve this, several network architecture modifications are required. In DUSt3R, the decoder follows symmetric design, i.e., two separate decoders Decoder1, Decoder2 are employed to handle two input views. To extend to an arbitrary number of inputs, we remove the symmetric design and only retain single decoder Decoder to process all the input frames. Specifically, each block in the decoder contains SelfAttn block for frame-wise attention, and CrossAttn block for causally attending to the features of all previous observations. Note that we process the first two frames following the convention of DUSt3R due to the lack of historical context. All incoming frames afterwards follow the causal operation in Eq. (5). Note that to indicate the canonical world space, we add learnable register token [reg] to the tokens of the first frame F1 = F1 + [reg], in an element-wise manner, as shown in Fig. 2. In this way, the model learns to output the global points without introducing separate decoders. Unlike existing work [12], we did not impose positional embedding for other frames for simplicity. with their corresponding confidence maps ˆClocal Prediction Heads. After the decoding operation, the 3D attributes corresponding to each frame can be predicted accordingly. Following existing works [12, 17, 13], we predict two sets of point maps , ˆX global ˆX local . Specifically, the local point is defined in the coordinate frame of the viewing camera, and the global point map ˆX global map ˆX local is in the coordinate frame of the first image I1. We use two DPT [66] heads for point map prediction: = Headlocal(G0 = Headglobal(G0 ˆPt = Headpose(G0 where this redundant prediction has been demonstrated to simplify training [67, 12] and facilitates training on 3D datasets with partial annotations, e.g., single-view depth datasets [68]. , . . . , GB ), , . . . , GB ), , . . . , GB ), ˆX local ˆX global , ˆClocal , ˆCglobal , ˆCglobal (8) (6) (7) t"
        },
        {
            "title": "4.3 Training Objective",
            "content": "STREAM3R is trained using generalized form of the pointmap loss introduced in DUSt3R. Given sequence of randomly sampled images, sourced either from video or an image collection, we train the model to produce pointmap predictions denoted by = { ˆX local, ˆX global}, where ˆX local = { ˆX local }N t=1. The corresponding confidence scores are denoted as ˆC. t=1 and ˆX global = { ˆX global }N Following DUSt3R [10], we apply confidence-aware regression loss to the pointmaps: Lconf = (cid:88) ( ˆx,ˆc)( ˆX , ˆC) (cid:18) ˆc (cid:13) (cid:13) (cid:13) (cid:13) ˆx ˆs (cid:13) (cid:13) (cid:13) (cid:13)2 (cid:19) α log ˆc , (9) 5 Figure 3: Qualitative results on in-the-wild Images. We compare our method with approaches MonST3R, Fast3R, and CUT3R, and demonstrate that it achieves superior visual quality. where ˆs and are scale normalization factors for ˆX and for scale-invariant supervision [69]. We also set ˆs := for metric-scale datasets as in MASt3R [11] to enable metric-scale pointmaps prediction. For the camera prediction loss, we parameterize pose ˆPt as quaternion ˆqt, translation ˆτt and focal ˆf t, and minimize the L2 norm between the prediction and ground truth: Lpose = (cid:88) (cid:18) t=1 ˆqt qt2 + (cid:13) (cid:13) (cid:13) (cid:13) ˆτt ˆs τt (cid:13) (cid:13) (cid:13) (cid:13)2 + (cid:13) ˆft ft (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)2 (cid:19) . (10)"
        },
        {
            "title": "5 Experiments",
            "content": "Datasets. We train our method on large and diverse collection of 3D datasets, e.g., Co3Dv2 [37], ScanNet++ [70], ScanNet [71], HyperSim [72], Dynamic Replica [73], DL3DV [36], BlendedMVS [74], Aria Synthetic Environments [75], TartanAir [76], MapFree [77], MegaDepth [78], and ARKitScenes [79]. Please check the supplement for the full dataset details. Implementation Details. We provide two version of STREAM3R, where STREAM3Rα is inspired and fine-tuned from DUSt3R [10] pre-trained weights, and STREAM3Rβ is initialized from the flagship VGG-T [13] model. For STREAM3Rα, we inherit the 24-layer CroCo ViT [80, 81] as our encoder, and retrofit its 12-layer decoder network by only retaining the first decoder Decoder = Decoder1. The DPT-L [66] heads are used to map the decoded tokens to the local and global point maps accordingly. For STREAM3Rβ, we replace the SelfAttn layer in the Global Attention of VGG-T with CausalAttn and fine-tune all the parameters. For memory-efficient and stable training, we inject QK-Norm [82] to each transformer layer and leverage FlashAttention [19] for BFloat16 mixed precision training. Training Details. Our model is trained with the AdamW optimizer on batch size of 64 with learning rate 1e-4 for 400K iterations. For each batch, we randomly sample 4 10 frames from random training scene. The input frames are cropped into diverse resolutions, ranging from 224 224 to 512 384 to improve generality. The training runs end-to-end on 8 NVIDIA A100 GPUs over seven days. Gradient checkpointing is also adopted to optimize memory usage. Baselines. We compare our methods against set of baselines that are designed to take pair of views as input: DUSt3R [10], MASt3R [11], and MonST3R [49]. Besides, we include the comparison against concurrent methods Spann3R [15], CUT3R [17], SLAM3R [55], and Fast3R [12] that are 6 Table 1: Single-frame Depth Evaluation. We report the performance on Sintel, Bonn, KITTI, and NYU-v2 (static) datasets. The best and second best results in each category are bold and underlined respectively. Our method achieves better or comparable performance against existing methods. Method VGG-T Fast3R DUSt3R MASt3R MonST3R 0.271 0.502 0.424 0.340 0.358 Spann3R CUT3R 0.470 0.428 STREAM3Rα 0.350 STREAM3Rβ 0.228 Sintel Bonn KITTI NYU-v2 Abs Rel δ<1.25 Abs Rel δ<1.25 Abs Rel δ<1.25 Abs Rel δ<1.25 67.7 52.8 58.7 60.4 54.8 53.9 55.4 59.0 70.7 0.053 0.192 0.141 0.142 0. 0.118 0.063 0.075 0.061 97.3 77.3 82.5 82.0 93.9 85.9 96.2 93.4 96.7 0.076 0.129 0.112 0.079 0.100 0.128 0.092 0.088 0.063 93.3 81.2 86.3 94.7 89. 84.6 91.3 91.3 95.5 0.060 0.099 0.080 0.129 0.102 0.122 0.086 0.091 0.057 94.8 88.9 90.7 84.9 88.0 84.9 90.9 89.9 95.7 specifically designed for handling varying number of input images. We also include the flagship 3D geometry model VGG-T [13] for reference. Note that Fast3R and VGG-T are bi-directional attention methods, and we group them together with methods that require global optimization (GA). We group other concurrent methods together as streaming methods that supports processing sequential inputs. Note that for all methods except for VGG-T and STREAM3Rβ, we conduct inference with the largest dimension of 512. For VGG-T based methods, we conduct inference with the largest dimension of 518 due to the requirement of DINO-V2 tokenizer [83]. Regarding FPS, we benchmark the inference speed on the A100 GPU with FP32. Comparisons of more concurrent methods [84, 38] are included in the supplement."
        },
        {
            "title": "5.1 Monocular and Video Depth Estimation",
            "content": "Mono-Depth Estimation. Following previous methods [49, 17], we first evaluate monocular depth estimation on Sintel [85], Bonn [86], KITTI [4], and NYU-v2 [87] datasets, which cover dynamic and static, indoor and outdoor, realistic and synthetic data. These datasets are not used for training and are suitable for benchmarking the zero-shot performance across different domains. Our evaluation includes the absolute relative error (Abs Rel) and percentage of inlier points within 1.25-factor of true depth δ < 1.25, following the convention of existing methods [40, 88]. Per-frame median scaling is imposed as in DUSt3R. We include the quantitative results in Tab. 1. As can be seen, our method achieves state-of-the-art compared to streaming-based methods, and even performs best compared to VGG-T on Sintel, KITTI, and NYU-2. Also note that our method uses fewer datasets and compute resources compared to CUT3R. Specifically, CUT3R adopts curriculum training of four stages for 100 + 35 + 40 + 10 = 185 epochs, while our method is trained end-to-end for only 7 epochs using partial of CUT3Rs datasets due to the computational resources constraints. Video Depth Estimation. We also benchmark our model on the video depth task, which evaluates both per-frame depth quality and inter-frame depth consistency by aligning the output depth maps to the ground truth depth maps using given per-sequence scale. Metric point map methods like MASt3R, CUT3R, and ours are also reported without alignment. The quantitative results for both methods are included in Tab. 5. Over per-sequence scale alignment, our method surpasses optimization-based baselines DUSt3R-GA [10] and MASt3R-GA [11] (static-scene assumption) and even MonST3R-GA [49] (dynamic-scene, optical flow [89] dependent). Against the streaming stateof-the-art CUT3R, we achieve higher accuracy on all three benchmarks while running 40% faster. STREAM3R also outperforms full-attention Fast3R [12], streaming approaches Spann3R [15], and the flagship model VGG-T on Sintel. Notably, STREAM3Rβ-W, using sliding-window attention [26] for constant cache, exceeds STREAM3Rβ on Bonn and KITTI despite accessing only five past frames. 5.2 3D Reconstruction We also benchmark scene-level 3D reconstruction on the 7-scenes [90] dataset and use accuracy (Acc), completion (Comp), and normal consistency (NC) metrics, following the convention of existing methods [15, 17, 10]. Following CUT3R, we assess the models performance on image collections with minimal or no overlap by evaluating using sparsely sampled images, i.e., 3 to 5 frames per scene. The quantitative results are included in Tab. 3. Our method achieves better performance compared to Table 2: Video Depth Evaluation. We evaluate scale-invariant and metric depth accuracy on the Sintel, Bonn, and KITTI datasets. Methods that require global alignment are denoted as \"GA\". The \"Type\" column indicates whether the method is Optimzation-based (\"Optim), streaming (\"Stream\"), or full-attention (\"FA) We also report inference speed in FPS on the KITTI dataset using 512144 resolution for all methods on an A100 GPU, except for Spann3R, which supports Stream 224224 inputs. Our method achieves performance that is better than CUT3R, while offering FAter inference. For STREAM3Rβ-W[5], we indicate using sliding window attention on STREAM3Rβ with window size 5. Note that STREAM3Rβ-W[5] achieves the fastest FPS among all streaming-based methods. Alignment Method Type Abs Rel δ<1.25 Abs Rel δ<1.25 Abs Rel δ <1.25 FPS Sintel Bonn KITTI FA FA VGG-T [13] Fast3R [12] 0.297 0.653 DUSt3R-GA [10] Optim 0.656 MASt3R-GA [11] Optim 0.641 MonST3R-GA [49] Optim 0.378 Spann3R [15] CUT3R [17] STREAM3Rα STREAM3Rβ Stream 0.622 Stream 0.421 Stream 0.478 Stream 0.264 STREAM3Rβ-W[5] Stream 0.279 MASt3R-GA [11] Optim 1.022 Stream 1.029 Stream 1.041 CUT3R STREAM3Rα 68.8 44.9 45.2 43.9 55.8 42.6 47.9 51.1 70.5 68.6 14.3 23.8 21. 0.055 0.193 0.155 0.252 0.067 0.144 0.078 0.075 0.069 0.064 0.272 0.103 0.084 97.1 77.5 83.3 70.1 96.3 81.3 93.7 94.1 95.2 96.7 70.6 88.5 94. 0.073 0.140 0.144 0.183 0.168 0.198 0.118 0.116 0.080 0.083 0.467 0.122 0.234 96.5 83.4 81.3 74.5 74.4 73.7 88.1 89.6 94.7 95.2 15.2 85.5 57. 7.32 47.23 0.76 0.31 0.35 13.55 16.58 23.48 12.95 32.93 0.31 16.58 23.48 Per-sequence scale Metric scale Table 3: 3D Reconstruction Evaluation on 7-Scenes [90]. Despite operating in the streaming setting, our method delivers competitive performance, matching or even exceeding that of offline optimization-based methods that leverage global alignment. STREAM3Rβ-FA indicates adopting full attention in our trained model for 3D reconstruction. Acc Comp NC Method Type FA VGG-T [13] FA Fast3R [12] Optim DUSt3R-GA [10] MASt3R-GA [11] Optim MonST3R-GA [49] Optim STREAM3Rβ-FA Stream Spann3R [15] SLAM3R [55] CUT3R [17] STREAM3Rα STREAM3Rβ Stream Stream Stream Stream Stream Mean 0.087 0.164 0.146 0.185 0.248 0.091 0.298 0.287 0.126 0.148 0.122 Med. 0.039 0.108 0.077 0.081 0.185 0.043 0.226 0.155 0.047 0.077 0.044 Mean 0.091 0.163 0.181 0.180 0.266 0.075 0.205 0.226 0.154 0.177 0.110 Med. 0.039 0.080 0.067 0.069 0.167 0.042 0.112 0.066 0.031 0.058 0.038 Mean 0.787 0.686 0.736 0.701 0.672 0.769 0.650 0.644 0.727 0.700 0.746 Med. 0.890 0.775 0.839 0.792 0.759 0.879 0.730 0.720 0.834 0.801 0.856 FPS 12.0 30.92 0.68 0.34 0.39 12.0 12.97 38.40 17.00 26.4 20.12 optimization-based methods and strong baselines including Spann3R, Fast3R, CUT3R, and SLAM3R. Compared to CUT3R, our method shows better performance with over 50% times faster during the inference. While SLAM3R achieves the fastest inference, it yields noticeably lower reconstruction accuracy than our method. This performance gap can be partially attributed to SLAM3R being trained and evaluated at lower input resolution of 224 224. We also include STREAM3Rβ-FA for comparison, which indicates replacing the causal attention in STREAM3Rβ into full attention (FA). Interestingly, STREAM3Rβ-FA yields comparable performance compared to VGG-T and even better results on the completion metric. This highlights the effectiveness and generality of our proposed method. The comparison results on NRGBD [91] benchmark is included in the supplement."
        },
        {
            "title": "5.3 Camera Pose Estimation",
            "content": "Following CUT3R [17], we evaluate camera pose estimation accuracy on the Sintel [85], TUMdynamics [92], and ScanNet [71] datasets. Sintel and TUM-dynamics both feature substantial dynamic motion, posing significant challenges to conventional SfM and SLAM pipelines. We report Absolute Translation Error (ATE), Relative Translation Error (RPEtrans), and Relative Rotation Error 8 Table 4: Camera Pose Evaluation on Sintel [85], TUM-dynamic [92], and ScanNet [71] datasets. Our method achieves comparable performance with CUT3R on most benchmarks. Sintel TUM-dynamics ScanNet Method Type ATE RPE trans RPE rot ATE RPE trans RPE rot ATE RPE trans RPE rot Particle-SfM [93] Optim 0.129 Robust-CVD [59] Optim 0.360 CasualSAM [94] Optim 0.141 DUSt3R-GA [10] Optim 0.417 MASt3R-GA [11] Optim 0.185 MonST3R-GA [49] Optim 0.111 DUSt3R [10] Spann3R [15] CUT3R [17] STREAM3Rβ Onl Onl Onl Onl 0.290 0.329 0.213 0. 0.031 0.154 0.035 0.250 0.060 0.044 0.132 0.110 0.066 0.076 0.535 3.443 0.615 5.796 1.496 0.869 7.869 4.471 0.621 0.868 - 0.153 0.071 0.083 0.038 0.098 0.140 0.056 0.046 0. - 0.026 0.010 0.017 0.012 0.019 0.106 0.021 0.015 0.013 - 3.528 1.712 3.567 0.448 0.935 3.286 0.591 0.473 0.330 0.136 0.227 0.158 0.081 0.078 0.077 0.246 0.096 0.099 0. 0.023 0.064 0.034 0.028 0.020 0.018 0.108 0.023 0.022 0.021 0.836 7.374 1.618 0.784 0.475 0.529 8.210 0.661 0.600 0.850 (RPErot) after Sim(3) alignment with the ground truth, following the protocol in [30, 49, 17]. Our approach operates without requiring camera calibration, similar to the compared baselines [30]. While many prior methods [59, 94] address this via test-time optimizationjointly estimating intrinsics and dense depth for each sequencewe focus on purely online processing. Table 4 reports results for Online (Onl) and Optimization (Optim) categories, with DUSt3R [10] included in the latter (aligning all frames to the first frame without global alignment). Although optimization-based systems still achieve the lowest errors overall, our method establishes the strongest performance among streaming approaches, and notably surpasses CUT3R [17] on both TUM-dynamics and ScanNet, demonstrating particular robustness in dynamic environments."
        },
        {
            "title": "5.4 Ablation on the Effectiveness of the Proposed Architecture",
            "content": "Here, we conduct detailed ablation analysis on STREAM3R to demonstrate the effectiveness of its designs. Due to the extensive computational resources required to train the model, we only train the ablation models on 224 224 resolution images. All the datasets are included to train the models. Note that for fair comparison, we initialize all the models below using the pre-trained MASt3R [11] checkpoints and train the models using the same hyper-parameters and compute resources. (a) Overall Training Curve (b) Training Curve of Local Branch (c) Training Curve of Global Branch Figure 4: Ablation of our proposed STREAM3R. Compared to RNN-based architecture [17], our decoder-only network yields better convergence with faster training speed in the 3D point map prediction task, especially in the global branch. We demonstrate the effectiveness of decoder-only transformer against RNN design in the sequential 3D pointmap prediction. The main baseline is CUT3R [17], which leverages the RNN design to achieve this. For fair comparison, we re-train CUT3R and our method using the same dataset and pre-trained model weights initialization. We include the training curve in Fig. 4a, where both models are trained with the same hyperparameters and compute resources. As can be observed, STREAM3R converges faster compared to CUT3R and performs 60% more training steps within the given time. This may sound counterintuitive since STREAM3R is attending to longer context against CUT3Rs constant state memory. However, since CUT3R architecture requires state-update operation after each state-readout interaction, while STREAM3R directly attends to cached features of existing observations. Table 5: Ablation on Video Depth Estimation. When evaluating the checkpoint trained for the same number of iterations, our proposed architecture consistently achieves better performance against RNN-based CUT3R on the video depth estimation task. Method Abs Rel δ<1.25 Abs Rel δ<1.25 Abs Rel δ<1.25 Sintel BONN KITTI CUT3R STREAM3Rα 0.598 0.535 40.7 47.0 0.102 0.083 90.7 94.2 0.157 0.141 77.4 81. Table 6: Ablation on 3D Reconstruction on 7-Scenes. Our proposed architecture consistently achieves better performance against RNN-based CUT3R on the 3D reconstruction task when trained under the same configurations. Note that our architecture is trained even faster. Acc Comp NC Method CUT3R STREAM3Rα Mean 0.480 0.328 Med. 0.365 0.261 Mean 0.330 0. Med. 0.148 0.095 Mean 0.555 0.605 Med. 0.583 0. We also notice in Fig. 4b that the convergence of Headlocal is similar among the two architectures, while for Headglobal, our proposed architecture shows noticeably faster convergence speed, as shown in Fig. 4c. This demonstrates that using single state makes the model harder to register incoming frames due to the limited memory capacity. Quantitatively, we benchmark the ablation models on both the video depth estimation in Tab. 5 and 3D reconstruction in Tab. 6, which evaluates the Headlocal and Headglobal correspondingly. For fair comparison, we evaluate the checkpoints trained for the same number of iterations. As can be observed, our proposed architecture consistently achieves better performance on both tasks."
        },
        {
            "title": "6 Conclusion and Discussions",
            "content": "We have introduced STREAM3R, decoder-only transformer framework for dense 3D reconstruction from unstructured or streaming image inputs. By reformulating reconstruction as sequential registration task with causal attention, STREAM3R overcomes the scalability bottlenecks of prior work and aligns naturally with LLM-style training and inference pipelines. Our design allows efficient integration of geometric context across frames, supports dual-coordinate pointmap prediction, and generalizes to novel-view synthesis over large-scale scenes without the need for global postprocessing. Through extensive experiments across standard benchmarks, we show that STREAM3R achieves competitive or superior performance in the monocular/video-depth estimation and 3D reconstruction tasks, with significantly improved inference efficiency. By bridging geometric learning with scalable sequence modeling, we hope this work paves the way toward more general-purpose, real-time 3D understanding systems. Our method comes with some limitations. First, the naïve causal modeling naturally suffers from error accumulation and drifting [95]. Some inference strategies can be proposed to alleviate this issue. Second, currently STREAM3R is still regression model with deterministic outputs. Extending it further into an autoregressive generative model [25, 95] shall further unlock series of downstream applications. Finally, since STREAM3R follows similar design of modern LLMs, more training techniques like MLA [65] can be introduced to further boost the training efficiency and performance."
        },
        {
            "title": "References",
            "content": "[1] Johannes Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In CVPR, pages 41044113, 2016. [2] Johannes Lutz Schönberger, Enliang Zheng, Marc Pollefeys, and Jan-Michael Frahm. Pixelwise view selection for unstructured multi-view stereo. In ECCV, 2016. [3] Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, and Hao Su. Mvsnerf: Fast generalizable radiance field reconstruction from multi-view stereo. In ICCV, pages 1412414133, 2021. [4] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. IJRR, 2013. [5] Yufeng Zheng, Wang Yifan, Gordon Wetzstein, Michael J. Black, and Otmar Hilliges. Pointavatar: Deformable point-based head avatars from videos. In CVPR, 2023. [6] Yushi Lan, Feitong Tan, Di Qiu, Qiangeng Xu, Kyle Genova, Zeng Huang, Sean Fanello, Rohit Pandey, Thomas Funkhouser, Chen Change Loy, and Yinda Zhang. Gaussian3diff: 3d gaussian diffusion for 3d full head synthesis and editing. In ECCV, 2024. [7] Muhammad Zubair Irshad, Mauro Comi, Yen-Chen Lin, Nick Heppert, Abhinav Valada, Rares Ambrus, Zsolt Kira, and Jonathan Tremblay. Neural fields in robotics: survey, 2024. [8] Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan. Mvsnet: Depth inference for unstructured multi-view stereo. ECCV, 2018. [9] Yao Yao, Zixin Luo, Shiwei Li, Tianwei Shen, Tian Fang, and Long Quan. Recurrent mvsnet for high-resolution multi-view stereo depth inference. CVPR, 2019. [10] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In CVPR, pages 2069720709, 2024. [11] Vincent Leroy, Yohann Cabon, and Jerome Revaud. Grounding image matching in 3d with mast3r, 2024. [12] Jianing Yang, Alexander Sax, Kevin J. Liang, Mikael Henaff, Hao Tang, Ang Cao, Joyce Chai, Franziska Meier, and Matt Feiszli. Fast3r: Towards 3d reconstruction of 1000+ images in one forward pass. In CVPR, June 2025. [13] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In CVPR, 2025. [14] Andrew Davison, Ian Reid, Nicholas Molton, and Olivier Stasse. Monoslam: Real-time single camera slam. TPAMI, 29(6):10521067, 2007. [15] Hengyi Wang and Lourdes Agapito. 3d reconstruction with spatial memory. arXiv preprint arXiv:2408.16061, 2024. [16] Ho Kei Cheng and Alexander G. Schwing. XMem: Long-term video object segmentation with an atkinson-shiffrin memory model. In ECCV, 2022. [17] Qianqian Wang, Yifei Zhang, Aleksander Holynski, Alexei Efros, and Angjoo Kanazawa. Continuous 3d perception model with persistent state. arXiv preprint arXiv:2501.12387, 2025. [18] Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization, 2015. [19] Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In ICLR, 2024. [20] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023. [21] Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, and Alexandre Défossez. Simple and controllable music generation. Advances in Neural Information Processing Systems, 36, 2023. [22] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding, 2019. 11 [23] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. [24] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. [25] Boyuan Chen, Diego Martí Monsó, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. NeurIPS, 37:2408124125, 2025. [26] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b, 2023. [27] Richard Hartley and Andrew Zisserman. Multiple view geometry in computer vision. Cambridge university press, 2003. [28] Chengzhou Tang and Ping Tan. BA-Net: Dense bundle adjustment network. arXiv preprint arXiv:1806.04807, 2018. [29] Raul Mur-Artal, Jose Maria Martinez Montiel, and Juan Tardos. ORB-SLAM: versatile and accurate monocular SLAM system. IEEE Transactions on Robotics, 31(5):11471163, 2015. [30] Zachary Teed and Jia Deng. DROID-SLAM: Deep visual SLAM for monocular, stereo, and RGB-D cameras. NeurIPS, pages 1655816569, 2021. [31] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020. [32] Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen Koltun. Nerf++: Analyzing and improving neural radiance fields. arXiv preprint arXiv:2010.07492, 2020. [33] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. NeuS: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. arXiv preprint arXiv:2106.10689, 2021. [34] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3D gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42(4):114, 2023. [35] Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, and Shenghua Gao. 2d gaussian splatting for geometrically accurate radiance fields. In SIGGRAPH 2024 Conference Papers. Association for Computing Machinery, 2024. [36] Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, et al. Dl3dv-10k: large-scale scene dataset for deep learning-based 3d vision. In CVPR, pages 2216022169, 2024. [37] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny. Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction. In ICCV, 2021. [38] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In CVPR, 2024. [39] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth estimation. In CVPR, pages 94929502, 2024. [40] Wenbo Hu, Xiangjun Gao, Xiaoyu Li, Sijie Zhao, Xiaodong Cun, Yong Zhang, Long Quan, and Ying Shan. Depthcrafter: Generating consistent long depth sequences for open-world videos. In CVPR, 2025. [41] Zhengqi Li, Richard Tucker, Forrester Cole, Qianqian Wang, Linyi Jin, Vickie Ye, Angjoo Kanazawa, Aleksander Holynski, and Noah Snavely. MegaSaM: Accurate, fast and robust structure and motion from casual dynamic videos. arXiv preprint, 2024. 12 [42] Qianqian Wang, Vickie Ye, Hang Gao, Weijia Zeng, Jake Austin, Zhengqi Li, and Angjoo Kanazawa. Shape of motion: 4d reconstruction from single video. 2024. [43] Jianyuan Wang, Nikita Karaev, Christian Rupprecht, and David Novotny. Vggsfm: Visual geometry grounded deep structure from motion. In CVPR, pages 2168621697, 2024. [44] Wei Yin, Chi Zhang, Hao Chen, Zhipeng Cai, Gang Yu, Kaixuan Wang, Xiaozhi Chen, and Chunhua Shen. Metric3d: Towards zero-shot metric 3d prediction from single image. In CVPR, pages 90439053, 2023. [45] Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, and Jianfei Cai. Mvsplat: Efficient 3d gaussian splatting from sparse multi-view images. arXiv preprint arXiv:2403.14627, 2024. [46] David Charatan, Sizhe Li, Andrea Tagliasacchi, and Vincent Sitzmann. pixelsplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction. In CVPR, 2024. [47] Jiale Xu, Shenghua Gao, and Ying Shan. Freesplatter: Pose-free gaussian splatting for sparse-view 3d reconstruction. arXiv preprint arXiv:2412.09573, 2024. [48] Stanislaw Szymanowicz, Christian Rupprecht, and Andrea Vedaldi. Splatter image: Ultra-fast single-view 3D reconstruction. In arXiv, 2023. [49] Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, and Ming-Hsuan Yang. Monst3r: simple approach for estimating geometry in the presence of motion. arXiv preprint arxiv:2410.03825, 2024. [50] Kai Zhang, Sai Bi, Hao Tan, Yuanbo Xiangli, Nanxuan Zhao, Kalyan Sunkavalli, and Zexiang Xu. Gs-lrm: Large reconstruction model for 3d gaussian splatting. ECCV, 2024. [51] Vincent Sitzmann, Justus Thies, Felix Heide, Matthias NieBner, Gordon Wetzstein, and Michael Zollhofer. DeepVoxels: Learning persistent 3D feature embeddings. In CVPR, pages 24322441. IEEE. [52] Georgia Gkioxari, Jitendra Malik, and Justin Johnson. Mesh R-CNN. In ICCV, pages 97859795, 2019. [53] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. In CVPR, pages 165174, 2019. [54] Riku Murai, Eric Dexheimer, and Andrew J. Davison. MASt3R-SLAM: Real-time dense SLAM with 3D reconstruction priors. arXiv preprint, 2024. [55] Yuzheng Liu, Siyan Dong, Shuzhe Wang, Yingda Yin, Yanchao Yang, Qingnan Fan, and Baoquan Chen. Slam3r: Real-time dense scene reconstruction from monocular rgb videos. arXiv preprint arXiv:2412.09401, 2024. [56] Botao Ye, Sifei Liu, Haofei Xu, Li Xueting, Marc Pollefeys, Ming-Hsuan Yang, and Peng Songyou. No pose, no problem: Surprisingly simple 3d gaussian splats from sparse unposed images. In ICLR, 2025. [57] Jiahui Lei, Yijia Weng, Adam Harley, Leonidas Guibas, and Kostas Daniilidis. Mosca: Dynamic gaussian fusion from casual videos via 4d motion scaffolds. arXiv preprint arXiv:2405.17421, 2024. [58] Wen-Hsuan Chu, Lei Ke, and Katerina Fragkiadaki. Dreamscene4d: Dynamic multi-object scene generation from monocular videos. NeurIPS, 2024. [59] Johannes Kopf, Xuejian Rong, and Jia-Bin Huang. Robust consistent video depth estimation. In CVPR, 2021. [60] Zihan Zhu, Songyou Peng, Viktor Larsson, Zhaopeng Cui, Martin Oswald, Andreas Geiger, and Marc Pollefeys. Nicer-slam: Neural implicit scene encoding for rgb slam. In 3DV, March 2024. [61] Christopher B. Choy, Danfei Xu, JunYoung Gwak, Kevin Chen, and Silvio Savarese. 3D-r2n2: unified approach for single and multi-view 3D object reconstruction. In ECCV, volume 9912 of Lecture Notes in Computer Science, pages 628644. Springer, 2016. [62] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. PixelNeRF: Neural radiance fields from one or few images. In CVPR, 2021. [63] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P. Srinivasan, Howard Zhou, Jonathan T. Barron, Ricardo Martin-Brualla, Noah Snavely, and Thomas A. Funkhouser. IBRNet: Learning Multi-View Image-Based Rendering. In CVPR, 2021. 13 [64] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. [65] DeepSeek-AI, Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Hanwei Xu, Hao Yang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jin Chen, Jingyang Yuan, Junjie Qiu, Junxiao Song, Kai Dong, Kaige Gao, Kang Guan, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruizhe Pan, Runxin Xu, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Size Zheng, T. Wang, Tian Pei, Tian Yuan, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Liu, Xin Xie, Xingkai Yu, Xinnan Song, Xinyi Zhou, Xinyu Yang, Xuan Lu, Xuecheng Su, Y. Wu, Y. K. Li, Y. X. Wei, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Zheng, Yichao Zhang, Yiliang Xiong, Yilong Zhao, Ying He, Ying Tang, Yishi Piao, Yixin Dong, Yixuan Tan, Yiyuan Liu, Yongji Wang, Yongqiang Guo, Yuchen Zhu, Yuduan Wang, Yuheng Zou, Yukun Zha, Yunxian Ma, Yuting Yan, Yuxiang You, Yuxuan Liu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhewen Hao, Zhihong Shao, Zhiniu Wen, Zhipeng Xu, Zhongyu Zhang, Zhuoshu Li, Zihan Wang, Zihui Gu, Zilin Li, and Ziwei Xie. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model, 2024. [66] René Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. ArXiv preprint, 2021. [67] Zeren Jiang, Chuanxia Zheng, Iro Laina, Diane Larlus, and Andrea Vedaldi. Geo4d: Leveraging video generators for geometric 4d scene reconstruction, 2025. [68] Yunze Liu, Yun Liu, Che Jiang, Kangbo Lyu, Weikang Wan, Hao Shen, Boqiang Liang, Zhoujie Fu, He Wang, and Li Yi. Hoi4d: 4d egocentric dataset for category-level human-object interaction. In CVPR, pages 2101321022, June 2022. [69] Ruicheng Wang, Sicheng Xu, Cassie Dai, Jianfeng Xiang, Yu Deng, Xin Tong, and Jiaolong Yang. Moge: Unlocking accurate monocular geometry estimation for open-domain images with optimal training supervision, 2024. [70] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner, and Angela Dai. Scannet++: high-fidelity dataset of 3d indoor scenes. In ICCV, pages 1222, 2023. [71] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In CVPR, 2017. [72] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb, and Joshua M. Susskind. Hypersim: photorealistic synthetic dataset for holistic indoor scene understanding. In ICCV, 2021. [73] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Dynamicstereo: Consistent dynamic depth from stereo videos. CVPR, 2023. [74] Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren, Lei Zhou, Tian Fang, and Long Quan. Blendedmvs: large-scale dataset for generalized multi-view stereo networks. CVPR, 2020. [75] Xiaqing Pan, Nicholas Charron, Yongqian Yang, Scott Peters, Thomas Whelan, Chen Kong, Omkar Parkhi, Richard Newcombe, and Yuheng (Carl) Ren. Aria digital twin: new benchmark dataset for egocentric 3d machine perception. In ICCV, pages 2013320143, October 2023. [76] Wenshan Wang, Delong Zhu, Xiangwei Wang, Yaoyu Hu, Yuheng Qiu, Chen Wang, Yafei Hu, Ashish Kapoor, and Sebastian Scherer. Tartanair: dataset to push the limits of visual slam. In IROS, 2020. [77] Eduardo Arnold, Jamie Wynn, Sara Vicente, Guillermo Garcia-Hernando, Áron Monszpart, Victor Adrian Prisacariu, Daniyar Turmukhambetov, and Eric Brachmann. Map-free visual relocalization: Metric pose relative to single image. In ECCV, 2022. [78] Zhengqi Li and Noah Snavely. Megadepth: Learning single-view depth prediction from internet photos. In ICCV, pages 20412050, 2018. [79] Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry, Yuri Feigin, Peter Fu, Thomas Gebauer, Brandon Joffe, Daniel Kurz, Arik Schwartz, and Elad Shulman. Arkitscenes: diverse real-world dataset for 3d indoor scene understanding using mobile rgb-d data, 2022. [80] Weinzaepfel, Philippe and Leroy, Vincent and Lucas, Thomas and Brégier, Romain and Cabon, Yohann and Arora, Vaibhav and Antsfeld, Leonid and Chidlovskii, Boris and Csurka, Gabriela and Revaud Jérôme. CroCo: Self-Supervised Pre-training for 3D Vision Tasks by Cross-View Completion. In NeurIPS, 2022. [81] Philippe Weinzaepfel, Thomas Lucas, Vincent Leroy, Yohann Cabon, Vaibhav Arora, Romain Brégier, Gabriela Csurka, Leonid Antsfeld, Boris Chidlovskii, and Jérôme Revaud. CroCo v2: Improved Crossview Completion Pre-training for Stereo Matching and Optical Flow. In ICCV, 2023. [82] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. In ICML, 2023. [83] Maxime Oquab, Timothée Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision, 2023. [84] Dong Zhuo, Wenzhao Zheng, Jiahe Guo, Yuqi Wu, Jie Zhou, and Jiwen Lu. Streaming 4d visual geometry transformer. arXiv preprint arXiv:2507.11539, 2025. [85] D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black. naturalistic open source movie for optical flow evaluation. In A. Fitzgibbon et al. (Eds.), editor, ECCV, Part IV, LNCS 7577, pages 611625. Springer-Verlag, October 2012. [86] E. Palazzolo, J. Behley, P. Lottes, P. Giguère, and C. Stachniss. ReFusion: 3D Reconstruction in Dynamic Environments for RGB-D Cameras Exploiting Residuals. arXiv, 2019. [87] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support inference from rgbd images. In ECCV, pages 746760. Springer-Verlag Berlin, October 2012. [88] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In CVPR, 2024. [89] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow, 2020. [90] Jamie Shotton, Ben Glocker, Christopher Zach, Shahram Izadi, Antonio Criminisi, and Andrew Fitzgibbon. Scene coordinate regression forests for camera relocalization in rgb-d images. In CVPR, June 2013. [91] Dejan Azinovic, Ricardo Martin-Brualla, Dan Goldman, Matthias Nießner, and Justus Thies. Neural rgb-d surface reconstruction. In CVPR, pages 62906301, June 2022. [92] Jürgen Sturm, Nikolas Engelhard, Felix Endres, Wolfram Burgard, and Daniel Cremers. benchmark for the evaluation of rgb-d slam systems. In IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 573580, 2012. [93] Wang Zhao, Shaohui Liu, Hengkai Guo, Wenping Wang, and Yong-Jin Liu. Particlesfm: Exploiting dense point trajectories for localizing moving cameras in the wild. In ECCV, 2022. [94] Zhoutong Zhang, Forrester Cole, Zhengqi Li, Michael Rubinstein, Noah Snavely, and William Freeman. Structure and motion from casual videos. In ECCV, pages 2037. Springer, 2022. [95] Lvmin Zhang and Maneesh Agrawala. Packing input frame contexts in next-frame prediction models for video generation. Arxiv, 2025. [96] Hongchi Xia, Yang Fu, Sifei Liu, and Xiaolong Wang. Rgbd objects in the wild: Scaling real-world 3d object learning from rgb-d videos, 2024. [97] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, Vijay Vasudevan, Wei Han, Jiquan Ngiam, Hang Zhao, Aleksei Timofeev, Scott Ettinger, Maxim Krivokon, Amy Gao, Aditya Joshi, Yu Zhang, Jonathon Shlens, Zhifeng Chen, and Dragomir Anguelov. Scalability in perception for autonomous driving: Waymo open dataset. In CVPR, June 2020. 15 [98] Michael J. Black, Priyanka Patel, Joachim Tesch, and Jinlong Yang. BEDLAM: synthetic dataset of bodies exhibiting detailed lifelike animated motion. In CVPR, pages 87268737, June 2023. [99] Simon Niklaus, Long Mai, Jimei Yang, and Feng Liu. 3d ken burns effect from single image. ACM Transactions on Graphics, 38(6):184:1184:15, 2019. [100] Qiang Wang, Shizhen Zheng, Qingsong Yan, Fei Deng, Kaiyong Zhao, and Xiaowen Chu. Irs: large naturalistic indoor robotics stereo dataset to train deep models for disparity and surface normal estimation, 2021. [101] Anastasiia Kornilova, Marsel Faizullin, Konstantin Pakulev, Andrey Sadkov, Denis Kukushkin, Azat Akhmetyanov, Timur Akhtyamov, Hekmat Taherinejad, and Gonzalo Ferrer. Smartportraits: Depth powered handheld smartphone dataset of human portraits for state estimation, reconstruction and synthesis, 2022. [102] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything V2. arXiv preprint arXiv:2406.09414, 2024. [103] Yiran Wang, Min Shi, Jiaqi Li, Zihao Huang, Zhiguo Cao, Jianming Zhang, Ke Xian, and Guosheng Lin. Neural video depth stabilizer. In ICCV, pages 94669476, October 2023. [104] Jiahao Shao, Yuanbo Yang, Hongyu Zhou, Youmin Zhang, Yujun Shen, Vitor Guizilini, Yue Wang, Matteo Poggi, and Yiyi Liao. Learning temporally consistent video depth from video diffusion priors, 2024. [105] Ioan Andrei Bârsan, Peidong Liu, Marc Pollefeys, and Andreas Geiger. Robust dense mapping for large-scale dynamic environments. In ICRA, pages 75107517, 2018."
        },
        {
            "title": "A Dataset Details",
            "content": "We train our model on 29 datasets that contains diverse range of scene types, including static and dynamic scene and objects. Specifically, we mainly follow the data splits of CUT3R [17], and the main 15 datasets with highest sampling ratio are: Co3Dv2 [37], ScanNet++ [70], ScanNet [71], HyperSim [72], Dynamic Replica [73], DL3DV [36], BlendedMVS [74], Aria Synthetic Environments [75], TartanAir [76], MapFree [77], MegaDepth [78], WildRGBD [96], Waymo [97], Bedlam [98], and ARKitScenes [79]. We do not include 3D Ken Burns [99], IRS [100], and SmartPortraits [101] for training since these datasets are either single view or fail to download successfully. We adapt the official scripts provided by CUT3R [17], DUSt3R [10], and Spann3R [15] for dataset processing. For training STREAM3Rβ, we remove all the single-view datasets as in VGG-T, leaving 19 datasets for training. We did not find performance degradation when removing the single-view datasets. Please refer to the Tab. 6 of the CUT3R for more dataset details."
        },
        {
            "title": "B More Implementation details",
            "content": "More Training Details. Our method conducts end-to-end training on all datasets on hybrid of 12 different resolutions, ranging from 224 224 to 512 384. Data augmentation side, we perform sequence-level color jittering by applying the same color jitter across all frames in sequence. Network Architecture Details. We follow DUSt3R and use the CroCoNet [81] pre-trained ViT for the encoder and decoder design. We directly use the DPT [66] head for Headglobal and Headlocal implementation. We apply RoPE to the query and key feature before each attention operation for the ViT encoder, but ignore it for the ViT decoder to generalize to an arbitrary number of input views. For ablation studies, we train our model on the same datasets but at resolution 224 224. For the sliding window attention version STREAM3Rβ-W[5], we always include the tokens of the first frame to keep the canonical coordinate space unchanged. We set window size W= 5 since it trades off performance and speed, and other window size also stably works. For the full attention version STREAM3Rβ-FA, we directly use the causally trained model STREAM3Rβ and remove the causal mask in the SelfAttn. This is similar to the revisit operation in CUT3R."
        },
        {
            "title": "C More Comparisons",
            "content": "Video Depth Estimation. We further expand the video depth comparison in the main paper and include wider range of baseline methods, including single-frame depth methods (Marigold [39] and DepthAnything-V2 [102]), video depth approaches (NVDS [103], DepthCrafter [40], and ChronoDepth [104]), and recent joint depth-and-pose estimation methods such as Robust-CVD [105], CausalSAM [94], DUSt3R [10], MASt3R [11], MonST3R [49], and Spann3R [15]. Extended results are shown in Tab. 7. STREAM3R consistently outperforms its RNN-based counterpart CUT3R under the per-sequence scale & shift setting, and even achieves state-of-the-art performance on the KITTI datasetwhile also being the fastest in terms of FPS. 3D Reconstruction on NRGBD. We further include the comparison on NRGBD benchmark [91] in Tab. 8. Here, we also include the comparison with concurrent work StreamVGGT [84], which fine-tunes VGG-T into streaming version similar to our method. We also include VGG-T[streaming], which indicates using VGG-T in the streaming setting by replace the full attention in VGG-T into the causal attention. As can be seen, our method clearly outperforms all optimization-based and online methods, including the official VGG-T model. Direct use of VGG-T in the streaming setting substantially degrades performance, underscoring the need for fine-tuning under causal constraints. 17 Table 7: Video Depth Evaluation. We report scale&shift-invariant depth, scale-invariant depth and metric depth accuracy on Sintel, Bonn, and KITTI datasets. Methods requiring global alignment are marked GA, while Optim and Stream indicate Optimzation-based and Streamne methods, respectively. We also report the FPS on KITTI dataset using 512 144 image resolution for all methods, except Spann3R which Stream supports 224224 inputs. Alignment Method Type Abs Rel δ<1.25 Abs Rel δ<1.25 Abs Rel δ <1.25 FPS Sintel BONN KITTI Marigold [39] Stream 0.532 Depth-Anything-V2 [102] Stream 0.367 Stream 0.408 NVDS [103] Stream 0.687 ChronoDepth [104] Stream 0.292 DepthCrafter [40] Stream 0.703 Robust-CVD [59] Optim 0.387 CasualSAM [94] Optim 0.531 DUSt3R-GA [10] Optim 0.327 MASt3R-GA [11] Optim 0.333 MonST3R-GA [49] Stream 0.508 Spann3R [15] Stream 0.540 CUT3R [17] Stream 0.356 STREAM3R DUSt3R-GA [10] MASt3R-GA [11] MonST3R-GA [49] Spann3R [15] Fast3R [12] CUT3R [17] STREAM3Rα MASt3R-GA [11] CUT3R STREAM3Rα Optim 0.656 Optim 0.641 Optim 0.378 Stream 0.622 0.653 Stream 0.421 Stream 0.478 FA Optim 1.022 Stream 1.029 Stream 1.041 51.5 55.4 48.3 48.6 69.7 47.8 54.7 51.2 59.4 59.0 50.8 55.7 58. 45.2 43.9 55.8 42.6 44.9 47.9 51.1 14.3 23.8 21.0 0.091 0.106 0.167 0.100 0.075 - 0.169 0.156 0.167 0.066 0.157 0.074 0.068 0.155 0.252 0.067 0.144 0.193 0.078 0.075 0.272 0.103 0.084 93.1 92.1 76.6 91.1 97.1 - 73.7 83.1 78.5 96.4 82.1 94.5 95. 83.3 70.1 96.3 81.3 77.5 93.7 94.1 70.6 88.5 94.4 0.149 0.140 0.253 0.167 0.110 - 0.246 0.135 0.137 0.157 0.207 0.106 0.099 0.144 0.183 0.168 0.198 0.140 0.118 0.116 0.467 0.122 0.234 79.6 80.4 58.8 75.9 88.1 - 62.2 81.8 83.6 73.8 73.0 88.7 91. 81.3 74.5 74.4 73.7 83.4 88.1 89.6 15.2 85.5 57.6 <0.1 3.13 - 1.89 0.97 - - 0.76 0.31 0.35 13.55 16.58 23.48 0.76 0.31 0.35 13.55 47.23 16.58 23.48 0.31 16.58 23.48 Per-sequence scale & shift Per-sequence scale Metric scale Table 8: 3D Reconstruction Comparison on NRGBD [91]. Our proposed method consistently achieves superior performance compared to optimization-based (Optim), streaming-based (Stream), and even full attention (FA) methods. Method Type Acc Comp NC VGG-T [13] DUSt3R-GA [10] MASt3R-GA [11] MonST3R-GA [49] Spann3R [15] CUT3R [17] StreamVGGT [84] VGG-T [Streaming] [13] STREAM3Rβ FA Optim Optim Optim Stream Stream Stream Stream Stream Mean 0.073 0.144 0.085 0.272 0.416 0.099 0.084 0.219 0.057 Mean 0.077 0.154 0.063 0.287 0.417 0.076 0.074 0.212 0. Med. 0.021 0.018 0.028 0.110 0.285 0.026 0.041 0.105 0.013 Mean 0.910 0.870 0.794 0.758 0.684 0.837 0.861 0.797 0. Med. 0.990 0.982 0.928 0.843 0.789 0.971 0.986 0.936 0.993 Med. 0.018 0.019 0.033 0.114 0.323 0.031 0.044 0.102 0."
        }
    ],
    "affiliations": [
        "S-Lab, Nanyang Technological University, Singapore",
        "Shanghai Artificial Intelligence Laboratory",
        "The University of Hong Kong",
        "WICT, Peking University"
    ]
}