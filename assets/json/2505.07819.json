{
    "paper_title": "H$^{\\mathbf{3}}$DP: Triply-Hierarchical Diffusion Policy for Visuomotor Learning",
    "authors": [
        "Yiyang Lu",
        "Yufeng Tian",
        "Zhecheng Yuan",
        "Xianbang Wang",
        "Pu Hua",
        "Zhengrong Xue",
        "Huazhe Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Visuomotor policy learning has witnessed substantial progress in robotic manipulation, with recent approaches predominantly relying on generative models to model the action distribution. However, these methods often overlook the critical coupling between visual perception and action prediction. In this work, we introduce $\\textbf{Triply-Hierarchical Diffusion Policy}~(\\textbf{H$^{\\mathbf{3}}$DP})$, a novel visuomotor learning framework that explicitly incorporates hierarchical structures to strengthen the integration between visual features and action generation. H$^{3}$DP contains $\\mathbf{3}$ levels of hierarchy: (1) depth-aware input layering that organizes RGB-D observations based on depth information; (2) multi-scale visual representations that encode semantic features at varying levels of granularity; and (3) a hierarchically conditioned diffusion process that aligns the generation of coarse-to-fine actions with corresponding visual features. Extensive experiments demonstrate that H$^{3}$DP yields a $\\mathbf{+27.5\\%}$ average relative improvement over baselines across $\\mathbf{44}$ simulation tasks and achieves superior performance in $\\mathbf{4}$ challenging bimanual real-world manipulation tasks. Project Page: https://lyy-iiis.github.io/h3dp/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 1 9 1 8 7 0 . 5 0 5 2 : r H3DP: Triply-Hierarchical Diffusion Policy for Visuomotor Learning Yiyang Lu1, Yufeng Tian4, Zhecheng Yuan1,2,3, Xianbang Wang1, Pu Hua1,2,3, Zhengrong Xue1,2,3, Huazhe Xu1,2,3 1 Tsinghua University IIIS, 2 Shanghai Qi Zhi Institute, 3 Shanghai AI Lab, 4 Harbin Institute of Technology luyy24@mails.tsinghua.edu.cn, huazhe xu@mail.tsinghua.edu.cn Figure 1: H3DP can not only achieve superior performance across 44 tasks on 5 simulation benchmarks, but also handle long-horizon challenging manipulation tasks in cluttered real-world scenarios. Abstract: Visuomotor policy learning has witnessed substantial progress in robotic manipulation, with recent approaches predominantly relying on generative models to model the action distribution. However, these methods often overlook the critical coupling between visual perception and action prediction. In this work, we introduce Triply-Hierarchical Diffusion Policy (H3DP), novel visuomotor learning framework that explicitly incorporates hierarchical structures to strengthen the integration between visual features and action generation. H3DP contains 3 levels of hierarchy: (1) depth-aware input layering that organizes RGB-D observations based on depth information; (2) multi-scale visual representations that encode semantic features at varying levels of granularity; and (3) hierarchically conditioned diffusion process that aligns the generation of coarse-to-fine actions with corresponding visual features. Extensive experiments demonstrate that H3DP yields +27.5% average relative improvement over baselines across 44 simulation tasks and achieves superior performance in 4 challenging bimanual real-world manipulation tasks. Project Page: https://lyy-iiis.github.io/h3dp/. Keywords: Imitation Learning, Representation Learning, Diffusion Model Equal Contribution"
        },
        {
            "title": "Introduction",
            "content": "Visuomotor policy learning has emerged as prevailing paradigm in robotic manipulation [1, 2, 3, 4, 5]. Existing approaches have increasingly adopted powerful generative methods, such as diffusion and auto-regressive models, to model the action generation process [6, 7, 8, 9, 10]. However, these predominant methods have focused primarily on separately refining either the representation of perception or actions, often overlooking establishing tight correspondence between perception and action. In contrast, human decision-making inherently involves hierarchical processing of information from perception to action [11, 12]. The visual cortex extracts features in layered fashion and performs hierarchical inference based on visual motion perception, ultimately resulting in the generation of structured motor behaviors [13, 14]. Inspired by this, we argue that enabling learned visuomotor agents to emulate such hierarchical behavior patterns is also critical for enhancing their decision-making capabilities. Prior works have primarily focused on hierarchically modeling the action generation process alone [15, 16], without explicitly incorporating hierarchical structure throughout the whole visuomotor policy pipeline. In this paper, we present H3DP, novel visuomotor policy learning framework grounded in three levels of hierarchy: input, representation, and action generation. This design reflects the hierarchical processing mechanisms that humans use the visual cortex to perceive environmental stimuli to guide motor behavior. At the input level, to better leverage the depth information in modern robotic benchmarks and datasets [17, 18, 19, 20], H3DP moves beyond prior 2D approaches that primarily rely on RGB or simple RGB-D concatenation, which has shown limited effectiveness in prior work [4, 21]. We introduce depth-aware layering strategy that partitions the RGB-D input into distinct layers based on depth cues. This approach not only enables the policy to explicitly distinguish between foreground and background, but also suppresses distractors and occlusions [22, 23], thereby enhancing the understanding and reasoning of spatial structure in the cluttered visual scenarios. For visual representation, to address the limitations of flattening image features into single vector, which can discard some spatial structures and semantic information [24, 25, 26], H3DP employs multi-scale visual representation, where different scales capture features at varying granularity levels, ranging from global context to fine visual details. In the action generation stage, H3DP incorporates key inductive bias inherent to the diffusion process: the tendency to progressively reconstruct features from low-frequency to high-frequency components [27, 28, 29], by hierarchical action generation. Specifically, coarse visual features guide initial denoising steps to shape the global structure (low-frequency components) of action, while fine-grained visual features inform the later steps to refine precise details (high-frequency components). This establishes tighter coupling between action generation and visual encoding, enabling the policy to generate actions that are semantically grounded in multi-scale perceptual features. We validate H3DP through extensive experiments on 44 simulation tasks across 5 diverse benchmarks, where it surpasses state-of-the-art methods by relative average margin of +27.5%. Furthermore, in real-world evaluations, we deploy bimanual robotic systems to tackle four challenging tasks situated in cluttered environments, involving high disturbances and long-horizon objectives. H3DP achieves +32.3% performance improvement over Diffusion Policy in these real-world scenarios."
        },
        {
            "title": "2 Related Work",
            "content": "Visual imitation learning. Numerous studies have proposed efficient policy learning algorithms from different aspects [1, 2, 30]. As representative approach, to endow the learned policy multimodality ability, Diffusion Policy [1] incorporates the diffusion process to better represent the action distribution. Based on DP, methods like DP3 [4, 31] and 3D-Actor [32], designed for point cloud inputs, enhance the policys scene understanding by refining the visual representation. Consistency 2 Policy [6] and ManiCM [33] modify the inference process to achieve the inference acceleration. However, these approaches focus solely on enhancing either the action generation or the visual feature extraction, without explicitly modeling the relationship between them. To address this issue, we propose hierarchical framework that couples multi-scale visual representations with the diffusion process, enabling more structured integration between visual features and action generation. Leveraging hierarchical information for policy learning. In the computer vision community, numerous studies have leveraged hierarchical information to address variety of downstream tasks [34, 35, 36, 37, 38, 39]. For example, standard diffusion models [40, 41, 42, 43] and flow matching [44, 45, 46] adopt the U-Net framework [25, 47], which exploits multi-scale feature representations to retain rich contextual information throughout the denoising process. VAR [48] innovatively employs multi-scale visual representations with quantization to perform image generation in an auto-regressive manner. In robot learning, recent works [16, 49, 50] have also begun to adopt hierarchical paradigms for policy learning. Dense Policy [15] leverages bidirectional extension strategy to enable hierarchical action prediction. ARP [50] predicts sequence of actions at different levels of abstraction in hierarchical way. CARP [16] draws inspiration from VAR by employing multi-scale VQ-VAE [34, 37] to construct action sequences and subsequently generating residual actions autoregressively using GPT-style architecture [51]. However, these algorithms model only the hierarchical structure of the action generation process, without explicitly addressing the crucial linkage between visual representation and action in visuomotor policy learning. In contrast, H3DP not only incorporates multi-scale visual representations but also leverages the inherent strengths of diffusion models to seamlessly integrate coarse-to-fine action generation into the diffusion process itself. Furthermore, by adopting depth-aware layering strategy, H3DP maximizes the utilization of hierarchical feature information across the input, latent, and output stages, thereby enriching the policy learning pipeline in structured and semantically aligned manner."
        },
        {
            "title": "3 Method",
            "content": "We employ three hierarchical structures to enhance the policys understanding of visual input and predict more accurate action distributions. At the input level, the RGB-D image is discretized into multiple layers to improve the policys ability to distinguish and interpret foreground-background variations. Upon this, we adopt multi-scale visual representation, wherein coarse-grained features capture global contextual information, while fine-grained features encode detailed scene attributes. On the action side, correspondingly, the representations at different scales are utilized to generate actions in coarse-to-fine manner, thus strengthening the correlation between action and visual representations. detailed discussion of each part will be provided in the following sections. 3.1 Depth-aware Layering Effective robotic manipulation hinges on robust spatial understanding. While RGB data provides rich texture and color information, depth supplies the critical geometric context, including the relative spatial arrangement of objects and their distances. Combining these modalities offers powerful foundation for scene comprehension. However, simply concatenating RGB images with depth maps does not lead to performance improvements [4, 21]. Hence, to fully exploit the geometric structure inherent in depth maps, we introduce depth-aware layering mechanism inspired by Zhang et al. [52]. Pixels with depth are assigned to layer using linear-increasing discretization: (cid:37) (cid:36) (cid:114) = 0.5 + 0.5 1 + 4(N + 1)(N + 2) , (1) dmin dmax dmin + ϵ which promotes the robot to focus more on its workspace. By explicitly encoding objects distributed across different depth planes, this structured representation retains all visual detail while strategically utilizing depth to impose meaningful foreground-background separation, thereby enabling the policy to selectively attend to different regions of the image. This design can effectively boost the agents capacity for spatial perception and interaction planning. Furthermore, we also conduct comparisons 3 Figure 2: Overview of H3DP. H3DP integrates three hierarchical design principles across the perception and action generation pipeline. At the input level, RGB-D images are decomposed into multiple layers based on their depth values. Then, we employ multi-scale visual representations to capture features at varying levels of granularity. During the action generation, denoising process is divided into several stages guided by multi-scale visual representations. against other discretization algorithms and perform additional experiments to substantiate the effectiveness of our proposed depth-aware layering method. The corresponding results are provided in Appendix E.3 and Appendix E.7. 3.2 Multi-Scale Visual Representation In visuomotor policy learning, visual representation plays crucial role in embedding input images and mapping them to actions. An effective visual encoder should capture various granularity features of the visual scenarios and guide the policy to predict the action distribution. However, existing methods typically extract features at single spatial scale or compress them into fixed-resolution representation, limiting the expressiveness of learned features [24, 25, 26]. To address this problem, we hierarchically partition the feature map into multiple scales, enabling the capture of both coarse structural information and detailed local cues. Interpolation and Quantization. After applying depth-aware layering to the input image I, each layer Im is independently encoded into multi-scale feature maps {fm,kfm,k RhkwkC}K k=1, where {(hk, wk)}K k=1 denotes the spatial resolutions across scales. Adopting the quantization design in VQ-VAE [34, 37], these feature maps {fm,k}K k=1 are quantized into discrete vectors drawn from learnable codebook Zm RV C. Specifically, each feature vector (i,j) m,k is mapped to its nearest neighbor in Euclidean distance: (i,j) m,k arg min zZm (i,j) m,k 2. (2) By applying differentiable interpolation and lightweight convolution to the quantized features fm,k, we then obtain the multi-scale visual representations { ˆfm,k}K k=1 for each layer Im. The pseudocode of full encoding procedure is detailed in Algorithm 1, Appendix B. Training. To ensure consistent representations across scales, we aim to minimize the consistency loss between the original feature fm = Em(Im) and the representation ˆfm,k at different scales: Lconsistency = 1 (cid:88) (cid:88) m=0 k= (cid:18)(cid:13) (cid:13) 2 ˆfm,k sg(fm) (cid:13) (cid:13) (cid:13) (cid:13) 2 + β (cid:13) (cid:13)fm sg( ˆfm,k) (cid:13) (cid:13) 2 (cid:13) (cid:13) 2 (cid:19) , (3) where sg() is the stop gradient operator and β balances the gradient flow between two terms. The visual encoder {Em}N 1 m=0 are trained end-to-end, as described in detail in Appendix B. m=0 and codebook {Zm}N 1 3.3 Hierarchical Action Generation To match the inherent inductive biases of denoising process [27, 28, 29], we leverage multi-scale visual representations to model action generation in coarse-to-fine manner. The early stage actions are derived from representations that capture global scene information, while fine-grained representations are responsible for generating detailed action components. This approach couples the visual representation and the action generation process via reinforcing their correspondence at the same hierarchical levels. Inference. Our action generation module is denoising diffusion model conditioned on multi-scale features = { ˆfk = { ˆfm,k}N 1 m=0}K k=1 and robot poses q. The denoising process unfolds over k=1(τk1, τk]. When (τk1, τk], the denoising network ϵ(t) steps partitioned into stages θ conditioning on the corresponding feature map ˆfk and robot poses q, predicts the noise component (4) ϵt = ϵ(t) θ (at ˆfk, q), then generates at1 from at via: at1 = αt (cid:18) at 1 αt ϵt αt (cid:19) + (cid:113) 1 αt1 σ2 ϵt + σtϵt, (5) gradually transforming the Gaussian noise aT into the noise-free action a0, where αt, σt are fixed parameters depending on the noise scheduler, and ϵt (0, I) is Gaussian noise. Training. To train the denoising network ϵ(t) θ , we randomly sample an observation-action pair ((I, q), a0) and noise ϵ (0, I). The network is optimized to predict ϵ given noisy action conditioned on the final feature map ˆfK and robot pose q, via the objective: (cid:104) Ldiffusion = Ea0,ϵ,t γtϵ(t) θ ( αta0 + 1 αtϵ ˆfK, q) ϵ2(cid:105) , (6) where {γt} are pre-defined coefficients. More implementation details can be found in Appendix A. By conditioning on the final feature ˆfK during training, gradients from the loss propagate through the entire hierarchical encoder, implicitly optimizing all { ˆfk}K k=1. This design promotes consistency of representations at each scale for action generation while enhancing training efficiency. t,ϵ,a0, Discussions. Diffusion models inherently aim to predict the posterior average of the target distribution conditioned on the provided features [53, 54], i.e., the optimal denoising network ϵ(t) θ follows ϵ(t) θ (atf, q) = 1αtϵ=at [ϵat, f, q]. Features at varying resolutions retain information across distinct frequency domains. Consequently, they provide robust guidance for generating specific frequency components of the action during relevant stages of the denoising process. Related experiments are shown in Section 4.1.3. By using lower-resolution features for earlier stages and gradually refining the predictions with higher-resolution features, the model benefits from both the stability of coarse representations and the precision of fine details. αta0+"
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we present extensive experiments across simulated and real-world settings to demonstrate the efficacy of H3DP. In addition, we perform thorough ablation analyses to evaluate the contribution of each hierarchical design. 5 Table 1: Simulation task results. Across 5 simulation benchmarks with various difficult levels, H3DP obtains +27.5% relative performance gains on average over 44 tasks. Method Tasks H3DP DP DP (w/ depth) DP3 MetaWorld MetaWorld MetaWorld ManiSkill ManiSkill Adroit DexArt RoboTwin Average (Medium 11) 98.3 78.2 77.7 89. (Deformable 4) 59.3 22.3 44.5 26.5 (44) 75.618.6 48.123.1 52.822.2 59.324.9 (Hard++ 5) 95.8 58.0 71.2 88.4 (Rigid 4) 65.3 27.5 40.8 33.5 (Hard 5) 87.8 52.6 57.2 52.6 (3) 87.3 79.0 76.0 84. (8) 57.4 22.8 12.6 45.9 (4) 53.3 44.3 42.0 54.8 4.1 Simulation Experiments 4.1.1 Experiment setup Simulation benchmarks and baselines: To sufficiently verify the effectiveness of H3DP, we evaluate H3DP on 5 simulation benchmarks, encompassing total of 44 tasks. These tasks span variety of manipulation challenges, including articulated object manipulation [55, 56, 57], deformable object manipulation [58], bimanual manipulation [59], and dexterous manipulation [55, 56]. The details of the expert demonstrations can be found in Appendix C. To comprehensively assess the performance of H3DP, we compare it against three baselines: Diffusion Policy [1], one of the most widely used visuomotor policy learning algorithms; Diffusion Policy (w/ depth), which extends Diffusion Policy to incorporate RGB-D input to bridge the information gap; and DP3 [4], an enhanced version of Diffusion Policy that leverages an efficient encoder for point cloud input. Evaluation metric: Each experiment is run with three different seeds to mitigate performance variance. For each seed, we evaluate 20 episodes every 200 training epoches. In simpler MetaWorld, Adroit and DexArt tasks, we compute the average of the highest five success rates as its success rate, while in other environments, only the hightest success rate is recorded. 4.1.2 Simulation performance As shown in Table 1, the simulation experiment results exhibit that H3DP outperforms or achieves comparable performance among the whole simulation benchmarks. Our method outperforms DP3 by relative average margin of +27.5%. Notably, DP3 requires manual segmentation of the point cloud to remove background and task-irrelevant elements. This process introduces additional human effort and renders performance susceptible to segmentation quality. Relevant experimental results are provided in Appendix E.6. In contrast, benefiting from our design, H3DP obtains superior performance using only raw RGB-D input, without the need for segmentation and human effort. Furthermore, on the Adroit and DexArt benchmark, while DP3 leverages multi-view cameras to restore the complete point clouds, H3DP attains comparable performance using only one single-camera RGB-D image. The whole simulation results in each task can be found in Appendix E.1. 4.1.3 Spectral analysis of actions To gain more comprehensive understanding of the action generation, we apply Discrete Fourier Transform (DFT) to examine how the frequency composition of actions evolves throughout the denoising process. Specifically, we the analysis across 4 conduct benchmarks and visualize the spectral characteristics of action chunks during generation. As shown in Figure 3, the results consistently indicate that the denoising process Figure 3: Action DFT results. As the denoising process progresses, the Gaussian noise (t = τ4) is gradually transformed into the predicted action (t = τ0). Timesteps τi is arranged in descending order of noise levels. The results reveal consistent frequency evolution pattern: low-frequency components predominantly emerge during the early stages of denoising, whereas high-frequency features are progressively introduced in the latter phases of the process. Figure 4: Success rate in real-world. Dark-colored bars correspond to H3DP, whereas the light-colored bars correspond to DP. H3DP outperforms DP in all 4 challenging real-world tasks. begins with the synthesis of low-frequency features, which are incrementally complemented by higher-frequency features in later stages. This observation not only shows that action, akin to image, exhibits an intrinsic inductive bias in the diffusion process, but also elucidates the action generation mechanism of H3DP, wherein actions are hierarchically composed to captured features across varying levels of granularity. 4.2 Real-world Experiments In terms of real-world experiments, we choose Galaxea R1 robot as our platform. We design four diverse challenging real-world tasks to evaluate the effectiveness of our method: Clean Fridge (CF): In cluttered refrigerator environment, the robot is required to relocate transparent bottle from the upper compartment to the lower one. The bottle is randomized within 30 cm 5 cm region on both the upper and lower shelves of the refrigerator. Pour Juice (PJ): This is long-horizon task. The robot is required to place cup in front of water dispenser, scoop spoonful of juice powder, then fill the cup with water, and finally put straw in the cup. The cup is placed within 7 cm 7 cm area, and both the color of the juice powder and the position of the water dispenser are subject to variation across trials. Place Bottle (PB): The robot must place bottle, initially located at random position, onto designated coaster. The bottle is placed within 15 cm 15 cm region, while the coaster is positioned within an around 25 cm 25 cm area. Sweep Trash (ST): This long-horizon task entails picking up broom, sweeping scattered debris on table into dustpan, and subsequently emptying the contents into trash bin. The trash is randomly distributed across the entire table surface, approximately within 40 cm 40 cm area. 4.2.1 Experiment Setup We use the ZED camera to acquire the depth image with 60Hz running frequency. The demonstrations are collected by Meta Quest3. Regarding the two long-horizon tasks, both the baseline and our method incorporate the pre-trained ResNet18 [60] encoders for RGB modality to enhance the policys perceptual capabilities in real-world environments. Each task is evaluated at 20 randomly sampled positions within the defined randomization range for each method. We record the success trials and calculate the corresponding success rate. In addition, during policy deployment, we adopt an asynchronous design to obtain an approximately 15Hz inference speed. We also introduce temporal ensembling and p-masking to improve temporal consistency and alleviate overfitting to the proprioception state. More setup details can be found in Appendix D. Figure 5: Experiment Setup. 7 Table 2: Instance generalization results. H3DP achieves +15.4% performance gain. Method Tasks H3DP Diffusion Policy Place Bottle sprite coke bottle 67 45 49 36 can 53 40 Sweep Trash 64 cm 216 cm3 8 cm3 Average 75 52 86 72 67 66.2 50.8 4.2.2 Experiment Results Spatial generalization: As shown in Figure 4, H3DP significantly outperforms the baseline across all four real-world tasks, achieving an average improvement of +32.3%. It should be noted that in CF and PJ tasks, the policy is required to not only identify target objects in cluttered visual environments but also perform long-horizon reasoning to accomplish the tasks. While DP struggles to complete either task, H3DP achieves substantial improvements, outperforming DP by +38% and +41% respectively. Therefore, H3DP demonstrates superior perceptual and decision-making capabilities compared to alternative algorithms. Meanwhile, it should be noted that in terms of the point cloud based method DP3, it requires precise segmentation and high-fidelity depth sensing, resulting in it being less effective in handling our four cluttered real-world scenes that we designed. Comparative experimental results for DP3 are presented in Appendix E.8. Instance generalization: Regarding instance generalization, we evaluate the model on two real-world tasks by varying the size and shape of bottles or trash items. As shown in Table 2, after replacing the objects with variants of differing sizes and shapes, H3DP maintains strong generalization capabilities attributable to its ability to hierarchically model features at multiple levels of granularity, and consistently outperforms baseline approaches across all settings. 4.3 Ablation Study In this section, we ablate each key component of our framework and conduct experiments on three benchmarks to further exhibit the effectiveness of H3DP. The entire results in each benchmark can be found in Appendix E.2. Table 3: Ablation on hierarchical features. Each hierarchical design. We ablate the three hierarchical components introduced in our framework and compare them against the baseline Diffusion Policy with RGB-D input. As shown in Table 3, each hierarchical component independently contributes to performance improvement, consistently outperforming the DP (w/ depth). Furthermore, Table 3 also demonstrates that the integration of all three hierarchical designs leads to substantial enhancement in overall performance. Methods Benchmarks H3DP w/o depth layering w/o hierarchical action w/o multi-scale representation DP (w/ depth) 59.6 46.5 49.0 48.7 42.1 45.0 32.0 40.0 40.0 32.0 65.7 55.0 57.0 53.7 46. 68.0 52.5 50.0 52.5 47.5 MW MS Average RT Methods Benchmarks MW MS Table 4: Ablation on number of layers . The choice of in depth-aware layering. For the depth-aware layering component, we investigate whether the policys performance is sensitive to the choice of the number of layers . As presented in Table 4, the trained policy achieves optimal and comparable performance when is set to 3 or 4, trend consistently observed across all evaluated benchmarks. When becomes excessively large, the image is over-partitioned, thus reducing the representation capacity of the policy. Nevertheless, even in such cases, the performance remains slightly better than the non-layered baseline. These findings highlight the critical role of depth-aware layering in enhancing policy effectiveness. H3DP (N = 1) H3DP (N = 2) H3DP (N = 3) H3DP (N = 4) H3DP (N = 5) H3DP (N = 6) 46.5 50.2 59.6 59.5 54.6 49.0 55.0 55.7 65.7 67.0 58.7 56.0 32.0 35.0 45.0 50.0 50.0 40.0 52.5 60.0 68.0 61.5 55.0 51. Average RT"
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we introduce H3DP, an efficient generalizable visuomotor policy learning framework that can obtain superior performance in wide range of simulations and challenging real-world tasks. Extensive empirical evidence suggests that establishing more cohesive integration between visual feature representations and the action generation process can enhance the generalization capacity and learning efficiency of our learned policies. The proposed three hierarchical designs not only facilitate the effective fusion of RGB and depth modalities, but also strengthen the correspondence between visual features and the generated actions at different granularity levels. In the future, we expect to extend the applicability of H3DP to more intricate and fine-grained dexterous real-world tasks."
        },
        {
            "title": "6 Limitations",
            "content": "Although H3DP has demonstrated effectiveness in variety of tasks, there exist several limitations. First, despite our use of asynchronous execution to improve inference speed in real-world settings, the overall inference time of diffusion-based models remains relatively slow. We could explore distilling the policy into consistency model, to enhance real-time performance. Second, the limited depth quality of the ZED camera may hinder the policys full potential in real-world deployment; employing higher-fidelity depth sensors could further boost the effectiveness of H3DP in practical scenarios. Acknowledgments We are thankful to all members of Galaxea for helping with hardware infrastructure and real-world experiments. We also thank the members of TEA Lab for their helpful discussions."
        },
        {
            "title": "References",
            "content": "[1] C. Chi, Z. Xu, S. Feng, E. Cousineau, Y. Du, B. Burchfiel, R. Tedrake, and S. Song. Diffusion policy: Visuomotor policy learning via action diffusion. The International Journal of Robotics Research, page 02783649241273668, 2023. [2] T. Z. Zhao, V. Kumar, S. Levine, and C. Finn. Learning fine-grained bimanual manipulation with low-cost hardware. arXiv preprint arXiv:2304.13705, 2023. [3] K. Black, N. Brown, D. Driess, A. Esmail, M. Equi, C. Finn, N. Fusai, L. Groom, K. Hausman, B. Ichter, S. Jakubczak, T. Jones, L. Ke, S. Levine, A. Li-Bell, M. Mothukuri, S. Nair, K. Pertsch, L. X. Shi, J. Tanner, Q. Vuong, A. Walling, H. Wang, and U. Zhilinsky. π0: vision-languageaction flow model for general robot control. arXiv preprint arXiv:2410.24164, 2024. [4] Y. Ze, G. Zhang, K. Zhang, C. Hu, M. Wang, and H. Xu. 3d diffusion policy: Generalizable visuomotor policy learning via simple 3d representations. In Proceedings of Robotics: Science and Systems (RSS), 2024. [5] Z. Yuan, T. Wei, S. Cheng, G. Zhang, Y. Chen, and H. Xu. Learning to manipulate anywhere: visual generalizable framework for reinforcement learning. arXiv preprint arXiv:2407.15815, 2024. [6] A. Prasad, K. Lin, J. Wu, L. Zhou, and J. Bohg. Consistency policy: Accelerated visuomotor policies via consistency distillation. arXiv preprint arXiv:2405.07503, 2024. [7] Z. Wang, Z. Li, A. Mandlekar, Z. Xu, J. Fan, Y. Narang, L. Fan, Y. Zhu, Y. Balaji, M. Zhou, et al. One-step diffusion policy: Fast visuomotor policies via diffusion distillation. arXiv preprint arXiv:2410.21257, 2024. [8] K. Frans, D. Hafner, S. Levine, and P. Abbeel. One step diffusion via shortcut models. arXiv preprint arXiv:2410.12557, 2024. [9] N. M. Shafiullah, Z. Cui, A. A. Altanzaya, and L. Pinto. Behavior transformers: Cloning modes with one stone. Advances in neural information processing systems, 35:2295522968, 2022. [10] S. Lee, Y. Wang, H. Etukuru, H. J. Kim, N. M. M. Shafiullah, and L. Pinto. Behavior generation with latent actions. arXiv preprint arXiv:2403.03181, 2024. [11] D. H. Hubel and T. N. Wiesel. Receptive fields, binocular interaction and functional architecture in the cats visual cortex. The Journal of physiology, 160(1):106, 1962. [12] J. Bill, H. Pailian, S. J. Gershman, and J. Drugowitsch. Hierarchical structure is employed by humans during visual motion perception. Proceedings of the National Academy of Sciences, 117(39):2458124589, 2020. [13] T. S. Lee and D. Mumford. Hierarchical bayesian inference in the visual cortex. Journal of the Optical Society of America A, 20(7):14341448, 2003. [14] J. Bill, S. J. Gershman, and J. Drugowitsch. Visual motion perception as online hierarchical inference. Nature communications, 13(1):7403, 2022. [15] Y. Su, X. Zhan, H. Fang, H. Xue, H.-S. Fang, Y.-L. Li, C. Lu, and L. Yang. Dense policy: Bidirectional autoregressive learning of actions. arXiv preprint arXiv:2503.13217, 2025. [16] Z. Gong, P. Ding, S. Lyu, S. Huang, M. Sun, W. Zhao, Z. Fan, and D. Wang. Carp: Visuomotor policy learning via coarse-to-fine autoregressive prediction. arXiv preprint arXiv:2412.06782, 2024. [17] S. James, Z. Ma, D. Rovick Arrojo, and A. J. Davison. Rlbench: The robot learning benchmark & learning environment. IEEE Robotics and Automation Letters, 2020. 10 [18] B. Liu, Y. Zhu, C. Gao, Y. Feng, Q. Liu, Y. Zhu, and P. Stone. Libero: Benchmarking knowledge transfer for lifelong robot learning. arXiv preprint arXiv:2306.03310, 2023. [19] C. Sferrazza, D.-M. Huang, X. Lin, Y. Lee, and P. Abbeel. Humanoidbench: Simulated humanoid benchmark for whole-body locomotion and manipulation. arXiv Preprint arxiv:2403.10506, 2024. [20] H. Geng, F. Wang, S. Wei, Y. Li, B. Wang, B. An, C. T. Cheng, H. Lou, P. Li, Y.-J. Wang, Y. Liang, D. Goetting, C. Xu, H. Chen, Y. Qian, Y. Geng, J. Mao, W. Wan, M. Zhang, J. Lyu, S. Zhao, J. Zhang, J. Zhang, C. Zhao, H. Lu, Y. Ding, R. Gong, Y. Wang, Y. Kuang, R. Wu, B. Jia, C. Sferrazza, H. Dong, S. Huang, K. Sreenath, Y. Wang, J. Malik, and P. Abbeel. Roboverse: Towards unified platform, dataset and benchmark for scalable and generalizable robot learning, April 2025. URL https://github.com/RoboVerseOrg/RoboVerse. [21] H. Zhu, Y. Wang, D. Huang, W. Ye, W. Ouyang, and T. He. Point cloud matters: Rethinking the impact of different observation spaces on robot learning. arXiv preprint arXiv:2402.02500, 2024. [22] D. Rao, Q. V. Le, T. Phoka, M. Quigley, A. Sudsang, and A. Y. Ng. Grasping novel objects with depth segmentation. In 2010 IEEE/RSJ international conference on intelligent robots and systems, pages 25782585. IEEE, 2010. [23] S. Ainetter, C. Bohm, R. Dhakate, S. Weiss, and F. Fraundorfer. Depth-aware object segmentation and grasp detection for robotic picking tasks. arXiv preprint arXiv:2111.11114, 2021. [24] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. IEEE transactions on pattern analysis and machine intelligence, 37(9): 19041916, 2015. [25] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234241. Springer, 2015. [26] T.-Y. Lin, P. Dollar, R. Girshick, K. He, B. Hariharan, and S. Belongie. Feature pyramid networks for object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 21172125, 2017. [27] S. Rissanen, M. Heinonen, and A. Solin. Generative modelling with inverse heat dissipation. arXiv preprint arXiv:2206.13397, 2022. [28] S. Dieleman. Diffusion is spectral autoregression, 2024. URL https://sander.ai/2024/ 09/02/spectral-autoregression.html. [29] S. Wang, Z. Tian, W. Huang, and L. Wang. Ddt: Decoupled diffusion transformer. arXiv preprint arXiv:2504.05741, 2025. [30] Z. Xue, S. Deng, Z. Chen, Y. Wang, Z. Yuan, and H. Xu. Demogen: Synthetic demonstration generation for data-efficient visuomotor policy learning. arXiv preprint arXiv:2502.16932, 2025. [31] Y. Ze, Z. Chen, W. Wang, T. Chen, X. He, Y. Yuan, X. B. Peng, and J. Wu. Generalizable humanoid manipulation with 3d diffusion policies. arXiv preprint arXiv:2410.10803, 2024. [32] T.-W. Ke, N. Gkanatsios, and K. Fragkiadaki. 3d diffuser actor: Policy diffusion with 3d scene representations. arXiv preprint arXiv:2402.10885, 2024. [33] G. Lu, Z. Gao, T. Chen, W. Dai, Z. Wang, and Y. Tang. Manicm: Real-time 3d diffusion policy via consistency model for robotic manipulation. arXiv preprint arXiv:2406.01586, 2024. 11 [34] A. Van Den Oord, O. Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. [35] X. Wang, G. Oxholm, D. Zhang, and Y.-F. Wang. Multimodal transfer: hierarchical deep convolutional neural network for fast artistic style transfer. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 52395247, 2017. [36] S. Li, R. Wang, M. Tang, and C. Zhang. Hierarchical reinforcement learning with advantagebased auxiliary rewards. Advances in Neural Information Processing Systems, 32, 2019. [37] A. Razavi, A. Van den Oord, and O. Vinyals. Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems, 32, 2019. [38] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo. Swin transformer: In Proceedings of the IEEE/CVF Hierarchical vision transformer using shifted windows. international conference on computer vision, pages 1001210022, 2021. [39] C. Ryali, Y.-T. Hu, D. Bolya, C. Wei, H. Fan, P.-Y. Huang, V. Aggarwal, A. Chowdhury, O. Poursaeed, J. Hoffman, et al. Hiera: hierarchical vision transformer without the bellsand-whistles. In International conference on machine learning, pages 2944129454. PMLR, 2023. [40] Y. Song and S. Ermon. Generative modeling by estimating gradients of the data distribution. In Advances in Neural Information Processing Systems, pages 1189511907, 2019. [41] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [42] J. Song, C. Meng, and S. Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. [43] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. [44] Y. Lipman, R. T. Chen, H. Ben-Hamu, M. Nickel, and M. Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [45] X. Liu, C. Gong, and Q. Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. [46] P. Esser, S. Kulal, A. Blattmann, R. Entezari, J. Muller, H. Saini, Y. Levi, D. Lorenz, A. Sauer, F. Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. [47] Z. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang. Unet++: nested u-net architecture for medical image segmentation. In Deep learning in medical image analysis and multimodal learning for clinical decision support: 4th international workshop, DLMIA 2018, and 8th international workshop, ML-CDS 2018, held in conjunction with MICCAI 2018, Granada, Spain, September 20, 2018, proceedings 4, pages 311. Springer, 2018. [48] K. Tian, Y. Jiang, Z. Yuan, B. Peng, and L. Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Advances in neural information processing systems, 37:8483984865, 2024. [49] S. Pateria, B. Subagdja, A.-h. Tan, and C. Quek. Hierarchical reinforcement learning: comprehensive survey. ACM Computing Surveys (CSUR), 54(5):135, 2021. [50] X. Zhang, Y. Liu, H. Chang, L. Schramm, and A. Boularias. Autoregressive action sequence learning for robotic manipulation. IEEE Robotics and Automation Letters, 2025. 12 [51] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al. Improving language understanding by generative pre-training. 2018. [52] R. Zhang, H. Qiu, T. Wang, Z. Guo, Z. Cui, Y. Qiao, H. Li, and P. Gao. Monodetr: Depth-guided transformer for monocular 3d object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 91559166, 2023. [53] P. Dhariwal and A. Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. [54] Q. Sun, Z. Jiang, H. Zhao, and K. He. Is noise conditioning necessary for denoising generative models? arXiv preprint arXiv:2502.13129, 2025. [55] C. Bao, H. Xu, Y. Qin, and X. Wang. Dexart: Benchmarking generalizable dexterous manipulation with articulated objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2119021200, 2023. [56] A. Rajeswaran, V. Kumar, A. Gupta, G. Vezzani, J. Schulman, E. Todorov, and S. Levine. Learning complex dexterous manipulation with deep reinforcement learning and demonstrations. arXiv preprint arXiv:1709.10087, 2017. [57] T. Yu, D. Quillen, Z. He, R. Julian, K. Hausman, C. Finn, and S. Levine. Meta-world: benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on robot learning, pages 10941100. PMLR, 2020. [58] J. Gu, F. Xiang, X. Li, Z. Ling, X. Liu, T. Mu, Y. Tang, S. Tao, X. Wei, Y. Yao, et al. Maniskill2: unified benchmark for generalizable manipulation skills. arXiv preprint arXiv:2302.04659, 2023. [59] Y. Mu, T. Chen, S. Peng, Z. Chen, Z. Gao, Y. Zou, L. Lin, Z. Xie, and P. Luo. Robotwin: Dual-arm robot benchmark with generative digital twins (early version). arXiv preprint arXiv:2409.02920, 2024. [60] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770 778, 2016. [61] D. P. Kingma and J. Ba. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [62] L. N. Smith and N. Topin. Super-convergence: Very fast training of neural networks using large learning rates. In Artificial intelligence and machine learning for multi-domain operations applications, volume 11006, pages 369386. SPIE, 2019. [63] P. Esser, R. Rombach, and B. Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. [64] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [65] C. Wang, X. Luo, K. Ross, and D. Li. Vrl3: data-driven framework for visual deep reinforcement learning. Advances in Neural Information Processing Systems, 35:3297432988, 2022. [66] D. A. Reynolds et al. Gaussian mixture models. Encyclopedia of biometrics, 741(659-663):3, 2009. [67] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023."
        },
        {
            "title": "A Hyperparameters",
            "content": "To effectively address the varying levels of difficulty and distinct properties inherent to different benchmarks, we adapt our hyperparameter settings for each specific dataset. The chosen configurations, detailed in Table 5, 6, 7, 8, are selected based on previous works [1, 4, 21, 59]. Table 5: Hyperparameters used for MetaWorld, DexArt. Hyperparameter Observation Horizon (To) Action Horizon (Ta) Prediction Action Horizon (Tp) Optimizer Betas (β1, β2) Learning Rate Weight Decay Learning Rate Scheduler Training Timesteps (T ) Inference Timesteps Prediction Type Image Resolution Scale Number (K) Multi-Scale Representation Resolutions ({(hk, wk)}K Stage Boundiaries ({τk/T }K k=0) k=1) Value 2 2 4 AdamW [61] [0.95, 0.999] 1.0e-4 1.0e-6 Cosine 50 20 ϵ-prediction 128 128 4 {(1,1),(3,3),(5,5),(7,7)} {0,0.4,0.6,0.8,1.0} Table 6: Hyperparameters used for Adroit. Hyperparameter Observation Horizon (To) Action Horizon (Ta) Prediction Action Horizon (Tp) Optimizer Betas (β1, β2) Learning Rate Weight Decay Learning Rate Scheduler Training Timesteps (T ) Inference Timesteps Prediction Type Image Resolution Scale Number (K) Multi-Scale Representation Resolutions ({(hk, wk)}K Stage Boundiaries ({τk/T }K k=0) k=1) Value 2 2 4 AdamW [0.95, 0.999] 1.0e-4 1.0e-6 Cosine 50 20 ϵ-prediction 84 84 4 {(1,1),(3,3),(5,5),(6,6)} {0,0.4,0.6,0.8,1.0} In addition to the hyperparameters reported in the table, the choice of the number of layers demonstrates great importance, as shown in Table 4. Empirically, we choose = 4 in Adroit, MetaWorld Hard and Hard++, and = 3 in other benchmarks. The noise scheduler for diffusion process is determined by αt, defined using function (t) αt = (t) (0) , where (t) = cos2 (cid:18) π 2 t/T + 1 + (cid:19) . (7) Here, is the total number of diffusion timesteps and is an offset parameter. 14 Table 7: Hyperparameters used for ManiSkill. Hyperparameter Observation Horizon (To) Action Horizon (Ta) Prediction Action Horizon (Tp) Optimizer Betas (β1, β2) Learning Rate Weight Decay Learning Rate Scheduler Training Timesteps (T ) Inference Timesteps Prediction Type Image Resolution Scale Number (K) Multi-Scale Representation Resolutions ({(hk, wk)}K Stage Boundaries ({τk}K k=0/T ) k=1) Value 2 8 16 AdamW [0.9, 0.95] 1.0e-4 1.0e-4 One Cycle LR [62] 100 100 ϵ-prediction 128 128 4 {(1,1),(3,3),(5,5),(7,7)} {0,0.4,0.6,0.8,1.0} Table 8: Hyperparameters used for RoboTwin. Hyperparameter Observation Horizon (To) Action Horizon (Ta) Prediction Action Horizon (Tp) Optimizer Betas (β1, β2) Learning Rate Weight Decay Learning Rate Scheduler Training Timesteps (T ) Inference Timesteps Prediction Type Image Resolution Scale Number (K) Multi-Scale Representation Resolutions ({(hk, wk)}K Stage Boundaries ({τk}K k=0/T ) k=1) Value 3 2 8 AdamW [0.95, 0.999] 1.0e-4 1.0e-6 Cosine 100 100 ϵ-prediction 180 320 4 {(1,3),(3,5),(5,7),(5,9)} {0,0.4,0.6,0.8,1.0} For the reverse process, we employ different formulations depending on the environment. In MetaWorld, Adroit and DexArt, we follow the DDIM [42] approach, formulating the reverse process as an ODE, which corresponds to setting σt = (8) for all t. In ManiSkill and RoboTwin, we follow the design of DDPM [41] and formulate the reverse process as Variance Preserving (VP) SDE [43]. In this case, for all t, σt = (cid:114) 1 αt1 1 αt (cid:114) 1 αt αt1 . (9) Furthermore, consider the weighting term γt in Equation 6. Since the choice of γt does not affect the optimal denoising network ϵθ , we set for all t. γt = 15 (10)"
        },
        {
            "title": "B Method Details",
            "content": "This section outlines the implementation details of our multi-scale encoding. The encoder Em for each depth layer adopts the architecture from VQGAN [63], ensuring strong representational capacity while preserving spatial information. We use interpolate to denote differentiable resizing operation (e.g. bilinear or nearest-neighbor interpolation), which is crucial for enabling gradient flow during training. The function represents the quantization process detailed in Equation 2. Finally, after interpolating feature map fm,k to the highest resolution, we apply lightweight convolutional network ϕm,k designed to help restore fine details from the potentially lower-resolution source features. The pseudocode for this process is outlined in Algorithm 1. Algorithm 1: Multi-scale Encoding 1 Inputs: raw image 2 Hyperparameters: depth layer number , scale number K, resolutions {(hk, wk)}K 3 Partition image into + 1 images {Im}N 4 for = 0, . . . , 1 do 5 m=0 according to Equation 1 k= fm Em(Im) RhK wK for = 1, . . . , do 6 7 8 9 fm,k interpolate(fm, hk, wk) RhkwkC fm,k Q(fm,k) fm,k ϕm,k(interpolate(fm,k, hK, wK)) RhK wK ˆfm,k (cid:80) fm fm fm,k kk fm,k 11 12 Return: multi-scale features = { ˆfk = { ˆfm,k}N 1 m=0}K k=1 All trainable parameters, including the visual encoders {Em}N 1 m=0, the CNN parameters {{ϕm,k}N 1 k=1, and the denoising network ϵθ, are trained jointly in an end-toend manner. The optimization minimizes the combined objective function L, defined as weighted sum of consistency loss (Equation 3) and the diffusion loss (Equation 6): m=0, the codebooks {Zm}N 1 m=0}K = Ldiffusion + αLconsistency, (11) where α is hyperparameter balancing the two loss terms."
        },
        {
            "title": "C Expert Demonstrations",
            "content": "Regarding the MetaWorld [57] and the RoboTwin [59] benchmarks, we utilize scripted policies to generate expert demonstrations. In the case of ManiSkill [58] tasks, we employ the officially provided demonstrations. Trajectories for other simulation benchmarks are collected with agents trained by RL algorithms [4, 64, 65]. The expert policies are evaluated over 200 episodes, and their success rates are detailed in Table 20. Given the varying difficulty levels across benchmarks, we provide different number of demonstrations for each. Specifically, we provide 50 trajectories per task for MetaWorld, Adroit, and RoboTwin. For DexArt, we follow the setup in [4] and provide 100 trajectories per task. For ManiSkill, we use all official demonstrations: 1000 for rigid tasks and 200 for deformable tasks. In real-world experiments, we collect demonstrations of varying quantity, depending on the complexity and horizon length of the tasks. For short-horizon tasks, the number of collected trajectories is relatively limited 100 for Clean Fridge and 200 for Place Bottle. In contrast, long-horizon tasks demand more comprehensive data coverage. We collect more demonstrations: 270 for Pour Juice and 500 for Sweep Trash. These demonstrations play crucial role in guiding the training process, especially in scenarios where exploration is challenging or unsafe. 16 Real-world Training Details As mentioned in [16], DP-based methods often suffer from low inference speed, which can cause the inference process to stall. Prior approaches, including DP3 [4], attempt to address this by increasing action horizon (e.g. Ta = 4 or Ta = 8) or reducing the number of model parameters (e.g. Simple DP3). However, these strategies often compromise manipulation accuracy and dexterity. further complication is that increasing Ta widens the temporal gap between consecutive inference steps, leading to greater discrepancies in observed information, and consequently, divergence in predicted actions. This often results in noticeable jitter and discontinuities in manipulation. In general, DP-based methods are hindered by low inference speed, temporal inconsistency and overfitting to proprioceptive information. To address these challenges and improve real-world performance, we employ several empirical techniques. D.1 Higher Inference Speed To mitigate slow inference rooted in DP, we adopt an asynchronous design, achieving final inference frequency of 10-15 Hz. Instead of waiting for the execution of all predicted actions before initiating the next inference cycle, our method performs inference concurrently with action execution. The predicted action is stored in queue to be executed at fixed inference speed (10-15 Hz in practice, 12 Hz as average). The inference speeds achieved in real-world scenarios are presented in Table 9. H3DP (asynchronous) demonstrates superior inference speed compared to standard DP [1] and DP3 [4], as well as our synchronous H3DP implementation. In addition to this speed advantage, H3DP features shorter action sequence length (Ta = 2), which contributes to more dexterous manipulation capabilities. Table 9: Comparison of real-world inference speeds for different methods. The asynchronous version of our method demonstrates significant speed-up by decoupling inference from action execution. Method Inference Speed (FPS) DP 12.4 DP3 12.7 H3DP H3DP (asynchronous) 12.1 24.2 D.2 Temporal Consistency Having adopted the asynchronous design, we have obtained action sequences with overlapping time intervals. To ensure temporal smoothness and reduce discontinuities, we incorporate temporal ensembling mechanism from ACT [2]. As in ACT, H3DP performs weighted average of actions with the same timestep across multiple overlapping sequences. This ensembling mitigates the gap between actions inferred from slightly different observations and effectively reduces jitter. D.3 Alleviate Overfitting Similar to other real-world robotic systems, H3DP is susceptible to overfitting on proprioceptive inputs, often neglecting the RGB-D information. This is evidenced by that the model generates similar actions regardless of variations in object positions. We hypothesize that this occurs because the simple, low-parameter MLP used to encode proprioception is easier to optimize than the more complex CNN used for RGB-D input, leading to reliance on the former. To mitigate this, we introduce p-masking strategy during training. This mechanism stochastically masks all proprioceptive inputs with probability p, which decays linearly over the training process. Specifically, for training timestep in total horizon , p(t) = 1 t/T . This schedule encourages the model to rely more on RGB-D features early in training, helping it avoid early-stage overfitting and develop stronger visual grounding."
        },
        {
            "title": "E Additional Experiment Results",
            "content": "E.1 Simulation Results for Each Task We present the simulation results for each task in Table 19, which serves as supplement to Table 1. For each experiment, we report the average success rate over three different random seeds. The final average result is obtained by averaging across all benchmarks. We also provide the training progress of 4 algorithms on 12 various tasks across 3 different benchmarks in Figure 6. The selected tasks span range of difficulties and are included without cherry picking to provide an unbiased view of each algorithm. E.2 The Whole Results of Ablation Study We present the entire results of our ablation study on each hierarchical design and number of layers in Table 10 and Table 11, as supplement to Table 3 and Table 4. For each experiment, the success rate is reported by averaging over 3 different random seeds. The final average result is obtained by averaging across benchmarks. Table 10: Whole results of ablation study on hierarchical features. Method Tasks MetaWorld ManiSkill Soccer Stick Pull Pick Out of Hole Fill Excavate RoboTwin Tool Adjust Average H3DP w/o depth layering w/o hierarchical action w/o multi-scale representation DP (w/ depth) 85 59 64 55 75 72 67 72 71 37 34 40 34 32 98 78 82 73 72 38 27 18 32 23 45 32 40 40 32 59.6 46.5 49.0 48.7 42. Table 11: Whole results of ablation study on number of layers . Method Tasks H3DP (N = 1) H3DP (N = 2) H3DP (N = 3) H3DP (N = 4) H3DP (N = 5) H3DP (N = 6) MetaWorld ManiSkill Soccer Stick Pull Pick Out of Hole Fill Excavate RoboTwin Tool Adjust Average 59 64 85 78 62 61 72 70 75 83 75 73 34 33 37 40 39 34 78 85 98 90 87 77 27 35 38 33 23 25 32 35 45 50 50 46.5 50.2 59.6 59.5 54.6 49.0 E.3 Comparison with GMM-based Layering Variant To highlight the advantages of depth-aware layering, we conduct comparison against variant where this module is substituted with classical foreground-background segmentation method, Gaussian Mixture Models (GMM) [66], named H3DP-GMM. As shown in Table 12, H3DP outperforms H3DPGMM in all benchmarks. Notably, H3DP-GMM yields results comparable to simple single-layer (N = 1) approach, further emphasizing the rationality and effectiveness of our proposed depth-aware layering strategy. E.4 Comparison with More Baselines Except diffusion-based algorithms, we also compare H3DP with the recent state-of-the-art method CARP [16], which uses multi-scale action VQ-VAE to build hierarchical action structures. Table 13 shows that H3DP outperforms CARP with an average improvement of 18.9%, indicating the importance of adopting hierarchical designs throughout visual features and action generation. Table 12: Comparison with GMM-based layering variant. H3DP with depth-aware layering achieves superior performance compared to using GMM for layering. Method Tasks H3DP H3DP-GMM H3DP (N = 1) MetaWorld ManiSkill Soccer Stick Pull Pick Out of Hole Fill Excavate RoboTwin Tool Adjust Average 85 45 59 83 67 72 40 32 34 98 75 78 38 27 27 45 37 64.8 47.2 50.3 Table 13: Comparison with CARP. H3DP outperforms CARP with an average improvement of 18.9%. Method Tasks Box Close Soccer Stick Pull MetaWorld Pick Out of Hole Peg Insert Side Hammer Sweep Average H3DP CARP DP 98 82 85 53 43 83 71 64 40 15 13 98 69 62 100 82 64 100 100 86.3 67.4 60.7 E.5 Comparison with DP with Pre-trained Visual Encoder Prior work suggests that pre-trained visual representation may enhance spatial generalization of policy [30]. Hence, we investigate the impact of integrating pre-trained visual encoder with the original DP. We specifically replace the standard ResNet encoder in DP with DINOv2 [67] model. This variant, named DP-DINOv2, is evaluated on randomly selected tasks from the MetaWorld benchmark. The comparative results are presented in Table 14. Although DP-DINOv2 shows marginal improvement on some tasks compared to the original DP baseline, this comes with drawbacks, including longer training time, inference latency and larger number of parameters(21M for DINOv2 with ViT-S) due to the DINOv2 architecture. In contrast, H3DP utilizes an efficient visual encoder with less than 0.7M parameters, which achieves strong performance improvements over the original DP without incurring the aforementioned overheads. Table 14: Comparison with DP with pre-trained visual encoder. While DP-DINOv2 yields small improvement after paying additional cost, H3DP demonstrates superior performance. Method Tasks H3DP DP DP-DINOv2 MetaWorld Hand Insert Pick Out of Hole Disassemble Stick Pull Soccer Sweep Into 100 73 91 40 13 24 96 81 77 83 64 85 43 41 100 74 78 Average 84.0 58.0 63.8 E.6 Importance of Segmentation in DP3 As highlighted in Section 4.1.2, DP3 relies on manual segmentation of point cloud for optimal performance. To demonstrate this dependency, we evaluate DP3s performance under two distinct segmentation conditions using randomly selected tasks from the MetaWorld benchmark. We compare the following two scenarios: DP3 with ideal segmentation, which utilizes clean segmented point clouds containing only the robot and task-relevant objects, as implemented in the original DP3 algorithm; DP3 without ideal segmentation, which utilizes point clouds that are intentionally processed to include desk surface upon which objects rest, while other background elements are still removed. This configuration simulates common real-world scenarios where simple or automated segmentation rules might fail to perfectly isolate the task-relevant objects. 19 As shown in Table 15, DP3s performance degrades substantially when operating on point clouds without ideal segmentation. This result confirms that DP3 is highly sensitive to the quality of the input point cloud segmentation. Table 15: Comparision of DP3 under different segmentation qualities. We compare DP3 success rates on selected tasks when provided with different segmentation qualities, highlighting significant performance degradation. Method Tasks Push Shelf Place Stick Pull Soccer Bin Picking Pick Place Wall MetaWorld DP3 DP3 (w/o ideal segmentation) 96 89 86 26 61 48 57 29 100 50 97 Average 82.8 54.3 In contrast, H3DP operates directly on raw image without requiring such pre-processing, thereby avoiding such failure mode and the associated need for careful, potentially manual, segmentation tuning, especially common in real-world scenarios. E.7 H3DP in Tasks with Significant Depth Variations As introduced in Section 3.1, our depth-aware layering mechanism discretizes the depth map into distinct layers. This layering offers crucial advantage in scenarios with significant depth variations by providing structured representation that preserves visual detail while emphasizing foregroundbackground separation. We will elaborate on this benefit and provide supporting comparative analysis here. We conduct further experiments on tasks involving complex spatial arrangements, such as reaching for an object from closer to further or manipulating items in cluttered scene, demanding finegrained understanding of relative object depth. Although raw RGB-D data contains this information implicitly, models may struggle to effectively utilize it, potentially treating the depth channel similarly to color channels or failing to prioritize significant depth discontinuities. Point cloud representations, inherently capturing 3D structures, often perform well in such scenarios as they directly encode geometric relationships. Our depth-aware layering mechanism explicitly addresses this challenge for RGB-D inputs. By assigning pixels to discrete layers based on their depth values, we impose structure that forces the model to differentiate between elements located at varying distances to camera. This discretization acts as an inductive bias, guiding the model to attend more strongly to the geometric layout and relative positioning of objects along the depth axis. To empirically support our hypothesis, we conduct an ablation study focusing on tasks exhibiting significant depth variations. We compare the performance of three distinct approaches: DP3, DP (w/ depth) and H3DP (only with depth-aware layering, i.e., without hierarchical action and multi-scale representation). As seen in Table 16, our observations reveal consistent pattern: in tasks involving significant depth variations, point cloudbased policy initially demonstrated superior performance compared to standard RGB-D processing, represented by DP (w/ depth). However, upon integrating the depth-aware layering mechanism, H3DP consistently outperforms the baseline on these tasks, which strongly supports our claim. E.8 Comparison with DP3 in Real-world Experiments DP3 [4] is renowned baseline succeeding DP [1] in imitation learning and robotic manipulation, achieving state-of-the-art results in multiple simulation environments. However, DP3 has notable limitations. In particular, it relies heavily on high-quality point clouds, typically requiring precision sensors such as the RealSense L515 to function effectively. Table 16: Performance comparison demonstrating the effectiveness of depth-aware layering. Tasks with significant depth variations show great improvement only with depth layering compared to DP (w/ depth), surpassing the point cloud baseline (DP3). Method Tasks H3DP (only w/ depth layering) DP (w/ depth) DP3 Push 100 79 96 Shelf Place Disassemble MetaWorld Soccer Pick Place Wall Peg Insert Side 95 29 86 98 76 98 55 37 100 80 97 86 53 92 Average 89.0 59.0 87.7 In our setup, the head-mounted camera is ZED, which produces relatively low-quality visual inputs. This hinders the direct application of DP3 in our experimental setting. To ensure fair comparison, we evaluate both H3DP and DP3 on two short-horizon real-world tasks both using the learning-from-scratch (LFS) encoder. The results are summarized in Table 17. Table 17: Comparison of H3DP and DP3 in realworld experiments. We make comparison in 2 short-horizon real-world tasks and both use LFS encoders. H3DP achieves +29.0% performance gain. Method Tasks H3DP DP3 CF"
        },
        {
            "title": "PB Average",
            "content": "51 12 52 33 51.5 22.5 It is evident that DP3 underperforms compared to H3DP in both tasks, highlighting H3DPs ability to robustly extract meaningful features from RGB-D inputs, even when the quality of visual input is suboptimal. Furthermore, we empirically find that employing spatially sparse convolution provides better performance than the DP3-style encoder, suggesting promising direction for improving point cloud encoding in low-fidelity settings. E.9 Inference Speed As shown in Table 18, we evaluate the inference speed of different methods within simulated environments. The results indicate that the primary bottleneck of the inference speed of H3DP lies in the diffusion process itself, whereas the additional operations introduced for processing visual inputs and managing multi-scale representations incur only minimal computational overhead. corresponding analysis of inference speed in real-world scenarios is available in Appendix D.1. Method Table 18: Comparison of inference speeds for DP, DP3 and H3DP in simulation tasks. The result indicates that additional operations introduced in H3DP are lightweight compared to the diffusion process. Inference Speed (FPS) DP 11.1 DP 12.2 H3DP 12.0 21 Table 19: Success rates on 44 simulation tasks. Results of four different methods for each task are provided in this table. The summary across domains is shown in Table 1. Method Tasks MetaWorld [57] (Medium) Basketball Bin Picking Box Close Coffee Pull Coffee Push Hammer Soccer Push Wall H3DP DP DP (w/ depth) DP3 Method Tasks H3DP DP DP (w/ depth) DP3 Method Tasks H3DP DP DP (w/ depth) DP 100 100 100 100 100 96 98 100 98 83 77 78 100 82 79 100 100 84 79 100 100 64 64 85 43 37 57 100 76 70 95 MetaWorld (Medium) MetaWorld (Hard) Peg Insert Side Sweep Sweep Into Assembly Hand Insert Pick Out of Hole Pick Place Push 98 62 53 100 96 98 100 100 74 100 61 100 100 100 100 100 73 75 37 40 13 32 30 99 0 0 100 77 79 96 MetaWorld (Hard++) DexArt [55] Shelf Place Diassemble Stick Pull Stick Push Pick Place Wall Laptop Faucet Toilet Bucket 100 20 29 86 96 81 76 83 64 71 61 100 70 100 100 100 55 80 97 81 69 63 80 34 23 20 33 70 58 62 28 27 23 27 Method Tasks Adroit [56] Hammer Door Pen ManiSkill [58] (Rigid) Peg Insertion Side (Grasp) Peg Insertion Side (Align) Pick Cube Turn Faucet H3DP DP DP (w/ depth) DP3 100 95 100 79 69 66 71 83 73 62 81 88 78 93 63 15 7 12 12 85 17 33 10 73 8 23 Method Tasks ManiSkill (Deformable) Excavate Hang Pour Fill Apple Cabinet Storage Dual Bottles Pick (Easy) Dual Bottles Pick (Hard) RoboTwin [59] H3DP DP DP (w/ depth) DP3 38 2 23 15 93 52 78 8 0 7 0 98 36 72 12 98 73 2 55 48 53 33 55 Method Tasks RoboTwin Block Handover Block Hammer Beat Diverse Bottles Pick Pick Apple Messy Tool Adjust H3DP DP DP (w/ depth) DP3 70 28 0 85 85 0 0 47 25 0 2 30 35 0 7 8 45 0 32 53 28 25 42 Average 75.618.6 48.123.1 52.822.2 59.324.9 22 Table 20: Success rates of experts on 44 simulation tasks. We evaluate 200 episodes for each task. For ManiSkill tasks, the demonstrations are provided officially, and we record the success rates as 100%. The final average result is obtained by averaging across all benchmarks. Method Tasks MetaWorld [57] (Medium) Basketball Bin Picking Box Close Coffee Pull Coffee Push Hammer Soccer Push Wall Expert 100.0 97.0 90.0 100. 100.0 100.0 90.5 100.0 Method Tasks MetaWorld (Medium) MetaWorld (Hard) Peg Insert Side Sweep Sweep Into Assembly Hand Insert Pick Out of Hole Pick Place Push Expert 92.0 100. 90.0 100.0 100.0 100.0 100.0 100. Method Tasks MetaWorld (Hard++) DexArt [55] Shelf Place Diassemble Stick Pull Stick Push Pick Place Wall Laptop Faucet Toilet Bucket Expert 99. 92.5 95.0 100.0 99.5 86.5 58. 66.5 80.0 Method Tasks Adroit [56] Hammer Door Pen ManiSkill [58] (Rigid) Peg Insertion Side (Grasp) Peg Insertion Side (Align) Pick Cube Turn Faucet Expert 99.0 100.0 97.0 100.0 100. 100.0 100.0 Method Tasks ManiSkill (Deformable) Excavate Hang Pour Fill RoboTwin [59] Apple Cabinet Storage Dual Bottles Pick (Easy) Dual Bottles Pick (Hard) Expert 100.0 100.0 100.0 100.0 96. 97.0 55.5 Method Tasks RoboTwin Block Handover Block Hammer Beat Diverse Bottles Pick Pick Apple Messy Tool Adjust Average Expert 98.0 97.0 72.0 88.5 86.5 93. 23 Figure 6: Learning curves of the four methods on 12 randomly sampled diverse simulation tasks. In most tasks, H3DP demonstrates faster convergence, higher final success rates, and lower variance compared to other three methods."
        }
    ],
    "affiliations": [
        "Harbin Institute of Technology",
        "Shanghai AI Lab",
        "Shanghai Qi Zhi Institute",
        "Tsinghua University IIIS"
    ]
}