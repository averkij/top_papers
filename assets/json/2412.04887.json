{
    "paper_title": "Momentum-GS: Momentum Gaussian Self-Distillation for High-Quality Large Scene Reconstruction",
    "authors": [
        "Jixuan Fan",
        "Wanhua Li",
        "Yifei Han",
        "Yansong Tang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "3D Gaussian Splatting has demonstrated notable success in large-scale scene reconstruction, but challenges persist due to high training memory consumption and storage overhead. Hybrid representations that integrate implicit and explicit features offer a way to mitigate these limitations. However, when applied in parallelized block-wise training, two critical issues arise since reconstruction accuracy deteriorates due to reduced data diversity when training each block independently, and parallel training restricts the number of divided blocks to the available number of GPUs. To address these issues, we propose Momentum-GS, a novel approach that leverages momentum-based self-distillation to promote consistency and accuracy across the blocks while decoupling the number of blocks from the physical GPU count. Our method maintains a teacher Gaussian decoder updated with momentum, ensuring a stable reference during training. This teacher provides each block with global guidance in a self-distillation manner, promoting spatial consistency in reconstruction. To further ensure consistency across the blocks, we incorporate block weighting, dynamically adjusting each block's weight according to its reconstruction accuracy. Extensive experiments on large-scale scenes show that our method consistently outperforms existing techniques, achieving a 12.8% improvement in LPIPS over CityGaussian with much fewer divided blocks and establishing a new state of the art. Project page: https://jixuan-fan.github.io/Momentum-GS_Page/"
        },
        {
            "title": "Start",
            "content": "4 2 0 2 6 ] . [ 1 7 8 8 4 0 . 2 1 4 2 : r Momentum-GS: Momentum Gaussian Self-Distillation for High-Quality Large Scene Reconstruction Jixuan Fan1,, Wanhua Li2,, Yifei Han1, Yansong Tang1,(cid:12) 1Tsinghua Shenzhen International Graduate School, Tsinghua University 2Harvard University fjx23@mails.tsinghua.edu.cn, wanhua@seas.harvard.edu, hyf23@mails.tsinghua.edu.cn tang.yansong@sz.tsinghua.edu.cn Figure 1. Comparison of reconstruction results on the Rubble [50] dataset. Momentum-GS reconstructs finer details, such as the clear structure of the vehicle in the zoomed-in view. Additionally, our method produces smoother transitions across blocks, demonstrating better consistency and avoiding the noticeable lighting discrepancies observed in other Gaussian-based methods."
        },
        {
            "title": "Abstract",
            "content": "3D Gaussian Splatting has demonstrated notable success in large-scale scene reconstruction, but challenges persist due to high training memory consumption and storage overhead. Hybrid representations that integrate implicit and explicit features offer way to mitigate these limitations. However, when applied in parallelized block-wise training, two critical issues arise since reconstruction accuracy deteriorates due to reduced data diversity when training each block independently, and parallel training restricts the number of divided blocks to the available number of GPUs. To address these issues, we propose MomentumGS, novel approach that leverages momentum-based selfdistillation to promote consistency and accuracy across the blocks while decoupling the number of blocks from the physical GPU count. Our method maintains teacher Gaussian decoder updated with momentum, ensuring stable reference during training. This teacher provides each block with global guidance in self-distillation manner, promotEqual contribution. (cid:12) Corresponding authors. ing spatial consistency in reconstruction. To further ensure consistency across the blocks, we incorporate block weighting, dynamically adjusting each blocks weight according to its reconstruction accuracy. Extensive experiments on large-scale scenes show that our method consistently outperforms existing techniques, achieving 12.8% improvement in LPIPS over CityGaussian with much fewer divided blocks and establishing new state of the art. Project page: https://jixuan-fan.github.io/Momentum-GS Page/ 1. Introduction Large-scale 3D scene reconstruction is essential for wide range of applications, including autonomous driving [24, 40, 48, 57], virtual reality [15, 17], environmental monitoring [30, 56], and aerial surveying [5, 6, 46]. The ability to accurately reconstruct large, complex scenes from collections of images is critical for creating realistic, navigable 3D models and supporting high-quality visualization, analysis, and simulation [8, 18, 21, 32]. 3D Gaussian Splatting (3D-GS) [19] has recently gained 1 promising approach to address memory and storage limitations by combining implicit and explicit features. To manage the complexity of large scenes, these representations integrate dense voxel grids or anchor-based structures with sparse 3D Gaussian fields. These methods typically use MLP as the Gaussian decoder, enabling the generation of neural Gaussians that achieve high reconstruction accuracy while ensuring efficient inference. The decoded Gaussians adapt dynamically to different viewing angles, distances, and scene details. For instance, in Scaffold-GS [33], during inference, the prediction of neural Gaussians is restricted to anchors within the visible frustum, and trivial Gaussians are filtered out based on opacity using learned selection process. This approach enables rendering speeds comparable to the original 3D-GS. Additionally, neural Gaussians are generated on-the-fly within the view frustum, allowing each anchor to adaptively predict Gaussians for diverse viewing directions and distances in real time. This adaptive mechanism enhances the robustness of novel view synthesis, delivering high-quality renderings across various perspectives while keeping acceptable computational overhead. However, applying hybrid representations in parallelized reconstruction for large 3D scenes presents two main challenges. First, training each block independently limits data diversity within each blocks Gaussian decoder, reducing reconstruction quality and producing separate models that cannot be merged due to their independent Gaussian decoders, as illustrated in Fig. 2 (a). In contrast, parallel training with shared Gaussian decoder, as in Fig. 2 (b), allows for merging the trained models but constrains scalability, as the number of blocks is limited by the available GPUs. These limitations underscore the need for an approach that balances inter-block consistency with scalability. To overcome these limitations, we propose MomentumGS, novel approach that combines the benefits of hybrid representations with strategy tailored to meet the unique demands of large-scale scene reconstruction. Our method decouples the number of blocks from GPU constraints, allowing flexible scaling of reconstruction tasks. This is achieved by periodically sampling blocks from set of blocks and distributing them across GPUs. To enhance consistency between blocks, we introduce scene momentum self-distillation, where teacher Gaussian decoder, updated with momentum, provides consistent global guidance to each block, as depicted in Fig. 2 (c). This framework encourages collaborative learning across blocks, ensuring that each block benefits from the broader context of the entire scene. Additionally, we introduce reconstructionguided block weighting, dynamic mechanism that adjusts the emphasis on each block based on its reconstruction quality. This adaptive weighting enables the shared decoder to prioritize underperforming blocks, enhancing global consistency and preventing convergence to local minima. Figure 2. Comparison of three approaches for using hybrid representations to reconstruct large-scale scenes in divideand-conquer manner. Examples with two blocks: (a) Independent training of each block, resulting in separate models that cannot be merged due to independent Gaussian Decoders, complicating rendering; (b) Parallel training with shared Gaussian decoder, allowing merged output but limited by GPU availability; (c) Our approach with Momentum Gaussian Decoder, providing global guidance to each block and improving consistency across blocks. attention for its high reconstruction quality and fast rendering speed, outperforming NeRF-based methods[2, 4, 36]. Building on this foundation, recent methods [9, 20, 27, 31] have further enhanced its performance on large-scale scenes. To handle large environments more efficiently, these approaches often employ divide-and-conquer strategy that partitions large scene into multiple independent blocks, allowing for multi-GPU training across these blocks. This method facilitates scalable training for complex, expansive reconstructions. However, representing millions of Gaussians explicitly creates substantial memory and storage demands [31], limiting the scalability of 3D-GS for extensive scenes. Additionally, due to unavoidable factors in large scene capture, such as lighting variations, auto-exposure adjustments, or inaccuracies in camera poses [23], independently training each block often disregards inter-block relationships, leading to inconsistencies across block boundaries. This issue can result in visible transitions, as seen in Fig. 1 with methods like CityGaussian[31], where abrupt lighting variations are incorrectly rendered. Addressing these concerns has become core focus in advancing the field of 3D scene reconstruction. Hybrid representations [26, 33, 43] have emerged as 2 To thoroughly evaluate the effectiveness of the proposed method, we conduct extensive experiments on five challenging large-scale scenes [25, 28, 50], including Building, Rubble, Residence, Sci-Art, and MatrixCity. Our MomentumGS achieves substantial improvements, demonstrating 12.8% gain in LPIPS over CityGaussian [31] while utilizing much fewer divided blocks. In summary, our contributions are: 1. We introduce scene momentum self-distillation to enhances Gaussian decoder performance and decouples the number of divided blocks from the number of GPUs, enabling scalable parallel training. 2. Our approach incorporates reconstruction-guided block weighting, dynamically adjusting block emphasis based on reconstruction quality to ensure focused improvement on weaker blocks, enhancing overall consistency. 3. Our approach, Momentum-GS, achieves better reconstruction quality than state-of-the-art methods, highlighting the strong potential of hybrid representations for large-scale scene reconstruction. 2. Related work Neural Rendering. Neural Radiance Fields (NeRF)[36] have pioneered breakthrough in novel view synthesis by representing 3D scene as continuous volumetric function, where each point along an emitted ray is sampled to produce color and density values. NeRF optimizes this representation by training neural network on large sets of posed images, enabling realistic image generation from novel viewpoints. Numerous extensions [2 4, 34, 37, 39, 41, 42, 49, 54] have been developed to improve various aspects of NeRF, including its efficiency and scalability. However, NeRFs require intensive sampling along rays for accurate results, leading to high computational costs and prolonged training and inference times. 3D Gaussian Splatting[19] has emerged as promising alternative, leveraging Gaussian splats for efficient scene representation. Compared to NeRFs, 3D-GS significantly reduces sampling requirements while maintaining high fidelity, making it more suitable for real-time applications. Another approach, hybrid representation, combines explicit and implicit elements to benefit from the strengths of both[26, 38, 43, 45, 51]. Often constructed on dense, uniform voxel grids, hybrid representations leverage mix of methods to improve scene reconstruction. For instance, K-Planes [14] uses planar factorization to represent multidimensional scenes, supporting efficient memory use and applying priors like temporal smoothness. Plenoxels [13] adopts sparse 3D grid with spherical harmonics, bypassing neural networks to directly optimize photorealistic view synthesis from images, achieving significant speedups over traditional radiance fields. Scaffold-GS [33] builds on 3D Gaussian Splatting by using anchor points to distribute local 3D Gaussians and predict their attributes dynamically based on viewing direction and distance. These hybrid approaches showcase the advantages of combining explicit and implicit elements for scalable, efficient scene reconstruction. Large Scene Reconstruction. Large-scale scene reconstruction has long history, with traditional methods often relying on Structure-from-Motion (SfM) [1, 47] to estimate camera poses and create sparse point cloud from image collections. Subsequent methods, such as MultiView Stereo (MVS), expanded on this foundation to produce denser reconstructions, advancing the capability of photogrammetry systems to handle large scenes. With the advent of Neural Radiance Fields (NeRF) [36], shift toward neural representations for photo-realistic view synthesis has enabled more detailed scene reconstructions. Many NeRF-based approaches[23, 35, 48, 50, 55, 61], use similar divide-and-conquer approach, representing each block independently to facilitate scalable reconstruction. However, these methods still face challenges in rendering speed and consistency across scene segments. Recently, 3D Gaussian Splatting[19] has emerged as promising alternative, offering real-time rendering with high visual fidelity. Numerous methods extend 3D-GS to large-scale scenes by enhancing its scalability and efficiency[7, 10 12, 16, 22, 29, 44, 52, 58]. Some methods[9, 20, 27, 31, 59] partition these large scenes into independent blocks for parallel training, allowing for efficient processing and reconstruction . VastGaussian and CityGaussian, by employing divide-and-conquer approach to reconstruct large-scale scenes, effectively ensure training convergence, though they lack cross-block interaction, which may limit consistency. DOGS introduces distributed training method that accelerates 3D-GS through scene decomposition and ADMM, while not focusing on optimizing the Gaussian representation for large-scale scenes. These recent 3D-GS-based methods demonstrate the potential of 3D Gaussian representations for scalable, high-quality large scene reconstruction, though challenges remain in achieving seamless transitions and efficient memory usage. 3. Methods Overview. Hybrid representations have demonstrated success in small, object-centric scenes. However, when applied to parallel training in divide-and-conquer manner for larger environments, they encounter fundamental dilemma. In this paper, we leverage hybrid representations for large-scale scene reconstruction, harnessing their high reconstruction capability while effectively decoupling the number of blocks from the physical GPU count. Section 3.1 introduces the essential foundations of 3D-GS. Section 3.2 then explores how Scene Momentum Self-Distillation effectively addresses the challenges of scaling hybrid repre3 Figure 3. Overview of the proposed Momentum-GS. Our method begins by dividing the scene into multiple blocks (left), periodically sampling subset of blocks (e.g., 4 blocks) and assigning them to available GPUs for parallel processing. The momentum Gaussian decoder provides stable global guidance to each block, ensuring consistency across blocks. To align the online Gaussians with the momentum Gaussian decoder, consistency loss is applied. During splatting, predicted images are compared with ground truth images, and the resulting reconstruction loss is used to update the shared online Gaussian decoder. Additionally, reconstruction-guided block weighting dynamically adjusts the emphasis on each block, prioritizing underperforming blocks to enhance overall scene consistency. sentations to large scenes. Lastly, Sec. 3.3 presents the Reconstruction-guided Block Weighting strategy, which enhances global scene consistency by dynamically adjusting each blocks weight based on its reconstruction quality. 3.1. Preliminaries 3D-GS offers an efficient solution for accurate scene reconstruction by leveraging the differentiable properties of Gaussian representations along with tile-based rendering. It models each 3D scene point as an anisotropic Gaussian, allowing for streamlined rendering through projection and blending without the computational overhead of dense ray marching typical in traditional volumetric methods. Each 3D point is represented as Gaussian function centered at µ R3, where is the spatial position, µ is the center, and Σ defines the Gaussians shape and orientation: G(x) = 1 2 (xµ)Σ1(xµ). (1) Rendering projects each 3D Gaussian onto the 2D image plane, resulting in 2D Gaussian G(x), where represents pixel. The projected Gaussian contributes to pixel color via alpha blending: C(x) = (cid:88) iN i1 (cid:89) ciσi (1 σj), j= (2) where is the set of Gaussians affecting x, ci is the color in view-dependent spherical harmonics form, and σi = i(x) is the opacity with αi as learnable parameter. αiG The training of Gaussians uses differentiable rendering to refine Gaussian parameters, starting from an initial point cloud. Gaussians are optimized based on image reconstruction error, with operations like cloning, densifying, and pruning to improve coverage and accuracy. For large scenes, the high Gaussian count presents memory and computational challenges, managed by controlling the active Gaussians during rendering. 3.2. Scene-Aware Momentum Self-Distillation Hybrid representations face fundamental challenge when applied to parallel training in divide-and-conquer approach. Specifically, the limitation of GPU availability restricts the number of blocks that can be processed simultaneously, reducing scalability, while the need for data diversity to maintain the Gaussian decoders predictive accuracy remains critical. To address these challenges, we propose Scene Momentum Self-Distillation, method that both decouples the block count from GPU limitations and enhances the Gaussian decoders robustness through improved data diversity. Our method ensures that the Gaussian decoder benefits from broader range of data, enabling more accurate and consistent predictions across large scenes. In our approach, we train each block simultaneously in parallel, with all blocks sharing single Gaussian decoder. During each forward pass, each block randomly selects viewpoint from its assigned data and uses the shared Gaussian decoder to predict the Gaussian parameters accurately. These predicted parameters are then used to render the corresponding image, which is compared to the ground truth 4 to calculate the reconstruction loss. We optimize the learnable parameters using loss function that combines the L1 loss on rendered pixel colors with an SSIM[53] term LSSIM, aiming to improve structural similarity: Lrecons = L1 + λSSIMLSSIM, (3) where λSSIM is weighting factor that balances the contributions of the L1 and SSIM terms. The gradients from each block are accumulated into the shared Gaussian decoder, allowing it to learn from the full range of scene information. To address the GPU-bound limitation on block count, our approach periodically loads single block onto the GPU, training multiple blocks on each GPU sequentially. After specified interval, we switch to different block to continue training. To reduce I/O overhead from switching, we use relatively large interval, switching blocks every 500 iterations. This method effectively decouples the block count from the number of GPUs, enabling scalable increases in the number of blocks as scene complexity grows. In order to maintain coherence across staggered training blocks and enhance global consistency, we incorporate momentum teacher Gaussian decoder Dt alongside shared student Gaussian decoder Ds. Let denote the index of each parallel training block, with the parameters of the teacher and student Gaussian decoders represented by θt and θs, respectively. We employ self-supervised strategy to stabilize the teacher Gaussian decoder Dt via momentum updates, mitigating inconsistencies introduced by staggered training. The teacher decoder thus serves as stable global reference, guiding the student decoder through consistency loss applied between the two. More formally, the parameters θt of the teacher Gaussian decoder are updated using momentum-based formula that ensures temporal stability: θt θt + (1 m) θs, (4) where is the momentum coefficient, set to 0.9 to balIf is too close to ance stability and update speed. 1, the decoder updates too slowly, hindering reconstruction efficiency, while smaller may lead to instability due to excessive fluctuations in the teacher decoder. This momentum-based update ensures that the teacher Gaussian decoder evolves smoothly, providing stable and consistent guidance to the student decoder across all blocks. For each block, Gaussian parameters are predicted by both the teacher and student decoders, with consistency loss applied to align the student decoder with the global guidance from the teacher. This approach leverages increased data diversity while decoupling the number of blocks from the GPU count, allowing scalability to arbitrarily large scenes. The consistency loss is computed as the mean squared error between the predictions of the teacher and student Gaussian decoders for each block B: Lconsistency = Dm(fb, vb; θt) Do(fb, vb; θs)2, (5) where fb represents the anchor feature and vb the relative viewing direction for each sample within block B. This loss encourages the student decoder Do to progressively align with the stable global guidance provided by the teacher decoder Dm, promoting spatial consistency across different blocks throughout the reconstruction process. Thus, the total loss function is defined as: = L1 + λSSIMLSSIM + λconsistencyLconsistency, (6) where λconsistency is weighting factor that balances the impact of the consistency loss relative to the reconstruction loss. This combined loss ensures that the model not only reconstructs the scene accurately but also maintains global spatial coherence across blocks. 3.3. Reconstruction-guided Block Weighting In order to balance training progress across blocks and mitigate issues arising from uneven initial scene partitioning, we introduce Reconstruction-guided Block Weighting. This method dynamically adjusts weights based on each blocks reconstruction quality, enhancing consistency by giving priority to blocks with lower reconstruction accuracy. To monitor and adjust the reconstruction performance of each block, we maintain table that tracks key reconstruction metrics, specifically PSNR (Peak Signal-to-Noise Ratio) and SSIM (Structural Similarity Index). These metrics provide quantitative measures of reconstruction quality, with higher values indicating better visual fidelity. To ensure that these metrics reflect stable performance across training iterations, we update them using momentumbased approach, which smooths fluctuations and provides more reliable indication of each blocks progress. Using these momentum-smoothed metrics, we identify the block with the highest reconstruction performance, labeling its PSNR and SSIM values as PSNRmax and SSIMmax, respectively. These reference values serve as benchmarks for evaluating the relative accuracy of each block. For every block in the scene, we calculate deviations δp and δs to quantify how closely its reconstruction aligns with the highest-performing block. Specifically, the PSNR deviation δp is obtained by subtracting the current blocks PSNR from PSNRmax. δs is derived similarly. With these deviations calculated, we assign each block weight wi that reflects its relative reconstruction performance. The weight wi is constructed to resemble Gaussian distribution, placing greater emphasis on blocks with larger deviations from the best-performing block. By prioritizing blocks with lower reconstruction accuracy, this approach directs the models attention to underperforming blocks, helping to improve overall consistency across 5 the scene. Additionally, wi is capped within range slightly above one, which prevents excessively high adjustments, ensuring stable training dynamics and avoiding overpenalization of blocks with moderate deviations. wi = 2 exp (cid:32) + λ δ2 δ2 2σ2 (cid:33) , (7) This design guiding the Gaussian decoder to focus on the global scene rather than converging on blocks with locally high-quality reconstructions. Consequently improves consistency across all blocks, ultimately enhancing the overall scene reconstruction quality. 4. Experiments 4.1. Experimental Setup Dataset and Metrics. We conducted experiments on five large-scale scenes across three datasets: Building and Rubble from the Mill19[50] dataset, Residence and Sci-Art from the UrbanScene3D[28] dataset, and Aerial from small city region within the MatrixCity[25] dataset. Each of these datasets includes thousands of high-resolution images. We used the same scene partitioning as MegaNeRF[50] and downsampled all images by 4 times, following previous methods[27, 31, 50] for fair comparison. We evaluated reconstruction accuracy using PSNR, SSIM[53], and LPIPS[60] metrics, and additionally reported the storage size (GB) to assess model compactness. Since VastGaussian[27] and DOGS[9] are not open-sourced yet, we were unable to include them in the storage size comparison. Please refer to the supplementary material for additional comparisons, including VRAM usage, storage size, and the number of divided blocks. Compared methods. We compare our method with six methods, categorized into NeRF-based methods, including Mega-NeRF[50] and Switch-NeRF[35], and gaussianbased methods, including 3D-GS[19], VastGaussian[27], CityGaussian[31], and DOGS[9]. Notably, DOGSs settings differ slightly from those of the other methods: DOGS downsamples all images by factor of 6+ and trains for 80,000 iterations, as opposed to the 4 times downsampling and 60,000 training iterations in prior works, which may introduce some performance advantages. Implementation. Following previous methods[27, 31, 35, 50], we downsample all images by factor of 4 and train for total of 60,000 iterations. Anchor points are adjusted from iteration 300 to 30,000 with an interval of 25 iterations. To ensure fair comparisons, we adopt the same scene partitioning strategy as CityGaussian but use notably fewer blocks. Specifically, we divide all scenes into 8 blocks, whereas CityGaussian partitions Building, Rubble, Residence, Sci-Art, and MatrixCity into 20, 9, 20, 9, and 36 blocks, respectively. VastGaussian applies color correction to the rendered images before evaluating metrics, resulting in evidently higher results. To ensure comparability across methods, we report the results of VastGaussian based on reproduced version[9] without the use of color correction and decoupled appearance encoding. 4.2. Results Analysis Quantitative Results. In Tab. 1, we report the PSNR, SSIM, and LPIPS metrics across four large-scale scenes. Our Momentum-GS achieves the best performance in SSIM and LPIPS across all scenes, significantly outperforming other methods in terms of perceptual quality. These results suggest that Momentum-GS effectively balances fine detail preservation with high rendering quality. Notably, NeRFbased methods achieve higher PSNR on the Sci-Art dataset. We observe that the Sci-Art dataset suffers from noticeable blur, likely due to out-of-focus capture conditions. NeRFbased methods tend to produce smoother, often slightly blurred reconstructions, which may align more closely with the inherent characteristics of the Sci-Art data, resulting in artificially elevated PSNR scores. However, when considering SSIM and LPIPS, Gaussian-based methods, including our Momentum-GS, outperform NeRF-based approaches, suggesting superior ability to preserve structural and perceptual details across different scenes. Visualization Results. In Fig. 4, we provide visual comparisons of reconstruction results across different scenes. Compared to other methods, our Momentum-GS demonstrates superior detail preservation and produces sharper, more realistic images. While other methods often suffer from noticeable blurring or loss of structure in complex areas, our approach achieves clean and well-defined renderings across all scenes. These results highlight the effectiveness of Momentum-GS in capturing fine-grained details and maintaining visual clarity. 4.3. Ablation Studies independent training. Parallel training vs. In Tab. 4, we demonstrate that parallel training achieves better reconstruction accuracy compared to independent training when the scene is divided into the same number of blocks, due to the increased data diversity accessible to each blocks Gaussian decoder. However, direct parallel training is limited by the constraint that the number of blocks is tied to the number of physical GPUs. As result, independent training can achieve further accuracy improvements by increasing the number of blocks, whereas simple parallel training cannot. Specifically, we show that dividing the scene into 8 blocks yields better results than 4 blocks in independent training. To increase data diversity in training the Gaussian decoder while decoupling the block count from the GPU count, we introduce scene momentum self-distillation, 6 Scene Metrics Mega-NeRF [50] Switch-NeRF [35] 3D-GS [19] VastGaussian [27] CityGaussian [31] DOGS [9] Momentum-GS (Ours) Building Rubble Residence Sci-Art PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS 20.93 21.54 20.46 21.80 21.55 22. 23.23 0.547 0.579 0.720 0.728 0.778 0.759 0.815 0.504 0.474 0.305 0.225 0.246 0.204 0.194 24.06 24.31 25.47 25.20 25.77 25. 25.93 0.553 0.562 0.777 0.742 0.813 0.765 0.827 0.516 0.496 0.277 0.264 0.228 0.257 0.201 22.08 22.57 21.44 21.01 22.00 21. 22.21 0.628 0.654 0.791 0.699 0.813 0.740 0.818 0.489 0.457 0.236 0.261 0.211 0.244 0.197 25.60 26.52 21.05 22.64 21.39 24. 23.02 0.770 0.795 0.830 0.761 0.837 0.804 0.856 0.390 0.360 0.242 0.261 0.230 0.219 0.205 Table 1. Quantitative comparison of our Momentum-GS against prior methods across four large-scale scenes. We present metrics for PSNR, SSIM, and LPIPS on test views. The best and second best scores are highlighted. denotes without applying the decoupled appearance encoding, indicates that the experimental setting has minor differences from other methods. Figure 4. Qualitative comparisons of our Momentum-GS and prior methods across four large-scale scenes. Red insets highlight patches that reveal notable visual differences between these methods. Our method (d) demonstrates better fidelity in capturing fine details, maintaining structural consistency, and accurately representing textures. This results in visual reconstructions that are closer to the ground truth (e) compared to Mega-NeRF (a), 3D-GS (b), and CityGaussian (c), which exhibit artifacts, blurring, or inconsistencies in these areas. which significantly improves accuracy compared to training 8 blocks independently. Additionally, reconstruction quality can be further enhanced by incorporating reconstructionguided block weighting (denoted as Full in Tab. 4). Block weighting. In Tab. 3, we evaluate different methods for measuring the reconstruction quality of each block. Our results show that incorporating both PSNR and SSIM yields better accuracy compared to relying solely on either metric. Scalability. We evaluated our method on various numbers of divided blocks, keeping the number of GPUs constant as four. The results, shown in Table 5, demonstrate consistent improvement in reconstruction accuracy as the number of blocks increases. This indicates that our method can effectively scale to train on any desired number of blocks, even with limited GPU resources. Our approach outperforms CityGaussian, achieving superior accu7 Figure 5. Qualitative comparisons of our Momentum-GS and other methods on large-scale urban scene MatrixCity. Our method (c) demonstrates greater detail preservation in challenging regions such as building facades and edges, closely matching the ground truth (d). In contrast, 3D-GS (a) and CityGaussian (b) exhibit obvious artifacts, blurring, or loss of structural detail in these areas. Method PSNR SSIM LPIPS #Block Mem (GB) Training strategy #Block PSNR SSIM LPIPS 3D-GS [19] CityGaussian [31] 23.67 27.46 0.735 0.865 0.384 0.204 Momentum-GS (Ours) 28. 0.880 0.179 1 36 8 2.40 5.40 2. Table 2. Quantitative comparison of our Momentum-GS against prior methods on the MatrixCity dataset. We report PSNR, SSIM, and LPIPS on test views, with the best results highlighted. The number of divided blocks and the storage size are additionally reported for each method. Models w/ PSNR w/ SSIM PSNR SSIM LPIPS 23.03 23.00 0.812 0. 0.196 0.203 Full (PSNR + SSIM) 23.23 0.815 0.194 baseline w/ Parallel training w/ Independent training w/ Independent training w/ momentum self-distillation Full 1 4 4 8 8 8 21.52 22.62 22.50 22.68 23.05 0.741 0.790 0.787 0.799 0.807 0.280 0.227 0.227 0.212 0. 23.23 0.815 0.194 Table 4. Ablation study on different training strategies."
        },
        {
            "title": "Method",
            "content": "#Block"
        },
        {
            "title": "PSNR SSIM LPIPS",
            "content": "CityGaussian Momentum-GS (Ours) Momentum-GS (Ours) Momentum-GS (Ours) 32 4 8 16 27.46 27.87 28.01 28.02 0.865 0.869 0.880 0.883 0.204 0.203 0.179 0.170 Table 5. Ablation study on the number of blocks used in our Momentum-GS compared to CityGaussian. Table 3. Ablation study on different strategy of measuring the reconstruction quality in block weighting. 5. Conclusion racy with fewer blocks, highlighting its scalability and effectiveness in large-scale scene reconstruction. In this paper, we have introduced Momentum-GS, novel momentum-based self-distillation framework that notably enhances 3D Gaussian Splatting for large-scale scene reconstruction. The core of Momentum-GS is momentum8 updated teacher Gaussian decoder, which serves as stable global reference to guide parallel training blocks, effectively promoting spatial consistency and coherence across the reconstructed scene. We further introduce reconstruction-guided block weighting mechanism, which dynamically adjusts the emphasis on each block based on reconstruction quality, further improving overall consistency. Our approach leverages hybrid representations, integrating both implicit and explicit features, to enable flexible scaling that decouples the number of blocks from GPU constraints. Experimental results demonstrate the strong capability of hybrid representations and momentum-based selfdistillation for robust, large-scale 3D scene reconstruction."
        },
        {
            "title": "References",
            "content": "[1] Sameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian Simon, Brian Curless, Steven Seitz, and Richard Szeliski. Building rome in day. Communications of the ACM, 54 (10):105112, 2011. 3 [2] Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul P. Srinivasan. Mip-nerf: multiscale representation for anti-aliasing neural radiance fields. In ICCV, pages 58555864, 2021. 2, 3 [3] Jonathan Barron, Ben Mildenhall, Dor Verbin, Pratul Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded In CVPR, pages 5470 anti-aliased neural radiance fields. 5479, 2022. [4] Jonathan Barron, Ben Mildenhall, Dor Verbin, Pratul Srinivasan, and Peter Hedman. Zip-nerf: Anti-aliased gridbased neural radiance fields. In ICCV, pages 1969719705, 2023. 2, 3 [5] Ilker Bozcan and Erdal Kayacan. Au-air: multi-modal unmanned aerial vehicle dataset for low altitude traffic surveillance. In 2020 IEEE International Conference on Robotics and Automation (ICRA), pages 85048510. IEEE, 2020. 1 [6] Guikun Chen and Wenguan Wang. survey on 3d gaussian splatting. arXiv preprint arXiv:2401.03890, 2024. 1 [7] Junyi Chen, Weicai Ye, Yifan Wang, Danpeng Chen, Di Huang, Wanli Ouyang, Guofeng Zhang, Yu Qiao, and Tong He. Gigags: Scaling up planar-based 3d gaussians for large scene surface reconstruction. arXiv preprint arXiv:2409.06685, 2024. [8] Timothy Chen, Ola Shorinwa, Joseph Bruno, Javier Yu, Weijia Zeng, Keiko Nagami, Philip Dames, and Mac Schwager. Splat-nav: Safe real-time robot navigation in gaussian splatting maps. arXiv preprint arXiv:2403.02751, 2024. 1 [9] Yu Chen and Gim Hee Lee. Dogs: Distributed-oriented gaussian splatting for large-scale 3d reconstruction via gaussian consensus. In NeurIPS, 2024. 2, 3, 6, 7, 1 [10] Jiadi Cui, Junming Cao, Yuhui Zhong, Liao Wang, Fuqiang Zhao, Penghao Wang, Yifan Chen, Zhipeng He, Lan Xu, Yujiao Shi, et al. Letsgo: Large-scale garage modeling and rendering via lidar-assisted gaussian primitives. arXiv preprint arXiv:2404.09748, 2024. 3 [11] Xiao Cui, Weicai Ye, Yifan Wang, Guofeng Zhang, Wengang Zhou, Tong He, and Houqiang Li. Streetsurfgs: Scalable urban street surface reconstruction with planar-based gaussian splatting. arXiv preprint arXiv:2410.04354, 2024. [12] Guofeng Feng, Siyan Chen, Rong Fu, Zimu Liao, Yi Wang, Tao Liu, Zhilin Pei, Hengjie Li, Xingcheng Zhang, and Bo Dai. Flashgs: Efficient 3d gaussian splatting for large-scale and high-resolution rendering. arXiv preprint arXiv:2408.07967, 2024. 3 [13] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: In CVPR, pages Radiance fields without neural networks. 55015510, 2022. [14] Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahbæk Warburg, Benjamin Recht, and Angjoo Kanazawa. K-planes: Explicit radiance fields in space, time, and appearance. In CVPR, pages 1247912488, 2023. 3 [15] Jiaming Gu, Minchao Jiang, Hongsheng Li, Xiaoyuan Lu, Guangming Zhu, Syed Afaq Ali Shah, Liang Zhang, and Mohammed Bennamoun. Ue4-nerf: Neural radiance field for real-time rendering of large-scale scene. NeurIPS, 36, 2024. 1 [16] Changjian Jiang, Ruilan Gao, Kele Shao, Yue Wang, Rong Xiong, and Yu Zhang. Li-gs: Gaussian splatting with lidar incorporated for accurate large-scale reconstruction. arXiv preprint arXiv:2409.12899, 2024. 3 [17] Ying Jiang, Chang Yu, Tianyi Xie, Xuan Li, Yutao Feng, Huamin Wang, Minchen Li, Henry Lau, Feng Gao, Yin Yang, et al. Vr-gs: physical dynamics-aware interactive In ACM SIGgaussian splatting system in virtual reality. GRAPH 2024 Conference Papers, pages 11, 2024. 1 [18] Rui Jin, Yuman Gao, Yingjian Wang, Haojian Lu, and Fei Gao. Gs-planner: gaussian-splatting-based planning arXiv framework for active high-fidelity reconstruction. preprint arXiv:2405.10142, 2024. 1 [19] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. TOG, 42(4), 2023. 1, 3, 6, 7, 8 [20] Bernhard Kerbl, Andreas Meuleman, Georgios Kopanas, Michael Wimmer, Alexandre Lanvin, and George Drettakis. hierarchical 3d gaussian representation for real-time rendering of very large datasets. TOG, 43(4):115, 2024. 2, 3 [21] Xiaohan Lei, Min Wang, Wengang Zhou, and Houqiang Li. Gaussnav: Gaussian splatting for visual navigation. arXiv preprint arXiv:2403.11625, 2024. [22] Bingling Li, Shengyi Chen, Luchao Wang, Kaimin Liao, Sijie Yan, and Yuanjun Xiong. Retinags: Scalable training for dense scene rendering with billion-scale 3d gaussians. arXiv preprint arXiv:2406.11836, 2024. 3 [23] Ruilong Li, Sanja Fidler, Angjoo Kanazawa, and Francis Williams. NeRF-XL: Scaling nerfs with multiple GPUs. In ECCV, 2024. 2, 3 [24] Wei Li, CW Pan, Rong Zhang, JP Ren, YX Ma, Jin Fang, FL Yan, QC Geng, XY Huang, HJ Gong, et al. Aads: Augmented autonomous driving simulation using data-driven algorithms. Science robotics, 4(28):eaaw0863, 2019. 1 [25] Yixuan Li, Lihan Jiang, Linning Xu, Yuanbo Xiangli, Zhenzhi Wang, Dahua Lin, and Bo Dai. Matrixcity: large-scale 9 city dataset for city-scale neural rendering and beyond. In ICCV, pages 32053215, 2023. 3, 6, 1 [26] Zhuopeng Li, Yilin Zhang, Chenming Wu, Jianke Zhu, and Liangjun Zhang. Ho-gaussian: Hybrid optimization of 3d gaussian splatting for urban scenes. arXiv preprint arXiv:2403.20032, 2024. 2, [27] Jiaqi Lin, Zhihao Li, Xiao Tang, Jianzhuang Liu, Shiyong Liu, Jiayue Liu, Yangdi Lu, Xiaofei Wu, Songcen Xu, Youliang Yan, and Wenming Yang. Vastgaussian: Vast 3d gaussians for large scene reconstruction. In CVPR, 2024. 2, 3, 6, 7, 1 [28] Liqiang Lin, Yilin Liu, Yue Hu, Xingguang Yan, Ke Xie, and Hui Huang. Capturing, reconstructing, and simulating: the urbanscene3d dataset. In ECCV, pages 93109, 2022. 3, 6, 1 [29] Jinpeng Liu, Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Ying Shan, and Yansong Tang. Novelgs: Consistent novel-view denoising via large gaussian reconstruction model. arXiv preprint arXiv:2411.16779, 2024. 3 [30] Shuhong Liu, Xiang Chen, Hongming Chen, Quanfeng Xu, and Mingrui Li. Deraings: Gaussian splatting for arXiv preprint enhanced scene reconstruction in rainy. arXiv:2408.11540, 2024. 1 [31] Yang Liu, He Guan, Chuanchen Luo, Lue Fan, Naiyan Wang, Junran Peng, and Zhaoxiang Zhang. Citygaussian: Real-time high-quality large-scale scene rendering with gaussians. In ECCV, 2024. 2, 3, 6, 7, 8, 1 [32] Guanxing Lu, Shiyi Zhang, Ziwei Wang, Changliu Liu, Jiwen Lu, and Yansong Tang. Manigaussian: Dynamic gaussian splatting for multi-task robotic manipulation. In ECCV, pages 349366, 2025. [33] Tao Lu, Mulin Yu, Linning Xu, Yuanbo Xiangli, Limin Wang, Dahua Lin, and Bo Dai. Scaffold-gs: Structured 3d gaussians for view-adaptive rendering. In CVPR, pages 2065420664, 2024. 2, 3 [34] Ricardo Martin-Brualla, Noha Radwan, Mehdi SM Sajjadi, Jonathan Barron, Alexey Dosovitskiy, and Daniel Duckworth. Nerf in the wild: Neural radiance fields for unconstrained photo collections. In CVPR, pages 72107219, 2021. 3 [35] Zhenxing Mi and Dan Xu. Switch-nerf: Learning scene decomposition with mixture of experts for large-scale neural radiance fields. In ICLR, 2023. 3, 6, 7, 1 [36] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. 2, 3 [37] Ben Mildenhall, Peter Hedman, Ricardo Martin-Brualla, Pratul Srinivasan, and Jonathan Barron. Nerf in the dark: High dynamic range view synthesis from noisy raw images. In CVPR, pages 1619016199, 2022. 3 [38] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with multiresolution hash encoding. TOG, 41(4):115, 2022. [39] Michael Niemeyer, Jonathan Barron, Ben Mildenhall, Mehdi SM Sajjadi, Andreas Geiger, and Noha Radwan. Regnerf: Regularizing neural radiance fields for view synthesis from sparse inputs. In CVPR, pages 54805490, 2022. 3 [40] Julian Ost, Fahim Mannan, Nils Thuerey, Julian Knodt, and Felix Heide. Neural scene graphs for dynamic scenes. In CVPR, pages 28562865, 2021. 1 [41] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural radiance fields for dynamic scenes. In CVPR, pages 1031810327, 2021. 3 [42] Christian Reiser, Rick Szeliski, Dor Verbin, Pratul Srinivasan, Ben Mildenhall, Andreas Geiger, Jon Barron, and Peter Hedman. Merf: Memory-efficient radiance fields for realtime view synthesis in unbounded scenes. TOG, 42(4):112, 2023. 3 [43] Kerui Ren, Lihan Jiang, Tao Lu, Mulin Yu, Linning Xu, Zhangkai Ni, and Bo Dai. Octree-gs: Towards consistent real-time rendering with lod-structured 3d gaussians. arXiv preprint arXiv:2403.17898, 2024. 2, 3 [44] Xuanchi Ren, Yifan Lu, Hanxue Liang, Jay Zhangjie Wu, Huan Ling, Mike Chen, Francis Fidler, Sanja annd Williams, and Jiahui Huang. Scube: Instant large-scale scene reconstruction using voxsplats. In NeurIPS, 2024. 3 [45] Jiansong Sha, Haoyu Zhang, Yuchen Pan, Guang Kou, and Xiaodong Yi. Nerf-is: Explicit neural radiance fields in semantic space. In Proceedings of the 5th ACM International Conference on Multimedia in Asia, pages 17, 2023. 3 [46] Surendra Pal Singh, Kamal Jain, and Ravibabu Mandla. 3d scene reconstruction from video camera for virtual 3d city modeling. American Journal of Engineering Research, 3(1): 140148, 2014. [47] Noah Snavely, Steven M. Seitz, and Richard Szeliski. Photo Tourism: Exploring Photo Collections in 3D. Association for Computing Machinery, 2023. 3 [48] Matthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Pradhan, Ben Mildenhall, Pratul Srinivasan, Jonathan Barron, and Henrik Kretzschmar. Block-nerf: Scalable large scene neural view synthesis. In CVPR, pages 82488258, 2022. 1, 3 [49] Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li, Brent Yi, Terrance Wang, Alexander Kristoffersen, Jake Austin, Kamyar Salahi, Abhik Ahuja, et al. Nerfstudio: modular framework for neural radiance field development. In ACM SIGGRAPH 2023 Conference Proceedings, pages 112, 2023. 3 [50] Haithem Turki, Deva Ramanan, and Mahadev Satyanarayanan. Mega-nerf: Scalable construction of large-scale nerfs for virtual fly-throughs. In CVPR, pages 1292212931, 2022. 1, 3, 6, 7 [51] Haithem Turki, Vasu Agrawal, Samuel Rota Bul`o, Lorenzo Porzi, Peter Kontschieder, Deva Ramanan, Michael Zollhofer, and Christian Richardt. Hybridnerf: Efficient neuIn CVPR, ral rendering via adaptive volumetric surfaces. pages 1964719656, 2024. 3 [52] Zipeng Wang and Dan Xu. Pygs: Large-scale scene reparXiv resentation with pyramidal 3d gaussian splatting. preprint arXiv:2405.16829, 2024. 3 [53] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. TIP, 13(4):600612, 2004. 5, 6 10 [54] Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Humphrey Shi, and Zhangyang Wang. Sinnerf: Training neural radiance In ECCV, fields on complex scenes from single image. pages 736753, 2022. 3 [55] Linning Xu, Yuanbo Xiangli, Sida Peng, Xingang Pan, Nanxuan Zhao, Christian Theobalt, Bo Dai, and Dahua Lin. Grid-guided neural radiance fields for large urban scenes. In CVPR, pages 82968306, 2023. 3 [56] Daniel Yang, John J. Leonard, and Yogesh Girdhar. Seasplat: Representing underwater scenes with 3d gaussian splatting and physically grounded image formation model. arxiv, 2024. [57] Zhenpei Yang, Yuning Chai, Dragomir Anguelov, Yin Zhou, Pei Sun, Dumitru Erhan, Sean Rafferty, and Henrik Kretzschmar. Surfelgan: Synthesizing realistic sensor data for autonomous driving. In CVPR, pages 1111811127, 2020. 1 [58] Chubin Zhang, Hongliang Song, Yi Wei, Yu Chen, Jiwen Lu, and Yansong Tang. Geolrm: Geometry-aware large reconstruction model for high-quality 3d gaussian generation. arXiv preprint arXiv:2406.15333, 2024. 3 [59] Hanyue Zhang, Zhiliu Yang, Xinhe Zuo, Yuxin Tong, Ying Long, and Chen Liu. Garfield++: Reinforced gaussian radiance fields for large-scale 3d scene reconstruction. arXiv preprint arXiv:2409.12774, 2024. 3 [60] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep In CVPR, pages 586595, features as perceptual metric. 2018. 6 [61] Yuqi Zhang, Guanying Chen, and Shuguang Cui. Efficient large-scale scene representation with hybrid of highresolution grid and plane features. Pattern Recognition, 158: 111001, 2025. 3 11 Momentum-GS: Momentum Gaussian Self-Distillation for High-Quality Large Scene Reconstruction"
        },
        {
            "title": "Supplementary Material",
            "content": "viewing positions, resulting in variable number of Gaussians. Scene Building Rubble Residence Sci-Art MatrixCity Voxels 8.33M 5.09M 6.79M 3.30M 7.08M Table 8. Voxel counts for each scene. Analysis of 6 Downsampling. In alignment with most previous methods [27, 31, 35, 50], we downsampled all images by factor of 4, whereas DOGS [9] applied higher downsampling rate of 6 or more. We observed that downsampling images by factor of 6 produces significantly better results compared to factor of 4, as the reconstruction difficulty decreases (e.g. high-frequency information is reduced) with higher downsampling rates. As shown in Tab. 9, our method achieves much better results when images are downsampled by factor of 6. 7. More Visual Comparisons for comparisons additional visual We provide the Building[50], Rubble[50], Residence[28], and Sci-Art[28] scenes. Our method consistently reconstructs finer details across these scenes. Notably, our approach demonstrates superior ability to reconstruct luminance, as illustrated by the Sci-Art example shown in Fig. 6. While NeRF-based methods are capable of capturing luminance by leveraging neural networks to learn global features such as lighting, they tend to produce blurrier results compared to 3D-GSbased methods. This underscores the effectiveness of our hybrid representation, which combines the strengths of both NeRF-based and 3D-GS-based approaches. 6. Quantitative Evaluation VRAM. We report the peak VRAM usage during inference across five large-scale scenes, as shown in Tab. 6. Despite achieving superior reconstruction quality, our method, Momentum-GS, requires less VRAM compared to the purely 3D-GS-based approach. The VRAM usage, measured in MB, highlights the efficiency of our method. Notably, as scene complexity increases (e.g., in MatrixCity[25]), the advantages of our method become even more pronounced. All experiments were conducted on Nvidia RTX 3090 GPUs with 24 GB of memory. Furthermore, CityGaussian[31] exceeds the 24 GB VRAM limit in certain scenes, such as Building[50] and MatrixCity, resulting in out-of-memory errors. Scene Building Rubble Residence Sci-Art MatrixCity CityGaussian [31] Momentum-GS (Ours) 8977 5830 5527 6494 6419 2726 6647 14677 4616 Table 6. Peak VRAM usage (in MB) during inference. Storage. We report the storage usage across five largescale scenes, as shown in Tab. 7. Leveraging our hybrid representation, our method significantly reduces the number of parameters required for storage compared to purely 3D-GS-based methods. This reduction is especially notable in larger and more complex scenes, such as MatrixCity[25], where the storage savings are most substantial. Notably, as scene complexity increases (e.g., in MatrixCity), the advantages of our method become even more pronounced, demonstrating its effectiveness in handling challenging scenarios. For clarity and consistency, storage usage is reported in GB. Scene Building Rubble Residence Sci-Art MatrixCity CityGaussian [31] Momentum-GS (Ours) 3.07 2.45 (20.2%) 2.22 1.50 (32.7%) 2.49 2.00 (19.7%) 0.88 0.97 5.40 2.08 (61.5%) Table 7. Storage usage (in GB). Number of Voxels. We report the voxel count for each reconstruction model across four large-scale scenes, as shown in Tab. 8. Leveraging our hybrid representation, all Gaussians are dynamically predicted on-the-fly based on the 1 Scene Metrics Building Rubble Residence Sci-Art PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS DOGS [9] (6 ) Momentum-GS (Ours, 4 ) Momentum-GS (Ours, 6 ) 22.73 23.23 23. 0.759 0.815 0.841 0.204 0.194 0.153 25.78 25.93 26.67 0.765 0.827 0.863 0.257 0.201 0.153 21.94 22.21 22. 0.740 0.818 0.849 0.244 0.197 0.150 24.42 23.02 23.20 0.804 0.856 0.876 0.219 0.205 0.156 Table 9. We present metrics for PSNR, SSIM, and LPIPS on test views. The best and second best scores are highlighted. Figure 6. Qualitative comparisons of our Momentum-GS and prior methods across four large-scale scenes."
        }
    ],
    "affiliations": [
        "Harvard University",
        "Tsinghua Shenzhen International Graduate School, Tsinghua University"
    ]
}