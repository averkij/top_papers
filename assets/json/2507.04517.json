{
    "paper_title": "DOTResize: Reducing LLM Width via Discrete Optimal Transport-based Neuron Merging",
    "authors": [
        "Neha Verma",
        "Kenton Murray",
        "Kevin Duh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Model compression offers a promising path to reducing the cost and inaccessibility of large pre-trained models, without significantly compromising their impressive performance. Large Transformer models, including large language models (LLMs), often contain computational redundancy, which can serve as a target for new model compression methods. In this work, we specifically target neuron-level redundancies in model layers by combining groups of similar neurons into fewer neurons. We frame this width reduction as a Discrete Optimal Transport problem, and propose DOTResize, a novel Transformer compression method that uses optimal transport theory to transform and compress model weights. To ensure applicability within the Transformer architecture, we motivate and incorporate entropic regularization and matrix factorization into the transportation maps produced by our method. Unlike pruning-based approaches which discard neurons based on importance measures, DOTResize re-projects the entire neuron width, allowing the retention and redistribution of useful signal across the reduced layer. Empirical results show that compared to simple or state-of-the-art neuron width-pruning techniques, DOTResize can outperform these methods across multiple LLM families and sizes, while achieving measurable reductions in real-world computational cost."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 ] . [ 1 7 1 5 4 0 . 7 0 5 2 : r DOTRESIZE: Reducing LLM Width via Discrete Optimal Transport-based Neuron Merging Neha Verma1 Kenton Murray1,2 Kevin Duh1,2 1Center for Language and Speech Processing 2Human Language Technology Center of Excellence Johns Hopkins University {nverma7, kenton}@jhu.edu, kevinduh@cs.jhu.edu"
        },
        {
            "title": "Abstract",
            "content": "Model compression offers promising path to reducing the cost and inaccessibility of large pre-trained models, without significantly compromising their impressive performance. Large Transformer models, including large language models (LLMs), often contain computational redundancy, which can serve as target for new model compression methods. In this work, we specifically target neuron-level redundancies in model layers by combining groups of similar neurons into fewer neurons. We frame this width reduction as Discrete Optimal Transport problem, and propose DOTRESIZE, novel Transformer compression method that uses optimal transport theory to transform and compress model weights. To ensure applicability within the Transformer architecture, we motivate and incorporate entropic regularization and matrix factorization into the transportation maps produced by our method. Unlike pruning-based approaches which discard neurons based on importance measures, DOTRESIZE re-projects the entire neuron width, allowing the retention and redistribution of useful signal across the reduced layer. Empirical results show that compared to simple or state-of-the-art neuron width-pruning techniques, DOTRESIZE can outperform these methods across multiple LLM families and sizes, while achieving measurable reductions in real-world computational cost."
        },
        {
            "title": "Introduction",
            "content": "High-quality compression techniques for large language models (LLMs) are increasingly important due to the ubiquity of pre-training and success of model and data scaling. Compression can reduce model size and latency, which helps facilitate reduced inference cost, on-device capabilities, and improved access to models by both users and researchers. Although prior work has shown the theoretical success of weight pruning techniques [12, 27], practical gains in on-disk compression and speed-up often depend on specialized hardware and specific tensor implementations. For example, unstructured and semi-structured pruning methods do not yield memory savings without dedicated sparse storage formats. Moreover, while semi-structured pruning can offer inference speed-ups, these benefits are limited to specific GPU architectures and typically come with greater performance degradation than unstructured approaches. Structured pruning techniques provides alternatives that solve some of these issues by design, where entire structures like attention heads, blocks of weight matrices, neurons, or entire layers can be removed, leading to direct inference speed-ups and memory savings. One variety of structured pruning involves reducing the width of weight matrices in model layers, resulting in compressed models with more computational throughput [18, 4]. Parallel to pruning-based methods, complementary line of research has focused on direct parameter intervention that exploits functional invariance to change model parameters without changing the underlying model function. In other words, these interventions involve designing functions π that Preprint. Under review. Figure 1: depiction of our neuron width merging strategy. In panel 1, we demonstrate computing activations from layer in preparation for panel 2, where we select subset of 3 neurons from this layer, and compute pairwise similarities between the activations of the 5 original neurons, and the activations of the subset. In panel 3, we compute the optimal transport map, depicted in green, by optimizing the map according to the similarities and entropic regularization. Finally, we demonstrate replacing layer with the subset of neurons, after transforming its weights with and layer + 1s weights with inv, resulting in new activations. act on model parameters θ such that the model preserves (xθ) = (xπ(θ)). This functional invariance is central to many model merging methods [24, 3], and has recently been explored for single-model compression applications. These applications tend to design functions π such that the model function is approximated due to the compression objective, i.e. (xθ) (xπ(θ)). Examples include QuaRot, where rotational matrices are applied to Transformer weights to reduce outliers for improved low bit quantization [5], and SliceGPT, where rotational PCA matrices are again applied to prepare weights for low-eigenvalue neuron pruning [4]. In this work, we take an approach similar to structured pruning, building upon previous work that prunes the width of individual layers in LLMs [18, 4]. However, we employ key difference: instead of determining neurons unimportant to model inference, we instead focus on combining neurons into fewer according to their activation similarities. In direct parameter intervention approach without additional training, we focus on OT-based techniques to target redundant processing, known phenomenon explored in previous work [9, 20]. This approach is inspired by model merging methods where combining parameters from disparate models via arithmetic operations without additional gradient updates is enough to create high-quality multi-task models [33]. Prior work has examined the use of transportation maps that align neurons from different models layers [24, 21, 16], but this line of work assumes that there exists clear alignment of neurons, as these models are generally trained on the same data, but from different initializations. We apply similar principle, but without this guiding assumption in the challenging setting of parameter width reduction. We find that compared to both simple activation pruning strategy and SliceGPT, adding OT-merging can further improve performance by incorporating additional signal from neurons that would otherwise be pruned. Our method can be summarized in Figure 1. Our contributions are as follows. 1. We propose novel, training-free neuron width reduction method that uses discrete optimal transport theory to frame the compression problem, in order to compress signal from the full neuron width rather than discard signal from pruned neurons. 2 2. We examine the computational invariance of Transformer weights to orthogonal maps proposed in Ashkboos et al. [4], and extend the set of invariant maps from orthogonal to any invertible map, via the application of QR-decomposition to respect the orthogonal-only invariance of RMSNorm. 3. We demonstrate the effectiveness of DOTRESIZE by comparing our method directly to similar and performant pruning alternatives on several LLMs. We show that across language modeling and zero-shot LLM tasks, our method can frequently regain some of the performance loss caused by neuron-level pruning in directly comparable methods, supporting the core motivation of our work."
        },
        {
            "title": "2 Related Work",
            "content": "Neuron-level pruning Given the practical limitations of unstructured weight pruning [17], including the lack of real-world memory and latency savings, prior work has examined converting unstructured sparsity patterns into semi-structured sparsity patterns [12, 27]. However, this conversion process leads to additional performance loss and only provides speed-ups on certain GPU architectures. Alternatively, structured pruning techniques that remove structured portions of models, like attention heads, layers, individual neurons, and blocked weight regions, provide hardware agnostic approach to achieving both real-world disk and latency savings. In this work, we focus on reducing individual neurons, which has been explored in prior work via neuron width pruning. Ma et al. [18] propose channel pruning technique that removes neurons sharing the same index across all LLM layers, effectively reducing the width of the entire model. Ashkboos et al. [4] propose related technique, but use PCA projections to perform the width reduction to prune low eigenvalue associated neurons in the re-projected eigenspace. Gao et al. [14] reduce neuron width in dimension-independent manner by using different indexing matrices per layer. However, their method trains separate GRU hypernetwork to predict neurons to remove. We depart from these pruning approach by instead focusing on merging-based strategies. Neuron-level redundancy Current LLMs are highly overparameterized, containing more parameters than necessary to fit their training data. This leads to consequences like the redundant encoding of concepts, which has been repeatedly identified in prior work. Srinivas and Babu [25] propose method to wire similar neurons together within layer based on their weight set similarities. Dalvi et al. [9] analyze redundancy between individual neurons and layers and propose clustering-based approach to pruning neurons. In model merging method, Stoica et al. [26] tackle redundancy by not only aligning similar neurons across models to merge, but also align similar neurons within model for merging. Nanda et al. [20] characterize redundancy in pre-trained models as diffuse where multiple random subsets of neurons in model layer can approximate the performance of the whole layer. In our work, we are inspired by this diffusely redundant nature of neurons, and design our technique to target potentially redundant groups of neurons within layer."
        },
        {
            "title": "3 DOTRESIZE",
            "content": "Our DOTRESIZE width reduction method draws on entropy-regularized discrete optimal transport, where we assign layers neurons to source distribution and subset of these neurons as target distribution, and compute an optimal transport map between source and target based on activation signatures over representative data. The result of this approach is method that reduces overall model width by transforming the signal from the entire neuron width, which is in direct contrast with pruning methods that only consider portion of the signal. demonstration of this width reduction can be seen in Figure 1. Reducing model width incidentally reduces activation width too, improving activation memory use. 3.1 Computing the transport maps Discrete optimal transport problems are formulated as transforming source distribution to target distribution subject to cost matrix that dictates the cost to transform each source item to each target item. Following Singh et al. [24], we define our source and target distributions as uniform, with the source as = 1dorig/dorig, and target as = 1dnew /dnew, where we select dnew < dorig neurons 3 from the original width. While selection strategies can differ, we select using average activation norm in our work, as detailed in later sections. We define the cost matrix of transporting mass from distribution to distribution b, also known as ground metric, as the pairwise distances between the activations over dorig source neurons, and the subset of activations corresponding to the dnew selected neurons. This means that at each layer we wish to compress, we compute activations across exemplar tokens and use these activations to help determine our ground metric. In our work, we use ℓ1 norm to compute pairwise distances. This results in cost matrix Rnk, where for rows of activations xi, xj Rt, Cij = xi xj1. In computing the discrete optimal transport map that dictates how to recombine these dorig neurons into dnew, we incorporate Sinkhorn entropic regularization into the OT optimization problem in order to incorporate soft alignments between neurons [8]. Soft alignments allow for each neuron to correspond to linear combinations of other neurons, whereas non-regularized alignments are much sparser. Additionally, Sinkhorn solutions are less costly to compute compared to non-regularized optimal transport distances. The following optimization problem reflects the Sinkhorn objective for finding map subject to costs Rnk. = arg min T, λH(T ) (1) The larger the parameter λ, the softer the transportation map is due to the entropic regularization reflected in H(T ). In practice, we multiply solution by dorig so that each row sums to 1 rather than 1 as is the case in optimal transport solutions. After applying map to the one or more weight dorig matrices that produced the activations used for alignment, we can apply an inverse transformation to the following weight matrix such that functional invariance is preserved. In practice, we must modify these maps in order to work with the Transformer architecture, as described in following sections. 3.2 Maintaining invariance via QR decomposition In applying the transportation matrices to transform model weights, we aim to approximate the original function to achieve compressed model without losing substantial performance. In other words, our transportation maps, when applied to weights, should not substantially change the overall function of the model. In Ashkboos et al. [4], orthogonal PCA matrices are used to transform model weights given their invariance to the commonly used root mean square layer normalization (RMSNorm) in Transformers [35]. This invariance allows for transformation matrices to apply to weights before and after residual connection, while not interfering with the function of RMSNorm in pre-norm architectures. For orthogonal matrix and data vector x, the relationship can be expressed as the following: RMSNorm(xQ)QT = RMSNorm(x) (2) depiction of this invariance is found in Figure 2. This invariance assumes that RMSNorm is unweighted, as weighted versions can be converted to unweighted RMSNorms by pre-multiplying weight matrices following pre-norm by the RMSNorm weights. For more details on this conversion, refer to Ashkboos et al. [4]. To apply our method, we relax the constraint of permissible transportation matrices from only orthogonal matrices (Q) to any invertible matrix. However, the original use of orthogonal matrices in prior work was important as they do not change vector norms, whereas general invertible matrices may. To address 4 Figure 2: Our QR-decomposition step allows for general invertible matrices to apply at residual junctions as depicted by the figure. The figure depicts general residual connection as (cid:76) in pre-norm Transformer layer. While orthogonal transformations are naturally invariant to RMSNorm, general invertible matrices are not unless decomposed via QR decomposition. The commutativity of matrix multiplication allows us to absorb matrix into the 1 calculation, allowing the orthogonal multiplicand to not change RMSNorm. this limitation of invertible transformation matrices, we use QR decomposition in order to decompose invertible transformation matrix = QR such that the matrix is applied to the input vector before RMSNorm, and the matrix is applied after the RMSNorm. We maintain the unweighted RMSNorm and pre-norm assumptions, and convert weighted RMSNorms to unweighted RMSNorms as previously described. Given decomposition = QR, inverse matrix 1 and data vector x, we have the following relationship: RMSNorm(xQ)RT 1 = xQ xQ RT 1 = xQRT 1 = RMSNorm(x) (3) simple explanation of steps in this equality appears in Appendix A. Given the associativity of matrix multiplication, we are able to QR-decompose our computed transport map and then store just = and inv = RT 1 as our transport and untransport maps. An overview of the application of these maps is summarized in Figure 2. While in the case of full rank this equality holds, we note that our transport maps are not full rank due to the intentional difference in the width of source and target supports. In our setting, the orthogonal matrix resulting from QR-decomposition is still critical to the application of our method, so much so that our method does not converge without its application due to its role in preserving layer normalization. Additionally, we use the pseudoinverse for non-invertible , and find that this approximation also works well. 3.3 Applying maps With this QR modification, we are able to apply our transformations at two key locations for each Transformer layer [29]. We label our transformations per layer as {MA, inv We label each of the linear weight matrices in modern LLMs as the following: } and {MF , inv }. WQ, WK, WV are the dorig dorig weight matrices for queries, keys, and values in multiheaded (or grouped-query [2], where dimensions are dorig (dorig/ngroups)) attention WO is the output projection of attention, size dorig dorig. Attention is abbreviated MHA(Q, K, ) Wup and Wgate are the dorig dff up-projection matrices in feed-forward layers with GLUbased activations, which we denote as σ for the nonlinearity and for gating [23]. Wdown is the dff dorig down-projection matrix for feed-forward layers. For each layer, we apply MA and inv then passed firstly through inv Wgate. We apply MF and inv and then passed firstly through RMSNorm and inv In summary, assuming input xattn in or from the input embedding layer, we have: xattn in such that the output of attention is multiplied by MA, but is before being passed through RMSNorm and multiplied by Wup and such that the output of the feed-forward layer is multiplied by MF , before being multiplied by WQ, WK and WV . in = xattn from prior layer is pre-multiplied by MF to give MF xattn ) + inv xattn in (4) xattn in in xattn in , WV inv , WKM inv out = WOMHA(WQM inv xattn This reverses the effect of pre-multiplying xattn compression map, we apply it to xattn out , resulting in: xattn , WV inv in out , which we will denote as xff in ) (WgateM inv out = MAWOMHA(WQM inv Now, to reverse the effect of MA on xattn out = Wdownσ (cid:0)(WupM inv xff , WKM inv MAxattn xattn in xff xff ) + MAM inv xattn in out , we have: xattn in in = MAxattn xff in in )(cid:1) + inv in by MF from the prior layer. To apply the MA And finally, to apply the MF compression map, we have MF xff out = MF Wdownσ (cid:0)WupM inv xff in WgateM inv xff in (cid:1) + MF inv xff in (5) (6) (7) All matrices can be absorbed into neighboring weight matrices due to the associativity of matrix multiplication. This application of maps mirrors several prior methods using similar project/unproject scheme for Transformer model merging and compression [16, 4, 30]."
        },
        {
            "title": "4 Experimental Settings",
            "content": "4.1 Models We focus our evaluation on three different families of LLMs: Llama 3.1 at 8B and 70B parameters [15], Mistral at 7B (v0.3) and 12B (NeMo) parameters [32], and Phi-4 at 12B parameters [1]. These models all reflect large Transformer models pre-trained on primarily English text data. We obtain all models using their huggingface transformers library implementations [31]. All models with the exception of Llama-3.1-70B can be compressed and evaluated on single 32GB V100 GPU, whereas we use 8 V100s for Llama-3.1-70B experiments. 4.2 Evaluation For language modeling capabilities, we evaluate our models by computing perplexity on the English Wikitext-2 dataset [19]. For zero-shot capabilities, we test the models on 5 evaluation sets sourced from lm-evaluation-harness [13]: ARC-Challenge and ARC-Easy [7], HellaSwag [34], PIQA [6], and Winogrande [22]. These five tasks reflect diverse and commonly used English tasks for evaluating language models. Following Ashkboos et al. [4], in all language modeling experiments, we use small subset (262K tokens) of the Wikitext-2 training data to compute our transportation maps. For all zero-shot experiments, we use the same number of tokens, but from the Stanford Alpaca dataset for instruction following [28]. 4.3 Baseline Comparisons We compare two versions of our method to two corresponding baselines. The first baseline is simpler magnitude pruning baseline where at each layer, we prune the neurons with the lowest average ℓ2 norm across the calibration data. These neuron indices can differ across each layer by projecting residual connections as well as in our method, as we found this to be superior to whole-model single-index pruning, as proposed in LLM-Pruner as channel-pruning [18]. When comparing our method to this baseline, Magnitude Pruning, we select these same neurons as the target support of DOTRESIZE, to get direct comparison of pruning versus OT-based merging in this setting. This means that to select the neurons that comprise the dnew support, we keep the neurons with highest average ℓ2 norm. The second baseline is SliceGPT with default parameters, which first applies PCA to reproject features, and then prunes neurons with least total variance [4]. We directly compare our method to SliceGPT by reprojecting activations by their PCA basis before applying DOTRESIZE to reduce neuron width. We differ from their method by forgoing the slicing matrix and instead using our transport map in its place. In this version, which we denote as PCA+DOTRESIZE, we use ℓ1 norm to select neurons. Unless stated otherwise, we use λ = 0.1 for the Sinkhorn regularization value."
        },
        {
            "title": "5 Results",
            "content": "5.1 Language Modeling Table 1 presents language modeling results for the uncompressed models, existing compression baselines, and both variants of our method. Compared to Magnitude Prune, DOTRESIZE achieves significantly lower perplexity by preserving and leveraging signal from neurons that would otherwise be discarded. While Magnitude Prune only retains the neurons with the highest ℓ2 activation norms, DOTRESIZE uses these high-norm neurons as support set and redistributes the full neuron width onto them based on similarity. Although this redistribution can sometimes reduce the influence of neurons that would have been retained by Magnitude Prune, the advantage of preserving signal from the pruned neurons more than compensates for this effect. In comparing the DOTRESIZE results and PCA+DOTRESIZE results, we see that firstly decorrelating the activations according to PCA improves performance. By firstly transforming the activations so that their dimensions are uncorrelated, first applying PCA can help DOTRESIZE more effectively identify meaningful similarities based on distance in our transportation map finding step. 6 Table 1: Perplexity performance of models on Wikitext-2 under different width reduction strategies and at different levels of width reduction (sparsity). Perplexity results that reflect > 0.1 PPL reduction are bolded. In comparing Magnitude Pruning to DOTRESIZE without pre-projection, we see stark improvement in perplexity, indicating that using optimal transport to reduce neuron width can be superior to pruning neurons. Sparsity Method Llama-3.1 Mistral 0% 20% 30% 20% 30% - Magnitude Prune DOTRESIZE 8B 6.24 29.33 16. Magnitude Prune DOTRESIZE 108.23 36.20 SliceGPT PCA+DOTRESIZE SliceGPT PCA+DOTRESIZE 11.25 10.24 19.42 16. 70B 2.81 12.72 9.74 86.48 32.48 6.92 7.01 9.30 9. 7B 5.32 15.68 10.08 43.42 20.26 6.76 6.98 8.69 8. 12B 5.74 36.78 16.78 128.04 32.15 8.11 7.96 10.73 10. Phi-4 14B 6.46 14.07 11.07 66.48 29.79 7.67 7. 9.14 9.20 In comparing the PCA+DOTRESIZE results, we see that for the Llama-3.1 8B model, our method reduces the perplexity by almost 3 points at 30% width reduction, and over 1 point for 20% width reduction. For the other models, while Mistral-7B does not improve from using PCA+DOTRESIZE, other models either maintain similar performance or improve with PCA+DOTRESIZE. Given some improved performance with the difference in objectives to reduce neuron width, we are able to demonstrate an interesting limitation of SliceGPT. Using PCA to reduce neuron width in SliceGPT is the ℓ2-optimal strategy for linear dimensionality reduction, as it solves the ℓ2 reconstruction problem between original and down-projected activations. However, by outperforming SliceGPT in comparable setting, we can demonstrate that minimizing ℓ2 activation distance does not always correlate with improved downstream performance. This indicates that relying on ℓ2 activation distance as proxy for model performance, which is common in compression applications [10, 11], may miss other aspects of model behavior. 5.2 Zero-Shot performance on downstream applications Given that PCA+DOTRESIZE outperforms DOTRESIZE without pre-projection in language modeling, we evaluate only SliceGPT and PCA+DOTRESIZE on zero-shot tasks. Accuracy results across all 5 tasks and an average accuracy are found in Table 2. As seen in the table, several model/sparsity combinations are improved using PCA+DOTRESIZE , including Mistral-Nemo 12B, which sees an over 11% average accuracy improvement across tasks at 20% sparsity. Additionally, using PCA+DOTRESIZE improves results for Phi-4, which is remarkably robust to both our compression method and SliceGPT. At 20% sparsity, this model retains 98% of its original average zero-shot performance, and even sees higher accuracy on the ARC-Easy and Challenge sets after compression. We also observe model-specific differences in both language modeling and zero-shot performance, with some models performing on par with or slightly worse than SliceGPT. These variations potentially stem from training differences across state-of-the-art LLMs, which can affect their compatibility with merging-based compression. In practice, we recommend evaluating both approaches and, where possible, lightly tuning our method, which is relatively inexpensive due to its transform-in-place nature. 7 Table 2: Zero shot performance (% Accuracy) of models on downstream tasks under different width removal settings. Bolded results reflect the higher result on model and sparsity comparisons. Model Sparsity Method ARC-C ARC-E HellaSWAG PIQA Winogrande Average 0% - Llama-3.1-8B 20% 30% 0% Llama-3.1-70B 20% Mistral-7B Mistral-12B Phi-4 12B 30% 0% 20% 30% 0% 20% 30% 0% 20% 30% SliceGPT PCA+DOTRESIZE SliceGPT PCA+DOTRESIZE - SliceGPT PCA+DOTRESIZE SliceGPT PCA+DOTRESIZE - SliceGPT PCA+DOTRESIZE SliceGPT PCA+DOTRESIZE - SliceGPT PCA+DOTRESIZE SliceGPT PCA+DOTRESIZE - SliceGPT PCA+DOTRESIZE SliceGPT PCA+DOTRESIZE 53.50 40.61 39. 34.13 34.47 65.10 55.20 45.39 50.26 49.91 52.22 42.83 42. 36.18 34.04 57.85 29.86 43.94 26.96 33.96 56.06 54.61 56. 49.83 48.55 81.14 68.22 67.63 58.67 57.41 86.66 80.43 69. 73.91 73.61 78.20 69.91 69.95 60.69 58.54 81.65 46.51 71. 40.24 57.83 72.77 76.09 79.29 74.12 75.17 78.88 59.53 60. 45.73 47.53 84.94 69.19 69.28 63.85 56.19 80.47 61.80 62. 49.04 47.65 82.79 53.19 60.88 42.88 46.71 81.90 73.08 72. 64.57 63.39 81.23 73.23 73.01 67.03 67.90 84.28 79.65 74. 77.20 76.28 82.26 75.57 75.35 70.02 69.10 82.37 68.17 75. 63.11 69.26 81.34 80.30 79.76 76.33 76.44 73.48 62.12 64. 53.91 55.96 79.56 72.93 72.22 70.24 68.98 73.80 64.33 65. 58.88 59.04 73.64 59.43 64.56 56.51 60.77 76.72 71.11 72. 69.38 68.67 73.65 60.74 60.94 51.89 52.65 80.11 71.48 67. 67.09 65.00 73.39 62.89 63.14 54.96 53.67 75.66 51.43 63. 45.94 53.71 73.76 71.04 72.08 66.84 66.44 5.3 Analysis Compute cost versus performance As our method is intended to reduce memory use and improve latency, we measure the total compute used in inference: We finding the minimum number of 32GB V100 GPUs our compressed model can run on without GPU-offloading, and measure the latency in ms/token during inference on sequences of 128 tokens [4]. This gives us the total compute cost in GPUms/token. We compress Llama-3.1-70B at different levels of width reduction, and evaluate both perplexity and inference cost, and report this trade-off in in Figure 3. After 20% sparsity, compute cost is reduced from the original model, dictating the region of practical use of width reduction methods. regularization Figure 3: Sparsity vs performance tradeoff across different levels of compression on Llama-3.1-70B. Sparsity (%) indicates how much width is removed from weight matrices. While 10% sparsity does not improve compute cost due to residual parameterization, after 20%, real world compute cost reduction can be observed. Sinkhorn parameter As stated, we select λ = 0.1 to serve as the regularization parameter found in Equation 1. To understand the effect of different values of λ on our method, we test three of our models on 5 different λ values, ranging from 0.01 to 10. As seen in Figure 4, the amount of regularization used does not change downstream language modeling performance to large extent, suggesting that there are several acceptable weighting strategies for combining neurons successfully. However, different 8 values of λ can result in some perplexity differences in the Llama-3.1-8B model as seen at λ = 0.01. Given training differences, different values of λ may be optimal for different models, but the values presented here reflect an acceptable range. Calibration Data Much like previous work that examines the behavior of neurons to dictate neuron alignment or pruning [24, 3, 4], our method uses small amount of relevant calibration data ( 262K tokens) to determine the behavior of individual neurons. We evaluate the sensitivity of our method to the amount of calibration data used by testing the use of only 65K tokens, 131K tokens, 262K tokens, and 524K tokens to compute transportation maps. Results are shown in Figure 5. Figure 4: Performance of models on Wikitext-2 at 20% and 30% width reduction with different amounts of entropic regularization applied to the Sinkhorn objective. Besides some notable performance loss at λ = 0.01 for Llama-3.1-8B at both 20% and 30% sparsity, our method is not sensitive to the amount of entropic regularization present for these models. Figure 5: Performance of models compressed with DOTRESIZE on Wikitext-2 at 20% sparsity, with varying amounts of exemplar tokens used to compute transport maps in our method. After approximately 130K tokens, the returns on using more calibration data appear diminishing. While DOTRESIZE is not notably sensitive to the amount of calibration data used, we recommend using at least 217 or 131K tokens for computing transport maps. This robustness is strength, as computing the cost matrices from Equation 1 can become computationally expensive with large datasets. Additionally, this robustness suggests that small amount of exemplar data is sufficient to capture meaningful neuron behavior, as reflected in the generalization ability of our compressed models."
        },
        {
            "title": "6 Conclusion and Future Work",
            "content": "In this work, we introduce DOTRESIZE, novel model compression method that frames neuron width reduction as an optimal transport problem in order to combine layers neurons based on their activation signatures, specifically avoiding the pruning of any particular neuron. In taking this approach, we are able to demonstrate improved performance across several LLMs compared to pruning-based alternatives, demonstrating the importance of incorporating the full signal of layer and the potential insufficiencies of pruning. Some models are hugely receptive to the addition of merging-based objective in width reduction, with Phi-4 maintaining almost 98% of zero-shot performance on downstream tasks while improving on some individual tasks at 20% sparsity, and Mistral-Nemo achieving an over 10% gain in average accuracy across tasks over SliceGPT. We also demonstrate an extension of computational invariance in Transformers by demonstrating that invertible matrices can be applied to Transformer weights via QR-decomposition step, and without changing the model function. While we focus on model compression as our application, our work also provides basis for exploring discrete optimal transport as general-purpose parameter transformation method applicable beyond compression."
        },
        {
            "title": "References",
            "content": "[1] Marah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell Hewett, Mojan Javaheripi, Piero Kauffmann, et al. Phi-4 technical report. arXiv preprint arXiv:2412.08905, 2024. [2] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 48954901, 2023. [3] Samuel Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa. Git re-basin: Merging models In The Eleventh International Conference on Learning modulo permutation symmetries. Representations, 2023. [4] Saleh Ashkboos, Maximilian L. Croci, Marcelo Gennari do Nascimento, Torsten Hoefler, and James Hensman. SliceGPT: Compress large language models by deleting rows and columns. In The Twelfth International Conference on Learning Representations, 2024. [5] Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian L. Croci, Bo Li, Pashmina Cameron, Martin Jaggi, Dan Alistarh, Torsten Hoefler, and James Hensman. Quarot: Outlier-free 4-bit inference in rotated LLMs. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [6] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 74327439, 2020. [7] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. [8] Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in neural information processing systems, 26, 2013. [9] Fahim Dalvi, Hassan Sajjad, Nadir Durrani, and Yonatan Belinkov. Analyzing redundancy in pretrained transformer models. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 49084926, Online, November 2020. Association for Computational Linguistics. [10] Xin Dong, Shangyu Chen, and Sinno Pan. Learning to prune deep neural networks via layer-wise optimal brain surgeon. Advances in neural information processing systems, 30, 2017. [11] Elias Frantar and Dan Alistarh. Optimal brain compression: framework for accurate posttraining quantization and pruning. Advances in Neural Information Processing Systems, 35:4475 4488, 2022. [12] Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot. In International Conference on Machine Learning, pages 1032310337. PMLR, 2023. [13] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, 07 2024. [14] Shangqian Gao, Chi-Heng Lin, Ting Hua, Zheng Tang, Yilin Shen, Hongxia Jin, and Yen-Chang Hsu. Disp-llm: Dimension-independent structural pruning for large language models. Advances in Neural Information Processing Systems, 37:7221972244, 2024. [15] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [16] Moritz Imfeld, Jacopo Graldi, Marco Giordano, Thomas Hofmann, Sotiris Anagnostidis, and Sidak Pal Singh. Transformer fusion with optimal transport. In The Twelfth International Conference on Learning Representations, 2024. [17] Yann LeCun, John Denker, and Sara Solla. Optimal brain damage. Advances in neural information processing systems, 2, 1989. [18] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On the structural pruning of large language models. Advances in neural information processing systems, 36:2170221720, 2023. [19] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. In International Conference on Learning Representations, 2017. [20] Vedant Nanda, Till Speicher, John Dickerson, Krishna Gummadi, Soheil Feizi, and Adrian Weller. Diffused redundancy in pre-trained representations. Advances in Neural Information Processing Systems, 36:40554079, 2023. [21] Fidel Guerrero Peña, Heitor Rapela Medeiros, Thomas Dubail, Masih Aminbeidokhti, Eric Granger, and Marco Pedersoli. Re-basin via implicit sinkhorn differentiation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2023720246, 2023. [22] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. [23] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. [24] Sidak Pal Singh and Martin Jaggi. Model fusion via optimal transport. Advances in Neural Information Processing Systems, 33:2204522055, 2020. [25] Suraj Srinivas and Venkatesh Babu. Data-free parameter pruning for deep neural networks. arXiv preprint arXiv:1507.06149, 2015. [26] George Stoica, Daniel Bolya, Jakob Brandt Bjorner, Pratik Ramesh, Taylor Hearn, and Judy In The Twelfth Hoffman. Zipit! merging models from different tasks without training. International Conference on Learning Representations, 2024. [27] Mingjie Sun, Zhuang Liu, Anna Bair, and Zico Kolter. simple and effective pruning approach for large language models. In The Twelfth International Conference on Learning Representations, 2024. [28] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023. [29] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [30] Neha Verma and Maha Elbayad. Merging text transformer models from different initializations. Transactions on Machine Learning Research, 2024. [31] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-theart natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 3845, Online, October 2020. Association for Computational Linguistics. [32] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [33] Enneng Yang, Li Shen, Guibing Guo, Xingwei Wang, Xiaochun Cao, Jie Zhang, and Dacheng Tao. Model merging in llms, mllms, and beyond: Methods, theories, applications and opportunities. arXiv preprint arXiv:2408.07666, 2024. [34] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 47914800, 2019. [35] Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019."
        },
        {
            "title": "A Simple explanation of QR Decomposition invariance",
            "content": "From Section 3.2, we provide an equality statement in Equation 3. We include short and simple proof for completeness. As reminder, for model dimension dorig we have input vector Rdorig, denotes orthogonal matrices where QT = QQT = I, = QR via QR decomposition, and is invertible. RMSNorm(xQ)RT 1 = RT 1 xQ xQ xQRT 1 = RMSNorm(x) = via RMSNorm definition via xQ = (cid:112) xQT Qx = via QR = substitution (8) (9) (10)"
        },
        {
            "title": "B Limitations",
            "content": "In discussing the limitations of this work, there are both limitations of the method proposed as well as limitations of the validation of this method. Focusing firstly on the method proposed, we acknowledge that while our method does lead to real-world compression and latency improvements, the compression attained is not comparable to methods like low-bit quantization. However, in practice, this method is orthogonal to methods like quantization and should be combined. Additionally, we acknowledge that despite being training-free method, our method is not data-free, as we require the use of some amount of data to observe neuron behavior when computing alignments and transportation maps. Focusing on the validation of this method, there are limitations in the scope of models tested in this work. To make focused contribution, we limit the scope to LLMs trained primarily on English text data. However, our method can be easily generalized to other architectures, and can be applied to pre-trained Transformer models with multimodal and/or multilingual capabilities. We leave this exploration for future work."
        },
        {
            "title": "C Broader Impacts",
            "content": "In reducing the memory cost and latency of LLM inference, the broader impacts of our work are several. Firstly, reducing the latency and power needed to run these models reduces both the monetary and environmental costs of LLMs. These downstream effects are intended positive consequences resulting from our work. Additionally, reducing the computational resources needed to run LLMs can have both positive and negative consequences with respect to accessibility. Much like LLMs may have positive uses in society, they have many negative uses as well. In improving the accessibility of these models, we contribute to amplifying this effect, in reaching both goodand poor-intentioned users."
        }
    ],
    "affiliations": [
        "Center for Language and Speech Processing",
        "Human Language Technology Center of Excellence",
        "Johns Hopkins University"
    ]
}