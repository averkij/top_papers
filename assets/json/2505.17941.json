{
    "paper_title": "VeriThinker: Learning to Verify Makes Reasoning Model Efficient",
    "authors": [
        "Zigeng Chen",
        "Xinyin Ma",
        "Gongfan Fang",
        "Ruonan Yu",
        "Xinchao Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Reasoning Models (LRMs) excel at complex tasks using Chain-of-Thought (CoT) reasoning. However, their tendency to overthinking leads to unnecessarily lengthy reasoning chains, dramatically increasing inference costs. To mitigate this issue, we introduce VeriThinker, a novel approach for CoT compression. Unlike conventional methods that fine-tune LRMs directly on the original reasoning task using synthetic concise CoT data, we innovatively fine-tune the model solely through an auxiliary verification task. By training LRMs to accurately verify the correctness of CoT solutions, the LRMs inherently become more discerning about the necessity of subsequent self-reflection steps, thereby effectively suppressing overthinking. Extensive experiments validate that VeriThinker substantially reduces reasoning chain lengths while maintaining or even slightly improving accuracy. When applied to DeepSeek-R1-Distill-Qwen-7B, our approach reduces reasoning tokens on MATH500 from 3790 to 2125 while improving accuracy by 0.8% (94.0% to 94.8%), and on AIME25, tokens decrease from 14321 to 10287 with a 2.1% accuracy gain (38.7% to 40.8%). Additionally, our experiments demonstrate that VeriThinker can also be zero-shot generalized to speculative reasoning. Code is available at https://github.com/czg1225/VeriThinker"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 1 4 9 7 1 . 5 0 5 2 : r VeriThinker: Learning to Verify Makes Reasoning Model Efficient Zigeng Chen, Xinyin Ma, Gongfan Fang, Ruonan Yu, Xinchao Wang National University of Singapore zigeng99@u.nus.edu, xinchao@nus.edu.sg Figure 1: The key distinction between VeriThinker and traditional SFT or RL-based long-to-short methods. We uniquely train LRMs on an auxiliary CoT verification task, achieving effective CoT compression without relying on synthetic target reasoning chains."
        },
        {
            "title": "Abstract",
            "content": "Large Reasoning Models (LRMs) excel at complex tasks using Chain-of-Thought (CoT) reasoning. However, their tendency to overthinking leads to unnecessarily lengthy reasoning chains, dramatically increasing inference costs. To mitigate this issue, we introduce VeriThinker, novel approach for CoT compression. Unlike conventional methods that fine-tune LRMs directly on the original reasoning task using synthetic concise CoT data, we innovatively fine-tune the model solely through an auxiliary verification task. By training LRMs to accurately verify the correctness of CoT solutions, the LRMs inherently become more discerning about the necessity of subsequent self-reflection steps, thereby effectively suppressing overthinking. Extensive experiments validate that VeriThinker substantially reduces reasoning chain lengths while maintaining or even slightly improving accuracy. When applied to DeepSeek-R1-Distill-Qwen-7B, our approach reduces reasoning tokens on MATH500 from 3790 to 2125 while improving accuracy by 0.8% (94.0% to 94.8%), and on AIME25, tokens decrease from 14321 to 10287 with 2.1% Correspoding Author Preprint. Under review. accuracy gain (38.7% to 40.8%). Additionally, our experiments demonstrate that VeriThinker can also be zero-shot generalized to speculative reasoning. Code is available at https://github.com/czg1225/VeriThinker"
        },
        {
            "title": "Introduction",
            "content": "Large Reasoning Models (LRMs), such as GPT-o1 [20], DeepSeek-R1 [13], Kimi-k1.5 [53], and QwQ [54], have shown exceptional performance in tackling complex reasoning tasks. Their success stems from the ability to generate effective Chain-of-Thought (CoT) reasoning [58, 32]. By decomposing intricate problems into manageable steps and meticulously verifying each intermediate result, these models achieve superior reasoning performance. Furthermore, this test-time scaling method has demonstrated notable potential beyond reasoning tasks, extending its impact to diverse fields such as vision-language models [19, 62, 71, 28], image generation [40, 21, 61], and video synthesis [7, 33]. However, the long reasoning chains generated by LRMs often contain numerous redundant selfchecking stepsa phenomenon commonly known as \"overthinking\" [5]. Specifically, these models incorporate frequent self-verification steps to ensure the accuracy of their reasoning, but most of these self-verifications prove ineffective or even add confusion to the reasoning process. This extensive overthinking dramatically inflates inference costs and hinders the efficient deployment of LRMs. Several recent studies [41, 14, 59, 35, 43, 38, 70, 2, 1] have focused on compressing lengthy reasoning chains into shorter, more concise forms. These methods typically generate concise reasoning chain data through extensive sampling, model merging, or selective truncation of original long reasoning chains. Subsequently, these synthetic reasoning chains serve as targets, with Supervised Fine-Tuning (SFT) [57] or Reinforcement Learning (RL) used to align the LRMs output distribution closely to these target chains. Thus, the performance of these compression strategies heavily depends on the quality and quantity of the synthesized reasoning chain data. However, generating high-quality concise reasoning chains is computationally intensive and time-consuming. Moreover, simultaneously achieving brevity while preserving essential self-reflection steps remains challenging. Consequently, models compressed through these methods struggle to balance conciseness and high accuracy, especially when tackling highly complex reasoning tasks. key question arises: Can we effectively compress lengthy reasoning chains without relying on synthetic target reasoning chains? Our Approach. To address this challenge, we introduce VeriThinker, straightforward yet effective method for CoT compression without the dependence on synthetic target data. VeriThinkers core innovation lies in novel fine-tuning strategy named Supervised Verification Fine-Tuning (SVFT). Unlike previous methods that directly fine-tune LRMs on reasoning tasks through SFT or RL, SVFT uniquely fine-tunes the model on an auxiliary verification task to achieve CoT compression. Specifically, we create CoT verification dataset, providing the LRM with question-solution pairs and training it to verify the correctness of the CoT solution through binary classification. By learning to distinguish between correct and incorrect solutions, the LRM can more accurately determine when self-reflection is necessary. Empirical studies demonstrate that after SVFT, the model significantly reduces unnecessary double-checking of correct reasoning steps while slightly increasing verification of incorrect steps. This effectively mitigates overthinking and maintains or slightly improves reasoning accuracy. Furthermore, by utilizing its ability to recognize correctness, VeriThinker can also generalize to solution-wise speculative reasoning for higher inference throughput. We evaluate VeriThinker on three state-of-the-art reasoning models: DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B, and DeepSeek-R1-Distill-Llama-8B [13]. Extensive experiments show that fine-tuning solely on the CoT verification task enables LRMs to substantially reduce token usage while preserving high accuracy, even on extremely challenging problems like the AIME dataset. Additional experiments show that VeriThinker can also be applied to speculative reasoning, achieving significant throughput increase when using short-CoT LLM as the draft model. In conclusion, we present VeriThinker, simple yet effective CoT compression method that eliminates the need for synthetic target chain data. Key to our approach is the proposed SVFT, which innovatively mitigates the overthinking problem by fine-tuning the LRM exclusively on an auxiliary CoT verification task. Comprehensive experiments demonstrate that VeriThinker efficiently shortens reasoning chains while preserving or slightly improving model accuracy, even on highly challenging reasoning tasks. Additionally, our method can also generalize to speculative decoding, achieving significant improvements in inference throughput."
        },
        {
            "title": "2 Related Works",
            "content": "Chain-of-thought. Chain-of-thought [58, 32] enables large reasoning models (LRMs) to solve complex reasoning tasks, such as mathematical and coding problems, by leveraging inference-time scaling laws. Advanced LRMs, including GPT-o1 [20], DeepSeek-R1 [13], Kimi-k1.5 [53], and QwQ [54], have set new benchmarks in reasoning capabilities. These models utilize reinforcement learning to encourage multiple self-reflection steps during inference, substantially increasing response accuracy. Furthermore, other methods, such as self-consistency [56], beam search, and Monte Carlo Tree Search (MCTS) [22, 12], have been employed to facilitate parallel inference scaling laws, further enhancing LRM reasoning effectiveness. Beyond language models, the principles underlying CoT have also demonstrated significant performance improvements in domains like visual reasoning [19, 62, 71], image generation [40, 21, 61], and video synthesis [7, 33]. Efficient Reasoning Models. Despite these substantial gains, the tendency of LRMs towards overthinking often results in excessively extended reasoning chains, significantly raising inference costs. Recent research has investigated methods for compressing CoT to enable more efficient reasoning processes. Specifically, [5] comprehensively analyzes the overthinking phenomenon and employs SimPO [42] to fine-tune LRMs. Kimi-K1.5 [53] proposes weight merging strategies between long-CoT and short-CoT models and introduces length-penalized loss functions to effectively compress CoT. Additional studies [41, 14] achieve lengthy-controllable CoT compression, while others [59, 35, 43, 68] synthesize concise reasoning chain data to guide long-to-short compression via SFT. Methods [38, 70, 2, 1, 26, 17] adopt reinforcement learning and extensive sampling to reduce the number of tokens required in CoT processes. Furthermore, adaptive methods [67, 37, 49, 10] have been proposed to conditionally activate long-CoT modes based on input query. Simpler yet effective prompt-guidance methods for direct CoT compression have also been explored [36, 64, 24, 52]. Additionally, there is increasing interest in converting explicit CoT processes into latent CoT [9, 8, 48, 11, 15]. However, these latent approaches currently face significant challenges about accuracy degradation. Beyond CoT compression, other techniques involving model compression and optimized decoding have also gained prominence. Knowledge distillation approaches have allowed smaller reasoning models to approximate the performance of larger counterparts effectively [63, 4, 27, 73]. Additionally, [50, 34, 72] explore model pruning and quantization techniques on LRMs. Decoding optimization techniques have also been widely adopted to accelerate inference [55, 51, 39, 63, 66]. Methods such as speculative decoding [31, 46] and parallel inference strategies [45, 44] demonstrate notable decoding acceleration ratio, further enhancing the practical applicability of LRMs."
        },
        {
            "title": "3.1 Problem Setup",
            "content": "Recent efforts in improving the efficiency of LRMs have focused on compressing lengthy reasoning chains into shorter, more concise reasoning sequences without redundant over-thinkings. Formally, let Mθ represent an LRM parameterized by θ. Given query q, the model generates long reasoning chain Cl, obtained by sampling from the models conditional output distribution: Cl Mθ( q). We hypothesize the existence of an ideal concise reasoning chain Ci, characterized by minimality in length and maximal retention of essential self-verification steps, ensuring the correctness of the final solution. The objective of CoT compression is thus to finetune Mθ such that the generated reasoning chain closely approximates the ideal concise chain Ci: Eq[D(Mθ( q), Ci)], min θ (1) where denotes suitable distance metric in the distribution space. commonly employed approach involves constructing synthetic short reasoning chain Cs. In SFT-based variants [59, 35, 43, 68, 41], this is typically done via model merging or by selectively truncating the original long chain data, whereas RL-based approaches [38, 70, 2, 1, 26, 17] rely on extensive sampling. Under the assumption that the synthetic chain Cs is distributionally close to the ideal chain Ci, they train the LRMs either by supervised fine-tuning on Cs or by reinforcement learningrewarding the shorter sequences Cs more highly than the longer ones Cl. 3 In essence, these approaches regard the synthesized short CoT sequences as the target reasoning chain and train the model to approximate them. Therefore, their efficacy depends critically on both the quality and volume of the generated Cs. However, generating high-quality short chains is extremely costly in computing and time, which limits efficient scalability. Furthermore, synthetic short reasoning chains typically struggle to simultaneously achieve brevity and retain the essential self-reflection steps, resulting in mismatch from the ideal distribution Ci. This discrepancy often inevitably leads to substantial degradation in the models reasoning capabilities during the long-to-short process, particularly when confronted with complex, high-difficulty problems. critical problem arises: Can we devise method that effectively compresses an LRMs overextended reasoning chains while preserving its original reasoning ability, without relying on explicitly synthesized target chains?"
        },
        {
            "title": "3.2 Supervised Verification Finetuning",
            "content": "To address the aforementioned issue, we begin by revisiting the fundamental principles underlying LRMs overthinking. During reasoning, when determining whether to engage in additional selfreflection, the LRM essentially operates as binary classifier that evaluates the correctness of previously generated reasoning steps. Formally, the LRM encodes prior solution into hidden state, denoted as h, and the language modeling head (LM head) subsequently classifies this state as either correct or incorrect. If the classification is correct, the model proceeds to subsequent reasoning steps; if incorrect, it triggers further self-verification. The prevalence of overthinking stems from insufficient accuracy in this binary classification task. Specifically, the hidden state fails to adequately encode the critical information required for precise correctness judgments. Consequently, LRMs frequently misclassify correct solutions as incorrect, leading to unnecessary self-reflection steps. Let p(acc h) denote the probability that the LRM accurately classifies the correctness of prior solution. By increasing p(acc h), redundant reflections are progressively reduced, preserving only essential verification steps for genuinely incorrect solutions. In the ideal case where p(acc h) = 100%, the LRM outputs the optimal reasoning chain Ci, invoking self-reflection solely when prior solutions are indeed erroneous. Existing methods that employ synthetic target chains for CoT compression implicitly aim to maximize p(acc h). These methods synthesize concise target chain approximating the optimal scenario where p(acc h) 100%. As the LRMs output distribution aligns with this target, p(acc h) naturally increases, thereby mitigating overthinking. However, such approaches are inefficient and indirect, as most tokens in the synthetic chains contribute negligibly to improving p(acc h). Consequently, these methods yield only marginal gains in p(acc h). Moreover, the probability of triggering self-reflection decreases even when errors occur in earlier steps, inadvertently leading to performance degradation on highly complex problems. Since the primary cause of overthinking in LRMs arises from their inability to accurately determine the correctness of solution, natural and intuitive strategy is to explicitly train LRMs to discern solution correctness, thus fundamentally addressing the overthinking issue. Motivated by this insight, we introduce VeriThinker, straightforward yet effective method for CoT compression. The key idea of VeriThinker is novel fine-tuning strategy, termed Supervised Verification Fine-Tuning (SVFT). Distinct from prior approaches, SVFT does not rely on optimizing the model to match target reasoning chain distribution. Instead, it aims to directly enhance the models capability to classify solutions as correct or incorrect, optimizing the binary classification accuracy p(acch). Specifically, we fine-tune the reasoning model on an auxiliary CoT verification task, providing the LRM with question paired with CoT solution, and the model is explicitly trained to accurately classify whether this provided solution is entirely correct or not. Firstly, we need to construct CoT verification dataset, denoted as: Dverif = (qi, si, vi) where each data instance consists of problem description qi, CoT solution si without self-reflection steps, and verification result vi indicating the correctness of the solution. The prompt for each instance is formed by the pair (qi, si), and the corresponding verification response vi is explicitly defined as: vi = (cid:26)\"Yes, Im sure that every step is absolutely correct.\", if si is correct, \"No, think there might be some mistakes in the proposed solution.\", otherwise. (2) 4 Figure 2: (a)-(b): Token counts and accuracy in reasoning tasks across different training datasets. (c) Probability of self-reflection during reasoning for correct and incorrect solutions. Then we used this CoT-verification dataset to fine-tune the LRMs. During finetuning, only tokens within the verification response vi contribute to the training loss. This design effectively mitigates potential undesirable biases introduced by the input solution from affecting the models output distribution. Formally, given tokenized prompt-response pair (x, y), the SVFT training loss is defined as: LSVFT = log pθ(ytx, y<t) (3) (cid:88) ty where pθ denotes the probability distribution output by the LRM parameterized by θ. Since the judgments of correct and incorrect in the data are represented as two completely fixed responses, and we only compute the loss on the response tokens, the entire SVFT essentially performs binary classification on the question-solution pairs which is fundamentally similar to how the LRM decides whether to perform self-reflection after each step during reasoning. Experiments demonstrate that by learning to verify the correctness of CoT solutions, SVFT effectively mitigates LRMs overthinking phenomenon. While significantly reducing the number of tokens required for reasoning, SVFT does not lead to noticeable degradation in reasoning capability."
        },
        {
            "title": "3.3 Empirical Analysis",
            "content": "To better understand why SVFT facilitates effective compression of CoT without compromising the original reasoning capability of LRMs, we conducted additional experiments to thoroughly investigate the underlying mechanisms of SVFT. What Capabilities Does SVFT Impart to the Model? primary question we sought to explore was the specific competencies instilled in the LRM through SVFT. To this end, we constructed five distinct CoT-verification datasets and conducted SVFT on the R1-Distill-Qwen-7B model, observing differences in outcomes across these datasets. These five datasets shared identical prompts, each consisting of question, CoT solution, and concise instructions. However, we varied the response tokens in five distinct ways: Dataset (1): Maintained the original setting. Dataset (2): Reversed responses, assigning affirmative responses to incorrect solutions and negative responses to correct solutions. Dataset (3): Simplified responses to single tokens, Yes and No,. Dataset (4): Further simplified responses to semantically neutral tokens North and South. Dataset (5): Randomly assigned responses irrespective of solution correctness. Figure 4 (a) and (b) illustrate the changes in average tokens per response and accuracy on the MATH500 [16] dataset after SVFT training with the aforementioned datasets. We observed that, apart from Dataset (5), all other datasets showed similar trends: significant reduction in reasoning chain length, accompanied by maintained or slightly improved accuracy. These findings suggest that SVFT is fundamentally akin to contrastive learning. The model learns the capability to distinguish differences between correct and incorrect solutions through binary classification rather than learning explicit correctness semantics. In contrast, Dataset (5), which utilized responses entirely unrelated to solution correctness, did not facilitate CoT compression. This indicates that SVFT does not merely cause the LRM to adopt more concise expression style or to directly output conclusions, reinforcing our earlier conclusions. Conclusion: In summary, SVFT empowers the LRM to differentiate between correct and incorrect solutions rather than intrinsically learning which solution is right. The core mechanism of SVFT closely resembles that of contrastive learning. 5 Why Does SVFT Enhance CoT Compression? Next, we investigate the reasons behind SVFTs effectiveness in achieving CoT compression. As SVFT trains the model to perform binary classification based on the correctness of CoT solutions, the model inherently learns to focus on the crucial elements in the CoT affecting solution correctness. We hypothesize that this enhanced attention allows the model, during reasoning tasks, to more accurately determine whether self-reflection step is necessary, thereby eliminating redundant double-checking and enabling effective CoT compression. To empirically validate this hypothesis, we randomly selected 50 problems from the MATH500 dataset and generated correct long CoT solutions for each problem using the R1-distill-Qwen-7B model. We extracted sub-solutions from each solution, spanning from the start until the appearance of the first pivot word \"Wait.\" We then performed forward computations using the original R1-DistillQwen-7B, the SFT model and the SVFT model on these question-sub-solution pairs, examining the generation probabilities of the pivot word \"Wait.\" As shown in Figure 4 (c), we found that both the SFT and SVFT models exhibit significantly lower probability of generating the pivot word for correct sub-solutions compared to the original model. Next, we manually introduced mistakes into each correct sub-solutionfor instance, changing \"3 7 = 21\" to \"3 7 = 27\"and evaluated the models probabilities for generating the pivot word \"Wait\" again. In this case, the SVFT model showed no probability reduction but instead marginal (1%) increase compared to the original model, while the SFT model continued to exhibit significantly reduced probabilities. These results strongly support our hypothesis: SVFT enhances the LRMs precision in evaluating reflection necessity. Specifically, for the SVFT model, we observe: (1) substantial reduction in redundant verification when preceding steps are correct, and (2) slight increase in reflection probability when mistakes are introduced. This adaptive behavior enables effective CoT compression while preserving and occasionally improving reasoning accuracy. In contrast, the SFT model uniformly reduces reflection probability regardless of solution correctness, explaining its chain-shortening capability at the cost of accuracy degradation. To further validate this hypothesis, we compared the changes in response token counts of the SVFTfinetuned model on the MATH500 dataset. For questions correctly answered by the SVFT model, the token count in the reasoning chain decreased by 41% compared to the original model. In contrast, for incorrectly answered questions, the token count decreased by only 8%. This experimental result further robustly supports our hypothesis. Conclusion: In summary, SVFT achieves effective CoT compression primarily by enhancing the accuracy of LRMs self-reflection decisions during reasoning. It minimizes unnecessary self-reflections for correct steps while retaining critical verification steps for mistaken steps, significantly reducing token usage without compromising reasoning accuracy."
        },
        {
            "title": "3.4 Solution-wise Speculative Reasoning",
            "content": "As LRMs acquire the capability to verify solution correctness through SVFT, we leverage this advancement to propose collaborative inference pipeline termed Solution-wise Speculative Reasoning (SSR). This approach significantly boosts inference throughput by orchestrating interactions between short-CoT LLM and an SVFT-enhanced LRM. Distinct from conventional token-wise speculative decoding methods [25, 3, 30, 60], where smaller draft model proposes multiple candidate tokens subsequently verified in parallel by larger LLM through token probability evaluation (accepting or rejecting each individual draft token), our approach introduces novel solution-level paradigm specifically tailored for reasoning tasks. Given problem, the short-CoT LLM first rapidly generates concise solution candidate. This candidate is then assessed by the SVFT-enhanced LRM for correctness verification. If the proposed solution is deemed correct, it is immediately output as final answer; otherwise, the system activates the LRMs full long-CoT reasoning process to ensure accurate results. This pipeline adaptively engages the LRM: For simple problems, the LRM only validates the draft solution without invoking costly long-CoT; For challenging problems, the LRM intervenes to ensure accuracy when the draft fails. By selectively triggering long-CoT reasoning based on problem difficulty and draft quality, our speculative reasoning pipeline achieves substantial throughput gains while preserving high reasoning accuracy. 6 Table 1: CoT compression performance. We evaluate the proposed VeriThinker on three advanced reasoning models, comparing token efficiency and accuracy against the original model and baseline methods over three mathematical reasoning benchmarks. Method Original Model Truncating Fast-Prompting CoT-Valve SFT VeriThinker Original Model Truncating Fast-Prompting CoT-Valve SFT VeriThinker Original Model Truncating Fast-Prompting CoT-Valve SFT VeriThinker MATH500 AIME 2024 AIME Average Tokens Accuracy Tokens Accuracy Tokens Accuracy Tokens Accuracy 3791 2306 2425 2440 2064 2125 3529 2479 2405 2102 2226 4361 3193 3378 4614 2439 2953 94.0% 80.4% 83.8% 92.4% 91.2% 94.8% 95.2% 85.8% 82.0% 91.0% 92.8% 95.0% 90.6% 81.6% 86.2% 81.4% 81.2% 89.9% Deepseek-R1-Distill-Qwen-7B 13108 9550 11867 11238 8843 54.1% 45.3% 53.3% 39.2% 49.1% 56.5% 14321 10232 13378 10884 9686 10287 Deepseek-R1-Distill-Qwen-14B 11724 8658 8288 8691 9021 7423 69.0% 57.1% 64.0% 41.7% 58.3% 73.0% 13409 9247 10645 9558 10644 Deepseek-R1-Distill-Llama-8B 14005 11577 12615 12650 9500 11285 44.6% 39.1% 40.4% 23.3% 31.3% 46.9% 14420 11635 13292 12520 10136 10557 38.7% 36.3% 38.3% 30.4% 32.1% 40.8% 49.6% 42.9% 49.3% 25.0% 43.3% 54.8% 30.1% 28.6% 29.4% 17.5% 26.0% 29.7% 10407 7363(-29%) 9223(-30%) 8187(-22%) 6864(-34%) 7264(-30%) 62.3% 54.0%(-8.3%) 58.5%(-3.8%) 54.0%(-8.3%) 57.5%(-4.8%) 64.0%(+1.7%) 9554 6795(-30%) 7113(-26%) 6783(-29%) 7297(-24%) 6327(-34%) 71.2% 61.9%(-9.3%) 65.1%(-6.1%) 52.6%(-18%) 64.8%(-7.4%) 74.3%(+3.1%) 10928 8802(-20%) 9762(-11%) 9928(-9%) 7358(-33%) 8265(-24%) 55.1% 49.8%(-5.3%) 52.0%(-3.1%) 40.7%(-14%) 46.2%(-8.7%) 55.5%(+0.4%)"
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "Models. To comprehensively evaluate our approach, we apply the proposed VeriThinker to three state-of-the-art long-CoT reasoning models with varying architectures and sizes, including DeepSeekR1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B, and DeepSeek-R1-Distill-LLaMA-8B [13]. Additionally, we also evaluate our approach on three short-CoT models: Qwen-2.5-Math-7B-Instruct, Qwen-2.5-Math-1.5B-Instruct [65], and Qwen-2.5-7B-Instruct [65]. Training and Evaluation Details. For the proposed SVFT, we adopt Low-Rank Adaptation (LoRA) [18] for efficient fine-tuning, which significantly improves training efficiency and effectively mitigates catastrophic forgetting, as training and inference occur on different tasks. All training procedures are conducted using Hugging Faces SFTTrainer integrated with DeepSpeed ZeRO-2 optimization [47], distributed across four RTX 6000 Ada GPUs. For evaluation, inference results and throughput metrics are also obtained using RTX 6000 Ada GPUs with the vLLM inference framework [23]. We adhere to the models default sampling configurations, specifically setting temperature to 0.6 and top-p to 0.95. During inference, the prompt appended to each question is: \"Please reason step by step, and put your final answer with boxed.\" To evaluate LRMs performance, we report the reasoning accuracy and the average token count of the reasoning chain (between <think> and </think>). Datasets. During the fine-tuning phase, we utilize our self-constructed CoT-verification dataset comprising approximately 340k question-CoT pairs, each labeled with correctness indicators. We provide more details about training set construction in the Appendix. For evaluation, we employ multiple mathematical benchmark datasets, including MATH500 [16], GSM8K [6], and two highly challenging competition datasets, AIME2024 and AIME2025. The results reported for each dataset represent averages computed over 2 to 16 independent runs, depending on the datasets size. Baselines. To demonstrate the superiority of our innovative method, we compare it against several conventional CoT compression approaches, including: (1) Truncation, which reduces the maximum allowable token length; (2) Fast-Prompting, method leveraging prompt engineering to enforce reasoning completion within specified token limit; (3) CoT-Valve [41], enabling model to dynamically adjust the length of reasoning chains; and (4) SFT [57], supervised fine-tuning method that uses synthesized concise CoT chains as targets for long-to-short compression. We measure reasoning accuracy under comparable CoT compression rates to compare effectiveness. 7 Table 2: CoT correctness verification results. We apply VeriThinker to three advanced LRMs and assess their verification accuracy on CoT solutions generated by two short-CoT models. For comprehensive analysis, we report classification accuracy, precision, recall, and F1 score. Method MATH500 GSM8K Acc. Precision Recall F1 Score Acc. Precision Recall F1 Score R1-Distill-Qwen-7B+VeriThinker R1-Distill-Qwen-14B+VeriThinker R1-Distill-Llama-8B+VeriThinker 88.4% 90.6% 88.0% QWEN-2.5-MATH-1.5B-Instruct 0.924 0.926 0.927 0.953 0.892 0.954 0.926 0.940 0. R1-Distill-Qwen-7B+VeriThinker R1-Distill-Qwen-14B+VeriThinker R1-Distill-Llama-8B+VeriThinker 89.4% 91.0% 87.4% QWEN-2.5-MATH-7B-Instruct 0.931 0.922 0.880 0.939 0.947 0.924 0.946 0.973 0.972 93.6% 95.7% 91.5% 94.8% 95.5% 93.9% 0.964 0.971 0.954 0.981 0.986 0.981 0.962 0.978 0.947 0.965 0.966 0.955 0.963 0.975 0. 0.973 0.976 0.968 Table 3: Speculative Reasoning Results. We applied VeriThinker to three LRMs and employed two short-CoT LLMs as draft models to evaluate the performance of solution-wise speculative reasoning. We report accuracy, token counts, throughput, and AcR (long-CoT reasoning Activation Rate). Underlined numbers indicate the token counts during the draft phase for short-CoT LLMs. Method MATH500 GSM8K Acc Tokens Throughput AcR Acc Tokens Throughput AcR R1-Distill-Qwen-7B +Qwen-2.5-Math-1.5B-Instruct +Qwen-2.5-Math-7B-Instruct 94.0% 91.8% 93.4% 3791 589+889 614+656 R1-Distill-Qwen-14B +Qwen-2.5-Math-1.5B-Instruct +Qwen-2.5-Math-7B-Instruct 3529 95.2% 93.0% 589+1130 614+954 95.7% R1-Distill-Llama-8B +Qwen-2.5-Math-1.5B-Instruct +Qwen-2.5-Math-7B-Instruct 90.6% 4361 90.4% 589+1589 91.6% 614+ 1.0x 4.4x 4.2x 1.0x 3.6x 3.5x 1.0x 3.3x 3.2x 100.0% 92.8% 1555 20.8% 93.3% 321+229 14.6% 96.1% 294+113 1309 100.0% 95.6% 22.8% 95.1% 321+175 294+93 17.6% 96.6% 100.0% 89.2% 1643 25.8% 91.7% 321+307 21.2% 96.4% 294+ 1.0x 5.1x 7.1x 1.0x 5.3x 5.4x 1.0x 4.7x 4.8x 100.0% 14.0% 5.0% 100.0% 13.2% 5.5% 100.0% 14.5% 6.0%"
        },
        {
            "title": "4.2 Experimental Results and Analysis",
            "content": "CoT Compression Results. We present comprehensive experimental results on CoT compression in Table 1. Our VeriThinker significantly reduces the number of tokens while maintaining or even slightly improving reasoning accuracy. In contrast, other baseline methods fail to achieve good trade-off between efficiency and accuracy. When the accuracy is successfully maintained, the length of the reasoning chain shows little variation; when the token count is significantly reduced, the reasoning accuracy inevitably decreases substantially - this phenomenon is particularly evident in challenging problems. Among the baselines, the SFT method using target chain as optimization objective, while significantly reducing token count, leads to notable degradation in the models self-reflection capability. Consequently, some incorrect steps fail to be double-checked, resulting in erroneous outcomes. Our VeriThinker, by contrast, enables the model to distinguish between correct and incorrect solutions, enhancing its ability to judge when self-checking is needed, thereby maximally preserving reasoning accuracy with even slight improvements. As shown in the Table 1, our method demonstrates consistent superior performance across all three LLMs, with particularly outstanding results on the challenging AIME dataset. Our approach reduces the CoT length by approximately 29% (from 13,108 to 9,381 tokens) for the R1-Distill-Qwen-7B model on AIME 2024 while increasing reasoning accuracy by 2.4%. On AIME 2025, the token count decreases by about 28% (from 14,321 to 10,287) with 2.1% accuracy improvement. We also observe that our method shows higher sensitivity to CoT compression on LLaMA architecture models. Although VeriThinker maintains good accuracy preservation on R1-Distill-LLaMA-8B, the compression ratio of reasoning chains is relatively smaller. These comprehensive experimental results strongly support and highlight the contributions of our method. CoT Correctness Verification Results. As VeriThinker trains LLMs to verify the correctness of CoT solutions, we conducted additional experiments to analyze its verification accuracy. Specifically, we generated CoT solutions using Qwen-2.5-Math-1.5B-Instruct and Qwen-2.5-Math-7B-Instruct on GSM8K and Math500 datasets. We then employed SVFT LRMs to evaluate the correctness of these solutions. As shown in Table 2, all three SVFT LRMs demonstrate high verification accuracy. The 8 Figure 3: Speculative reasoning results on three reasoning models. When using Qwen-2.5-MathInstruct-7B as the draft model, most problems in MATH500 and GSM8K can be solved with short CoT model, while only few require activation of the long CoT model for more complex solutions. verification accuracy on GSM8K is higher than on MATH500, indicating that more concise and clear solutions are easier to classify correctly. We also observe positive correlation between the models verification accuracy and its reasoning capability. For reasoning tasks, the performance ranking is R1-Distill-14B > R1-Distill-7B > R1-Distill-8B, and the verification accuracy follows exactly the same trend. Furthermore, we find that LLMs generally exhibit notably higher precision than recall. This suggests that LLMs adopt conservative approach when judging solution correctness - they rarely misclassify incorrect solutions as correct ones. Speculative Reasoning Results. The above experiments demonstrate that SVFT LRMs achieve high verification accuracy, particularly in precision. Building on this strength, we conduct extensive experiments to assess their performance in speculative reasoning tasks. We employ Qwen-2.5-Math1.5B-Instruct and Qwen-2.5-Math-7B-Instruct as draft models to observe the performance of SVFTenhanced R1-Distill-7B, R1-Distill-8B, and R1-Distill-14B on solution-wise speculative reasoning. In addition to accuracy and average token count, we report throughput (the time cost for completing all problems in the dataset) and AcR (the activation ratio of LRM), which indicates the proportion of problems that activate the LRMs long-CoT reasoning. As shown in Table 3, speculative reasoning significantly improves the LRMs throughput while maintaining or even enhancing reasoning accuracy. This is attributed to its remarkably low AcR. For instance, as presented in Figure 3, when using Qwen-2.5-Math-7B-Instruct as the draft model, R1-Distill-7B only needs to conduct its own reasoning for 14.6% of problems in MATH500 and 5% in GSM8K. This implies that the draft model solves the majority of problems, while the LRM is only engaged for the few truly challenging problems requiring long-CoT reasoning, resulting in substantial efficiency gains. Speculative reasoning also boosts accuracy, particularly on GSM8K, because the LRM sometimes underperforms the short-CoT model on simple problems, and in these cases, it accepts the correct draft solution. The overall accuracy of speculative reasoning approximates the union of the draft models and LRMs accuracies. Unlike traditional speculative decoding, we observe that for speculative reasoning, smaller draft model does not necessarily yield higher throughput. This is because the computational cost of short-CoT LLMs is negligible compared to the substantial inference cost of LRMs. For example, on the MATH500 dataset, Qwen-2.5-Math-7B-Instruct achieves approximately 20 the throughput of R1-Distill-7B. Thus, the primary factor affecting speedup is AcR: the fewer problems requiring LRM activation, the greater the throughput improvement. Larger draft models typically exhibit lower AcR due to their higher accuracy, thus not suffering throughput disadvantages compared to smaller draft models. However, this raises potential issue: as problem difficulty increases, the throughput benefits of speculative reasoning may diminish. Applied to Short-CoT LLM. We also applied VeriThinker to Qwen-2.5-Math-Instruct and Qwen-2.5-Instruct models to explore the effect of SVFT on short-CoT LLMs. As shown in Table 4, since short-CoT does not suffer from overthinking, our method does not significantly alter the average token count. However, SVFT improves reasoning accuracy on both MATH500 and GSM8K datasets, indicating that learning to verify helps the LLM better focus on key elements that influence reasoning correctness, thereby enhancing reasoning performance. Table 4: Results on Short-CoT LLMs. We apply VeriThinker to three short-CoT models and evaluate their reasoning performance. Method MATH GSM8K Tokens Acc. Tokens Acc. Qwen-2.5-Math-7B-Instruct + VeriThinker Qwen-2.5-Math-1.5B-Instruct + VeriThinker Qwen-2.5-7B-Instruct + VeriThinker 700 614 568 589 635 577 85.0% 87.6% 77.0% 80.2% 78.6% 80.2% 319 304 314 321 294 285 95.6% 96.7% 85.6% 86.1% 91.8% 92.4%"
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we introduce VeriThinker, novel approach for CoT compression. Our key innovation is new fine-tuning method called supervised verification fine-tuning. For the first time, we fine-tune the LRM on an auxiliary verification task instead of the reasoning task itself to achieve effective CoT compression. This approach eliminates the dependency on target chain data, which can be difficult and expensive to obtain. We have performed an in-depth analysis to explain the underlying principles of the proposed methods. Extensive experiments demonstrate that our method significantly shortens the reasoning chain while preserving accuracy, even on highly challenging problems. Additionally, VeriThinker can be generalized to speculative decoding, achieving substantially higher throughput."
        },
        {
            "title": "References",
            "content": "[1] Pranjal Aggarwal and Sean Welleck. L1: Controlling how long reasoning model thinks with reinforcement learning. arXiv preprint arXiv:2503.04697, 2025. [2] Daman Arora and Andrea Zanette. Training language models to reason efficiently. arXiv preprint arXiv:2502.04463, 2025. [3] Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason Lee, Deming Chen, and Tri Dao. Medusa: Simple llm inference acceleration framework with multiple decoding heads. arXiv preprint arXiv:2401.10774, 2024. [4] Xinghao Chen, Zhijing Sun, Wenjin Guo, Miaoran Zhang, Yanjun Chen, Yirong Sun, Hui Su, Yijie Pan, Dietrich Klakow, Wenjie Li, et al. Unveiling the key factors for distilling chain-of-thought reasoning. arXiv preprint arXiv:2502.18001, 2025. [5] Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, et al. Do not think that much for 2+ 3=? on the overthinking of o1-like llms. arXiv preprint arXiv:2412.21187, 2024. [6] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [7] Karan Dalal, Daniel Koceja, Gashon Hussein, Jiarui Xu, Yue Zhao, Youjin Song, Shihao Han, Ka Chun Cheung, Jan Kautz, Carlos Guestrin, et al. One-minute video generation with test-time training. arXiv preprint arXiv:2504.05298, 2025. [8] Yuntian Deng, Yejin Choi, and Stuart Shieber. From explicit cot to implicit cot: Learning to internalize cot step by step. arXiv preprint arXiv:2405.14838, 2024. [9] Yuntian Deng, Kiran Prasad, Roland Fernandez, Paul Smolensky, Vishrav Chaudhary, and Stuart Shieber. Implicit chain of thought reasoning via knowledge distillation. arXiv preprint arXiv:2311.01460, 2023. [10] Gongfan Fang, Xinyin Ma, and Xinchao Wang. Thinkless: Llm learns when to think. arXiv preprint arXiv:2505.13379, 2025. [11] Jonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, and Tom Goldstein. Scaling up test-time compute with latent reasoning: recurrent depth approach. arXiv preprint arXiv:2502.05171, 2025. [12] Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang. rstar-math: Small llms can master math reasoning with self-evolved deep thinking. arXiv preprint arXiv:2501.04519, 2025. [13] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [14] Tingxu Han, Zhenting Wang, Chunrong Fang, Shiyu Zhao, Shiqing Ma, and Zhenyu Chen. Token-budgetaware llm reasoning. arXiv preprint arXiv:2412.18547, 2024. [15] Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. Training large language models to reason in continuous latent space. arXiv preprint arXiv:2412.06769, 2024. 10 [16] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. [17] Bairu Hou, Yang Zhang, Jiabao Ji, Yujian Liu, Kaizhi Qian, Jacob Andreas, and Shiyu Chang. Thinkprune: Pruning long chain-of-thought of llms via reinforcement learning. arXiv preprint arXiv:2504.01296, 2025. [18] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. [19] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. [20] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [21] Dongzhi Jiang, Ziyu Guo, Renrui Zhang, Zhuofan Zong, Hao Li, Le Zhuo, Shilin Yan, Pheng-Ann Heng, and Hongsheng Li. T2i-r1: Reinforcing image generation with collaborative semantic-level and token-level cot. arXiv preprint arXiv:2505.00703, 2025. [22] Levente Kocsis and Csaba Szepesvári. Bandit based monte-carlo planning. In European conference on machine learning, pages 282293. Springer, 2006. [23] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pages 611626, 2023. [24] Ayeong Lee, Ethan Che, and Tianyi Peng. How well do llms compress their own chain-of-thought? token complexity approach. arXiv preprint arXiv:2503.01141, 2025. [25] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pages 1927419286. PMLR, 2023. [26] Chen Li, Nazhou Liu, and Kai Yang. Adaptive group policy optimization: Towards stable training and token-efficient reasoning. arXiv preprint arXiv:2503.15952, 2025. [27] Chenglin Li, Qianglong Chen, Liangyue Li, Caiyu Wang, Yicheng Li, Zulong Chen, and Yin Zhang. Mixed distillation helps smaller language model better reasoning. arXiv preprint arXiv:2312.10730, 2023. [28] Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vulic, and Furu Wei. Imagine while reasoning in space: Multimodal visualization-of-thought. arXiv preprint arXiv:2501.07542, 2025. [29] Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu. Numinamath. [https://huggingface.co/AI-MO/NuminaMath-1.5](https: //github.com/project-numina/aimo-progress-prize/blob/main/report/numina_dataset. pdf), 2024. [30] Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. Eagle: Speculative sampling requires rethinking feature uncertainty. arXiv preprint arXiv:2401.15077, 2024. [31] Baohao Liao, Yuhui Xu, Hanze Dong, Junnan Li, Christof Monz, Silvio Savarese, Doyen Sahoo, and Caiming Xiong. Reward-guided speculative decoding for efficient llm reasoning. arXiv preprint arXiv:2501.19324, 2025. [32] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. [33] Fangfu Liu, Hanyang Wang, Yimo Cai, Kaiyan Zhang, Xiaohang Zhan, and Yueqi Duan. Video-t1: Test-time scaling for video generation. arXiv preprint arXiv:2503.18942, 2025. [34] Ruikang Liu, Yuxuan Sun, Manyi Zhang, Haoli Bai, Xianzhi Yu, Tiezheng Yu, Chun Yuan, and Lu Hou. Quantization hurts reasoning? an empirical study on quantized reasoning models. arXiv preprint arXiv:2504.04823, 2025. 11 [35] Tengxiao Liu, Qipeng Guo, Xiangkun Hu, Cheng Jiayang, Yue Zhang, Xipeng Qiu, and Zheng Zhang. Can language models learn to skip steps? arXiv preprint arXiv:2411.01855, 2024. [36] Yule Liu, Jingyi Zheng, Zhen Sun, Zifan Peng, Wenhan Dong, Zeyang Sha, Shiwen Cui, Weiqiang Wang, and Xinlei He. Thought manipulation: External thought can be efficient for large reasoning models. arXiv preprint arXiv:2504.13626, 2025. [37] Haotian Luo, Haiying He, Yibo Wang, Jinluan Yang, Rui Liu, Naiqiang Tan, Xiaochun Cao, Dacheng Tao, and Li Shen. Adar1: From long-cot to hybrid-cot via bi-level adaptive reasoning optimization. arXiv preprint arXiv:2504.21659, 2025. [38] Haotian Luo, Li Shen, Haiying He, Yibo Wang, Shiwei Liu, Wei Li, Naiqiang Tan, Xiaochun Cao, and Dacheng Tao. O1-pruner: Length-harmonizing fine-tuning for o1-like reasoning pruning. arXiv preprint arXiv:2501.12570, 2025. [39] Chang Ma, Haiteng Zhao, Junlei Zhang, Junxian He, and Lingpeng Kong. Non-myopic generation of language models for reasoning and planning. arXiv preprint arXiv:2410.17195, 2024. [40] Nanye Ma, Shangyuan Tong, Haolin Jia, Hexiang Hu, Yu-Chuan Su, Mingda Zhang, Xuan Yang, Yandong Li, Tommi Jaakkola, Xuhui Jia, et al. Inference-time scaling for diffusion models beyond scaling denoising steps. arXiv preprint arXiv:2501.09732, 2025. [41] Xinyin Ma, Guangnian Wan, Runpeng Yu, Gongfan Fang, and Xinchao Wang. Cot-valve: Lengthcompressible chain-of-thought tuning. arXiv preprint arXiv:2502.09601, 2025. [42] Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with reference-free reward. Advances in Neural Information Processing Systems, 37:124198124235, 2024. [43] Tergel Munkhbat, Namgyu Ho, Seo Hyun Kim, Yongjin Yang, Yujin Kim, and Se-Young Yun. Self-training elicits concise reasoning in large language models. arXiv preprint arXiv:2502.20122, 2025. [44] Xuefei Ning, Zinan Lin, Zixuan Zhou, Zifu Wang, Huazhong Yang, and Yu Wang. Skeleton-of-thought: Prompting llms for efficient parallel generation. arXiv preprint arXiv:2307.15337, 2023. [45] Jiayi Pan, Xiuyu Li, Long Lian, Charlie Snell, Yifei Zhou, Adam Yala, Trevor Darrell, Kurt Keutzer, and Alane Suhr. Learning adaptive parallel reasoning with language models. arXiv preprint arXiv:2504.15466, 2025. [46] Rui Pan, Yinwei Dai, Zhihao Zhang, Gabriele Oliaro, Zhihao Jia, and Ravi Netravali. Specreason: Fast and accurate inference-time compute via speculative reasoning. arXiv preprint arXiv:2504.07891, 2025. [47] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pages 35053506, 2020. [48] Xuan Shen, Yizhou Wang, Xiangxi Shi, Yanzhi Wang, Pu Zhao, and Jiuxiang Gu. Efficient reasoning with hidden thinking. arXiv preprint arXiv:2501.19201, 2025. [49] Yi Shen, Jian Zhang, Jieyun Huang, Shuming Shi, Wenjing Zhang, Jiangze Yan, Ning Wang, Kai Wang, and Shiguo Lian. Dast: Difficulty-adaptive slow-thinking for large reasoning models, 2025. URL https://arxiv. org/abs/2503.04472. [50] Gaurav Srivastava, Shuxiang Cao, and Xuan Wang. Towards reasoning ability of small language models. arXiv preprint arXiv:2502.11569, 2025. [51] Hanshi Sun, Momin Haider, Ruiqi Zhang, Huitao Yang, Jiahao Qiu, Ming Yin, Mengdi Wang, Peter Bartlett, and Andrea Zanette. Fast best-of-n decoding via speculative rejection. arXiv preprint arXiv:2410.20290, 2024. [52] Yi Sun, Han Wang, Jiaqiang Li, Jiacheng Liu, Xiangyu Li, Hao Wen, Huiwen Zheng, Yan Liang, Yuanchun Li, and Yunxin Liu. Time up! an empirical study of llm reasoning ability under output length constraint. arXiv preprint arXiv:2504.14350, 2025. [53] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. [54] Qwen Team. https://qwenlm.github.io/blog/qwq-32b-preview/, 2024. 12 [55] Junlin Wang, Shang Zhu, Jon Saad-Falcon, Ben Athiwaratkun, Qingyang Wu, Jue Wang, Shuaiwen Leon Song, Ce Zhang, Bhuwan Dhingra, and James Zou. Think deep, think fast: Investigating efficiency of verifier-free inference-time-scaling methods. arXiv preprint arXiv:2504.14047, 2025. [56] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. [57] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew Dai, and Quoc Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021. [58] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [59] Heming Xia, Yongqi Li, Chak Tou Leong, Wenjie Wang, and Wenjie Li. Tokenskip: Controllable chain-of-thought compression in llms. arXiv preprint arXiv:2502.12067, 2025. [60] Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, and Zhifang Sui. Unlocking efficiency in large language model inference: comprehensive survey of speculative decoding. arXiv preprint arXiv:2401.07851, 2024. [61] Enze Xie, Junsong Chen, Yuyang Zhao, Jincheng Yu, Ligeng Zhu, Chengyue Wu, Yujun Lin, Zhekai Zhang, Muyang Li, Junyu Chen, et al. Sana 1.5: Efficient scaling of training-time and inference-time compute in linear diffusion transformer. arXiv preprint arXiv:2501.18427, 2025. [62] Guowei Xu, Peng Jin, Li Hao, Yibing Song, Lichao Sun, and Li Yuan. Llava-o1: Let vision language models reason step-by-step. arXiv preprint arXiv:2411.10440, 2024. [63] Haoran Xu, Baolin Peng, Hany Awadalla, Dongdong Chen, Yen-Chun Chen, Mei Gao, Young Jin Kim, Yunsheng Li, Liliang Ren, Yelong Shen, et al. Phi-4-mini-reasoning: Exploring the limits of small reasoning language models in math. arXiv preprint arXiv:2504.21233, 2025. [64] Silei Xu, Wenhao Xie, Lingxiao Zhao, and Pengcheng He. Chain of draft: Thinking faster by writing less. arXiv preprint arXiv:2502.18600, 2025. [65] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024. [66] Chenxu Yang, Qingyi Si, Yongjie Duan, Zheliang Zhu, Chenyu Zhu, Zheng Lin, Li Cao, and Weiping Wang. Dynamic early exit in reasoning models. arXiv preprint arXiv:2504.15895, 2025. [67] Junjie Yang, Ke Lin, and Xing Yu. Think when you need: Self-adaptive chain-of-thought learning. arXiv preprint arXiv:2504.03234, 2025. [68] Wenkai Yang, Shuming Ma, Yankai Lin, and Furu Wei. Towards thinking-optimal scaling of test-time compute for llm reasoning. arXiv preprint arXiv:2502.18080, 2025. [69] Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning. arXiv preprint arXiv:2502.03387, 2025. [70] Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. Demystifying long chain-ofthought reasoning in llms. arXiv preprint arXiv:2502.03373, 2025. [71] Runpeng Yu, Xinyin Ma, and Xinchao Wang. Introducing visual perception token into multimodal large language model. arXiv preprint arXiv:2502.17425, 2025. [72] Nan Zhang, Yusen Zhang, Prasenjit Mitra, and Rui Zhang. When reasoning meets compression: Benchmarking compressed large reasoning models on complex reasoning tasks. arXiv preprint arXiv:2504.02010, 2025. [73] Xunyu Zhu, Jian Li, Can Ma, and Weiping Wang. Improving mathematical reasoning capabilities of small language models via feedback-driven distillation. arXiv preprint arXiv:2411.14698, 2024. 13 In this document, we provide supplementary materials that extend beyond the scope of the main manuscript, constrained by space limitations. These additional materials include in-depth information about training details, dataset construction, limitations, social impact, and case studies."
        },
        {
            "title": "A Training Details",
            "content": "We provide additional training details in this section. We employ Low-Rank Adaptation (LoRA) [18] for efficient fine-tuning, which significantly enhances training efficiency and effectively mitigates catastrophic forgetting, as training and inference are performed on different tasks. Our LoRA configurations are presented in Table 5. We utilized different LoRA ranks and alpha values for the three distinct models to achieve the optimal balance between underfitting and catastrophic forgetting. All other training hyperparameters remain consistent across models: learning rate = 3e-5, LoRA dropout = 0.05, weight decay = 0.01, and batch size = 64. All models were trained for 2 epochs on our self-constructed CoT-Verification dataset. Table 5: The LoRA configuration in our training process. Models LoRA Module LoraLoRARank LoRA Alpha DeepSeek-R1-Distill-Qwen-7B DeepSeek-R1-Distill-Qwen-14B DeepSeek-R1-Distill-Llama-8B QKVO QKVO QKVO 256 128 128 512 128 128 Construct the CoT-Verification Dataset As SVFT trains the LLM to directly distinguish whether CoT solution is correct, crucial challenge lies in constructing the CoT verification dataset for SVFT. Problem Collection. The first step involves collecting problems for our dataset. To ensure diverse range of topics and difficulty levels, we aggregate problems from four mathematical datasets known for their breadth of content and varying difficulties: PRM12K [32], GSM8K [6], LIMO [69], and Numina-Math [29]. Specifically, we extract all problems from the training set of PRM12K, GSM8K, and LIMO. For Numina-Math, we only select problems whose solutions are integer-valued math-word-problems , simplifying correctness labeling. Following this procedure, we collected approximately 300K mathematical problems spanning diverse topics and difficulty levels. CoT Solution Collection. The second step entails generating numerous CoT solutions for the collected problems, which will serve as targets for our verification task. To achieve efficiency and variety, we employ several small, non-reasoning language models: Qwen-2.5-0.5B-Instruct, Qwen-2.5-1.5B-Instruct, Qwen-2.5-7B-Instruct, Qwen-Math-1.5B-Instruct, and Qwen-Math-7B-Instruct [65]. We utilize CoT prompting strategy to generate step-by-step solutions for the previously collected 300K problems. Notably, none of our CoT solutions are generated by reasoning models, and none include explicit self-reflection steps. Experiments indicate that reasoning models producing long-chain solutions complicate optimization in SVFT, as single response might contain multiple correct or incorrect sub-steps. This collection method offers two main advantages: Computational efficiency: All selected models are lightweight and only generating shortCoT solutions, significantly accelerating the data generation process. For instance, using Qwen-Math-1.5B-Instruct, we generate solutions for all 300K problems within 3 to 4 hours using only four NVIDIA A6000 GPUs. Solution diversity: The selected models exhibit varying reasoning capabilities, producing diverse set of correct and incorrect solutions for each problem. This diversity greatly enhances the robustness of subsequent fine-tuning. Consequently, we obtain five different short CoT solutions per problem, yielding total of 1.5 million problem-solution pairs. Figure 4: Training data format comparison. Correctness Labeling. In the third step, we label the correctness of each generated CoT solution. Rather than evaluating each reasoning step, we simplify labeling by verifying only the final answers against known ground-truth solutions. As all collected problems have deterministic solutions, correctness labeling is straightforward: we employ the Hugging Face math_verify function to automatically extract final answers from CoT solutions and compare them against ground truths. Following this procedure, each of the 1.5 million problem-solution pairs is labeled as either correct or incorrect. Verification Data Selection. Finally, we select subset from the 1.5 million labeled pairs for fine-tuning. Initially, we discard problems where all five CoT solutions are uniformly correct or incorrect. Such problems lack informative training signals due to being either too trivial or excessively challenging, and this process also helps filter out inherently problematic data. Next, we apply straightforward deduplication strategy using Qwen-Math-1.5B-Instruct as reference model. Specifically, for problems correctly solved by the reference model, we retain its correct solutions along with incorrect solutions generated by the other models. Conversely, for problems incorrectly solved by the reference model, we retain its incorrect solutions and also incorrect solutions from the other models. This selection strategy ensures each problem contributes at least one correct and one incorrect CoT solution. Ultimately, this process yields fine-tuning dataset consisting of 350K instances, comprising approximately 160K correct and 190K incorrect CoT solutions. Each instance is reformatted according to the structure illustrated in Figure 1."
        },
        {
            "title": "C Limitations",
            "content": "The primary limitation of our approach is that it does not effectively enable CoT compression in smaller models (e.g., 1.5B-parameter reasoning model). This is largely due to our fine-tuning strategy: instead of optimizing directly on the original reasoning task, we fine-tune the model on CoT verification as an auxiliary task. Since smaller models have limited capacity, they are more prone to catastrophic forgetting during supervised verification fine-tuning (SVFT). As result, they struggle to maintain high reasoning accuracy while performing effective CoT compression. 15 Figure 5: Case study 1 on CoT Compression."
        },
        {
            "title": "D Social Impacts",
            "content": "In this paper, we propose VeriThinker, simple yet effective method for compressing reasoning chains while preserving high accuracy. As large reasoning models become increasingly prevalent, their lengthy reasoning chains lead to dramatically higher inference costs, hindering efficient deployment and limiting practical utility. Our method addresses this critical challenge by significantly reducing token usage without compromising reasoning performance, thereby enhancing the real-world applicability of reasoning models."
        },
        {
            "title": "E Case Study",
            "content": "We also present additional case studies on cot compression in Figure 5, Figure 6, and Figure 7. 16 Figure 6: Case study 2 on CoT Compression. 17 Figure 7: Case study 3 on CoT Compression."
        }
    ],
    "affiliations": [
        "National University of Singapore"
    ]
}