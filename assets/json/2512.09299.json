{
    "paper_title": "VABench: A Comprehensive Benchmark for Audio-Video Generation",
    "authors": [
        "Daili Hua",
        "Xizhi Wang",
        "Bohan Zeng",
        "Xinyi Huang",
        "Hao Liang",
        "Junbo Niu",
        "Xinlong Chen",
        "Quanqing Xu",
        "Wentao Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in video generation have been remarkable, enabling models to produce visually compelling videos with synchronized audio. While existing video generation benchmarks provide comprehensive metrics for visual quality, they lack convincing evaluations for audio-video generation, especially for models aiming to generate synchronized audio-video outputs. To address this gap, we introduce VABench, a comprehensive and multi-dimensional benchmark framework designed to systematically evaluate the capabilities of synchronous audio-video generation. VABench encompasses three primary task types: text-to-audio-video (T2AV), image-to-audio-video (I2AV), and stereo audio-video generation. It further establishes two major evaluation modules covering 15 dimensions. These dimensions specifically assess pairwise similarities (text-video, text-audio, video-audio), audio-video synchronization, lip-speech consistency, and carefully curated audio and video question-answering (QA) pairs, among others. Furthermore, VABench covers seven major content categories: animals, human sounds, music, environmental sounds, synchronous physical sounds, complex scenes, and virtual worlds. We provide a systematic analysis and visualization of the evaluation results, aiming to establish a new standard for assessing video generation models with synchronous audio capabilities and to promote the comprehensive advancement of the field."
        },
        {
            "title": "Start",
            "content": "VABench: Comprehensive Benchmark for Audio-Video Generation Daili Hua1*, Xizhi Wang4*, Bohan Zeng1*, Xinyi Huang1, Hao Liang1, Junbo Niu1 Xinlong Chen3, Quanqing Xu2, Wentao Zhang1 1Peking University 2Ant Group 3Institute of Automation, Chinese Academy of Sciences 4 Huazhong University of Science and Technology 5 2 0 2 0 1 ] . [ 1 9 9 2 9 0 . 2 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in video generation have been remarkable, enabling models to produce visually compelling videos with synchronized audio. While existing video generation benchmarks provide comprehensive metrics for visual quality, they lack convincing evaluations for audio-video generation, especially for models aiming to generate synchronized audio-video outputs. To address this gap, we introduce VABench, comprehensive and multi-dimensional benchmark framework designed to systematically evaluate the capabilities of synchronous audio-video generation. VABench encompasses three primary task types: textto-audio-video (T2AV), image-to-audio-video (I2AV), and stereo audio-video generation. It further establishes two major evaluation modules covering 15 dimensions. These dimensions specifically assess pairwise similarities (textvideo, text-audio, video-audio), audio-video synchronization, lip-speech consistency, and carefully curated audio and video question-answering (QA) pairs, among others. Furthermore, VABench covers seven major content categories: animals, human sounds, music, environmental sounds, synchronous physical sounds, complex scenes, and virtual worlds. We provide systematic analysis and visualization of the evaluation results, aiming to establish new standard for assessing video generation models with synchronous audio capabilities and to promote the comprehensive advancement of the field. 1. Introduction Video generation technology is rapidly evolving from early content synthesis to intelligent creation, driving innovations in automation across fields such as film production [35, 53] and artistic creation [4, 14, 28, 52]. Significant progress has been made in pure visual generation regarding reso- *These authors contributed equally. Corresponding Author: wentao.zhang@pku.edu.cn Project Repository: https : / / github . com / tanABCC /"
        },
        {
            "title": "VABench",
            "content": "lution and spatiotemporal consistency, supported by wellestablished evaluation systems (e.g., VBench [19], VBench 2.0 [59], Evaluation agent [58]). With growing user demands and the inherent coexistence of audio and video in the real world, video generation models with synchronous audio (e.g., Veo 3 [7], Sora 2 [34], Wan 2.5 [12]) are becoming new technical focus, marking the advent of truly unified audio-visual generation era. lacking. Despite preliminary achievements, systematic joint audio-video benchmark remains Existing works(e.g., JAVISDiT [27]), while exploratory, generally suffer from limited evaluation dimensions and constrained scenarios. More critically, they largely overlook unique multimodal coupling phenomena inherent in joint audiovideo generation, such as the Doppler effect caused by motion, the synergistic expression of character emotions across audio-visual modalities, and the coordination between background music and visual rhythm. Furthermore, while most current synchronous audio-video models output stereo audio, existing benchmarks lack evaluation tailored to the spatial acoustic properties of such audio. Therefore, there is an urgent need for comprehensive evaluation system that can balance generation quality, cross-modal semantic consistency, physical plausibility, and emotional expressiveness to address the complex challenges emerging in joint audio-video generation. To bridge this gap, we introduce VABench, comprehensive benchmark specifically designed for the holistic evaluation of synchronous audio-video generation. As shown in Fig. 1, VABench encompasses two primary audiovideo generation tasks, text-to-audio-video (T2AV) and image-to-audio-video (I2AV), and features diverse set of test content covering seven core sound categories including animals, human sounds, music, environmental sounds, synchronous physical sounds, complex scenes, and virtual worlds. This requires models not only to achieve synchronized audio but also to maintain fidelity to real-world internal logic. We employed human workers and large language models to filter testing samples and adjust the distribution of test data. To effectively evaluate audio-video generation 1 Figure 1. Overview of the VABench framework, illustrating its three main components: (1) The audio-video generation tasks being evaluated (T2AV, I2AV, and stereo), (2) the detailed taxonomy of evaluation contexts (e.g., human sounds, complex scenes), and (3) the evaluation pipeline. performance, VABench incorporates 15 fine-grained metrics, including 8 based on expert models and 7 based on multimodal large language models (MLLMs). These metrics cover critical dimensions such as audio-visual synchronization, lip-speech consistency, and cross-modal similarity, ensuring precise and domain-aware assessment of multimodal generation quality. Additionally, VABench introduces dual-channel stereo audio evaluation. Addressing the spatial acoustic properties often overlooked by current benchmarks, we provide dedicated test cases to measure the spatial auditory perception and sound field rendering capabilities of generated content. This enables VABench to more comprehensively evaluate audio-video generation capabilities and provides guidance for future joint audio-video generation technologies advancing toward higher-dimensional realism. Our main contributions are as follows: We propose VABench, comprehensive benchmark for audio-video generation covering two mainstream tasks (T2AV and I2AV). It introduces suite of 15 fine-grained metrics designed for systematic and effective evaluation. Our benchmarks test set spans seven major content categories. This comprehensive design pushes evaluation beyond simple perceptual coherence, assessing models grasp of complex real-world dynamics, including its understanding of physical logic and its ability to capture nuanced human emotional contexts. We also introduce stereo dual-channel audio evaluation into video generation benchmark. VABench provides dedicated test cases and metrics for spatial audio rendering, making our benchmark more comprehensive and offering guidance for the future development of audio-video generation technologies. 2. Related Works 2.1. Video Generation Models In recent years, diffusion models [3, 8, 1517, 30, 31, 48, 51] and Transformer architectures [9, 18, 31, 56, 57, 60] have jointly driven remarkable progress in video generation. Building upon these foundations, Sora [33] emerged as the first text-to-video model to deliver visually stunning and coherent long-duration results, showcasing the potential of large-scale training despite early challenges in spatial consistency. Subsequently, robust open-source models like Wan [47] set new benchmarks for text-to-video generation, while Seedance [10] pioneered reinforcement learning from human feedback (RLHF) to greatly improve motion quality and visual fidelity. 2.2. Audio-Visual Generation Video generation has evolved from visual-only synthesis to unified audiovideo generation, exemplified by Veo3 [7], Sora2 [34], and Wan2.5 [12], which pursue precise temporal synchronization between sound and motion. Some methods achieve audiovideo synthesis by integrating videoto-audio modules(V2A), making performance largely depend on V2A quality. Recent V2A models like MMAudio [6] and Kling-Foley [49] improved semantic controllability and event alignment through joint videotext conditioning, while DeepSound-V1 [23] and Thinksound [26] leveraged MLLM-based reasoning for better video understanding and audio fidelity. Yet, maintaining long-term consistency in complex scenes remains challenging. 2.3. Evaluation Benchmarks Evaluation methodologies have evolved alongside generation models. For visual content, traditional single metrics such as Inception Score (IS) [38] and Frechet Video Distance (FVD) [46] have been replaced by multi-dimensional frameworks. With the development of large models [1, 2, 5, 24, 25, 29, 40, 55], VBench [19] and VBench2.0 [59], cover frame-level quality, temporal consistency, and higherlevel realism like physical plausibility. While important for visual evaluation, they lack mechanisms to assess crossmodal consistency. Compared with the rapid progress in generation methods, evaluation benchmarks for joint audio-visual synthesis remain underdeveloped. Existing benchmarks [13, 41], mostly for V2A tasks, are reference-based, requiring real video and audio ground truthan approach unsuitable for T2AV generation, which instead needs reference-free evaluation of triangular consistency among text, video, and audio. Current evaluations are also constrained by (1) reliance on manual assessment (e.g., Movie Gen [35]), limiting scalability, and (2) limited coverage in holistic benchmarks (e.g., JavisBench [27]), lacking quantitative measures for higher-order couplings such as physical or emotional coherence. To address these gaps, VABench introduces finegrained, automated framework for multi-dimensional evaluation of audio-video generation models. 3. VABench In this section, we present the details of VABench. Sec. 3.1 details our three tasks and data categories. Sec. 3.2 briefly outlines the data generation and collection methods. Sec. 3.3 introduces the five levels of evaluation metrics and their specific implementation details. 3.1. Data Category 3.1.1. Task Category Text-to-Audio-Video Generation (T2AV). Translating textual semantics into coherent audio-visual sequences. Key challenges include achieving high-fidelity motion consistency and precise cross-modal semantic alignment. Image-to-Audio-Video Generation (I2AV). Inferring motion and synchronized audio from static image, with key challenges in ensuring action plausibility, temporal coherence, and precise audio-visual alignment. Stereophonic Audio Generation. Converting text into stereo audio with explicit spatial cues, challenged by accurate spatial interpretation and channel separation. Evaluation uses 116 prompts specifying distinct left/right sounds. 3.1.2. Content Category To evaluate sound realism, A-V consistency, and semantic plausibility, we developed seven-category taxonomy grounded in human auditory perception. Its rigor is shown as our dimensions encompass Kling-Foley-Evals [49] acoustic classes and address physical plausibilitya core Figure 2. Data distribution of VABench. The sunburst chart illustrates the hierarchical breakdown of our dataset across the seven major content categories and their sub-divisions. principle aligned with VBench 2.0s [59] visual-centric focus. The taxonomy spans basic sources, physical interactions, complex semantics, and non-realistic content, organized as follows. Animals. This category encompasses vocalizations across diverse species, focusing on accurate species-specific acoustic modeling and audio-visual behavioral consistency. Human Sounds. This category is subdivided into linguistic sounds involving semantic content, and non-linguistic sounds related to physiological states or actions. Music. This category covers structured audio content across genres. Evaluation focuses on melodic and rhythmic coherence, timbre authenticity, and alignment with visual emotional tone. Environmental Sounds. This category integrates three major soundscapes: natural, urban, and indoor environments. Synchronous Physical Sounds. This category targets immediate, rhythmic, or physical interaction sounds, demanding strict adherence to material properties and motion dynamics for precise audio-visual alignment. Complex Scenes. This category targets high-order scenarios spanning five dimensions: complex soundscapes, subjective feelings, world knowledge, symbolic associations, and invisible sound sources, demanding synergistic audiovisual reasoning. Virtual Worlds. This category targets non-realistic scenarios that transcend physical laws yet demand internal logical and stylistic coherence. Due to the unique nature of this category, it is exclusively featured in the T2AV task. To make our VABench more intuitive, we provide specific test cases in Fig. 3. 3 Text-to-Audio-Video (T2AV). We use expert templates and LLMs to batch-generate raw prompts, which are then used to create visual question-answer pairs (VQA) and audio question-answer pairs (AQA) pairs for evaluation. An LLM also structurally decouples these prompts into visual and auditory sub-prompts. final human verification step ensures correct categorization, element observability, and adherence to physical/commonsense constraints. Image-to-Audio-Video (I2AV). We curate and manually classify high-quality images, carefully excluding any content with privacy concerns. An MLLM then generate unified audio-visual descriptions detailing objective visuals and commonsense-inferred audio. These descriptions are simultaneously used to construct VQA/AQA pairs (verifying fidelity and plausibility) and decoupled by an LLM into sub-prompts. Human rechecks then validate auditory inferences and question discriminability. 3.3. Evaluation Metrics To comprehensively assess generated content, we propose dual-track framework combining specialized precision with holistic understanding. It includes two components: Expert Model-based Evaluation, which quantifies perceptual quality via specialized models, and MLLM-based Evaluation, which simulates human judgment of complex audio-visual semantics. 3.3.1. Expert Model-based Evaluation To assess high-quality synergy among text, visual, and audio modalities, our framework evaluates three critical dimensions: uni-modal quality, cross-modal semantic alignment, and temporal synchronization. Uni-modal Audio Quality - SpeechClarity. Assesses background noise and perceptual speech quality for the human linguistic sub-category. For implementation, we leverage the Overall Quality (OVRL) score from DNSMOS [37]. Due to space constraints, detailed discussion of this metric is provided in the supplementary material. Uni-modal Audio Quality - SpeechQual&Nat. Assesses overall speech quality and naturalness, again restricting this metric to the human linguistic subset. For implementation, we derive single Mean Opinion Score (MOS) prediction, leveraging NISQAv2 [32] (mos-only weights). Uni-modal Audio Quality - AudioAesthetic. Assesses audio aesthetic and production value via four key dimensions: Content Enjoyment (CE), Content Usefulness (CU), Production Complexity (PC), and Production Quality (PQ). We utilize the Audiobox [45] module for implementation. Inspired by [39], which suggests PC is inversely correlated with perceived quality, we define our aggregated score (higher is better) as: Saudioaesthetic = CE + CU + PQ PC 4 (1) Figure 3. VABenchs seven content categories, illustrated with example text prompts and representative images. Figure 4. Overview of the pipeline for benchmark data curation. This process is used to generate the text conditions for T2AV tasks and the image conditions for I2AV tasks. 3.2. Data Collection We employ dual-path strategy (T2AV and I2AV) to build high-fidelity dataset spanning seven categories, comprising 778 T2AV and 521 I2AV samples  (Fig. 2)  . Both pipelines utilize LLMs and VLMs to generate structured prompts and QA pairs, followed by rigorous human verification to ensure semantic accuracy and audio-visual consistency  (Fig. 4)  . 4 Cross-modal Semantic Alignment - Text-Video Align. Evaluates semantic consistency between generated videos and input text prompts. Given the complex temporal information in videos, we select ViCLIP [50], specifically designed for video understanding to more accurately capture dynamic correspondences. Cross-modal Semantic Alignment - Text-Audio Align. Assesses semantic consistency between generated audio and input text prompts. We utilize the CLAP [54] model to calculate the cosine similarity between audio and text embeddings. Cross-modal Semantic Alignment - Audio-Visual Align. Measures semantic matching between visual frames and audio track. We leverage ImageBinds [11] robust joint embedding space to calculate the similarity between visual and audio embeddings. Temporal Synchronization - Desync. Assesses finegrained temporal alignment between audio and visual streams. For implementation, we compute the predicted desynchronization offset using Synchformer [20]. Inspired by MMAudio [6], we analyze the first and last 4.8s of the video (allowing overlap) to assess alignment. Temporal Synchronization - Lip-Sync. Assesses the synchronization between lip movements and speech, crucial metric for talking heads. This metric is applied exclusively to the human linguistic subset and only where talking head is detected. For implementation, we calculate the alignment confidence, inspired by the evaluation method of LatentSync [22]. Macro - Audio Realism. Assesses the physical plausibility of the audio track, examining if attributes like loudness and timbre conform to real-world acoustic laws. This metric is computed excluding the virtual worlds category. Macro - Visual Realism. Assesses the physical plausibility of the video frames, examining if aspects like lighting and motion fluency adhere to real-world physical laws. This metric is computed excluding the virtual worlds category. Micro - Audio QA Pairs. Designed to assess generation quality from fine-grained acoustic perspective. For each video sample, set of 3 to 7 questions is designed to focus on the physical properties of sound and details of environmental interactions. Micro - Visual QA Pairs. Designed to assess generation quality from fine-grained visual perspective. Similarly, specific question set (3-7 questions) is tailored for each sample, focusing on visual elements and physical action details mentioned in the input text prompts. In the micro-level evaluation, for test set containing video samples, let Ki be the number of detail-oriented questions generated for the i-th sample, and let Ci denote the number of those questions for which the LLM judge determines that the video sample satisfies the corresponding detail requirement. The final fine-grained score is defined as the average accuracy across all samples: ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i="
        },
        {
            "title": "Ci\nKi",
            "content": "(2) 3.3.2. MLLM-based Evaluation 3.3.3. Stereophonic Analysis While traditional human-based Mean Opinion Scores (MOS) for video quality are labor-intensive, unscalable, and subjective, emerging Omni-modal LLMs [55] offer an efficient, standardized alternative. Our framework leverages these models for evaluation at two complementary levels: coarse-grained (macro, scored 1-5) and fine-grained (micro).Specific implementation details are provided in the supplementary material. Macro - Alignment. Assesses audio-visual coherence, assessing both temporal synchronization (alignment of audiovisual events) and semantic correspondence (natural, logical coordination of multimodal content). Macro - Artistry. Assesses the aesthetic and expressive quality of the audio-visual fusion, distinct from technical realism. It evaluates the stylistic unity, creative intent, and resulting artistic impact of the synergy. Due to space constraints, detailed discussion of this metric is provided in the supplementary material. Macro - Expressiveness. Assesses audios storytelling effectiveness, evaluating how its emotional alignment reinforces the intended mood and its narrative function clarifies or enhances the on-screen story. We also evaluated the stereophonic performance of generated audio based on human check and nine core acoustic metrics, categorized into two primary dimensions: Spatial Imaging Quality and Signal Integrity & Compatibility. Spatial Imaging Quality. This dimension assesses spatial distribution and sound image clarity. We measure stereo width (Mid/Side energy ratio [36]), imaging stability (fluctuations in ITD [42]), level stability (fluctuations in ILD [42]), and inter-channel temporal consistency (envelope correlation and transient synchronization [21]). Signal Integrity & Compatibility. This dimension focuses on technical stability and cross-device compatibility. We calculate phase coherence across low, mid, and high frequency bands [21]. We also assess mono downmixing fidelity using the mono compatibility metric (mono loss percent) [44] and its inverted form, the Mono Compat score (defined as 1 - normalized mono loss). For directional consistency, we apply inverse normalization to Mono Compat, Imaging Stability, and Level Stability. Finally, we present nine-dimensional radar chart to visualize and quantify model performance across spatial imaging and signal integrity for stereophonic generation. Table 1. T2AV evaluation results. The results for AV and V+A models are separated by horizontal line. Underlined scores indicate the highest within each category (AV or V+A), and bolded scores indicate the overall best for each metric."
        },
        {
            "title": "Models",
            "content": "sora2 veo3 wan2.5 seed think seed mm wan2.2 think wan2.2 mm kling think kling mm Speech Q&N 2.672 3.073 2.562 2.274 2.352 2.116 2.159 2.369 2.465 Audio Aes 2.867 3.543 3.061 2.793 2.900 2.825 2.839 2.901 2.954 T-V Align 0.2256 0.2304 0.2275 0.2215 0. 0.2304 T-A Align 0.3465 0.3582 0.3033 0.2730 0.3365 0.2735 0.3385 0.2692 0.3500 A-V Align 0.2376 0.3164 0.2099 0.1960 0.2817 0.2090 0.2775 0.2203 0.2929 LipSync 2.655 3.294 3.671 2.785 1.743 1.559 1.401 2.144 1.740 Desync 0.7141 0.5180 0.4568 0.5686 0.4820 0.5910 0.5360 0.6008 0. Alignment 4.546 4.553 4.465 4.459 4.506 4.279 4.377 4.455 4.440 Expressiveness 4.379 4.424 4.441 4.338 4.354 4.310 4.318 4.409 4.408 Visual Realism 4.805 4.773 4.674 4.724 4.649 4. Audio Realism 4.375 4.309 4.185 4.159 4.175 4.069 4.109 4.182 4.197 Audio QA 0.8082 0.7999 0.7993 0.6698 0.6741 0.5647 0.5861 0.6932 0.7300 Visual QA 0.7994 0.8095 0.7933 0.7265 0.6363 0. Table 2. I2AV evaluation results, following the same presentation protocol as Tab. 1 (T2AV)."
        },
        {
            "title": "Models",
            "content": "sora2 veo3 wan2.5 seed think seed mm wan2.2 think wan2.2 mm kling think kling mm Audio Aes 2.974 3.574 3.455 2.834 2.974 2.833 2.933 2.882 2.948 T-V Align 0.2188 0.2334 0.2374 0.2276 0.2292 0. T-A Align 0.4045 0.4130 0.2865 0.3183 0.4074 0.3216 0.4121 0.3169 0.4052 A-V Align 0.2623 0.3215 0.2112 0.2341 0.3185 0.2145 0.3053 0.2310 0.3128 Desync Alignment 0.9002 0.6146 0.3532 0.6994 0.5854 0.8038 0.6964 0.7232 0.6795 4.885 4.906 4.812 4.879 4.918 4.846 4.898 4.860 4.879 Expressiveness 4.390 4.631 4.495 4.528 4.585 4.503 4.516 4.507 4. Visual Realsim 4.964 4.921 4.766 4.901 4.913 4.893 Audio Realism 4.597 4.660 4.395 4.491 4.526 4.392 4.420 4.369 4."
        },
        {
            "title": "Audio QA Visual QA",
            "content": "0.8287 0.8584 0.8084 0.7833 0.8020 0.7334 0.7553 0.7425 0.7918 0.7611 0.7982 0.7889 0.7453 0.7338 0.7418 4. Experiment 4.1. Implement We evaluated T2AV and I2AV tasks on VABench using two system categories. The first is end-to-end AV models: Veo3-fast, Wan2.5 Preview, and Sora2. The combinations of second is decoupled V+A models: video generator (Seedance-1.0-lite [10], Wan2.2-TI2V [47], Kling2.5 Turbo [43]) and an audio model (ThinkSound light, MMAudio). Veo3-fast, Wan2.5 Preview, Sora2, Seedance-1.0-lite, Kling2.5 Turbo were acessed via official APIs. ThinkSound light, Wan2.2-TI2V, MMAudio were deployed locally. Video outputs were set to 720P (or closest aspect ratio; Wan2.2/2.5 adjust automatically), with frame rate and duration following default settings. For audio, 48kHz stereo tracks were extracted from AV models, while V+A models outputs (ThinkSound, MMAudio) were retained in their native output formats. Prompts for audio models were modified following official guidelines to satisfy the input length limits while maintaining core semantics. In all tables and figures, we use unified abbreviations. AV models: veo3 (Veo3-Fast), wan2.5 (Wan2.5 Preview), sora2 (Sora2). V+A models: prefixes seed (Seedance-1.0Lite), wan2.2 (Wan2.2 TI2V), kling (Kling2.5-Turbo) denote the video generator, while suffixes mm (MMAudio), think (ThinkSound light) denote the audio model, with the two parts joined by an underscore ( ). We use simplified names (e.g., Seedance) in the main text for clarity. Generated results are shown in Fig. 5. 4.2. Main Results Text to Audio-Video Generation. As shown in Tab. 1, among AV models, Veo3 demonstrates the strongest overall performance, particularly in audio quality and crossmodal semantic alignment. Sora2 excels in realism but lags in audio aesthetics and synchronization. Wan2.5 achieves the best audio-visual synchronization, especially Lip-Sync, though its semantic alignment is slightly lower than Veo3 and Sora2. The results of the three models indicate that semantic consistency, synchronization, and realism are difficult to achieve simultaneously. Among audio models, MMAudio generally outperforms ThinkSound, while ThinkSound shows advantages in LipSync. For pure visual models, Kling achieves the highest visual quality across nearly all vision metrics. Seedance, while slightly weaker overall, unlocks the potential for superior Lip-Sync from the audio model. Kling+MMAudio, combining the strongest audio and video models, stands out as the strongest V+A model, indicating that higher-quality video generation can also facilitate improved audio generation. 6 (a) Audio QA Figure 5. Qualitative comparison of model performance. We visualize pairwise comparisons across three tasks (I2AV, Stereo, T2AV) by showing key video frames and audio waveforms. Overall, integrated AV models tend to hold an advantage, suggesting that end-to-end joint training more effectively captures cross-modal synergies, forms unified semantic space, and generates more natural, coherent audiovisual content. Nonetheless, the V+A approach remains viable alternative. Image to Audio-Video Generation. As shown in Tab. 2, the overall performance of the three AV models mirrors that of T2AV. Among pure visual models, Seedance achieves the strongest results, ranking first across nearly all metrics, while Wan2.2 and Kling perform slightly lower. Seedance+MMAudio attains the best results on major metrics. Overall, integrated AV models generally outperform V+A models, highlighting the effectiveness of end-to-end joint training. T2AV and I2AV joint analysis. Compared with T2AV, the stronger visual constraints brought by input images in I2AV reduce performance gaps among models. Some V+A combinations (e.g., Seedance+MMAudio) even surpass AV models in Alignment, though AV models retain clear advantage in T2AV. For Expressiveness, the gap between AV and V+A models narrows. For fine-grained semantics (QA), AV models lead in both tasks, but visual input reduces differences and improves score balance. Overall, the (b) Vision QA Figure 6. Fine-grained QA evaluation across seven audio categories for different model architectures. results of T2AV and I2AV show consistent trends with stable variations across evaluation dimensions. 4.3. Additional Analysis Multi-Categories Analysis. The result of AQA is shown in Fig. 6a. Veo3 achieves the strongest overall performance among AV models, excelling in Animals, while Sora2 delivers the most balanced results. Wan2.5 performs best in the Virtual Worlds category. Within the V+A framework, clear performance hierarchy emerges: Kling ranks first, followed by Seedance, with Wan2.2 trailing. On the audio side, MMAudio performs robustly overall, whereas ThinkSound demonstrates distinct advantages in Music. Overall, current systems perform well on weakly correlated audio types such as Music and Animals but struggle with Human Sounds. For highly synchronized tasks, AV modelsbenefiting from end-to-end joint modelingconsistently outperform V+A ones. Notably, even the best V+A combination fails to surpass the weakest AV 7 Figure 7. Comparative radar chart of three models: Phase Coherence (Coh Low/Mid/High), Mono Compatibility (Mono Compat), Soundstage Width (Width), Transient Synchronization (Transient Sync), Level Stability, Envelope Correlation (Env Corr), and Imaging Stability. Higher values indicate better performance. Figure 8. Human preference consistency validation. Each subplot shows one evaluation dimension, where each point denotes models win rate (x: human, y: VABench). reference line indicates their correlation, with the Pearson coefficient (œÅ) annotated. model, while all three AV models exhibit small performance gaps and stable results, reflecting their technical maturity and architectural advantages. The VQA results (Fig. 6b) follow similar trend as the AQA analysis. Among AV models, Veo3 delivers the strongest overall performance, excelling in Animals, Synchronous Physical Sounds, and Virtual Worlds. Sora2 ranks first in Human Sounds, while Wan2.5 performs robustly in Music and Complex Scenes. The distribution of pure visual models mirrors the AQA results. Models perform best in the Virtual Worlds category, whereas Complex Scenes receives the lowest scores, revealing persistent challenges in multi-source dynamic interactions. Notably, AV models dominate the top positions across all categories, with the largest gaps over V+A models observed in Human Sounds and Virtual Worlds. These results highlight that integrating audio cuesspatial, material, and rhythmicenhances not only visual understanding but also emotional tone and expressiveness, underscoring the systemic advantage of unified audio-visual modeling. Stereo Audio Video Generation. Based on nine normalized acoustic metrics  (Fig. 7)  , the three VA models exhibit clear trade-off between spatial width and signal fidelity. Wan2.5 demonstrates the best technical fidelity, with highly consistent leftright channels in both time and amplitude domains, but presents the narrowest soundstage. Sora2 features the widest spatial field and stable level balance, yet its width mainly arises from inter-channel phase offsets, leading to unstable localization and energy loss. Veo3 maintains balance between the two, achieving the most stable sound image and natural stereo structure. Human evaluation confirms that none reliably generate stereo separation from text prompts. Sora2 shows noticeable channel-level differences in loudness without seman8 tic distinction; Wan2.5 is nearly monophonic; Veo3 occasionally produces subtle inter-channel alternation and depth cues in natural scenes (e.g., waves, thunder). In certain samples, Veo3 generates moving spatial sources aligned with visual motion, while Sora2 produces distinct leftright vocal tracks in multi-speaker scenes. Overall, current models lack consistent stereo generation, while localized spatialization in Veo3 and Sora2 implies spatial audio cues in their training data, informing future research on spatial hearing in audio-video generation. 4.4. User Study To validate VABenchs alignment with human senses, we conducted pilot user study. Balancing evaluation fidelity and cost, we had six professional evaluators rate representative subset of videos from three models (Veo3, Seedance+MMAudio, Wan2.2+ThinkSound) on 15 scale. The evaluation focused on three key dimensionssemantics, synchronization, and realismwhich directly correspond to aggregated benchmark metrics (Semantics: Text-Video Align, Text-Audio Align, AudioVisual Align; Synchronization: Desync; Realism: Audio Realism, Video Realism). We then computed pairwise win rates (Win=1, Loss=0, Tie=0.5) for both human ratings and benchmark scores, averaging all comparisons to get per-model win rates. Finally, we calculated the Pearson correlation between humanand benchmark-derived win rates. The results, shown in Fig. 8, demonstrate strong correlation between VABench and human preferences across all dimensions. 5. Conclusion We present VABench, for evaluating synchronous comprehensive benchmark generation audio-video Featuring autoacross T2AV, I2AV, and stereo tasks. mated, multidimensional, and human-aligned evaluation, VABench enables reliable and interpretable performance assessment while revealing the challenge of balancing semantics, synchronization, and realism. We believe that VABench provides valuable insights for achieving more coherent and perceptually grounded audio-video generation, and will serve as an important and robost contribution to research and evaluation in this field."
        },
        {
            "title": "References",
            "content": "[1] Ruichuan An, Sihan Yang, Renrui Zhang, Zijun Shen, Ming Lu, Gaole Dai, Hao Liang, Ziyu Guo, Shilin Yan, Yulin Luo, et al. Unictokens: Boosting personalized understanding and generation via unified concept tokens. arXiv preprint arXiv:2505.14671, 2025. 3 [2] Tianyi Bai, Zengjie Hu, Fupeng Sun, Jiantao Qiu, Yizhen Jiang, Guangxin He, Bohan Zeng, Conghui He, Binhang Yuan, and Wentao Zhang. Multi-step visual reasoning with visual tokens scaling and verification. arXiv preprint arXiv:2506.07235, 2025. 3 [3] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Guanghui Liu, Amit Raj, et al. Lumiere: space-time diffusion model for video generation. In SIGGRAPH Asia 2024 Conference Papers, pages 111, 2024. 2 [4] Weifeng Chen, Yatai Ji, Jie Wu, Hefeng Wu, Pan Xie, Jiashi Li, Xin Xia, Xuefeng Xiao, and Liang Lin. Control-a-video: Controllable text-to-video generation with diffusion models. arXiv e-prints, pages arXiv2305, 2023. 1 [5] Xinlong Chen, Yuanxing Zhang, Yushuo Guan, Bohan Zeng, Yang Shi, Sihan Yang, Pengfei Wan, Qiang Liu, Liang Wang, and Tieniu Tan. Versavid-r1: versatile video understanding and reasoning model from question answering to captioning tasks. arXiv preprint arXiv:2506.09079, 2025. 3 [6] Ho Kei Cheng, Masato Ishii, Akio Hayakawa, Takashi Shibuya, Alexander Schwing, and Yuki Mitsufuji. Mmaudio: Taming multimodal joint training for high-quality video-toaudio synthesis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2890128911, 2025. 2, 5, [7] Google DeepMind. Veo 3, 2025. 1, 2 [8] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In Advances in Neural Information Processing Systems, pages 87808794. Curran Associates, Inc., 2021. 2 [9] Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. Cogview2: faster and better text-to-image generation via hiIn Proceedings of the 36th Intererarchical transformers. national Conference on Neural Information Processing Systems, Red Hook, NY, USA, 2022. Curran Associates Inc. 2 [10] Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint arXiv:2506.09113, 2025. 2, 6, 1 [11] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Imagebind: One embedding space to bind them all. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1518015190, 2023. 5 [12] Alibaba Tongyi Group. Wan 2.5: Unified multi-modal video generation framework, 2025. 1, 2 [13] Tianyu Guo, Hongyu Chen, Hao Liang, Meiyi Qiang, Bohan Zeng, Linzhuang Sun, Bin Cui, and Wentao Zhang. Brace: benchmark for robust audio caption quality evaluation. In The Thirty-ninth Annual Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2025. [14] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized textto-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. 1 [15] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation, 2023. 2 [16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [17] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in neural information processing systems, 35:86338646, 2022. 2 [18] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers, 2022. 2 [19] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. 1, [20] Vladimir Iashin, Weidi Xie, Esa Rahtu, and Andrew Zisserman. Synchformer: Efficient synchronization from sparse cues. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 53255329. IEEE, 2024. 5 [21] James M. Kates. Signal Processing for Hearing Aids, pages 235277. Springer US, Boston, MA, 2002. 5 [22] Chunyu Li, Chao Zhang, Weikai Xu, Jingyu Lin, Jinghui Xie, Weiguo Feng, Bingyue Peng, Cunjian Chen, and Weiwei Xing. Latentsync: Taming audio-conditioned latent diffusion models for lip sync with syncnet supervision. arXiv preprint arXiv:2412.09262, 2024. 5 [23] Yunming Liang, Zihao Chen, Chaofan Ding, and Xinhan Di. Deepsound-v1: Start to think step-by-step in the audio generation from videos. arXiv preprint arXiv:2503.22208, 2025. 2 [24] Weifeng Lin, Xinyu Wei, Ruichuan An, Peng Gao, Bocheng Zou, Yulin Luo, Siyuan Huang, Shanghang Zhang, and Hongsheng Li. Draw-and-understand: Leveraging visual prompts to enable mllms to comprehend what you want. arXiv preprint arXiv:2403.20271, 2024. 9 [25] Weifeng Lin, Xinyu Wei, Ruichuan An, Tianhe Ren, Tingwei Chen, Renrui Zhang, Ziyu Guo, Wentao Zhang, Lei Zhang, and Hongsheng Li. Perceive anything: Recognize, explain, caption, and segment anything in images and videos, 2025. 3 [26] Huadai Liu, Jialei Wang, Kaicheng Luo, Wen Wang, Qian Chen, Zhou Zhao, and Wei Xue. Thinksound: Chainof-thought reasoning in multimodal large language modarXiv preprint els for audio generation and editing. arXiv:2506.21448, 2025. 2 [27] Kai Liu, Wei Li, Lai Chen, Shengqiong Wu, Yanhao Zheng, Jiayi Ji, Fan Zhou, Rongxin Jiang, Jiebo Luo, Hao Fei, et al. Javisdit: Joint audio-video diffusion transformer with hierarchical spatio-temporal prior synchronization. arXiv preprint arXiv:2503.23377, 2025. 1, 3 [28] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya Jia. Video-p2p: Video editing with cross-attention control. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 85998608, 2024. 1 [29] Yulin Luo, Ruichuan An, Bocheng Zou, Yiming Tang, Jiaming Liu, and Shanghang Zhang. Llm as dataset analyst: Subpopulation structure discovery with large language model. In European Conference on Computer Vision, pages 235252. Springer, 2024. [30] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao, Jinren Zhou, and Tieniu Tan. Decomposed diffusion models for high-quality video generation. arXiv preprint arXiv:2303.08320, 3, 2023. 2 [31] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024. 2 [32] Gabriel Mittag, Babak Naderi, Assmaa Chehadi, and Sebastian Moller. Nisqa: deep cnn-self-attention model for multidimensional speech quality prediction with crowdsourced datasets. arXiv preprint arXiv:2104.09494, 2021. 4 [33] OpenAI. Sora, 2024. 2 [34] OpenAI. Sora 2: Video generation model, 2025. 1, 2 [35] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. 1, 3 [36] Ville Pulkki and Matti Karjalainen. Communication acoustics: an introduction to speech, audio and psychoacoustics. John Wiley & Sons, 2015. 5 [37] Chandan KA Reddy, Vishak Gopal, and Ross Cutler. Dnsmos: non-intrusive perceptual objective speech quality metric to evaluate noise suppressors. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 64936497. IEEE, 2021. 4, [38] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. 3 [39] Sizhe Shan, Qiulin Li, Yutao Cui, Miles Yang, Yuehai Wang, Qun Yang, Jin Zhou, and Zhao Zhong. Hunyuanvideofoley: Multimodal diffusion with representation alignment arXiv preprint for high-fidelity foley audio generation. arXiv:2508.16930, 2025. 4 [40] Yang Shi, Jiaheng Liu, Yushuo Guan, Zhenhua Wu, Yuanxing Zhang, Zihao Wang, Weihong Lin, Jingyun Hua, Zekun Wang, Xinlong Chen, et al. Mavors: Multi-granularity video representation for multimodal large language model. In Proceedings of the 33rd ACM International Conference on Multimedia, pages 1099411003, 2025. 3 [41] Yang Shi, Huanqian Wang, Wulin Xie, Huanyao Zhang, Lijie Zhao, Yi-Fan Zhang, Xinfeng Li, Chaoyou Fu, Zhuoer Wen, Wenting Liu, et al. Mme-videoocr: Evaluating ocr-based capabilities of multimodal llms in video scenarios. arXiv preprint arXiv:2505.21333, 2025. 3 [42] John William Strutt. On the perception of the direction of sound. Proceedings of the Royal Society of London. Series A, Containing Papers of Mathematical and Physical Character, 83(559):6164, 1909. 5 [43] Kuaishou Technology. Kling 2.5 turbo, 2025. 6, 1 [44] Thilo Thiede, William Treurniet, Roland Bitto, Christian Schmidmer, Thomas Sporer, John Beerends, and Catherine Colomes. Peaq-the itu standard for objective measurement of perceived audio quality. Journal of the Audio Engineering Society, 48(1/2):329, 2000. [45] Andros Tjandra, Yi-Chiao Wu, Baishan Guo, John Hoffman, Brian Ellis, Apoorv Vyas, Bowen Shi, Sanyuan Chen, Matt Le, Nick Zacharov, et al. Meta audiobox aesthetics: Unified automatic quality assessment for speech, music, and sound. arXiv preprint arXiv:2502.05139, 2025. 4 [46] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Fvd: new metric for video generation. 2019. 3 [47] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 2, 6, 1 [48] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. 2 [49] Jun Wang, Xijuan Zeng, Chunyu Qiang, Ruilong Chen, Shiyao Wang, Le Wang, Wangjing Zhou, Pengfei Cai, Jiahui Zhao, Nan Li, et al. Kling-foley: Multimodal diffusion transformer for high-quality video-to-audio generation. arXiv preprint arXiv:2506.19774, 2025. 2, 3 [50] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, et al. Internvid: large-scale video-text dataset for multimodal understanding and generation. arXiv preprint arXiv:2307.06942, 2023. 5 [51] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. International Journal of Computer Vision, 133(5):30593078, 2025. 10 [52] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 76237633, 2023. 1 [53] Weijia Wu, Zeyu Zhu, and Mike Zheng Shou. Automated movie generation via multi-agent cot planning. arXiv preprint arXiv:2503.07314, 2025. 1 [54] Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, and Shlomo Dubnov. Large-scale contrastive language-audio pretraining with feature fusion and In ICASSP 2023-2023 keyword-to-caption augmentation. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2023. 5 [55] Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, arXiv preprint et al. Qwen2. 5-omni technical report. arXiv:2503.20215, 2025. 3, 5, 1, 7 [56] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 2 [57] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jose Lezama, Han Zhang, Huiwen Chang, Alexander Hauptmann, MingIrfan Essa, et al. Magvit: Hsuan Yang, Yuan Hao, In Proceedings of Masked generative video transformer. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1045910469, 2023. [58] Fan Zhang, Shulin Tian, Ziqi Huang, Yu Qiao, and Ziwei Liu. Evaluation agent: Efficient and promptable evaluation framework for visual generative models. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7561 7582, 2025. 1 [59] Dian Zheng, Ziqi Huang, Hongbo Liu, Kai Zou, Yinan He, Fan Zhang, Lulu Gu, Yuanhan Zhang, Jingwen He, WeiShi Zheng, et al. Vbench-2.0: Advancing video generation benchmark suite for intrinsic faithfulness. arXiv preprint arXiv:2503.21755, 2025. 1, 3 [60] Wendi Zheng, Jiayan Teng, Zhuoyi Yang, Weihan Wang, Jidong Chen, Xiaotao Gu, Yuxiao Dong, Ming Ding, and Jie Tang. Cogview3: Finer and&nbsp;faster text-to-image In Computer Vision generation via&nbsp;relay diffusion. ECCV 2024: 18th European Conference, Milan, Italy, September 29October 4, 2024, Proceedings, Part LXXVII, page 122, Berlin, Heidelberg, 2024. Springer-Verlag. 2 11 VABench: Comprehensive Benchmark for Audio-Video Generation"
        },
        {
            "title": "Supplementary Material",
            "content": "6. Additional evaluation metrics 6.1. Supplementry results analysis in SpeechClarity and Artistry Table 3. Supplementary results for T2AV and I2AV"
        },
        {
            "title": "Models",
            "content": "sora2 veo3 wan2.5 seed think seed mm wan2.2 think wan2.2 mm kling think kling mm T2AV I2AV SpeechClarity Artisry Artisry 3.931 3.983 3.929 3.956 3.971 3.942 3.962 3.950 3.958 2.367 2.554 2.396 2.008 2.202 1.882 2.016 2.051 2.221 3.735 3.825 3.838 3.717 3.707 3.630 3.609 3.844 3.844 In this part, we present supplementary metrics excluded from the main text(Tab. 3), along with further analysis of these results. Additionally, we extend and validate the primary conclusions of our study based on these supplementary findings. On the SpeechClarity metric(leveraging DNSMOS [37], the AV models collectively achieve the best overall performance, which aligns with our previous analysis: AV models significantly outperform V+A models in representing human language, thereby enabling more comprehensive optimization of speech quality. For the V+A approach, Kling [43], Seedance [10], and Wan2.2 [47] exhibit descending performance trend, reaffirming that higher-quality visual generation substantially enhances the latent capabilities of audio models. On the Artistry metric(leveraging Qwen2.5 Omni 7B [55], AV models maintain an overall lead, yet Kling + MMAudio [6] reaches the current stateof-the-art level. Beyond the primary conclusions presented in the main text, our extended evaluation across comprehensive metrics reveals several representative performance differentiations. Specifically, Veo3 [7] demonstrates precise synergistic control over acoustic and visual details within its joint modeling framework, while Sora2 [34] exhibits more prominent capabilities in Synchronous Physical Sounds (hereafter referred to as Physical)) plausibility and event consistency. Among visual-only models, Kling shows superior performance in both artistic style and visual fidelity compared to peer approaches. Furthermore, Kling+MMAudio surpasses Sora2 on cross-modal metrics such as Text-Audio Align and 1 Audio-Visual Align, while also demonstrating robust performance across subjective dimensions including Artistry, Expressiveness, and Audio Realism. These supplementary observations provide nuanced substantiation for the main conclusions. 6.2. Category Analysis This part presents more complete exposition of the core findings from the Multi-Categories Analysis in Section 4.3, supplemented by additional discoveries. Audio QA. The three AV models demonstrate robust performance with distinct specializations. Veo3 leads in Animals but shows relative weakness in Environmental Sounds (hereafter referred to as Environment) and Complex Scenes categories. Sora2 delivers the most balanced performance, consistently ranking within the top two, which highlights its strong generalization capability. Wan2.5 [12] dominates the Virtual Worlds (hereafter referred to as Virtual) category yet exhibits noticeable shortcomings in Music, Physical, and Animals. Notably, Human Sounds remains the most challenging domain for all models, indicating shared limitation in simulating human vocal signals. Within the V+A architecture group, MMAudio exhibits comprehensive capabilities, showing particular strength in the Environment and Virtual categories, where its performance approaches that of top-tier AV models. Meanwhile, ThinkSound demonstrates aspecialization in the Music category, highlighting its specific proficiency in musical generation. Comparing Wan2.2 and Wan2.5, both models exhibit performance gap relative to peers in the Physical category, suggesting that modeling physical laws remains challenging area for this model family. similar trend is observed in the Music category, indicating that these specific semantic domains may benefit from further optimization. Among video generation models, Kling achieves the highest performance in the Human Sounds and Virtual categories. Seedance occupies the middle tier, marginally surpassing Kling in the Environment category, while Wan2.2s performance trails in Music, Physical, and Virtual. Experimental results confirm that high-quality video inputs can significantly augment audio generation potential, synergy clearly evident in the performance gains of the Kling + MMAudio combination. Overall, the consistent superiority of AV models underscores the architectural advantage of end-to-end joint training in achieving high-fidelity, tightly-coupled generation. This advantage is particularly pronounced in the Human Sounds category, where precise spatiotemporal synchronization is paramount. These findings not only delineate specific deficiencies but also inform targeted optimization strategies; for instance, V+A architectures could be significantly enhanced by strengthening speech generation modules or integrating specialized vocal models to address current limitations in human sounds synthesis. Visual QA. The evaluation reveals distinct capability specializations among the three AV models. Veo3 distinguishes itself in scenarios demanding sophisticated physical logic and complex dynamics. Sora2 demonstrates superior proficiency in human-centric modeling. Meanwhile, Wan2.5 exhibits exceptional performance in multi-element, non-biological environments. Among pure visual models, Kling surpasses certain V+A models in the most challenging Complex Scenes category, demonstrating the significant potential of top-tier visualonly generation frameworks.In contrast, Wan2.2 trails behind in multiple critical categories (e.g., Physical, Virtual, and Music), suggesting that complex scene comprehension and cross-element consistency remain challenging aspects for its architecture. The results underscore the structural advantages of AV models, which consistently occupy the top three positions across all categories. Beyond multimodal alignment, the integration of audio signals enhances holistic scene understanding; spatial cues, material properties, and event dynamics provide critical context, contributing to more physically plausible and temporally coherent visuals. This benefit is most critical in the Human Sounds category, where the millisecond-level precision required for lip synchronization leverages the joint architecture to address alignment challenges that remain significant for pure visual frameworks. Further analysis suggests that audio is critical for visual generation, extending beyond temporal synchronization. This is evidenced by the Virtual category, which exhibits the second-largest performance gap between model types. In abstract or surreal scenarios, auditory cuessuch as rhythm, energy distribution, and emotional toneprovide essential structural guidance. AV models leverage these signals to enhance visual dynamism and narrative coherence, whereas pure visual models face the increased challenge of inferring these attributes without multimodal guidance. These results demonstrate that even in the most creative, free-generation domains, end-to-end audio-visual co-design significantly enhances both generation quality and creative consistency. 7. Audio-Video Generation Models in Evaluation In our experiments, we adhered to the default configuration parameters provided by each video generation model, Table 4. The attributes of the videos generated by each model. Models sora2 veo3 wan2.5 seedance 1.0 lite wan2.2 kling2.5 turbo Length 10s 8s 5s 5s 5s 5s FPS 30 24 24 24 24 24 as summarized in Tab. 4 . Specifically, these settings include the default output duration (Length) and frame rate (FPS). For instance, Sora2 generates videos of 10 seconds at 30 FPS by default, while Veo3-fast, Wan2.5 Preview, Seedance-1.0-Lite, Wan2.2-TI2V, and Kling2.5 Turbo all produce videos of 58 seconds at 24 FPS. 8. Detail Analysis of Different Tasks This section provides comprehensive analysis of experimental results across different categories for various models under both T2AV (Tab. 5) and I2AV (Tab. 6) tasks. The study aims to identify common patterns across tasks and elucidate the specific impact of image-conditioned input (I2AV) on the final outcomes. 8.1. Common Strengths and Core Challenges Despite differences in input modalities, the models ability to handle specific content categories shows high consistency across both tasks, revealing universal strengths and core bottlenecks in current technologies. Common Strength: Robustness in the Music Category. In both T2AV and I2AV tasks, the Music category yields superior scores across most metrics. This trend suggests that current generative frameworks are particularly adept at processing structured, melodic content. Benefiting from the inherent correlation between musical audio and visual dynamics, models demonstrate sustained stability in achieving high-fidelity synchronization and emotional expression across input modalities. Common Challenges: Human Sounds and Complex Scenes. Human Sounds and Complex Scenes emerge as persistent challenges across tasks, consistently exhibiting lower scores in alignment and macro-evaluation metrics. These categories represent significant technical bottlenecks in current generation frameworks. The difficulty in Human Sounds is attributed to the nuanced and abstract nature of the content (e.g., contemplation), combined with rigorous demands for temporal synchronization regarding character actions, lip movements, and realistic detail. Meanwhile, the Complex Scenes category is constrained by challenges in multi-element interference, multi-source fusion, and comprehensive scene reasoning, resulting in persistent limitations. Addressing these bottlenecks in human sounds syn2 Figure 9. Doppler-effect video for analysis. From top to bottom, the results correspond to the outputs of Veo3, Sora2 and Wan2.5, respectively. thesis and complex scene generation remains priority for future research. 8.2. Impact of Image Input The use of images as conditional inputs is the key difference between I2AV and T2AV tasks. The experimental results reveal the significant influence of image inputs on the evaluation of generation outcomes. Convergence and Constraint of Artistry Scores. In T2AV tasks, the Virtual category yields the highest Artistry scores, suggesting that models demonstrate peak creative expression when unencumbered by strict physical constraints. Conversely, I2AV tasks exhibit minimal variance in Artistry scores across categories, with values converging toward central mean (3.84.0). This pattern implies that static image inputs impose constraint on free artistic expression, biasing the generation process toward the physical fidelity and realism inherent in the visual reference.Future methodologies can therefore explore ways to enable high-level artistic expression within these constraints. Convergence and Stabilization of Alignment and Realism Metrics. Compared to T2AV, the inclusion of image inputs in I2AV reduces performance variance across models and raises the minimum performance floor across categories in Alignment, Visual Realism, and Audio Realism. This stabilization effect is particularly pronounced in the Physical category, where explicit visual grounding more effectively constrains physical states and spatial relationships, resulting in markedly more consistent and realistic audiovideo generation. Figure 10. Spectrograms of the video generated by three models. From top to bottom, the results correspond to Veo3, Sora2, and Wan2.5, respectively. 9. Qualitative Analysis In this section, we conduct more detailed analysis based on several specific scenarios. These scenarios are selected to examine how the models handle challenging multimodal cues involving physical principles, temporal constraints, and spatial structures. 9.1. Doppler Effect This part evaluates whether the models can generate acoustically plausible variations that conform to the physical principles of the approach-pass-recede dynamic when explicitly prompted for Doppler effect synthesis, thereby authentically reproducing the auditory characteristics of high-speed moving sound sources. For this purpose, we select three AV models and analyze an example  (Fig. 9)  through spectrogram visualization of its audio content (see Fig. 10). The prompt used for this example is as follows: In the night sky, an airplane flies at high speed, leaving long trail behind. The audio should include the roaring sound of the airplane engine, which gradually fades as the plane moves away, demonstrating the Doppler effect. Faint wind noise and occasional sounds of nocturnal insects 3 From top to botFigure 11. Lightning video for analysis. tom, the results correspond to the outputs of Veo3, Wan2.5 and Kling+MMAudio, respectively. are present in the background, creating vast auditory atmosphere of the night sky. Analysis of the results reveals that Veo3s spectrogram most clearly demonstrates the Doppler effectits frequency trajectory shows smooth temporal descent, accurately simulating the pitch variation of an aircraft approaching and receding, while simultaneously rendering the environmental atmosphere specified in the prompt. In comparison, although Wan2.5 captures the gradual attenuation of engine roar with changing distance, its Doppler shift characteristics are less pronounced than Veo3s. As for Sora2, while its Doppler effect is not as prominent as the other two models, the overall auditory perception aligns more closely with human intuition: given the aircrafts altitude and distance in the visual scene, the engine sound should inherently exhibit lower fundamental frequency accompanied by moderate degree of frequency shifting, and Sora2 delivers more perceptually plausible representation in this regard. 9.2. Lighting This section evaluates whether the models can adhere to the natural physical principle of thunder following lightning when generating videos with the prompt lightning, thereby producing thunder scenes with physical consistency and perceptual plausibility. We examine three modelsVeo3, Wan2.5, and Kling+MMAudiousing sample  (Fig. 11)  with spectral analysis conducted on the corresponding audio signals (see Fig. 12). The prompt used for this example is as follows: On pitch-black night, the distant sky is split by lightning, accompanied by the rumbling of Figure 12. Spectrograms of the lightning video generated by the three models. From top to bottom, the results correspond to Veo3, Wan2.5, and Kling+MMAudio, respectively. Yellow vertical lines mark the approximate timestamps of visible lightning strikes, while blue vertical lines indicate the thunder events (excluding those occurring at the very beginning of the video). thunder. The flash of lightning is brief and bright, while the thunder rolls in from afar, gradIn the ually intensifying and shaking the soul. surrounding environment, wind howls fiercely, power poles sway slightly in the gusts, and the occasional hum of vibrating wires can be heard. The entire soundscape brims with the power and dynamism of nature, as the low-frequency rumbles of thunder contrast sharply with the highfrequency whistling of the wind. From the spectrogram of the Veo3-generated audio, the first thunderclap occurs after lightning is already visible in the video, which does not contradict the physical principle that light arrives before sound. For Wan2.5, the thunder continues for short duration and gradually attenuates after the lightning has faded, indicating certain degree of physical plausibility. As for Kling+MMAudio, both thunder events in its generated sample occur after the corresponding lightning appears in the video, likewise not violating the expected physical order. Overall, all three models reflect the lightsound temporal relationship to some extent, though 4 Figure 13. Stereo-sound video for analysis. From top to bottom, the results correspond to the outputs of Veo3, Sora2 and Wan2.5, respectively. Figure 15. Stereophonic analysis of the video generated by Sora2 Figure 14. Stereophonic analysis of the video generated by Veo3 Figure 16. Wan2."
        },
        {
            "title": "Stereophonic analysis of the video generated by",
            "content": "their generated dynamics still show room for improvement when compared with real-world physical behavior. 9.3. Double channels This section evaluates the stereophonic spatial construction capabilities of three AV models. We selected coastal video example  (Fig. 13)  and conducted systematic analysis of the left/right channel waveforms, spectrograms, spectral differences, and cross-correlation characteristics. The prompt used is as follows: In stereo audio, ocean waves crash powerfully against rocks on the left channel, while seagulls cry and the wind whispers gently through dunes from the right, set under vast, cloudless blue sky, forming an expansive coastal vista. Veo3  (Fig. 14)  demonstrates significantly better channel differentiation. The waveform amplitude alternates between channels, and the spectral difference map reveals Figure 17. sample from Veo3s generated results, illustrating both the Doppler effect and stereophonic audio. dynamic energy shifts, confirming the presence of stereophonic information despite high cross-correlation. However, this variation manifests primarily as energy panning rather than the semantically-grounded separation (waves left vs. seagulls right) requested in the prompt. Consequently, while it provides perceptible soundstage movement and depth, the source localization remains ambiguous. Sora2  (Fig. 15)  exhibits nearly identical left and right channels in terms of waveform and frequency spectrum, showing high correlation. This indicates failure to achieve the specified source separation; ocean waves, seagulls, and wind are blended centrally rather than distributed spatially. Despite perfect synchronization, the output essentially resembles mono audio stored in dual-channel format, lacking stereophonic width and directionality. Wan2.5  (Fig. 16)  shows nearly 100% channel alignment (correlation value: 0.9998) with consistent spectral characteristics, representing typical mono audio. Consequently, the model did not effectively implement the spatial layout of left: waves, right: seagulls, resulting in centralized soundfield lacking perceptible directionality or stereophonic width. Overall, the performance regarding semantic-driven stereo generation consistent with the prompt indicates substantial room for improvement across all evaluated models. These results highlight that semantic spatial localization remains significant challenge for current generation frameworks. 10. Special samples Analysis 10.1. Veo3 Case Analysis We examine case  (Fig. 17)  where Veo3 autonomously generated stereophonic audio featuring distinct Doppler effects, notably without explicit spatial specifications in the input prompt. We conducted time-domain waveform and spectrogram analyses for both channels, as shown in Fig. 18. The specific prompt used is as follows: On the racetrack, two high-speed racing cars are engaged in an intense competition. The audio should feature the Doppler effect of engine roars changing with the direction and speed of the cars, sharp tire screeches varying rhythmically, Figure 18. Analysis of the sample generated by Veo3, showing the Doppler effect and leftright channel characteristics. and background sounds including crowd cheers and distant wind. The sound field should reflect the distance and positional relationship between the cars, with volume dynamically adjusted as the cars approach or move away from the microphone. Spectral analysis confirms the accurate reproduction of physical phenomena. The left-channel spectrogram displays characteristic Doppler arcrising from 2s, peaking at 15 kHz near 3.5s, and subsequently descendingaccompanied by concentrated high-frequency bursts (>10 kHz) during peak intensity (34s) that effectively simulate tire friction. Regarding spatial dynamics, waveform analysis reveals clear left-channel dominance during the 04s interval, coinciding with the cars initial visual position. Subjective evaluation further corroborates this synchronization: the auditory frequency modulation and channel balancing align strictly with the vehicles visual approach and recession, satisfying the prompts requirements for both physical realism and spatial consistency. Collectively, this case exemplifies Veo3s capacity to reproduce complex physical phenomena (Doppler effect) and dynamic stereophonic soundfields, highlighting its potential for achieving high physical consistency and spatial accuracy. 10.2. Sora2 Case Analysis This analysis examines sample generated by Sora2, where the model generated distinct dual-channel audio to capture requested emotional features, despite the absence of explicit spatial constraints. We conducted time-domain waveform and spectrogram analyses for both channels, as illustrated in Fig. 20, based on the following prompt: 6 Figure 19. Video generated by Sora2, showing dual-channel audio construction reflecting the intended emotional characteristics. (a) Sample generated by Veo3, showing strong preference toward Caucasian facial features. (b) Sample generated by Seedance, demonstrating tendency to produce subjects with Asian appearances. Figure 21. Demographic tendencies in generated human subjects across models. This figure illustrates appearance biases observed during manual inspection. Collectively, this case exemplifies the models great potential in integrating technical spatial controllability with coherent emotional narrative. 10.3. Demographic Bias and Data Distribution: During our manual inspection of generated samples, we observed distinct demographic bias in the representation of human subjects across different models. Specifically for example, Veo3 predominantly generates characters with Caucasian features, whereas Seedance exhibits strong tendency towards generating subjects with Asian appearances. We hypothesize that this disparity is closely correlated with the geographical origins of the models and the implicit distributions of their private training data. Veo3, developed by US-based entity, likely relies heavily on Western-centric datasets, while Seedance originating from Asian developers, likely incorporate higher proportion of Asian-centric data. This observation suggests that generative models tend to reflect the demographic characteristics inherent in their training corpora.Video screenshots illustrating this bias can be seen in Fig. 21. 11. MLLM Based Evaluation Cases 11.1. Macro Evaluation System Prompt Sample As introduced in the main paper, our evaluation framework leverages Qwen2.5 Omni 7B [55] to provide scalable and standardized alternative to traditional MOS. This supplementary section provides the specific implementation details for the coarse-grained (macro) evaluation level. To achieve this, we design suite of detailed system prompts. Figure 20. Analysis of the Sora2-generated sample, showing dualchannel emotional rendering and leftright spectral characteristics. For video showing persons strained facial expressions during difficult moral decision, create an inner conflict by generating two conflicting layers of whisper-like background audio tracksone representing temptation and the other consciencealternating between left and right channels, as if arguing inside the mind. The analysis confirms that Sora2 precisely executed the instruction to alternate left-right channels. Waveform and spectral data reveal clear temporal partitioning: primary emission originates from the left channel (03s, 59s), alternating with dual-channel activity (35s, 910s) via staggered, non-overlapping energy bursts. This spatial separation, combined with rhythmic structure of brief bursts interspersed with silence, effectively simulates the mechanics of contentious dialogue. Subjective evaluation further validates that this structure successfully materializes the abstract internal debate. The model establishes clear adversarial relationship where provocative temptation track and rational conscience track alternate in coherent sequence. The integration of distinct emotional tones with spatial positioning strictly adheres to the prompts requirements, demonstrating the models capability to translate psychological conflict into structured stereophonic narrative. 7 Each prompt casts the MLLM into the role of specialized expert and provides comprehensive, five-point scoring rubric (scored 1-5) and output requirements. This methodology ensures that the MLLMs assessment is constrained, consistent, and targeted to specific quality dimension. Below, we provide two examples from our macro-evaluation prompt suite. Macro - Visual-Realism: You are Visual Realism Analyst. Assess whether the video obeys real-world physics, material behavior, and human visual perception. Evaluate these five core aspects: 1. Object permanence & occlusion: Objects should not appear/disappear abruptly; when one object passes behind another, it must be partially hidden consistently. 2. Biomechanically plausible motion: Human/animal movement must respect joint limits, weight, and momentum (e.g., no floating limbs, unnatural gait, or instant direction changes). 3. Physically consistent rendering: Lighting, shadows, color temperature, and perspective must align with single, coherent light source and spatial layout (e.g., shadows should point away from light, parallel lines converge correctly). 4. Temporal coherence: Motion must be smooth across framesno sudden jumps, speed glitches, or inconsistent frame-to-frame transitions without physical cause. 5. Material & environmental interaction: Objects should respond realistically to forces and surroundings (e.g., fabric drapes, water splashes on impact, footsteps deform soft ground, or glass reflects surroundings). Use this scoring scale: 5: Perfect realism every frame respects physics, perception, and material behavior. No anomalies detected. 4: Minor, brief flaws e.g., shadow slightly misaligned for one frame, or limb briefly stiff. Does not break believability. 3: Noticeable but isolated issues e.g., character walks with robotic knees, an object briefly pops into place, or water fails to splash on impact. Realism is weakened but still functional. 2: Frequent violations e.g., objects teleport, shadows flip direction, joints bend impossibly, or motion stutters unnaturally. Disrupts immersion consistently. 1: Physically incoherent chaotic visuals: people vanish mid-step, lighting shifts randomly, perspective collapses, or materials behave like abstract textures. Feels like broken CGI or hallucination. Output Requirements: - Return ONLY single JSON object. - Must contain exactly two keys: score (integer 1-5) and reason (string, 15 characters). - In reason, cite at least one specific anomaly with approximate timestamp (e.g., 0:12, the chair reappears after being fully occluded by person) and explain how it violates realism. - Do NOT include markdown, extra text, or additional fields. Example valid output: score: 2, reason: At 0:09, the characters elbow bends backward during reach, violating joint biomechanics. Macro - Expressiveness: You are Narrative Analyst. Evaluate how effectively the audio supports the videos emotional tone and storytelling. Focus on two key dimensions: 1. Emotional alignment: Does the sound (music, effects, silence, etc.) match the intended moodsuch as tension, joy, grief, or suspenseat each moment? 2. Narrative function: Does audio actively clarify or enhance the story? Examples include: - Highlighting key action (e.g., heartbeat during reveal) - Conveying character perspective (e.g., muffled sound during dazed POV) - Bridging scenes through sound continuity (e.g., train whistle fading into next location) - Providing offscreen context (e.g., distant sirens implying danger) Use this scoring scale: 5: Exceptional narrative and emotional synergy audio is integral to the story, powerfully shaping mood and meaning (e.g., silence used as dramatic punctuation, sound design reveals inner state). 4: Strong support clear emotional match and helpful narrative cues; enhances understanding and immersion without being revolutionary. 3: Minimal contribution audio is present but generic or neutral (e.g., ambient pad with no emotional inflection); neither helps nor hurts significantly. 2: Misaligned or confusing emotional tone clashes with visuals (e.g., upbeat music over funeral) or omits critical cues (e.g., silence during pivotal line). 1: Actively harmful audio contradicts the scenes intent or creates narrative chaos (e.g., laugh track over violence), impairing viewer comprehension. Output Requirements: - Return ONLY single JSON object. - Must contain exactly two keys: score (integer 1-5) and reason (string, 15 characters). - In reason, cite at least one specific moment with approximate timestamp (e.g., At 0:14, tense strings swell as the character reaches for the gun, amplifying suspense) and explain 8 how it supports or undermines story/emotion. - Do NOT include markdown, extra text, or additional fields. Example valid output: score: 4, reason: At 0:22, muffled audio during the car crash effectively conveys the protagonists disorientation and enhances emotional impact. 11.2. Micro Evaluation Sample Display This section demonstrates the concrete implementation of the QA scoring workflow under the micro-level metric framework, using one video example from each of the T2AV and I2AV tasks, with outputs generated by one AV model and one V+A model for evaluation. Example for T2AV: The sampled prompt is: In the quiet streets of residential neighborhood at dawn, the distant rumble of the first bus engine echoes in the background. Nearby, an earlyrising bird chirps. The milkman places glass bottles into doorstep crate, producing soft clinking sound. jogger runs past in athletic shoes, their footsteps making rhythmic slap-slap noise. Veo3 (Overall Score 0.8): Figure 22. Video example generated by Veo3 on the T2AV task. Q:Is the distant rumble of the bus engine audible in the background, creating an echo effect? A:No:"
        },
        {
            "title": "The distant rumble of the",
            "content": "bus engine is not audible in the background. Q:Can the chirping of the early-rising bird be heard clearly amidst the quiet streets? A:Yes:"
        },
        {
            "title": "The chirping of the",
            "content": "early-rising bird is clearly audible. Q:Does the sound of glass bottles clinking softly occur when the milkman places them into the doorstep crate? The sound of glass bottles A:Yes: clinking softly occurs when the milkman places them into the doorstep crate. Q:Are the rhythmic slap-slap noises of the joggers footsteps audible as they run past? A:Yes: The rhythmic slap-slap noises of the joggers footsteps are audible as they run past. Q:Is the overall soundscape predominantly quiet, with only the described sounds being prominent? A:Yes:"
        },
        {
            "title": "The audio captures the",
            "content": "described sounds of quiet residential neighborhood at dawn, including the distant rumble of bus engine, bird chirping, the milkmans clinking bottles, and the joggers footsteps. Seedance+MMAudio (Overall Score 0.4): Figure 23. Video example generated by Seedance+MMAudio on the T2AV task. Q:Is the distant rumble of the bus engine audible in the background, creating an echo effect? A:No:"
        },
        {
            "title": "The distant rumble of the",
            "content": "bus engine is not audible in the background, nor is there an echo effect. Q:Can the chirping of the early-rising bird be heard clearly amidst the quiet streets? A:No:"
        },
        {
            "title": "The chirping of the",
            "content": "early-rising bird is not clearly audible in the audio. Q:Does the sound of glass bottles clinking softly occur when the milkman places them into the doorstep crate? The sound of glass bottles A:Yes: clinking softly occurs when the milkman places them into the doorstep crate. Q:Are the rhythmic slap-slap noises of the joggers footsteps audible as they run past? A:Yes: The rhythmic slap-slap noises of the joggers footsteps are audible as they run past. Q:Is the overall soundscape predominantly quiet, with only the described sounds being prominent? A:No:"
        },
        {
            "title": "The soundscape includes",
            "content": "traffic noise and footsteps, which are not described in the reference. Example for I2AV: The sampled prompt is: On large outdoor music festival night, lights flicker on stage as the music pulses powerfully. In the foreground, the crowds cheers intertwine with the background music, creating rich sound field. The rhythm is intense, with deep, resonant bass that spreads from the center of the stage throughout the venue, accompanied by lighting changes. Environmental sounds include the footsteps of the crowd, occasional cheers, and applause, building up an enthusiastic atmosphere. The timbres are diverse, with high-frequency electronic tones contrasting sharply against low-frequency rhythms, forming clearly layered sound field where foreground and background sound sources are distinctly separated. Wan2.5 (Overall Score 0.8): that contribute to the enthusiastic atmosphere? A: Yes: The videos audio includes the footsteps of the crowd, contributing to the enthusiastic atmosphere. Q:Are the lighting changes synchronized with the rhythm of the music? A:No:"
        },
        {
            "title": "The lighting changes are not",
            "content": "synchronized with the rhythm of the music. Kling+ThinkSound (Overall Score 1.0): Figure 24. Video example generated by Wan2.5 on the I2AV task. Q:Does the audio include powerful music with deep, resonant bass that spreads from the center of the stage throughout the venue? A:Yes:"
        },
        {
            "title": "The audio includes powerful",
            "content": "music with deep, resonant bass that spreads from the center of the stage throughout the venue. Q:Are the crowds cheers and applause audible and intertwined with the background music, contributing to rich sound field? A:Yes: The crowds cheers and applause are audible and intertwined with the background music, contributing to rich sound field. Q:Is there clear distinction between high-frequency electronic tones and low-frequency rhythms, forming layered sound field? A:Yes: The audio clearly distinguishes between high-frequency electronic tones and low-frequency rhythms, forming layered sound field. Q:Is Do the environmental sounds include footsteps of the crowd 10 Figure 25. Video example generated by Kling+ThinkSound on the I2AV task. Q:Does the audio include powerful music with deep, resonant bass that spreads from the center of the stage throughout the venue? A:Yes:"
        },
        {
            "title": "The audio includes powerful",
            "content": "music with deep, resonant bass that spreads from the center of the stage throughout the venue. Q:Are the crowds cheers and applause audible and intertwined with the background music, contributing to rich sound field? A:Yes: The crowds cheers and applause are audible and intertwined with the background music, contributing to rich sound field. Q:Is there clear distinction between high-frequency electronic tones and low-frequency rhythms, forming layered sound field? A:Yes: The audio clearly distinguishes between high-frequency electronic tones and low-frequency rhythms, creating layered sound field. Q:Is Do the environmental sounds include footsteps of the crowd that contribute to the enthusiastic atmosphere? A:Yes:"
        },
        {
            "title": "The video includes the",
            "content": "footsteps of the crowd, contributing to the enthusiastic atmosphere. Q:Are the lighting changes synchronized with the rhythm of the music? A:Yes:"
        },
        {
            "title": "The lighting changes are",
            "content": "synchronized with the rhythm of the music. 11 Table 5. Results of different models on the T2AV task across seven audio categories, including Virtual. For each model, the highest score within each metric is highlighted in bold, and the lowest score is denoted with . Model Category sora2 sora2 sora2 sora2 sora2 sora2 sora veo3 veo3 veo3 veo3 veo3 veo3 veo3 wan2.5 wan2.5 wan2.5 wan2.5 wan2.5 wan2.5 wan2.5 seed think seed think seed think seed think seed think seed think seed think seed mm seed mm seed mm seed mm seed mm seed mm seed mm Animals Human Sounds Music Environment Physical Complex Scene Virtual Animals Human Sounds Music Environment Physical Complex Scene Virtual Animals Human Sounds Music Environment Physical Complex Scene Virtual Animals Human Sounds Music Environment Physical Complex Scene Virtual Animals Human Sounds Music Environment Physical Complex Scene Virtual Animals wan2.2 think wan2.2 think Human Sounds wan2.2 think wan2.2 think wan2.2 think wan2.2 think Complex Scene wan2.2 think Music Environment Physical Virtual Animals Human Sounds Music Environment Physical wan2.2 mm wan2.2 mm wan2.2 mm wan2.2 mm wan2.2 mm wan2.2 mm Complex Scene wan2.2 mm"
        },
        {
            "title": "Virtual",
            "content": "kling think kling think kling think kling think kling think kling think kling think kling mm kling mm kling mm kling mm kling mm kling mm kling mm"
        },
        {
            "title": "Animals\nHuman Sounds\nMusic\nEnvironment\nPhysical\nComplex Scene\nVirtual",
            "content": "AudioAes 3.309 3.047 3.512 2.509 2.842 2.758 2.675 3.546 3.669 4.402 3.298 3.411 3.464 3.620 3.375 3.265 3.785 2.950 3.013 2.898 2.803 2.996 2.689 3.998 2.501 2.892 2.688 2.752 3.288 2.922 3.814 2.618 2.940 2.755 2.832 2.902 2.682 4.053 2.524 3.033 2.689 2. 3.121 2.848 3.743 2.570 2.948 2.674 2.733 2.963 2.701 4.101 2.813 3.135 2.672 2.920 3.139 3.073 4.011 2.582 3.049 2.722 2.859 T-V Align 0.2647 0.2179 0.2300 0.2345 0.2193 0.2225 0.2361 0.2617 0.2157 0.2345 0.2430 0.2291 0.2238 0.2512 0.2545 0.2161 0.2194 0.2368 0.2234 0.2246 0. 0.2519 0.2025 0.2234 0.2378 0.2250 0.2121 0.2450 0.2519 0.2025 0.2237 0.2378 0.2250 0.2121 0.2450 0.2542 0.1963 0.2144 0.2287 0.2133 0.2074 0.2258 0.2542 0.1963 0.2144 0.2287 0.2133 0.2074 0.2258 0.2555 0.2103 0.2236 0.2363 0.2339 0.2304 0.2505 0.2555 0.2103 0.2236 0.2362 0.2339 0.2304 0. T-A Align 0.3428 0.2848 0.5521 0.4008 0.3458 0.3432 0.3621 0.3434 0.3254 0.5319 0.3988 0.3826 0.3287 0.3470 0.3702 0.2495 0.4122 0.3255 0.3112 0.3160 0.2880 0.3259 0.2194 0.4561 0.3101 0.2941 0.2760 0.2189 0.3127 0.2709 0.4772 0.4224 0.3720 0.3354 0.2894 0.3324 0.2227 0.4134 0.3297 0.2879 0.2730 0. 0.3152 0.2887 0.4505 0.4167 0.3740 0.3259 0.2989 0.3122 0.2344 0.4374 0.2911 0.2730 0.2658 0.2456 0.3446 0.3037 0.4937 0.4279 0.3766 0.3386 0.3019 A-V Align 0.2923 0.2308 0.3328 0.2531 0.2229 0.2313 0.2278 0.3692 0.3280 0.3795 0.2986 0.3097 0.3214 0.2694 0.2534 0.2201 0.2692 0.2034 0.2126 0.2080 0.1570 0.2433 0.1869 0.3187 0.1859 0.1966 0.1934 0.1664 0.3453 0.2743 0.4252 0.3158 0.2722 0.2804 0.2178 0.2630 0.2074 0.3332 0.1975 0.2019 0.2102 0.1707 0.3452 0.2820 0.3903 0.2916 0.2651 0.2865 0.2008 0.2670 0.2026 0.3502 0.2046 0.2114 0.2239 0.2166 0.3491 0.2827 0.4082 0.2988 0.2952 0.2920 0.2453 Alignment 4.914 4.383 4.968 4.795 4.592 4.344 4.696 4.914 4.383 4.968 4.740 4.565 4.432 4.674 4.343 4.289 4.806 4.753 4.571 4.301 4.630 4.571 4.150 4.936 4.753 4.462 4.448 4.641 4.629 4.178 5.000 4.781 4.652 4.377 4.685 4.257 3.922 4.774 4.726 4.370 4.186 4. 4.429 4.011 4.806 4.575 4.522 4.333 4.565 4.743 4.250 4.968 4.863 4.380 4.322 4.663 4.543 4.144 4.968 4.836 4.489 4.339 4.587 Artistry 3.714 3.689 3.968 3.671 3.418 3.896 4.120 3.829 3.717 4.064 3.795 3.543 3.934 4. 3.829 3.733 4.064 3.795 3.625 3.967 4.174 3.629 3.533 3.968 3.753 3.467 3.863 4.207 3.600 3.561 3.968 3.726 3.391 3.885 4.207 3.457 3.511 3.839 3.671 3.332 3.858 3.967 3.486 3.472 3.839 3.712 3.288 3.869 3.891 3.800 3.694 4.032 3.781 3.636 4.000 4. 3.800 3.661 4.032 3.767 3.658 4.016 4.250 Desync 0.7714 0.5950 0.4710 0.8342 0.7277 0.7399 0.8337 0.8229 0.3822 0.2613 0.7603 0.3859 0.6197 0.6239 0.4514 0.2078 1.065 0.4877 0.4359 0.5530 0.5674 0.7714 0.4778 0.2194 0.7452 0.4826 0.5825 0.7913 0.6514 0.3633 0.2968 0.8000 0.4120 0.4699 0. 0.7829 0.5100 0.2258 0.7945 0.4598 0.6109 0.8609 0.7086 0.4589 0.3484 0.7233 0.3772 0.5956 0.7348 0.6114 0.5889 0.2903 0.6685 0.3924 0.7563 0.7783 0.6629 0.5256 0.3419 0.6384 0.3989 0.6142 0.8109 12 Expressiveness 4.657 4.367 4.613 4.233 4.168 4.432 4. Visual Realism 4.971 4.794 4.968 4.890 4.875 4.650 3.087 Audio Realism 4.629 4.406 4.903 4.575 4.332 4.169 2.674 4.429 4.350 4.581 4.260 4.342 4.481 4.696 4.600 4.344 4.645 4.288 4.402 4.497 4.587 4.543 4.150 4.548 4.329 4.293 4.404 4.522 4.429 4.156 4.581 4.315 4.348 4.404 4. 4.371 4.172 4.452 4.315 4.239 4.432 4.402 4.486 4.161 4.581 4.288 4.261 4.410 4.424 4.486 4.244 4.710 4.397 4.332 4.470 4.641 4.514 4.244 4.548 4.370 4.370 4.475 4.609 4.914 4.717 4.968 4.863 4.793 4.710 2.891 4.857 4.667 4.806 4.822 4.685 4.552 2.848 4.829 4.700 5.000 4.842 4.796 4.560 2.761 4.829 4.700 5.000 4.842 4.796 4.560 2.761 4.500 4.597 4.832 4.884 4.736 4.530 3.103 4.500 4.597 4.823 4.884 4.736 4.530 3.103 4.857 4.708 4.935 4.884 4.812 4.508 2.989 4.857 4.708 4.935 4.884 4.812 4.508 2.989 4.200 4.322 4.936 4.438 4.310 4.159 2.707 4.029 4.178 4.806 4.356 4.228 4.005 2.663 3.829 4.139 4.903 4.425 4.163 4.005 2.413 4.000 4.128 4.936 4.438 4.212 3.984 2.511 3.743 4.089 4.677 4.315 4.005 3.973 2.717 3.800 4.072 4.677 4.411 4.141 3.956 2.717 4.114 4.172 4.871 4.384 4.217 3.973 2.772 4.057 4.183 4.903 4.384 4.207 4.033 2.707 Audio QA 0.8300 0.7597 0.9091 0.8057 0.8177 0.8173 0.8255 0.8424 0.7668 0.8668 0.7873 0.8382 0.7669 0.8247 0.7914 0.7437 0.8421 0.8116 0.7689 0.8110 0.8739 0.6667 0.4635 0.8281 0.7661 0.7269 0.7232 0. 0.7133 0.4645 0.8198 0.7868 0.7233 0.7113 0.7581 0.7105 0.4175 0.6458 0.6375 0.5323 0.6395 0.6280 0.6905 0.4653 0.5988 0.6894 0.5794 0.6446 0.5937 0.8286 0.4937 0.9012 0.7807 0.7035 0.7519 0.7548 0.7691 0.5203 0.8991 0.8347 0.7910 0.7420 0.8398 Visual QA 0.9023 0.8197 0.7660 0.7867 0.7849 0.7453 0. 0.8796 0.7985 0.7395 0.7965 0.8327 0.7560 0.8981 0.8676 0.7909 0.7582 0.7822 0.8020 0.7686 0.8732 0.7995 0.6735 0.7069 0.7377 0.7436 0.7030 0.8124 0.7995 0.6735 0.7069 0.7377 0.7436 0.7030 0.8124 0.8322 0.6220 0.5422 0.6974 0.6284 0.5870 0.6872 0.8322 0.6220 0.5422 0.6974 0.6284 0.5870 0. 0.8828 0.7171 0.7408 0.7980 0.7950 0.7436 0.8662 0.8828 0.7171 0.7408 0.7980 0.7950 0.7436 0.8662 Table 6. Results of different models on the I2AV task across seven audio categories, excluding Virtual. Same notation as Tab. 5. Model Category sora2 sora2 sora2 sora2 sora2 sora veo3 veo3 veo3 veo3 veo3 veo3 wan2.5 wan2.5 wan2.5 wan2.5 wan2.5 wan2.5 seed think seed think seed think seed think seed think seed think seed mm seed mm seed mm seed mm seed mm seed mm Animals Human Sounds Music Environment Physical Complex Scene Animals Human Sounds Music Environment Physical Complex Scene Animals Human Sounds Music Environment Physical Complex Scene Animals Human Sounds Music Environment Physical Complex Scene Animals Human Sounds Music Environment Physical Complex Scene Animals wan2.2 think wan2.2 think Human Sounds wan2.2 think wan2.2 think wan2.2 think wan2.2 think Complex Scene Music Environment Physical wan2.2 mm wan2.2 mm wan2.2 mm wan2.2 mm wan2.2 mm wan2.2 mm Complex Scene Animals Human Sounds Music Environment Physical kling think kling think kling think kling think kling think kling think kling mm kling mm kling mm kling mm kling mm kling mm"
        },
        {
            "title": "Animals\nHuman Sounds\nMusic\nEnvironment\nPhysical\nComplex Scene",
            "content": "Alignment 4.830 4.600 5.000 4.888 4.923 4.910 4.830 4.700 5.000 4.925 4.971 4.930 4.746 4.900 4.980 4.763 4.913 4.770 4.831 4.400 4.940 4.888 4.962 4.890 4.876 4.700 5.000 4.938 4.952 4.920 4.808 4.400 4.980 4.850 4.913 4.820 4.831 4.900 4.960 4.938 4.952 4.900 4.791 4.700 5.000 4.862 4.942 4. 4.825 4.800 4.980 4.925 4.913 4.860 Artistry 3.921 4.000 4.000 3.950 3.875 3.950 3.966 4.000 4.000 3.987 3.923 4.060 3.904 3.900 4.000 3.925 3.913 3.960 3.932 3.900 3.980 3.938 3.904 4. 3.949 4.000 3.980 4.013 3.923 4.020 3.949 3.900 3.980 3.938 3.894 3.970 3.932 3.900 3.980 3.975 3.952 4.010 3.915 4.000 4.020 3.987 3.894 4.000 3.932 4.000 3.980 3.975 3.923 4.010 Expressiveness 4.525 4.200 4.420 4.287 4.279 4. Visual Realism 4.949 4.800 4.980 5.000 4.981 4.950 Audio Realism 4.627 4.500 4.860 4.487 4.548 4.560 4.780 4.300 4.680 4.537 4.510 4.580 4.667 4.200 4.420 4.463 4.356 4.430 4.650 4.100 4.680 4.463 4.433 4.430 4.734 4.300 4.660 4.513 4.462 4. 4.638 4.000 4.620 4.400 4.394 4.450 4.678 4.300 4.580 4.425 4.375 4.440 4.627 4.100 4.580 4.425 4.413 4.460 4.706 4.500 4.520 4.412 4.375 4.440 4.927 4.700 4.980 4.963 4.952 4.840 4.706 4.500 4.880 4.825 4.846 4. 4.915 4.650 4.950 4.906 4.947 4.825 4.915 4.650 4.950 4.906 4.947 4.825 4.895 4.650 4.940 4.944 4.942 4.900 4.895 4.650 4.940 4.944 4.942 4.900 4.904 4.650 4.910 4.981 4.870 4.840 4.904 4.650 4.910 4.981 4.870 4. 4.678 4.400 4.860 4.638 4.615 4.620 4.378 4.300 4.700 4.275 4.423 4.350 4.508 4.300 4.780 4.375 4.433 4.490 4.599 4.200 4.820 4.450 4.413 4.460 4.435 4.100 4.680 4.250 4.337 4.370 4.458 4.200 4.660 4.400 4.317 4. 4.367 4.400 4.680 4.225 4.356 4.340 4.503 4.500 4.660 4.275 4.365 4.340 Audio QA 0.8210 0.8667 0.9100 0.7855 0.8636 0.7960 0.8904 0.8650 0.9263 0.7653 0.8709 0.8288 0.7877 0.6850 0.8622 0.7918 0.8546 0.7958 0.7895 0.7100 0.8363 0.7321 0.8111 0. 0.8145 0.7833 0.8405 0.7789 0.8257 0.7561 0.7696 0.6667 0.8091 0.6289 0.7599 0.6941 0.7600 0.7967 0.7842 0.7583 0.7476 0.7339 0.7776 0.6933 0.8093 0.6425 0.7675 0.7058 0.8113 0.7567 0.8171 0.7610 0.8136 0.7499 Visual QA 0.7708 0.8405 0.8474 0.7540 0.7596 0.6998 0.8083 0.8467 0.8696 0.7841 0.7926 0.7568 0.7827 0.8610 0.8873 0.8005 0.7630 0.7609 0.7485 0.8014 0.8316 0.7251 0.7405 0.7118 0.7485 0.8014 0.8316 0.7251 0.7405 0.7118 0.7440 0.8310 0.7977 0.7248 0.7062 0.7097 0.7440 0.8310 0.7977 0.7248 0.7062 0. 0.7503 0.8240 0.8313 0.7213 0.7287 0.7039 0.7503 0.8240 0.8313 0.7213 0.7287 0.7039 AudioAes 3.164 2.536 3.880 2.642 2.850 2.622 3.573 3.245 4.651 3.461 3.255 3.492 3.759 3.120 3.877 3.320 3.297 3.013 2.890 2.356 3.981 2.496 2.841 2. 3.117 2.962 4.013 2.614 2.822 2.648 2.837 2.641 4.010 2.440 2.946 2.452 3.015 2.899 3.995 2.550 2.919 2.582 3.009 2.431 3.980 2.439 2.952 2.433 3.054 3.196 3.867 2.547 2.895 2.651 T-V Align 0.2321 0.2233 0.1934 0.2154 0.2142 0. 0.2498 0.2347 0.2022 0.2318 0.2265 0.2284 0.2515 0.2491 0.2183 0.2321 0.2296 0.2332 0.2426 0.2342 0.2035 0.2218 0.2231 0.2216 0.2426 0.2342 0.2035 0.2218 0.2231 0.2216 0.2459 0.2413 0.2008 0.2226 0.2226 0.2245 0.2459 0.2413 0.2008 0.2226 0.2226 0. 0.2413 0.2346 0.2021 0.2234 0.2215 0.2216 0.2413 0.2346 0.2021 0.2234 0.2215 0.2216 T-A Align 0.4556 0.3870 0.4709 0.3420 0.3782 0.3646 0.4884 0.3661 0.4683 0.3030 0.4131 0.3448 0.3229 0.2349 0.2802 0.2278 0.2922 0.2714 0.3789 0.2102 0.3691 0.2271 0.2935 0. 0.4690 0.2940 0.3608 0.3610 0.4185 0.3584 0.3884 0.1911 0.3629 0.2377 0.3006 0.2846 0.4739 0.3111 0.3552 0.3759 0.4156 0.3665 0.3867 0.2206 0.3785 0.2147 0.3020 0.2694 0.4660 0.3308 0.3701 0.3628 0.4035 0.3582 A-V Align 0.2952 0.1997 0.2618 0.2585 0.2356 0. 0.3665 0.2689 0.3612 0.2905 0.2961 0.2785 0.2555 0.1453 0.2053 0.1717 0.2012 0.1846 0.2590 0.1802 0.2992 0.1927 0.2133 0.2179 0.3572 0.2690 0.3257 0.2951 0.2888 0.3007 0.2410 0.1693 0.2757 0.1764 0.2082 0.1785 0.3462 0.2729 0.3377 0.2676 0.2797 0. 0.2707 0.1351 0.3128 0.1661 0.2241 0.1884 0.3636 0.2959 0.3462 0.2719 0.2739 0.2809 Desync 0.9237 1.200 0.5200 1.052 0.7106 1.094 0.4898 0.6200 0.3940 0.8988 0.4808 0.8570 0.2633 0.1400 0.7480 0.3275 0.3846 0. 0.7198 1.100 0.3240 0.7000 0.6981 0.8120 0.5706 0.6400 0.4520 0.6175 0.5077 0.7280 0.8192 1.220 0.3160 0.9175 0.6077 1.092 0.6554 0.7600 0.3800 0.7350 0.6750 0.9120 0.7367 1.120 0.3120 0.8550 0.5961 0.8920 0.6169 1.140 0.3400 0.8800 0.5961 0."
        }
    ],
    "affiliations": [
        "Ant Group",
        "Huazhong University of Science and Technology",
        "Institute of Automation, Chinese Academy of Sciences",
        "Peking University"
    ]
}