{
    "paper_title": "Zero-Shot Multi-Spectral Learning: Reimagining a Generalist Multimodal Gemini 2.5 Model for Remote Sensing Applications",
    "authors": [
        "Ganesh Mallya",
        "Yotam Gigi",
        "Dahun Kim",
        "Maxim Neumann",
        "Genady Beryozkin",
        "Tomer Shekel",
        "Anelia Angelova"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multi-spectral imagery plays a crucial role in diverse Remote Sensing applications including land-use classification, environmental monitoring and urban planning. These images are widely adopted because their additional spectral bands correlate strongly with physical materials on the ground, such as ice, water, and vegetation. This allows for more accurate identification, and their public availability from missions, such as Sentinel-2 and Landsat, only adds to their value. Currently, the automatic analysis of such data is predominantly managed through machine learning models specifically trained for multi-spectral input, which are costly to train and support. Furthermore, although providing a lot of utility for Remote Sensing, such additional inputs cannot be used with powerful generalist large multimodal models, which are capable of solving many visual problems, but are not able to understand specialized multi-spectral signals. To address this, we propose a training-free approach which introduces new multi-spectral data in a Zero-Shot-only mode, as inputs to generalist multimodal models, trained on RGB-only inputs. Our approach leverages the multimodal models' understanding of the visual space, and proposes to adapt to inputs to that space, and to inject domain-specific information as instructions into the model. We exemplify this idea with the Gemini2.5 model and observe strong Zero-Shot performance gains of the approach on popular Remote Sensing benchmarks for land cover and land use classification and demonstrate the easy adaptability of Gemini2.5 to new inputs. These results highlight the potential for geospatial professionals, working with non-standard specialized inputs, to easily leverage powerful multimodal models, such as Gemini2.5, to accelerate their work, benefiting from their rich reasoning and contextual capabilities, grounded in the specialized sensor data."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 7 8 0 9 1 . 9 0 5 2 : r Zero-Shot Multi-Spectral Learning: Reimagining Generalist Multimodal Gemini 2.5 Model for Remote Sensing Applications Ganesh Mallya Yotam Gigi Dahun Kim Maxim Neumann Genady Beryozkin Tomer Shekel Anelia Angelova Google DeepMind Google Research"
        },
        {
            "title": "Abstract",
            "content": "Multi-spectral imagery plays crucial role in diverse Remote Sensing applications including land-use classification, environmental monitoring and urban planning. These images are widely adopted because their additional spectral bands correlate strongly with physical materials on the ground, such as ice, water, and vegetation. This allows for more accurate identification, and their public availability from missions, such as Sentinel-2 and Landsat, only adds to their value. Currently, the automatic analysis of such data is predominantly managed through machine learning models specifically trained for multi-spectral input, which are costly to train and support. Furthermore, although providing lot of utility for Remote Sensing, such additional inputs cannot be used with powerful generalist large multimodal models, which are capable of solving many visual problems, but are not able to understand specialized multi-spectral signals. To address this, we propose training-free approach which introduces new multispectral data in Zero-Shot-only mode, as inputs to generalist multimodal models, trained on RGB-only inputs. Our approach leverages the multimodal models understanding of the visual space, and proposes to adapt to inputs to that space, and to inject domain-specific information as instructions into the model. We exemplify this idea with the Gemini2.5 model and observe strong Zero-Shot performance gains of the approach on popular Remote Sensing benchmarks for land cover and land use classification and demonstrate the easy adaptability of Gemini2.5 to new inputs. These results highlight the potential for geospatial professionals, working with non-standard specialized inputs, to easily leverage powerful multimodal models, such as Gemini 2.5, to accelerate their work, benefiting from their rich reasoning and contextual capabilities, grounded in the specialized sensor data."
        },
        {
            "title": "Introduction",
            "content": "Large Multimodal Models (LMM) are trained on variety of tasks and serve as generalist learners. They are powerful visual understanding tools with generalization going beyond the training data, and often exhibit surprising capabilities across many unseen tasks. Remote Sensing applications can take advantage of the broad visual understanding capabilities of such large generalist task-agnostic models. However, most generalist multimodal models are available for only RGB images inputs and are not suitable for other input formats or modalities, e.g. multi-spectral or other inputs, which are common in Remote Sensing and are often available e.g. from Sentinel-2 and Landsat sensors. At the same time, multi-sensor inputs for Remote Sensing are very valuable as they contribute complementary information, e.g. different input types provide different perspectives on the same data, Figure 1: We demonstrate here how generalist Gemini2.5 multimodal model, trained on RGB-only inputs, can be adapted, when queried Zero-Shot and without any training, to understand new and unfamiliar multi-spectral inputs. This improves Geminis already strong performance on these tasks and extends its applicability to more Remote Sensing tasks which often rely on extra visual inputs. for example the multi-spectral bands capture additional frequencies which can detect various aspects of the material properties on the ground. Combining multiple sensors can lead to more accurate and detailed responses, whereas individual sensors, e.g. RGB, have blind spots and limitations. common approach is to train domain-specific foundational models [1, 2, 3, 4] which are designed to input these specialized sensing modalities e.g. RGB, NIR and SWIR channels. These models tend to replicate the large multimodal models, but instead train on collection of sensor-specific data, which is very costly process and requires domain-specific data collection which is non-trivial for Remote Sensing. Furthermore, the new input sensors and their input specifications might change in time, which makes it hard to support such specialized foundational models. For example, the data collection is very sensor-dependent which might not be compatible with previous or subsequent sensors: new generation of sensors might extend the number of inputs or produce different or higher quality data than previous ones; new types of sensors can also become available. It will be desirable that the multimodal models do not need to be re-trained for each new change in the input sources. The models should be resilient to newer or different data collection efforts and be easily adaptable to sensors with similar but not exactly the same set of inputs. But how can we leverage these generalist multimodal models for specialized tasks without additional training or fine-tuning? We here propose to re-purpose generalist multimodal models to understand new and unknown input sensing modalities leveraging the generalist models ability to understand visual data, and the models ability to understand detailed and specific instructions about the input. This is done without any need for training or model adaptations. The key idea is to represent the new inputs as pseudo-image inputs and for each of them to contextualize the inputs with extensive text descriptions which specifically describe what the visual embodiment of the inputs represents (Figure 1). For example, for mutli-spectral input of 12 possible bands, we create number of pseudo-color images, e.g. some of which typically represent NIR, SWIR, NDMI, as well as others which are new. These pseudo images will still be easily understood by the models visual space. At the same time, with an interpretative detailed text prompt we describe how each of these images is created which bands were included and what each band potentially measures in physical space, as well as, interpret its meaning e.g. is indicative of the presence of moisture, or represents green vegetation. Our approach takes full advantage of the inherent and powerful visual representations of generalist LMMs but is interpreting them in way that is helpful for the specific application. This makes generalist LMMs much more versatile and useful for Remote Sensing applications. 2 We demonstrate this idea with the Gemini2.5 model [5]. We observe strong performance of the model with multi-spectral inputs in an entirely Zero-Shot setting, with large improvements in Zero-Shot performance, as well as, achieving new state-of-the-art (SOTA) performances. These results show that large generalist multimodal models can be easily enabled to understand data from new input sensors, thus improving their performance levels without the need for additional training and further increasing their applicability to many more tasks in Remote Sensing."
        },
        {
            "title": "2 Previous work",
            "content": "Multimodal foundational models are widely used for Remote Sensing [6]: These models are specifically trained with Remote Sensing data in order to target these applications. For example, RemoteCLIP [7], SkyCLIP [8], RS-CLIP [9] are commonly used. Another common approach is to create domain-specific foundational models which are fine-tuned from already pre-existing foundational multimodal models, with data specific to Remote Sensing or to pre-train foundational models for Remote Sensing, e.g. [10, 11, 12, 7, 8, 9, 6, 13, 14, 15, 16]. Some other approaches proposed variations of light-weight fine-tuning or adaptation, e.g. [17]. The above-mentioned Remote Sensing models, though, all work in the RGB-only domain. Building foundational models for multi-spectral or multi-sensor Remote Sensing data [11, 1, 2, 18, 19, 20, 3, 21, 22, 23] has been also very beneficial, although it is more challenging due to the diverse set of inputs. SatMAE [1] proposes specialized extension for pre-trained Remote Sensing models where pre-training with multi-spectral imagery is specifically done. The Prithvi model series [2] have developed foundational models which input the Blue, Green, Red, Narrow NIR. SWIR 1, SWIR 2 as input channels. SkySense [11] proposes model for RGB, multi-spectral, as well as, temporal inputs. In [24] the authors propose training on multi-spectral data by separating the bands according to their resolutions. They present results with M1 60x60m (multi-spectral bands 1, 9), M2 (bands 5, 6, 7, 8A, 11, 12) 20x20m and M3 (bands 2, 3, 4, 8) which are RGB and Near infra-red (NIR) at 10x10 resolution. Joint-training across multi-spectral (Sentinel-2 and Sentinel-1) inputs shows more robust performances [25]. Some other foundational models focus on incorporating the rich sensing capabilities of hyper-spectral imagery. For example, SpectralGPT [3] proposed spectral foundational model with 3D Generalized Transformer. HyperSIGMA [4] propose sparse attention learning to optimize the spectral-spatial feature extraction across hyper-spectral inputs. While more comprehensive in their input modalities, these foundational models are not flexible in terms of adding new input channels. Furthermore training multior hyper-spectral models is very resource-intensive task. In contrast, our work leverages generalist (RGB-based) large multimodal models which are not necessarily specific to Remote Sensing, neither in pre-training data nor in input modalities. We show how that such multimodal models, e.g. here Gemini 2.5, can accommodate various multi-spectral new and unfamiliar inputs in Zero-Shot fashion, i.e. without any model training or adaptations needed. Another type of approach is to learn semantically meaningful embeddings from large number of Remote Sensing inputs. Remote Sensing embeddings treat prediction tasks as two-step process, featurization step, which transforms input multi-sensor inputs into learned compact features, or emebeddings, and regression step which associates the embeddings to the outcomes or labels per tasks, for example MOSAIKS [26], SatlasPretrain [10], EarthPT [12], S2Vec [27], AlphaEarth Embeddings [28]. These features can integrate various input sensors, but are appropriate for clustering, further fine-tuning, of low-shot learning scenarios. They are not easy to use in zero-shot settings, as our model is."
        },
        {
            "title": "3 Multi-Sensor Zero-Shot Inference",
            "content": "Generalist multimodal models are trained on images harvested from the web. As result, their performance on natural images in Zero-Shot settings is quite impressive. However, they are not able to understand additional inputs, such as, for example, multi-spectral inputs, commonly used for Remote Sensing. In this section we demonstrate how we can adapt these models to other sensing modalities, still performing Zero-Shot inference, and without the need to re-train these models. 3 We propose simple adaptation of the new and unfamiliar inputs and specialized informative prompt explaining the process of input generation, by which we easily turn generalist multimodal model into model which understands and performs well on new multi-sensor data (Figure 1). The key idea is to combine the multi-spectral inputs in various ways and represent them as images or pseudo-image inputs, and for each of them, to contextualize the inputs with extensive prompts, i.e. text descriptions which specifically describe what the inputs visual embodiment represents. For example, for multi-spectral input of 12 possible bands, we create number of pseudo-color images which typically represent NIR, SWIR, NDMI, and others, which are common and can be helpful for Remote Sensing tasks. Example pseudo-image inputs, generated from various multispectral bands, are visualized in Figure 2 for the BigEarthNet [29] dataset, and in Figure 3 for the EuroSat [30] dataset. The above-mentioned newly-generated images are provided as inputs to the model. Then, with an descriptive and detailed prompt we describe how each of these images is created, that is, which bands were included and what each band potentially measures in physical space, as well as, interpret its meaning via text prompt. In this way the multi-spectral input modalities can still be represented as image inputs which the generalist multimodal models are able to understand, providing format which is well suited for image representations. Furthermore, the text prompt provides the necessary information and context about the new inputs. For example, we can list the input sensors within the prompt: The images are generated by selecting different bands of the Sentinel-2 satellite. The band information is as follows: 1. B02: Blue band at 10-meter resolution 2. B03: Green band at 10-meter resolution 3. B04: Red band at 10-meter resolution 4. B05: Red edge band (Central wavelength of 704.1 nm) at 20-meter resolution 5. B06: Red edge band (Central wavelength of 740.5 nm) at 20-meter resolution 6. B07: Red edge band (Central wavelength of 782.8 nm) at 20-meter resolution 7. B08: NIR band at 10-meter resolution 8. B8A: Narrow NIR band at 20-meter resolution 9. B01: Costal Aerosol band at 60-meter resolution 10. B09: Water vapor band at 60-meter resolution 11. B11: SWIR band (Central wavelength 1613.7 nm) at 20-meter resolution 12. B12: SWIR band (Central wavelength 2202.4 nm) at 20-meter resolution. Using this information, we can further describe the inputs within the prompt, as follows (please see the Appendix for the full prompt): The first image is the RGB image generated using B04, B03 and B02 bands. The second image is the False Color Composite image generated using B08, B04 and B03 bands. The third image is the NDVI image which is colormap with values Red, Yellow and Green generated using B08 and B04 bands. The fourth image is NDWI image whose values are in the range of -0.8 to 0.8 and using color map varying linearly as [(1, 1, 1), (1, 1, 1), (0, 0, 1)]. The fifth image is the NDMI images generated using B8A and B11 bands with the colormap varying linearly as [(1, 0, 0), (0, 1, 0), (0, 0, 1)]. The sixth image is the NDMI images generated using B8A and B12 bands with the colormap varying linearly as [(1, 0, 0), (0, 1, 0), (0, 0, 1)]. This approach takes full advantage of the inherent powerful visual representations of generalist LMMs but is interpreting the new visual input in way that is helpful for the task at hand and is specialized to the inputs of the Remote Sensing applications. 4 Figure 2: Examples of the six inputs from BigEarthNet, which is multi-spectral dataset. The inputs used here are RGB images (Column 1), and five different multi-spectral inputs, represented here as pseudo-color images, so that the model can understand them. Column 2 is constructed from the 8th multi-spectral band, which contains broader range of the visible and near-infrared spectrum and the 4th and 3rd bands. Column 3 contains an pseudo image, similar to NDVI, which is sensitive to vegetation. Column 4 shows set of multi-spectral inputs which are more sensitive to water and Columns 5 and 6 might be also helpful for vegetation. With that simple adaptation and specialized prompt explaining the process of input generation we easily turn generic foundational model into model understanding and performing well on multi-sensor Remote Sensing data. 5 Figure 3: Examples of the six inputs which are obtained from the multi-spectral EuroSat dataset and represented here as pseudo-color images. See Figure 2 for description of the images. 3. Implementation details For concreteness, we here create five additional input pseudo images, in addition to the standard RGB input. They are as follows: 1) false color composite image generated using bands B08, B04 and B03 bands; 2) an NDVI image using bands B08 and B04 bands; 3) an NDWI image 4) an NDMI image using bands B8A and B11; 5) an NDMI images generated using B8A and B12 bands. Color maps are applied when necessary. When generating the true color image and false color composite image (i.e. the first and the second images), we perform normalization of each band to bring the input value in the range [0, 1]. We then scale these values to fall in the range [0, 255] before stacking the bands together to be saved as RGB image."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Zero-Shot Results with Multi-Spectral Inputs on BigEarthNet We first show the performance of the proposed approach on the popular BigEarthNet dataset [29], which is land cover classification dataset which provides 12 multi-spectral bands. The dataset is multi-label dataset, which means that more than one label are considered correct per example. For that reason the main metrics in evaluating this dataset is the F1 metrics, which is also consistent with performance reporting in the literature [31]. We also report Precision and Recall values to gain understanding of these components and their contribution to the final F1 metrics. Furthermore, since the dataset is multi-label, we prompt the model specifying that more than one class is possible as an output. We do not limit the number of output classes for the model. See the Appendix for the specific prompt. We here report Zero-Shot results, demonstrating the performance of Gemini 2.5, without the need for any fine-tuning for the new multi-spectral inputs or to specific task or dataset. Our results are aligned with recent work [31] which benchmarked GPT-4V and several other methods in the Zero-Shot setting, as well. Model F1 Precision Recall Gemini2.5 with RGB Gemini2.5 with Multi-Spectral (Ours) 0.388 0.429 0.516 0.526 0.311 0.363 Table 1: BigEarthNet land cover classification Zero-Shot results (multi-label, multi-class). The same base multimodal model is used without modifications (Gemini 2.5). The main performance metrics is F1. The gain realized from multi-spectral inputs is significant by +0.04 in F1. Classification with 43 classes. Model F1 Precision Recall Gemini2.5 with RGB Gemini2.5 with Multi-Spectral (Ours) 0.414 0.453 0.542 0.566 GPT-4V [31] Qwen-VL-Chat [31] InstructBLIP-FLAN-T5-xxl [31] InstructBLIP-Vicuna-13b [31] LLaVA-v1.5 [31] 0.38 0.40 0.02 0.01 0.39 0.49 0.57 0.41 0.01 0.27 0.335 0.377 0.43 0.39 0.01 0.06 0.83 Table 2: BigEarthNet land cover classification Zero-Shot results (multi-label, multi-class). The same base multimodal model is used without modifications (Gemini 2.5). The main performance metrics is F1. The gain realized from multi-spectral inputs is about +0.04 in F1. Classification with with 19 classes. At the bottom, we compare to SOTA Zero-Shot results on the 19-classes variant and observe +0.053 gain in F1 over the best SOTA results. Table 1 shows the gains obtained from the proposed multi-spectral approach, compared to RGB-only. In both cases we report Zero-Shot performances and use the same Gemini 2.5 model for inference, where the same model is used for both RGB and multi-spectral inputs. We see that multi-spectral inputs provide significant improvements, and are realized without needing any additional training or adaptation. The results reported are for the original version of the dataset with 43 target categories. Since some of the original classes can be quite ambiguous and have overlapping semantic meaning, newer version of the dataset is proposed with 19 classes. In Table 2 we show the results on BigEarthNet with 19 classes, where the 43 classes are merged into fewer and less ambiguous classes, as adopted in the literature [32, 31]. As seen, the proposed approach performs consistently well, with similar margins of improvements, on this version of the dataset, as well. Table 2 (bottom) shows the Zero-Shot results compared to the state-of-the-art (SOTA) Zero-Shot methods. As seen, Gemini 2.5 already has very strong performance in the RGB-only setting,"
        },
        {
            "title": "Model",
            "content": "Classification Accuracy, Top 1 (%) Gemini2.5 with RGB Gemini2.5 with Multi-Spectral (Ours) CLIP [33] (CLIP-VIT L/14-336px) CLIP [33] (CLIP-VIT L/14) ZLaP [34] (Inductive inference) 66.3 69.1 59.6 59.9 63.2 Table 3: Zero-Shot classification on the EuroSat benchmark. As seen, our proposed approach with inference-only multi-spectral inputs performs best. We further report SOTA result and the best results of ZLaP which uses Inductive Zero-Shot inference, meaning it is allowed to use the dataset examples, but without labels. Our approach, which is Zero-Shot only, outperforms this method too."
        },
        {
            "title": "Model",
            "content": "F"
        },
        {
            "title": "Precision Recall",
            "content": "RGB only RGB + NDVI RGB + NDVI + NDWI RGB + All Multi-Spectral RGB only RGB + NDVI RGB + NDVI + NDWI RGB + All Multi-Spectral 0.406 0.415 0.437 0.462 0.410 0.448 0.442 0.479 0.560 0.509 0.535 0.557 0.533 0.548 0.538 0. 0.318 0.350 0.369 0.394 0.334 0.379 0.375 0.415 Table 4: Ablation results with adding NDVI and NDWI channels. The experiments are conducted on the BigEarthNet multi-class multi-label benchmark, conducted on subset of the dataset. Zero-Shot classification. Top: results with 43 classes, Bottom: results with 19 classes. compared to powerful other models. Its performance is further improved by the proposed multispectral approach. 4.2 Zero-Shot Results with Multi-Spectral Inputs EuroSat We further evaluate the performance of our Zero-Shot inference-only approach on the popular EuroSat land use classification benchmark [30]. This benchmark is of larger resolutions RGB imagery. It has 10 classes and thus accuracy is its main performance metric. Table 3 shows the results. As seen, with our proposed approach, we can obtain +3% accuracy gains on this benchmark. We report Zero-Shot results for this benchmark, as well as, SOTA Zero-Shot results, including the results of ZLaP which uses more information. More specifically ZLaP uses Inductive Zero-Shot inference, meaning it is allowed to use the dataset examples, but without labels. As seen, our approach, outperforms the SOTA approaches, and is preferred to this method, as well. While this dataset obtains high performance when fine-tuned, with classification accuracies in the 90s, we see here that zero-shot performance has larger gap. The reason might stem from the fact that some classes are hard to distinguish without any priori information e.g. Annual Crop vs Permanent Crop and the model cannot distinguish between them without an association of how each one might look visually. Fine-tuning is helpful here as it allows machine learning model to associate visual input to specific class label, which is not available in the Zero-Shot setting. We also note that there are also some noisy and mislabeled examples in this dataset. Nevertheless, we observe improved performance with good accuracy gains of our approach for this benchmark, as well, and are able to decrease the gap to the fully-fine-tuned performance further. 4.3 Ablations In Table 4 we show an ablation of the Zero-Shot performance of the model, when varying the multi-spectral inputs. The experiment is conducted on smaller subset of the multi-label multiclass BigEarthNet dataset (on 1000 images only). As seen, adding all multi-spectral images is 8 Figure 4: Example results on the EuroSat data. The multi-spectral inputs are helping for improved performance. Top: The answer provided by the proposed model with multi-spectral inputs is River (9), which is the correct answer. Whereas the answer for the RGB-only input is Forest (2). As seen by the multi-spectral bands, while the input RGB image has blue/green visible features, the multispectral bands are able to detect water (bottom, left-most image which is an NDWI i.e. Normalized Difference Water Index image which is specifically targeting water bodies). Bottom: An example of Forest (2) which the multi-spectral model identifies correctly. The baseline model is mistaken focusing on the green/blue and classifies it as Sea lake (10) which is incorrect. performing best. Secondly, the performance when including NDVI images alone (together with RGB) is responsible for good portion of these gains, but is not as good as when using all inputs. 4.4 Visualizations Figure 4 shows an example of how the model performs in Zero-Shot with multi-spectral inputs and where differences to RGB-only are seen. As seen, the additional multi-spectral inputs are crucial to help the model understand situations where the RGB-only image might be ambiguous, e.g. an image with forest which has deep blue/green hue can be often confused with an image with water i.e. sea or ocean regions and vice versa. The presence of inputs from multi-spectral bands help with better outcomes. Specifically, at the top where an image of river is seen, the RGB-only model predicts Forest. The answer provided by the proposed model with multi-spectral inputs is River, which is the correct answer. At the bottom we see an example of forest area, which the multi-spectral model identifies correctly. The baseline RGB-only model classifies it as Sea lake which is incorrect, likely confusing green/blue colors in the RGB image. Figure 5 further shows examples where both models are in agreement. It shows an example of river image where both models are correct, here comparing to the top image of Figure 4, we see that the RGB input is sufficient, as the colors much less ambiguous. An example where both models are mistaken is shown in the bottom image. We note that across the examples in both Figure 4 and 5, the prediction using the RGB input alone might be quite hard, thus the multi-spectral inputs are key in more accurate understanding. 4.5 Limitations As indicated in this work, we are leveraging the ability of generalist multimodal models to understand visual information, as well as, the associated text prompts, and propose to enrich and adapt the models inputs in order to expand their multi-modal capabilities to additional sensors. Many types of aerial Earth observation data, e.g. overhead Remote Sensing imagery, multi-spectral, hyper-spectral, thermal, LiDAR, Radar, and SAR inputs, all comply with visual input and are particularly fitting for the proposed approach. However, this might not apply to other sources of scientific or Earth observation data as other types of Earth observations may not necessarily map well to visual inputs. Furthermore, we note that the model outputs could be sensitive to the text prompt and the order of prompting, and some changes in the prompt might yield different results. 9 Figure 5: Example results on the EuroSat data where both models are in agreement. Top: Another example of River (9) which both models identify correctly. Bottom: Both models are mistaken, the proposed model identifies this as Annual crop (1), whereas the baseline model classifies it as Sea lake . The ground truth is Pasture (6). As compared to Figure 4, it is quite challenging to correctly identify the only from RGB image."
        },
        {
            "title": "5 Conclusions and future work",
            "content": "Remote Sensing or aerial Earth observation models require lots of resources and datasets for pretraining. Particularly if these models need to be extended to capture multiple additional sensors e.g. multi-spectral or depth information. We propose to leverage already pre-trained generic multimodal models, trained on RGB-only, and to adapt them, on-the-fly in Zero-Shot setting, to the additional new sensing modalities e.g. the multi-spectral imagery, modality type the models have not seen in training. To exemplify the idea we use the generalist Multimodal model, here Gemini 2.5, which is trained on vision tasks with RGB-only inputs, and which has not used any specialized multi-spectral data during training. We show that one can very easily leverage the visual understanding of the model and its prompting capability to easily adapt the model to these new unseen inputs and unfamiliar tasks. We observe improved performance for the Gemini 2.5 model, which already performs very strongly on these RemoteSensing tasks. We explored several inputs from several multi-spectral bands, with successful results. Other types of sensors e.g. SAR, depth, or heat sensors, etc ,can also be naturally mapped to visual inputs, and the approach will not need any changes, in order to use them. However, as noted above, there are sensors which may not be particularly fitting. We leave these explorations for future work. We further note that out of 12 multi-spectral bands we only created five example image combinations, whereas many other useful combinations can be included. This can improve performance further, without needing to change the approach. The implications of the proposed work are that such easy-to-use multimodal adaptation of generalist large multimodal models, provides an alternative to the hard-to-generalize and expensive multimodal Remote Sensing training, as well as, for the independent and costly training with specialized Remote Sensing-specific inputs which might be hard to support. Instead, the proposed approach leverages more general pre-training sources."
        },
        {
            "title": "References",
            "content": "[1] Yezhen Cong, Samar Khanna, Chenlin Meng, Patrick Liu, Erik Rozi, Yutong He, Marshall Burke, David B. Lobell, and Stefano Ermon. SatMAE: Pre-training transformers for temporal and multi-spectral satellite imagery. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. [2] Johannes Jakubik, Sujit Roy, C. E. Phillips, Paolo Fraccaro, Denys Godwin, Bianca Zadrozny, Daniela Szwarcman, Carlos Gomes, Gabby Nyirjesy, Blair Edwards, Daiki Kimura, Naomi Simumba, Linsong Chu, S. Karthik Mukkavilli, Devyani Lambhate, Kamal Das, Ranjini Bangalore, Dario Oliveira, Michal Muszynski, Kumar Ankur, Muthukumaran Ramasubramanian, Iksha Gurung, Sam Khallaghi, Hanxi (Steve) Li, Michael Cecil, Maryam Ahmadi, Fatemeh Kordi, Hamed Alemohammad, Manil Maskey, Raghu Ganti, 10 Kommy Weldemariam, and Rahul Ramachandran. Foundation Models for Generalist Geospatial Artificial Intelligence. Preprint Available on arxiv:2310.18660, October 2023. [3] Danfeng Hong, Bing Zhang, Xuyang Li, Yuxuan Li, Chenyu Li, Jing Yao, Naoto Yokoya, Hao Li, Pedram Ghamisi, Xiuping Jia, Antonio Plaza, Paolo Gamba, Jon Atli Benediktsson, and Jocelyn Chanussot. Spectralgpt: Spectral remote sensing foundation model. In IEEE TPAMI, 2024. [4] Di Wang, Meiqi Hu, Yao Jin, Yuchun Miao, Jiaqi Yang, Yichu Xu, Xiaolei Qin, Jiaqi Ma, Lingyu Sun, Chenxing Li, Chuan Fu, Hongruixuan Chen, Chengxi Han, Naoto Yokoya, Jing Zhang, Minqiang Xu, Lin Liu, Lefei Zhang, Chen Wu, Bo Du, Dacheng Tao, and Liangpei Zhang. Hypersigma: Hyperspectral intelligence comprehension foundation model. In IEEE TPAMI, 2025. [5] Gemini Team. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. In arxiv.org/abs/2507.06261, 2025. [6] Siqi Lu, Junlin Guo, James Zimmer-Dauphinee, Jordan Nieusma, Xiao Wang, Parker VanValkenburgh, Steven Wernke, and Yuankai Huo. Vision foundation models in remote sensing: survey. In arxiv:2408.03464, 2024. [7] Fan Liu, Delong Chen, Zhangqingyun Guan, Xiaocong Zhou, Jiale Zhu, Qiaolin Ye, Liyong Fu, and Jun Zhou. RemoteCLIP: vision language foundation model for remote sensing. In IEEE Transactions on Geoscience and Remote Sensing (TGRS), 2024. [8] Zhecheng Wang, Rajanie Prabha1, Tianyuan Huang, Jiajun Wu, and Ram Rajagopal. Skyscript: large and semantically diverse vision-language dataset for remote sensing. In AAAI, 2024. [9] Xiang Li, Congcong Wen, Yuan Hu, and Nan Zhou. Rs-clip: Zero shot remote sensing scene classification via contrastive vision-language supervision. In International Journal of Applied Earth Observation and Geoinformation, 2023. [10] Favyen Bastani, Piper Wolters, Ritwik Gupta, Joe Ferdinando, and Aniruddha Kembhavi. Satlaspretrain: large-scale dataset for remote sensing image understanding. In ICCV, 2023. [11] Xin Guo, Jiangwei Lao, Bo Dang, Yingying Zhang, Lei Yu, Lixiang Ru, Liheng Zhong, Ziyuan Huang, Kang Wu, Dingxiang Hu, Huimei He, Jian Wang, Jingdong Chen, Ming Yang, Yongjun Zhang, and Yansheng Li. Skysense: multi-modal remote sensing foundation model towards universal interpretation for earth observation imagery. In CVPR, 2024. [12] Michael J. Smith, Luke Fleming, and James E. Geach. Earthpt: time series foundation model for earth observation. In arxiv:2309.07207, 2023. [13] Kartik Kuckreja, Muhammad S. Danish, Muzammal Naseer, Abhijit Das, Salman Khan, and Fahad S. Khan. Geochat: Grounded large vision-language model for remote sensing. 2024. [14] Colorado Reed, Ritwik Gupta, Shufan Li, Sarah Brockman, Christopher Funk, Brian Clipp, Kurt Keutzer, Salvatore Candido, Matt Uyttendaele, and Trevor Darrell. Scale-mae: scale-aware masked autoencoder for multiscale geospatial representation learning. ICCV, 2023. [15] Matias Mendieta, Boran Han, Xingjian Shi, Yi Zhu, and Chen Chen. Towards geospatial foundation models via continual pretraining. ICCV, 2023. [16] Taekyung Lee Keumgang Cha, Junghoon Seo. billion-scale foundation model for remote sensing images. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing (IEEE J-STARS), 2023. [17] Irem Ulku, O. Ozgur Tanriover, and Erdem Akagündüz. Lora-nir: Low-rank adaptation of vision transformers for remote sensing with near-infrared imagery. In IEEE Geoscience and Remote Sensing Letters, 2024. [18] Damian Ibañez, Ruben Fernandez-Beltran, Filiberto Pla, and Naoto Yokoya. Masked auto-encoding spectralspatial transformer for hyperspectral image classification. IEEE Transactions on Geoscience and Remote Sensing, 2022. [19] Danfeng Hong, Zhu Han, Jing Yao, Lianru Gao, Bing Zhang, Antonio Plaza, and Jocelyn Chanussot. Spectralformer: Rethinking hyperspectral image classification with transformers. IEEE Trans. Geosci. Remote Sens., 60:115, 2022. DOI: 10.1109/TGRS.2021.3130716. [20] Linus Scheibenreif, Michael Mommert, and Damian Borth. Masked vision transformers for hyperspectral image classification. EEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2023. 11 [21] Anthony Fuller, Koreen Millard, and James R. Green. Croma: Remote sensing representations with contrastive radar-optical masked autoencoders. 2023. [22] Guillaume Astruc, Nicolas Gonthier, Clement Mallet, and Loic Landrieu. Omnisat: Self-supervised modality fusion for earth observation. ECCV, 2024. [23] Vishal Nedungadi, Ankit Kariryaa, Stefan Oehmcke, Serge Belongie, Christian Igel, and Nico Lang. Mmearth: Exploring multi-modal pretext tasks for geospatial representation learning. ECCV, 2024. [24] Ushasi Chaudhuri, Subhadip Dey, Mihai Datcu, Biplab Banerjee, and Avik Bhattacharya. Interband retrieval and classification using the multilabeled sentinel-2 bigearthnet archive. In IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2021. [25] Ori Linial, George Leifman, Yochai Blau, Nadav Sherman, Yotam Gigi, Wojciech Sirko, and Genady Beryozkin. Enhancing remote sensing representations through mixed-modality masked autoencoding. In Winter Conference on Applications of Computer Vision (WACV) Workshops, 2025. [26] Esther Rolf, Jonathan Proctor, Tamma Carleton, Ian Bolliger, Vaishaal Shankar, Miyabi Ishihara, Benjamin Recht, and Solomon Hsiang. generalizable and accessible approach to machine learning with global satellite imagery. In arxiv.org/abs/2010.08168, 2020. [27] Shushman Choudhury, Elad Aharoni, Chandrakumari Suvarna, Iveel Tsogsuren, Abdul Rahman In S2vec: Self-supervised geospatial embeddings. Kreidieh, Chun-Ta Lu, and Neha Arora. https://arxiv.org/abs/2504.16942, 2025. [28] Christopher F. Brown, Michal R. Kazmierski, Valerie J. Pasquarella, William J. Rucklidge, Masha Samsikova, Chenhui Zhang, Evan Shelhamer, Estefania Lahera, Olivia Wiles, Simon Ilyushchenko, Noel Gorelick, Lihui Lydia Zhang, Sophia Alj, Emily Schechter, Sean Askay, Oliver Guinan, Rebecca Moore, Alexis Boukouvalas, and Pushmeet Kohli. Alphaearth Foundations: An embedding field model for accurate and efficient global mapping from sparse label data. In arxiv.org/pdf/2507.22291, 2025. [29] Gencer Sumbul, Marcela Charfuelan, Begüm Demir, and Volker Markl. Bigearthnet: large-scale benchmark archive for remote sensing image understanding. IEEE International Geoscience and Remote Sensing Symposium (IGARSS), 2019. [30] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: novel dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2019. [31] Chenhui Zhang and Sherrie Wang. Good at captioning, bad at counting: Benchmarking gpt-4v on earth observation data. In arxiv.org/pdf/2401.17600, 2024. [32] Gencer Sumbul, Arne de Wall, Tristan Kreuziger, Filipe Marcelino, Hugo Costa, Pedro Benevides, Mário Caetano, Begüm Demir, and Volker Markl. Bigearthnet-mm: large scale multi-modal multi-label benchmark archive for remote sensing image classification and retrieval. IEEE Geoscience and Remote Sensing Magazine, 2021. [33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021. [34] Vladan Stojnic, Yannis Kalantidis, and Giorgos Tolias. Label propagation for zero-shot classification with vision-language models. In CVPR, 2024. Additional information on example Zero-Shot prompts We here provide the prompts used for Multi-Spectral inputs, followed by RGB-only prompt examples. The first prompt examples are for the BigEarthNet, and the last one is for EuroSat. For different benchmark, the set of available classes will need to change. Since BigEarthNet is multi-label multi-class dataset, we ask for multiple answers, i.e. by adding Select all that apply, which is simply removed for classification datasets, e.g. for EuroSat. Prompt for Multi-spectral inputs on BigEarthNet: 12 Instructions: Answer the question asked after the given 6 images of the same scene. The images are generated by selecting different bands of the Sentinel-2 satellite. The band information is as follows: 1. B02: Blue band at 10-meter resolution 2. B03: Green band at 10-meter resolution 3. B04: Red band at 10-meter resolution 4. B05: Red edge band (Central wavelength of 704.1 nm) at 20-meter resolution 5. B06: Red edge band (Central wavelength of 740.5 nm) at 20-meter resolution 6. B07: Red edge band (Central wavelength of 782.8 nm) at 20-meter resolution 7. B08: NIR band at 10-meter resolution 8. B8A: Narrow NIR band at 20-meter resolution 9. B01: Costal Aerosol band at 60-meter resolution 10. B09: Water vapor band at 60-meter resolution 11. B11: SWIR band (Central wavelength 1613.7 nm) at 20-meter resolution 12. B12: SWIR band (Central wavelength 2202.4 nm) at 20-meter resolution. The first image is the RGB image generated using B04, B03 and B02 bands. The second image is the False Color Composite image generated using B08, B04 and B03 bands. The third image is the NDVI image which is colormap with values Red, Yellow and Green generated using B08 and B04 bands. The fourth image is NDWI image whose values are in the range of -0.8 to 0.8 and using color map varying linearly as [(1, 1, 1), (1, 1, 1), (0, 0, 1)]. The fifth image is the NDMI images generated using B8A and B11 bands with the colormap varying linearly as [(1, 0, 0), (0, 1, 0), (0, 0, 1)]. This image is used for identifying wet and dry areas. The sixth image is the NDMI image generated using B8A and B12 bands with the colormap varying linearly as [(1, 0, 0), (0, 1, 0), (0, 0, 1)]. This image is used for identifying wet and dry areas. Output format: Output the option numbers corresponding to the correct answer in the format \"(X)\" where is the correct number choice (among 1 to 43). In case of multiple correct answers, output the answer choices in the same format separated by commas. For example, if the correct answers are choices 1 and 3, output \"(1),(3)\". Only output the option numbers in the above format and DO NOT OUTPUT the full answer text or any other extra words. Question: To which of the following classes do the given images belong to? Select all that apply. Possible answer choices: (1)Agro-forestry areas (2)Airports (3)Annual crops associated with permanent crops (4)Bare rock (5)Beaches, dunes, sands (6)Broad-leaved forest (7)Burnt areas (8)Coastal lagoons (9)Complex cultivation patterns (10)Coniferous forest (11)Construction sites (12)Continuous urban fabric (13)Discontinuous urban fabric (14)Dump sites (15)Estuaries (16)Fruit trees and berry plantations (17)Green urban areas (18)Industrial or commercial units (19)Inland marshes (20)Intertidal flats (21)Land principally occupied by agriculture, with significant areas of natural vegetation (22)Mineral extraction sites 13 (23)Mixed forest (24)Moors and heathland (25)Natural grassland (26)Non-irrigated arable land (27)Olive groves (28)Pastures (29)Peatbogs (30)Permanently irrigated land (31)Port areas (32)Rice fields (33)Road and rail networks and associated land (34)Salines (35)Salt marshes (36)Sclerophyllous vegetation (37)Sea and ocean (38)Sparsely vegetated areas (39)Sport and leisure facilities (40)Transitional woodland/shrub (41)Vineyards (42)Water bodies (43)Water courses Answer: Prompt for RGB-only input for BigEarthNet Instructions: Answer the question asked after the given image. Output format: Output the option numbers corresponding to the correct answer in the format \"(X)\" where is the correct digit choice (among 1 to 43). In case of multiple correct answers, output the answer choices in the same format separated by commas. For example, if the correct answers are choices 1 and 3, output \"(1),(3)\". Only output the option numbers in the above format and DO NOT OUTPUT the full answer text or any other extra words. Question: To which of the following class does the given image belong to? Select all that apply. Possible answer choices: (1)Agro-forestry areas (2)Airports (3)Annual crops associated with permanent crops (4)Bare rock (5)Beaches, dunes, sands (6)Broad-leaved forest (7)Burnt areas (8)Coastal lagoons (9)Complex cultivation patterns (10)Coniferous forest (11)Construction sites (12)Continuous urban fabric (13)Discontinuous urban fabric (14)Dump sites (15)Estuaries (16)Fruit trees and berry plantations (17)Green urban areas (18)Industrial or commercial units (19)Inland marshes (20)Intertidal flats (21)Land principally occupied by agriculture, with significant areas of natural vegetation (22)Mineral extraction sites (23)Mixed forest (24)Moors and heathland (25)Natural grassland (26)Non-irrigated arable land 14 (27)Olive groves (28)Pastures (29)Peatbogs (30)Permanently irrigated land (31)Port areas (32)Rice fields (33)Road and rail networks and associated land (34)Salines (35)Salt marshes (36)Sclerophyllous vegetation (37)Sea and ocean (38)Sparsely vegetated areas (39)Sport and leisure facilities (40)Transitional woodland/shrub (41)Vineyards (42)Water bodies (43)Water courses Answer: Prompt for RGB-only input for BigEarthNet, 19 classes Instructions: Answer the question asked after the given image. Output format: Output the option numbers corresponding to the correct answer in the format \"(X)\" where is the correct digit choice (among 1 to 19). In case of multiple correct answers, output the answer choices in the same format separated by commas. For example, if the correct answers are choices 1 and 3, output \"(1),(3)\". Only output the option numbers in the above format and DO NOT OUTPUT the full answer text or any other extra words. Question: To which of the following class does the given image belong to? Select all that apply. Possible answer choices: (1)Agro-forestry areas (2)Arable land (3)Beaches, dunes, sands (4)Broad-leaved forest (5)Coastal wetlands (6)Complex cultivation patterns (7)Coniferous forest (8)Industrial or commercial units (9)Inland waters (10)Inland wetlands (11)Land principally occupied by agriculture, with significant areas of natural vegetation (12)Marine waters (13)Mixed forest (14)Moors, heathland and sclerophyllous vegetation (15)Natural grassland and sparsely vegetated areas (16)Pastures (17)Permanent crops (18)Transitional woodland, shrub (19)Urban fabric Answer: Prompt for RGB-only input for EuroSat Instructions: Answer the question asked after the given image. Output format: Output the option number corresponding to the correct answer in the format \"(X)\" where is the correct number choice (among 1 to 10) for the single label classification task. Only output the option number in the above format and DO NOT OUTPUT the full answer text or any other extra words. Question: To which of the following class does the given image belong to? 15 Possible answer choices: (1)AnnualCrop (2)Forest (3)HerbaceousVegetation (4)Highway (5)Industrial (6)Pasture (7)PermanentCrop (8)Residential (9)River (10)SeaLake Answer:"
        },
        {
            "title": "B Dataset details",
            "content": "The experiments conducted in this study are done in evaluation-only Zero-Shot manner. We provide details about the datasets used. The BigEarth dataset [29] has 590k images. The main inputs are of resolution 120x120 pixels, the last two images are 60x60 pixels. It is multi-label dataset with more than one correct label per image. It has 43 classes. Because of ambiguity within the original version, there is an 19-class variant which is often used in the literature [32]. The EuroSat dataset [30] has 27k images with image resolution of 64x64 pixels. It has 10 classes."
        }
    ],
    "affiliations": [
        "Google DeepMind",
        "Google Research"
    ]
}