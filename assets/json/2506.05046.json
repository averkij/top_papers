{
    "paper_title": "FlowDirector: Training-Free Flow Steering for Precise Text-to-Video Editing",
    "authors": [
        "Guangzhao Li",
        "Yanming Yang",
        "Chenxi Song",
        "Chi Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Text-driven video editing aims to modify video content according to natural language instructions. While recent training-free approaches have made progress by leveraging pre-trained diffusion models, they typically rely on inversion-based techniques that map input videos into the latent space, which often leads to temporal inconsistencies and degraded structural fidelity. To address this, we propose FlowDirector, a novel inversion-free video editing framework. Our framework models the editing process as a direct evolution in data space, guiding the video via an Ordinary Differential Equation (ODE) to smoothly transition along its inherent spatiotemporal manifold, thereby preserving temporal coherence and structural details. To achieve localized and controllable edits, we introduce an attention-guided masking mechanism that modulates the ODE velocity field, preserving non-target regions both spatially and temporally. Furthermore, to address incomplete edits and enhance semantic alignment with editing instructions, we present a guidance-enhanced editing strategy inspired by Classifier-Free Guidance, which leverages differential signals between multiple candidate flows to steer the editing trajectory toward stronger semantic alignment without compromising structural consistency. Extensive experiments across benchmarks demonstrate that FlowDirector achieves state-of-the-art performance in instruction adherence, temporal consistency, and background preservation, establishing a new paradigm for efficient and coherent video editing without inversion."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 6 4 0 5 0 . 6 0 5 2 : r FlowDirector: Training-Free Flow Steering for Precise Text-to-Video Editing Guangzhao Li1,2 Yanming Yang1 Chenxi Song1 Chi Zhang1 1AGI Lab, Westlake University 2Central South University https://flowdirector-edit.github.io Figure 1: Qualitative results produced by our training-free video editing framework. Given source video and target textual prompt, our method performs precise and semantically faithful edits while preserving the spatial content and motion dynamics of unedited regions. The results exhibit strong alignment with the editing instructions, high visual fidelity, and consistent temporal coherence across frames."
        },
        {
            "title": "Abstract",
            "content": "Text-driven video editing aims to modify video content according to natural language instructions. While recent training-free approaches have made progress by leveraging pre-trained diffusion models, they typically rely on inversion-based techniques that map input videos into the latent space, which often leads to temporal inconsistencies and degraded structural fidelity. To address this, we propose FlowDirector, novel inversion-free video editing framework. Our framework models the editing process as direct evolution in data space, guiding the video via an Ordinary Differential Equation (ODE) to smoothly transition along its inherent spatiotemporal manifold, thereby preserving temporal coherence and structural details. To achieve localized and controllable edits, we introduce an attention-guided masking mechanism that modulates the ODE velocity field, preserving non-target regions both spatially and temporally. Furthermore, to address incomplete edits and enhance semantic alignment with editing instructions, we present guidanceenhanced editing strategy inspired by Classifier-Free Guidance, which leverages *This work was done during Guangzhao Lis visit at AGI Lab, Westlake University. denotes corresponding author. differential signals between multiple candidate flows to steer the editing trajectory toward stronger semantic alignment without compromising structural consistency. Extensive experiments across benchmarks demonstrate that FlowDirector achieves state-of-the-art performance in instruction adherence, temporal consistency, and background preservation, establishing new paradigm for efficient and coherent video editing without inversion."
        },
        {
            "title": "Introduction",
            "content": "Text-driven video editing aims to modify video content based on natural language instructions, offering powerful and intuitive interface for content creation and manipulation. The rapid development of this field has been largely fueled by recent advances in generative artificial intelligence, particularly the emergence of large-scale pre-trained diffusion models [12, 42] such as Stable Diffusion [35, 32]. These models are trained on extensive image-text datasets [36, 52, 1, 25, 4, 38] and encode rich visual and semantic priors, enabling them to synthesize high-quality and semantically aligned images from textual descriptions. Building upon these capabilities, recent research efforts [49, 28, 39, 33, 20, 8, 6, 16, 17] have extended diffusion models from static image generation to dynamic video domains. Such extensions have given rise to training-free video editing pipelines [33, 8, 6, 17, 48, 27, 53, 47], which directly manipulate video frames under textual guidance by reusing the knowledge embedded in pre-trained diffusion backbones. Despite promising progress, text-driven video editing presents unique and significant challenges that distinguish it from image editing. Videos are inherently high-dimensional sequences with rich temporal dynamics, requiring coherence not only within each frame but also across time. Most existing training-free methods [33, 8, 6, 17, 48] rely on inversion-based [31] strategies, which map the input video into the latent space of pre-trained diffusion model. While such techniques have proven effective for static image editing, their direct extension to videos introduces significant complications. Inverting an entire video sequence coherently requires generating temporally smooth latent trajectory, which existing image-based inversion techniques are not designed to produce. Furthermore, the inversion process can distort structural integrity, especially when dealing with complex spatiotemporal patterns and dynamic scenes. These limitations often result in artifacts such as content misalignment, inconsistency in style across frames, and implausible motion. In this work, we explore novel inversion-free framework for text-driven video editing that overcomes the fundamental limitations of prior methods. Inspired by recent progress in flow-based inversion-free image editing [51, 24], we develop continuous editing paradigm that models the transformation from the original video to the edited result as direct evolution in data space, governed by learned Ordinary Differential Equation (ODE). Unlike traditional approaches that attempt to recover precise latent representations for each frame through inversion, where even minor errors can disrupt inter-frame structure, our method constructs smooth transformation path that guides the video along its native spatiotemporal manifold. This progressive editing trajectory ensures that each intermediate state remains close to the original videos dynamics, preserving fine-grained motion details, maintaining temporal consistency, and avoiding structural artifacts. critical challenge in video editing is maintaining the integrity of unedited regions, both spatially and temporally. Inversion-based methods often suffer from unintended alterations to background or context regions, due to the information loss and reconstruction inaccuracies inherent in the inversion process. While previous methods [33, 8, 6, 48] attempt to control this via attention modulation in latent space, they cannot ensure precise preservation of the spatial and temporal integrity within these unedited regions. To address this, our framework introduces Spatially Attentive Flow Correction (SAFC), an explicit attention-guided control mechanism that enforces localized editing. Specifically, during the ODE-driven transformation, we modulate the velocity field [26] such that regions designated as non-editable have zero velocity, effectively freezing them throughout the editing process. This ensures that these regions remain identical to the original video throughout the transformation. Compared with previous methods that only implicitly constrain edit regions in latent space, this mechanism provides strong guarantees for the preservation of spatial and temporal integrity in non-target regions, offering fine-grained control over the extent and scope of the edits. While strict structural preservation helps maintain coherence, it can also hinder the models ability to perform large semantic transformations, especially when the editing instruction requires significant departure from the original content. This inherent trade-off between editability and fidelity remains central bottleneck in existing approaches. To address this challenge, we introduce guidanceenhanced editing strategy inspired by the principles of Classifier-Free Guidance (CFG) [13], termed Differential Averaging Guidance (DAG). Our goal is to drive the editing process toward semantically meaningful editing outcomes while maintaining overall structural and temporal consistency. Specifically, from the current intermediate editing state, we generate multiple candidate flows, each representing potential direction of semantic transformation. These flows collectively capture diverse set of editing hypotheses. We compute the high-quality estimate of the editing direction by averaging all candidate flows, which reinforces their consensus and attenuates random noise. Simultaneously, we compute baseline estimate by averaging only smaller subset of the flows, representing more conservative direction. The difference between the two estimates serves as differential guidance signalanalogous to the contrast between conditional and unconditional branches in CFG [13], which highlights the direction along which semantic alignment can be further improved. This differential signal is then applied to adjust and refine the ODE-driven editing trajectory, enabling the model to pursue stronger semantic updates without the need to average over an excessive number of directions. In doing so, our strategy accelerates convergence, enhances semantic alignment with the target prompt, and effectively breaks through the inertia imposed by overly rigid structural constraints. To validate the effectiveness of our approach, we conduct comprehensive experiments across multiple standard video editing benchmarks. The empirical results demonstrate that our method consistently outperforms existing training-free baselines in several key dimensions, including adherence to editing instructions, temporal coherence, and background preservation. Overall, our contributions are summarized as: We introduce FlowDirector, novel, training-free video editing framework. To the best of our knowledge, this is the first work to apply the inversion-free editing paradigm to video editing using pre-trained Text-to-Video (T2V) models. We propose Spatially Attentive Flow Correction (SAFC), an explicit attention-guided control mechanism that enforces fine-grained control over editable regions. By modulating the ODE-driven velocity field, our method ensures that non-target areas remain unchanged, thereby preserving spatial and temporal consistency in unedited regions. We present Differential Averaging Guidance (DAG), guidance-enhanced editing strategy inspired by CFG. This strategy leverages differential signals between high-quality and baseline flow estimates to refine the editing trajectory, enhancing semantic alignment while reducing computational overhead. Extensive experiments demonstrate that our method achieves state-of-the-art performance across diverse editing scenarios. Code will be made publicly available to facilitate further research."
        },
        {
            "title": "2 Related Works",
            "content": "Text-to-Image Editing Diffusion model-based [12, 42, 26] image editing is primarily classified into two paradigms. Inversion-based methods [11, 31, 19, 3, 30, 50, 21] use an invert-then-denoise strategy: source images are mapped to latent noise via deterministic sampling (e.g., DDIM Inversion [41, 31]), with editing by manipulating internal representations during denoising. Their limitation is that inversion approximation errors degrade reconstruction quality and editing fidelity. To overcome this, inversion-free methods emerged. InfEdit [51] uses Denoising Diffusion Consistent Models for virtual inversion. FlowEdit [24] uses Flow Matching Models for direct Ordinary Differential Equation (ODE) path between source/target distributions, bypassing inversion. To better preserve original image structure, FlowEdit [24] averages velocity field differences over multiple noise samples, but this incurs high computational overhead in video, rendering it nearly infeasible for video editing. Text-to-Video Editing is primarily categorized by the underlying generative model. Early approaches adapted T2I models [35, 37, 7, 32], facing temporal consistency challenges due to lack of temporal understanding. Some are zero-shot (FateZero [33], TokenFlow [8], Flatten [6], RAVE [17]), others require fine-tuning (Tune-A-Video [49], Video-P2P [28]). These T2I-based methods often struggle with temporal consistency. Recent work leverages native T2V models [40, 15, 14, 2, 10, 23, 4446, 5] and their learned spatiotemporal priors for improved temporal consistency (e.g., VideoSwap [9], 3 VideoDirector [48]). However, all aforementioned video editing methods are inversion-based. Their inherent limitations hinder substantial visual transformations and often result in deficiencies in editing accuracy and structural consistency. Figure 2: Framework overview. (a) Editing Flow Generation: Our method first infers velocity flows, then combines cross-attention maps with our mask generation technique to spatially refine the initial flow. (b) Differential Averaging Guidance: We iteratively generate high-quality token and multiple baseline tokens, establish stable evolution direction through their difference calculation, which guides token refinement for the final editing vector field."
        },
        {
            "title": "3 Method",
            "content": "3.1 Preliminary Our framework builds upon two foundational concepts in flow-based generative modeling: Flow Matching (FM) [26]: class of generative models aiming to learn smooth transformation from simple initial distribution (e.g., standard normal (0, I)) to target data distribution (e.g., real images). The core method involves training neural network vθ(x, t) to model time-dependent vector field, learned by minimizing the loss function LFM (Equation 1): LFM = Et,p1(x1),pt(xx1) (cid:2)vθ(x, t) vt(xx1)2(cid:3) , (1) where pt(xx1) denotes the conditional probability path from the initial distribution to the target sample x1 at time = 1. Rectified Flow (RF) [29]: As an advancement over FM [26], RF focuses on learning straighter (approximately linear) flow paths connecting the initial distribution p0 and the target distribution p1. This linearization of paths aims to optimize the sample generation process, enabling more efficient and stable integration of the Ordinary Differential Equation (ODE) driven by the learned velocity field vθ. 3.2 Editing Flow Generation Our video editing framework is designed to circumvent the time-consuming and potentially structuredamaging inversion process inherent in traditional methods. It leverages pre-trained text-to-video (T2V) model, vθ, to smoothly and directly transform source video content (Xsrc, described by source prompt csrc) into new video that conforms to target semantics, as specified by target prompt ctar. Inspired by FlowEdit [24], we formulate this editing process as direct evolution path from the source video to the target edited video. The entire editing process can be decomposed into the following key steps: 4 Construction and Evolution of the Direct Editing Path: The core idea is to construct path directly from the source video to the target edited state, rather than traversing through noise space. We represent the state of the video sequence at any given time [0, 1] during the editing process as edit . This state is constructed via the following unified editing equation:"
        },
        {
            "title": "Z edit",
            "content": "t = Xsrc src + tar . (2) Here, src represents the intermediate perturbed state of the source video sequence at time t, and tar represents corresponding, constructed intermediate perturbed state of the target video sequence at time t. Consequently, the evolution of the entire editing path, i.e., edit from = 1 to = 0, is governed by the following ordinary differential equation (ODE): dZ edit dt = Vedit(t) = vθ(Z tar , t, ctar) vθ(Z src , t, csrc), (3) at time t, conditioned on the source prompt csrc. Similarly, vθ(Z tar where Vedit(t) is the direct editing velocity flow that drives the editing. vθ(Z src , t, csrc) is the evolution velocity predicted by the pre-trained T2V model vθ for the intermediate perturbed state of the source video src , t, ctar) is the evolution velocity predicted by the model for the intermediate perturbed state of the target video tar at time t, conditioned on the target prompt ctar. The starting point of this path (t = 1) is the source video, i.e., edit converges to the final edited result. This direct path bypasses the inversion process, which is expected to better preserve the original structural integrity of the video. t=1 = Xsrc. By solving this ODE, as 0, edit 0 , t, csrc). and the perturbed state of the target video tar Generation of Editing Velocity Flow: To compute the editing velocity flow Vedit(t) at time t, we utilize the pre-trained T2V model vθ and construct two critical intermediate perturbed states: the perturbed state of the source video src . Following Rectified Flow [29], the perturbed state of the source video is constructed via linear interpolation as src = (1 t)Xsrc + tNt, where Nt (0, I). This state simulates an intermediate stage in conceptual forward noising process of the source video. We then perform forward pass through the T2V model vθ with this intermediate state src and its corresponding prompt csrc to obtain the required velocity field vθ(Z src For the perturbed state of the target video tar , since its true distribution is unknown, we do not = edit Xsrc. construct it directly. and src This construction method ensures that tar share the same random noise instance Nt. Consequently, when calculating the difference between their respective model-predicted velocities, Vedit, the models response to this common noise will cancel out. This allows Vedit to more purely reflect the semantic changes induced by the different text prompts, effectively guiding the edit towards the target semantics and eliminating interference from irrelevant noise, thereby achieving more precise and structure-preserving edits. We then input this intermediate state tar and its corresponding prompt ctar into the T2V model vθ to perform forward pass and obtain the required velocity field vθ(Z tar , t, ctar). The editing velocity flow Vedit(t) that ultimately drives the editing is the difference between these two velocity fields. Instead, we rearrange Equation 2 to obtain: tar + src Intuitively, through this direct editing path, our method gradually deforms the source video into video that aligns semantically with the target prompt. The velocity difference Vedit(t) captures the directional shift in the latent space between the source and target semantics at each timestep, with the influence of noise already nullified. This enables temporally smooth, spatially localized, and structure-preserving video editing without the need for inversion. 3.3 Spatially Attentive Flow Correction The editing flow Vedit, derived from Equation (3), effectively drives the video towards the target semantic transformation. However, because it operates on the entire video features, it can cause unintended modifications in non-edited regions. To address this issue, we introduce an explicit attention-guided mask mechanism, termed Spatial Attention Flow Correction (SAFC). Its core idea is to modulate the ODE-driven editing velocity field Vedit by resetting the velocity to zero in regions designated for non-editing, thereby freezing these regions throughout the entire editing process. At denoising step t, the latent representation src and source prompt csrc are input to the pre-trained T2V model. During the models forward pass to estimate vsrc, cross-attention maps corresponding to key editing keywords in csrc (e.g., \"jeep\") are extracted from specific internal model blocks. The 5 optimal block location varies by T2V model architecture, and its selection is based on the block whose cross-attention maps exhibit the highest activation and strongest semantic similarity to these keywords. Analogously, the target latent representation tar and target prompt ctar are input to the same T2V model. During model inference to predict vtar, the identical method and block layers are employed to extract cross-attention maps for editing keywords in ctar (e.g., \"Porsche\"). , tar Thus, each denoising iteration yields instantaneous spatial attention information closely related to the current editing states (Z src ) and intents (csrc, ctar). Once obtained, the source and target cross-attention maps are processed by our mask generation method. This involves spatial smoothing of raw attention maps followed by thresholding based on global average activation values to isolate target regions, resulting in binary masks Msrc and Mtar(A more detailed description is provided in Appendix B.). For most editing tasks, Msrc delineates the source object shape, and Mtar indicates the post-edit object shape. Consequently, their union logically forms the target region. As illustrated in Figure 2 (a), this generated mask is used to correct the editing flow: Vedit = Vedit (Msrc Mtar). (4) Compared to previous methods [11, 6, 33, 48] that often guide semantic transformations by directly intervening in the internal generation process of pre-trained models, such as replacing attention maps or scaling attention weights within specific architectural layers, our SAFC fundamentally differs in its implementation mechanism. SAFC entirely avoids any modification or intervention within the models internal structure. It solely extracts required cross-attention information from the pre-trained model and utilizes this information externally to generate spatial masks. These masks subsequently modulate the models output editing velocity field Vedit without interfering with any internal model computations. This highly non-intrusive design ensures SAFCs independence from the pre-trained model, thereby granting it enhanced generality and adaptability. 3.4 Differential Averaging Guidance While strict structural preservation helps maintain coherence, it can also hinder the models ability to perform large semantic transformations. To address this limitation, we introduce guidanceenhanced editing strategy inspired by the principles of Classifier-Free Guidance (CFG) [13, 18], termed Differential Averaging Guidance (DAG). Specifically, as illustrated in Figure 2 (b), at each ODE integration step t, we first compute high-quality edit velocity estimate, denoted as VHQ. This estimate is obtained by averaging Vedit (Equation 4) over LHQ distinct noise samples Nt: , t) = ENtN (0,I);LHQ samples[ Vedit(Z tar (5) where LHQ is the number of averaging operations used to obtain this high-quality estimate. VHQ serves as relatively reliable directional benchmark for the current step. , t, ctar, csrc)], VHQ(Z edit , src t Concurrently, we generate baseline edit velocity estimates, VBL,i (i = 1, . . . , K, where = (cid:1), denotes the number of combinations of LBL flows chosen from the LHQ different streams), (cid:0)LHQ LBL each employing fewer averaging operations, LBL (where LBL < LHQ). Crucially, these baseline estimates incur negligible additional computational overhead, as they are obtained by subsampling LBL velocities (or their underlying noise samples) from the set of LHQ samples already processed for VHQ: , src VBL,i(Z edit , t, ctar, csrc)]i. , t) = ENtN (0,I);LBL samples[ Vedit(Z tar (6) Each VBL,i represents lower-cost, albeit potentially noisier or more biased, editing direction. The crux of DAG lies in leveraging the discrepancies between VHQ and VBL,i to distill differential guidance signal that directs towards improved editing quality and stronger semantic alignment. By applying this differential guidance signal to adjust and refine the Ordinary Differential Equation (ODE)-driven editing trajectory, the model can pursue stronger semantic updates without requiring averaging over an excessive number of directions. Therefore, we compute differential signals Di: Di = VHQ VBL,i. Each Di indicates direction of improvement aimed at improving editing quality and enhancing semantic alignment. By averaging these differential signals, we obtain unified evolutionary trend vector, = 1 i=1 Di. This vector captures consensus direction, aggregated from multiple lower-fidelity pathways, indicating consistent trend towards the high-fidelity pathway. Finally, this averaged differential signal is used to guide (or augment) the high-quality estimate VHQ, yielding the final DAG-guided edit velocity, VDAG: (cid:80)K VDAG = VHQ + D, (7) 6 where is hyperparameter controlling the strength of the differential guidance. Intuitively, VHQ provides relatively robust direction for the edit. represents an aggregated improvement signal, capable of propelling the edit towards more profound semantic transformation. By incorporating into VHQ, DAG effectively enables the editing process to overcome the inertia imposed by structure preservation. This is achieved with minimal additional computational overhead, as the generation of the baseline estimates largely reuses computations performed for VHQ. The primary added cost stems from aggregating the differential signals. Figure 3: Qualitative results of FlowDirector. Our method showcases superior performance across wide range of video editing challenges. The results highlight high fidelity to prompts, preservation of unedited regions and motion, temporal coherence, and visual plausibility. Best viewed zoomed-in."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setups In the experiment, we adopt Wan 2.1 model [44] for FlowDirector to perform inversion-free video editing across 50 denoising steps. To balance structural consistency with editing freedom, the first 10 denoising steps are skipped. We implement Spatially Attentive Flow Correction (SAFC) by extracting effective masks from the attention maps of the 18th DiT block. Differential Averaging Guidance (DAG) is applied during denoising, where the higher-quality estimate is configured with 4 iterative inference steps, and baseline estimate is established by averaging two of these steps. For both qualitative and quantitative evaluation, we select videos from the internet and generated by the Wan model. All experiments for FlowDirector were run on single NVIDIA 4090 GPU unless otherwise specified. We compare FlowDirector with 5 baseline methods: FateZero [33], FLATTEN [6], TokenFlow [8], RAVE [17], and VideoDirector [48]. For all these baseline methods, we follow the default settings provided in their official GitHub repositories. More detailed experimental settings for our method are provided in the Appendix A. 7 Figure 4: Qualitative comparison. Our method outperforms previous methods across diverse editing tasks, demonstrating superior visual quality and temporal consistency. Best viewed zoomed-in. 4.2 Qualitative Results We perform qualitative evaluation of FlowDirector on diverse video content and under various editing prompts. Our method demonstrates superior performance across diverse range of video editing tasks (Figure 3) including object editing, texture transformation, local attribute modification, object addition/removal, and combinations thereof. For object editing, our method seamlessly converts the primary subject of video from one category to another (e.g., Fifth row, first column. Transforming sea turtle into dolphin). In texture editing, our approach can endow objects with entirely new materials (e.g., Third row, second column. Reimagining jeep in LEGO style). For local attribute modification, the model exhibits fine-grained control (e.g., Second row, first column. Altering attire from black dress to blue shirt and jeans). Furthermore, FlowDirector adeptly handles object addition and removal (e.g., Second row, first column. Adding red baseball cap to the woman; Fifth row, second column. Removing flower held by golden retriever). It also supports the composition of multiple distinct editing operations (e.g., Sixth row, second column. Simultaneously removing the flower and adding colorful dog leash for the golden retriever). More comprehensive experimental results are provided in Appendix H. The edits maintain high fidelity to textual prompts while preserving details and motion in unedited regions, and ensure temporal coherence and visual plausibility, producing high-quality results even in complex scenes and with substantial semantic changes. We also compare FlowDirector with several state-of-the-art video editing methods (Figure 4). The experimental results demonstrate that FlowDirector excels at high-quality video editing, significantly outperforming the previous methods. Further detailed comparison results are presented in Appendix Appendix G. 4.3 Quantitative Results We conduct quantitative comparison between FlowDirector and other state-of-the-art methods. Following prior work [33, 6, 17], we evaluate text-content alignment using CLIP-T, the average CLIP [34] embedding distance between the text prompt and video frames. Temporal coherence is 8 Table 1: Quantitative Comparison. We report Frame-Acc, Pick-Score, CLIP-T, CLIP-F, WarpSSIM, and Qedit metrics. For each metric, the results are presented from left to right, corresponding to 8-frame, 22-frame, and 36-frame videos, respectively. The \"-\" symbol denotes cases where method could not be evaluated for specific length, either due to excessive memory requirements or lack of support for that video length. Method FateZero [33] FLATTEN [6] TokenFlow [8] RAVE [17] VideoDirector [48] Frame-Acc (%) Pick Score (%) CLIP-T (102) CLIP-F (102) WarpSSIM (102) Qedit (105) 45.83 46.67 - 75.00 74.29 74.22 71.88 73.39 72.58 77.82 72.87 21.01 20.98 32.13 32.06 - 20.93 20.88 20.88 33.45 33.45 21.42 21.42 21.40 32.96 32.96 20.99 20.98 20.99 33.33 33.25 32.93 32.57 - 20.76 20.60 25.39 25.25 26.00 26.17 26.08 25.23 25.16 25.02 25.78 25.78 25.64 25.44 24.86 78.88 78.50 77.47 77.84 75.62 75.42 77.01 77.04 76.67 75.88 96.83 96.61 96.54 96.54 97.04 97.03 97.13 97.14 95.40 91.19 - 77.60 75.19 76.61 - - 73.96 72.83 72.67 - - 33.50 32.81 33.25 - - 96.20 96.85 96.91 - - FlowDirector (ours) 84.38 84.09 84. 21.83 21.82 21.80 33.72 33.79 33.84 98.37 98.07 97.85 77.21 77.14 77. 26.17 26.17 26.16 measured by CLIP-F, the average pairwise frame CLIP similarity. Structure preservation during editing is assessed via WarpSSIM, the average SSIM between the final edited video and the source video warped using RAFT [43] optical flow. Consistent with [6, 17], we utilize the composite metric Qedit (WarpSSIMCLIP-T) for holistic evaluation of editing performance. Furthermore, PickScore [22] evaluates overall perceptual quality and prompt alignment based on human preferences, and Frame-Acc measures the average percentage of frames more aligned with the target prompt than the source. Our experimental results  (Table 1)  demonstrate that our proposed method significantly outperforms current state-of-the-art techniques regarding both text alignment (CLIP-T) and temporal consistency (CLIP-F), and achieves strong performance on Pick-Score and Frame-Acc. Furthermore, our method achieves superior results on the comprehensive Qedit metric for all tested video lengths. Notably, our method does not achieve the highest WarpSSIM score. This is attributed to FlowDirectors capacity for enabling more substantial and pronounced object deformations. For more detailed explanation, please refer to the appendix C. Figure 5: Ablation study of Spatially Attentive Flow Correction. The Flow row displays editing flows from two consecutive late denoising steps. Left: with SAFC. Right: without SAFC. Figure 6: Ablation study of Differential Averaging Guidance. Best viewed zoomed-in. 4.4 Ablation Study We conducted extensive ablation studies to illustrate the effectiveness of Spatially Attentive Flow Correction (SAFC) and Differential Averaging Guidance (DAG). Experimental results (Figure 5) demonstrate that without SAFC, the direct editing flow also exerted guidance in non-edited regions, leading to significant changes in irrelevant areas of the edited video (the road and road signs highlighted by the orange box in Figure 5), thereby severely compromising the fidelity of the edited video. Conversely, with the application of SAFC, the direct editing flow successfully focused on the target region, eliminating its influence on non-edited regions and better preserving the original state and details of irrelevant areas such as the background. We also demonstrated the effectiveness of DAG. In the absence of DAG guidance, it is difficult to effectively overcome the inherent structural 9 preservation bias, leading to suboptimal editing results. This is manifested in editing results often exhibiting insufficient transformation of target features, with residual outlines of the original object (as indicated by the yellow box in Figure 6). With the introduction of DAG, even when facing challenging scenarios involving significant modifications, the model can more thoroughly achieve the transformation of target semantics, generating editing results that are more complete and more consistent with the text prompt. In Appendix D, we compare the editing results and required time of directly employing multi-round inference averaging strategy versus using DAG."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we propose FlowDirector, novel training-free video editing framework that operates without explicit inversion, instead modeling video transformation as continuous evolution in data space. Our method incorporates Spatially Attentive Flow Correction (SAFC), which utilizes explicit spatial masking to precisely preserve unedited regions by controlling the editing flow, and Differential Averaging Guidance (DAG), guidance-enhanced strategy leveraging differences between multiple flow estimates for stronger semantic alignment with the target prompt. Comprehensive experiments demonstrate that FlowDirector achieves state-of-the-art performance in text-driven video editing, particularly in terms of semantic accuracy, temporal coherence, and background preservation."
        },
        {
            "title": "References",
            "content": "[1] Haoran Bai, Di Kang, Haoxian Zhang, Jinshan Pan, and Linchao Bao. 2022. FFHQ-UV: Normalized Facial UV-Texture Dataset for 3D Face Reconstruction. arXiv preprint arXiv: 2211.13874 (2022). 2 [2] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. 2023. Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models. Cvpr (2023). 3 [3] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. 2023. InstructPix2Pix: Learning to Follow Image Editing Instructions. Cvpr (2023). 3 [4] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. 2021. Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts. Cvpr (2021). 2 [5] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, Chao Weng, and Ying Shan. 2023. VideoCrafter1: Open Diffusion Models for High-Quality Video Generation. doi:10.48550/ arXiv.2310.19512 arXiv:2310.19512 [cs] [6] Yuren Cong, Mengmeng Xu, Christian Simon, Shoufa Chen, Jiawei Ren, Yanping Xie, JuanManuel Perez-Rua, Bodo Rosenhahn, Tao Xiang, and Sen He. 2023. FLATTEN: Optical FLowguided ATTENtion for Consistent Text-to-Video Editing. https://arxiv.org/abs/2310.05922v3. 2, 3, 6, 7, 8, 9 [7] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. 2024. Scaling Rectified Flow Transformers for High-Resolution Image Synthesis. https://arxiv.org/abs/2403.03206v1. 3 [8] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. 2023. TokenFlow: Consistent Diffusion Features for Consistent Video Editing. https://arxiv.org/abs/2307.10373v3. 2, 3, 7, 9 [9] Yuchao Gu, Yipin Zhou, Bichen Wu, Licheng Yu, Jia-Wei Liu, Rui Zhao, Jay Zhangjie Wu, David Junhao Zhang, Mike Zheng Shou, and Kevin Tang. 2023. VideoSwap: Customized Video Subject Swapping with Interactive Semantic Point Correspondence. Computer Vision and Pattern Recognition (2023). doi:10.1109/CVPR52733.2024.00728 3 10 [10] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. 2024. AnimateDiff: Animate Your Personalized Text-toImage Diffusion Models without Specific Tuning. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. https://openreview.net/forum?id=Fx2SbBgcte [11] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohenor. 2023. Prompt-to-Prompt Image Editing with Cross-Attention Control. In The Eleventh International Conference on Learning Representations. https://openreview.net/forum? id=_CDixzkzeyb 3, 6 [12] Jonathan Ho, Ajay Jain, and P. Abbeel. 2020. Denoising Diffusion Probabilistic Models. Neural Information Processing Systems (2020). 2, 3 [13] Jonathan Ho and Tim Salimans. 2022. Classifier-Free Diffusion Guidance. arXiv preprint arXiv: 2207.12598 (2022). 3, 6 [14] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J. Fleet. 2022. Video Diffusion Models. Neural Information Processing Systems (2022). doi:10.48550/arXiv.2204.03458 [15] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. 2023. CogVideo: Largescale Pretraining for Text-to-Video Generation via Transformers. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. https://openreview.net/forum?id=rB6TpjAuSRy 3 [16] Hyeonho Jeong and Jong Chul Ye. 2023. Ground-A-Video: Zero-shot Grounded Video Editing Using Text-to-image Diffusion Models. https://arxiv.org/abs/2310.01107v2. 2 [17] Ozgur Kara, Bariscan Kurtkaya, Hidir Yesiltepe, James M. Rehg, and Pinar Yanardag. 2023. RAVE: Randomized Noise Shuffling for Fast and Consistent Video Editing with Diffusion Models. https://arxiv.org/abs/2312.04524v1. 2, 3, 7, 8, 9 [18] Tero Karras, Miika Aittala, Tuomas Kynkäänniemi, Jaakko Lehtinen, Timo Aila, and Samuli Laine. 2024. Guiding diffusion model with bad version of itself. Advances in Neural Information Processing Systems 37 (2024), 5299653021. 6 [19] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. 2023. Imagic: Text-Based Real Image Editing with Diffusion Models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023. IEEE, 60076017. doi:10.1109/CVPR52729.2023.00582 [20] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. 2023. Text2Video-Zero: Text-to-Image Diffusion Models Are Zero-Shot Video Generators. https://arxiv.org/abs/2303.13439v1. 2 [21] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. 2022. DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022. IEEE, 24162425. doi:10.1109/CVPR52688.2022.00246 3 [22] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. 2023. Pick-a-Pic: An Open Dataset of User Preferences for Text-to-Image Generation. Neural Information Processing Systems (2023). doi:10.48550/arXiv.2305.01569 9 [23] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, Kathrina Wu, Qin Lin, Junkun Yuan, Yanxin Long, Aladdin Wang, Andong Wang, Changlin Li, Duojun Huang, Fang Yang, Hao Tan, Hongmei Wang, Jacob Song, Jiawang Bai, Jianbing Wu, Jinbao Xue, Joey Wang, Kai Wang, Mengyang Liu, Pengyu Li, Shuai Li, Weiyan Wang, Wenqing Yu, Xinchi Deng, Yang Li, Yi Chen, Yutao Cui, Yuanbo Peng, Zhentao Yu, Zhiyu He, Zhiyong Xu, Zixiang Zhou, Zunnan Xu, Yangyu Tao, Qinglin Lu, Songtao Liu, Dax Zhou, Hongfa Wang, Yong Yang, Di Wang, Yuhong Liu, Jie Jiang, and Caesar Zhong. 2024. HunyuanVideo: Systematic Framework For Large Video Generative Models. arXiv preprint arXiv: 2412.03603 (2024). 3 11 [24] Vladimir Kulikov, Matan Kleiner, Inbar Huberman-Spiegelglas, and Tomer Michaeli. 2024. FlowEdit: Inversion-Free Text-Based Editing Using Pre-Trained Flow Models. arXiv preprint arXiv: 2412.08629 (2024). 2, 3, [25] Tsung-Yi Lin, M. Maire, Serge J. Belongie, James Hays, P. Perona, Deva Ramanan, Piotr Dollár, and C. L. Zitnick. 2014. Microsoft COCO: Common Objects in Context. European Conference on Computer Vision (2014). doi:10.1007/978-3-319-10602-1_48 2 [26] Y. Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. 2022. Flow Matching for Generative Modeling. International Conference on Learning Representations (2022). 2, 3, 4 [27] Chang Liu, Rui Li, Kaidong Zhang, Yunwei Lan, and Dong Liu. 2024. StableV2V: Stablizing Shape Consistency in Video-to-Video Editing. https://arxiv.org/abs/2411.11045v1. 2 [28] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya Jia. 2023. Video-P2P: Video Editing with Cross-attention Control. https://arxiv.org/abs/2303.04761v1. 2, [29] Xingchao Liu, Chengyue Gong, and Qiang Liu. 2023. Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. https://openreview.net/forum?id=XVjTT1nw5z 4, 5 [30] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. 2021. SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations. International Conference on Learning Representations (2021). 3 [31] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. 2023. Null-text Inversion for Editing Real Images using Guided Diffusion Models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023. IEEE, 60386047. doi:10.1109/CVPR52729.2023.00585 2, 3 [32] Dustin Podell, Zion English, Kyle Lacey, A. Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. 2023. SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis. International Conference on Learning Representations (2023). 2, 3 [33] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen. 2023. FateZero: Fusing Attentions for Zero-shot Text-based Video Editing. https://arxiv.org/abs/2303.09535v3. 2, 3, 6, 7, 8, 9 [34] Alec Radford, Jong Wook Kim, Chris Hallacy, A. Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and I. Sutskever. 2021. Learning Transferable Visual Models From Natural Language Supervision. International Conference on Machine Learning (2021). [35] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick Esser, and B. Ommer. 2021. HighResolution Image Synthesis with Latent Diffusion Models. Computer Vision and Pattern Recognition (2021). doi:10.1109/CVPR52688.2022.01042 2, 3 [36] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. 2015. Imagenet large scale visual recognition challenge. International journal of computer vision 115 (2015), 211252. 2 [37] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. 2022. Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding. doi:10.48550/arXiv. 2205.11487 arXiv:2205.11487 [cs] 3 [38] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. 2022. LAION-5B: An open large-scale dataset for training next generation 12 image-text models. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (Eds.). http://papers.nips.cc/paper_files/paper/2022/hash/ a1859debfb3b59d094f3504d5ebb6c25-Abstract-Datasets_and_Benchmarks.html [39] Chaehun Shin, Heeseung Kim, Che Hyun Lee, Sang-gil Lee, and Sungroh Yoon. Single Video Editing with Object-Aware Consistency. 2023. Edit-A-Video: https://arxiv.org/abs/2303.07945v4. 2 [40] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. 2023. Make-A-Video: Text-to-Video Generation without Text-Video Data. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. https://openreview.net/forum?id=nJfylDvgzlq 3 [41] Jiaming Song, Chenlin Meng, and Stefano Ermon. 2021. Denoising Diffusion Implicit Models. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. https://openreview.net/forum?id=St1giarCHLP 3 [42] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. 2021. Score-Based Generative Modeling through Stochastic Differential Equations. In International Conference on Learning Representations. https://openreview.net/ forum?id=PxTIG12RRHS 2, 3 [43] Zachary Teed and Jia Deng. 2020. RAFT: Recurrent All-Pairs Field Transforms for Optical Flow. European Conference on Computer Vision (2020). doi:10.1007/978-3-030-58536-5_24 [44] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. 2025. Wan: Open and Advanced Large-Scale Video Generative Models. arXiv preprint arXiv: 2503.20314 (2025). 3, 7 [45] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. 2023. ModelScope Text-to-Video Technical Report. doi:10.48550/arXiv.2308.06571 arXiv:2308.06571 [cs] [46] Weimin Wang, Jiawei Liu, Zhijie Lin, Jiangqiao Yan, Shuo Chen, Chetwin Low, Tuyen Hoang, Jie Wu, Jun Hao Liew, Hanshu Yan, Daquan Zhou, and Jiashi Feng. 2024. MagicVideoV2: Multi-Stage High-Aesthetic Video Generation. doi:10.48550/arXiv.2401.04468 arXiv:2401.04468 [cs] 3 [47] Yuanzhi Wang, Yong Li, Mengyi Liu, Xiaoya Zhang, Xin Liu, Zhen Cui, and Antoni B. Chan. 2024. Re-Attentional Controllable Video Diffusion Editing. https://arxiv.org/abs/2412.11710v1. 2 [48] Yukun Wang, Longguang Wang, Zhiyuan Ma, Qibin Hu, Kai Xu, and Yulan Guo. 2024. VideoDirector: Precise Video Editing via Text-to-Video Models. https://arxiv.org/abs/2411.17592v2. 2, 4, 6, 7, 9 [49] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. 2022. Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation. https://arxiv.org/abs/2212.11565v2. 2, [50] Zongze Wu, Nicholas Kolkin, Jonathan Brandt, Richard Zhang, and Eli Shechtman. 2024. TurboEdit: Instant text-based image editing. European Conference on Computer Vision (2024). doi:10.48550/arXiv.2408.08332 3 13 [51] Sihan Xu, Yidong Huang, Jiayi Pan, Ziqiao Ma, and Joyce Chai. 2023. Inversion-Free Image Editing with Natural Language. arXiv preprint arXiv: 2312.04965 (2023). 2, 3 [52] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. 2015. LSUN: Construction of Large-scale Image Dataset using Deep Learning with Humans in the Loop. arXiv preprint arXiv: 1506.03365 (2015). 2 [53] Shuheng Zhang, Yuqi Liu, Hongbo Zhou, Jun Peng, Yiyi Zhou, Xiaoshuai Sun, and Rongrong Ji. 2025. AdaFlow: Efficient Long Video Editing via Adaptive Attention Slimming And Keyframe Selection. https://arxiv.org/abs/2502.05433v1."
        },
        {
            "title": "A Detailed Experimental Settings",
            "content": "In our experimental setup, we utilized source videos standardized to resolution of 832x480 pixels. We employed the Wan 2.1 t2v-1.3B model, released via the official Huggingface repository, as the foundational Text-to-Video (T2V) generation model. To achieve balance between maintaining structural consistency and affording sufficient editing flexibility, the sampling process involved 50 steps, omitting the first 10 steps. In the mask generation phase, spatial smoothing window size of 11 was employed, and an edge softening (feathering) decay factor of 0.25 was applied. For the sampling phase guidance parameters, the Classifier-Free Guidance (CFG) scale for the source video was configured to 3.5, the Classifier-Free Guidance (CFG) scale for the target video was set to 10.5. The guidance strength coefficient ω for Differential Average Guidance (DAG) was set to 2.75."
        },
        {
            "title": "B Mask Generation Method",
            "content": "To achieve spatially selective editing and precisely constrain the editing flow Vedit, we devise mask generation strategy rooted in the cross-attention mechanism. This strategy first extracts crossattention maps, Asrc and Atar, from specific layers of the Text-to-Video (T2V) model, corresponding to the source editing prompt csrc and the target editing prompt ctar, respectively. These attention maps are then spatially smoothed to enhance regional continuity and robustness. Subsequently, an adaptive binarization method is employed, using the global average activation value of each smoothed attention map as dynamic threshold, to generate initial binary masks Msrc and Mtar. Finally, an element-wise union operation is performed on these two masks (implemented as logical OR in code) to obtain the combined editing region mask Msum. To prevent abrupt transitions or noticeable stratification at editing boundaries, an optional decreasing softening process can be applied to this combined mask. This is achieved by basing it on the Euclidean distance of pixels to the nearest mask boundary and employing an exponential decay function to create smooth transitions. This final generated mask is then used to spatially modulate the editing flow Vedit, thereby focusing the editing effects on semantically relevant core regions while effectively preserving the content and structure of non-target areas. The pseudocode for this mask generation strategy is provided in Algorithm 1."
        },
        {
            "title": "C Comparison of WarpSSIM indicators",
            "content": "Table 2: Quantitative Comparison of WarpError Metrics for the Porscar Car Example. Method FateZero FLATTEN TokenFlow RAVE VideoDirector FlowDirector (ours) WarpL1 (103) WarpL2 (105) WarpSSIM (102) Pick Score (%) 8-frames 22-frames 36-frames 8-frames 22-frames 36-frames 8-frames 22-frames 36-frames 8-frames 22-frames 36-frames 44.11 44.43 49.23 49.83 54.64 55.53 46.94 47.77 51.30 52.86 54. 61.29 - 50.97 53.53 57.30 - 63.56 8.81 8.88 9.01 10.24 10.90 10.09 9.18 9.52 9.48 10.72 10. 11.05 - 10.10 9.93 11.58 - 11.52 72.64 68.14 60.75 70.74 65.20 57.69 72.12 67.50 61.00 70.09 65. 55.92 - 67.10 60.82 68.97 - 55.67 19.64 20.18 21.84 21.70 19.83 23.32 19.43 20.06 21.82 21.64 19. 22.95 - 19.97 21.66 21.59 - 22.80 Our method does not achieve the highest WarpSSIM score. For the Porscar Car example illustrated in Figure 4, Table 2 presents quantitative comparison of warp fidelity metrics (WarpL1, WarpL2, WarpSSIM). As the table indicates, while our method yields higher WarpL1 and WarpL2 scores (indicating greater pixel-level reconstruction discrepancies) and the lowest WarpSSIM score (where higher values are preferable), it notably achieves the top Pick-Score, metric more aligned with human perceptual judgment. Considering the direct visual comparisons in Figure 4, we posit that this outcome is primarily attributable to FlowDirectors capacity to facilitate more substantial and visually pronounced object deformations. This advanced editing capability, while key advantage of our approach, results in significant spatio-temporal structural alterations, which in turn penalize its performance under the WarpSSIM evaluation framework (reliant on optical flow warping). Therefore, this finding not Algorithm 1 Generation of Spatially Selective Editing Mask (Mf inal) 1: procedure GENERATEEDITINGMASK(Asrc, Atar, n, δ, apply_softening) Require: Asrc, Atar: Source and Target attention maps (Shape: [C, T, Hp, Wp]). n: Kernel size for spatial smoothing (integer, e.g., 11). δ: Decay factor for edge softening (float, δ > 0, e.g., 0.1). apply_softening: Boolean flag to enable/disable edge softening. Apply average pooling Asrc SpatialSmooth(Asrc, n, average) Atar SpatialSmooth(Atar, n, average) τsrc GlobalAverage( Asrc) Msrc ( Asrc τsrc) τtar GlobalAverage( Atar) Mtar ( Atar τtar) Mcombined Msrc Mtar if apply_softening then Ensure: Mf inal: Final editing mask (Shape: [C, T, Hp, Wp], values in [0, 1]). 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: Mf inal SoftenMaskEdges(Mcombined, δ) Mf inal CastToFloat(Mcombined) else Element-wise logical OR operation Apply feathering Use binary mask directly Threshold is the global mean activation Element-wise comparison yields binary mask (0/1) return Mf inal 13: 14: procedure SOFTENMASKEDGES(Mbinary, δ) Require: Mbinary: Input binary mask (Shape: [C, T, Hp, Wp], values 0 or 1). δ: Decay factor (positive float). Ensure: Msof t: Mask with softened edges (Shape: [C, T, Hp, Wp], values in [0, 1]). 15: 16: Minverted Mbinary ComputeDistanceTransform(Minverted) background pixel to the nearest foreground pixel. Calculate Euclidean distance from each 17: 18: 19: Vsof exp(δ D) = 0. Msof where(Mbinary = 1, 1.0, Vsof t) Mbinary[p] = 1, else Msof t[p] = Vsof t[p] return Msof Element-wise calculation. = 0 = = 1. Equivalent to: Msof t[p] = 1.0 if only underscores the efficacy of our method in handling extensive deformations but also reveals the inherent limitations and potential biases of traditional WarpError metrics. Such pixel-based or local structure-centric metrics are less suited for evaluating advanced video editing tasks that necessitate drastic deformations and prioritize global semantic coherence over strict pixel-wise fidelity."
        },
        {
            "title": "D Ablation Study of Differential Averaging Guidance",
            "content": "A conventional averaging method involves using multiple rounds of iterative inference to obtain several flows, each indicating different editing directions. These flows are then averaged to derive more robust, consolidated direction, which is subsequently used to update our video features. In this section, we compare our Differential Averaging Guidance (DAG) approach with the aforementioned conventional averaging strategy to demonstrate the effectiveness and efficiency of our DAG method. As illustrated in Figure 7, when editing the source video (e.g., bear) into the target video (e.g., dinosaur), the regions experiencing significant editing changes are primarily concentrated on the bears back. When no specific strategy is employed, noticeable artifacts appear in the area corresponding to the original bears back (e.g., the second row, Figure 7). These artifacts stem from the inherent structure-preserving tendency of direct editing. When the conventional averaging strategy is applied, using the results from twenty rounds of iterative inference (Figure 7, third column), these artifacts are largely eliminated. However, this comes at significant computational cost (editing 41-frame video requires 29 minutes on four 4090 GPUs).. In contrast, our DAG approach requires only four rounds of iterative inference to achieve high-quality estimation (editing 41-frame video requires 5 minutes on four 4090 GPUs). This estimation is 16 Figure 7: Qualitative comparison between the editing results of multi-round inference averaging strategy and using DAG. The Sample Average strategy is set to use regular averaging strategy for 20 rounds of iterative inference at every denoising step to obtain the editing flow. The DAG setting uses 4 rounds of iterative inference to obtain high-quality estimate and perform reinforcement-guided generation of the editing flow. used to generate differential signal that guides and reinforces our editing flow, producing comparable or even superior results. As shown in Figure 7 (fourth row), compared to the twenty-round averaging result, our method more effectively eliminates artifacts, resolves issues of incomplete editing, and the resulting dinosaur exhibits morphology more distinct from the original bear. This provides strong evidence for the efficiency and effectiveness of our proposed method."
        },
        {
            "title": "E Broader Impacts",
            "content": "Our work has made significant progress in the field of text-driven video editing, achieving high-quality video editing results with strong alignment to text instructions. Users can perform high-quality edits on existing videos using straightforward textual descriptions, thereby significantly reducing the cost and complexity of video editing. However, like all powerful generative artificial intelligence technologies, high-quality and easily controllable video editing tools also carry the risk of misuse. The most significant concern is their potential use for creating and disseminating disinformation or deepfake content. These fabricated videos could be employed for malicious purposes, such as damaging personal reputations, committing fraud, or manipulating public opinion. Furthermore, generative models themselves may exhibit biases, and the editing method could inadvertently amplify or perpetuate these biases. Therefore, it is imperative for the research community, developers, and policymakers to strengthen ethical review and regulation of AI-generated content, and to refine relevant laws and regulations, ensuring the legitimate use of such methods."
        },
        {
            "title": "F Limitation",
            "content": "Our method aims to construct direct editing path from the source video to the target video, bypassing the inversion process, which is prone to structural loss. Since the primary driving force for this direct editing path stems from the discrepancy between the source and target texts, varying degrees of textual difference can lead to markedly different editing outcomes. This results in incomplete 17 Figure 8: An example of editing failure due to incomplete target text replacement. When attempting to edit bear into dinosaur, if the target prompt erroneously retains descriptions of the bear (e.g., ...capturing the bears deliberate movements instead of full replacement with dinosaur-related descriptions), the edited video exhibits significant residual features of the original bear. text replacement causing substantial remnants of the original video content (Figure 8). For example, modifying the source prompt csrc (i.e., large brown bear is walking slowly across rocky terrain in zoo enclosure, surrounded by stone walls and scattered greenery. The camera remains fixed, capturing the bears deliberate movements.) to an incompletely substituted target prompt ctar (i.e., large dinosaur is walking slowly across rocky terrain in zoo enclosure, surrounded by stone walls and scattered greenery. The camera remains fixed, capturing the bears deliberate movements.) leads to significant residual bear information in the edited video. Furthermore, we observe that the quality of the source text csrc also substantially affects the editing results; more comprehensive source texts tend to yield better editing outcomes compared to simpler prompts. Similarly, our method excels in structure preservation, which is evident in tasks such as significant object editing, texture replacement, object addition/deletion, or compositional tasks. However, its performance in video style transfer is relatively limited. We attribute this to combination of its tendency towards result preservation and being less driven by textual differences."
        },
        {
            "title": "G ADDITIONAL QUALITATIVE RESULTS",
            "content": "We provide comprehensive additional qualitative comparisons to rigorously evaluate the performance of our FlowDirector method against several advanced text-to-video editing approaches. As illustrated in the figures within this section, including Figures 9 to 11, these side-by-side results visually demonstrate the significant advantages of FlowDirector. They showcase our methods capability to perform precise and semantically faithful edits with exceptional visual fidelity, while crucially preserving the original videos spatial content and motion dynamics in unedited regions. Furthermore, the comparisons highlight FlowDirectors superior ability to maintain temporal consistency and coherence across frames, effectively handling challenging motion dynamics often problematic for other methods. These comparative examples visually validate FlowDirectors effectiveness and its superiority in achieving high-quality, controllable, and temporally stable text-based video edits."
        },
        {
            "title": "H More Qualitative Results",
            "content": "In this section, we present additional qualitative results to further demonstrate the effectiveness and high quality of our video editing method. Figures 12 to 16 provide further examples of our method performing precise and semantically faithful edits while preserving the spatial content and motion 18 Figure 9: Qualitative comparison between advanced text-to-video editing approaches and FlowDirector. Best viewed zoomed-in. dynamics of unedited regions. These results consistently exhibit strong alignment with the editing instructions, high visual fidelity, and consistent temporal coherence across frames. 19 Figure 10: Qualitative comparison between advanced text-to-video editing approaches and FlowDirector. Best viewed zoomed-in. 20 Figure 11: Qualitative comparison between advanced text-to-video editing approaches and FlowDirector. Best viewed zoomed-in. 21 Figure 12: More Qualitative Results. Our method performs precise and semantically faithful edits while preserving the spatial content and motion dynamics of unedited regions. The results exhibit strong alignment with the editing instructions, high visual fidelity, and consistent temporal coherence across frames. Best viewed zoomed-in. 22 Figure 13: More Qualitative Results. Our method performs precise and semantically faithful edits while preserving the spatial content and motion dynamics of unedited regions. The results exhibit strong alignment with the editing instructions, high visual fidelity, and consistent temporal coherence across frames. Best viewed zoomed-in. 23 Figure 14: More Qualitative Results. Our method performs precise and semantically faithful edits while preserving the spatial content and motion dynamics of unedited regions. The results exhibit strong alignment with the editing instructions, high visual fidelity, and consistent temporal coherence across frames. Best viewed zoomed-in. 24 Figure 15: More Qualitative Results. Our method performs precise and semantically faithful edits while preserving the spatial content and motion dynamics of unedited regions. The results exhibit strong alignment with the editing instructions, high visual fidelity, and consistent temporal coherence across frames. Best viewed zoomed-in. 25 Figure 16: More Qualitative Results. Our method performs precise and semantically faithful edits while preserving the spatial content and motion dynamics of unedited regions. The results exhibit strong alignment with the editing instructions, high visual fidelity, and consistent temporal coherence across frames. Best viewed zoomed-in."
        }
    ],
    "affiliations": [
        "AGI Lab, Westlake University",
        "Central South University"
    ]
}