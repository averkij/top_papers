{
    "paper_title": "REPAIR: Robust Editing via Progressive Adaptive Intervention and Reintegration",
    "authors": [
        "Yisu Wang",
        "Ming Wang",
        "Haoyuan Song",
        "Wenjie Huang",
        "Chaozheng Wang",
        "Yi Xie",
        "Xuming Ran"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Post-training for large language models (LLMs) is constrained by the high cost of acquiring new knowledge or correcting errors and by the unintended side effects that frequently arise from retraining. To address these issues, we introduce REPAIR (Robust Editing via Progressive Adaptive Intervention and Reintegration), a lifelong editing framework designed to support precise and low-cost model updates while preserving non-target knowledge. REPAIR mitigates the instability and conflicts of large-scale sequential edits through a closed-loop feedback mechanism coupled with dynamic memory management. Furthermore, by incorporating frequent knowledge fusion and enforcing strong locality guards, REPAIR effectively addresses the shortcomings of traditional distribution-agnostic approaches that often overlook unintended ripple effects. Our experiments demonstrate that REPAIR boosts editing accuracy by 10%-30% across multiple model families and significantly reduces knowledge forgetting. This work introduces a robust framework for developing reliable, scalable, and continually evolving LLMs."
        },
        {
            "title": "Start",
            "content": "REPAIR: Robust Editing via Progressive Adaptive Intervention and Reintegration Yisu Wang, Ming Wang, Haoyuan Song, Wenjie Huang, Chaozheng Wang, Yi Xie & Xuming Ran ContiAI Research xuming@contiai.com.sg Abstract Post-training for large language models (LLMs) is constrained by the high cost of acquiring new knowledge or correcting errors and by the unintended side effects that frequently arise from retraining. To address these issues, we introduce REPAIR (Robust Editing via Progressive Adaptive Intervension and Reintegration), lifelong editing framework designed to support precise and low-cost model updates while preserving non-target knowledge. REPAIR mitigates the instability and conflicts of large-scale sequential edits through closed-loop feedback mechanism coupled with dynamic memory management. Furthermore, by incorporating frequent knowledge fusion and enforcing strong locality guards, REPAIR effectively addresses the shortcomings of traditional distribution-agnostic approaches that often overlook unintended ripple effects. Our experiments demonstrate that REPAIR boosts editing accuracy by 10%-30% across multiple model families and significantly reduces knowledge forgetting. This work introduces robust framework for developing reliable, scalable, and continually evolving LLMs."
        },
        {
            "title": "1 Introduction",
            "content": "Large language models (LLMs) have demonstrated remarkable capabilities across diverse tasks. However, their inherent rigidity prevents them from autonomously updating knowledge after pretraining, rendering them unable to correct errors (e.g., hallucinations or outdated facts) or integrate new information. Consequently, lifelong model editing has emerged as critical research paradigm. It aims to enable continuous, efficient, and low-cost local updates that ensure models remain accurate and relevant over time [1]. In contrast to full re-training or broad fine-tuning, editing focuses on precisely fine-grained modifications that preserve unrelated competencies while delivering immediate corrections at deployment time. Figure 1: Problems and our solutions. REPAIR achieves closedloop feedback, fine-grained knowledge integration, weighted knowledge merging, and robust editing performance. 1 5 2 0 2 2 ] . [ 1 9 7 8 1 0 . 0 1 5 2 : r a"
        },
        {
            "title": "ContiAI",
            "content": "Despite steady progress, important gaps remain as shown in Figure 1. (1) Large-scale sequential editing & coarse knowledge fusion. As edits accumulate, models can exhibit routing instability, conflicts among edits, and even collapse. Thus, stabilizing sequential updates without broad side effects remains challenging [2, 3]. Semi-parametric designs (e.g., SERAC [4]) and discrete keyvalue adaptors (e.g., GRACE [5]) alleviate some failure modes and support long edit streams, but still face scope and auditing trade-offs [4, 5]. The strategy for knowledge fusion remains underexplored, despite being the stage most prone to information loss[6]. (2) Few-shot editing. Under data-scarce conditions, editors often struggle to form robust, generalizable changes beyond the exact prompt, motivating gradient-transformation editors trained for locality (e.g., MEND [7]) and broader taxonomies of edit generalization [1, 7]. (3) Open-loop and distribution-agnostic learning. Many pipelines operate without reflective feedback, optimize on indiscriminate batches, and under-stress-test ripple effects on related knowledge and reasoning, calling for tighter evaluation and integration mechanisms [1, 3]. Overall, these issues reveal fundamental trade-off among reliability, specificity, and scalability that any practical editing system must reconcile. To address these challenges, we propose the framework named REPAIR (Robust Editing via Progressive Adaptive Intervension and Reintegration), with targeted strategies: (1) Closed-loop feedback with dynamic memory management that monitors edit performance and selectively re-initializes underperforming modules to stabilize routing and consolidation at scale. Concretely, our controller triggers health checks after each edit window and performs scoped resets or compaction when drift is detected. (2) Distribution-aware optimization that reorganizes samples by similarity and applies inner-batch distillation to enhance consistency and robustness in few-shot settings, encouraging edits to generalize across paraphrases and nearby contexts rather than overfitting to single prompts. (3) Frequent knowledge fusion that increases fusion cadence to prevent information loss and ensure timely consolidation of new and existing knowledge, with guardrails that validate locality before integration to avoid unintended side effects. We compare REPAIR with several foundational model editing methods across three dimensions: Memory, Attributes, and Behaviors  (Table 1)  . Its core innovation lies in integrating dual memory system with parametric editing, complemented by error feedback, inner-batch knowledge distillation, and loss-aware subspaces merging. This design achieves high success rates and broad editing coverage while minimizing side effects. In contrast, previous methods struggle with knowledge overlap and loss, particularly in sequential editing, where large differences between adjacent samples hinder effective correction. Table 2 showcases cases where REPAIR outperforms baselines, offering better balance of Reliability, Generalization, and Locality. Table 1: Comparison of current model editing methods. refers to yes and well-supported, refers to no or badly-supported, and refers to less-supported. The three metrics of Reliability, Generalization, and Locality denote the performance on lifelong editing. Memory Attributes Behaviors Methods Long-term Memory Working Memory Parametric Lifelong Reliability Generalization Locality Error Feedback Knowledge Distillation FT-EWC [8] ROME [9] MEMIT [10] MEND [7] DEFER [4] GRACE [5] WISE [6] REPAIR In summary, the main contributions are as follows."
        },
        {
            "title": "ContiAI",
            "content": "Table 2: Failure cases study.Previous baselines([6][5])often encounter issues of repeating answers from previous questions and difficulty in correcting adjacent knowledge during editing. MethodPrompt Edit Target Post-Edit Output Metrics a) The genus Platypatrobus is part of the family? b) The genus Platypatrobus is part of what family c) The genus Platypatrobus is part of the family? Arctiinae - - Arctiuc Yemen Arctiinae c) When was the IAAF Combined Events Challenge launched? d) When does season 5 of ruby come out? e)when does season 5 of ruby come out? 2006 October 14, 2017 - Armand 2006 2017 Reliability Generalization Reliability Locality Figure 2: The overall structure of REPAIR. An edit, such as changing the capital of France from \"Lyon\" to \"Paris,\" is stored as parameter update, θ, in the Side Memory. An Error Sample Monitor evaluates the performance of each edit (Oute ). If the error rate, Errthresh, for an edit on new sample exceeds threshold ϵ, the Side Memory Pruning module removes the erroneous update. The system then reintegrates new and error-prone samples for continuous learning. We identify three critical challenges in model editing: (1) instability under large-scale sequential edits, (2) limited generalization in few-shot scenarios, and (3) inefficiency in open-loop, distribution-agnostic pipelines. We propose REPAIR, novel framework to address these challenges by integrating dual-memory system with parametric editing. It introduces closed-loop error feedback, distribution-aware optimization, and loss-aware subspaces merging to ensure robust and precise updates. We validate the performance of REPAIR across diverse models (including LLaMA-3, Qwen-2.5, DeepSeek-R1-1.5B, and GPT-2-XL), demonstrating 15%20% improvement in overall editing performance over state-of-the-art methods and showing consistent, robust generalization."
        },
        {
            "title": "2 Methodology",
            "content": "We propose novel closed-loop lifelong model editing framework, denoted as REPAIR, which addresses the limitations of open-loop editing in distributed side-memory methods. Our framework, as shown in Figure 2, integrates (1) closed-loop error feedback with dynamic memory management; (2) distribution-aware batch reassembly with inner-batch knowledge distillation; (3) loss-aware weighted knowledge merging."
        },
        {
            "title": "ContiAI",
            "content": "Definition 2.1 (Lifelong Model Editing). Given pre-trained model fθ0(yx), sequential edit stream {Et}T i=1, and auxiliary distributions G(x) (paraphrased inputs) and (unrelated contexts), the objective is to obtain updated parameters θT that optimize the multi-objective trade-off: t=1 where Et = {(x(t) )}N , y(t) θt = arg min θ α 1 (cid:124) (cid:88) (cid:16) ℓ fθ(x(t) ), y(t) i=1 (cid:123)(cid:122) reliability +β (cid:17) (cid:125) (cid:88) i=1 1 (cid:124) xG(x(t) ) (cid:16) (cid:104) ℓ fθ(x), y(t) (cid:17)(cid:105) (cid:123)(cid:122) generalization (cid:125) + γ ExU (cid:124) (cid:2)KL (cid:0)fθt1(x) fθ(x)(cid:1)(cid:3) (cid:125) (cid:123)(cid:122) locality + R(θ, θt1) (cid:123)(cid:122) (cid:125) stability (cid:124) (1) where (α, β, γ) are hyperparameters controlling the reliability-generalization-locality-stability trade-off, and denotes regularization term enforcing parameter smoothness across sequential edits."
        },
        {
            "title": "2.2 Dual Memory Mechanism and Routing",
            "content": "As shown in Figure 2, block 1: For dual memory-based editing methods, the dual memory mechanism is typically deployed in the deep layers of the network. Specifically, for the value matrix Wv of the target FFN layer, here create copy as the side memory pool Ms, i.e.: (0) = Wv If the side memory pool is activated, the output is computed as: os = ϕ(f Wk) Ms, where ϕ denotes the non-linear activation function, and os represents the FFN output based on the side memory([6]). During the inference phase, for moemory pool i, the activation score is defined as (i) act(x) = A(x) (W v,i Wv)2. (2) where A() = is the activation of the side memorys corresponding FFN layer. Routing selects the pool with the activation score. If maxi (i) act(x) ε, the main memory Wv is used. Otherwise, the side memory pool Ms is selected. To enforce discriminative routing, we use margin-based loss. The objective of the routing mechanism is to establish clear decision boundary: min xeE R(xe) min xU R(x) > τ > max xiG R(xi) (3) where τ is preset threshold, and and Gi represent the edit and edit-irrelevant datasets, respectively. This selective activation mechanism ensures that edited knowledge is only retrieved in relevant contexts, thereby minimizing interference with the original models performance."
        },
        {
            "title": "2.3 Distribution-Aware inner-batch Knowledge Distillation",
            "content": "As shown in Figure 2 block 2: sample batch = {x1, x2, . . . , xn}, and denote the corresponding feature representations by oi = Norm(fθ(xi), = 1, . . . , n,). To improve the consistency and stability of model updates during sequential edits, we organized samples into homogeneous batches and performed intrabatch knowledge distillation. Samples with high mutual similarity are grouped into batch = {x(0), x(1), . . . , x(b1)}. Within each batch, the first sample x(0) acts as teacher, while the remaining 4 samples are students. We define the inner-batch knowledge distillation loss as LKD = λ Lcosine + θ Lvariance (4)"
        },
        {
            "title": "ContiAI",
            "content": "oio0 where Lcosine = 1 oio0 i=1 oi omean2. Minimizing Lkd encourages all samples in the batch to share similar knowledge, which in turn reduces potential conflicts when updating the same network parameters θ. The regularization term is used to maintain diversity among features, preventing excessive uniformity. and Lvariance = 1 (cid:80)N If certain samples cannot be well-aligned with the batch (i.e., their Lkd remains high after optimization), this indicates that they do not belong to the same distribution cluster and are unlikely to be effectively edited together. Such samples are removed from the batch and reclustered with other samples to form new homogeneous groups. Formally, the final batch reassembly can be expressed as = Recluster(cid:0){x Lkd(x, B) < ϵ}(cid:1), (5) where ϵ is threshold controlling inner-batch consistency. This procedure ensures that sequential parameter edits are performed on groups of samples with aligned knowledge, improving both stability and effectiveness of the model update. The convergence proof is provided in the Appendix 4 and Appendix 2."
        },
        {
            "title": "2.4 Closed-Loop Error Feedback and Memory Pruning",
            "content": "As shown in Figure 2 block 4: After each editing cycle, we evaluate the performance in feedback pool of error response samples by comparing to the correctness threshold τcorrect. For each shard i, we define the pool for each side memory pool, defined as the error set Ei = {x i(x) = i} and compute the error rate proportion of failed edits within the corresponding sample set: = {xEia(x)τcorrect} pool Ei When the pruning conditions are met (ri > τprune or > τE), we execute the following procedure: 1. Memory pool screening & pruning: Identify the side memory pool with the highest error rate pool = arg maxi .Remove the identified memory pool from the system. 2. Sample Reintegration & retraining: Recombine the remaining error samples to form new training set Eretrain. Retrain the new side memory pools using Eretrain. This closed-loop feedback mechanism enables the system to dynamically identify and eliminate underperforming memory units while optimizing the overall editing performance through sample reorganization and iterative retraining. The time-convergence proof is provided in the Appendix 2."
        },
        {
            "title": "2.5 Merging with Weighted TIES",
            "content": "As shown in Figure 2 block 3: After multiple updates, shards {W merge them with the weighted TIES [11] operator based on : v,i} produce deltas τi = v,i Wv. We Wv + ωi TIES (cid:0){τi}k i=1; Wv (cid:1). The total loss integrates all components: Ltotal = Ledit + λaLa + λKDLKD. (6)"
        },
        {
            "title": "ContiAI",
            "content": "Ledit is the autoregressive cross-entropy. Ledit(W we use margin-based loss: v) = log PW (y x). To enforce discriminative routing, (cid:26) La = min max(0, act(xi) γ1) + max(0, γ2 act(xe)) + max(0, γ (act(xe) act(xi))) (cid:27) (7) For shard i, consider subspaces {θ1, . . . , θk}, each trained on subset of samples Ei. Let the average ℓ(f (x; θi), y), where ℓ() is the task loss. We define the training loss of subspaces θi be: Li = 1 Ei merging weight of each subspaces as wi = exp(αLi) , with α > 0 controlling sensitivity to the loss. The global network parameters are then obtained via weighted averaging: θ = (cid:80)M i=1 wi θi. This loss-aware merging favors subspaces that achieve lower training loss on their corresponding samples, promoting reliable knowledge integration. j=1 exp(αLj ) (x,y)Ei (cid:80)M (cid:80)"
        },
        {
            "title": "3 Experiments",
            "content": "In the experimental section, we design six evaluations to answer the following questions: Q1, do the three key innovations (closed-loop feedback, discriminative pruning, and distribution reintegration) improve edit accuracy, generalization, and locality? Q2, does the method generalize well to knowledge-intensive tasks such as question answering and hallucination mitigation? Q3, is the method effective across different parameter scales and diverse architectures, including recent open-source models? Q4, under distribution shift (e.g., on the Wikibig Edit dataset), does the method remain robust and outperform existing methods? Q5, can the method maintain long-term stability and reliability in large-scale sequential editing scenarios? Q6, what are the contributions and sensitivities of each component and hyperparameter to overall performance?"
        },
        {
            "title": "3.1 Experimental Setup",
            "content": "Datasets and Models. Autoregressive LLMs are ideal for evaluating model editing due to their unidirectional causal structure, which allows predictable and traceable edits. This ensures clear interpretability of edit generalization and locality. We evaluate widely used models (LLaMA-3-8B, GPT2-XL) and recent models (Qwen2.5-7B, DeepSeek-R1-1.5B), using datasets such as ZsRE for closed-book QA, Wikibig Edit for editing performance, and hallucination dataset to assess generalization. For more details, refer to the Appendix 5. Baselines. Direct Parameter Editors: Directly modify model weights (e.g., ROME [2], MEMIT [10], MEMITmass [10]). Hypernetwork-Based Editors: Use an auxiliary network to generate parameter updates at inference (e.g., MEND [7]). External Memory-Based Editors: Leave the model unchanged and store edits in external memory, retrieved via routing mechanism (e.g., SERAC [4], GRACE [5], WISE [1]). Implementation Details. experiments were conducted simultaneously using two GPUs: an A100 PCIe 80GB 6 and an A100 SXM4 40GB. The code was implemented based on PyTorch 2.1, with modifications built upon the original EasyEditor framework. The specific hyperparameter settings are detailed in Appendix C."
        },
        {
            "title": "ContiAI",
            "content": "Evaluation Metrics Each edited corpus instance comprises three components: the descriptor ke used to to verify locality and rephrase prompt kloc to evaluate perform the edit, an irrelevant prompt-answer pair generalization performance across different expressions. To comprehensively evaluate the optimization capability of the proposed method in addressing the continual learning trilemma, we employ four metricsedit ), accuracy: Rel = 1 ), rephrase accuracy : Gen = 1 ) = yn locality : Loc = 1 loc). We use the geometric mean of Rel., Gen., and Loc. to evaluate the overall editing performance, which balances metric sensitivity and interpretability, exhibits sensitivity to weak performance areas, and is suitable for scenarios where all three metrics are equally Rel. Gen. Loc. to assess the holistic editing effectiveness. Here, l() is the indicator important. OP = 3 function used to count the number of successful predictions. n=1 l(fωN (xn n=1 l(fωN (xn ) = yn loc) = fω0(xn n=1 l(fωN (xn (cid:80)N (cid:80)N (cid:80)N For the hallucination dataset specifically, we utilize perplexity(PPL) as the metric to assess editing performance. PPL can be interpreted as the \"average branching factor in predicting the next token,\" where lower value indicates more accurate model predictions and suggests reduced likelihood of the edited model generating hallucinations. PPL = exp (cid:17) i=1 log (yicontexti) (cid:80)N (cid:16) 1 N"
        },
        {
            "title": "3.2 Main Results",
            "content": "Table 3 effevtively addressed Q1, Q4 and Q5. It has been rigorously evaluated across diverse models and scales (N = 1, 30, 120, 1000) of QA editing tasks, demonstrating state-of-the-art performance. Fine-tuning-based methods achieve good accuracy and generalization at small scales but suffer from catastrophic forgetting and knowledge conflicts in large-scale edits, leading to performance degradation. GRACE excels in accuracy but has limited generalization, while WISE maintains strong locality but sacrifices critical knowledge, reducing editing accuracy. ROME-style methods are stable but overfit and struggle with generalization. Figure 3: Average Editing Performance of WikiBigEdit Across Different Models To address Q2, Table 4 shows REPAIRs effectiveness in reducing hallucinations on the SelfCheckGPT dataset for LLaMA-3-8B across different editing scales. REPAIR balances reduced hallucinations with preserved locality, making it highly effective for large-scale model editing. To address Q3 and Q4, Table 3 and Figure 3 show that REPAIRs closed-loop error feedback, together with distribution-aware clustering and redistribution, yields consistently superior performance across edit scales and exceptional stability for large-scale edits. Smaller models concentrate knowledge in narrower parameter subsets, enabling reliable local corrections but weakening long-term stability and generalization (i.e., maintaining accuracy while preserving unrelated knowledge). Accordingly, DeepSeek-R1-1.5B attains higher immediate correction rates at small edit_Num, yet degrades quickly as grows. For locality, LLaMA-3-8B and Qwen2.5-7B are marginally stronger due to parameter redundancy; DeepSeek-R1-1.5B remains competitive only at low N, then collapses under extreme multi-point editing. In contrast, larger models distribute knowledge more broadly, and though harder to modifysuccessful edits generalize better across"
        },
        {
            "title": "ContiAI",
            "content": "Table 3: Comparative results for QA on multi-scale editing (ZsRE and WikiBigEdit) : Num Edits. Method = 1 = 30 = 120 = Rel. Gen. Loc. OP. Rel. Gen. Loc. OP. Rel. Gen. Loc. OP. Rel. Gen. Loc. OP. LLaMA-3-8B (ZsRE) FT-L 0.57 0.52 0.96 0.66 0.35 0.35 0.52 0.39 0.29 0.26 0.21 0.25 0.19 0.15 0.02 0.08 FT-EWC 0.96 0.93 0.02 0.26 0.78 0.76 0.02 0.23 0.76 0.76 0.08 0.36 0.69 0.67 0.08 0.33 MEND 0.95 0.93 0.96 0.95 0.24 0.25 0.18 0.22 0.08 0.07 0.00 0.00 0.00 0.00 0.00 0.00 0.85 0.80 0.99 0.88 0.61 0.60 0.68 0.63 0.22 0.22 0.04 0.12 0.01 0.01 0.01 0.01 ROME MEMIT-M 0.84 0.81 0.99 0.88 0.73 0.72 0.95 0.79 0.70 0.65 0.82 0.72 0.63 0.63 0.62 0.63 0.68 0.58 0.56 0.61 0.65 0.47 0.36 0.49 0.20 0.12 0.27 0.20 0.03 0.03 0.74 0.27 DEFER 0.97 0.36 1.00 0.71 0.96 0.17 1.00 0.55 0.94 0.14 1.00 0.51 0.93 0.08 1.00 0.42 GRACE 0.94 0.92 1.00 0.95 0.62 0.60 0.86 0.68 0.57 0.58 0.87 0.66 0.45 0.44 0.51 0.47 WISE REPAIR 0.94 0.92 1.00 0.95 0.93 0.90 0.87 0.89 0.76 0.74 1.00 0.83 0.68 0.65 0.89 0.73 Qwen2.5-7B (ZsRE) 0.68 0.63 0.93 0.74 0.28 0.23 0.44 0.30 0.13 0.11 0.10 0.11 0.08 0.06 0.02 0.05 FT-L FT-EWC 0.97 0.92 0.05 0.35 0.82 0.80 0.02 0.24 0.71 0.69 0.05 0.29 0.58 0.56 0.03 0.21 0.96 0.95 0.96 0.96 0.31 0.31 0.27 0.29 0.15 0.14 0.03 0.09 0.02 0.02 0.00 0.00 MEND ROME 0.90 0.89 0.99 0.93 0.77 0.73 0.52 0.66 0.31 0.28 0.03 0.14 0.01 0.02 0.00 0.00 MEMIT-M 0.84 0.81 0.99 0.88 0.73 0.72 0.95 0.79 0.70 0.65 0.82 0.72 0.52 0.51 0.57 0.53 0.74 0.67 0.88 0.76 0.58 0.51 0.44 0.51 0.22 0.21 0.43 0.27 0.14 0.08 0.25 0.14 DEFER 0.97 0.41 0.98 0.73 0.97 0.2 1.00 0.58 0.95 0.08 0.98 0.42 0.94 0.02 1.00 0.27 GRACE 0.97 0.95 0.98 0.97 0.79 0.73 0.91 0.80 0.59 0.57 0.92 0.68 0.44 0.41 0.72 0.51 WISE REPAIR 0.98 0.95 1.00 0.98 0.93 0.90 0.93 0.92 0.81 0.80 0.92 0.84 0.72 0.70 0.67 0.69 DeepSeek-R1-1.5B (WikiBigEdit) FT-L 0.71 0.68 0.93 0.77 0.26 0.20 0.76 0.34 0.13 0.11 0.37 0.17 0.02 0.02 0.08 0.03 FT-EWC 0.93 0.91 0.33 0.65 0.70 0.70 0.18 0.45 0.42 0.41 0.07 0.23 0.18 0.15 0.02 0.08 0.91 0.87 0.95 0.91 0.43 0.38 0.10 0.25 0.24 0.23 0.08 0.16 0.03 0.03 0.02 0.05 MEND ROME 0.86 0.83 0.97 0.88 0.72 0.71 0.67 0.70 0.18 0.18 0.02 0.09 0.01 0.0 0.01 0.00 MEMIT-M 0.86 0.87 0.97 0.90 0.78 0.77 0.82 0.79 0.54 0.51 0.77 0.60 0.38 0.38 0.62 0.45 0.68 0.58 0.47 0.35 0.63 0.61 0.51 0.58 0.17 0.15 0.33 0.20 0.07 0.07 0.12 0.08 DEFER 0.96 0.47 0.99 0.76 0.93 0.24 0.91 0.59 0.76 0.13 0.89 0.44 0.63 0.07 0.81 0.33 GRACE 0.89 0.91 0.98 0.93 0.76 0.74 0.89 0.79 0.64 0.65 0.83 0.70 0.47 0.38 0.61 0.48 WISE REPAIR 0.98 0.93 0.98 0.96 0.84 0.83 0.91 0.86 0.71 0.69 0.90 0.76 0.58 0.54 0.81 0.63 contexts. At medium scale (N=120), MEMIT-M and WISE show higher Rel., likely because REPAIRs pruning/reassembly introduces transient instability before sufficient error signals accumulate; however, at N=1000 their performance drops sharply, while REPAIRs dynamic adjustment preserves robustness and achieves the best overall metric. The error distribution can be seen in Appendix. Figure 3 further addresses Q1 regarding the effectiveness of distillation. For external memory-based editors, the ability to select the correct network for inference directly determines editing performance. The activation score, which serves as critical routing criterion in memory networks, must exhibit statistically significant differences between new knowledge and irrelevant knowledge to ensure both reliability and locality of edits. As shown in Figure 4 (a) and (c), prior methods relying solely on triple-boundary loss fail to adequately separate the activation scores of Dataedit, Datarephrase, and Dataloc, particularly in large-scale continual editing scenarios, leading to breakdown of the routing mechanism. This deficiency fundamentally limits their editing performance. In contrast, by introducing inner-batch knowledge distillation, sample filtering, and samples reintegration, KD, as shown in Figure 4 (b) and (d), achieves clear separation among the three types"
        },
        {
            "title": "ContiAI",
            "content": "Table 4: Main editing results for Hallucination task (SelfCheckGPT). Method FT-L FT-EWC MEND ROME MEMIT-M DEFER GRACE WISE REPAIR = 1 = = 120 = 500 Rel. (PPL ) Loc. () Rel. () Loc. () Rel. () Loc. () Rel. () Loc. () 4.27 2.18 5.34 1.88 1.62 1.29 2.21 1.91 1.43 LLaMA-3-8B 0.96 0.24 0.87 0.99 1.00 0.23 1.00 1.00 1.00 3.15 3.51 1.24 2.47 1.78 4.12 8.67 1.59 1.37 0.71 0.09 0.86 0.94 0.99 0.28 1.00 1.00 1. 34.52 2.90 9.17 84.56 8.03 8.91 7.24 1.14 1.12 0.43 0.21 0.89 0.03 0.99 0.19 1.00 0.99 1.00 51.31 3.48 564.9 73.4 7.43 15.16 6.18 2.08 1. 0.26 0.24 0.00 0.02 0.94 0.12 1.00 0.99 1.00 of samples, thereby ensuring the proper functioning of the routing mechanism."
        },
        {
            "title": "3.3 Ablation Studies",
            "content": "The evaluation of the overhead and throughput of REPAIR can be found in the Appendix 7. To answer Q6, we conducted comprehensive evaluations across four dimensions to assess the effectiveness of each component of REPAIR and analyze critical hyperparameter sensitivity under different editing scales. Notably, REPAIR demonstrates robustness in large-scale editing scenarios that prior methods fail to achieve. As the number of edits increases, REPAIR exhibits increasingly pronounced advantages in overall performance: effective routing ensures strong locality, while the error-feedback mechanism maintains continual reliability. As shown in Figure 5 (a)(d), the relative contributions of REPAIRs components vary across sample regimes but complement each other seamlessly. In small-scale edits, pruning with error feedback substantially improves reliability, while in large-scale scenarios, distribution-aware recognition and knowledge distillation become more critical. Regarding hyperparameter analysis in Figure 6, we observe distinct performance patterns: low thresholds fail to filter low-quality samples, limiting corrective opportunities; The total number of edits is limited, and the filtered erroneous samples cannot receive sufficient corrective training, which limits overall performance. large number of erroneous samples in the early stage undergo continuous learning, causing the model to quickly fall into local optima, leading to catastrophic degradation of generalization. Subsequent learning yields minimal improvement, resulting in poor performance. In the upper-right quadrant, the absence of error feedback leaves many suboptimal samples, and the model editing efficiency is relatively high, approximating an open-loop editing process. In the lower-right quadrant, the model training efficiency is the lowest, but excessive editing can introduce overfitting risks, wasting computational resources on edits with low marginal utility."
        },
        {
            "title": "4 Conclusion",
            "content": "In this work, we proposed REPAIR, robust framework for lifelong model editing integrating error closed-loop feedback, inner-batch knowledge distillation, and loss-aware subspaces merging. Extensive experiments demonstrate that REPAIR maintains high performance under small-scale edits and exhibits remarkable"
        },
        {
            "title": "ContiAI",
            "content": "(a) (c) (b) (d) Figure 4: Activation Score Visualization. Results on LLaMA-3 for the WikiBigEdit dataset (N=1550) for the QA task and the SelfCheckGPT dataset for hallucination (N=600). (a) N= (b) N=60 (c) N=120 (d) N=1000 Figure 5: Performance comparison of different components. Each radar chart shows performance on four metrics: Rel., gen., loc., and OP. on Qwen2.5 with ZsRE."
        },
        {
            "title": "ContiAI",
            "content": "(a) (b) Figure 6: Performance heatmap for the N=120 QA task on the LLaMA3 model. Figure (a) shows the sensitivity analysis of two hyperparameters: the number of subspaces and the amount of updated parameters; Figure (b) analyzes the impact of error threshold and maximum iteration count on performance, with optimal performance observed at intermediate values. robustness in large-scale editing scenarios, consistently outperforming existing baselines. These results highlight the potential of combining memory-aware strategies with optimization-driven editing for reliable and precise model updates. The intra-group distillation explicitly encourages feature alignment among similar samples, guiding the elimination and recombination of inconsistent samples. The loss-aware merging assigns higher weights to subspaces achieving lower training loss, effectively preserving reliable knowledge and reducing information dilution. Extensive experiments show that REPAIR consistently improves reliability and generalization, and demonstrates clear advantages in large-scale editing scenarios, highlighting the effectiveness of coordinated sample-level alignment and global reliability-aware merging."
        },
        {
            "title": "References",
            "content": "[1] Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, and Jundong Li. Knowledge editing for large language models: survey. ACM Computing Surveys, 2024. doi: 10.1145/3698590. URL https://dl.acm.org/doi/10.1145/3698590. Also available as arXiv:2310.16218. [2] Ayush Gupta, Han Liu, Agastya Sharma, Di Jin, Neil Zhenqiang Gong, Sameer Singh, and Chenhao Tan. Rebuilding ROME: Resolving model collapse during sequential model editing. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2024. URL https://aclanthology.org/2024.emnlp-main.1210/. [3] Roi Cohen, Eden Biran, Ori Yoran, Amir Globerson, and Mor Geva. Evaluating the ripple effects of knowledge editing in language models. Transactions of the Association for Computational Linguistics, 12: 283298, 2024. doi: 10.1162/tacl_a_00644. URL https://aclanthology.org/2024.tacl-1.16/. [4] Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher D. Manning, and Chelsea Finn. Memory-based model editing at scale. In Proceedings of the 39th International Conference on Machine Learning (ICML), volume 162 of Proceedings of Machine Learning Research, pages 1582815846. PMLR, 2022. URL https://proceedings.mlr.press/v162/mitchell22a.html."
        },
        {
            "title": "ContiAI",
            "content": "[5] Thomas Hartvigsen, Swami Sankaranarayanan, Hamid Palangi, Yoon Kim, and Marzyeh Ghassemi. Aging with GRACE: Lifelong model editing with discrete key-value adaptors. In Advances in Neural Information Processing Systems (NeurIPS), 2023. URL https://proceedings.neurips.cc/paper_files/ paper/2023/file/95b6e2ff961580e03c0a662a63a71812-Paper-Conference.pdf. NeurIPS 2023. [6] Peng Wang, Zexi Li, Ningyu Zhang, Ziwen Xu, Yunzhi Yao, Yong Jiang, Pengjun Xie, Fei Huang, and Huajun Chen. Wise: Rethinking the knowledge memory for lifelong model editing of large language models. In Advances in Neural Information Processing Systems (NeurIPS), 2024. doi: 10.48550/arXiv.2405.14768. URL https://arxiv.org/abs/2405.14768. NeurIPS 2024. Also available as arXiv:2405.14768. [7] Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher D. Manning, and Chelsea Finn. Fast model editing at scale. In International Conference on Learning Representations (ICLR), 2022. URL https://openreview.net/pdf?id=0DcZxeWfOPt. [8] James Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwińska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13):35213526, 2017. doi: 10.1073/pnas.1611835114. URL https://www.pnas.org/doi/10.1073/pnas.1611835114. [9] Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in gpt. Advances in neural information processing systems, 35:1735917372, 2022. [10] Kevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, and David Bau. Mass-editing memory in transformer. In International Conference on Learning Representations (ICLR), 2023. URL https://openreview.net/forum?id=MkbcAHIYgyS. [11] Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal. Ties-merging: Resolving interference when merging models, 2023. URL https://arxiv.org/abs/2306.01708. [12] Michael McCloskey and Neal J. Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. In The Psychology of Learning and Motivation, volume 24, pages 109165. Academic Press, 1989. doi: 10.1016/S0079-7421(08)60536-8. URL https://www.sciencedirect. com/science/article/pii/S0079742108605368. [13] Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(12):29352947, 2018. doi: 10.1109/TPAMI.2017.2773081. URL https://doi.org/10.1109/TPAMI.2017.2773081. [14] David Lopez-Paz and MarcAurelio Ranzato."
        },
        {
            "title": "Gradient",
            "content": "episodic memory for continual learning. 2017), Advances 64676476, (NeurIPS 7225-gradient-episodic-memory-for-continual-learning. in Neural"
        },
        {
            "title": "In\npages",
            "content": "2017."
        },
        {
            "title": "Systems",
            "content": "[15] Hanul Shin, Jung Kwon Lee, Jaehong Kim, with deep generative replay. In Advances 30 (NeurIPS 2017), pages 29902999, 2017. 6892-continual-learning-with-deep-generative-replay. in Neural and Jiwon Kim."
        },
        {
            "title": "Continual",
            "content": "learning Information Processing Systems URL https://papers.nips.cc/paper/"
        },
        {
            "title": "ContiAI",
            "content": "[16] Mehrdad Farajtabar, Navid Azizan, Alex Mott, and Ang Li. Orthogonal gradient descent for continual learning. In Silvia Chiappa and Roberto Calandra, editors, Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics (AISTATS), volume 108 of Proceedings of Machine Learning Research, pages 37623773. PMLR, Aug 2020. URL https://proceedings.mlr.press/ v108/farajtabar20a.html. [17] Enrico Fini, Victor G. Turrisi da Costa, Xavier Alameda-Pineda, Elisa Ricci, Karteek Alahari, and In Proceedings of the IEEE/CVF Julien Mairal. Self-supervised models are continual learners. Conference on Computer Vision and Pattern Recognition (CVPR), pages 96219630, June 2022. URL https://openaccess.thecvf.com/content/CVPR2022/html/Fini_Self-Supervised_ Models_Are_Continual_Learners_CVPR_2022_paper.html. [18] Sanket Vaibhav Mehta, Darshan Patil, Sarath Chandar, and Emma Strubell. An empirical investigation of the role of pre-training in lifelong learning. Journal of Machine Learning Research, 24(214):150, 2023. URL https://www.jmlr.org/papers/v24/22-0496.html. [19] Xuming Ran, Juntao Yao, Yusong Wang, Mingkun Xu, and Dianbo Liu. Brain-inspired continual pre-trained learner via silent synaptic consolidation. ArXiv, abs/2410.05899, 2024. URL https: //api.semanticscholar.org/CorpusID:273228983. [20] Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016. URL https://arxiv.org/abs/1606.04671. [21] Arun Mallya and Svetlana Lazebnik. Packnet: Adding multiple tasks to single network by iterative pruning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 77657773, 2018. doi: 10.1109/CVPR.2018.00810. URL https://openaccess.thecvf.com/ content_cvpr_2018/papers/Mallya_PackNet_Adding_Multiple_CVPR_2018_paper.pdf. [22] Zhenyi Wang, Zhen Zhang, Xiaojuan E, Ziyu Zhang, Zhipeng Luo, Zhipeng He, and Guangyao Li. comprehensive survey on continual learning: From definitions to applications. arXiv preprint arXiv:2302.00487, 2023. URL https://arxiv.org/abs/2302.00487. [23] Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in GPT. In Advances in Neural Information Processing Systems (NeurIPS), 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/ 6f1d43d5a82a37e89b0665b33bf3a182-Paper-Conference.pdf. NeurIPS 2022. [24] Nicola De Cao, Wilker Aziz, and Ivan Titov. Editing factual knowledge in language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6491 6506, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.522. URL https://aclanthology.org/2021.emnlp-main.522/. [25] Ningyu Zhang, Bozhong Tian, Siyuan Cheng, Xiaozhuan Liang, Yi Hu, Kouying Xue, Yanjie Gou, Xi Chen, and Huajun Chen. Instructedit: Instruction-based knowledge editing for large language models. In Proceedings of the 33rd International Joint Conference on Artificial Intelligence (IJCAI 2024), 2024. doi: 10.24963/ijcai.2024/733. URL https://www.ijcai.org/proceedings/2024/0733.pdf."
        },
        {
            "title": "ContiAI",
            "content": "[26] Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov, and David Bau Lab. Counterfact: benchmark for evaluating knowledge editing locality and generalization. https://rome.baulab. info/, 2022. Dataset introduced alongside ROME. [27] Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. Zero-shot relation extraction via reading comprehension. In Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 333342, Vancouver, Canada, August 2017. Association for Computational Linguistics. doi: 10.18653/v1/K17-1034. URL https://aclanthology.org/K17-1034/. [28] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, , and Slav Petrov. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452466, 2019. [29] Together Computer. RedPajama: An open dataset for training large language models, 2023. URL https://www.together.ai/blog/redpajama."
        },
        {
            "title": "A Statement",
            "content": "A.1 Ethics Statement This work studies safe, auditable editing of large language models using only publicly available datasets (ZsRE, WikiBigEdit, and hallucination set) and off-the-shelf pretrained models; no human subjects or personally identifiable data were collected. We follow all dataset/model licenses and the double-blind review policy. Potential risks include misuse of editing to inject misinformation or to weaken safety constraints, and unintended spillover of edits to unrelated behaviors. To mitigate these risks, our framework emphasizes locality and closed-loop error checks before and after integration, and we report reliabilitygeneralizationand locality metrics to surface side effects. Upon release, we will include guardrails such as edit logs, validation suites, reversible edits, and instructions for responsible use. These design choices align with REPAIRs stated goal of precise updates with locality safeguards. A.2 Reproducibility Statement We will release our code, configs, and seeds to reproduce all results end-to-end after acceptance. Scripts fetch data/models, fix environments, and regenerate all tables/figures with the exact metrics (Rel./Gen./Loc./OP., PPL); hardware and hyperparameters are documented. A.3 AI Usage Statement We used large language modelbased tools during writing and implementation for text polishing, grammar and usage checks, and programming assistance (e.g., example code, refactoring, comments, and script templates). All AI-generated suggestions were reviewed, revised, and validated by the authors. The experimental design, data processing, result analysis, and conclusions were conducted independently by the authors; AI tools do not 14 constitute authorship or academic credit. No sensitive or restricted data were provided to the tools, and they were not used to automatically generate experimental results or to replace essential human judgment."
        },
        {
            "title": "B Related Work",
            "content": "B.1 Continual Learning Continual Learning (CL)also known as Incremental Learning or Lifelong Learningaims to enable models to learn sequentially from stream of tasks without forgetting previously acquired knowledge. The core challenge in CL is catastrophic forgetting, where adapting to new tasks leads to significant degradation in performance on earlier tasks [8, 12]. To address this, numerous methods have been proposed, which can be broadly categorized into five groups: regularization-based, replay-based, optimization-based, representation-based, and architecture-based approaches. Regularization-based methods mitigate forgetting by adding constraints to the loss function to preserve important parameters or behaviors from previous tasks. For example, Elastic Weight Consolidation (EWC) leverages Fisher information to regularize parameter updates [8], while Learning without Forgetting (LwF) uses knowledge distillation to maintain output consistency [13]. Replay-based methods retain or generate samples from previous tasks to approximate old data distributions. Experience replay stores subset of prior samples in memory buffer [14], whereas generative replay synthesizes pseudo-samples using deep generative models such as GANs or VAEs [15]. Optimization-based methods manipulate the optimization process itself to avoid interference between tasks. Gradient Episodic Memory (GEM) projects gradients so as not to increase loss on previous tasks [14], while Orthogonal Gradient Descent (OGD) promotes updates that are orthogonal to gradient directions associated with past tasks [16]. Representation-based methods focus on learning robust and transferable features that are less prone to forgetting. Self-supervised learning [17] and large-scale pre-training [18] have been shown to bolster CL performance by providing more stable representations. Architecture-based methods [1921]dynamically expand or partition the network to allocate task-specific parameters. Progressive Networks add new columns for each incoming task with lateral connections to prior columns [20], while PackNet iteratively prunes and reuses weights to free capacity for new tasks [21]. Recent trends extend CL to more realistic and challenging settings, including class-incremental learning (CIL), task-free CL (TFCL), online CL (OCL), and applications across object detection, semantic segmentation, reinforcement learning, and natural language processing [22]. B.2 Model editing Model editing targets post-hoc modification of trained models behavior to insert, correct, or remove specific knowledge, ideally without harming unrelated capabilities. common taxonomy distinguishes (i) direct / training-free parameter edits, (ii) learning-based editors that predict weight updates, and (iii) semi-parametric systems that externalize edits via retrieval or memory; recent surveys consolidate definitions, benchmarks, and open challenges [1]."
        },
        {
            "title": "ContiAI",
            "content": "Table 5: Dataset statistics Task Editing Data Pre-edit(LLaMA/Qwen) Locality Data QA Hallu. ZsRE WikiBigEdit SelfCheckGPT 1000 500K 600 0.25/0.21 ACC 0.36/0.32 ACC 28.7/29.1 PPL NQ [28] NQ RedPajama [29] ROME locates causal mediators of factual associations in mid-layer feed-forward (MLP) modules of Transformers and applies rank-one update to edit single fact [23]. MEMIT extends this idea to mass editing, deriving multi-layer closed-form updates that scale to thousands of edits in large models while maintaining stronger locality than prior methods [10]. Although effective, subsequent analyses highlight stability issues under sequential edits and propose remedies [2]. Early work framed editing as learning small hypernetwork to predict weight deltas from an edit specification: KnowledgeEditor (KE) learns constrained updates to change models factual prediction while preserving behavior on paraphrases [24]. MEND trains lightweight editor networks to transform fine-tuning gradients, enabling fast, local edits at scale across architectures [7]. Instruction-driven variants further condition edits on natural-language instructions to improve usability and control [25]. Semi-parametric approaches such as SERAC store edits in an external keyvalue memory and learn to route between the base model and retrieved counterfactuals, achieving strong reliability and specificity without permanently altering base parameters [4]. This design is attractive when edits must be audited, reverted, or scoped to contexts. Editing methods are typically assessed along reliability (does the change take effect), locality/specificity (does unrelated behavior remain intact), and generalization (do edits transfer to paraphrases and contexts). Standard benchmarks include CounterFact and zsRE [26, 27]. Recent studies examine ripple effects beyond targeted facts, revealing broader side impacts on reasoning and distributed knowledge, and call for more rigorous, stress-testing evaluations [3]. Overall, direct, learning-based, and semi-parametric approaches offer complementary trade-offs in edit scalability, controllability, and safety; combining precise localization with guardrails (e.g., retrieval gating, edit scopes, or validation filters) remains an active direction [1]."
        },
        {
            "title": "C Experiments details",
            "content": "The experiment details are given in Table 5, and hyperparameters are in Table 6. Under identical hardware and batch configurations, the WISE baseline exhibits lower per-unit overhead. REPAIR demonstrates similar scaling slope but with higher intercept, primarily attributable to: Distribution-aware clustering and reorganization; Additional forward/backward passes for in-batch distillation; The triggering frequency and cost of closed-loop pruning and retraining at large scales; The final merging (TIES) cost."
        },
        {
            "title": "ContiAI",
            "content": "Table 6: Hyperparameter settings ZsRE on LLaMA-3 HYPER VALUE HYPER VALUE HYPER VALUE Mask ratio λa Temperature γ1 niter Mask ratio λa Temperature γ1 niter Mask ratio λa Temperature γ1 niter 0.20 1.00 2.00 2.00 30.00 0.20 2.00 2.00 5.00 50.00 Edit_lr λKD Act ratio γ2 λ 0.90 1.00 0.20 20.00 0.20 ZsRE on Qwen2.5 Err_Thresh Max_iter Layer_ID γ Act_ratio Edit_lr λKD Act ratio γ20 λ 0.90 1.00 0.88 20.00 0.30 Err_Thresh Max_iter Layer_ID γ Act_ratio Selfcheck GPT on LLaMA-3-8B 0.20 5.00 2.00 5.00 50.00 Edit_lr λKD Act ratio γ2 λ 1.00 1.00 0.88 20.00 0.20 Err_Thresh Max_iter Layer_ID γ Act_ratio 0.85 10000 29.00 10.00 0.30 0.85 10000 23.00 10.0 0. 0.85 5000 27.00 10.00 0.80 As increases, the runtime curves of ROME, MEND, and FT-L exhibit significantly steeper growth, becoming substantially expensive or nearly infeasible at = 103. Throughput demonstrates that WISE maintains approximately 1.8 edits/min at large , followed by GRACE. REPAIR achieves 0.8-0.9 edits/min at scale, lower than WISE and MEMIT-M, consistent with expectations given its additional computational procedures. Error bars (representing standard deviation across multiple runs) indicate that REPAIR exhibits slightly higher variance than WISE, attributable to fluctuations in retriggering frequency and sample distribution characteristics. Relative overhead shows the time ratio of REPAIR to WISE increasing from 1.6 to 2.2 with increasing scale."
        },
        {
            "title": "D Theoretical Analysis and Proof Sketches",
            "content": "We now provide theoretical justifications for the stability and convergence of the proposed REPAIR framework. We introduce formal assumptions and derive lemmas and theorems that characterize the behavior of our method."
        },
        {
            "title": "ContiAI",
            "content": "Table 7: Main results for QA on DeepSeek-R1-1.5B : Num Edits. Method = 1 = 30 = 120 = Rel. Gen. Loc. OP. Rel. Gen. Loc. OP. Rel. Gen. Loc. OP. Rel. Gen. Loc. OP. DeepSeek-R1-1.5B (ZsRE) FT-L 0.43 0.42 0.95 0.56 0.32 0.33 0.46 0.36 0.21 0.21 0.15 0.19 0.17 0.15 0.09 0.13 FT-EWC 0.97 0.94 0.15 0.52 0.82 0.81 0.02 0.24 0.63 0.64 0.02 0.20 0.57 0.56 0.02 0.19 MEND 0.95 0.94 0.98 0.96 0.42 0.42 0.18 0.32 0.18 0.12 0.07 0.11 0.8 0.03 0.00 0.00 0.87 0.87 0.99 0.91 0.66 0.64 0.72 0.67 0.17 0.18 0.09 0.14 0.01 0.01 0.01 0.01 ROME MEMIT-M 0.88 0.87 0.99 0.91 0.71 0.72 0.92 0.78 0.63 0.65 0.78 0.68 0.48 0.47 0.53 0.49 0.62 0.60 0.82 0.67 0.58 0.57 0.57 0.57 0.34 0.31 0.23 0.29 0.07 0.06 0.02 0.04 DEFER 0.98 0.31 0.99 0.67 0.92 0.22 0.98 0.58 0.89 0.13 1.00 0.49 0.83 0.05 0.94 0.34 GRACE 0.92 0.90 1.00 0.94 0.86 0.85 0.92 0.88 0.72 0.72 0.87 0.77 0.49 0.47 0.47 0.48 WISE REPAIR 0.93 0.93 1.00 0.95 0.91 0.89 0.87 0.89 0.74 0.74 0.82 0.77 0.59 0.57 0.61 0.59 Table 8: Main results for QA (ZeRE) on multi-model editing with error distribution. Method = Gen. Rel. Loc. Rel. = 30 Gen. Loc. LLaMA-3-8B 0.94 0.008 0.92 0.01 1.00+0.00 0.02 0.93 0.003 0.90 0.003 0.87 0.004 Qwen2.5-7B 0.98 0.02 0.95 0.03 1.00+0.00 0.93 0.04 0.90 0.03 0.93 0.01 0.02 DeepSeek-R1 0.93 0.02 0.92 0.03 0.99 0.01 0.91 0.01 0.89 0.03 0.87 0.01 GPT2-XL 0.91 0.03 0.92 0.03 0.99 0.01 0.88 0.03 0.88 0.02 0.84 0.01 Method = 120 = 1000 Rel. Gen. Loc. Rel. Gen. Loc. LLaMA-3-8B 0.76 0.03 0.74 0.02 1.00+0.00 0.68 0.05 0.65 0.01 0.89 0.04 0.04 Qwen2.5-7B 0.81 0.04 0.80 0.05 0.92 0.03 0.72 0.05 0.70 0.04 0.67 0.03 DeepSeek-R1 0.74 0.03 0.74 0.04 0.82 0.05 0.59 0.02 0.57 0.01 0.61 0.03 GPT2-XL 0.79 0.02 0.77 0.01 0.80 0.03 0.61 0.03 0.62 0.01 0.68 0. D.1 Preliminaries Assumption 1 (Standard Optimization Setting). We assume that the loss function L(Θ) is L-smooth, i.e., L(Θ1) L(Θ2) LΘ1 Θ2, and bounded below by > . Learning rates satisfy ηt > 0 and (cid:80) D.2 Stability of Masked Gradient Updates ηt = , (cid:80) η2 < . Lemma 1 (Norm Bound under Masked Updates). Let gi = masked update v,i and Mi be Bernoulli mask. Then the v,i = η(Mi gi) satisfies v,i2 ηgi2. Proof. Since Mi is coordinate projection, Mi gi removes certain entries of gi and never increases its magnitude. Hence Mi gi2 gi2. Multiplying by η yields the claim."
        },
        {
            "title": "ContiAI",
            "content": "Figure 7: Cost-Performance Assessment. The total runtime of each method scales approximately linearly with the editing scale , appearing as straight lines with slopes close to 1 in log-log coordinates. This indicates that the primary overhead is proportional to the number of edited entries. Theorem 1 (Inter-Shard Stability). Assume masks {Mi} are sampled independently with overlap probability ρ2. Then in expectation, E[Mi gi, Mj gj] = ρ2gi, gj. Thus, masking reduces the expected conflict between gradients of different shards. Proof. For each coordinate p, Pr[Mi(p) = 1, Mj(p) = 1] = ρ2. Therefore, the expected inner product between masked gradients is ρ2 times the original inner product. This reduces cross-shard interference and improves stability. D.3 Closed-Loop Re-trigger Analysis Assumption 2 (Error Reduction per Re-trigger). Suppose that each re-trigger reduces the error rate of shard by at least fixed constant δ > 0, unless it is already below the pruning threshold τprune. Lemma 2 (Linear Error Decrease). Let r(n) denote the error rate after re-triggers. Under Assumption 2, Theorem 2 (Finite-Time Convergence). If r(0) is the initial error rate, then after at most r(0) r(n) nδ. r(0) τprune δ re-triggers, the error rate satisfies r(N ) τprune."
        },
        {
            "title": "ContiAI",
            "content": "Proof. By Lemma 3, r(N ) below threshold in finite time. r(0) δ. Choosing such that r(0) δ τprune ensures convergence D.4 Overall Convergence Intuition Theorem 3 (Closed-Loop Stability of REPAIR). Under Assumptions 1 and 2, the iterative process combining masked updates, inner-batch distillation, and closed-loop re-trigger forms contractive mapping in expectation. Consequently, the system converges to stable edited state with bounded error rate and without catastrophic forgetting. Proof Sketch. Masked updates reduce the variance of parameter updates, inner-batch distillation aligns outputs across samples, and re-trigger guarantees finite-time reduction of shard-level error rates. Together, these components yield monotone improvement. By standard stochastic contraction arguments, the process converges to fixed point characterized by consistent batch predictions and an error rate below τprune. Lemma 3 (Zero-variance at any global minimizer). Let µ = 1 oi µ2. If not all oi are equal, then Lvar > 0, while if o1 = = om = (with = 1) then Lvar = 0. Hence every global minimizer of LKD on (Sd1)m must satisfy o1 = = om =: v. Lemma 4 (Unique global minimizer). Under the conclusion of Lemma 3, minimizing LKD(v) = λ(cid:0)1 v, u(cid:1) over = 1 gives the unique solution = u. Therefore the unique global minimizer of LKD on (Sd1)m is = [u, . . . , u]. i=1 oi and Lvar = 1 (cid:80)m (cid:80) Lemma 5 (Riemannian smoothness). Let = (Sd1)m and endow each block with the canonical metric. Then LKD is LR-smooth on in the Riemannian sense: there exists constant LR 2λ + 4ϑ such that for all S, M, grad LKD(S) grad LKD(S) LR distM(S, S). Sketch. For each block oi, oiLcos = (λ/m)u (constant), and oiLvar = (2ϑ/m)(oi µ) with µ depending linearly on {oj}. ) and using the Lipschitzness of the projection map on Sd1 yields Projecting to the tangent space by (I oio the bound. Theorem 4 (Convergence of cosine+variance KD on the sphere). Consider Riemannian gradient descent on = (Sd1)m: o(t+1) = (cid:0)ηt gradoi LKD(St)(cid:1) o(t) (i = 1, . . . , m), with the retraction Ro(v) = (o + v)/o + v. 0 < ηt < 2/LR, or (b) diminishing stepsizes (cid:80) ηt = , (cid:80) η2 < , then: If the step sizes satisfy either (a) constant stepsize LKD(St) LKD(S), grad LKD(St) 0, and every limit point of {St} is Riemannian critical point. By Lemma 4, the unique global minimizer is = [u, . . . , u]; thus the sequence converges to S. Proof sketch. Riemannian smoothness (Lemma 5) on the compact manifold ensures the standard descent lemma and monotone decrease for RGD under 0 < η < 2/LR, implying convergence of function values and"
        },
        {
            "title": "ContiAI",
            "content": "gradients to zero. By Lemmas 34, the only global minimizer is S, hence all limit points coincide with S. D.5 Stability of Masked Gradient Updates Let gi = v,i Rd. coordinate mask Mi {0, 1}d acts by (Mi gi)p = Mi(p) gi,p. Lemma 6 (Norm Bound under Masked Updates). For any stepsize η > 0, the masked update η(Mi gi) satisfies v,i = v,i2 η gi2. Proof. Coordinate-wise, Mi(p) gi,p gi,p because Mi(p) {0, 1}. Hence Mi gi2 gi2, and multiplying by η yields the claim. Theorem 5 (Inter-Shard Inner-Product Scaling). Suppose that for each coordinate p, the masks Mi(p), Mj(p) {0, 1} are sampled independently with Pr[Mi(p) = 1] = Pr[Mj(p) = 1] = ρ, 0 ρ 1, and masks are independent across coordinates and independent of gi, gj. Then, conditional on gi, gj, E[Mi gi, Mj gj gi, gj] = ρ2 gi, gj. In particular, masking scales the expected cross-shard alignment/conflict by the factor ρ2. Proof. By linearity of expectation and independence, for each coordinate p, E[Mi(p)Mj(p)] = E[Mi(p)] E[Mj(p)] = ρ2. Summing over yields the result. D.6 Closed-Loop Re-trigger Analysis Assumption 3 (Error Reduction per Re-trigger). Let r(n) There exists δ > 0 such that each re-trigger reduces error by at least δ whenever r(n) denote the error rate of shard after re-triggers. > τprune. Lemma 7 (Piecewise-Linear Error Decrease). Under Assumption 3, for all 0, max(cid:8) τprune, r(0) r(n) nδ (cid:9). Proof. If r(k) all k. Unrolling gives the stated maximum form. > τprune, then r(k+1) δ. Once r(k) r(k) i τprune, the bound r(n) τprune propagates for Theorem 6 (Finite-Time Hitting the Pruning Threshold). Let = (cid:108) (r(0) τprune)+ δ (cid:109) where (x)+ := max{x, 0}. After at most re-triggers, we have r(N) τprune. Proof. By Lemma 7, choose the smallest integer such that r(0) Nδ τprune. Then r(N) τprune. 21 Algorithm 1 REPAIR: Closed-Loop Lifelong Model Editing (Training) Require: Pretrained model fθ0 thresholds (ϵ, τE, τprune, τcorrect, ϵcons); margins (γ1, γ2, γ); KD weights (λ, ϑ) for Eq.(4); routing-loss weight λa; batch size b; optional temperature for soft KD. target FFN value matrix Wv; #shards K; mask ratio ρ; ;"
        },
        {
            "title": "ContiAI",
            "content": "1: Initialize side memories v,i Wv and masks Mi Bernoulli(ρ) for = 1..K; feedback pool ; residual pool . AssignShard(xe) FormBatches({xe} R, b) for each batch = {x(0), . . . , x(b1)} do AssignShard(x(0)) Ledit AutoregCE(B) LKD IntraBatchKD(B, λ, ϑ, ) Lact RoutingMargin(B, γ1, γ2, γ) Lbatch Ledit + λaLact + LKD MaskedUpdate(W v,i, Mi, Lbatch) FilterAndRecluster(B, ϵcons, R) 2: for each incoming edit triple (xe, ye, xloc) do 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: end for 22: LossAwareTIESMerge({W end for (ˆy, c) Evaluate(xe, ye) if = 0 then {(xe, ye)} ReTrigger(E) end if i=1, Wv) v,i}K end if if > τE or maxi ErrorRate(E, i) > τprune then Shard assignment by activation score Distribution-aware batching Target shard for this batch Autoregressive cross-entropy Eq.(4); optional soft KD Eq.(7) v,i v,i η(Mi L) Move high-LKD samples to residual pool {0, 1} indicates success Prune worst shard, rebuild, and retrain Loss-aware weighted TIES merge"
        },
        {
            "title": "E Algorithms",
            "content": "The pseudocode for error feedback, network pruning, sample knowledge distillation and reintegration, and the loss-based weighted ties merge strategy is as follows: Algorithm 2 REPAIR Inference with Dual-Memory Routing 1: function RouteAndPredict(x) 2: compute a(x) FFNActivation(x) for = 1..K do act(x) a(x) (W v,i Wv) 2 3: 4: 5: 6: 7: 8: 9: 10: (i) end for if maxi (i) else act(x) ϵ then return fθ0(x; Wv) arg maxi (i) act(x) return fθ0(x; v,i) end if 11: 12: end function Activation A(x) at the target FFN layer Route to main memory Route to side memory i"
        },
        {
            "title": "ContiAI",
            "content": "Use the most active shard during training Distribution-aware batching Autoregressive edit loss Ledit Eq.(4); optional soft-KD Algorithm 3 Subroutines 1: function AssignShard(x) 2: 3: 4: end function 5: function FormBatches(S, b) 6: 7: FFNActivation(x); (i) (W return arg maxi (i) v,i Wv)2, = 1..K oi Norm(cid:0)ModelFeat(x(i))(cid:1) for = 0, . . . , 1 cos(o(x), o(x)) Greedy seeding: pick x(0) = arg maxxS Build {x(0)} Top-(b1) nearest by cosine; remove from Repeat until is empty; return list of batches 1 (cid:80) Compute oi Norm(cid:0)ModelFeat(x(i))(cid:1) for = 0, . . . , 1 o0 Lcos 1 oio0 (cid:80)b1 i=1 (cid:16) (cid:17) (cid:80)b1 i=0 oi omean2 2 Get logits zi; pi = softmax(zi/T ); + 1 L 0 for with target sequence do 8: 9: 10: end function 11: function AutoregCE(B) 12: 13: 14: 15: 16: 17: end function 18: function IntraBatchKD(B, λ, ϑ, ) 19: end for return L/B (cid:80)y t=1 log pθ(yt y<t, x) 20: 1 i=0 oi; Lvar 1 (cid:80)b1 omean 1 λ Lcos + ϑ Lvar if > 0 then 21: 22: 23: 24: 25: end if 26: return 27: end function 28: function RoutingMargin(B, γ1, γ2, γ) 29: 30: 31: 32: 33: 34: 35: end function 36: function MaskedUpdate(W 37: 0 for each edit sample xe do end for return L/B v,i, Mi, L) v,i v,i, gm) for do L; gm Mi v,i OptimizerStep(W ℓKD(x) per-sample KD vs. x(0) if ℓKD(x) ϵcons then move to W 38: 39: end function 40: function FilterAndRecluster(B, ϵcons, R) 41: 42: 43: 44: 45: 46: 47: 48: end function 49: function Evaluate(xe, ye) 50: 51: 52: end function end for return ˆy RouteAndPredict(xe); 1[ˆy = ye] return (ˆy, c) end if sample unrelated xi; compute = ActDelta(xe), = ActDelta(xi) + max(0, γ1) + max(0, γ2 e) + max(0, γ (e i)) Optional: KL distillation for added stability (cid:80)b1 i=1 KL(p0pi) Eq.(7) Masked gradient to reduce cross-shard interference Mi {0, 1}shape(Wv) SGD/Adam, etc."
        },
        {
            "title": "ContiAI",
            "content": "Error rate for shard 8: arg maxi ErrorRate(E, i) Remove or reinitialize shard j: Build Eretrain from E; form batches; retrain shards via MaskedUpdate + IntraBatchKD v,j Wv + σinit (0, 1); resample Mj Closed-loop pruning and retraining Identify worst-performing shard v,i}, Wv) Loss-aware weighted TIES merge v,i Wv; compute training loss Li on its assigned data else arg maxi{wi τi[p]}; δ[p] τi[p] Conflict: keep most trustworthy shard end if Consistent signs: weighted sum Algorithm 4 Subroutines2 1: function ErrorRate(E, i) 2: 3: Ei {x arg maxj (j) ri {xEiCorrectness(x)τcorrect} 4: return ri 5: end function 6: procedure ReTrigger(E) 7: Ei act (x) = i} 13: 9: 10: end procedure 11: function LossAwareTIESMerge({W For each shard i: τi 12: wi eαLi (cid:80) eαLj for each parameter index do {(i, τi[p], wi)}K i=1 if all τi[p] share the same sign then wi τi[p] δ[p] (cid:80) 17: 14: 16: 15: return activation A(x) at the target FFN layer v,i Wv)2 18: 19: 20: 21: end for Wv Wv + δ; return Wv return maxi A(x) (W 22: 23: end function 24: function FFNActivation(x) 25: 26: end function 27: function ActDelta(x) 28: 29: end function 30: function ModelFeat(x) 31: 32: end function 33: function Norm(v) 34: return v/v2 35: end function 36: function Correctness(x) 37: 38: end function return predicted correctness score for return feature used for similarity (e.g., A(x) or last-token state)"
        }
    ],
    "affiliations": [
        "ContiAI Research"
    ]
}