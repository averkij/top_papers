{
    "paper_title": "ORION: Teaching Language Models to Reason Efficiently in the Language of Thought",
    "authors": [
        "Kumar Tanmay",
        "Kriti Aggarwal",
        "Paul Pu Liang",
        "Subhabrata Mukherjee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Reasoning Models (LRMs) achieve strong performance in mathematics, code generation, and task planning, but their reliance on long chains of verbose \"thinking\" tokens leads to high latency, redundancy, and incoherent reasoning paths. Inspired by the Language of Thought Hypothesis, which posits that human reasoning operates over a symbolic, compositional mental language called Mentalese, we introduce a framework that trains models to reason in a similarly compact style. Mentalese encodes abstract reasoning as ultra-compressed, structured tokens, enabling models to solve complex problems with far fewer steps. To improve both efficiency and accuracy, we propose SHORTER LENGTH PREFERENCE OPTIMIZATION (SLPO), a reinforcement learning method that rewards concise solutions that stay correct, while still allowing longer reasoning when needed. Applied to Mentalese-aligned models, SLPO yields significantly higher compression rates by enabling concise reasoning that preserves the benefits of detailed thinking without the computational overhead. Across benchmarks including AIME 2024 and 2025, MinervaMath, OlympiadBench, Math500, and AMC, our ORION models produce reasoning traces with 4-16x fewer tokens, achieve up to 5x lower inference latency, and reduce training costs by 7-9x relative to the DeepSeek R1 Distilled model, while maintaining 90-98% of its accuracy. ORION also surpasses Claude and ChatGPT-4o by up to 5% in accuracy while maintaining 2x compression. These results show that Mentalese-style compressed reasoning offers a step toward human-like cognitive efficiency, enabling real-time, cost-effective reasoning without sacrificing accuracy."
        },
        {
            "title": "Start",
            "content": "ORION: TEACHING LANGUAGE MODELS TO REASON EFFICIENTLY IN THE LANGUAGE OF THOUGHT Kumar Tanmay1 Kriti Aggarwal2 2Hippocratic AI 1Harvard University Paul Pu Liang3 3Massachusetts Institute of Technology Subhabrata Mukherjee"
        },
        {
            "title": "ABSTRACT",
            "content": "Large Reasoning Models (LRMs) achieve state-of-the-art performance in mathematics, code generation, and task planning. However, their reliance on long chains of verbose thinking tokens results in high latency, redundancy, and incoherent reasoning paths. Inspired by the Language of Thought Hypothesis which posits that human reasoning operates over symbolic, compositional mental language called Mentalesewe introduce cognitively motivated framework that trains models to reason in similar compact style. Mentalese encodes abstract reasoning as ultra-compressed, structured tokens, enabling models to solve complex problems with far fewer steps. To achieve both efficiency and accuracy, we propose SHORTER LENGTH PREFERENCE OPTIMIZATION (SLPO), reinforcement learning method that directly optimizes models to generate concise yet correct reasoning by rewarding shorter solutions that maintain high accuracy while flexibly allowing longer reasoning when complexity demands it. When applied to Mentalese-aligned models, SLPO achieves much larger compression rates by enabling compressed reasoning that preserves the benefits of detailed thinking without the computational overhead, allowing us to present the best-performing models at each compression level along the performance-efficiency Pareto frontier. Across mathematical benchmarks including AIME 2024 & 2025, MinervaMath, OlympiadBench, Math500, and AMC our ORION models generate reasoning traces with 416 fewer tokens, achieve up to 5 lower inference latency, and reduce training costs by 79 relative to the base DeepSeek R1 Distilled model, while maintaining 90-98% of the baseline accuracy. ORION models also surpass Claude and ChatGPT-4o by up to 5% in accuracy while maintaining 2 compression. Our findings demonstrate Mentalese-style compressed reasoning offers breakthrough toward human-like cognitive efficiency, opening new possibilities for real-time, cost-effective reasoning without sacrificing accuracy. 1 5 2 0 2 8 2 ] . [ 1 1 9 8 2 2 . 1 1 5 2 : r Figure 1: Performance-efficiency trade-offs of various model families across six mathematical reasoning benchmarks (including AIME2025). The dotted curve indicates the Pareto frontier, which illustrates the trade-off between higher compression rates and loss in accuracy. Our proposed method, combining Mentalese alignment with SLPO, consistently lies on this frontier, identifying an optimal operating point that achieves balance between accuracy and efficiency. Work done kriti@hippocraticai.com. during internship at Hippocratic AI. Correspondence to: kumartanmay@fas.harvard.edu, 1 Codebase will be released soon here: https://github.com/Hippocratic-AI-Research/Orion"
        },
        {
            "title": "Under review as a conference paper",
            "content": ""
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent advances such as OpenAI o1 (OpenAI et al., 2024b) and DeepSeek R1 (DeepSeek-AI et al., 2025) have reshaped how we think about language model reasoning. By letting models think before they answer, these systems dramatically improved credibility and performanceachievements that were once thought impossible for LLMs (Wu et al., 2024). Explicit reasoning has thus emerged as central focus of LLM research (Xu et al., 2025). Recent work such as DeepScaleR: Surpassing o1-Preview with 1.5B Model by Scaling RL demonstrates that even relatively small models (1.5B parameters) can outperform OpenAIs O1-Previewwhich is widely assumed to be significantly larger, though its scale has not been publicly disclosedby leveraging reasoning-focused reinforcement learning techniques such as RLVR, where models generate intermediate thinking tokens for self-verification (Luo et al., 2025b). This finding underscores that scaling in reasoning depth can, in some contexts, rival scaling in parameter size. The key challenge now lies in transforming this promise into robust, efficient, and trustworthy deployments, which we address in the next section. However, the promise of RLVR comes with significant trade-offs. Training is computationally expensive, with rollout generation leaving GPUs idle for long periods (Fu et al., 2025). Even relatively small models such as 1.5B parameters can take days to train under RL fine-tuning regimes (Zheng et al., 2025). Moreover, R1-style reasoning traces (shown in Figure 2) are often verbose, redundant, and unnatural far cry from human cognition, which tends to rely on short and efficient thought steps (Sui et al., 2025). Building on the Language of Thought hypothesis (Fodor, 1975), which suggests that human cognition unfolds through short compositional thought units rather than verbose natural language traces, we propose training framework that restructures the reasoning style of current reasoning-oriented LLMs in symbolic internal language that we call Mentalese. Figure 2: Contrast between human and machine reasoning (response from DeepSeek-R1). While humans arrive at intuitive and concise solutions, LLMs often produce verbose and redundant reasoning chains even for simple problems. We bridge this gap by developing methods that encourage models to reason more like humansclear, efficient, and directwhile preserving accuracy. Grounded in the Language of Thought hypothesis, human reasoning compresses complex ideas into minimal symbolic steps, reflecting cognitive efficiency. Emulating this compact reasoning reduces redundancy in machine outputs, improving both interpretability and token efficiency. In our framework, models are first aligned with this reasoning process through supervised finetuning on reasoning traces in Mentalese, namely, concise compositional sequences that capture only the essential steps required for problem solving. However, aligning models to Mentalese by supervised fine-tuning alone leads to substantial drop in performance relative to the base model. To overcome this, we introduce Shorter Length Preference Optimization (SLPO), reinforcement learning objective with verifiable reward that balances brevity and correctness. Unlike tokenpenalization methods (e.g., L1-style objectives (Aggarwal & Welleck, 2025)) that impose arbitrary length budgetsoften forcing models to under-reason on difficult problems and over-reason on easy onesSLPO instead rewards relative efficiency: among correct rollouts, shorter solutions receive bonus. This naturally biases the model toward concise reasoning when tasks are simple, while still allowing it to allocate more steps when necessary in the same reasoning structure. By reward-"
        },
        {
            "title": "Under review as a conference paper",
            "content": "ing concise but correct solutions, SLPO avoids verbosity while recovering most of the performance lost during supervised fine-tuning on Mentalese, thereby yielding efficient reasoning that scales at inference time. To highlight both domain-specific performance and generalization, we evaluated our suite of trained models (ORION 1.5B) on mathematical reasoning in the domain and out-of-the-domain tasks such as GPQA, LSAT, and MMLU. Our ORION-AG-SLPO 1.5B surpasses GPT-4o (OpenAI et al., 2024a), Claude 3.5 Sonnet (Anthropic, 2024), and Llama 3.3 70B (Grattafiori et al., 2024) by an average of 6 pp2 in mathematical reasoning and outperforms DeepSeek-R1 1.5B (DeepSeek-AI et al., 2025) by 7 pp with 7 reduction in reasoning length (Figure 1). ORION-DS-GRPO 1.5B achieves 14 compression relative to DeepSeek-R1 1.5B. Beyond in-domain gains, our ORION models also generalize well: on out-of-domain tasks, they improve over the base model by 1 pp while achieving 15 compression  (Table 2)  . In addition to token efficiency, our experiments show that training with Mentalese stabilizes optimization and reduces training time by 7-9 compared to directly training the base model with RLVR, leading to substantial savings in training cost. Beyond benchmarks, we hypothesize that these ideas are especially relevant for agentic LLM systems, where reasoning models are rarely deployed due to latency and cost: verbose generations can overwhelm communication channels (Kim et al., 2025). compressed reasoning style, reinforced through SLPO, has the potential to dramatically reduce this overheadmaking reasoning-capable agents not only more accurate but also more practical to deploy in real-world settings. Our main contributions are as follows: Reasoning compression framework. We propose novel and efficient reasoning compression framework via Mentalese alignment for restructuring the reasoning style of current LLM, producing compact yet faithful symbolic reasoning. Reward function. We propose Shorter Length Preference Optimization (SLPO), an adaptive objective that dynamically balances correctness with brevity, eliminating the need for rigid length penalties. Dataset. We release MentaleseR-40k, dataset of ultra-compressed reasoning traces for 40k math problems, generated under symbolic constraints inspired by the Language of Thought Hypothesis (LOTH), to support future developments and foster research on efficient reasoning. Experiments and best practices. We conduct extensive evaluations and identify best practices to apply GRPO and SLPO, showing how they achieve different levels of compression and where each method is most effective."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Efficient Reasoning in Large Language Models. Since Wei et al. (2022b) demonstrated the effectiveness of chain-of-thought (CoT) prompting, subsequent work has focused on scaling testtime computation to improve performance in mathematical problem-solving, code generation, and complex reasoning tasks. Strategies include parallel sampling of multiple reasoning paths (Wang et al., 2022; Yue et al., 2024; Chen et al., 2023), tree search and planning (Yao et al., 2023; Besta et al., 2024), and iterative refinement methods (Madaan et al., 2023). Recent reasoningspecialized models, such as OpenAIs o1 (OpenAI et al., 2024b), DeepSeek-R1 (DeepSeek-AI et al., 2025), and Qwen-QwQ (Yang et al., 2025), internalize the ability to generate extended reasoning traces.However, these methods often suffer from the overthinking phenomenon (Sui et al., 2025; Chen et al., 2025), where models generate excessively long reasoning traces. While increased length can improve accuracy up to point (Wu et al., 2025), it also introduces redundancy, higher inference latency, and even accuracy degradation due to compounding errors (Hassid et al., 2025; Lee et al., 2025). This trade-off has motivated work on more efficient reasoning. RL-based post-training methods have been widely explored to control reasoning length. L1 (Aggarwal & Welleck, 2025) enforces user-specified budgets, DAST (Shen et al., 2025) adapts budgets based on problem difficulty, while O1-Pruner (Luo et al., 2025a) uses reference-based pruning. Other approaches, such as Kimi 1.5 (Team et al., 2025) and Training Efficient (Arora & Zanette, 2025), use sampled rollouts to reward shorter or average lengths. ShorterBetter (Yi et al., 2025) further introduces the idea of rewarding the shortest correct response, highlighting the existence of problem-dependent optimal 2pp = percentage points, denoting absolute differences between percentages (e.g., 22% vs. 16% = 6 pp)."
        },
        {
            "title": "Under review as a conference paper",
            "content": "reasoning lengths. Our work complements these by introducing SLPO, which adaptively prefers concise correct reasoning without penalizing necessary longer derivations, enabling over 10 compression with minimal loss in accuracy. Chain-of-Thought and Alternative Reasoning Formats. CoT reasoning has become dominant paradigm for enhancing reasoning in LLMs, either via prompting (Wei et al., 2022a; Fu et al., 2022; Zhou et al., 2022) or through post-training with supervised finetuning (Yue et al., 2023; Yu et al., 2023) and reinforcement learning (Trung et al., 2024; Shao et al., 2024b; Zhou et al., 2025). Theoretical analyses link CoT to increased expressivity and effective depth in transformers (Feng et al., 2023; Merrill et al., 2023; Li et al., 2024). However, natural-language CoT traces are verbose, redundant, and not always faithful to the models underlying reasoning process (Turpin et al., 2023; Chuang et al., 2024). Recent research has explored alternatives. Structured or symbolic CoT formats aim to compress reasoning into more compact representations, such as symbolic operators, patterns, or abstract primitives (Madaan & Yazdanbakhsh, 2022; Paul et al., 2024). Other works examine latent reasoning, where intermediate computation is implicit in hidden representations rather than externalized tokens (Yang et al., 2024; Biran et al., 2024; Shalev et al., 2024). Techniques such as back-patching (Biran et al., 2024), filler tokens (Pfau et al., 2024), or knowledge distillation into latent reasoning (Deng et al., 2023; 2024) push beyond explicit CoT. Our proposed Mentalese Chain-of-Thought builds on this line of work by introducing symbolic, cognitively motivated reasoning language inspired by the Language of Thought Hypothesis. By replacing verbose natural language with structured symbolic primitives, Mentalese CoT achieves order-of-magnitude compression while retaining faithfulness and interpretability. Combined with SLPO, this framework demonstrates that both representation and optimization are critical for efficient and reliable reasoning."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "In this section, we present our methodology, which integrates symbolic reasoning alignment with reinforcement learning for concise yet accurate performance. We introduce Mentalese, compact symbolic reasoning format, and Group Relative Policy Optimization (GRPO), group-based extension of PPO for reasoning optimization. Our main contribution, Shorter Length Preference Optimization (SLPO), refines GRPO by rewarding brevity without penalizing necessary longer reasoning. Finally, we propose RLVR, two-stage pipeline that first aligns models to Mentalese via supervised finetuning, then applies GRPO or SLPO with verifier feedback. Together, these components yield 10-20 compression in reasoning traces while maintaining accuracy and efficiency across benchmarks. 3.1 MENTALESE: MENTAL LANGUAGE OF THOUGHT We first introduce Mentalese, cognitively motivated reasoning format inspired by the Language of Thought Hypothesis (Fodor, 1975; Rescorla, 2024). According to this hypothesis, human cognition operates not directly in natural language, but in an internal representational system characterized by compact, symbolic structures. Translating this perspective to Large Reasoning Models (LRMs), we hypothesize that verbose natural language explanations commonly used in chain-of-thought prompting especially the DeepSeek R1 reasoning style, are not essential for reasoning, and that more efficient symbolic primitives can better capture the core logical operations underlying problem-solving. Formal definition. Let be finite set of operators (e.g., SET, EQ, CASE, SOLVE, CALC, DIFF, ANS) and let be the set of symbolic expressions over variables, numbers, and function symbols (e.g., +, , , , abs). Mentalese step is pair st = (ot, ct) with ot and expression ct rendered as the string OPERATION:expression;. Mentalese trace for question is finite sequence = (s1; . . . ; sT ) that is well-typed and executable under the step semantics below and that culminates in exactly one terminal ANS:e; step. The boxed final answer is e, where is the value denoted by e. We denote the set of valid traces by M. Unlike traditional CoT, which uses free-form text, Mentalese encodes reasoning in canonical steps of the form OPERATION:expression;, joined by semicolons to form minimal yet complete traces. This yields three advantages: (i) Compression eliminating redundant tokens for up to"
        },
        {
            "title": "Under review as a conference paper",
            "content": "10 shorter reasoning; (ii) Faithfulness each step is necessary and sufficient; (iii) Cognitive alignment resembling structured mental representations rather than verbose text. To build MENTALESE-40K, we adapted the DEEPSCALER-PREVIEW-DATASET (Luo et al., 2025b), covering 40k+ math problems from AIME (1983-2023), Omni-Math, and STILL. We used GPT-4.1 with structured prompting framework (Figure 3)including formal definition, syntactic rules, and examplesto generate Mentalese traces. After light curation (removing 65 malformed cases), the resulting dataset was used for supervised fine-tuning. For RLVR, we instead relied on the original QA pairs, letting the verifier assess correctness while optimizing for concise reasoning. Refer Appendix A.3 for some of the samples from MENTALESER-40K. Figure 3: Illustration of symbolic, logic-based chain of thought (mentalese). This figure shows the definition (top), an example of symbolic reasoning steps (left) with rules governing the reasoning style (right). 3.2 GROUP RELATIVE POLICY OPTIMIZATION (GRPO) While PPO (Schulman et al., 2017) provides strong baseline for policy optimization, it operates at the single-sample level: each rollout is evaluated independently using value function to estimate its advantage. However, in reasoning tasks where multiple candidate solutions can be generated for the same question, evaluating rollouts in isolation discards useful information about the relative quality of responses within group. For example, if model generates five candidate solutions, some correct and some incorrect, we are less interested in their absolute values than in how each compares relative to others in the same set. This motivates Group Relative Policy Optimization (GRPO) (Shao et al., 2024a), which eliminates the explicit value function and instead estimates the advantage by normalizing rewards within groups of samples drawn for the same prompt. Concretely, for questionanswer pair (q, a), we sample group of responses {oi}G i=1 from the current policy. The reward of each response ri is converted into group-relative advantage via normalization: ˆAi = ri mean({rj}G j=1) + ϵ std({rj}G j=1) . This design ensures that advantages highlight which responses are better or worse relative to the group, rather than depending on an absolute critic model."
        },
        {
            "title": "Under review as a conference paper",
            "content": "GRPO then optimizes clipped surrogate objective similar to PPO but with directly imposed KL penalty: (cid:34) JGRPO(θ) = (cid:88)"
        },
        {
            "title": "1\nG",
            "content": "1 oi t=1 where ri,t(θ) = πθ(oi,tq,oi,<t) i=1 πold(oi,tq,oi,<t) is the token-level importance ratio. oi (cid:88) (cid:16) min (cid:0)ri,t(θ) ˆAi,t, clip(ri,t(θ), 1ε, 1+ε) ˆAi,t (cid:17) (cid:1)βDKL(πθπref ) (cid:35) ,"
        },
        {
            "title": "3.3 SHORTER LENGTH PREFERENCE OPTIMIZATION (SLPO)",
            "content": "There has been growing interest in adaptive reasoning methods that redefine the GRPO formulation by incorporating explicit thinking budgets. For instance, prior works such as LCPO constrain reasoning lengths by enforcing unnatural fixed or maximum token budgets. More recently, grouprelative formulations have also been proposed that define rewards based on the relative lengths of responses within group. However, these methods tend to be overly rigid: they over-penalize the longest solutions even when they converge to right answer, and in cases where no correct solution exists, their length-normalization can still distort the reward landscape. This strictness can suppress necessary reasoning and lead to degenerate behavior. To overcome these issues, we introduce Shorter Length Preference Optimization (SLPO), reinforcement learning strategy that balances conciseness with correctness softly. Crucially, SLPO never penalizes correct but necessarily long reasoning when it is the only valid option, and it does not distort rewards in cases with no correct solution. Instead, it adaptively rewards shorter correct traces when multiple valid derivations exist, while preserving correctness as the primary training signal. Different problems naturally require different amounts of reasoning. For example, simple arithmetic task such as 2 + 2 requires no intermediate steps, while Olympiad-style geometry problems demand much longer derivations. reward function that ignores this variability either pushes the model toward artificially verbose chains (reward hacking under fixed budgets) or toward overly terse and often incorrect responses (under strict length penalties). SLPO resolves this by defining preferences relative to the observed range of correct reasoning lengths for each problem instance. Formally, for given rollout group G(xi) = {y1, y2, . . . , yn} corresponding to prompt xi, let C(xi) = {yj G(xi) : Rcorrectness(yj) = 1} denote the set of correct responses. We define: ℓ(y), Lmin = min ℓ(y), (1) yC(xi) Lmax = max yC(xi) where ℓ(y) is the token length of response y. The total reward for candidate ycurr is then: RSLPO(ycurr) = 1, if C(xi) = 1 or (cid:0)C(xi) > 1 & Lmin = Lmax (cid:1), Rcorrectness + α Lmax Lcurr Lmax Lmin , if C(xi) > 1 and Lmin = Lmax, 0, if C(xi) = 0, (2) where Lcurr is the length of the current response, Rcorrectness {0, 1} is determined by verifier, and α controls the trade-off between accuracy and conciseness. Larger values of α emphasize brevity, whereas smaller values prioritize correctness regardless of length. In all experiments, we set α = 0.1, which provided stable balance across benchmarks. By construction, SLPO avoids the failure modes of previous group-relative and L1-based formulations: it does not over-penalize long but uniquely correct solutions, and it does not distort reward landscapes when no valid solutions exist. Instead, it consistently encourages models to discover the shortest correct reasoning trace whenever possible. This makes SLPO especially well-suited for mathematical reasoning, where optimal reasoning lengths vary significantly across problems. 3.4 MENTALESE ALIGNMENT THROUGH SFT FOLLOWED BY RLVR We now present our complete training pipeline, which consists of two stages: supervised alignment on Mentalese traces, followed by reinforcement learning with verifiable rewards (RLVR)."
        },
        {
            "title": "Under review as a conference paper",
            "content": "Stage 1: Supervised Finetuning on Mentalese. Let = {(qi, ai, mi)}M i=1 be our dataset with question qi, ground-truth final answer ai, and Mentalese reasoning trace mi. Each training prompt is structured as: τ (qi) = qi + Lets think step-by-step and answer within boxed{}. with target output as: = <think> mi </think> boxed{ai}. Starting from pretrained base model π0, we obtain Mentalese-aligned model πSFT via supervised finetuning: πSFT = arg min θ 1 (cid:88) i=1 log πθ(y τ (qi)). Stage 2: Reinforcement Learning with Verifier Rewards (RLVR). The SFT model πSFT is further refined using verifier-based reinforcement learning. For each question qi, the policy generates candidates G(qi) = {y(1), . . . , y(N )}; verifier checks the boxed answer ˆa against ai and assigns correctness reward Racc {0, 1}. The clipped surrogate objective with KL regularization is: JRLVR(θ) = EqD, yπθ(q) (cid:104) (cid:105) min (cid:0)rθ(y) ˆA(y), clip(rθ(y), 1ε, 1+ε) ˆA(y)(cid:1)βDKL(πθ πSFT) , where rθ(y) = πθ(yq) previous subsections). πold(yq) and ˆA(y) is computed from either the GRPO or SLPO formulation (see Depending on the chosen reward function, RLVR yields policy πGRPO or πSLPO: πSFT RLVR (GRPO) πGRPO, πSFT RLVR (SLPO) πSLPO. SFT alignment anchors the model to compact single-chain reasoning format, ensuring that outputs conform to the Mentalese structure as shown in Figure 4. However, this alignment often comes at the cost of reduced accuracy, since the base model initially performs well with long and verbose reasoning chains. RLVR provides the complementary step: by instantiating it with either GRPO or SLPO, the model learns to recover accuracy while retaining the compact reasoning format. RLVR enables the model to refine and extend its reasoning inside the learned structure, adding useful steps when necessary but avoiding unnecessary verbosity. This combination not only restores the lost performance but also yields consistent improvements in overall reasoning efficiency and accuracy across benchmarks."
        },
        {
            "title": "4 EXPERIMENT DESIGN",
            "content": "Our experimental study is organized around five research questions. First, we ask how effective the proposed Mentalese representation is in compressing reasoning traces while preserving task performance. Second, we investigate which reinforcement learning algorithm (GRPO or SLPO) best recovers the performance gap introduced by compression. Third, we evaluate the standalone effectiveness of SLPO in balancing conciseness and correctness. Fourth, we explore best practices by identifying when each algorithm is most suitable, particularly across different regions of compression. Finally, we analyze the efficiency and stability of our RLVR methods during training. Models and Baselines. To evaluate our method, we consider both base models and competitive baselines. Our primary comparison point is DeepSeek-R1-Distill-Qwen-1.5B (DeepSeek-AI et al., 2025), distilled variant of Qwen-2.5-1.5B-Instruct fine-tuned on reasoning traces from DeepSeeks R1 model, which we denote as DEEPSEEK-R1-1.5B. We also include DeepScaleR1.5B-Preview (Luo et al., 2025b), the original release without length-control modifications, referred to as AGENTICA-24K. For completeness, we tested the base Qwen-2.5-Math-1.5B model (Qwen et al., 2025); however, it collapsed under RLVR fine-tuning due to NaN gradient norms, so it is excluded from final evaluations. In addition, we benchmark against L1-Max (Aggarwal & Welleck, 2025), strong baseline derived from AGENTICA-24K using Length-Controlled Policy Optimization (LCPO), which achieves more than 10 compression. While this approach effectively reduces verbosity by enforcing fixed token budget, it lacks adaptability to varying problem difficulties."
        },
        {
            "title": "Under review as a conference paper",
            "content": "Figure 4: Comparison of reasoning traces on AIME 2024. Agentica-24k model use approximately 7800 tokens, ORION-AG 150 tokens, and ORION-AG-SLPO 300 tokens, achieving similar accuracy. Beyond these 1.5B-scale models, we also report results from frontier-scale systems for context. Specifically, we include GPT-4o (OpenAI et al., 2024a), Claude 3.5 Sonnet (Anthropic, 2024), and LLaMA-3 70B-Instruct (Grattafiori et al., 2024) as strong reference points, situating our results relative to state-of-the-art closed-source and large-scale open-source LLMs. Evaluation and Metrics. We evaluate our models on five in-domain mathematical reasoning datasets: AIME 2024 (Mathematical Association of America, 2024), AIME 2025 (Mathematical Association of America, 2025), MATH-500 (Hendrycks et al., 2021b), AMC (American Mathematics Competitions , 2023), Minerva-Math (Lewkowycz et al., 2022), and OlympiadBench (He et al., 2024). Additionally, we test on three out-of-domain benchmarks: GPQA (Rein et al., 2023), LSAT (Zhong et al., 2023), and MMLU (Hendrycks et al., 2021a), in order to assess generalization beyond mathematical reasoning. We report results using three primary metrics. Pass@1 measures the fraction of problems correctly solved under single-sample decoding, i.e., the proportion of test questions for which the model produces correct solution on its first attempt. Token Length denotes the average number of tokens generated per response on given benchmark, computed by averaging output lengths across all test questions. Compression Rate (CR) quantifies the degree of response shortening relative to DeepSeek-R1-1.5B, with higher values indicating greater compression (e.g., CR of 10 means the models responses are ten times shorter on average). Full formal definitions are provided in Appendix A.1. Implementation Details. For supervised fine-tuning on the MENTALESER-40K dataset, we used LLAMA-FACTORY (Zheng et al., 2024), an open-source library for instruction tuning and posttraining. For reinforcement learning, we adopted VERL (Sheng et al., 2025), an open-source RL training library. We fine-tuned our 1.5B base models with batch size of = 128 and rollout group size of = 16. Training was conducted for 1500 steps with fixed learning rate of 1 106. For reinforcement learning experiments, we used 32 H100 GPUs, while supervised finetuning was performed on 8 H100 GPUs. Inference was accelerated using the VLLM (Kwon et al., 2023) engine, which enables efficient large-scale generation. For length constraints, we set different maximum generation lengths depending on the training setup: 8K tokens for direct SLPO on base models, 2K tokens for SLPO on MENTALESER-40K fine-tuned models, and 1K tokens for GRPO"
        },
        {
            "title": "Under review as a conference paper",
            "content": "Table 1: Model performance across benchmarks. Each block shows Pass@1, average tokens, and compression rate (CR). Model AIME2024 AIME2025 MATH AMC MINERVA OLYMPIAD P@1 Tokens CR P@1 Tokens CR P@1 Tokens CR P@1 Tokens CR P@1 Tokens CR P@1 Tokens CR L1 Max GPT-4o Claude 3.5 Sonnet Llama3.3 70B DeepSeek R1 1.5b + SLPO (Direct) ORION-DS ORION-DS-GRPO ORION-DS-SLPO Agentica-24k + SLPO (Direct) ORION-AG ORION-AG-GRPO ORION-AG-SLPO 0.04 0.10 0.16 0.25 0.19 0.26 0.004 0.2 0.23 0.36 0.33 0.01 0.19 0.24 390 923 819 7481 6510 184 571 976 7049 4379 126 649 1307 19.18 8.11 9.14 6.72 1.00 1.15 40.65 13.10 7.66 1.06 1.71 59.37 11.52 5.72 0.03 0.06 0.03 0. 0.2 0.2 0.01 0.13 0.16 0.27 0.23 0.01 0.12 0.21 424 998 764 1067 7631 5910 136 524 785 7270 3889 126 541 971 Baselines 17.99 7.65 9.99 7.15 0.61 0.78 0.75 0.74 220 607 458 516 11.57 4.19 5.56 4.93 0.38 0.52 0.47 0.47 Base Models and our Finetuned Variants 1.00 1.29 56.10 14.56 9.72 1.05 1.95 60.56 14.11 7.86 0.77 0.78 0.38 0.75 0.61 0.86 0.83 0.40 0.77 0.81 2545 1017 156 301 68 1841 504 97 257 1.00 2.50 16.31 8.45 37.43 1.38 5.05 26.23 9.91 5.99 0.51 0.60 0.16 0.51 0.44 0.67 0.68 0.14 0.53 0.6 310 1003 775 845 6136 2797 168 459 4062 1696 128 493 966 19.79 6.12 7.92 7.26 1.00 2.19 36.52 13.37 13.98 1.51 3.59 47.94 12.44 6.35 0.17 0.49 0.46 0.36 0.23 0.24 0.12 0.26 0. 0.29 0.27 0.13 0.26 0.29 190 837 694 736 4115 1393 147 317 86 3466 640 119 266 542 21.66 4.92 5.93 5.59 1.00 2.96 27.99 12.98 47. 1.19 6.43 34.58 15.47 7.59 0.24 0.42 0.36 0.36 0.35 0.41 0.12 0.36 0.32 0.47 0.46 0.12 0.37 0.43 290 956 701 780 6858 3031 157 439 4377 1668 131 453 889 23.65 7.17 9.78 8.79 1.00 2.26 43.68 15.68 26.68 1.57 4.11 52.34 15.13 7.71 on MENTALESER fine-tuned models. During evaluation on benchmarks, we fixed maximum generation budget of 8K tokens. More details are provided in Appendix A.2. Table 2: Model performance on out-of-domain (OOD) benchmarks GPQA, LSAT, MMLUPRO and Average. Each block shows Pass@1, average tokens, and compression rate (CR). Model / Training Regime GPQA LSAT MMLU-1000 Average P@1 Tokens CR P@1 Tokens CR P@1 Tokens CR P@1 Tokens CR Baselines L1 Max 0.29 518 14. 0.22 554 14.79 0.45 288 2. 0.32 453 10.61 DeepSeek R1 1.5b + SLPO (Direct) ORION-DS ORION-DS-GRPO ORION-DS-SLPO Agentica-24k + SLPO (Direct) ORION-AG ORION-AG-GRPO ORION-AG-SLPO Base Models and our Finetuned Variants 7551 3434 142 366 393 5010 2101 135 362 735 1.00 2.20 53.21 20.64 19. 1.51 3.59 55.93 20.85 10.28 0.24 0.25 0.2 0.23 0.23 0.29 0.28 0.21 0.23 0.25 8192 5608 316 587 851 7614 4077 201 564 1200 1.00 1.46 25.92 13.95 9. 1.08 2.01 40.76 14.52 6.83 0.45 0.45 0.35 0.46 0.43 0.48 0.48 0.35 0.47 0.48 0.31 0.33 0.25 0.31 0.32 0.36 0.33 0.26 0.30 0.33 710 490 67 166 934 390 62 157 282 1.00 1.45 10.60 4.28 14.49 0.76 1.82 11.45 4.52 2.52 0.33 0.34 0.27 0.33 0.33 0.38 0.36 0.27 0.33 0.35 5484 3177 175 373 4519 2189 133 361 739 1.00 1.70 29.91 12.96 14.45 1.12 2.47 36.05 13.30 6.54 Figure 5: This figure compares direct SLPO on the base model with Intermediate SFT followed by RLHF methods (SLPO/GRPO) on the MENTALESER-40K dataset across five metrics. The Mentalese alignment yields greater training stability and efficiency: (1) Response Length reveals direct SLPO collapses due to gradient instability, while ORION models stay stable; (2) Clip Ratio indicates more controlled updates in Mentalese methods, driven by reduced response truncation.; (3) Entropy Loss reflects better exploration-exploitation balance; (4) Training Time per RL Step shows higher computational efficiency; (5) Test Performance on AIME 2024 (22% Pass@1) confirms ORION models outperform direct SLPO on the base model. Shaded regions denote min-max ranges across runs. These results highlight the importance of structured intermediate representations (Mentalese) for stable, efficient RL in large language models."
        },
        {
            "title": "5 DISCUSSION",
            "content": "Loss During SFT, Recovery Through RLVR. As shown in Table 1, we observed substantial performance drop after the SFT stage on MENTALESE, with average accuracy decreasing by 35.5 p.p. relative to the base model. This decline stems from the fact that SFT encourages the model to restructure reasoning into symbolic MENTALESE format, typically resulting in single linear reasoning path. In contrast, DeepSeek R1-style reasoning traces often include forking tokens such as wait, but, or so, which allow the model to self-verify and revise its reasoning mid-generationboosting accuracy through exploratory pathways. The strict structure imposed by SFT sacrifices these benefits, limiting the models flexibility and test-time scaling. However, applying RLVR largely reverses this effect: models regain most of the lost accuracy while maintaining significantly shorter reasoning tracestypically just few hundred tokens longer than their SFT outputs. This highlights the complementary roles of the two stages: SFT enforces symbolic conciseness, while RLVR restores accuracy by reintroducing adaptive reasoning behaviors within that compact framework. Training Time Efficiency. Large-scale reinforcement learning typically demands substantially more computational resources than supervised fine-tuning. In our experiments, we found that applying RLVR directly on the base models required 5-6 days of training on our dataset of 40k samples for 1500 RL steps. This inefficiency arises primarily from the generation of long reasoning chains (often exceeding 8k tokens), which introduces high latency in the VLLM inference engine and becomes the main bottleneck of RLVR training. The cost further increases with larger rollout group sizes. As shown in Figure 5, introducing an intermediate supervised fine-tuning stage on the MENTALESER dataset significantly reduced training cost by 7-10, while achieving performance close to the base model but with 10 shorter reasoning traces. This demonstrates that aligning models to more compact reasoning language before RL training not only improves efficiency but also provides scalable mechanism for reinforcement learning in reasoning tasks. Training Collapse Under Direct SLPO. As shown in Figure 5, applying SLPO directly to Initially, AGENTICA-24K resulted in sudden collapse after approximately 300 training steps. the response length decreased by nearly half while accuracy improved marginally. However, beyond this point, the average response length rapidly expanded to the maximum generation limit (8k tokens), and the gradient norm curve exhibited NaN values. This instability ultimately caused the training to collapse, highlighting the difficulty of applying SLPO on raw base models without intermediate alignment. In contrast, introducing an intermediate SFT stage on the MENTALESER dataset maintained stability throughout the entire training process, underscoring the reliability of our proposed two-stage approach. Reversion to Verbose Reasoning Under Large Generation Budgets During RLVR. We observed that when the maximum generation length was set to 4k or 8k tokens, the model tended to drift away from the compact reasoning style learned during the SFT stage and revert to its original base behavior of producing verbose chains. In some cases, this even led to model collapse (Figure 5). likely explanation is that longer reasoning traces, although verbose, occasionally produced correct answers and were therefore rewarded, inadvertently steering the model away from the MENTALESE format. To mitigate this effect, we restricted the maximum generation length to 1k tokens for GRPO-based training and 2k tokens for SLPO-based training. These limits preserved the symbolic reasoning behavior acquired during SFT while still allowing sufficient space for problem-solving."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We introduced cognitively inspired framework for efficient reasoning that combines Mentalese, compact symbolic reasoning format, with Shorter Length Preference Optimization (SLPO), reinforcement learning strategy that adaptively balances conciseness and correctness. Our model achieves over 10 compression in reasoning traces while maintaining accuracy close to that of verbose large reasoning models, reducing both training and inference costs significantly. By aligning models toward concise and structured reasoning, we provide pathway for deploying large reasoning capabilities within real-time and resource-constrained environments. Our results suggest that reasoning does not inherently require verbosity, and that carefully designed representations and optimization objectives can yield models that reason more like humanssymbolically, compositionally, and efficiently. This is particularly valuable for agentic systems, where efficient and reliable decision-making is critical, and inference overhead can quickly become bottleneck."
        },
        {
            "title": "ACKNOWLEDGMENT",
            "content": "We would like to express our sincere gratitude to Munjal Shah, Debajyoti Datta, Bibek Paudel, Markel Sanz Ausin, Tanmay Laud, Kumar Ayush, Ayush Agrawal, and Sanchit Ahuja for their valuable feedback and insightful discussions that greatly contributed to this work."
        },
        {
            "title": "REFERENCES",
            "content": "Pranjal Aggarwal and Sean Welleck. L1: Controlling how long reasoning model thinks with In Second Conference on Language Modeling, 2025. URL https: reinforcement learning. //openreview.net/forum?id=4jdIxXBNve. American Mathematics Competitions . Maa invitational competitions. https://maa.org/ student-programs/amc//, 2023. Accessed: 2025-09-24. Anthropic. Claude 3.5 Sonnet. https://www.anthropic.com/news/claude-3-5sonnet, 2024. Accessed: YYYY-MM-DD. Daman Arora and Andrea Zanette. Training language models to reason efficiently, 2025. URL https://arxiv.org/abs/2502.04463. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoefler. Graph of thoughts: Solving elaborate problems with large language models. Proceedings of the AAAI Conference on Artificial Intelligence, 38(16):17682-17690, March 2024. ISSN 2159-5399. doi: 10.1609/aaai.v38i16.29720. URL http://dx.doi.org/10.1609/ aaai.v38i16.29720. Eden Biran, Daniela Gottesman, Sohee Yang, Mor Geva, and Amir Globerson. Hopping too late: Exploring the limitations of large language models on multi-hop queries, 2024. URL https: //aclanthology.org/2024.emnlp-main.781/. Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks, 2023. URL https: //arxiv.org/abs/2211.12588. Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. Do not think that much for 2+3=? on the overthinking of o1-like llms, 2025. URL https://arxiv. org/abs/2412.21187. Yifan Chuang, Gaoang Wang, Chen Chang, Ruoyu Tang, Shichuan Zhong, Fei Yang, Min Du, Xin Cai, and Xuanjing Hu. Faithlm: Towards faithful explanations for large language models, 2024. URL https://arxiv.org/abs/2402.04678. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong"
        },
        {
            "title": "Under review as a conference paper",
            "content": "Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Yuntian Deng, Kiran Prasad, Roland Fernandez, Paul Smolensky, Vishrav Chaudhary, and Stuart Shieber. Implicit chain of thought reasoning via knowledge distillation, 2023. URL https: //arxiv.org/abs/2311.01460. Yuntian Deng, Yejin Choi, and Stuart Shieber. From explicit cot to implicit cot: Learning to internalize cot step by step, 2024. URL https://arxiv.org/abs/2405.14838. Xiaolan Feng, Zhilin Yu, Hongkai Wang, et al. Chain-of-thought reasoning in transformers: theoretical perspective, 2023. http://arxiv.org/abs/2503.08679. Jerry Fodor. The language of thought, volume 5. Harvard university press, 1975. Wei Fu, Jiaxuan Gao, Xujie Shen, Chen Zhu, Zhiyu Mei, Chuyi He, Shusheng Xu, Guo Wei, Jun Mei, Jiashu Wang, Tongkai Yang, Binhang Yuan, and Yi Wu. Areal: large-scale asynchronous reinforcement learning system for language reasoning, 2025. URL https://arxiv.org/ abs/2505.24298. Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. Complexity-based prompting for multi-step reasoning, 2022. URL https://arxiv.org/abs/2210.00720. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Michael Hassid, Gabriel Synnaeve, Yossi Adi, and Roy Schwartz. Dont overthink it: Preferring shorter thinking chains for improved llm reasoning, 2025. URL https://arxiv.org/abs/ 2505.17813. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems, 2024. URL https://arxiv.org/abs/2402.14008. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding, 2021a. URL https: //arxiv.org/abs/2009.03300. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset, 2021b. URL https://arxiv.org/abs/2103.03874. Jiin Kim, Byeongjun Shin, Jinha Chung, and Minsoo Rhu. The cost of dynamic reasoning: Demystifying ai agents and test-time scaling from an ai infrastructure perspective, 2025. URL https://arxiv.org/abs/2506.04301. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention, 2023. URL https://arxiv.org/abs/2309.06180."
        },
        {
            "title": "Under review as a conference paper",
            "content": "Ayeong Lee, Ethan Che, and Tianyi Peng. How well do llms compress their own chain-of-thought? token complexity approach, 2025. URL https://arxiv.org/abs/2503.01141. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in neural information processing systems, 35:38433857, 2022. Zhaoyi Li, Gangwei Jiang, Hong Xie, Linqi Song, Defu Lian, and Ying Wei. Understanding and patching compositional reasoning in llms, 2024. URL https://aclanthology.org/ 2024.findings-acl.576/. Haotian Luo, Li Shen, Haiying He, Yibo Wang, Shiwei Liu, Wei Li, Naiqiang Tan, Xiaochun Cao, and Dacheng Tao. O1-pruner: Length-harmonizing fine-tuning for o1-like reasoning pruning, 2025a. URL https://arxiv.org/abs/2501.12570. Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Deepscaler: https://pretty-radioColin Cai, Jeffrey Luo, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Surpassing o1-preview with 1.5b model by scaling rl. b75.notion.site/DeepScaleR-Surpassing-O1-Preview-with-a-1-5BModel-by-Scaling-RL-19681902c1468005bed8ca303013a4e2, 2025b. Notion Blog. Aman Madaan and Amir Yazdanbakhsh. Text and patterns: For effective chain of thought, it takes two to tango, 2022. URL https://arxiv.org/abs/2209.07686. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Selfrefine: Iterative refinement with self-feedback, 2023. URL https://arxiv.org/abs/ 2303.17651. Mathematical Association of America. Maa invitational competitions. https://maa.org/ maa-invitational-competitions/, 2024. Accessed: 2025-09-24. Mathematical Association of America. Maa invitational competitions. https://maa.org/ maa-invitational-competitions/, 2025. Accessed: 2025-09-24. William Merrill, Florian Tramèr, et al. On the expressive power of transformers with chain-ofthought, 2023. https://arxiv.org/pdf/2310.07923. OpenAI, :, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander adry, Alex BakerWhitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex Nichol, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexander Kirillov, Alexi Christakis, Alexis Conneau, Ali Kamali, Allan Jabri, Allison Moyer, Allison Tam, Amadou Crookes, Amin Tootoochian, Amin Tootoonchian, Ananya Kumar, Andrea Vallone, Andrej Karpathy, Andrew Braunstein, Andrew Cann, Andrew Codispoti, Andrew Galu, Andrew Kondrich, Andrew Tulloch, Andrey Mishchenko, Angela Baek, Angela Jiang, Antoine Pelisse, Antonia Woodford, Anuj Gosalia, Arka Dhar, Ashley Pantuliano, Avi Nayak, Avital Oliver, Barret Zoph, Behrooz Ghorbani, Ben Leimberger, Ben Rossen, Ben Sokolowsky, Ben Wang, Benjamin Zweig, Beth Hoover, Blake Samic, Bob McGrew, Bobby Spero, Bogo Giertler, Bowen Cheng, Brad Lightcap, Brandon Walkin, Brendan Quinn, Brian Guarraci, Brian Hsu, Bright Kellogg, Brydon Eastman, Camillo Lugaresi, Carroll Wainwright, Cary Bassin, Cary Hudson, Casey Chu, Chad Nelson, Chak Li, Chan Jun Shern, Channing Conger, Charlotte Barette, Chelsea Voss, Chen Ding, Cheng Lu, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christina Kim, Christine Choi, Christine McLeavey, Christopher Hesse, Claudia Fischer, Clemens Winter, Coley Czarnecki, Colin Jarvis, Colin Wei, Constantin Koumouzelis, Dane Sherburn, Daniel Kappler, Daniel Levin, Daniel Levy, David Carr, David Farhi, David Mely, David Robinson, David Sasaki, Denny Jin, Dev Valladares, Dimitris Tsipras, Doug Li, Duc Phong Nguyen, Duncan Findlay, Edede Oiwoh, Edmund Wong, Ehsan Asdar, Elizabeth Proehl, Elizabeth Yang, Eric Antonow,"
        },
        {
            "title": "Under review as a conference paper",
            "content": "Eric Kramer, Eric Peterson, Eric Sigler, Eric Wallace, Eugene Brevdo, Evan Mays, Farzad Khorasani, Felipe Petroski Such, Filippo Raso, Francis Zhang, Fred von Lohmann, Freddie Sulit, Gabriel Goh, Gene Oden, Geoff Salmon, Giulio Starace, Greg Brockman, Hadi Salman, Haiming Bao, Haitang Hu, Hannah Wong, Haoyu Wang, Heather Schmidt, Heather Whitney, Heewoo Jun, Hendrik Kirchner, Henrique Ponde de Oliveira Pinto, Hongyu Ren, Huiwen Chang, Hyung Won Chung, Ian Kivlichan, Ian OConnell, Ian OConnell, Ian Osband, Ian Silber, Ian Sohl, Ibrahim Okuyucu, Ikai Lan, Ilya Kostrikov, Ilya Sutskever, Ingmar Kanitscheider, Ishaan Gulrajani, Jacob Coxon, Jacob Menick, Jakub Pachocki, James Aung, James Betker, James Crooks, James Lennon, Jamie Kiros, Jan Leike, Jane Park, Jason Kwon, Jason Phang, Jason Teplitz, Jason Wei, Jason Wolfe, Jay Chen, Jeff Harris, Jenia Varavva, Jessica Gan Lee, Jessica Shieh, Ji Lin, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joanne Jang, Joaquin Quinonero Candela, Joe Beutler, Joe Landers, Joel Parish, Johannes Heidecke, John Schulman, Jonathan Lachman, Jonathan McKay, Jonathan Uesato, Jonathan Ward, Jong Wook Kim, Joost Huizinga, Jordan Sitkin, Jos Kraaijeveld, Josh Gross, Josh Kaplan, Josh Snyder, Joshua Achiam, Joy Jiao, Joyce Lee, Juntang Zhuang, Justyn Harriman, Kai Fricke, Kai Hayashi, Karan Singhal, Katy Shi, Kavin Karthik, Kayla Wood, Kendra Rimbach, Kenny Hsu, Kenny Nguyen, Keren Gu-Lemberg, Kevin Button, Kevin Liu, Kiel Howe, Krithika Muthukumar, Kyle Luther, Lama Ahmad, Larry Kai, Lauren Itow, Lauren Workman, Leher Pathak, Leo Chen, Li Jing, Lia Guy, Liam Fedus, Liang Zhou, Lien Mamitsuka, Lilian Weng, Lindsay McCallum, Lindsey Held, Long Ouyang, Louis Feuvrier, Lu Zhang, Lukas Kondraciuk, Lukasz Kaiser, Luke Hewitt, Luke Metz, Lyric Doshi, Mada Aflak, Maddie Simens, Madelaine Boyd, Madeleine Thompson, Marat Dukhan, Mark Chen, Mark Gray, Mark Hudnall, Marvin Zhang, Marwan Aljubeh, Mateusz Litwin, Matthew Zeng, Max Johnson, Maya Shetty, Mayank Gupta, Meghan Shah, Mehmet Yatbaz, Meng Jia Yang, Mengchao Zhong, Mia Glaese, Mianna Chen, Michael Janner, Michael Lampe, Michael Petrov, Michael Wu, Michele Wang, Michelle Fradin, Michelle Pokrass, Miguel Castro, Miguel Oom Temudo de Castro, Mikhail Pavlov, Miles Brundage, Miles Wang, Minal Khan, Mira Murati, Mo Bavarian, Molly Lin, Murat Yesildal, Nacho Soto, Natalia Gimelshein, Natalie Cone, Natalie Staudacher, Natalie Summers, Natan LaFontaine, Neil Chowdhury, Nick Ryder, Nick Stathas, Nick Turley, Nik Tezak, Niko Felix, Nithanth Kudige, Nitish Keskar, Noah Deutsch, Noel Bundick, Nora Puckett, Ofir Nachum, Ola Okelola, Oleg Boiko, Oleg Murk, Oliver Jaffe, Olivia Watkins, Olivier Godement, Owen Campbell-Moore, Patrick Chao, Paul McMillan, Pavel Belov, Peng Su, Peter Bak, Peter Bakkum, Peter Deng, Peter Dolan, Peter Hoeschele, Peter Welinder, Phil Tillet, Philip Pronin, Philippe Tillet, Prafulla Dhariwal, Qiming Yuan, Rachel Dias, Rachel Lim, Rahul Arora, Rajan Troll, Randall Lin, Rapha Gontijo Lopes, Raul Puri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza Zamani, Ricky Wang, Rob Donnelly, Rob Honsby, Rocky Smith, Rohan Sahai, Rohit Ramchandani, Romain Huet, Rory Carmichael, Rowan Zellers, Roy Chen, Ruby Chen, Ruslan Nigmatullin, Ryan Cheu, Saachi Jain, Sam Altman, Sam Schoenholz, Sam Toizer, Samuel Miserendino, Sandhini Agarwal, Sara Culver, Scott Ethersmith, Scott Gray, Sean Grove, Sean Metzger, Shamez Hermani, Shantanu Jain, Shengjia Zhao, Sherwin Wu, Shino Jomoto, Shirong Wu, Shuaiqi, Xia, Sonia Phene, Spencer Papay, Srinivas Narayanan, Steve Coffey, Steve Lee, Stewart Hall, Suchir Balaji, Tal Broda, Tal Stramer, Tao Xu, Tarun Gogineni, Taya Christianson, Ted Sanders, Tejal Patwardhan, Thomas Cunninghman, Thomas Degry, Thomas Dimson, Thomas Raoux, Thomas Shadwell, Tianhao Zheng, Todd Underwood, Todor Markov, Toki Sherbakov, Tom Rubin, Tom Stasi, Tomer Kaftan, Tristan Heywood, Troy Peterson, Tyce Walters, Tyna Eloundou, Valerie Qi, Veit Moeller, Vinnie Monaco, Vishal Kuo, Vlad Fomenko, Wayne Chang, Weiyi Zheng, Wenda Zhou, Wesam Manassra, Will Sheu, Wojciech Zaremba, Yash Patil, Yilei Qian, Yongjik Kim, Youlong Cheng, Yu Zhang, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, and Yury Malkov. Gpt-4o system card, 2024a. URL https://arxiv.org/abs/2410.21276. OpenAI, :, Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko, Alex Tachard Passos, Alexander Neitz, Alexander Prokofiev, Alexander Wei, Allison Tam, Ally Bennett, Ananya Kumar, Andre Saraiva, Andrea Vallone, Andrew Duberstein, Andrew Kondrich, Andrey Mishchenko, Andy Applebaum, Angela Jiang, Ashvin Nair, Barret Zoph, Behrooz Ghorbani, Ben Rossen, Benjamin Sokolowsky, Boaz Barak, Bob McGrew, Borys Minaiev, Botao Hao, Bowen Baker, Brandon Houghton, Brandon McKinzie, Brydon Eastman, Camillo Lugaresi, Cary Bassin, Cary Hudson, Chak Ming Li, Charles de Bourcy, Chelsea Voss, Chen Shen, Chong Zhang, Chris Koch, Chris Orsinger, Christopher Hesse, Claudia Fischer, Clive Chan, Dan Roberts, Daniel Kappler, Daniel Levy, Daniel Selsam, David Dohan, David Farhi, David Mely, David Robinson,"
        },
        {
            "title": "Under review as a conference paper",
            "content": "Dimitris Tsipras, Doug Li, Dragos Oprica, Eben Freeman, Eddie Zhang, Edmund Wong, Elizabeth Proehl, Enoch Cheung, Eric Mitchell, Eric Wallace, Erik Ritter, Evan Mays, Fan Wang, Felipe Petroski Such, Filippo Raso, Florencia Leoni, Foivos Tsimpourlas, Francis Song, Fred von Lohmann, Freddie Sulit, Geoff Salmon, Giambattista Parascandolo, Gildas Chabot, Grace Zhao, Greg Brockman, Guillaume Leclerc, Hadi Salman, Haiming Bao, Hao Sheng, Hart Andrin, Hessam Bagherinezhad, Hongyu Ren, Hunter Lightman, Hyung Won Chung, Ian Kivlichan, Ian OConnell, Ian Osband, Ignasi Clavera Gilaberte, Ilge Akkaya, Ilya Kostrikov, Ilya Sutskever, Irina Kofman, Jakub Pachocki, James Lennon, Jason Wei, Jean Harb, Jerry Twore, Jiacheng Feng, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joaquin Quiñonero Candela, Joe Palermo, Joel Parish, Johannes Heidecke, John Hallman, John Rizzo, Jonathan Gordon, Jonathan Uesato, Jonathan Ward, Joost Huizinga, Julie Wang, Kai Chen, Kai Xiao, Karan Singhal, Karina Nguyen, Karl Cobbe, Katy Shi, Kayla Wood, Kendra Rimbach, Keren Gu-Lemberg, Kevin Liu, Kevin Lu, Kevin Stone, Kevin Yu, Lama Ahmad, Lauren Yang, Leo Liu, Leon Maksin, Leyton Ho, Liam Fedus, Lilian Weng, Linden Li, Lindsay McCallum, Lindsey Held, Lorenz Kuhn, Lukas Kondraciuk, Lukasz Kaiser, Luke Metz, Madelaine Boyd, Maja Trebacz, Manas Joglekar, Mark Chen, Marko Tintor, Mason Meyer, Matt Jones, Matt Kaufer, Max Schwarzer, Meghan Shah, Mehmet Yatbaz, Melody Y. Guan, Mengyuan Xu, Mengyuan Yan, Mia Glaese, Mianna Chen, Michael Lampe, Michael Malek, Michele Wang, Michelle Fradin, Mike McClay, Mikhail Pavlov, Miles Wang, Mingxuan Wang, Mira Murati, Mo Bavarian, Mostafa Rohaninejad, Nat McAleese, Neil Chowdhury, Neil Chowdhury, Nick Ryder, Nikolas Tezak, Noam Brown, Ofir Nachum, Oleg Boiko, Oleg Murk, Olivia Watkins, Patrick Chao, Paul Ashbourne, Pavel Izmailov, Peter Zhokhov, Rachel Dias, Rahul Arora, Randall Lin, Rapha Gontijo Lopes, Raz Gaon, Reah Miyara, Reimar Leike, Renny Hwang, Rhythm Garg, Robin Brown, Roshan James, Rui Shu, Ryan Cheu, Ryan Greene, Saachi Jain, Sam Altman, Sam Toizer, Sam Toyer, Samuel Miserendino, Sandhini Agarwal, Santiago Hernandez, Sasha Baker, Scott McKinney, Scottie Yan, Shengjia Zhao, Shengli Hu, Shibani Santurkar, Shraman Ray Chaudhuri, Shuyuan Zhang, Siyuan Fu, Spencer Papay, Steph Lin, Suchir Balaji, Suvansh Sanjeev, Szymon Sidor, Tal Broda, Aidan Clark, Tao Wang, Taylor Gordon, Ted Sanders, Tejal Patwardhan, Thibault Sottiaux, Thomas Degry, Thomas Dimson, Tianhao Zheng, Timur Garipov, Tom Stasi, Trapit Bansal, Trevor Creech, Troy Peterson, Tyna Eloundou, Valerie Qi, Vineet Kosaraju, Vinnie Monaco, Vitchyr Pong, Vlad Fomenko, Weiyi Zheng, Wenda Zhou, Wes McCabe, Wojciech Zaremba, Yann Dubois, Yinghai Lu, Yining Chen, Young Cha, Yu Bai, Yuchen He, Yuchen Zhang, Yunyun Wang, Zheng Shao, and Zhuohan Li. Openai o1 system card, 2024b. URL https://arxiv.org/abs/2412.16720. Sushrut Paul, Jinbo Yu, Jip J. Dekker, Alexey Ignatiev, and Peter J. Stuckey. Formal explanations for neuro-symbolic ai, 2024. URL https://arxiv.org/abs/2410.14219. Jacob Pfau, William Merrill, and Samuel R. Bowman. Lets think dot by dot: Hidden computation in transformer language models, 2024. URL https://arxiv.org/abs/2404.15758. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: graduate-level google-proof q&a benchmark, 2023. URL https://arxiv.org/abs/2311.12022. Michael Rescorla. The Language of Thought Hypothesis. In Edward N. Zalta and Uri Nodelman (eds.), The Stanford Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford University, Summer 2024 edition, 2024. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. URL https://arxiv.org/abs/1707.06347. Yuval Shalev, Amir Feder, and Ariel Goldstein. Distributional reasoning in llms: Parallel reasoning processes in multi-hop reasoning, 2024. URL https://arxiv.org/abs/2406.13858."
        },
        {
            "title": "Under review as a conference paper",
            "content": "Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024a. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024b. URL https://arxiv.org/abs/2402.03300. Introduces Group Relative Policy Optimization (GRPO). Yi Shen, Jian Zhang, Jieyun Huang, Shuming Shi, Wenjing Zhang, Jiangze Yan, Ning Wang, Kai Wang, Zhaoxiang Liu, and Shiguo Lian. Dast: Difficulty-adaptive slow-thinking for large reasoning models, 2025. URL https://arxiv.org/abs/2503.04472. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, In ProHaibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. ceedings of the Twentieth European Conference on Computer Systems, EuroSys 25, pp. 12791297. ACM, March 2025. doi: 10.1145/3689031.3696075. URL http://dx.doi.org/10. 1145/3689031.3696075. Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Na Zou, Hanjie Chen, and Xia Hu. Stop overthinking: survey on efficient reasoning for large language models, 2025. URL https://arxiv.org/abs/ 2503.16419. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Weixin Xu, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y. Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, Zonghan Yang, and Zongyu Lin. Kimi k1.5: Scaling reinforcement learning with llms, 2025. URL https://arxiv.org/abs/2501.12599. Luong Trung, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, and Hang Li. ReFT: Reasoning In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Prowith reinforced fine-tuning. ceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 76017614, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.410. URL https://aclanthology. org/2024.acl-long.410/. Miles Turpin, Julian Michael, Ethan Perez, and Samuel R. Bowman. Language models dont always say what they think: Unfaithful explanations in chain-of-thought prompting, 2023. URL https: //arXiv.org/abs/2305.04388. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2022a. URL https://arxiv.org/abs/2201.11903. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022b."
        },
        {
            "title": "Under review as a conference paper",
            "content": "Tianhao Wu, Janice Lan, Weizhe Yuan, Jiantao Jiao, Jason Weston, and Sainbayar Sukhbaatar. Thinking llms: General instruction following with thought generation, 2024. URL https: //arxiv.org/abs/2410.10630. Yuyang Wu, Yikai Wang, Zichao Ye, Tengyu Du, Stefanie Jegelka, and Yiqing Wang. When more is less: Understanding chain-of-thought length in llms, 2025. URL https://arxiv.org/ abs/2502.07266. Fengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong Lan, Jiahui Gong, Tianjian Ouyang, Fanjin Meng, Chenyang Shao, Yuwei Yan, Qinglong Yang, Yiwen Song, Sijian Ren, Xinyuan Hu, Yu Li, Jie Feng, Chen Gao, and Yong Li. Towards large reasoning models: survey of reinforced reasoning with large language models, 2025. URL https://arxiv.org/abs/2501.09686. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. Sohee Yang, Elena Gribovskaya, Nora Kassner, Mor Geva, and Sebastian Riedel. Do large language models latently perform multi-hop reasoning? In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, 2024. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models, 2023. URL https://arxiv.org/abs/2305.10601. Jingyang Yi, Jiazheng Wang, and Sida Li. Shorterbetter: Guiding reasoning models to find optimal inference length for efficient reasoning, 2025. URL https://arxiv.org/abs/2504. 21370. Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models, 2023. URL https://arxiv.org/abs/2309.12284. Murong Yue, Jie Zhao, Min Zhang, Liang Du, and Ziyu Yao. Large language model cascades with mixture of thoughts representations for cost-efficient reasoning, 2024. URL https://arxiv. org/abs/2310.03094. Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning, 2023. URL https://arxiv.org/abs/2309.05653. Haizhong Zheng, Yang Zhou, Brian R. Bartoldson, Bhavya Kailkhura, Fan Lai, Jiawei Zhao, and Beidi Chen. Act only when it pays: Efficient reinforcement learning for llm reasoning via selective rollouts, 2025. URL https://arxiv.org/abs/2506.02177. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. arXiv preprint arXiv:2403.13372, 2024. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: human-centric benchmark for evaluating foundation models, 2023. URL https://arxiv.org/abs/2304.06364. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language models, 2022. URL https://arxiv.org/abs/2205.10625."
        },
        {
            "title": "Under review as a conference paper",
            "content": "Zijian Zhou, Ao Qu, Zhaoxuan Wu, Sunghwan Kim, Alok Prakash, Daniela Rus, Jinhua Zhao, Bryan Kian Hsiang Low, and Paul Pu Liang. Mem1: Learning to synergize memory and reasoning for efficient long-horizon agents. arXiv preprint arXiv:2506.15841, 2025."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 METRICS FORMULATION Pass@1: Defined as Pass@1 = 1 (cid:88) i= pi, (3) where pi denotes whether the i-th problem was solved correctly, and is the total number of test problems. Intuitively, Pass@1 measures the fraction of correctly solved problems under single-sample decoding. Token Length: The average number of tokens produced per response for given benchmark. We first compute the mean output length across all questions in the dataset, and then report the overall average. Compression Rate (CR): Defined as CR = LDeepSeek-R1-1.5B Lmodel , where denotes the average response length in tokens for given benchmark. higher CR indicates greater compression. For example, CR = 10 means that the model generates responses ten times shorter than DEEPSEEK-R1-1.5B. A.2 HYPERPARAMETERS Table 3: Key Hyperparameters for Supervised Fine-tuning. Parameter Cutoff Length Validation Split Fine-tuning Type Learning Rate LR Scheduler Warmup Steps Train Epochs Per Device Train Batch Gradient Accumulation Save Strategy Save Steps Eval Steps Template Precision Flash Attention Value 15000 0.05 Full 1e-6 Cosine 20 5 1 2 Steps 0.20 0.05 Description Data Configuration Maximum sequence length Fraction of data used for validation Model & Training Full parameter fine-tuning Optimizer learning rate Learning rate scheduling strategy Number of warmup steps Total number of training epochs Batch Configuration Training batch size per device Steps to accumulate gradients Evaluation & Saving Save checkpoints by steps Save every 20% of total steps Evaluate every 5% of total steps System Configuration DeepSeek-R1 BF16 FA2 Model template/format Mixed precision training Attention optimization"
        },
        {
            "title": "Under review as a conference paper",
            "content": "Table 4: Key Hyperparameters for RLVR Training. Parameter Value Description Train Batch Size Max Prompt Length Max Response Length Data Configuration 128 1024 8192(Direct SLPO), 2048 (SLPO), 1024 (GRPO) Batch size for training data Maximum length of input prompts Maximum length of generated responses Model & Training Learning Rate PPO Mini Batch Size 1e-6 64 Actor model learning rate Mini-batch size for PPO updates KL Loss Coefficient KL Loss Type 0.001 low_var_kl Weight for KL divergence loss Type of KL loss computation KL Divergence Control Temperature Number of Samples Total Epochs Rollout Configuration 0.6 16 5 Sampling temperature for generation Samples per prompt during training Training Schedule Number of training epochs A.3 MENTALESER-40K EXAMPLES Example 1 Instruction: The operation $otimes$ is defined for all nonzero numbers by $a otimes = frac{a^{2}}{b}$. Determine $[(1 otimes 2) otimes 3] - [1 otimes (2 otimes 3)]$. Mentalese: DEF: $otimes(a,b)=a^2/b$; CALC1: $x = 1 otimes 2 = 1/2$; CALC2: $y = otimes 3 = (1/2)^2/3 = 1/12$; CALC3: $z = 2 otimes 3 = 4/3$; CALC4: $w = 1 otimes = 1/(4/3) = 3/4$; SUB: $y-w = 1/12 - 3/4 = -8/12 = -2/3$; ANS: $boxed{-frac{2}{3}}$. Example Instruction: Doug constructs square window using $8$ equal-size panes of glass. The ratio of the height to width for each pane is $5 : 2$, and the borders around and between the panes are $2$ inches wide. In inches, what is the side length of the square window? Mentalese: LET: $x$; PANE_HEIGHT: $5x$; PANE_WIDTH: $2x$; ROWS: $2$; COLS: $4$; HEIGHT_PANES: $2 cdot 5x = 10x$; WIDTH_PANES: $4 cdot 2x = 8x$; HEIGHT_BORDERS: $3 cdot 2 = 6$; WIDTH_BORDERS: $5 cdot 2 = 10$; TOTAL_HEIGHT: $10x + 6$; TOTAL_WIDTH: $8x + 10$; EQ: $10x + 6 = 8x + 10$; SOLVE: $x = 2$; WINDOW_SIDE: $10 cdot 2 + 6 = 26$; ANS: $boxed{26}$."
        },
        {
            "title": "Under review as a conference paper",
            "content": "Example 3 Instruction: Let $P(x)$ be polynomial of degree $3n$ such thatnbegin{align*} P(0) = P(3) = dots = P(3n) &= 2, P(1) = P(4) = dots = P(3n+1-2) &= 1, P(2) = P(5) = dots = P(3n+2-2) &= 0. end{align*}nAlso, $P(3n+1) = 730$. Determine $n$. Mentalese: DEF: $P(x)$, $deg = 3n$; SET: $P(3k) = 2$, $k = 0 ldots n$; SET: $P(3k+1) = 1$, $k = 0 ldots n-1$; SET: $P(3k+2) = 0$, $k = 0 ldots n-1$; COUNT: pts $= 3n+1$; EQ: $deg + 1 = text{pts}$; SOLVE: $n = 1$; ANS: $boxed{1}$. Example 4 Instruction: At Euclid Middle School the mathematics teachers are Mrs. Germain, Mr. Newton, and Mrs. Young. There are $11$ students in Mrs. Germains class, $8$ students in Mr. Newtons class, and $9$ students in Mrs. Youngs class taking the AMC $8$ this year. How many mathematics students at Euclid Middle School are taking the contest? Mentalese: SUM: $s = 991 + 993 + 995 + 997 + 999$; CALC: $s = 4975$; EQ: $4975 = 5000 - N$; SOLVE: $N = 5000 - 4975$; CALC: $N = 25$; ANS: $boxed{25}$. Example 5 Instruction: Let $f$ be the function defined by $f(x)=ax^2-sqrt{2}$ for some positive $a$. If $f(f(sqrt{2}))=-sqrt{2}$ then $a=$ Mentalese: DEF: $f(x) = ax^2 - sqrt{2}$; EVAL1: $f(sqrt{2}) = 2a - sqrt{2}$; EVAL2: $f(f(sqrt{2})) = a(2a - sqrt{2})^2 - sqrt{2}$; EXPAND1: $(2a - sqrt{2})^2 = 4a^2 - 4asqrt{2} + 2$; EVAL3: $a(4a^2 - 4asqrt{2} + 2) - sqrt{2} = 4a^3 - 4a^2sqrt{2} + 2a - sqrt{2}$; SET: $4a^3 - 4a^2sqrt{2} + 2a - sqrt{2} = -sqrt{2}$; ADD: $4a^3 - 4a^2sqrt{2} + 2a = 0$; FACTOR: $2a(2a^2 - 2asqrt{2} + 1) = 0$; SOLVE: $2a^2 - 2asqrt{2} + 1 = 0$; $a = dfrac{2sqrt{2} pm sqrt{8 - 8}}{4}$; $a = dfrac{sqrt{2}}{2}$; ANS: $boxed{dfrac{sqrt{2}}{2}}$."
        },
        {
            "title": "Under review as a conference paper",
            "content": "Figure 6: Violin plots of token usage in Agentica-24k responses across six benchmarks before and after fine-tuning. The Base model generates very long responses, while Direct SLPO provides only limited compression. Mentalese-based methods (SFT, SFT+GRPO, SFT+SLPO) achieve 1020 reduction in response length, approaching an optimal reasoning length that balances efficiency with performance. Although some performance degradation occurs, the Mentalese training pipeline with RLVR methods offers the best trade-off between token efficiency and problem-solving ability. A.4 PROMPT STRUCTURE Direct Prompting Baseline System prompt: You are mathematical reasoning and cognitive modeling assistant. Your job is to generate detailed step-by-step solution, and then compress it into the given format for cognitive modeling. You are always given the final answer to guide your solution. Please output the following: 1. Mentalese CoT symbolic, logic-based chain of thought using compact primitives and logical steps only. Each step should be separated by semicolon (;) with no natural language. All core logic steps must be preserved. Mentalese Format Rules Use ONLY symbolic notation in the form: OP:params or OP:result No natural language No redundancy Every step must be necessary and sufficient for solving the problem Example: SET:w;EQ:abs(180-5.5*w)=110;CASE1:180-5.5*w=110;SOLVE1:w=70/5.5; CASE2:180-5.5*w=-110;SOLVE2:w=290/5.5;CALC1:w1=70/5.5=12+8/11; CALC2:w2=290/5.5=52+8/11;DIFF:t=w2-w1=40;ANS:40 Wrap the output in single JSON object with the following keys: { \"mentalese_cot\"} User: QUESTION: [Your question here] ANSWER: [Your answer here] RESPONSE:"
        },
        {
            "title": "Under review as a conference paper",
            "content": "A.5 MODEL RESPONSES Question (Taken from Math-500) What is the smallest positive perfect cube that can be written as the sum of three consecutive integers? Ground Truth 27 ORION-AG Length: 122 Tokens <think>nSET:n;SUM:n+(n+1)+(n+2)=3n+3;EQ:3n+3=k^3;DIV:3n+3=3(n+1); SET:k=3m;EQ:3(n+1)=(3m)^3=27m^3;EQ:n+1=9m^3;SOLVE:n=9m^3-1; CHECK:smallest m=1;CALC:n=8;SUM:8+9+10=27;ANS:27n</think>nnboxed{27} ORION-AG-SLPO Length: 351 Tokens <think>nGiven: Find the smallest positive perfect cube that can be written as the sum of three consecutive integers.nnLet me denote the three consecutive integers as n-1, n, n+1. Their sum is (n-1) + + (n+1) = 3n.nnSo, we need 3n to be perfect cube.nnLet me denote the perfect cube as k^3. So, 3n = k^3 => = k^3 / 3.nnSince must be an integer, k^3 must be divisible by 3. Therefore, must be divisible by 3. Let me set = 3m, where is an integer.nnThen, k^3 = (3m)^3 = 27m^3.nnSo, 3n = 27m^3 => = 9m^3.nnTherefore, the sum of three consecutive integers is 3n = 27m^3, which is perfect cube.nnNow, we need the smallest positive perfect cube. Since is positive integer, the smallest is 1.nnTherefore, the smallest perfect cube is 27*1^3 = 27.nnLet me verify: Find three consecutive integers that sum to 27.nnn = 9m^3 = 9*1 = 9.nnSo, the integers are 8, 9, 10. Their sum is 8+9+10=27.nnYes, that works.nnTherefore, the smallest positive perfect cube is 27.n</think>nnboxed{27} ORION-AG-GRPO Length: 236 Tokens <think>Lets denote the three consecutive integers as n-1, n, n+1. Their sum is (n-1)+n+(n+1)=3n. We need 3n to be perfect cube. So, must be multiple of 3. Let n=3k. Then 3n=9k. We need 9k to be perfect cube. Lets write 9k=m^3. Then k=m^3/9. For to be an integer, m^3 must be divisible by 9, so must be divisible by 3. Let m=3t. Then m^3=27t^3. So k=27t^3/9=3t^3. Therefore, n=3k=9t^3. Then 3n=27t^3=(3t^3)^3? Wait, no. 27t^3 is (3t)^3. Yes, 27t^3=(3t)^3. So the smallest positive perfect cube is when t=1, so 27.n</think>nnboxed{27} Agentic-24k-SLPO Length: 526 Tokens <think>nOkay, so need to find the smallest positive perfect cube that can be written as the sum of three consecutive integers. Hmm, lets break this down step by step.nnFirst, lets denote the three consecutive integers. Lets say the integers are n-1, n, and n+1. So their sum would be (n-1) + + (n+1). Simplifying that, its n-1 + + n+1 = 3n. So the sum of three consecutive integers is 3n.nnNow, we need this sum to be perfect cube. So, 3n must be perfect cube. Lets denote the perfect cube as k^3, where is positive integer. Therefore, 3n = k^3, which means = k^3 / 3.nnSince has to be an integer (because were dealing with integers), k^3 must be divisible by 3. That implies that itself must be divisible by 3, because if 3 divides k^3, then 3 must divide k. So lets let = 3m, where is positive integer.nnSubstituting back, we have k^3 = (3m)^3 = 27m^3. Therefore, = 27m^3 / 3 = 9m^3.nnSo, the sum of three consecutive integers is 3n = 3 * 9m^3 = 27m^3. Wait, that seems bit off. Let me correct that.nnIf = 9m^3, then the sum is 3n = 27m^3. So, the sum is 27m^3, which is perfect cube since 27 is 3^3 and m^3 is cube. Therefore, the sum is (3m)^3, which is indeed perfect cube.nnNow, we need the smallest positive perfect cube. So we need the smallest such that 27m^3 is positive. Since is positive integer, the smallest is 1.nnTherefore, the smallest positive perfect cube is 27 * 1^3 = 27.nnLet me verify this. If m=1, then n=9*1^3=9. So the three consecutive integers are 8,9,10. Their sum is 8+9+10=27, which is 3^3. That checks out.n</think>nnThe smallest positive perfect cube that can be written as the sum of three consecutive integers is boxed{27}."
        },
        {
            "title": "Under review as a conference paper",
            "content": "Agentica-24k Length: 1512 Tokens <think>nOkay, so need to find the smallest positive perfect cube that can be written as the sum of three consecutive integers. Hmm, lets break this down step by step.nn First, lets understand what it means for number to be perfect cube. perfect cube is an integer that can be written as n^3, where is an integer. The smallest positive perfect cubes are 1^3=1, 2^3=8, 3^3=27, and so on.nn Now, the problem states that this perfect cube must be expressible as the sum of three consecutive integers. Lets denote these three consecutive integers as n-1, n, and n+1. Wait, actually, maybe its better to represent them as x, x+1, x+2, where is an integer. That might be more straightforward.nn So, the sum of three consecutive integers can be written as + (x+1) + (x+2). Lets compute this:nn + (x+1) + (x+2) = + + 1 + + 2 = 3x + 3 = 3(x + 1).nn So, the sum of three consecutive integers is equal to 3 times (x + 1). That means that the sum must be multiple of 3.nn Now, since were looking for perfect cube, lets denote the perfect cube as k^3, where is positive integer. So, we have:nn k^3 = 3(x + 1).nn Therefore, k^3 must be divisible by 3. Which implies that itself must be divisible by 3, because if prime number divides cube, it must divide the base. Since 3 is prime, must be multiple of 3. Lets denote as 3m, where is positive integer.nn Substituting back, we get:nn (3m)^3 = 3(x + 1)nn Compute the left side:nn 27m^3 = 3(x + 1)nn Divide both sides by 3:nn 9m^3 = + 1nn Therefore, + 1 = 9m^3, which implies = 9m^3 - 1.nn Now, since is an integer, and were looking for positive integers, lets see what can be.nn We need k^3 to be positive, so must be at least 1. But since is multiple of 3, the smallest possible is 3.nn Lets compute the smallest possible k^3, which would be when m=1, because m=1 gives k=3.nn So, k=3, then k^3=27.nn Now, lets check if 27 can be expressed as the sum of three consecutive integers.nn From earlier, we know that the sum is 3(x + 1). So, 27 = 3(x + 1) => + 1 = 9 => = 8.nn Therefore, the three consecutive integers are x=8, x+1=9, x+2=10. Lets check their sum: 8+9+10=27. Yes, that works.nn But wait, lets confirm if 27 is indeed the smallest such perfect cube. Lets check smaller values that are multiples of 3.nn The next smaller would be k=0, but that would give k^3=0, which isnt positive. Then k=3 is the next one, which gives k^3=27. So, 27 is indeed the smallest positive perfect cube that can be written as the sum of three consecutive integers.nn But just to be thorough, lets check if theres any smaller k, perhaps non-multiples of 3? Wait, earlier reasoning showed that must be multiple of 3 because k^3 must be divisible by 3. Therefore, the smallest is 3, leading to k^3=27. So, yes, 27 is the answer.nn Wait, hold on, let me think again. The problem says \"the smallest positive perfect cube that can be written as the sum of three consecutive integers.\" So, perhaps should check if theres smaller cube than 27 that can be expressed as such.nn Lets consider the cubes:nn 1^3=1: Can 1 be written as the sum of three consecutive integers? Lets see: Let + (x+1) + (x+2) = 3x + 3 =1. Then 3x= -2 => x= -2/3. Not an integer, so no.nn 2^3=8: Lets see if 8 can be written as the sum of three consecutive integers. Lets set up the equation: 3x + 3 =8 => 3x=5 => x=5/3. Not an integer. So, no.nn 3^3=27: As before, x=8, which is an integer, so yes.nn So, indeed, 27 is the smallest positive perfect cube that can be written as the sum of three consecutive integers.nn Therefore, the answer is 27.nn **Final Answer**nThe smallest positive perfect cube is boxed{27}.n</think>n To find the smallest positive perfect cube that can be written as the sum of three consecutive integers, we start by noting that the sum of three consecutive integers can be expressed as ( + (x+1) + (x+2) = 3x + 3 = 3(x + 1) ).nn This sum must be perfect cube, denoted as ( k^3 ). Therefore, we have:n[ k^3 = 3(x + 1) ]nn Since ( k^3 ) must be divisible by 3, ( ) itself must be multiple of 3. Let ( = 3m ) where ( ) is positive integer. Substituting this, we get:n[ (3m)^3 = 3(x + 1) ]n[ 27m^3 = 3(x + 1) ]n[ 9m^3 = + 1 ]n[ = 9m^3 - 1 ]nn We need the smallest ( ) such that ( k^3 ) is positive. The smallest ( ) is 3, leading to:n[ k^3 = 3^3 = 27 ]nn We verify that 27 can be expressed as the sum of three consecutive integers:n[ 3(x + 1) = 27 ]n[ + 1 = 9 ]n[ = 8 ]nn Thus, the three consecutive integers are 8, 9, and 10, whose sum is 27.nn Therefore, the smallest positive perfect cube is (boxed{27})."
        }
    ],
    "affiliations": [
        "Harvard University",
        "Hippocratic AI",
        "Massachusetts Institute of Technology"
    ]
}