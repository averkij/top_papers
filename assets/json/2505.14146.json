{
    "paper_title": "s3: You Don't Need That Much Data to Train a Search Agent via RL",
    "authors": [
        "Pengcheng Jiang",
        "Xueqiang Xu",
        "Jiacheng Lin",
        "Jinfeng Xiao",
        "Zifeng Wang",
        "Jimeng Sun",
        "Jiawei Han"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Retrieval-augmented generation (RAG) systems empower large language models (LLMs) to access external knowledge during inference. Recent advances have enabled LLMs to act as search agents via reinforcement learning (RL), improving information acquisition through multi-turn interactions with retrieval engines. However, existing approaches either optimize retrieval using search-only metrics (e.g., NDCG) that ignore downstream utility or fine-tune the entire LLM to jointly reason and retrieve-entangling retrieval with generation and limiting the real search utility and compatibility with frozen or proprietary models. In this work, we propose s3, a lightweight, model-agnostic framework that decouples the searcher from the generator and trains the searcher using a Gain Beyond RAG reward: the improvement in generation accuracy over naive RAG. s3 requires only 2.4k training samples to outperform baselines trained on over 70x more data, consistently delivering stronger downstream performance across six general QA and five medical QA benchmarks."
        },
        {
            "title": "Start",
            "content": "s3: You Dont Need That Much Data to Train Search Agent via RL Pengcheng Jiang, Xueqiang Xu, Jiacheng Lin, Jinfeng Xiao2, Zifeng Wang, Jimeng Sun, and Jiawei Han"
        },
        {
            "title": "University of Illinois Urbana Champaign",
            "content": "{pj20,jimeng,hanj}@illinois.edu Amazon jfx@amazon.com 5 2 0 2 0 2 ] . [ 1 6 4 1 4 1 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Retrieval-augmented generation (RAG) systems empower large language models (LLMs) to access external knowledge during inference. Recent advances have enabled LLMs to act as search agents via reinforcement learning (RL), improving information acquisition through multi-turn interactions with retrieval engines. However, existing approaches either optimize retrieval using search-only metrics (e.g., NDCG) that ignore downstream utility or fine-tune the entire LLM to jointly reason and retrieveentangling retrieval with generation and limiting the real search utility and compatibility with frozen or proprietary models. In this work, we propose s3, lightweight, modelagnostic framework that decouples the searcher from the generator and trains the searcher using Gain Beyond RAG reward: the improvement in generation accuracy over naïve RAG. s3 requires only 2.4k training samples to outperform baselines trained on over 70 more data, consistently delivering stronger downstream performance across six general QA and five medical QA benchmarks."
        },
        {
            "title": "Introduction",
            "content": "Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to access and reason over external knowledge by retrieving relevant documents and conditioning generation on them (Lewis et al., 2020). As shown in Figure 2, we categorize the evolution of RAG systems into three phases. Classic RAG. Early approaches relied on static retrieval methods, where queries were fixed and retrieval quality was decoupled from downstream generation performance. Despite their simplicity, these systems often underperformed on queries that need contextual or multi-hop reasoning. 1Code available at https://github.com/pat-jj/s3. 2Independent of co-authors role at Amazon. 1 Figure 1: Training Data vs Averaged Performance across six general and five medical QA Datasets (tested with Claude-3-Haiku as the generator LLM). Pre-RL-Zero. To improve retrieval quality, subsequent methods enabled more active participation of the LLM during inference. Active RAG techniques (Yao et al., 2022; Jiang et al., 2023; Trivedi et al., 2023a) interleaved query generation, retrieval, and reasoning in multi-turn loop. These systems introduced iterative retrieval but typically relied on zero-shot prompting and lacked trainable components. Self-RAG (Asai et al., 2023) distilled such behaviors from larger models into smaller ones via supervised fine-tuning, teaching smaller models to reason and retrieve effectively without external rewards. While these methods improved flexibility and reduced supervision cost, they still did not optimize retrieval using outcome signals. RL-Zero. The recent emergence of reinforcement Figure 2: RAG has progressed from fixed or supervised retrieval to RL-based agentic methods. While prior work trains retrieval or generation jointly, s3 focuses solely on the searcher, improving generation without tuning the generator LLM. semantically correct answers phrased differently. This motivates shift toward modular framework where search and generation are cleanly separated, and optimization focuses purely on search quality with respect to downstream utility (Dai et al., 2025). We propose s3, simple yet powerful framework that trains search-only agent using novel reward signal: Gain Beyond RAG (GBR). GBR measures how much better the generator performs when conditioned on retrieved documents from s3, compared to naive top-k retrieval. This setup keeps the generator LLM frozen, sidesteps answer token overfitting, and directly optimizes the retrieval component to serve any black-box LLM. Remarkably, s3 achieves strong gains with only 2.4k training examples, outperforming DeepRetrieval (focused on retrieval metrics) and Search-R1 (entangled optimization) both in terms of context quality and final answer performance. Our main contributions are: We introduce s3, modular, RL-based search framework that optimizes for generation quality without touching the generator. We define Gain Beyond RAG (GBR), principled, model-agnostic reward signal that quantifies improvements over standard retrieval. We show that s3 outperforms state-of-the-art agentic RAG methods on six general and five medical QA benchmarks, using 70 less training data (see Figure 1)."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Retrieval-Augmented Generation Large language models (LLMs) have shown impressive generative capabilities (Touvron et al., 2023; OpenAI, 2023), but their factuality remains bounded (Peng et al., 2023) by their training corpora. Retrieval-Augmented Generation Figure 3: Decomposition of Agentic RAG. End-to-end approaches fine-tune the entire model using the entire generation accuracy, making it difficult to isolate the contribution of search. In contrast, s3 freezes the generator and trains only the searcher with Gain Beyond RAG (GBR), novel reward that quantifies the added value of retrieved context over naïve RAG, enabling modular, efficient optimization. learning for retrieval marks new phasewhat we refer to as the RL-Zero era. DeepSeek-R1Zero (Guo et al., 2025) showed that even rulebased, outcome-driven rewards (e.g., answer correctness) can train strong reasoning agents. Building on this idea, DeepRetrieval (Jiang et al., 2025) applied RL to train query generators using searchoriented metrics like recall and NDCG. However, these metrics are disconnected from downstream answer quality. Search-R1 (Jin et al., 2025) trained single model to jointly retrieve and generate via reinforcement learning, using exact match (EM) as the reward. While this approach improves answer accuracy, the tight entanglement between search and generation makes it difficult to isolate genuine retrieval improvements (see Figure 3). Moreover, EM is brittle reward signalfailing to reward 2 Figure 4: Overview of the s3 framework. The searcher iteratively generates queries, retrieves documents, and selects useful documents until completion. The final context Ds3 is then passed to frozen generator LLM. The searcher is trained using Gain Beyond RAG (GBR), which quantifies improvement over naïve top-k retrieval from the original question. (RAG) (Lewis et al., 2020; Gao et al., 2023) augments LLMs by prepending retrieved documents to their input, enabling access to up-to-date or domain-specific information. The effectiveness of this setup, however, depends heavily on the retrieval quality. Early efforts improve retrieval through supervised query rewriting (Nogueira and Cho, 2019; Lin et al., 2023a), where LLMs are fine-tuned to generate better search queries from manually labeled or distilled training data. These methods require significant annotation effort and often optimize for imitation rather than end-task performance. Recent works have introduced Active RAG methods (Yao et al., 2022; Trivedi et al., 2023a; Asai et al., 2023; Lyu et al., 2024), which prompt LLMs to iteratively retrieve and reason in zero-shot or few-shot manner. While flexible, these methods typically rely on handcrafted prompting patterns and lack direct optimization by interacting with environment. 2.2 RL for Agentic Retrieval and Searcher-Centric Optimization The emergence of reinforcement learning (RL) for large language models has given rise to agentic retrieval, where models interact with search engines and improve by receiving outcome-based feedbacksuch as whether the final answer is correct. We refer to this shift as the RL-Zero period, sparked by the insight that even simple rewards like answer correctness can elicit strong reasoning and search behavior (Guo et al., 2025). Within this paradigm, retrieval-centric methods like DeepRetrieval (Jiang et al., 2025) optimize query generation for search metrics (e.g., recall, NDCG), which often fail to reflect answer utilityi.e., whether the retrieved context helps generate correct answer. Conversely, end-to-end approaches like SearchR1 (Jin et al., 2025) train LLMs to retrieve and generate jointly using exact match rewards, but require full model access and entangle search with answer token alignment. In contrast, s3 takes searcher-centric approach that avoids generator fine-tuning. It directly optimizes retrieval quality using generation-aware reward, enabling lightweight and modular training that is compatible with black-box LLMs. 3 s3: Optimized Search-Select-Serve Flow with Reinforcement Learning We introduce s3, lightweight, model-agnostic framework that equips tunable search agent with structured, multi-turn access to external knowledge. As illustrated in Figure 4, the searcher LLM interacts with search engine iteratively: it generates queries, retrieves documents, selects subset of useful evidence, and decides whether to continue searching. frozen generator LLM then consumes the accumulated evidence to produce final answer. To ensure fair reward baseline, s3 begins by retrieving top-k (k = 3 in our experiments) documents from the original question, just like naïve 3 RAG. The searcher is trained using the Gain Beyond RAG (GBR) reward, which measures the improvement in generation accuracy when using its retrieved context versus this baseline. This modular design enables targeted optimization of retrieval quality, decoupled from answer generation. 3.1 Multi-Turn Search-Select Loop Given question Q, the system consists of (1) searcher LLM (policy) πs3, (2) search engine R, (3) frozen generator LLM G. s3 first retrieves top-k documents using q0 = Q, yielding D0 = R(Q). subset Dsel 0 D0 is selected to form the initial context. It then performs sequence of search rounds = 1, 2, . . . , , structured as follows: s3 Loop 1. Query Generation: The searcher emits query qt in <query>...</query>. 2. Search: Documents Dt = R(qt) are retrieved in <information>...</information> 3. Select: Useful documents are selected between <important_info>...</important_info>, corresponding to subset Dsel Dt. 4. Stop decision: The model declares <search_complete>[1/0]</search_complete>. instantiate as Generation Accuracy (see 4.1) for RAG performance. This reward ensures the searcher is incentivized to retrieve documents that meaningfully enhance independent of the generators output quality, surface-form answer similarity. To improve training efficiency, we precompute the baseline accuracy term Acc(G(Q, DRAG), A) and restrict training to examples where it equals 0. This effectively filters out questions already solvable by naïve RAG, allowing s3 to focus on harder queries where improved retrieval is essential for generation success. 3.3 Search Policy Optimization We optimize the search policy πs3 via reinforcement learning using the Gain Beyond RAG (GBR) reward. Each rollout consists of complete search trajectory: emitted queries, document selections, and stop decision. Once the final context Ds3 is constructed, the generator produces an answer, and the GBR reward is computed. The generator remains frozen; gradients are backpropagated only through the search policy. Our method is agnostic to the specific advantage estimation algorithm. In this work, we use Proximal Policy Optimization (PPO) (Schulman et al., 2017) due to its strong empirical stability (Jiang et al., 2025; Jin et al., 2025). The PPO objective is: The loop continues until search_complete is True (1) or the turn limit is reached. The final context is Ds3 = (cid:83)T , which is passed (served) to the generator to produce the final output: t=0 Dsel LPPO(θ) = Eτ πθ (cid:104) (cid:88) t=1 (cid:16) min rt(θ) ˆAt, clip(rt(θ), 1ϵ, 1+ϵ) ˆAt (cid:17)(cid:105) (2) ˆA = G(Q, Ds3) Initialization (Begin with Search). Initializing with q0 = ensures the loop begins with the same context as naïve RAG, making the Gain Beyond RAG reward reflect true search improvements. where rt(θ) = πθ(atst) πold(atst) is the probability ratio between the current and reference policies, ˆAt is the estimated advantage, and ϵ is clipping threshold."
        },
        {
            "title": "4 Experiments",
            "content": "3.2 Training via Gain Beyond RAG (GBR) 4.1 Experimental Setups To train πs3, we frame search as reinforcement learning problem. The reward signal, Gain Beyond RAG (GBR), quantifies the improvement in generation accuracy over fixed top-k baseline: GBR(Q) = Acc(G(Q, Ds3), A) Acc(G(Q, DRAG), A) (1) where is the gold-standard answer, and DRAG = R(Q) is the top-k retrieval from the original question. Acc() is task-specific metric, which we Evaluation Metric. We measure performance using Generation Accuracy, which combines fast span-matching test (Ma et al., 2021; Lin et al., 2021) with lightweight LLM-based correctness check (Figure 13). Given model prediction and set of gold answers A, we compute: GenAcc = span_check judge_check (3) which can be either 1 or 0, determined by the following evaluation flow: 4 Single-Hop Multi-Hop Searcher #Train NQ 3,610 TriviaQA PopQA HotpotQA 2wiki Musique Avg. 11,313 14,267 7,405 12, 2,417 Methods #Test Data SFTQwen2.5-3B-Inst R1Qwen2.5-7B-Inst Search-R1-3B Search-R1-7B Direct Inference CoT RAGBM25 RAGE5 IRCoT IRCoT Search-R1-3B (Ret) Search-R1-7B (Ret) s3 - - (self) 3B (self) 7B - - - - (self) 7B 14B 3B 7B 7B Direct Inference CoT RAGBM25 RAGE5 IRCoT IRCoT Search-R1-3B (Ret) Search-R1-7B (Ret) s3 - - - - 7B (self) 14B 3B 7B 7B Direct Inference CoT RAGBM25 DeepRetrievalBM25 RAGE5 IRCoT IRCoT Search-o1 Search-R1-3B (Ret) Search-R1-7B (Ret) s3 - - - 3B - 7B 14B 14B 3B 7B 7B End-to-End Fine-Tuning 23.7(17.5) 35.6(28.8) 47.0(27.9) 56.9(48.2) 41.6(34.3) 60.2(53.4) 65.6(46.2) 73.8(64.0) 18.1(14.0) 22.4(20.5) 46.4(34.9) 50.6(46.8) 18.0(13.7) 29.4(24.0) 33.5(22.1) 54.6(43.5) Generator (Qwen2.5-7b-Instruct) Frozen 37.3(4.4) 37.7(10.3) 43.6(3.8) 62.1(5.8) 63.2(6.2) 63.9(6.3) 56.6(6.6) 61.3(8.1) 66.1(7.2) 55.1(32.9) 60.6(35.4) 69.8(29.7) 74.5(33.8) 75.6(34.3) 75.5(34.9) 68.6(32.5) 73.7(35.9) 78.5(36.8) 19.9(8.3) 22.2(11.3) 34.6(12.4) 54.5(20.3) 54.5(19.3) 55.5(20.3) 49.4(18.8) 51.9(20.7) 57.4(21.9) 28.1(7.6) 31.1(13.4) 45.3(15.1) 46.6(13.6) 50.9(15.4) 52.5(16.0) 41.5(13.6) 58.6(20.0) 59.0(21.8) Generator (Qwen2.5-14b-Instruct) Frozen 38.8(8.2) 40.5(10.2) 54.8(16.4) 62.4(18.7) 63.0(18.8) 63.9(19.2) 59.2(16.5) 63.8(18.0) 67.2(18.3) 62.7(39.0) 66.2(41.6) 76.7(44.8) 77.4(50.7) 77.7(50.1) 78.2(51.7) 75.6(47.4) 76.3(49.5) 79.5(48.9) 24.5(10.8) 24.6(13.6) 41.5(22.7) 55.1(34.0) 56.3(33.5) 56.1(33.8) 52.3(30.3) 54.6(33.3) 57.8(35.7) Generator (Claude-3-Haiku) Frozen 48.1(25.7) 61.5(2.9) 50.5(3.8) 64.4(3.7) 66.5(4.3) 68.0(4.2) 68.3(4.2) 67.3(4.7) 60.7(3.3) 68.1(4.1) 70.5(3.2) 76.5(64.8) 81.0(30.0) 75.5(28.4) 80.2(23.2) 80.7(28.9) 81.7(29.3) 81.6(29.5) 81.2(29.8) 74.5(24.8) 80.9(25.9) 84.0(24.6) 35.7(30.9) 43.2(9.1) 35.9(8.0) 45.5(8.2) 55.7(8.9) 55.5(8.9) 56.1(8.6) 50.2(9.3) 50.1(6.9) 55.7(7.0) 57.7(5.9) 30.2(9.5) 32.9(12.3) 50.4(18.3) 47.4(20.9) 50.7(22.7) 51.6(23.7) 45.5(18.3) 56.7(25.3) 57.1(23.3) 35.5(24.2) 48.8(8.8) 50.2(11.4) 54.5(10.2) 50.7(11.5) 54.8(11.7) 55.5(11.9) 58.1(12.6) 45.7(10.0) 62.0(11.2) 62.4(11.1) 170k 170k 170k 170k 0 0 0 0 0 0 170k 170k 2.4k 0 0 0 0 0 0 170k 170k 2.4k 0 0 0 70k 0 0 0 0 170k 170k 2.4k 22.1(20.8) 30.0(29.1) 28.5(24.4) 51.6(38.4) 36.9(9.1) 31.6(18.9) 38.5(10.3) 40.1(7.8) 48.7(9.6) 47.4(9.3) 33.2(7.8) 50.8(12.2) 51.6(12.4) 38.6(7.2) 33.2(13.8) 49.9(6.4) 44.9(10.1) 53.2(12.4) 54.0(12.0) 44.0(8.3) 56.7(11.0) 57.1(11.6) 28.9(24.0) 46.2(6.8) 40.7(8.1) 47.1(8.0) 39.2(7.8) 46.5(8.1) 47.7(8.4) 48.8(8.4) 33.1(7.0) 51.0(7.2) 52.4(8.3) 5.1(2.9) 10.7(7.8) 6.0(2.8) 28.5(20.6) 10.6(1.2) 10.6(4.2) 11.5(1.5) 13.0(2.0) 16.4(2.5) 17.2(2.7) 12.1(1.9) 27.6(7.1) 23.9(6.1) 12.5(1.8) 12.6(5.2) 17.7(3.1) 16.1(3.3) 17.5(4.1) 19.1(5.2) 16.0(2.9) 30.2(9.1) 26.7(7.8) 8.8(4.3) 21.2(2.3) 11.8(0.8) 22.2(1.7) 14.0(1.2) 17.4(1.6) 18.9(1.7) 14.2(1.2) 12.7(1.3) 29.3(3.2) 26.2(7.9) 21.4(17.2) 31.4(27.3) 37.8(26.4) 52.7(43.6) 31.3(10.6) 32.3(15.6) 40.6(12.1) 48.5(13.9) 51.6(14.5) 52.0(14.9) 43.6(13.5) 54.0(17.3) 56.1(17.7) 34.5(12.8) 35.0(16.1) 48.5(18.6) 50.6(23.0) 53.1(23.6) 53.8(24.3) 48.8(20.6) 56.4(24.4) 57.6(24.3) 38.9(29.0) 50.3(10.0) 44.1(10.1) 52.3(8.1) 51.1(10.4) 54.0(10.6) 54.7(10.7) 53.3(11.0) 46.1(8.9) 57.8(9.8) 58.9(10.2) Table 1: Performance comparison on general-domain QA datasets. Datasets marked with are the source of training data used by Search-R1 and s3. We show generation accuracy (4.1) as the main results, and exact match scores in brackets. We use E5-base-v2 as the retriever and Wikipedia-2018 as the corpus. Searcher shows the number of parameters of the searcher model. #Train shows the amount of training data used to train the searcher. DeepRetrievalBM25 is trained on NQ, Search-R1 and s3 are trained on NQ+HotpotQA with different training size (170k vs 2.4k). Results are averaged by three runs. Evaluation Flow of Generation Accuracy Input: Prediction p, Gold Answers Step 1: Normalize and (lowercase, remove punctuation and articles). Step 2: span_check If any is token span in p, return GenAcc = 1. Step 3: judge_check Prompt LLM: Does contain any of A? Step 4: Return GenAcc = 1 if LLM says yes; else 0. Why Exact Match Falls Short - An Example Golden answer: \"Barack Obama\" LLM response: \"The 44th President of the United States was Barack Obama.\" (response = golden) Exact match: 0 Generation Accuracy: 1 (span_check succeeds) We choose this metric because it better captures 5 semantic correctness and aligns more closely with human judgment than traditional exact match (see Appendix for supporting evidence). Datasets. Following prior study (Jin et al., 2025), we construct the training set by combining samples from Natural Questions (NQ) and HotpotQA. Since span_check may incorrectly accept answers for questions with semantic negation (e.g., treating not true as matching true), we remove all yes/no and true/false questions from the training set to ensure reliable reward signals. To focus training on harder examples, we filter out samples where the generator LLM (Qwen2.5-14BInstruct) already produces correct answer using naïve RAG retrieval. This reduces the dataset size from 169,615 to 70,286. As later shown in Figure 5, s3 rapidly converges within 15 training Methods #Test Data w/o retrieval RAGBM25 DeepRetrievalBM25 RAGE5 IRCoT IRCoT Search-o1 Search-R1-3B (Ret) Search-R1-7B (Ret) s3 RAGBM25 DeepRetrievalBM25 RAGE5 IRCoT IRCoT Search-o1 Search-R1-3B (Ret) Search-R1-7B (Ret) s3 Searcher #Train MedQA-US MedMCQA PubMedQA BioASQ-Y/N MMLU-Med Avg. Medical RAG-QA Datasets (MIRAGE) 1,273 4,183 500 - 61.7(45.8) 55.8(29.3) 55.6(0.0) Corpus: Wikipedia 2018 (Karpukhin et al., 2020) 57.5(45.2) 61.3(44.8) 58.0(44.7) 60.5(45.4) 60.3(46.7) 59.6(47.7) 53.7(41.4) 59.2(42.8) 61.5(44.3) 61.6(48.2) 62.5(45.4) 61.5(46.7) 62.8(45.1) 61.7(48.9) 64.5(55.4) 58.8(47.2) 62.6(45.7) 65.7(47.1) 52.8(4.6) 56.2(8.2) 54.6(3.8) 54.2(8.6) 53.0(7.6) 52.2(1.8) 53.8(4.4) 55.4(5.2) 56.6(5.2) 0 70k 0 0 0 0 170k 170k 2.4k 618 76.9(0.0) 73.6(6.3) 77.3(9.2) 73.3(5.3) 73.0(13.8) 75.2(11.8) 74.9(0.2) 63.6(4.4) 71.2(6.5) 77.3(7.1) Corpus: Wikipedia+PubMed+Textbook (Xiong et al., 2024) 59.9(44.4) 65.1(44.2) 60.1(45.0) 62.7(45.3) 62.3(46.6) 61.1(47.6) 54.8(40.7) 61.9(44.2) 65.3(45.4) 65.4(43.1) 65.0(35.1) 64.1(43.4) 63.9(38.6) 62.7(43.8) 65.0(50.1) 57.5(45.5) 62.1(43.2) 65.7(45.7) 79.4(10.8) 78.6(16.2) 79.4(10.8) 75.4(13.0) 74.0(10.8) 74.2(12.0) 71.4(7.8) 78.6(8.0) 81.5(13.6) 0 70k 0 0 0 0 170k 170k 2.4k 88.4(6.5) 89.5(7.4) 89.8(5.0) 87.2(5.8) 87.9(5.3) 89.3(5.3) 73.3(3.6) 86.3(5.3) 92.1(6.5) - 3B - 7B 14B 14B 3B 7B 7B - 3B - 7B 14B 14B 3B 7B 7B 1, 76.4(35.8) 65.3(22.2) 77.6(61.9) 79.2(57.9) 77.9(62.2) 78.7(58.2) 77.2(61.9) 77.7(63.9) 68.4(55.4) 69.3(53.3) 76.0(56.3) 79.6(57.1) 79.3(49.1) 78.8(58.8) 79.7(54.9) 79.6(59.0) 78.1(59.5) 62.0(47.6) 69.9(48.9) 78.3(56.2) 64.6(33.2) 67.3(33.1) 65.1(32.5) 65.8(34.2) 65.5(35.4) 65.8(33.8) 59.7(30.6) 63.5(30.7) 68.3(32.0) 74.5(32.4) 75.8(30.4) 74.6(32.6) 73.8(31.5) 73.3(33.1) 73.5(34.1) 63.8(29.0) 71.8(29.9) 76.6(33.5) Table 2: Performance on medical-domain QA datasets (Xiong et al., 2024), using Claude-3-Haiku as the generator. We report judge_check as the primary metric (see 4.1), with exact match in brackets. Retrieval is performed with E5-base-v2 under two corpus settings: Wikipedia-2018 and Wikipedia+PubMed+Textbook. s3 achieves the highest overall accuracy among all retrieval-augmented methods in both settings. None of the methods is trained on medical data: DeepRetrievalBM25 is trained on 70k NQ, Search-R1 on 170k NQ+HotpotQA, and s3 on 2.4k NQ+HotpotQA. Results are averaged by three runs. steps. For evaluation, we use the checkpoints at step 20. Given batch size of 120, this corresponds to approximately 2.4k training examples, highlighting the data efficiency of our method. We evaluate on six general-domain QA benchmarks: NQ (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), PopQA (Mallen et al., 2022), HotpotQA (Yang et al., 2018), 2WikiMultihopQA (Ho et al., 2020), and Musique (Trivedi et al., 2022), as well as MIRAGE (Xiong et al., 2024), suite of five medical-domain QA datasets. Baselines. We compare s3 against diverse set of RAG systems: (1) End-to-End Fine-tuning. Fully fine-tuned models that jointly retrieve and generate using outcome-based RL or supervision: SearchR1 (3B/7B), SFT (3B), and R1 (7B) where 3B/7B for SFT and R1 are based on Qwen2.5-3B/7BInstruct. (2) Static Retrieval+Frozen Generator. Methods that retrieve documents using fixed or scripted strategy, then pass them to frozen generator: RAG-BM25, RAG-E5: retrieval via BM25 or E5-base (Wang et al., 2022). DeepRetrievalBM25 (3B): RL-trained searcher optimizing recall, paired with BM25. (3) Active Retrieval+Frozen Generator. diagnostic setting where we extract the documents retrieved during models reasoning trajectory and feed them to frozen generator: Search-R1-3B/7B (Ret), IRCoT (7B/14B), and Search-o1 (14B) all fall under this category, differing only in whether retrieval is learned (Search-R1) or prompted (IRCoT (Trivedi et al., 2023b), Searcho1 (Li et al., 2025)). (4) s3. Our s3 trains only the searcher using GBR and forwards selected documents to frozen generator, with no fine-tuning. We place more details in Appendix A.1. Models for Training and Evaluation. Throughout all the training processes, we use Qwen2.5-7BInstruct (Yang et al., 2024) as the base searcher LLM to train, and use Qwen2.5-14B-Instruct1 as the frozen generator for both answer generation and judge_check for reward computation. For evaluation, we use Claude-3-Haiku as the LLM for judge_check to ensure high evaluation quality. We test three frozen generators: Qwen2.5-7b/14bInstruct and Claude-3-Haiku. Both training and evaluation are conducted on five NVIDIA A100 80GB PCIe GPUs. RAGEN (Wang et al., 2025) and VERL (Sheng et al., 2024) are used as the base architecture for multi-turn RL training. We place more details in Appendix A.2."
        },
        {
            "title": "5 Results",
            "content": "We evaluate s3 across six general-domain and five medical-domain QA benchmarks, with frozen generators ranging from Qwen2.5-7B/14B to Claude1In this paper, we use GPTQ-Int4 version of Qwen2.514B-Instruct for its high efficiency. We deploy frozen LLMs using vLLM (Kwon et al., 2023) for fast inference. 6 Single-Hop Multi-Hop #Retrieval #Select #Turns #MaxContexts NQ TriviaQA PopQA HotpotQA 2wiki Musique Avg. 8 3 5 3 5 3 3 3 3 3 3 3 4 4 3 9 9 12 12 9 70.5(3.2) 69.6(3.5) 70.0(3.5) 68.9(3.7) 69.4(3.5) 84.0(24.6) 83.4(24.3) 83.8(24.8) 82.0(24.9) 82.3(24.4) 57.7(5.9) 57.4(5.8) 57.7(5.8) 56.4(6.1) 57.0(5.7) 62.4(11.1) 62.0(11.9) 62.5(12.3) 62.0(11.9) 61.8(11.7) 52.4(8.3) 53.8(7.8) 54.7(8.0) 51.7(7.7) 51.5(8.2) 26.2(7.9) 24.5(2.3) 25.7(3.2) 24.7(2.8) 25.1(2.3) 58.9(10.2) 58.5(9.3) 59.1(9.6) 57.7(9.5) 57.9(9.3) Table 3: Study of the numbers of retrieved documents (#Retrieval) and turns (#Turns). Maximum selection is set to 3 across all settings. We use the frozen Claude-3-Haiku as the generator LLM for this study. 3-Haiku. We report generation accuracy as the primary metric and provide detailed comparisons across baselines, reward functions, and training efficiency. General Domain RAG Performance. Table 1 summarizes results across general QA datasets. s3 achieves the highest average accuracy of 58.9%, outperforming all static, zero-shot, and end-to-end tuned baselines. This is particularly notable given its extreme data efficiencytrained on just 2.4k examples, compared to 70k for DeepRetrieval and 170k for Search-R1. Takeaway #1: Searcher-Only is better than End-to-End Optimization for RAG s3 consistently outperforms Search-R1 on search quality, revealing that most of the performance gain in RAG stems from improving the search capability instead of aligning generation outputs. Compared to IRCoT-14B, which conducts zeroshot retrieval with 2 the parameter count, s3 gains +4.6 points on average. Relative to Search-R1-7B (Ret), which uses the same backbone, s3 improves by +1.5 points while avoiding any generator tuning. These gains are consistent across both singlehop datasets (e.g., 70.0% on NQ) and multi-hop datasets (e.g., 62.4% on HotpotQA), showing that learned search behavior transfers across reasoning complexity. Medical Domain QA Performance. Table 2 reports performance on the MIRAGE suite (Xiong et al., 2024) under both corpus settings. s3 achieves the highest average accuracy (76.6%) when using the combined Wikipedia+PubMed+Textbook corpus, surpassing all retrieval-augmented baselines. Interestingly, while Search-R1 shows competitive scores on Wikipedia-only corpora, its performance deteriorates on richer corpora, indicating overfitting to shallow heuristics or memorized formats. In contrast, s3 and DeepRetrieval remain robust, with s3 achieving 81.5% on PubMedQA Figure 5: Reward Curves for top = {3, 5, 8} and #turns = {3, 4}. The maximum selection is kept as 3. and outperforming IRCoT across four of five tasks. Takeaway #2: Searcher-Only Training enables Domain Transfer s3s zero-shot success on medical QA, despite training only on general QA, suggests that reinforcement-learned search skills generalize more reliably than generation-tuned approaches. Retrieval Behavior and Search Dynamics We analyze the effect of retrieval parameters (#retrieved documents and #turns) in Table 3 and reward progression in Figure 5. s3 reaches peak performance with (k=8, turns=3), and adding more turns or broader retrieval brings limited improvement. This indicates that the policy rapidly learns to emit focused and early queries, capturing most useful content without unnecessary expansion. Training Efficiency Table 4 shows that it takes 20 PPO steps (2.4k examples) to train s3, while Search-R1 requires 2,100 steps (170k examples). Even accounting for the higher per-step cost due to LLM-based reward computation, the total wallclock time is reduced by 33. Moreover, s3 avoids retriever pretraining and operates with smaller 7B policy model, making it practical method for low-resource RL training. s3 achieves 7 Figure 6: Ablation study on s3 components. Each row corresponds to different configuration of Retrieval:Selection:Turns = 8:3:3, 5:3:3, and 3:3:3. The first six columns report generation accuracy. Begin with Search refers to initializing the first query with the original question. Document Selection refers to the selection step within the s3 loop (Step 3). We observe that removing Begin with Search leads to significant drop in performance. While removing Document Selection sometimes yields better performance, the full s3 system still performs competitivelyand most importantly, drastically reduces input token usage (2.6 4.2 less tokens), improving overall efficiency. Time/Step Training Steps Total GenAcc LLMJudge Span EM Search-R1 DeepRetrievalBM25 s3 1.8m 1.3m 5.7m 2,100 1,600 3,780m 2,080m 114m General QA Medical QA 58.9 76.6 59.6 77.3 57.1 74.3 50.5 70. Table 4: Comparison of Training Efficiency (tested with batch size=120 on five NVIDIA A100 GPUs). Note: s3 is slower stepwise since we need to conduct generation and evaluation by frozen LLM for reward computation during training. Table 5: Comparison of RAG performance under different reward functions. LLMJudge (judge_check) yields the highest scores but is computationally expensive. GenAcc offers good balance of accuracy and efficiency, while Span (span_check) and EM underperform due to limited semantic coverage. state-of-the-art performance with orders of magnitude less data and compute, suggesting more sustainable path for RAG optimization. Reward Function Comparison Table 5 compares different reward signals used for computing GBR. LLMJudge provides slightly higher final scores, but is too costly for scalable training. In contrast, GenAcc offers strong performance while remaining efficient and aligning better with human evaluation than EM or span-based heuristics. Appendix shows that GenAcc matches human judgment on 96.4% of samples, while Exact Match used by Search-R1 captures only 15.8%. Takeaway #3: Reward Choice directly shapes Search Quality Using semantically or human preference aligned metrics like our GenAcc (4.1) encourages the search policy to retrieve substantively helpful documents, rather than optimizing for brittle string overlap. Effects of Selection and Begin with Search. We investigate the role of two components in the s3 loop: document selection and initialization with the original question (Begin with Search). As shown in Figure 6, removing the selection step degrades performance on four out of six datasets. This is expected, as passing all retrieved documents to the generator increases token length, up to 4 with = 8, and introduces more noise. Still, performance improves slightly on NQ and 2Wiki, likely because broader context benefits multi-hop reasoning or compensates for overly aggressive pruning. Disabling Begin with Search consistently causes significant drop, underscoring the importance of seeding the search process with strong initial query. Interestingly, when both selection and initialization are removed, performance recovers slightly compared to removing only initialization. This suggests that selection and initialization interact conditionallyselection may amplify the downsides of poor initialization by prematurely filtering out useful context."
        },
        {
            "title": "6 Conclusion",
            "content": "We present s3, framework that trains searchonly agent using the Gain Beyond RAG reward. By decoupling search from generation and optimizing only the retriever, s3 outperforms strong baselines with just 2.4k examples. Our results show that targeted search policy learning yields substantial gains in both efficiency and generalization, offering scalable path for improving RAG systems."
        },
        {
            "title": "7 Limitations",
            "content": "While s3 demonstrates strong empirical performance with remarkable data efficiency, several limitations warrant discussion. Dependency on Frozen Generators. Our framework assumes the availability of capable frozen generator LLM. Although this enables modelagnostic training, it implicitly relies on the generators ability to make use of improved context. For lower-capacity or instruction-weak generators, the gains from better retrieval may not fully translate into better outputs. Reward Estimation Bottleneck. The use of generation-based rewards such as GenAcc necessitates LLM inference during training to compute reward signals. This introduces computational overhead compared to token-level or retrieval-only objectives, limiting scalability. Although we show that s3 achieves high performance with minimal steps, online reward computation remains more costly than offline retrieval optimization. Broader Impacts. On the positive side, s3 reduces the data and compute burden for training effective retrieval agents, making RAG systems more accessible to low-resource communities. It may also benefit domains such as healthcare or scientific QA where labeled data is scarce. However, like all retrieval-augmented systems, s3 inherits the biases of both its searcher and generator. If deployed without careful curation of source corpora, it may propagate misinformation or reflect existing societal biases. We encourage practitioners to audit both retrieval sources and downstream outputs when applying this framework in sensitive domains. Overall, while s3 advances the state of searchagent training, further work is needed to address these limitations and ensure safe, robust deployment in real-world settings."
        },
        {
            "title": "References",
            "content": "Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023. Self-rag: Learning to retrieve, generate, and critique through self-reflection. In The Twelfth International Conference on Learning Representations. Lu Dai, Yijie Xu, Jinhui Ye, Hao Liu, and Hui Xiong. 2025. Seper: Measure retrieval utility through the lens of semantic perplexity reduction. In The Thirteenth International Conference on Learning Representations. Tri Dao. 2023. Flashattention-2: Faster attention with arXiv better parallelism and work partitioning. preprint arXiv:2307.08691. Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-Emmanuel Mazaré, Maria Lomeli, Lucas Hosseini, and Hervé arXiv preprint Jégou. 2024. The faiss library. arXiv:2401.08281. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. 2023. Retrieval-augmented generation for large language models: survey. arXiv preprint arXiv:2312.10997. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in arXiv preprint llms via reinforcement learning. arXiv:2501.12948. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020. Constructing multihop QA dataset for comprehensive evaluation of reasoning steps. In Proceedings of the 28th International Conference on Computational Linguistics, pages 66096625, Barcelona, Spain (Online). International Committee on Computational Linguistics. Pengcheng Jiang, Jiacheng Lin, Lang Cao, Runchu Tian, SeongKu Kang, Zifeng Wang, Jimeng Sun, and Jiawei Han. 2025. Deepretrieval: Hacking real search engines and retrievers with large language models via reinforcement learning. arXiv preprint arXiv:2503.00223. Zhengbao Jiang, Frank Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Active retrieval augmented generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 79697992. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arık, Dong Wang, Hamed Zamani, and Jiawei Han. 2025. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516. Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. 2021. What disease does this patient have? large-scale open domain question answering dataset from medical exams. Applied Sciences, 11(14):6421. 9 Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. 2019. Pubmedqa: dataset for biomedical research question answering. arXiv preprint arXiv:1909.06146. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick SH Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In EMNLP (1), pages 67696781. Anastasia Krithara, Anastasios Nentidis, Konstantinos Bougiatiotis, and Georgios Paliouras. 2023. Bioasqqa: manually curated corpus for biomedical question answering. Scientific Data, 10(1):170. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, and 1 others. 2019. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453466. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano, Sean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, Daniel Haziza, Luca Wehrstedt, Jeremy Reizenstein, and Grigxformers: modular and ory Sizov. 2022. https: hackable transformer modelling library. //github.com/facebookresearch/xformers. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, and 1 others. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 33:9459 9474. Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. 2025. Search-o1: Agentic searchenhanced large reasoning models. arXiv preprint arXiv:2501.05366. Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, JhengHong Yang, Ronak Pradeep, and Rodrigo Nogueira. 2021. Pyserini: python toolkit for reproducible information retrieval research with sparse and dense In Proceedings of the 44th Interrepresentations. national ACM SIGIR Conference on Research and Development in Information Retrieval, pages 2356 2362. Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Richard James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, and 1 others. 2023a. Ra-dit: Retrieval-augmented dual instruction tuning. arXiv preprint arXiv:2310.01352. Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Richard James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, and 1 others. 2023b. Ra-dit: Retrieval-augmented dual instruction tuning. In The Twelfth International Conference on Learning Representations. Yuanjie Lyu, Zihan Niu, Zheyong Xie, Chao Zhang, Tong Xu, Yang Wang, and Enhong Chen. 2024. Retrieve-plan-generation: An iterative planning and answering framework for knowledge-intensive llm generation. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 46834702. Xueguang Ma, Kai Sun, Ronak Pradeep, and Jimmy Lin. 2021. replication study of dense passage retriever. arXiv preprint arXiv:2104.05740. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Hannaneh Hajishirzi, and Daniel Khashabi. 2022. When not to trust language models: Investigating effectiveness and limitations of parametric and nonparametric memories. arXiv preprint. Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang, Melih Elibol, Zongheng Yang, William Paul, Michael Jordan, and 1 others. 2018. Ray: distributed framework for emerging {AI} applications. In 13th USENIX symposium on operating systems design and implementation (OSDI 18), pages 561577. Rodrigo Nogueira and Kyunghyun Cho. 2019. PasarXiv preprint sage re-ranking with bert. arXiv:1901.04085. OpenAI. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. 2022. Medmcqa: large-scale multi-subject multi-choice dataset for medical domain question answering. In Conference on health, inference, and learning, pages 248260. PMLR. Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023. Check your facts and try again: Improving large language models with external knowledge and automated feedback. arXiv preprint arXiv:2302.12813. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347. 10 Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, and 1 others. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824 24837. Guangzhi Xiong, Qiao Jin, Zhiyong Lu, and Aidong Zhang. 2024. Benchmarking retrieval-augmented generation for medicine. In Findings of the Association for Computational Linguistics ACL 2024, pages 62336251. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, and 1 others. 2024. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. 2018. Hotpotqa: dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. 2024. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256. Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and JiRong Wen. 2025. R1-searcher: Incentivizing the search capability in llms via reinforcement learning. arXiv preprint arXiv:2503.05592. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, and 1 others. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022. MuSiQue: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2023a. Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. arXiv preprint arXiv:2212.10509. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2023b. Interleaving retrieval with chain-of-thought reasoning for knowledgeIn Proceedings of intensive multi-step questions. the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1001410037, Toronto, Canada. Association for Computational Linguistics. George Tsatsaronis, Georgios Balikas, Prodromos Malakasiotis, Ioannis Partalas, Matthias Zschunke, Michael Alvers, Dirk Weissenborn, Anastasia Krithara, Sergios Petridis, Dimitris Polychronopoulos, and 1 others. 2015. An overview of the bioasq large-scale biomedical semantic indexing and question answering competition. BMC bioinformatics, 16:128. Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, Shengyi Huang, Kashif Rasul, and Quentin Gallouédec. 2020. Trl: Transformer reinforcement learning. https://github.com/huggingface/trl. Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022. Text embeddings by weaklysupervised contrastive pre-training. arXiv preprint arXiv:2212.03533. Zihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, Linjie Li, Zhengyuan Yang, Kefan Yu, Minh Nhat Nguyen, Licheng Liu, Eli Gottlieb, and 1 others. 2025. Ragen: Understanding self-evolution in llm agents via multi-turn reinforcement learning. arXiv preprint arXiv:2504.20073."
        },
        {
            "title": "Contents of Appendix",
            "content": "A. Implementation Details . . . . . . . . . . . . . . . . . . 12 A.1 Baselines Details . . . . . . . . . . . . . . . . . . . . . . . 12 A.2 Setup Details . . . . . . . . . . . . . . . . . . . . . . . . . . 12 A.3 Datasets & Corpora . . . . . . . . . . . . . . . . . . . . 13 A.4 Generation Accuracy Computation . . . . . . .13 A.5 Document Extraction Logic . . . . . . . . . . . . . 15 B. Alignment Study of Evaluation Metrics . . . . 15 D. Prompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 E. Scalability Study . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "A Implementation Details",
            "content": "For static retrieval baselines running on MIRAGE, we use the question itself instead of question+options to retrieve. A.1 Baselines Details IRCoT (7B and 14B). IRCoT2 (Trivedi et al., 2023b) is prompting-based method that alternates between chain-of-thought reasoning and retrieval. It requires no fine-tuning: the model is instructed via prompt to iteratively reason about question and issue retrieval queries, integrating newly retrieved evidence into its reasoning process. We apply IRCoT using both Qwen2.5-7B-Instruct and Qwen2.5-14B-Instruct. DeepRetrieval-BM25-3B (Jiang et al., 2025). This baseline employs 3B-parameter language model trained with reinforcement learning on retrieval metrics such as recall and NDCG. It learns to generate search queries that maximize the retrieval of relevant documents using BM25 search engine. Training is conducted on 70k QA examples in NQ dataset with answer span reward (evidence-seeking task in (Jiang et al., 2025)), focusing exclusively on improving retrieval performance, not generation. We use its publicly released checkpoint3. Search-R1-3B and Search-R1-7B (Jin et al., 2025). These baselines4 use 3B and 7B parameter models, respectively, and are trained end-to-end to jointly retrieve and generate answers. Reinforcement learning is applied on 170k training examples, using an exact match (EM) reward to guide both retrieval query formulation and answer generation. 2https://github.com/StonyBrookNLP/ircot 3https://huggingface.co/DeepRetrieval/ DeepRetrieval-NQ-BM25-3B 4https://huggingface.co/PeterJinGo/ SearchR1-nq_hotpotqa_train-qwen2.5-3b-em-ppo, https://huggingface.co/PeterJinGo/SearchR1-nq_ hotpotqa_train-qwen2.5-7b-em-ppo The model directly integrates search results into its reasoning steps within single retrieval round. Search-o1. Search-o1 (Li et al., 2025) is an inference-time retrieval controller designed to enhance long-form reasoning in o1-style models such as QwQ and OpenAIs o1-preview. It is not trained with reinforcement learning or fine-tuned at all. Instead, Search-o1 leverages frozen LLMs and augments them with retrieval by prompting the model to emit search queries mid-reasoning, enclosed in special tokens (e.g., <begin_search_query>... Retrieved documents are then post-processed using Reason-in-Documents module before being injected back into the reasoning flow. RAG-BM25 and RAG-E5 (Lewis et al., 2020). These are naive retrieval-augmented generation baselines with no model training. RAG-BM25 uses top-k documents retrieved from BM25 index, while RAG-E5 retrieves passages using dense retrieval based on E5 embeddings. In both settings, the retrieved documents are prepended to the input prompt and fed into frozen generator LLM. We set = 3, following prior study (Lin et al., 2023b; Jin et al., 2025). SFT and R1. On general-domain RAG datasets, we train an SFT model with Qwen2.5-3B-Instruct using the same dataset as Search-R1s 170k NQ+HotpotQA with TRL (von Werra et al., 2020) framework. R1 is the no search version of SearchR1 (Jin et al., 2025), replicating Deepseek-R1Zero (Guo et al., 2025) with small LLM. We use its publicly released checkpoint5. CoT (Wei et al., 2022) and Direct Inference. CoT (Chain-of-Thought) prompting instructs the LLM to generate intermediate reasoning steps before producing an answer, without any external retrieval. Direct Inference simply feeds the raw question into the LLM. Neither baseline involves any form of training or finetuning. To ensure fair comparison, we set the maximum number of turns to 4 and limit the context to 3 documents per turn for all multi-turn baselines (IRCoT, Search-R1, and Search-o1) and s3, aligning with prior study (Jin et al., 2025). A.2 Setup Details Hardware. All training and evaluation processes are run on five NVIDIA A100 80GB PCIe on system with an AMD EPYC 7513 32-Core Processor and 1.0 TB of RAM. 5https://huggingface.co/PeterJinGo/R1-nq_ hotpotqa_train-qwen2.5-7b-em-ppo-v0. 12 Software. We built s3 using Python 3.9, leveraging the VERL framework (Sheng et al., 2024)6 (v0.1) as the backbone for reinforcement learning with language models, and RAGEN (Wang et al., 2025)7 as the underlying multi-turn RL architecture. Our implementation uses vLLM (v0.8.5) (Kwon et al., 2023) for fast LLM inference and evaluation, PyTorch (v2.4.0) with CUDA 12.1 for deep learning, and Ray (Moritz et al., 2018) for distributed training and serving. To improve performance, we integrate Flash Attention 2 (Dao, 2023) for efficient attention computation, PySerini (v0.22.1) (Lin et al., 2021) for retrieval and evaluation, and FAISS-GPU (v1.7.2) (Douze et al., 2024) for high-speed dense retrieval. Model parameters. We fine-tune Qwen2.5-7BInstruct using Proximal Policy Optimization (PPO) via VERL. Training is conducted with total batch size of 120, using micro-batches of size 15 for the actor and 10 for the critic, and rollout temperature of 0.6. The actor and critic learning rates are set to 1 106 and 1 105, respectively, with no warm-up for the actor and 1% warmup ratio for the critic. Both models use gradient checkpointing and parameter offloading to reduce memory overhead. Following prior work (Jin et al., 2025), we adopt XFORMERS (Lefaudeux et al., 2022) as the attention backend in vLLM and enable state masking to prevent incorrect supervision signals. KL regularization is applied with coefficient of 0.001. For answer generation and LLM-based judge_check during training, we run Qwen2.5-14B-Instruct-GPTQ-Int48 on dedicated A100 80GB GPU with vLLM. The retriever (E5base) is deployed alongside PySerini on the same five GPUs used for PPO training. The context window is set to 8,000 tokens, with maximum of 1,400 tokens allocated to the top-k retrieved documents per turn. A.3 Datasets & Corpora Datasets. We evaluate on six general-domain QA datasets and five medical-domain QA datasets. General-domain datasets include Natural Questions (NQ) (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), PopQA (Mallen et al., 2022), HotpotQA (Yang et al., 2018), 2WikiMultihopQA (Ho et al., 2020), and Musique (Trivedi 6https://github.com/volcengine/verl 7https://github.com/RAGEN-AI/RAGEN 8https://huggingface.co/Qwen/Qwen2. 5-14B-Instruct-GPTQ-Int et al., 2022). 9 For medical-domain, we adopt the MIRAGE benchmark (Xiong et al., 2024), which includes five datasets: MedQA-US (Jin et al., 2021), MedMCQA (Pal et al., 2022), PubMedQA* (Jin et al., 2019), BioASQ-Y/N (Tsatsaronis et al., 2015; Krithara et al., 2023), and MMLU-Med (Hendrycks et al., 2020).10 Corpora. For general-domain QA, we follow prior work (Jin et al., 2025) and use the Wikipedia 2018 dump (Karpukhin et al., 2020) as the sole knowledge source.11 For medical-domain QA, we evaluate under two corpus settings: (1) the Wikipedia 2018 dump (Karpukhin et al., 2020) alone, and (2) composite biomedical corpus introduced by (Xiong et al., 2024), which combines Wikipedia, PubMed, and textbook documents to provide broader domain coverage.12 Use of Artifacts. All datasets and models are used strictly within research contexts, consistent with their intended use and licensing. Our derived artifacts (e.g., retrieved documents, trained models) are likewise restricted to non-commercial academic use. A.4 Generation Accuracy Computation To evaluate the effectiveness of retrieval strategies in improving answer generation, we adopt composite metric called Generation Accuracy (GenAcc), which is designed to better reflect semantic correctness than surface-form exact match. Overview. Given model prediction and set of gold answers A, GenAcc is defined in Eq. 3. This metric returns 1 if either string-normalized token span of any is found within p, or if frozen LLM judge deems the answer semantically correct. It returns 0 otherwise. 1. Span-Based Matching. We first apply deterministic span check using normalized string comparison. Specifically, we: 9All the general-domain QA datasets are available https://huggingface.co/datasets/RUC-NLPIR/ at FlashRAG_datasets. 10All the medical-domain QA datasets are available https://github.com/Teddy-XiongGZ/MIRAGE/blob/ at main/benchmark.json. 11Wikipedia dump //huggingface.co/datasets/RUC-NLPIR/FlashRAG_ datasets (retrieval-corpus folder) available 2018 at is https: 12All the corpora for medical RAG are available at https: //huggingface.co/MedRAG. 13 Convert both prediction and gold answers to Success Case (Units and Symbols): lowercase. Remove punctuation and articles (a, an, the). Apply whitespace normalization. We then use tokenizer to compare whether any token span in the prediction matches any normalized gold answer. If match is found, the score is 1. Examples: Success Case: Prediction: \"The 44th President of the United States was Barack Obama.\" Gold Answer: \"Barack Obama\" Result: Span match succeeds because the normalized gold answer is token span in the prediction. Failure Case (Negation): Prediction: \"That statement is not true.\" Gold Answer: \"true\" Result: Span match incorrectly succeeds due to token overlap, despite the semantic meaning being opposite. We exclude such yes/no cases from training to avoid this issue. Failure Case (Paraphrase): Prediction: \"He led the civil rights movement in the 1960s.\" Gold Answer: \"Martin Luther King Jr.\" Result: Span match fails because the gold answer does not appear verbatim in the response, even though the answer is implied. LLM-Based Semantic Judging. 2. If the span check fails (0), we invoke lightweight correctness check using frozen LLM (e.g., Qwen2.5-14B-Instruct-GPTQ-Int4 for training or Claude-3-Haiku for evaluation). We prompt the model with: Please check if any of the golden answers is contained in the following response: {p} Golden answers: {str(A)} Directly answer with yes or no. If the LLM outputs yes, we consider the prediction correct and set the score to 1. Examples: Success Case (Numerical Format): Prediction: \"The answer is twenty-five.\" Gold Answer: \"25\" Result: Span match fails due to different formats, but the LLM outputs yes based on numerical equivalence. Prediction: \"It weighs 3 kilograms.\" Gold Answer: \"3 kg\" Result: Span match fails due to token mismatch, but the LLM recognizes them as equivalent and answers yes. Failure Case (Incorrect Entity): Prediction: \"The capital of France is Marseille.\" Gold Answer: \"Paris\" Result: Span match fails, and the LLM also outputs no, indicating semantic disagreement. Motivation. This design avoids brittle behavior from exact match metrics and aligns more closely with human judgments. For instance, if the gold answer is \"Einstein\" and the model prediction is \"Albert Einstein was the scientist who developed the theory of relativity\", our metric returns 1, while exact match fails due to surface mismatch. Empirically, GenAcc matches human labels on 96.4% of samples (see Appendix B), whereas EM only achieves 15.8%. Implementation. The full reward computing pipeline is implemented through the following components: span_check: This function (1) normalizes both prediction and gold answers by applying casefolding, punctuation and article removal, and whitespace normalization; and (2) performs token-level span matching using tokenizer. This step follows the evaluation strategy introduced in prior work (Ma et al., 2021) and leverages the has_answer utility from PySerini13. judge_check: If the span check fails, this fallback invokes frozen LLM to assess whether the prediction semantically entails any gold answer. The LLM is prompted to respond with binary judgment (\"yes\" or \"no\"). check_answer_correct: This function coordinates the evaluation process. It first applies span_check; if that fails, it falls back to judge_check for semantic validation. Note: For the medical RAG benchmark (MIRAGE (Xiong et al., 2024)) evaluation, we exclusively use judge_check, as most questions are multiplechoice and span_check can incorrectly accept wrong answers due to its strict matching criteria. 13https://github.com/castorini/pyserini/blob/ master/pyserini/eval/evaluate_dpr_retrieval.py 14 This hybrid strategy combines the efficiency of lexical matching with the robustness of LLM-based semantic evaluation, ensuring reliable and scalable answer correctness assessment. A.5 Document Extraction Logic We extract document titles and texts from information blocks using structured approach that prioritizes important documents. Our extraction algorithm processes text with the following format: <information> Doc 1 (Title: \"Document Title 1\") ... Doc 2 (Title: \"Document Title 2\") ... </information> <important_info> [1, 3] </important_info> The algorithm follows these key rules: Human Evaluation Instruction You are an evaluator for question-answering systems. Your task is to determine whether the system-generated answer aligns with the provided gold (reference) answers. Evaluation Criteria: An answer should be marked as correct (1) if it: Contains the same key information as the golden answers; Expresses the same meaning, even if using different wording; Is factually consistent with the golden answers. Please input only: \"1\" if the systems answer aligns with <important_info> tags apply only to the most the golden answers; recent <information> block If no <important_info> tag exists for <information> block, all documents from that block are included Documents are deduplicated based on content The implementation uses regular expressions to: 1. Identify all information blocks and important document tags 2. Associate each important info tag with its corresponding information block 3. Extract document IDs, titles, and text content 4. Filter documents based on importance markers The document pattern is matched using regex that handles variations in spacing and optional quotes around titles. Our implementation includes appropriate error handling to manage parsing failures and maintains the original order of documents. The algorithm has O(n) time complexity where is the input string length, with additional factors related to the number of documents and information blocks."
        },
        {
            "title": "B Human Alignment Study of Evaluation",
            "content": "Metrics (GenAcc and EM) \"0\" if it does not. Figure 7: Instruction for human evaluation of LLM generation. randomly sampled 1,000 answer generations from the general-domain QA test set. Each sample was labeled as Correct (1) or Incorrect (0) by human annotators, consisting of two Ph.D. students and one M.S. student majoring in computer science who evenly divided the annotation workload. Figure 7 shows the instruction, and the anonymous sheet14 shows the raw results. We then compared these human labels against the binary decisions made by Generation Accuracy and Exact Match. As shown in Figure 8, Generation Accuracy demonstrates strong alignment with human evaluation, correctly identifying 96.4% of answers that were judged correct by humans. In contrast, Exact Match only captures 15.8% of such answers, largely due to its strict reliance on string matching. These results confirm that Generation Accuracy is more reliable and human-aligned metric, especially for evaluating free-form and abstractive answers where surface forms may differ despite To assess the alignment of our primary evaluation metric, Generation Accuracy, with human judgment, we conducted human annotation study. We 14(Anonymous) raw results of human-metric alignment https://docs.google.com/spreadsheets/d/e/ study: 2PACX-1vQ-aAC6FNJYFJk1Ca8-EGN1zHa5z8WoF0Fm2VIHoWO_ CA0Gaa-f_uy8JGX-NiRO9l2yDaJTxU0nObjG/pubhtml 15 Configuration NQ TriviaQA PopQA HotpotQA 2Wiki Musique Retrieval: 8, Selection: 3, Turns: 3 Full Implementation w/o Selection w/o Begin with Search w/o Both 70.5(3.2) 70.7(2.7) 68.6(3.6) 70.8(2.5) 84.0(24.6) 83.1(18.0) 82.2(25.5) 83.2(18.2) 57.7(5.9) 57.2(8.1) 55.0(7.7) 56.5(7.8) 62.4(11.1) 61.1(8.4) 57.0(11.8) 60.1(8.7) Retrieval: 5, Selection: 3, Turns: 3 Full Implementation w/o Selection w/o Begin with Search w/o Both 69.6(3.5) 70.8(2.6) 67.6(4.0) 70.6(2.6) 83.4(24.3) 81.8(19.6) 81.2(26.6) 81.9(19.4) 57.4(5.8) 56.3(9.6) 55.0(8.3) 56.0(8.6) 62.0(11.9) 60.8(9.4) 57.4(12.0) 60.0(9.1) Retrieval: 3, Selection: 3, Turns: 3 Full Implementation w/o Selection w/o Begin with Search w/o Both 69.4(3.5) 69.7(5.0) 67.7(3.8) 69.2(3.4) 82.3(24.4) 81.6(27.8) 81.1(25.5) 81.5(24.4) 57.0(5.7) 56.1(12.5) 54.2(6.7) 55.2(8.9) 61.8(11.7) 59.7(11.4) 58.1(11.9) 58.3(10.4) 55.1(8.3) 58.9(3.3) 46.8(7.9) 57.4(3.5) 53.8(7.8) 57.8(3.0) 50.0(9.3) 57.6(3.2) 51.5(8.2) 56.2(4.3) 50.2(7.4) 54.6(3.3) 26.2(7.9) 22.5(1.6) 20.9(2.3) 21.8(1.7) 24.5(2.3) 22.4(2.0) 21.1(2.2) 22.3(1.7) 25.1(2.3) 23.5(2.2) 22.1(2.5) 22.5(2.4) Table 6: Ablation Studies of s3 on General Domain RAG. We show generation accuracy as the main results and exact match scores in brackets. semantic correctness, which also syncs with findings by prior studies applying similar evaluation methods (Song et al., 2025)."
        },
        {
            "title": "C Prompts",
            "content": "To train and evaluate the s3 framework effectively, we design three system prompts targeting distinct modules: the search policy (Searcher), answer generation, and judge-based evaluation. Each prompt is carefully constructed to ensure modularity, interpretability, and compatibility with frozen LLMs. Searcher Prompt. The prompt for the Searcher (Figure 11) guides trained policy to perform structured multi-turn search. It defines loop-based instruction set that mimics real-world decisionmaking: the model emits search query, inspects results, selects key documents, and decides whether to continue searching. This design supports iterative refinement and selection via: <query>: the generated search query in JSON format. <information>: the retrieved documents returned by the search engine. <important_info>: subset of documents deemed most relevant (up to 3). <search_complete>: binary decision on whether to stop searching. only Importantly, in <important_info> are visible to the generator, encouraging the policy to focus on high-quality documents selected Figure 8: Confusion matrices comparing Generation Accuracy (top) and Exact Match (bottom) against human judgment. Each cell indicates the proportion of samples falling into the corresponding category. 16 Figure 9: Scalability study: mean reward curve when training s3 (5-3-4) for 300 steps. Figure 10: Performance comparison at Step 20 vs. Step 300 across datasets. data and training. Specifically, we train the 5-3-4 configuration for up to 300 steps. Figure 9 shows the reward curve over training steps. We observe consistent upward trend, indicating that the search policy continues to improve with more data and training iterations. To quantify this improvement, Figure 10 compares the models QA performance at step 20 and step 300 across six datasets. The results show that s3 scales gracefully: most datasets exhibit steady gains, with improvements particularly noticeable on PopQA, HotpotQA, and Musique. These findings suggest that s3 can also benefit from larger-scale training, making it flexible framework that performs well both in low-resource and high-resource settings. evidence rather than breadth. By isolating retrieval behavior from generation, this prompt allows reinforcement learning with frozen black-box LLM using downstream answer quality as reward. Answer Generation Prompt. Figure 12 shows the prompt used for final answer generation. It provides the accumulated context from selected documents along with the users original question. The generator is instructed to produce direct, succinct answer without verbosity. This format simplifies reward computation and ensures generation outputs are consistent and easy to evaluate. Judge_Check Prompt. To enable scalable, automated evaluation during training and inference, we employ lightweight correctness prompt shown in Figure 13. This prompt asks an LLM to verify whether any gold answer appears in the predicted response. Unlike brittle exact-match metrics, this approach captures semantically valid completions even if they differ in surface form. During training, quantized Qwen2.5-14B model is used for cost-effective inference, while evaluation employs Claude-3-Haiku for higher reliability. Together, these prompts form coherent pipeline that supports modular training and evaluation of retrieval-augmented generation systems. The clear separation of roles allows s3 to focus learning solely on the search agent, and our prompt designs play key role in realizing this clean decoupling."
        },
        {
            "title": "D Scalability Study",
            "content": "While s3 demonstrates strong performance with just 20 training steps (i.e., 2.4k examples), we investigate how performance evolves with additional 17 Prompt Instructions for Searcher You are search copilot for generation model. Based on users query and initial searched results, you will first determine if the searched results are enough to produce an answer. If the searched results are enough, you will use <search_complete>True</search_complete> to indicate that you have gathered enough information for the generation model to produce an answer. If the searched results are not enough, you will go through loop of <query> <information> <important_info> <search_complete> <query> (if not complete) ..., to help the generation model to generate better answer with more relevant information searched. You should show the search query between <query> and </query> in JSON format. Based on the search query, we will return the top searched results between <information> and </information>. You need to put the doc ids of the important documents (up to 3 documents, within the current information window) between <important_info> and </important_info> (e.g., <important_info>[1, 4]</important_info>). search query must be followed by <search_complete> tag if the search is not complete. After reviewing the information, you must decide whether to continue searching with new query or indicate that the search is complete. If you need more information, use <search_complete>False</search_complete>. Otherwise, use <search_complete>True</search_complete> to terminate the search. Note: Only the content between <important_info> will be used by the generation model to produce an answer. [An abstractive example of s3 loop] <question> {question} </question> <information> {initial_search_result} </information> Figure 11: The prompt used for search policy (searcher). Prompt for Answer Generation Use the following contexts (some might be irrelevant) on demand: Contexts: {context} Question: {question} Important: You MUST directly answer the question without any other text. Prompt for Judge_Check Please check if any of the golden answers is contained in the following response: {response} Golden answers: {golden_answers} Please directly answer with yes or no. Figure 12: The prompt used for answer generation by frozen LLM. Figure 13: The prompt used for judge_check. Qwen2.514B-Instruct-GPTQ-Int4 is used in training, and Claude3-Haiku is used in evaluation. Question <question> What year was the film made that was about the inventor of type of passive solar house that is made of both natural and upcycled materials such as earth-packed tires? </question> s3 Search Trajectory <question> What year was the film made that was about the inventor of type of passive solar house that is made of both natural and upcycled materials such as earth-packed tires? </question> <information> Doc 1 (Earthship): An Earthship is type of passive solar house... pioneered by architect Michael Reynolds... Doc 2 (Mike Reynolds): Known for Earthship houses using recycled materials like tires... Doc 3 (Don Stephens): Discusses earth-integrated designs and upcycled materials... </information> <search_complete> False </search_complete> <query> What year was the film made about the inventor of Earthship?\" </query> <information> Doc 1 (Garbage Warrior): Garbage Warrior is 2007 film about architect Mike Reynolds, inventor of the Earthship style of building... Doc 2 (Garbage Warrior): 2007 film following Reynolds and his Earthship building... Doc 3 (Earthship): Mentions the film Garbage Warrior and its focus on off-grid housing... </information> <important_info>[1, 2]</important_info> <search_complete> True </search_complete> Doc 1 (Earthship): An Earthship is type of passive solar house... pioneered by architect Michael Reynolds... Doc 2 (Mike Reynolds): Known for Earthship houses using recycled materials like tires... Doc 3 (Don Stephens): Discusses earth-integrated designs and upcycled materials... Doc 4 (Garbage Warrior): Garbage Warrior is 2007 film about architect Mike Reynolds, inventor of the Earthship style of building... Doc 5 (Garbage Warrior): 2007 film following Reynolds and his Earthship building... s3 Searched Docs Golden Answer RAG Answer s3 Answer 2007 There is no specific year mentioned for film made about the inventor of the Earthship, which is type of passive solar house made of natural and upcycled materials like earth-packed tires. The information provided does not include details about particular film or its release year. 2007 Table 7: An example showing how s3 searches and selects correct evidence after issuing focused search query. RAG fails to answer correctly without temporal grounding."
        }
    ],
    "affiliations": [
        "Amazon",
        "University of Illinois"
    ]
}