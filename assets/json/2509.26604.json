{
    "paper_title": "Video Object Segmentation-Aware Audio Generation",
    "authors": [
        "Ilpo Viertola",
        "Vladimir Iashin",
        "Esa Rahtu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Existing multimodal audio generation models often lack precise user control, which limits their applicability in professional Foley workflows. In particular, these models focus on the entire video and do not provide precise methods for prioritizing a specific object within a scene, generating unnecessary background sounds, or focusing on the wrong objects. To address this gap, we introduce the novel task of video object segmentation-aware audio generation, which explicitly conditions sound synthesis on object-level segmentation maps. We present SAGANet, a new multimodal generative model that enables controllable audio generation by leveraging visual segmentation masks along with video and textual cues. Our model provides users with fine-grained and visually localized control over audio generation. To support this task and further research on segmentation-aware Foley, we propose Segmented Music Solos, a benchmark dataset of musical instrument performance videos with segmentation information. Our method demonstrates substantial improvements over current state-of-the-art methods and sets a new standard for controllable, high-fidelity Foley synthesis. Code, samples, and Segmented Music Solos are available at https://saganet.notion.site"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 4 0 6 6 2 . 9 0 5 2 : r Video Object Segmentation-Aware Audio Generation Ilpo Viertola1, Vladimir Iashin2, and Esa Rahtu1 1 Tampere University, Tampere, Finland ilpo.viertola@tuni.fi 2 University of Oxford, Oxford, UK Abstract. Existing multimodal audio generation models often lack precise user control, which limits their applicability in professional Foley workflows. In particular, these models focus on the entire video and do not provide precise methods for prioritizing specific object within scene, generating unnecessary background sounds, or focusing on the wrong objects. To address this gap, we introduce the novel task of video object segmentation-aware audio generation, which explicitly conditions sound synthesis on object-level segmentation maps. We present SAGANet, new multimodal generative model that enables controllable audio generation by leveraging visual segmentation masks along with video and textual cues. Our model provides users with fine-grained and visually localized control over audio generation. To support this task and further research on segmentation-aware Foley, we propose Segmented Music Solos, benchmark dataset of musical instrument performance videos with segmentation information. Our method demonstrates substantial improvements over current state-of-the-art methods and sets new standard for controllable, high-fidelity Foley synthesis. Code, samples, and Segmented Music Solos are available at saganet.notion.site. Keywords: Artificial Foley Multimodal Generative Modeling"
        },
        {
            "title": "Introduction",
            "content": "Multimodal audio generation focuses on synthesizing audio, given conditional video feed, textual description, or conditions in other modalities. These models can be utilized in Foley processing, where the goal is to produce soundtrack for video. For the artificial Foley models to thrive, e.g., as video post-processing tool, the end-user has to have high control over the synthesized result. In addition to controllability, the model has to produce high-quality samples with great semantic and temporal accuracy. Recent advances in diffusion [29] and conditional flow matching (CFM) [54] models have improved the audio quality of the artificial Foley models [5,6,36,40, 57, 58, 62]. However, existing models still lack precise user guidance and control. Although recent models [5,6,36,40] introduce more modalities to guide the audio generation process, they still lack precision. In complex scenes, describing the target object using e.g., text, quickly becomes unfeasible. 2 I. Viertola et al. To improve control in Foley generation, we propose new audio synthesis task: video object segmentation-aware audio generation. This task focuses on nuanced control, highlighting the models ability to generate audio for specific object in the video rather than for the full scene. To the best of our knowledge, we are first to utilise visual segmentation information in audio generation task. Existing state-of-the-art methods [5,6,40,56,58] focus on training the models from scratch, utilizing large uniand multimodal datasets. It demands significant computational resources and quickly becomes infeasible in academic environments. For example, Movie Gen Audio [40] was pretrained on 384 high-end H100 GPUs for 14 days and later fine-tuned on 64 H100s for 24 hours. MMAudio [6] was trained on the same high-end GPU hardware, only utilizing fewer GPU hours per training run. On the other hand, many recent works aim to tackle the need for exhaustive training procedures by training smaller aligners or adapters for pretrained text-to-audio models to condition them using video sequence [19, 36, 57, 61]. These models require less compute power during training, but often struggle with temporal alignment. Alignment is challenging to achieve by fine-tuning text-to-audio model that lacks precise temporal control. Our approach draws on recent advances in multimodal audio generation [6] and localized image captioning [27]. To enable segmentation-aware audio generation, we develop self-supervised control module on top of pretrained network that enables users to select specific object in video to generate sound for. By training our control component on small yet high-quality dataset, we achieve better controllability, temporal synchronization, and semantic quality compared to the original model, training only fraction of the parameters. Although our model is trained on videos with single audio source, it can generalize to scenes with multiple audio sources and generate audio for the target object. To train the segmentation-aware control module, we propose high-quality dataset with sounding object segmentation maps and high audio-video correspondence. The dataset consists of solo acts played on variety of musical instruments. We curate the dataset based on Solos [37], AVSBench [66, 67], and MUSIC21 [64,65] datasets. We design pipeline that generates short video clips given the original videos, ensuring that the target sounding object is present in both the auditory and visual modalities. Additionally, we extract visual segmentation maps of the target (sounding) object. We refer to this dataset as Segmented Music Solos. For testing purposes, we utilize the University of Rochester Multi-Modal Music Performance (URMP) dataset [25]. Our contributions can be summarized as follows: i) new audio synthesis task, namely video object segmentation-aware audio generation, ii) video object segmentation-aware control for state-of-the-art multimodal generative audio model, iii) we show that by training our model with single-source samples it can generate audio for target object in multi-source scenes, and iv) new benchmark dataset, Segmented Music Solos, with sounding object segmentation information. Video Object Segmentation-Aware Audio Generation"
        },
        {
            "title": "2.1 Video Object Segmentation",
            "content": "Video Object Segmentation (VOS) refers to the task of segmenting and tracking objects at the pixel level across video frames while maintaining temporal consistency [41]. It typically involves distinguishing foreground objects, such as people, animals, or vehicles, from the background. Different types of VOS tasks include semi-supervised and unsupervised VOS. In semi-supervised VOS, ground-truth mask is provided for the target object in the first frame, and the goal is to segment the object in the remaining frames [41]. In unsupervised VOS, no initial mask is given. The goal is to discover and segment prominent objects automatically [9]. Recent advancements have introduced promptable VOS models. SAM2 [43] is widely adopted and powerful semi-supervised video object segmentation model capable of real-time object segmentation with images and videos. The initial mask of the segmented object can be provided manually or based on location coordinates. SAM2 facilitates applications such as video editing, mixed reality experiences, and efficient annotation of visual data for training computer vision systems. GroundedSAM2 [46] combines SAM2 with grounding models in single pipeline, enabling grounding and tracking anything in videos. GroundedSAM2 enables segmenting objects based on natural language queries, making it powerful tool, e.g., in data generation. We utilize GroundedSAM2 with Florence-2 [60] foundation model and SAM2 by prompting it with location coordinates in our data generation pipeline. We utilize the segmentation maps of sounding objects later during training of our method"
        },
        {
            "title": "2.2 Artificial Foley Models",
            "content": "Artificial Foley models have gained considerable popularity. Many published models are built on top of autoregressive transformer architecture [16,35,49,56]. Also, another transformer-based approach is to utilize Masked Generative Image Transformer (MaskGIT) [2] schema for the audio generation task [32, 38, 51, 68]. Another popular approach is to utilize diffusion [3, 34, 62] or flow-matching methods [6, 58]. To avoid resourceand time-exhaustive training from scratch, prior work has explored using pretrained text-to-audio models for video-to-audio generation by training lightweight feature aligners or control modules between the modalities [19,36,57,61]. Although these models achieve good audio quality, they often struggle to generate temporally aligned audio. When the text modality is fixed during training, learning an aligned feature space between modalities becomes challenging. The current Foley methods lack controllability. For artists to fulfil their needs in applications such as video post-processing, they must have fine-grained control over the model. One way to add control is to introduce conditioning signals from other modalities, e.g., text. 4 I. Viertola et al. Although early multimodal approaches, e.g. text-and-video-to-audio models, did not meet the generation quality compared with dedicated video-to-audio models [21,47,52,53], recent work [5,6,40] has shown that artificial Foley models can benefit from multimodality. For example, Cheng et al. [6] show that training generative audio network with combined text-audio and text-video-audio data enables high generation quality while preserving temporal and semantic alignment with video and text. However, the user controllability remains limited since describing the target objects with textual prompts in complex scenes can be difficult or infeasible. To tackle this, we develop novel model that supports audio generation conditioned with text and video, but also allows users to define the sounding object with semantic mask. In practice, user can click an object from video frame, and our model generates audio for that specific object."
        },
        {
            "title": "2.3 Controlled Generation with Pretrained Audio Networks",
            "content": "Adapting large pretrained networks to new conditioning inputs is widely studied topic. In the diffusion model domain, training ControlNet [63] is popular approach [14,19,36,59]. ControlNet enables fine-grained control by injecting conditioning features, such as visual cues, semantic tags, or motion information, into the denoising process of frozen diffusion model. However, training parallel ControlNet [63] to condition MMAudio [6] with video object segmentation information is unnecessary. Lian et al. [27] show that in visual captioning, combining local and global visual features from the same extractor using learnable gated cross-attention [1,26], along with object segmentation masks, improves localized captions. Their large transformer-based [55] captioning model is kept frozen, and only the feature extraction process is modified. Motivated by their findings, we design and implement localized visual feature extraction model in parallel to MMAudios [6] Synchformer-based [18] global feature extractor. Introducing the segmentation information already at the feature extraction stage allows us to use the same control module across all variants of the generative model. Thus, the number of trainable parameters remains the same even though the generative network size increases. In contrast, fusing the segmentation information via ControlNet-based approach would require training separate control module per model variant. Our approach integrates visual segmentation masks as new conditioning modality, without requiring full model fine-tuning or the training of variant-specific control modules."
        },
        {
            "title": "3 Method",
            "content": "We propose generative audio network that offers fine-grained user control through multiple input modalities: text, video, and video object segmentation masks. To enable this, we introduce novel video object segmentation-aware audio generation task. Our approach builds on top of the MMAudio model [6], state-of-the-art audio generation model conditioned on text and video. We extend MMAudio with segmentation-aware control module, resulting in the Video Object Segmentation-Aware Audio Generation 5 Fig. 1. Overview of SAGANet control module. Given video and its corresponding segmentation masks, the model combines global and local information streams. Gated Cross-Attention layers [1, 26] are used to fuse global and local features extracted by Synchformer [18], with shared weights across both branches. Only the layers highlighted in orange are updated during training. The final audio is generated following the same procedure as in the base MMAudio model. For additional details on MMAudio, refer to [6]. proposed model, Segmentation-Aware Generative Audio Network (SAGANet). To the best of our knowledge, we are the first to utilize visual segmentation information in the audio generation task. MMAudio is CFM-based [28, 31, 54, 58] model, utilizing Diffusion Transformer (DiT) [39] to approximate the velocity vector field. In addition to the improved usability of the original model, segmentation information enhances the quality and alignment of the synthesized audio. The overall architecture is shown in Fig. 2."
        },
        {
            "title": "Formulation",
            "content": "To challenge the precise control in artificial Foley models, we introduce new audio synthesis task: video object segmentation-aware audio generation. To support the proposed task, we curate dataset tailored to it (Sec. 4). In VOS-aware audio generation, the goal is to generate audio focused on the specified region within the video. Current artificial Foley models focus on the full scene, lacking fine-grained user controllability. However, focusing only on the small region is not enough, since the overall context is still vital in the generation of credible audio. Formally, given visual stream RTvHW 3, (Tv is frame count, is height, is width, and 3 is RGB color channels), and corresponding stream of binary masks {0, 1}TvHW 1, the goal is to produce audio focusing on the specified region in the visual stream = GM (V, M, ...) RTa , (GM is the 6 I. Viertola et al. generative model and Ta is temporal dimension). However, input modalities are model-dependent and might also include additional ones."
        },
        {
            "title": "3.2 SAGANet Architecture",
            "content": "Drawing on the success of DAM [27] in localized captioning, we adapt it into audio generation. The video object segmentation-aware control module is fused with the Synchformers [18] Vision Transformer (ViT) [8] based feature extractor. Synchformer employs TimeSformer, pretrained contrastively on sub-clip level with audio. Focal Prompt. To provide detailed information coupled with the global context and segmentation information for the visual feature extractor, Lian et al. [27] introduce focal prompt. The focal prompt consists of two different visual stream inputs. One is the original unmodified visual stream with the corresponding mask stream. To provide detailed information, the other stream is cropped around the region of interest along with its mask. cropped stream is referred to as focal crop or focal video. The focal prompt integrates the original and cropped streams with their corresponding masks, providing both global overview and detailed view of the target region. Cropping is done based on the mask. We crop the original video so that the masked area is visible throughout the video, but enforce minimum size of 48 48 pixels [27]. Bounding boxes are calculated for each provided mask, and their coordinates are used to determine the average center point and dimensions of the cropped frames. Localized Vision Backbone with Temporal Mask Embedding. Our approach builds on Lian et al. [27], who introduced localized visual backbone using mask streams to guide vision-language alignment. We extend this idea to video-based multimodal generation, where our goal is to extract temporallyaware and localized visual features to control an audio generation process. We define two video streams: the global video and its focal crop , along with their corresponding spatial segmentation masks and . These masks highlight semantically meaningful regions (e.g., instruments) to steer audio generation. Unlike Lian et al., who paired vision with language, we use these masks directly with vision transformer tailored for audio-video synchronization. To process these inputs, we first embed them into shared spatiotemporal representation. Specifically, both video and mask streams are passed through respective 3D patch embedding layers, EV and EM . We apply learnable positional encodings to inject temporal ordering and spatial locality, critical for enabling the transformer to learn meaningful correspondences across frames. The resulting embeddings are: = EV (V ) + EM (M ) + P, = EV (V ) + EM (M ) + P. (1) Video Object Segmentation-Aware Audio Generation 7 Here, and represent the embedded global and focal inputs, respectively. The mask embedding layer EM is initialized to output zeros, preventing earlystage training instabilities from disrupting the backbone. This design follows the initialization strategy in DAM [27]. Next, we extract visual features at two scales. The global stream is processed by global feature extractor FG, while the focal stream is passed through regional feature extractor FR that shares self-attention weights with FG. This weight sharing encourages alignment and reuse of representations across global and local views: Fsyn = FG(x), syn = FR(x, Fsyn). (2) To allow focal features to condition on the global visual context, we adopt gated cross-attention adapters [1, 26]. These modules are inserted after the selfattention and feedforward layers in each Synchformer block, enabling fine-grained integration of focal and global information. Each block in FR is updated as follows: h(l) = h(l) + tanh(γ(l)) CrossAttn(h(l), Fsyn) h(l) Adapter = h(l) + tanh(β(l)) FFN(h(l) ). (3) (4) Here, γ(l) and β(l) are learnable scale parameters initialized to zero to suppress noisy gradients at early training stages. The cross-attention enables focal tokens to selectively attend to relevant global features from Fsyn, improving localization and temporal consistency in the fused visual representation. The final fused representation syn is used to condition the DiT-based audio generator. By processing video and mask inputs in both global and localized views, we are able to capture fine-grained spatial cues and their temporal dynamics, essential for generating semantically aligned audio. Audio Generation. Fused visual features are used to condition the audio generation process of MMAudio [6] instead of the Synchformer [18] features. Otherwise, the conditioning is kept similar. Adapting the task formulation described in Sec. 3.1, we get = GM (F syn is the fused visual features, Fv is the visual CLIP [42] features, Ft is the textual features, and is the noisy audio latent). For details of the extraction of Fv and Ft, please refer to [6]. syn, Fv, Ft, a), (GM is the MMAudio model, To further enhance SAGANet performance, we experiment with fine-tuning the generative model by using Low-Rank Adaptation (LoRA) [15] during training. Specifically, we add low-rank matrices for query and value projections of the DiT attention blocks associated with the segmentation-aware visual features. I. Viertola et al. Fig. 2. Samples from Segmented Music Solos. The top row indicates the musical instrument label. The second row displays the first frames of the video and the corresponding mask stream. Last row displays the audio associated with the sample."
        },
        {
            "title": "4 Segmented Music Solos Dataset",
            "content": "To facilitate the training of video object segmentation-aware generative audio model, we propose Segmented Music Solos. We draw inspiration from audiovisual segmentation datasets, where data consists of single and multi-source videos [66, 67]. We hypothesize that by training our model using single-source videos, accompanied by the sounding object masks, our model learns to use the segmentation information for the audio generation. When multi-source scene is introduced during test time, the model is capable of generating sound for the segmented object. Our data pipeline draws inspiration from VGGSound [4]. Stage 1: Source Videos. The training data consists of video clips of people playing single instrument and segmentation masks of the instruments. The raw videos are gathered across multiple datasets. For the training and validation data, we combine solo performance videos from MUSIC21 [64, 65], AVSBench [66, 67], and Solos [37]. For testing, we use the URMP dataset [25]. It comprises several multi-instrument musical pieces assembled from separately recorded performances of individual tracks. Meaning that the final audio is combination of separate audio recordings. Thus, each instrument in multisource scenario has separately recorded audio, which is used as the ground truth in video object segmentation-aware audio generation. Stage 2: Visual Verification. We verify that the target instrument is visually present in the video. First, we split the video into scenes based on abrupt changes, then clip each scene to avoid transitions that could confuse the segmentation model. This helps SAM2 [43] maintain consistent tracking of the object across frames. Next, we check for the target object in each scene. We sample frames at 2 FPS and classify them using pretrained CNN-based model [20] trained on ImageNet [7]. If the target appears in the top 5 predictions for frame, we mark it as present. Because our target classes dont always match ImageNet labels directly, we use semantic similarity. We embed both our audio classes (depends on the dataset) and the 1000 ImageNet classes using MPNet [50], then compute cosine similarities to find the top 5 closest ImageNet labels for each Video Object Segmentation-Aware Audio Generation 9 audio class. These act as the visual signatures. sample is accepted if at least one of these matching labels appears in the top 5 predictions with confidence above 0.2. Stage 3: Auditory Verification. For each visually verified scene, we window the audio to match the temporal dimension of the visual verification process. For audio classification, we utilize Audio Spectrogram Transformer (AST) [12]. We follow the same procedure as in visual verification to compute the label similarities between the AST classifier and our audio classes. If the target object is present (or silence is detected) in the top 5 predictions for an audio window, we classify the object as present. We include silence to support learning of natural pauses during instrument play. Stage 4: Clipping Raw Videos. Given the scenes, we use the presence information to clip the raw videos into 5-second sequences. We require that the object be detected within the visual and auditory streams throughout the clip. Stage 5: Mask Generation For train data, we utilize the GroundedSAM2 framework [22, 43, 45] and obtain the initial mask by prompting the Florence2 [60] foundation model with the instrument label. Florence-2 produces the initial segmentation mask based on textual prompt, and SAM2 propagates the mask throughout the clip. Florence-2 was chosen based on its performance on small manually verified data subset compared with GroundingDINO versions 1.5 and 1.6 [30, 44]. For test data, we manually provide location coordinates of the target objects to prompt SAM2. Manual prompting yielded more coherent masks compared to using grounding model, but limited resources prevented us from manually annotating target objects in the training data. Finally, Segmented Music Solos consists of 5 395 training, 665 validation, and 745 test 5-second samples spanning over 25 different musical instruments. Every video is accompanied by segmentation masks that have the same number of frames as the video and the label of the segmented object. The frame rate of all samples is 25 FPS, and the audio sample rate is 44 100 Hz."
        },
        {
            "title": "5 Experiments",
            "content": "5."
        },
        {
            "title": "Implementation Details",
            "content": "We train and evaluate our method using Segmented Music Solos (Sec. 4). Training data consists of solo musical instrument performance videos, instrument text labels, and segmentation masks. During training, we drop the textual label with probability of 50% to facilitate learning of the segmentation information. Evaluation data consists of multi-instrument performance videos where the audio is from the segmented instrument, instrument text labels, and segmentation masks. 10 I. Viertola et al. Following [6, 17], we use H.264 and AAC video and audio encodings, resampled to 25 FPS and 44.1 kHz. The audio length is set to five seconds. For our pretrained model, we utilize MMAudio [6]. We train the video object segmentation-aware control module on top of the pretrained model and compare our method against the base model. We use learning rate of 1 104, and AdamW optimizer [33] with β = [0.9, 0.95]. Other training parameters are initialized following [6]. We train on 4 NVIDIA A100 40GB GPUs for 40 epochs until convergence. We also experiment with LoRA [15] fine-tuning of the query and value projections of the DiT layers associated with the segmentation-aware features. We use LoRA rank of 16 and set α = 32. During testing, we utilize classifier-free guidance [13] with the scale of 7.0. We use scale of 4.5 for the base model, as it yields the best performance."
        },
        {
            "title": "5.2 Evaluation Metrics",
            "content": "For fair comparison, we utilize the same evaluation pipeline as described in MMAudio [6]. Quality is evaluated over four different aspects: distribution matching, audio quality, semantic alignment, and temporal alignment. Distribution Matching. We compute Fréchet Distance (FD) and KullbackLeibler Distance (KL) between generated and ground truth samples. FD is calculated using VGGish [10] (FDVGG), PANNs (FDPANNs) [23], and PaSST (FDPaSST) [24] embeddings. KL is calculated using PANN (KLPANNs) and PaSST (KLPaSST) embeddings. Audio Quality. We utilize PANNs to calculate Inception Score (IS) [48]. IS does not compare the generated sample to the ground truth. It is metric of objective quality and diversity. Semantic Alignment. Using ImageBind [11] to calculate similarity score (IB-score) between the video and generated audio [56]. IB-score is the cosine distance between the audio and video embeddings. The ground truth video is cropped to primarily show the segmented instrument and its player. Temporal Alignment. We use Synchformer [18] to compute the average of absolute offset predictions between audio and video (DeSync) [56]. The ground truth videos are processed similarly to the IB-score."
        },
        {
            "title": "5.3 Results",
            "content": "SAGANet shows the benefit of video object segmentation-aware control compared to the base model. Added control is crucial in the scenes where the target object is presented among other sounding instruments. Note that the evaluation data consists of multi-source videos containing multiple instruments. Despite Video Object Segmentation-Aware Audio Generation 11 Table 1. SAGANet outperforms the base model. The added video object segmentationaware control module helps our model to focus on the correct object. MMAudio [6] allows guiding the focus only through the textual condition. Results were averaged over 5 samples of Segmented Music Solos (Sec. 4) evaluation data [25]. We utilize the MMAudio-S-44.1kHz variant in our experiments. : Fine-tuned DiT-layers associated with visual segmentation-aware features using LoRA [15]. FDPaSST FDPANNs FDVGG KLPANNs KLPaSST IS IB-score DeSync MMAudio [6] SAGANet SAGANet 530.60 475.09 378.90 23.83 21.12 18. 13.26 10.95 16.78 1.17 0.94 0.81 1.00 0.74 0.62 35.94 2.24 2.75 38.50 2.89 40.87 0.95 0.43 0.31 being trained solely on single-source samples, our model demonstrates strong generalization to multi-source scenarios, effectively focusing on the target regardless of multiple instruments in the visual input. Textually describing the target object, accompanied by visual feed, does not provide strong enough guidance for the base model to generate temporally and semantically aligned audio. Main Results. We report the results in Table 1. We use Segmented Music Solos evaluation set (Sec. 4) and average over 5 samples. During evaluation, MMAudio [6] is conditioned on the full frames and with the textual label of the target object. Still, MMAudio also focuses on other instruments within the scene. Adding segmentation-aware control guides the model to focus solely on the target object. This is evident from the strong semantic similarity and temporal alignment achieved by our approach. Finetuning the DiT layers related to visual segmentation-aware features using LoRA [15] improves performance by helping the generative model better adapt to these features. Our method exceeds all the metrics compared to the base model. The greatest difference is with the temporal synchronization. Without the segmentation-aware control module, the model fails to attend to the correct object. Thus, even though the overall audio quality is sufficient, the temporal alignment is missing. Ablation Study. In Table 2, we analyze the effect of different visual prompts on the generated audio quality. Masks are embedded and fused in same way as described in Sec. 3.2. Last row indicates the performance of the proposed model that fuses information across both streams. Our study highlights that both the global and local visual information are crucial in generating high-quality audio. Using only global information yields the best audio quality (IS), but temporal synchronization (DeSync) is poor. Using cropped frames strengthens the temporal generation quality while decrading the overall audio quality. Incorporating mask information strengthens the temporal performance when only global information is used. On the other hand, fusing segmentation masks with local visual information improves the generated audio quality. 12 I. Viertola et al. Fusing local and global features results in superior temporal performance while maintaining comparable audio quality. Also, it results in semantically most relevant audio (IB-Score), highlighting the importance of combining the global context with detailed local information. Table 2. Ablation study of different visual prompts. Focal crop refers to using only the detailed crop without global visual information. Masks are embedded and fused with the same strategy as with the proposed method. The base model is MMAudio-S44.1kHz [6]. We utilize the MMAudio-S-44.1kHz variant in our experiments. : Finetuned DiT-layers associated with visual segmentation-aware features using LoRA [15]. FDPaSST FDPANNs FDVGG KLPANNs KLPaSST IS IB-score DeSync Full Frames 530. 23.83 13.26 1.17 1.00 2.24 35. 0.96 Full Frames + Mask 394.46 19.81 19. 0.84 0.70 3.11 39.25 0.56 Focal Frames 419. 17.41 11.88 0.86 0.70 2.56 39. 0.40 Focal Frames + Mask 363.86 17.74 17. 0.74 0.63 3.05 39.96 0.52 SAGANet 378.90 18.38 16.78 0.81 0.62 2.89 40. 0."
        },
        {
            "title": "6 Conclusion",
            "content": "Current generative audio models lack precise controllability, which limits their introduction e.g., in video post-production. To address the gap, we introduced novel audio synthesis task: Video Object Segmentation-aware Audio Generation. It enables precise control over audio synthesis by conditioning the audio synthesis on visual segmentation masks. For the task, we proposed SAGANet, generative audio model conditioned on text, video, and video object segmentation masks. Our method incorporates both global and localized visual information using dedicated control module, allowing the model to focus on specific objects within video. Our approach significantly improves semantic relevance and temporal alignment, particularly in complex, multi-source scenes where textual or global visual cues fall short. Furthermore, we presented Segmented Music Solos, benchmark dataset that supports the development and evaluation of VOS-aware audio generation models. We show that by training our model with single-source videos, our method can generalize to multi-source samples during test time. This work lays the foundation for the development of more controllable and user-friendly Foley models. Acknowledgments. The work was supported by the Academy of Finland projects 353139 and 362409. We also acknowledge CSC IT Center for Science, Finland, for computational resources. Video Object Segmentation-Aware Audio Generation 13 Disclosure of Interests. The authors have no competing interests to declare that are relevant to the content of this article."
        },
        {
            "title": "References",
            "content": "1. Alayrac, J.B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al.: Flamingo: visual language model for few-shot learning. Advances in neural information processing systems 35, 23716 23736 (2022) 2. Chen, C., Peng, P., Baid, A., Xue, Z., Hsu, W.N., Harwath, D., Grauman, K.: Action2sound: Ambient-aware generation of action sounds from egocentric videos. In: European Conference on Computer Vision. pp. 277295. Springer (2024) 3. Chen, C., Peng, P., Baid, A., Xue, Z., Hsu, W.N., Harwath, D., Grauman, K.: Action2sound: Ambient-aware generation of action sounds from egocentric videos. In: European Conference on Computer Vision. pp. 277295. Springer (2024) 4. Chen, H., Xie, W., Vedaldi, A., Zisserman, A.: Vggsound: large-scale audiovisual dataset. In: ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). pp. 721725. IEEE (2020) 5. Chen, Z., Seetharaman, P., Russell, B., Nieto, O., Bourgin, D., Owens, A., Salamon, J.: Video-guided foley sound generation with multimodal controls. arXiv preprint arXiv:2411.17698 (2024) 6. Cheng, H.K., Ishii, M., Hayakawa, A., Shibuya, T., Schwing, A., Mitsufuji, Y.: Taming multimodal joint training for high-quality video-to-audio synthesis. arXiv preprint arXiv:2412.15322 (2024) 7. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: largescale hierarchical image database. In: 2009 IEEE conference on computer vision and pattern recognition. pp. 248255. Ieee (2009) 8. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 (2020) 9. Fragkiadaki, K., Arbelaez, P., Felsen, P., Malik, J.: Learning to segment moving objects in videos. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 40834090 (2015) 10. Gemmeke, J.F., Ellis, D.P., Freedman, D., Jansen, A., Lawrence, W., Moore, R.C., Plakal, M., Ritter, M.: Audio set: An ontology and human-labeled dataset for audio events. In: 2017 IEEE international conference on acoustics, speech and signal processing (ICASSP). pp. 776780. IEEE (2017) 11. Girdhar, R., El-Nouby, A., Liu, Z., Singh, M., Alwala, K.V., Joulin, A., Misra, I.: Imagebind: One embedding space to bind them all. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 15180 15190 (2023) 12. Gong, Y., Chung, Y.A., Glass, J.: Ast: Audio spectrogram transformer. arXiv preprint arXiv:2104.01778 (2021) 13. Ho, J., Salimans, T.: Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598 (2022) 14. Hou, S., Liu, S., Yuan, R., Xue, W., Shan, Y., Zhao, M., Zhang, C.: Editing music with melody and text: Using controlnet for diffusion transformer. In: ICASSP 20252025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). pp. 15. IEEE (2025) I. Viertola et al. 15. Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W.: Lora: Low-rank adaptation of large language models. arxiv 2021. arXiv preprint arXiv:2106.09685 (2021) 16. Iashin, V., Rahtu, E.: Taming visually guided sound generation. arXiv preprint arXiv:2110.08791 (2021) 17. Iashin, V., Xie, W., Rahtu, E., Zisserman, A.: Sparse in space and time: Audiovisual synchronisation with trainable selectors. arXiv preprint arXiv:2210.07055 (2022) 18. Iashin, V., Xie, W., Rahtu, E., Zisserman, A.: Synchformer: Efficient synchronization from sparse cues. In: ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). pp. 53255329. IEEE (2024) 19. Jeong, Y., Kim, Y., Chun, S., Lee, J.: Read, watch and scream! sound generation from text and video. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 39, pp. 1759017598 (2025) 20. Jocher, G., Chaurasia, A., Stoken, A., Borovec, J., Kwon, Y., Michael, K., Fang, J., Yifu, Z., Wong, C., Montes, D., et al.: ultralytics/yolov5: v7. 0-yolov5 sota realtime instance segmentation. Zenodo (2022) 21. Kim, G., Martinez, A., Su, Y.C., Jou, B., Lezama, J., Gupta, A., Yu, L., Jiang, L., Jansen, A., Walker, J., et al.: versatile diffusion transformer with mixture of noise levels for audiovisual generation. arXiv preprint arXiv:2405.13762 (2024) 22. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A.C., Lo, W.Y., Dollár, P., Girshick, R.: Segment anything. arXiv:2304.02643 (2023) 23. Kong, Q., Cao, Y., Iqbal, T., Wang, Y., Wang, W., Plumbley, M.D.: Panns: Largescale pretrained audio neural networks for audio pattern recognition. IEEE/ACM Transactions on Audio, Speech, and Language Processing 28, 28802894 (2020) 24. Koutini, K., Schlüter, J., Eghbal-Zadeh, H., Widmer, G.: Efficient training of audio transformers with patchout. arXiv preprint arXiv:2110.05069 (2021) 25. Li, B., Liu, X., Dinesh, K., Duan, Z., Sharma, G.: Creating multitrack classical music performance dataset for multimodal music analysis: Challenges, insights, and applications. IEEE Transactions on Multimedia 21(2), 522535 (2018) 26. Li, J., Li, D., Xiong, C., Hoi, S.: Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In: International conference on machine learning. pp. 1288812900. PMLR (2022) 27. Lian, L., Ding, Y., Ge, Y., Liu, S., Mao, H., Li, B., Pavone, M., Liu, M.Y., Darrell, T., Yala, A., et al.: Describe anything: Detailed localized image and video captioning. arXiv preprint arXiv:2504.16072 (2025) 28. Lipman, Y., Chen, R.T., Ben-Hamu, H., Nickel, M., Le, M.: Flow matching for generative modeling. arXiv preprint arXiv:2210.02747 (2022) 29. Liu, H., Chen, Z., Yuan, Y., Mei, X., Liu, X., Mandic, D., Wang, W., Plumbley, M.D.: Audioldm: Text-to-audio generation with latent diffusion models. arXiv preprint arXiv:2301.12503 (2023) 30. Liu, S., Zeng, Z., Ren, T., Li, F., Zhang, H., Yang, J., Jiang, Q., Li, C., Yang, J., Su, H., et al.: Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In: European Conference on Computer Vision. pp. 38 55. Springer (2024) 31. Liu, X., Gong, C., Liu, Q.: Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003 (2022) 32. Liu, X., Su, K., Shlizerman, E.: Tell what you hear from what you seevideo to audio generation through text. arXiv preprint arXiv:2411.05679 (2024) Video Object Segmentation-Aware Audio Generation 15 33. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 (2017) 34. Luo, S., Yan, C., Hu, C., Zhao, H.: Diff-foley: Synchronized video-to-audio synthesis with latent diffusion models. Advances in Neural Information Processing Systems 36, 4885548876 (2023) 35. Mei, X., Nagaraja, V., Le Lan, G., Ni, Z., Chang, E., Shi, Y., Chandra, V.: Foleygen: Visually-guided audio generation. In: 2024 IEEE 34th International Workshop on Machine Learning for Signal Processing (MLSP). pp. 16. IEEE (2024) 36. Mo, S., Shi, J., Tian, Y.: Text-to-audio generation synchronized with videos. arXiv preprint arXiv:2403.07938 (2024) 37. Montesinos, J.F., Slizovskaia, O., Haro, G.: Solos: dataset for audio-visual music analysis. In: 2020 IEEE 22nd International Workshop on Multimedia Signal Processing (MMSP). pp. 16. IEEE (2020) 38. Pascual, S., Yeh, C., Tsiamas, I., Serrà, J.: Masked generative video-to-audio transformers with enhanced synchronicity. In: European Conference on Computer Vision. pp. 247264. Springer (2024) 39. Peebles, W., Xie, S.: Scalable diffusion models with transformers. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 41954205 (2023) 40. Polyak, A., Zohar, A., Brown, A., Tjandra, A., Sinha, A., Lee, A., Vyas, A., Shi, B., Ma, C., Chuang, C., et al.: Movie gen: cast of media foundation models, 2025. URL https://arxiv. org/abs/2410.13720 p. 51 41. Pont-Tuset, J., Perazzi, F., Caelles, S., Arbeláez, P., Sorkine-Hornung, A., Van Gool, L.: The 2017 davis challenge on video object segmentation. arXiv preprint arXiv:1704.00675 (2017) 42. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International conference on machine learning. pp. 87488763. PmLR (2021) 43. Ravi, N., Gabeur, V., Hu, Y.T., Hu, R., Ryali, C., Ma, T., Khedr, H., Rädle, R., Rolland, C., Gustafson, L., et al.: Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714 (2024) 44. Ren, T., Jiang, Q., Liu, S., Zeng, Z., Liu, W., Gao, H., Huang, H., Ma, Z., Jiang, X., Chen, Y., Xiong, Y., Zhang, H., Li, F., Tang, P., Yu, K., Zhang, L.: Grounding dino 1.5: Advance the \"edge\" of open-set object detection (2024) 45. Ren, T., Liu, S., Zeng, A., Lin, J., Li, K., Cao, H., Chen, J., Huang, X., Chen, Y., Yan, F., Zeng, Z., Zhang, H., Li, F., Yang, J., Li, H., Jiang, Q., Zhang, L.: Grounded sam: Assembling open-world models for diverse visual tasks (2024) 46. Ren, T., Liu, S., Zeng, A., Lin, J., Li, K., Cao, H., Chen, J., Huang, X., Chen, Y., Yan, F., et al.: Grounded sam: Assembling open-world models for diverse visual tasks. arXiv preprint arXiv:2401.14159 (2024) 47. Ruan, L., Ma, Y., Yang, H., He, H., Liu, B., Fu, J., Yuan, N.J., Jin, Q., Guo, B.: Mm-diffusion: Learning multi-modal diffusion models for joint audio and video generation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1021910228 (2023) 48. Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., Chen, X.: Improved techniques for training gans. Advances in neural information processing systems 29 (2016) 49. Sheffer, R., Adi, Y.: hear your true colors: Image guided audio generation. In: ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). pp. 15. IEEE (2023) 16 I. Viertola et al. 50. Song, K., Tan, X., Qin, T., Lu, J., Liu, T.Y.: Mpnet: Masked and permuted pretraining for language understanding. Advances in neural information processing systems 33, 1685716867 (2020) 51. Su, K., Liu, X., Shlizerman, E.: From vision to audio and beyond: unified model for audio-visual representation and generation. arXiv preprint arXiv:2409.19132 (2024) 52. Tang, Z., Yang, Z., Khademi, M., Liu, Y., Zhu, C., Bansal, M.: Codi-2: Incontext interleaved and interactive any-to-any generation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 27425 27434 (2024) 53. Tang, Z., Yang, Z., Zhu, C., Zeng, M., Bansal, M.: Any-to-any generation via composable diffusion. Advances in Neural Information Processing Systems 36, 16083 16099 (2023) 54. Tong, A., Fatras, K., Malkin, N., Huguet, G., Zhang, Y., Rector-Brooks, J., Wolf, G., Bengio, Y.: Improving and generalizing flow-based generative models with minibatch optimal transport. arXiv preprint arXiv:2302.00482 (2023) 55. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł., Polosukhin, I.: Attention is all you need. Advances in neural information processing systems 30 (2017) 56. Viertola, I., Iashin, V., Rahtu, E.: Temporally aligned audio for video with autoregression. In: ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). pp. 15. IEEE (2025) 57. Wang, H., Ma, J., Pascual, S., Cartwright, R., Cai, W.: V2a-mapper: lightweight solution for vision-to-audio generation by connecting foundation models. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 38, pp. 1549215501 (2024) 58. Wang, Y., Guo, W., Huang, R., Huang, J., Wang, Z., You, F., Li, R., Zhao, Z.: Frieren: Efficient video-to-audio generation with rectified flow matching. arXiv eprints pp. arXiv2406 (2024) 59. Wu, S.L., Donahue, C., Watanabe, S., Bryan, N.J.: Music controlnet: Multiple time-varying controls for music generation. IEEE/ACM Transactions on Audio, Speech, and Language Processing 32, 26922703 (2024) 60. Xiao, B., Wu, H., Xu, W., Dai, X., Hu, H., Lu, Y., Zeng, M., Liu, C., Yuan, L.: Florence-2: Advancing unified representation for variety of vision tasks. arXiv preprint arXiv:2311.06242 (2023) 61. Xing, Y., He, Y., Tian, Z., Wang, X., Chen, Q.: Seeing and hearing: Opendomain visual-audio generation with diffusion latent aligners. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 7151 7161 (2024) 62. Xu, M., Li, C., Tu, X., Ren, Y., Chen, R., Gu, Y., Liang, W., Yu, D.: Video-to-audio generation with hidden alignment. arXiv preprint arXiv:2407.07464 (2024) 63. Zhang, L., Rao, A., Agrawala, M.: Adding conditional control to text-to-image diffusion models. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 38363847 (2023) 64. Zhao, H., Gan, C., Ma, W.C., Torralba, A.: The sound of motions. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 17351744 (2019) 65. Zhao, H., Gan, C., Rouditchenko, A., Vondrick, C., McDermott, J., Torralba, A.: The sound of pixels. In: Proceedings of the European conference on computer vision (ECCV). pp. 570586 (2018) Video Object Segmentation-Aware Audio Generation 66. Zhou, J., Shen, X.: Audio-visual segmentation with semantics. arXiv preprint arXiv:2301.13190 (2023) 67. Zhou, J., Wang, J., Zhang, J., Sun, W., Zhang, J., Birchfield, S., Guo, D., Kong, L., Wang, M., Zhong, Y.: Audiovisual segmentation. In: European Conference on Computer Vision. pp. 386403. Springer (2022) 68. Ziv, A., Gat, I., Lan, G.L., Remez, T., Kreuk, F., Défossez, A., Copet, J., Synnaeve, G., Adi, Y.: Masked audio generation using single non-autoregressive transformer. arXiv preprint arXiv:2401.04577 (2024)"
        }
    ],
    "affiliations": [
        "Tampere University, Tampere, Finland",
        "University of Oxford, Oxford, UK"
    ]
}