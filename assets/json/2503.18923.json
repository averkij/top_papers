{
    "paper_title": "Video SimpleQA: Towards Factuality Evaluation in Large Video Language Models",
    "authors": [
        "Meng Cao",
        "Pengfei Hu",
        "Yingyao Wang",
        "Jihao Gu",
        "Haoran Tang",
        "Haoze Zhao",
        "Jiahua Dong",
        "Wangbo Yu",
        "Ge Zhang",
        "Ian Reid",
        "Xiaodan Liang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in Large Video Language Models (LVLMs) have highlighted their potential for multi-modal understanding, yet evaluating their factual grounding in video contexts remains a critical unsolved challenge. To address this gap, we introduce Video SimpleQA, the first comprehensive benchmark tailored for factuality evaluation of LVLMs. Our work distinguishes from existing video benchmarks through the following key features: 1) Knowledge required: demanding integration of external knowledge beyond the explicit narrative; 2) Fact-seeking question: targeting objective, undisputed events or relationships, avoiding subjective interpretation; 3) Definitive & short-form answer: Answers are crafted as unambiguous and definitively correct in a short format, enabling automated evaluation through LLM-as-a-judge frameworks with minimal scoring variance; 4) External-source verified: All annotations undergo rigorous validation against authoritative external references to ensure the reliability; 5) Temporal reasoning required: The annotated question types encompass both static single-frame understanding and dynamic temporal reasoning, explicitly evaluating LVLMs factuality under the long-context dependencies. We extensively evaluate 41 state-of-the-art LVLMs and summarize key findings as follows: 1) Current LVLMs exhibit notable deficiencies in factual adherence, particularly for open-source models. The best-performing model Gemini-1.5-Pro achieves merely an F-score of 54.4%; 2) Test-time compute paradigms show insignificant performance gains, revealing fundamental constraints for enhancing factuality through post-hoc computation; 3) Retrieval-Augmented Generation demonstrates consistent improvements at the cost of additional inference time overhead, presenting a critical efficiency-performance trade-off."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 3 2 9 8 1 . 3 0 5 2 : r Video SimpleQA: Towards Factuality Evaluation in Large Video Language Models Meng Cao1, Pengfei Hu1, Yingyao Wang2, Jihao Gu2, Haoran Tang3, Haoze Zhao1, Jiahua Dong1, Wangbo Yu3, Ge Zhang4, Ian Reid1, Xiaodan Liang1 1MBZUAI 2Alibaba Group 3Peking University 4M-A-P https://videosimpleqa.github.io/ (a) The taxonomy of Video SimpleQA benchmark; (b) Illustrations of existing knowledge-based video benchmarks Figure 1. [21, 28, 30, 84, 85] which may involve hypothetical or subjective reasoning; (c) Illustrations of our Video SimpleQA benchmark with the fact-seeking question and definitive & short-form answer with external-source verified."
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in Large Video Language Models (LVLMs) have highlighted their potential for multi-modal understanding, yet evaluating their factual grounding in video contexts remains critical unsolved challenge. To address this gap, we introduce Video SimpleQA, the first comprehensive benchmark tailored for factuality evaluation of LVLMs. Our work distinguishes from existing video benchmarks through the following key features: 1) Knowledge required: demanding integration of external knowledge beyond the videos explicit narrative; 2) Fact-seeking question: targeting objective, undisputed events or relationships, avoiding subjective interpretation; 3) Definitive *Equal contributions. Corresponding author. & short-form answer: Answers are crafted as unambiguous and definitively correct in short format, enabling automated evaluation through LLM-as-a-judge frameworks with minimal scoring variance; 4) External-source verified: All annotations undergo rigorous validation against authoritative external references to ensure the reliability; 5) Temporal reasoning required: The annotated question types encompass both static single-frame understanding and dynamic temporal reasoning, explicitly evaluating LVLMs factuality under the long-context dependencies. We extensively evaluate 41 state-of-the-art LVLMs and summarize key findings as follows: 1) Current LVLMs exhibit notable deficiencies in factual adherence, particularly for opensource models. The best-performing model Gemini-1.5-Pro achieves merely an F-score of 54.4%; 2) Test-time compute 1 paradigms show insignificant performance gains, revealing fundamental constraints for enhancing factuality through post-hoc computation; 3) Retrieval-Augmented Generation demonstrates consistent improvements at the cost of inference time overhead, presenting critiadditional cal efficiency-performance trade-off. We position Video SimpleQA as the cornerstone benchmark for video factuality assessment, with the explicit goal of directing LVLM development toward verifiable real-world grounding. 1. Introduction The substantial advancements in Large Language Models (LLMs) [1, 6, 58, 63, 64] over the past few years have inaugurated new frontier in artificial intelligence. Despite their remarkable capabilities, the factuality concern [2, 65, 70] remains critical challenge, i.e., how to ensure that the generated contents are consistent with factual knowledge and grounded in credible sources*. Existing research has primarily focused on evaluating factuality in text-based [15, 22, 29, 40, 56, 72, 78] and image-based [31, 49, 66, 67, 80] scenarios. However, extending factuality evaluation to video contexts is under-explored and presents unique challenges due to the inherent temporal dynamics, causal relationships, and procedural knowledge. To bridge this gap, we present Video SimpleQA, comprehensive factuality evaluation benchmark tailored for Large Video Language Models (LVLMs). As shown in Figure 2, Video SimpleQA is composed of short, fact-seeking questions and short-form, undisputed answers. Compared to previous video benchmarks, Video SimpleQA stands out with the following advancements: Knowledge required: Beyond comprehending the visual content, Video SimpleQA necessitates the integration of external knowledge that is not explicitly presented in the video narrative, e.g., domain-specific information, contextual background, commonsense, etc. As illustrated in Figure 1(c), recognizing the Pythagorean theorem demonstrated in the video necessitates understanding its dynamic content combined with external geometric and mathematical knowledge. Fact-seeking question: Questions necessitate strict adherence to factual grounding principles, eliminating any hypothetical constructs and subjective inferences. Definitive answer: All the answers are unambiguous, universally agreed upon, consistent over time and invariant to individual perspectives. Short-form answer: to LLMs propenIn contrast sity for lengthy completions containing multiple factual statements, the short-form answer paradigm of Video *Please refer to [65, 70] for the differentiation between the factuality and the similar hallucination concepts. Table 1. Comparisons with existing video benchmarks regarding the video domain, their knowledge-driven focus, emphasis on factuality, and provision of supporting evidence."
        },
        {
            "title": "Knowledge Factuality Evidence\nSource\nbased",
            "content": "driven Video-MME [20] MMBench-Video [19] Video-Bench [51] TempCompass [45] MVBench [37] AutoEval-Video [12] Video-MMMU [30] MMVU [85] MMWorld [28] WorldQA [84] KnowIT-VQA [21] Video SimpleQA"
        },
        {
            "title": "Open\nOpen\nOpen\nOpen\nOpen\nOpen\nProfessional\nDiscipline\nDiscipline\nOpen\nTV shows\nOpen",
            "content": "SimpleQA constrains responses to verifiable atomic facts, which establishes reliable framework for factual assessment with low run-to-run variance. External-source verified: All question-answer (QA) pairs are rigorously validated against external sources to ensure the accuracy and reliability of reference answers. Specifically, each question is accompanied by link to credible webpage (e.g., the Wikipedia page) that substantiates the provided answer. Temporal reasoning required: The task types of our annotated questions encompass both static single-frame understanding and those requiring long-context temporal comprehension (e.g., causal reasoning and process verification shown in Figure 5b). Table 1 delineates the distinctions between our Video SimpleQA and existing video understanding benchmarks. While knowledge-based benchmarks [21, 28, 30, 84, 85] may appear superficially analogous, several critical limitations emerge: KnowIT-VQA [21] is constrained by TV show-specific content. WorldQA [84] incorporates subjective knowledge assessments (e.g., the analysis of the future ability of Atlas in Figure 1(b) that inherently lack definitive answers due to interpretive subjectivity. VideoMMMU [30] focuses on cross-modal knowledge transfer from videos to images. The recent pre-prints MMVU [85] and MMWorld [28] restrict their scope to discipline-specific knowledge comprehension. In contrast, our benchmark emphasizes factuality by aligning more closely with factgrounding, i.e., verifying whether model outputs reflect real-world facts. Moreover, Video SimpleQA provides an external evidence source for validation, further enhancing the fairness and reliability of factuality evaluation. We conduct comprehensive evaluations of 41 stateof-the-art LVLMs on Video SimpleQA, revealing sev2 Figure 2. Sampled examples in Video SimpleQA and the responses of typical LVLMs [5, 53, 62]. eral critical insights through detailed experimental analysis: 1) Significant performance gap: Both proprietary and open-source LVLMs substantially underperform compared to human expertise; 2) Overconfidence bias: LVLMs exhibit systematic overconfidence in their predictions despite output inaccuracies; 3) Ineffective test-time compute: Test-time compute [61] yields minimal performance gains; 4) Efficiency-performance tradeoff : Retrieval-Augmented Generation (RAG) yields significant gains at the cost of inference efficiency; 5) Persistent scaling benefits: Model size and frame number scaling remain effective; 6) Temporal reasoning required: Long-term context modeling is critical. 2. Related Work that aligns with factual Factuality Benchmarks. Factuality is the capability of LLMs to generate content information, which can be substantiated by authoritative sources such as Wikipedia or textbooks [2, 70]. Evaluating LLM factuality presents non-trivial challenge and various benchmarks are proposed in the text-based [15, 22, 29, 40, 56, 72, 78] and image-based scenarios [25, 31, 49, 66, 67, 80]. As one of the pioneering works, TruthfulQA [40] specifically targets imitative falsehoods in LLM-generated responses, which stem from erroneous preconceptions or knowledge gaps. Recently, the SimpleQA series of works [14, 25, 29, 72] facilitate factuality evaluation by constraining the scope to short, fact-seeking questions with single answers, making factuality assessment more tractable compared to previous long, open-ended model outputs. Despite the progress, there is still lack of dedicated benchmark for assessing factuality in video LLMs. well-designed video-based factuality evaluation benchmark is crucial for measuring the ability of LLMs to generate factually grounded contents based on dynamic spatiotemporal information and complex event sequences. Our proposed Video SimpleQA bridges this gap by incorporating videos from diverse domains, questions covering various factual knowledge types, and answers in the easily verifiable short-form format. Video Understanding Benchmarks. The primary video benchmarks are primarily designed for task-specific video understanding tasks such as action recognition [23, 33], grounding [59, 79], captioning [35, 76], etc. More recently, video benchmarks designed for LLM evaluation have shifted towards more comprehensive tasks, including temporal perception [37, 38, 46, 57], reasoning [8, 9, 12, 16, 34, 45, 47, 51, 60, 75, 82], navigation [10, 77], longform comprehension [11, 1820, 69, 73], etc. However, current video benchmarks largely overlook factuality evaluation, resulting in lack of assessment for video LLMs ability to generate factually accurate responses. Compared to video hallucination benchmarks [26, 71, 83] which primarily assesses models adherence to video contents, the factuality evaluation focuses on the models alignment with verifiable external world knowledge [65, 70]. Differentiation from Knowledge-based Benchmarks. Existing knowledge-based video understanding benchmarks [21, 28, 30, 84, 85] either contain hypothetical/subjective reasoning (e.g., the categories of societal norms and social interactions in WorldQA [84]) or narrow their scopes to single TV show or disciplinerelated knowledge [21, 85]. Our Video SimpleQA addresses these limitations by enforcing objective verification through the factually grounded questions, and ensuring diversity via the systematic organization across 4 primary categories, 15 secondary categories, and 84 tertiary categories. 3. Video SimpleQA We introduce Video SimpleQA to benchmark factuality evaluation in LVLMs. The dataset construction pipeline is illustrated in Figure 3, which includes the procedures of video & encyclopedia collection (Sec 3.1), QA annotations (Sec 3.2), and quality control (Sec 3.3). The dataset statistics are illustrated in Sec 3.4. 3.1. Video & Encyclopedia Collection Video Collection: To ensure broad coverage across different domains, we curate the knowledge-intensive videos from the Media of the Day page of Wikimedia Commons https : / / commons . wikimedia . org / wiki / Commons : Media_of_the_day 3 Figure 3. An overview of the construction pipeline of Video SimpleQA including the video & encyclopedia collection (Sec. 3.1), QA annotation (Sec. 3.2), and quality control (Sec. 3.3). sequently, the critic systematically evaluates output compliance with predefined quality criteria, providing targeted feedback for refinement. This iterative process continues for up to three refinement cycles, with non-compliant outputs being discarded post-final iteration to ensure rigorous quality control. Both components leverage GPT-4o [53] implementations, enabling scalable verification while maintaining evaluation consistency. The explicit construction criteria are as follows: 1) Knowledge required: The questions should necessitate both video content and relevant external factual knowledge. Those that can be answered solely based on either source should be excluded. For example, two questions that should be eliminated are: What color is the insect in the video? (which relies solely on video content) and Which president of the United States was Obama? (which relies solely on external knowledge); 2) Fact-seeking question: The generated question should be factually grounded without any hypothetical or subjective reasoning; 3) Definitive answer: To ensure rigorous evaluation, each question must have single, unambiguous, and indisputable answer. To achieve this, we explicitly define the level of granularity in question phrasing. For example, we use which year instead of when and which city instead of where to eliminate ambiguity; 4) Short-form answer: The answers should be in short-form format; 5) Answers should be time-invariant and reflect enduring facts. Human-in-the-loop Verification: Through the iterative generation, we obtain QA annotations of reasonable quality. To further enhance the reliability, we train expert annotators to refine the LLM-generated QA annotations. The expert annotators are first required to watch the complete video and examine the collected encyclopedic knowledge. They then evaluate whether the LLM-generated QA annotations meet the specified criteria and manually revise them if necessary. Additionally, annotators are required to provide verifiable evidence sources for each QA pair, documenting the knowledge provenance essential for factual validation. 3.3. Quality Control Difficulty Filtering. To ensure an appropriate level of assessment difficulty, we establish filtering rules to exclude questions that are easy to answer. In particular, questions Figure 4. The encyclopedia collection process including the raw associated description in Wikimedia and the RAG results for the specialized terms extracted by GPT-4o. together with the accompanied brief descriptions or scientific illustrations. Note that files on the Media of the Day page are freely licensed, which avoids introducing any potential copyright concerns. Encyclopedia Collection: As shown in Figure 4, although the associated textual descriptions in the Wikimedia page provide related descriptions, the explanations for the specialized terms (e.g., Barbary Ground Squirrel, Fuerteventura) still lack formal definitions. To construct more comprehensive encyclopedia, we leverage GPT-4o [53] to extract key terms from the original descriptions and then obtain detailed explanations for these terms via RAG. Specifically, we apply LlamaIndex [42] as the RAG method, with search results from Google and Bing as data sources. 3.2. QA Annotations The annotation pipeline for Video SimpleQA follows two-stage process: (1) automated LLM-based iterative generation and (2) human-in-the-loop verification refinement. LLM-based Iterative Generation: The iterative generation process involves two LLMs, generator LLM for initial QA pair synthesis and critic LLM for quality assessment. The generator receives video content and encyclopedic knowledge (Sec 3.1) along with manually crafted seed QA pairs spanning diverse video types and questioning patterns, which serve as in-context learning exemplars. Sub-"
        },
        {
            "title": "Statistics",
            "content": "Total number of QA pairs Question Length (avg/max) Answer Length (avg/max) Unique Videos Video Length (Seconds, avg/max) Number of primary category Number of secondary category Number of tertiary category"
        },
        {
            "title": "Value",
            "content": "2030 9.71 / 23 1.98 / 19 1293 181 / 8763 4 15 84 (a) (b) (c) Figure 5. (a) Video distribution at the secondary level; (b) Question type distribution; (c) Key statistics of Video SimpleQA . correctly answered by all four state-of-the-art models including GPT-4o [53], Claude 3.5 Sonnet [3], Gemini 1.5 Pro [62] and Qwen-VL-Max [4] are deemed insufficiently challenging and consequently excluded from our benchmark. This filtering strategy ensures our dataset maintains sufficient level of difficulty for meaningful model evaluation. Human Cross-verification. To further enhance dataset quality, rigorous human validation process is impleEach question is independently evaluated by mented. two annotators for compliance with our predefined criteria. Questions are discarded if either annotator deems them noncompliant. Meanwhile, annotators are required to verify answers against authoritative sources (such as Wikipedia). Finally, the final dataset undergoes security auditing to address potential security issues. All these stringent human verification processes ensure both the accuracy of our dataset and its adherence to established criteria. 3.4. Dataset Statistics The key statistics of Video SimpleQA are demonstrated in Table 5c. As shown, it consists of 1293 videos with 2,030 expert-annotated QA pairs. The video distribution spans 4 primary categories, 15 secondary categories and 84 tertiary categories. The average lengths of questions and answers are 9.71 and 1.98 words, respectively, aligning with our intended short-form design. The video distribution at the secondary level is demonstrated in Figure 5a. The question type distribution is visualized in Figure 5b. 4. Experiments 4.1. Setup Evaluated Models. With Video SimpleQA, we benchmark diverse array of state-of-the-art LVLMs including 14 proprietary models, including o1-preview [55], Gemini2.0-Flash series [17], Doubao-vision series [7], GPT-4omini [54], GPT-4o [53], GPT-4V [52], Claude-3.5-Sonnet series [3], Gemini-1.5-Pro series [62] and Qwen-VL-MAX [4], and 27 open-source models, including DeepSeek-VL2 series [74], LLaVA-OneVision series [36], Qwen2.5-VL series [5], Qwen2-VL series [68], InternVL2.5 series [13], LLaVA-NeXT-Video series [41], ST-LLM [43], Chat-UniVi [32], PPLLaVA series [44], VideoLLaMA3 [81] and VideoLLaVA [39]. Following Video-MME [20], we maximize frame utilization of each model by inputting the maximum number of frames that fit within its context window. Evaluation Metrics. Following SimpleQA [72], we evaluate the performance using five metrics: (1) Correct: The predicted answer comprehensively contains all key information from the reference answer while containing no contradictory elements. (2) Incorrect: The predicted answer contradicts the reference answer. The indirect or equivocal responses (e.g., possibly, think, although Im not sure) are also considered incorrect. (3) Not attempted: The reference answer is not fully given in the predicted answer, and no statements in the answer contradict the gold target. (4) Correct given attempted: The ratio of correctly answered (5) F-score: The harquestions among attempted ones. monic mean between correct and correct given attempted metrics. We follow the paradigm of LLM-as-a-Judge [24] and employ GPT-4o-0806 as the judge model. The specific prompts are available in the supplementary material. 4.2. Experimental Findings The evaluation results on Video SimpleQA are presented in Table 2 and key findings are summarized as follows: Video SimpleQA is challenging: To assess human performance levels on Video SimpleQA , we randomly sample 200 instances and recruit five participants to independently complete the tasks under two distinct conditions: with access to external resources (e.g., internet or textbooks) and without such access. These configurations correspond to the human open-book and human closed-book settings documented in Table 2. Compared to the human open-book performance, both open-source and closed-source models demonstrate suboptimal performance. Specifically, the top-performing proprietary model, Gemini-1.5-Pro [62], achieves an F-score of 54.4%. Open-source models exhibit even poorer results, with the best-performing one, Qwen2.5-VL-72B [68] at5 Table 2. Evaluation results (%) of open-source and proprietary multi-modal LLMs on Video SimpleQA . For metrics, CO, NA, IN, and CGA denote Correct, Not attempted, Incorrect, and Correct given attempted, respectively. For subtopics, ENG, NAT, SCI and SAC represent Engineering, Nature, Science and Society and Culture."
        },
        {
            "title": "Model",
            "content": "Overall results on 5 metrics F-score on 4 primary categories CO IN NA"
        },
        {
            "title": "CGA",
            "content": "F-score"
        },
        {
            "title": "SAC",
            "content": "Human Open-book Human Closed-book o1-preview [55] Gemini-2.0-Flash [17] Gemini-2.0-Flash-Thinking [17] Doubao-1.5-vision-pro [7] Doubao-vision-pro [7] Doubao-vision-lite [7] GPT-4o-mini [54] GPT-4o [53] GPT-4V [52] Claude-3.5-Sonnet [3] Claude-3.5-SonnetV2 [3] Gemini-1.5-Pro [62] Gemini-1.5-Pro-Flash [62] Qwen-VL-MAX [4] DeepSeek-VL2 [74] Deepseek-VL2-Small [74] Deepseek-VL2-Tiny [74] LLaVA-OneVison-72B [36] LLaVA-OneVision-7B [36] LLaVA-OneVison-0.5B [36] Qwen2.5-VL-72B [5] Qwen2.5-VL-7B [5] Qwen2.5-VL-3B [5] Qwen2-VL-72B [68] Qwen2-VL-7B [68] Qwen2-VL-2B [68] InternVL2.5-78B [13] InternVL2.5-38B [13] InternVL2.5-26B [13] InternVL2.5-8B [13] InternVL2.5-4B [13] InternVL2.5-2B [13] InternVL2.5-1B [13] LLaVA-NeXT-Video-34B [41] LLaVA-NeXT-Video-7B [41] ST-LLM [43] Chat-UniVi [32] PPLLaVA-Qwen [44] PPLLaVA-Vicuna [44] VideoLLaMA3 [81] Video-LLaVA [39] 66.7 25.0 47.1 41.5 45.9 29.6 37.0 17.3 38.9 49.9 29.7 36.9 42.1 50.1 40.5 36.7 23.6 24.0 20.0 27.8 19.6 16.5 36.1 33.0 28.0 28.0 26.3 26.6 31.2 29.3 28.0 22.1 21.2 16.7 15.7 16.1 10.9 26.6 8.5 20.1 10.0 25.3 15."
        },
        {
            "title": "Human Performance",
            "content": "11.7 13.3 21.7 61.7 85.1 65.2 74.8 36.1 Proprietary Multi-modal LLMs 35.3 28.7 41.0 18.5 38.1 15.2 50.6 35.8 28.0 40.2 38.1 34.2 29.2 43. 17.6 29.8 13.1 51.9 24.9 67.5 10.5 14.3 42.2 22.9 19.8 15.7 30.3 19.9 57.1 59.1 52.8 61.6 49.2 53.3 43.4 58.2 51.5 47.8 52.5 59.4 58.1 45.9 51.6 48.8 49.1 40.0 42.2 26.2 41.0 53.7 37.7 41.7 46.7 54.4 47.7 40.8 Open-source Multi-modal LLMs 26.0 26.4 22.5 30.3 25.7 18.8 43.1 36.9 31.0 33.3 31.5 30.7 33.8 32.4 31.4 23.7 22.9 18.3 17.8 18.1 14.2 28.5 10.1 23.8 13.2 27.2 17.3 57.7 57.8 57.5 55.9 33.1 58.6 31.6 45.8 52.2 40.1 40.6 47.0 53.7 51.1 50.5 64.0 64.0 65.6 60.1 61.3 43.0 59.9 58.5 48.8 41.4 60.8 64. 29.0 29.3 25.8 33.2 37.2 22.0 53.3 41.9 34.9 41.1 39.3 36.2 36.8 36.4 35.7 25.6 24.9 20.3 20.7 20.8 20.3 30.7 12.6 29.2 19.4 29.4 19.5 18.8 18.2 22.5 16.2 47.3 24.9 32.3 21.2 19.9 31.9 33.1 26.4 15.1 19.6 21.5 13.9 14.8 17.7 24.2 22.6 46.1 13.6 33.0 31.2 48.6 13.9 19.8 6 78.6 38.1 80.0 56.8 41.7 39.4 46.8 28.4 45.5 58.1 39.8 49.2 57.5 57.0 50.7 47.7 30.0 31.0 27.1 37.9 29.7 26.9 46.4 38.2 32.3 35.3 36.8 35.0 38.7 38.5 35.9 26.9 28.1 22.5 24.3 25.3 19.6 31.5 11.1 26.2 15.4 36.9 23. 66.7 30.0 47.1 41.8 42.4 35.1 35.5 24.7 35.9 47.3 33.4 35.2 38.0 50.3 46.2 33.0 21.6 22.0 17.8 24.0 20.1 12.3 37.9 35.3 29.3 28.7 28.3 27.4 28.2 27.2 27.6 19.1 17.5 13.9 13.2 11.3 9.2 23.4 8.4 17.8 6.6 18.4 9.7 66.7 28.6 33.3 30.6 34.8 29.8 27.3 14.0 19.2 33.3 27.5 29.7 33.7 44.1 36.9 23.5 19.0 14.7 14.8 20.7 20.0 14.0 27.0 23.1 18.6 25.0 23.0 19.6 24.6 21.2 19.9 13.8 17.2 12.6 10.6 14.5 9.1 18.8 5.6 14.4 14.0 20.0 11. 85.7 47.6 50.0 55.8 70.6 50.1 52.3 29.0 50.0 63.9 45.2 47.9 53.9 60.6 49.5 51.1 31.0 31.4 27.5 35.5 32.3 23.4 51.8 41.2 35.1 40.4 33.5 34.2 40.3 37.5 35.2 30.1 28.2 22.7 21.1 23.4 19.7 35.8 13.1 33.1 22.4 34.5 24.2 Figure 7. Evaluations of test-time compute including Best-of-N and Self-refine. Figure 6. Calibration curves based on the self-stated confidence scores and interval-level accuracy; Brier scores to quantify the deviation from the ideal calibration line. Table 3. The comparisons between vanilla models and models with RAG in terms of F-score and the total inference time (min). taining only 43.1% F-score. This demonstrates that LVLMs still exhibit limited capability in factuality adherence within video contexts, while also highlighting the necessity of establishing Video SimpleQA . LVLMs are overconfident in what they generate: Among all the 41 evaluated models, 32 models exhibit higher IN values (incorrect predictions) than NA values (nonattempted responses), indicating prevalent tendency to generate answers despite insufficient factual knowledge. To further investigate this overconfidence phenomenon, we conduct calibration experiments [27] to examine whether language models know what they know, i.e., whether the models assessed confidence scores align with the actual likelihood of its responses being correct. Specifically, we instruct LVLMs to self-assess confidence scores (0-100) for their predictions. Responses are grouped into confidence intervals (10-point bins), and we calculate interval accuracy (correct predictions per bin). As shown in Figure 6, all models fall below the perfect calibration line, further demonstrating systematic overconfidence. To measure the overconfidence degree, we additionally compute Brier scores [50] (lower is better) to quantify the deviation from ideal calibration. Specifically, for the stated confidence score, the Brier score is computed as the mean squared error between the predicted accuracy and ideal values. As shown in the left-top part of Figure 6, GPT-4o achieves the best calibration among the evaluated LVLMs, while Claude-3.5-Sonnet exhibits the poorest alignment. Test-Time compute yields limited benefits: We empirically investigate the effectiveness of test-time compute [61] strategies on Video SimpleQA by conducting experiments with 200 randomly sampled instances. Two ap-"
        },
        {
            "title": "Model",
            "content": "F-score Inference Time vanilla w/ RAG vanilla w/ RAG GPT-4o Claude-3.5-SonnetV2 GPT-4V Gemini-1.5-Pro-Flash Qwen-VL-MAX 53.7 46.7 37.7 47.7 40.8 53.8 56.7 50.0 54.6 51.1 37.2 28.0 24.1 31.5 16. 63.9 51.2 51.6 51.8 53.9 proaches are evaluated: 1) Best-of-N: The model independently generates responses and selects the one it considers the best as the final answer; 2) Self-refine: The model is prompted to iteratively refine the initial outputs using selfgenerated feedback [48]. Figure 7 presents the accuracy (i.e., the proportion of correct answers) under varying inference-time (for Bestof-N) and different refinement iterations (for Self-refine). Experimental results reveal that both strategies fail to proIn duce significant or consistent accuracy improvements. some cases, these strategies even degrade performance. For instance, when increasing from 8 to 16 in Best-of-N trials, Qwen-VL-Max [4], GPT-4V [52], and Gemini-1.5-Pro [58] exhibit reduced accuracy, suggesting that these models struggle to reliably select the best answer from multiple inferences. These findings highlight the challenges in improving factuality through post-hoc test-time compute strategies. RAG yields significant gains at the cost of inference efficiency: We explore RAG to enhance knowledge-intensive Video SimpleQA benchmark comprehension. Specifically, we implement RAG using LlamaIndex [42] with Google and Wikipedia as retrieval sources, where retrieved documents are appended to the original LVLM input to generate RAG-enhanced responses. As shown in Table 3, RAG achieves consistent and significant F-score improvements Figure 8. The relationship between model size and F-score. over vanilla models. For instance, when integrated with Qwen-VL-Max [4], RAG delivers an absolute improvement of 10.3% (40.8% vs. 51.1%). However, this performance gain comes with substantial computational overhead. Table 3 also quantifies the total inference time, demonstrating that RAG significantly impairs inference efficiency. Our findings highlight the critical trade-off between performance gains and computational practicality. Model size scaling remains effective: As evidenced by the experiments across various model sizes in the InternVL2.5, Qwen2.5-VL, Qwen2-VL, and LLaVA-Onevison series presented in Table 2, model size scaling continues to demonstrate effectiveness, where larger architectures exhibit consistently superior performance. This observation aligns with the widely recognized scaling law principle. Figure 8 delineates the relationship between model size and F-score. Notably, Qwen2.5-VL-7B [5] demonstrates remarkable performance that surpasses even larger models (e.g., InternVL2.5-78B [13] and InternVL2.5-38B [13]). This counter-intuitive observation suggests that while the model size scaling law persists, model scale cannot serve as the sole determinant of model capability. Frame number scaling remains effective: In Figure 9, we demonstrate the impact of the number of sampled frames on performance. The results reveal positive correlation between the video frame number and the F-score, thereby validating the effectiveness of frame number scaling. Temporal understanding is critical for Video SimpleQA: To investigate the necessity of temporal understanding in the Video SimpleQA benchmark, we randomly sampled 200 QA pairs and instructed expert annotators to categorize them based on the temporal scope needed for accurate answers: 1) single-frame dependency; 2) short-term scope required (<10 seconds); 3) medium-term scope required (10s1min); or 4) long-term scope required (>1min). It should be emphasized that our definitions of short/medium/longFigure 9. Frame number scaling experiments. Table 4. The proportion and performance for QA pairs requiring single-frame or short/medium/long-term understanding."
        },
        {
            "title": "Type",
            "content": "Prop CO IN NA CGA F-score"
        },
        {
            "title": "13.0\nSingle-frame\n37.0\nShort-term\nMedium-term 34.0\n16.0\nLong-term",
            "content": "57.7 51.4 47.1 37.5 23.1 32.4 36.8 40.6 19.2 16.2 16.1 21.9 71.4 61.3 56.1 48.0 63.8 55.9 51.2 42.1 term specifically denote the temporal scope required to correctly answer QA pairs, distinct from the absolute video duration referenced in existing long-form video understanding benchmarks [20, 69]. Table 4 summarizes the proportion of videos requiring different temporal reasoning scopes, which reveals that half of cases require either short-term or medium-term temporal understanding to answer correctly, while only 13% depend solely on single-frame information. This distribution demonstrates that our Video SimpleQA benchmark indeed necessitates temporal reasoning capabilities rather than simple frame-level understanding. Furthermore, as indicated in Table 4, videos with long-term temporal scope exhibit significantly lower performance metrics compared to the other three categories, which highlights the importance of long-context temporal understanding. 5. Conclusions We present Video SimpleQA, the first benchmark explicitly designed for evaluating factual grounding in video contexts. Distinct from prior works, our framework introduces five diagnostic dimensions: knowledge integration, fact-seeking questioning, short-form unambiguous evaluation, external-source verification, and temporal reasoning demands. Through an extensive evaluation of 41 state-ofthe-art LVLMs, we reveal notable deficiencies in factual adherence, limited improvements from test-time compute paradigms, and the trade-offs associated with RAG."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 2 [2] Mubashara Akhtar, Michael Schlichtkrull, Zhijiang Guo, Oana Cocarascu, Elena Simperl, and Andreas Vlachos. MulIn Findings timodal automated fact-checking: survey. of the Association for Computational Linguistics: EMNLP 2023, pages 54305448, 2023. 2, 3 [3] Anthropic. Claude 3.5 sonnet, 2024. 5, 6, 13 [4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 5, 6, 7, 8, 13 [5] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 3, 5, 6, 8, 13, 15 [6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. 2 [7] ByteDance. Doubao. https://www.volcengine. com/product/doubao, 2024. 5, [8] Mu Cai, Reuben Tan, Jianrui Zhang, Bocheng Zou, Kai Zhang, Feng Yao, Fangrui Zhu, Jing Gu, Yiwu Zhong, Yuzhang Shang, et al. Temporalbench: Benchmarking finegrained temporal understanding for multimodal video models. arXiv preprint arXiv:2410.10818, 2024. 3 [9] Meng Cao, Haoran Tang, Haoze Zhao, Hangyu Guo, Jiaheng Liu, Ge Zhang, Ruyang Liu, Qiang Sun, Ian Reid, and Xiaodan Liang. Physgame: Uncovering physical comarXiv preprint monsense violations in gameplay videos. arXiv:2412.01800, 2024. 3 [10] Keshigeyan Chandrasegaran, Agrim Gupta, Lea Hadzic, Taran Kota, Jimming He, Cristobal Eyzaguirre, Zane Durante, Manling Li, Jiajun Wu, and Li Fei-Fei. Hourvideo: arXiv preprint 1-hour video-language understanding. arXiv:2411.04998, 2024. 3 [11] Keshigeyan Chandrasegaran, Agrim Gupta, Lea Hadzic, Taran Kota, Jimming He, Cristobal Eyzaguirre, Zane Durante, Manling Li, Jiajun Wu, and Li Fei-Fei. Hourvideo: In The Thirty1-hour video-language understanding. eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. 3 [12] Xiuyuan Chen, Yuan Lin, Yuchen Zhang, and Weiran Huang. Autoeval-video: An automatic benchmark for assessing large vision language models in open-ended video question In European Conference on Computer Vision, answering. pages 179195. Springer, 2024. 2, 3 [13] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and testtime scaling. arXiv preprint arXiv:2412.05271, 2024. 5, 6, 8, 13 [14] Xianfu Cheng, Wei Zhang, Shiwei Zhang, Jian Yang, Xiangyuan Guan, Xianjie Wu, Xiang Li, Ge Zhang, Jiaheng Liu, Yuying Mai, et al. Simplevqa: Multimodal factuality evaluation for multimodal large language models. arXiv preprint arXiv:2502.13059, 2025. 3 [15] Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou, Junxian He, Graham Neubig, Pengfei Liu, et al. Factool: Factuality detection in generative aia tool augmented framework for multi-task and multi-domain scenarios. arXiv preprint arXiv:2307.13528, 2023. 2, 3 [16] Daniel Cores, Michael Dorkenwald, Manuel Mucientes, Tvbench: RearXiv preprint Cees GM Snoek, and Yuki Asano. designing video-language evaluation. arXiv:2410.07752, 2024. 3 [17] Google DeepMind. Introducing gemini 2.0: our new ai model for the agentic era. Google Blog, 2024. 5, 6 [18] Yifan Du, Kun Zhou, Yuqi Huo, Yifan Li, Wayne Xin Zhao, Haoyu Lu, Zijia Zhao, Bingning Wang, Weipeng Chen, and Ji-Rong Wen. Towards event-oriented long video understanding. arXiv preprint arXiv:2406.14129, 2024. 3 [19] Xinyu Fang, Kangrui Mao, Haodong Duan, Xiangyu Zhao, Yining Li, Dahua Lin, and Kai Chen. Mmbench-video: long-form multi-shot benchmark for holistic video understanding. arXiv preprint arXiv:2406.14515, 2024. 2 [21] Noa Garcia, Mayu Otani, Chenhui Chu, [20] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. 2, 3, 5, 8 and Yuta Nakashima. Knowit vqa: Answering knowledge-based questions about videos. In Proceedings of the AAAI conference on artificial intelligence, pages 1082610834, 2020. 1, 2, 3 [22] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. Critic: Large language models can self-correct with tool-interactive critiquing. arXiv preprint arXiv:2305.11738, 2023. 2, 3 [23] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The something something video database for learning and evaluating visual common sense. In Proceedings of the IEEE international conference on computer vision, pages 58425850, 2017. 3 [24] Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, et al. survey on llm-as-a-judge. arXiv preprint arXiv:2411.15594, 2024. [25] Jihao Gu, Yingyao Wang, Pi Bu, Chen Wang, Ziming Wang, Tengtao Song, Donglai Wei, Jiale Yuan, Yingxiu Zhao, Yancheng He, et al. see the world, discover knowledge: chinese factuality evaluation for large vision language models. arXiv preprint arXiv:2502.11718, 2025. 3 [26] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, 9 Yaser Yacoob, et al. Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual ilIn Proceedings of lusion in large vision-language models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1437514385, 2024. 3 [27] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Weinberger. On calibration of modern neural networks. In International conference on machine learning, pages 13211330. PMLR, 2017. 7 [28] Xuehai He, Weixi Feng, Kaizhi Zheng, Yujie Lu, Wanrong Zhu, Jiachen Li, Yue Fan, Jianfeng Wang, Linjie Li, Zhengyuan Yang, et al. Mmworld: Towards multi-discipline arXiv multi-faceted world model evaluation in videos. preprint arXiv:2406.08407, 2024. 1, 2, [29] Yancheng He, Shilong Li, Jiaheng Liu, Yingshui Tan, Weixun Wang, Hui Huang, Xingyuan Bu, Hangyu Guo, Chengwei Hu, Boren Zheng, et al. Chinese simpleqa: chinese factuality evaluation for large language models. arXiv preprint arXiv:2411.07140, 2024. 2, 3 [30] Kairui Hu, Penghao Wu, Fanyi Pu, Wang Xiao, Yuanhan Zhang, Xiang Yue, Bo Li, and Ziwei Liu. Video-mmmu: Evaluating knowledge acquisition from multi-discipline professional videos. arXiv preprint arXiv:2501.13826, 2025. 1, 2, 3 [31] Aman Jain, Mayank Kothyari, Vishwajeet Kumar, Preethi Jyothi, Ganesh Ramakrishnan, and Soumen Chakrabarti. Select, substitute, search: new benchmark for knowledgeaugmented visual question answering. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 24912498, 2021. 2, 3 [32] Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual representation empowers large language models with image and video unIn Proceedings of the IEEE/CVF Conference derstanding. on Computer Vision and Pattern Recognition, pages 13700 13710, 2024. 5, 6 [33] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017. 3 [34] Ilker Kesen, Andrea Pedrotti, Mustafa Dogan, Michele Cafagna, Emre Can Acikgoz, Letitia Parcalabescu, Iacer Calixto, Anette Frank, Albert Gatt, Aykut Erdem, et al. Vilma: zero-shot benchmark for linguistic and temporal grounding in video-language models. In The Twelfth International Conference on Learning Representations, 2024. [35] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. In Proceedings of the IEEE international conference on computer vision, pages 706715, 2017. 3 [36] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 5, 6, 13 [37] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22195 22206, 2024. 2, 3 [38] Shicheng Li, Lei Li, Shuhuai Ren, Yuanxin Liu, Yi Liu, Rundong Gao, Xu Sun, and Lu Hou. Vitatecs: diagnostic dataset for temporal concept understanding of videolanguage models. arXiv preprint arXiv:2311.17404, 2023. 3 [39] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual represenarXiv preprint tation by alignment before projection. arXiv:2311.10122, 2023. 5, [40] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: In ProMeasuring how models mimic human falsehoods. ceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 32143252, 2022. 2, 3 [41] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 5, 6 [42] Jerry Liu. LlamaIndex, 2022. 4, 7 [43] Ruyang Liu, Chen Li, Haoran Tang, Yixiao Ge, Ying Shan, and Ge Li. St-llm: Large language models are effective temporal learners. arXiv preprint arXiv:2404.00308, 2024. 5, 6 [44] Ruyang Liu, Haoran Tang, Haibo Liu, Yixiao Ge, Ying Shan, Chen Li, and Jiankun Yang. Ppllava: Varied video sequence understanding with prompt guidance. arXiv preprint arXiv:2411.02327, 2024. 5, 6 [45] Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. Tempcompass: Do video llms really understand videos? arXiv preprint arXiv:2403.00476, 2024. 2, 3 [46] Ye Liu, Zongyang Ma, Zhongang Qi, Yang Wu, Ying Shan, and Chang Wen Chen. Et bench: Towards open-ended event-level video-language understanding. arXiv preprint arXiv:2409.18111, 2024. [47] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023. 3 [48] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36:4653446594, 2023. 7 [49] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: visual question answering In Proceedings benchmark requiring external knowledge. of the IEEE/cvf conference on computer vision and pattern recognition, pages 31953204, 2019. 2, 3 [50] Matthias Minderer, Josip Djolonga, Rob Romijnders, Frances Hubis, Xiaohua Zhai, Neil Houlsby, Dustin Tran, and Mario Lucic. Revisiting the calibration of modern neural networks. Advances in neural information processing systems, 34:1568215694, 2021. 7 10 [51] Munan Ning, Bin Zhu, Yujia Xie, Bin Lin, Jiaxi Cui, Lu Yuan, Dongdong Chen, and Li Yuan. Video-bench: comprehensive benchmark and toolkit for evaluating video-based large language models. arXiv preprint arXiv:2311.16103, 2023. 2, [52] OpenAI. Gpt-4v(ision) system card. https://api. semanticscholar . org / CorpusID : 263218031, 2023. 5, 6, 7 [53] OpenAI. Hello gpt-4o. OpenAI Blog, 2024. 3, 4, 5, 6, 13, 15 [54] OpenAI. Gpt-4o mini: advancing cost-efficient intelligence. OpenAI Blog, 2024. 5, 6, 13 [55] OpenAI. Introducing openai o1-preview. OpenAI Blog, 2024. 5, [56] Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong Wu. Unifying large language models and knowledge graphs: roadmap. IEEE Transactions on Knowledge and Data Engineering, 2024. 2, 3 [57] Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adria Recasens, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Mateusz Malinowski, Yi Yang, Carl Doersch, et al. Perception test: diagnostic benchmark for multimodal video models. Advances in Neural Information Processing Systems, 36, 2024. 3 [58] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 2, 7 [59] Anna Rohrbach, Marcus Rohrbach, Wei Qiu, Annemarie Friedrich, Manfred Pinkal, and Bernt Schiele. Coherent multi-sentence video description with variable level of detail. In Pattern Recognition: 36th German Conference, GCPR 2014, Munster, Germany, September 2-5, 2014, Proceedings 36, pages 184195. Springer, 2014. 3 [60] Ziyao Shangguan, Chuhan Li, Yuxuan Ding, Yanan Zheng, Yilun Zhao, Tesca Fitzgerald, and Arman Cohan. Tomato: Assessing visual temporal reasoning capabilities in multimodal foundation models. arXiv preprint arXiv:2410.23266, 2024. 3 [61] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more arXiv preprint effective than scaling model parameters. arXiv:2408.03314, 2024. 3, [62] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 3, 5, 6, 13, 15 [63] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 2 [64] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 2 [65] Cunxiang Wang, Xiaoze Liu, Yuanhao Yue, Xiangru Tang, Tianhang Zhang, Cheng Jiayang, Yunzhi Yao, Wenyang Gao, Xuming Hu, Zehan Qi, et al. Survey on factuality in large language models: Knowledge, retrieval and domainspecificity. arXiv preprint arXiv:2310.07521, 2023. 2, 3 [66] Peng Wang, Qi Wu, Chunhua Shen, Anton van den Hengel, and Anthony Dick. Explicit knowledge-based reaarXiv preprint soning for visual question answering. arXiv:1511.02570, 2015. 2, 3 [67] Peng Wang, Qi Wu, Chunhua Shen, Anthony Dick, and Anton Van Den Hengel. Fvqa: Fact-based visual question answering. IEEE transactions on pattern analysis and machine intelligence, 40(10):24132427, 2017. 2, [68] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 5, 6 [69] Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Shiyu Huang, Bin Xu, Yuxiao Dong, Ming Ding, et al. Lvbench: An extreme long video understanding benchmark. arXiv preprint arXiv:2406.08035, 2024. 3, 8 [70] Yuxia Wang, Minghan Wang, Muhammad Arslan Manzoor, Fei Liu, Georgi Georgiev, Rocktim Das, and Preslav Nakov. Factuality of large language models: survey. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1951919529, 2024. 2, 3 [71] Yuxuan Wang, Yueqian Wang, Dongyan Zhao, Cihang Xie, and Zilong Zheng. Videohallucer: Evaluating intrinsic and extrinsic hallucinations in large video-language models. arXiv preprint arXiv:2406.16338, 2024. 3 [72] Jason Wei, Nguyen Karina, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia Glaese, John Schulman, and William Fedus. Measuring short-form factuality in large language models. arXiv preprint arXiv:2411.04368, 2024. 2, 3, 5 [73] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. interLongvideobench: benchmark for long-context arXiv preprint leaved video-language understanding. arXiv:2407.15754, 2024. [74] Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, et al. Deepseek-vl2: Mixture-ofexperts vision-language models for advanced multimodal understanding. arXiv preprint arXiv:2412.10302, 2024. 5, 6 [75] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to explaining In Proceedings of the IEEE/CVF contemporal actions. ference on computer vision and pattern recognition, pages 97779786, 2021. 3 [76] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 52885296, 2016. 3 11 [77] Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. arXiv preprint arXiv:2412.14171, 2024. 3 [78] Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang. Generate rather than retrieve: Large language models are strong context generators. arXiv preprint arXiv:2209.10063, 2022. 2, 3 [79] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa: dataset for understanding complex web videos via question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 91279134, 2019. [80] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition: Visual commonsense reaIn Proceedings of the IEEE/CVF conference on soning. computer vision and pattern recognition, pages 67206731, 2019. 2, 3 [81] Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng, Yuming Jiang, Hang Zhang, Xin Li, et al. Videollama 3: Frontier multimodal foundation models for image and video understanding. arXiv preprint arXiv:2501.13106, 2025. 5, 6 [82] Haoran Zhang, Hangyu Guo, Shuyue Guo, Meng Cao, Wenhao Huang, Jiaheng Liu, and Ge Zhang. Ing-vp: Mllms cannot play easy vision-based games yet. arXiv preprint arXiv:2410.06555, 2024. 3 [83] Jiacheng Zhang, Yang Jiao, Shaoxiang Chen, Jingjing Chen, and Yu-Gang Jiang. Eventhallusion: Diagnosing event hallucinations in video llms. arXiv preprint arXiv:2409.16597, 2024. 3 [84] Yuanhan Zhang, Kaichen Zhang, Bo Li, Fanyi Pu, Christopher Arif Setiadharma, Jingkang Yang, and Ziwei Liu. Worldqa: Multimodal world knowledge in videos through arXiv preprint arXiv:2405.03272, long-chain reasoning. 2024. 1, 2, 3 [85] Yilun Zhao, Lujing Xie, Haowei Zhang, Guo Gan, Yitao Long, Zhiyuan Hu, Tongyan Hu, Weiyuan Chen, Chuhan Li, Junyang Song, et al. Mmvu: Measuring expertlevel multi-discipline video understanding. arXiv preprint arXiv:2501.12380, 2025. 1, 2, 12 Figure 10. Results with different judge models. 6. Supplementary This supplementary material is organized as follows. First, we provide more detailed experimental setup (Sec 6.1). Then we present more experimental results in Sec 6.2. The error type analysis and visualizations are respectively presented in Sec 6.3 and Sec 6.4. 6.1. Experiment Setup Configuration of Evaluated Models. Table 5 details the configuration of each evaluated models. We use the default settings from the official implementation of each model to process vision input. All inferences are reproducible on workstation equipped with 8 NVIDIA A100 GPUs. Prompts. The prompts for the grader, along with instructions guiding the model to output answers and confidence levels, are illustrated in Figure 14, Figure 15 and Figure 16, respectively. The taxonomy of videos in Video SimpleQA . In Table 6 and Table 7, we present the detailed taxonomy of Video SimpleQA including 4 primary categories, 15 secondary categories, and 84 tertiary categories. 6.2. More Experiments Results with different judge models. The short-form answer paradigm of Video SimpleQA enables automated evaluation through LLM-as-a-judge frameworks with low run-to-run variance. To demonstrate this, we select five typical LVLMs, and evaluate them using various judge models including Gemini-1.5-Pro-Flash [62], Gemini-1.5-Pro [62], Claude-3.5-SonnetV2 [3], GPT-4o [53], and GPT-4o-mini [54]. As shown in Figure 10, while the specific scores from different judge models vary, the relative rankings of the evaluated models remain consistent. This demonstrates the robustness of our evaluation, indicating that smaller judge models (such as GPT-4o-mini[54]) can achieve high consistency rates, enabling efficient evaluation even with limited resources. Performance across secondary categories. Figure 11 demonstrates the F-score performance across 15 secondary Figure 11. The performance of different models across 15 secondary categories in Video SimpleQA . categories. As shown, we observe distinct performance patterns among the compared LVLMs. Capability distribution: Gemini-1.5-Pro [62] demonstrates the most consistent performance with superior F-scores across domains, particularly excelling in Language & Material Culture and Meteorology. GPT-4o [53] and Claude-3.5-SonnetV2 [3] follow with complementary strengths, where GPT-4o [53] exhibits notable advantages in Beliefs & Institutions compared to other LVLMs. Imbalanced performance profiles: Qwen2.5-VL-72B [5] and Qwen-VL-MAX [4] show significant performance variance, with Qwen-VL-MAX [4] severely underperforming in Applied sciences compared to its moderate capabilities in other domains. LLaVA-Onevision72B [36] and InternVL2.5-78B [13] consistently underperform across most domains, with particularly low Fscores in technical areas. Disciplinary performance gap: All models exhibit systematically lower F-scores in scientific domains (Physical/Applied Sciences) compared to humanities-oriented categories (History & Heritage, Civil & Architecture), with LLaVA-Onevision-72B [36] and InternVL2.5-78B [13] showing the most pronounced disparities. 6.3. Error Types Analysis This section presents case study analyzing error patterns in GPT-4o [53], Gemini-1.5-pro [62], and Qwen2.5-VL-72B [5] through systematic examination of 200 randomly selected samples per model spanning diverse question types. We categorize observed errors into four primary classes, each illustrated with representative examples: Perception Error: Incorrect identification of objects. This occurs when LVLMs misidentify or fail to detect key visual elements in input frames (cf . Figure 12 (a)). 13 Table 5. Configurations of evaluated LVLMs in Video SimpleQA . Organization Model Release Version Input Frames OpenAI Google Anthropic Alibaba ByteDance Proprietary Multi-modal LLMs o1-preview GPT-4o GPT-4o-mini GPT-4V Gemini-2.0-Flash-Thinking Gemini-2.0-Flash Gemini-1.5-Pro Gemini-1.5-Pro-Flash Claude-3.5-Sonnet Claude-3.5-SonnetV Qwen-VL-MAX Doubao-1.5-vision-pro Doubao-vision-pro Doubao-vision-lite 2024-9 2024-8 2024-7 2023-9 2024-12 2024-12 2024-9 2024-9 2024-6 2024-10 20242025-1 2025-1 2025-1 o1-preview-2024-09-12 gpt-4o-2024-08-06 gpt-4o-mini-2024-07-18 gpt-4-vision gemini-2.0-flash-thinking-exp-1219 gemini-2.0-flash-exp gemini-1.5-pro gemini-1.5-pro-flash claude-3-5-sonnet claude-3-5-sonnetV2 Qwen-VL-MAX Doubao-1.5-vision-pro Doubao-vision-pro Doubao-vision-lite Open-source Multi-modal LLMs Shanghai AI Lab Alibaba DAMO DeepSeek Llava Hugging Face PKU InternVL2.5-1B InternVL2.5-2B InternVL2.5-4B InternVL2.5-8B InternVL2.5-26B InternVL2.5-38B InternVL2.5-78B Qwen2-VL-2B Qwen2-VL-7B Qwen2-VL-72B Qwen2.5-VL-3B Qwen2.5-VL-7B Qwen2.5-VL-72B VideoLLaMA3 DeepSeek-VL2 DeepSeek-VL2-Small DeepSeek-VL2-Tiny LLaVA-OneVision-0.5B LLaVA-OneVision-7B LLaVA-OneVision-72B LLaVA-NeXT-Video-7B LLaVA-NeXT-Video-34B ST-LLM Chat-UniVi PPLLaVA-Qwen PPLLaVA-Vicuna Video-LLaVA 2024-11 2024-11 2024-11 2024-11 2024-11 2024-11 2024-11 2024-8 2024-8 2024-9 2025-2 2025-2 2025-2 2025-1 2024-12 2024-12 2024-12 2024-9 2024-9 2024-9 2024-6 20242024-3 2023-11 2024-10 2024-10 2023-11 InternVL2.5-1B InternVL2.5-2B InternVL2.5-4B InternVL2.5-8B InternVL2.5-26B InternVL2.5-38B InternVL2.5-78B Qwen2-VL-2B-Instruct Qwen2-VL-7B-Instruct Qwen2-VL-72B-Instruct Qwen2.5-VL-3B-Instruct Qwen2.5-VL-7B-Instruct Qwen2.5-VL-72B-Instruct VideoLLaMA3 deepseek-vl2 deepseek-vl2-small deepseek-vl2-tiny llava-onevision-qwen2-0.5b-ov-chat-hf llava-onevision-qwen2-7b-ov-chat-hf llava-onevision-qwen2-72b-ov-chat-hf LLaVA-NeXT-Video-7B-hf LLaVA-NeXT-Video-34B-hf ST-LLM Chat-UniVi PPLLaVA-Qwen PPLLaVA-Vicuna Video-LLaVA 32 32 32 32 32 32 32 32 32 32 32 32 32 4 4 4 4 4 4 4 16 16 16 16 16 16 128 2 2 2 16 16 4 4 4 32 100 32 32 Lack of Knowledge: Correct identification but lacking relevant knowledge. LVLMs accurately perceive the visual content but cannot provide accurate information due to knowledge limitations (cf . Figure 12 (b)). Refusal to Answer: LVLMs recognize their inability to make confident determination and opts to abstain from answering (cf . Figure 12 (c)). Failure to Follow Instructions: LVLMs understand the input but fail to properly execute the given instructions. This typically manifests in two ways: (1) the generated outputs do not conform to the specified format requirements, or (2) the responses are irrelevant to the question posed, e.g., addressing different topics than what was actually requested (cf . Figure 12 (d)). The error type distribution is depicted in Figure 13. Our analysis reveals the following critical insights: 1) Lack of knowledge emerges as the predominant error source. For all Figure 12. Visualizations of typical error types including (a) perception error; (b) lack of knowledge; (c) refusal to answer; (d) failure to follow instructions. illustrated in Figure 17 Figure 21. Figure 13. Error type distributions across Qwen2.5-VL-72B [5], GPT-4o [53] and Gemini-1.5-Pro [62]. three LVLMs, it constitutes the largest error category. This highlights the urgent need to explore strategies for incorporating more accurate knowledge during pre-training and instruction tuning; 2) Qwen2.5-VL-72B demonstrates weaker instruction-following capabilities compared to its counterparts, as evidenced by its higher rate of failure to follow instructions errors (12%); 3) Perception errors remain relatively low across all models, underscoring the advanced visual comprehension abilities of modern LVLMs. 6.4. Visualizations The visualization results of three typical LVLMs (GPT-4o [53], Gemini-1.5-Pro [62] and Qwen2.5-VL-72B [5]) are 15 Figure 14. Prompt for grading: Part 1 16 Figure 15. Prompt for grading: Part 2 Figure 16. Prompt for calibration experiments. Table 6. The taxonomy of videos in Video SimpleQA (part 1)."
        },
        {
            "title": "Nature",
            "content": "Geology & Landscapes"
        },
        {
            "title": "Nature",
            "content": "Flora & Fauna"
        },
        {
            "title": "Engineering",
            "content": "Civil & Architecture"
        },
        {
            "title": "Engineering",
            "content": "Mechanical & Electrical"
        },
        {
            "title": "Engineering",
            "content": "Chemical & Process"
        },
        {
            "title": "Engineering",
            "content": "Environmental & Geophysical"
        },
        {
            "title": "Applied sciences",
            "content": "18 Fossils Landscapes Rocks & Minerals Geomorphology Volcanic Features Coastal Landforms Animalia Marine Organisms Plantae Fungi Microorganisms Endangered Species Weather Climate Atmospheric Phenomena Forecasting Architecture Civil Structural Engineering Urban Planning Mechanical Electrical Mechatronics Aerospace Engineering Chemical Process Biochemical Engineering Polymer Engineering Environmental Geophysical Hydrology Climate Engineering Physics Chemistry Astronomy Earth sciences Materials Science Atmospheric Science Geophysics Biology Medicine Ecology Genetics Neuroscience Mathematics Computer Science Statistics Logic Technology Robotics Agricultural Science Data Science 43 19 7 5 7 6 18 91 61 37 82 490 3 6 6 3 56 65 56 46 52 47 50 43 4 3 3 3 22 21 26 20 2 4 5 2 9 3 6 10 11 11 10 7 3 2 3 1 6 3 4 3 Table 7. The taxonomy of videos in Video SimpleQA (part 2)."
        },
        {
            "title": "Count",
            "content": "Society & Culture Arts & Recreation Society & Culture Beliefs & Institutions Society & Culture History & Heritage Society & Culture Language & Material Culture"
        },
        {
            "title": "Art\nLiterature\nMusic\nEntertainment\nSports\nDance\nTheatre\nFilm\nPhotography\nGames\nBelief\nReligion\nPhilosophy\nEthics\nPolitics\nFlags\nGovernment\nLaw\nPeople\nHistory\nEvents\nPlaces\nArchaeology\nHeritage Sites\nGenealogy\nLanguage\nObjects\nFood\nClothing\nTransportation\nInstruments\nTools",
            "content": "16 6 30 40 12 12 62 25 10 31 5 20 7 9 11 8 3 3 5 118 9 9 24 9 9 2 9 7 2 5 3 3 19 Figure 17. Sampled examples in Video SimpleQA and the responses of typical LVLMs: part 1 20 Figure 18. Sampled examples in Video SimpleQA and the responses of typical LVLMs: part 2 Figure 19. Sampled examples in Video SimpleQA and the responses of typical LVLMs: part 3 22 Figure 20. Sampled examples in Video SimpleQA and the responses of typical LVLMs: part 4 23 Figure 21. Sampled examples in Video SimpleQA and the responses of typical LVLMs: part"
        }
    ],
    "affiliations": [
        "Alibaba Group",
        "M-A-P",
        "MBZUAI",
        "Peking University"
    ]
}