{
    "paper_title": "FedNano: Toward Lightweight Federated Tuning for Pretrained Multimodal Large Language Models",
    "authors": [
        "Yao Zhang",
        "Hewei Gao",
        "Haokun Chen",
        "Weiguo Li",
        "Yunpu Ma",
        "Volker Tresp"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal Large Language Models (MLLMs) excel in tasks like multimodal reasoning and cross-modal retrieval but face deployment challenges in real-world scenarios due to distributed multimodal data and strict privacy requirements. Federated Learning (FL) offers a solution by enabling collaborative model training without centralizing data. However, realizing FL for MLLMs presents significant challenges, including high computational demands, limited client capacity, substantial communication costs, and heterogeneous client data. Existing FL methods assume client-side deployment of full models, an assumption that breaks down for large-scale MLLMs due to their massive size and communication demands. To address these limitations, we propose FedNano, the first FL framework that centralizes the LLM on the server while introducing NanoEdge, a lightweight module for client-specific adaptation. NanoEdge employs modality-specific encoders, connectors, and trainable NanoAdapters with low-rank adaptation. This design eliminates the need to deploy LLM on clients, reducing client-side storage by 95%, and limiting communication overhead to only 0.01% of the model parameters. By transmitting only compact NanoAdapter updates, FedNano handles heterogeneous client data and resource constraints while preserving privacy. Experiments demonstrate that FedNano outperforms prior FL baselines, bridging the gap between MLLM scale and FL feasibility, and enabling scalable, decentralized multimodal AI systems."
        },
        {
            "title": "Start",
            "content": "FedNano: Toward Lightweight Federated Tuning for Pretrained Multimodal Large Language Models Yao Zhang 1,4* Weiguo Li 5 Hewei Gao 2* Yunpu Ma 1,4 Haokun Chen1,3 Volker Tresp1,4 1 LMU Munich 2 Technical University of Munich 3 Siemens Technology 4 Munich Center for Machine Learning 5 University Heidelberg yzhang@dbs.ifi.lmu.de yunpu.ma@ifi.lmu.de tresp@dbs.ifi.lmu.de 5 2 0 2 2 1 ] . [ 1 4 2 8 4 1 . 6 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Multimodal Large Language Models (MLLMs) excel in tasks like multimodal reasoning and cross-modal retrieval but face deployment challenges in real-world scenarios due to distributed multimodal data and strict privacy requirements. Federated Learning (FL) offers solution by enabling collaborative model training without centralizing data. However, realizing FL for MLLMs presents significant challenges, including high computational demands, limited client capacity, substantial communication costs, and heterogeneous client data. Existing FL methods assume client-side deployment of full models, an assumption that breaks down for large-scale MLLMs due to their massive size and communication demands. To address these limitations, we propose FedNano, the first FL framework that centralizes the LLM on the server while introducing NanoEdge, lightweight module for client-specific adaptation. NanoEdge employs modality-specific encoders, connectors, and trainable NanoAdapters with low-rank adaptation. This design eliminates the need to deploy LLM on clients, reducing client-side storage by 95%, and limiting communication overhead to only 0.01% of the model parameters. By transmitting only compact NanoAdapter updates, FedNano handles heterogeneous client data and resource constraints while preserving privacy. Experiments demonstrate that FedNano outperforms prior FL baselines, bridging the gap between MLLM scale and FL feasibility, and enabling scalable, decentralized multimodal AI systems."
        },
        {
            "title": "Introduction",
            "content": "Multimodal Large Language Models (MLLMs) (Zhu et al., 2023; Liu et al., 2024b; Peng et al., 2023b; Alayrac et al., 2022; Li et al., 2023) excel in tasks like cross-modal retrieval (Yin et al., 2024), making them indispensable for applications such *Equal contribution Corresponding authors 1 Figure 1: Comparison between traditional PEFT-based FL (left) and our proposed FedNano (right). FedNano keeps the LLM centralized on the server and performs lightweight tuning on clients, reducing both computation and communication overhead. as visual question answering (VQA) (Antol et al., 2015). However, real-world deployment remains fundamentally constrained: multimodal data is inherently decentralized and privacy-sensitive, while the large parameter footprint of MLLMs renders on-device execution infeasible for edge clients. Federated learning (FL) (McMahan et al., 2017) offers promising solution for decentralized multimodal training. However, applying FL to MLLMs presents fundamental system-level challenges. First, although parameter-efficient finetuning (PEFT) (Houlsby et al., 2019; Lester et al., 2021; Zaken et al., 2021; Hu et al., 2021) reduces the number of trainable parameters, it still requires deploying the full MLLMoften exceeding 10 billion parameterson each client, which is impractical for resource-constrained devices such as mobile phones or IoT systems. Second, PEFT methods typically insert adapters into internal layers of the language model, requiring structural access and fullmodel execution on clients, as seen in recent FL adaptations such as FedDPA-F (Yang et al., 2024), pFedLoRA (Yi et al., 2023), and FedIT (Zhang et al., 2024). Third, the resulting adapter updates remain sizable, imposing substantial communication overhead across training rounds. Finally, non-IID client data introduces statistical heterogeneity that degrades global model convergence. These limitations collectively constrain the scalability and practicality of existing FL approaches for MLLMs. To address these challenges, we propose FedNano, the first FL framework that enables MLLM adaptation without deploying LLM on clients. As illustrated in Fig. 1, FedNano centralizes LLM on the server in frozen state, and equips each client with NanoEdgea lightweight adaptation module comprising modality-specific encoders, connectors, and trainable NanoAdapters. These adapters operate externally to LLM and are optimized using low-rank decomposition (Hu et al., 2021), minimizing both parameter size and transmission cost. This design removes the need for local LLM deployment, reduces client storage by over 95%, as shown in Tab. 1. Only compact NanoAdapter updates are exchanged across training rounds, achieving over 99% communication reduction compared to PEFT-based FL methods (Chen et al., 2023; Yang et al., 2024). By decoupling adaptation from the LLM, FedNano provides scalable and communication-efficient solution for realworld MLLM deployment. To address client heterogeneity, FedNano adapts Fisher Merging (Matena and Raffel, 2022) to align global updates with client-specific data distributions. This adaptation improves performance on non-IID datasets and outperforms traditional aggregation methods such as FedAvg (McMahan et al., 2017) and FedProx (Li et al., 2020). By integrating these innovations, FedNano effectively bridges the gap between the computational complexity of MLLMs and the constraints of FL, enabling efficient deployment in real-world scenarios. Experiments across diverse MLLM and multimodal tasks demonstrate that FedNano not only outperforms existing methods but also significantly reduces resource and communication costs, enabling the scalable, efficient, and privacy-preserving deployment of MLLMs. This framework lays strong foundation for advancing multimodal AI systems in decentralized real-world applications, including personalized healthcare, cross-device collaboration, and multimodal user interfaces. The key contributions of this work are: Approach Client Params Server Uploads FedNano 304.55M (4.30%) 1.05M (0.01%) FedDPA-F 7222.81M (100%) 180.89M (2.50%) Reduction 99.4% 95.7% Table 1: Comparison of parameter distribution and communication efficiency between FedNano and FedDPA-F (Yang et al., 2024) on LLaVA-1.5-7B (Liu et al., 2024b). Client Params denotes parameters retained on client devices, while Server Uploads denotes parameter updates sent to the server per round. Both use rank-64 adapters. side adaptation via NanoEdge, reducing client storage by over 95% and enabling practical deployment on resource-constrained devices. 2. Communication-Efficient Adaptation: FedNano employs low-rank decomposition in NanoAdapters, achieving an over 99% reduction in the number of transmitted parameters, allowing efficient deployment in bandwidth-constrained environments. 3. Improved Generalization on Non-IID Data: We adapt Fisher Merging for FL, aligning global updates with client-specific distributions to improve model performance on heterogeneous datasets. 4. Comprehensive Validation: Extensive experiments demonstrate the effectiveness and efficiency of FedNano, establishing it as scalable solution for real-world MLLM deployment."
        },
        {
            "title": "2.1 Multimodal Large Language Models",
            "content": "MLLMs (Zhu et al., 2023; Liu et al., 2024b; Peng et al., 2023b; Alayrac et al., 2022; Li et al., 2023; Dai et al., 2023) extend LLMs (Touvron et al., 2023; Peng et al., 2023a; Bai et al., 2023) by integrating modality-specific encoders and connectors to process multimodal inputs. Recent works focus on efficient alignment, using lightweight connectors such as the linear projection in MiniGPT-4 (Zhu et al., 2023) or the MLP bridge in LLaVA (Liu et al., 2024b). However, these models assume full model access, which is incompatible with federated settings due to privacy and resource constraints. FedNano resolves this by freezing the LLM on the server and enabling lightweight client-side adaptation via NanoAdapters."
        },
        {
            "title": "2.2 Parameter Efficient Fine-tuning",
            "content": "1. Novel FL Framework for MLLMs: We propose FedNano, the first framework that centralizes the LLM on the server and enables lightweight clientPEFT techniques (Houlsby et al., 2019; Lester et al., 2021; Zaken et al., 2021; Hu et al., 2021) adapt large pretrained models by updating only 2 small set of parameters, significantly reducing training costs. They include additive methods like adapters (Houlsby et al., 2019) and soft prompts (Lester et al., 2021), selective tuning such as BitFit (Zaken et al., 2021), and reparameterization methods like LoRA (Hu et al., 2021). While effective in centralized settings, PEFT-based FL methods (Chen et al., 2023; Wang et al., 2024; Zhang et al., 2024) assume the full model, including LLM, can be deployed on clients. This becomes impractical for MLLMs, where LLM accounts for the vast majority of parameters and cannot be hosted on resource-limited devices. To overcome this, FedNano introduces new paradigm: the LLM is frozen and centralized on the server, while lightweight NanoAdapters are deployed on clients. This design eliminates the need for full-model access, reduces client overhead, and enables scalable FL for MLLM. Unlike conventional PEFT, which are inserted into LLM, NanoAdapters operate externally, interfacing solely through the modality connector. This allows adaptation without modifying or executing LLM on clients."
        },
        {
            "title": "2.3 Multimodal Federated Learning",
            "content": "Multimodal FL has gained increasing attention for handling data heterogeneity and privacy constraints in real-world deployments. Prior work has focused on vision-language models, proposing strategies for modality imbalance (Yu et al., 2023; Che et al., 2024), non-IID distributions (Yang et al., 2024; Zhang et al., 2024; Chen and Zhang, 2024), and client personalization (Yi et al., 2023; Chen et al., 2023). Benchmarks like FedMultimodal (Feng et al., 2023) and FedMLLM (Xu et al., 2024) further standardize evaluation in heterogeneous multimodal settings. However, these methods still rely on client-side full model deployment. For MLLMs, this becomes infeasible due to their scale. Even with PEFT, deploying full MLLMs locally remains out of reach, and transmitting adapter updates still incurs significant communication overhead. FedNano departs from this design by keeping the LLM on the server and transmitting only compact NanoAdapter updates from clients. This makes it the first scalable FL framework tailored for large-scale MLLMs, enabling efficient multimodal collaboration without sacrificing practicality."
        },
        {
            "title": "3 Methodology",
            "content": "3.1 Problem Definition k, qi k, ai This work addresses federated fine-tuning for MLLMs in decentralized environments with statistical data heterogeneity. Each client holds private multimodal dataset Dk = {(vi k)}, comprising image-question-answer triplets. We assume complete modality availability and shared model architecture across all clients; only data distributions differ (Chen et al., 2023). The marginal k, and ai distributions of vi vary across clients, resulting in shifts in both visual and textual representations, as well as answer semantics. Such heterogeneity poses challenges for achieving consistent generalization, as standard aggregation strategies struggle to align diverse local updates. k, qi Our objective is to collaboratively fine-tune shared global MLLM for VQA (Antol et al., 2015). Following (Liu et al., 2024a), we formulate this as an open-ended generation problem, where the model generates free-form answers given imagequestion pairs. Existing approaches assume that the full MLLM can be deployed on each client, which is infeasible in practice due to the massive size of LLM backbones. Client devices often lack sufficient compute, memory, and bandwidth to support such models, and privacy regulations further restrict centralized data access. These constraints call for new FL framework that avoids clientside LLM deployment while enabling efficient adaptation and communication. To address these challenges, we propose FedNano, parameterefficient framework that centralizes the computationally intensive LLM on the server while enabling lightweight, client-specific tuning. In the following sections, we detail the design of FedNano, focusing on how it minimizes computational and communication overhead and addresses data heterogeneity."
        },
        {
            "title": "3.2 Overview of FedNano Architecture",
            "content": "FedNano is designed to address the inherent challenges of deploying MLLMs in FL environments. As shown in Fig. 2, it introduces new architecture that centralizes the computationally intensive LLM on the server, while clients maintain only lightweight NanoEdge modules for task-specific adaptation. NanoEdge freezes the modality encoders and connector, restricting training to taskspecific NanoAdapters. This design eliminates the need to deploy the full model on resourceconstrained devices, reducing client-side computaFigure 2: Overview of the FedNano framework. The server hosts the frozen LLM, while each client performs local tuning via NanoEdge, which includes NanoAdapter-T for text and NanoAdapter-I for vision. Clients upload low-rank adapter updates, which are aggregated on the server using Fisher merging. This design reduces client overhead and supports scalable, multimodal federated learning under data heterogeneity. tion and enabling edge deployment on mobile or IoT systems. The complete training and aggregation algorithm is provided in Appendix A. FedNano jointly addresses three key challenges in MLLM-based FL: high computation, communication cost, data heterogeneity. By offloading the LLM to the server, clients train only the NanoEdge module, which includes frozen encoders and connector, and optimizes small set of NanoAdapters for task-specific adaptation. The total client-side module accounts for less than 5% of the model parameters, while the trainable NanoAdapters comprise only 0.01%. During aggregation, only NanoAdapters updates are uploaded, significantly reducing communication overhead. NanoAdapters are optimized via low-rank decomposition, enabling expressive local tuning while preserving pretrained alignment with the frozen LLM. This compact update mechanism supports lowbandwidth environments and enhances training efficiency. To address data heterogeneity, FedNano integrates Fisher Merging (Matena and Raffel, 2022) into FL as an advanced aggregation strategy, leveraging client-specific posterior estimates to align local updates with global objectives. By weighting and combining NanoAdapter updates based on their estimated importance, this method improves robustness across diverse datasets, even under nonIID conditions. Together with its architectural and optimization designs, FedNano bridges the gap between the computational barriers of MLLM deployment and the practical constraints of FL, offering scalable, efficient, and privacy-preserving solution for decentralized multimodal learning. 3.3 NanoEdge: Client-Side Tuning Module MLLMs are composed of three key components: modality encoders, connector, and pretrained LLM backbone. The modality encoders extract embeddings from raw inputs, such as images and text, while the connector aligns these embeddings into unified representation compatible with the LLM. Together, these components enable MLLMs to effectively handle diverse multimodal tasks by leveraging their pretrained capabilities. Building on this structure, NanoEdge introduces NanoAdapters at the interface between the connector and the LLM to facilitate efficient task-specific tuning while preserving the pretrained alignment across modalities. By freezing the modality encoders and the connector, NanoEdge maintains their alignment with the LLM, ensuring the foundational structure of the pretrained model remains intact. This design allows NanoAdapters to focus solely on learning task-specific patterns from local client data and integrating federated knowledge updates, avoiding any disruption to the pretrained alignment. By restricting training to the lightweight NanoAdapter parameters, NanoEdge minimizes client-side computational demands while enabling efficient and privacy-preserving adaptation. The NanoAdapters employ low-rank decomposition mechanism, inspired by LoRA (Hu et al., 2021), consisting of down-projection to reduce embedding dimensionality and an up-projection to restore it. This design balances parameter efficiency and adaptation capability, enabling NanoEdge to perform localized tuning and transmit 4 updates efficiently. Each modality is equipped with dedicated NanoAdapterAI for images and AT for textcapturing modality-specific patterns essential for multimodal tasks. Unlike traditional adapters that are inserted into LLM, NanoAdapters remain externally attached to the modality connector, requiring no structural access to or execution of LLM. This makes them uniquely compatible with server-hosted LLMs in federated environments. 3.4 Fisher-Guided Adaptive Aggregation In FL, model aggregation can be interpreted as maximizing the joint posterior likelihood across clients. Traditional methods like FedAvg implicitly assume isotropic Gaussian posteriors (Matena and Raffel, 2022), which oversimplifies client uncertainty and leads to degraded performance under data heterogeneity. FedNano addresses this limitation by adopting Fisher Merging (Matena and Raffel, 2022), which leverages the Laplace approximation for more accurate posterior estimation. The global update is computed as: θglobal = Fkθk (cid:80)K k=1 (cid:80)K (cid:80)K Dk k=1 Dk Dk k=1 Dk k=1 (cid:80)K , (1) Fk where θk denotes the NanoAdapter parameters of client k, Fk is the Fisher Information Matrix (FIM), which serves as the precision matrix of the Laplace approximation, and Dk is the local dataset. This weighting improves the alignment of local updates with their estimated importance, enhancing generalization under non-IID data. To ensure scalability, FedNano approximates the full FIM with its diagonal (Kirkpatrick et al., 2017), and computes it efficiently from squared gradients during backpropagation (Wu et al., 2023). This reduces computation from O(θ2) to O(θ) without sacrificing aggregation accuracy. Compared to uniform averaging, this method dynamically prioritizes impactful updates, achieving stronger global performance under statistical heterogeneity."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "We evaluate our approach on the Visual Question Answering (VQA) task using two established benchmarks: ScienceQA (Lu et al., 2022) and IconQA (Lu et al., 2021). These datasets were selected for their well-defined categorical structures 5 and multimodal complexities, making them particularly suitable for assessing the performance of FL in non-IID settings. To simulate FL in non-IID setting, we partitioned the datasets using Dirichlet distributions following (Che et al., 2023; Lai et al., 2022; Zhang et al., 2024) with concentration parameter α = 1 to create strongly non-IID splits. Partitioning was guided by topic annotations in ScienceQA and skill annotations in IconQA, ensuring heterogeneous yet meaningful distributions across five simulated clients. Each partition, representing an individual client dataset, maintains consistent train-validation-test splits for evaluation. We evaluate our approach on MiniGPT-4 (Zhu et al., 2023) and LLaVA-1.5 (Liu et al., 2024b). 4.2 Implementation Details Baselines To the best of our knowledge, FedNano is the first FL framework specifically designed to support MLLMs by centralizing the LLM on the server. This architectural shift renders existing PEFT-based FL methods inapplicable, as they assume full-model access and local integration with the LLM. Given the absence of prior work addressing this setting, we evaluate FedNano against three representative FL baselines: FedAvg (McMahan et al., 2017), foundational aggregation method with limited handling of data heterogeneity; FedProx (Li et al., 2020), which mitigates client drift through proximal term but lacks parameterspecific adaptation; and FedDPA-F (Yang et al., 2024), which integrates advanced alignment strategies but incurs high computational and communication overheads. We further include comparisons with centralized model, representing the performance upper bound achieved with access to all data, and locally fine-tuned models, which operate in isolation without collaboration. Training Configurations The training process includes 10 communication rounds (R = 10), with each client performing one local epoch per round using batch size of 8. All experiments were conducted on NVIDIA A100 80G GPUs."
        },
        {
            "title": "4.3 Main Results",
            "content": "Results in Tab. 2 demonstrate that FL methods consistently outperform locally fine-tuned models (LocFT), emphasizing the benefit of global knowledge sharing in distributed, heterogeneous settings. FedNano achieves the highest average performance among all FL methods, more effectively narBackbone Approach ScienceQA (Clients) IconQA (Clients) C2 C3 C4 C5 Avg C2 C3 C4 C5 Avg MiniGPTLLaVA-1.5 Centralized 73.70 88.34 89.83 84.52 87.41 84.76 80.76 86.62 81.16 82.74 85.36 83.33 67.74 74.69 77.42 72.46 74.07 73.28 67.70 73.48 70.63 70.86 77.53 72.04 LocFT 70.22 79.65 79.65 75.19 75.56 76.05 70.31 75.61 74.98 72.76 81.25 74.98 FedAvg 70.97 80.40 80.15 75.19 75.80 76.50 70.94 77.36 74.58 71.50 80.70 75.01 FedProx 71.96 78.41 81.14 76.42 75.80 76.75 70.94 77.91 74.51 73.08 80.30 75.35 FedDPA-F 68.98 81.89 80.89 76.43 77.04 77.05 72.21 77.28 75.85 74.27 82.52 76.42 FedNano Centralized 83.87 91.07 89.33 90.57 89.38 88.84 86.62 88.92 84.88 87.25 88.45 87.22 71.96 80.89 76.92 79.65 75.80 77.04 75.93 78.94 72.53 74.35 76.50 75.65 LocFT 73.20 84.37 83.62 82.13 80.49 80.76 71.18 79.89 76.80 77.51 83.23 77.72 FedAvg 73.95 84.37 83.87 81.39 80.00 80.71 70.23 80.13 76.72 77.51 82.36 77.39 FedProx 73.70 84.12 84.12 81.89 79.51 80.67 72.12 79.65 76.80 77.43 82.36 77.68 FedDPA-F 74.94 84.12 84.86 82.88 80.25 81.41 72.13 80.44 77.36 77.43 82.83 78.04 FedNano Table 2: Performance comparison. Results include centralized training, local fine-tuning (LocFT), and various federated approaches. FedNano achieves superior average performance on both datasets compared to other federated approaches, demonstrating its effectiveness in handling client heterogeneity. Approach α = 0. α = 5 C1 C2 C3 C4 Avg C1 C2 C3 C4 Avg 69.94 75.80 75.48 73.18 77.00 74.28 65.71 70.62 71.41 72.76 70.64 70.22 LocFT 72.80 76.80 75.50 73.20 73.60 74.38 74.34 75.61 72.92 76.08 74.68 74.72 FedAvg FedProx 71.54 74.79 74.15 69.72 70.06 73.05 68.48 70.30 70.15 70.15 71.04 70.02 FedDPA-F 70.25 76.40 74.10 72.50 78.55 74.27 71.52 76.83 74.51 73.24 75.84 74.38 73.85 78.22 80.14 76.28 74.94 76.68 74.90 76.16 74.18 74.82 73.73 74.75 FedNano Table 3: Performance of MiniGPT-4 on IconQA under different data heterogeneity levels. FedNano consistently outperforms all baselines, with the largest gains under highly non-IID conditions. rowing the gap to centralized training than existing baselines. While FedAvg performs competitively with simple weighted averaging, its inability to adapt to non-IID data results in suboptimal performance under heterogeneous distributions. FedProx mitigates client drift by constraining local updates toward the global model, but this rigid constraint limits flexibility, making it insufficient for complex multimodal tasks. FedDPA-F, though designed for personalization, requires careful tuning of global training epochs and risks overwriting the global adapter during local updates, potentially degrading performance due to catastrophic forgetting. In contrast, the superior performance of FedNano is attributed to its novel design and optimization strategies. As shown in Tab. 2, FedNano achieves an average accuracy of 77.05% on ScienceQA and 76.42% on IconQA for MiniGPT-4, exceeding FedAvg and FedProx, indicating improved generalization in heterogeneous client environments. For LLaVA, FedNano attains 81.41% on ScienceQA and 78.04% on IconQA, surpassing FedDPA-F and FedProx, demonstrating enhanced robustness in multimodal FL. These results validate the effectiveness of NanoAdapters for modalityspecific adaptation, while substantially reducing client-side computational and storage demands, enabling deployment on resource-limited devices. Moreover, FedNano integrates Fisher Merging with diagonal approximation of the FIM, allowing the system to prioritize critical parameter updates based on client-specific confidence. This results in more effective aggregation than uniform averaging, improving stability under non-IID distributions while reducing overfitting to local client noise. By balancing generalization and personalization, FedNano consistently delivers strong performance across diverse client settings, all while maintaining minimal communication overhead."
        },
        {
            "title": "4.4 Analysis",
            "content": "Robustness under Data Heterogeneity To assess the robustness of FedNano under varying levels of data heterogeneity, we evaluate its performance on IconQA using the MiniGPT-4 backbone across different Dirichlet concentration val6 Approach C1 C2 C4 C5 C6 C7 C8 C10 Avg 67.56 69.77 73.89 67.24 79.90 72.15 69.77 64.71 71.67 67.35 70.40 LocFT 74.52 81.01 78.00 78.63 85.91 79.90 75.94 75.63 70.90 77.86 77.83 FedAvg FedProx 73.89 76.74 77.37 75.63 84.01 76.58 73.41 71.36 78.79 72.29 76.00 FedDPA-F 74.52 81.01 78.00 78.63 85.91 79.90 75.94 75.63 70.90 77.86 77.83 77.03 82.77 78.22 79.67 88.57 80.35 81.34 72.84 73.77 79.47 78.86 FedNano Table 4: Performance of MiniGPT-4 on IconQA with 10 simulated clients. FedNano achieves the best average accuracy, demonstrating strong scalability to larger federated setups. ues (α = 0.1 and α = 5). As shown in Tab. 3, FedNano consistently achieves the highest average accuracy in the highly non-IID setting (α = 0.1), outperforming all FL baselines. This demonstrates the effectiveness of its Fisher-guided aggregation in aligning heterogeneous client updates. While the performance gap narrows under near-IID conditions (α = 5), FedNano remains competitive, indicating that its advantages are most pronounced in realistic heterogeneous federated scenarios. Scalability to Larger Client Populations To evaluate the scalability of FedNano, we extend the number of clients from 5 to 10 on the IconQA dataset using the MiniGPT-4 backbone. As shown in Tab. 4, FedNano achieves the highest average accuracy, consistently outperforming all baselines. This demonstrates that the framework retains its effectiveness even as the federated environment becomes more fragmented. The results confirm that FedNano scales robustly with increasing client population, reinforcing its practicality for real-world large-scale federated deployments. Generalization under Cross-Task Client Distribution We evaluate FedNano in challenging cross-task setup where four clients are respectively assigned A-OKVQA, OK-VQA, IconQA, and GQA, introducing significant task-level heterogeneity. As shown in Tab. 5, FedNano achieves stable and strong performance across all clients. This robustness stems from its modular design and Fisher-guided aggregation, which enable effective alignment of heterogeneous updates and support generalization across semantically diverse tasks. The Necessity of Combining Both AT and AI To evaluate the necessity of the textual adapter AT and the visual adapter AI , we conduct ablation experiments using three configurations: AT only, AI only, and both. For MiniGPT-4, AT achieves 45.91% on ScienceQA and 57.77% on IconQA, while AI improves to 74.57% and 75.17%. Their Approach C1 C2 C4 Avg 34.35 28.83 29.00 29.53 30.86 FedAvg FedProx 52.45 50.82 59.80 42.15 51.30 FedDPA-F 52.76 51.12 60.10 42.46 51.61 54.20 52.60 60.36 43.32 52.62 FedNano Table 5: Performance under cross-task federated setup on MiniGPT-4. FedNano achieves the best average accuracy across clients with distinct VQA tasks. Backbone Variants ScienceQA IconQA MiniGPT-4 LLaVA-1.5 AT AI AT + AI AT AI AT + AI 45.91 74.57 76.42 50.08 77.03 78. 57.77 75.17 76.04 48.15 77.12 77.83 Table 6: Performance of different adapters. Combining AT and AI consistently yields the best results across backbones, confirming their complementarity. combination further boosts accuracy to 76.42% and 76.04%, outperforming AI alone by +1.85% and +0.87%. As shown in Tab. 6, similar trends are observed with LLaVA-1.5, confirming the robustness of combining both adapters. The poor performance of AT alone suggests that textual inputs provide insufficient task-relevant information in these visioncentric VQA tasks. These results validate the dualadapter design of NanoEdge, where AI handles visual adaptation and AT enhances generalization. Trade-offs in Fisher-Guided Adaptive Aggregation FIM is specific to particular set of model parameters and plays key role in the ability of FedNano to achieve superior global alignment by capturing parameter importance. To compute the FIM precisely, FedNano employs additional forward and backward passes per communication round, ensuring accurate parameter estimation. While this enhances accuracy, it introduces modest compuDataset Variants MiniGPT-4 LLaVA-1.5 ScienceQA IconQA FedNano FedNano-EF FedAvg FedProx FedDPA-F FedNano FedNano-EF FedAvg FedProx FedDPA-F 77.05 76.55 76.05 76.50 76.75 76.42 76.04 74.98 75.01 75.35 81.41 80.81 80.76 80.71 80.67 78.04 77.83 77.72 77.39 77. Table 7: Performance comparison of FedNano and FedNano-EF on ScienceQA and IconQA. FedNano achieves the highest accuracy, while FedNano-EF offers trade-off with reduced computational overhead, demonstrating strong performance across both datasets. tational overhead. To explore the trade-offs between precision and efficiency, we conduct an ablation study with FedNano-EF, variant that approximates the FIM during standard training, eliminating the need for additional computation steps. This modification reduces computational overhead to the level of FedAvg. Despite this simplification, FedNano-EF incurs only slight accuracy trade-off and consistently outperforms baselines, as shown in Tab. 7. These results demonstrate the adaptability of FedNano: the standard version excels in accuracy-critical tasks by leveraging precise FIM computation to optimize alignment, while FedNano-EF provides practical alternative for resource-constrained environments, achieving strong performance with reduced overhead. Frequent Communication Amplifies the Advantages of FedNano As shown in Fig. 3a, reduced communication frequency leads to general decline in global model performance across all methods due to increased parameter divergence, which hinders effective aggregation. Importantly, the results highlight that FedNano outperforms FedAvg by larger margin when communication is more frequent. With shorter intervals, FIM mechanism of FedNano can better leverage aligned client parameters to prioritize impactful updates, amplifying its advantages in handling data heterogeneity. In contrast, FedAvg struggles with parameter divergence regardless of communication frequency, showing minimal improvement with more frequent updates. These findings underscore that while frequent communication benefits all methods, it sig8 (a) Communication Freq. (b) Adapter Rank Figure 3: (a) Impact of communication frequency. FedNano outperforms FedAvg, with more frequent communication amplifying its advantages; (b) Effect of adapter rank. FedNano consistently achieves superior performance, demonstrating its ability to capture task-specific and client-specific information effectively. nificantly enhances the effectiveness of FedNano, reinforcing its superior ability to integrate clientspecific updates and maintain robust performance in federated learning environments. Higher Adapter Ranks Enhance FedNano Performance Fig. 3b illustrates the impact of adapter rank, comparing FedNano with FedAvg on the ScienceQA dataset. As the adapter rank increases, accuracy improves due to the enhanced capacity to encode task-specific and client-specific information, which is particularly important in non-IID settings. However, higher ranks also incur greater communication costs, necessitating trade-off between performance and resource efficiency in FL. FedNano consistently outperforms FedAvg across all ranks, with the performance gap widening at higher ranks. This improvement is driven by the FIM aggregation, which leverages richer client-specific updates at higher ranks to achieve better alignment between local contributions and the global model. In contrast, at lower ranks, the limited adapter capacity constrains the quality of updates, reducing the effectiveness of FIM aggregation."
        },
        {
            "title": "5 Conclusion",
            "content": "This work introduced FedNano, an FL framework that tackles the unique challenges of deploying MLLMs in decentralized settings. By centralizing the LLM on the server and employing lightweight NanoAdapters on clients, FedNano achieves significant gains in both resource and communication efficiency, while effectively addressing data heterogeneity in non-IID environments. Comprehensive evaluations on ScienceQA and IconQA benchmarks demonstrate that FedNano consistently outperforms state-of-the-art FL baselines, further narrowing the gap between federated and centralized training. By combining scalable design with robust performance, FedNano offers practical and privacy-preserving solution, advancing the realworld deployment of MLLMs. could further establish FedNano as foundational framework for real-world federated multimodal AI systems."
        },
        {
            "title": "References",
            "content": "While FedNano demonstrates strong performance and efficiency, several aspects remain open for further improvement. One limitation lies in the assumption that all clients possess similar hardware capabilities to manage NanoAdapters, condition that may not hold in real-world federated settings with highly heterogeneous devices. Future research could explore adaptive mechanisms that dynamically adjust NanoAdapter configurations to fit each clients resource constraints, enabling broader applicability across diverse edge platforms. Although FedNano is designed to address high heterogeneity across clients, practical deployments may involve incomplete modality settings, where some clients lack access to specific input modalities. Handling such partial data scenarios requires new strategies to ensure effective collaboration and generalization without assuming full modality availability. Moreover, while the current framework supports vision and language inputs, extending it to incorporate additional modalities such as audio, sensor data, or time-series streams could enable applications in multimodal healthcare, industrial IoT, and autonomous systems. Another promising direction is integrating FedNano into federated multi-agent systems, where distinct agents learn collaboratively, which could enable novel applications in domains like logistics and autonomous driving, further demonstrating the flexibility of the framework. Finally, while FedNano already offers strong privacy guarantees by transmitting only lightweight adapter updates, incorporating advanced privacypreserving techniques such as differential privacy could provide even stronger safeguards. key challenge will be achieving such privacy enhancements without sacrificing the computational and communication efficiency that underpins FedNanos practicality. In summary, while FedNano addresses key challenges in federated learning for MLLMs, these future directions highlight its potential for continued innovation. Enhancing its adaptability to incomplete modalities, extending cross-modal coverage, and incorporating stronger privacy mechanisms Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. 2022. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, and Devi Parikh. 2015. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 24252433. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. 2023. Qwen technical report. arXiv preprint arXiv:2309.16609. Liwei Che, Jiaqi Wang, Xinyue Liu, and Fenglong Ma. 2024. Leveraging foundation models for multimodal federated learning with incomplete modality. Preprint, arXiv:2406.11048. Tianshi Che, Ji Liu, Yang Zhou, Jiaxiang Ren, Jiwen Zhou, Victor Sheng, Huaiyu Dai, and Dejing Dou. 2023. Federated learning of large language models with parameter-efficient prompt tuning and adaptive optimization. arXiv preprint arXiv:2310.15080. Haokun Chen, Yao Zhang, Denis Krompass, Jindong Gu, and Volker Tresp. 2023. Feddat: An approach for foundation model finetuning in multimodal heterogeneous federated learning. Preprint, arXiv:2308.12305. Jiayi Chen and Aidong Zhang. 2024. On disentanglement of asymmetrical knowledge transfer for modality-task agnostic federated learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1131111319. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, and Steven Hoi. Boyang Li, Pascale Fung, 2023. Instructblip: Towards general-purpose visionlanguage models with instruction tuning. Preprint, arXiv:2305.06500. Tiantian Feng, Digbalay Bose, Tuo Zhang, Rajat Hebbar, Anil Ramakrishna, Rahul Gupta, Mi Zhang, Salman Avestimehr, and Shrikanth Narayanan. 2023. Fedmultimodal: benchmark for multimodal federated learning. Preprint, arXiv:2306.09486. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. 9 Parameter-efficient transfer learning for nlp. In International conference on machine learning, pages 27902799. PMLR. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685. James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. 2017. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):35213526. Fan Lai, Yinwei Dai, Sanjay Singapuram, Jiachen Liu, Xiangfeng Zhu, Harsha Madhyastha, and Mosharaf Chowdhury. 2022. Fedscale: Benchmarking model and system performance of federated learning at scale. In International conference on machine learning, pages 1181411827. PMLR. Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR. Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. 2020. Federated optimization in heterogeneous networks. Proceedings of Machine learning and systems, 2:429 450. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2024a. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2024b. Visual instruction tuning. Advances in neural information processing systems, 36. Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, KaiWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. 2022. Learn to explain: Multimodal reasoning via thought chains for science question answering. In The 36th Conference on Neural Information Processing Systems (NeurIPS). Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. 2021. Iconqa: new benchmark for abstract diagram understanding and visual language reasoning. In The 35th Conference on Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks. Michael Matena and Colin Raffel. 2022. Merging models with fisher-weighted averaging. Advances in Neural Information Processing Systems, 35:17703 17716. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera Arcas. 2017. Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics, pages 12731282. PMLR. Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023a. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277. Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. 2023b. Kosmos-2: Grounding multimodal large arXiv preprint language models to the world. arXiv:2306.14824. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971. Ziyao Wang, Zheyu Shen, Yexiao He, Guoheng Sun, Hongyi Wang, Lingjuan Lyu, and Ang Li. 2024. Flora: Federated fine-tuning large language models with heterogeneous low-rank adaptations. arXiv preprint arXiv:2409.05976. Chengyue Wu, Teng Wang, Yixiao Ge, Zeyu Lu, Ruisong Zhou, Ying Shan, and Ping Luo. 2023. πtuning: transferring multimodal foundation models with optimal multi-task interpolation. In Proceedings of the 40th International Conference on Machine Learning, pages 3771337727. Binqian Xu, Xiangbo Shu, Haiyang Mei, Guosen Xie, Basura Fernando, Mike Zheng Shou, and Jinhui Tang. 2024. Fedmllm: Federated fine-tuning mllm on multimodal heterogeneity data. arXiv preprint arXiv:2411.14717. Yiyuan Yang, Guodong Long, Tao Shen, Jing Jiang, and Michael Blumenstein. 2024. Dual-personalizing arXiv adapter for federated foundation models. preprint arXiv:2403.19211. Liping Yi, Han Yu, Gang Wang, and Xiaoguang Liu. 2023. Fedlora: Model-heterogeneous personalized federated learning with lora tuning. arXiv preprint arXiv:2310.13283. Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. 2024. survey on multimodal large language models. National Science Review, page nwae403. Qiying Yu, Yang Liu, Yimu Wang, Ke Xu, and Jingjing Liu. 2023. Multimodal federated learning via contrastive representation ensemble. Preprint, arXiv:2302.08888. 10 Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. 2021. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked languagemodels. arXiv preprint arXiv:2106.10199. Jianyi Zhang, Saeed Vahidian, Martin Kuo, Chunyuan Li, Ruiyi Zhang, Tong Yu, Guoyin Wang, and Yiran Chen. 2024. Towards building the federatedgpt: Federated instruction tuning. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 69156919. IEEE. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592. 11 FedNano: Pseudocode Overview Algorithm 1 FedNano. The clients are indexed by k; is the number of communication rounds, and is the number of local steps. Server Update: 1: Randomly initialize A0 2: for = 1 to do 3: in NanoAdapter, and distribute to clients and 4: 5: 6: for = 1 to in parallel do ClientUpdate(θr1 θr Compute FIM Fk global, Dk) k, k }) end for global ServerAgg({θr θr 7: 8: end for Client Update: 1: θr1 global 2: for local step = 1 to do 3: θr1 Sample {(vk, qk, ak)} from Dk θr (t) Optimization(θr (t1), vk, qk, ak) 4: 5: end for 6: return θr Eq."
        }
    ],
    "affiliations": [
        "LMU Munich",
        "Munich Center for Machine Learning",
        "Siemens Technology",
        "Technical University of Munich",
        "University Heidelberg"
    ]
}