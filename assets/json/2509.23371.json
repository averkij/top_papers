{
    "paper_title": "Alignment through Meta-Weighted Online Sampling: Bridging the Gap between Data Generation and Preference Optimization",
    "authors": [
        "Junming Yang",
        "Ning Xu",
        "Biao Liu",
        "Shiqi Qiao",
        "Xin Geng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Preference optimization is crucial for aligning large language models (LLMs) with human values and intentions. A significant challenge in this process is the distribution mismatch between pre-collected offline preference data and the evolving model policy. Existing methods attempt to reduce this gap using static heuristics or decoupled online sampling strategies, but they often fail to adapt to the model's dynamic learning state. To bridge this gap, we propose Meta-Weighted Adaptive Preference Optimization (MetaAPO), a novel framework that dynamically couples data generation with model training. MetaAPO employs a lightweight meta-learner, as an \"alignment gap estimator\", to evaluate the potential benefits of on-policy sampling in relation to offline data. This guides targeted online generation and assigns sample-wise meta-weights to the optimization objective, dynamically balancing the quality and distribution of online and offline data. Experiments on AlpacaEval 2, Arena-Hard and MT-Bench demonstrate that MetaAPO consistently outperforms existing preference optimization approaches across various settings, while reducing 42% in online annotation costs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 1 7 3 3 2 . 9 0 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "ALIGNMENT THROUGH META-WEIGHTED ONLINE SAMPLING: BRIDGING THE GAP BETWEEN DATA GENERATION AND PREFERENCE OPTIMIZATION Junming Yang, Ning Xu, Biao Liu, Shiqi Qiao & Xin Geng School of Computer Science and Engineering Southeast University Nanjing, China {jmingyang,xning,liubiao01,sqqiao,xgeng}@seu.edu.cn"
        },
        {
            "title": "ABSTRACT",
            "content": "Preference optimization is crucial for aligning large language models (LLMs) with human values and intentions. significant challenge in this process is the distribution mismatch between pre-collected offline preference data and the evolving model policy. Existing methods attempt to reduce this gap using static heuristics or decoupled online sampling strategies, but they often fail to adapt to the models dynamic learning state. To bridge this gap, we propose Meta-Weighted Adaptive Preference Optimization (MetaAPO), novel framework that dynamically couples data generation with model training. MetaAPO employs lightweight meta-learner, as an alignment gap estimator, to evaluate the potential benefits of on-policy sampling in relation to offline data. This guides targeted online generation and assigns sample-wise meta-weights to the optimization objective, dynamically balancing the quality and distribution of online and offline data. Experiments on AlpacaEval 2, Arena-Hard and MT-Bench demonstrate that MetaAPO consistently outperforms existing preference optimization approaches across various settings, while reducing 42% in online annotation costs."
        },
        {
            "title": "INTRODUCTION",
            "content": "Learning from human feedback is essential for aligning large language models (LLMs) with human values and intentions (Ouyang et al., 2022), ensuring they are helpful, honest and harmless (Achiam et al., 2023; Grattafiori et al., 2024). This advancement is primarily achieved in the model alignment stage, which involves training reward models or LLMs directly on datasets curated from human preferences. Reinforcement learning from human feedback (RLHF) (Stiennon et al., 2020; Ouyang et al., 2022) is widely used method for fine-tuning LLMs to achieve this goal. However, RLHF often faces challenges, particularly in computational efficiency and training stability. Recently, alternative offline algorithms like DPO (Rafailov et al., 2023), SimPO (Meng et al., 2024) and KTO (Ethayarajh et al., 2024) have been explored. These methods reparameterize the RLHF objective to directly optimize the policy using pre-collected offline preference datasets, simplifying the training process and improving efficiency. As alignment research progresses, recent studies have emphasized that the quality and distribution of preference data are critical for effective model alignment (Zhou et al., 2023; Deng et al., 2025; Xia et al., 2024; Ding et al., 2024). Although offline preference datasets are diverse and scalable, they often suffer from distribution mismatch between the pre-collected training data and current policy, as the offline data is usually generated by different model. This mismatch leads to out-of-distribution (OOD) issues that hinder preference alignment performance (Yan et al., 2024; Xu et al., 2024; Tajwar et al., 2024). To improve the quality and distribution of training data, existing methods for mitigating distribution shifts generally fall into two categories. Some methods improve the reliability of offline data by filtering out low-quality or OOD samples using manually designed metrics, such as average validation loss, as exemplified by Selective DPO (Gao et al., 2025). Others attempt to mitigate distribution shifts by constructing training data through online sampling (Zhang et al., 2024b; Ko et al., 2024; Badrinath et al., 2024). For instance, Iterative DPO (Xiong et al., 2023; Guo et al., 2024b) and SPPO (Wu et al., 2024) follow an iterative sampling-alignment loop: the current policy"
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Left: Overview of MetaAPO. MetaAPO employs meta-learner to couple online data generation (top) and model training (bottom). The meta-learner adaptively assigns weights by evaluating offline data, guiding both targeted online sample generation and training on the weighted combination of offline and online samples. Right: Performance (left y-axis, line plots) and online generation and annotation ratio relative to Online DPO (right y-axis, bar plots) across training iterations for different methods. first generates responses, which are then annotated by reward model to collect preference data for direct preference alignment. The updated policy is subsequently used to repeat this process. While human-annotated offline data typically offers high efficiency and quality, the distribution mismatch between static datasets and the evolving model policy can negatively impact alignment performance. On the other hand, model-generated data obtained through online sampling more accurately reflects the current policy distribution, which helps address the distribution shifts. However, online data often lacks diversity and quality due to its reliance on the models capabilities and current alignment state, leading to preference data that may be inefficient or even noisy. This trade-off underscores the need for adaptive mechanisms that can dynamically leverage the efficiency and diversity of offline data while taking advantage of the distribution benefits of online data. However, existing methods primarily depend on heuristic, manually designed metrics (e.g., static thresholds) for data selection and generation (Dong et al., 2024; Gao et al., 2025; Zhang et al., 2024b; Ji et al., 2024), overlooking the interaction between the data sampling process and preference optimization. An integrated approach that simultaneously accounts for data quality, distribution alignment, and the dynamics of preference optimization is essential for more effective alignment. Motivated by the above considerations, we propose Meta-Weighted Adaptive Preference Optimization (MetaAPO), novel framework that closely couples data sampling with model alignment through lightweight, learnable meta-learner. As demonstrated in Figure 1 (Left), the meta-learner acts as an alignment gap estimator, predicting the potential improvements that online generations can provide relative to offline data. This enables targeted online sampling, prioritizing prompts expected to yield the highest alignment gains. In addition, the meta-learner adjusts the training process by assigning sample-wise meta-weights to hybrid loss function, adaptively balancing the contributions of offline data and online samples according to their respective quality and distribution advantages. This dynamic interaction ensures that the model continually adapts its learning focus to mitigate distribution mismatches while avoiding redundant online sampling for already aligned data, thus improving both training efficiency and performance. Empirical results demonstrate that MetaAPO significantly improves alignment across various base models, outperforming both offline and online approaches. Our contributions can be summarized as follows: We propose MetaAPO, an extensible alignment framework compatible with various preference optimization methods. It bridges the gap between online data generation and model training by adaptively coupling these two stages, enabling model-aware alignment process. Our method employs meta-learner that adaptively assigns sample-wise weights to data. These weights serve two purposes: guiding online data sampling based on the models training needs to mitigate distribution mismatches and adjusting the training by emphasizing beneficial samples. Extensive experiments demonstrate that MetaAPO achieves superior alignment performance compared to baselines, even reducing 42% online annotation requirements (Figure 1, Right)."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Alignment with Preference Data. RLHF is widely used method for aligning LLMs with human values and intentions (Stiennon et al., 2020; Ouyang et al., 2022; Bai et al., 2022). While effective, RLHF suffers from high computational costs and training instability, prompting the exploration of alternative alignment strategies. One prominent direction is offline preference alignment (Azar et al., 2024; Dong et al., 2023; Yuan et al., 2023; Lu et al., 2024), which directly trains models using pre-collected datasets of human preferences, such as DPO (Rafailov et al., 2023), SimPO (Meng et al., 2024) and KTO (Ethayarajh et al., 2024). These methods are attractive for their efficiency and simplicity. However, key limitation is the distribution mismatch between static training data and the evolving outputs of the policy model, which often leads to poor generalization on out-of-distribution responses (Xu et al., 2024; Yan et al., 2024; Yang et al., 2025; Tajwar et al., 2024). To overcome this limitation, online preference alignment methods such as Iterative DPO (Xiong et al., 2023) and SPPO (Wu et al., 2024) have been proposed. These approaches adopt an iterative training paradigm in which each round involves generating new responses from the current policy and optimizing the model using annotated preferences (Yuan et al., 2024; Ko et al., 2024; Chen et al., 2024). By enabling exploration beyond the limitations of the initial static dataset, they can improve model performance (Su et al., 2025; Xu et al., 2024; Tajwar et al., 2024). However, they primarily rely on on-policy data and often overlook valuable offline preference datasets. Bridging both paradigms, hybrid methods like SELM (Zhang et al., 2024b) integrate offline and online data by using reward model to compare responses from both sources, thereby constructing more diverse preference sets. Nonetheless, these hybrid approaches lack strategic sampling and generation mechanisms, failing to account for the distribution relationship between offline and online data, which leads to inefficient resource utilization and suboptimal performance. Preference Data Selection and Augmentation. These limitations underscore the growing importance of strategic data sampling and targeted augmentation in preference alignment. The less is more principle suggests that selecting high-quality and relevant data often leads to better outcomes than relying on large volumes of unfiltered data (Zhou et al., 2023; Deng et al., 2025; Liu et al., 2023; Ding et al., 2024). Consequently, various data selection methods aim to identify the most informative and helpful samples from general-purpose datasets to improve training effectiveness (Wang et al., 2025; Khaki et al., 2024; Badrinath et al., 2024; Yang et al., 2024b). For instance, Selective DPO (Gao et al., 2025) employs loss-based filtering to discard uninformative or harmful samples, while MAP (Huang et al., 2025) uses internal and external reward margins to guide sample inclusion. Beyond sample selection, data augmentation further enhance alignment (Muldrew et al., 2024; Gou & Nguyen, 2024; Guo et al., 2024a; Liu et al., 2024; Das et al., 2024). ADPO (Ji et al., 2024) filters offline data based on manual threshold and then performs online augmentation on the selected samples. The Reward-Augmented method (Zhang et al., 2024a) augments the dataset by reversing preference pairs under the guidance of prompts. DICE (Chen et al., 2024) and SeRA (Ko et al., 2024) combine data augmentation with filtering methods, guided by length regularization and reward margins to enhance training data. Despite these advancements, most prior work treats data sampling as an auxiliary step, often decoupled from the optimization process and guided by static heuristics. In contrast, MetaAPO proposes framework that closely integrates online data generation with preference optimization via adaptive meta-weighting, mitigating the distribution mismatch in preference alignment."
        },
        {
            "title": "3 PRELIMINARIES",
            "content": "Reinforcement Learning from Human Feedback. RLHF aims to align the language model policy πθ(x) with human preferences over prompts . typical RLHF pipeline (Ouyang et al., 2022) starts by training reward model on preference dataset = {(x, yw, yl)i}N i=1, where each preference pair is annotated such that yw yl indicates that yw is preferred over yl for prompt x. The reward model rψ(x, y) is trained using the BradleyTerry (BT) model (Bradley & Terry, 1952), by minimizing negative log-likelihood loss that encourages larger reward gaps between preferred and dispreferred responses: LRM(ψ) = E(x,yw,yl)D [σ (rψ(x, yw) rψ(x, yl))] , (1) where σ() denotes the logistic function. The policy πθ is then optimized via reinforcement learning to maximize the expected reward while staying close to reference policy πref : ExD,yπθ (x)[rψ(x, y)] βDKL[πθ(yx)πref(yx)], (2) max πθ"
        },
        {
            "title": "Preprint",
            "content": "where β controls the trade-off between reward maximization and deviation from the reference policy. Direct Preference Optimization. While effective, RLHF can be complex, unstable, and computationally demanding. DPO (Rafailov et al., 2023) simplifies the process by directly optimizing the policy using supervised learning objective over preference data, without explicitly learning reward model. DPO reparameterizes the reward as r(x, y) = β(log πθ (yx) πref(yx) + log Z(x)), where Z(x) is the partition function and πref is typically the supervised fine-tuned (SFT) model. By minimizing the negative log-likelihood of reward modeling in Eq. 1, DPO derives the following loss: LDPO(θ) = E(x,yw,yl)D (cid:20) (cid:18) log σ β log πθ(ywx) πref(ywx) β log πθ(ylx) πref(ylx) (cid:19)(cid:21) . (3) DPO supports both offline preference datasets Doffline (e.g., curated in advance) and online datasets Donline (generated by the policy model πθ)."
        },
        {
            "title": "4 METHOD",
            "content": "In this section, we present the technical details of MetaAPO. The alignment process is conducted within single training epoch, where the dataset is divided into sequential subsets and processed iteratively over {0, 1, . . . , }. In each iteration t, we start by performing meta-weighted adaptive online sampling to generate online data from policy πθ(t1) (Section 4.1). The policy is then finetuned on the hybrid dataset using the meta-weighted objective to obtain the updated policy πθ(t) (Section 4.2). Concurrently, the meta-learner is periodically updated to better estimate the potential gain from online data (refer to Section 4.3). The overall workflow is illustrated in Algorithm 1. 4.1 META-WEIGHTED ADAPTIVE ONLINE SAMPLING We begin with model-aware sampling strategy for acquiring new preference data. This process involves using preference scoring function (derived from objectives like DPO) to assess the utility of existing offline samples. Given prompt and pair of preferred and dispreferred responses (yw, yl), the instance-level preference score is computed as: ℓ(x, yw, yl) = log σ β log (cid:18) πθ(ywx) πref(ywx) β log πθ(ylx) πref(ylx) (cid:19) , (4) where higher score ℓ() indicates stronger agreement between the current policy πθ and human preferences, compared to fixed reference policy πref. This scoring function is applied to each instance in the offline preference dataset Doffline = {(x, yoff i=1, producing scores ℓoff(x, yoff , yoff , yoff )i}N ). To enable the sampling process to dynamically adapt to the models current state and requirement, we introduce meta-learner that adaptively re-weights each offline sample based on its preference score. The meta-learner hϕ, implemented as two-layer MLP, maps the preference score ℓ() to weight = hϕ(ℓoff()) [0, 1] (Line 7 in Alg. 1). high weight indicates that the sample aligns well with the models current behavior, while low weight suggests potential misalignment that may require further online exploration. This adaptive meta-weight subsequently guides the online sampling and generation phase. (Line }i, value ui is sampled from Uniform(0, 1). 8-12 in Alg. 1). For each offline tuple {x, yoff , yoff If ui > wi, the current policy πθ generates candidate responses {yon } for prompt x. These responses are then ranked by an external reward model to produce online preference pairs yon pair = (yon ). Finally, both the sampled offline and corresponding online preferences are merged into an augmentation set Daug. This augmented dataset forms dynamic training stream that evolves with the model, enabling the model to better attend to regions where alignment gaps may exist. 1 , . . . , yon , yon 4.2 META-WEIGHTED PREFERENCE OPTIMIZATION During preference optimization, we employ the sample-wise meta-weight to dynamically balance the influence of offline preference data and online model-generated samples (Line 16-17 in Alg. 1). The overall objective is to minimize the meta-weighted preference loss: L(θ) = (x,yoff pair,yon pair)Daug (cid:104) + (1 w) ℓθ(x, yon (cid:124) (cid:123)(cid:122) adaptive online loss , yon ) (cid:125) ℓθ(x, yoff (cid:124) (cid:123)(cid:122) adaptive offline loss , yoff ) (cid:125) 4 (cid:105) , (5)"
        },
        {
            "title": "Preprint",
            "content": "Meta-Weighted Adaptive Online Sampling , yoff Language model πθ, reference model πref, reward model R, meta-learner hϕ i=1; )) , yoff , yoff )i}N )i Doffline do Compute meta-weight: wi hϕ(ℓ(x, yoff Sample ui Uniform(0, 1) if ui > wi then Initialize augmentation dataset Daug and meta buffer Bmeta for each (x, yoff Algorithm 1 Training Scheme of MetaAPO 1: Input: Offline preference dataset Doffline = {(x, yoff 2: 3: Initialize: Load model parameters θ, initialize meta-learner parameters ϕ 4: for each iteration do 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: end for Compute sample-wise meta-weight: wj hϕ(ℓ(x, yoff Update language model by Eq. 5: θ θ αθL(θ) Add batch data to meta buffer: Bmeta Bmeta Dbatch if batch iteration % Tmeta = 0 then Generate responses: {yon , yon Evaluate with R: {yon Add to augmentation set: Daug Daug {(x, yoff Update meta-learner by Eq. 6: ϕ ϕ ηϕLmeta(ϕ; Bmeta) Reset meta buffer: Bmeta end for for each batch Dbatch Daug do 1 , . . . , yon } R(x, {yon } πθ( x) }K , yoff w , yoff end for end if end if j=1) )) , yon , yon )i} Meta-Weighted Model Training (freeze hϕ) Meta-Learner Update (freeze πθ) where ℓθ() denotes standard preference alignment objective (e.g., DPO in Eq. 4 or SimPO) used to align the current policy πθ with the target preferences. The first term captures contributions from offline preferences, while the second focuses on online model-generated preference samples. This joint optimization enriches data diversity. The balance coefficient = hϕ(ℓoff()) is predicted by the meta-learner and indicates the confidence in offline preference data. When the models behavior aligns well with the offline preference, the meta-learner assigns higher weight to reinforce stable learning from reliable offline human-labeled examples. Conversely, when misalignment is detected, is reduced, prompting greater reliance on adaptive corrections from online feedback. 4.3 LEARNING TO WEIGHT: META-LEARNER FOR ALIGNMENT GAP ESTIMATION The effectiveness of both adaptive online sampling (Section 4.1) and meta-weighted preference optimization (Section 4.2) critically depends on the quality of the dynamically assigned weights = hϕ(ℓoff()). Instead of relying on pre-defined heuristics or static weights, we propose to learn the weighting function hϕ through meta-learning approach. The goal is for hϕ to act as an alignment gap estimator, dynamically assessing the utility of each offline sample relative to potential online preference data, and assigning weights that effectively guide the alignment process. This allows training data to be weighted based on the models current state, enabling continuous adaptation throughout the alignment process. Specifically, the meta-learner hϕ is trained in an alternating manner with the policy model πθ. At intervals specified by hyperparameter Tmeta, the parameters of πθ are temporarily frozen, and hϕ is updated using data stored in the meta buffer Bmeta (Lines 1921 in Alg. 1). This buffer aggregates preference data from multiple recent training batches of πθ, enabling the meta-learner to derive more stable and generalized weighting strategy from accumulated experience. The meta-learner hϕ is optimized by minimizing the following loss: Lmeta(ϕ) = (x,yoff pair,yon pair)Bmeta (cid:104) hϕ(ℓoff()) ℓ(x, yoff (cid:124) (cid:123)(cid:122) adaptive offline loss , yoff ) (cid:125) + (1 hϕ(ℓoff())) ℓ(x, yon (cid:124) (cid:123)(cid:122) adaptive online loss , yon ) (cid:125) (cid:105) , (6)"
        },
        {
            "title": "Preprint",
            "content": "where ℓ(x, yoff preference scores for the offline and online response pairs, respectively, for the same prompt x. ) (abbreviated as ℓoff()) and ℓ(x, yon ) (abbreviated as ℓon()) denote the , yoff , yon What does meta-learner update do? For mechanistic understanding of meta-learner, it is useful to analyze the gradient of the loss function Lmeta(ϕ). The gradient with respect to the meta-learner parameters ϕ can be written as: ϕLmeta(ϕ) = EBmeta (cid:104) (cid:0)ℓon() ℓoff()(cid:1) ϕ(hϕ(ℓoff()) (cid:105) . (7) This gradient drives the adaptation of the weighting function hϕ. When online samples yield higher preference scores than offline samples (i.e., ℓon() > ℓoff()), the meta-learner reduces the weight assigned to offline data, encouraging exploration guided by online feedback. Conversely, when offline preferences remain superior, their weights are increased to reinforce stable learning from reliable human annotations. The full gradient derivation is provided in Appendix A.1. Moreover, we provide theoretical guarantee for the meta-learner hϕ in Theorem 1. The result demonstrates that the risk of the learned weighting function is provably close to the oracle risk, with the alignment gap controlled by the hypothesis complexity and meta-buffer size. Theorem 1 (Generalization Bound for Meta-Learner). Let ˆhϕ denote the meta-learner function learned by minimizing the empirical meta-risk over the meta-buffer Bmeta of size m, and let be the oracle function that minimizes the true meta-risk over the hypothesis space H. Assume the meta-loss Lmeta(ϕ) is bounded in [0, ] for any sample (x, yoff pair). Then, for any δ > 0, with probability at least 1 δ, the following inequality holds: pair, yon R(ˆhϕ) R(h) + 4 Radm(Lmeta(ϕ) H) + (cid:113) 2 ln(1/δ) , where R(hϕ) and (cid:98)Rm(hϕ) denote the true and empirical risk respectively, and Radm is the Rademacher complexity of the meta-loss hypothesis. The detailed proof is provided in Appendix A.2. Theorem 1 shows that as the meta-buffer size increases, the risk of the learned meta-learner R(ˆhϕ) converges to that of the oracle function R(h). And the Rademacher complexity term favors simple hypothesis space for better generalization. This justifies that sufficient meta-buffer combined with simple meta-learner can achieve effective estimation and assign near-optimal weights."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "5.1 EXPERIMENTAL SETUP Datasets and Models. We use the Ultrafeedback dataset (Cui et al., 2024) , which contains 64K preference annotations collected from four LLM-generated outputs and labeled preferences by GPT4. To demonstrate the general effectiveness of our method, we experiment with two supervised fine-tuned models from different-sized base model: Llama-3.1-8B (Grattafiori et al., 2024) and Qwen2.5-7B (Yang et al., 2024a). For online preference annotation, we train the reward models on UltraFeedback using the corresponding base model to avoid any preference leakage. We also provide comparison with existing open-source reward models in Appendix C.2. Evaluation Metric. We evaluate our method using three widely adopted automatic evaluation benchmarks for LLMs: AlpacaEval 2 (Dubois et al., 2024), MT-Bench (Zheng et al., 2023), and Arena-Hard (Li et al., 2024). All evaluations are conducted using GPT-4o-latest as the judge model. For AlpacaEval 2, we report the raw win rate (WR) and the length-controlled win rate (LC) against the reference model GPT-4o-2024-05-13. For Arena-Hard, we report the win rate (WR) and the style-controlled win rate (SC), comparing our model against the GPT-4-Preview-1106 baseline. For MT-Bench, we report the average multi-turn score (Score) assigned by GPT-4o-latest, which rates each response on 10-point scale. Baselines. We evaluate the following baseline methods relevant to the setting of our method: Offline Alignment: Methods that optimize models solely using offline human preference datasets, such as DPO (Rafailov et al., 2023), IPO (Azar et al., 2024) and KTO (Ethayarajh et al., 2024). This category also includes Selective DPO (Gao et al., 2025), which improves data quality by filtering out out-of-distribution samples based on hand-crafted metric."
        },
        {
            "title": "Preprint",
            "content": "Table 1: Overall performance of our proposed MetaAPO method with Llama-3.1-8B and Qwen2.57B, compared with offline, online and hybrid baseline methods on AlpacaEval 2, Arena-Hard and MT-Bench. The best results are highlighted in bold. Models SFT DPO IPO KTO SimPO 18.15 13.11 20.28 37.31 Selective DPO 21. ADPO Online DPO PPO SELM 19.33 43. 45.33 43.56 Llama-3.1-8B Qwen2.5-7B AlpacaEval 2 Arena-Hard MT-Bench AlpacaEval 2 Arena-Hard MT-Bench WR(%) LC(%) WR(%) SC(%) Score WR(%) LC(%) WR(%) SC(%) Score 10. 17.28 18.5 21.6 6.63 34.03 34. 48.5 44.3 7.71 25.20 23.50 29. 38.20 20.67 17.26 39.98 40.49 40. 28.9 23.6 21.4 37.8 28.5 32. 38.0 40.8 36.0 Offline Alignment 30.9 26. 26.4 33.0 31.9 6.94 7.02 7. 7.51 7.11 37.24 37.95 38.12 40. 38.02 Online & Hybrid Alignment 30.7 32.1 35.3 34. 6.69 7.33 7.35 7.28 MetaAPO 40.32 48. 48.52 48.08 36.84 36.43 36.51 40. 39.21 35.21 46.64 44.26 44.91 49. 54.6 43.9 54.6 51.7 49.9 53. 55.1 56.5 47.2 48.3 44.1 48. 48.2 48.7 48.4 49.2 51.7 7. 7.64 7.63 7.58 7.74 7.58 7. 7.65 7.48 MetaAPO 47.48 43.21 43. 40.8 7.56 49.14 47.66 58.4 53. 7.78 Online Alignment: Methods that collect model-generated responses and align them using preferences from an external reward modele.g., Online DPO (Xiong et al., 2023) and PPO. Hybrid Alignment: Methods that integrate both offline annotated preferences and online modelgenerated data to optimize alignment, as ADPO (Ji et al., 2024) and SELM (Zhang et al., 2024b). Implementation. All experiments are conducted using 880GB NVIDIA H100 GPUs with batch size of 128 and maximum sequence length of 4096 tokens for 1 epoch. At each iteration round, we sample responses from the current policy using different decoding configurations: temperature = 1.0, top-p = 1.0 for the Llama3.1 setting, and = 0.8, top-p = 0.95 for the Qwen2.5 setting. For each prompt, we generate = 8 diverse responses, which are then scored by reward model to construct preference pairs. The meta-learner is updated every Tmeta = 8 steps. Additional details about model configurations, evaluation procedure and hyperparameters are provided in Appendix B. 5.2 OVERALL PERFORMANCE In Table 1, we present the overall results of MetaAPO compared to offline, online and hybrid state-of-the-art alignment methods on the AlpacaEval 2, Arena-Hard and MT-Bench benchmarks. MetaAPO consistently outperforms preference optimization baselines. Compared to offline alignment methods, which rely solely on fixed, pre-collected datasets, MetaAPO avoids the performance degradation caused by distribution shifts between static data and evolving policies. Enhanced offline approaches like Selective DPO, which attempt to mitigate this mismatch through data filtering, yield only marginal improvements and still fall short of MetaAPO. Online and hybrid methods leverage model-generated samples and online feedback to significantly outperform purely offline baselines. While online and hybrid methods significantly outperform purely offline baselines by leveraging on-policy feedback, they are often constrained by the quality and diversity of their own model generations. The heavy reliance on self-generated data can introduce noisy preferences. MetaAPO surpasses these strong baselines by avoiding regions prone to noisy feedback and effectively leveraging diverse offline data. We also compare performance across different reward models, preference optimization methods and datasets in Appendix C.2, demonstrating the robustness of our approach. MetaAPO achieves superior performance with fewer online samples. As shown in Figure 1 (Right), MetaAPO outperforms strong baselines such as Online DPO, SELM and PPO, while using only 58% of the online generated and annotated samples on average compared to standard online generation methods. This efficiency comes from meta-weighted sampling mechanism, which"
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Left: The dynamic changes in Offline Score (left y-axis) and Reward (right y-axis) across training iterations. Middle: Scatter plot of offline preference score (ℓoff()) versus the online-offline score gap (ℓon() ℓoff()). Points are colored by their sampling status: blue for Sampled (selected for online generation) and orange for Unsampled. Right: Comparison of independent reward score distributions (via kernel density estimation) for testset responses generated by DPO and MetaAPO. prioritizes the most informative data points and skips unnecessary online generation for already aligned or low-gain predicted samples. This adaptive data sampling significantly reduces labeling costs and time consumption, cutting time by 80.1% compared to PPO and by 52.9% compared to Online DPO (detailed in Appendix C.6). 5.3 FURTHER ANALYSES Training dynamics reveal an explore-integrate behavior. To examine the dynamic interplay during training, we tracked the offline preference score ℓoff and generation reward on held-out test set across iterations. As shown in Figure 2 (Left), the average reward steadily increases (from 1.06 to 1.33) while its standard deviation decreases (from 0.49 to 0.37), indicating that MetaAPO robustly enhances generation quality under reward guidance. In contrast, the mean offline preference score drops sharply at Iteration 2 before recovering, suggesting that MetaAPO does more than fit static offline data. The drop reflects active exploration of response strategies diverging from offline preferences, while the subsequent recovery in Iteration 3 reflects the integration of these discoveries with foundational offline knowledge. Overall, these dynamics highlight MetaAPOs adaptive behavior: it continuously identifies and addresses gaps between the evolving policy and offline data. The dynamics are driven by high-gain sampling strategy. Figure 2 (Middle) shows MetaAPOs online sampling strategy based on two signals: the initial offline preference score ℓoff() (x-axis), and the potential gain ℓon() ℓoff() (y-axis). Each point represents an offline sample, marked as Sampled (blue) if selected for generation, or Unsampled (orange) otherwise. Two trends drive sampling. First, points with low ℓoff() (indicating weak alignment or OOD) receive smaller meta-learner weights w, increasing their selection probability. Second, the meta-learner prioritizes samples with high potential gain (ℓon() > ℓoff()), concentrating its selection in the upper-left region where these two factors converge. In contrast, samples with high offline scores or minimal online gain are less likely to be selected, avoiding redundant computation. Effectiveness of the meta-learner. key component of MetaAPO is the meta-learner, simple two-layer MLP that effectively captures the models dynamic alignment state. Its effectiveness stems from two factors. First, we observed clear, learnable relationship between the offline score ℓoff() and potential online-offline gain ℓon() ℓoff(), as shown in Figure 2 (Left). This discovery allows even lightweight network to serve as reliable \"alignment gap estimator.\" Second, the meta-learner is continuously updated alongside the policy, adapting its sampling and weighting decisions to the evolving model state. Experiments with deeper networks and more complex inputs (Appendix C.5) show that the two-layer MLP provides sufficient expressive power, confirming its efficiency and effectiveness. Additional analyses further confirm the generalizability of MetaAPO. As shown in Figure 2 (Right), its generated responses on the test set achieve higher scores distribution under an independent reward model OpenRLHF/Llama-3-8b-rm-mixture. 5.4 ABLATION STUDY To analyze the impact of each core component in MetaAPO, we conduct series of ablation studies under the Llama-3.1-8B setting with the DPO objective. Table 2 summarizes the effectiveness of different sampling strategies, loss weighting mechanisms and the meta-learner."
        },
        {
            "title": "Preprint",
            "content": "Table 2: Ablation study on key components of MetaAPO. We assess the impact of the sampling strategy, loss weighting, and meta-learner. Metrics include AlpacaEval2, Arena-Hard, MT-Bench and the proportion of annotations used during training. Model AlpacaEval 2 Arena-Hard WR(%) LC(%) WR(%) SC(%) MT-Bench Annotations Ratio(%) Score MetaAPO w/ Random Sampling w/ Threshold Sampling w/ All Sampling w/ Uniform Loss Weighting w/o Meta-Learner 47.48 42.61 44.13 46. 39.25 43.07 43.21 38.53 42.92 42. 37.70 40.48 43.9 37.3 40.7 40. 39.2 38.3 40.8 32.1 38.5 37. 33.3 33.1 7.56 7.14 7.14 6. 7.15 7.06 58.4 50.0 53.3 100. 67.1 61.1 Effects of sampling strategies. The choice of which offline samples to augment with online responses is critical factor to achieve efficient and effective alignment. We ablate three design choices and keep the preference optimization and meta-learner training process consistent with the original MetaAPO setup: (1) Random Sampling selects half of the offline data uniformly for online augmentation. (2) Threshold Sampling selects samples whose initial preference scores fall below fixed threshold (resulting in 53.3% annotation ratio). (3) All Sampling augments the entire offline dataset. As shown in Table 2, MetaAPO outperforms both random and threshold sampling across all metrics. All Sampling leverages larger volume of data, yet the absence of consideration for the dynamic distribution relationship between offline and online data during sampling results in suboptimal alignment performance. Effects of loss weighting. We evaluate the impact of meta-weighted preference optimization by replacing the learned weights in Eq. 5 with uniform coefficient of 0.5 for all augmented samples, assigning equal importance to offline and online preference pairs during training. Other components, including adaptive sampling and meta-learner updates, remain unchanged. As shown in Table 2, this modification leads to significant performance drop with consistent declines on all benchmarks. These results suggest that assigning equal importance forces the model to learn from low-margin online data for already-aligned samples, which provides weak signal and can destabilize learning, underscoring the need for dynamic balancing mechanism. Effects of Meta-learner. We also evaluate the necessity of trainable meta-learner by replacing meta-learner function hϕ with fixed sigmoid-based heuristic (w/o Meta-Learner in Table 2). This heuristic also uses offline preference scores as input but remains static during the meta-learner update phase. Other components, including sampling and loss weighting mechanisms, were kept identical. The results show performance drops across all metrics. For example, AlpacaEval 2 WR decreased from 47.48% to 43.07%, and Arena-Hard WR dropped from 43.9% to 38.3%. Despite using more online annotations (61.1% vs. 58.4%), the fixed heuristic underperformed, confirming the importance of learnable meta-learner for adaptive weighting and effective alignment. More ablation studies on reward model annotations and offline data utilization are provided in Appendix C.2."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper, we propose MetaAPO, novel framework that bridges the gap between data generation and model training in LLM alignment. The core idea is to leverage meta-learner to assess the alignment of the current policy and identify the potential for online exploration. To achieve this, we introduce the techniques of adaptive online sampling, meta-weighted optimization and meta-learner update, which jointly enable dynamic integration of data and training. Experiments across three alignment benchmarks validate the effectiveness of MetaAPO in consistently outperforming existing alignment methods. While our current approach effectively utilizes preference scores as input, it may be beneficial to explore additional input signals, such as gradient or representation-based features, to further enhance flexibility and generalization."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland, Michal Valko, and Daniele Calandriello. general theoretical paradigm to understand learning from In International Conference on Artificial Intelligence and Statistics, pp. human preferences. 44474455. PMLR, 2024. Anirudhan Badrinath, Prabhat Agarwal, and Jiajing Xu. Hybrid preference optimization: Augmenting direct preference optimization with auxiliary objectives. arXiv preprint arXiv:2405.17956, 2024. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. Changyu Chen, Zichen Liu, Chao Du, Tianyu Pang, Qian Liu, Arunesh Sinha, Pradeep Varakantham, and Min Lin. Bootstrapping language models with dpo implicit rewards. arXiv preprint arXiv:2406.09760, 2024. Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Bingxiang He, Wei Zhu, Yuan Ni, Guotong Xie, Ruobing Xie, Yankai Lin, et al. Ultrafeedback: Boosting language models with scaled ai feedback. In International Conference on Machine Learning, pp. 97229744. PMLR, 2024. Nirjhar Das, Souradip Chakraborty, Aldo Pacchiano, and Sayak Ray Chowdhury. Active preference optimization for sample efficient rlhf. arXiv preprint arXiv:2402.10500, 2024. Xun Deng, Han Zhong, Rui Ai, Fuli Feng, Zheng Wang, and Xiangnan He. Less is more: Improving llm alignment via preference data selection. arXiv preprint arXiv:2502.14560, 2025. Bosheng Ding, Chengwei Qin, Ruochen Zhao, Tianze Luo, Xinze Li, Guizhen Chen, Wenhan Xia, Junjie Hu, Luu Anh Tuan, and Shafiq Joty. Data augmentation using llms: Data perspectives, learning paradigms and challenges. In Findings of the Association for Computational Linguistics ACL 2024, pp. 16791705, 2024. Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. Raft: Reward ranked finetuning for generative foundation model alignment. arXiv preprint arXiv:2304.06767, 2023. Qingxiu Dong, Li Dong, Xingxing Zhang, Zhifang Sui, and Furu Wei. Self-boosting large language models with synthetic preference data. arXiv preprint arXiv:2410.06961, 2024. Yann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori Hashimoto. Length-controlled alpacaeval: simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024. Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024. Chengqian Gao, Haonan Li, Liu Liu, Zeke Xie, Peilin Zhao, and Zhiqiang Xu. Principled data selection for alignment: The hidden risks of difficult examples. arXiv preprint arXiv:2502.09650, 2025. Qi Gou and Cam-Tu Nguyen. Mixed preference optimization: Reinforcement learning with data selection and better reference model. arXiv preprint arXiv:2403.19443, 2024. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024."
        },
        {
            "title": "Preprint",
            "content": "Hongyi Guo, Yuanshun Yao, Wei Shen, Jiaheng Wei, Xiaoying Zhang, Zhaoran Wang, and Yang Liu. Human-instruction-free llm self-alignment with limited samples. arXiv preprint arXiv:2401.06785, 2024a. Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, et al. Direct language model alignment from online ai feedback. arXiv preprint arXiv:2402.04792, 2024b. Kexin Huang, Junkang Wu, Ziqian Chen, Xue Wang, Jinyang Gao, Bolin Ding, Jiancan Wu, Xiangnan He, and Xiang Wang. Larger or smaller reward margins to select preferences for alignment? arXiv preprint arXiv:2503.01864, 2025. Kaixuan Ji, Jiafan He, and Quanquan Gu. Reinforcement learning from human feedback with active queries. arXiv preprint arXiv:2402.09401, 2024. Saeed Khaki, JinJin Li, Lan Ma, Liu Yang, and Prathap Ramachandra. Rs-dpo: hybrid rejection sampling and direct preference optimization method for alignment of large language models. arXiv preprint arXiv:2402.10038, 2024. Jongwoo Ko, Saket Dingliwal, Bhavana Ganesh, Sailik Sengupta, Sravan Bodapati, and Aram Galstyan. Sera: Self-reviewing and alignment of large language models using implicit reward margins. arXiv preprint arXiv:2410.09362, 2024. Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph Gonzalez, and Ion Stoica. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. arXiv preprint arXiv:2406.11939, 2024. Biao Liu, Ning Xu, and Xin Geng. Progressively label enhancement for large language model alignment. arXiv preprint arXiv:2408.02599, 2024. Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He. What makes good data for alignment? comprehensive study of automatic data selection in instruction tuning. arXiv preprint arXiv:2312.15685, 2023. Chris Lu, Samuel Holt, Claudio Fanconi, Alex Chan, Jakob Foerster, Mihaela van der Schaar, and Robert Lange. Discovering preference optimization algorithms with and for large language models. Advances in Neural Information Processing Systems, 37:8652886573, 2024. Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with referencefree reward. Advances in Neural Information Processing Systems, 37:124198124235, 2024. Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. MIT press, 2018. William Muldrew, Peter Hayes, Mingtian Zhang, and David Barber. Active preference learning for large language models. In International Conference on Machine Learning, pp. 3657736590. PMLR, 2024. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730 27744, 2022. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize with human feedback. Advances in neural information processing systems, 33:30083021, 2020. Xuerui Su, Yue Wang, Jinhua Zhu, Mingyang Yi, Feng Xu, Zhiming Ma, and Yuting Liu. Reveal the mystery of dpo: The connection between dpo and rl algorithms. arXiv preprint arXiv:2502.03095, 2025."
        },
        {
            "title": "Preprint",
            "content": "Fahim Tajwar, Anikait Singh, Archit Sharma, Rafael Rafailov, Jeff Schneider, Tengyang Xie, Stefano Ermon, Chelsea Finn, and Aviral Kumar. Preference fine-tuning of llms should leverage suboptimal, on-policy data. In International Conference on Machine Learning, pp. 4744147474. PMLR, 2024. Yifan Wang, Runjin Chen, Bolian Li, David Cho, Yihe Deng, Ruqi Zhang, Tianlong Chen, Zhangyang Wang, Ananth Grama, and Junyuan Hong. More is less: The pitfalls of multi-model synthetic preference data in dpo safety alignment. arXiv preprint arXiv:2504.02193, 2025. Yue Wu, Zhiqing Sun, Huizhuo Yuan, Kaixuan Ji, Yiming Yang, and Quanquan Gu. Self-play preference optimization for language model alignment. arXiv preprint arXiv:2405.00675, 2024. Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. Less: Selecting influential data for targeted instruction tuning. arXiv preprint arXiv:2402.04333, 2024. Wei Xiong, Hanze Dong, Chenlu Ye, Ziqi Wang, Han Zhong, Heng Ji, Nan Jiang, and Tong Zhang. Iterative preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint. arXiv preprint arXiv:2312.11456, 2023. Shusheng Xu, Wei Fu, Jiaxuan Gao, Wenjie Ye, Weilin Liu, Zhiyu Mei, Guangju Wang, Chao Yu, and Yi Wu. Is dpo superior to ppo for llm alignment? comprehensive study. In International Conference on Machine Learning, pp. 5498354998. PMLR, 2024. Yuzi Yan, Yibo Miao, Jialian Li, Yipin Zhang, Jian Xie, Zhijie Deng, and Dong Yan. 3d-properties: Identifying challenges in dpo and charting path forward. arXiv preprint arXiv:2406.07327, 2024. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024a. Sen Yang, Leyang Cui, Deng Cai, Xinting Huang, Shuming Shi, and Wai Lam. Not all preference pairs are created equal: recipe for annotation-efficient iterative preference learning. In Findings of the Association for Computational Linguistics: EMNLP 2024, pp. 65496561, 2024b. Xiliang Yang, Feng Jiang, Qianen Zhang, Lei Zhao, and Xiao Li. Dpo-shift: Shifting the distribution of direct preference optimization. arXiv preprint arXiv:2502.07599, 2025. Hongyi Yuan, Zheng Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. Rrhf: Rank responses to align language models with human feedback. Advances in Neural Information Processing Systems, 36:1093510950, 2023. Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models. ArXiv, abs/2401.10020, 2024. Shenao Zhang, Zhihan Liu, Boyi Liu, Yufeng Zhang, Yingxiang Yang, Yongfei Liu, Liyu Chen, Tao Sun, and Zhaoran Wang. Reward-augmented data enhances direct preference alignment of llms. arXiv preprint arXiv:2410.08067, 2024a. Shenao Zhang, Donghan Yu, Hiteshi Sharma, Han Zhong, Zhihan Liu, Ziyi Yang, Shuohang Wang, Hany Hassan, and Zhaoran Wang. Self-exploring language models: Active preference elicitation for online alignment. arXiv preprint arXiv:2405.19332, 2024b. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. Advances in Neural Information Processing Systems, 36:5500655021, 2023."
        },
        {
            "title": "APPENDIX",
            "content": "In the appendix, we provide the following supplementary materials: (A) Details of the formulation derivation, including meta-learner gradient derivation, theorem proof and SimPO objective formulation. (B) Implementation details of the experimental setup, including models and datasets, evaluation benchmarks, baselines, model training and hyperparameters. (C) presents further experimental analyses, including robustness of reward models, our tuning strategy for the meta-learner update interval Tmeta, deeper investigations into the meta-learners behavior with supporting visualizations, and results from an extended ablation study. (D) provides qualitative case studies that illustrate MetaAPOs output quality in comparison to baseline methods."
        },
        {
            "title": "A MATHEMATICAL DERIVATIONS AND FORMULATIONS",
            "content": "A.1 DERIVATION OF THE META-LEARNER LOSS GRADIENT The meta-learner aims to capture the trade-off between offline and online preference and maximize the expected weighted sum of these preference scores. Therefore, its update is guided by minimizing the following loss: Lmeta(ϕ) = EBmeta (cid:104) (cid:105) hϕ(ℓoff()) ℓoff() + (1 hϕ(ℓoff())) ℓon() . (A.1) This expression can be reformulated to more effectively represent the optimal objective hϕ(ℓoff()): Lmeta(ϕ) = EBmeta (cid:105) (cid:104) (cid:0)ℓoff() ℓon()(cid:1) hϕ(ℓoff()) + ℓon() (cid:105) (cid:104) (cid:0)ℓon() ℓoff()(cid:1) hϕ(ℓoff()) ℓon() . =EBmeta Differentiating both sides of Lmeta with respect to the meta-learner parameters ϕ yields: ϕLmeta(ϕ) = EBmeta (cid:105) (cid:104) (cid:0)ℓon() ℓoff()(cid:1) ϕ(hϕ(ℓoff()) . (A.2) (A.3) This gradient governs the meta-learners update. The term (ℓon() ℓoff()) can be seen as an advantage signal, indicating how much better (or worse) the online score is compared to the offline score for the same underlying prompt. Interpretation. When online samples yield higher preference scores than their offline counterparts (i.e., ℓon() > ℓoff()), the factor (ℓon() ℓoff()) is positive. The gradient update will then tend to adjust ϕ such that hϕ(ℓoff()) (the weight assigned to the offline sample) decreases. This encourages more reliance on online exploration and data for instances where it proves more beneficial. Conversely, when offline samples yield scores that are comparable to or better than online samples (i.e., ℓoff() ℓon()), the factor ℓon() ℓoff() is zero or negative. This leads to updates that preserve or increase the weight hϕ(ℓoff()), thereby promoting stable exploitation of reliable, curated offline data. A.2 PROOF OF THEOREM 1 We provide complete proof of Theorem 1. First, we establish uniform convergence bound between the empirical and true risk of the meta-learner, using the Rademacher complexity of the meta-loss hypothesis class. Then, we decompose the excess risk of the learned meta-learner relative to the oracle into terms that can be bounded by the uniform convergence result. Finally, we obtain high-probability generalization guarantee for ˆhϕ as stated in the theorem. A.2.1 PRELIMINARIES Definition 1 (Meta-Loss Function). For meta-sample (x, yoff meta-loss is defined as: pair, yon pair) and meta-learner hϕ, the pair, yon where is an upper bound on the loss. The meta-loss is always non-negative and does not exceed for any samples (x, yoff pair) [0, ]. Lmeta(hϕ; x, yoff pair, yon pair)."
        },
        {
            "title": "Preprint",
            "content": "Definition 2 (True Risk and Empirical Risk). Let be the distribution over meta-samples, and Bmeta = {(xi, yoff i=1 meta-buffer of size m. The true and empirical risk of hϕ are defined as: , yon )}m R(hϕ) = (x,yoff pair,yon pair)D[Lmeta(hϕ)], (cid:98)Rm(hϕ) = 1 (cid:88) i=1 Lmeta(hϕ; xi, yoff , yon ). Definition 3 (Rademacher Complexity). For function class F, the empirical Rademacher complexity with respect to the meta-buffer Bmeta is: (cid:100)Radm(F) = Eσ (cid:104) sup 1 (cid:88) i=1 σif (xi, yoff , yon ) (cid:12) (cid:12) (cid:12) Bmeta (cid:105) , where σi are i.i.d. Rademacher variables. Its expectation over Bmeta defines Radm(F). This quantity captures the capacity/complexity of the meta-loss hypothesis class and is standard tool in statistical learning theory (Mohri et al., 2018). A.2.2 PROOF OF THEOREM 1 Step 1: Uniform Convergence. By standard results in statistical learning theory (Mohri et al., 2018), for any function class bounded in [0, ], with probability at least 1 δ over the choice of the meta-buffer Bmeta: (cid:12)E[f ] (cid:98)Em[f ](cid:12) (cid:12) (cid:12) 2 Radm(F) + (cid:113) ln(1/δ) 2m . sup (A.4) Step 2: Excess Risk Decomposition. Let = suphϕH R(hϕ) (cid:98)Rm(hϕ). For the empirical minimizer ˆhϕ = arg minhϕH (cid:98)Rm(hϕ) and the oracle = arg minhϕH R(hϕ), we have R(ˆhϕ) R(h) = R(ˆhϕ) (cid:98)Rm(ˆhϕ) (cid:125) (cid:124) (cid:123)(cid:122) + (cid:98)Rm(ˆhϕ) (cid:98)Rm(h) (cid:123)(cid:122) (cid:125) 0 (cid:124) + (cid:98)Rm(h) R(h) (cid:123)(cid:122) (cid:125) (cid:124) (A.5) 2 sup hϕH R(hϕ) (cid:98)Rm(hϕ). Step 3: Substituting the Uniform Bound. Applying (A.4) to the meta-loss class Lmeta(ϕ) H, with probability at least 1 δ: sup hϕH R(hϕ) (cid:98)Rm(hϕ) 2 Radm(Lmeta(ϕ) H) + (cid:114) ln(1/δ) 2m . Combining with (A.5) yields the desired bound: R(ˆhϕ) R(h) + 4 Radm(Lmeta(ϕ) H) + (cid:114) 2 ln(1/δ) . (A.6) (A.7) This completes the proof. A.3 METAAPO WITH SIMPO OBJECTIVE The MetaAPO framework is designed to be largely agnostic to the choice of the underlying preference optimization loss, enabling it to be applied in conjunction with various alignment algorithms. To showcase this flexibility, we demonstrate its integration with Simple Preference Optimization (SimPO) (Meng et al., 2024), reference-free alignment method. This integration results in the MetaAPO-SimPO variant. The original objective of SimPO involves minimizing the following loss function: LSimPO(πθ) = E(x,yw,yl)D (cid:20) log σ (cid:18) β yw log πθ(ywx) β yl log πθ(ylx) γ . (A.8) (cid:19)(cid:21)"
        },
        {
            "title": "Preprint",
            "content": "From this formulation, the corresponding instance-level preference score ℓ(x, yoff ) in SimPO, used to quantify how well the current policy πθ aligns with the given preference pair (yw, yl) for prompt x, is defined as the term whose negative log-likelihood is minimized in the SimPO objective: , yoff ℓ(x, yw, yl) = log σ (cid:18) β yw log πθ(ywx) β yl log πθ(ylx) γ . (A.9) (cid:19) When applying MetaAPO in conjunction with SimPO (i.e., MetaAPO-SimPO), this SimPO-specific preference score (as shown in Eq. A.9) is used in place of the DPO-based preference score within the MetaAPO framework."
        },
        {
            "title": "B EXPERIMENTAL SETUP",
            "content": "B.1 MODELS AND DATASET For the SFT models used in training, we adopt existing open-source models that have been supervised fine-tuned, specifically: allenai/Llama-3.1-Tulu-3-8B-SFT based on the Llama architecture, and FuseAI/FuseChat-Qwen-2.5-7B-SFT based on the Qwen architecture. All methods are trained on UltraFeedback-Binarized dataset, which contains 64K preference data. Reward models. For the external reward model annotation, we trained the reward model for one epoch on the UltraFeedback-Binarized dataset from the SFT model corresponding to the policy model. This procedure ensures that any online improvement does not stem from potential leakage of the reward model, thus providing fully fair comparison between online and offline methods. We also report experimental results using other open-source reward models in the Appendix. Meta-Learner architecture. The meta-learner hϕ is compact two-layer MLP that maps the scalar offline preference score ℓoff() to weight [0, 1]. It uses 100-dimensional hidden layer with Tanh activation, followed by Sigmoid output. Due to its small size and simple input, the meta-learner adds negligible overhead in both prediction and training. B.2 EVALUATION BENCHMARKS We evaluate our models on three representative alignment benchmarks: AlpacaEval 2(Dubois et al., 2024), MT-Bench(Zheng et al., 2023), and Arena-Hard (Li et al., 2024). These benchmarks are widely adopted for their comprehensive coverage of diverse tasks and their effectiveness in assessing the subjective preference capabilities of LLMs. AlpacaEval 2 includes 805 instructions from five different datasets and assesses models using two metrics: length-controlled (LC) win rate and raw win rate (WR). In this benchmark, GPT4o-2024-05-13 serves as the baseline model and GPT-4o-latest as the evaluator for the other models. Arena-Hard is more challenging benchmark that closely aligns with the human preference ranking from Chatbot Arena, crowd-sourced platform for evaluating LLMs. It spans 250 high-quality topic clusters, including 500 well-defined technical problem-solving queries. We report the Win Rate against GPT-4-0314 using GPT-4o as the judge model. MT-Bench contains 80 multi-turn dialogues with 160 questions across eight categories, including writing, roleplay, reasoning, math, coding, extraction, STEM, and humanities. Each response is evaluated by GPT-4o on scale from 1 to 10, with the average score reported for each dialogue turn across the 80 dialogues. Different from the official setting, we follow the latest works to adopt GPT-4o as the evaluator and baseline. B.3 BASELINES. To evaluate the effectiveness of MetaAPO, we compare it against range of SOTA baseline methods from current LLM alignment literature. These baselines include purely offline approaches, methods that enhance offline data processing, techniques incorporating online data generation, and hybrid strategies:"
        },
        {
            "title": "Preprint",
            "content": "Offline Preference Alignment: These methods, such as DPO (Rafailov et al., 2023), SimPO (Meng et al., 2024), and IPO (Azar et al., 2024), directly learn from fixed, pre-collected preference dataset, typically by optimizing log-likelihood objective. Selective DPO (Gao et al., 2025): This approach aims to improve data quality by splitting the dataset into training and validation sets. It calculates the average loss of all samples on the validation set and then filters out samples presumed to be low-quality (those with higher validation loss) to curate sub-dataset for model training. Online DPO (Xu et al., 2024): Online DPO follows an iterative process of data generation and preference optimization. In each iteration, the current model generates multiple responses for given prompts. These responses are subsequently evaluated by an external reward model to identify the best (chosen, yon ) options, forming new preference pairs. The policy optimization then exclusively utilizes these self-generated preference pairs. Then the policy model and reference model are updated and continue next iteration. ) and worst (rejected, yon ADPO (Ji et al., 2024): Active DPO (ADPO) heuristically selects offline samples for online augmentation if their preference loss falls below fixed threshold. New online preference pairs are generated for these selected prompts. These newly created online pairs are then used in conjunction with the original offline samples for preference optimization via standard DPO algorithm. SELM (Zhang et al., 2024b): Self-Exploring Language Model (SELM) operates with an iterative generation and annotation cycle similar to Online DPO. However, when ranking the multiple responses generated by the current policy (using reward model), SELM distinctively includes the original chosen yoff responses from the offline dataset (for the same prompt) within the ranking pool. This method allows for more integrated utilization of both offline and newly generated online data to construct the training preference pairs. and rejected yon PPO (Ouyang et al., 2022): Proximal Policy Optimization (PPO) is foundational online alignment method from Reinforcement Learning from Human Feedback. Unlike direct preference methods, PPO first trains separate reward model on human preference dataset. Subsequently, the policy model generates responses to prompts, and the RM scores these responses to provide reward signal. PPO then uses this reward to update the policy and critic model, aiming to maximize the expected reward while KL-divergence penalty keeps the policy from straying too far from reference model. B.4 MODEL TRAINING In our main experiments, we integrate MetaAPO into the iterative alignment framework (Ouyang et al., 2022; Xu et al., 2024; Dong et al., 2024), which consists of two stages: data generation and preference optimization. To obtain the policy and reference models, we first warm up the SFT model on the UltraFeedback-Binarized dataset using the DPO loss. The warmed-up model serves as the initial policy model and the SFT model is used as the reference model for MetaAPO training. For fair comparison, all models are trained for 1 epoch over the dataset and run for 3 alignment iterations. During each generation phase, we first employ the meta-learner to assign sampling probabilities for the offline data. Based on these probabilities, subset of offline preference data is sampled. For each sampled prompt, the current policy model generates = 8 candidate responses. An external reward model is then used to annotate scores for each promptresponse pair, from which we select the highest and lowest scoring responses as the chosen and rejected samples, respectively. In the optimization phase, we jointly leverage both the original offline preference pairs and the newly generated online preference pairs for each augmented instance, forming the dataset Daug. These pairs are integrated through our meta-weighting mechanism, as defined in the preference optimization objective (Eq. 5). Specifically, for each instance (x, yoff pair) Daug, the meta-learner computes dynamic weight = hϕ(ℓoff(x, yoff )), which determines the relative importance of the offline component ℓθ(x, yoff ) and the online component ℓθ(x, yon ) in the total loss. pair, yon , yoff , yoff , yon The policy model πθ is then updated for one epoch over the weighted dataset Daug. Concurrently, the meta-learner hϕ is updated every Tmeta steps using its dedicated meta-objective (Eq. 6), based on the data stored in the meta buffer Bmeta, as described in Algorithm 1."
        },
        {
            "title": "Preprint",
            "content": "Table 3: Hyperparameters for Llama-3.1-8B and Qwen2.5-7B during generation and training. Hyperparameters Notation Llama-3.1-8B Qwen2.5-7B Temperature Top-p Rollout nums Max new token"
        },
        {
            "title": "Generation",
            "content": "- - Lnew"
        },
        {
            "title": "Training",
            "content": "Learning rate Meta-learner lr Batch size Max prompt length Max generation length Training max length Reward model max length KL loss Target reward margin Update Interval Epochs α η Lprompt Lgen Ltrain Lreward β γ Tmeta - 1 1 0.8 0.95 8 2048 5e-7 5e-3 128 2048 2048 4096 4096 0.1 (2.5 for SimPO) 0.6 8 B.5 HYPERPARAMETERS We list the key hyperparameters used in our experiments for both Llama-3.1-8B and Qwen2.5-7B in Table 3. The generation settings (e.g., temperature, top-p, and number of responses per prompt) are adjusted to suit each models characteristics, while training-related parameters such as learning rates, batch size, and sequence length are kept consistent to ensure fair comparison. For MetaAPO, we adopt moderate meta-learner update interval and use shared weighting hyperparameters across all models and baselines. For PPO training, we set the rollout batch size to 1024 to ensure small distribution shift. All other hyperparameters, such as the batch size and learning rate, are kept consistent with those used in the other methods to enable fair comparison. Evaluation. We follow the generation protocols specified by each benchmark. The maximum generation length is set to 4096, while the temperature is determined according to the default configuration of the respective benchmarks."
        },
        {
            "title": "C ADDITIONAL EXPERIMENTAL RESULTS AND ANALYSES",
            "content": "In this section, we provide extended experiments and analyses to further validate MetaAPO. (C.1) examines its impact on general downstream tasks to ensure no degradation of broad capabilities. (C.2) test robustness under different reward models and datasets. (C.4) analyzes the effect of the meta-learner update interval and its dynamic behavior during training. Finally, we conduct extended ablation studies (C.5) and report time efficiency comparisons (C.6). C.1 PERFORMANCE ON GENERAL TASKS We evaluated the aligned models performance on other tasks involving mathematics, logical reasoning, general knowledge and instruction following. To ensure the reliability of our results, we followed the standard evaluation setup provided by the LM-Evaluation-Harness, where the number in parentheses denotes the few-shot examples provided for each task. The results for Llama-3.1-8B are"
        },
        {
            "title": "Preprint",
            "content": "Table 4: We evaluated the performance of the Llama-3.1-8B aligned models on tasks related to mathematics, logical reasoning, general knowledge, et al. The numbers in parentheses indicate the few-shot examples given for each task, and the results are presented in the table. Models GSM8K(8) MMLU(5) ARC(25) EQBench(0) TruthfulQA(0) IFEval(0) Average SFT DPO Online DPO SELM MetaAPO 72.71 74.22 76. 76.33 77.18 64.81 64.98 64.93 65. 65.15 57.59 59.39 60.24 61.01 61. 65.51 67.58 63.27 63.92 67.27 46. 48.62 54.40 55.21 55.47 70.98 70. 63.40 61.74 68.21 63.08 64.17 63. 63.87 65.79 Table 5: Overall performance of our proposed MetaAPO method with Llama-3.1-8B and Qwen2.57B, compared with offline and online baseline methods on AlpacaEval 2, Arena-Hard and MT-Bench with the reward model OpenRLHF/Llama-3-8b-rm-mixture. The best and second-best results are highlighted in bold and underline. Llama-3.1-8B Qwen2.5-7B Models AlpacaEval 2 Arena-Hard MT-Bench AlpacaEval 2 Arena-Hard MT-Bench WR(%) LC(%) WR(%) SC(%) Score WR(%) LC(%) WR(%) SC(%) Score Online & Hybrid Alignment ADPO Online DPO PPO SELM 18.24 48.11 50.07 48.61 MetaAPO-DPO 50. MetaAPO-SimPO 49.11 18.32 40.09 48.95 36.55 50. 50.56 32.0 48.7 54.1 50.4 55. 52.5 31.9 32.0 43.5 37.0 43. 41.4 6.64 7.37 7.43 7.60 MetaAPO 44. 50.80 48.11 51.33 38.90 50.45 42. 50.99 7.57 7.66 52.62 51.06 52. 51.34 61.9 65.5 65.2 65.7 67. 66.5 53.2 54.1 56.2 52.3 57. 58.6 7.97 8.00 7.78 7.60 7. 7.96 shown in the table, indicating that MetaAPOs alignment does not lead to significant performance degradation on general tasks. C.2 EXPERIMENTS ON OTHER REWARD MODELS AND DATASETS. We further evaluate MetaAPO under different reward models to examine its robustness and generalizability. Specifically, we adopt an open-source reward model, OpenRLHF/Llama-3-8b-rm-mixture, which is trained on preference data disjoint from our alignment datasets. As shown in Table 5, MetaAPO consistently outperforms both offline and online baseline methods across AlpacaEval 2, Arena-Hard, and MT-Bench. We note that online and hybrid methods inherently depend on the accuracy and preference biases of the reward model, which can introduce variability in performance. Nevertheless, under the same reward model, our approach achieves stable and superior improvements, demonstrating the general applicability and robustness of MetaAPO. We also evaluated Llama-3.1-8B trained on the Argilla/DPO-Mix-7k dataset, which consistently demonstrates the stability and effectiveness of MetaAPO across different data sources. C.3 TUNING STRATEGIES FOR META-LEARNER UPDATE INTERVAL TMETA In MetaAPO, we introduce meta-learner update interval Tmeta to accumulate experiences in the meta buffer Bmeta for more stable and generalizable updates to hϕ. We investigate how varying Tmeta affects alignment performance. As shown in Figure 3 (Left), the choice of Tmeta significantly impacts performance on AlpacaEval 2 Win Rate (WR) and Length-Controlled win rate (LC). When Tmeta is small (e.g., 1 or 2), Bmeta contains limited training data, leading to unstable or suboptimal updates of hϕ and degraded alignment performance. As Tmeta gradually increases (e.g., to 4 and 8), the meta-learner benefits from more"
        },
        {
            "title": "Preprint",
            "content": "Table 6: Performance comparison of different alignment methods on Argilla/DPO-Mix-7k dataset with Llama-3.1-8B. Model AlpacaEval2 WR (%) AlpacaEval2 LR (%) Arena-Hard WR (%) Arena-Hard SC (%) SFT Online DPO SELM PPO MetaAPO 10.02 26.10 27.66 27. 28.56 17.28 25.43 25.91 25.01 27. 18.5 27.3 28.1 28.7 30.4 21. 32.6 31.5 31.5 34.0 Figure 3: Left: Impact of meta-Learner update interval (Tmeta) on MetaAPO performance. Performance (AlpacaEval 2 WR/LC (%)) is shown for different Tmeta values. Right: Meta-Learner input-output relationship across training iterations. comprehensive set of recent experiences stored in the buffer. This richer information allows for more stable and generalizable updates to hϕ, leading to consistent improvement in alignment performance. The results indicate that performance tends to plateau or stabilize for Tmeta 8. Balancing responsiveness and stability, we set Tmeta = 8 in our main experiments. This observation is consistent with Theorem 1, which guarantees improved generalization when the meta-learner is updated with sufficiently rich set of meta-samples. C.4 FURTHER ANALYSIS OF META-LEARNER BEHAVIOR To understand how the meta-learner hϕ adapts during training, we visualize its input-output relationship over time (Figure 3, Right). The input is the offline preference score ℓoff(), and the output is the weight w, which determines the likelihood of selecting sample for online generation (lower higher sampling probability). At initialization (gray curve), the output resembles scaled sigmoid: highly negative scores (indicating misalignment) receive low weights, encouraging augmentation. Positive scores receive moderate weights (about 0.5), allowing some exploration of well-aligned samples. After the first iteration (blue dashed), the meta-learner becomes more selective: weights for moderately negative and near-zero scores drop further, increasing focus on less-aligned samples. Weights for positive scores start to decrease, reducing redundant augmentation. This trend continues (red and green curves): weights for well-aligned samples steadily decline, while those for poorly aligned ones remain low. By iteration 3, the function stabilizes, showing convergence to an effective sampling policy. Overall, the meta-learner dynamically adjusts its strategy based on policy alignment, focusing augmentation where it is most needed and avoiding overfitting to already aligned data. C.5 EXTENDED ABLATION STUDY We conduct additional ablation studies to further examine two key components of MetaAPO: (i) the effect of its selective utilization strategy for offline samples, and (ii) the influence of the meta-learners network complexity along with the incorporation of richer preference information. All experiments"
        },
        {
            "title": "Preprint",
            "content": "Table 7: Further ablation study on key components of MetaAPO. We assess the impact of the offline samples and the influence of the meta-learners network and input features. Metrics include AlpacaEval2, Arena-Hard, MT-Bench and the proportion of annotations used during training. Model AlpacaEval 2 Arena-Hard WR(%) LC(%) WR(%) SC(%) MT-Bench Annotations Ratio(%) Score MetaAPO w/ All Offline Samples w/ 5-layers Meta-Learner w/ Multi-Feature Input 50.19 50.32 47.28 49. 50.37 38.40 45.62 49.17 55.2 50. 46.0 53.1 43.6 40.1 39.7 41. 7.57 7.23 7.41 7.56 59.4 62. 63.0 61.4 are carried out under the Llama-3.1-8B setting with the DPO objective, using the reward model OpenRLHF/Llama-3-8b-rm-mixture. The results are summarized in Table 7. Impact of Utilizing All Offline Data. In the standard MetaAPO setup, the augmented dataset Daug comprises tuples formed by pairing selected offline preference samples with their corresponding online counterparts. Only those offline samples identified by the meta-learner as beneficial for augmentation are included. The remaining offline samples, those not selected for online augmentation, are excluded from the policy training phase. To evaluate the effect of this selective inclusion, we tested variant (denoted w/ All Offline Samples) in which the training set was expanded to include all original offline samples. In this setting, augmented pairs in Daug are still weighted by and 1 as usual, while the additional unaugmented offline samples are incorporated with fixed weight of 1. As shown in Table 7, this broader inclusion leads to notable drop in overall performance compared to standard MetaAPO. The performance degrades on AlpacaEval 2 LC, Arena-Hard WR and MT-Bench score. These results suggest that MetaAPOs selective augmentation strategy is more effective, likely because the policy is already sufficiently trained on the offline data via DPO warm up. Consequently, the meta-learner can effectively identify samples that are either well-aligned or less informative, thus excluding them from redundant retraining. Including these may dilute the effect of the more informative online augmentations or risk overfitting to noise and less impactful preferences. The Sufficiency of Meta-Learner. To further investigate the capacity of the two-layer MLP, we experimented with deeper 5-layer meta-learner. As shown in Table 7, the two-layer network is sufficient to map scalar preference scores to effective weights. Given the simplicity of the input and output, increasing model complexity introduces higher risk of overfitting without noticeable performance gains. This observation is consistent with Theorem 1, which guarantees improved generalization when the meta-learner with low complexity. πref(ywx) , and log πθ (ylx) In addition, we investigate more flexible input setting, where the meta-learner is provided with ℓoff(), log πθ (ywx) πref(ylx) as inputs, denoted as Multi-Feature Input. As shown in Table 7, incorporating these richer inputs does not lead to further improvements. possible explanation is that, as discussed in Section 5.3, there exists clear mapping between the offline score and the potential gain. The single offline score input already captures this relationship, whereas introducing additional inputs may require more complex learning mechanism to effectively exploit the mapping. C.6 TIME CONSUMPTION Despite introducing an additional meta-learner module, MetaAPO remains highly efficient thanks to its simple structure and scalar input, making both its training and inference extremely lightweight. As shown in Table 8, we compare MetaAPOs time cost against strong online baselines: Online DPO and SELM. In the online sampling phase, MetaAPO completes in just 106 minutes on 8H100 GPUs, more than 3 faster than Online DPO (337m) and SELM (363m). This substantial speedup stems from MetaAPOs adaptive sampling, which selectively augments only subset of offline samplesdrastically reducing the number of costly response generations and reward model queries. In the training phase, despite the added meta-learner and joint optimization over both offline and online data, MetaAPOs training time (59m) remains comparable to Online DPO (58m) and SELM"
        },
        {
            "title": "Preprint",
            "content": "Table 8: Comparison of time consumption in minutes for the sampling phase, training phase and overall between MetaAPO, Online DPO and SELM. Method Sampling Time(m) Training(m) Overall(m) Annotation Ratio(%)"
        },
        {
            "title": "Online DPO\nSELM\nPPO\nMetaAPO",
            "content": "337 363 - 106 58 60 - 59 395 423 936 186 100 125 100 58 (60m), since fewer preference pairs are used. Overall, MetaAPO completes the entire process in just 186 minutes, less than half the time required by Online DPO (395m) and SELM (423m). Compared to PPO, MetaAPO reduces the overall training time by 80.1%, primarily due to eliminating the costly sampling and annotation process as well as the additional critic model training. This highlights MetaAPOs scalability and efficiency in achieving high-quality alignment."
        },
        {
            "title": "D CASE STUDY",
            "content": "We further demonstrate the effectiveness of MetaAPO through qualitative examples from AlpacaEval 2, challenging benchmark for subjective instruction-following evaluation. We compare responses generated by MetaAPO with those from WPO, Online DPO, and SELM. As shown in Figure 4, when answering concise factual question, WPO tends to over-provide information in lengthy and rigid manner. In contrast, MetaAPO delivers more concise and direct response, better aligning with human conversational preferences. In Figure 5, SELM fails to identify and explain all the errors in the input transcription. MetaAPO, however, demonstrates more comprehensive and helpful error correction. Lastly, in Figure 6, MetaAPO produces more natural, engaging, and contextaware response than Online DPO, showcasing stronger interactivity and scenario alignment. Overall, these examples illustrate that MetaAPO is capable of generating more helpful, user-friendly, and human-aligned responses, highlighting its effectiveness in instruction alignment tasks."
        },
        {
            "title": "Example",
            "content": "Question: How did US states get their names? WPO: The names of US states often have historical, geographical, or cultural origins that reflect the regions history, the indigenous peoples names, explorers, early settlers, or significant events. Heres look at the origins of some state names: 1. Alabama - The name is believed to derive from the Alabama tribe, which is part of the larger Muscogee (Creek) Nation. The tribes name is thought to mean \"Here we rest\" or similar sentiment in their language. 2. Alaska - The name \"Alaska\" is derived from the Aleut word \"Alyeska,\" meaning \"the great land\" or \"mainland.\" This reflects the vast and diverse geography of the state. 3. Arizona - This name comes from the Spanish name \"Ari-zona,\" which means \"place of the Arizona\" or \"the good oak land.\" Arizona refers to desert valley located in the northwestern part of the state. 4. Arkansas - The origin of the name \"Arkansas\" is bit murky, but it is often related to the Quapaw word \"akakaze,\" meaning \"land of the downriver people,\" referring to the people who just lived along the Arkansas River. 5. California - The name \"California\" has romantic and mythical origin. It was popularized by 1510 romance novel \"Las sergas de Esplandián\" (The Adventures of Esplandián) by Garci Rodríguez de Montalvo. The novel featured an island called California, inhabited by Queen Calafia and her Amazons, where the queens realm was rich in gold and diamonds. ... 6. Many states also have additional names or nicknames that capture their essence or history, such as \"The Peach State\" for Georgia, \"The Evergreen State\" for Washington, and \"The Keystone State\" for Pennsylvania. The naming of US states is fascinating blend of indigenous influences, European exploration, colonial history, and the diverse cultures that have shaped America over centuries. MetaAPO: The names of US states often have historical, geographical, or Native American origins. For example, \"Florida\" comes from Spanish term meaning \"flowery,\" reflecting the states abundant flora. \"California\" is thought to be named after fictional island described in Spanish novel. Each states name has its own unique story, reflecting its history and characteristics. Please note that this is simplified overview and the details can be quite complex. If youre interested in the history behind specific states name, could provide more detailed information. Figure 4: concise alignment example."
        },
        {
            "title": "Example",
            "content": "Question: Correct the transcription of an excerpt containing errors. \"I got got charged interest on ly credit card but paid my pull balance one day due date. not missed pavement year yet. Man you reverse the interest charge?\" SELM: Certainly! Heres the corrected transcription of the given excerpt: \"I got charged interest on my credit card, but paid my full balance on the due date. havent missed payment yet. Man, you reverse the interest charge?\" Changes made: 1. Removed extra \"got\" in \"got got charged.\" 2. Corrected \"pull\" to \"full\" in \"my pull balance.\" 3. Changed \"pavement\" to \"payment\" in \"I not missed pavement year yet.\" 4. Replaced \"Man you\" with \"Man, you\" for proper punctuation and clarity. MetaAPO: Certainly! Below is corrected version of the excerpt. \"I got charged interest on my credit card, but paid my full balance one day before the due date. havent missed payment yet. Man, can you reverse the interest charge?\" Corrections applied: 1. \"got got\" was corrected to \"got\" 2. \"on ly\" was corrected to \"on my\" 3. \"pull balance\" was corrected to \"full balance\" 4. \"pavement\" was corrected to \"payment\" 5. \"Man you\" was corrected to \"Man, can you\" 6. \"reverse the interest charge?\" was maintained as is, but the ellipsis after \"charge?\" was removed for clarity. This transcription now reads more naturally and clearly conveys the speakers message. Figure 5: comprehensive alignment example. Example Question: Hello there Obi One Kenobi Online DPO: Greetings, friend. It seems you are addressing me by the name of the esteemed Jedi Master Obi-Wan Kenobi from the Star Wars universe. While am not the Jedi Master himself, am programmed to engage with you as if were. How can assist you today? Are you seeking information about the Star Wars galaxy, the Jedi Order, or perhaps discussing philosophical or ethical dilemmas, as Obi-Wan Kenobi often did? Please, feel free to ask your questions or share your thoughts on any topic youd like to explore. Whether its the wisdom of the Force, the intricacies of galactic politics, or the lore behind the saga, Im here to help expand your knowledge or engage in thoughtful conversation. MetaAPO: Hello there! Its not often get to interact with Jedi Knight like yourself. If youre looking for assistance or have questions on variety of topics, feel free to ask. Whether its information on star systems, the Force, galactic politics, or perhaps strategy for overcoming Sith threat, Im here to help. If youre not in the middle of mission and just curious about general knowledge, can provide insights on wide range of subjects, from the history of the Jedi Order to the intricacies of hyperdrive technology, or even just share amusing anecdotes from the galaxy far, far away. Remember, the Force flows through us and binds us all together. Lets make sure its for the good of the galaxy. How may assist you today? Figure 6: An engaging alignment example."
        }
    ],
    "affiliations": [
        "School of Computer Science and Engineering Southeast University Nanjing, China"
    ]
}