{
    "paper_title": "Feedback-Driven Tool-Use Improvements in Large Language Models via Automated Build Environments",
    "authors": [
        "Junjie Ye",
        "Changhao Jiang",
        "Zhengyin Du",
        "Yufei Xu",
        "Xuesong Yao",
        "Zhiheng Xi",
        "Xiaoran Fan",
        "Qi Zhang",
        "Xuanjing Huang",
        "Jiecao Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Effective tool use is essential for large language models (LLMs) to interact meaningfully with their environment. However, progress is limited by the lack of efficient reinforcement learning (RL) frameworks specifically designed for tool use, due to challenges in constructing stable training environments and designing verifiable reward mechanisms. To address this, we propose an automated environment construction pipeline, incorporating scenario decomposition, document generation, function integration, complexity scaling, and localized deployment. This enables the creation of high-quality training environments that provide detailed and measurable feedback without relying on external tools. Additionally, we introduce a verifiable reward mechanism that evaluates both the precision of tool use and the completeness of task execution. When combined with trajectory data collected from the constructed environments, this mechanism integrates seamlessly with standard RL algorithms to facilitate feedback-driven model training. Experiments on LLMs of varying scales demonstrate that our approach significantly enhances the models' tool-use performance without degrading their general capabilities, regardless of inference modes or training algorithms. Our analysis suggests that these gains result from improved context understanding and reasoning, driven by updates to the lower-layer MLP parameters in models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 1 1 9 7 8 0 . 8 0 5 2 : r Feedback-Driven Tool-Use Improvements in Large Language Models via Automated Build Environments Junjie Ye1,2,, Changhao Jiang1, Zhengyin Du2, Yufei Xu2, Xuesong Yao2, Zhiheng Xi1, Xiaoran Fan1, Qi Zhang1, Xuanjing Huang1, Jiecao Chen 1Fudan University, 2ByteDance Seed Work done at ByteDance Seed"
        },
        {
            "title": "Abstract",
            "content": "Effective tool use is essential for large language models (LLMs) to interact meaningfully with their environment. However, progress is limited by the lack of efficient reinforcement learning (RL) frameworks specifically designed for tool use, due to challenges in constructing stable training environments and designing verifiable reward mechanisms. To address this, we propose an automated environment construction pipeline, incorporating scenario decomposition, document generation, function integration, complexity scaling, and localized deployment. This enables the creation of high-quality training environments that provide detailed and measurable feedback without relying on external tools. Additionally, we introduce verifiable reward mechanism that evaluates both the precision of tool use and the completeness of task execution. When combined with trajectory data collected from the constructed environments, this mechanism integrates seamlessly with standard RL algorithms to facilitate feedback-driven model training. Experiments on LLMs of varying scales demonstrate that our approach significantly enhances the models tool-use performance without degrading their general capabilities, regardless of inference modes or training algorithms. Our analysis suggests that these gains result from improved context understanding and reasoning, driven by updates to the lower-layer MLP parameters in models. Correspondence: Junjie Ye at jjye23@m.fudan.edu.cn Code & Data: https://github.com/bytedance/FTRL"
        },
        {
            "title": "Introduction",
            "content": "Tool use in large language models (LLMs) [2, 17, 23] refers to their ability to interact with the external world by invoking tools to retrieve information [30] or respond to environmental stimuli [40, 45]. Consequently, the effectiveness of an LLMs tool-use capabilities significantly impacts its performance on complex real-world tasks [20] and represents key step toward achieving general-purpose AI [34]. Recent research has focused on improving LLM tooluse capabilities through post-training methods [22]. common approach involves fine-tuning opensource models on interaction trajectories generated by proprietary LLMs with diverse APIs [24, 30]. To increase the diversity and quality of these trajectories, various techniques have been proposed, including Monte Carlo sampling [47], graph-based inference [15], and other optimization strategies [49]. In parallel, reinforcement learning (RL) methods have been explored to improve adaptability and decisionmaking through active environment interaction [19, 42, 46]. However, current RL-based frameworks for training tool-use face significant limitations that hinder the 1 form feedback-driven framework for training LLMs in robust and effective tool use. We validate our approach through extensive experiments on LLMs of varying sizes. Results show that our method consistently improves tool-use performance across four distinct benchmarks, while preserving the models general capabilities, regardless of the model architecture, inference patterns, or training algorithms employed. parameter-level analysis suggests that these gains are largely driven by updates in lower-layer MLP parameters, which enhance the models contextual understanding and decision-making. In summary, our main contributions are: We propose an automated environment construction pipeline that ensures scalable, stable, and verifiable tool-use training. We introduce verifiable reward mechanism that jointly evaluates precision and completeness, compatible with preference-based optimization strategies. We conduct extensive experiments demonstrating substantial improvements in tool use, supported by parameter-level analysis."
        },
        {
            "title": "2 Related Work",
            "content": "Methods for Designing Tool-Use Environments The construction of diverse tool-use environments is both necessary and critical for advancing research in this area. Early studies primarily focused on limited set of predefined tools, for which large volumes of data were either manually curated or synthetically generated to support research within narrowly defined scenarios [14, 24, 37]. To support more diverse tool-use tasks, later efforts collected various existing APIs from the web and manually organized them into large toolsets. Synthetic user inputs were then generated to evaluate the generalization capabilities of LLMs [21, 28, 45, 49]. However, these approaches remain constrained by the quality and coverage of the collected toolsets and often suffer from issues such as unstable execution environments and unreliable feedback. To address these limitations, Ye et al. [43] introduced controllable method for constructing multi-hop tool-use evaluation environments based on existing multi-hop question-answering datasets. While this represented progress, their approach was restricted to the multi-hop scenario and focused solely on building test data. In contrast, we propose robust automated framework for constructing Figure 1 Illustrative examples of four scenarios, categorized by varying sub-question pattern combinations. development of robust models. One major challenge is the difficulty in constructing stable training environments. These frameworks typically rely on large set of online tools, which are prone to service disruptions due to factors like API rate limits [28, 48]. Additionally, standardizing the organization and deployment of these tools imposes high infrastructure costs [21]. Another key limitation is the lack of verifiable reward signals. The complexity of tool interactions and the diversity of valid action trajectories often necessitate the use of high-level LLMs for evaluation, which introduces model bias and undermines both training efficiency and algorithmic stability [19, 46]. To address these limitations, we propose an automated pipeline for generating large number of high-quality tool-use training environments. This pipeline includes scenario decomposition, document generation, function integration, complexity scaling, and localized deployment. By executing all tools locally as code, it enables the dynamic creation of diverse, stable environments while ensuring controlled free from reliance on external online feedback, toolsets. Furthermore, each environment is built around well-defined set of target sub-questions, allowing for precise evaluation of model behavior. Building on this environment, we introduce verifiable reward mechanism that evaluates the precision of tool use and the completeness of task execution solely by analyzing feedback from the environment. When combined with interaction trajectory data collected, this mechanism can be seamlessly integrated into any preference-based optimization method. Together, these components Figure 2 Overview of our proposed approach. The automated environment construction follows five-stage pipeline to generate diverse tool-use training environments. Feedback-driven model training then collects data within these environments, incorporates verifiable reward mechanisms, and optimizes performance using preference-based RL algorithms. tool-use environments. Our method supports the generation of diverse tool-use scenarios and enables both effective training and evaluation, thereby providing more comprehensive foundation for research in this domain. Techniques for Enhancing Tool-Use Abilities To improve the tool-use capabilities of LLMs, early research typically relied on limited number of predefined tool environments. These efforts either used in-context learning to prompt the model to invoke tools [18] or manually constructed large datasets of tool-use instances for supervised training [12]. To further enhance tool comprehension, some approaches directly encoded tools as part of the models vocabulary during training [9, 33]. To encourage generalization to out-of-domain tools, many existing studies format tool document using unified JSON Schema and train models to perform tool calls using structured formats such as ReAct [38] or CodeAct [32]. With advances in RL, more recent work has explored enabling models to improve tool-use capabilities through interaction with their environments [19, 42, 46]. However, these approaches have yet to deliver universal training framework, primarily due to the difficulty of efficiently constructing diverse and stable training environments, and the lack of reliable reward signals for guiding learning. In contrast, we introduce feedback-driven training scheme grounded in the automated construction of diverse tool-use environments. Combined with verifiable reward mechanism, our approach effectively enhances the tool-use capabilities of LLMs using only feedback from stable environment."
        },
        {
            "title": "3 Approaches",
            "content": "As illustrated in Figure 2, our approach comprises two core components: 1) five-stage automated pipeline for constructing tool-use training environments for LLMs; and 2) feedback-driven model training framework that leverages these environments to enhance tool-use capabilities."
        },
        {
            "title": "3.1 Automated Environment Construction",
            "content": "A complete training environment for tool use consists of user input and toolset T. The user input contains question and its corresponding answer a, where can be decomposed into sequence of sub-questions q1, q2, . . . , qn. Each sub-question qi is 3 solvable via tool from T, producing intermediate answers a1, a2, . . . , an. Each tool ti is specified by document di and an implementation component ci. The document di defines the tools name ni, function description fi, and set of parameters Pi. Each parameter pij Pi is defined by its name pnij, type ptij, description pfij, and binary flag prij indicating whether it is required. To efficiently construct robust training environments for tool use, we propose fivestage automated environment construction pipeline. Datasets Train Ours Ours # Scenarios # Number # Avg Tools 4 2215 9.39 4 200 9.26 Test ToolHop 3 2985 3.93 τ -bench 2 165 12. RoTBench 5 840 8.50 Table 1 Statistics of datasets. # Scenarios refers to the number of tool-use scenarios; # Number indicates the total number of data instances; and # Avg Tools denotes the average number of tools included in each instance. Scenario Decomposition To ensure the diversity of training environments, we account for different logical relationships among sub-questions, defining four scenarios of tool use, as illustrated in Figure 1: 1) Single-hop, containing only one sub-question q1; 2) Parallel single-hop, where is decomposed into multiple independent sub-questions q1, q2, . . . , qn that can be solved in parallel; 3) Multi-hop, where is broken down into sequence of dependent subquestions, such that each qi+1 relies on the answer ai of the preceding sub-question; and 4) Parallel multi-hop, hybrid structure consisting of both independent and dependent sub-questions. We manually construct user inputs for each scenario to ensure broad coverage and task variability. Functional generalization: Expand each tools function fi beyond its original sub-questions to cover broader range of tasks, thereby increasing its versatility and data processing capability; 2) Parameter expansion: Enrich the parameter set Pi to support the extended functionality; 3) Parameter type generalization: Replace simple data types (e.g., strings) with more complex structures such as arrays, dictionaries, and nested types; and 4) Toolset extension: Introduce additional non-essential tools to increase the complexity of tool selection and elevate the overall difficulty of the environment. These enhancements result in more realistic and challenging setting for tool use, better suited for training models capable of robust generalization. Document Generation After constructing diverse user inputs, we ensure their solvability by generating corresponding tool document di for every subquestion qi. For every sub-question qi, we generate tool document di with function fi explicitly designed to solve the task described by qi. The parameter set Pi is also abstracted directly from qi, establishing precise one-to-one mapping between sub-questions and tool interfaces. Function Integration While generating separate tool for each sub-question ensures full coverage, it often results in redundancy within the toolset. To reduce duplication, we analyze tool documents d1, d2, . . . , dn and merge those with overlapping functionality. The resulting set d1, d2, . . . , dm (m n) offers improved modularity and efficiency, while preserving logical consistency with the original task. Complexity Scaling While integrating functionality leads to more organized toolset, the resulting tools often exhibit relatively simple functionality and limited parameter sets [43], which can constrain the models ability to generalize to more complex tools. To address this limitation, we enhance tool complexity through four key strategies: 1) is finalized, Localized Deployment Once the tool document it is mapped to corresponding di Python function and deployed locally. Key elements such as ni, pnij, and ptij are mapped to function name, arguments, and type annotations, respectively. Additionally, the associated sub-question qi and answer ai are used as prior conditions to ensure that the function returns correct outputs when invoked with valid arguments and produces appropriate error messages for incorrect inputs. This design establishes stable and controlled environment, facilitating the effective learning of tool use through interaction. Through these five stages, we are able to construct complete tool-use training environments without relying on external toolsets, significantly enhancing the scalability of training data. Since all tools are locally deployable, the environment can provide stable and consistent feedback. Moreover, by analyzing this feedback, we can accurately evaluate each step of the tool invocation process."
        },
        {
            "title": "3.2 Feedback-Driven Model Training",
            "content": "Building on the constructed training environments, we propose feedback-driven training framework. By designing verifiable reward mechanisms that rely 4 Ours ToolHop τ -bench RoTBench Solve-P Solve-R Solve-F1 AC Pass^1 TS PI CF Family Version Gemini Gemini Claude GPT GPT 2.5-Flash 2.5-Pro 4.0-Sonnet 3.5-Turbo 4o Qwen2.5-32B Instruct Qwen2.5-72B Instruct Qwen3-32B Qwen3-32B Non-Reasoning Reasoning 52.78 51.77 42.33 25.77 33.43 30.12 31.68 33.20 28.94 Qwen2.5-7B 27.44 Instruct FTRL-Reinforce++ 41.61 47.82 FTRL-GRPO 33. Qwen2.5-14B Instruct Qwen3-8B Qwen3-14B Qwen3-8B Qwen3-14B FTRL-Reinforce++ 49.51 48.67 FTRL-GRPO Non-Reasoning 21.18 FTRL-Reinforce++ 41.96 46.44 FTRL-GRPO Non-Reasoning 28.29 FTRL-Reinforce++ 44.06 49.41 FTRL-GRPO Reasoning 27.78 FTRL-Reinforce++ 38.11 40.67 FTRL-GRPO 31.91 Reasoning FTRL-Reinforce++ 44.15 41.67 FTRL-GRPO Closed-Source LLMs 23.13 31.96 56.07 29.29 42.01 23.24 29.98 42.59 25.66 33.39 39.37 45.32 53.97 25.73 36.31 Open-Source LLMs (32B - 72B) 37.80 43.41 46.24 40. 32.42 35.22 36.34 31.30 19.70 30.29 35.34 50.12 Open-Source LLMs (7B - 14B) 27.69 41.36 47.37 34.64 47.20 50.56 30.71 47.37 55.84 37.58 59.30 56.71 38.38 43.70 40.06 40.23 48.11 47.67 25.97 40.36 46.78 32.76 47.57 49.20 23.48 43.63 49.54 29.97 48.24 51.82 29.52 39.40 38.58 32.96 44.04 43.36 11.99 23.45 29.68 25.80 36.42 26.63 28.54 37.79 38.93 24.19 38.32 38.26 40.70 41.24 40.50 40.10 44.79 41.64 40.04 47. 50.22 15.13 37.43 21.91 34.26 27.39 31.00 5.91 11.91 6.91 16.74 26.83 25.43 10.13 21.96 23.35 13.74 17.61 24.26 16.43 32.52 28.91 18.87 27.09 31.70 53.62 49.05 39.72 65.71 78.76 75.43 72.19 73.24 53. 70.38 70.95 74.00 71.81 73.05 71.52 75.52 78.10 77.62 75.90 77.81 75.33 54.00 53.52 54.57 57.33 58.95 60.57 34.95 42.38 34.95 29.81 49.43 36.86 26.29 41.71 33.81 27.33 40.95 30.86 38.29 45.24 36.19 36.29 42.57 44.67 36.38 42.57 40.95 34.29 37.52 39.24 36.19 37.24 38.10 22.76 27.52 21.81 17. 31.52 20.00 17.14 24.86 20.19 15.43 26.10 18.95 21.33 28.19 21.62 22.19 25.90 27.05 20.67 25.24 22.48 20.38 22.29 22.95 20.29 21.62 21.43 Avg 36.24 40.63 42.71 29.30 42.79 34.28 36.31 39.79 36. 26.52 37.09 37.80 34.33 44.25 41.23 31.01 42.41 45.43 33.34 44.14 44.90 32.68 38.54 38.19 34.74 40.75 40.77 Table 2 Performance of different LLMs on each test set. The best result for each dataset is bolded, and the second-best is underlined. Performance improvements over the base model after training with our method (i.e., FTRL-Reinforce++ and FTRL-GRPO) are indicated with , while performance declines are marked with . solely on environment feedback, we flexibly support various preference-based training strategies. This allows the model to continually improve its tool-use capabilities through interaction. Verifiable Reward Design To enable continuous model improvement, accurate and informative reward signals are essential [16]. Leveraging stable environments we construct, the model receives direct feedback after each tool invocation, which allows us to verify whether sub-question has been successfully solved. This enables the generation of step-level, verifiable rewards. To encourage both correctness and efficiency, we draw inspiration from the F1 score [5], balancing the precision of tool invocation and the completeness of task execution. We also assess the validity of the models final answer, resulting Importantly, in comprehensive reward signal. the framework operates without external models or predefined solution paths, enhancing generality and applicability. Specially, let be the models output, where is the number of tool invocations, is the number of sub-questions successfully solved, 0 is the number of remaining unsolved sub-questions, and is the correct final answer in the environment. The reward is defined as: = 2q p+1 , 0.5, 0.3, 1 t+1 , 0.5, 0, if > 0 elif is None elif there is format error elif is in elif = 0 else Trajectory Data Collection Suppose we have an LLM to be optimized, leveraging the reward mechanism we have designed, we use to sample training trajectories within the constructed environments. Specifically, performs multi-step interactions with the environment, and at each step, we record the sampled interaction trajectory, the set of available Layer 1 2 2 1 0 26 3 9 13 17 Module MLP MLP MLP MLP MLP Self_Attn MLP Self_Attn Self_Attn Self_Attn Sub-Module down_proj.weight up_proj.weight down_proj.weight up_proj.weight down_proj.weight k_proj.weight down_proj.weight v_proj.bias v_proj.bias v_proj.bias Count Cumulation 11 8 6 3 2 2 1 1 1 1 30.56 52.78 69.44 77.78 83.33 88.89 91.67 94.44 97.22 100. Table 3 Distribution of the top three modules with the highest relative parameter change rates before and after training, ordered by descending proportion. tools, the environments final answer, the remaining unsolved sub-questions, and their corresponding answers. This collection of information constitutes single training instance used for optimization of Ms behavior. Preference-Based Training With the collected data and defined reward signals, we apply preference-based RL algorithms [13, 25, 26] to optimize the models tool-use policy, which encourage behavior that improves along reward gradients. Through repeated interaction and feedback, the model can progressively improve its precision in tool invocation, task-solving ability, and final output validitywithout the need for manually annotated solution paths."
        },
        {
            "title": "4 Experimental Setup",
            "content": "In this section, we present our experimental setup.1 Datasets We train the model on our self-constructed dataset and evaluate it across four distinct tooluse benchmarks. These include Ours as the indomain test set, and ToolHop [43], τ -bench [39], and RoTBench [41] as out-of-domain test sets. Detailed dataset information is provided in Table 1. Metrics We adopt the original evaluation metrics defined for each dataset to objectively assess the performance of LLMs. For Ours, we use Solve-P to measure the precision of tool invocations, SolveR to evaluate task completeness, and Solve-F1, the harmonic mean of Solve-P and Solve-R, to provide an overall performance score. For ToolHop, we use Answer Correctness (AC) to assess the models ability to correctly use tools in answering multi-hop questions. For τ -bench, we report Pass^1, which Dataset MMLU [10] BBH [29] GSM8K [6] MATH [11] HumanEval [4] MBPP [1] Capability # Number General General Reasoning Reasoning Coding Coding 14042 6511 1319 5000 164 257 Table 4 Datasets used for testing general performance. estimates the probability that the model produces the correct answer on its first attempt. For RoTBench, we use three metrics: Tool Selection (TS) to evaluate whether the model selects the correct tool, Parameter Identification (PI) to assess whether the correct parameters are identified given the correct tool, and Content Filling (CF) to measure the accuracy of parameter value generation, conditional on correct tool and parameter selection. Baselines To enable comprehensive comparison of model performance, we evaluate 12 representative LLMs. From closed-source LLMs, we include Gemini-2.5-Flash [8], Gemini-2.5-Pro [8], Claude-4.0Sonnet [31], GPT-3.5-Turbo [3], and GPT-4o [17]. From open-source LLMs, we include the Qwen2.5 [35] and Qwen3 [36] series, covering both non-reasoning and reasoning modes. Additionally, we include two versions of our own approaches: FTRL-Reinforce++ and FTRL-GPRO, which apply the Reinforce++ [13] and GPRO [26] algorithms, respectively, within our training framework. In the environment conImplementation details struction stage, we manually create user inputs and use GPT-4o to assist in building the environments.2 To ensure stable and reproducible results, we set the temperature to 0 and the max_tokens parameter to 2048. In the training stage, we employ the VeRL [27] framework with the following hyperparameters: learning rate of 1e6, batch size of 512, mini-batch size of 32, and 16 rollouts per update. The maximum response length per step is set to 1024 for nonreasoning mode and 8192 for reasoning mode. Both the KL loss and entropy loss coefficients are fixed at 0.01. Training is performed over three epochs, with training trajectories resampled at the beginning of each epoch using the current model. All training is conducted on 8 NVIDIA A100 GPUs. Due to resource constraints, we restrict all open-source LLMs used in training to the 7B14B parameter range. In the 1Additional details are available in Appendix A. 2Detailed prompts are provided in Appendix B. 6 Figure 3 Performance of each generalized capability before and after training across different models. test stage, we ensure consistent and stable results by using each models built-in chat template,3 applying greedy decoding, and setting the maximum response length per step to 1024 for non-reasoning mode and 8192 for reasoning mode. For evaluation on τ -bench, we simulate the user using GPT-4o, with temperature set to 0 and max_tokens set to 512."
        },
        {
            "title": "5 Main Results",
            "content": "We evaluate the performance of various LLMs, and present the average performance across scenarios for each dataset in Table 2.4 Based on it, we make the following observations. various capabilities across Our approach consistently enhances the models conditions. tool-use Across different model families, RL algorithms, and inference modes, our method reliably improves tool-use performance and demonstrates strong generalization on out-of-domain data. On average, it yields over 10% performance gain to LLMs across multiple benchmarks. Remarkably, while open-source LLMs generally underperform compared to closed-source LLMs, models with 8B and 14B 3Chat templates are detailed in Appendix C. 4Detailed performance by scenario is provided in Appendix D. 7 parameters trained using our method outperform the strongest available closed-source models on average. Furthermore, despite the absence of multi-turn user interactions and noisy environments in our training data, the models still achieved substantial performance gains on both τ -bench and RoTBench. These results suggest that our approach effectively enhances the models ability to use tools, even under challenging or previously unseen conditions. Performance gains achieved by our method appear to primarily stem from updates to the models lower-layer MLP parameters. To better understand why our approach effectively enhances tool-use capabilities, we conduct parameter-level analysis. Following the methodology of Ye et al. [44], we compare the relative update rates of different parameter modules before and after training across 12 trained LLMs. For each model, we identify the top three modules with the highest rates of change and summarize their distribution in Table 3. Interestingly, we find that most of these frequently updated modules are concentrated in the MLP components of the lower layers (i.e., layers 02), regardless of the model architecture or training algorithm. This suggests that our method improves performance primarily by enhancing the models ability to understand and represent contextual information in the early stages Figure 4 Performance of Qwen 2.5-7B trained using different reward mechanisms. Figure 5 Solve-F1 of various LLMs across training epochs. of processing. Current open-source LLMs do not necessarily exhibit stronger tool-use performance in reasoning mode compared to non-reasoning mode. Although reasoning mode has demonstrated effectiveness in enhancing the performance of Qwen3-family models on complex tasks [36], it does not consistently offer advantages for tool use. Specifically, while reasoning mode improves performance on datasets such as ToolHop and τ -bench, it also leads to notable reduction in RoTBench. An in-depth analysis of our dataset reveals that reasoning mode boosts performance in multi-hop and parallel multi-hop scenarios but significantly degrades performance in the single-hop case.6 This suggests that the current implementation of reasoning enhances the models capability in complex scenarios at the cost of performance in simpler ones. This trade-off likely stems from the fact that existing reasoning mechanisms in open-source LLMs are primarily optimized for mathematically tasks, limiting their adaptability across diverse tooluse settings. These findings underscore gap in current modeling strategies and highlight the need for more refined reasoning mechanisms that better balance complexity and generalization."
        },
        {
            "title": "6 Further Studies",
            "content": "In this section, we compare the general capabilities of the models before and after training to demonstrate that our approach does not degrade existing model abilities. We further evaluate the impact of different reward mechanisms on performance, providing empirical justification for our chosen design. Additionally, we analyze performance trends 5Case studies are provided in Appendix E. 6Details can be found in Table 6 in Appendix D. 8 throughout the training process, highlighting the continuous improvements enabled by our method. Performance on General Tasks To evaluate the impact of our approach on general abilities alongside its improvements in tool-use performance, we evaluate models before and after training on the six public test sets listed in Table 4. As shown in Figure 3, training with our method significantly enhances tool-use capabilities without compromising generalization performance. These results suggest that our method improves the models ability to understand and represent contextual information, rather than merely overfitting to the training data. This indicates that our approach is compatible with existing LLM training paradigms and holds broad potential for real-world applications. Impact of Reward Mechanisms The design of the reward mechanism is critical component of any RL algorithm, as it directly influences the performance of the trained model [7, 16]. As described in Section 3.2, our reward mechanism is designed to balance both the precision of tool invocations and overall task completion. To validate the effectiveness of this design, we compare the performance of Qwen2.57B trained with four distinct reward functions, each differing only in how tool-calling behavior is scored: focuses solely on precision; 2) RSolve-R = q: rewards only for task completion; 3) RSolve-PR = qq : directly multiplies task completion with precision; and 4) = 2q : our p+1 proposed reward, which balances both precision and completeness. As shown in Figure 4, optimizing for precision alone (i.e., RSolve-P) leads to incomplete task execution, while optimizing for completion alone (i.e., RSolve-R) can severely degrade tool precision, as the model tends to overuse tools to maximize Although RSolve-PR incorporates both reward. 1) RSolve-P = : components, its discrete reward distribution hinders stable training. In contrast, our proposed reward function enables better balance between tool-use precision and task completion, resulting in improved overall performance. Effect of Iteration Count We train each model over three epochs, resampling new training trajectories at the beginning of each epoch to expand the exploration space. Figure 5 presents model performance across training epochs. The results show substantial performance improvements after the first epoch, highlighting the efficiency of our data and training method. Furthermore, as training progresses, most models continue to exhibit consistent performance indicating that our strategy maintains gains, This leads sufficiently rich exploration space. to better data utilization and enhanced training efficiency."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we propose an automated strategy for constructing tool-use training environments that progresses through five-stage pipeline. This approach enables the creation of diverse and comprehensive training settings without relying on external toolsets. Building on these environments, we introduce feedback-driven training framework that enhances models tool-use capabilities by leveraging verifiable reward function. This reward balances tool invocation precision and task completion, and relies solely on feedback from the environment. Extensive experimental results demonstrate the effectiveness and generalizability of our method. Furthermore, analysis suggests that the observed performance gains primarily stem from updates to the models lower-layer MLP parameters, suggesting improved contextual understanding during tool interaction."
        },
        {
            "title": "Limitations",
            "content": "Although we propose an automated pipeline for constructing tool-use training environments and feedback-driven training framework that effectively enhance models tool-use capabilities, our approach primarily focuses on improving tool invocation rather than optimizing the models underlying reasoning process. As discussed in Section 5, the reasoning patterns of current open-source models are not well aligned with the tool use task, resulting in significant gap between their reasoning behavior and actual tool-use performance. Therefore, it is key direction for our future work to explore methods to better align and optimize the reasoning process for tool use."
        },
        {
            "title": "References",
            "content": "[1] Jacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. Program synthesis with large language models. CoRR, abs/2108.07732, 2021. URL https://arxiv.org/abs/2108.07732. [2] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosiute, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemí Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional AI: harmlessness CoRR, from AI abs/2212.08073, 2022. doi: 10.48550/ARXIV.2 212.08073. URL https://doi.org/10.48550/arX iv.2212.08073. feedback. [3] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel HerbertVoss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, MarcAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/202 0/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abs tract.html. [4] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021. URL https://arxiv. org/abs/2107.03374. [5] Nancy Chinchor. MUC-4 evaluation metrics. In Proceedings of the 4th Conference on Message Understanding, MUC 1992, McLean, Virginia, USA, June 16-18, 1992, pages 2229. ACL, 1992. doi: 10.3115/1072064.1072067. URL https://doi.org/ 10.3115/1072064.1072067. [6] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. CoRR, abs/2110.14168, 2021. URL https://ar xiv.org/abs/2110.14168. [7] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, and S. S. Li. Deepseekr1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025. doi: 10.48550/ARXIV.2501.12948. URL https://doi.org/10.48550/arXiv.2501.12948. [8] Google Gemini Team. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities, 2025. URL https://storage.googleapis.com/dee pmind-media/gemini/gemini_v2_5_report.pdf. [9] Shibo Hao, Tianyang Liu, Zhen Wang, and Zhiting Hu. Toolkengpt: Augmenting frozen language models with massive tools via tool embeddings. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper /2023/hash/8fd1a81c882cd45f64958da6284f4a3 f-Abstract-Conference.html. [10] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language In 9th International Conference understanding. on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?i d=d7KBjmI3GmQ. [11] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Joaquin Vanschoren and Sai-Kit Yeung, editors, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021. URL https://datasets-benchmarks-pro ceedings.neurips.cc/paper/2021/hash/be83ab 3ecd0db773eb2dc1b0a17836a1-Abstract-round2. html. [12] Cheng-Yu Hsieh, Si-An Chen, Chun-Liang Li, Yasuhisa Fujii, Alexander Ratner, Chen-Yu Lee, Ranjay Krishna, and Tomas Pfister. Tool documentation enables zero-shot tool-usage with large language models. CoRR, abs/2308.00675, 2023. doi: 10.48550/ARXIV.2308.00675. URL https://doi.org/10.48550/arXiv.2308.00675. [13] Jian Hu, Xibin Wu, Wei Shen, Jason Klein Liu, Zilin Zhu, Weixun Wang, Songlin Jiang, Haoran Wang, Hao Chen, Bin Chen, Weikai Fang, Xianyu, Yu Cao, and Haotian Xu. Reinforce++: An efficient rlhf algorithm with robustness to both prompt and 10 reward models. CoRR, abs/2501.03262, 2025. URL https://arxiv.org/abs/2501.03262. [14] Yue Huang, Jiawen Shi, Yuan Li, Chenrui Fan, Siyuan Wu, Qihui Zhang, Yixin Liu, Pan Zhou, Yao Wan, Neil Zhenqiang Gong, and Lichao Sun. Metatool benchmark for large language models: Deciding whether to use tools and which to use. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https: //openreview.net/forum?id=R0c2qtalgG. [15] Zhaoyang Liu, Zeqiang Lai, Zhangwei Gao, Erfei Cui, Ziheng Li, Xizhou Zhu, Lewei Lu, Qifeng Chen, Yu Qiao, Jifeng Dai, and Wenhai Wang. Controlllm: Augment language models with tools by searching on graphs. In Ales Leonardis, Elisa Ricci, Stefan Roth, Olga Russakovsky, Torsten Sattler, and Gül Varol, editors, Computer Vision - ECCV 2024 - 18th European Conference, Milan, Italy, September 29-October 4, 2024, Proceedings, Part XII, volume 15070 of Lecture Notes in Computer Science, pages 89105. Springer, 2024. doi: 10.1007/978-3-031-732 54-6_6. URL https://doi.org/10.1007/978-3 -031-73254-6_6. [16] Zijun Liu, Peiyi Wang, Runxin Xu, Shirong Ma, Chong Ruan, Peng Li, Yang Liu, and Yu Wu. Inference-time scaling for generalist reward modeling. CoRR, abs/2504.02495, 2025. doi: 10.48550/ARX IV.2504.02495. URL https://doi.org/10.48550/a rXiv.2504.02495. [17] OpenAI. GPT-4 technical CoRR, abs/2303.08774, 2023. doi: 10.48550/ARXIV.2 303.08774. URL https://doi.org/10.48550/arX iv.2303.08774. report. [18] Bhargavi Paranjape, Scott M. Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, and Marco Túlio Ribeiro. ART: automatic multi-step reasoning and tool-use for large language models. CoRR, abs/2303.09014, 2023. doi: 10.48550/ARX IV.2303.09014. URL https://doi.org/10.48550/a rXiv.2303.09014. [19] Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-Tür, Gokhan Tur, and Heng Ji. Toolrl: Reward is all tool learning needs. CoRR, abs/2504.13958, 2025. doi: 10.48550 /ARXIV.2504.13958. URL https://doi.org/10.4 8550/arXiv.2504.13958. [20] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, 11 Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and Maosong Sun. Tool learning with foundation models. CoRR, abs/2304.08354, 2023. doi: 10.48550/ARX IV.2304.08354. URL https://doi.org/10.48550/a rXiv.2304.08354. [21] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. Toolllm: Facilitating large language In models to master 16000+ real-world apis. The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https: //openreview.net/forum?id=dHng2O0Jjr. [22] Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, and Ji-Rong Wen. Tool learning with large language models: survey. Frontiers Comput. Sci., 19(8):198343, 2025. doi: 10.1007/S11704-024-40678-2. URL https: //doi.org/10.1007/s11704-024-40678-2. [23] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy P. Lillicrap, JeanBaptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, Ioannis Antonoglou, Rohan Anil, Sebastian Borgeaud, Andrew M. Dai, Katie Millican, Ethan Dyer, Mia Glaese, Thibault Sottiaux, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, James Molloy, Jilin Chen, Michael Isard, Paul Barham, Tom Hennigan, Ross McIlroy, Melvin Johnson, Johan Schalkwyk, Eli Collins, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, Clemens Meyer, Gregory Thornton, Zhen Yang, Henryk Michalewski, Zaheer Abbas, Nathan Schucher, Ankesh Anand, Richard Ives, James Keeling, Karel Lenc, Salem Haykal, Siamak Shakeri, Pranav Shyam, Aakanksha Chowdhery, Roman Ring, Stephen Spencer, Eren Sezener, and et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. CoRR, abs/2403.05530, 2024. doi: 10.48550/ARXIV.2403.05530. URL https: //doi.org/10.48550/arXiv.2403.05530. [24] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http: //papers.nips.cc/paper_files/paper/2023/ha sh/d842425e4bf79ba039352da0f658a906-Abstrac t-Conference.html. [25] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv.org/abs/1707.06347. [26] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. CoRR, abs/2402.03300, 2024. doi: 10.48550/ARX IV.2402.03300. URL https://doi.org/10.48550/a rXiv.2402.03300. [27] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient RLHF framework. In Proceedings of the Twentieth European Conference on Computer Systems, EuroSys 2025, Rotterdam, The Netherlands, 30 March 2025 - 3 April 2025, pages 12791297. ACM, 2025. doi: 10.1145/36 89031.3696075. URL https://doi.org/10.1145/ 3689031.3696075. [28] Yifan Song, Weimin Xiong, Dawei Zhu, Cheng Li, Ke Wang, Ye Tian, and Sujian Li. Restgpt: Connecting large language models with real-world applications via restful apis. CoRR, abs/2306.06624, 2023. doi: 10.48550/ARXIV.2306.06624. URL https://doi.org/10.48550/arXiv.2306.06624. [29] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. Challenging bigbench tasks and whether chain-of-thought can solve them. In Anna Rogers, Jordan L. BoydGraber, and Naoaki Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023, pages 1300313051. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.FINDING S-ACL.824. URL https://doi.org/10.18653/v1/ 2023.findings-acl.824. [30] Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, and Le Sun. Toolalpaca: Generalized tool learning for language models with 3000 simulated cases. CoRR, abs/2306.05301, 2023. doi: 10.48550/ARXIV.2306.05301. URL https: //doi.org/10.48550/arXiv.2306.05301. [31] Anthropic Team. Introducing claude 4, 2025. URL https://www.anthropic.com/news/claude-4. [32] Harsh Trivedi, Tushar Khot, Mareike Hartmann, Ruskin Manku, Vinty Dong, Edward Li, Shashank Gupta, Ashish Sabharwal, and Niranjan Balasubramanian. Appworld: controllable world of apps and people for benchmarking interactive coding agents. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 1602216076. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.ACL-LONG.850. URL https: //doi.org/10.18653/v1/2024.acl-long.850. [33] Renxi Wang, Xudong Han, Lei Ji, Shu Wang, Timothy Baldwin, and Haonan Li. Toolgen: Unified tool retrieval and calling via generation. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum?id=XLMAMmowdY. [34] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huang, and Tao Gui. The rise and potential of large language model based agents: survey. CoRR, abs/2309.07864, 2023. doi: 10.48550/ARXIV.2309.07864. URL https://doi.org/10.48550/arXiv.2309.07864. [35] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. CoRR, abs/2412.15115, 2024. doi: 10.48550/ARXIV.2412. 15115. URL https://doi.org/10.48550/arXiv.2 412.15115. [36] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, 12 Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report. CoRR, abs/2505.09388, 2025. URL https://arxiv.org/abs/2505.09388. [37] Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, and Ying Shan. Gpt4tools: Teaching large language model to use tools via self-instruction. CoRR, abs/2305.18752, 2023. doi: 10.48550/ARX IV.2305.18752. URL https://doi.org/10.48550/a rXiv.2305.18752. [38] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/pdf?id=WE_vlu YUL-X. [39] Shunyu Yao, Noah Shinn, Pedram Razavi, and Karthik Narasimhan. τ -bench: benchmark for tool-agent-user interaction in real-world domains. CoRR, abs/2406.12045, 2024. doi: 10.48550/ARX IV.2406.12045. URL https://doi.org/10.48550/a rXiv.2406.12045. [40] Junjie Ye, Sixian Li, Guanyu Li, Caishuang Huang, Songyang Gao, Yilong Wu, Qi Zhang, Tao Gui, and Xuanjing Huang. Toolsword: Unveiling safety issues of large language models in tool learning across three stages. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 2181 2211. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.ACL-LONG.119. URL https://doi.org/10.18653/v1/2024.acl-l ong.119. [41] Junjie Ye, Yilong Wu, Songyang Gao, Caishuang Huang, Sixian Li, Guanyu Li, Xiaoran Fan, Qi Zhang, Tao Gui, and Xuanjing Huang. Rotbench: multilevel benchmark for evaluating the robustness of In Yaser large language models in tool learning. Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, pages 313333. Association for Computational Linguistics, 2024. URL https://aclanthology.o rg/2024.emnlp-main.19. [42] Junjie Ye, Yilong Wu, Sixian Li, Yuming Yang, Tao Gui, Qi Zhang, Xuanjing Huang, Peng Wang, Zhongchao Shi, Jianping Fan, and Zhengyin Du. Tl-training: task-feature-based framework for training large language models in tool use. CoRR, abs/2412.15495, 2024. doi: 10.48550/ARXIV.2412. 15495. URL https://doi.org/10.48550/arXiv.2 412.15495. [43] Junjie Ye, Zhengyin Du, Xuesong Yao, Weijian Lin, Yufei Xu, Zehui Chen, Zaiyuan Wang, Sining Zhu, Zhiheng Xi, Siyu Yuan, Tao Gui, Qi Zhang, Xuanjing Huang, and Jiecao Chen. Toolhop: query-driven benchmark for evaluating large language models in multi-hop tool use. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar, editors, Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025, Vienna, Austria, July 27 - August 1, 2025, pages 29953021. Association for Computational Linguistics, 2025. URL https: //aclanthology.org/2025.acl-long.150/. [44] Junjie Ye, Caishuang Huang, Zhuohan Chen, Wenjie Fu, Chenyuan Yang, Leyi Yang, Yilong Wu, Peng Wang, Meng Zhou, Xiaolong Yang, Tao Gui, Qi Zhang, Zhongchao Shi, Jianping Fan, and Xuanjing Huang. multi-dimensional constraint framework for evaluating and improving instruction following in large language models. CoRR, abs/2505.07591, 2025. doi: 10.48550/ARX IV.2505.07591. URL https://doi.org/10.48550/a rXiv.2505.07591. [45] Junjie Ye, Guanyu Li, Songyang Gao, Caishuang Huang, Yilong Wu, Sixian Li, Xiaoran Fan, Shihan Dou, Tao Ji, Qi Zhang, Tao Gui, and Xuanjing Huang. Tooleyes: Fine-grained evaluation for tool learning capabilities of large language models in real-world scenarios. In Owen Rambow, Leo Wanner, Marianna Apidianaki, Hend Al-Khalifa, Barbara Di Eugenio, and Steven Schockaert, editors, Proceedings of the 31st International Conference on Computational Linguistics, COLING 2025, Abu Dhabi, UAE, January 19-24, 2025, pages 156187. Association for Computational Linguistics, 2025. URL https://aclanthology.org/2025.coling -main.12/. [46] Yuanqing Yu, Zhefan Wang, Weizhi Ma, Zhicheng Guo, Jingtao Zhan, Shuai Wang, Chuhan Wu, Zhiqiang Guo, and Min Zhang. Steptool: stepgrained reinforcement learning framework for tool learning in llms. CoRR, abs/2410.07745, 2024. doi: 10.48550/ARXIV.2410.07745. URL https: //doi.org/10.48550/arXiv.2410.07745. [47] Siyu Yuan, Zehui Chen, Zhiheng Xi, Junjie Ye, Zhengyin Du, and Jiecao Chen. Agent-r: Training language model agents to reflect via iterative selftraining. CoRR, abs/2501.11425, 2025. doi: 10.485 50/ARXIV.2501.11425. URL https://doi.org/10 .48550/arXiv.2501.11425. [48] Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and Chao Zhang. Toolqa: dataset for LLM question answering with external tools. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper /2023/hash/9cb2a7495900f8b602cb10159246a01 6-Abstract-Datasets_and_Benchmarks.html. [49] Yuchen Zhuang, Xiang Chen, Tong Yu, Saayan Mitra, Victor S. Bursztyn, Ryan A. Rossi, Somdeb Sarkhel, and Chao Zhang. Toolchain*: Efficient action space navigation in large language models with a* search. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=B6pQ xqUcT8."
        },
        {
            "title": "A Details of Experimental Setup",
            "content": "In this section, we detail our experimental setup, including the datasets used (Section A.1), metrics for each benchmark (Section A.2), and baselines for comparison (Section A.3). A.1 Datasets Environments # Questions # Avg Sub-Qs # Avg Tools Single-Hop 500 1.00 8.00 Train Prallel Single-Hop Multi-Hop 500 2.10 8.14 500 4.71 9.81 Parallel Multi-Hop 715 6.97 10.92 Single-Hop 50 1.00 7.96 Prallel Single-Hop Multi-Hop 50 2.02 7.48 50 5.72 10.34 Parallel Multi-Hop 50 7.66 11.26 Test Table 5 Statistical overview of constructed environments. # Questions indicates the total number of questions; # Avg Sub-Qs refers to the average number of sub-questions derived from each question; and # Avg Tools denotes the average number of tools associated with each data instance. Train Based on the automated environment construction scheme described in Section 3.1, we create diverse training data across four distinct scenarios, each paired with corresponding locally executable toolset. This results in comprehensive and varied set of training environments. Detailed data distributions are provided in Table 5. Test To comprehensively evaluate the performance gains introduced by our approach, we construct custom test set for in-domain evaluation and employ three publicly available tool-use datasets for out-of-domain evaluation. Ours refers to the dataset constructed using the scheme described in Section 3.1, containing 50 entries per scenario. This dataset serves as the in-domain test set for evaluating model performance. We ensure that there is no overlap between the test and training sets. ToolHop is the first dataset specifically designed to evaluate model performance in the multi-hop tool use task. It contains 995 data instances and defines three scenarios to comprehensively assess models ability to use tools to answer multi-hop questions under varying conditional constraints. τ -bench is dataset designed to evaluate models ability to use various data manipulation tools to meet user needs related to airline ticketing or book reservations. It is characterized by multi-turn interactions between the user and the model and contains total of 165 data instances. RoTBench is dataset designed to evaluate the robustness of LLMs in tool use. It includes five different noise-level scenarios and consists of total of 840 data instances. A.2 Metrics We adopt the original evaluation metrics defined for each dataset to objectively evaluate the performance of LLMs. Ours Assume that for given test instance, there are sub-questions in total. During its interaction with the environment, the model invokes tools times and successfully solves sub-questions. We evaluate the models performance using the following three metrics: Solve-P: Solve-P = (cid:40) , 1, if > 0 if = 0 This metric measures the precision of the tool invocations. 15 Solve-R: Solve-R = This metric measures the completeness of the task. Solve-F1: Solve-F1 = 2 Solve-P Solve-R Solve-P + Solve-R This computes the harmonic mean of Solve-P and Solve-R, providing an overall performance measure. ToolHop Assume that for test instance, the standard answer is and the models final response is o. We evaluate the models performance using the following metric: Answer Correctness (AC): AC = (cid:40) if is in 1, 0, otherwise This metric assesses the models ability to correctly use tools to answer multi-hop questions. τ -bench Assume that for test set, the model makes total of num attempts, out of which are correct. We evaluate the models performance using the following metric: Pass^1: Passˆ1 = num This metric estimates the probability that the model produces the correct answer on its first attempt. RoTBench Assume that for test instance, the correct tool is t, the correct parameter set is P, and the corresponding parameter values are V. The model predicts tool t, parameter set P, and parameter values V. We evaluate the models performance using the following three metrics: Tool Selection (TS): TS = (cid:40) if = 1, 0, otherwise This metric measures the models ability to select the correct tool. Parameter Identification (PI): PI = (cid:40)TS, 0, if = otherwise This metric evaluates whether the model correctly identifies the required parameter set, conditional on correct tool selection. Content Filling (CF): CF = (cid:40)PI, 0, if = otherwise This metric assesses the models ability to accurately fill in the parameter values, conditional on correct tool and parameter set selection. A.3 Baselines To enable comprehensive comparison of model performance, we select 12 representative LLMs for evaluation. 16 Closed-Source LLMs We select five of the most representative closed-source models for evaluation, reflecting the current state-of-the-art in LLMs. Gemini-2.5-Flash and Gemini-2.5-Pro, the latest generation of LLMs released by Google, demonstrating strong performance across wide range of complex tasks. Claude-4.0-Sonnet, developed by Anthropic, known for its exceptional capabilities in code-related tasks. GPT-3.5-Turbo and GPT-4o, released by OpenAI, which excel at broad spectrum of general-purpose tasks. Open-Source LLMs We select all models ranging from 7B to 72B in the Qwen2.5 and Qwen3 series as representative open-source models for evaluation, as they reflect the current state-of-the-art among open-source LLMs of similar scale. Additionally, since the Qwen3 models support both reasoning and non-reasoning modes, we evaluate them under both configurations to comprehensively evaluate their performance. Ours To demonstrate the generalizability of our approach, we apply the Reinforce++ and GPRO algorithms to our training framework, resulting in FTRL-Reinforce++ and FTRL-GPRO. Considering resource constraints, we conduct experiments using these algorithms on all open-source LLMs range in the 7B14B."
        },
        {
            "title": "B Prompts for Environment Construction",
            "content": "With the support of GPT-4os powerful natural language understanding capabilities, we successfully carry out automated environment construction. The prompts used in this process are listed in this section. Document_Generation = ''' Identify the appropriate tool to solve the given problem and provide an analysis of the tool design. The output should be in JSON format, following the specified structure. (cid:44) # Steps 1. **Analyze the Problem**: Understand the question and determine the type of information required to answer it. (cid:44) 2. **Tool Design**: Design tool that can solve the problem, considering the complexity and additional functionalities it might need. (cid:44) 3. **Parameter Specification**: Define the parameters for the tool, ensuring they are comprehensive and flexible for various use cases. (cid:44) 4. **Output Construction**: Format the output in JSON, including both the analysis and the tool schema. (cid:44) # Notes - Ensure the tool is versatile enough to handle different but similar queries. - Consider edge cases. # Output Format The output should be JSON object with the following structure **without any other contents**: (cid:44) - \"analysis\": detailed analysis of the ideas behind the tool design. - \"tool\": JSON schema characterizing the tool, including its name, description, and parameters. (cid:44) # Example **Question**: What is the walking distance (km) from 8 Oceanside Road to Hope Elementary School? (cid:44) **Output**: {{ \"analysis\": \"The problem involves calculating the walking distance between two (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) locations. This requires more generalized tool that can handle different types of routes and distances based on the mode of transportation. The tool should integrate mapping or routing service capable of computing distances for various transport modes, such as pedestrian, cycling, and driving routes. It should also support route preferences like avoiding toll roads or specific types of routes. The design must be flexible enough to allow for diverse transportation methods while providing options to fine-tune distance calculations based on user needs.\", (cid:44) \"tool\": {{ \"name\": \"distance_calculator\", \"description\": \"A versatile tool to calculate distances between two locations for various modes of transportation (e.g., walking, biking, driving). It provides route-based distances and adjusts for real-world conditions such as road types and traffic.\", (cid:44) (cid:44) (cid:44) \"parameters\": {{ \"type\": \"object\", \"properties\": {{ \"origin\": {{ \"type\": \"string\", \"description\": \"Starting point address or coordinates for the distance calculation.\" (cid:44) }}, \"destination\": {{ \"type\": \"string\", \"description\": \"Ending point address or coordinates for the distance calculation.\" (cid:44) }}, \"mode\": {{ \"type\": \"string\", \"description\": \"Mode of transportation to calculate the distance for. Options include 'walking', 'biking', 'driving', etc.\", (cid:44) \"enum\": [\"walking\", \"biking\", \"driving\", \"public_transport\"] }}, \"route_preference\": {{ \"type\": \"string\", \"description\": \"Preferred route type (e.g., shortest, fastest, scenic, etc.). Default is 'shortest'.\", (cid:44) \"enum\": [\"shortest\", \"fastest\", \"scenic\", \"avoid_tolls\"], \"default\": \"shortest\" }}, \"unit\": {{ \"type\": \"string\", \"description\": \"Unit of distance to return. Can be 'km', 'miles', or 'meters'. Default is 'km'.\", (cid:44) \"enum\": [\"km\", \"miles\", \"meters\"], \"default\": \"km\" }}, \"avoid_tolls\": {{ \"type\": \"boolean\", \"description\": \"Indicates whether to avoid toll roads. Default is false.\", (cid:44) \"default\": false }}, \"traffic_conditions\": {{ \"type\": \"string\", \"description\": \"Accounts for traffic conditions. Options include 'light', 'moderate', 'heavy'. Default is 'light'.\", (cid:44) \"enum\": [\"light\", \"moderate\", \"heavy\"], \"default\": \"light\" }}, \"use_pedestrian_routes\": {{ \"type\": \"boolean\", \"description\": \"Whether to prioritize pedestrian paths (e.g., walking or biking routes). Default is false.\", (cid:44) \"default\": false }} }}, 19 \"required\": [ \"origin\", \"destination\", \"mode\" ] }} }} }} # Example 2 **Question**: Which forest is near Mount Everest? **Output**: {{ \"analysis\": \"The problem involves identifying nearby forests to specific (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) geographical landmark, Mount Everest. This requires geographical search tool that can pull information based on proximity to known location. The tool needs to account for various types of forests, their proximity to landmarks, and possibly the specific region around Mount Everest. It should be able to return results from large database or map of forests and other natural landmarks. The tool should also consider different geographic boundaries, such as countries or regions (e.g., Nepal, Tibet), and support query flexibility for range of similar queries. Additionally, the tool should handle edge cases such as locations in remote or lesser-documented areas.\", (cid:44) \"tool\": {{ \"name\": \"nearby_forest_locator\", \"description\": \"A tool designed to identify forests or wooded areas near (cid:44) specific geographic landmark or location, providing proximity-based results for various regions, including remote or hard-to-reach areas.\", (cid:44) \"parameters\": {{ \"type\": \"object\", \"properties\": {{ \"landmark\": {{ \"type\": \"string\", \"description\": \"The name of the landmark or location to search for nearby forests.\" (cid:44) }}, \"radius\": {{ \"type\": \"number\", \"description\": \"The search radius (in kilometers) within which to look for nearby forests. Default is 50 km.\", (cid:44) \"default\": 50 }}, \"region\": {{ \"type\": \"string\", \"description\": \"The region or country where the search should be focused (e.g., 'Nepal', 'Tibet'). This can help narrow down results.\" (cid:44) (cid:44) }}, \"forest_type\": {{ \"type\": \"string\", \"description\": \"Filter for specific types of forests (e.g., 'tropical', 'boreal', 'temperate'). Optional.\", (cid:44) \"enum\": [\"tropical\", \"boreal\", \"temperate\", \"mixed\", \"dry\"] }}, \"include_protected\": {{ \"type\": \"boolean\", \"description\": \"Whether to include protected areas or national parks in the search. Default is false.\", (cid:44) \"default\": false }} }}, \"required\": [\"landmark\"] }} }} }} **Question**: {question} **Output**: ''' Function_Integration = ''' Your task is to analyze set of tool documents in JSON schema format to identify any tools with the same functionality and merge them if needed. Retain the functionality from each separate document, ensure the final merged document remains fully compatible, and then output your reasoning and the merged documents strictly in the specified JSON structure. (cid:44) (cid:44) (cid:44) (cid:44) # Steps 1. **Parse and Understand**: Begin by parsing each tool document's JSON schema to understand its functionality, inputs, and outputs. Identify key features that define its purpose and operations. (cid:44) (cid:44) 2. **Compare Documents**: Systematically compare each document to identify tools with identical or overlapping functionalities. Look for description of each tool to determine similarities. (cid:44) (cid:44) 3. **Merge Tools**: For each group of functionally identical tools, merge them into single new schema. Ensure the merged schema accommodates all functionalities from the original tools without loss of essential detail or compatibility. (cid:44) (cid:44) 4. **Compose Analysis**: Draft your reasoning process, describing how the schemas were compared, how conclusions on identical functionalities were reached, and details of how they were merged. (cid:44) (cid:44) # Output Format Your output must be valid JSON according to the following structure: - `\"analysis\"`: string detailing your reasoning, including how you compared schemas, identified identical functionalities, and performed merges. (cid:44) - `\"merged\"`: An array of objects, each containing: - `\"id\"`: list of tool numbers (in ascending order) that you decided to merge. - `\"document\"`: The merged document if applicable. - Tools that do not need to be merged must not be displayed here. - If no tools need merging, use `\"merged\": null`. # Notes - Ensure that the final merged document is fully compatible and retains all original functionalities. (cid:44) - Use clear reasoning and specify the parameters or features that led to the decision to merge the tools. (cid:44) - Maintain the integrity and accuracy of the original tool documents in the merged results. (cid:44) **Tool Documents**: {documents} **Output**: ''' Complexity_Scaling = ''' Refine the design of tool by enhancing its description and increasing the complexity of parameters, while maintaining compatibility with the original functionality. (cid:44) # Steps 1. **Analyze the Current Tool**: Examine the existing tool's description and parameters to understand its functionality and limitations. (cid:44) 2. **Identify Areas for Refinement**: Determine which aspects of the tool can be improved or expanded to better meet real-world requirements. (cid:44) 3. **Refine the Description**: Refine existing parameters so that each parameter value is an objective entity. Introduce new parameters to increase complexity and utility, but ensure full compatibility with legacy functionality. (cid:44) 4. **Ensure Compatibility**: Verify that the refined version remains compatible with the (cid:44) original tool's purpose and structure. (cid:44) # Output Format The output should be in JSON format with the following structure **without any other contents**: (cid:44) - \"analysis\": Analysis of ideas about refining the tool. - \"refined_version\": The version after refinement, should be follow JSON SCHEMA format as the original tool. (cid:44) # Notes - Ensure that any new parameters added are relevant and enhance the tool's functionality. - Maintain backward compatibility with the original tool's design and purpose. **Tool**: {tool} **Output**: ''' 22 Localized_Deployment = ''' Create function implementation based on provided tool document and question-answer pairs. The implementation should strictly adhere to the tool's specifications and include robust error handling. (cid:44) (cid:44) # Steps (cid:44) 1. **Understand the Tool Document**: Carefully review the tool document to identify the function name, parameter names, and types. Ensure that these details are used as-is in the function implementation. (cid:44) 2. **Analyze Question-Answer Pairs**: Examine these pairs to understand how questions map to function inputs and how answers should be derived from function outputs. (cid:44) 3. **Implement the Function**: - Use the tool-specified function name. - Define parameters exactly as specified in the tool document. - Implement logic to correctly derive answers for questions based on the input parameters. (cid:44) - (cid:44) When parameters are assigned default values, Make sure that the function return value contains the complete given answer, i.e., the answer is substring of the return value. (cid:44) - Ensure the function is capable of returning various responses, simulating additional return values if necessary. (cid:44) 4. **Error Handling**: Develop comprehensive mechanism to return error messages for incorrect inputs or other issues, ensuring the function operates reliably in all scenarios. (cid:44) (cid:44) # Output Format The result should be output in JSON format, adhering to the following structure **without anything else**: (cid:44) - \"analysis\": detailed explanation of the function design, including reasoning for parameter choices and exception handling logic. (cid:44) - \"function\": The specific implementation of the function, with code and comments that describe each component. (cid:44) # Notes - Match parameter names and types exactly with those specified in the tool document. - Simulate additional return values as needed, based on the tool's documentation. - Rely only on Python3's built-in libraries for all function implementations. - Ensure comprehensive error handling to anticipate and manage potential issues effectively. (cid:44) **Tool Document**: {document} **Question-Answer Pairs**: {pairs} **Output**: ''' 23 CODE_VERIFY = ''' Analyze the relationship between given piece of function code and problem to provide the appropriate function call to solve the problem, ensuring that all parameter values are derived from the problem. (cid:44) (cid:44) # Steps 1. **Understand the Problem**: Read and comprehend the details of the given problem. 2. **Analyze the Code**: Examine the provided function code to ascertain how it addresses the problem. (cid:44) 3. **Confirm Code-to-Problem Suitability**: Determine if the function correctly solves the problem as described. (cid:44) 4. **Derive Function Call**: Craft function call using the problem's specific details for parameter values. (cid:44) # Output Format Output the result in the following JSON format without any additional text: - \"analysis\": description analyzing how the given code relates to and addresses the problem. (cid:44) - \"call\": The function call formatted as func(param=\"value\"), using the problem's details for the parameter values. (cid:44) - If the fucntion can not used to solve the problem, return \"call\": null. # Examples **Example 1:** - Input Problem: \"Calculate the area of rectangle with width=5 and height=10.\" - Input Code: `def calculate_area(width, height): return width * height` - JSON Output: - {{ \"analysis\": \"The function calculate_area computes the area of rectangle by multiplying width and height. Given the problem, it appropriately solves it.\", (cid:44) \"call\": \"calculate_area(width=5, height=10)\" }} **Example 2:** - Input Problem: \"Find the square of the number 7.\" - Input Code: `def square(num): return num * num` - JSON Output: - {{ \"analysis\": \"The function square returns the square of the input number. It is suitable for finding the square of 7 as described in the problem.\", (cid:44) \"call\": \"square(num=7)\" }} # Notes 24 - Ensure the function call uses the exact parameter names as defined in the function code. (cid:44) - The analysis should concisely justify the relationship between the function and the problem solution. (cid:44) - Input Problem: {question} - Input Code: {code} - JSON Output: '''"
        },
        {
            "title": "C Chat Templates of Various LLMs",
            "content": "To ensure fair and stable evaluation of each models performance, we use the built-in chat templates provided by each model. The specific chat templates for all open-source LLMs are listed in this section. Qwen2_5_chat_template = ''' {%- if tools %} {{- '<im_start>systemn' }} {%- if messages[0].role == 'system' %} {{- messages[0].content + 'nn' }} {%- endif %} {{- \"# ToolsnnYou may call one or more functions to assist with the user (cid:44) query.nnYou are provided with function signatures within <tools></tools> XML tags:n<tools>\" }} (cid:44) {%- for tool in tools %} {{- \"n\" }} {{- tool tojson }} {%- endfor %} {{- \"n</tools>nnFor each function call, return json object with function name (cid:44) and arguments within <tool_call></tool_call> XML tags:n<tool_call>n{\"name\": <function-name>, \"arguments\": <args-json-object>}n</tool_call><im_end>n\" }} (cid:44) {%- else %} {%- if messages[0].role == 'system' %} {{- '<im_start>systemn' + messages[0].content + '<im_end>n' }} {%- endif %} {%- endif %} {%- set ns = namespace(multi_step_tool=true, last_query_index=messageslength - 1) %} {%- for message in messages[::-1] %} {%- set index = (messageslength - 1) - loop.index0 %} {%- if ns.multi_step_tool and message.role == \"user\" and message.content is string (cid:44) (cid:44) and not(message.content.startswith('<tool_response>') and message.content.endswith('</tool_response>')) %} {%- set ns.multi_step_tool = false %} {%- set ns.last_query_index = index %} {%- endif %} {%- endfor %} {%- for message in messages %} {%- if message.content is string %} {%- set content = message.content %} {%- else %} {%- set content = '' %} {%- endif %} {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %} {{- '<im_start>' + message.role + 'n' + content + '<im_end>n' }} {%- elif message.role == \"assistant\" %} {%- set reasoning_content = '' %} {%- if message.reasoning_content is string %} {%- set reasoning_content = message.reasoning_content %} {%- else %} {%- if '</think>' in content %} 26 {%- set reasoning_content = (cid:44) content.split('</think>')[0].rstrip('n').split('<think>')[-1]. lstrip('n') %} (cid:44) {%- set content = content.split('</think>')[-1].lstrip('n') %} {%- endif %} {%- endif %} {%- if loop.index0 > ns.last_query_index %} {%- if loop.last or (not loop.last and reasoning_content) %} {{- '<im_start>' + message.role + 'n<think>n' + (cid:44) reasoning_content.strip('n') + 'n</think>nn' + content.lstrip('n') }} (cid:44) {%- else %} {{- '<im_start>' + message.role + 'n' + content }} {%- endif %} {%- else %} {{- '<im_start>' + message.role + 'n' + content }} {%- endif %} {%- if message.tool_calls %} {%- for tool_call in message.tool_calls %} {%- if (loop.first and content) or (not loop.first) %} {{- 'n' }} {%- endif %} {%- if tool_call.function %} {%- set tool_call = tool_call.function %} {%- endif %} {{- '<tool_call>n{\"name\": \"' }} {{- tool_call.name }} {{- '\", \"arguments\": ' }} {%- if tool_call.arguments is string %} {{- tool_call.arguments }} {%- else %} {{- tool_call.arguments tojson }} {%- endif %} {{- '}n</tool_call>' }} {%- endfor %} {%- endif %} {{- '<im_end>n' }} {%- elif message.role == \"tool\" %} {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %} {{- '<im_start>user' }} {%- endif %} {{- 'n<tool_response>n' + content + 'n</tool_response>' }} {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %} {{- '<im_end>n' }} {%- endif %} {%- endif %} {%- endfor %} {%- if add_generation_prompt %} {{- '<im_start>assistantn' }} {%- if enable_thinking is defined and enable_thinking is false %} {{- '<think>nn</think>nn' }} {%- endif %} {%- endif %} ''' Qwen3_non_reasoning_chat_template = ''' {%- if tools %} {{- '<im_start>systemn' }} {%- if messages[0].role == 'system' %} {{- messages[0].content + 'nn' }} {%- endif %} {{- \"# ToolsnnYou may call one or more functions to assist with the user (cid:44) query.nnYou are provided with function signatures within <tools></tools> XML tags:n<tools>\" }} (cid:44) {%- for tool in tools %} {{- \"n\" }} {{- tool tojson }} {%- endfor %} {{- \"n</tools>nnFor each function call, return json object with function name (cid:44) and arguments within <tool_call></tool_call> XML tags:n<tool_call>n{\"name\": <function-name>, \"arguments\": <args-json-object>}n</tool_call><im_end>n\" }} (cid:44) {%- else %} {%- if messages[0].role == 'system' %} {{- '<im_start>systemn' + messages[0].content + '<im_end>n' }} {%- endif %} {%- endif %} {%- for message in messages %} {%- if message.content is string %} {%- set content = message.content %} {%- else %} {%- set content = '' %} {%- endif %} {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %} {{- '<im_start>' + message.role + 'n' + content + '<im_end>n' }} {%- elif message.role == \"assistant\" %} {{- '<im_start>' + message.role + 'n' + content }} {%- if message.tool_calls %} {%- for tool_call in message.tool_calls %} {{- 'n' }} {%- if tool_call.function %} {%- set tool_call = tool_call.function %} {%- endif %} {{- '<tool_call>n{\"name\": \"' }} {{- tool_call.name }} {{- '\", \"arguments\": ' }} {%- if tool_call.arguments is string %} {{- tool_call.arguments }} {%- else %} {{- tool_call.arguments tojson }} 28 {%- endif %} {{- '}n</tool_call>' }} {%- endfor %} {%- endif %} {{- '<im_end>n' }} {%- elif message.role == \"tool\" %} {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %} {{- '<im_start>user' }} {%- endif %} {{- 'n<tool_response>n' + content + 'n</tool_response>' }} {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %} {{- '<im_end>n' }} {%- endif %} {%- endif %} {%- endfor %} {%- if add_generation_prompt %} {{- '<im_start>assistantn' }} {%- endif %} ''' Qwen3_reasoning_chat_template = ''' {%- if tools %} {{- '<im_start>systemn' }} {%- if messages[0].role == 'system' %} {{- messages[0].content + 'nn' }} {%- endif %} {{- \"# ToolsnnYou may call one or more functions to assist with the user (cid:44) query.nnYou are provided with function signatures within <tools></tools> XML tags:n<tools>\" }} (cid:44) {%- for tool in tools %} {{- \"n\" }} {{- tool tojson }} {%- endfor %} {{- \"n</tools>nnFor each function call, return json object with function name (cid:44) and arguments within <tool_call></tool_call> XML tags:n<tool_call>n{\"name\": <function-name>, \"arguments\": <args-json-object>}n</tool_call><im_end>n\" }} (cid:44) {%- else %} {%- if messages[0].role == 'system' %} {{- '<im_start>systemn' + messages[0].content + '<im_end>n' }} {%- endif %} {%- endif %} {%- set ns = namespace(multi_step_tool=true, last_query_index=messageslength - 1) %} {%- for message in messages[::-1] %} {%- set index = (messageslength - 1) - loop.index0 %} {%- if ns.multi_step_tool and message.role == \"user\" and message.content is string (cid:44) (cid:44) and not (message.content.startswith('<tool_response>') and message.content.endswith('</tool_response>')) %} {%- set ns.multi_step_tool = false %} {%- set ns.last_query_index = index %} {%- endif %} {%- endfor %} 29 {%- for message in messages %} {%- if message.content is string %} {%- set content = message.content %} {%- else %} {%- set content = '' %} {%- endif %} {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %} {{- '<im_start>' + message.role + 'n' + content + '<im_end>n' }} {%- elif message.role == \"assistant\" %} {%- set reasoning_content = '' %} {%- if message.reasoning_content is string %} {%- set reasoning_content = message.reasoning_content %} {%- elif '</think>' in content %} {%- set reasoning_content = (cid:44) content.split('</think>')[0].rstrip('n').split('<think>')[-1]. lstrip('n') %} (cid:44) {%- set content = content.split('</think>')[-1].lstrip('n') %} {%- endif %} {{- '<im_start>' + message.role + 'n<think>n' + reasoning_content.strip('n') + 'n</think>nn' + content.lstrip('n') }} (cid:44) {%- if message.tool_calls %} {%- for tool_call in message.tool_calls %} {{- 'n' }} {%- if tool_call.function %} {%- set tool_call = tool_call.function %} {%- endif %} {{- '<tool_call>n{\"name\": \"' }} {{- tool_call.name }} {{- '\", \"arguments\": ' }} {%- if tool_call.arguments is string %} {{- tool_call.arguments }} {%- else %} {{- tool_call.arguments tojson }} {%- endif %} {{- '}n</tool_call>' }} {%- endfor %} {%- endif %} {{- '<im_end>n' }} {%- elif message.role == \"tool\" %} {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %} {{- '<im_start>user' }} {%- endif %} {{- 'n<tool_response>n' + content + 'n</tool_response>' }} {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %} {{- '<im_end>n' }} {%- endif %} {%- endif %} {%- endfor %} 30 {%- if add_generation_prompt %} {{- '<im_start>assistantn<think>nn</think>nn' }} {%- endif %} '''"
        },
        {
            "title": "D Detailed Results for Each Dataset",
            "content": "We conduct comprehensive and detailed evaluation of the tool-use capabilities of various LLMs on self-constructed in-domain dataset and three publicly available out-of-domain datasets. The detailed results for each dataset are presented from Tables 6 to Table 9. Single-Hop Parallel-Single-Hop Multi-Hop Parallel-Multi-Hop Avg Solve-P Solve-R Solve-F1 Solve-P Solve-R Solve-F Solve-P Solve-R Solve-F1 Solve-P Solve-R Solve-F Solve-P Solve-R Solve-F1 Family Version Gemini Gemini Claude GPT GPT 2.5-Flash 2.5-Pro 4.0-Sonnet 3.5-Turbo 4o Qwen2.5-32B Instruct Qwen2.5-72B Instruct Qwen3-32B Qwen3-32B Non-Reasoning Reasoning 37.00 46.67 25.17 23.50 18.83 21.29 27.07 29.97 18. Qwen2.5-7B 19.83 Instruct FTRL-Reinforce++ 27.67 41.00 FTRL-GRPO 25.50 Qwen2.5-14B Instruct Qwen3-8B Qwen3-14B Qwen3-8B Qwen3-14B FTRL-Reinforce++ 34.00 36.00 FTRL-GRPO Non-Reasoning 31.15 FTRL-Reinforce++ 33.00 43.00 FTRL-GRPO Non-Reasoning 34.73 FTRL-Reinforce++ 36.92 42.33 FTRL-GRPO Reasoning 23.33 FTRL-Reinforce++ 28.00 25.67 FTRL-GRPO Reasoning 25.17 FTRL-Reinforce++ 34.83 31.67 FTRL-GRPO 20.00 30.00 36.00 32.00 26.00 26.00 34.00 44.00 28.00 26.00 30.00 42.00 26.00 36.00 38.00 40.00 40.00 50.00 42.00 50.00 52.00 28.00 36.00 30.00 32.00 42.00 40.00 18.00 27.67 26.73 25.80 19.47 22.50 29.00 33.31 20.93 21.47 28.33 40.00 24.13 34.67 36.67 33.29 35.00 45.00 36.67 40.35 45.33 24.67 30.33 27.00 27.13 35.47 34.33 40.67 37.22 35.14 25.14 28.44 33.60 24.17 25.39 22.36 27.30 31.37 37.67 24.85 34.67 40.00 26.58 35.73 35.92 29.77 33.59 37.73 25.59 31.60 37.83 25.32 31.96 30.18 Closed-Source LLMs 68.80 59.57 54.33 19.21 35.34 30.50 32.81 35.75 27.49 30.48 32.00 41.00 53.00 33.33 40.33 15.26 18.11 56.02 23.37 43.24 Open-Source LLMs (32B - 72B) 43.33 36.67 46.00 40. 25.55 32.51 33.43 30.87 36.85 27.37 30.09 25.81 33.55 45.72 42.36 39.11 Open-Source LLMs (7B - 14B) 29.67 34.33 40.00 30.67 38.33 41.33 37.67 40.33 46.33 44.00 51.33 48.00 37.67 36.67 44.67 40.33 42.00 43.00 26.81 31.93 38.53 26.71 36.13 40.47 28.69 37.23 39.31 32.32 37.89 41.35 24.93 31.99 39.07 29.05 33.78 34.57 25.59 44.74 47.26 32.58 55.20 52.07 8.86 39.28 50.15 21.89 48.62 51.84 25.43 36.84 43.07 33.08 42.67 44.64 25.46 43.08 47.76 33.31 48.59 56.39 16.12 45.22 60.21 25.75 62.47 58.43 37.84 43.16 37.41 37.74 43.03 49.55 16.89 20.75 45.09 19.14 31.48 28.03 36.52 35.95 32.28 24.09 42.03 47.11 31.86 50.42 53.53 10.56 40.94 53.61 21.31 52.57 54.02 28.00 38.69 37.49 30.79 41.32 46.00 64.65 63.64 54.67 35.23 51. 40.06 42.98 44.01 44.20 37.03 62.66 65.35 50.30 74.16 66.62 18.12 59.81 56.67 26.75 57.12 65.71 36.76 56.02 56.11 44.07 67.15 60.19 25.28 38.72 79.24 28.45 58.46 48.32 57.26 52.60 54.53 29.63 58.04 59.71 48.57 65.86 66.53 29.05 63.94 66.82 38.56 73.38 68.42 50.00 58.96 48.18 50.86 65.40 58.12 27.59 38.68 62.77 30.22 52.13 42.29 47.97 46.00 46.19 31.52 59.16 61.47 48.33 69.07 66.13 21.37 61.35 60.24 29.58 62.16 66.56 40.47 56.61 50.75 44.85 65.59 58.52 52.78 51.77 42.33 25.77 33.43 30.12 31.68 33.20 28.94 27.44 41.61 47.82 33.31 49.51 48.67 21.18 41.96 46.44 28.29 44.06 49.41 27.78 38.11 40.67 31.91 44.15 41.67 23.13 31.96 56.07 29.29 42.01 37.80 43.41 46.24 40.41 27.69 41.36 47.37 34.64 47.20 50.56 30.71 47.37 55.84 37.58 59.30 56.71 38.38 43.70 40.06 40.23 48.11 47.67 23.24 29.98 42.59 25.66 33.39 32.42 35.22 36.34 31.30 25.97 40.36 46.78 32.76 47.57 49.20 23.48 43.63 49.54 29.97 48.24 51.82 29.52 39.40 38.58 32.96 44.04 43.36 Table 6 Detailed evaluation results on our own dataset across different scenarios. The best result for each scenario is bolded, and the second-best is underlined. Performance improvements over the base model after training with our method (i.e., FTRL-Reinforce++ and FTRL-GRPO) are indicated with , while performance declines are marked with . 32 Family Version Direct Mandatory Free AC AC AC Gemini Gemini Claude GPT GPT 2.5-Flash 2.5-Pro 4.0-Sonnet 3.5-Turbo 4o Closed-Source LLMs 49.55 54.07 55.48 13.37 18.99 35.18 38.99 49.35 31.56 44.32 Open-Source LLMs (32B - 72B) Qwen2.5-32B Instruct Qwen2.5-72B Instruct Qwen3-32B Qwen3-32B Non-Reasoning Reasoning 14.57 12.76 13.77 38.89 24.22 43.02 47.54 55.58 Qwen2.5-7B Open-Source LLMs (7B - 14B) 8. Instruct FTRL-Reinforce++ 8.74 6.93 FTRL-GRPO 14.07 Qwen2.5-14B Instruct Qwen3-8B Qwen3-14B Qwen3-8B Qwen3-14B FTRL-Reinforce++ 13.37 14.37 FTRL-GRPO Non-Reasoning 20.30 FTRL-Reinforce++ 23.02 20.80 FTRL-GRPO 12.56 Non-Reasoning FTRL-Reinforce++ 14.57 13.07 FTRL-GRPO Reasoning 32.66 FTRL-Reinforce++ 34.17 31.76 FTRL-GRPO Reasoning 36.68 FTRL-Reinforce++ 38.89 35.48 FTRL-GRPO 11.36 30.95 44.32 35.48 47.94 37.49 34.07 46.23 48.74 29.55 50.15 50.25 46.43 48.94 51.06 42.21 50.75 47.34 33.37 42.91 57.09 32.26 45.63 20.30 35.08 44.72 55.88 15.68 30.65 37.79 27.84 47.94 28.04 31.26 44.12 47.24 30.45 50.25 51.46 43.02 40.60 38.69 41.41 44.72 42.11 Avg AC 39.37 45.32 53.97 25.73 36.31 19.70 30.29 35.34 50.12 11.99 23.45 29.68 25.80 36.42 26.63 28.54 37.79 38.93 24.19 38.32 38.26 40.70 41.24 40.50 40.10 44.79 41.64 Table 7 Detailed evaluation results on ToolHop across different scenarios. The best result for each scenario is bolded, and the second-best is underlined. Performance improvements over the base model after training with our method (i.e., FTRL-Reinforce++ and FTRL-GRPO) are indicated with , while performance declines are marked with . 33 Family Version Retail Airline Avg Pass^1 Pass^1 Pass^1 Closed-Source LLMs 46."
        },
        {
            "title": "Gemini\nGemini\nClaude\nGPT\nGPT",
            "content": "2.5-Flash 2.5-Pro 4.0-Sonnet 3.5-Turbo 4o 34.00 42.00 50.00 12.00 34.00 52.17 50.43 18.26 40.87 Open-Source LLMs (32B - 72B) Qwen2.5-32B Instruct Qwen2.5-72B Instruct Qwen3-32B Qwen3-32B Non-Reasoning Reasoning 27.83 36.52 34.78 40.00 16.00 32.00 20.00 22.00 Open-Source LLMs (7B - 14B) Qwen2.5-7B Instruct 7.83 FTRL-Reinforce++ 7.83 7.83 FTRL-GRPO 23. Qwen2.5-14B Instruct Qwen3-8B Qwen3-14B Qwen3-8B Qwen3-14B FTRL-Reinforce++ 35.65 40.87 FTRL-GRPO Non-Reasoning 18.26 FTRL-Reinforce++ 33.91 28.70 FTRL-GRPO 23.48 Non-Reasoning FTRL-Reinforce++ 25.22 36.52 FTRL-GRPO Reasoning 20.87 FTRL-Reinforce++ 33.04 27.83 FTRL-GRPO Reasoning 21.74 FTRL-Reinforce++ 32.17 37.39 FTRL-GRPO 4.00 16.00 6.00 10.00 18.00 10.00 2.00 10.00 18.00 4.00 10.00 12.00 12.00 32.00 30.00 16.00 22.00 26.00 40.04 47.09 50.22 15.13 37.43 21.91 34.26 27.39 31.00 5.91 11.91 6.91 16.74 26.83 25.43 10.13 21.96 23.35 13.74 17.61 24.26 16.43 32.52 28.91 18.87 27.09 31.70 Table 8 Detailed evaluation results on τ -bench across different scenarios. The best result for each scenario is bolded, and the second-best is underlined. Performance improvements over the base model after training with our method (i.e., FTRL-Reinforce++ and FTRL-GRPO) are indicated with , while performance declines are marked with . 34 Clean Slight Medium Heavy Union TS PI CF TS PI CF TS PI CF TS PI CF TS PI CF 58.10 49.52 34.29 75.24 85.71 81.90 80.00 77.14 57. Family Version Gemini Gemini Claude GPT GPT 2.5-Flash 2.5-Pro 4.0-Sonnet 3.5-Turbo 4o Qwen2.5-32B Instruct Qwen2.5-72B Instruct Qwen3-32B Qwen3-32B Non-Reasoning Reasoning Qwen2.5-7B Qwen2.5-14B Instruct 78.10 76.19 79.05 Instruct FTRL-Reinforce++ 79.05 80.95 FTRL-GRPO 83.81 FTRL-Reinforce++ 83.81 FTRL-GRPO Non-Reasoning FTRL-Reinforce++ 83.81 82.86 FTRL-GRPO Non-Reasoning 81.90 FTRL-Reinforce++ 82.86 78.10 FTRL-GRPO Reasoning 62.86 FTRL-Reinforce++ 59.05 60.00 FTRL-GRPO Reasoning 64.76 FTRL-Reinforce++ 67.62 67.62 FTRL-GRPO Qwen3-8B Qwen3-14B Qwen3-8B Qwen3-14B 37.14 45.71 31.43 33.33 50.48 42.86 26.67 40.00 36.19 31.43 48.57 34.29 47.62 55.24 39.05 35.24 42.86 47.62 38.10 45.71 43.81 36.19 40.95 41.90 40.95 43.81 40. 25.71 30.48 18.10 19.05 31.43 22.86 18.10 22.86 18.10 16.19 30.48 20.95 28.57 35.24 23.81 20.00 25.71 27.62 22.86 26.67 22.86 23.81 25.71 23.81 20.00 26.67 20.95 56.67 50.00 45.24 65.71 84.29 81.43 79.05 78.10 60. 73.81 72.86 77.62 77.14 80.00 76.19 80.95 81.43 83.33 82.86 81.43 80.00 51.43 57.14 57.14 61.90 57.62 62.86 40.00 44.29 40.00 27.14 51.90 42.38 29.05 45.71 38.10 26.19 42.38 31.90 40.95 49.05 38.57 40.00 46.67 48.10 39.52 44.29 43.81 32.86 43.81 42.86 40.00 38.57 41.90 Closed-Source LLMs 26.19 25.24 24.76 16.67 33.81 62.86 58.10 36.19 73.33 84.76 39.52 52.86 32.86 36.67 57. 25.24 34.76 21.43 21.43 36.19 Open-Source LLMs (32B - 72B) 22.38 18.57 30.48 24.76 80.48 76.67 78.57 60.00 38.10 28.10 49.52 37.62 22.38 19.05 26.67 24. Open-Source LLMs (7B - 14B) 16.19 25.71 19.52 25.24 31.90 23.33 22.38 25.71 25.71 21.43 27.14 23.33 21.43 24.76 25.24 20.48 20.95 26.19 73.33 73.81 75.24 76.19 76.67 77.62 80.48 81.90 82.38 82.86 83.81 80.00 55.71 56.67 54.76 60.48 61.90 67.14 29.52 41.90 32.38 42.86 49.05 39.05 37.14 44.76 44.76 40.95 47.14 43.81 38.57 41.43 43.33 38.10 40.00 43.81 14.76 28.10 20.48 23.33 30.48 24.29 26.67 29.05 32.38 20.95 26.19 24.29 20.00 25.71 26.19 25.24 20.95 23.81 42.86 38.10 38.10 58.10 64.76 60.95 55.71 60.95 44. 59.05 63.33 64.76 57.14 60.00 60.00 65.71 68.10 66.19 62.38 67.62 64.29 46.67 49.05 49.52 50.95 52.38 51.90 26.67 28.10 29.52 23.33 40.00 30.48 21.90 34.29 29.52 23.81 37.62 30.00 30.48 33.81 30.95 32.86 36.67 38.10 30.95 37.62 34.29 29.52 30.00 30.00 31.43 30.48 31.43 17.62 18.57 17.14 13. 25.71 18.10 13.81 21.43 16.67 13.81 23.33 19.52 17.14 21.43 17.62 20.00 23.33 21.90 20.00 24.29 20.00 14.76 18.10 16.67 20.48 19.52 18.10 47.62 49.52 44.76 56.19 74.29 72.38 69.52 71.43 46.67 67.62 65.71 71.43 64.76 64.76 67.62 71.43 75.24 73.33 69.52 73.33 74.29 53.33 45.71 51.43 48.57 55.24 53.33 31.43 40.95 40.95 28.57 47.62 30.48 25.71 39.05 27.62 25.71 34.29 25.71 29.52 39.05 33.33 36.19 41.90 44.76 32.38 38.10 39.05 34.29 31.43 38.10 30.48 33.33 32.38 19.05 28.57 27.62 16.19 30. 14.29 16.19 22.86 17.14 16.19 22.86 14.29 12.38 21.90 19.05 21.90 25.71 27.62 18.10 21.90 21.90 21.90 17.14 22.86 15.24 20.00 18.10 Avg TS 53.62 49.05 39.72 65.71 78. 75.43 72.19 73.24 53.90 70.38 70.95 74.00 71.81 73.05 71.52 75.52 78.10 77.62 75.90 77.81 75.33 54.00 53.52 54.57 57.33 58.95 60.57 PI CF 34.95 42.38 34.95 29.81 49. 36.86 26.29 41.71 33.81 27.33 40.95 30.86 38.29 45.24 36.19 36.29 42.57 44.67 36.38 42.57 40.95 34.29 37.52 39.24 36.19 37.24 38.10 22.76 27.52 21.81 17.33 31.52 20.00 17.14 24.86 20.19 15.43 26.10 18.95 21.33 28.19 21.62 22.19 25.90 27.05 20.67 25.24 22.48 20.38 22.29 22.95 20.29 21.62 21.43 Table 9 Detailed evaluation results on RoTBench across different scenarios. The best result for each scenario is bolded, and the second-best is underlined. Performance improvements over the base model after training with our method (i.e., FTRL-Reinforce++ and FTRL-GRPO) are indicated with , while performance declines are marked with ."
        },
        {
            "title": "E Case Study",
            "content": "To clearly demonstrate the effectiveness of our method in improving tool-use performance in LLMs, this section presents and analyzes several representative cases. Specifically, Table 10 and Table 11 compare the performance of Qwen2.5-7B-Instruct with Qwen2.5-7B-FTRL-GRPO, and Qwen3-8B-Reasoning with Qwen38B-Reasoning-FTRL-Reinforce++, highlighting the advantages of our approach in enhancing contextual understanding and decision-making capabilities. Furthermore, Table 12 compares Qwen3-8Bs performance in reasoning versus non-reasoning modes, reinforcing our claim from Section 5 that existing reasoning modes are insufficient for effectively distinguishing between different tools. Qwen2.5-7B-Instruct Qwen2.5-7B-FTRL-GRPO Question: Who succeeded Emir Nawaf Al-Ahmad Al-Jaber Al-Sabah after his death on December 16, 2023? Answer: Mishal Al-Ahmad Al-Jaber Al-Sabah Called Tool: political_successor_finder Nawaf Filled Parameters: {political_figure: Emir, Al-Ahmad Al-Jaber Al-Sabah, role: date_of_transition: country: Kuwait, political_system: monarchy, government_level: national} Called Tool: political_successor_finder Emir Filled Parameters: {political_figure: Nawaf Al-Ahmad Al-Jaber Al-Sabah, role: Emir, date_of_transition: 2023-12-16, country: Kuwait, political_system: monarchy} 2023-12-16, Environment Feedback: No successor found for the given political figure and date. Environment Feedback: Mishal Al-Ahmad AlJaber Al-Sabah. It seems there is no recorded Final Output: successor for Emir Nawaf Al-Ahmad Al-Jaber Al-Sabah as of December 16, 2023. This might be due to the fact that the transition has not been officially announced or documented yet. Lets try again with slightly later date to see if any information becomes available. Final Output: The successor to Emir Nawaf AlAhmad Al-Jaber Al-Sabah after his death on December 16, 2023, is Mishal Al-Ahmad Al-Jaber Al-Sabah. Analysis: Qwen2.5-7B-Instruct failed to accurately extract information from the user input due to contextual oversights, leading to incomplete parameter filling and, ultimately, an incorrect response. In contrast, the FTRL-GRPO variant maintained full contextual fidelity and successfully answered the question. Table 10 Performance comparison between Qwen2.5-7B-Instruct and Qwen2.5-7B-FTRL-GRPO. 36 Qwen3-8B-Reasoning Qwen3-8B-Reasoning-FTRL-Reinforce++ Question: How many seats did the SNS coalition win in the December 17, 2023 Serbian parliamentary election? Answer: 128 Called Tool: election_results_analyzer {election_date: Called Tool: election_results_analyzer Filled Parameters: {election_date: 2023-1217, country: Serbia, political_entity: SNS, election_type: parliamentary, data_source: media} country: 2023Filled Parameters: 12-17, political_entity: Serbia, SNS coalition, election_type: parliamentary, data_source: include_contested: false} official, Environment Feedback: No data available for the specified parameters. Environment Feedback: 128 Final Output: The provided tools do not have access to data for the specified election date (December 17, 2023) or the SNS coalitions results in Serbia. This could indicate an error in the date, as Serbias last parliamentary election was held on June 21, 2022. For accurate results, please verify the election date or consult official Serbian electoral commission sources. Final Output: The SNS coalition won 128 seats in the December 17, 2023 Serbian parliamentary election. Analysis: Qwen3-8B-Reasoning struggles to correctly interpret user input and distinguish between entities, leading to incorrect parameter entries and failure to retrieve accurate information. In contrast, the FTRL-Reinforce++ version enhances the models contextual understanding and decision-making ability, allowing it to input the correct parameters and resolve the question effectively. Table 11 Performance comparison between Qwen3-8B-Reasoning and Qwen3-8B-Reasoning-FTRL-Reinforce++. 37 Qwen3-8B-Reasoning Qwen3-8B-Non Reasoning Question: Where did clashes occur between Palestinians and Israeli police on April 5, 2023? Answer: Al-Aqsa Mosque Called Tool: event_locator Thought: ... the event_locator seems appropriate because its for locating geopolitical events ... Maybe the tool can handle general location, like Palestinian territories or Israel, but Im not sure. ... Filled Parameters: {event_type: clash, date: 2023-04-05, location: Palestinian territories, verify_sources: true, source_credibility_score: 0.8, event_severity: high} Called Tool: event_locator Filled Parameters: {keywords: clashes Palestinians Israeli police 2023-04-05, date_range: {start_date: 2023-04-05, end_date: 2023-0405}, topic: conflict, include_context: true, location: Palestinian territories, significance: national, source_reliability: high} Environment matching the criteria. Feedback: No events found Final Output: No information was found regarding clashes between Palestinians and Israeli police on It is April 5, 2023, using the available tools. possible that no significant events occurred on this date in the specified context, or the data may not be recorded in the sourced databases. For the most accurate and up-to-date information, consulting news archives or official reports from the period would be recommended. Al-Aqsa Mosque. Environment Clashes occurred between Palestinians and Israeli police. Feedback: Final Output: The clashes between Palestinians and Israeli police on April 5, 2023, occurred at the Al-Aqsa Mosque. Analysis: In reasoning mode, although Qwen3-8B correctly identified the appropriate tool to use, its reasoning of parameter values lacked accuracy, leading to incorrect entries and failure to retrieve the correct information. This limitation arises because the models reasoning process is primarily optimized for mathematical tasks, making it poorly suited to the specific demands of tool-use scenarios. Table 12 Performance comparison between reasoning and non reasoning modes of Qwen3-8B."
        }
    ],
    "affiliations": [
        "ByteDance Seed",
        "Fudan University"
    ]
}