{
    "paper_title": "TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding",
    "authors": [
        "Boshen Xu",
        "Zihan Xiao",
        "Jiaze Li",
        "Jianzhong Ju",
        "Zhenbo Luo",
        "Jian Luan",
        "Qin Jin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce TimeViper, a hybrid vision-language model designed to tackle challenges of long video understanding. Processing long videos demands both an efficient model architecture and an effective mechanism for handling extended temporal contexts. To this end, TimeViper adopts a hybrid Mamba-Transformer backbone that combines the efficiency of state-space models with the expressivity of attention mechanisms. Through this hybrid design, we reveal the vision-to-text information aggregation phenomenon, where information progressively flows from vision tokens to text tokens across increasing LLM depth, resulting in severe vision token redundancy. Motivated by this observation, we propose TransV, a token information transfer module that transfers and compresses vision tokens into instruction tokens while maintaining multimodal understanding capabilities. This design enables TimeViper to process hour-long videos exceeding 10,000 frames. Extensive experiments across multiple benchmarks demonstrate that TimeViper competes with state-of-the-art models while extending frame numbers. We further analyze attention behaviors of both Mamba and Transformer layers, offering new insights into hybrid model interpretability. This work represents an initial step towards developing, interpreting, and compressing hybrid Mamba-Transformer architectures."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 2 5 9 5 6 1 . 1 1 5 2 : r TimeViper: Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding Boshen Xu1 Zihan Xiao1 Jiaze Li2 Jianzhong Ju2 Zhenbo Luo2 Jian Luan2 Qin Jin1 1 AIM3 Lab, Renmin University of China 2 MiLM Plus, Xiaomi Inc. Project Page: https://xuboshen.github.io/TimeViper/ Figure 1. We present TimeViper, hybrid Mamba-Transformer vision-language model for efficient long video understanding. We reveal the severe vision token redundancy and vision-to-text information aggregation phenomenon in hybrid models. To this end, we introduce TransV, the first token-transfer module that compresses vision tokens into text tokens inside the LLM, enabling the model to process over 10,000 frames. Benefitting from the Mamba layers O(n) computation and O(1) cache cost, TimeViper generates 40.1% more tokens per second than Qwen3 [97] when processing 32k input tokens (approximately 2k frames at 16 tokens per frame) and producing 1k output tokens with batch size 32. TimeViper delivers performance competitive with Transformer-based MLLMs on public benchmarks, including multi-choice QA on VideoMME [29] (vs. Video-XL [73]), temporal video grounding on Charades [74] (vs. VTimeLLM [36]), video detailed captioning on VDC [14] (vs. AuroraCap [14]), and hour-long video understanding on LVBench [85] (vs. Gemini-1.5-Pro [80])."
        },
        {
            "title": "Abstract",
            "content": "We introduce TimeViper, hybrid vision-language model designed to tackle challenges of long video understanding. Processing long videos demands both an efficient model architecture and an effective mechanism for handling extended temporal contexts. To this end, TimeViper adopts hybrid Mamba-Transformer backbone that combines the efficiency of state-space models with the expressivity of attention mechanisms. Through this hybrid design, we reveal the vision-to-text information aggregation phenomenon, where information progressively flows from vision tokens to text tokens across increasing LLM depth, resulting in severe vision token redundancy. Motivated by this observation, we propose TransV, token information transfer module that transfers and compresses vision tokens into instruction tokens while maintaining multimodal understanding capabilities. This design enables TimeViper to process hour-long videos exceeding 10,000 frames. Extensive experiments across multiple benchmarks demonstrate that TimeViper competes with state-of-the-art models while Corresponding author; Project lead; Equal contribution. spacThis work was done during their internship at Xiaomi. extending frame numbers. We further analyze attention behaviors of both Mamba and Transformer layers, offering new insights into hybrid model interpretability. This work represents an initial step towards developing, interpreting, and compressing hybrid Mamba-Transformer architectures. 1. Introduction Understanding long videos is an essential yet long-standing challenge in computer vision, holding great potential for applications across video platforms [79, 94], household scenarios [32, 66, 98], and embodied agents [10, 109]. Recent advances in multimodal large language models (MLLMs) [21] have made general long video understanding increasingly feasible. Nevertheless, existing models still struggle to achieve balance between effectiveness and efficiency when dealing with extended video contexts. We argue that building truly capable long-video understanding model requires addressing two key challenges: constructing an efficient MLLM backbone, and handling redundancy in long-context processing. Most prior works [50, 73] adopt Transformer-based 1 LLMs as the backbone, owing to their strong reasoning and language understanding capabilities. However, the quadratic computational complexity of attention makes them inherently inefficient for long-context modeling. To improve efficiency, recent efforts have explored linearized architectures such as Mamba [23, 33], which replace attention with state-space models for linear-time inference. Despite their efficiency advantages, these models often depend heavily on distillation from Transformer-based models [52, 53] or suffer from limited performance on complex multimodal tasks [86, 115]. Encouragingly, new generation of hybrid Mamba-Transformer LLMs [9, 25, 44, 45, 68, 122] have recently emerged, combining the efficiency of statespace models with the expressivity of attention. Inspired by these developments, we explore hybrid architecture tailored for long video understanding that inherits the complementary strengths of both model families. Another major bottleneck arises from the redundancy in long video sequences. For example, one-hour video sampled at 1 frame per second, with each frame encoded into 768 vision tokens [107], produces approximately 2.7 million tokens, exceeding even the million-token context limit of Gemini [21]. Thus, reducing contextual length is crucial for scalable long-video modeling. Most prior works [20, 39, 51, 58, 72, 93] address this issue by performing vision token compression and merging at the projection layer before feeding the tokens into the LLM, leveraging redundancy within ViT representations [11]. However, for long videos, the LLM itself remains the primary computational bottleneck of an MLLM, as it processes the compressed sequences through billions of parameters. Recent works have attempted to alleviate this by internal token dropping [18, 76, 92, 112] or compression [70, 73] within the LLM, typically guided by attention scores to identify redundant vision tokens. Yet, developing such strategies for hybrid architectures remains largely unexplored and challenging, where the mechanism for storing token information could differ fundamentally from Transformers. In this work, we propose TimeViper, an efficient hybrid MLLM designed for long video understanding. Through information exchange analysis within the LLM, we identify vision-to-text information aggregation phenomenon. As layer depth increases, information from vision tokens progressively converges into text tokens, across both instruction-centric tasks (e.g., video QA) and vision-centric tasks (e.g.,video captioning). At deeper layers, even removing all vision tokens causes no performance degradation, suggesting severe token redundancy within the model. Motivated by this observation, we introduce TransV, token compression mechanism within the LLM. TransV progressively shortens the context length by transferring partial vision tokens into instruction tokens via gated cross-attention, preserving critical visual information while eliminating redundancy. Extensive experiments demonstrate that TimeViper achieves promising performance to Transformer-based MLLMs across long video understanding benchmarks, including multi-choice video QA, temporal video grounding, and detailed video captioning. Our main contributions are summarized as follows: We introduce TimeViper, hybrid Mamba-Transformer vision-language model for efficient long video understanding, featuring internal LLM token compression that enables processing over 10,000 frames. We discover the phenomenon of vision-to-text aggregation and vision token redundancy in hybrid architectures, and propose TransV, mechanism that eliminates visual redundancy through explicit token information transfer. Extensive experiments demonstrate that TimeViper achieves comparable performance to Transformer-based MLLMs while accelerating inference speed. 2. Related Works MLLM for long video understanding. Long video understanding [29, 43, 60, 87, 104, 108] has long been challenging problem in computer vision. Towards this goal, MLLMs emerge as promising approach, but they struggle to process long videos while comprehending content. Existing methods aim to balance computational efficiency and performance, typically categorized into subsampling or compression strategies. Subsampling strategies [28, 31, 35, 63, 100, 101] shorten video length by using language queries to retrieve the most relevant video segments. For instance, VideoAgent [87] iteratively selects frames and generates captions for them using vision-language models, which are then provided to an LLM to answer the question. Meanwhile, compression strategies condense redundant video embeddings into more compact representations. Most works [20, 22, 3941, 47, 51, 58, 69, 72, 93, 99, 120] merge visual features before feeding them into the LLM. For instance, LLaMA-VID [51] employs dual-token strategy that compresses each frame into two tokens. However, these methods fail to resolve the computational bottleneck of LLM. To further improve efficiency, another line of work drops [18, 50, 92] or compresses [3, 70, 73, 102, 112] tokens within LLMs. For example, PDrop [92] progressively prunes vision tokens across LLM layers. Although efficient, dropping tokens based on attention scores [5, 121] can cause irreversible information loss. While token dropping is convenient and can be applied in training-free manner, token compression avoids information loss. VoCoLLaMA [102] and Video-XL [73] suggest compressing vision tokens into new special tokens. Nevertheless, as existing methods rely heavily on Transformers, the token compression approaches in hybrid MLLM remain unexplored. In this work, we pioneer this direction by introducing TimeViper, hybrid MLLM with token information 2 Figure 2. Illustration of TimeViper, our proposed hybrid MLLM for long video understanding. The model consists of ViT visual encoder, projector with token merging, and hybrid Mamba-Transformer LLM equipped with TransV. The token merging [11] compresses each frame into 16 vision tokens. Inside the LLM, TransV transfers information from redundant vision tokens to instruction tokens to reduce the number of vision tokens. Specifically, TransV uniformly drops vision tokens in shallow layers and removes low-attention vision tokens in deeper layers. The compression module is implemented through Gated Cross-Attention mechanism [3] with adaptive learnable weights. Note that TransV is illustrated before the attention layer for clarity, though it may be applied before any layer in practice. transfer module within LLM named TransV to compress vision tokens into instruction tokens. State-space model for visual perception. Transformers attention with quadratic computational cost remains fundamental bottleneck for efficiency. Linearized architectures [33, 64] have evolved again in NLP community, aiming to reduce complexity to linear time and elimSome inate the need for KV-cache during inference. recent efforts are pushing LLMs with linearized modules or Mamba-Transformer hybrid architectures, such as Nemotron-Nano [9], Samba [68], and Hymba [25]. Inspired by these works, researchers have begun to explore linearized architectures for computer vision tasks like image [27, 103, 118], video [48], and 3D [56] understanding. The rise of linearized LLM has also inspired efficient multimodal models [53, 67, 70, 86, 93, 115]. However, since images and short clips involve relatively limited sequence lengths, the advantage from linearized architectures is still controversial problem [103]. In contrast, long video understanding naturally demands models capable of processing extremely long contexts, posing far stricter efficiency requirements. Recently, AuroraLong [93] combines ViT with RWKV6 [65] and employs token merging in projector to compress vision tokens. In this work, TimeViper is the first hybrid MLLM that performs token compression within the hybrid LLM, achieving promising performance among 7B-sized MLLMs while maintaining high efficiency. 3. Method We propose TimeViper, hybrid Mamba-Transformer vision-language model equipped with an internal LLM compression module, TransV, for long video understanding. Our method is built upon two key ideas: 1) hybrid MLLM construction, which integrate the efficiency of state-space models with the expressivity of attention mechanisms, and 2) performing LLM token compression through vision-totext information transfer. We first introduce the hybrid model structure in Section 3.1. Next, in Section 3.2, we analyze the token information exchange between vision and text tokens and present our compression module TransV. Finally, we describe the training strategy in Section 3.3. 3.1. Model Architecture Our model follows the standard multimodal design [54] and consists of three components: visual encoder (ViT) [107], projector, and hybrid Mamba-Transformer LLM [9]. The LLM backbone includes 27 Mamba-2 [23] layers, 4 self-attention layers, and 25 MLP layers. Following prior work [50, 120], we apply token merging (ToMe) [11] in the projection layer to reduce intra-frame redundancy. Given long video with corresponding textual instruction, the ViT encodes video frames, and the projector with ToMe produces sequence of compressed vision tokens X0 RT0D, while the instruction is tokenized into text instruction tokens X1 RT1D, where typically T0 T1, and is the hidden dimension. The LLM processes the concatenated multimodal input = [X0, X1] RT of sequence length and generates response tokens . The hybrid backbone integrates Mamba-2 and selfattention layers, each contributing complementary capabilities: the Mamba-2 layer is mainly responsible for sequence position modeling, encoding historical sequence in3 Figure 3. Comparison of information blocking to illustrate the vision-to-text information aggregation phenomenon in hybrid MLLMs. For instruction-centric tasks (e.g., multi-choice video QA), information is first aggregated from vision tokens to instruction tokens, which are then used for response generation. In contrast, for vision-centric tasks (e.g., detailed video captioning), vision tokens directly contribute to response generation. formation into fixed-size implicit hidden memory through forgetting and memorization mechanisms, while the selfattention layer preserves the entire history of the sequence and performs retrieval and querying based on the importance of the tokens. Mamba-2 Layer. Mamba-2 layer is built around core state-space model (SSM) block, which recurrently maintains compact hidden state summarizing past information. Let xt denote the input at step t, and ht RN the hidden memory. The SSM update is defined as: ht = Atht1 + Btxt yt = ht (1) where At, Bt, and Ct are discretized SSM parameters [23]. This mechanism encodes temporal dependencies via learnable decay and gating dynamics, enabling efficient information propagation over long sequences. Self-Attention Layer. In contrast, the self-attention layer directly models token interactions: = SoftMax(L QK ) (2) where [Q, K, ] = [WQ, WK, WV ]X, and WQ, WK, WV are learnable parameters. is the causal attention mask. By integrating these two mechanisms, the hybrid LLM retains the contextual expressivity of attention while benefiting from the efficiency of SSMs. 3.2. Token Information Transfer To analyze information dynamics within hybrid MLLMs, given the lack of hybrid MLLMs, we first train hybrid model on open-source datasets for subsequent experiments and analyses. To ensure the generalizability of our experiments on downstream tasks, we conduct the following analyses on high-quality and widely used benchmarks, including instruction-centric tasks such as multi-choice video QA (MCQ) on VideoMME [29] and temporal video grounding (TVG) on Charades [74], and vision-centric tasks such as video detailed captioning (VDC) on VDC [14]. We employ standard evaluation metrics for each task, i.e., accuracy for VQA, mIoU for TVG, and LLM-judged scores for VDC. Vision-to-text information aggregation phenomenon. To understand the pattern of information flow within the hybrid model, we investigate the mechanism of information exchange among vision, instruction, and response tokens in attention layers during autoregressive generation, following the methodology of [42]. We apply attention masks and set the corresponding matrix values to 0 to block information exchange among different types of tokens. For clarity, we illustrate our two information-blocking configurations, i.e., vision-to-instruction (V2I) and vision-toresponse (V2R), by assuming that X0, X1, and each contain only single token at the l-th layer: block the information from vision to instruction tokens: (cid:2)X l+ 0 , l+1 1 , l+1 :t (cid:3) = 1 0 1 0 1 0 (cid:2)X 0 1 0, 1, :t (cid:3) (3) block the information from vision to response tokens: 1 1 0 (cid:2)X 0 1 (cid:2)X l+1 , l+1 1 , l+1 :t 0, 1, :t (cid:3) = 0 1 1 (cid:3) 0 (4) As shown in Figure 3, we observe consistent vision-totext aggregation phenomenon: in instruction-centric tasks, visual information is progressively absorbed into instruction tokens until deeper layers, whereas in vision-centric tasks, vision tokens directly contribute to response generation. Blocking V2I drastically degrades performance in early layers for MCQ and TVG, but has negligible impact in later layers, confirming that instruction tokens eventually internalize visual cues. Conversely, in VDC, blocking V2R causes sharp drop in shallow layers, highlighting the dominant role of vision tokens in direct response generation. Vision token redundancy in hybrid MLLM. While many previous studies have shown vision token redundancy in Transformers [11, 18, 92], it remains unclear how such visual redundancy manifests in hybrid models. To investigate this, we drop vision tokens at different layers to observe performance changes on benchmarks. Specifically, we use two vision token dropping strategies: uniform dropping (uni) and attention-guided dropping (attn), which keeps the topk vision tokens most attended by the last instruction token XT1 . Let denote the token dropping rate, Td = pT0 be the number of tokens to be dropped. We define the dropping operator TD() as: (cid:40) TD(X) = Uniform(X, Td) Topk(X, Attn(XT1 , X), Td) uni attn (5) 3.3. Training Procedure To effectively adapt TimeViper for long video understanding, we divide the training process into two stages and train the model using fully open-source data: (1) Imagetext alignment stage: We first pretrain the projector to align the ViT and LLM modalities using 3M high-quality imagetext pairs sampled from CC12M [15] and PixelProse [75]. Token compression is disabled during this stage. (2) Visual instruction tuning: We then fine-tune the projector and LLM, including the compression modules, on approximately 4.8M multimodal instruction pairs, consisting of 1.8M video instruction data [8, 13, 17, 50, 71, 76, 113] primarily sourced from LLaVA-Video, 2.8M singleimage instruction data [46] from LLaVA-OneVision, and diverse downstream task-specific datasets including 26K dense video captioning samples [12, 37, 78, 105, 117] and 250K temporal video grounding samples [2, 6, 34, 57, 61, 62, 69, 88, 89, 95, 106]. This stage adapts TimeViper for instruction-following and video understanding while learning effective internal compression through TransV. 4. Experiments We first describe the experimental setups in Section 4.1, followed by the ablation studies in Section 4.2 and main results in Section 4.3. Finally, in Section 4.4, we provide qualitative analysis illustrating how hybrid models and Transformers interpret visual content through attention visualizations. 4.1. Experimental Setup Downstream benchmarks. We evaluate TimeViper across diverse suite of video understanding benchmarks: (1) VideoMME [29]: comprehensive video QA benchmark covering multiple domains. It includes 2.7K QA samples over videos ranging from 11 seconds to 1 hour. We eval- (2) LVBench [85]: uate models without textual subtitles. benchmark on hour-long video understanding across six dimensions, comprising 2094 multiple-choice QA samples. (3) MLVU [116]: Designed for minute-level video understanding, with 2174 QA samples spanning diverse domains. We evaluate the average performance of multiple-choice tasks (M-Avg), where videos have an average duration of 653 seconds. (4) LongVideoBench [90]: Targets long-form referring reasoning that requires retrieval-based QA, with videos averaging 473 seconds. (5) MVBench [49]: shortterm video QA benchmark emphasizing temporal reasoning, containing 4K QA pairs over 20 task categories. (6) Charades [74]: temporal video grounding benchmark containing 6672 minute-level indoor activity videos paired with natural language queries. (7) VDC [14]: An efficient and high-fidelity video captioning benchmark evaluated via query-conditioned scoring using LLaMA3-8B [26]. It consists of 1027 videos and we evaluate on the detailed split. Figure 4. Illustration of token redundancy. We compare performance under different vision-token dropping rates using uniform dropping and attention-guided dropping strategies. In the hybrid MLLM, vision token redundancy increases progressively with layer depth, allowing more aggressive token removal in deeper layers with minimal performance loss. where Uniform(X, k) uniformly drops tokens from RN , TopK(X, S, k) discards tokens with the highest scores in according to RN , and Attn(, ) computes the attention scores using the first argument as the query and the second as the key. Results in Figure 4 show that redundancy increases with depth. For MCQ and VDC, tokens can be uniformly dropped at all layers, but attention-guided dropping is reliable only in deeper layers. In TVG, excessive token dropping before the first attention layer, i.e., the 14-th layer, harms performance, but the drop ratio can increase in later layers. Vision tokens are critical in shallow layers but become nearly 100% redundant in deep layers across our testbeds. For all tasks, even discarding all vision tokens in deep layers, the model can still achieve high performance by relying solely on the instruction tokens, which is surprisingly similar to observations from previous Transformer-based image MLLMs [18, 111, 114]. Token information transfer via TransV. Motivated by these findings that information implicitly transfers from vision to text tokens and vision token redundancy is severe for all tasks, we propose TransV, lightweight in-LLM compression module that explicitly transfers visual information into instruction tokens, reducing redundant computation while preserving task performance. At the l-th layer, the token information transfer from vision to instruction tokens is formulated as: X l+1 1 = 1 = CrossAttnl(X 1, TDl(X 1 + tanh(αl)( 1) 0)) (6) where the CrossAttn(, ) computes the attention with the first term as the query and the second as both the key and value. αl is learnable scalar controlling the degree of information aggregation, and its value is normalized to the range [1, 1] via tanh(). αl is initialized to zero to ensure instruction understanding. 5 Table 1. Ablation of TransV choices. The uni 7 0.5-attn 39 0.9 denotes applying uniform TranV at the 7th layer with dropping rate of = 50% and attention-guided TransV at the 39th layer with = 90%. TDuni denotes uniform token dropping. Method none TDuni 7 0.5 uni 7 0.5 uni 2 0.5 uni 7 0.9 uni 7 0.5-uni 39 0.9 uni 7 0.5-attn 39 0.9 1 2 3 4 5 6 7 max frame VideoMME VDC Charades 5k 8k 8k 9k >10k >10k >10k 58.8 57.3 56.7 56.1 53.4 56.2 56. 39.8 39.0 38.9 39.7 37.9 39.4 39.1 40.5 26.1 38.1 38.2 34.6 37.9 37. Figure 5. Comparison of GPU memory usage during inference. While ToMe extends the context window to about 5K frames, TransV efficiently scales beyond 10K frames. Implementation details. Our data are organized in the order of system prompt tokens, video tokens, and instruction token as the LLM inputs. For all training and evaluation processes, videos are sampled at 1 frame per second. During training, videos longer than 256 frames are uniformly sampled to 256 frames; during evaluation, we use at most the first 256 frames. Each input frame is resized to 384384 and initially encoded into 768 vision tokens. After being projected with ToMe, each frame is compressed into 16 tokens [50, 120]. We apply TransV at the 7th shallow LLM layer with token dropping rate = 50%, and the 39th deep LLM layer where TransV is applied using an attentionguided strategy with = 90%. Introducing TransV adds approximately 100M parameters to the model. To accelerate training, we implement training with data packing [16] that supports training with varied sequence length caused by TransV. Across all training stages, the model uses learning rate of 1e-5, AdamW optimizer with weight decay of 0.01, warmup rate of 0.03, and cosine annealing scheduler. For TransV modules, we adopt higher learning rate of 5e-5. 4.2. Ablation Study For representation simplicity, each TransV is denoted uni 7 0.5as type layer ratio-..., example, for Figure 6. Comparison of prefilling time. TransV incurs no additional latency at low frame inputs (e.g., 64 frames) while significantly reducing prefilling time at high frame inputs. For instance, at 4,096 frames, TransV reduces prefilling time by 15.7% compared to the ToMe baseline. attn 39 0.9 represents applying uniform TransV in the 7th layer with ratio of 50% and attention-guided TransV in the 39th layer with ratio of 90%. Impact of compression components on GPU memory consumption. We apply ToMe in the projector and TransV in the LLM. Benefiting from the hybrid MambaTransformer backbone, both memory usage and prefilling time generally grow approximately linearly with input length. Figure 5 reports memory usage as the number of frames increases. The vanilla model runs Out of Memory error at merely 128 frames. TimeViper applying ToMe in the projector alleviates the initial token burden, extending the limit to approximately 5K frames. TimeViper with TransV further enables better scalability: at 4,096 frames, it reduces memory consumption by 54.8% compared to TimeViper, and can handle 10K+ frames with ample margin. This highlights the complementary roles of token compression in the projector and within the LLM. Impact of compression components on prefilling time. As shown in Figure 6,the vanilla model already incurs 4.5s latency at 64 frames. TimeViper drastically reduces this to 0.4s, and TransV further decreases prefilling time, with the effect becoming more pronounced as the number of frames increases. Notably, at 4,096 frames, TransV reduces prefilling time by 15.7% compared to TimeViper. TransV placement in shallow layers. As shown in Table 1, the TimeViper baseline can process approximately 5K input frames. By comparing rows 1, 2, and 3, we observe that introducing token dropping or token compression enables the model to handle over 8K frames. Comparing rows 2 and 3, using TransV effectively mitigates the Charades performance drop, from 26.1 to 38.1, indicating that it successfully facilitates token transfer. Comparing rows 3 and 4, compressing at the 7th layer does not necessarily outperform compression at the 2nd layer on the VDC or TVG 6 Table 2. Comparison with state-of-the-art models. Our work differs from previous studies both the choice of LLM backbone and the design of token compression strategy, while achieving competitive performance across benchmarks. Most existing methods fine-tune the ViT (indicated with *), whereas we do not due to computational constraints. Additionally, while the concurrent work Nanov2-VL [24] is trained on 46.7M samples, we uses only 7.8M, making Nanov2-VL reasonable upper bound for hybrid models. Model LLM >10K frame input MVBench LongVideoBench MLVU VideoMME LVBench Charades-STA VDC avg.acc val M-Avg overall long avg.acc mIoU avg.acc GPT-4V [1] GPT-4o [38] Gemini-1.5-Pro [80] LLaMA-VID [51] LongVA [110] LongVU [72] VILA1.5-7B [30] LLaVA-OneVision* [46] LLaVA-Video* [113] Qwen2-7B-VL* [84] Qwen2.5-VL* [7] LongVILA* [19] Kangaroo* [55] Video-XL* [73] Vamba* [70] VideoChat-Flash* [50] VTimeLLM [73] AuroraCap* [73] Qwen2.5-7B (ours) - - - Vicuna-1.5-7B [82] Qwen2-7B [81] Qwen2-7B [81] Qwen2-7B [81] Qwen2-7B [81] Qwen2-7B [81] Qwen2-7B [81] Qwen2.5-7B [96] Qwen2-7B [81] LLaMA3-8B [26] Qwen2-7B [81] Qwen2-VL-7B [81] Qwen2-7B [81] Vicuna-1.5-13B [82] Vicuna-1.5-7B [82] Qwen2.5-7B [96] LongLLaVA* [86] AuroraLong* [93] Nanov2-VL* (upper bound) [24] TimeViper (ours) TimeViper (ours w/ TransV) Jamba-52B [44] RWKV6-2B [65] Nanov2-12B [9] Nanov2-9B [9] Nanov2-9B [9] Proprietary Models 43.7 64.6 60.5 59.1 66.7 64.0 Transformer-based Video MLLMs 41.9 - 66.9 56.8 56.7 58.6 67.0 69.6 67.1 61.0 55.3 60.4 73.2 - - 57. - - - - 56.3 58.2 - 56.0 57.1 54.8 50.7 55.9 64.2 - - 55.4 Linearized/Hybrid Video MLLMs 64.6 53.2 - 57.2 56.2 53.5 - 63.6 54.1 52.0 - - - 49.2 64.6 - 33.2 56.3 65.4 56.8 64.7 70.8 - 70.2 - 61.0 64.9 65.9 74.5 - - 64.9 - 52.7 73.6 65.6 63.1 59.9 71.9 75.0 25.9 52.6 60.6 58.8 58.2 63.3 63.3 65.1 60.1 56.0 55.5 57.8 64.0 - - 56. 53.8 - 66.0 58.8 56.9 53.5 65.3 67.4 - 46.2 59.5 - - - - - 47.0 46.7 - - 53.6 - - 48.7 46.4 - - 48.8 48.2 - 30.8 33.1 23.9 - - - - - - 45.3 - 39.4 - - 47.2 - - 36. - - - 35.5 35.6 - 35.7 - - - - - 13.5 - - 43.6 - - - - 48.4 34.6 - 40.8 - - - 40.5 37.9 - - 43.1 25.6 27.9 - - 41.2 - 41.6 - - - - - - - 39.0 42. - 42.5 - 39.7 39.1 Figure 7. Comparison of performance as the number of input frames increases on long-video understanding benchmarks. We train our models with 256 frames as inputs, and sample 1 frame per second during evaluation. The x-axis here denotes the maximum number of frames. If video exceeds this length, we take only the first max frames for inference. benchmarks. For example, compression at the 7th layer outperforms the 2nd layer by 0.6 points on MCQ, but performs 0.8 points worse on VDC. TransV placement in deep layers. From rows 6 and 7 in Table 1, attention-guided TransV yields higher MCQ performance of 56.6 than uniform TransVs 56.2, with minor differences on VDC and Charades. Moreover, transferring token information in deeper layers significantly increases the models long-context capacity: comparing rows 1 and 3, the model handles tens of thousands of frames with only 0.1 drop on VideoMME. Compression rate for TransV in shallow layers. We evaluate higher compression rate of = 90% at the 7th layer in Table 1. Larger compression rate allows the model to process more frames, but it comes with significant performance drop. Comparing rows 4 and 5, after increasing compression rate from 50% to 90%, the accuracy on VideoMME decreases from 56.7 to 53.4. Impact of different LLM backbones under identical training recipe. To isolate the effect of model architecture from scaling up, we train Transformer-based baseline using Qwen2.5 following exactly the same training recipe as our hybrid model. As shown in Table 2, Qwen2.5 performs on par with TimeViper when trained on our dataset, suggest7 Figure 8. Illustration of attention score matrices in Nanov2 [9] and Qwen2.5 [96] at shallow and deep layers. White lines divide the input sequence into four distinct segments: system prompt, vision tokens, user instruction, and the generated response. ing that purely Transformer-based architectures do not offer clear advantage under comparable training conditions. Notably, the concurrent work Nanov2-VL, which adopts standard MLLM architecture but is trained on 46.7M samples, substantially larger than our 7.8M, achieves state-ofthe-art performance. This indicates that hybrid MLLMs can benefit significantly from scaling. 4.3. Main Results TimeViper achieves competitive performance with current models across video understanding benchmarks. For MCQ tasks, as shown in Table 2, despite not finetuning ViT, TimeViper with TransV achieves an average accuracy of 56.2 on VideoMME, +0.7 points higher than Video-XL (55.5), which compresses tokens into new ones within Qwen2. For VDC task, TimeViper achieves strong performance with an accuracy of 39.7, excqeeding the taskspecific model Auroracap by +0.7 points. For TVG task, TimeViper establishes surprisingly strong baseline with an mIoU of 40.5 on Charades, significantly outperforming the task-specific model VTimeLLM-13B with an mIoU of 34.6. This is particularly notable because TimeViper uses only SigLIP positional embedding for vision tokens and relies on the implicit temporal modeling of Mamba layers. Yet the model learns robust temporal alignments between videos and language query, matching or exceeding prior models such as Qwen2.5-VL-7B that explicitly employ MRoPE for fine-grained timestamp modeling. These results collectively demonstrate that hybrid Mamba-Transformer architectures are highly competitive for long video understanding. Effect of increasing the number of inference frames. Since the model is trained with 256 frames, we evaluate test-time scalability by varying the number of input frames. As shown in Figure 7, TimeViper scales robustly with longer contexts across four long video understanding benchmarks. For example, when increasing the input frames from 256 to 512 frames, MLVU improves from 65.64 to 69.00, and LVBench increases from 35.53 to 37.0. 4.4. Qualitative Analysis To better understand how hybrid MLLMs differ from Transformer-based MLLMs in processing multimodal inputs, we first formalize the definitions of attention scores used in both Mamba-2 and self-attention layers and then analyze attention behaviors across layers. For Mamba layers, we follow [4] to define the attention pattern, while for Transformer layers, we use the attention weights. Definition of attention score. For self-attention (Equation (2)), the attention score Mj,i from xj to xi is: yi = Softmax( QiK ) Vi = (cid:88) j=1 Mi,jVj (7) For the SSM block, we rewrite Equation (1) to express its attention pattern as the weighted sum from inputs [x1, . . . , xi] to the output yi: yi = (cid:88) j=1 (cid:89) i ( k=j+1 Ak)Bjxj = (cid:88) j=1 i,jxj (8) i,j R+ serves as the attention score [4, 119] Here, from xj to xi within the SSM block. Although both the self-attention and Mamba mechanisms employ multi-head design [23, 83] along the hidden dimension, we omit this detail in the equations for simplicity. Diverse and specialized attention patterns in Mamba layers. Figure 8 visualizes the attention score matrices of Nano (hybrid) and Qwen (Transformer) from shallow to deep layers. Mamba layers exhibit diverse attention patterns, ranging from sparsity, locality to globality, suggesting that different layers and heads specialize in modeling different types of dependencies. For example, Layer 0, Head 8 of Mamba layer (ML0) exhibits sparsity, where only 8 Figure 9. Qualitative results of TimeViper on three long video understanding tasks. (1) MCQ: The model demonstrates reasoning capability by correctly answering multi-choice question about the videos content. (2) TVG: It accurately localizes the temporal boundaries for specific event, reaching an IoU of 0.75. (3) VDC: The model generates detailed description that showcases its fine-grained comprehension. Green text highlights accurate detailed descriptions. Some output in the middle is omitted for brevity. few tokens receive dominant attention from the following tokens, revealing Mambas capability to selectively highlight salient tokens. In contrast, ML20 exhibits globality, where all tokens attend uniformly to preceding tokens, reflecting its effective integration of prior information. ML52 displays locality, focusing primarily on neighboring tokens. This diversity highlights the complementary strengths of state-space modeling within hybrid architectures. Attention sink in self-attention layer of hybrid MLLM. The attention sink [77, 91] is clearly observed in the selfattention layers of Nano, as shown in Figure 8, where the majority of attention scores are concentrated on the initial few tokens. This behavior aligns with observations in traditional Transformer models, as exemplified by Qwens attention maps such as AL24 and AL27. Decrease of attention to vision tokens across layers. Comparing the second row with the first row in Figure 8, we observe clear downward trend in attention scores assigned to vision tokens as the layers deepen. This phenomenon is consistent with our broader findings: as the model processes more layers, visual information becomes increasingly redundant and is subsequently deprioritized by instruction and response tokens. Qualitative results. Figure 9 illustrates qualitative examples from MCQ, TVG, and VDC tasks. TimeViper accurately answers complex multi-choice video questions, localizes temporal boundaries with an IoU of 0.75, and produces rich, fine-grained video descriptions. 5. Conclusion This work takes an initial step toward understanding and compressing hybrid vision-language models for long videos. We introduce TimeViper, Mamba-Transformer hybrid model equipped with TransV, an internal LLM token transfer module that compresses vision information into text tokens. We reveal that visual information gradually shifts into text tokens as depth of hybrid LLM layer increases, resulting in substantial redundancy among vision tokens in deep layers. TimeViper can efficiently process hour-long videos while maintaining strong multimodal understanding. TimeViper achieves promising performance for long video understanding across multi-choice video QA, temporal video grounding, and detailed video captioning."
        },
        {
            "title": "Appendix",
            "content": "A. Limitations First, our current performance still falls short of the SOTA models due to limited training data and insufficient model training. Second, while TransV enables processing over 10,000 frames, the model has not been trained on videos of such duration. B. Experimental Setups Implementation details. When training attention-based dropping in multi-turn dialogue scenarios, the attention distribution is computed using the last token of the final instruction as the query. For temporal video grounding data, we incorporate time-aware prompt [50]: The video lasts for {} seconds, and {} frames are uniformly sampled from it. During training, we randomly sample one instruction from pool of 15 manually constructed task prompts, such as: From the video, locate the portion that aligns with the textual query , and output the start and end timestamps in seconds. The output format of the predicted timestamp should be like: start to end seconds. specific example is : 12.0 to 20.0 seconds. We do not use system prompt, but we retain the BOS token to act as an attention sink [91]. Training data summary. Our training pipeline adopts two-stage strategy. We summarize the training data in Table 3. Specifically, in the first image-text alignment stage, we utilize 3 million images randomly sampled from the CC12M dataset [15], paired with captions sourced from PixelProse [75]. In the second video instruction tuning stage, we assemble composite dataset to enhance MLLMs video understanding and timestamp prediction ca- (1) 1.3M samples from LLaVApabilities, comprising: Video [113]; (2) 253K data from Kinetics400 [13] and WebVid [8] that are recaptioned with GPT-4o or Gemini by ShareGemini [71] and ShareGPT-4 [17]; (3) 100K samples from ET-Instruct [57]; (4) 112K samples from VideoGPTPlus [59]; (5) 11K samples from LongVid [50] and MovieChat [76]; (6) 26K dense video captioning (DVC) samples aggregated from ActivityNet [12], COIN [78], HiREST [105], ViTT [37], and YouCook2 [117]; and (7) 250K temporal video grounding (TVG) samples [89] from YT-Temporal [95], DiDeMo [6], QuerYD [62], InternVid [88], and HowTo100M [61]. We obtain grounding data with annotations from VTG-IT [34], TimeIT [69], TimePro [106], HTStep [2], and LongVid [50]. This data collection process yields 339K temporal grounding samples. To ensure data quality, we apply simple cleaning protocol to the TVG data. Specifically, we filter out coarsegrained samples where the ground truth duration exceeds 30 seconds or spans more than one-third of the total video length. We also discard invalid entries containing out-ofFigure 10. Comparison of average attention scores across all layers in Nanov2 and Qwen2.5. The visualization shows both attention and Mamba layers for Nano, and attention layers for Qwen. For Mamba layers, we normalize each row of the attention scores using the L1 norm so that all values fall within the range [0, 1]. bound timestamps. Consequently, our TVG training data remains 250K. C. Main Results TransV can be also applied to Qwen2.5. Qwen can also use TransV to handle ultra-long sequences, as shown in Table 4. We observe that on LVBench, this even brings +0.4 improvement. However, the performance drop on VDC is more severe for Qwen than for Nano. For example, Qwen drops from 42.0 to 40.7 (a decrease of 1.3 points), whereas Nano drops from 39.7 to 39.1 (a decrease of 0.6 points). D. Qualitative Results We define average attention scores used in both Mamba-2 and self-attention layers to analyze attentions received by different types of tokens. Average attention score computation. We adopt the category-level attention score definition from LLaVAMini [111]. Tokens are grouped into instruction, vision, and response categories: Tins, Tvis, and Tres. Let aij denote the attention score from token ti to token tj, averaged over all attention heads. For two token categories A, {Tins, Tvis, Tres}, we define their category-level attention score as: Attn(A B) = (cid:110) (cid:12) (cid:12) (cid:12) (cid:80) tiA (cid:80) ti (cid:12) (cid:12) (cid:80) tj aij tj aij > 0 (cid:111)(cid:12) (cid:12) (cid:12) . (9) The denominator counts the number of tokens in that attend to any token in with non-zero weight, ensuring that tokens masked by the causal attention mask are excluded. In Figure 10, we analyze the overall attention scores from the entire sequence = Tins Tvis Tres to target category 10 Table 3. Data recipe. Overview of the datasets used in our two-stage training pipeline. Stage 1: Projector Alignment Image caption data (3M) CC12M (3M) [15] with PixelProse captions [75] Stage 2: Video Instruction-Tuning Image instruction data (2.8M) LLaVA-OneVision (2.8M) [46]; Video instruction data (1.8M) LLaVA-Video (1.3M) [113]; Kinetics400 & WebVid (253K) [8, 13] (recaptioned via ShareGemini [71] & ShareGPT-4 [17]); VideoGPT-Plus (112K) [59]; ET-Instruct (100K) [57]; LongVid [50] & MovieChat (11K) [76] Dense video captioning (26K) ActivityNet [12], COIN [78], HiREST [105], ViTT [37], YouCook2 [117] Temporal video grounding (250K) YT-Temporal [95], DiDeMo [6], QuerYD [62], InternVid [88], HowTo100M [61] (Annotated by VTG-IT [34], TimeIT [69], TimePro [106], HTStep [2], LongVid [50]) Table 4. Performance of applying TransV to Qwen2.5 and Nano. Model LLM >10K frame input Qwen2.5-7B (ours) Qwen2.5-7B [96] Qwen2.5-7B (ours w/ TransV) Qwen2.5-7B [96] TimeViper (ours) TimeViper (ours w/ TransV) Nanov2-9B [9] Nanov2-9B [9] MVBench LongVideoBench MLVU VideoMME LVBench Charades-STA VDC avg.acc 57.6 55.7 57.2 56. val 55.4 53.7 54.1 52.0 M-Avg overall long avg.acc mIoU avg.acc 64.9 63.3 65.6 63.1 56.6 55. 58.8 56.9 48.7 47.4 48.8 48.2 36.6 37.0 35.5 35.6 40.8 38. 40.5 37.9 42.0 40.7 39.7 39.1 B. To ensure equal contribution from each category, we compute the arithmetic mean of the scores: Attn(T B) = (cid:0)Attn(Tins B) 1 3 +Attn(Tvis B) +Attn(Tres B)(cid:1). (10) Hybrid MLLMs preserve stronger attention to vision tokens. To quantify model behavior, we compute the average attention received by instruction, vision, and response tokens across all layers. As shown in Figure 10, Qwen rapidly down-weights vision tokens after the early layers, instead favoring instruction and response tokens. In contrast, Nano maintains noticeably higher attention to vision tokens throughout the network. These findings suggest that the hybrid model is more effective at attending to visual information than the Transformer-based architecture. Qualitative results on VideoMME. Figure 11c provides qualitative examples illustrating the effectiveness of TransV on the MCQ task. In the first case (top row), the query requires retrieving fine-grained visual information, specifically, the defending layers about the Berlin Wall. The compressed model, TimeViper w/ TransV, successfully attends to the critical frame at 03:36, which clearly depicts the structure of the wall, enabling it to select the correct answer. This example highlights TransVs capability to identify and In the secattend to key visual cues within long videos. ond case (bottom row), the query requires long-term temporal reasoning to infer the chronological order of topics in biology lecture. The correct answer relies on aggregating information across the entire video duration. In this case, model must accurately align the textual concepts (e.g., structure, photosynthesis) with their corresponding temporal segments (00:52, 07:37, etc.), As shown in the visual evidence, TimeViper w/ TransV correctly deduces the sequence (c)-(d)-(e)-(a)-(b), demonstrating its capability in modeling global temporal dependencies and understanding narrative structure. Qualitative results on Charades. For temporal video grounding results shown in Figure 11b, we observe that incorporating the compression module yields only minimal changes. Both the original and compressed models correctly interpret timestamps and successfully localize video segments that correspond to the natural language query. Qualitative results on VDC. Figure 11a presents the qualitative comparison for the video detailed captioning task. In the generated captions, green text denotes accurate visual details, while red text indicates hallucinations or facIn the first example showing person painttual errors. ing (top row), the baseline model suffers from severe object hallucination, fabricating elements such as sponge which are absent in the video. Surprisingly, TimeViper w/ TransV generates more faithful description, accurately recognizing specific objects like paintbrushes. This suggests that compression may help reduce hallucination by filtering out irrelevant or misleading visual information. In the second example (bottom row), the compression module TransV largely retains the original model behavior where there are both correct and incorrect captions. 11 (a) Qualitative results on VDC. (b) Qualitative results on Charades. (c) Qualitative results on VideoMME. Figure 11. Qualitative results on three benchmarks."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 7 [2] Triantafyllos Afouras, Effrosyni Mavroudi, Tushar Nagarajan, Huiyu Wang, and Lorenzo Torresani. Ht-step: Aligning instructional articles with how-to videos. Advances in Neural Information Processing Systems, 36:5031050326, 2023. 5, 10, 11 [3] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Iain Barr, Yana Hasson, Karel Antoine Miech, Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. 2, 3 [4] Ameen Ali Ali, Itamar Zimerman, and Lior Wolf. The hidden attention of mamba models. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15161534, Vienna, Austria, 2025. Association for Computational Linguistics. 8 [5] Saeed Ranjbar Alvar, Gursimran Singh, Mohammad Akbari, and Yong Zhang. Divprune: Diversity-based visual token pruning for large multimodal models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 93929401, 2025. 2 [6] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan Russell. Localizing moments in video with natural language. In Proceedings of the IEEE international conference on computer vision, pages 58035812, 2017. 5, 10, 11 [7] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 7 [8] Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF international conference on computer vision, pages 17281738, 2021. 5, 10, [9] Aarti Basant, Abhijit Khairnar, Abhijit Paithankar, Abhinav Khattar, Adithya Renduchintala, Aditya Malte, Akhiad Bercovich, Akshay Hazare, Alejandra Rico, Aleksander Ficek, et al. Nvidia nemotron nano 2: An accurate and efficient hybrid mamba-transformer reasoning model. arXiv preprint arXiv:2508.14444, 2025. 2, 3, 7, 8, 11 [10] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, 13 Lachy Groom, Karol Hausman, Brian Ichter, et al. π0: vision-language-action flow model for general robot control. arXiv preprint arXiv:2410.24164, 2024. 1 [11] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster. In The Eleventh International Conference on Learning Representations, 2023. 2, 3, 4 [12] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: large-scale video benchmark for human activity understanding. In Proceedings of the ieee conference on computer vision and pattern recognition, pages 961970, 2015. 5, 10, [13] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? new model and the kinetics dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 62996308, 2017. 5, 10, 11 [14] Wenhao Chai, Enxin Song, Yilun Du, Chenlin Meng, Vashisht Madhavan, Omer Bar-Tal, JenqNeng Hwang, Saining Xie, and Christopher Manning. Auroracap: Efficient, performant video detailed captioning and new benchmark. In The Thirteenth International Conference on Learning Representations, 2025. 1, 4, 5 [15] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual In Proceedings of the IEEE/CVF conconcepts. ference on computer vision and pattern recognition, pages 35583568, 2021. 5, 10, 11 [16] Guo Chen, Zhiqi Li, Shihao Wang, Jindong Jiang, Yicheng Liu, Lidong Lu, De-An Huang, Wonmin Byeon, Matthieu Le, Max Ehrlich, Tong Lu, Limin Wang, Bryan Catanzaro, Jan Kautz, Andrew Tao, Zhiding Yu, and Guilin Liu. Eagle 2.5: Boosting long-context post-training for frontier visionlanguage models, 2025. 6 [17] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Zhenyu Tang, Li Yuan, et al. Sharegpt4video: Improving video understanding and generation with better captions. Advances in Neural Information Processing Systems, 37:1947219495, 2024. 5, 10, 11 [18] Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. An image is worth 1/2 tokens after layer 2: Plug-andplay inference acceleration for large vision-language In European Conference on Computer Vimodels. sion, pages 1935. Springer, 2024. 2, 4, [19] Yukang Chen, Fuzhao Xue, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, Ethan He, Hongxu Yin, Pavlo Molchanov, Jan Kautz, Jim Fan, Yuke Zhu, Yao Lu, and Song Han. Longvila: Scaling longcontext visual language models for long videos. In International Conference on Representation Learning, pages 1822718246, 2025. 7 [20] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. 2 [21] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Inderjit Ice Pasupat, Noveen Sachdeva, Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 1, 2 [22] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. Advances in neural information processing systems, 36:4925049267, 2023. 2 [23] Tri Dao and Albert Gu. Transformers are ssms: generalized models and efficient algorithms through structured state space duality. In Proceedings of the 41st International Conference on Machine Learning. JMLR.org, 2024. 2, 3, 4, 8 [24] Amala Sanjay Deshmukh, Kateryna Chumachenko, Tuomas Rintamaki, Matthieu Le, Tyler Poon, Danial Mohseni Taheri, Ilia Karmanov, Guilin Liu, Jarno Seppanen, Guo Chen, et al. Nvidia nemotron nano v2 vl. arXiv preprint arXiv:2511.03929, 2025. 7 [25] Xin Dong, Yonggan Fu, Shizhe Diao, Wonmin Byeon, ZIJIA CHEN, Ameya Sunil Mahabaleshwarkar, Shih-Yang Liu, Matthijs Van keirsbilck, Min-Hung Chen, Yoshi Suhara, Yingyan Celine Lin, Jan Kautz, and Pavlo Molchanov. Hymba: hybridhead architecture for small language models. In The Thirteenth International Conference on Learning Representations, 2025. 2, [26] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pages arXiv2407, 2024. 5, 7 [27] Qihang Fan, Huaibo Huang, Yuang Ai, and Ran He. Rectifying magnitude neglect in linear attention. In ICCV, 2025. 3 [28] Yue Fan, Xiaojian Ma, Rujie Wu, Yuntao Du, Jiaqi Li, Zhi Gao, and Qing Li. Videoagent: memoryaugmented multimodal agent for video understandIn European Conference on Computer Vision, ing. pages 7592. Springer, 2024. 2 [29] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2410824118, 2025. 1, 2, 4, 5 [30] Chaoyou Fu, Haojia Lin, Xiong Wang, YiFan Zhang, Yunhang Shen, Xiaoyu Liu, Haoyu Cao, Zuwei Long, Heting Gao, Ke Li, Long MA, Xiawu Zheng, Rongrong Ji, Xing Sun, Caifeng Shan, and Ran He. VITA-1.5: Towards GPT-4o level real-time vision and speech interaction. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. [31] Gabriele Goletto, Tushar Nagarajan, Giuseppe Averta, and Dima Damen. Amego: Active memory In European Conferfrom long egocentric videos. ence on Computer Vision, pages 92110. Springer, 2024. 2 [32] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1899519012, 2022. 1 [33] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. In First Conference on Language Modeling, 2024. 2, 3 [34] Yongxin Guo, Jingyu Liu, Mingda Li, Dingxin Cheng, Xiaoying Tang, Dianbo Sui, Qingbin Liu, Xi Chen, and Kevin Zhao. Vtg-llm: Integrating timestamp knowledge into video llms for enhanced video In Proceedings of the AAAI temporal grounding. Conference on Artificial Intelligence, pages 3302 3310, 2025. 5, 10, 11 [35] Wei Han, Hui Chen, Min-Yen Kan, and Soujanya Poria. Self-adaptive sampling for accurate video question answering on image text models. In Findings of the Association for Computational Linguistics: NAACL 2024, pages 25222534, 2024. 2 [36] Bin Huang, Xin Wang, Hong Chen, Zihan Song, and Wenwu Zhu. Vtimellm: Empower llm to grasp video moments. In Proceedings of the IEEE/CVF Confer14 ence on Computer Vision and Pattern Recognition, pages 1427114280, 2024. ing foundation models with lightning attention. arXiv preprint arXiv:2501.08313, 2025. 2 [37] Gabriel Huang, Bo Pang, Zhenhai Zhu, Clara Rivera, and Radu Soricut. Multimodal pretraining for dense video captioning. In Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 470490, Suzhou, China, 2020. Association for Computational Linguistics. 5, 10, 11 [38] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec RadarXiv preprint ford, et al. Gpt-4o system card. arXiv:2410.21276, 2024. 7 [39] Md Mohaiminul Islam, Tushar Nagarajan, Huiyu Wang, Gedas Bertasius, and Lorenzo Torresani. Bimba: Selective-scan compression for long-range In Proceedings of the video question answering. Computer Vision and Pattern Recognition Conference, pages 2909629107, 2025. 2 [40] Jindong Jiang, Xiuyu Li, Zhijian Liu, Muyang Li, Guo Chen, Zhiqi Li, De-An Huang, Guilin Liu, Zhiding Yu, Kurt Keutzer, et al. Storm: Token-efficient long video understanding for multimodal llms. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 58305841, 2025. [41] Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual representation empowers large language models with In Proceedings of image and video understanding. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1370013710, 2024. 2 [42] Omri Kaduri, Shai Bagon, and Tali Dekel. Whats in the image? deep-dive into the vision of vision In Proceedings of the Computer language models. Vision and Pattern Recognition Conference, pages 1454914558, 2025. [43] Kumara Kahatapitiya, Kanchana Ranasinghe, Jongwoo Park, and Michael Ryoo. Language repository for long video understanding. In Findings of the Association for Computational Linguistics: ACL 2025, pages 56275646, Vienna, Austria, 2025. Association for Computational Linguistics. 2 [44] Barak Lenz, Opher Lieber, Alan Arazi, Amir Bergman, Avshalom Manevich, Barak Peleg, Ben Aviram, Chen Almagor, Clara Fridman, Dan Padnos, et al. Jamba: Hybrid transformer-mamba language models. In The Thirteenth International Conference on Learning Representations, 2025. 2, 7 [45] Aonian Li, Bangwei Gong, Bo Yang, Boji Shan, Chang Liu, Cheng Zhu, Chunhao Zhang, Congchao Guo, Da Chen, Dong Li, et al. Minimax-01: Scal- [46] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. Transactions on Machine Learning Research, 2024. 5, 7, 11 [47] Jiaze Li, Yaya Shi, Zongyang Ma, Haoran Xu, Yandong.bai Yandong.bai, Huihui Xiao, Ruiwen Kang, iMOVE Fan Yang, Tingting Gao, and Di Zhang. : Instance-motion-aware video understanding. In Findings of the Association for Computational Linguistics: ACL 2025, pages 2395923975, Vienna, Austria, 2025. Association for Computational Linguistics. 2 [48] Kunchang Li, Xinhao Li, Yi Wang, Yinan He, Yali Wang, Limin Wang, and Yu Qiao. Videomamba: State space model for efficient video understanding. In European conference on computer vision, pages 237255. Springer, 2024. [49] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2219522206, 2024. 5 [50] Xinhao Li, Yi Wang, Jiashuo Yu, Xiangyu Zeng, Yuhan Zhu, Haian Huang, Jianfei Gao, Kunchang Li, Yinan He, Chenting Wang, et al. Videochatflash: Hierarchical compression for long-context video modeling. arXiv preprint arXiv:2501.00574, 2024. 1, 2, 3, 5, 6, 7, 10, 11 [51] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llamavid: An image is worth 2 tokens in large language In European Conference on Computer Vimodels. sion. Springer, 2024. 2, 7 [52] Yingyue Li, Bencheng Liao, Wenyu Liu, and Xinggang Wang. Matvlm: Hybrid mamba-transformer for efficient vision-language modeling. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2087820888, 2025. 2 [53] Bencheng Liao, Hongyuan Tao, Qian Zhang, Tianheng Cheng, Yingyue Li, Haoran Yin, Wenyu Liu, and Xinggang Wang. Multimodal mamba: Decoderonly multimodal state space model via quadratic to linear distillation. arXiv preprint arXiv:2502.13145, 2025. 2, 3 [54] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:34892 34916, 2023. 15 [55] Jiajun Liu, Yibing Wang, Hanghang Ma, Xiaoping Wu, Xiaoqi Ma, xiaoming Wei, Jianbin Jiao, Enhua Wu, and Jie Hu. Kangaroo: powerful videolanguage model supporting long-context video input. arXiv preprint arXiv:2408.15542, 2024. 7 [56] Yunze Liu and Li Yi. Map: Unleashing hybrid mamba-transformer vision backbones potential with masked autoregressive pretraining. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 96769685, 2025. 3 [57] Ye Liu, Zongyang Ma, Zhongang Qi, Yang Wu, Chang Wen Chen, and Ying Shan. E.t. bench: Towards open-ended event-level video-language understanding. In Neural Information Processing Systems (NeurIPS), 2024. 5, 10, 11 [58] Zhijian Liu, Ligeng Zhu, Baifeng Shi, Zhuoyang Zhang, Yuming Lou, Shang Yang, Haocheng Xi, Shiyi Cao, Yuxian Gu, Dacheng Li, Xiuyu Li, Yunhao Fang, Yukang Chen, Cheng-Yu Hsieh, DeAn Huang, An-Chieh Cheng, Vishwesh Nath, Jinyi Hu, Sifei Liu, Ranjay Krishna, Daguang Xu, Xiaolong Wang, Pavlo Molchanov, Jan Kautz, Hongxu Yin, Song Han, and Yao Lu. Nvila: Efficient arXiv preprint frontier visual language models. arXiv:2412.04468, 2024. 2 [59] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Videogpt+: Integrating image and video encoders for enhanced video understanding. arxiv, 2024. 10, [60] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very long-form video language understanding. Advances in Neural Information Processing Systems, 36:4621246244, 2023. 2 [61] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. Howto100m: Learning text-video embedding by watching hundred million narrated video clips. In Proceedings of the IEEE/CVF international conference on computer vision, pages 26302640, 2019. 5, 10, 11 [62] Andreea-Maria Oncescu, Joao Henriques, Yang Liu, Andrew Zisserman, and Samuel Albanie. Queryd: video dataset with high-quality text and audio narrations. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 22652269. IEEE, 2021. 5, 10, 11 [63] Junting Pan, Ziyi Lin, Yuying Ge, Xiatian Zhu, Renrui Zhang, Yi Wang, Yu Qiao, and Hongsheng Li. Retrieving-to-answer: Zero-shot video question answering with frozen large language models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 272283, 2023. 2 [64] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023. 3 [65] Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Xingjian Du, Teddy Ferdinan, Haowen Hou, et al. Eagle and finch: Rwkv with matrix-valued arXiv preprint states and dynamic recurrence. arXiv:2404.05892, 2024. 3, 7 [66] Toby Perrett, Ahmad Darkhalil, Saptarshi Sinha, Omar Emara, Sam Pollard, Kranti Kumar Parida, Kaiting Liu, Prajwal Gatti, Siddhant Bansal, Kevin Flanagan, et al. Hd-epic: highly-detailed egocentric video dataset. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2390123913, 2025. [67] Yanyuan Qiao, Zheng Yu, Longteng Guo, Sihan Chen, Zijia Zhao, Mingzhen Sun, Qi Wu, and Jing Liu. Vl-mamba: Exploring state space arXiv preprint models for multimodal learning. arXiv:2403.13600, 2024. 3 [68] Liliang Ren, Yang Liu, Yadong Lu, yelong shen, Chen Liang, and Weizhu Chen. Samba: Simple hybrid state space models for efficient unlimited context language modeling. In International Conference on Representation Learning, pages 5355153575, 2025. 2, 3 [69] Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. Timechat: time-sensitive multimodal large language model for long video understandIn Proceedings of the IEEE/CVF Conference ing. on Computer Vision and Pattern Recognition, pages 1431314323, 2024. 2, 5, 10, 11 [70] Weiming Ren, Wentao Ma, Huan Yang, Cong Wei, Ge Zhang, and Wenhu Chen. Vamba: Understanding hour-long videos with hybrid mamba-transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 21197 21208, 2025. 2, 3, 7 [71] Share. Sharegemini: Scaling up video caption data for multimodal large language models, 2024. 5, 10, 11 [72] Xiaoqian Shen, Yunyang Xiong, Changsheng Zhao, Lemeng Wu, Jun Chen, Chenchen Zhu, Zechun Liu, Fanyi Xiao, Balakrishnan Varadarajan, Florian Bordes, Zhuang Liu, Hu Xu, Hyunwoo J. Kim, Bilge Soran, Raghuraman Krishnamoorthi, Mohamed Elhoseiny, and Vikas Chandra. LongVU: Spatiotemporal adaptive compression for long video-language 16 understanding. ference on Machine Learning, 2025. 2, 7 In Forty-second International Con- [73] Yan Shu, Zheng Liu, Peitian Zhang, Minghao Qin, Junjie Zhou, Zhengyang Liang, Tiejun Huang, and Bo Zhao. Video-xl: Extra-long vision language In Promodel for hour-scale video understanding. ceedings of the Computer Vision and Pattern Recognition Conference, pages 2616026169, 2025. 1, 2, 7 [74] Gunnar Sigurdsson, Abhinav Gupta, Cordelia Schmid, Ali Farhadi, and Karteek Alahari. Charadesego: large-scale dataset of paired third and first arXiv preprint arXiv:1804.09626, person videos. 2018. 1, 4, 5 [75] Vasu Singla, Kaiyu Yue, Sukriti Paul, Reza Shirkavand, Mayuka Jayawardhana, Alireza Ganjdanesh, Heng Huang, Abhinav Bhatele, Gowthami Somepalli, and Tom Goldstein. From pixels to prose: arXiv large dataset of dense image captions. preprint arXiv:2406.10328, 2024. 5, 10, [76] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, et al. Moviechat: From dense token to sparse memory for In Proceedings of the long video understanding. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1822118232, 2024. 2, 5, 10, 11 [77] Mingjie Sun, Xinlei Chen, Zico Kolter, and Zhuang Liu. Massive activations in large language models. In First Conference on Language Modeling, 2024. 9 [78] Yansong Tang, Dajun Ding, Yongming Rao, Yu Zheng, Danyang Zhang, Lili Zhao, Jiwen Lu, and Jie Zhou. Coin: large-scale dataset for comprehensive instructional video analysis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12071216, 2019. 5, 10, 11 [79] Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba, Raquel Urtasun, and Sanja Fidler. Movieqa: Understanding stories in movies through question-answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 46314640, 2016. 1 [80] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 1, 7 [81] Qwen Team et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2(3), 2024. [82] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023. 7 [83] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 8 [84] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at arXiv preprint arXiv:2409.12191, any resolution. 2024. 7 [85] Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Ming Ding, Xiaotao Gu, Shiyu Huang, Bin Xu, et al. Lvbench: An extreme In Proceedlong video understanding benchmark. ings of the IEEE/CVF International Conference on Computer Vision, pages 2295822967, 2025. 1, 5 [86] Xidong Wang, Dingjie Song, Shunian Chen, Chen Zhang, and Benyou Wang. Longllava: Scaling multimodal llms to 1000 images efficiently via hybrid architecture. arXiv preprint arXiv:2409.02889, 2024. 2, 3, 7 [87] Xiaohan Wang, Yuhui Zhang, Orr Zohar, and Serena Yeung-Levy. Videoagent: Long-form video understanding with large language model as agent. In European Conference on Computer Vision, pages 58 76. Springer, 2024. 2 [88] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, Ping Luo, Ziwei Liu, Yali Wang, Limin Wang, and Yu Qiao. Internvid: large-scale video-text dataset for multimodal understanding and generation. In The Twelfth International Conference on Learning Representations, 2024. 5, 10, [89] Ye Wang, Ziheng Wang, Boshen Xu, Yang Du, Kejun Lin, Zihan Xiao, Zihao Yue, Jianzhong Ju, Liang Zhang, Dingyi Yang, Xiangnan Fang, Zewen He, Zhenbo Luo, Wenxuan Wang, Junqi Lin, Jian Luan, and Qin Jin. Time-r1: Post-training large vision language model for temporal video grounding. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. 5, 10 [90] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for longcontext interleaved video-language understanding. 17 Advances in Neural Information Processing Systems, 37:2882828857, 2024. 5 [91] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language In The Twelfth Intermodels with attention sinks. national Conference on Learning Representations, 2024. 9, 10 [92] Long Xing, Qidong Huang, Xiaoyi Dong, Jiajie Lu, Pan Zhang, Yuhang Zang, Yuhang Cao, Conghui He, Jiaqi Wang, Feng Wu, et al. Pyramiddrop: Accelerating your large vision-language models via pyramid visual redundancy reduction. In Proceedings of the Computer Vision and Pattern Recognition Conference, 2025. 2, [93] Weili Xu, Enxin Song, Wenhao Chai, Xuexiang Wen, Tian Ye, and Gaoang Wang. Bringing rnns back to In Proefficient open-ended video understanding. ceedings of the IEEE/CVF International Conference on Computer Vision, pages 2345323465, 2025. 2, 3, 7 [94] Antoine Yang, Arsha Nagrani, Ivan Laptev, Josef Sivic, and Cordelia Schmid. Vidchapters-7m: Video chapters at scale. Advances in Neural Information Processing Systems, 36:4942849444, 2023. 1 [95] Antoine Yang, Arsha Nagrani, Paul Hongsuck Seo, Antoine Miech, Jordi Pont-Tuset, Ivan Laptev, Josef Sivic, and Cordelia Schmid. Vid2seq: Large-scale pretraining of visual language model for dense video captioning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1071410726, 2023. 5, 10, 11 [96] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. 7, 8, 11 [97] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. 1 [98] Jingkang Yang, Shuai Liu, Hongming Guo, Yuhao Dong, Xiamengwei Zhang, Sicheng Zhang, Pengyun Wang, Zitang Zhou, Binzhu Xie, Ziyue Wang, et al. In ProEgolife: Towards egocentric life assistant. ceedings of the Computer Vision and Pattern Recognition Conference, pages 2888528900, 2025. 1 [99] Senqiao Yang, Yukang Chen, Zhuotao Tian, Chengyao Wang, Jingyao Li, Bei Yu, and Jiaya Jia. Visionzip: Longer is better but not necessary in vision language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1979219802, 2025. [100] Linli Yao, Haoning Wu, Kun Ouyang, Yuanxing Zhang, Caiming Xiong, Bei Chen, Xu Sun, and Junnan Li. Generative frame sampler for long video understanding. In Findings of the Association for Computational Linguistics: ACL 2025, pages 17900 17917, Vienna, Austria, 2025. Association for Computational Linguistics. 2 [101] Jinhui Ye, Zihan Wang, Haosen Sun, Keshigeyan Chandrasegaran, Zane Durante, Cristobal Eyzaguirre, Yonatan Bisk, Juan Carlos Niebles, Ehsan Adeli, Li Fei-Fei, et al. Re-thinking temporal search In Proceedfor long-form video understanding. ings of the Computer Vision and Pattern Recognition Conference, pages 85798591, 2025. 2 [102] Xubing Ye, Yukang Gan, Xiaoke Huang, Yixiao Ge, and Yansong Tang. Voco-llama: Towards vision In Procompression with large language models. ceedings of the Computer Vision and Pattern Recognition Conference, pages 2983629846, 2025. 2 [103] Weihao Yu and Xinchao Wang. Mambaout: Do we In Proceedings of really need mamba for vision? the Computer Vision and Pattern Recognition Conference, pages 44844496, 2025. 3 [104] Zihao Yue, Qi Zhang, Anwen Hu, Liang Zhang, Ziheng Wang, and Qin Jin. Movie101: new movie In Proceedings of the understanding benchmark. 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 46694684, Toronto, Canada, 2023. Association for Computational Linguistics. 2 [105] Abhay Zala, Jaemin Cho, Satwik Kottur, Xilun Chen, Barlas Oguz, Yashar Mehdad, and Mohit Bansal. Hierarchical video-moment retrieval and In Proceedings of the IEEE/CVF step-captioning. Conference on Computer Vision and Pattern Recognition, pages 2305623065, 2023. 5, 10, 11 [106] Xiangyu Zeng, Kunchang Li, Chenting Wang, Xinhao Li, Tianxiang Jiang, Ziang Yan, Songze Li, Yansong Shi, Zhengrong Yue, Yi Wang, Yali Wang, Yu Qiao, and Limin Wang. Timesuite: Improving MLLMs for long video understanding via grounded tuning. In The Thirteenth International Conference on Learning Representations, 2025. 5, 10, 11 [107] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language imIn Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1369113701, 2025. 5 [117] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web instructional videos. In Proceedings of the AAAI conference on artificial intelligence, 2018. 5, 10, 11 [118] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang. Vision mamba: Efficient visual representation learning with bidirectional state space model. In Forty-first International Conference on Machine Learning, 2024. 3 [119] Itamar Zimerman, Ameen Ali, and Lior Wolf. Explaining modern gated-linear rnns via unified implicit attention formulation. In ICLR, 2025. 8 [120] Orr Zohar, Xiaohan Wang, Yann Dubois, Nikhil Mehta, Tong Xiao, Philippe Hansen-Estruch, Licheng Yu, Xiaofang Wang, Felix Juefei-Xu, Ning Zhang, et al. Apollo: An exploration of video understanding in large multimodal models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1889118901, 2025. 2, 3, 6 [121] Xin Zou, Di Lu, Yizhou Wang, Yibo Yan, Yuanhuiyi Lyu, Xu Zheng, Linfeng Zhang, and Xuming Hu. Dont just chase highlighted tokens in mllms: Revisiting visual holistic context retention. Advances in Neural Information Processing Systems, 2025. 2 [122] Jingwei Zuo, Maksim Velikanov, Ilyas Chahed, Younes Belkada, Dhia Eddine Rhayem, Guillaume Kunsch, Hakim Hacid, Hamza Yous, Brahim Farhat, Ibrahim Khadraoui, et al. Falcon-h1: family of hybrid-head language models redefining efficiency and performance. arXiv preprint arXiv:2507.22448, 2025. 2 In Proceedings of the IEEE/CVF age pre-training. international conference on computer vision, pages 1197511986, 2023. 2, [108] Ce Zhang, Taixi Lu, Md Mohaiminul Islam, Ziyang Wang, Shoubin Yu, Mohit Bansal, and Gedas Bertasius. simple LLM framework for long-range In Proceedings of the video question-answering. 2024 Conference on Empirical Methods in Natural Language Processing, pages 2171521737, Miami, Florida, USA, 2024. Association for Computational Linguistics. 2 [109] Jiazhao Zhang, Kunyu Wang, Shaoan Wang, Minghan Li, Haoran Liu, Songlin Wei, Zhongyuan Wang, Zhizheng Zhang, and He Wang. Uni-navid: videobased vision-language-action model for unifying embodied navigation tasks. Robotics: Science and Systems, 2025. 1 [110] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. 7 [111] Shaolei Zhang, Qingkai Fang, Zhe Yang, and Yang Feng. LLaVA-mini: Efficient image and video large multimodal models with one vision token. In The Thirteenth International Conference on Learning Representations, 2025. 5, 10 [112] Yuan Zhang, Chun-Kai Fan, Junpeng Ma, Wenzhao Zheng, Tao Huang, Kuan Cheng, Denis Gudovskiy, Tomoyuki Okuno, Yohei Nakata, Kurt Keutzer, et al. Sparsevlm: Visual token sparsification for efficient In International vision-language model inference. Conference on Machine Learning, 2025. 2 [113] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun MA, Ziwei Liu, and Chunyuan Li. LLaVA-video: Video instruction tuning with synthetic data. Transactions on Machine Learning Research, 2025. 5, 7, 10, [114] Zhi Zhang, Srishti Yadav, Fengze Han, and Ekaterina Shutova. Cross-modal information flow in mulIn Proceedings of timodal large language models. the Computer Vision and Pattern Recognition Conference, pages 1978119791, 2025. 5 [115] Han Zhao, Min Zhang, Wei Zhao, Pengxiang Ding, Siteng Huang, and Donglin Wang. Cobra: Extending mamba to multi-modal large language model for efficient inference. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1042110429, 2025. 2, 3 [116] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Zhengyang Liang, Shitao Xiao, Minghao Qin, Xi Yang, Yongping Xiong, Bo Zhang, et al. Mlvu: Benchmarking multi-task long video understanding."
        }
    ],
    "affiliations": [
        "AIM3 Lab, Renmin University of China",
        "MiLM Plus, Xiaomi Inc."
    ]
}