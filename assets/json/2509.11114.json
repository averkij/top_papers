{
    "paper_title": "WildSmoke: Ready-to-Use Dynamic 3D Smoke Assets from a Single Video in the Wild",
    "authors": [
        "Yuqiu Liu",
        "Jialin Song",
        "Manolis Savva",
        "Wuyang Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose a pipeline to extract and reconstruct dynamic 3D smoke assets from a single in-the-wild video, and further integrate interactive simulation for smoke design and editing. Recent developments in 3D vision have significantly improved reconstructing and rendering fluid dynamics, supporting realistic and temporally consistent view synthesis. However, current fluid reconstructions rely heavily on carefully controlled clean lab environments, whereas real-world videos captured in the wild are largely underexplored. We pinpoint three key challenges of reconstructing smoke in real-world videos and design targeted techniques, including smoke extraction with background removal, initialization of smoke particles and camera poses, and inferring multi-view videos. Our method not only outperforms previous reconstruction and generation methods with high-quality smoke reconstructions (+2.22 average PSNR on wild videos), but also enables diverse and realistic editing of fluid dynamics by simulating our smoke assets. We provide our models, data, and 4D smoke assets at [https://autumnyq.github.io/WildSmoke](https://autumnyq.github.io/WildSmoke)."
        },
        {
            "title": "Start",
            "content": "WildSmoke: Ready-to-Use Dynamic 3D Smoke Assets from Single Video in the Wild"
        },
        {
            "title": "Jialin Song Manolis Savva Wuyang Chen",
            "content": "Simon Fraser University {yuqiu_liu, jialin_song, msavva, wuyang_chen}@sfu.ca 5 2 0 2 4 1 ] . [ 1 4 1 1 1 1 . 9 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We propose pipeline to extract and reconstruct dynamic 3D smoke assets from single in-the-wild video, and further integrate interactive simulation for smoke design and editing. Recent developments in 3D vision have significantly improved reconstructing and rendering fluid dynamics, supporting realistic and temporally consistent view synthesis. However, current fluid reconstructions rely heavily on carefully controlled clean lab environments, whereas real-world videos captured in the wild are largely underexplored. We pinpoint three key challenges of reconstructing smoke in real-world videos and design targeted techniques, including smoke extraction with background removal, initialization of smoke particles and camera poses, and inferring multi-view videos. Our method not only outperforms previous reconstruction and generation methods with high-quality smoke reconstructions (+2.22 average PSNR on wild videos), but also enables diverse and realistic editing of fluid dynamics by simulating our smoke assets. We provide our models, data, and 4D smoke assets at https://autumnyq.github.io/WildSmoke. 1. Introduction Fluid phenomena, from the swirling eddies around highspeed train to the rolling smoke rings emitted by jet engine, are ubiquitous in the physical world. fundamental yet challenging task is to reconstruct unobserved physical quantities like velocity and density in the full spatiotemporal domain (3+1D), given visual inputs such as 2D photographs or video sequences. This problem is formally known as inferring 3D fluid fields from imagery, i.e., reconstructing fluid dynamics from visual observations. Successfully solving this problem enables applications such as high-fidelity smoke rendering in cinematic special effects [19, 34] and enhanced diagnostics in industrial fluid systems [2, 26, 31]. Recent breakthroughs in 3D vision have substantially advanced this area. Key contributions include multi-view benchmarks [8] that deliver high-quality flow videos with accurately calibrated camera poses. In addition, recent methods reconstruct fluid fields from video by jointly optimizing differentiable physics (via physics-based constraints) and neural representations (via rendering losses) [57, 11, 12, 38]. Despite notable advances, most existing approaches still rely heavily on controlled multi-view recordings for reconstructing fluid phenomena, which differ substantially from real-world smoke in the wild. Datasets such as ScalarFlow [8], TomoFluid [40], and FluidNexus [11] rely on fixed, precisely calibrated cameras, controlled smoke generation in laboratory settings, and pre-captured simplistic backgrounds. Such conditions are impractical for in-the-wild smoke, which is often recorded with single moving camera (e.g., handheld or drone footage), with cluttered backgrounds and unpredictable camera motion. At the same time, demand for ready-to-use 3D assets has surged across graphics and simulation applications [25, 35, 42]. Dynamic 3D smoke assets are particularly valuable for downstream visual effects editing and real-time simulations [1, 4, 43], owing to their intricate motion and evolving volumetric properties. Yet, despite their importance, the generation of dynamic 3D smoke assets remains largely underexplored. Motivated by the above challenges, we ask: Given single in-the-wild video, how can we recover the underlying fluid field and enable downstream applications using the reconstructed smoke assets? In this work, we aim to reconstruct ready-to-use dynamic 3D smoke assets from single in-the-wild video. We identify three key challenges of reconstructing smoke fields from the wild (Figure 1, Section 2.2): noisy backgrounds, unknown camera poses, and coupled camera viewpoints and timesteps. Correspondingly, we introduce three key techniques: 1) Incontext smoke segmentation, plus dehazing for light smoke background removal; 2) Initializations of smoke particles and pose estimation; 3) Decoupling spatial and temporal camera trajectories by multi-view generation and local pose perturbation. More importantly, we demonstrate practical applications of our reconstructed smoke assets through fluid 1 2.2. Smoke Reconstruction in the Wild: Challenges We identify three challenges when reconstructing smoke from single video in the wild (Figure 1): Noisy Backgrounds and Boundaries. High-quality videos of real smoke like ScalarFlow [8], TomoFluid [40], and FluidNexus [11] were collected with carefully controlled backgrounds and further post-processed to remove environmental noise. However, videos in the wild are inevitably contaminated by complex backgrounds visible through semitransparent smoke. Unknown Camera Poses. Although videos of smoke in the wild are captured with moving cameras, their camera poses are typically neither measured nor released with the footage. Single-Camera Trajectory. As video of smoke in the wild typically provides only one camera trajectory over time, spatial viewpoints and temporal frames become inherently entangled: there is an (approximately) one-to-one correspondence between camera viewpoints and timesteps. 3. Methods To address the above challenges, we propose unified pipeline to reconstruct clean, background-free smoke from unconstrained real-world videos, as shown in Figure 2. 3.1. Smoke Extraction In real-world scenarios, smoke often exhibits complex, irregular boundaries and is intertwined with significant noise from backgrounds. To address this issue, we propose pipeline that automatically extracts clean smoke regions from single in-the-wild video. (a) Smoke Mask Extraction. We first segment the smoke and obtain binary mask sequence = {Mt} over all timesteps in the video. Because off-the-shelf semantic segmentation does not generalize well to in-the-wild smoke, we adopt one-shot learning, using SAM [20] for annotation and SegGPT [33] for propagation. For each video, we select an early reference frame Iref , interactively annotate its 1, and then feed the pair smoke with SAM to obtain Mref (Iref , Mref ) to SegGPT [33] to perform one-shot inference and produce masks Mt for the remaining frames It. This combines SAMs high-quality per-frame annotation with SegGPTs frame-wise generalization. The boundary of segmented smoke could still be noisy. We apply Gaussian filter fg to smooth the boundary of inferred masks, yielding more natural smoke contour. The smoke areas are extracted via St = fg(Mt) It. As shown in Figure 3(a), this in-context segmentation can reliably segment smoke boundaries. See Appendix B.1 in the supplement for more smoke segmentation examples. 1We use an online SAM-based annotator https://roboflow.com/. Figure 1. Challenges of smoke reconstruction from single inthe-wild video: 1) noisy backgrounds and boundaries; 2) unknown camera poses; 3) single video with coupled camera viewpoints and timesteps. simulation. Our main contributions are summarized as follows: 1. We design comprehensive pipeline that extracts and reconstructs dynamic 3D smoke assets from diverse and noisy real-world videos (Section 3). 2. Compared with current fluid field reconstruction or 3D generation methods, our pipeline can faithfully reconstruct accurate and dynamic 3D smoke assets from single in-the-wild video, achieving an average improvement of +2.22 dB PSNR on in-the-wild videos (Section 4.3). 3. Our smoke assets are ready-to-use: we support realistic editing of smoke through interactive fluid simulations (Section 3.5 and 4.4). 2. Background 2.1. Smoke Reconstruction from Videos Given single-view video capturing upward-rising smoke, our goal is to reconstruct dynamic 3D representation of the smoke field over time (t = 1, , ) that is coherent, view-consistent, and editable. Following FluidNexus [11], we represent fluid with two types of particles. , ut RN phy 3, where phy 1. 3D physical particles for positions and velocities: pphy is the number of physt ical particles at t. The density field œÅt : R3 (cid:55) and velocity field Vt : R3 (cid:55) R3 can be further mapped from particles to grids via kernel-weighted interpolation [11, 1517, 24]. 2. vis visual particles (grayscale) at with attributes: 3, ct RN vis RN vis 3, ot {pvis RN vis , rt RN vis 4}, representing position, color, scale, opacity, and rotation, respectively. , st RN vis 2 Figure 2. To reconstruct smoke from single video in the wild, our pipeline includes five steps: 1) Smoke extraction, with background removed for light smoke due to translucence (Sec. 3.1); 2) Pose estimation and coarse initialization for 3D point cloud (Sec. 3.2); 3) Inferring multi-views for smoke (Sec. 3.3); 4) Training smoke particles (Sec. 3.4); 5) composition and simulation of 4D smoke assets (Sec. 3.5). SAM: segment anything [20]. DUSt3R [32]. At this stage, it is necessary to further distinguish dense from light smoke. We categorize them based on the visibility of the background. For dense smoke, the background is completely occluded due to high optical density (low transmittance). In this case, background contamination is negligible, and the smoke can be directly treated as the masked region. However, for the light smoke, the background remains partially visible (nonzero transmittance). Here, dehazing step is required to remove background leakage before smoke reconstruction. (b) Dehazing for Background Removal. Similar to fog or haze, background objects remain visible through light smoke due to light transmission, introducing artifacts during reconstruction. For light smoke, we fine-tune pretrained DehazeFormer [29] to isolate the smoke and remove the background. We construct fine-tuning dataset of synthetic smoky inputs (pixels normalized to [0, 1]) by blending coarsely extracted smoke with video frames of clean background without smoke clean, which serve as supervision targets: = clean + S, (1) where = 1 is the coarse transmission map, representing the amount of background light that reaches the camera through the smoke. We estimate the atmospheric light using the dark channel prior [13]. As shown in Figure 3 (c), the fine-tuned DehazeFormer can remove the foreground smoke and recover the background clean. For dense smoke where background contamination is negligible, the extracted smoke from masks can be used directly without this dehazing step. (c) Extract Clean Smoke. For light smoke, we extract the clean foreground smoke based on Equation (1): = 1 clean , (2) 3 where is the smoky frame in the original video, and clean is the background recovered by DehazeFormer. The resulting foreground-smoke images are then used for smoke-field reconstruction. 3.2. Pose Estimation and Coarse Geometry To obtain camera intrinsics and extrinsics for frames, we initialize camera poses and focal lengths with estimates from the pretrained DUSt3R [32]. We further initialize both physical and visual particles from sparse 3D point cloud {pi}N i=1 estimated by DUSt3R, which provides rough geometry of the scene, including both smoke and background regions. We filter the DUSt3R-generated 3D point cloud using the extracted smoke masks, retaining only the smoke-relevant points to initialize our particles. Although DUSt3R produces coarse results, this initialization provides meaningful spatial priors for optimizing our particles; without it, training may fail to converge. See more details in Appendix A.2 in the supplement. 3.3. Inferring Multi-View Videos In wild smoke videos, cameras typically follow single trajectory. For example, drone may fly from the ground upwards along with the smoke plume to capture the video. Although the camera may span wide spatiotemporal extent, camera viewpoints and timesteps are highly coupled, i.e., each camera viewpoint is associated with unique timestep, and vice versa. This coupling can cause overfitting and degrade novel-view synthesis for unseen viewpoints or timesteps. To decouple the spatiotemporal camera trajectory, in our work we employ generative multi-view synthesis. To obtain multi-view supervision from single-view input, we use pretrained SV4D 2.0 [37] to generate smoke videos from novel views. We choose azimuth angles of [10, 10, 20, 30] Figure 3. Smoke extraction, with background removal for light smoke. relative to the pose of the current frame as our novel viewpoints. Since SV4D 2.0 does not support long temporally consistent video generation, we split our video sequences into short clips, and overlap one frame between neighboring clips, ensuring valid conditioning for every novel-view segment. Per-frame camera-to-world poses are obtained by applying the angle set to the DUSt3R-initialized poses. Moreover, to address the unreliable frames produced by SV4D at later timesteps, we apply an exponentially decaying weight over the frame index. This strategy progressively down-weights the influence of later frames generated by SV4D 2.0 during Gaussian-particle training. See Appendix A.4 and A.5 for more details. 3.4. Training Gaussian Particles Following the training strategy of FluidNexus [11], we separately train visual and physical particle representations. Particles are optimized by minimizing photometric errors between input frames and rendered views using 3D Gaussian Splatting [18]. Physical particles are further regularized using position-based fluid (PBF) simulation [22, 23] with an incompressibility constraint. Local Pose Perturbation. To further disentangle spatial and temporal viewpoints in single-camera trajectories from in-the-wild videos, we not only train with our generated multi-view trajectories (Sec. 3.3), but also progressively enrich the view trajectory by perturbing local camera poses. Specifically, after the particles at time converge, we introduce perturbed viewpoints by shifting the camera pose forward by along the trajectory (modulo the sequence length ). Concretely, together with the original pose (Rt, t), we also include (R(t+t) mod , t) as an input and its corresponding rendering result as the target. Here, is the rotation matrix of the cameras extrinsics; , i.e., we perturb within very short period relative to the whole temporal domain. Essentially, we effectively decouple viewpoint and timestep by associating pose R(t+t) mod with timestep via local pose perturbation. Since is small, perturbed novel viewpoints still reside in neighborhoods of the original video trajectory. In practice, we progressively increase from 2 to 4 during training. We refer readers to Figure 4. Local pose perturbation. Given the original camera pose (Rt, t) on single-video trajectory, we additionally introduce perturbed viewpoint by shifting the camera pose by along the trajectory with wrap-around (t + t) mod . For this perturbed viewpoint, the rendered image serves as supervision. Both original and perturbed samples are used during following training. Appendix in the supplement for more training details. 3.5. Ready-to-Use 4D Smoke Assets key use case of our reconstructed smoke assets is visualIn our effects (VFX) editing of novel smoke scenes. work, we consider simulating smoke interactions using PhiFlow [14]. The simulation is initialized with the reconstructed density field œÅ from visual particles, and velocity field from physical particles: Density Field from Visual Particles. For each visual particle with center pvis t,i, scale st,i, rotation rt,i, and opacity ot,i, we define Gaussian kernel (cid:18) œït,i(x) = exp 1 2 (x pvis t,i)Œ£1 t,i (x pvis t,i) (cid:19) , (3) t,i) R(rt,i), rt,i R4 is where Œ£t,i = R(rt,i) diag(s2 the unit quaternion representing the particle rotation, and R(rt,i) R33 is the corresponding rotation matrix obtained from rt,i. The density field is the weighted sum of all 4 particle kernels: œÅt(x) = vis t(cid:88) i=1 ot,i œït,i(x). (4) Velocity Field from Physical Particles. Given physical particle with position pphy t,i and velocity ut,i, we map them to the grid. The Gaussian kernel œï t,i for the velocity field has the same form as œït,i but is centered at pphy t,i : Vt(x) = (cid:80)N phy i=1 œï i=1 œï (cid:80)N phy t,i(x) ut,i t,i(x) + Œµ , (5) here we use Œµ = 108. To ensure spatial consistency between the density and velocity fields, both visual and physical splatting are restricted to the same bounding region defined by the visual particles. After the initialization of density and velocity fields, we continue the fluid dynamics in PhiFlow [14] with the standard MacCormack semi-Lagrangian method [27] for advection and the projection-based pressure-Poisson solver for incompressible flow. We consider either external wind forces or an inserted obstacle to interact with the smoke. 4. Experiments 4.1. Settings Datasets. We evaluate our pipeline on both synthetic and real-world collections of smoke, assessing the reconstruction quality using view rendering. We synthesize two high-resolution and photorealistic smoke videos by combining 4D smoke VDB sequence with real 3D scene. These paired videos, rendered with distinct camera trajectories, allow direct comparison between our reconstructed results and the ground-truth. Specifically, we download synthetic 4D smoke VDB sequence and 3D scene from CGTrader2, combine them and render two videos in Blender3 with two camera trajectories. One trajectory is used for training and another for evaluation. See Appendix A.1 for more rendered samples. We also evaluate on two real-world testbeds: 1) FLAME dataset [28] is fire-imaging dataset collected by drones during prescribed burning of piled detritus in an Arizona pine forest. The dataset includes video recordings and thermal heatmaps captured by infrared cameras. 2) We further collect three videos of smoke from Pixabay4, covering diverse scenarios. 2https://www.cgtrader.com/3d-models 3https://www.blender.org 4Pixabay shares royalty-free images, videos, audio, and other media. All content is released by Pixabay under the Content License, which makes it safe to use without asking for permission or giving credit to the artist. See their license at https://pixabay.com/service/terms 5 To unify our training and evaluation settings, we standardize all our videos to 270 frames; training timesteps are = 1, . . . , with = 240, and unseen future timesteps are = + 1, . . . , with = 270. We measure view changes along the video trajectory via the cameras relative rotation angle Œ∏t1,t2 = arccos (cid:18) tr(Rt1,t2)1 (cid:19) 2 , where Rt1,t2 = t Rt1 (R is the rotation matrix of the cameras extrinsics). In our synthetic data setting, the camera undergoes rotation of 53 from = 1 to = , which is larger than the 7 rotation during the future steps from = + 1 to = . Smoke in our videos is always centered in the scene. Tasks. As explained by Œ∏1,T and Œ∏T +1,T above, larger pose changes occur within the training timesteps than in the future steps. Synthesizing frames from the fixed viewpoint at the beginning of video (t = 1) is more challenging than following the camera pose trajectory. Therefore, in our evaluation, the novel view is defined with camera pose at = 1, and the input view follows the ground-truth camera trajectory. Following [38], we consider the following two tasks: Novel view synthesis: We render smoke views from the fixed novel viewpoint over the training timesteps. Specifically, we fix the camera pose at = 1 and synthesize novel views through = 2, , . We study novel view synthesis only on our synthetic dataset, due to the lack of ground-truth multi-view videos on real-world videos (FLAME and Pixabay). Future prediction: We extrapolate the fluid dynamics into the future steps = + 1, , . No model is ever trained with ground-truth future frames from videos. On synthetic videos, we study both quantitative and visual results, and predict futures for both input view (i.e. follow the ground-truth camera pose trajectory during = + 1, , ) and novel view (fixed camera pose at = 1). Due to the lack of ground-truth multi-view videos, on realworld videos (FLAME and Pixabay), we can only study future predictions based on the input view. During inference, the learned velocity field is used to advect (evolve) the visual particles for future prediction. We refer the reader to [38] for more details about these tasks. Evaluation Metrics. We report the peak signal-to-noise ratio (PSNR) averaged over frames, and defer the structural similarity index measure (SSIM) and the perceptual metric LPIPS [41] in Appendix B.3. These metrics are also widely adopted in prior deblurring works [9, 10]. Baselines. We compare with both reconstruction and generation methods, including HyFluid [38], FluidNexus [11], and Trellis [35]. We follow FluidNexus to train with the default grayscale input setting. Figure 5. Visualization of novel view synthesis and future predictions on synthetic smoke videos. Novel view uses the camera pose at = 1, and input view means the camera poses along the source video. Training uses = 240 frames, and the unseen future extends to = 270 frames. GT: Ground Truth. Table 1. Comparing PSNR (higher is better) of smoke reconstruction by different methods on the synthetic dataset. Novel view uses the camera pose at = 1, and input view means the camera poses along the source video. Methods Novel View Synthesis Future Prediction (Input View) Future Prediction (Novel View) Trellis [35] HyFluid [38] FluidNexus [11] Ours 19.98 24.26 29.26 29.78 - 22.54 23.61 25.26 - 22.18 21.54 25. Table 2. Cumulative ablation study (PSNR) on the synthetic smoke video. Novel view uses the camera pose at = 1, and input view means the camera poses along the source video. Novel View Synthesis Future Prediction (Input View) Future Prediction (Novel View) Baseline + Smoke Extraction + DUSt3R Init. + Local Perturbation + Multi-Views (Ours) 22.55 29.26 29.48 29.77 29. 16.69 23.61 26.45 26.85 25.26 18.17 21.54 22.92 23.59 25.04 4.2. Synthetic Data Results. We first show quantitative results in Table 1 and visualizations in Figure 5. Trellis [35] fits static 3D asset per frame (image-to-3D), estimating neither time-varying particles nor velocities; hence future prediction is ill-defined. Moreover, per-frame inconsistency in the novel-view synthesis setting causes the smoke to progressively fade, yielding black frame at time . Compared with HyFluid [38] and FluidNexus [11], our pipeline achieves higher PSNR and more stable visualizations. Ablation Study. We further provide ablation studies in Table 2; see visualization results in Appendix B.5. Smoke extraction (segmentation), initializing poses/particles, and adding inferred multi-view supervision progressively improve PSNR and visual quality. Note that, due to significant spatiotemporal deviations, the task of future prediction at novel view is substantially more challenging than either novel view synthesis or future prediction at input view. As result, future prediction at novel view demands additional geometric cues, even coarse ones generated by SV4D, and therefore benefits most (+1.45 PSNR from 23.59 to 25.04) from incorporating + Multi-Views. 6 Figure 6. Visualization of future predictions (input view) on the FLAME dataset [28]. Training uses = 240 frames, and the unseen future extends to = 270 frames. GT: Ground Truth. Table 3. Comparing PSNR (higher is better) of smoke reconstruction by different methods on the FLAME dataset [28]. Future Prediction (Input View) HyFluid [38] FluidNexus [11] Ours 21.67 21.78 22.88 Table 4. Comparing PSNR (higher is better) of smoke reconstruction by different methods on videos collected from Pixabay. Future Prediction (Input View) City Valley Forest HyFluid [38] FluidNexus [11] Ours 23.24 24.18 24.68 12.43 16.44 20.42 13.70 14.63 17. 4.3. Smoke Videos in the Wild We further evaluate on in-the-wild smoke videos. Due to the lack of ground-truth novel views in real videos, here we evaluate only the task of future prediction at input view, meaning the camera poses along the source video. FLAME [28]. The FLAME dataset includes dronecaptured videos in the wild forest. The smoke is light, and thus background removal is necessary. We show the results in Table 3 and Figure 6. Pixabay. We evaluate three thick-smoke videos from Pixabay. We show the results in Table 4 and Figure 7. Overall, across both light and dense smoke over diverse real-world videos, our method outperforms prior work, achieving an average PSNR improvement of +2.22 dB. Qualitatively, the reconstructions remain consistent and blend back into the original backgrounds. Figure 7. Visualization of future predictions (input view) on videos collected from Pixabay. Training uses = 240 frames, and the unseen future extends to = 270 frames. GT: Ground Truth. 4.4. Interactive Simulations Finally, we demonstrate interactive simulations enabled by our fluid reconstruction. We consider two scenarios: (i) external wind (both global and local forces), and (ii) rigid spherical obstacle. For the global wind, we apply global constant force fw = (0.005, 0, 0) uniformly across the domain. The local wind uses the same magnitude and direction but is confined to sphere of radius 30 at the scene center. For the obstacle case, we place rigid ball (radius 10) centered at (50, 70) and simulate the resulting interactions. All simulations use the reconstruction from the city video from Pixabay (Figure 7 top); results are shown in Figure 8 (XY-plane from Z-axis). We can see that our smoke assets support realistic editing via diverse simulation scenarios. 7 Table 5. Video resolutions (W ) and costs of training and inference of our pipeline. Dataset Synthetic (1080 1920 270) FLAME (1920 1080 270) Pixabay (1920 1080 270) Stage (GPU Hours) Training Inference Training Inference Training Inference Smoke Extraction Background Removal Multi-Views Gaussian Particles - - - 1.03 0.03 0.02 2.35 0.01 - 2.06 - 1.17 0.03 0.01 2.56 0. - - - 1.76 0.03 0.01 2.55 0.01 Total 3.44 5.84 4. smoke assets remains largely underexplored. 5.2. Physics-based Fluid Simulation and Editing Classical fluid simulation in computer graphics has been broadly categorized into Eulerian [30] and Lagrangian formulations [3]. Production solvers edited flows by manipulating external forces, boundary conditions, and solidfluid interactions (obstacles), which underpinned VFX and interactive applications. Recent differentiable-physics systems such as DiffTaichi [16] and PhiFlow [14] made long-horizon, gradient-based editing practical. These tools provided useful platforms for controlled physical interactions. Building on these advances, our work reconstructs smoke directly from in-the-wild videos and integrates simulation for realistic editing, extending such physics-based editing to unconstrained real-world scenarios. Figure 8. Visualizations of simulations with our smoke assets in different interaction scenarios. means simulation time, different from the video frames we used in previous figures. The simulation starts with particles reconstructed at the last frame of the city video from Pixabay (Figure 7 top). 4.5. Training and Inference Cost 5.3. 4D Generation Table 5 reports the per-stage GPU hours. Across datasets and resolutions, our pipeline reconstructs smoke from inthe-wild videos in 4.47 GPU-hours on average among all 5 videos. Notably, with the DUSt3R-based pose initialization, smoke can be localized without seeding large number of particles, reducing the Gaussian-particle training stage by 1.5 GPU-hours. 5. Related Works 5.1. Fluid Field Extraction from Videos series of recent works tackled fluid reconstruction from videos, but most relied on controlled, multi-view recordings that differed substantially from in-the-wild footage. PINF [5] coupled NavierStokes PDEs with continuous spatiotemporal NeRF to recover flow; NeuroFluid [12] introduced particle-driven neural renderer that embedded fluid properties and particle transition model; HyFluid [38] estimated hybrid neural fields for density and velocity using physicsbased losses; FluidNexus [11] reconstructed smoke from single video by leveraging generative priors, yet its multiview generator was fine-tuned on laboratory smoke with fixed, calibrated cameras and clean backgrounds. To our knowledge, reconstructing fluid fields from single in-thewild video and turning them into ready-to-use dynamic 3D Recent advances in generative modeling have markedly improved image-to-3D and image-to-novel-view-synthesis, and video diffusion models with stabilized virtual cameras produced short clips with degree of multi-view consistency [21, 39, 44]. SV4D [36] and SV4D 2.0 [37] extended view-consistent generation to dynamic scenes, reinforcing temporal continuity. In our pipeline, we build on this generative consistency to obtain auxiliary multi-view trajectories, but go further by reconstructing physically plausible dynamic smoke fields. 6. Conclusion In this paper, we addressed the challenge of recovering time-varying 3D smoke fields from single in-the-wild video and producing ready-to-use dynamic assets. The proposed pipeline tackles three core challengesnoisy backgrounds, unknown camera poses, and coupled spatiotemporal viewsthrough one-shot segmentation and dehazing, pose estimation, and decoupled spatiotemporal trajectory through multi-view generation and local pose perturbation. We further showed that the reconstructed assets support physically consistent editing through fluid simulation, aligning visual fidelity with physical plausibility and supporting downstream simulation workflows."
        },
        {
            "title": "Acknowledgement",
            "content": "We thank Dr. Jiajun Wu and Hong-Xing Koven Yu for helpful comments and suggestions."
        },
        {
            "title": "References",
            "content": "[1] Kai Bai, Wei Li, Mathieu Desbrun, and Xiaopei Liu. Dynamic upsampling of smoke through dictionary-based learning. ACM Transactions on Graphics (TOG), 40(1):119, 2020. [2] Samuel Baker, Michael Hobley, Isabel Scherl, Xiaohang Fang, Felix CP Leach, and Martin Davy. Enginebench: flow reconstruction in the transparent combustion chamber iii optical engine. arXiv preprint arXiv:2406.03325, 2024. [3] Andrew Bennett. Lagrangian fluid dynamics. Cambridge University Press, 2006. [4] Mengyu Chu, Nils Thuerey, Hans-Peter Seidel, Christian Theobalt, and Rhaleb Zayer. Learning meaningful controls for fluids. ACM Transactions on Graphics (TOG), 40(4):113, 2021. [5] Mengyu Chu, Lingjie Liu, Quan Zheng, Erik Franz, HansPeter Seidel, Christian Theobalt, and Rhaleb Zayer. Physics informed neural fields for smoke reconstruction with sparse data. ACM Transactions on Graphics (ToG), 41(4):114, 2022. [6] Yitong Deng, Hong-Xing Yu, Jiajun Wu, and Bo Zhu. Learning vortex dynamics for fluid inference and prediction. arXiv preprint arXiv:2301.11494, 2023. [7] Yitong Deng, Hong-Xing Yu, Diyang Zhang, Jiajun Wu, and Bo Zhu. Fluid simulation on neural flow maps. ACM Transactions on Graphics (TOG), 42(6):121, 2023. [8] Marie-Lena Eckert, Kiwon Um, and Nils Thuerey. Scalarflow: large-scale volumetric data set of real-world scalar transport flows for computer animation and machine learning. ACM Transactions on Graphics (TOG), 38(6):116, 2019. [9] Kupyn et al. Deblurgan: Blind motion deblurring using conditional adversarial networks. In CVPR, 2018. [10] Nah et al. Deep multi-scale convolutional neural network for dynamic scene deblurring. In CVPR, 2017. [11] Yue Gao, Hong-Xing Yu, Bo Zhu, and Jiajun Wu. Fluidnexus: 3d fluid reconstruction and prediction from single video. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2609126101, 2025. [12] Shanyan Guan, Huayu Deng, Yunbo Wang, and Xiaokang Yang. Neurofluid: Fluid dynamics grounding with particledriven neural radiance fields. In International Conference on Machine Learning, pages 79197929. PMLR, 2022. [13] Kaiming He, Jian Sun, and Xiaoou Tang. Single image haze IEEE transactions on removal using dark channel prior. pattern analysis and machine intelligence, 33(12):23412353, 2010. [14] Philipp Holl and Nils Thuerey. Œ¶flow (PhiFlow): Differentiable simulations for pytorch, tensorflow and jax. In International Conference on Machine Learning. PMLR, 2024. [15] Yuanming Hu, Tzu-Mao Li, Luke Anderson, Jonathan RaganKelley, and Fr√©do Durand. Taichi: language for highperformance computation on spatially sparse data structures. ACM Transactions on Graphics (TOG), 38(6):201, 2019. [16] Yuanming Hu, Luke Anderson, Tzu-Mao Li, Qi Sun, Nathan Carr, Jonathan Ragan-Kelley, and Fr√©do Durand. Difftaichi: Differentiable programming for physical simulation. ICLR, 2020. [17] Yuanming Hu, Jiafeng Liu, Xuanda Yang, Mingkuan Xu, Ye Kuang, Weiwei Xu, Qiang Dai, William T. Freeman, and Fr√©do Durand. Quantaichi: compiler for quantized simulations. ACM Transactions on Graphics (TOG), 40(4), 2021. [18] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk√ºhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. [19] Byungsoo Kim, Vinicius Azevedo, Markus Gross, and Barbara Solenthaler. Lagrangian neural style transfer for fluids. ACM Transactions on Graphics (TOG), 39(4):521, 2020. [20] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 40154026, 2023. [21] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object. In Proceedings of the IEEE/CVF international conference on computer vision, pages 9298 9309, 2023. [22] Miles Macklin and Matthias M√ºller. Position based fluids. ACM Transactions on Graphics (TOG), 32(4):112, 2013. [23] Miles Macklin, Matthias M√ºller, Nuttapong Chentanez, and Tae-Yong Kim. Unified particle physics for real-time applications. ACM Transactions on Graphics (TOG), 33(4):112, 2014. [24] Matthias M√ºller, David Charypar, and Markus Gross. Particlebased fluid simulation for interactive applications. In Proceedings of the 2003 ACM SIGGRAPH/Eurographics symposium on Computer animation, pages 154159, 2003. [25] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. [26] Pankaj Saini, Christoph Arndt, and Adam Steinberg. Development and evaluation of gappy-pod as data reconstruction technique for noisy piv measurements in gas turbine combustors. Experiments in Fluids, 57(7):122, 2016. [27] Andrew Selle, Ronald Fedkiw, Byungmoon Kim, Yingjie Liu, and Jarek Rossignac. An unconditionally stable maccormack method. Journal of Scientific Computing, 35(2):350371, 2008. [28] Alireza Shamsoshoara, Fatemeh Afghah, Abolfazl Razi, Liming Zheng, Peter Ful√©, and Erik Blasch. Aerial imagery pile burn detection using deep learning: The flame dataset. Computer Networks, 193:108001, 2021. [29] Yuda Song, Zhuqing He, Hui Qian, and Xin Du. Vision transformers for single image dehazing. IEEE Transactions on Image Processing, 32:19271941, 2023. [30] Mhamed Souli and David Benson. Arbitrary Lagrangian Eulerian and fluid-structure interaction: numerical simulation. John Wiley & Sons, 2013. 9 videos with generative gaussian splatting. arXiv preprint arXiv:2503.00868, 2025. [44] Jensen Zhou, Hang Gao, Vikram Voleti, Aaryaman Vasishta, Chun-Han Yao, Mark Boss, Philip Torr, Christian Rupprecht, and Varun Jampani. Stable virtual camera: Generative view synthesis with diffusion models. arXiv preprint arXiv:2503.14489, 2025. [31] Nils Thuerey, Konstantin Wei√üenow, Lukas Prantl, and Xiangyu Hu. Deep learning methods for reynolds-averaged navierstokes simulations of airfoil flows. AIAA Journal, 58 (1):2536, 2020. [32] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d In Proceedings of the IEEE/CVF Convision made easy. ference on Computer Vision and Pattern Recognition, pages 2069720709, 2024. [33] Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Images speak in images: generalist Tiejun Huang. In Proceedings of painter for in-context visual learning. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 68306839, 2023. [34] Xiaokun Wang, Yanrui Xu, Sinuo Liu, Bo Ren, Jiri Kosinka, Alexandru Telea, Jiamin Wang, Chongming Song, Jian Chang, Chenfeng Li, et al. Physics-based fluid simulation in computer graphics: Survey, research trends, and challenges. Computational Visual Media, pages 156, 2024. [35] Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation. arXiv preprint arXiv:2412.01506, 2024. [36] Yiming Xie, Chun-Han Yao, Vikram Voleti, Huaizu Jiang, and Varun Jampani. Sv4d: Dynamic 3d content generation with multi-frame and multi-view consistency. arXiv preprint arXiv:2407.17470, 2024. [37] Chun-Han Yao, Yiming Xie, Vikram Voleti, Huaizu Jiang, and Varun Jampani. Sv4d 2.0: Enhancing spatio-temporal consistency in multi-view video diffusion for high-quality 4d generation. arXiv preprint arXiv:2503.16396, 2025. [38] Hong-Xing Yu, Yang Zheng, Yuan Gao, Yitong Deng, Bo Zhu, and Jiajun Wu. Inferring hybrid neural fluid fields from videos. Advances in Neural Information Processing Systems, 36, 2024. [39] Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, Tien-Tsin Wong, Ying Shan, and Yonghong Tian. Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis. arXiv preprint arXiv:2409.02048, 2024. [40] Guangming Zang, Ramzi Idoughi, Congli Wang, Anthony Bennett, Jianguo Du, Scott Skeen, William Roberts, Peter Wonka, and Wolfgang Heidrich. Tomofluid: Reconstructing dynamic fluid from sparse view videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18701879, 2020. [41] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. [42] Zibo Zhao, Zeqiang Lai, Qingxiang Lin, Yunfei Zhao, Haolin Liu, Shuhui Yang, Yifei Feng, Mingxin Yang, Sheng Zhang, Xianghui Yang, et al. Hunyuan3d 2.0: Scaling diffusion models for high resolution textured 3d assets generation. arXiv preprint arXiv:2501.12202, 2025. [43] Zhiwei Zhao, Alan Zhao, Minchen Li, and Yixin Hu. 3d dynamic fluid assets from single-view Vid2fluid: WildSmoke: Ready-to-Use Dynamic 3D Smoke Assets from Single Video in the Wild"
        },
        {
            "title": "Supplementary Material",
            "content": "ent conventions (GS looks along Z), we convert DUSt3R camera-to-world poses by negating the and axes (i.e., multiply both by 1). An example of estimated poses and point cloud is shown in Figure 10. Formally, for each DUSt3R-predicted pose C, we apply = C, = diag(1, 1, 1, 1), where flips the and axes. A. Technical Details A.1. Synthetic Data To construct the synthetic dataset for our evaluation on novel view synthesis, we obtain 4D smoke sequence and static 3D scene from CGTrader. The sequence is placed into the 3D scene and rendered along prescribed camera path. For the training video, the camera pitch is fixed at Œ∏p = 10, and the azimuth is swept linearly from 105 to 45 at constant speed: œï(t) = 105 + 60 , [0, ], (6) where = 270. Rendering is performed with Blender Cycles (max samples = 200, i.e., each pixel is estimated by averaging up to 200 light paths to reduce Monte Carlo noise). Representative frames are shown in Figure 9. In the testing video for the novel view synthesis, the camera is fixed at œï(t = 0) = 105 and Œ∏p = 10, i.e., the camera pose at the first frame. During inference, we use the pose estimated by DUSt3R at the first frame from the training video as the input pose. Figure 10. Camera poses and foreground smoke points (for the valley video in Figure 7 (middle) from Pixabay) predicted by DUSt3R. Blue arrows are DUSt3R camera-to-world poses, gray dots are the reconstructed foreground point clouds. A.3. Particle Initialization Figure 9. Frames in our synthetic smoke video. When training the reconstruction, the camera view is moving along trajectory defined by Equation 6. During testing, we evaluate the novel view synthesis with the fixed camera pose from the first frame (œï(t = 0)). A.2. Pose Estimation We infer per-frame camera poses with pretrained DUSt3R in the one-reference mode, using the first frame as reference. Since DUSt3R and Gaussian Splatting (GS) use differTo accelerate convergence, we initialize both physical and visual particles from the foreground point cloud predicted by DUSt3R. Let RT HW 3 denote the 3D points (camera-to-world coordinates) for all frames, where Xt,h,w R3 is the 3D position of pixel (h, w) at time t. We obtain reliable smoke foreground by intersecting the segmentation masks Mt with the DUSt3R confidence mask conf , and collect the foreground set: R3 (cid:12) (cid:110) (cid:12) = Xt,h,w, Mt(h, w)M conf (cid:12) (h, w) = 1 (cid:111) . = 1 As DUSt3R and Gaussian Splatting (GS) follow different axis conventions (GS looks along Z), we convert point coordinates by negating the and axes (multiply both by 1, not swapping them). With = diag(1, 1, 1), we define (cid:110) = (cid:12) (cid:12) (cid:12) (cid:111) . After that, we downsample with voxel grid, which merges points falling into the same 3D cell into single representative, to control the initial particle count. We retain 100300 foreground points per video. The coordinates of the points are then used to initialize our physical and visual particles. A.4. Generated Videos SV4D 2.0 takes yaw/pitch in radians, so we recover each frames camera pose by applying the generation-time angle offsets to the DUSt3R pose, while using the DUSt3Rpredicted point cloud center as the fixed look-at target. Let the DUSt3R pose of the input frame be P0 = [R0 t0] R34, and let R3 be the scene center estimated from the DUSt3R point cloud. Given per-frame angle offsets (in radians) œït (yaw/azimuth) and Œ∏t (pitch), define Ry(œï) = Rx(Œ∏) = cos œï 0 0 1 sin œï 0 cos œï sin œï 0 , 1 0 cos Œ∏ sin Œ∏ cos Œ∏ 0 sin Œ∏ 0 . A.7. High-Frequency Information Loss To sharpen fine details, we add frequency-domain loss during visual-particle training on original input frames only (generated views are excluded due to unreliable highfrequency content). Given an RGB frame and ground truth ÀÜI, we compute per-channel 2D FFTs = F{I}, ÀÜF = F{ ÀÜI}, and define amplitude = , ÀÜA = ÀÜF and phase Œ≤ = , ÀÜŒ≤ = ÀÜF . The loss is computed as the average absolute difference of amplitude and phase over all frequency bins and RGB channels: Lfreq = mean(cid:0)A ÀÜA(cid:1) + mean(cid:0)Œ≤ ÀÜŒ≤(cid:1). We apply linear warm-up weight wt = min(cid:0)1, iter/t) and scale by Œªfreq: LFFT = Œªfreq wt Lfreq, here we set Œªfreq = 0.001. B. More Results B.1. Smoke Extraction Extracted Smoke Examples. Figure 11 shows representative results of our smoke extraction across several in-the-wild videos. For light smoke, we apply dehazing to remove background leakage; for dense smoke, we use the masked result directly as the clean foreground. apply the world-space = We Ry(œït) Rx(Œ∏t) about pivot c. The pose of frame is: rotation Rt Rt = Rt R0, tt = Rt (t0 c) + c, Pt = [Rt tt]. A.5. Down-Weighting of Generated Frames To reduce the impact of the unreliability over time (see Section B.2) of generated frames on reconstruction, the loss of generated frames is multiplied by an exponential decay when training visual particles: wt = wmin + (1 wmin) exp(cid:0)k (t t0)(cid:1), where is the frame index, t0 = 0, = 0.02, wmin = 0.0. A.6. Learnable Buoyancy Strength In well-controlled scenes [11], the relative strength between buoyancy and gravity is fixed, whereas in the wild, buoyancy strength is underdetermined. To account for this, we make the buoyancy coefficient in the PBF simulation (Appendix in [11]) learnable and optimize it jointly with reconstruction. Figure 11. Visualizations of extracted smoke from wild videos. 2 Results without Smoke Extraction. We further train HyFluid [38] and FluidNexus [11] on inputs without our smoke extraction method. The results are shown in Table 6. Table 8. Comparing LPIPS (smaller is better) of smoke reconstruction by different methods on videos collected from FLAME [28] and Pixabay. Table 6. with/without our smoke extraction (SE) method. Comparing PSNR (higher is better) of methods Future Prediction (Input View) FLAME City Valley Forest HyFluid HyFluid (w. our SE) FluidNexus FluidNexus (w. our SE) Ours 10.37 21.67 6.67 21.78 22.88 12.37 23.24 8.78 24.18 24.68 11.79 12.43 9.63 16.44 20.42 9.88 13.70 4.46 14.63 17. B.2. Generated Videos from SV4D 2.0 We previously noted that the quality of generated multi-view frames from SV4D 2.0 decays over time. Figure 12 shows early and late frames of the sequence. As time progresses, the smoke structure gradually collapses. Future Prediction (Input View) FLAME City Valley Forest HyFluid [38] FluidNexus [11] Ours 0.13 0.16 0.09 0.33 0.14 0.12 0.18 0.20 0.13 0.36 0.31 0.24 B.4. Visualizations of Novel View Synthesis on Wild"
        },
        {
            "title": "Videos",
            "content": "Figure 13 renders novel views for several wild videos. For all frames, the novel-view camera is fixed to the pose of the first frame of the training video. The reconstructed smoke remains stable over time. Figure 13. Novel view synthesis on wild videos. Our reconstructions preserve stable smoke structure across views. B.5. Visualizations for Ablation Study on Synthetic"
        },
        {
            "title": "Data",
            "content": "We provide additional qualitative ablations (Figure 14) complementing the quantitative table in the main paper. Figure 12. SV4D 2.0 multi-view generation over time. Later frames exhibit structural collapse. B.3. SSIM and LPIPS Beyond PSNR, we further report the structural similarity index measure (SSIM) in Table 7 and the perceptual metric LPIPS [41] in Table 8. Our method is still shown to perform better based on these two additional metrics. Table 7. Comparing SSIM (higher is better) of smoke reconstruction by different methods on videos collected from FLAME [28] and Pixabay. Future Prediction (Input View) FLAME City Valley Forest HyFluid [38] FluidNexus [11] Ours 0.87 0.88 0.92 0.55 0.84 0.89 0.79 0.76 0. 0.46 0.70 0.80 3 Figure 14. Ablation study of novel view synthesis and future predictions on synthetic smoke videos. Novel view uses the camera pose at = 1, and input view means the camera poses along the source video. Training uses = 240 frames, and the unseen future extends to = 270 frames. GT: Ground Truth."
        }
    ],
    "affiliations": [
        "Simon Fraser University"
    ]
}