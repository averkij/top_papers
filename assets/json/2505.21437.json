{
    "paper_title": "CoDA: Coordinated Diffusion Noise Optimization for Whole-Body Manipulation of Articulated Objects",
    "authors": [
        "Huaijin Pi",
        "Zhi Cen",
        "Zhiyang Dou",
        "Taku Komura"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Synthesizing whole-body manipulation of articulated objects, including body motion, hand motion, and object motion, is a critical yet challenging task with broad applications in virtual humans and robotics. The core challenges are twofold. First, achieving realistic whole-body motion requires tight coordination between the hands and the rest of the body, as their movements are interdependent during manipulation. Second, articulated object manipulation typically involves high degrees of freedom and demands higher precision, often requiring the fingers to be placed at specific regions to actuate movable parts. To address these challenges, we propose a novel coordinated diffusion noise optimization framework. Specifically, we perform noise-space optimization over three specialized diffusion models for the body, left hand, and right hand, each trained on its own motion dataset to improve generalization. Coordination naturally emerges through gradient flow along the human kinematic chain, allowing the global body posture to adapt in response to hand motion objectives with high fidelity. To further enhance precision in hand-object interaction, we adopt a unified representation based on basis point sets (BPS), where end-effector positions are encoded as distances to the same BPS used for object geometry. This unified representation captures fine-grained spatial relationships between the hand and articulated object parts, and the resulting trajectories serve as targets to guide the optimization of diffusion noise, producing highly accurate interaction motion. We conduct extensive experiments demonstrating that our method outperforms existing approaches in motion quality and physical plausibility, and enables various capabilities such as object pose control, simultaneous walking and manipulation, and whole-body generation from hand-only data."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 7 3 4 1 2 . 5 0 5 2 : r CoDA: Coordinated Diffusion Noise Optimization for Whole-Body Manipulation of Articulated Objects Huaijin Pi1 Zhi Cen2 Zhiyang Dou1 Taku Komura1 1The University of Hong Kong 2Zhejiang University https://phj128.github.io/page/CoDA Figure 1: Our approach enables: (a) generating whole-body manipulation of articulated objects from text input (e.g., person uses the mixer); (b) manipulating the object to target pose and articulation (the blue object is the target pose); (c) synthesizing whole-body motion guided by trajectories from hand-only data; (d) generating motions involving simultaneous walking and object manipulation (e.g., opening box while walking)."
        },
        {
            "title": "Abstract",
            "content": "Synthesizing whole-body manipulation of articulated objects, including body motion, hand motion, and object motion, is critical yet challenging task with broad applications in virtual humans and robotics. The core challenges are twofold. First, achieving realistic whole-body motion requires tight coordination between the hands and the rest of the body, as their movements are interdependent during manipulation. Second, articulated object manipulation typically involves high degrees of freedom and demands higher precision, often requiring the fingers to be placed at specific regions to actuate movable parts. To address these challenges, we propose novel coordinated diffusion noise optimization framework. Specifically, we perform noise-space optimization over three specialized diffusion models for the body, left hand, and right hand, each trained on its own motion dataset to improve generalization. Coordination naturally emerges through gradient flow Preprint. Under review. along the human kinematic chain, allowing the global body posture to adapt in response to hand motion objectives with high fidelity. To further enhance precision in hand-object interaction, we adopt unified representation based on basis point sets (BPS), where end-effector positions are encoded as distances to the same BPS used for object geometry. This unified representation captures fine-grained spatial relationships between the hand and articulated object parts, and the resulting trajectories serve as targets to guide the optimization of diffusion noise, producing highly accurate interaction motion. We conduct extensive experiments demonstrating that our method outperforms existing approaches in motion quality and physical plausibility, and enables various capabilities such as object pose control, simultaneous walking and manipulation, and whole-body generation from hand-only data. The code will be released for reproducibility."
        },
        {
            "title": "Introduction",
            "content": "Human-object interaction (HOI) motion generation has broad applications in virtual reality, character animation, and robotics. These interactions range from simple activities like sitting on chair [89, 79] to more complex tasks involving articulated object manipulation [6, 34], such as opening box or microwave. This paper focuses on the challenging setting of whole-body manipulation of articulated objects. Given an initial pose of the human and the object, along with textual instruction, our goal is to synthesize realistic, physically plausible interaction sequences that involve coordinated body, hand, and articulated object motion. Most prior works on HOI generation [26, 79, 121, 4, 46, 47, 13] suffer from two key limitations. First, they typically focus on either body-only motion [79, 46, 47] or hand-only manipulation [118, 132, 6, 34, 13]. Although hand-only methods can produce plausible contact behaviors in short-range scenarios, they fail to capture important whole-body dynamics such as bending down, reaching forward, or walking while manipulating objects. Such whole-body behaviors are essential for generating realistic human-object interactions, especially when manipulation is not restricted to fixed space. Second, most existing works target rigid objects [115, 47, 13], while articulated objects introduce more complex motion patterns and require continuous in-hand adjustments. Whole-body manipulation of articulated objects is highly challenging. First, it demands coordinated motion between the body and hands to reflect natural physical behaviors. Body movement affects how the hands approach and manipulate objects, and conversely, hand-object interactions can influence global posture. Second, precise control of finger positions is essential to maintain accurate, physically plausible contact throughout the sequence. This is especially important for articulated objects, where the manipulation often requires placing the fingers at specific regions to actuate the articulation while avoid colliding with other parts. To address these challenges, we propose novel framework called CoDA (Coordinated Diffusion noise optimization for whole-body manipulation of Articulated objects), which jointly synthesizes the motions of the human body, hands, and articulated objects. Our core idea is to optimize the input noise vectors of three specialized diffusion models, which independently model the body, left hand, and right hand, to generate coordinated whole-body motion. This decoupled design allows each component to be trained on its own data source, such as using large-scale datasets like AMASS [63] for body motion, manipulation datasets like ARCTIC [19] and GRAB [93] for hand motion, thereby improving generalization across diverse motions. Coordination naturally emerges during optimization, as gradients from hand motion objectives flow through the human kinematic chain, allowing the global posture to adapt in response to fine-grained hand motion. This optimization further enables precise control over hand-object contact, while the diffusion noise space [40] provides strong motion priors to preserve naturalness in the generated sequences. To enable precise manipulation while accounting for object geometry and articulation, we adopt basis point set (BPS) representation [81, 132] to encode both the object surface and end-effector trajectories in unified form. Specifically, we represent the positions of the end-effectors, namely the wrists and fingertips, by their distances to the same BPS used for encoding the object geometry. The unified representation captures the relative spatial relationship between the hand and the object geometry as well as its articulation during complex manipulation tasks. The generated trajectories, based on this representation, provide continuous target signal for optimizing whole-body motion. We evaluate our approach on both the ARCTIC [19] dataset of articulated object manipulation and the GRAB [93] dataset of rigid object interactions. Our method achieves state-of-the-art performance on both benchmarks, outperforming existing approaches in motion quality and physical plausibility. Beyond benchmark evaluation, our framework enables several compelling capabilities, as illustrated in Figure 1. It supports object pose control at specific times, and coordinated whole-body behaviors involving simultaneous locomotion and manipulation, which are absent from the ARCTIC dataset. In addition, our framework allows us to leverage hand-only datasets [2] to generate whole-body motion, enabling broader data usage and generalization. To the best of our knowledge, this is the first work to jointly generate body, hand, and articulated object motions for whole-body manipulation tasks."
        },
        {
            "title": "2 Related work",
            "content": "Human-object interaction. Human-object interaction (HOI) generation [89, 46, 14] has received increasing attention due to its potential to enable virtual humans to perform various actions in 3D environments. Early works focus on generating static interactions such as sitting or lying on furniture [89, 26, 133, 137, 131], using either auto-regressive pipelines or whole-sequence generation [103, 105, 67, 1, 139]. Recent methods explore diffusion-based models [35, 79, 45, 4, 121, 38] and apply guidance techniques [16, 30] to improve human-scene contact quality. Beyond static objects, several works consider dynamic objects [115, 116] or generate human motion conditioned on given object trajectories [46, 15]. For example, OMOMO [46] proposes two-stage framework that first generates wrist trajectories and then completes body motion accordingly. Other approaches [74, 115, 47, 17, 88, 117] jointly generate body and object motion, and incorporate contact-aware guidance into the diffusion process to improve the quality. Another line of research [27, 71, 113, 96, 72, 106] enables physically simulated characters to perform scene-level interactions by learning control policies through environment interaction. These methods mainly focus on navigation and interactions with large-scale objects such as furniture or obstacles. While generating plausible body motion, they ignore finger motion, which is crucial for fine-grained manipulation. Hand-object interaction. ManipNet [126] synthesizes object manipulation given wrist and object trajectories, using multiple representations to model the hand-object relationship. GRIP [95] design temporal hand-object spatial feature for stable grasping. Some works [140, 55] address the task of denoising noisy hand motion to recover clean interaction sequences. While these methods explore various representations for modeling hand-object spatial relationships, they rely on access to predefined wrist and object trajectories. Some works [138, 132, 128] explore settings where only the object trajectory is provided. CAMS [138] introduces canonicalized representation to enable precise contact generation. [132, 128] generate manipulation by predicting contact maps as intermediate representations. Other works generate hand and object motion jointly, without relying on predefined trajectories. DiffH2O [13] applies grasp guidance to diffusion models for more coherent hand-object interactions.. Text2HOI [6] employs cascaded diffusion to iteratively refine the results. HOIGPT [34] leverages separate codebooks for hands and objects, and jointly predicts motion and text. Physics-based approaches [12, 127] generate grasping motions through reinforcement learning in simulated environments. Despite their differences, all these methods ignore the body context. Whole-body interaction. Although there are several whole-body manipulation datasets [93, 19, 39, 38, 123, 62, 57], only few works consider body and hand interaction simultaneously. [94, 110] assume the object is static and only synthesize approaching and grasp motion. IMoS [20] demonstrates full-body manipulation with given finger motion; it generates body motion auto-regressively and optimizes object trajectories by assuming static hand-object contact frame. TOHO [49] synthesizes whole-body interactions using implicit representations [28], relying on the same contact assumption to recover object motion. DiffGrasp [135] generates whole-body motion conditioned on given object trajectories using diffusion models, and introduces hand-object guidance to improve interaction quality. HOIFHLI [111] employs LLM [70] to analyze the scene and plan motions for grasping and relocating rigid objects. Other works [107, 108, 122] employ physics-based tracking to mimic manipulation behaviors. More recent work extends this to synthesize humanoid grasping [3, 60, 51], but the generated motions remain unnatural and do not involve complex manipulation. All of the above methods focus exclusively on rigid object interaction and do not address articulated objects. Compared to rigid object interaction, articulated object manipulation is more complex, as it often requires placing the fingers at specific regions to actuate the articulation. 3 Figure 2: Pipeline overview. (a) Given the initial human pose, object pose, and text, we first generate the articulated object trajectory and the corresponding end-effector trajectories via two conditional diffusion models. (b) We then optimize the latent noise inputs of three decoupled diffusion models by propagating gradients through the kinematic chain, guided by end-effector tracking, penetration, and regularization losses. Finally, we forward the optimized noise through the diffusion models to synthesize coherent whole-body motion aligned with the generated object motion."
        },
        {
            "title": "3 Preliminary",
            "content": "In this section, we define the input and output in this paper. Given the initial pose of human and an articulated object, along with textual instruction, our goal is to generate full sequence including the whole-body human motion (body and fingers) and the articulated object motion over time. Object representations. The objects from the ARCTIC [19] dataset are two-part articulated objects with 7 degrees of freedom. We use So = {To, Ro, a} to indicate the object pose, where the object state So R7 consists of object translation To R3, object rotation Ro R3, and the angle of the rotational joint R1 between the two parts of the object. Motion representations. We use SMPL-X [73], which is parametric human body model to represent the whole body, including the face and fingers. SMPL-X is differentiable function that takes input shape, pose, and expression parameters and outputs 3D mesh with 10, 475 vertices and 20, 908 triangles. The vertices are posed with linear blend skinning with rigged skeleton which is learned from the data. As we focus on the body motion with two hands, we remove the face related parameters. Θ = {θ, t} is the pose parameters to drive the SMPL-X model, where θ R523 represents joint angles and R3 is the root translation. Text descriptions. In the ARCTIC [19] and the GRAB [93] dataset, each sequence is annotated with an action label. Following previous work [20, 6], we construct the text description using the template person <action> the <object>.. For example, person uses the box.."
        },
        {
            "title": "4 Method",
            "content": "The overview of our pipeline is shown in Figure 2. We first generate the motion of the articulated object (Section 4.1), then predict the end-effector trajectories (Section 4.2), and finally synthesize the whole-body motion by optimizing the noise of decoupled diffusion models (Section 4.3). 4 4.1 Object motion generation Given the initial object pose and the textual instruction, we train diffusion model [97] to generate the object future trajectory. The input includes the CLIP [82] feature of the text, the initial object pose, and the object geometry embedding. We represent the object geometry using the normalized part-based BPS descriptor [132], which will be formally defined in Section 4.2 and Figure 3. The output is sequence of object states over time. 4.2 End-effector trajectory generation Given the generated object trajectory, we extract its geometry representation and combine it with the trajectory itself and the textual instruction as input to diffusion model that predicts end-effector trajectories. Instead of directly predicting 3D joint coordinates [46], we design distance-based representation that encodes end-effector positions in the same space as the object geometry. Unified BPS-based representation for object and end-effectors. We first present the object geometry representation. Following previous work [132], we adopt the normalized part BPS [81] to represent the object geometry. Specifically, the object mesh is first normalized to the unit scale by dividing all vertex coordinates by the maximum distance from the object origin to any vertex. Then pre-defined fixed set of basis points RK3, shared across all objects, are uniformly sampled within the unit sphere centered at the object origin. The BPS representation is computed as the distances from each basis point to the nearest vertex on each of the two rigid object parts, resulting in an object geometry vector RK2. We then introduce end-effector BPS, distance-based representation tailored for encoding the positions of end-effectors in the object coordinate system. The end-effectors include both wrists and fingertips, comprising total of 12 joints (2 wrists and 10 fingertips). As shown in figure 3, at each frame, for each of the 12 end-effectors, we compute K-dimensional vector of Euclidean distances to the basis points. We use the same pre-defined set of basis points RK3 in object geometry representation [132]. This results in (12 K)-dimensional end-effector BPS vector per frame. The diffusion model outputs sequence of end-effector BPS over time, along with binary contact labels for each fingertip, indicating whether it is close to the object surface. Given the generated end-effector BPS sequence, we recover the end-effector trajectories by solving simple optimization problem. For each end-effector at each frame, we minimize the following loss to infer its 3D position: = arg min L(pe), pe L(pe) = (cid:88) pe Pj2 dj2, (1) (2) where pe is the optimized 3D position and dj is the predicted distance to the j-th basis point Pj. By sharing the basis point set with the object BPS representation, our method provides consistent spatial reference frame that facilitates geometric alignment between end-effectors and object parts. Figure 3: The illustration of the endeffector BPS. (a) is the object BPS [132]. (b) is the proposed end-effector BPS representation. Gray points denote the basis points; pink/yellow are two object parts; blue indicates fingertip. Only one end-effector and 64 basis points are visualized for simplicity. RoPE-based object motion encoding. To better encode the object trajectory, we adapt the idea of CaPE [44], which encodes relative camera pose information via RoPE [92]. In our case, each object pose is also represented as 4 4 transformation matrix. Inspired by CaPE [44], we use the object pose to transform the query and key features in each attention layer. This enables the model to encode the relative object motion within local temporal window, providing temporally-aware conditioning for the generation. We refer readers to Section for more details. 4.3 Whole-body motion generation The goal of this stage is to generate coherent whole-body motion that aligns with the predicted end-effector trajectories and articulated object motion. Rather than directly predicting whole-body 5 poses conditioned on end-effectors [46], we adopt an optimization-based approach inspired by DNO [40]. Specifically, we optimize the noise input to the diffusion models (Figure 2 (b)), and then forward the optimized noise through the diffusion models to generate the final motion. To further improve motion quality, we decouple the body into three components: body, left hand, and right hand, and train separate diffusion models for each. This decoupled design enables us to train each module using individual data, such as training the hand models using the ARCTIC [19] and GRAB [93], and the body-only model without hands on the AMASS [63]. Such specialization improves generalization by allowing novel combinations of finger and body motion to be synthesized. Moreover, this formulation facilitates gradient flow through the kinematic chain during optimization, which improves coordination between the body and hands. Decoupled motion diffusion model. We adopt decoupled human representation for whole-body motion, dividing the human pose into three components: body, left hand, and right hand. Formally, for each frame i, the whole-body pose Θi is represented as: = {xb, xlh, xrh}, xb = { rx, rz, ry, ra, θb}, xlh = {θlh}, xrh = {θrh}, (3) (4) (5) (6) where xb denotes the body component, including root velocities rx, rz (projected on the XZplane), root height ry R, angular velocity ra R, and body joint rotations θb R6Jb , while θlh R6Jlh and θrh R6Jrh represent the left and right hand joint rotations, respectively. All joint rotations are encoded using the 6D representation [142], with Jb = 22, Jlh = Jrh = 15 joints for the body and each hand. We train three separate diffusion models, Mb, Mlh, and Mrh, to model the motion manifolds of the body and hands individually. Optimization over diffusion noise. Given the trained diffusion models for body, left hand, and right hand, we optimize the noise vectors = {zb, zlh, zrh} to generate whole-body motion as shown in Figure 2 (b). Let (z) denote the process that maps the input noise to global joint positions through diffusion models and forward kinematics: (z) = FK(Mb(zb), Mlh(zlh), Mrh(zrh)), where FK() converts root translation and local joint rotations into global joint positions. We formulate motion generation as minimizing loss over the diffusion noise: (7) = arg min L(f (z)). (8) The overall loss function consists of three components with different weights λee, λpen, and λreg: = λeeLee + λpenLpen + λregLreg, (9) where Lee, Lpen, and Lreg are the end-effector tracking, penetration, and regularization losses. We encourage the generated global fingertip positions ˆpf to follow the predicted trajectories pf from the previous stage. We also constrain the relative fingertip positions to the wrist joints: Lee = ˆpf pf 1 + ˆpr pr 1, (10) where pr and ˆpr denote the relative fingertip positions with respect to the wrist. To reduce hand-object interpenetration, we penalize fingertip joints that fall inside the object mesh: Lpen = (cid:88) (cid:13) (cid:13)min (cid:0)SDF(Jj) 0.01, 0.00(cid:1)(cid:13) (cid:13)1 (11) where SDF(Jj) is the signed distance at the j-th hand joint, assuming 1cm finger thickness. We add regularization term to discourage foot floating and foot sliding: (cid:13)1 + 1right. (cid:13) (cid:13) and Jr Lreg = min (Jy) 0.021 + 1left. (cid:13) i1 where Jy denotes the height of all joints in the body, and Jl denote the 3D positions of the left and right foot joints at frame i, respectively. The binary indicators 1 denote whether the left or right foot is in contact with the ground, based on height threshold of 0.02 meters. Jr Jl (cid:13) (cid:13)1 , (cid:13)Jr (cid:13)Jl (12) i1 6 We adopt DDIM [87] sampling to efficiently generate motion sequences during optimization following DNO [40]. The loss is computed on the final output, and gradients are propagated back through the DDIM solver to update the noise. After optimization, we pass the optimized noise into the decoupled diffusion models to generate the final whole-body motion. Combined with the previously generated object trajectory, this yields complete human-object manipulation sequence. This noisespace optimization avoids high-dimensional pose regression, reduces artifacts, and produces natural whole-body motions aligned with the object manipulation process."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Implementation details We adopt transformer-based diffusion architecture similar to MDM [97] for all models in our framework. During inference, we perform noise optimization using DDIM [87] with =10 for 800 steps and cosine-decayed learning rate, following the DNO [40] strategy. All experiments are conducted on single NVIDIA A100 GPU. More training details are in Section D. 5.2 Dataset and evaluation metrics Dataset. We evaluate on ARCTIC [19] for articulated object manipulation and on GRAB [93] for rigid object interaction. ARCTIC contains around 2 hours of motion data featuring 10 subjects interacting with 11 articulated objects, including complex motions such as bimanual grasps and in-hand manipulation. Following the protocol in [132], we randomly sample 4 sequences per object category to construct the test set. The GRAB dataset covers about 4 hours of interaction from 10 subjects with 51 rigid objects, focusing primarily on grasping and simple lifting actions. Similar to [20], we use data from the last subject as the test set. For training object motion and end-effector trajectories generation, ARCTIC is used for articulated objects, and GRAB is used for rigid objects. The body motion model is trained on ARCTIC, GRAB, and AMASS [63], while the two hand motion models are trained on ARCTIC and GRAB. Evaluation metrics. Similar to [13, 6], we evaluate the motion quality using the following metrics: (1) Frechet Inception Distance (FID) measures the feature-level distance between generated and real motions, using motion feature extractor trained on the dataset following [23]. (2) R-Precision quantifies the alignment between generated motion and the corresponding textual prompt, measured using Top-3 accuracy. (3) Diversity reflects the variation among generated motion samples. (4) Foot skating indicates motion realism by detecting undesired foot sliding, following the computation in [53, 79]. We additionally report physical realism metrics following [13]: (5) Interpenetration volume (IV) computes the number of hand vertices that penetrate the object mesh. (6) Interpenetration depth (ID) measures the maximum penetration depth of hand vertices into the object. (7) Contact ratio (CR) is defined as the average proportion of hand vertices within 5 mm of the object surface. We also conduct user study involving 16 participants to evaluate the generated motion sequences. 5.3 Comparison with baselines Baselines. As there is no existing method that jointly generates body, hand, and articulated object motion, we adapt several representative methods to our task: IMoS [20], MDM [97], OMOMO [46], Text2HOI [6], and CHOIS [47]. IMoS is CVAE-based [86] auto-regressive model, while MDM is full-sequence diffusion-based [31] model. Text2HOI is originally designed for hand-object interaction with multiple diffusion models for iterative refinement. CHOIS is diffusion-based model that incorporates contact guidance during inference. We extend them to jointly generate whole-body motion and object motion. OMOMO first generates wrist motion and then synthesizes body motion. We extend it to three-stage model: first generating object motion, then predicting fingertip and wrist trajectories, and finally producing whole-body motion. Quantitative results. We report quantitative results on ARCTIC and GRAB in Table 1 and Table 3, and user study results in Table 2. Our method achieves the best performance on nearly all metrics across both datasets. While it ranks slightly lower in diversity, it significantly outperforms all baselines in the user study, indicating superior perceptual quality and physical plausibility. 7 Table 1: Comparison on the ARCTIC [19] dataset. The right arrow means the closer to real motion the better. IV, ID, and CR denote interpenetration volume, interpenetration depth, and contact ratio. The best and second-best results are highlighted green and yellow, respectively. Methods Real IMoS [20] MDM [97] Text2HOI [6] OMOMO [46] CHOIS [47] Ours FID R-Precision Diversity Foot skating 6.686 3.972 6.654 3.710 3. 2.283 0.531 0.305 0.209 0.234 0.406 0.367 0.477 8.664 6.144 8.167 5.923 6.110 7. 7.208 0.002 1.469 0.027 0.028 0.028 0.023 0.002 IV 4. 14.28 16.90 12.72 13.77 17.19 ID CR 11.47 0.085 13.24 15.85 17.14 15.16 15. 0.010 0.033 0.010 0.061 0.030 5.25 12.87 0.086 Table 2: User study on the ARCTIC [19] dataset. Metrics Ours CHOIS [47] OMOMO [46] Text2HOI [6] Best Motion Realism Rate Best Physical Plausibility Rate 88.7% 87.3% 1.1% 1.4% 9.9% 10.2% 0.3% 1.1% Qualitative results. As demonstrated in Figure 4, our method achieves significantly better hand-object contact compared to baselines. We provide more results in the supplementary material. 5.4 Ablation study We ablate key components of our framework to understand their impact on overall performance: (a) single model to jointly predict object motion and end-effector trajectories. (b) Predicting relative coordinate of end-effectors to the object center without end-effector BPS. (c) Using object velocity and rotational velocity as the trajectory input without RoPE-based representation. (d) Removing the optimization process and using conditional diffusion model with fingertip trajectories as input. (e) Using single diffusion model for the entire body without the decoupled body-hand representation. (f) Excluding the AMASS [63] dataset during training the body motion model. As shown in Table 4, each component contributes to the performance improvement. 5.5 More discussions Generalization to different object geometry. To further validate generalization to unseen object geometries of the same category, we train the object motion and end-effector trajectory models on the hand-only dataset [138], using 7 training and 3 testing objects. Despite the dataset containing only hand motion, our method successfully generates whole-body motion, as shown in Figure 5. Various capabilities. Our approach enables various capabilities. First, it allows control over keyframe object poses by setting them as optimization targets for object trajectory generation (Figure 6). Second, it can synthesize whole-body motions that involve simultaneous locomotion and manipulation, even though such combinations are not present in the training dataset [19] (Figure 7). Third, it enables generating whole-body motion guided by hand-only datasets [2], using wrist and fingertip trajectories as optimization targets (Figure 9). Furthermore, our generated whole-body Methods Real IMoS [20] MDM [97] Text2HOI [6] OMOMO [46] CHOIS [47] Ours Table 3: Comparison on the GRAB [93] dataset. IV R-Precision Diversity Foot skating FID ID CR 52.290 26.734 30.101 25.017 25.835 21. 0.727 0.180 0.289 0.320 0.391 0.320 0.438 15.045 8.374 8.627 10.302 9.294 9.887 9. 8 0.010 0.152 0.109 0.086 0.094 0.055 0.046 5.84 13. 0.049 11.57 12.96 12.52 11.03 9.31 20.35 16.03 14.55 14.03 14.37 0.000 0.001 0.000 0.004 0.002 4.93 10. 0.040 Figure 4: Qualitative comparison. Given the text person uses the ketchup., our method generates the whole-body motion with better hand-object contact compared to baselines. Methods Real (a) w/o separate models (b) w/o end-effector BPS (c) w/o RoPE motion (d) w/o optimization (e) w/o decoupled (f) w/o AMASS Ours Table 4: Ablation study on the ARCTIC [19] dataset. IV FID R-Precision Diversity Foot skating ID CR 3.790 4.069 2.714 4.883 2.699 3.305 2. 0.531 0.438 0.453 0.469 0.414 0.438 0.453 0.477 8.664 6.939 6.888 7.021 6.406 7.142 6.859 7. 0.002 0.002 0.002 0.002 0.030 0.008 0.003 0.002 4.68 11.47 0. 8.21 8.09 6.12 16.39 12.45 5.46 13.16 13.54 12.66 16.13 16.29 13.04 0.103 0.093 0.093 0.095 0.082 0.089 5.25 12.87 0. motion can serve as reference for controlling humanoids [75, 59, 107] in physics-based simulators [64], where the object is physically manipulated by the humanoid, rather than being directly assigned target trajectory (Figure 8). We provide more qualitative results in Section E. Limitations. First, the optimization process is slower than other generative methods [97], limiting real-time applications. Second, due to the limited object diversity in existing datasets [19], the model struggles to generalize to novel object categories. Third, our framework only focuses on single-object manipulation; extending it to handle multiple interacting objects or multi-step sequential interactions remains an open direction. Finally, enabling both the body and fingers to reason about and avoid obstacles in complex scenes, such as surrounding geometry or other objects, is still difficult problem. More limitations are discussed in Section F."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we present coordinated diffusion noise optimization framework for synthesizing wholebody manipulation of articulated objects. By optimizing over the noise space of separately trained diffusion models for the body, left hand, and right hand, our method enables natural coordination between the body and hands. We introduce unified distance-based representation built on basis point sets to generate end-effector trajectories, facilitating precise hand-object interactions. Extensive experiments demonstrate that our approach achieves state-of-the-art performance in motion quality and physical plausibility. It also supports various capabilities such as object pose control, simultaneous manipulation and locomotion, and whole-body motion generation from hand-only data."
        },
        {
            "title": "References",
            "content": "[1] Joao Pedro Araújo, Jiaman Li, Karthik Vetrivel, Rishi Agarwal, Jiajun Wu, Deepak Gopinath, Alexander William Clegg, and Karen Liu. Circle: Capture in rich contextual environments. In CVPR, 2023. 3 [2] Prithviraj Banerjee, Sindi Shkodrani, Pierre Moulon, Shreyas Hampali, Shangchen Han, Fan Zhang, Linguang Zhang, Jade Fountain, Edward Miller, Selen Basol, Richard Newcombe, Robert Wang, Jakob Julian Engel, and Tomas Hodan. HOT3D: Hand and object tracking in 3D from egocentric multi-view videos. CVPR, 2025. 3, 8 [3] Jona Braun, Sammy Christen, Muhammed Kocabas, Emre Aksan, and Otmar Hilliges. Physically plausible full-body hand-object interaction synthesis. In International Conference on 3D Vision (3DV), 2024. 3 [4] Zhi Cen, Huaijin Pi, Sida Peng, Zehong Shen, Minghui Yang, Zhu Shuai, Hujun Bao, and Xiaowei Zhou. Generating human motion in 3d scenes from text descriptions. In CVPR, 2024. 2, 3 [5] Zhi Cen, Huaijin Pi, Sida Peng, Qing Shuai, Yujun Shen, Hujun Bao, Xiaowei Zhou, and Ruizhen Hu. Ready-to-react: Online reaction policy for two-character interaction generation. In ICLR, 2025. 19 [6] Junuk Cha, Jihyeon Kim, Jae Shin Yoon, and Seungryul Baek. Text2hoi: Text-guided 3d motion generation for hand-object interaction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15771585, 2024. 2, 3, 4, 7, 8, [7] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1131511325, 2022. 19 [8] Changan Chen, Juze Zhang, Shrinidhi Kowshika Lakshmikanth, Yusu Fang, Ruizhi Shao, Gordon Wetzstein, Li Fei-Fei, and Ehsan Adeli. The language of motion: Unifying verbal and non-verbal language of 3d human motion. In arXiv, 2024. 19 [9] Ling-Hao Chen, Shunlin Lu, Wenxun Dai, Zhiyang Dou, Xuan Ju, Jingbo Wang, Taku Komura, and Lei Zhang. Pay attention and move better: Harnessing attention for interactive motion generation and training-free editing. arXiv preprint arXiv:2410.18977, 2024. 19 [10] Rui Chen, Mingyi Shi, Shaoli Huang, Ping Tan, Taku Komura, and Xuelin Chen. Taming diffusion probabilistic models for character control. In ACM SIGGRAPH 2024 Conference Papers, pages 110, 2024. 19 [11] Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao Chen, Jingyi Yu, and Gang Yu. Executing your Commands via Motion Diffusion in Latent Space. arXiv e-prints, art. arXiv:2212.04048, December 2022. doi: 10.48550/arXiv.2212.04048. 19 [12] Sammy Christen, Muhammed Kocabas, Emre Aksan, Jemin Hwangbo, Jie Song, and Otmar Hilliges. D-grasp: Physically plausible dynamic grasp synthesis for hand-object interactions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2057720586, 2022. [13] Sammy Christen, Shreyas Hampali, Fadime Sener, Edoardo Remelli, Tomas Hodan, Eric Sauser, Shugao Ma, and Bugra Tekin. Diffh2o: Diffusion-based synthesis of hand-object interactions from textual descriptions. In SIGGRAPH Asia 2024 Conference Papers, 2024. 2, 3, 7 [14] Peishan Cong, Ziyi Wang, Zhiyang Dou, Yiming Ren, Wei Yin, Kai Cheng, Yujing Sun, Xiaoxiao Long, Xinge Zhu, and Yuexin Ma. Laserhuman: Language-guided scene-aware human motion generation in free environment. arXiv preprint arXiv:2403.13307, 2024. 3 [15] Peishan Cong, Ziyi Wang, Yuexin Ma, and Xiangyu Yue. Semgeomo: Dynamic contextual human motion generation with semantic and geometric guidance. arXiv preprint arXiv:2503.01291, 2025. 3 10 [16] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In NeurIPS, 2021. 3, [17] Christian Diller and Angela Dai. Cg-hoi: Contact-guided 3d human-object interaction generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1988819901, 2024. 3 [18] Zhiyang Dou, Xuelin Chen, Qingnan Fan, Taku Komura, and Wenping Wang. ase: Learning conditional adversarial skill embeddings for physics-based characters. In SIGGRAPH Asia 2023 Conference Papers, pages 111, 2023. 19 [19] Zicong Fan, Omid Taheri, Dimitrios Tzionas, Muhammed Kocabas, Manuel Kaufmann, Michael J. Black, and Otmar Hilliges. ARCTIC: dataset for dexterous bimanual hand-object manipulation. In Proceedings IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 2, 3, 4, 6, 7, 8, 9, 21, 23 [20] Anindita Ghosh, Rishabh Dabral, Vladislav Golyanik, Christian Theobalt, and Philipp Imos: Intent-driven full-body motion synthesis for human-object interactions. Slusallek. In Computer Graphics Forum, 2023. 3, 4, 7, 8 [21] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Commun. ACM, 2020. [22] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. 19 [23] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. Generating diverse and natural 3d human motions from text. In CVPR, 2022. 7, 19 [24] Chuan Guo, Yuxuan Mu, Muhammad Gohar Javed, Sen Wang, and Li Cheng. Momask: Generative masked modeling of 3d human motions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19001910, 2024. 19 [25] Félix Harvey, Mike Yurick, Derek Nowrouzezahrai, and Christopher Pal. Robust motion in-betweening. ACM Trans. Graph., 2020. 19 [26] Mohamed Hassan, Duygu Ceylan, Ruben Villegas, Jun Saito, Jimei Yang, Yi Zhou, and Michael J. Black. Stochastic scene-aware motion prediction. In ICCV, 2021. 2, 3, 19 [27] Mohamed Hassan, Yunrong Guo, Tingwu Wang, Michael Black, Sanja Fidler, and Xue Bin Peng. Synthesizing physical character-scene interactions. In ACM SIGGRAPH 2023 Conference Proceedings, pages 19, 2023. 3 [28] Chengan He, Jun Saito, James Zachary, Holly Rushmeier, and Yi Zhou. Nemf: Neural motion fields for kinematic animation. In NeurIPS, 2022. 3 [29] Gustav Eje Henter, Simon Alexanderson, and Jonas Beskow. Moglow: Probabilistic and controllable motion synthesis using normalising flows. ACM Trans. Graph., 2020. [30] Jonathan Ho and Tim Salimans. Classifier-Free Diffusion Guidance. arXiv e-prints, art. arXiv:2207.12598, July 2022. 3, 19 [31] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. 7, 19, 20 [32] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J. Fleet. Video Diffusion Models. arXiv e-prints, art. arXiv:2204.03458, April 2022. 19 [33] Daniel Holden, Taku Komura, and Jun Saito. Phase-functioned neural networks for character control. ACM Trans. Graph., 2017. 19 11 [34] Mingzhen Huang, Fu-Jen Chu, Bugra Tekin, Kevin Liang, Haoyu Ma, Weiyao Wang, Xingyu Chen, Pierre Gleize, Hongfei Xue, Siwei Lyu, Kris Kitani, Matt Feiszli, and Hao Tang. Hoigpt: Learning long sequence hand-object interaction with language models. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Nashville, USA, 2025. 2, 3 [35] Siyuan Huang, Zan Wang, Puhao Li, Baoxiong Jia, Tengyu Liu, Yixin Zhu, Wei Liang, and Song-Chun Zhu. Diffusion-based generation, optimization, and planning in 3d scenes. In CVPR, 2023. 3 [36] Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton. Adaptive mixtures of local experts. Neural Computation, 3(1):7987, 1991. doi: 10.1162/neco.1991.3. 1.79. 19 [37] Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and Tao Chen. Motiongpt: Human motion as foreign language. Advances in Neural Information Processing Systems, 36:2006720079, 2023. [38] Nan Jiang, Zimo He, Zi Wang, Hongjie Li, Yixin Chen, Siyuan Huang, and Yixin Zhu. Autonomous character-scene interaction synthesis from text instruction, 2024. URL https: //arxiv.org/abs/2410.03187. 3 [39] Nan Jiang, Zhiyuan Zhang, Hongjie Li, Xiaoxuan Ma, Zan Wang, Yixin Chen, Tengyu Liu, Yixin Zhu, and Siyuan Huang. Scaling up dynamic human-scene interaction modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17371747, 2024. 3 [40] Korrawe Karunratanakul, Konpat Preechakul, Emre Aksan, Thabo Beeler, Supasorn Suwajanakorn, and Siyu Tang. Optimizing diffusion noise can serve as universal motion priors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13341345, 2024. 2, 6, 7, 19, 20 [41] Diederik P. Kingma and Jimmy Ba. Adam: Method for Stochastic Optimization. arXiv e-prints, 2014. 20 [42] Diederik Kingma and Max Welling. Auto-Encoding Variational Bayes. arXiv e-prints, 2013. 19 [43] Durk Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. NeurIPS, 2018. 19 [44] Xin Kong, Shikun Liu, Xiaoyang Lyu, Marwan Taher, Xiaojuan Qi, and Andrew Davison. Eschernet: generative model for scalable view synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95039513, 2024. 5, 19 [45] Nilesh Kulkarni, Davis Rempe, Kyle Genova, Abhijit Kundu, Justin Johnson, David Fouhey, and Leonidas Guibas. Nifty: Neural object interaction fields for guided human motion synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 947957, 2024. 3 [46] Jiaman Li, Jiajun Wu, and Karen Liu. Object motion guided human motion synthesis. ACM Transactions on Graphics (TOG), 42(6):111, 2023. 2, 3, 5, 6, 7, 8 [47] Jiaman Li, Alexander Clegg, Roozbeh Mottaghi, Jiajun Wu, Xavier Puig, and Karen Liu. Controllable human-object interaction synthesis. In European Conference on Computer Vision, pages 5472. Springer, 2024. 2, 3, 7, 8, 23 [48] Peizhuo Li, Kfir Aberman, Zihan Zhang, Rana Hanocka, and Olga Sorkine-Hornung. Ganimator: Neural motion synthesis from single sequence. ACM Trans. Graph., 2022. 19 [49] Quanzhou Li, Jingbo Wang, Chen Change Loy, and Bo Dai. Task-oriented human-object interactions generation with implicit neural representations. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 30353044, 2024. 3 [50] Ruilong Li, Shan Yang, David Ross, and Angjoo Kanazawa. Ai choreographer: Music conditioned 3d dance generation with aist++. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1340113412, 2021. 19 [51] Yitang Li, Mingxian Lin, Zhuo Lin, Yipeng Deng, Yue Cao, and Li Yi. Learning physicsbased full-body human reaching and grasping from brief walking references. arXiv preprint arXiv:2503.07481, 2025. 3 [52] Jing Lin, Ailing Zeng, Shunlin Lu, Yuanhao Cai, Ruimao Zhang, Haoqian Wang, and Lei Zhang. Motion-x: large-scale 3d expressive whole-body human motion dataset. Advances in Neural Information Processing Systems, 36, 2024. 19 [53] Hung Yu Ling, Fabio Zinno, George Cheng, and Michiel Van De Panne. Character controllers using motion vaes. ACM Trans. Graph., 2020. 7 [54] Hanchao Liu, Xiaohang Zhan, Shaoli Huang, Tai-Jiang Mu, and Ying Shan. Programmable In Proceedings of the IEEE/CVF motion generation for open-set motion control tasks. Conference on Computer Vision and Pattern Recognition, pages 13991408, 2024. 19 [55] Xueyi Liu and Li Yi. Geneoh diffusion: Towards generalizable hand-object interaction denoising via denoising diffusion. In The Twelfth International Conference on Learning Representations, 2024. 3 [56] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum? id=Bkg6RiCqY7. 20 [57] Jintao Lu, He Zhang, Yuting Ye, Takaaki Shiratori, Sebastian Starke, and Taku Komura. Choice: Coordinated human-object interaction in cluttered environments for pick-and-place actions. arXiv preprint arXiv:2412.06702, 2024. 3 [58] Shunlin Lu, Jingbo Wang, Zeyu Lu, Ling-Hao Chen, Wenxun Dai, Junting Dong, Zhiyang Dou, Bo Dai, and Ruimao Zhang. Scamo: Exploring the scaling law in autoregressive motion generation model. arXiv preprint arXiv:2412.14559, 2024. 19 [59] Zhengyi Luo, Jinkun Cao, Kris Kitani, Weipeng Xu, et al. Perpetual humanoid control for real-time simulated avatars. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1089510904, 2023. 9, 19, [60] Zhengyi Luo, Jinkun Cao, Sammy Christen, Alexander Winkler, Kris M. Kitani, and Weipeng Xu. Omnigrasp: Simulated humanoid grasping on diverse objects. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview. net/forum?id=Glt37xoU7e. 3, 24 [61] Zhengyi Luo, Jinkun Cao, Josh Merel, Alexander Winkler, Jing Huang, Kris M. Kitani, and Weipeng Xu. Universal humanoid motion representations for physics-based control. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=OrOd8PxOO2. 19 [62] Xintao Lv, Liang Xu, Yichao Yan, Xin Jin, Congsheng Xu, Shuwen Wu, Yifan Liu, Lincheng Li, Mengxiao Bi, Wenjun Zeng, and Xiaokang Yang. Himo: new benchmark for fullbody human interacting with multiple objects, 2024. URL https://arxiv.org/abs/2407. 12371. 3 [63] Naureen Mahmood, Nima Ghorbani, Nikolaus Troje, Gerard Pons-Moll, and Michael Black. Amass: Archive of motion capture as surface shapes. In ICCV, 2019. 2, 6, 7, 8, 19, 21 [64] Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, et al. Isaac gym: High performance gpu-based physics simulation for robot learning. arXiv preprint arXiv:2108.10470, 2021. 9, 19, 23 [65] Julieta Martinez, Michael J. Black, and Javier Romero. On human motion prediction using recurrent neural networks. In CVPR, 2017. 19 13 [66] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations. In International Conference on Learning Representations, 2022. URL https://openreview. net/forum?id=aBsCjcPu_tE. 19 [67] Aymen Mir, Xavier Puig, Angjoo Kanazawa, and Gerard Pons-Moll. Generating continual human motion in diverse 3d scenes. In 2024 International Conference on 3D Vision (3DV), pages 903913. IEEE, 2024. 3 [68] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In ICML, 2021. [69] Zachary Novack, Julian McAuley, Taylor Berg-Kirkpatrick, and Nicholas Bryan. Ditto: Diffusion inference-time t-optimization for music generation. arXiv preprint arXiv:2401.12179, 2024. 19 [70] OpenAI. Openai: Introducing chatgpt. https://openai.com/blog/chatgpt, 2022. 3, 19 [71] Liang Pan, Jingbo Wang, Buzhen Huang, Junyu Zhang, Haofan Wang, Xu Tang, and Yangang Wang. Synthesizing physically plausible human motions in 3d scenes. In 2024 International Conference on 3D Vision (3DV), pages 14981507. IEEE, 2024. 3 [72] Liang Pan, Zeshi Yang, Zhiyang Dou, Wenjia Wang, Buzhen Huang, Bo Dai, Taku Komura, and Jingbo Wang. Tokenhsi: Unified synthesis of physical human-scene interactions through task tokenization. In CVPR, 2025. 3 [73] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and Michael J. Black. Expressive body capture: 3d hands, face, and body from single image. In CVPR, 2019. 4 [74] Xiaogang Peng, Yiming Xie, Zizhao Wu, Varun Jampani, Deqing Sun, and Huaizu Jiang. Hoi-diff: Text-driven synthesis of 3d human-object interactions using diffusion models. arXiv preprint arXiv:2312.06553, 2023. [75] Xue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel Van de Panne. Deepmimic: Exampleguided deep reinforcement learning of physics-based character skills. ACM Transactions On Graphics (TOG), 37(4):114, 2018. 9, 19, 22 [76] Xue Bin Peng, Ze Ma, Pieter Abbeel, Sergey Levine, and Angjoo Kanazawa. Amp: Adversarial motion priors for stylized physics-based character control. ACM Transactions on Graphics (ToG), 40(4):120, 2021. 19 [77] Xue Bin Peng, Yunrong Guo, Lina Halper, Sergey Levine, and Sanja Fidler. Ase: Large-scale reusable adversarial skill embeddings for physically simulated characters. ACM Transactions On Graphics (TOG), 41(4):117, 2022. 19 [78] Mathis Petrovich, Michael J. Black, and Gül Varol. Action-conditioned 3D human motion synthesis with transformer VAE. In ICCV, 2021. 19 [79] Huaijin Pi, Sida Peng, Minghui Yang, Xiaowei Zhou, and Hujun Bao. Hierarchical generation of human-object interactions with diffusion probabilistic models. In ICCV, 2023. 2, 3, [80] Matthias Plappert, Christian Mandery, and Tamim Asfour. The kit motion-language dataset. Big data, 2016. 19 [81] Sergey Prokudin, Christoph Lassner, and Javier Romero. Efficient learning on point clouds with basis point sets. In Proceedings of the IEEE/CVF international conference on computer vision, pages 43324341, 2019. 2, 5 [82] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 5 [83] Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. In NeurIPS, 2019. 14 [84] Davis Rempe, Zhengyi Luo, Xue Bin Peng, Ye Yuan, Kris Kitani, Karsten Kreis, Sanja Fidler, and Or Litany. Trace and pace: Controllable pedestrian animation via guided trajectory diffusion. In Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 19 [85] Zehong Shen, Huaijin Pi, Yan Xia, Zhi Cen, Sida Peng, Zechen Hu, Hujun Bao, Ruizhen Hu, and Xiaowei Zhou. World-grounded human motion recovery via gravity-view coordinates. In SIGGRAPH Asia Conference Proceedings, 2024. 20 [86] Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using deep conditional generative models. In NeurIPS, 2015. 7 [87] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021. URL https://openreview. net/forum?id=St1giarCHLP. 7, [88] Wenfeng Song, Xinyu Zhang, Shuai Li, Yang Gao, Aimin Hao, Xia Hou, Chenglizhao Chen, Ning Li, and Hong Qin. Hoianimator: Generating text-prompt human-object animations using novel perceptive diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 811820, June 2024. 3 [89] Sebastian Starke, He Zhang, Taku Komura, and Jun Saito. Neural state machine for characterscene interactions. ACM Trans. Graph., 2019. 2, 3, 19 [90] Sebastian Starke, Yiwei Zhao, Taku Komura, and Kazi Zaman. Local motion phases for learning multi-contact character movements. ACM Trans. Graph., 2020. [91] Sebastian Starke, Ian Mason, and Taku Komura. Deepphase: Periodic autoencoders for learning motion phase manifolds. ACM Trans. Graph., 2022. 19 [92] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 5 [93] Omid Taheri, Nima Ghorbani, Michael Black, and Dimitrios Tzionas. Grab: dataset of whole-body human grasping of objects. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part IV 16, pages 581600. Springer, 2020. 2, 3, 4, 6, 7, 8 [94] Omid Taheri, Vasileios Choutas, Michael J. Black, and Dimitrios Tzionas. Goal: Generating 4d whole-body motion for hand-object grasping. In CVPR, 2022. 3 [95] Omid Taheri, Yi Zhou, Dimitrios Tzionas, Yang Zhou, Duygu Ceylan, Soren Pirk, and Michael J. Black. GRIP: Generating interaction poses using latent consistency and spatial cues. In International Conference on 3D Vision (3DV), 2024. URL https://grip.is.tue. mpg.de. [96] Chen Tessler, Yunrong Guo, Ofir Nabati, Gal Chechik, and Xue Bin Peng. Maskedmimic: Unified physics-based character control through masked motion inpainting. ACM Transactions on Graphics (TOG), 43(6):121, 2024. 3 [97] Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel Cohen-or, and Amit Haim Bermano. Human motion diffusion model. In ICLR, 2023. 5, 7, 8, 9, 19, 20, 23 [98] Jonathan Tseng, Rodrigo Castellon, and Karen Liu. Edge: Editable dance generation from music. In CVPR, 2023. 19 [99] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. 19, [100] Bram Wallace, Akash Gokul, Stefano Ermon, and Nikhil Naik. End-to-end diffusion latent optimization improves classifier guidance. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 72807290, 2023. 19 [101] Bram Wallace, Akash Gokul, and Nikhil Naik. Edict: Exact diffusion inversion via coupled transformations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2253222541, 2023. 19 15 [102] Weilin Wan, Zhiyang Dou, Taku Komura, Wenping Wang, Dinesh Jayaraman, and Lingjie Liu. Tlcontrol: Trajectory and language control for human motion synthesis. In ECCV 2024, pages 3754. Springer Nature Switzerland, 2024. 19 [103] Jiashun Wang, Huazhe Xu, Jingwei Xu, Sifei Liu, and Xiaolong Wang. Synthesizing long-term 3d human motion and interaction in 3d scenes. In CVPR, 2021. [104] Jiashun Wang, Jessica Hodgins, and Jungdam Won. Strategy and skill learning for physicsbased table tennis animation. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. 19 [105] Jingbo Wang, Yu Rong, Jingyuan Liu, Sijie Yan, Dahua Lin, and Bo Dai. Towards diverse and natural scene-aware 3d human motion synthesis. In CVPR, 2022. 3 [106] Wenjia Wang, Liang Pan, Zhiyang Dou, Zhouyingcheng Liao, Yuke Lou, Lei Yang, Jingbo Wang, and Taku Komura. Sims: Simulating human-scene interactions with real world script planning. arXiv preprint arXiv:2411.19921, 2024. 3 [107] Yinhuai Wang, Jing Lin, Ailing Zeng, Zhengyi Luo, Jian Zhang, and Lei Zhang. Physhoi: Physics-based imitation of dynamic human-object interaction. arXiv preprint arXiv:2312.04393, 2023. 3, 9, 22 [108] Yinhuai Wang, Qihan Zhao, Runyi Yu, Ailing Zeng, Jing Lin, Zhengyi Luo, Hok Wai Tsui, Jiwen Yu, Xiu Li, Qifeng Chen, et al. Skillmimic: Learning reusable basketball skills from demonstrations. arXiv preprint arXiv:2408.15270, 2024. [109] Jungdam Won, Deepak Gopinath, and Jessica Hodgins. Physics-based character controllers ISSN 0730-0301. doi: using conditional vaes. ACM Trans. Graph., 41(4), July 2022. 10.1145/3528223.3530067. URL https://doi.org/10.1145/3528223.3530067. 19 [110] Yan Wu, Jiahao Wang, Yan Zhang, Siwei Zhang, Otmar Hilliges, Fisher Yu, and Siyu Tang. Saga: Stochastic whole-body grasping with contact. In ECCV, 2022. 3 [111] Zhen Wu, Jiaman Li, Pei Xu, and Karen Liu. Human-object interaction from human-level instructions. arXiv preprint arXiv:2406.17840, 2024. [112] Lixing Xiao, Shunlin Lu, Huaijin Pi, Ke Fan, Liang Pan, Yueer Zhou, Ziyong Feng, Xiaowei Zhou, Sida Peng, and Jingbo Wang. Motionstreamer: Streaming motion generation via diffusion-based autoregressive model in causal latent space. arXiv preprint arXiv:2503.15451, 2025. 19 [113] Zeqi Xiao, Tai Wang, Jingbo Wang, Jinkun Cao, Wenwei Zhang, Bo Dai, Dahua Lin, and Jiangmiao Pang. Unified human-scene interaction via prompted chain-of-contacts. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=1vCnDyQkjg. 3 [114] Yiming Xie, Varun Jampani, Lei Zhong, Deqing Sun, and Huaizu Jiang. Omnicontrol: Control any joint at any time for human motion generation. In The Twelfth International Conference on Learning Representations, 2024. 19 [115] Sirui Xu, Zhengyuan Li, Yu-Xiong Wang, and Liang-Yan Gui. Interdiff: Generating 3d human-object interactions with physics-informed diffusion. In ICCV, 2023. 2, [116] Sirui Xu, Yu-Xiong Wang, Liangyan Gui, et al. Interdreamer: Zero-shot text to 3d dynamic human-object interaction. Advances in Neural Information Processing Systems, 37:52858 52890, 2024. 3 [117] Mengqing Xue, Yifei Liu, Ling Guo, Shaoli Huang, and Changxing Ding. Guiding humanobject interactions with rich geometry and relations. arXiv preprint arXiv:2503.20172, 2025. 3 [118] Lixin Yang, Kailin Li, Xinyu Zhan, Fei Wu, Anran Xu, Liu Liu, and Cewu Lu. OakInk: large-scale knowledge repository for understanding hand-object interaction. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 2 16 [119] Heyuan Yao, Zhenhua Song, Baoquan Chen, and Libin Liu. Controlvae: Model-based learning of generative controllers for physics-based characters. ACM Transactions on Graphics (TOG), 41(6):116, 2022. 19 [120] Heyuan Yao, Zhenhua Song, Yuyang Zhou, Tenglong Ao, Baoquan Chen, and Libin Liu. Moconvq: Unified physics-based motion control via scalable discrete representations. ACM Transactions on Graphics (TOG), 43(4):121, 2024. [121] Hongwei Yi, Justus Thies, Michael Black, Xue Bin Peng, and Davis Rempe. Generating human interaction motions in scenes with text control. In European Conference on Computer Vision, pages 246263. Springer, 2025. 2, 3 [122] Runyi Yu, Yinhuai Wang, Qihan Zhao, Hok Wai Tsui, Jingbo Wang, Ping Tan, and Qifeng Chen. Skillmimic-v2: Learning robust and generalizable interaction skills from sparse and noisy demonstrations. arXiv preprint arXiv:2505.02094, 2025. 3 [123] Xinyu Zhan, Lixin Yang, Yifei Zhao, Kangrui Mao, Hanlin Xu, Zenan Lin, Kailin Li, and Cewu Lu. Oakink2: dataset of bimanual hands-object manipulation in complex task completion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 445456, June 2024. 3 [124] Haotian Zhang, Ye Yuan, Viktor Makoviychuk, Yunrong Guo, Sanja Fidler, Xue Bin Peng, and Kayvon Fatahalian. Learning physically simulated tennis skills from broadcast videos. ACM Trans. Graph., 2023. 19 [125] He Zhang, Sebastian Starke, Taku Komura, and Jun Saito. Mode-adaptive neural networks for quadruped motion control. ACM Trans. Graph., 2018. [126] He Zhang, Yuting Ye, Takaaki Shiratori, and Taku Komura. Manipnet: Neural manipulation synthesis with hand-object spatial representation. ACM Trans. Graph., 2021. 3 [127] Hui Zhang, Sammy Christen, Zicong Fan, Luocheng Zheng, Jemin Hwangbo, Jie Song, and Otmar Hilliges. Artigrasp: Physically plausible synthesis of bi-manual dexterous grasping and articulation. In 2024 International Conference on 3D Vision (3DV), pages 235246. IEEE, 2024. 3 [128] Jiajun Zhang, Yuxiang Zhang, Liang An, Mengcheng Li, Hongwen Zhang, Zonghai Hu, and Yebin Liu. Manidext: Hand-object manipulation synthesis via continuous correspondence embeddings and residual-guided diffusion. arXiv preprint arXiv:2409.09300, 2024. 3, 21 [129] Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Yong Zhang, Hongwei Zhao, Hongtao Lu, Xi Shen, and Ying Shan. Generating human motion from textual descriptions with discrete representations. In CVPR, 2023. 19 [130] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondiffuse: Text-driven human motion generation with diffusion model. arXiv preprint, 2022. [131] Wanyue Zhang, Rishabh Dabral, Thomas Leimkühler, Vladislav Golyanik, Marc Habermann, and Christian Theobalt. Roam: Robust and object-aware motion generation using neural pose descriptors. In 2024 International Conference on 3D Vision (3DV), pages 13921402. IEEE, 2024. 3 [132] Wanyue Zhang, Rishabh Dabral, Vladislav Golyanik, Vasileios Choutas, Eduardo Alvarado, Thabo Beeler, Marc Habermann, and Christian Theobalt. Bimart: unified approach for the synthesis of 3d bimanual interaction with articulated objects. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. 2, 3, 5, 7, 21 [133] Xiaohan Zhang, Bharat Lal Bhatnagar, Sebastian Starke, Vladimir Guzov, and Gerard PonsMoll. Couch: Towards controllable human-chair interactions. In ECCV, 2022. 3 [134] Yaqi Zhang, Di Huang, Bin Liu, Shixiang Tang, Yan Lu, Lu Chen, Lei Bai, Qi Chu, Nenghai Yu, and Wanli Ouyang. Motiongpt: Finetuned llms are general-purpose motion generators. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 73687376, 2024. 19 [135] Yonghao Zhang, Qiang He, Yanguang Wan, Yinda Zhang, Xiaoming Deng, Cuixia Ma, and Hongan Wang. Diffgrasp: Whole-body grasping synthesis guided by object motion using diffusion model. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 1032010328, 2025. 3 [136] Zeyu Zhang, Akide Liu, Ian Reid, Richard Hartley, Bohan Zhuang, and Hao Tang. Motion mamba: Efficient and long sequence motion generation. In European Conference on Computer Vision, pages 265282. Springer, 2025. 19 [137] Kaifeng Zhao, Yan Zhang, Shaofei Wang, Thabo Beeler, and Siyu Tang. Synthesizing diverse human motions in 3d indoor scenes. In ICCV, 2023. 3 [138] Juntian Zheng, Qingyuan Zheng, Lixing Fang, Yun Liu, and Li Yi. Cams: Canonicalized manipulation spaces for category-level functional hand-object manipulation synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 585594, 2023. 3, 8, 21 [139] Yang Zheng, Yanchao Yang, Kaichun Mo, Jiaman Li, Tao Yu, Yebin Liu, Karen Liu, and Leonidas Guibas. Gimo: Gaze-informed human motion prediction in context. In ECCV, 2022. [140] Keyang Zhou, Bharat Lal Bhatnagar, Jan Eric Lenssen, and Gerard Pons-Moll. Toch: Spatiotemporal object-to-hand correspondence for motion refinement. In European Conference on Computer Vision, pages 119. Springer, 2022. 3 [141] Wenyang Zhou, Zhiyang Dou#, Zeyu Cao, Zhouyingcheng Liao, Jingbo Wang, Wenjia Wang, Yuan Liu, Taku Komura, Wenping Wang, and Lingjie Liu. Emdm: Efficient motion diffusion model for fast and high-quality motion generation. In ECCV 2024, pages 1838. Springer Nature Switzerland, 2024. 19 [142] Yi Zhou, Connelly Barnes, Lu Jingwan, Yang Jimei, and Li Hao. On the continuity of rotation representations in neural networks. In CVPR, 2019. 6 [143] Qingxu Zhu, He Zhang, Mengting Lan, and Lei Han. Neural categorical priors for physicsISSN 0730-0301. doi: based character control. ACM Trans. Graph., 42(6), dec 2023. 10.1145/3618397. URL https://doi.org/10.1145/3618397."
        },
        {
            "title": "A Introduction",
            "content": "This supplementary document provides additional details and analysis of our proposed method. In Section B, we provide more comprehensive review of related work. Section includes more details about our method. In Section D, we describe implementation details and experimental settings. Section provides deeper analysis of the generated motions and showcases various capabilities enabled by our method. Finally, Section discusses more limitations and broader impacts of this work. We provide supplementary webpage that presents high-resolution visualizations of all generated motion sequences."
        },
        {
            "title": "B Additional Related Work",
            "content": "Motion generation. Human motion generation is long-standing research problem [33, 125, 89 91, 23, 97, 10, 102, 141]. Recent approaches adopt wide range of neural architectures, including Mixture of Experts (MoE) [36, 125], recurrent neural network [65], transformer [99, 25, 78], and Mamba [22, 136]. To enhance motion diversity and realism, various generative paradigms have been explored, such as generative adversarial networks [21, 48], normalizing flow [43, 29], variational autoencoder [42, 78, 26], VQ-VAE [83, 129, 37, 134, 58], diffusion models [31, 97, 130, 11, 114, 112, 5] and mask modeling [7, 24]. With the availability of the large-scale datasets [80, 63, 50, 23, 52], motion generation has been conditioned on diverse modalities such as text [129, 24, 9], music [98], and audio [8]. In parallel, physics-based methods [75, 76, 59, 84] have enabled simulated humanoids [64] to perform various motor skills [124, 143, 104] through reinforcement learning. Several recent works [77, 109, 119, 18, 120, 61] learn latent representations of human motion that support skill reuse. Diffusion noise optimization. Diffusion models [31] have shown great success in various generative tasks [32]. To better control the generation process, classifier guidance [16] and classifier-free guidance [30] have been proposed. SDEdit [66] enables image editing by injecting noise into the reverse stochastic differential equation (SDE) process. DOODL [100] introduces an optimizationbased approach that directly updates the input noise of diffusion models by leveraging the invertible ordinary differential equation (ODE) [101]. This framework has been extended to other domains such as music generation [69]. In the motion domain, DNO [40] applies this idea for body-only motion editing, while ProgMoGen [54] uses LLMs [70] to select constraints and performs noise-space optimization for open-set motion control. However, these works are limited to body motion. In contrast, our work tackles the more complex setting of whole-body manipulation of articulated objects. We perform coordinated optimization over three diffusion models specialized for the body, left hand, and right hand, enabling coherent and physically plausible motion across the whole body."
        },
        {
            "title": "C Method Details",
            "content": "We adopt RoPE-based encoding scheme for object motion, inspired by 6-DoF CaPE [44], to effectively capture the temporal dynamics of objects. Given the object pose at i-th frame, we could use Pi to denote its 4 4 transformation matrix: (cid:20)Ri 0 where Ri is the rotation matrix and ti is the translation vector. The relative position embedding function π(v, P) should statisfy the following properties: Pi = ti 1 (13) (cid:21) , (14) where is the position embedding. Under this constraint, the attention between two transformed features can be equivalently rewritten as: π (v1, P1) , π (v2, P2) = (cid:10)π (cid:0)v1, P1 (cid:1) , π (v2, I)(cid:11) . 2 (cid:1) (cid:1) v1 2 P1 1 ϕ (cid:0)P (cid:0)ϕ (cid:0)P1 = (cid:0)v = (cid:10)π (v1, ϕ (P1)) , π (cid:0)v2, ϕ (cid:0)P (ϕ(I)v2) = (cid:0)v (cid:1) v2 (cid:1)(cid:1) (cid:0)ϕ (cid:0)P 1 2 2 (cid:1)(cid:1)(cid:11) . 1 ϕ (cid:0)P 1 2 (cid:1) = (ϕ (P1) v1) (cid:0)ϕ (cid:0)P (cid:1)(cid:1) v2 2 (cid:1) v2 (cid:1) (15) (16) (17) 19 Therefore, the relative embedding function can be implemented as π(v, P) = ϕ(P)v, where the transformation ϕ(P) is defined as: ϕ(P) = Ψ 0 0 ... 0 Ψ 0 ... . . . 0 0 Ψ 0 0 , Ψ = (cid:26)P if key if query (18) Here, the embedding dimension is assumed to be divisible by 4, and Ψ is either or depending on whether the input is key or query. This formulation allows each frames object pose to attend to others within temporal window using relative transformations, enabling more expressive modeling of object trajectories compared to simple velocity inputs. Following [85], we restrict the attention window to 120 neighboring frames during training and inference."
        },
        {
            "title": "D Implementation Details",
            "content": "D.1 Training details Model architectures. We adopt transformer-based diffusion architecture [99, 31], similar to MDM [97], for all models in our framework. All diffusion models are 8 transformer encoder layers, with 4 attention heads and 512 hidden units, and the feed-forward layer has 1024 hidden units. Network training. For object motion and end-effector trajectories diffusion models, we train them for 500 epochs with batchsize 32 and AdamW [56] optimizer, with learning rate 1 104. For body motion and hand motion diffusion models, we train them for 2000 epochs with batchsize 128 and AdamW optimizer, with learning rate 1 104. All diffusion models use 1000 sampling steps during training [31], with variance schedule increasing from β1 = 0.0001 to βT = 0.02 using the cosine schedule [68]. At inference time, we accelerate sampling for object motion and fingertip distance generation using DDIM [87] with = 100. All our experiments are conducted on single NVIDIA A100 GPU. D.2 Optimization details End-effector trajectories generation. Given the generated end-effector BPS, we use the Adam [41] optimizer with cosine-decayed learning rate 0.05 for 800 steps to calculate the end-effector trajectories. Whole-body motion generation. During inference, we perform noise optimization using DDIM [87] with =10 for 800 steps and cosine-decayed learning rate 0.05, following the DNO [40] strategy. The optimization loss with different weights λee, λpen, and λreg is as follows: = λeeLee + λpenLpen + λregLreg, (19) where Lee, Lpen, and Lreg are the end-effector tracking, penetration, and regularization losses. For first 300 steps, we set fingertip part as zero and only use the wrist targets, as the body motion might be very different from the generated end-effector trajectories, with λee = 1, λpen = 0, λreg = 0. For 300 to 500 steps, we optimize with all end-effectors, and set λee = 1, λpen = 0, λreg = 0. For 500 to 800 steps, we enable the penetration loss and regularization loss, and set λee = 1, λpen = 5.0, λreg = 1.0. D.3 Ablation study implementation details We present more details about the implementation of each variant in the ablation study. (a) single model to jointly predict object motion and end-effector trajectories. In our proposed pipeline, we utilize different models for object motion and end-effector trajectories. Instead, we train variant where single model predicts both object motion and end-effector trajectories. The difference is that in our pipeline, the input of the end-effector trajectory model includes the frame-wise object geometry embedding, as the object geometry of articulated objects is dynamic and changes over time. (b) Predicting the relative coordinate of end-effectors to the object center without end-effector BPS. In this variant, we directly predict the coordinate of end-effectors in the object coordinate system. This variant validates our design of using end-effector BPS to represent the end-effector trajectories. (c) Using object velocity and rotational velocity as the trajectory input without RoPEbased representation. This variant is to validate the effectiveness of the RoPE-based representation of object trajectories, which could capture the relative object trajectories in local temporal window of each frame. (d) Removing the optimization process and using conditional diffusion model with fingertip trajectories as input. This variant is to validate the effectiveness of the optimization process, which could generate more realistic whole-body motion. (e) Using single diffusion model for the entire body without the decoupled body-hand representation. This variant is to validate the effectiveness of the decoupled representation. (f) Excluding the AMASS [63] dataset during training the body motion model. This variant validates the effectiveness of including more body data (the AMASS [63] dataset) for training the body motion model."
        },
        {
            "title": "E More Analysis",
            "content": "E.1 Generalization to different object geometry Figure 5: Generalization to different object geometry. We train the object motion and end-effector trajectory models on hand-only data [138] with different object geometries. These models are integrated into our framework to provide optimization targets, enabling realistic whole-body motion synthesis for unseen object geometry. To validate the generalization to different object geometries, we train the object motion and endeffector trajectory models on the hand-only dataset [138]. Following previous work [138, 128, 132], we use 7 training and 3 testing objects. Thanks to our multi-stage design, the optimization only relies on the object motion and end-effector trajectories. Therefore, our method could generate whole-body motion, as shown in Figure 5. E.2 Object motion control To enable controllable object motion, we apply diffusion noise optimization to the object motion generation model by specifying keyframe object poses as targets. As shown in Figure 6, our method successfully generates manipulation sequences where the object is guided to reach the desired poses, producing plausible whole-body interactions. E.3 Simultaneous locomotion and manipulation We demonstrate simultaneous locomotion and manipulation in Figure 7. Starting from the generated object motion, we manually add horizontal translation to simulate object movement in different directions. This translation is then set as the root position target in the optimization process. Our method successfully produces whole-body motions that combine manipulation with walking forward, backward, left, and right. Notably, such combinations are not present in the ARCTIC [19] dataset, which only features manipulation while standing still. 21 Figure 6: Object motion control. Our method could generate coherent whole-body motion with the object motion keyframe. The blue object indicates the object motion keyframe. Figure 7: Simultaneous locomotion and manipulation. Our method enables the human to manipulate objects while simultaneously a) walking forward, b) walking backward, c) walking to the right, and d) walking to the left. The transparency of the meshes indicates time progression, where more transparent frames correspond to earlier frames. E.4 Deployment on simulated humanoids As shown in Figure 8, our generated whole-body motion can serve as reference for controlling humanoids in physics-based simulators. We apply physical motion tracking methods [75, 59, 107] to track the synthesized motions. The humanoid is able to physically interact with objects and perform coordinated manipulation behaviors in the simulated environment. E.5 Generating whole-body motion from hand-only dataset We demonstrate that our framework can generate whole-body motion from hand-only datasets. Specifically, we use the object trajectories provided by the dataset, and extract the end-effector trajectories (fingertips and wrists) as optimization targets. Our method only requires 3D positions of the fingertips and wrists, without the need for full joint rotations or detailed hand mappings, making it easier to apply. As shown in Figure 9, our method successfully produces realistic whole-body motions aligned with the provided hand-object interactions. 22 Figure 8: Deployment on simulated humanoids. We apply existing motion tracking techniques to deploy the generated motion to simulated humanoid. The articulated object is physically manipulated by the humanoid within the physics simulator [64]. Figure 9: Generating whole-body motion from hand-only dataset. We use the fingertip and object trajectories from the dataset and assign them as the optimization targets. After the optimization, we could get the whole-body motion. E.6 Inference speed Table 5 presents the inference time of each module in our pipeline. All results are measured on single NVIDIA A100 GPU, generating motion sequence of 300 frames. The majority of the time cost comes from the whole-body motion optimization process, which involves iterative diffusion sampling and gradient-based updates. Although slower than feed-forward models [47], this process enables high-quality, physically plausible whole-body interactions."
        },
        {
            "title": "F More Discussions",
            "content": "F.1 Limitations The optimization process is relatively slow compared to feed-forward generative models such as MDM [97], which may hinder real-time deployment or interactive applications. Reducing inference time while maintaining quality remains an important direction for future work. Due to the limited diversity of object categories in current datasets like ARCTIC [19], our model struggles to generalize to novel objects with significantly different geometries, topologies, or manipulation affordances. Our method only considers the rotational articulated objects, but other manipulation tasks, such as pushing or pulling, are not considered. Scaling to broader object types would require more diverse and high-quality motion data. Our current framework focuses on single-object manipulation. Extending it to multi-object scenarios, such as opening drawer and retrieving an item, or performing sequential multi-step tasks, poses both modeling and optimization challenges and remains an open problem. Our method does not explicitly model obstacle avoidance. While the resulting body and hand motions are physically plausible, the character may occasionally intersect with surrounding geometry in Table 5: Inference time. Module Object motion End-effector Whole-body motion Time 0.52 secs 3.66 secs 16.93 mins 23 cluttered or constrained scenes. Enabling both the fingers and the full body to reason jointly about nearby obstacles and environment geometry is an important direction for improving interaction realism. Our text conditioning is currently limited by the simplicity of available annotations. While Text2HOI [6] provides rule-based captions for hand-object interactions, they are typically segmented into short atomic motions, whereas we aim to model longer and more coherent manipulation sequences. Developing richer textual annotations and grounding them to temporally extended actions is promising avenue for future work. Our method primarily focuses on manipulation tasks, with the body posture adapting naturally to support the behaviors. However, some interactions require direct contact between the object and other body parts, such as pressing box against the torso or holding an item between the arm and the body. Modeling such whole-body contact behaviors remains largely unexplored and could further expand the expressiveness of interaction generation. F.2 Broader impact Our method can be used to create realistic manipulation sequence, which could be rendered as video. It also has the potential to be transferred to humanoid robots [60]. Therefore, our method has potential positive social impact to help build the development of character animation and humanoid robotics."
        }
    ],
    "affiliations": [
        "The University of Hong Kong",
        "Zhejiang University"
    ]
}