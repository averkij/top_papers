{
    "paper_title": "Understand Before You Generate: Self-Guided Training for Autoregressive Image Generation",
    "authors": [
        "Xiaoyu Yue",
        "Zidong Wang",
        "Yuqing Wang",
        "Wenlong Zhang",
        "Xihui Liu",
        "Wanli Ouyang",
        "Lei Bai",
        "Luping Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent studies have demonstrated the importance of high-quality visual representations in image generation and have highlighted the limitations of generative models in image understanding. As a generative paradigm originally designed for natural language, autoregressive models face similar challenges. In this work, we present the first systematic investigation into the mechanisms of applying the next-token prediction paradigm to the visual domain. We identify three key properties that hinder the learning of high-level visual semantics: local and conditional dependence, inter-step semantic inconsistency, and spatial invariance deficiency. We show that these issues can be effectively addressed by introducing self-supervised objectives during training, leading to a novel training framework, Self-guided Training for AutoRegressive models (ST-AR). Without relying on pre-trained representation models, ST-AR significantly enhances the image understanding ability of autoregressive models and leads to improved generation quality. Specifically, ST-AR brings approximately 42% FID improvement for LlamaGen-L and 49% FID improvement for LlamaGen-XL, while maintaining the same sampling strategy."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 5 8 1 5 1 . 9 0 5 2 : r Understand Before You Generate: Self-Guided Training for Autoregressive Image Generation Xiaoyu Yue1,2 Zidong Wang3 Yuqing Wang4 Wenlong Zhang1 Xihui Liu4 Wanli Ouyang1,3 Lei Bai1 Luping Zhou2 1Shanghai AI Laboratory 3Chinese University of Hong Kong 2University of Sydney 4University of Hong Kong"
        },
        {
            "title": "Abstract",
            "content": "Recent studies have demonstrated the importance of high-quality visual representations in image generation and have highlighted the limitations of generative models in image understanding. As generative paradigm originally designed for natural language, autoregressive models face similar challenges. In this work, we present the first systematic investigation into the mechanisms of applying the next-token prediction paradigm to the visual domain. We identify three key properties that hinder the learning of high-level visual semantics: local and conditional dependence, inter-step semantic inconsistency, and spatial invariance deficiency. We show that these issues can be effectively addressed by introducing self-supervised objectives during training, leading to novel training framework, Self-guided Training for AutoRegressive models (ST-AR). Without relying on pre-trained representation models, ST-AR significantly enhances the image understanding ability of autoregressive models and leads to improved generation quality. Specifically, ST-AR brings approximately 42% FID improvement for LlamaGen-L and 49% FID improvement for LlamaGen-XL, while maintaining the same sampling strategy1."
        },
        {
            "title": "Introduction",
            "content": "The field of image generation has witnessed remarkable progress through various approaches, including diffusion models [6, 19, 34], Generative Adversarial Networks (GANs) [21, 22, 38], and autoregressive models (AR) [20, 39]. Among these, autoregressive models, originally developed for natural language processing (NLP), have demonstrated exceptional generative capabilities as the foundational paradigm for large language models (LLMs) such as GPT [2] and Llama [42, 43]. When adapted to image generation, autoregressive models achieve performance comparable to modalityspecific methods, indicating their potential as unified generative framework across diverse data modalities [5, 9, 17, 40, 44]. Recent studies have highlighted the importance of image understanding in enhancing generation performance. For instance, REPA [47] enhances the generative capabilities of diffusion models by distilling self-supervised representations into their intermediate layers. Similarly, ImageFolder [30] introduces semantic regularization to the quantizer of the tokenizer to inject semantic constraints. These methods rely on pre-trained representation models to provide additional semantic information, as denoising and compressing may not be appropriate tasks for learning semantically meaningful image representations [47]. In contrast, the next-token prediction paradigm used by autoregressive models has proven to be an effective pre-training approach for capturing contextual information in natural language processing [35, 27, 36]. However, when adapted to vision, due to the inherent differences between image and text modalities, next-token prediction also faces challenges in learning high-level visual representations. 1https://github.com/yuexy/ST-AR Preprint. Under review. (a) (b) (c) Figure 1: Illustration of three properties of LlamaGen-B model. (a) Attention map from the last layer, highlighting the current token in red and tokens with larger attention weights in yellow. (b) Linear probing results on features from the 6-th layer at 8 uniformly selected steps. (c) Visual token indices from two slightly different views of the same image. In this work, we aim to enhance the learning of high-level visual representations in autoregressive models to improve the generative capability. Employing the popular autoregressive model LlamaGen [39], we first conduct an in-depth investigation into the intrinsic mechanisms of autoregressive image generation and identify three key properties that impact visual understanding: (1) Local and conditional dependence. Autoregressive models predominantly depend on local and conditional information. Our analysis of attention maps, as shown in Figure 1 (a), reveals strong dependence on the initial step (conditioning token) and spatially adjacent steps, highlighting that the model primarily utilizes conditional and local information for its predictions. (2) Inter-step semantic inconsistency. Figure 1 (b) demonstrates inconsistent semantic information across different timesteps, as evidenced by the top-1 linear probing accuracy. Specifically, while accuracy increases in early timesteps with more visible image tokens, its subsequent decline reveals that autoregressive models fail to maintain previously learned semantic information, thereby limiting the global modeling capability. (3) Spatial invariance deficiency. Autoregressive image generation models typically employ visual tokenizers, such as VQ-GAN [20, 28] to quantize images into discrete tokens. However, slight perturbations in image space can result in completely different tokens, as shown in Figure 1 (c). This ambiguity of objects significantly increases the difficulty for autoregressive models in encoding visual signals. These three problems create bottlenecks for autoregressive models in learning high-quality image representations, mirroring the challenge faced by diffusion models as revealed by REPA [47]. To this end, we propose ST-AR, short for Self-guided Training for AutoRegressive models, novel training paradigm that leverages techniques well-explored in self-supervised learning to enhance the modeling of visual signals. Specifically, for property 1, inspired by masked image modeling [46, 25] that forces the network to attend to larger regions of the image [33, 48], we randomly mask portion of the tokens in the attention map of the transformer layers. Meanwhile, for properties 2 & 3, we employ contrastive learning [16] to ensure the consistency of feature vectors from different time steps and views, referred to as inter-step contrastive loss and inter-view contrastive loss, respectively. The resulting training paradigm, ST-AR, incorporates MIM loss and two contrastive losses in addition to the token prediction loss, forming an iBOT-style [49] framework. ST-AR is utilized only during training, and the trained models retain the autoregressive sampling strategy, thus preserving their potential for unification with other modalities. By integrating visual self-supervised paradigms into next-token prediction, ST-AR eliminates the need for pre-trained representation learning models to provide additional knowledge, achieving stronger image understanding solely through self-guided training. Specifically, ST-AR significantly improves the linear probing top-1 accuracy of LlamaGen-B from 21.00% to 55.23% and demonstrates semantically meaningful attention maps. Furthermore, the enhancement in image understanding facilitates image generation. On class-conditional ImageNet, ST-AR boosts LlamaGen-B by 7.82 FID score. Notably, LlamaGen-XL trained with ST-AR for just 50 epochs achieves approximately 49% improvement in FID over the baseline, and is even comparable to LlamaGen-3B trained for 300 epochs, despite the latter having about 4 more parameters. Our contributions can be summarized as follows: 2 Conceptually, we conduct an in-depth investigation into the mechanisms of autoregressive image generation, identifying three key properties that hinder visual representation learning. Technically, we propose novel training paradigm, ST-AR, which enhances image understanding by integrating self-supervised training techniques into the next-token prediction paradigm. Experimentally, we conduct comprehensive experiments to validate the design of each component of ST-AR, demonstrating its effectiveness in both image understanding and generation."
        },
        {
            "title": "2 Related Work",
            "content": "Autoregressive Image Generation. The autoregressive (AR) generation paradigm has established itself as leading approach in language modeling [36, 14] due to its simplicity, scalability, and zeroshot generalization capabilities. When extended to image generation, AR methods can be categorized into three types according to the sampling strategies. Causal AR methods, such as VQ-GAN [20] and LlamaGen [39], directly adapt AR architectures for image synthesis, utilizing the traditional rasterorder next-token prediction paradigm as language models. Masked AR methods, like MaskGiT [12] and MAR [29], employ bi-directional attention within an encoder-decoder framework, supporting iterative generation with flexible orders. Parallelized AR methods introduce vision-specific designs to enhance visual signal modeling capability. VAR [41] proposes next-scale prediction that progressively generates tokens at increasing resolutions. PAR [45] and NPP [32] propose token grouping strategies to generate image tokens in parallel. Although masked and parallelized AR methods enhance the modeling of bidirectional image contexts, they require adjustments to sampling strategies. Our STAR focuses on improving the modeling of visual modalities within AR models without altering the sampling strategy, thereby enhancing image generation performance while preserving compatibility with language models. Self-Supervised Learning. In the field of visual self-supervised learning, methods can be broadly categorized into two types: contrastive learning and masked image modeling. The first to emerge was contrastive learning, exemplified by methods such as SimCLR[13], BYOL[23], MoCo[15, 24, 16], SwAV[10], and DINO[11]. These approaches typically employ image augmentation techniques to construct sets of positive samples and optionally use augmented views from other images as negative samples. They learn semantic information by aligning the representations of positive samples. Masked Image Modeling (MIM) [7, 46, 25] adapts the concept of Masked Language Modeling from NLP, training networks to reconstruct randomly masked portions of image content, thereby learning visual context. Some studies have shown that MIM primarily learns low-level pixel correlations and can adjust the effective receptive field size of the network by modifying the mask ratio. Our ST-AR leverages the strengths of both contrastive learning and MIM, using random masking on attention maps to increase the attention distance of autoregressive models, as well as employing MoCo-like contrastive losses to align representations across different time steps and different views."
        },
        {
            "title": "3 Method",
            "content": "3.1 Preliminaries We provide brief review of visual autoregressive models operating in discrete space. Given an input image I, quantized autoencoder is employed to convert to sequence of discrete tokens: = q(I), where = [x1, x2, ..., xT ] is the output token sequence, and q() denote the encoder and quantizer of the quantized autoencoder. The autoregressive model is trained to maximize the joint conditional probability of predicting the token xt at the current step t, based on the conditional vector and the preceding tokens [x1, x2, ..., xt1]. The condition can be class label or text vector. The training objective can be formalized as: max θ pθ(x) = (cid:89) t=1 pθ(xtc, x1, x2, ..., xt1), (1) 3 Figure 2: Attention maps of LlamaGen-B across layers and steps. These attention maps consistently show that conditional and spatially adjacent tokens receive the highest attention weights, while other tokens have significantly lower weights. where pθ is the autoregressive model parameterized by θ. And the token prediction loss is: LAR = 1 (cid:88) t=1 log pθ(xtc, x<t). (2) After training, pθ can iteratively generate new sequences. This process known as the next-token prediction, has been proven effective in text modeling. 3.2 Observations We conduct an in-depth investigation into the intrinsic mechanisms of autoregressive models in image generation, evaluating visual understanding capabilities through two aspects: attention maps and linear probing. For the class-conditional LlamaGen-B model trained on ImageNet [18], we first analyze the behavior of the transformer module by visualizing attention maps. Attention maps reveal what the model relies on for predictions and whether it can capture image context. Then, we evaluate the quality of learned representations by comparing linear probing results across intermediate layers at different time steps. Specifically, we closely followed the training protocol in MAE [25] and set the input class embedding to the null unconditional embedding for classifier-free guidance to prevent knowledge leakage. We uniformly select 8 out of 256 steps and feed the features from the sixth layer at the corresponding steps into trainable linear layers. Our observations are as follows: Obs. 1. Autoregressive models primarily rely on local and conditional information. In Figure 2, we present attention maps across various depths and positions, all exhibiting consistent pattern: the highlighted areas predominantly include spatially adjacent tokens and the first token. As indicated in Eq. 1, the input at the initial step is the conditional token, which significantly influences subsequent sampling, thereby holding considerable importance in the attention maps. Tokens surrounding the current token also receive elevated attention weights due to the inherent locality of images. Despite all preceding tokens being visible during training, the spatial proximity of tokens dictates that the most informative tokens for predicting the current token are typically those nearby. Excessive reliance on local information can impede the generation quality, as minor errors in adjacent tokens may be accumulated for subsequent steps. Obs. 2. Causal Attention Challenges Bi-directional Image Context Modeling. The application of causal attention to images presents two critical challenges: semantic inconsistency across different steps and limited global modeling capability. The inherent sequential nature of causal attention, which restricts each step to accessing only previously generated content, fundamentally limits the models capacity to capture comprehensive global information. As illustrated in Figure 1 (b), the linear probing accuracy at the initial steps is extremely low, indicating that AR models struggle to establish the correct semantic context in the early steps. Furthermore, the observed deterioration in linear probing performance beyond the 192-th step indicates progressive semantic misalignment in the learned representations as generation proceeds. This phenomenon underscores critical limitation in the models ability to maintain and leverage global contextual information effectively throughout 4 Figure 3: Overview of Self-Guided Training Pipeline. We incorporate masked image modeling (LMIM) to expand the effective field of visual autoregressive models. Additionally, we introduce inter-step contrastive learning (Lstep) to ensure global consistency, as well as inter-view contrastive learning (Lview) for consistency in visual representations. the generation process. Such constraints pose significant challenges for achieving coherent and semantically consistent image generation. Obs. 3. Visual tokens lack invariance. Autoregressive models utilize visual tokenizer like VQ-GAN to transform continuous image signals into discrete tokens. However, visual tokenizers are primarily trained for image compression and reconstruction, lacking invariance constraints. Consequently, when transformations are applied to an image of given object, the tokenizer may produce entirely different visual tokens, as demonstrated in Figure 1 (c). This variability in visual signals can confuse the model, resulting in redundant learning of identical semantic concepts. 3.3 Self-Guided Training for Autoregressive Models Building upon these observations, we introduce Self-guided Training for AutoRegressive models (ST-AR) to enhance the visual understanding capabilities of autoregressive models. ST-AR provides targeted solutions for the aforementioned challenges within unified training paradigm. Overview. The overall pipeline of ST-AR is illustrated in Figure 3. ST-AR borrows ideas from self-supervised representation learning, employing masked learning to expand attention regions while utilizing contrastive learning to ensure feature alignment across both steps and views. non-trainable teacher network[15, 24, 11] is employed to provide additional training objectives. It shares the same architecture as the autoregressive model (student model), and weights θ are updated through the Exponential Moving Average (EMA) of the student model parameters θ. ST-AR integrates reconstruction loss and two contrastive losses into the training of autoregressive models, eliminating dependence on pretrained representation models. We refer to it as Self-Guided Training. 3.3.1 Masked Learning for Longer Contexts As revealed in [33, 48], masked image modeling (MIM) can expand the effective receptive field of image encoding models. This insight motivates our approach to leverage MIM for addressing the challenge of AR models outlined in Obs. 1, i.e., the excessive dependence on local information. However, traditional MIM methods, which substitute input image tokens with special mask token, is unsuitable for autoregressive models. This is because autoregressive models, unlike autoencoders, necessitate the use of image tokens from the preceding step for next-token prediction. To overcome this, ST-AR utilizes random masking directly on the attention maps within transformer blocks, rather than on the input tokens. sequence mask is applied to the attention map, assigning negative infinity (-inf ) to ratio of the total tokens (masked tokens), while normal tokens are assigned as zero. Formally: Attn(Qi, Ki, Vi) = Softmax + Vi, (3) (cid:18) QiK dk (cid:19) where Qi, Ki, and Vi are the query, key, and value matrices for the i-th head. As the masking operation on attention may lead to information loss for next-token prediction, we employ teacher model to extract features and align the student model accordingly. Specifically, for given input tokens, we solely mask the attention maps of the student model and align the final hidden states of the student network to the teacher network. Given token length , the MIM loss can be 5 formalized as: LMIM ="
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) D(ht, ˆht), (4) t=1 where D(, ) is the distance function, defaulting to cosine distance, ht and ˆht are the features extracted from the last transformer layer of the student and teacher networks. 3.3.2 Contrastive Learning for Consistency The essence of Obs. 2 and Obs. 3 lies in the inconsistency of representations during the autoregressive iterative process. Specifically, Obs. 2 pertains to inconsistencies between different steps in the same image, while Obs. 3 relates to inconsistencies between different augmented image views. Inspired by the SSL methods, we use contrastive learning paradigm to solve such inconsistency. Given batch of images {I (b)}B b=1, ST-AR applies random augmentations to each image, resulting in set of augmented views {I (b,m)}M m=1. These augmented images are then encoded by VQ-GAN q(), producing discrete token sequences ZBM , where denotes the token sequence length (i.e., the steps of AR models). The resulting tokens are fed into both the student network pθ and EMA teacher network pθ, yielding token features hs = pθ(X) RBM D and ht = pθ(X) RBM D, where is the feature dimension. Following SimSiam[14], we employ projector (), which consists of several MLPs, on the student features: zs = (hs). The projector helps prevent model collapse and enhances training stability. To compute the contrastive loss, we randomly select token positions from the sequence length , denoted as RandomK(K, ). The sampled features used for loss computation are: ˆzs = zs[:, :, I, :] RBM KD, ˆht = ht[:, :, I, :] RBM KD. (5) We use inter-step contrastive loss Lstep to enforce semantic consistency across different steps, addressing Obs. 2. For each sampled student feature vector ˆz(b,m,i) , we define the positive sample as the teacher feature ˆh(b,m,j) extracted from the same view but different position, while the negative samples come from other images in the batch. Formally: Lstep = 1 (cid:88) (cid:88) (cid:88) b=1 m=1 i=j exp(ˆz(b,m,i) v=1 exp(ˆz(b,m,i) ˆh(b,m,j) ˆh(v,m,j) ) (cid:80)B . ) (6) In addition, we introduce inter-view contrastive loss Lview to ensure semantic consistency across different augmented views, addressing Obs. 3. Specifically, for student feature ˆz(b,i,k) , the positive sample is the teacher feature ˆh(b,j,k) extracted from the same token position but different view of the same image. Negative samples come from other images in the batch. The loss is defined as: Lview = 1 B (cid:88) (cid:88) (cid:88) b=1 i=j k= exp(ˆz(b,i,k) v=1 exp(ˆz(b,i,k) ˆh(b,j,k) ) ˆh(v,j,k) (cid:80)B . ) (7) To improve training efficiency, we set the number of image views = 2 in our implementation. We conduct an ablation study about the effects of the number of different steps on generation quality, which is detailed in Table 6. 3.3.3 Training Losses. We incorporate masked image modeling (Eq. 4) and contrastive learning (Eq. 6 and Eq. 7) into the conventional next-token prediction loss (Eq. 2). The final loss function can be formalized as: LST -AR = LAR + αLMIM + β 1 (Lstep + Lview), (8) where α and β are the weights for the reconstruction loss and contrastive losses, respectively. 6 Table 1: Comparisons between LlamaGen model and ST-AR. All the results are evaluated without using CFG on ImageNet. means the model is trained on 384 384 resolution and resized to 256 256 resolution for evaluation. Model #Params Epochs FID sFID IS Prec. Rec. LlamaGen-B + ST-AR LlamaGen-B + ST-AR LlamaGen-L + ST-AR LlamaGen-L + ST-AR Figure 4: Linear probing results of LlamaGen-B and our ST-AR. Our method demonstrates consistent improvements in image understanding. LlamaGen-XL LlamaGen-XXL LlamaGen-3B LlamaGen-XL + ST-AR + ST-AR 111M 50 111M 50 111M 300 111M 300 343M 50 343M 50 343M 300 343M 300 775M 300 300 1.4B 300 3.1B 775M 50 775M 50 775M 31.35 26.58 26.26 18.44 21.81 12.59 13.45 9.38 15.55 14.65 9.38 19.42 9.81 6.20 8.75 7.70 9.22 6.71 8.77 6.79 8.32 6. 7.05 8.69 8.24 8.91 6.94 6.47 39.58 49.91 48.07 66.18 0.57 0.60 0.59 0.64 0.62 59.18 0.65 91.19 82.29 0.66 112.71 0.70 0.62 79.16 86.33 0.63 112.88 0. 66.20 0.61 109.77 0.71 147.47 0.73 0.61 0.62 0.62 0.62 0.64 0.64 0.64 0.65 0.69 0.68 0.67 0.67 0.63 0.65 Figure 5: Attention maps of LlamaGen-B model trained with our ST-AR method. We utilize features from the final transformer layer, selecting random steps to draw attention maps. These maps exhibit an expanded effective receptive field, moving beyond mere focus on spatially adjacent and conditional tokens, and reveal distinct semantic patterns."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Implementation Details Dataset. We evaluate the effectiveness of ST-AR on the class-conditional image generation task using the widely adopted ImageNet-256256 dataset. We employ the same VQGAN[20] as LlamaGen[39] for tokenization, precomputing the image token sequences before training. Following LlamaGen, we also compute tokens for ten crops of the original image. Evaluation metrics. Since our ST-AR generative model is trained with self-supervised losses to enhance its visual modeling capabilities, we holistically evaluate ST-AR on image understanding and generation. For image understanding, we use the top-1 accuracy of linear probing as the primary metric. We adopt the linear probing setup of MAE[25], training linear layer for 90 epochs using the representations from the sixth layer. For image generation, we use the ADM evaluation suite and report Fréchet Inception Distance (FID)[26] as the main evaluation metric. Training & Inference. All the models are trained with the same setting as LlamaGen: base learning rate of 1 104 per 256 batch size, AdamW optimizer with β1 = 0.9, β2 = 0.05, weight decay set to 0.05 and gradient clipping set to 1.0. We train our models on images with 256 256 resolution, rather than LlamaGen with 384 384 training images. The teacher model is updated through the exponential moving average of the student model with an EMA decay of 0.9999. The class token embedding dropout ratio is 0.1 for classifier-free guidance. The contrastive loss is added on the medium of the transformer network, i.e. the 6-th layer for LlamaGen-B, 18-th layer for LlamaGen-L and 18-th layer for LlamaGen-XL. The masking ratio used for mask image modeling in Eq. 3 is set to = 0.25. The number of steps used in Eq. 6 and Eq. 7 is set as = 4. The weights of 7 Table 2: Model comparisons on ImageNet-256 256 Benchmark. All the results are evaluated with CFG. means the model is trained on 384 384 resolution and resized to 256 256 resolution for evaluation. ST-AR consistently beats baseline LlamaGen on all model sizes and training costs. Type GAN Diff. Masked AR Parallelized AR Casual AR Casual AR Model #Params Epochs FID sFID IS Prec. Rec. BigGAN [8] StyleGan-XL [38] LDM-4[37] DiT-XL[34] SiT-XL[31] MaskGIT[12] MaskGIT-re[12] VAR-d16[41] VAR-d20[41] VQGAN[20] VQGAN-re[20] LlamaGen-B [39] LlamaGen-L [39] LlamaGen-XL [39] LlamaGen-B + ST-AR LlamaGen-L + ST-AR LlamaGen-XL + ST-AR + ST-AR 112M 166M 400M 675M 675M 227M 227M 310M 600M 1.4B 1.4B 111M 343M 775M 111M 111M 343M 343M 775M 775M 775M 6.95 2.30 3.60 2.27 2.15 6.18 4.02 3.30 2. 15.78 5.20 6.09 3.08 2.63 5.46 4.09 3.81 2.98 3.39 2.72 2.37 7.36 4.02 5.12 4.60 4.50 - - - - - - 7.24 6.09 5.59 7.50 6.72 8.49 6.44 7.02 6.03 6.05 171.40 265.12 247.67 278.24 258.09 182.10 355.60 274.4 302. 74.30 280.30 182.54 256.07 244.09 193.61 246.29 248.28 264.11 227.08 254.59 270.59 0.87 0.78 0.87 0.83 0.81 0.80 0.83 0.84 0. - - 0.85 0.83 0.81 0.84 0.86 0.83 0.85 0.81 0.83 0.82 0.28 0.53 0.48 0.57 0.60 0.51 0.50 0.51 0. - - 0.42 0.52 0.58 0.46 0.47 0.52 0.53 0.54 0.57 0.58 1400 1400 300 300 300 300 300 300 300 300 300 50 50 Table 3: The effects of proposed losses. ST-AR improves linear probing and generation quality. Model LMIM Lstep Lview FID sFID IS Prec. Rec. LP Acc.(%) LlamaGen-B + ST-AR (Ours) 31.35 30.58 28.02 27.78 26.58 8.75 8.94 8.21 7.52 7.70 39.58 41.95 46.20 45.88 49.91 0.57 0.59 0.59 0.60 0. 0.61 0.59 0.61 0.61 0.62 18.68 22.71 27.73 38.31 45.27 reconstruction loss and contrastive loss in Eq. 8 are set to α = 1.0 and β = 0.5 by default. For inference, we use the same sampling strategy as LlamaGen. 4.2 Main Results Image understanding. The linear probing results are shown in Figure 4. ST-AR significantly enhances the linear probing performance of the baseline model, LlamaGen-B, across all steps, demonstrating improved image understanding capabilities. Importantly, the accuracy does not degrade after the 192-th step, indicating that LlamaGen trained with ST-AR effectively preserves semantic information from previous iterations during the sampling process. In Figure 5, we visualize the attention maps of the last layer at different steps. Compared to the baseline model (Figure 2), ST-AR not only significantly expands the scope of attention but also focuses on semantically relevant regions, further demonstrating that ST-AR effectively enhances the learning of visual semantic representations. Class-conditional image generation. As previously stated, the enhancement in image understanding also leads to higher generation quality. We first compare the LlamaGen models trained with STAR to their vanilla counterparts. As shown in Table 1, ST-AR achieves significant performance improvements across all LlamaGen variants. Specifically, for LlamaGen-XL, training with ST-AR for 50 epochs improves the FID score by approximately 10, reducing it from 19.42 to 9.81 compared to the vanilla counterpart. Further training for 300 epochs leads to an FID of 6.20, which is even stronger than LlamaGen-3B with 4 parameters. In Table 2, we provide results using classifier-free guidance (CFG) and comparisons with methods from other paradigms, including GANs, diffusion models, masked AR, and parallelized AR. ST-AR 8 Table 4: Ablation on mask ratio. Table 5: Ablation on contrastive loss depth. Table 6: Ablation on the number of selected steps. Ratio FID sFID IS Depth FID sFID IS #Steps FID sFID IS 0.15 0.25 0.35 0.45 28.62 26.58 26.36 27.50 7.28 7.70 8.20 8.31 44.58 49.91 49.73 47.15 3 (1/4-d) 6 (1/2-d) 9 (3/4-d) 12 (1-d) 27.34 26.58 28.76 29. 7.49 7.70 8.66 8.56 46.23 49.91 44.73 43.32 2 4 8 16 27.50 26.58 26.54 25.78 8.31 7.70 7.61 7.86 47.15 49.91 48.70 50. achieves consistent and significant improvements over LlamaGen while also delivering performance comparable to other state-of-the-art methods. Qualitative comparisons can be found in the supplementary material. 4.3 Ablation Studies We conduct comprehensive experiments on different configurations of ST-AR. All reported results are obtained using LlamaGen-B model trained for 50 epochs. Effectiveness of Training Losses. We conduct experiments to validate the effectiveness of the three loss functions in ST-AR, namely LMIM, Lstep, and Lview. The results are shown in Table 3. All three losses improve linear probing accuracy, thereby enhancing generation quality. Among them, the inter-view contrastive loss Lview contributes more to the improvement in linear probing accuracy compared to the inter-step contrastive loss Lstep. Notably, equipping LlamaGen-B with all three losses significantly increases its linear probing accuracy from 18.68% to 45.27%. Effect of Mask Ratio. Masked image modeling is key design in ST-AR, as discussed in Section 3.3.1, it expands the effective receptive field of the network. In Table 4, we examine the effect of the mask ratio on generation performance. The FID score is lowest when the mask ratio is 0.35. However, increasing the mask ratio leads to degradation in sFID, indicating that masking too many tokens can negatively affect the learning of low-level spatial structures. Effect of Contrastive Loss Depth. We validate the impact of incorporating the two contrastive losses, Lstep and Lview, at different depths of the network. There has long been view that image generators consist of an encoder and decoder. The results shown in Table 5 align with this perspective, demonstrating that applying contrastive losses at the 6 th layer (half the depth) yields the best performance. Effect of the Number of Steps. As described in Section 3.3.2, we randomly select different steps for contrastive learning. In Table 6, we examine the impact of the number of steps K. Larger values of lead to better generation performance. However, the improvement becomes marginal for > 4. Therefore, we set = 4 by default."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we focus on investigating the visual understanding capabilities of autoregressive models for image generation, offering an in-depth analysis and identifying three fundamental challenges that hinder the learning of high-level visual semantics. We demonstrate that these challenges can be effectively addressed by incorporating representation learning objectives, leading to novel training framework: Self-guided Training for AutoRegressive models (ST-AR). ST-AR employs masked image modeling to broaden attention regions while utilizing contrastive learning to maintain semantic consistency across steps and views. Extensive experiments validate ST-ARs effectiveness in enhancing visual understanding, which consequently improves image generation quality. Limitations & societal impacts. The main limitation of this work lies in increased training costs, which we will address in future research. While ST-AR establishes novel training paradigm for autoregressive image generation with potential industry applications, it may also raise concerns regarding image manipulation risks."
        },
        {
            "title": "References",
            "content": "[1] Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [3] Rohan Anil, Andrew Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023. [4] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. [5] Yutong Bai, Xinyang Geng, Karttikeya Mangalam, Amir Bar, Alan Yuille, Trevor Darrell, Jitendra Malik, and Alexei Efros. Sequential modeling enables scalable learning for large vision models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2286122872, 2024. [6] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: vit backbone for diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2266922679, 2023. [7] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint arXiv:2106.08254, 2021. [8] Andrew Brock. Large scale gan training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018. [9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [10] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. Advances in neural information processing systems, 33:99129924, 2020. [11] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the International Conference on Computer Vision (ICCV), 2021. [12] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1131511325, 2022. [13] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. In International conference on machine learning, pages 15971607. PMLR, 2020. [14] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. arXiv preprint arXiv:2011.10566, 2020. [15] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020. [16] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision In Proceedings of the IEEE/CVF international conference on computer vision, pages transformers. 96409649, 2021. [17] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):153, 2024. [18] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In EEE/CVF Conference on Computer Vision and Pattern Recognition, 2009. [19] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. 10 [20] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. [21] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. [22] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11): 139144, 2020. [23] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems, 33:2127121284, 2020. [24] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. arXiv preprint arXiv:1911.05722, 2019. [25] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1600016009, 2022. [26] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. [27] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of naacL-HLT, volume 1. Minneapolis, Minnesota, 2019. [28] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1152311532, 2022. [29] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. arXiv preprint arXiv:2406.11838, 2024. [30] Xiang Li, Kai Qiu, Hao Chen, Jason Kuen, Jiuxiang Gu, Bhiksha Raj, and Zhe Lin. Imagefolder: Autoregressive image generation with folded tokens. arXiv preprint arXiv:2410.01756, 2024. [31] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. arXiv preprint arXiv:2401.08740, 2024. [32] Yatian Pang, Peng Jin, Shuo Yang, Bin Lin, Bin Zhu, Zhenyu Tang, Liuhan Chen, Francis EH Tay, SerNam Lim, Harry Yang, et al. Next patch prediction for autoregressive visual generation. arXiv preprint arXiv:2412.15321, 2024. [33] Namuk Park, Wonjae Kim, Byeongho Heo, Taekyung Kim, and Sangdoo Yun. What do self-supervised vision transformers learn? arXiv preprint arXiv:2305.00729, 2023. [34] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. [35] Alec Radford. Improving language understanding by generative pre-training. 2018. [36] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. [37] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [38] Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-xl: Scaling stylegan to large diverse datasets. In ACM SIGGRAPH 2022 conference proceedings, pages 110, 2022. 11 [39] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. [40] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. [41] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. arXiv preprint arXiv:2404.02905, 2024. [42] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [43] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [44] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. [45] Yuqing Wang, Shuhuai Ren, Zhijie Lin, Yujin Han, Haoyuan Guo, Zhenheng Yang, Difan Zou, Jiashi Feng, and Xihui Liu. Parallelized autoregressive visual generation. arXiv preprint arXiv:2412.15119, 2024. [46] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: simple framework for masked image modeling. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 96539663, 2022. [47] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. arXiv preprint arXiv:2410.06940, 2024. [48] Xiaoyu Yue, Lei Bai, Meng Wei, Jiangmiao Pang, Xihui Liu, Luping Zhou, and Wanli Ouyang. Understanding masked autoencoders from local contrastive perspective. arXiv preprint arXiv:2310.01994, 2023. [49] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. ibot: Image bert pre-training with online tokenizer. arXiv preprint arXiv:2111.07832, 2021."
        }
    ],
    "affiliations": [
        "Chinese University of Hong Kong",
        "Shanghai AI Laboratory",
        "University of Hong Kong",
        "University of Sydney"
    ]
}