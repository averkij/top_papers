{
    "paper_title": "Patience Is The Key to Large Language Model Reasoning",
    "authors": [
        "Yijiong Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in the field of large language models, particularly through the Chain of Thought (CoT) approach, have demonstrated significant improvements in solving complex problems. However, existing models either tend to sacrifice detailed reasoning for brevity due to user preferences, or require extensive and expensive training data to learn complicated reasoning ability, limiting their potential in solving complex tasks. To bridge this gap, following the concept of scaling test-time, we propose a simple method by encouraging models to adopt a more patient reasoning style without the need of introducing new knowledge or skills. To employ a preference optimization approach, we generate detailed reasoning processes as positive examples and simple answers as negative examples, thereby training the model to favor thoroughness in its responses. Our results demonstrate a performance increase of up to 6.7% on GSM8k with training just on a lightweight dataset."
        },
        {
            "title": "Start",
            "content": "aTsinghua University 4 2 0 2 0 2 ] . [ 1 2 8 0 3 1 . 1 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in the field of large language models, particularly through the Chain of Thought (CoT) approach, have demonstrated significant improvements in solving complex problems. However, existing models either tend to sacrifice detailed reasoning for brevity due to user preferences, or require extensive and expensive training data to learn complicated reasoning ability, limiting their potential in solving complex tasks. To bridge this gap, following the concept of scaling test-time, we propose simple method by encouraging models to adopt more patient reasoning style without the need of introducing new knowledge or skills. To employ preference optimization approach, we generate detailed reasoning processes as positive examples and simple answers as negative examples, thereby training the model to favor thoroughness in its responses. Our results demonstrate performance increase of up to 6.7% on GSM8k with training just on lightweight dataset."
        },
        {
            "title": "Introduction",
            "content": "In the rapidly evolving field of artificial intelligence, Large Language Models (LLMs) have emerged as cornerstone for tackling complex problem-solving tasks. Amongst these tasks, mathematical problem-solving stands out due to its intrinsic complexity and the rigorous logical reasoning required. Recent consensus acknowledges that the Chain-of-Thought (CoT) (Wei et al., 2022) methodology significantly enhances the performance of LLMs on such intricate tasks. By intuitively breaking down problem into manageable steps with prompts such as \"Lets think step by step,\" models can showcase improved problemsolving capabilities. This approach has been further developed and diversified into numerous studies (Yu et al., 2023) aiming to meticulously craft prompts that dissect complex problems into sequential steps, thereby elevating model performance. 1 Groundbreaking research, including the exploration of scaling effects at test-time (Snell et al., 2024) and OpenAIs release of the o1 model (OpenAI, 2024b), continues to advance this concept. These innovations underscore the potent effect of spending more time on the inference phase, such as using the combination of problem decomposition and reflective thinking to substantially boost LLMs performance on complex issues. Nonetheless, models akin to o1 necessitate extensive training on high-quality CoT like data, which is notoriously expensive to collect and often not publicly available. Consequently, how to design data which can most effectively enhance LLMs ability to solve complex problems remains largely uncharted territory. Similar to OpenAI o1, many other works (Qin et al., 2024; Yang et al., 2024b; Lai et al.) have attempted to construct more valuable CoTlike reasoning data to optimize LLMs. However, due to the very precise requirements for data formats, these methods involve using many elaborate steps to construct data and also incur higher costs. Different from those methods, we attempt to use more simple and direct way: just teach LLMs to be patient. The motivation behind our research stems from critical observation: in many LLM applications where low latency is paramount, inference time is typically constrained, and models are expected to deliver concise answers promptly, leaving little to no room for elaborate reasoning processes. Consequently, the full potential of existing models is limited. But when using some specially designed prompting methods to break this limit, the performance will significantly improved (Yu et al., 2024). Therefore, based on the philosophy of scaling test-time, we posit that the more meticulous and painstaking the models analysis, the better its performance. However, because most current LLMs have undergone the user preference alignment phase, leading LLMs to favoring succinct answers (a common Figure 1: The overall process of our methods. user preference), they tend not to generate extensive and detailed reasoning processes, even with prompts encouraging step-by-step thinking. This often leads to oversimplified problem decompositions when facing complex problems, unless the user use targeted prompt design to indicate the specific reasoning methods. Inspired by previous researches (Qin et al., 2024), we posit that by minutely fine-tuning models with data encouraging detailed and patient reasoning, we could enhance their problem-solving abilities without imparting new knowledge or skills, circumventing the need for complex and costly data. Thus, we choose to use training-based method to make models spontaneously adopt more patient reasoning process, which is expected to generate less errors and give more accurate answer. To align models towards providing patient and detailed responses, we embrace preference optimization approach. By treating more elaborate reasoning processes as positive examples and simplistic answers as negative, we trained models to foster detailed reasoning. Initially, we generated basic reasoning sequences using gpt-4o (OpenAI, 2024a) and subsequently expanded these into more detailed, comprehensible steps as positive examples. Then, the base model are fine-tuned using the DPO technique (Rafailov et al., 2023) to internalize these comprehensive reasoning processes. The overall process of our methods is shown in Figure 1. itive and negative responses with our method. Our training is based on the Qwen2-7b-instruct (Yang et al., 2024a). Through evaluations, the optimized model shows great progress in reasoning ability, achieving 6.7% improvement on gsm8k (Cobbe et al., 2021). Despite utilizing fewer data and more straightforward method, this improvement surpassed many previous works (Lai et al.; Yang et al., 2024b), confirming the efficacy of our approach. Although our method unavoidably extends the models reasoning time, we believe the trade-off is justifiable by the achieved performance boost. Given the current research trajectory, pursuing avenues that potentially elevate LLMs complexproblem-solving proficiency with more time is worthwhile."
        },
        {
            "title": "2 Method",
            "content": "Our method mainly contains 4 steps, including original data collection, original solution generation, solution refinement and training, as follows: First, We collect thousands of math problems and their answers. The math problem is mostly in grade school level. Second, we use an LLM (such as GPT-4 (OpenAI, 2023)) to generate solutions of the problems with relatively concise CoT reasoning steps. We then check if the model give the correct answer in its solution, and only the solutions with the correct answer will be reserved. We collect about 5k mathematical problems from open-source datasets to generate both posThird, base on the solutions we get, we let the LLM further refine the solution, to make it more patient process. We does not strictly constrain the refined solutions to some specific format, but just ask the LLM to decompose the original solution into more intuitive, understandable and detailed operations, assuming the reader is novice. The prompts we used to generate the CoT solutions are: The prompt used to generate original CoT solutions {problem} Please think step by step to solve this problem. You need to give the number and title of every step, for example: Step 1: List all the ...... Step 2: Calculate the ...... Now, begin your solution."
        },
        {
            "title": "The prompt used to refine CoT solutions",
            "content": "Here is math problem: {problem} And this is its reference solution: {solution} However, as novice, cannot very clearly understand some steps of it. Maybe some operations are too direct and too brief. Therefore, please rewrite the whole solution with more patience and more detailed and understandable steps. For example, you can convert high-level operation into the combination of more intuitive sub-operations. Multiplication within 100 can be considered as simple and does not need to be split. However, you can use vertical forms for calculating large numbers. If you need to write down the final answer in the last step, please put it in box like \" boxed{2.5}\" Now directly begin your new solution. And here is an example showing the original solution and the refined solution generated by gpt4o (OpenAI, 2024a): 3 Problem: If Jeremy played three-letter word on triple word score to earn thirty points, and before the word score was tripled, the first and third letters were each worth one point, what was the value of the middle letter before the word score was tripled? Original Solution: Step 1: The word is threeletter word, and the first and third letters were each worth one point, so the total value of the first and third letters before the word score was tripled is 1+1 = 2 points. Step 2: The word score was tripled to earn 30 points, so the original word score before tripling was 30/3 = 10 points. Step 3: The total value of the first and third letters was 2 points, and the value of the middle letter was the remaining points, so the value of the middle letter before the word score was tripled is 10 - 2 = 8 points. The value of the middle letter before the word score was tripled is 8. The answer is: 8 Patient Solution: Step 1: Understand the Total Points After Tripling Jeremy played three-letter word on triple word score, and he earned total of 30 points. This means that the score for the word was tripled to reach 30 points. Step 2: Calculate the Original Score Before Tripling Since the word score was tripled to reach 30 points, we need to find out what the original score was before it was tripled. To do this, we divide the total points by 3: Original Score = 30 3 = 10 So, the original score of the word before tripling was 10 points. Step 3: Determine the Contribution of the First and Third Letters We know that the first and third letters of the word are each worth 1 point. Therefore, the combined value of these two letters is: 1 + 1 = 2 Step 4: Calculate the Value of the Middle Letter Now, we know that the total original score of the word (before tripling) was 10 points, and the first and third letters together account for 2 points. This means the middle letter must account for the remaining points. To find the value of the middle letter, we subtract the points contributed by the first and third letters from the original score: Value of Middle Letter = 10 2 = 8 Therefore, the value of the middle letter before the word score was tripled is 8 . Finally, we use the refined solutions as the positive samples and the concise solutions as negative samples to apply DPO (Rafailov et al., 2023) to optimize the base model. After the training, the model will tend to give longer reasoning process patiently."
        },
        {
            "title": "3 Experiment",
            "content": "3.1 Experiment Settings Since there already have been many researches about enhancing LLMs ability in solving math problems, we do not need to collect math problems from scratch. Therefore, in our experiement, we directly use the dataset provided by Step-DPO (Lai et al.), which contains the problem, its answer and correct solution with relatively concise reasoning process. Based on this dataset, we let GPT-4o (OpenAI, 2024a) to rewrite the original solution by decompose it into more intuitive and understandable steps. Then we get the dataset used for DPO. In DPO training, we use Qwen2-7B-Instruct (Yang et al., 2024a) as the base model and use LoRA to train all its linear layers with lora rank 16. We train for 1 epoch and the learning rate is constant at 5e-5. The training process only takes less than 5 minutes on 8 A100 GPUs. We use two typical math and reasoning ability benchmarks, gsm8k (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021), to evaluate the effect our methods. When evaluating, we adopt 0-shot prompt and add CoT prompt \"Please reason step by step, and put your final answer within boxed\" after the math problem. Meanwhile, we record the time required for the model to complete each problem on single A100 GPU."
        },
        {
            "title": "3.2 Results",
            "content": "The results in Table 1 show the accuracy of the base model and the model optimized by our method on the gsm8k benchmark. It is amazing that our method achieves the 6.7% improvement on gsm8k, and also increased the accuracy by 0.2% on MATH, although the cost of our methods is extremely low. From Table 1, we can also observe the inference time is increased. However, it is still within acceptable range. Therefore, in practice, using this more time-consuming but more accurate method is still advisable. Method gsm8k math time baseline ours 81.2 87. 48.8 49.0 7.2 10.9 Table 1: The accuracy (%) and the average time consumption (seconds) of the based model and the model optimized by out methods on gsm8k and MATH."
        },
        {
            "title": "4 Conclusion",
            "content": "In this study, we present simple but effective approach to enhance the complex-problem-solving capabilities of LLMs by fostering more patient and detailed reasoning process. Through the generation of enriched CoT-like responses and the implementation of preference optimization technique, our method demonstrated significant improvements in mathematical problem-solving performance. While our approach necessitates slight increase in inference time, the trade-off for enhanced accuracy was deemed worthwhile. These findings underscore the potential benefits of prioritizing the reasoning depth and length in LLM applications, paving the way for future research into optimizing complex-problem-solving strategies without the need for expensive datasets."
        },
        {
            "title": "References",
            "content": "Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training Verifiers to Solve Math Word Problems. arXiv preprint. ArXiv:2110.14168. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring Mathematical Problem Solving With the MATH Dataset. arXiv preprint. ArXiv:2103.03874 [cs]. Xin Lai, Zhuotao Tian, Yukang Chen, Senqiao Yang, Xiangru Peng, and Jiaya Jia. Step-DPO: Step-wise preference optimization for long-chain reasoning of LLMs. Preprint, arxiv:2406.18629 [cs]. OpenAI. 2023. GPT-4 Technical Report. Technical report. OpenAI. 2024a. GPT-4o System Card. arXiv preprint. ArXiv:2410.21276. OpenAI. 2024b. Introducing OpenAI o1. Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector 4 Liu, Yuanzhi Li, and Pengfei Liu. 2024. O1 Replication Journey: Strategic Progress Report Part 1. arXiv preprint. ArXiv:2410.18982. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct Preference Optimization: Your Language Model is Secretly Reward Model. Advances in Neural Information Processing Systems, 36:53728 53741. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. 2024. Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters. arXiv preprint. ArXiv:2408.03314 [cs]. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. 2024a. Qwen2 technical report. Yuqing Yang, Yan Ma, and Pengfei Liu. 2024b. arXiv preprint. Weak-to-Strong Reasoning. ArXiv:2407.13647. Yijiong Yu, Ma Xiufa, Fang Jianwei, Zhi Xu, Su Guangyao, Wang Jiancheng, Yongfeng Huang, Zhixiao Qi, Wei Wang, Weifeng Liu, Ran Chen, and Ji Pei. 2024. Hyper-multi-step: The Truth Behind Difficult Long-context Tasks. arXiv preprint. ArXiv:2410.04422. Zihan Yu, Liang He, Zhen Wu, Xinyu Dai, and Jiajun Chen. 2023. Towards Better Chain-of-Thought Prompting Strategies: Survey. arXiv preprint. ArXiv:2310.04959."
        }
    ],
    "affiliations": [
        "Tsinghua University"
    ]
}