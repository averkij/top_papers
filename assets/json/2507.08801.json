{
    "paper_title": "Lumos-1: On Autoregressive Video Generation from a Unified Model Perspective",
    "authors": [
        "Hangjie Yuan",
        "Weihua Chen",
        "Jun Cen",
        "Hu Yu",
        "Jingyun Liang",
        "Shuning Chang",
        "Zhihui Lin",
        "Tao Feng",
        "Pengwei Liu",
        "Jiazheng Xing",
        "Hao Luo",
        "Jiasheng Tang",
        "Fan Wang",
        "Yi Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Autoregressive large language models (LLMs) have unified a vast range of language tasks, inspiring preliminary efforts in autoregressive video generation. Existing autoregressive video generators either diverge from standard LLM architectures, depend on bulky external text encoders, or incur prohibitive latency due to next-token decoding. In this paper, we introduce Lumos-1, an autoregressive video generator that retains the LLM architecture with minimal architectural modifications. To inject spatiotemporal correlations in LLMs, we identify the efficacy of incorporating 3D RoPE and diagnose its imbalanced frequency spectrum ranges. Therefore, we propose MM-RoPE, a RoPE scheme that preserves the original textual RoPE while providing comprehensive frequency spectra and scaled 3D positions for modeling multimodal spatiotemporal data. Moreover, Lumos-1 resorts to a token dependency strategy that obeys intra-frame bidirectionality and inter-frame temporal causality. Based on this dependency strategy, we identify the issue of frame-wise loss imbalance caused by spatial information redundancy and solve it by proposing Autoregressive Discrete Diffusion Forcing (AR-DF). AR-DF introduces temporal tube masking during training with a compatible inference-time masking policy to avoid quality degradation. By using memory-efficient training techniques, we pre-train Lumos-1 on only 48 GPUs, achieving performance comparable to EMU3 on GenEval, COSMOS-Video2World on VBench-I2V, and OpenSoraPlan on VBench-T2V. Code and models are available at https://github.com/alibaba-damo-academy/Lumos."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 1 0 8 8 0 . 7 0 5 2 : r Lumos-1: On Autoregressive Video Generation from Unified Model Perspective Hangjie Yuan1,2,3, Weihua Chen1,2,, Jun Cen1,2,3, Hu Yu1, Jingyun Liang1,2, Shuning Chang1,2,3, Zhihui Lin1,2,3, Tao Feng4, Pengwei Liu1,3, Jiazheng Xing1,3, Hao Luo1,2, Jiasheng Tang1,2, Fan Wang1, Yi Yang3 1DAMO Academy, Alibaba Group 2Hupan Lab 3Zhejiang University 4Tsinghua University Corresponding author Autoregressive large language models (LLMs) have unified vast range of language tasks, inspiring preliminary efforts in autoregressive video generation. Existing autoregressive video generators either diverge from standard LLM architectures, depend on bulky external text encoders, or incur prohibitive latency due to next-token decoding. In this paper, we introduce Lumos-1, an autoregressive video generator that retains the LLM architecture with minimal architectural modifications. To inject spatiotemporal correlations in LLMs, we identify the efficacy of incorporating 3D RoPE and diagnose its imbalanced frequency spectrum ranges. Therefore, we propose MM-RoPE, RoPE scheme that preserves the original textual RoPE while providing comprehensive frequency spectra and scaled 3D positions for modeling multimodal spatiotemporal data. Moreover, Lumos-1 resorts to token dependency strategy that obeys intra-frame bidirectionality and inter-frame temporal causality. Based on this dependency strategy, we identify the issue of frame-wise loss imbalance caused by spatial information redundancy and solve it by proposing Autoregressive Discrete Diffusion Forcing (AR-DF). AR-DF introduces temporal tube masking during training with compatible inference-time masking policy to avoid quality degradation. By using memory-efficient training techniques, we pre-train Lumos-1 on only 48 GPUs, achieving performance comparable to EMU3 on GenEval, COSMOSVideo2World on VBench-I2V, and OpenSoraPlan on VBench-T2V. Code and models are available at https://github.com/alibaba-damo-academy/Lumos. Date: July 14,"
        },
        {
            "title": "1 Introduction",
            "content": "Autoregressive models have demonstrated significant advancement in the field of language processing [6, 44, 70, 45, 3, 37] by unifying diverse language tasks into one single framework and expanding the scale of large language models to unprecedented sizes. Following this trail of research, significant efforts have emerged in autoregressive visual generation [33, 7, 8, 42]. By utilizing similar architectural and generation designs as those used in LLMs, there is substantial potential for advancing LLMs towards unified model capable of both visual generation and understanding [77, 81, 95, 68]. Pioneering research efforts have been dedicated to instantiating autoregressive visual generation, with focus on key aspects, spanning improving autoregressive paradigm [46, 67, 8, 42], improving tokenizer format [33] (e.g., from discrete to continuous), and enhancing tokenizer capabilities [64, 1, 74, 58]. However, an autoregressive video generation paradigm that is 1 fully compatible with unified models remains underexplored. Preliminary attempts either exhibit architectural distinctions with LLM architectures [14, 72], rely on external text encoders [14], or trail in generation efficiency [79], leaving much room for advancement. In this work, we propose Lumos-1, model that leverages the exact architecture of LLM with minimal architectural modifications to achieve autoregressive video generation, in such way that it enables video generation without pre-trained external text encoder (figure 1 visualizes examples generated by Lumos-1). To ensure inference efficiency, we build Lumos-1 upon the discrete diffusion paradigm [81, 63], whose lower bound is the cross-entropy loss employed in MaskGIT [8]. Distinct from texts, videos inherently exhibit strong spatiotemporal correlation. Although previous works have incorporated this prior into generative models by overturning the next-token prediction paradigm [67, 8], we dive into basic technique thats widely used in Figure 1 Visualization of examples generated by Lumos-1. Lumos-1 supports text-to-image, image-to-video and text-tovideo tasks. LLMsRotary Position Embeddings (RoPE), which is heavily underexplored in autoregressive video generation. Preliminary experiments conducted with Lumos-1 reveal that 1D RoPE is far from optimal, and the incorporation of 3D RoPE facilitates autoregressive generative learning. However, existing 3D RoPE suffers from imbalanced frequency spectrum ranges for temporal and spatial modeling. Building on this insight, we propose MM-RoPE, an improved version of RoPE that better accommodates the structure of video data. This enhancement involves more distributed frequency allocations for more comprehensive context modeling and the appropriate scaling of 3D position information for modality balancing. Though MM-RoPE modifies RoPE for visual tokens, the RoPE for text tokens remains consistent with the one from original LLMs, therefore preserving the language learning capabilities. Distinct from images, videos inherently exhibit 1) temporal causality between frames and 2) spatial information redundancy between frames. The temporal causality renders the paradigm of autoregressive image generation suboptimal for videos, which relies on global sequence bidirectional dependency [8] or rasterscan order dependency [62]. In response, Lumos-1 resorts to token dependency strategy that emphasizes intra-frame bidirectional dependencies and interframe temporally causal dependencies, which resonates with the nature of video data. Meanwhile, spatial information redundancy degrades the efficacy of training with random mask prediction, as spatial information leakage allows the model to easily predict the masked token by attending to unmasked ones in previous frames. To alleviate this, Lumos-1 introduces Autoregressive Discrete Diffusion Forcing (AR-DF). The core of AR-DF training involves temporal tube masking [69], which repeats frame-level random mask pattern across the temporal dimension. To some extent, such design mitigates the issue of frame-wise imbalanced loss [79], thereby enabling more effective learning. The core of AR-DF inference includes an inference-time masking strategy that aligns with partial observation of history during training, enabling inference without degraded frame 2 quality and motion. We leverage Llama [71] as our architecture and leverage unified discrete codebook for language and visual data. Through stage-wise training with GPU memory-friendly techniques, we pre-tarin Lumos-1 from scratch on 60 million images and 10 million videos using only 48 GPUs, achieving performance comparable to EMU3 on GenEval, COSMOSVideo2World on VBench-I2V, and OpenSoraPlan on VBench-T2V."
        },
        {
            "title": "2 Related Work",
            "content": "Autoregressive image generation. Spurred by the progress of LLMs, visual generation research is rapidly shifting from diffusion paradigms [55, 24, 93, 15] to an autoregressive one [62, 67, 86, 27, 52, 96, 90, 23, 43, 75, 53], due to its potential of being integrated into unified models [65, 97] with minimal modification. Preliminary research, such as Parti [87], DALL-E [51], MaskGiT [8] and LLamaGen [62], have demonstrated the efficacy of autoregressive generation using discrete tokenizers. MAR [33] incorporates diffusion-style objective into autoregressive training, thereby accommodating continuous tokens. VAR [67] and FAR [86] reformulate the autoregressive target as next-scale and next-frequency prediction, respectively, improving generation fidelity. Unlike them, we dive into the design of the RoPE technique in visual generation. Autoregressive video generation. Autoregressive video synthesis follows trajectory similar to images, but the substantial computational overhead encouraged researchers to explore different granularities of autoregression. At the macro level, methods such as CausVid [84], Pyramidal Flow [29], and MAGI-1 [56] generate video clips by recursively invoking diffusion models. second line of work adopts hybrid AR-diffusion strategy that couples pre-trained diffusion backbone with an external autoregressive planner, as exemplified by ARLON [34] and Mardini [40]. Our focus, however, is on micro AR approaches [21, 96, 88, 89, 72, 30, 77, 14, 79, 85], which treat the entire spatiotemporal token sequence as single context and generate videos end-to-end with one autoregressive transformer (e.g., Phenaki [72], VideoPoet [30], Loong [79], EMU3 [77], NOVA [14]). Building on this paradigm, we introduce AR-DF, which is compatible with intra-frame bidirectionality and inter-frame temporal causality, enabling effective training and inference with LLM-style architectures. RoPE for vision-language data. RoPE was first proven effective for LLMs [61, 70] and later adopted in 3 DiTs [47] to inject spatial priors. However, its potential for autoregressive video generation remains heavily underexplored. Related works, including MRoPE [76], RoPE-2D [2], TAD-RoPE [17] and VideoRoPE [80], incorporated structure priors (i.e., spatiotemporal information or its subsets) into RoPE for better vision-language understanding. VoPE [10] improves high-resolution image generation by continuous resolution learning. We propose MM-RoPE, distributed and scaled RoPE technique that improves AR video synthesis while remaining plug-and-play for unified multimodal models."
        },
        {
            "title": "3 Lumos-1",
            "content": "This section aims to introduce the design philosophy behind Lumos-1. In Sec. 3.1, we introduce MM-RoPE that enables better spatiotemporal awareness of an LLM for modeling visual data. In Sec. 3.2, we introduce AR-DF that enables effective training and inference. In Sec. 3.3, we introduce important techniques for implementing Lumos-1, including architecture, memory-friendly techniques, etc."
        },
        {
            "title": "3.1 Spatiotemporal Correlation Injection via",
            "content": "MM-RoPE Preliminaries of 3D RoPE. One de facto design of contemporary LLM is the RoPE [61] technique, whose overall aim is to encode the absolute position with rotation matrix while incorporating the explicit relative position dependency in the attention mechanism. This can be formulated as: fq(xm, m), fk(xn, n) = g(xm, xn, n) (1) where fq(xm, m) encodes the position for the embedding xm to obtain the query feature (fk(xn, n) is analogously defined); g(xm, xn, n) is function that defines the inner product between the query and key vectors that explicitly encodes the relative position n. The resultant form of function f{q,k} can be formulated as: f{q,k}(xm, m) = Rd Θ,mWq,kxm (2) where Wq,k is the projection matrix; Rd Θ,m is the rotary matrix with pre-defined parameters Θ = {θi = β2(i1)/d, = [1, 2, ..., d/2]} with acting as the dimension of the embeddings and β acting as the base frequency. In such formulation, the attention calculation can be rewritten as: fq(xm, m)Tfk(xn, n) = xT Rd Θ,τ Wkxn mW τ = (3) Figure 2 (a) Initial exploration of 3D RoPE in autoregressive video generation. The sequence is comprised of text tokens and visual tokens (default arrangement in this paper), with global indices starting from 0. Two schemes are exemplified using one text token and one visual token. In Scheme 1 and 2, temporal, height and width positions start from 0. (b) Details of MM-RoPE compared to M-RoPE. The figure illustrates the distributed channel allocation of MM-RoPE. Temporal, height and width positions are indexed after text tokens. where the detailed formulation of Rd Θ,τ can be formulated using base rotary matrix Rθ,τ , with θ as the frequency and τ as the relative position: Rθ1,τ 0 ..."
        },
        {
            "title": "0\nRθ2,τ\n...\n0",
            "content": "(cid:20)cos τ θ sin τ θ cos τ θ sin τ θ 0 0 . . . Rθd/2,τ (cid:21) 0 Rd Θ,τ = Rθ,τ = (4) However, the application of the original RoPE to modeling visual data remains suboptimal considering the spatiotemporal correlation of visual tokens. One popular type of generative models, diffusion models [25, 59, 24], have improved upon this technique by proposing 3D RoPE that jointly injects spatiotemporal latent coordinates during attention calculation and demonstrates its effectiveness [26, 31, 66]. If we slightly abuse the annotation by denoting xT mW and Wkxn as and Xn, we can formulate the attention calculation in equation (1) based on 3D RoPE as: Xn,ts:te + m,ts:te Rθts+1,τt ... 0 m,hs:he Rθhs+1,τh ... 0 Rθws+1,τw ... m,ws:we 0 ... . . . Rθte ,τt 0 ... . . . Rθhe ,τh . . . Rθwe ,τw 16 d}, {hs, he} = { 0 ... Xn,ws:we where {ts, te} = {0, 2 {ws, we} = { 5 16 d, 1 16 d} and 2 d} denote the start and end di16 d, 5 Xn,hs:he + (5) mension index for encoding temporal, height and width relative position; denotes the submatrix extracted from using row indices [2ts, 2te); other matrices are similarly defined. m,ts:te Initial exploration of incorporating 3D RoPE. Our preliminary exploration starts with the introduction of 3D RoPE to autoregressive video generation. We follow previous works [16, 33] to utilize the validation loss to observe the effectiveness due to its strong correlation with evaluation metrics. By default, we use the cross-entropy loss (C-Loss), following the standard LLM training objective. We compare the vanilla LLM RoPE with three schemes, as shown in figure 2: 1) Scheme 1, which allocates the first 1/2 channels to encode the global position (i.e., the index in the global sequence) and the second 1/2 channels to encode the temporal, height and width positions with ratio of 2 : 3 : 3. For text tokens, we only use the first half channels to encode the global positions to ensure the language modeling capability, while for visual tokens, we only use the second half to encode 3D positions; 2) Scheme 2, which extends upon scheme 1 by leveraging the first half channels of the visual tokens to encode the global positions. 3) M-RoPE [76], which uses all channels of the visual tokens to encode the 3D positions. From figure 3(a), we can observe that: 1) The incorporation of the spatiotemporal correlation in RoPE can significantly improve the performance of the model fitting data by comparing vanilla RoPE and Scheme 1. 2) The injection of the raster-scan order position information to visual tokens (i.e., the global position in Scheme 2 of figure 2(a)) can degrade the performance. 3) comprehensive channel utilization (M-RoPE) is better than partial channel utilization (Scheme 1). Therefore, it is promising to inject such priors in this 4 Figure 3 (a) Validation loss curve of toy experiment exploring the necessity of 3D RoPE using the 0.5B model; (b) Frequency allocation in the vanilla 3D RoPE; (c) Comparison of rotary speed of the first dimension for temporal and height dimensions (high and middle frequencies) in the vanilla 3D RoPE. generative model. Peering into 3D RoPE and identifying its limitations. Although 3D RoPE proves effective in practice, its design remains suboptimal. In figure 3(b), we visualize how the frequencies are allocated to model temporal, height, and width dimensions. We observe that the temporal channels dominate large frequency spectrum, whereas the height and width channels are relegated to near-zero frequencies. For the sine function, the relative positions τ (when τ >= 0) should not exceed one period to avoid ambiguity since radians beyond 2π start repeating patterns in the function. Beyond this range, the model cannot distinguish fine-grained positional differences since rotary embeddings of this channel lose uniqueness. For the low-indexed channels, the embeddings can rotate significantly faster than high-indexed channels (as shown in figure 3(c)), leading to accelerated aliasing and loss of embedding uniqueness. For the high-indexed channels, the embeddings rotate so slowly that they lack sufficient resolution to model subtle local changes. Finally, while height and width are symmetrically important, they occupy disproportionately small and different segments of the overall frequency spectrum, reducing their capacity to capture spatial details effectively. MM-RoPE: distributed and scaled 3D RoPE mechanism. To elegantly solve the aforementioned limitation, we propose MM-RoPE, distributed 3D RoPE mechanism. Compared with M-RoPE [76] that is widely adopted in vision-language models, one core idea of MM-RoPE is to encode relative positions across comprehensive frequency spectra for all 3D information. As illustrated in figure 2(b), the RoPE for text tokens in MM-RoPE follows the standard design of an LLM, whereas the RoPE for visual tokens is comprised of multiple meta MM-RoPE components. Within each meta MM-RoPE, we keep the ratio of 3D information identical to 3D RoPE (i.e., 2 : 3 : 3), while minimizing the overall dimensionality to maintain more distributed design. Concretely, we first allocate channels for temporal information, then symmetrically interleave height and width channels to model spatial information. We can formulate the attention calculation of the first meta MM-RoPE as: m,0:2 m,2:4 m,4: m,6:8 (cid:20)Rθ1,τt 0 (cid:20)Rθ3,τh 0 (cid:20)Rθ5,τh 0 (cid:20)Rθ7,τh 0 (cid:21) 0 Rθ2,τt 0 Rθ4,τw 0 Rθ6,τw 0 Rθ8,τw Xn,0:2+ (cid:21) (cid:21) (cid:21) Xn,2:4+ Xn,4:6+ Xn,6: (6) where each meta MM-RoPE component comprises 16 channels; additional components are defined analogously and collectively form the RoPE strategy for visual tokens. Moreover, for model that jointly processes text and visual tokens, the interplay between two modalities is significant to ensure the vision-language alignment. However, the range of positions for representing texts or visual data tends to differ. Contemporary visual generation systems are typically trained with extremely long and descriptive captions [4, 66], despite the low latent resolution of visual data (e.g., video of resolution 448 256 25 becomes 56 32 7 after 8 8 4 compression). To balance the two modalities, we propose scaling the 3D positions to ensure balanced learning. Specifically, we empirically scale the latent 3D positions to the RGB space by multiplying the compression ratio, as shown in figure 5(a). This simple scaling operation, from another perspective, improves the RoPE resolution for visual tokens by slightly accelerating the rotation speed. Experiments in the experiment section demonstrate its efficacy, thereby showing the importance of balancing the two modalities from the RoPE perspective. 5 ρ Uniform([ρtra, 1]); Bernoulli(1 ρ) Xp, Xv tokenize text, frames Sample mask pattern for all frames: Algorithm 1 AR-DF Training Procedure Require: video dataset D, training mask ratio ρtra, generative model Gϕ, number of latent frames 1: for each video do 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: end for end for Form (cid:101)X = {Xp, (cid:101)X (1) Generate attention mask AttnM ask for (cid:101)X (cid:98)X Gϕ( (cid:101)X, AttnM ask) Compute loss L( (cid:98)X, X, ) Backprop & update ϕ + (1 ) [MASK] Apply to all frames = (t) , . . . , (cid:101)X (T ) } for = 1 do (cid:101)X (t) Gϕ(Xp, AttnM ask(p)) Algorithm 2 AR-DF Inference Procedure Require: text prompt Xp, trained model Gϕ, inference mask ratio ρinf , number of latent frames , number of generation steps Nsteps, number of tokens in latent frame Nf , KV cache = , generated latent list = 1: Initialize text cache: 2: Generate text causal mask AttnM ask(p) for Xp 3: 4: Sample cache mask: 5: Minf Bernoulli(1 ρinf ) 6: for = 1 do 7: 8: 9: 10: 11: 12: 13: end for Initialize all tokens in (t) Generate temporal causal mask AttnM ask(t) for {Xp, (t) Generate t-th frame (t) Cache partial observation of the generated frame: (cid:101)X (t) Gϕ( (cid:101)X (t) + (1 Minf ) [MASK] Store cache for the t-th frame , AttnM ask(t), C) Store cache for the prompt = Minf (t) Reused for all frames and append it to as [MASK] } Figure 4 Left: Training algorithm with AR-DF. For simplicity, this only includes training on videos. Right: Inference algorithm with AR-DF. more detailed version for inference is included in the Appendix. leading to degradation of temporal learning. Training scheme. To resolve this problem, we build upon the basic characteristics of videosspatial information redundancy. The core problem of imbalanced loss during training stems from spatial information leakage. It is worth noting that the original video diffusion transformer [60] that adopts diffusion forcing does not encounter this issue due to its usage of bidirectional dependency. Facing this challenge, we introduce autoregressive discrete diffusion forcing (AR-DF), technique that adopts temporal tube masking during the training of autoregressive video generation. Concretely, for every video, we randomly generate mask pattern for the first frame and then apply this mask pattern repeatedly to later frames in this video. If we denote the multi-modal token sequence composed of text tokens Xp and visual tokens Xv and sample mask ratio ρ, the training masking strategy in AR-DF can be formulated as: Mi Bernoulli(1 ρ) for = 1, . . . , Nf (7) = 1, . . . , (8) (9) (cid:9) (cid:102)X (t) = (t) + (1M ) [MASK], (cid:102)X = (cid:8)Xp, (cid:102)X (1) , . . . , (cid:102)X (T ) , (cid:102)X (2) where Nf and denote the number of tokens in one latent frame and the number of latent frames in Xv; (t) denotes the visual tokens of the t-th frame; (cid:102)X denotes the masked multi-modal token sequence prepared for training; indicates Hadamard multiplication; denotes the mask pattern; [MASK] denotes the mask token. After the preparation of the token sequence, it is fed into the model for processing. To ensure the consistency with contemporary LLMs and temporal causality in videos, we adopt temporal causal mask AttnM ask for attention processing, as shown in figure 5(b). To train the model, we use the cross-entropy loss and compute the loss only on the unmasked token, which can be denoted as L( (cid:99)X, X, ) Figure 5 (a) Details of scaled 3D positions in MM-RoPE. (b) Temporal causal mask used in Lumos-1. We acknowledge, however, that given the autoregressive generation nature of videos, this scaling may not be the optimal solution. More advanced and sophisticated solutions are left for future work."
        },
        {
            "title": "3.2 Autoregressive Discrete Diffusion Forc-",
            "content": "ing The most naive generation paradigm (i.e., next-token prediction) suffers from low generation efficiency, rendering it impractical for autoregressive visual generation. In this work, we resort to discrete diffusion [81, 63] to generate visual content, together with temporally causal dependency to enable temporally autoregressive generation paradigm. However, due to the autoregressive nature of Lumos-1, naive random masking (i.e., globally random mask) or temporally independent masking (i.e., diffusion forcing [9, 60]) both lead to significant loss imbalance, i.e., visual tokens in the later frames tend to have much lower loss. Since the task difficulty of predicting frames with ample history frame context is considerably easier than predicting the first image given text prompt or predicting the second frame given the first frame, the model would lead towards optimizing simpler tasks, 6 Table 1 Performance comparison on GenEval. In the #Params\" column, we present the parameter counts of the visual generative model and the significant language encoders for fair comparison with unified models following [95]. Tables below follow this paradigm. Model #Params #Images Overall Single Obj. Two Obj. Counting Colors Position Attr. Bind Diffusion models SD v1.5 [55] SD v2.1 [55] SD-XL [48] SD 3 [15] DALL-E 2 [50] FLUX [32] Autoregressive models LlamaGen [62] Show-o [81] Chameleon [65] Transfusion [95] EMU3 [95] LWM [39] SEED-X [18] 0.9B + 0.1B 0.9B + 0.3B 2.6B + 0.8B 8.2B + 2.8B 4.2B + 1.0B 12B + 2.5B 0.8B + 2.9B 1.3B 34B 7.3B 8B 7B 17B Lumos-1 (352 352) Lumos-1 (352 352) 1.5B 3.6B 2B 2B 650M 60M 2B 1.4B 3.5B 1B 60M 60M 0.43 0.50 0.55 0.68 0.52 0.67 0.32 0.53 0.39 0.63 0.66 0.47 0.49 0.97 0.98 0.98 0.98 0.94 0.99 0.71 0.95 0.99 0.93 0. 0.38 0.51 0.74 0.84 0.66 0.85 0.34 0.52 0.81 0.41 0.58 0.35 0.44 0.39 0.66 0.49 0.75 0.21 0.49 0.42 0.46 0.26 0.76 0.85 0.85 0.74 0.77 0.77 0.58 0.82 0.80 0.79 0. 0.04 0.07 0.15 0.40 0.10 0.22 0.07 0.11 0.49 0.09 0.19 0.06 0.17 0.23 0.43 0.19 0.42 0.04 0.28 0.45 0.15 0.14 0.601 0.664 0.959 0. 0.732 0.806 0.375 0.463 0.774 0.806 0.365 0.483 0.400 0.475 and (cid:99)X is the token sequence after model processing. The algorithm is formalized in algorithm 1. Inference scheme. After training with AR-DF, the most naive inference scheme (i.e., autoregressively generating video frames) would result in significant frame quality and motion degradation. We observe that this is caused by inconsistent inference with training. During training, later frames consistently have partial observation of history frames, while the inference stage does not align with this observation pattern. Therefore, given caption, we first generate the first frame by running multiple steps and then randomly replace pre-defined ratio ρinf of tokens with the [MASK] token for the generated image. We infer the model with this partially observed image and cache the Keys and Values of this image for swift inference. This process is repeated until the entire video is generated. The algorithm is formalized in algorithm 2."
        },
        {
            "title": "3.3 Implementation",
            "content": "The architecture of Lumos-1 follows Architecture. Llama [71, 20], meaning that it incorporates RMSNorm [92] and SwiGLU [57] by default. To stabilize training, we integrate query-key normalization (QKNorm) following Chameleon [65]. We have models of three scales (Lumos-1 0.5B, 1B and 3B), whose architectural details are placed in the Appendix. It is worth noting that we use the 0.5B version for fast ablation studies. Sequence formatting. The visual tokens and text tokens are interleaved within sequence [38], with the text tokens specifying metadata, including the text prompt, video resolution, video fps and the number of frames in this video. With this design, we train the model using images and videos of varying aspect ratios without resizing them. The details of the sequence formatting (e.g., organization of special tokens) are placed in the Appendix. GPU memory friendly implementation. By default, we leverage flash attention [13] to accelerate attention calculation and reduce memory overhead during training and inference of Lumos-1. Moreover, we observe significant amount of GPU memory consumption during the training of model with large codebook size. To address it, we eliminate the use of language-related loss (i.e., next-token prediction on texts), reducing the final logit matrix size to match only visual tokens. While text token embeddings, which map the text token index to token embeddings, remain trainable, this focuses the model on video generation. This loss can be added if we target unified model that is capable of learning within the language modality. Finally, we observe substantially high GPU memory consumption during loss computation over 129k token types, which easily leads to the out-of-memory issue. To solve this, we utilize chunked cross-entropy loss, which maintains full softmax accuracy by upcasting and calculating softmax logits one chunk of token sequences at time. This approach significantly reduces peak memory usage. By default, we set the chunk size to 2,000. Tokenizers. To unify visual and text token processing, we adopt the discrete version of Cosmos Tokenizer [1] that achieves spatiotemporal compression rates of 8 8 4. For text tokens, we retain Chameleons text tokenizer [65]. Therefore, Lumos-1s total codebook size is 129,536, partitioned into 65,536 text tokens and 64,000 visual tokens. Stage-wise training. Due to the autoregressive nature of Lumos-1, the training of video generation can be categorized into the training of two capabilities: 1) textto-image and 2) image/images to video. Although the incorporation of AR-DF training substantially ameliorates the imbalanced learning issue, we still observe that Table 2 Performance comparison on VBench-I2V benchmark. We list partial metrics due to space limits. Model #Params #Videos Total I2V Score Quality Score I2V Sub. I2V Back. Sub. Cons. Back. Cons. Img. Quality Diffusion models VideoCrafter-I2V [11] ConsistI2V [54] SEINE [12] I2VGen-XL [94] CogVideoX [83] Autoregressive models 2.6B 20M 82.57 1.3B + 0.3B 10M 84.07 0.9B + 0.1B 10M 84.88 1.4B + 1.0B 35M 85.28 86.70 5.6B + 4.8B COSMOS-Video2World [62] 5B + 11B 100M 84.16 Lumos-1 (672 384 25) Lumos-1 (672 384 25) 1.5B 3.6B 10M 84.16 10M 84. 86.31 91.91 92.39 92.11 94.79 92.51 91.80 93.34 78.84 76.22 77.37 78.44 78.61 75.81 76.53 76. 91.17 95.82 96.57 96.48 97.19 95.99 96.06 97.42 91.31 95.95 96.80 96.83 96.74 97.36 96.58 97. 97.86 95.27 94.2 94.18 94.34 97.12 95.90 97.42 98.79 98.28 97.26 97.09 96.42 96.59 96.25 96. 71.68 66.92 70.97 69.14 70.01 59.90 67.06 69.23 Table 3 Performance comparison on VBench-T2V benchmark. We list partial metrics due to space limits. Model Diffusion models #Params #Videos Total Quality Semantic Sub. Cons. Back. Cons. Img. Quality Obj. Class Color Overall Cons. ModelScopeT2V [73] InstructVideo [91] LaVie [78] OpenSoraPlan V1.3 [35] LTX-Video [22] CogVideoX [83] 1.4B + 0.3B 10M 75.75 1.4B + 0.3B 10M 76.61 2.5B + 0.5B 25M 77.08 70M 77.23 2.7B + 13B 80.00 1.9B + 4.8B 5.6B + 4.8B 81.91 Autoregressive models CogVideo [26] EMU3 [77] Lumos-1 (448 256 25) Lumos-1 (448 256 25) 9B 8B 1.5B 3.6B 5.4M 67.01 80. 10M 75.78 10M 78.32 78.05 81.56 78.78 80.14 82.30 83.05 72.06 84.09 77.54 79.52 66.54 56.81 70.31 65.62 70.79 77. 46.83 68.43 68.74 73.51 89.87 95.30 91.41 97.79 96.56 96.45 92.19 95.32 94.97 95.51 95.29 96.97 97.47 97.24 97.20 96. 96.20 97.69 96.08 96.50 58.57 68.01 61.90 56.21 60.28 63.33 41.03 54.58 58.04 82.25 73.26 91.82 85.56 83.45 85. 73.40 86.17 90.08 90.05 81.72 77.14 86.39 79.30 81.45 83.03 79.57 75.34 82.00 25.67 19.91 26.41 24.47 25.19 27. 7.70 24.71 25.29 the later task is relatively easier. Therefore, stage-wise training scheme is mandatory to ensure successful video generation training. Concretely, the training commences with dedicated text-to-image training with 256p resolution, and then we perform image-video joint training to train the video generation capability on 256p resolution. Finally, we perform joint training on visual data of 384p resolution. to 0.7 by default. The number of steps to generate one latent frame is set to Nsteps = 50 by default. When evaluating on GenEval [19] and VBench [28], the guidance scale is set to 16 if not otherwise specified. Since we train our model only on detailed and descriptive captions, it is mandatory for us to rewrite captions using Qwen 32B [82] when evaluating on GenEval [19]. When evaluating on VBench [28], we use its official long captions by default."
        },
        {
            "title": "4.1 Experimental Details",
            "content": "Datasets. To train Lumos-1, we curate image dataset containing 60 million images and video dataset containing 10 million videos. We preserve their original aspect ratios, with videos being clipped to 25 frames for training. To ensure fine-grained vision-language alignment [50, 66], the visual data is re-captioned using visual-language models [36] to obtain long and descriptive captions. Training, inference and model evaluation. We train the model from scratch and the basic training hyper-parameters are placed in the Appendix. We adopt the Adam [41] optimizer to optimize the model, set the weight decay to 0.1 and set β1 and β2 to 0.9 and 0.95. During ARDF training, the training mask ratio ρtra is set to 0.7 following [33]. During AR-DF inference, classier-free guidance (CFG) is utilized by default to enhance the generation quality. The inference mask ratio ρinf is set"
        },
        {
            "title": "4.2 Comparison with Other Methods on Vi-",
            "content": "sual Generation Text-to-image generation. We compare Lumos-1 with competitive image generation methods in Tab. 1. Compared with diffusion models, we observe that Lumos-1 outperforms models of similar sizes by margin (e.g., SDXL [48]) and is even on par with FLUX [32]. Compared with autoregressive models, we observe that Lumos-1 is on par with EMU3 [77], while being significantly more efficient due to the discrete diffusion inference. Lumos-1 achieves superior results in terms of position and attribute binding, demonstrating excellent language understanding and vision-language alignment even without textual pre-training. Image-to-video generation. Thanks to the autoregressive nature of Lumos-1, we can perform image-to-video generation by specifying the first frame, although we did not specifically train on this task. Results are listed in Tab. 2. Lumos-1 outperforms the popular VideoCrafter-I2V model and is on par with the leading 8 Figure 6 Visual comparison of Lumos-1 with other methods on text/image-to-video tasks. COSMOS-Video2World model, which uses substantially more data (100M>10M) and training resources (10000 H100s>48 H20s), demonstrating the promising performance of Lumos-1. Text-to-video generation. We list the comparison in Tab. 3. Although we utilize discrete tokenizers, Lumos-1 can still be on par with diffusion models like OpenSoraPlan even if we do not rely on heavy pretrained text encoder. Due to the autoregressive generation nature, the video quality can be ensured by first frame quality, enabling Lumos-1 to excel in object-centric metrics (object class and color)."
        },
        {
            "title": "4.3 Analysis and Ablation Studies",
            "content": "Qualitative visual comparison. We compare Lumos-1 with popular video generation methods in figure 6. For T2V, the visual quality of our 384p videos does not trail 512p videos from LTX-Video. In the provided case, Lumos-1 generates better natural motion (water waves) and aligns with prompts better (skier in red and waves). For I2V, Lumos-1 handles multiple-object (multiple floating hot air balloons in example 1) and fine-grained motion (subtle ripples around the shoreline in example 3) substantially better than Stable Video Diffusion [5] (SVD), which generates global camera movement only. In example 2, SVD produces significant blurs, whereas Lumos-1 animates subjects smoothly. More visualizations are placed in the Appendix. Effectiveness of temporal-tube masks during AR-DF training. In figure 7(a), we compare the frame-wise validation loss (frame 0, 3, 6) when using global random masks and temporal tube masks. The training mask ratios ρtra are both set to 0.7 following MAR [33]. For random masks, the loss of frame 6 decreases immediately and becomes lower than that of earlier frames. This rapid fall indiTable 4 Inference time with different RoPEs using 1B / 3B model. Results are reported on one H20 with batch size set to 1, Nsteps = 50 and CFG used by default. 1D RoPE M-RoPE MM-RoPE Image (448 256) Video (448 256 25) 7.4s / 16.3s 75.1s / 173.6s 7.7s / 16.9s 77.8s / 178.5s 7.7s / 16.9s 77.8s / 178.5s cates pronounced information leakage: the model can reconstruct masked token by attending to unmasked tokens in neighbouring frames instead of modelling genuine temporal dynamics, rendering the task overly easy. For temporal tube masks, frame 6 is the hardest because all pixels in the same spatial locations are masked across the temporal axis, eliminating the shortcut available with random masks. As iterations proceed, the gap between frames narrows and eventually levels out, demonstrating that the model is learning to propagate information through time rather than copying it. Effect of AR-DF inference masks and sensitivity to ρinf . AR-DF requires the same partial-context masking at inference as during training; omitting these masks severely harms quality. In figure 8(b), we observe that the w/o inference mask\" setting produces visible artifacts and flickering, whereas using masks preserves coherence. In figure 8(a), we select two metrics (imaging quality and dynamic degree) from VBench to quantitatively assess the impact of ρinf . broad plateau between 0.3 and 0.7 yields smooth, visually pleasing videos. When ρinf is below 0.3, insufficient context forces the model to degrade both motion and per-frame quality, thus inflating dynamic degree values. When ρinf is above 0.7, overly aggressive masking disrupts temporal continuity, driving up dynamic degree values. We empirically set ρinf to 0.7 to ensure obvious motion. 9 Figure 7 (a) Effectiveness of temporal-tube masks during AR-DF training (on videos) using the 0.5B model; (b) Effectiveness of MM-RoPE; (c) Sensitivity analysis of CFG scale on GenEval using 1B model. Figure 8 (a) Selected VBench metrics with varying inference-time mask ratio ρinf ; (b) Text-to-video visualization comparing the effect of inference-time masks. (w/o: ρinf = 0.0, w/: ρinf = 0.7) Effectiveness of MM-RoPE. figure 7(b) plots the validation loss of the 0.5B model under four RoPE settings. Note that M-RoPE means that both designs are removed. We can observe that MM-RoPE consistently converges faster and settles at the lowest loss, confirming the benefit of modelling fine-grained spatiotemporal information. Although ablating either component raises the loss, dropping the distributed design hurts more than dropping the design of scaled position, indicating that comprehensive frequency allocation is the dominant factor. Removing both enhancements gives the slowest convergence and the highest plateau, showing that the two mechanisms are complementary for effective video generation. Effect of the number of meta MM-RoPEs in MM-RoPE. MMRoPE slices the embedding channels into several meta groups. More groups mean that one kind of information (temporal, height or width) receives wider and more comprehensive frequency spectrum for modeling. figure 9(a) plots the validation loss for 0.5B model under four settings: 1) w/o distributed design: This setting uses the previous design, which allocates the first 2 channels 8 for temporal modeling and 3 channels for height and 8 width modeling, respectively. 2) #meta MM-RoPE=1 : This setting equips meta MM-RoPE of 64 channels, while maintaining the ratios (2 : 3 : 3) for modeling temporal, height and width information. This variant improves upon the previous one by enabling interleaved height and width channels, which increases the frequency spectrum range for the two spatial dimensions. 3) #meta MM-RoPE=2 : This setting equips meta MM-RoPE of 32 channels. This variant improves the frequency spectrum range for temporal, height and width information compared with the previous one. 4) #meta MM-RoPE=4 : This setting is the default design that keeps the number of channels for every meta MM-RoPE minimal (16 channels). This means that the frequency spectrum range is most comprehensive for the temporal, height, or width dimension. These results confirm that widening the frequency spectrum for every dimension through increasing the number of meta MM-RoPEs substantially improves spatiotemporal modeling and overall training effectiveness. Effect of the scaling factors in MM-RoPE. figure 9(b) presents the validation loss curves of varying the scaling factors for modeling temporal, height and width positional information in MM-RoPE. Two clear trends emerge: Moving from (1, 1, 1) (2, 4, 4) (4, 8, 8) steadily lowers the curve, but further enlarging to (8, 16, 16) or (16, 32, 32) yields no additional gain since the three curves almost overlap throughout training. Therefore, moderate scale of (4, 8, 8) is sufficient to balance vision-language ranges and fully realize the benefit of higher-resolution RoPE, while avoiding unnecessary frequency inflation. We therefore adopt (4, 8, 8) as the default scaling for MM-RoPE. Inference overhead analysis for MM-RoPE. Similar to MRoPE, MM-RoPE needs to locate the start of visual 10 Figure 9 (a) Effect of the number of meta MM-RoPEs in MM-RoPE; (b) Effect of the scaling factors in MM-RoPE. Both curves are plotted using the 0.5B model trained on videos. Table 5 GenEval score across different aspect ratios. 448 256 (7 : 4) 352 352 (1 : 1) 256 448 (4 : 7) 0. 0.605 0.569 GenEval"
        },
        {
            "title": "Acknowledgement",
            "content": "The LaTeX template is built upon Metas original template. tokens and then apply the RoPE mechanism, which requires small amount of computation. In Tab. 4, we compare the inference speed of generating images and videos using the vanilla 1D RoPE, M-RoPE and MMRoPE. We can observe that: 1) Compared with 1D RoPE, the incorporation of 3D priors introduces only 3.5%-4.1% inference latency. 2) Compared to M-RoPE, MM-RoPE introduces no additional latency. Sensitivity analysis of CFG scale. We study the impact of the guidance scale on GenEval using 1B model in figure 7(c). We find that scale values from 13 to 16 (default value) lead to decent results. Robustness to aspect ratios. Although the aspect ratio of training data is mostly 7 : 4, Tab. 5 indicates that Lumos-1 1B adapts well to visual generation with different aspect ratios due to the unified codebook design."
        },
        {
            "title": "5 Conclusion\nIn this paper, we introduce Lumos-1, a model that utilizes\nthe LLM architecture for autoregressive video genera-\ntion. We propose MM-RoPE for better spatiotemporal\ndynamics modeling and propose AR-DF for effective train-\ning and inference considering intra-frame bidirectionality\nand inter-frame temporal causality. We anticipate that\nLumos-1 represents a significant step toward building a\nfoundational unified model.",
            "content": ""
        },
        {
            "title": "Appendix",
            "content": "In this Appendix, we provide additional content organized as follows: appendix presents detailed version of the AR-DF inference algorithm. appendix presents more details concerning architectures and implementation. appendix provides more visualizations to demonstrate image and video generation capabilities. appendix presents discussions covering 1) limitations and future work (appendix D.1) and 2) potential societal impact and safeguards (appendix D.2). Detailed AR-DF Inference Algorithm In this subsection, we present the detailed version of the AR-DF inference algorithm. After the text prompt Xp is encoded once and its KV pairs are cached (Lines 13), we iterate over the latent frames (Line 6). Each frame is generated by running Nsteps iterations (Lines 922). During the frame generation process, the model predicts token distribution, samples tokens only at positions that are still masked, gathers their confidences, and sets the scores of already-revealed tokens to +. After adding Gumbel noise, we re-mask the lowest-confidence tokens. As the generation proceeds, fewer tokens are re-masked as the running steps grow and the frame becomes sharper and clearer. The partially revealed frame is then added to the cache before the next frame is generated. Once all frames are finished, they are decoded to RGB space to form the final video. More Architectural and Implementation Details Model architectures and more training details. Tab. B1 presents the architectural details of three versions of Lumos-1 and their corresponding training details, including their training batch sizes and their learning rates. For 256p joint training, we use all 60M images and 10M videos. For 384p joint training, we curate higher-quality subset from 256p training data, consisting of 8M images and 0.6M videos. Following the alternating strategy of MovieGen [49], we interleave image and video batches during joint training. Detailed sequence formatting. The visual tokens and text tokens are interleaved within sequence [38], with the text tokens specifying metadata, including the text prompt, video resolution, video fps and the number of frames in this video. With this design, we train the model using images and videos of varying aspect ratios without resizing them. The text tokens are formatted as Generate video with resolution of < video_resolution >, consisting of < video_#f rames > frames at < video_f ps > frames per second, according to the following prompt:n < text_prompt >\". The visual tokens are extracted and organized according to the following structured format: < video_start_token >, < video_duration_token >, < video_f ps_token >, < rame_tokens >,...,< video_end_token >\". < rame_tokens > contains tokens for each latent frame, which are formatted as: < image_start_token >, < h_grid_token >, < w_grid_token >, < image_content_tokens >, ..., < image_end_token >\", where < image_content_tokens > are raster-scan visual tokens with < new_line_token > inserted after each row of visual tokens. To harmonize text and visual tokens, we utilize MM-RoPE, which encodes spatiotemporal coordinates for visual tokens and global positions for text."
        },
        {
            "title": "C More Visualizations",
            "content": "Qualitative visual comparison on text-to-image generation. We compare Lumos-1 with popular text-to-image generation methods in figure C1. All models are configured to their default inference settings and use the same version of detailed prompts to ensure quality generation. We can observe that: 1) Although EMU3 is significantly larger than Lumos-1, Lumos-1 generates images that reproduces fine prompt details more faithfully, such as the dappled shadows on the bench (example 1) and the correct STOP text and the yellow color on the road sign (example 3). EMU-3 sometimes drops or warps these details and exhibits occasional anatomical distortions, such as the distorted zebra (example 5) and malformed fingers on the woman holding an apple (example 7). 2) Stable Diffusion 3 offers slightly crisper textures, which is expected from its continuous tokenizer, but it frequently ignores descriptive constraints, such as the dappled shadows (example 1) and gentle shadows (example 4), and the incorrect dress color (example 7). By contrast, Lumos-1 meets these constraints while maintaining convincing overall composition. These examples illustrate that Lumos-1 delivers strong visionlanguage alignment compared with both diffusion and autoregressive baselines, while remaining competitive in visual quality. More text-to-image visualizations. figure C2 presents more results generated by Lumos-1 using descriptive captions. Due to the unified codebook, we can train Lumos-1 using the original resolution of the visual data, enabling generation with different aspect ratios such as 4 : 7, 7 : 4, and 1 : 1, although we do not have significant amount of data with aspect ratio 4 : 7. Even though Lumos-1 is trained from scratch, it transforms lengthy descriptions into complex scenes that are aligned faithfully with the prompts. This can be exemplified by the second row of images, where these images clearly reflect the prompts: 12 Algorithm 3 AR-DF Inference Procedure Require: text prompt Xp, trained model Gϕ, inference mask ratio ρinf , number of latent frames , number of generation steps Nsteps, number of tokens in latent frame Nf , KV cache = , generated latent list = 1: Initialize text cache: 2: Generate text causal mask AttnM ask(p) for Xp 3: 4: Sample cache mask: 5: Minf Bernoulli(1 ρinf ) 6: for = 1 do 7: Store cache for the prompt Gϕ(Xp, AttnM ask(p)) Reused for all frames 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: Initialize all tokens in (t) Generate temporal causal mask AttnM ask(t) for {Xp, (t) } for = 1 Nsteps do as [MASK] (cid:0)X (t) , AttnM ask(t), C(cid:1) Gϕ sampled_ids multinomial(P , 1) sampled_ids where(cid:0)X (t) Gather(cid:0)P , sampled_ids(cid:1) SetKnownInfinity(cid:0)Q, (t) Mask the lowest confident (Q) tokens in sampled_ids with randomness: = [MASK], sampled_ids, (t) = [MASK](cid:1) Perturb with gumbel noise α cos( π ) Nsteps 2 floor(α Nf ) low_conf_pos SelectLowConfidence(cid:0)Q, (cid:1) sampled_ids[low_conf_pos] [MASK] (t) sampled_ids end for Append (t) Cache partial observation of the generated frame: to + (1 Minf ) [MASK] = Minf (t) (cid:102)X (t) Gϕ( (cid:102)X (t) , AttnM ask(t), C) 26: 27: end for 28: Decode latents to RGB space 29: return the generated video frames Predicted token distribution for all tokens (cid:1) Select confidence score of every chosen token Set + where the token was not masked Cosine schedule factor in [0,1] Number of tokens to mask in this iteration Store cache for the t-th frame Figure A1 Detailed inference algorithm with AR-DF. photo of three handbags contains the description: To the left, classic black leather handbag with structured design and gold-tone hardware catches the eye. In the center, vibrant red quilted handbag with chain strap adds pop of color and touch of luxury. On the right, casual beige canvas tote bag with simple, clean lines offers more relaxed aesthetic. photo of four vases contains the description: The first vase on the left is tall, cylindrical glass vase with slight teal tint, showcasing clear transparency and modern design. Next to it is vintage ceramic vase with an off-white base adorned with hand-painted blue floral patterns, adding touch of traditional elegance. The third vase is short, squat terracotta pot with rustic, earthy texture and visible firing marks, evoking natural, bohemian feel. Finally, sleek, metallic silver vase with geometric shape completes the set, providing contemporary contrast. photo of four dogs contains the description: To the left is golden retriever, its fur glowing warmly in the sunlight, with gentle, friendly gaze. Next to it is sleek black Labrador, its coat shining with health, head tilted slightly as if listening intently. In the center, small, energetic Jack Russell terrier bounces with excitement, its white and brown fur clearly defined. On the right, large German Shepherd sits calmly, its regal posture and dark, expressive eyes adding dignified presence to the group. These qualitative results underscore the strong visionlanguage alignment achieved by Lumos-1. More image-to-video and text-to-video visualizations. figure C3 presents more image-to-video results and figure C4 presents more text-to-video results generated by Lumos-1. The prompts span wide semantic and dynamical range, containing 1) close-up animal/human scenes (example 4, 5 in figure C4), 2) fast human and vehicle motion (example 1, 4 in figure C3 and example 1, 2 in figure C4), 3) fluid dynamics (example 1, 5 in figure C3 and example 2 in figure C4), and 4) complex multi-object collective motion (example 3 in figure C3 and example 3 in figure C4). The Table B1 Model architectural and training details. Batch sizes (*/*) are for images and videos during joint training. Learning rates (*/*) are for 256p and 384p. The 0.5B version is only for fast ablation studies. Model Lumos-1 0.5B Lumos-1 1B Lumos-1 3B #Params #Layers Hidden size #Heads Head dim Batch (256p) Batch (384p) Learning rate 0.5B 1.5B 3.6B 16 16 28 1024 2048 16 32 24 64 64 128 896 / 128 640 / 96 768 / 96 192 / 32 192 / 32 288 / 48 5e-5 / 2.5e-5 5e-5 / 2.5e-5 1e-4 / 2.5e-5 Figure C1 Visual comparison of Lumos-1 with EMU3 and Stabel Diffusion 3 (Medium) on the text-to-image task. We place the simple version of the prompts above the images for reference. Fragments of the detailed prompt are placed in parentheses, with the critical attributes highlighted in red. All images are generated using the detailed prompts for all models. results demonstrate that Lumos-1 can 1) handle videos of different aspect ratios and 2) generate single-subject and multi-subject motion that is temporally coherent and physically plausible."
        },
        {
            "title": "D Discussions",
            "content": "D.1 Limitations and Future Work Discussions We recognize that Lumos-1, as an initial endeavor in this area, comes with its limitations. Most prominently, its training corpus (60 million images and 10 million videos) is modest compared with datasets used by recent foundation models [66, 32], which usually contain billions of samples. Consequently, Lumos-1 can under-generalize in scenarios that require fine-grained human actions or highly intricate scene dynamics. Considering this, our immediate research plan therefore includes scaling along three axes: 1) Data volume and diversity expanding both image and video coverage to narrow the generalisation gap. 2) Model capacity training larger backbones while retaining the MM-RoPE and AR-DF designs. 3) Multimodal knowledge infusion initializing with strong visionlanguage models or co-training with visual understanding tasks so the generator can better ground its outputs in world knowledge. D.2 Potential Societal Impacts and Safeguards As the pioneering efforts in autoregressive visual generation, Lumos-1 can be significant step towards largescale unified model for general visual understanding and generation. Through substantially large-scale training, the obtained model could stimulate intelligence that exceeds existing models trained on unimodal data. The obtained model could serve as foundation for various downstream applications, thereby reducing the carbon and economic cost that society spends on training various specialized models. At the same time, unrestricted deployment carries nontrivial risks: the model might be used to fabricate deceptive media, produce disallowed or disturbing visuals, or amplify harmful biases present in the training data. To mitigate these concerns, we recommend the following safeguards before any real-world release: 1) Alignment tuning - apply preferenceor instruction-tuning so that outputs respect human aesthetic and moral prefer14 Figure C2 More text-to-image results generated by Lumos-1. All images are generated using detailed prompts, but we only place short prompts here due to space limits. The resolutions for three rows of images are 384 672, 672 384 and 512 512. ences [91]. 2) Dual-stage filtering i) prompt filters that refuse generation requests involving illicit content, and ii) post-generation classifiers that block or watermark unsafe outputs. 3) Red-team evaluation continuous adversarial testing to uncover failure modes in new domains or under distribution shift. Lumos-1 is research-oriented work, and any broader deployment should proceed only with thorough oversight and evaluation to ensure responsible and ethical use."
        },
        {
            "title": "References",
            "content": "[1] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025. [2] Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Baptiste Bout, Devendra Chaplot, Jessica Chudnovsky, Diogo Costa, Baudouin De Monicault, Saurabh Garg, Theophile Gervet, et al. Pixtral 12b. arXiv preprint arXiv:2410.07073, 2024. [3] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. [4] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Improving Zhuang, Joyce Lee, Yufei Guo, et al. image generation with better captions. Computer Science, 2023. [5] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [7] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023. [8] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1131511325, 2022. 15 Figure C3 More image-to-video results generated by Lumos-1. All images are generated using detailed prompts, but we only place short prompts here due to space limits. The results encompass videos of resolution 672 384 and 768 320. [9] Boyuan Chen, Diego Marti Monso, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets fullsequence diffusion. arXiv preprint arXiv:2407.01392, 2024. [15] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for highresolution image synthesis. 2024. [10] Dengsheng Chen, Jie Hu, Tiezhu Yue, and Xiaoming Wei. High-resolution image synthesis via next-token prediction. arXiv preprint arXiv:2411.14808, 2024. [11] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, Videocrafter1: Open diffusion models et al. for high-quality video generation. arXiv preprint arXiv:2310.19512, 2023. [12] Xinyuan Chen, Yaohui Wang, Lingjun Zhang, Shaobin Zhuang, Xin Ma, Jiashuo Yu, Yali Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Seine: Short-tolong video diffusion model for generative transition and prediction. In The Twelfth International Conference on Learning Representations, 2023. [13] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. [14] Haoge Deng, Ting Pan, Haiwen Diao, Zhengxiong Luo, Yufeng Cui, Huchuan Lu, Shiguang Shan, Yonggang Qi, and Xinlong Wang. Autoregressive video generation without vector quantization. arXiv preprint arXiv:2412.14169, 2024. [16] Lijie Fan, Tianhong Li, Siyang Qin, Yuanzhen Li, Chen Sun, Michael Rubinstein, Deqing Sun, Kaiming He, and Yonglong Tian. Fluid: Scaling autoregressive text-to-image generative models with continuous tokens. arXiv preprint arXiv:2410.13863, 2024. [17] Mingze Gao, Jingyu Liu, Mingda Li, Jiangtao Xie, Qingbin Liu, Bo Zhao, Xi Chen, and Hui Xiong. Tcllava: Rethinking the transfer from image to video understanding with temporal considerations. arXiv preprint arXiv:2409.03206, 2024. [18] Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x: Multimodal models with unified multigranularity comprehension and generation. arXiv preprint arXiv:2404.14396, 2024. [19] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36:52132 52152, 2023. [20] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad 16 Figure C4 More text-to-video results generated by Lumos-1. All images are generated using detailed prompts, but we only place short prompts here due to space limits. The resolution is 672 384. Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [21] Yuchao Gu, Weijia Mao, and Mike Zheng Shou. Longcontext autoregressive video modeling with nextframe prediction. arXiv preprint arXiv:2503.19325, 2025. [22] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, et al. Ltx-video: Realtime video latent diffusion. arXiv preprint arXiv:2501.00103, 2024. [23] Yefei He, Yuanyu He, Shaoxuan He, Feng Chen, Hong Zhou, Kaipeng Zhang, and Bohan Zhuang. Neighboring autoregressive modeling for efficient visual generation. arXiv preprint arXiv:2503.10696, 2025. [24] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:68406851, 2020. [25] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. arXiv preprint arXiv:2204.03458, 2022. [26] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. [27] Zhihao Huang, Xi Qiu, Yukuo Ma, Yifu Zhou, Chi Zhang, and Xuelong Li. Nfig: Autoregressive image generation with next-frequency prediction. arXiv preprint arXiv:2503.07076, 2025. [28] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. [29] Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, and Zhouchen Lin. Pyramidal flow matching for efficient video generative modeling. arXiv preprint arXiv:2410.05954, 2024. [30] Dan Kondratyuk, Lijun Yu, Xiuye Gu, José Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, et al. Videopoet: large language model for zero-shot video generation. arXiv preprint arXiv:2312.14125, 2023. [31] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 17 [32] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. [33] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. arXiv preprint arXiv:2406.11838, 2024. [34] Zongyi Li, Shujie Hu, Shujie Liu, Long Zhou, Jeongsoo Choi, Lingwei Meng, Xun Guo, Jinyu Li, Hefei Ling, and Furu Wei. Arlon: Boosting diffusion transformers with autoregressive models for long video generation. arXiv preprint arXiv:2410.20502, 2024. [35] Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Liuhan Chen, et al. Open-sora plan: Open-source large video generation model. arXiv preprint arXiv:2412.00131, 2024. [36] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pretraining for visual language models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2668926699, 2024. [37] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseekv3 technical report. arXiv preprint arXiv:2412.19437, 2024. [38] Dongyang Liu, Shitian Zhao, Le Zhuo, Weifeng Lin, Yu Qiao, Hongsheng Li, and Peng Gao. Luminamgpt: Illuminate flexible photorealistic text-toimage generation with multimodal generative pretraining. arXiv preprint arXiv:2408.02657, 2024. [39] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with blockwise ringattention. arXiv preprint arXiv:2402.08268, 2024. [40] Haozhe Liu, Shikun Liu, Zijian Zhou, Mengmeng Xu, Yanping Xie, Xiao Han, Juan Pérez, Ding Liu, Kumara Kahatapitiya, Menglin Jia, et al. Mardini: Masked autoregressive diffusion for video generation at scale. arXiv preprint arXiv:2410.20280, 2024. [41] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2018. [42] Zhuoyan Luo, Fengyuan Shi, Yixiao Ge, Yujiu Yang, Limin Wang, and Ying Shan. Openmagvit2: An open-source project toward democratizing auto-regressive visual generation. arXiv preprint arXiv:2409.04410, 2024. [43] Xu Ma, Peize Sun, Haoyu Ma, Hao Tang, ChihYao Ma, Jialiang Wang, Kunpeng Li, Xiaoliang Dai, Yujun Shi, Xuan Ju, et al. Token-shuffle: Towards high-resolution image generation with autoregressive models. arXiv preprint arXiv:2504.17789, 2025. Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:2773027744, 2022. [46] Yatian Pang, Peng Jin, Shuo Yang, Bin Lin, Bin Zhu, Zhenyu Tang, Liuhan Chen, Francis EH Tay, SerNam Lim, Harry Yang, et al. Next patch prediction for autoregressive visual generation. arXiv preprint arXiv:2412.15321, 2024. [47] William Peebles and Saining Xie. Scalable diffusion In Proceedings of the models with transformers. IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. [48] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. In ICLR, 2024. [49] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. [50] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical textconditional image generation with CLIP latents. arXiv preprint arXiv:2204.06125, 2022. [51] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 88218831. Pmlr, 2021. [52] Sucheng Ren, Qihang Yu, Ju He, Xiaohui Shen, Alan Yuille, and Liang-Chieh Chen. Beyond nexttoken: Next-x prediction for autoregressive visual generation. arXiv preprint arXiv:2502.20388, 2025. [53] Sucheng Ren, Yaodong Yu, Nataniel Ruiz, Feng Wang, Alan Yuille, and Cihang Xie. M-var: Decoupled scale-wise autoregressive modeling for high-quality image generation. arXiv preprint arXiv:2411.10433, 2024. [54] Weiming Ren, Huan Yang, Ge Zhang, Cong Wei, Xinrun Du, Wenhao Huang, and Wenhu Chen. Consisti2v: Enhancing visual consistency for image-tovideo generation. arXiv preprint arXiv:2402.04324, 2024. [55] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1068410695, 2022. [56] Sand-AI. Magi-1: Autoregressive video generation [44] OpenAI. GPT-4 technical report, 2023. at scale, 2025. [45] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, [57] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. 18 [58] Fengyuan Shi, Zhuoyan Luo, Yixiao Ge, Yujiu Yang, Ying Shan, and Limin Wang. Taming scalable visual tokenizer for autoregressive image generation. arXiv preprint arXiv:2412.02692, 2024. [59] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. [60] Kiwhan Song, Boyuan Chen, Max Simchowitz, Yilun Du, Russ Tedrake, and Vincent Sitzmann. History-guided video diffusion. arXiv preprint arXiv:2502.06764, 2025. [61] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [62] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. [63] Alexander Swerdlow, Mihir Prabhudesai, Siddharth Gandhi, Deepak Pathak, and Katerina Fragkiadaki. Unified multimodal discrete diffusion. arXiv preprint arXiv:2503.20853, 2025. [64] Anni Tang, Tianyu He, Junliang Guo, Xinle Cheng, Li Song, and Jiang Bian. Vidtok: versatile and open-source video tokenizer. arXiv preprint arXiv:2412.13061, 2024. [65] Chameleon Team. early-fusion foundation models. arXiv:2405.09818, 2024. Chameleon: Mixed-modal arXiv preprint [66] Wan Team. Wan: Open and advanced large-scale video generative models. 2025. [67] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. arXiv preprint arXiv:2404.02905, 2024. [68] Shengbang Tong, David Fan, Jiachen Zhu, Yunyang Xiong, Xinlei Chen, Koustuv Sinha, Michael Rabbat, Yann LeCun, Saining Xie, and Zhuang Liu. Metamorph: Multimodal understanding and generation via instruction tuning. arXiv preprint arXiv:2412.14164, 2024. [69] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient learners for self-supervised video pretraining. Advances in neural information processing systems, 35:1007810093, 2022. [70] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023. 19 [71] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [72] Ruben Villegas, Mohammad Babaeizadeh, PieterJan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video generation from open domain textual descriptions. In International Conference on Learning Representations, 2022. [73] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. [74] Junke Wang, Yi Jiang, Zehuan Yuan, Binyue Peng, Zuxuan Wu, and Yu-Gang Jiang. Omnitokenizer: joint image-video tokenizer for visual generation. arXiv preprint arXiv:2406.09399, 2024. [75] Junke Wang, Zhi Tian, Xun Wang, Xinyu Zhang, Weilin Huang, Zuxuan Wu, and Yu-Gang Jiang. Simplear: Pushing the frontier of autoregressive visual generation through pretraining, sft, and rl. arXiv preprint arXiv:2504.11455, 2025. [76] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [77] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. [78] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. arXiv preprint arXiv:2309.15103, 2023. [79] Yuqing Wang, Tianwei Xiong, Daquan Zhou, Zhijie Lin, Yang Zhao, Bingyi Kang, Jiashi Feng, and Xihui Liu. Loong: Generating minute-level long videos with autoregressive language models. arXiv preprint arXiv:2410.02757, 2024. [80] Xilin Wei, Xiaoran Liu, Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Jian Tong, Haodong Duan, Qipeng Guo, Jiaqi Wang, et al. Videorope: What makes for good video rotary position embedding? arXiv preprint arXiv:2502.05173, 2025. [81] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. [93] Qinsheng Zhang, Molei Tao, and Yongxin Chen. gddim: Generalized denoising diffusion implicit models. arXiv preprint arXiv:2206.05564, 2022. [94] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and Jingren Zhou. I2vgen-xl: High-quality image-to-video synthesis via cascaded diffusion models. arXiv preprint arXiv:2311.04145, 2023. [95] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. [96] Deyu Zhou, Quan Sun, Yuang Peng, Kun Yan, Runpei Dong, Duomin Wang, Zheng Ge, Nan Duan, Xiangyu Zhang, Lionel Ni, et al. Taming teacher forcing for masked autoregressive video generation. arXiv preprint arXiv:2501.12389, 2025. [97] Xianwei Zhuang, Yuxin Xie, Yufan Deng, Liming Liang, Jinghan Ru, Yuguo Yin, and Yuexian Zou. Vargpt: Unified understanding and generation in visual autoregressive multimodal large language model. arXiv preprint arXiv:2501.12327, 2025. [82] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [83] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. [84] Tianwei Yin, Qiang Zhang, Richard Zhang, William Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From slow bidirectional to fast autoregressive video diffusion models. arXiv preprint arXiv:2412.07772, 2, 2024. [85] Hu Yu, Biao Gong, Hangjie Yuan, DanDan Zheng, Weilong Chai, Jingdong Chen, Kecheng Zheng, and Feng Zhao. Videomar: Autoregressive video generatio with continuous tokens. arXiv preprint arXiv:2506.14168, 2025. [86] Hu Yu, Hao Luo, Hangjie Yuan, Yu Rong, and Feng Zhao. Frequency autoregressive image generation with continuous tokens. arXiv preprint arXiv:2503.05305, 2025. [87] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Scaling autoregressive models for Ayan, et al. content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022. [88] Lijun Yu, Yong Cheng, Kihyuk Sohn, José Lezama, Han Zhang, Huiwen Chang, Alexander Hauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: Masked generative video transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1045910469, 2023. [89] Lijun Yu, José Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al. Language model beats diffusiontokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. [90] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and Liang-Chieh Chen. Randomized autoregressive visual generation. arXiv preprint arXiv:2411.00776, 2024. [91] Hangjie Yuan, Shiwei Zhang, Xiang Wang, Yujie Wei, Tao Feng, Yining Pan, Yingya Zhang, Ziwei Liu, Samuel Albanie, and Dong Ni. Instructvideo: Instructing video diffusion models with human feedback. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 64636474, 2024. [92] Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019."
        }
    ],
    "affiliations": [
        "DAMO Academy, Alibaba Group",
        "Hupan Lab",
        "Tsinghua University",
        "Zhejiang University"
    ]
}