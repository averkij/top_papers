{
    "paper_title": "SPARC: Separating Perception And Reasoning Circuits for Test-time Scaling of VLMs",
    "authors": [
        "Niccolo Avogaro",
        "Nayanika Debnath",
        "Li Mi",
        "Thomas Frick",
        "Junling Wang",
        "Zexue He",
        "Hang Hua",
        "Konrad Schindler",
        "Mattia Rigotti"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite recent successes, test-time scaling - i.e., dynamically expanding the token budget during inference as needed - remains brittle for vision-language models (VLMs): unstructured chains-of-thought about images entangle perception and reasoning, leading to long, disorganized contexts where small perceptual mistakes may cascade into completely wrong answers. Moreover, expensive reinforcement learning with hand-crafted rewards is required to achieve good performance. Here, we introduce SPARC (Separating Perception And Reasoning Circuits), a modular framework that explicitly decouples visual perception from reasoning. Inspired by sequential sensory-to-cognitive processing in the brain, SPARC implements a two-stage pipeline where the model first performs explicit visual search to localize question-relevant regions, then conditions its reasoning on those regions to produce the final answer. This separation enables independent test-time scaling with asymmetric compute allocation (e.g., prioritizing perceptual processing under distribution shift), supports selective optimization (e.g., improving the perceptual stage alone when it is the bottleneck for end-to-end performance), and accommodates compressed contexts by running global search at lower image resolutions and allocating high-resolution processing only to selected regions, thereby reducing total visual tokens count and compute. Across challenging visual reasoning benchmarks, SPARC outperforms monolithic baselines and strong visual-grounding approaches. For instance, SPARC improves the accuracy of Qwen3VL-4B on the $V^*$ VQA benchmark by 6.7 percentage points, and it surpasses \"thinking with images\" by 4.6 points on a challenging OOD task despite requiring a 200$\\times$ lower token budget."
        },
        {
            "title": "Start",
            "content": "SPARC: Separating Perception And Reasoning Circuits for Test-time Scaling of VLMs Niccolo Avogaro * 1 2 Nayanika Debnath * 1 2 Li Mi 1 Thomas Frick 2 Junling Wang 1 Zexue He 3 Hang Hua 3 Konrad Schindler 1 Mattia Rigotti 2 6 2 0 2 6 ] . [ 1 6 6 5 6 0 . 2 0 6 2 : r Abstract Despite recent successes, test-time scalingi.e., dynamically expanding the token budget during inference as neededremains brittle for visionlanguage models (VLMs): unstructured chainsof-thought about images entangle perception and reasoning, leading to long, disorganized contexts where small perceptual mistakes may cascade into completely wrong answers. Moreover, expensive reinforcement learning with hand-crafted rewards is required to achieve good performance. Here, we introduce SPARC (Separating Perception And Reasoning Circuits), modular framework that explicitly decouples visual perception from reasoning. Inspired by sequential sensory-to-cognitive processing in the brain, SPARC implements two-stage pipeline where the model first performs explicit visual search to localize question-relevant regions, then conditions its reasoning on those regions to produce the final answer. This separation enables independent test-time scaling with asymmetric compute allocation (e.g., prioritizing perceptual processing under distribution shift), supports selective optimization (e.g., improving the perceptual stage alone when it is the bottleneck for end-to-end performance), and accommodates compressed contexts by running global search at lower image resolutions and allocating high-resolution processing only to selected regions, thereby reducing total visual tokens count and compute. Across challenging visual reasoning benchmarks, SPARC outperforms monolithic baselines and strong visual-grounding approaches. For instance, SPARC improves the accuracy of Qwen3VL-4B on the VQA benchmark by 6.7 percentage points, and it surpasses thinking with images by 4.6 points on challenging OOD task despite requiring 200 lower token budget. 1ETH Zurich 2IBM Research 3MIT-IBM Watson AI Lab. Correspondence to: Niccolo Avogaro <niccolo.avogaro1@ibm.com>. Preprint. February 9, 2026. 1 1. Introduction Multimodal Vision Language Models (VLMs) have become the de-facto standard in visual reasoning and perception (Li et al., 2025). VLMs are architectures that combine visual and textual inputs. By aligning vision backbone with an LLM (Huang et al., 2023), they extend the impressive NLP capabilities of LLMs to the vision realm (Alayrac et al., 2022; Chen et al., 2023b; Hua et al., 2025; Li et al., 2023; Chen et al., 2023a; Liu et al., 2023a; Zhu et al., 2023; Peng et al., 2023; Achiam et al., 2024; Karlinsky et al., 2025). Among the capabilities that VLMs inherit from LLMs is Chain-of-Thought (CoT) reasoning (Wei et al., 2023), test-time compute mechanism to iteratively generate the output step-by-step, which can be optimized via Reinforcement Learning and has been popularized by models like ChatGPT-o1 (OpenAI, 2024) and DeepSeek-R1 (Guo et al., 2025). Works like ViGoRL (Sarch et al., 2025) and DeepEyes (Zheng et al., 2025) have demonstrated that multimodal chain-of-thought reasoning, obtained by interleaving pure text CoT reasoning with image content, can be explicitly grounded to the relevant visual evidence in the image via multi-turn workflow that calls appropriate image analysis tools. In this so-called thinking with images paradigm firstly introduced in the OpenAI ChatGPT-o3 report (OpenAI Research, 2025), the model alternates between reasoning steps and perceptual actions (like selecting region of interest in the image). Such grounded multi-modal CoTs can yield significantly better performance in visual reasoning tasks, especially when it comes to high-resolution perception where one must repeatedly focus attention on small but decisive image details. core issue of thinking with images, and multi-modal CoT reasoning in general, is that learning is lot more complex than for standard, text-only reasoning: the LLM must acquire the ability to manage multi-turn conversations and tool calls that repeatedly mix visual and reasoning tokens within the context window (Sarch et al., 2025; Wang et al., 2025b; Zheng et al., 2025; Kumar et al., 2025). This is not only computationally expensive, but also more brittle, particularly for smaller models whose performance rapidly degrades when faced with visually heavy contexts (and Submission and Formatting Instructions for ICML 2026 Figure 1. Overview of the SPARC framework. We decouple the VLM inference process into two distinct functional circuits. Stage 1 (Perception): The What and Where Circuits perform Implicit Relevance Detection (IRD), taking the image and question as input to output relevant crop coordinates (e.g., localizing the womans ear). Stage 2 (Reasoning): The Prefrontal Cortex Circuit synthesizes CoT by reasoning over the high-resolution crops identified in the first stage and outputs the final answer (blue). This separation enables independent optimization and robust, efficient test-time scaling. therefore long token sequences). Furthermore, monolithic approach is inflexible and lacks mechanism to adapt the allocated compute to the difficulty of the vision task: when to terminate the response is left to the LLM. Here, we propose new, more efficient test-time scaling strategy for VLMs, motivated by context-engineering principles (Mei et al., 2025) that maintain that operating CoTs in unstructured fashion (in our case, entangling perception and reasoning tokens), hinders effective organization of the context and can thereby impair task performance. Our architecture draws inspiration from systems and visual neuroscience, and specifically the biological brains hierarchical information processing architecture, where early visual areas first extract low-level features that are elaborated through parallel what and where visual pathways (Mishkin et al., 1983; Kravitz et al., 2011). This information then converges and is mixed in high-dimensional codes in prefrontal cortex (Rigotti et al., 2013; Tye et al., 2024), the cortical area viewed as responsible for integrating sensory inputs and contextual information, and supporting the implementation of our flexible goal-oriented thoughts, learning and behavior (Miller & Cohen, 2001; Sung et al., 2025). Based on this view, we propose two-stage pipeline as illustrated in Figure 1. Given visual question and its associated answer, we do not directly prompt the model to return the answer, but rather ask it to find the relevant image content. Image crops detected by this Implicit Relevance Detection (IRD) are then used to re-prompt the model for an answer to the actual question, given the relevant image regions. That prompting strategy, by itself, turns out to lead to better results than native thinking with images; moreover, we show that it has number of interesting properties. First, when using the two-step pipeline with an efficient initial IRD step it becomes possible to scale perception at test time independently from reasoning. As an example, employing self-consistency over eight roll-outs of the IRD step, with shared KV -cache, creates only few additional text tokens and an additional crop, but boosts performance of the full pipeline by up to 9.3%. Second, the two steps can be trained separately, without forgetting the models generic, pretrained capabilities. For instance, when training for the particular perception needs of some technical domain, there is no danger of losing the ability to reason and produce CoTs. Third, dedicated training for the IRD task is extremely efficient in terms of both data and training time, because one needs to rollout only small number of tokens for the crop coordinates instead of going through long multi-modal multi-turn reasoning chains in every iteration. In summary, our contributions are: We introduce SPARC, an effective prompting scheme that enables reliable test-time scaling of perception tasks, in zero-shot mode and with very small computational overhead. We show that SPARC enables asymmetric compute allocation between perception and reasoning, allowing targeted self-consistency mechanism that scales more favorably than naive ensembling. We demonstrate that decoupling visual reasoning into separate perception and reasoning stages enables efficient training of the perception model without degrading the reasoning models original capabilities. 2. Related Work Vision-Language Models and Grounding. Recent advancements in Vision-Language Models (VLMs) have primarily focused on extending Large Language Models 2 Submission and Formatting Instructions for ICML (LLMs) with visual perception capabilities, typically via visual encoder (e.g., CLIP (Radford et al., 2021), SigLIP (Zhai et al., 2023)) connected to the LLM backbone through projection layer (Liu et al., 2023b; Li et al., 2024; Bai et al., 2023). Modern architectures like LLaVA-OneVision (An et al., 2025), MM1 (McKinzie et al., 2024), and Qwen3VL (Bai et al., 2025) have scaled this paradigm, improving resolution handling and multi-image reasoning. While most VLMs output text only, critical evolution is the integration of fine-grained visual grounding, enabling models to output spatial coordinates (bounding boxes or points) alongside text. Models such as Kosmos-2 (Peng et al., 2023), Qwen3VL, and PaliGemma (Beyer et al., 2024) natively support this grounding capability, treating coordinates as text or special tokens. More recent works like GLaMM (Rasheed et al., 2024), DeepSeek-VL2 (Wei et al., 2025), and Molmo2 (Clark et al., 2026) further refine this by interleaving segmentation or point-based grounding with reasoning, establishing strong paradigm where precise spatial localization is intrinsic to the generation process (Cho et al., 2025; Wang et al., 2025c), achieving surprising level of performance (Avogaro et al., 2025), sometimes even emerging from attention maps (Zhang et al., 2025), even though having limits when it comes to extremely fine-grained localization (Zhang et al., 2024). Test-Time Scaling of Large Language Models. The paradigm of test-time scalingallowing autoregressive models to generate additional intermediate tokens before outputting final answerhas emerged as powerful, training-free method for enhancing performance. Foundational techniques such as Chain-of-Thought (CoT) (Wei et al., 2023) and Self-Consistency (Wang et al., 2023) demonstrated that linear reasoning traces significantly improve problem-solving capabilities. Subsequent works expanded this into non-linear structures, such as Tree of Thoughts (ToT) (Yao et al., 2023) and Graph of Thoughts (GoT) (Besta et al., 2024), which enable more deliberate exploration of the solution space. More recently, this extensive reasoning capability has been baked directly into models via strong post-training techniques. Systems like OpenAI o1 (OpenAI, 2024) and DeepSeek-R1 (Guo et al., 2025) utilize Reinforcement Learning (RL) with sparse rewards to encourage the model to autonomously verify and refine its internal chain of thought. Furthermore, recent studies indicate that such sophisticated reasoning patterns can also be induced in training-free manner, for instance through training-free Group Relative Policy Optimization (GRPO) (Cai et al., 2025) or by eliciting reasoning via external cognitive tools (Ebouky et al., 2025). Test-time Scaling of Vision-Language Models. growing body of work adapts R1-style test-time scaling to VLMs through thinking with images (Su et al., 2025), which interleaves and entangles intermediate visual operations (e.g., zooming, cropping, pointing) with textual CoTs. This paradigm has been mostly developed through reinforcement learning frameworks that incentivize explicit visual reasoning. For instance, Point-RFT (Ni et al., 2025) and ViGoRL (Sarch et al., 2025) utilize reinforcement fine-tuning to align reasoning traces with precise grounded spatial references. Similarly, DeepEyes (Zheng et al., 2025) and Pixel Reasoner (Wang et al., 2025b) employ curiosity-driven or specific reward structures to encourage models to actively query visual data during the reasoning process, with some work such as (Kumar et al., 2025) mostly focusing on efficiency. Beyond static images, Video-R4 (Tang et al., 2025) extends this concept to video understanding. 3. Test-time scaling of perception Detailed visual perception is prerequisite for wide array of applications, ranging from document understanding and outdoor robotics to satellite image analysis. Test-time scaling has emerged as the primary paradigm to boost the performance of both LLMs and VLMs. Allowing the model to generate additional tokens prior to the final answer during inference enables it to reference broader range of contextual information during reasoning. That mechanism has been shown to consistently enhance both predictive accuracy and robustness (Wei et al., 2023; OpenAI, 2024; Guo et al., 2025). In the present work, we focus on the perceptual abilities of VLMs. Our goal is to enhance the models handling of task-relevant visual input at test time in an efficient and robust manner. Intuitively, more detailed image understanding necessitates richer visual representation. We posit that this translates directly to an increased number of image tokens at test time. This intuition aligns with the recently popular thinking with images approach to VLM test-time scaling. In that framework, models generate extended reasoning traces that are interleaved with zoom-in actions, i.e., the model invokes tool to retrieve relevant image crops in higher resolution. While we agree with the high-level concept of zooming into image locations that may be important for the task, we argue that for tasks that are primarily perceptual, lengthy intermediate text generation and complex multi-turn handling are superfluous and can even be counter-productive. The open-form traces produced by such procedure bring little benefit for the actual perception task, on the contrary, they contradict context engineering principles that suggest to employ structured and modular composition (Mei et al., 2025) and can promote excessively long traces resulting in hallucinations (Diao et al., 2026). Instead, we propose shift towards visual context engineering: our hypothesis is that well-structured context that contains only the necessary high-resolution image content offers more compact and robust perceptual representation than long, unorganized 3 Submission and Formatting Instructions for ICML 2026 distance relative to the true location. By progressively reducing from the half-diagonal of the bounding box down to zero, we generate set of images that partially overlap the ground truth crop, and use those figures as visual prompts. Figure 2 illustrates the models VQA accuracy as function of the overlap. We repeat the experiment with varying token budget by resizing images to smaller resolutions (longer side 256, respectively 512 pixels, keeping the original aspect ratio). The purpose of the latter comparison is to verify whether more accurate localization can compensate lower resolution. 3.2. Findings We observe that supplying the model with increasingly precise crops drives performance up towards the theoretical upper bound. In other words, if one had an oracle to guide the selection of the bounding box within the image, then that would be sufficient to unlock the models existing reasoning ability. Furthermore, our results point to an important efficiency trade-off: if model operating at 256 pixel resolution achieves even modest localization accuracy (20% overlap with the ground truth), it already surpasses 512-pixel model without object localization, at fraction of the computational cost. This effect is most pronounced in extreme low-resolution regimes. In other words, sufficient performance is achieved if small crops are positioned accurately, whereas high resolution over larger context does not seem to bring much benefit. Together, the empirical findings suggest an inference scheme in which perceptual computations are offloaded to specialized modules: on the one hand, perception is unsurprisingly important for visual reasoning, but on the other hand, it can evidently run as fairly independent low-level process, thus minimizing the burden on the reasoning backbone. We note this layout mirrors the layout of biological brains, where dedicated perceptual stream (the occipital lobe) processes visual stimuli, whereas high-level reasoning about the visual input is left to separate region (the prefrontal cortex). 4. Two-stage Architecture: decoupling"
        },
        {
            "title": "Perception and Reasoning",
            "content": "Building on the neuroscience motivation that perception and reasoning are distinct cognitive faculties, we propose decoupling these processes in VLMs. To operationalize this, we implement sequential two-step prompting protocol. In the first phase (Relevance Detection), the model acts as perceptual circuit, strictly tasked with localizing salient image regions. In the second phase (Perceptual Reasoning), the model serves as reasoning circuit, generating the final answer conditioned on the extracted regions. Employing sequential prompting stages employs the context engineering best practice of managing the context window Question: What is the color of the scarf? Answer: The color of the scarf is green. Figure 2. The plot shows downstream reasoning accuracy against the crop overlap ratio. While performance generally degrades as overlap decreases, this effect is most pronounced for lower resolutions. Crucially, at high overlap ratios, the 256px model converges to the performance of the full-resolution model. This demonstrates that accurate perceptual guidance can fully compensate for the loss of global visual detail, allowing for highly efficient inference. multi-modal roll-outs. 3.1. Experimental Setup In order to validate the intuition that well-structured prompt is enough to solve hard perception tasks, we run experiments on the benchmark (Wu & Xie, 2023), standard testbed for thinking with images. Featuring highresolution images that contain small objects, requires model to find and inspect objects and to resolve complex spatial relationships. We select that benchmark because its main difficulty is detailed visual perception: the hardest task is to locate small objects and look at them at sufficient resolution. Once one has zoomed in on the relevant visual region, providing the correct answer is straightforward. For our investigation, we prompt VLM to solve the VQA task on the benchmark. As the benchmark comes with ground truth image locations, we modify the input prompts to include image crops with varying degrees of misalignment to the ground truth and measure the impact of that misalignment on VQA performance. Results for the off-theshelf version of Qwen3VL-4B (Bai et al., 2025) are shown in Figure 2. See Appendix A.1 for experimental results conducted with Molmo2 (Clark et al., 2026). We systematically modulate the usefulness of the crops through controlled spatial perturbations. For each target object, we generate crop with the same height and width as the ground truth bounding box, but shift its center by 4 Submission and Formatting Instructions for ICML 2026 via structured, modular composition (Mei et al., 2025), and steers the model towards sampling from two distinct output distributions (Xie et al., 2022; Min et al., 2022), effectively activating separate functional circuits for visual search and logical deduction, rather than entangling them. Separating distinct relevant detection and reasoning operations intuitively enables the model to focus on addressing each specific demand. The relevance detection step, for instance, demands that the model localize pertinent image regions based on specific query. Unlike standard Referring Expression Comprehension (REC) (Mao et al., 2016; Yu et al., 2016), where the target is explicitly named (e.g., find the red ball), this objective requires the model to infer latent visual relevance from high-level reasoning prompt. Consequently, defining an optimal crop becomes an ill-posed problem: strictly speaking, the ideal crop is not necessarily the tightest bounding box around an object, but rather the visual window that maximizes the probability of correct prediction in the subsequent reasoning step. While some queries demand precise object detection, others benefit from looser crops that preserve contextual relationshipsa phenomenon we analyze in Appendix A.2. Given these distinct requirements, we term this task Implicit Relevance Detection (IRD). Table 1. In-domain (ID) and Out-of-domain (OOD) average performance of SPARC. The ID metric is computed as the mean over , HRBench-4K and HRBench-8K, while OOD is computed as the average over the XLRS remote sensing benchmark. ID Average OOD Average Method 256 512 Full 256 512 Full Qwen3VL 4B Native Performance 41.7 48.8 72.6 46.2 48.4 53.5 Thinking w/ images 36.8 52.2 73.1 43.1 48.3 48.3 SPARC (Ours) 51.0 60.6 74.8 48.7 52.9 54.8 Qwen3VL 8B Native Performance 41.4 49.4 79.0 47.5 47.9 53.2 Thinking w/ images 40.9 56.5 78.1 44.0 48.0 50.1 SPARC (Ours) 45.4 54.8 79.5 52.4 53.3 56.0 Molmo2 4B Native Performance 48.1 53.2 60.8 38.1 39.1 39.9 Thinking w/ images SPARC (Ours) 48.7 57.0 62.9 40.8 42.1 43.2 Molmo2 8B 45.8 52.4 57.8 37.5 39.4 39.1 Native Performance Thinking w/ images SPARC (Ours) 47.4 55.0 59.1 39.1 39.9 39. 4.1. Experimental Setup We evaluate our method across two distinct model families, selected to represent two different spatial grounding modalities: Qwen3-VL, which performs relevance detection via bounding box generation (Bai et al., 2025), and Molmo2, 5 which uses point-based detection (Clark et al., 2026). For Molmo2, squared 256 256 crop resolution is extracted from the point. We evaluate at different token budgets: full resolution, and downsized at 512 and 256 longest image size. In this case, the crops are taken from the original full-size image and downsized accordingly if exceeding the resizing size, in order to avoid hacking. The evaluation is conducted at 4B and 8B sizes. We employ two-step prompting technique. We first instruct the model to strictly output the coordinates (or points) of image regions relevant to the query. In the second step, we re-prompt the model with the image content and append the newly generated crops from the first step. We present our comprehensive quantitative analysis in Table 1. This evaluation aggregates performance across highfidelity perception benchmarks, specifically and the highresolution suites HRBench-4k and HRBench-8k (Wang et al., 2024) with in-domain (ID) settings. Additionally, we report robustness results on the XLRS remote sensing suite (Wang et al., 2025a), with extremely large and highresolution remote sensing images. We treat XLRS as an out-of-distribution (OOD) proxy, given that remote sensing imagery is scarce in standard instruction-tuning corpora, typically dominated by documents, UIs, and natural images. Consequently, this benchmark offers challenging evaluation of the models ability to generalize its cropping mechanism to non-standard visual domains. We employ greedy decoding to ensure deterministic evaluation and minimize variance, particularly given the multiple-choice nature of the datasets. We benchmark our decoupled method against the state-of-the-art thinking with images paradigm, which is natively supported by the Qwen3-VL architecture. In this case, we use the off-the-shelf generation parameters. The full tables of results are provided in Appendix A.6. To rigorously quantify the trade-off between efficiency and accuracy, we conduct granular analysis of SPARCs performance across varying computational budgets. Specifically, we evaluate Qwen3-VL-4B on the benchmark under spectrum of input resolutions. By mapping these configurations against their respective inference costs, we construct the Pareto frontier reported in Figure 3. 4.2. Findings Our results demonstrate that we can significantly enhance model performance in completely training-free manner. Notably, this approach not only enables effective perception scaling for the Molmo family but also surpasses the native thinking with images paradigm in Qwen3-VL. We observed consistent performance gains across all benchmarks, with SPARC consistently outperforming its native baseline. Regarding specific architectures, we observe that Molmo2-8B underperforms relative to its 4B counterpart, Submission and Formatting Instructions for ICML 2026 5. Scaling via Perceptual Consistency distinct advantage of our disentangled architecture is the ability to allocate inference budgets asymmetrically. While ensemble methods like Self-Consistency (Wang et al., 2023) are known to enhance performance by aggregating multiple rollouts, they typically incur linear increase in total compute. In contrast, SPARC permits applying self-consistency selectively to the perception branch. Crucially, this yields unique efficiency property: because the perception module outputs simple coordinates in token space, generating multiple detection hypotheses is computationally inexpensive. By aggregating these lightweight rollouts via standard bounding-box fusion algorithm, we can construct single, high-confidence visual context for the reasoning step. Consequently, the expensive reasoning backbone processes only one refined input, avoiding the prohibitive cost of running full-chain reasoning traces. As illustrated in Figure 2, we observe sharp performance cliff: accuracy degrades rapidly as the intersection with the ground truth decreases. This confirms that maximizing spatial coverage of the target region is strict prerequisite for correct reasoning. Motivated by this, we propose strategy that prioritizes recall over precision during the crop aggregation phase. By merging redundant overlapping proposals while retaining distinct non-overlapping regions, we ensure that the model maximizes its effective receptive field over the relevant features, even at the cost of including marginally more background context. We offload the detection of these low-consistent crops to the reasoning step. 5.1. Experimental Setup We perform independent inference IRD rollouts (e.g., = 8) on the global image using non-zero temperature (T = 0.7). This encourages the model to explore diverse localization hypotheses, which are then aggregated using Weighted Boxes Fusion (WBF). Unlike standard NonMaximum Suppression (NMS) which simply discards proposals, WBF computes weighted average of overlapping predictions to derive consensus bounding box. We merge bounding boxes having 50% intersection over union. Distinct, non-overlapping bounding boxes are retained and forwarded directly to the reasoning stage. Results for = 4 and = 8 can be found in Table 2. 5.2. Findings The results in Table 2 demonstrate that enforcing consistency in bounding box generation is robust strategy for enhancing test-time performance. Across all evaluated models, we observe monotonic increase in accuracy as the number of initial rollouts (N ) grows from 1 to 8. This confirms that the stochastic aggregation of multiple percepFigure 3. SPARC outperforms the thinking with images paradigm of Qwen3VL-4B, providing more robust and efficient inference paradigm. This advantage is particularly pronounced in perceptually demanding scenarios, where SPARC achieves superior localization and reasoning with significantly fewer tokens. phenomenon consistent with findings reported in the original work. We note that we experienced reproducibility issues with the Qwen3-VL family of models: in our experiments, there are quite number of cases where thinking with images does not even reach the native model performance. We provide the full quantitative analysis in Appendix A.6 and qualitative analysis of failure modes in Appendix A.7. The advantages of our methodology are most pronounced in low-resolution regimes. In these settings, the generated crops do not merely provide token redundancy; they actively restore critical high-frequency visual information lost during downsizing, effectively bypassing the perceptual bottlenecks of the base resolution. This effect is evident in the pareto front, but it is even more striking in the OOD remote sensing evaluation. On XLRS, for instance, Molmo2 operating at 256-pixel resolution with crops surpasses the performance of the standard model prompted at full resolution. This represents paradigm shift in efficiency: given the datasets average dimension of 8500 8500 pixels, our method achieves superior accuracy while processing approximately 0.1% of the visual tokens required for naive full-resolution forward pass. SPARC minimizes latency by sharing visual KV-caches between the two steps and truncating context to avoid the quadratic costs of entangled chains of thought. This decoupling unlocks asymmetric test-time scaling, enabling dynamic compute allocation, such as for example enforcing consistency between predicted crops. Furthermore, it facilitates independent optimization, allowing the perceptual circuit to be fine-tuned without retraining the reasoning backbone. We dedicate the following sections to rigorously benchmarking these capabilities, showing how modularity allows for more scalable and data-efficient VLM paradigm. 6 Submission and Formatting Instructions for ICML 2026 Table 2. Performance gains and average crop counts using Weighted Box Fusion (WBF) with = 4 and = 8 rollouts. The method allows for effective test-time scaling by refining crop proposals in the text space, drastically reducing the volume of image tokens processed during the final reasoning phase of SPARC. Average Crops Number Method 256 512 Full 512 Full Qwen3VL 4B SPARC SPARC WBF 4 SPARC WBF 8 Qwen3VL 8B SPARC SPARC WBF 4 SPARC WBF 8 Molmo2 4B SPARC SPARC WBF 4 SPARC WBF 8 Molmo2 8B SPARC SPARC WBF 4 SPARC WBF 8 51.0 54.2 55. 45.4 48.9 49.9 48.7 48.7 52.6 47.4 47.9 48.0 60.6 65.4 67.0 54.8 63.4 64.1 57.0 57.8 58. 55.0 54.9 55.9 74.8 81.7 82.0 79.5 80.9 81.1 62.9 63.3 64.1 59.1 58.2 58.3 1.59 2.38 3. 1.23 1.96 2.54 1.96 3.53 5.57 1.55 2.57 3.76 1.63 2.05 2.54 1.31 1.77 2.19 1.62 2.76 4. 1.63 2.22 3.15 1.64 1.72 1.88 1.46 1.66 1.82 1.72 2.73 4.03 1.54 2.14 2.92 tual hypotheses effectively denoises the localization step, leading to more reliable visual contexts for the downstream reasoning task. key advantage of our Weighted Box Fusion (WBF) approach is its ability to improve accuracy without proportional linear increase in downstream computational cost. While we initiate 8 independent rollouts during the relevance detection phase, the de-duplication mechanism ensures that the final number of crops forwarded to the reasoning module remains significantly lower. For instance, with Qwen2VL 4B at 256 resolution, employing 8 rollouts results in an average of only 3.30 final crops. This highlights the efficiency of our asymmetric scaling: we gain the benefits of broad exploration in the cheap perceptual space while maintaining lean context for the expensive reasoning phase. qualitative analysis on how the WBF merging combines predictions at different resolutions can be found in Appendix A.7. An interesting trend emerges when analyzing the relationship between input resolution and crop count. As the input image size increases (from 256 to Full resolution), the average number of final crops consistently decreases. We hypothesize that at higher resolutions, the models ability to solve the Implicit Relevance Detection (IRD) task improves, leading to higher confidence and greater consensus among the rollouts. Consequently, the WBF algorithm merges these highly overlapping predictions into fewer, more unified bounding boxes. This suggests that as perceptual fidelity improves, the model naturally converges on the correct region, reducing the need for extensive ensemble de-duplication. 6. Fine-Tuning for Pure Perception Complementary to test-time scaling, performance can be enhanced by shifting the computational burden to the training phase. By explicitly training the VLM to execute IRD more robustly, we can directly improve accuracy on downstream VQA tasks. Our objective is to enhance the models perceptual capabilities. However, naively fine-tuning VLM on Implicit Relevance Detection (IRD) risks catastrophic forgetting, effectively degrading its reasoning performance. SPARCs disentangled architecture offers solution: because perception and reasoning occur in distinct steps, we can optimize them independently. We implement this by training specialized Low-Rank Adaptation (LoRA) module exclusively for the detection phase. At test time, this adapter is dynamically activated only during perceptual search, ensuring improved localization accuracy without compromising the integrity of the reasoning backbone. distinct advantage of this modular approach is its training simplicity. Unlike the Thinking with Images paradigm which necessitates complex reinforcement learning frameworks, custom reward shaping, and extensively curated process-supervision datasetsour method relies on standard supervised fine-tuning of lightweight LoRA. This allows us to inject specialized perceptual capabilities without the engineering overhead or instability associated with inducing latent reasoning traces in monolithic models. 6.1. Experimental Setup Training our explicit perception modules requires VQA dataset enriched with spatial relevance annotations. As we pointed out in Section 4, IRD correctness is not based on training on unique label, but instead prediction is defined correct if it is leading to correct answer. In order to obtain such annotations, we perform round of synthetic data generation on the DeepEyes dataset (Zheng et al., 2025), tailoring the annotation format to the grounding modality of each target architecture: Bounding-Box Annotations (Qwen3-VL): We utilize the large-scale Qwen3-VL-235B-A22 model, leveraging its native thinking with images capabilities. We extract the crop coordinates generated during the models intermediate tool calls and apply rejection samplingretaining only those traces that yield correct final answer. This process results in high-quality dataset of approximately 23,000 samples. Point-Based Annotations (Molmo2): For the Molmo Submission and Formatting Instructions for ICML 2026 family, we employ the 8B variant (the largest publicly available model at the time of writing). We execute the two-step inference pipeline described in Section 4 to generate relevance points. Following the same filtering protocol as for Qwen3-VL, we retain only the successful traces, yielding curated dataset of approximately 14,000 samples. We perform Supervised Fine-Tuning (SFT) for two epochs using standard autoregressive next-token prediction objective. Crucially, we conduct this training across three distinct resolution scales to evaluate the necessity of high-fidelity inputs for the detection task. This experimental design is motivated by our findings in Section 4, where the base models demonstrated high baseline proficiency in the IRD task at full resolution. We hypothesize that training exclusively at native resolution may render the optimization task too trivial, potentially inducing overfitting due to lack of sufficient difficulty. Moreover, training at full resolution would mean performing pure knowledge distillation of the bigger model, which is much weaker training signal than trying to solve the same task at lower resolution. More details about the training setup are in Appendix A.3 Table 3. We compare the SPARC baseline against specialized adapters fine-tuned at varying resolutions. Counter-intuitively, the model trained on the lowest resolution (SPARC SFT 256, highlighted) achieves the highest accuracy across most test settings. This supports the hypothesis that low-resolution training acts as regularizer: by forcing the model to infer relevance from coarser signals, it learns more robust perceptual features than models trained via trivial high-resolution distillation. 6.2. Findings As detailed in Table 3, fine-tuning the explicit perception module yields systematic performance improvements across all evaluated dimensions, spanning diverse model families, parameter counts, and prompting strategies. The sole exception is Molmo2-4B, where the fine-tuned model performs comparably to the baseline at lower resolutions. We attribute this plateau to distillation bottleneck: the synthetic dataset was generated using relatively weak Molmo2-8B teacher, which likely failed to provide sufficiently highquality supervision for the 4B student. We hypothesize that employing stronger teacher for data generation would resolve this limitation and unlock further gains. Nevertheless, with this single exception, our results confirm that base modelsdespite their strong zero-shot capabilitiesbenefit significantly from targeted optimization for the Implicit Relevance Detection task. Our resolution ablation study reveals counter-intuitive but favorable result: training at reduced image resolutions is not only computationally cheaper but also more effective than full-resolution training. This validates our hypothesis that training on the high-resolution task is prone to overfitting. When trained at native resolution, the model faces trivial optimization path, easily mimicking the teacher model without developing deep understanding of visual relevance. By artificially degrading the input resolution, we increase the task difficulty, forcing the model to rely on structural and semantic context rather than perfect memorization. This constraint prevents the optimization from collapsing into shallow distillation, ensuring that the learned perceptual circuit is robust and capable of generalization. Method 256 512 Full 7. Conclusion Average Score Qwen3VL 4B SPARC SPARC SFT Full SPARC SFT 512 SPARC SFT 256 Qwen3VL 8B SPARC SPARC SFT Full SPARC SFT 512 SPARC SFT 256 Molmo2 4B SPARC SPARC SFT Full SPARC SFT 512 SPARC SFT 256 Molmo2 8B SPARC SPARC SFT Full SPARC SFT 512 SPARC SFT 256 51.0 50.8 51.0 51. 45.4 46.3 44.4 53.1 48.7 46.9 47.2 48.2 47.4 47.8 48.5 48.1 60.6 58.6 59.1 64.0 54.8 55.5 57.1 64.3 57.0 52.7 53.4 56. 55.0 54.2 53.2 55.8 74.8 75.6 74.6 76.8 79.5 80.4 80.9 82.4 62.9 61.4 61.0 63.8 59.1 57.5 59.3 59.6 In this work, we introduced SPARC, biologically inspired framework that decouples VLM inference into distinct perception and reasoning circuits. This separation unlocks robust and efficient inference by leveraging prefix KV-caching and context engineering principles to reduce computational overhead. Moreover, it enables disentangled scaling and optimization: compute can be allocated asymmetrically based on task needsfor instance, by aggregating extensive perceptual search in the text space while passing only compact set of merged visual tokens to the reasoning step. This modularity also simplifies training, allowing for highly efficient, targeted improvements to perception using limited low-resolution synthetic data. Ultimately, SPARC paves the way for advanced test-time strategies, such as iterative zooming via disjoint perceptual cycles or graph-based search for active vision. 8 Submission and Formatting Instructions for ICML"
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., Avila, R., Babuschkin, I., Balaji, S., Balcom, V., Baltescu, P., et al. GPT-4 Technical Report. preprint arXiv:2303.08774, 2024. Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., Ring, R., Rutherford, E., et al. Flamingo: visual language model for few-shot learning. preprint arXiv:2204.14198, 2022. An, X., Xie, Y., Yang, K., Zhang, W., Zhao, X., Cheng, Z., Wang, Y., Xu, S., Chen, C., Zhu, D., et al. Llavaonevision-1.5: Fully open framework for democratized multimodal training. arXiv preprint arXiv:2509.23661, 2025. Avogaro, N., Frick, T., Rigotti, M., Bartezzaghi, A., Janicki, F., Malossi, C., Schindler, K., and Assaf, R. Show or tell? effectively prompting vision-language models for semantic segmentation, 2025. URL https://arxiv. org/abs/2503.19647. Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., and Zhou, J. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond, 2023. URL https://arxi v.org/abs/2308.12966. Bai, S., Cai, Y., Chen, R., Chen, K., Chen, X., Cheng, Z., Deng, L., Ding, W., Gao, C., Ge, C., Ge, W., Guo, Z., Huang, Q., Huang, J., Huang, F., Hui, B., Jiang, S., Li, Z., et al. Qwen3-vl technical report. preprint arXiv:2511.21631, 2025. Besta, M., Blach, N., Kubicek, A., Gerstenberger, R., Podstawski, M., Gianinazzi, L., Gajda, J., Lehmann, T., Niewiadomski, H., Nyczyk, P., and Hoefler, T. Graph of thoughts: Solving elaborate problems with large language models. Proceedings of the AAAI Conference on Artificial Intelligence, 38(16):1768217690, March 2024. ISSN 2159-5399. doi: 10.1609/aaai.v38i16.29720. URL http://dx.doi.org/10.1609/aaai.v38i16 .29720. Koppula, S., Liu, F., Grycner, A., Gritsenko, A., Houlsby, N., Kumar, M., Rong, K., Eisenschlos, J., Kabra, R., Bauer, M., BoË‡snjak, M., Chen, X., Minderer, M., Voigtlaender, P., Bica, I., Balazevic, I., Puigcerver, J., Papalampidi, P., Henaff, O., Xiong, X., Soricut, R., Harmsen, J., and Zhai, X. Paligemma: versatile 3b vlm for transfer, 2024. URL https://arxiv.org/abs/2407.0 7726. Cai, Y., Cai, S., Shi, Y., Xu, Z., Chen, L., Qin, Y., Tan, X., Li, G., Li, Z., Lin, H., Mao, Y., Li, K., and Sun, X. Training-free group relative policy optimization, 2025. URL https://arxiv.org/abs/2510.08191. Chen, K., Zhang, Z., Zeng, W., Zhang, R., Zhu, F., and Zhao, R. Shikra: Unleashing multimodal LLMs referential dialogue magic. preprint arXiv:2306.15195, 2023a. Chen, X., Wang, X., Changpinyo, S., Piergiovanni, A. J., Padlewski, P., Salz, D., Goodman, S., Grycner, A., Mustafa, B., Beyer, L., Kolesnikov, A., Puigcerver, J., Ding, N., Rong, K., Akbari, H., et al. PaLI: jointlyscaled multilingual language-image model. preprint arXiv:2209.06794, 2023b. Cho, J. H., Madotto, A., Mavroudi, E., Afouras, T., Nagarajan, T., Maaz, M., Song, Y., Ma, T., Hu, S., Jain, S., et al. Perceptionlm: Open-access data and models for detailed visual understanding. arXiv preprint arXiv:2504.13180, 2025. Clark, C., Zhang, J., Ma, Z., Park, J. S., Salehi, M., Tripathi, R., Lee, S., Ren, Z., Kim, C. D., Yang, Y., et al. Molmo2: Open weights and data for vision-language models with video understanding and grounding. arXiv preprint arXiv:2601.10611, 2026. Daniel Han, M. H. and team, U. Unsloth, 2023. URL http://github.com/unslothai/unsloth. Diao, X., Liu, Z., Zhang, C., Wu, W., Kong, K., Shi, L., Ding, K., Vosoughi, S., and Gui, J. Addressing Overthinking in Large Vision-Language Models via Gated Perception-Reasoning Optimization, January 2026. Ebouky, B., Bartezzaghi, A., and Rigotti, M. Eliciting reasoning in language models with cognitive tools, 2025. URL https://arxiv.org/abs/2506.12115. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X., Yu, X., Wu, Y., Wu, Z. F., Gou, Z., et al. DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning. preprint arXiv:2501.12948, 2025. Beyer, L., Steiner, A., Pinto, A. S., Kolesnikov, A., Wang, X., Salz, D., Neumann, M., Alabdulmohsin, I., Tschannen, M., Bugliarello, E., Unterthiner, T., Keysers, D., Hua, H., Liu, Q., Zhang, L., Shi, J., Kim, S. Y., Zhang, Z., Wang, Y., Zhang, J., Lin, Z., and Luo, J. Finecaption: Compositional image captioning focusing on wherever 9 Submission and Formatting Instructions for ICML 2026 you want at any granularity. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2476324773, 2025. Huang, S., Dong, L., Wang, W., Hao, Y., Singhal, S., Ma, S., Lv, T., Cui, L., Mohammed, O. K., Patra, B., Liu, Q., Aggarwal, K., Chi, Z., Bjorck, J., Chaudhary, V., Som, S., Song, X., and Wei, F. Language is not all you need: Aligning perception with language models. preprint arXiv:2302.14045, 2023. Karlinsky, L., Arbelle, A., Daniels, A., Nassar, A., Alfassi, A., Wu, B., Schwartz, E., Joshi, D., Kondic, J., Shabtay, N., Li, P., Herzig, R., Abedin, S., Perek, S., Harary, S., et al. Granite Vision: lightweight, open-source multimodal model for enterprise intelligence. preprint arXiv:2502.09927, 2025. Kravitz, D. J., Saleem, K. S., Baker, C. I., and Mishkin, M. new neural framework for visuospatial processing. Nature Reviews Neuroscience, 12(4):217230, April 2011. ISSN 1471-0048. doi: 10.1038/nrn3008. Kumar, S., Zhao, B., Dirac, L., and Varshavskaya, P. Reinforcing VLMs to use tools for detailed visual reasoning under resource constraints. preprint arXiv:2506.14821, 2025. Li, F., Zhang, R., Zhang, H., Zhang, Y., Li, B., Li, W., Ma, Z., and Li, C. Llava-next-interleave: Tackling multiimage, video, and 3d in large multimodal models, 2024. URL https://arxiv.org/abs/2407.07895. Li, J., Li, D., Savarese, S., and Hoi, S. BLIP-2: Bootstrapping language-image pre-training with frozen impreprint age encoders and large language models. arXiv:2301.12597, 2023. Li, Z., Wu, X., Du, H., Liu, F., Nghiem, H., and Shi, G. survey of state of the art large vision language models: Alignment, benchmark, evaluations and challenges. In IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pp. 15781597, 2025. Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. Advances in Neural Information Processing Systems (NeurIPS), 36:3489234916, 2023a. Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning, 2023b. URL https://arxiv.org/abs/ 2304.08485. Mao, J., Huang, J., Toshev, A., Camburu, O., Yuille, A., and Murphy, K. Generation and comprehension of unambiguous object descriptions, 2016. URL https: //arxiv.org/abs/1511.02283. McKinzie, B., Gan, Z., Fauconnier, J.-P., Dodge, S., Zhang, B., Dufter, P., Shah, D., Du, X., Peng, F., Weers, F., Belyi, A., Zhang, H., Singh, K., Kang, D., Jain, A., H`e, H., Schwarzer, M., Gunter, T., Kong, X., Zhang, A., Wang, J., Wang, C., Du, N., Lei, T., Wiseman, S., Yin, G., Lee, M., Wang, Z., Pang, R., Grasch, P., Toshev, A., and Yang, Y. Mm1: Methods, analysis & insights from multimodal llm pre-training, 2024. URL https: //arxiv.org/abs/2403.09611. Mei, L., Yao, J., Ge, Y., Wang, Y., Bi, B., Cai, Y., Liu, J., Li, M., Li, Z.-Z., Zhang, D., Zhou, C., Mao, J., Xia, T., Guo, J., and Liu, S. Survey of Context Engineering for Large Language Models. preprint arXiv:2507.13334, 2025. Miller, E. K. and Cohen, J. D. An integrative theory of Prefrontal Cortex function. Annual Review of Neuroscience, 24(1):167202, March 2001. ISSN 0147-006X. doi: 10.1146/annurev.neuro.24.1.167. Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi, H., and Zettlemoyer, L. Rethinking the role of demonstrations: What makes in-context learning work?, 2022. URL https://arxiv.org/abs/2202.1 2837. Mishkin, M., Ungerleider, L. G., and Macko, K. A. Object vision and spatial vision: Two cortical pathways. Trends in Neurosciences, 6:414417, January 1983. ISSN 01662236, 1878-108X. doi: 10.1016/0166-2236(83)90190-X. Ni, M., Yang, Z., Li, L., Lin, C.-C., Lin, K., Zuo, W., and Wang, L. Point-rft: Improving multimodal reasoning with visually grounded reinforcement finetuning. arXiv preprint arXiv:2505.19702, 2025. OpenAI. Learning to reason with LLMs. https://op enai.com/index/learning-to-reason-wit h-llms/, 2024. accessed: 2026-01-27. OpenAI Research. Thinking with images. https://op enai.com/index/thinking-with-images/, 2025. accessed: 2026-01-27. Peng, Z., Wang, W., Dong, L., Hao, Y., Huang, S., Ma, S., and Wei, F. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning transferable visual models from natural language supervision, 2021. URL https://arxiv.org/abs/2103.00020. 10 Submission and Formatting Instructions for ICML 2026 Rasheed, H., Maaz, M., Shaji, S., Shaker, A., Khan, S., Cholakkal, H., Anwer, R. M., Xing, E., Yang, M.-H., and Khan, F. S. Glamm: Pixel grounding large multimodal model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13009 13018, 2024. Rigotti, M., Barak, O., Warden, M., Wang, X.-J., Daw, N., Miller, E., and Fusi, S. The importance of mixed selectivity in complex cognitive tasks. Nature, 497(7451): 585590, May 2013. doi: 10.1038/nature12160. Sarch, G., Saha, S., Khandelwal, N., Jain, A., Tarr, M. J., Kumar, A., and Fragkiadaki, K. Grounded reinforcement learning for visual reasoning. preprint arXiv:2505.23678, 2025. Su, Z., Xia, P., Guo, H., Liu, Z., Ma, Y., Qu, X., Liu, J., Li, Y., Zeng, K., Yang, Z., et al. Thinking with images for multimodal reasoning: Foundations, methods, and future frontiers. arXiv preprint arXiv:2506.23918, 2025. Sung, Y., Rigotti, M., and Lee, S. Factorized embedding of goal and uncertainty in the lateral prefrontal cortex guides stably flexible learning. Nature Communications, November 2025. ISSN 2041-1723. doi: 10.1038/s41467 -025-66677-w. Tang, Y. Y., Shimada, D., Hua, H., Huang, C., Bi, J., Feris, R., and Xu, C. Video-r4: Reinforcing text-rich video reasoning with visual rumination. arXiv preprint arXiv:2511.17490, 2025. Tye, K., Miller, E., Taschbach, F., Benna, M., Rigotti, M., and Fusi, S. Mixed selectivity: Cellular computations for complexity. Neuron, 2024. ISSN 0896-6273. doi: 10.1016/j.neuron.2024.04.017. von Werra, L., Belkada, Y., Tunstall, L., Beeching, E., Thrush, T., Lambert, N., Huang, S., Rasul, K., and Gallouedec, Q. TRL: Transformers Reinforcement Learning, 2020. URL https://github.com/huggingfa ce/trl. Wang, F., Wang, H., Guo, Z., Wang, D., Wang, Y., Chen, M., Ma, Q., Lan, L., Yang, W., Zhang, J., et al. XLRS-bench: Could your multimodal LLMs understand extremely large ultra-high-resolution remote sensing imagery? In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1432514336, 2025a. Wang, H., Su, A., Ren, W., Lin, F., and Chen, W. Pixel reasoner: Incentivizing pixel-space reasoning with curiosity-driven reinforcement learning. arXiv preprint arXiv:2505.15966, 2025b. Wang, W., Ding, L., Zeng, M., Zhou, X., Shen, L., Luo, Y., and Tao, D. Divide, conquer and combine: trainingfree framework for high-resolution image perception in multimodal large language models, 2024. URL https: //arxiv.org/abs/2408.15556. Wang, W., Gao, Z., Gu, L., Pu, H., Cui, L., Wei, X., Liu, Z., Jing, L., Ye, S., Shao, J., et al. Internvl3. 5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025c. Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., and Zhou, D. Self-consistency improves chain of thought reasoning in language models. preprint arXiv:2203.11171, 2023. Wei, H., Sun, Y., and Li, Y. Deepseek-ocr: Contexts optical compression. arXiv preprint arXiv:2510.18234, 2025. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-of-Thought prompting elicits reasoning in large language models. preprint arXiv:2201.11903, 2023. Wu, P. and Xie, S. V*: Guided visual search as core mechanism in multimodal llms, 2023. URL https: //arxiv.org/abs/2312.14135. Xie, S. M., Raghunathan, A., Liang, P., and Ma, T. An explanation of in-context learning as implicit bayesian inference, 2022. URL https://arxiv.org/abs/ 2111.02080. Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y., and Narasimhan, K. Tree of thoughts: Deliberate problem solving with large language models, 2023. URL https://arxiv.org/abs/2305.10601. Yu, L., Poirson, P., Yang, S., Berg, A. C., and Berg, T. L. Modeling context in referring expressions, 2016. URL https://arxiv.org/abs/1608.00272. Zhai, X., Mustafa, B., Kolesnikov, A., and Beyer, L. Sigmoid loss for language image pre-training, 2023. URL https://arxiv.org/abs/2303.15343. Zhang, J., Hu, J., Khayatkhoei, M., Ilievski, F., and Sun, M. Exploring perceptual limitation of multimodal large language models, 2024. URL https://arxiv.org/ abs/2402.07384. Zhang, J., Khayatkhoei, M., Chhikara, P., and Ilievski, F. Mllms know where to look: Training-free perception of small visual details with multimodal llms, 2025. URL https://arxiv.org/abs/2502.17422. Zheng, Z., Yang, M., Hong, J., Zhao, C., Xu, G., Yang, L., Shen, C., and Yu, X. Deepeyes: Incentivizing thinking 11 Submission and Formatting Instructions for ICML 2026 with images via reinforcement learning. arXiv preprint arXiv:2505.14362, 2025. Zhu, D., Chen, J., Shen, X., Li, X., and Elhoseiny, M. MiniGPT-4: Enhancing vision-language understandpreprint ing with advanced large language models. arXiv:2304.10592, 2023. Submission and Formatting Instructions for ICML 2026 A. Appendix A.1. Test-time scaling generalization to Molmo2 Architecture To verify that the resolution compensation phenomenon is not specific to the Qwen3 architecture, we replicate our crop overlap ablation using the Molmo2-4B (Clark et al., 2026) family. As shown in Figure 4, we observe an identical behavioral pattern: while the performance of downscaled models (256px, 512px) drops precipitously when crop alignment is poor, it recovers dramatically as the Intersection-over-Union (IoU) with the ground truth increases. When provided with oracle-level crops (Overlap Ratio 1.0), the 256px and 512px baselines effectively close the performance gap with the full-resolution model, offering computationally efficient alternative to processing high-resolution images. This validates our core premise: investing compute in precise localization (via SPARC) allows us to offload the heavy reasoning step to much lighter, low-resolution inference passes without sacrificing accuracy. Figure 4. We extend our analysis to the Molmo2 architecture, plotting accuracy against crop overlap ratio. Consistent with our findings on Qwen3VL, the low-resolution variants exhibit steep performance recovery as crop precision improves. Notably, high-quality crops allow the efficient 256px and 512px models to approach the performance upper bound of the Full-resolution baseline, further supporting the motivation of the SPARC pipeline. A.2. Implicit relevant detection as an ill-posed problem To empirically determine the optimal field of view, we evaluate performance while progressively upscaling the ground truth crop size by factor of up to 10 (Figure 5). Initially, we observe consistent performance gain across all resolutions as the crop expands (e.g., peaking around scale 2.5 for the 256px model), validating that strictly tight bounding boxes often exclude necessary semantic context. However, critical trade-off emerges for the resolution-constrained variants (256px and 512px). Since these crops are resized to fit fixed pixel buffer (e.g., max 256px), excessively enlarging the physical crop region forces aggressive downsampling, diluting the objects visual fidelity. Consequently, while the Full-resolution model remains robust at large scales, the 256px model suffers sharp performance collapse beyond scale 4, as the loss of high-frequency detail outweighs the benefit of added context. A.3. Training Details We fine-tuned the Qwen3-VL-Instruct architecture using the Unsloth (Daniel Han & team, 2023) and TRL (von Werra et al., 2020) framework. To mitigate computational costs while maintaining performance, we employed LoRA finetuning across both the vision and language components of the model. Specifically, we applied LoRA adapters to the attention mechanisms, MLP modules, and vision encoders. Optimization was performed using the 8-bit AdamW optimizer, coupled with linear learning rate scheduler. The training was executed using the TRL framework with gradient checkpointing enabled to support longer context windows and higher batch sizes. Molmo2 was trained on the same framework using in this case naive huggingfaces transformer implementation. Qwen3VL-8B was trained on single A100-80GB for approximately 12 hours, while Molmo2 naive implementation is more computationally expensive, requiring double the computation budget. Hyperparameters can be found in Table 4 13 Submission and Formatting Instructions for ICML 2026 Figure 5. We measure reasoning accuracy as function of crop expansion factor (up to 100 the original box area). While moderate expansion (scales 24) improves performance by providing necessary context, excessive scaling leads to sharp decline for resolutionconstrained models. A.4. Prompts We provide the specific prompts employed for the Implicit Relevance Detection (IRD) phase (Step 1) and the subsequent Reasoning phase (Step 2) for both the Qwen and Molmo architectures. Empirically, we observed that the single most critical factor for ensuring robust instruction adherence was the explicit definition of the output format. Enforcing strict structural constraintspecifically, requesting JSON output for Qwen and Point coordinates for Molmosignificantly reduced syntax errors and hallucinations compared to less constrained prompts. IRD Qwen3-VL Prompt {image} You are helpful assistant capable of doing object detection. be given an image and question for context. the question, but identify the objects the user will ask for and return their 2D bounding box and label in JSON format. low resolution, but the objects will be there. following question: {question} DO NOT ANSWER THE QUESTION. Identify the relevant objects and return their 2D bounding box and label in JSON format. The images will be very Given this image and the Your role is not to answer You will QA Qwen3-VL Prompt {image} {crop}N You are helpful assistant. You are given an image and relevant crops to answer the following question: {question} Question: Answer with the options letter from the given choices directly. letter only. Predict the 14 Submission and Formatting Instructions for ICML IRD Molmo2 Prompt {image} Question: You are helpful assistant. Your task is to POINT to the objects relevant to the users question. {question} QA Molmo2 Prompt {image} {crop}N You are helpful assistant. You are given an image and relevant crops to answer the following question: Question: {question} Answer with the options letter from the given choices directly. letter only. Predict the A.5. Comparison with Thinking with Images approaches on V* Table 5 compares SPARC (using Qwen3-VL-8B) against state-of-the-art scaling and RL-based approaches. Notably, our fully optimized pipeline (w/ SFT) achieves 91.2%, narrowly surpassing the biggest model of the family Qwen3-VL-235B-A22B (91.1%) and significantly outperforming sophisticated RL baselines like DeepEyes (90.1%) and ViGoRL-7B (86.4%). This result confirms that explicitly disentangling perception allows an 8B model to rival architectures with 30 more parameters, suggesting that the primary performance bottleneck is often perceptual. Moreover, SPARC achieves these gains via stable Supervised Fine-Tuning (SFT) of the perception circuit, avoiding the instability and complexity of the reinforcement learning recipes required by competing methods. A.6. Expanded results tables We present expanded performance metrics across varying computational budgets for , HRBench-4k, and HRBench-8k. Table 7 details the results for the SFT-based SPARC experiments. Table 8 provides the corresponding performance data for WBF, while Table 9 reports the associated crop counts. A.7. Qualitative analysis Figure 6 presents qualitative results from the WBF experiment. The merging algorithm proves particularly effective at lower resolutions, where lower confidence levels lead the model to generate diverse set of bounding boxes. At full resolution, the WBF results in deduplication of virtually the same bounding boxes. Additionally, Figures 6 through 9 provide qualitative comparisons between the thinking with images baseline and SPARC across various use cases. 15 Submission and Formatting Instructions for ICML 2026 Table 4. Hyperparameter configuration for SPARC SFT fine-tuning. Category Hyperparameter Value Model Configuration Base Model LoRA Configuration Optimization Training Schedule Precision Max Context Length Image Resolution Rank (r) Alpha (Î±) Dropout Target Modules Bias Optimizer Learning Rate Weight Decay Scheduler Type Warmup Steps Epochs Batch Size (per device) Gradient Accumulation Train/Val Split unsloth/Qwen3-VL-8B-Instruct 16-bit (LoRA) 2048 256 256 16 32 0.0 Vision, Language, Attn, MLP None AdamW (8-bit) 2 104 0.001 Linear 100 5 32 4 99% / 1% Table 5. Performance of out proposed SPARC Framework compared to the existing thinking with images approaches in the literature. Metrics marked with were reproduced by the authors. Model Name VBench Thinking with Images ChatGPT-o1 (OpenAI, 2024) Reinforcing VLMs (Kumar et al., 2025) Pixel-Reasoner (Wang et al., 2025b) ViGoRL-7B (Sarch et al., 2025) DeepEyes (Zheng et al., 2025) Qwen3-VL 235B-A22B (Bai et al., 2025) ChatGPT-o3 (OpenAI Research, 2025) Qwen3-VL-8B Native Performance SPARC (Ours) w/ Consistency w/ SFT 69.7 80.1 84.3 86.4 90.1 91.1 95.7 87.0 88.7 89.5 91.2 16 Submission and Formatting Instructions for ICML 2026 8 Rollout 8 WBF Ground Truth Bounding Box 6 5 2 2 1 5 F Table 6. Qualitative comparison across resolutions (256, 512, Full) for 8 Rollouts WBF, and Ground Truth. 17 Submission and Formatting Instructions for ICML 2026 Table 7. Expanded table of results for the SPARC SFT experiments HRBench-4K HRBench-8K"
        },
        {
            "title": "Setting",
            "content": ""
        },
        {
            "title": "Full",
            "content": ""
        },
        {
            "title": "Full",
            "content": ""
        },
        {
            "title": "Full",
            "content": ""
        },
        {
            "title": "Full",
            "content": "Qwen3VL 4B Native Performance Thinking with images SPARC SPARC SFT Full SPARC SFT 512 SPARC SFT 256 Qwen3VL 8B Native Performance Thinking with images SPARC SPARC SFT Full SPARC SFT 512 SPARC SFT 256 Molmo2 4B Native Performance SPARC SPARC SFT Full SPARC SFT 512 SPARC SFT 256 Molmo2 8B Native Performance SPARC SPARC SFT Full SPARC SFT 512 SPARC SFT 256 40.3 32.1 54.6 53.4 54.6 55.9 38.2 35.3 50.0 52.5 46.6 52. 48.3 55.0 49.6 53.4 53.4 46.2 52.9 53.4 52.9 51.7 47.9 51.5 66.4 66.0 65.5 69.3 46.2 55.9 62.6 64.3 69.3 69.7 56.7 70.2 60.1 62.2 66.8 56.3 62.6 63.0 61.3 66. 84.5 83.1 86.1 87.8 88.2 91.2 87.0 86.6 88.7 91.6 92.9 91.2 71.4 76.5 74.8 74.4 79.4 68.5 70.2 67.7 71.9 72.3 43.5 43.1 53.8 52.8 52.3 53.9 43.6 47.5 43.6 43.6 43.6 59. 50.0 47.5 47.3 46.1 47.4 47.8 46.5 47.4 48.0 48.4 52.8 58.4 60.0 59.4 59.4 63.4 55.6 61.4 55.8 55.8 55.8 65.6 55.8 54.1 54.1 53.5 56.8 56.4 56.6 53.9 54.8 56. 68.8 72.0 69.0 70.5 68.5 70.6 77.0 76.6 76.9 76.8 76.8 79.5 57.9 57.8 57.8 57.5 58.5 55.3 58.1 55.9 55.8 57.6 41.4 35.1 44.8 46.4 46.0 45.4 42.3 40.0 42.5 42.9 42.9 47. 46.1 43.5 44.0 42.0 43.8 43.4 42.8 42.8 44.6 44.4 45.6 46.8 55.4 50.4 52.4 59.4 46.3 52.4 46.1 46.4 46.3 57.6 47.1 46.8 44.0 44.5 46.9 44.5 45.6 45.6 43.5 45. 64.6 64.4 69.4 68.4 67.1 68.5 73.0 71.0 73.0 73.0 73.0 76.4 53.1 54.5 51.6 51.1 53.4 49.6 49.1 49.0 50.4 48.9 41.7 36.8 51.0 50.8 51.0 51.7 41.4 40.9 45.4 46.3 44.4 53. 48.1 48.7 46.9 47.2 48.2 45.8 47.4 47.8 48.5 48.1 48.8 52.2 60.6 58.6 59.1 64.0 49.4 56.5 54.8 55.5 57.1 64.3 53.2 57.0 52.7 53.4 56.8 52.4 55.0 54.2 53.2 55. 72.6 73.1 74.8 75.6 74.6 76.8 79.0 78.1 79.5 80.4 80.9 82.4 60.8 62.9 61.4 61.0 63.8 57.8 59.1 57.5 59.3 59.6 18 Submission and Formatting Instructions for ICML Table 8. Expanded table of results for the SPARC WBF experiments HRBench-4K HRBench-8K"
        },
        {
            "title": "Setting",
            "content": ""
        },
        {
            "title": "Full",
            "content": ""
        },
        {
            "title": "Full",
            "content": ""
        },
        {
            "title": "Full",
            "content": ""
        },
        {
            "title": "Full",
            "content": "Qwen3VL 4B Native Performance Thinking with images SPARC SPARC WBF 4 SPARC WBF 8 Qwen3VL 8B Native Performance Thinking with images SPARC SPARC WBF 4 SPARC WBF 8 Molmo2 4B Native Performance SPARC SPARC WBF 4 SPARC WBF 8 Molmo2 8B Native Performance SPARC SPARC WBF 4 SPARC WBF 8 40.3 32.1 54.6 54.6 56.6 38.2 35.3 50.0 48.3 49. 48.3 55.0 55.0 64.7 46.2 52.9 52.9 53.4 47.9 51.5 66.4 69.3 68.9 46.2 55.9 62.6 63.9 64.7 56.7 70.2 71.0 72.3 56.3 62.6 64.7 66. 84.5 83.1 86.1 87.0 87.0 87.0 86.6 88.7 88.2 89.5 71.4 76.5 78.6 80.7 68.5 70.2 68.1 68.9 43.5 43.1 53.8 58.1 60.3 43.6 47.5 43.6 55.3 56. 50.0 47.5 47.1 48.6 47.8 46.5 48.6 48.8 52.8 58.4 60.0 68.1 70.8 55.6 61.4 55.8 67.9 68.9 55.8 54.1 54.5 54.4 56.4 56.6 56.8 57. 68.8 72.0 69.0 79.4 80.4 77.0 76.6 76.9 80.9 79.6 57.9 57.8 57.6 57.9 55.3 58.1 57.8 57.3 41.4 35.1 44.8 50.0 50.1 42.3 40.0 42.5 43.0 44. 46.1 43.5 44.0 44.5 43.4 42.8 42.3 41.9 45.6 46.8 55.4 58.6 61.3 46.3 52.4 46.1 58.5 58.6 47.1 46.8 47.8 48.4 44.5 45.6 43.4 43. 64.6 64.4 69.4 78.8 78.6 73.0 71.0 73.0 73.5 74.1 53.1 54.5 53.8 53.9 49.6 49.1 48.8 48.6 41.7 36.8 51.0 54.2 55.7 41.4 40.9 45.4 48.9 49. 48.1 48.7 48.7 52.6 45.8 47.4 47.9 48.0 48.8 52.2 60.6 65.4 67.0 49.4 56.5 54.8 63.4 64.1 53.2 57.0 57.8 58.3 52.4 55.0 54.9 55. 72.6 73.1 74.8 81.7 82.0 79.0 78.1 79.5 80.9 81.1 60.8 62.9 63.3 64.1 57.8 59.1 58.2 58.3 19 Submission and Formatting Instructions for ICML Table 9. Expanded table of results on the number of crops for the SPARC WBF experiments HRBench-4K HRBench-8K"
        },
        {
            "title": "Setting",
            "content": ""
        },
        {
            "title": "Full",
            "content": ""
        },
        {
            "title": "Full",
            "content": ""
        },
        {
            "title": "Full",
            "content": ""
        },
        {
            "title": "Full",
            "content": "Qwen3VL 4B Predicted Crops WBF 4 WBF 8 Qwen3VL 8B Predicted Crops WBF 4 WBF 8 MOLMO2 4B Predicted Crops WBF 4 WBF 8 MOLMO2 8B Predicted Crops WBF 4 WBF 8 1.84 2.19 3.14 1.08 1.79 2. 2.20 2.48 3.64 1.50 1.71 2.09 1.79 1.83 2.20 1.18 1.59 1.90 1.64 1.97 2.48 1.47 1.48 1. 1.67 1.43 1.52 1.30 1.36 1.48 1.58 1.91 2.46 1.36 1.44 1.66 1.50 2.40 3.27 1.36 2.05 2. 1.85 3.46 5.30 1.59 2.60 3.96 1.58 2.07 2.52 1.45 1.93 2.42 1.53 2.71 3.92 1.72 2.27 3. 1.62 1.83 2.06 1.57 1.79 1.98 1.83 2.78 4.03 1.65 2.25 3.03 1.42 2.54 3.48 1.26 2.03 2. 1.84 4.66 7.78 1.56 3.40 5.24 1.53 2.26 2.90 1.30 1.79 2.25 1.70 3.60 5.86 1.69 2.92 4. 1.63 1.91 2.06 1.50 1.82 1.99 1.77 3.50 5.60 1.60 2.73 4.06 1.59 2.38 3.30 1.23 1.96 2. 1.96 3.53 5.57 1.55 2.57 3.76 1.63 2.05 2.54 1.31 1.77 2.19 1.62 2.76 4.09 1.63 2.22 3. 1.64 1.72 1.88 1.46 1.66 1.82 1.72 2.73 4.03 1.54 2.14 2.92 20 Submission and Formatting Instructions for ICML Figure 6. While both models answer correctly, Thinking with Images (left) relies on dense, unstructured chain-of-thought, consuming large token budget to plan and describe the scene. On the other hand SPARC (right) isolates the target object and answers instantly with significantly lower computational cost. 21 Submission and Formatting Instructions for ICML 2026 Figure 7. On the left, Thinking with Images initially isolates the correct crop but misinterprets the visual content due to deceptive text description. This misalignment triggers series of wasteful search steps, leading the model to confuse stone lamp post base for trash can. Consequently, it hallucinates metallic and reflective properties, resulting in an incorrect Silver prediction. On the right, SPARC correctly localizes the actual black bin immediately, avoiding the reasoning trap and returning the correct answer in single step. 22 Submission and Formatting Instructions for ICML Figure 8. Thinking with Images correctly spots the green clock but hallucinates that it must be large and functional, causing it to discard valid visual evidence, classic example of how reasoning errors cascade in monolithic models. On the right, SPARC succeeds by decoupling perception: it explicitly localizes the clock first via visual search, isolating the relevant region before reasoning begins, effectively preventing prior bias and arriving at the correct answer with significantly fewer tokens. 23 Submission and Formatting Instructions for ICML 2026 Figure 9. Thinking with Images concentrates on the most prominent foreground subject, correctly reasoning that this person has no scarf, but failing to scan the background for the actual target. On the right, SPARC demonstrates the benefit of explicit visual search. It successfully localizes the smaller background figure wearing the green scarf."
        }
    ],
    "affiliations": [
        "ETH Zurich",
        "Hugging Face"
    ]
}