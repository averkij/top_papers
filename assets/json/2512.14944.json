{
    "paper_title": "Puzzle Curriculum GRPO for Vision-Centric Reasoning",
    "authors": [
        "Ahmadreza Jeddi",
        "Hakki Can Karaimer",
        "Hue Nguyen",
        "Zhongling Wang",
        "Ke Zhao",
        "Javad Rajabi",
        "Ran Zhang",
        "Raghav Goyal",
        "Babak Taati",
        "Radek Grzeszczuk"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent reinforcement learning (RL) approaches like outcome-supervised GRPO have advanced chain-of-thought reasoning in Vision Language Models (VLMs), yet key issues linger: (i) reliance on costly and noisy hand-curated annotations or external verifiers; (ii) flat and sparse reward schemes in GRPO; and (iii) logical inconsistency between a chain's reasoning and its final answer. We present Puzzle Curriculum GRPO (PC-GRPO), a supervision-free recipe for RL with Verifiable Rewards (RLVR) that strengthens visual reasoning in VLMs without annotations or external verifiers. PC-GRPO replaces labels with three self-supervised puzzle environments: PatchFit, Rotation (with binary rewards) and Jigsaw (with graded partial credit mitigating reward sparsity). To counter flat rewards and vanishing group-relative advantages, we introduce a difficulty-aware curriculum that dynamically weights samples and peaks at medium difficulty. We further monitor Reasoning-Answer Consistency (RAC) during post-training: mirroring reports for vanilla GRPO in LLMs, RAC typically rises early then degrades; our curriculum delays this decline, and consistency-enforcing reward schemes further boost RAC. RAC correlates with downstream accuracy. Across diverse benchmarks and on Qwen-7B and Qwen-3B backbones, PC-GRPO improves reasoning quality, training stability, and end-task accuracy, offering a practical path to scalable, verifiable, and interpretable RL post-training for VLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 4 4 9 4 1 . 2 1 5 2 : r Puzzle Curriculum GRPO for Vision-Centric Reasoning Ahmadreza Jeddi*1,2,3 Hakki C. Karaimer*1 Hue Nguyen1 Zhongling Wang1 Ke Zhao1 Javad Rajabi1,2,3 Ran Zhang1 Raghav Goyal1 Babak Taati2,3 Radek Grzeszczuk1 1AI Center-Toronto, Samsung Electronics 2University of Toronto 3Vector Institute ajeddi@cs.toronto.edu, hakki.k@samsung.com"
        },
        {
            "title": "Abstract",
            "content": "reinforcement learning (RL) approaches Recent like outcome-supervised GRPO have advanced chain-ofthought reasoning in Vision Language Models (VLMs), (i) reliance on costly and noisy yet key issues linger: hand-curated annotations or external verifiers; (ii) flat and sparse reward schemes in GRPO; and (iii) logical inconsistency between chains reasoning and its final answer. We present Puzzle Curriculum GRPO (PC-GRPO), supervision-free recipe for RL with Verifiable Rewards (RLVR) that strengthens visual reasoning in VLMs without annotations or external verifiers. PC-GRPO replaces labels with three self-supervised puzzle environments: PatchFit, Rotation (with binary rewards) and Jigsaw (with graded partial credit mitigating reward sparsity). To counter flat rewards and vanishing group-relative advantages, we introduce difficulty-aware curriculum that dynamically weights samples and peaks at medium difficulty. We further monitor Reasoning-Answer Consistency (RAC) during post-training: mirroring reports for vanilla GRPO in LLMs, RAC typically rises early then degrades; our curriculum delays this decline, and consistency-enforcing reward schemes further boost RAC. RAC correlates with downstream accuracy. Across diverse benchmarks and on Qwen-7B and Qwen-3B backbones, PC-GRPO improves reasoning quality, training stability, and end-task accuracy, offering practical path to scalable, verifiable, and interpretable RL post-training for VLMs. Project page: https://pcgrpo.github.io/ 1. Introduction Recent progress in VLMs has been driven by RL posttraining, which shapes policies beyond supervised instruction tuning [6, 30, 62, 81]. In particular, GRPO-style objectives adapted to RLVR have become practical recipe for inducing stepwise reasoning in VLMs while preserving *Equal contribution Equal contribution Figure 1. Performance of our model against state-of-the-art methods on diverse visual reasoning benchmarks. The chart compares PC-GRPO model (Ours) with strong baselines, including Qwen-2.5-VL-7B base model. Each axis represents different benchmark. Our method achieves competitive or superior results across the board, demonstrating that the supervision-free puzzle curriculum effectively enhances the models visual reasoning capabilities. Additionally, the reasoning abilities of PC-GRPO reveal critical levels of noise in popular vision benchmarks. We audit and clean some of these benchmarks (denoted with the clean suffix) using high performance VLMs. We then benchmark PC-GRPO and existing baselines on the clean subsets. general utility [25, 44, 54, 55]. Despite rapid advances, two clusters of challenges per- (i) obtaining verifisist. Optimization/data issues: able, vision-centric rewards for RLVR remains costly and noisy; [9, 70] (ii) group-relative optimization suffers from flat rewards, where easy/medium/hard instances contribute Figure 2. PC-GRPO overcomes fundamental reasoning failures in VLMs When asked simple visual reasoning question, existing GRPO-tuned models often fail by overthinking irrelevant details, shortcutting to statistically likely but incorrect answer, or producing final answer that contradicts their own reasoning trace. PC-GRPO learns to produce faithful and visually-grounded answer. with nearly the same influence on updates [80, 83], and from vanishing advantages [62], which arise when sample is too easy or too hard so that rollouts within group become homogeneous-driving group-relative advantages toward zero and weakening learning. Reasoning-behavior issues (illustrated in Fig. 2): the use of chain-of-thought (CoT) introduces valuable interpretability channel for monitoring and iterative refinement, but it also exposes characteristic failure modesshortcutting (minimal reasoning) [72], overthinking (off-track chains) [17, 74], perception deficiencies rooted in the visual front end [61, 68], and reasoning-answer inconsistency (the chain supports option while the final <answer> is B) [11, 12, 29, 56, 77]. We present Puzzle Curriculum GRPO (PC-GRPO), supervision-free RLVR framework that addresses both clusters jointly. First, to replace costly supervision, we instantiate verifiable, self-supervised puzzle environments inspired by classic pretext tasks: PatchFit (identify the masked patch among confusable candidates), Rotation [24] (predict the image rotation from fixed angle set), and Jigsaw [47, 48, 69, 70] (reconstruct tiled image by assigning each tile to its correct grid position with proper permutation). Rotation and PatchFit yield binary rewards; Jigsaw provides graded reward equal to the fraction of correctly placed tiles. Our hypothesis is that graded, partialcredit signals reward intermediate progress and penalize localized errors in the chain, reducing the need for separate process/step-wise rewards while alleviating reward sparsity. Unlike pipelines that rely on SFT or external teacher models [14, 30, 66, 75], PC-GRPO is fully supervision-free. Second, to counter flat rewards and vanishing advantages, we introduce difficulty-aware curriculum. For binary-reward puzzles we weight by reward variance (peaking at medium difficulty); for Jigsaw we use distinctsolution statistic tailored to its combinatorial nature. The weight w(d) prioritizes medium-hard examples and adapts as the policy improves, concentrating learning signal where it is most informative. Third, to address the mismatch between the reasoning chain and the final answer, we introduce ReasoningAnswer Consistency (RAC) metric and track it throughout post-training. Empirically, consistent with observations in LLMs [11, 77], vanilla GRPO shows drift: RAC is relatively high early, then degrades as training progresses, even as puzzle rewards continue to rise. Our puzzle curriculum slows this degradation and raises RAC overall; moreover, combined with lightweight consistency-aware variant (GRPO-CARE [12]), we observe further gains. No single training-time signal (reward, RAC, or rollout statistics) perfectly predicts downstream accuracy; yet monitoring RAC alongside downstream performance reveals that higher RAC correlates with better accuracy and, notably, later checkpoints are often not the best. notable by-product of our supervision-free RLVR setup is that PC-GRPO surfaces noisy or ambiguous items across popular visual benchmarks. Targeted user studies reveal widespread label errors and underspecified prompts. To mitigate this, we validate signals with stronger external VLMs (e.g., GPT, Gemini, Claude) used strictly as auditors, and design simple remedies from their agreement patterns. We believe that, alongside documented data contamination [8, 76], benchmark noise is material bottleneck to progress in this space. Empirically, PC-GRPO delivers consistent gains across diverse benchmarks  (Fig. 1)  , using Qwen-family backbones [3] and fully supervision-free pipeline. Contributions PC-GRPO: we propose supervision-free RLVR framework that uses puzzle environments and difficultyaware curriculum to dynamically emphasize medium difficulty and improve training dynamics. RAC monitoring & selection: we define and track ReasoningAnswer Consistency (RAC) using an open-source VLM as the judge, and select intermediate checkpoints based on the emergence and subsequent decline of RAC peaks observed during training. Benchmark auditing: we highlight pervasive benchmark noise (underspecified prompts, label errors) and propose practical auditing/cleaning remedies aided by stronger VLM auditors, and release cleaned subsets to support future evaluation. 2. Related Work LLM/VLM RL post-training. RL post-training has been central to recent advances in language models [50, 52] and, more recently, to RL with verifiable rewards (RLVR) for inducing stepwise reasoning [25]. Group-relative policy optimization (GRPO) [54] and its variants have been studied along several fronts: comparing gains against alternative objectives [43, 79, 82], understanding interactions with supervised finetuning (SFT) [6, 15], and analyzing efficiency and scaling [5, 41]. Motivated by this progress, the VLM community has begun to adapt these paradigms [18, 21, 30, 55, 62]. Most efforts strengthen multimodal reasoning on mathand science-oriented benchmarks, while emerging work extends RL post-training to vision-centric tasks such as grounding and segmentation [4, 42, 55]. However, these approaches remain primarily text-driven or taskspecific and continue to rely heavily on user annotations, which is significant bottleneck for the visual domain. Towards supervision-free post-training. Obtaining clean ground-truth answers is costly, sometimes impractical, and often noisy, motivating methods that reduce or eliminate dependence on annotations. techniques such as entropy minimization [51], majority voting across rollouts [10, 84], and even robustness to imperfect rewards have been explored [53]. For VLMs, verifier-based pipelines [65, 66] (e.g., critic/judge models that assess capIn LLMs, tions or textual responses) and gamified or self-play environments [14, 64] improve perception and reasoning but introduce new costs and biases through external evaluators. Closer to our setting, recent work introduces visual puzzle tasks (e.g., jigsaw-style objectives) for VLMs posttraining [22, 69, 70]. These studies, while promising, typically cover narrow puzzle set, rely on vanilla GRPO, and offer limited analysis of training dynamics, difficulty, and generalization. Existing challenges with GRPO. growing body of work examines failure modes of CoT-enabled VLMs under GRPO-style training [34, 40]. From an optimization perspective, vanilla GRPO is largely difficulty-agnostic: when rollouts within group become homogeneous, grouprelative advantages collapse toward zero [26, 63, 67]. Sparse rewards further exacerbate this effect [12, 73]. Recent approaches address these issues via offline/online curricula or by proposing more efficient or stabilized GRPO variants [27, 35, 82]. On the interpretability side, chain-ofthought exposes phenomena such as hallucination [28, 58], perception errors [13, 19, 68], shortcutting [73], overthinking, and reasoninganswer inconsistency [56, 77]. Prior reports note that faithfulness can initially improve during GRPO but later plateau or degrade, motivating closer monitoring of post-training dynamics [11]. Complementary work proposes consistency-aware objectives or auxiliary checks to better couple the final answer with the reasoning chain [12, 29]. In our setting, we observe that vanilla GRPO on visual puzzles exhibits worsening consistency over training. We therefore track dedicated consistency metric throughout post-training, study its relationship with downstream accuracy, and find that curriculum learning improves consistency; combined with lightweight consistency-aware GRPO variant, the gains are further amplified. 3. Method This section presents PC-GRPO, our supervision-free RLVR framework for vision-centric reasoning. Building on the challenges highlighted for VLM RLVR (see Figure 2), we target four friction points that systematically degrade GRPO in the visual setting: sparse rewards from binary verifiers, flat rewards that ignore difficulty, reasoninganswer inconsistency, and the cost/bias of external verifiers and teacher models. Prior work typically tackles these in isolation. In contrast, we offer unified, supervision-free recipe that addresses them jointly. PC-GRPO has three components. (i) Verifiable puzzle rewards inspired by self-supervised pretext tasks, including graded signal for Jigsaw that grants partial credit and mit- (ii) difficulty-aware curriculum igates reward sparsity. that dynamically emphasizes medium-difficulty instances by weighting groups according to their within-group reFigure 3. An overview of our GRPO post-training framework. The process starts with input puzzles which are dynamically weighted by difficulty using curriculum learning approach. The agent iteratively generates solutions over multiple rounds. These solutions are evaluated using GRPO rewards, which in turn are used for policy evolution. We track reasoning-answer consistency during post-training and show that PC-GRPO boosts RAC and downstream performance. ward dispersion, counteracting flat rewards. (iii) consistency signal that monitors the alignment between chain-ofthought and the final <answer> throughout training, providing actionable insight into post-training dynamics; our experiments suggest that this consistency correlates with downstream performance, therefore, we design our recipe to raise it via curriculum and, when used, lightweight consistency-enforcing variants (e.g., GRPO-CARE [12]). Figure 3 illustrates the method. 3.1. PC-GRPO (I) Verifiable rewards via SSL-inspired puzzles. Motivated by classic pretext tasks in self-supervised vision, we instantiate three programmatically verifiable puzzle environments (see Figure 3): Jigsaw (assign tiles to grid positions), Rotation (predict an angle from fixed set), and PatchFit (select the masked patch among confusable candidates). Rotation and PatchFit yield binary rewards {0, 1} via exact checks; Jigsaw provides graded reward [0, 1] equal to the fraction of tiles placed correctly (partial credit) under valid-permutation constraint. We hypothesize that the partial-credit design in Jigsaw rewards intermediate progress and penalizes localized errors without requiring process-level supervision, thereby alleviating reward sparsity. We parameterize complexity by grid size for Jigsaw, angle-set cardinality for Rotation, and distractor hardness for PatchFit; to isolate the effects of graded rewards and our curriculum, we keep these complexity settings fixed in the main experiments. (II) Difficulty-aware curriculum. For each prompt x, we sample group of rollouts {oi}G i=1 with rewards {ri}. Our goal is to assign dynamic instance weight w() that emphasizes medium difficulty. Binary-reward puzzles (Rotation, PatchFit). We define difficulty as the mean success rate = = 1 (cid:88) i=1 ri [0, 1], with 1 indicating easy groups and 0 indicating hard groups. After mapping (see Equation 1), both extremes receive low weight, while balanced (medium) groups peak. Graded-reward puzzle (Jigsaw). Because multiple distinct permutations can yield the same graded reward (e.g., identical hit counts at different locations), reward dispersion alone cannot capture difficulty. We therefore adopt permutation-aware statistic: let Π(yi) denote the permutation induced by rollout yi on the grid, and define = (cid:12) (cid:12){Π(oi)}G (cid:12) i=1 (cid:12) (cid:12) (cid:12), = 1 1 [0, 1], which measures solution diversity across the group; collapsed groups have d=0, while fully diverse groups approach 1. This directly addresses the combinatorial ambiguity where equal rewards do not imply equal solutions. Curriculum weight. Following Observe-R1 [26], with fixed σ = 1.8, we map to curriculum weight w(d) = 4 σ (1 d), (1) so that w(0)=w(1)=0 and w() peaks at medium difficulty. The statistic is computed per prompt over its rollouts; if w(d)=0, the group contributes no gradient. Optimization objective. We adopt token-level GRPO objective with curriculum weighting and no KL-to-reference term (β=0), following recent guidance [12, 26] that KL anchoring can over-constrain exploration in GRPO-style posttraining: JPC-GRPO(θ) = (cid:34) 1 (cid:88) i=1 1 oi w(d(q)) (cid:124) (cid:123)(cid:122) (cid:125) curriculum (q,a)D, {oi}G i=1πθold (q) (2) (cid:16) min ρi,t ˆAi,t, ρi,t ˆAi,t (cid:17) (cid:35) . oi (cid:88) t=1 Where given ϵ > 0: Ai = ri r, ˆAi,t = Ai, ρi,t = (cid:0)oi,t q, oi,<t (cid:1) (cid:0)oi,t q, oi,<t πθ πθold (cid:1) , ρi,t = clip(cid:0)ρi,t, 1ϵ, 1+ϵ(cid:1) (III) Consistency monitoring. Because pretext reward alone is an unreliable proxy for downstream generalizationit can keep rising while reasoning quality degradeswe monitor ReasoningAnswer Consistency (RAC) as training-dynamics signal rather than selector. Concretely, after post-training we uniformly sample rollouts across the timeline and, at regular intervals, compute RAC by prompting fixed, inference-mode open-source judge (Qwen-VL-2.5-72B) on each samples rationale and final <answer> to decide whether the rationale explicitly supports the emitted answer; each trial is scored in {0, 1} and we report moving average over post-training In line with observations for vanilla GRPOan steps. early rise followed by degradation in faithfulness [11], we observe similar trend for VLM post-training with GRPO. Our hypothesis is that higher RAC during training correlates with stronger downstream performance; empirically, our Difficulty-aware curriculum mitigates the latestage decline in RAC. Moreover, we ablate lightweight consistency-enforcing add-on (GRPO-CARE [12]) to our method which further boosts RAC and improves downstream performance. This noise is widespread and in some popular benchmarks reaches about 20%, posing substantial obstacle to reliable evaluation. Although errors are more common in visioncentric and subjective tasks, we also find them in logical and mathematical questions due to label mistakes. Upon investigation, many mismatches are due to annotation noise: the models answer is verifiably correct to human, but the benchmark label is wrong. Fig. 4 illustrates common noise types: (a) Wrong Annotation, where the benchmark annotation is factually incorrect; (b) Subjective Interpretation, where question admits multiple valid answers; and (c) Insufficient Context, where the prompt is ambiguous or under-specified. Such noise compromises the reliability of benchmark-based evaluation, and we invite the community to further investigate this issue and develop practical remedies. Motivated by these findings, we conduct systematic auditing process. First, we run user studies on random subsets of three vision-centric benchmarks to estimate noise prevalence and obtain noise-free annotations. Next, we develop scalable, automated proxy for human judgment: an ensemble of VLM experts tuned to replicate human decisions. Using this proxy, we flag items whose proxy answers conflict with benchmark labels and filter them out, relying on calibrated thresholds to remove mislabeled samples with high accuracy. 4.1. User Study We audit three vision-centric benchmarks: MMStar [7], SEEDBench [37], and ColorBench [39]. For each, we uniformly sample =100 multiple-choice items drawn from general, non-expert subsets that layperson can answer. We recruit 1015 participants with diverse backgrounds to answer the sampled items. To capture ambiguity, each question with options is augmented with cannot be determined / none of the above choice, yielding O+1 options. Participants provide probability distribution over the O+1 options. For each question, we average the distributions across participants and select the option with the highest mean probability as the user answer, producing = (U1, . . . , UN ). Across the three benchmarks, we observe 10%20% label noise. Additional details of the study protocol and analysis are provided in Supplementary Section D. 4. Benchmark Auditing 4.2. Human Judgment Proxy in Leveraging the interpretability of chain of thought GRPO-trained VLMs, our supervision-free PC-GRPO, which is free from human annotation and external-verifier dependencies, exhibits useful emergent ability: it surfaces benchmark noise. We frequently observe cases where the models rationale and final answer disagree with the provided benchmark annotation yet are verifiably correct. In order to build heuristic for cleaning benchmarks, we construct committee of expert VLMs as proxy for human judgment. Naive majority voting correlates suboptimally with users, so we jointly optimize expert selection and the consensus rule. From the Vision Arena leaderboard, we form candidate pool of seven state-of-the-art VLMs: Claude Sonnet 4.5 [2], Claude Opus 4.1 [1], GemFigure 4. Examples of three major types of annotation noise in vision-centric benchmarks. User studies show that 10% 20% samples are noisy in these benchmarks. Nevertheless, our proposed method learns to produce faithful and visually-grounded answers. Left image taken from MME by Fu et al. is licensed for academic use (Source). Middle image taken from MMStar by Chen et al. is licensed under CC BY 4.0 (Source). Right image taken from MMBench by Liu et al. is licensed under Apache 2.0 (Source). ini 2.5 Pro [16], Gemini 2.5 Flash [16], GPT-5 [49], GPT4o [32], and Grok 4 Fast (reasoning) [71]. Our objective is to find subset and consensus threshold with 1 that best reproduces the user-study answers. For any configuration (S, K), we produce committee label vector = (J1, . . . , JN ), where Ji is the option agreed upon by at least models in S. We select (S , ) via grid search over all nonempty and valid K, maximizing agreement with the human labels on the audited subset. This search jointly optimizes precision and the False Omission Rate (FOR): (S , ) = argmax SM,S= 1KS LP recision + λ(1 LF OR) (3) LP recision = (cid:18) {i Ji = Gi = Ui} {i Ji = Gi} (cid:19) LF OR = (cid:18) {i Ji = Gi Ui = Gi} {i Ji = Gi} (cid:19) (4) (5) Here Ji is the committee label, Gi the benchmark label, and Ui the user-study label; λ=0.3 balances the two criteria. (cid:1): The primary objective (4) maximizes precision (cid:0) TP among items where the committee agrees with the benchmark (J=G), what fraction are truly correct (U =G)? The (cid:1): among secondary objective (5) minimizes FOR (cid:0) FN items flagged for removal (J=G), what fraction were actually correct (U =G)? Minimizing FOR avoids discarding valid, challenging examples. TN+FN TP+FP We optimize (S, K) per benchmark subset and find single configuration that performs well across all three: (S = { Claude Sonnet 4.5 [2], Gemini 2.5 Flash [16], GPT-5 [49] }, = 2). This attains LPrecision [0.95, 0.98] on the audited subsets, indicating the proxy closely matches human judgment. Further details appear in D.2; the full-scale cleaning pipeline is described in D.3. 5. Experiments We first outline implementation details (with additional specifics in B). We then analyze reasoning-answer consistency during Jigsaw post-training, ablating design choices and reporting downstream effects. After identifying the best recipe, we extend it to Rotation and PatchFit, evaluate puzzle performance, and assess transfer to general visioncentric benchmarks. Finally, motivated by 4, we report results for our models and baselines on cleaned variants of three benchmarks. 5.1. Implementation Details We train on the COCO 2014 training split (82,783 images). We choose COCO for reproducibility and to avoid introducing new data to the base models (which were already exposed to COCO during pretraining); this isolates the effect of GRPO in our setup. For each image, we synthesize one puzzle instance (Jigsaw, Rotation, or PatchFit), so, unless stated otherwise, each model sees 82,783 training instances. We apply no preprocessing or filtering. Examples appear in Figure 3 (top). Further puzzle and prompt details are in A. We initialize from Qwen-VL-2.5-Instruct checkpoints. Most ablations use the 7B model to enable broader baseline comparisons; we also train 3B variant. We adopt the public VLM-R1 implementation, and for GRPO-CARE we use the authors release, matching hyperparameters where possible. Full setup details are provided in B. (a) Reward variance (b) Reasoning-answer consistency (c) Response length (in tokens) (d) Reward score Figure 5. Tracking GRPO metrics during post-training across four puzzle environments. All charts report moving average with window size of 100 over training steps. (a) Variance among the rollout rewards (b) Consistency rate between rollout reasoning and final answer, measured by Qwen2.5-VL-72B model (c) Average numbers of tokens decoded by each trajectory (d) Reward score which is the partially graded Jigsaw solution reward. Model Baselines Qwen 2.5 VL [3] ViCrit [66] Vision-Zero [64] Visual Jigsaw [70] VisualSphinx [22] GRPO-CARE [12] Our variants Jigsaw Jigsaw + CARE Jigsaw + CL Jigsaw + CL + CARE PatchFit + CL + CARE Rotation + CL + CARE Mix + CL + CARE MME [23] MMStar [7] POPE [38] MMT-Bench [78] CV-Bench-2D [59] MMVP [60] ColorBench [39] Lisa-Grounding [36] SEED-Bench [37] 2242.52 2167.36 2247.93 2243.09 2296.09 2351.53 2339.72 2389.23 2315.11 2393.07 2315.92 2357.38 2358.85 64.67 62.27 63.47 62.53 63.20 64.13 64.47 65.80 63.27 63.60 59.87 64.60 65.20 81.77 80.50 82.67 84.78 83.93 88.18 86.17 86.95 84.35 85.02 85.05 87.36 85. 59.64 59.83 60.47 57.76 60.63 62.62 59.96 61.18 62.62 62.52 58.94 61.91 62.65 73.62 70.84 73.53 74.46 73.98 74.91 72.13 77.76 75.37 75.52 73.37 75.08 76.96 77.67 75.33 78.00 76.33 77.33 80.33 76.33 77.00 74.67 78.67 78.00 79.67 78. 37.38 39.06 35.81 40.21 39.90 35.39 37.07 42.20 41.26 39.69 34.66 40.73 40.84 72.86 63.20 73.82 75.69 73.28 74.55 74.0 73.72 73.28 72.62 64.50 74.61 70.91 75.48 76.64 75.45 74.80 75.47 76.36 77.01 75.34 74.64 75.17 73.97 76.05 76. Table 1. Performance on vision-centric benchmarks with 7B baselines. Jigsaw and Rotation setups within our PC-GRPO framework outperform other annotation-free baselines; which indicates the impact of our curriculum training and the importance of reasoning-answer consistency. CL denotes our curriculum learning. 5.2. Reasoning-Answer Consistency We analyze the dynamics of reasoning-answer consistency during Jigsaw post-training. Our hypothesis is that, for reasonably capable model, higher consistency between the rationale and the final answer indicates greater faithfulness. We study four variants: vanilla GRPO, +curriculum, +GRPO-CARE, and +curriculum+GRPO-CARE. We track four metrics over training: reward variance, response length, reward, and RAC (Figure 5). Vanilla GRPO shows the largest late-stage inconsistency: near the end of training, answers converge while rationales diverge, reducing RAC. All variants exhibit an initial rise in RAC, consistent with LLM findings [11]. Adding our difficulty-aware curriculum attenuates the late-stage decline. Incorporating consistency-enforcing scheme [12], which scores the reasoning-answer alignment via an EMA judge, further improves RAC. The combined recipe (curriculum + GRPO-CARE) attains the highest RAC throughout most of training. Nonetheless, RAC typically decreases toward the final steps, suggesting that the later checkpoints are often suboptimal due to over-optimization on the puzzle environment. 5.3. Main Results by the extend analysis above, we Guided the curriculum+GRPO-CARE recipe to Rotation and PatchFit. Initial results suggest that different puzzle environments cultivate different skills, so we also train on mixed setting to encourage broader transfer. Concretely, we sample 40K training instances (15K Jigsaw, 15K PatchFit, 10K Rotation) and continue post-training. We compare against six recent baselines (including Qwen base) on eight vision-centric benchmarks. Table 1 reports 7B results; the 3B variant appears in C. We use VLMEvalKit [20] for all benchmarks except LISA-Grounding [36] and ColorBench [39], which follow their official evaluators. Unless noted, all models are run in thinking mode (thinkanswer); direct-answer results are in the supplement. Findings. Table 1 highlights three trends: Curriculum and consistency-enforcing (GRPO-CARE) reliably improve over vanilla GRPO, with the combined recipe yielding the largest and most stable gains across benchmarks. Puzzle environments transfer differently: Rotation often matches or exceeds Jigsaw on spatial/perceptual tasks; PatchFit transfers poorly despite higher apparent difficulty. mixed-puzzle recipe mitigates specialization and improves average transfer. Despite using no human labels or external verifiers during training, PC-GRPO outperforms the Qwen baseline and other supervision-free or gamified post-training methods on most benchmarks, and is competitive with GRPOCARE variants trained on human-annotated data. 5.4. Performance on Puzzles Motivated by the reported weakness of LLMs/VLMs on puzzle environments [45, 57], we evaluate how well our puzzle training improves model performance and whether the learned skills transfer across puzzle types. To create the test set, we randomly select 1000 samples from the test images of COCO2014 and create Jigsaw, PatchFit, and Rotation puzzles. As shown in Table 2, we clearly observe the challenges of reasoning models with our setup as well. Specially, we can see that post-training on puzzle environment improves performance on that one, but the gains do not transfer to other puzzle setups, but the performance even degrades compared to the Qwen baseline, mixture of environments alleviates this problem, however, there is still no reliable way to indicate whether learning one skills transfers to others. Model Qwen 2.5 VL 7B 25.59 25.55 Rotation 17.88 PatchFit 36.65 Jigsaw 36.83 Mix Jigsaw PatchFit Rotation 21.2 20.9 62.6 21.7 48.6 53.3 70.8 51.8 33.9 83.2 Table 2. Evaluating the inter-puzzle transferability of our 3 puzzle environments. Training on specific puzzle consistently improves performance; however, gains in puzzle does not transfer to others. mixed-puzzle training provides strong gains across all tasks. Model Removed Samples Qwen2.5 VL [3] ViCrit [66] Vision-Zero [64] Visual Jigsaw [70] VisualSphinx [22] GRPO-CARE [12] Our variants Jigsaw Jigsaw + CARE Jigsaw + CL + CARE Mix + CL + CARE MMStar [7] MME [23] ColorBench [39] SEED-Bench [37] 21.7% 76.61 73.12 74.46 74.11 75.27 75.36 74.46 77.59 74.82 76.34 15.3% 2442.58 2341.19 2458.85 2385.70 2494.79 2516.54 2524.95 2565.72 2554.78 2556.54 16.8% 51.12 52.35 46.01 55.21 53.99 48. 50.31 57.87 54.60 57.46 21.5% 87.15 88.30 87.76 86.22 87.27 88.58 88.64 87.11 87.10 88.31 Table 3. Evaluation results on the cleaned benchmarks. CL refers to curriculum learning. Our variants show competitive or improved performance compared to strong baselines. 5.5. Performance on Cleaned Benchmarks Motivated by 4, we apply our optimized auditing proxy (S = { Claude Sonnet 4.5 [2], Gemini 2.5 Flash [16], GPT-5 [49] }, = 2) to three benchmarks (MME [23], ColorBench [39], and MMStar [7]) and filter items where the proxy disagrees with the benchmark label. We then evaluate our models and baselines on these cleaned subsets. Table 3 reports the filtered fractions and post-cleaning scores. Compared to Table 1, most GRPO-based models improve on the cleaned versions, consistent with the removal of mislabeled items and greater concentration of informative cases. The proxy is used strictly for auditing. We plan to scale this cleaning to full benchmark releases. 6. Conclusion We presented Puzzle Curriculum GRPO, supervisionfree recipe for RL with verifiable rewards that strengthens visual reasoning in VLMs without supervised finetuning or external verifiers. We defined ReasoningAnswer Consistency (RAC) and tracked it using an open-source VLM, observed RAC decline under vanilla GRPO, and showed that curriculum and consistency training improve RAC and downstream performance. Finally, we highlighted pervasive benchmark noise and proposed practical auditing/cleaning remedies aided by stronger VLM auditors; we will release cleaned subsets to support future evaluation."
        },
        {
            "title": "References",
            "content": "[1] Anthropic. System Card: Claude Opus 4.1. Technical report, 2025. 5 [2] Anthropic. System Card: Claude Sonnet 4.5. Technical report, 2025. 5, 6, 8 [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-VL Technical Report, 2025. 3, 7, 8, 4 [4] Sule Bai, Mingxing Li, Yong Liu, Jing Tang, Haoji Zhang, Lei Sun, Xiangxiang Chu, and Yansong Tang. UniVG-R1: Reasoning Guided Universal Visual Grounding with Reinforcement Learning, 2025. [5] Yuzheng Cai, Siqi Cai, Yuchen Shi, Zihan Xu, Lichao Chen, Yulei Qin, Xiaoyu Tan, Gang Li, Zongyi Li, Haojia Lin, Yong Mao, Ke Li, and Xing Sun. Training-Free Group Relative Policy Optimization, 2025. 3 [6] Hardy Chen, Haoqin Tu, Fali Wang, Hui Liu, Xianfeng Tang, Xinya Du, Yuyin Zhou, and Cihang Xie. SFT or RL? An Early Investigation into Training R1-Like Reasoning Large Vision-Language Models, 2025. 1, 3 [7] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, and Feng Zhao. Are We on the Right Way for Evaluating Large Vision-Language Models?, 2024. 5, 7, 8, 3, 4, 6 [8] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are We on the Right Way for Evaluating Large Vision-Language Models? Advances in Neural Information Processing Systems, 37:2705627087, 2024. 3 [9] Lili Chen, Mihir Prabhudesai, Katerina Fragkiadaki, Hao Liu, and Deepak Pathak. Self-questioning Language Models. arXiv preprint arXiv:2508.03682, 2025. 1 [10] Lili Chen, Mihir Prabhudesai, Katerina Fragkiadaki, Hao Liu, and Deepak Pathak. Self-Questioning Language Models, 2025. 3 [11] Yanda Chen, Joe Benton, Ansh Radhakrishnan, Jonathan Uesato, Carson Denison, John Schulman, Arushi Somani, Peter Hase, Misha Wagner, Fabien Roger, Vlad Mikulik, Samuel R. Bowman, Jan Leike, Jared Kaplan, and Ethan Perez. Reasoning Models Dont Always Say What They Think, 2025. 2, 3, 5, 7, [12] Yi Chen, Yuying Ge, Rui Wang, Yixiao Ge, Junhao Cheng, Ying Shan, and Xihui Liu. GRPO-CARE: ConsistencyAware Reinforcement Learning for Multimodal Reasoning, 2025. 2, 3, 4, 5, 7, 8, 1 [13] Yan Chen, Long Li, Teng Xi, Long Zeng, and Jingdong Wang. Perception Before Reasoning: Two-Stage Reinforcement Learning for Visual Reasoning in Vision-Language Models, 2025. 3 [14] Yang Chen, Yufan Shen, Wenxuan Huang, Sheng Zhou, Qunshu Lin, Xinyu Cai, Zhi Yu, Jiajun Bu, Botian Shi, and Yu Qiao. Learning Only with Images: Visual Reinforcement Learning with Reasoning, Rendering, and Visual Feedback, 2025. 2, 3 [15] Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc V. Le, Sergey Levine, and Yi Ma. SFT Memorizes, RL Generalizes: Comparative Study of Foundation Model Post-training, 2025. 3 [16] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities, 2025. 6, 8 [17] Muzhi Dai, Chenxu Yang, and Qingyi Si. S-GRPO: Early Exit via Reinforcement Learning in Reasoning Models. arXiv preprint arXiv:2505.07686, 2025. 2 [18] Yihe Deng, Hritik Bansal, Fan Yin, Nanyun Peng, Wei Wang, and Kai-Wei Chang. OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning via Iterative Self-Improvement. 2025. [19] Yizhuo Ding, Mingkang Chen, Zhibang Feng, Tong Xiao, Wanying Qu, Wenqi Shao, and Yanwei Fu. VTPerceptionR1: Enhancing Multimodal Reasoning via Explicit Visual and Textual Perceptual Grounding, 2025. 3 [20] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for In Proceedings evaluating large multi-modality models. of the 32nd ACM international conference on multimedia, pages 1119811201, 2024. 8 [21] Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Junfei Wu, Xiaoying Zhang, Video-R1: ReinBenyou Wang, and Xiangyu Yue. arXiv preprint forcing Video Reasoning in MLLMs. arXiv:2503.21776, 2025. 3 [22] Yichen Feng, Zhangchen Xu, Fengqing Jiang, Yuetai Li, Bhaskar Ramasubramanian, Luyao Niu, Bill Yuchen Lin, and Radha Poovendran. VisualSphinx: Large-Scale Synthetic Vision Logic Puzzles for RL, 2025. 3, 7, 8, 4 [23] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. MME: comprehensive evaluation benchmark for multimodal large language models. In The Thirtyninth Annual Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2025. 7, 8, 4, 12 [24] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised Representation Learning by Predicting Image Rotations. 2018. [25] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv preprint arXiv:2501.12948, 2025. 1, 3 [26] Zirun Guo, Minjie Hong, and Tao Jin. Observe-R1: Unlocking Reasoning Abilities of MLLMs with Dynamic Progressive Reinforcement Learning, 2025. 3, 4, 5 [27] Hasan Abed Al Kader Hammoud, Kumail Alhamoud, Abed Hammoud, Elie Bou-Zeid, Marzyeh Ghassemi, and Bernard Ghanem. Train Long, Think Short: Curriculum Learning for Efficient Reasoning, 2025. 3 [28] Mingfei Han, Haihong Hao, Jinxing Zhou, Zhihui Li, Yuhui Zheng, Xueqing Deng, Linjie Yang, and Xiaojun Chang. Self-Consistency as Free Lunch: Reducing Hallucinations in Vision-Language Models via Self-Reflection, 2025. 3 [29] Minbin Huang, Runhui Huang, Chuanyang Zheng, Jingyao Li, Guoxuan Chen, Han Shi, and Hong Cheng. AnswerConsistent Chain-of-thought Reinforcement Learning For Multi-modal Large Langauge Models, 2025. 2, 3 [30] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Incentivizing Reasoning Capability in Lin. Vision-R1: arXiv preprint Multimodal Large Language Models. arXiv:2503.06749, 2025. 1, 2, 3 [31] Drew Hudson and Christopher Manning. GQA: New Dataset for Real-world Visual Reasoning and Compositional Question Answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 67006709, 2019. 3, [32] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. GPT-4o System Card, 2024. 6 [33] Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan Jin, Claire Guo, Shen Yan, Bo Zhang, Chaoyou Fu, Peng Gao, and Hongsheng Li. MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for Reasoning Quality, Robustness, and Efficiency, 2025. 3 [34] Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan Jin, Claire Guo, Shen Yan, Bo Zhang, Chaoyou Fu, Peng Gao, and Hongsheng Li. MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for Reasoning Quality, Robustness, and Efficiency, 2025. 3 [35] Guochao Jiang, Wenfeng Feng, Guofeng Quan, Chuzhan Hao, Yuewei Zhang, Guohua Liu, and Hao Wang. VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models, 2025. 3 [36] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 95799589, 2023. 7, 8, 1, 4, 14 [37] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. SEED-Bench: Benchmarking Multimodal Large Language Models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1329913308, 2024. 5, 7, 8, 3, 4, 6, [38] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating Object Hallucination in Large Vision-Language Models. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. 7, 4, 13 [39] Yijun Liang, Ming Li, Chenrui Fan, Ziyue Li, Dang Nguyen, Kwesi Cobbina, Shweta Bhardwaj, Jiuhai Chen, Fuxiao Liu, and Tianyi Zhou. ColorBench: Can VLMs See and Understand the Colorful World? Comprehensive Benchmark for Color Perception, Reasoning, and Robustness, 2025. 5, 7, 8, 1, 3, 4, 6 [40] Yuan-Hong Liao, Sven Elflein, Liu He, Laura Leal-Taixe, Yejin Choi, Sanja Fidler, and David Acuna. LongPerceptualThoughts: Distilling System-2 Reasoning for System-1 Perception, 2025. 3 [41] Zhihang Lin, Mingbao Lin, Yuan Xie, and Rongrong Ji. CPPO: Accelerating the Training of Group Relative Policy Optimization-Based Reasoning Models, 2025. 3 [42] Yuqi Liu, Bohao Peng, Zhisheng Zhong, Zihao Yue, Fanbin Lu, Bei Yu, and Jiaya Jia. Seg-Zero: Reasoning-Chain Guided Segmentation via Cognitive Reinforcement, 2025. 3 [43] Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding R1-Zero-Like Training: Critical Perspective, 2025. 3 [44] Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. VisualRFT: Visual Reinforcement Fine-Tuning. 2025. 1 [45] Zesen Lyu, Dandan Zhang, Wei Ye, Fangdi Li, Zhihang Jiang, and Yao Yang. Jigsaw-puzzles: From seeing to understanding to reasoning in vision-language models. arXiv preprint arXiv:2505.20728, 2025. [46] Ahmed Masry, Do Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: Benchmark for Question Answering about Charts with Visual and Logical Reasoning. In Findings of the Association for Computational Linguistics: ACL 2022, pages 22632279, Dublin, Ireland, 2022. Association for Computational Linguistics. 3, 11 [47] Ishan Misra and Laurens van der Maaten. Self-Supervised Learning of Pretext-Invariant Representations. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 67076717, 2020. 2 [48] Mehdi Noroozi and Paolo Favaro. Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles. In European Conference on Computer Vision, pages 6984. Springer, 2016. 2 [49] OpenAI. GPT-5 System Card. Technical report, 2025. 6, 8 [50] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training Language Models to Follow Instructions with Human Feedback, 2022. 3 [51] Mihir Prabhudesai, Lili Chen, Alex Ippoliti, Katerina Fragkiadaki, Hao Liu, and Deepak Pathak. Maximizing Confidence Alone Improves Reasoning, 2025. 3 [52] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct Preference Optimization: Your Language Model is Secretly Reward Model, 2024. [53] Rulin Shao, Shuyue Stella Li, Rui Xin, Scott Geng, Yiping Wang, Sewoong Oh, Simon Shaolei Du, Nathan Lambert, Sewon Min, Ranjay Krishna, Yulia Tsvetkov, Hannaneh Hajishirzi, Pang Wei Koh, and Luke Zettlemoyer. Spurious Rewards: Rethinking Training Signals in RLVR, 2025. 3 [54] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models, 2024. 1, 3 [55] Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, Ruochen Xu, and Tiancheng Zhao. VLMR1: Stable and Generalizable R1-style Large VisionLanguage Model, 2025. 1, 3, 4 [56] Si Shen, Peijun Shen, Wenhua Zhao, and Danhao Zhu. Mitigating Think-Answer Mismatch in LLM Reasoning Through Noise-Aware Advantage Reweighting. 2025. 2, 3 [57] Parshin Shojaee, Iman Mirzadeh, Keivan Alizadeh, Maxwell Horton, Samy Bengio, and Mehrdad Farajtabar. The illusion of thinking: Understanding the strengths and limitations of reasoning models via the lens of problem complexity, 2025. 8 [58] Zhongxiang Sun, Qipeng Wang, Haoyu Wang, Xiao Zhang, and Jun Xu. Detection and Mitigation of Hallucination in Large Reasoning Models: Mechanistic Perspective, 2025. [59] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, Austin Wang, Rob Fergus, Yann LeCun, and Saining Xie. Cambrian-1: Fully Open, Vision-Centric Exploration of Multimodal LLMs. In Proceedings of the 38th International Conference on Neural Information Processing Systems, Red Hook, NY, USA, 2024. Curran Associates Inc. 7, 4 [60] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs, 2024. 7, 4 [61] Songjun Tu, Qichao Zhang, Jingbo Sun, Yuqian Fu, Linjing Li, Xiangyuan Lan, Dongmei Jiang, Yaowei Wang, and Dongbin Zhao. Perception-Consistency Multimodal Large Language Models Reasoning via Caption-Regularized Policy Optimization, 2025. 2 [62] Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, and Wenhu Chen. VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models with Reinforcement Learning. 2025. 1, 2, 3 [63] Qinsi Wang, Jinghan Ke, Hancheng Ye, Yueqian Lin, Yuzhe Fu, Jianyi Zhang, Kurt Keutzer, Chenfeng Xu, and Yiran Chen. Angles Dont Lie: Unlocking Training-Efficient RL Through the Models Own Signals, 2025. 3 [64] Qinsi Wang, Bo Liu, Tianyi Zhou, Jing Shi, Yueqian Lin, Yiran Chen, Hai Helen Li, Kun Wan, and Wentian Zhao. Vision-Zero: Scalable VLM Self-Improvement via Strategic Gamified Self-Play, 2025. 3, 7, 8, 4 [65] Xiyao Wang, Chunyuan Li, Jianwei Yang, Kai Zhang, Bo Liu, Tianyi Xiong, and Furong Huang. LLaVA-Critic-R1: Your Critic Model is Secretly Strong Policy Model, 2025. [66] Xiyao Wang, Zhengyuan Yang, Chao Feng, Yongyuan Liang, Yuhang Zhou, Xiaoyu Liu, Ziyi Zang, Ming Li, Chung-Ching Lin, Kevin Lin, Linjie Li, Furong Huang, and Lijuan Wang. ViCrit: Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs, 2025. 2, 3, 7, 8, 4 [67] Zhenting Wang, Guofeng Cui, Yu-Jhe Li, Kun Wan, and Wentian Zhao. DUMP: Automated Distribution-Level Curriculum Learning for RL-based LLM Post-training, 2025. 3 [68] Zhenhailong Wang, Xuehang Guo, Sofia Stoica, Haiyang Xu, Hongru Wang, Hyeonjeong Ha, Xiusi Chen, Yangyi Chen, Ming Yan, Fei Huang, and Heng Ji. Perception-Aware Policy Optimization for Multimodal Reasoning, 2025. 2, 3 [69] Zifu Wang, Junyi Zhu, Bo Tang, Zhiyu Li, Feiyu Xiong, Jiaqian Yu, and Matthew B. Blaschko. Jigsaw-R1: Study of Rule-based Visual Reinforcement Learning with Jigsaw Puzzles, 2025. 2, 3, 4 [70] Penghao Wu, Yushan Zhang, Haiwen Diao, Bo Li, Lewei Lu, and Ziwei Liu. Visual Jigsaw Post-Training Improves MLLMs, 2025. 1, 2, 3, 7, 8, 4 [71] xAI. Grok 4 Fast Model Card. Technical report, 2024. 6 [72] Jiaer Xia, Yuhang Zang, Peng Gao, Sharon Li, and Kaiyang Zhou. Visionary-R1: Mitigating Shortcuts in Visual Reasoning with Reinforcement Learning. 2025. 2 [73] Jiaer Xia, Yuhang Zang, Peng Gao, Sharon Li, and Kaiyang Zhou. Visionary-R1: Mitigating Shortcuts in Visual Reasoning with Reinforcement Learning, 2025. [74] Wenyi Xiao and Leilei Gan. Fast-Slow Thinking GRPO for Large Vision-Language Model Reasoning. 2025. 2 [75] Guowei Xu, Peng Jin, Ziang Wu, Hao Li, Yibing Song, Lichao Sun, and Li Yuan. LLaVa-CoT: Let Vision Language Models Reason Step-by-Step. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 20872098, 2025. 2 [76] Yue Yang, Shuibai Zhang, Wenqi Shao, Kaipeng Zhang, Yi Bin, Yu Wang, and Ping Luo. Dynamic Multimodal Evaluation with Flexible Complexity by Vision-Language Bootstrapping. 2025. 3 [77] Zijun Yao, Yantao Liu, Yanxu Chen, Jianhui Chen, Junfeng Fang, Lei Hou, Juanzi Li, and Tat-Seng Chua. Are Reasoning Models More Prone to Hallucination?, 2025. 2, 3 [78] Kaining Ying, Fanqing Meng, Jin Wang, Zhiqian Li, Han Lin, Yue Yang, Hao Zhang, Wenbo Zhang, Yuqi Lin, Shuo Liu, Jiayi Lei, Quanfeng Lu, Runjian Chen, Peng Xu, Renrui Zhang, Haozhe Zhang, Peng Gao, Yali Wang, Yu Qiao, Ping Luo, Kaipeng Zhang, and Wenqi Shao. MMT-bench: comprehensive multimodal benchmark for evaluating large vision-language models towards multitask AGI. In Proceedings of the 41st International Conference on Machine Learning, pages 5711657198. PMLR, 2024. 7, 4 [79] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. DAPO: An Open-Source LLM Reinforcement Learning System at Scale, 2025. [80] Jixiao Zhang and Chunsheng Zuo. GRPO-LEAD: Difficulty-Aware Reinforcement Learning Approach for Concise Mathematical Reasoning in Language Models, 2025. 2 [81] Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu, Xikun Zhang, Shijian Lu, and Dacheng Tao. R1-VL: Learning to Reason with Multimodal Large Language Models via Step-wise Group Relative Policy Optimization, 2025. 1 [82] Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, Jingren Zhou, and Junyang Lin. Group Sequence Policy Optimization, 2025. 3 [83] Jingyu Zhou, Lu Ma, Hao Liang, Chengyu Shen, Bin Cui, and Wentao Zhang. DARO: Difficulty-Aware Reweighting Policy Optimization, 2025. 2 [84] Yuxin Zuo, Kaiyan Zhang, Li Sheng, Shang Qu, Ganqu Cui, Xuekai Zhu, Haozhan Li, Yuchen Zhang, Xinwei Long, Ermo Hua, Biqing Qi, Youbang Sun, Zhiyuan Ma, Lifan Yuan, Ning Ding, and Bowen Zhou. TTRL: Test-Time Reinforcement Learning, 2025. 3 Puzzle Curriculum GRPO for Vision-Centric Reasoning"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Additional Discussion and Details on our"
        },
        {
            "title": "Puzzles",
            "content": "Measuring the intrinsic complexity of visual puzzle is nontrivial. In practice, we expose single difficulty knob per puzzle type. For Rotation, the knob is the cardinality of the angle set; in our experiments we fix it to {0, 90, 180, 270} and consider standard variations such as clockwise vs. counterclockwise phrasing. For PatchFit, the knob is distractor hardness: given ground-truth patch, we sample {3, 5, 7} decoys drawn from mirror/rotation/color perturbations of the true patch or visually similar patches from other regions. For Jigsaw, difficulty is controlled by grid size: given an grid, we allow any integer pair with 2 9 (i.e., up to 33). Sampling is uniform over the chosen configurations. 4 , 1 Under random guessing, the success rates are as follows. Rotation: 1/4 = 25%. PatchFit: averaging over 6 , 1 {3, 5, 7} decoys yields an expected success of 1 8 respectively, i.e., 18% on average. Jigsaw: with graded reward defined as the fraction of tiles placed correctly, the expected score under random permutation depends on ; in our sampling it averages to 26% (grid-size dependent). Exact puzzle-generation details will be provided in the released code. Figure S1 shows Jigsaw training sample, Figure S2 Rotation sample, and Figure S3 PatchFit sample. B. Experimental Setup We conduct all GRPO post-training on 8A100 (80 GB) GPUs. Training on 82,783 samples takes approximately 100 hours, while the mixed setting with 40,000 samples takes approximately 48 hours. Frameworks and variants. We use VLM-R1 [55] vanilla GRPO and GRPO+curriculum, for and GRPO-CARE [12] the consistency-enhanced variant. for Hyperparameters. Unless noted, we follow VLM-R1 defaults with two changes: KL coefficient β=0 and learning rate 5107. With VLM-R1 we use batch size 16; with GRPO-CARE batch size 8. We train for 1 epoch. Maximum decoding length is 2048 tokens. Each prompt uses G=8 rollouts with temperature 0.9 and one iteration per update. We use bfloat16 and cap vision-encoder tokens at 1024 during post-training. We set the PPO clipping parameter ϵ=0.2 following VLM-R1. GRPO-CARE specifics. We adopt the authors defaults: ref ema decay 0.995, EMA update every 10 steps, bonus coefficient 0.5, confidence upper bound 0.95, and consistency margin 0.01. For GRPO-CARE we use ϵ=0. B.1. Evaluation Setup We evaluate primarily with VLMEvalKit. LISAGrounding [36] and ColorBench [39] follow their official evaluators. Unless specified, results are obtained in thinking mode: we append standardized prompt to induce thinkanswer formatting and use Qwen-VL-2.5-72B within VLMEvalKit for post-processing and format checking. The base prompt is: First output the thinking process in <think></think> tags and then output the final answer in <answer></answer> tags. For LISA-Grounding, we additionally require boundingbox format: First output the thinking process in <think></think> tags and then output the final answer in <answer></answer> tags. Only put the bounding box as [x1,y1,x2,y2] between the <answer></answer> tags. Direct-answer results (no think tags) for the 7B models are reported in C. B.2. RAC Measurement and Checkpoint Reporting To measure RAC, after post-training we uniformly sample rollouts across the training timeline and, at regular intervals, query fixed, inference-mode open-source judge (QwenVL-2.5-72B) on each rationale and final <answer> to determine whether the rationale explicitly supports the answer; each trial is scored in [0, 1]. The prompt used for the judge appears in Fig. S4. Observed patterns and usage. Figure 5b summarizes RAC dynamics under vanilla GRPO, +curriculum, +CARE, and +curriculum+CARE on Jigsaw. Consistent with LLM findings [11], we observe an early rise in faithfulness; in our VLM setting, RAC later declines for vanilla GRPO, while curriculum mitigates this decline and CARE further raises RAC. In practice, we treat RAC as diagnostic signal rather than strict selector: higher RAC tends to align with better downstream accuracy, and intermediate checkpoints near local RAC peaks often perform strongly Figure S1. Example Jigsaw puzzle used in PC-GRPO training. The image taken from the Microsoft COCO dataset by Lin et al. is licensed under CC BY 4.0. Source: https://cocodataset.org/. Figure S2. Example Rotation puzzle used in PC-GRPO training. The image taken from the Microsoft COCO dataset by Lin et al. licensed under CC BY 4.0. Source: https://cocodataset.org/. is Figure S3. Example PatchFit puzzle used in PC-GRPO training. The image taken from the Microsoft COCO dataset by Lin et al. licensed under CC BY 4.0. Source: https://cocodataset.org/. is on downstream tasks. However, formalizing checkpoint selection solely from RAC (or any single training-time signal) remains nontrivial; we leave principled selection rules to future work. our supervision-free models achieve large gains relative to VLM-R1, which relies on human-annotated visual grounding data; surprisingly, our Jigsaw variant also exceeds VLM-R1 on LISA-Grounding. C. Additional Results We provide two additional result sets. (1) Direct-mode inference (7B). Following recent work (e.g., MME-CoT [33] and Jigsaw-R1 [69]) that reports both direct-mode and Chain-of-Thought (CoT) performance, we evaluate the 7B Qwen-VL-2.5 variants in direct mode. Unlike Table 1, where all methods are evaluated under thinkanswer prompt, here models are prompted to produce only the final answer in single token or short phrase by appending: Please answer the question directly without reasoning. Results appear in Table S1. Consistent with prior observations on Qwen baselines and post-trained variants, some benchmarks exhibit higher direct-mode accuracy than CoT. Explaining this gap remains an active research topic; nonetheless, our models are competitive with or outperform baselines across vision-centric tasks. (2) CoT evaluation (3B). We also report CoT results for the 3B model in Table S2, using the same thinkanswer scheme as in the main table. Our models substantially outperform the Qwen 3B baseline, and the Jigsaw variant surpasses Jigsaw-R1, highlighting the benefits of our curriculumand consistency-aware post-training. Notably, D. Benchmark Auditing Details D.1. User Study Analysis Here we analyze the user study statistics. sample of the user study GUI is shown in Figure S6. We define the real noise ratio αnoise as the disagreement ratio between average user answer and benchmark annotation as {iUi!=Gi} . For the MMStar [7] subset on which we conducted the user study, αnoise = 16%. For the SEEDBench [37] subset, αnoise = 21%. For the ColorBench [39] subset, αnoise = 9%. Examples of the noisy annotations can be found in Figure S7 for MMStar [7] and Figure S8 for SEEDBench [37]. While our user studies are limited to small subset of vision benchmarks, our empirical findings reveal that this phenomenon is widespread in vision-centric benchmarks. We include some noise examples for GQA [31] in Figure S9 and ChartQA [46] in Figure S10. We also invite the community to further analyze and improve the existing benchmarks. D.2. Human Judgment Proxy the = Using {Claude Sonnet 4.5, Gemini 2.5 Flash, GPT-5}, = 2)), we get LP recision = 0.98 and LF OR = 0.63 on the 100optimized setup ((S Model Baselines Qwen 2.5 VL [3] ViCrit [66] Vision-Zero [64] Visual Jigsaw [70] VisualSphinx [22] GRPO-CARE [12] Our variants Jigsaw Jigsaw + CARE Jigsaw + CL Jigsaw + CL + CARE PatchFit + CL + CARE Rotation + CL + CARE Mix + CL + CARE MME [23] MMStar [7] POPE [38] MMT-Bench [78] CV-Bench-2D [59] MMVP [60] ColorBench [39] 2308.21 2178.64 2306.41 2313.23 2341.13 2355.35 2371.42 2350.09 2373.97 2348.20 2336.85 2364.87 2370.72 63.27 64.13 63.53 64.13 63.73 63.93 64.00 64.87 64.07 64.33 63.93 63.73 64.73 86.37 85.96 85.92 86.37 86.34 86. 86.96 86.62 86.28 86.05 86.53 85.87 86.65 62.74 62.58 63.06 62.20 62.65 63.58 63.77 63.26 63.38 63.70 61.82 63.61 64.09 76.29 76.59 76.00 76.47 76.76 75.81 77.33 77.28 76.38 76.12 76.25 75.94 77.36 78.33 76.33 77.67 79.00 77.33 78. 78.00 77.00 76.67 77.33 78.33 78.00 78.67 46.60 47.12 45.45 46.07 45.65 46.07 45.76 45.65 45.55 46.91 46.60 45.34 45.03 Table S1. Performance of PC-GRPO variants and other 7B baselines on vision-centric benchmarks under direct-mode prompt that requests single-letter or single-word answer, without explicit CoT. Model Baselines Qwen 2.5 VL [3] VLM-R1 [55] Jigsaw-R1 [69] Our variants Jigsaw Mix MME [23] MMStar [7] POPE [38] MMT-Bench [78] CV-Bench-2D [59] MMVP [60] ColorBench [39] Lisa-Grounding [36] SEED-Bench [37] 2179.83 2207.17 2184.29 2222.55 2127.45 54.73 55.20 55.53 55.40 57. 77.41 79.66 78.05 78.68 77.30 53.31 52.06 57.53 57.88 57.72 65.62 66.65 70.87 71.33 72. 63.33 69.67 69.66 68.67 69.00 31.51 36.02 32.25 36.96 36.02 61.52 67.79 61.28 68.21 67. 70.78 70.38 72.46 71.90 71.85 Table S2. Qwen-VL-2.5 3B: comparison between our PC-GRPO variants and baselines under CoT prompting. Our approach yields consistent improvements and indicates scalability across model sizes. sample subset in MMStar [7]. For the SEEDBench [37] subset, LP recision = 0.95 and LF OR = 0.37. For the ColorBench [39] subset, LP recision = 0.98 and LF OR = 0.59. On the contrast, if we adopt the naive setup that take the majority voting among the entire candidate pool, (S = { Claude Sonnet 4.5, Claude Opus 4.1, Gemini 2.5 Pro, Gemini 2.5 Flash, GPT-5, GPT-4o, Grok 4 Fast (reasoning) }, = 4), we get suboptimal precision and FOR tradeoff: LP recision = 0.96 and LF OR = 0.66 on the MMStar subset. The optimization provides the statistical confidence needed to use this setup to clean the benchmarks at full-scale. Details of the agreement among proxy, user and benchmark annotation are shown in Figure S5. Note that the Proxy-User-Annotation Match ( {iJi=Gi=Ui} ) is very close to Proxy-Annotation Match ( {iJi=Gi} ) for all three benchmarks. This indicates that the LP recision is very high for the optimized proxy. We also find that the User-Annotation Match ( {iGi=Ui} ) is generally higher than Proxy-User Match ( {iJi=Ui} indicating users agree with the original annotation more than the proxy. This is expected since the original annotation is labeled by human as well. Nevertheless, this observation does not J J ), Benchmark # Samples # Removed Samples MME [23] SEEDBench [37] ColorBench [39] MMStar [7] 2,374 14,215 5,885 1,500 514 3,062 991 380 αnoise 21.7% 21.5% 16.8% 15.3% Table S3. Statistics of the full-scale benchmark cleaning. undermine the effectiveness of the proxy since the objective of the proxy is to remove noise annotations, rather than fixing them. D.3. Benchmark Cleaning To clean the benchmarks at full-scale, we identified those questions where the proxy answer disagree with the benchmark annotation and remove them. The proxy noise ratio is defined as αnoise = {iJi!=Gi} . In Table S3, we show αnoise for each of the benchmarks. Note that the proxy noise ratio αnoise and the real noise ratio αnoise are very close for MMStar and SEEDBench, indicating this optimized proxy aligns with human judgment well. Figure S4. Prompt for measuring Reasoning-Answer Consistency (RAC). (a) MMStar [7] (b) SEEDBench [37] (c) ColorBench [39] Figure S5. Validating the Human Judgment Proxy and Quantifying Benchmark Noise. The charts compare the agreement rates between our optimized proxy, aggregated human user judgments, and the original annotations for MMStar [7], SEEDBench [37], and ColorBench [39]. The notable disagreement between user judgments and annotations, αnoise can be calculated as 100% User-Annotation Match, highlighting significant annotation noise. The Proxy-User-Annotation Match is also very close to Proxy-Annotation Match for all three benchmarks, indicating that the LP recision is very high for the optimized proxy. Figure S6. The User Study Interface for Benchmark Auditing. Participants were shown an image and question and asked to provide an answer as probability distribution across the available choices. By using sliders to allocate percentages, users could express nuanced confidence. The interface includes None of the above / cannot decide option to explicitly capture ambiguity in the benchmarks. Image taken from the SEED-Bench dataset by Li et al. is licensed under CC BY-NC 4.0. Source: https://github.com/AILabCVC/SEED-Bench. (a) What is the overall theme of the image? A: Beach vacation, B: Athletic lifestyle, C: Summer fashion, D: Urban street style Benchmark Annotation: User Study: A: 1%, B: 2%, C: 35%, D: 47%, E: 15% (b) What is the fraction of females facing the camera? A: 0, B: 1, C: 0.8, D: 0.2 Benchmark Annotation: User Study: A: 0.0%, B: 47%, C: 45%, D: 0%, E: 8% (c) How many women are present in the image? A: 0, B: 1, C: 2, D: 3 Benchmark Annotation: User Study: A: 0.0%, B: 73%, C: 27%, D: 0.0%, E: 0.0% (d) If you were to sit on the chair closest to the window, which color would the chair be? A: Green, B: Blue, C: Red, D: White Benchmark Annotation: User Study: A: 27%, B: 67%, C: 0%, D: 0%, E: 6% (e) What is the image primarily displaying? A: Architecture, B: Animals, C: Interior design, D: Landscaping Benchmark Annotation: User Study: A: 71%, B: 0%, C: 16%, D: 4%, E: 9% (f) What is the main color of the large neon sign in the image? A: Black, B: White, C: Pink, D: Red Benchmark Annotation: User Study: A: 0%, B: 0%, C: 29%, D: 45%, E: 26% Figure S7. Noise Samples in MMStar [7]. Figs. (a)-(g): images 61.jpg, 71.jpg, 74.jpg, 76.jpg, 38.jpg, and 63.jpg taken from the MMStar dataset by Chen et al. is licensed under CC BY 4.0. Source: https://github.com/MMStar-Benchmark/MMStar. (a) What is the most noticeable feature of the image? A: The ocean, B: The dining table, C: The sunset, D: The chairs Benchmark Annotation: User Study: A: 11%, B: 37%, C: 51%, D: 1%, E: 0.0 (b) Where is the man in uniform positioned in the court in relation to the player with the ball? A: Behind the player with the ball, B: To the right of the player with the ball, C: To the left of the player with the ball, D: In front of the player with the ball Benchmark Annotation: User Study: A: 4%, B: 20%, C: 36%, D: 12%, E: 27 (c) Where is the grass on the birthday cake located? A: Its not shown in the image, B: In the middle, C: In the corners, D: Around the edges Benchmark Annotation: User Study: A: 0%, B: 47%, C: 0%, D: 0%0, E: 53% (d) What type of furniture is located in the center of the room in the image? A: Coffee table, B: Desk, C: Dining table, D: Side table Benchmark Annotation: User Study: A: 56%, B: 0%, C: 44%, D: 0%, E: 0% (e) What is the main feature in the background of the image? A: park bench near the water, B: couple sitting on bench, C: body of water and the Golden Gate Bridge, D: mountain in the distance. Benchmark Annotation: User Study: A: 1%, B: 14%, C: 53%, D: 32%, E: 0% (f) How would you describe the color of the sand in the image? A: Dark brown, B: White, C: Light gray, D: Golden Benchmark Annotation: User Study: A: 22%, B: 0%, C: 0%, D: 75%, E: 3% Figure S8. Noise Samples in SEEDBench [37]. Figs. (a)-(i): images 63856.jpg, 75478.jpg, 98071.jpg, 81719.jpg, 84847.jpg, 90925.jpg, 12416.jpg, 30778.jpg, and 69134.jpg taken from the SEED-Bench dataset by Li et al. is licensed under CC BY-NC 4.0. Source: https: //github.com/AILab-CVC/SEED-Bench. (a) Whos weaning the dress? Benchmark Annotation: Woman (b) How tall is the chair in the bottom of the photo? Benchmark Annotation: Short (c) What kind of device is on top of the desk? Benchmark Annotation: Keyboard (d) What is around the open window? Benchmark Annotation: Drapes (e) Who is standing at the table? Benchmark Annotation: Woman Figure S9. Noise Samples in GQA [31]. Figs. (a)-(e): images of couple in restaurant, formal event in tent, computer desk with red car on screen, bedroom with green sofa, and group party with wine taken from the GQA dataset by Hudson and Manning (sourced from Visual Genome) is licensed under CC BY 4.0. Source: https://cs.stanford.edu/people/dorarad/gqa/. (a) Whats the ratio(A:B) of yellow bar and blue bar for Ages 18-29? Benchmark Annotation: 1.684722222 (b) How many colors are used in the graph? Benchmark Annotation: (c) Whats the ratio of the lowest value of green bars and blue bars? Benchmark Annotation: 1.216666667 (d) How many factors are shown in the chart? Benchmark Annotation: 3 Figure S10. Noise Samples in ChartQA [46]. Figs. (a)-(d): images Older U.S. adults see COVID-19 outbreak as major threat to their personal health..., Share that agrees that vaccines are important for children to have, 2018, subset of legislators dominates the Twitter conversation, and Grades, test scores top list of factors Americans say should be considered in college admissions taken from the ChartQA dataset by Masry et al. is licensed under GPL-3.0. Source: https://github.com/vis-nlp/ChartQA. (a) The image shows python code. Is the output of the code 11? Benchmark Annotation: Yes (b) Is the actor inside the red bounding box called William Shatner? Benchmark Annotation: Yes (c) Is the area of the square in the picture equal to 40? Benchmark Annotation: Yes (d) Is there total of two display devices in the image? Benchmark Annotation: Yes (e) Is this photo taken in place of auto factory? Benchmark Annotation: Yes (f) Is there zipper in the picture? Benchmark Annotation: No (g) Are there yellow poles in the image? Benchmark Annotation: Yes (h) All apples are shown in the picture. If eat an apple every day, can eat it for three days? Benchmark Annotation: No Figure S11. Noise Samples in MME [23]. Figs. images 37.jpg, 453.jpg, 803.jpg, 895.jpg, 1713.jpg, 874.jpg, 971.jpg, and 1002.jpg taken from the MME dataset by Fu et al. is licensed for academic research use. Source: https://github.com/BradyFU/ Awesome-Multimodal-Large-Language-Models/tree/Evaluation. (a)-(h): (a) Is there skis in the image? Benchmark Annotation: Yes (b) Is there cow in the image? Benchmark Annotation: Yes (c) Is there bed in the image? Benchmark Annotation: Yes (d) Is there tv in the image? Benchmark Annotation: Yes (e) Is there car in the image? Benchmark Annotation: Yes (f) Is there bicycle in the image? Benchmark Annotation: Yes (g) Is there broccoli in the image? Benchmark Annotation: Yes (h) Is there car in the image? Benchmark Annotation: No Figure S12. Noise Samples in POPE [38]. Figs. (a)-(h): images 4.jpg, 166.jpg, 34.jpg, 132.jpg, 240.jpg, 353.jpg, 364.jpg, and 7.jpg taken from the POPE dataset by Li et al. is licensed under MIT License. Source: https://github.com/RUCAIBox/POPE. (a) Please provide the bounding box coordinate of the region this sentence describes: the persons who graduate (b) In cold winter when snow covers the ground, what part of the car in the picture needs to be cleared before the car can be safely driven? Please provide the bounding box coordinate of this region. (c) In classroom setting, students often use electronic devices to assist their learning. Can you identify an object that could provide visual information and display educational content in the picture? Please provide the bounding box coordinate of this region. (d) Please provide the bounding box coordinate of the region this sentence describes: something indicating the identity of the car Figure S13. Noise Samples in LISA-Grounding [36]. Red box indicates the annotated bounding box and the blue shows the one our model PC-GRPO predicted. Figs. (a)-(d): images 10.jpg, 65.jpg, 21.jpg, and 60.jpg taken from the LISA-Grounding (ReasonSeg) dataset by Lai et al. is licensed under CC BY-NC 4.0. Source: https://github.com/dvlab-research/LISA."
        }
    ],
    "affiliations": [
        "AI Center-Toronto, Samsung Electronics",
        "University of Toronto",
        "Vector Institute"
    ]
}