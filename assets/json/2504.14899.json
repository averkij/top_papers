{
    "paper_title": "Uni3C: Unifying Precisely 3D-Enhanced Camera and Human Motion Controls for Video Generation",
    "authors": [
        "Chenjie Cao",
        "Jingkai Zhou",
        "Shikai Li",
        "Jingyun Liang",
        "Chaohui Yu",
        "Fan Wang",
        "Xiangyang Xue",
        "Yanwei Fu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Camera and human motion controls have been extensively studied for video generation, but existing approaches typically address them separately, suffering from limited data with high-quality annotations for both aspects. To overcome this, we present Uni3C, a unified 3D-enhanced framework for precise control of both camera and human motion in video generation. Uni3C includes two key contributions. First, we propose a plug-and-play control module trained with a frozen video generative backbone, PCDController, which utilizes unprojected point clouds from monocular depth to achieve accurate camera control. By leveraging the strong 3D priors of point clouds and the powerful capacities of video foundational models, PCDController shows impressive generalization, performing well regardless of whether the inference backbone is frozen or fine-tuned. This flexibility enables different modules of Uni3C to be trained in specific domains, i.e., either camera control or human motion control, reducing the dependency on jointly annotated data. Second, we propose a jointly aligned 3D world guidance for the inference phase that seamlessly integrates both scenic point clouds and SMPL-X characters to unify the control signals for camera and human motion, respectively. Extensive experiments confirm that PCDController enjoys strong robustness in driving camera motion for fine-tuned backbones of video generation. Uni3C substantially outperforms competitors in both camera controllability and human motion quality. Additionally, we collect tailored validation sets featuring challenging camera movements and human actions to validate the effectiveness of our method."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 9 9 8 4 1 . 4 0 5 2 : r Uni3C: Unifying Precisely 3D-Enhanced Camera and Human Motion Controls for Video Generation Chenjie Cao1,2,3, Jingkai Zhou1,3, Shikai Li1,3, Jingyun Liang1,3, Chaohui Yu1,3, Fan Wang1, Xiangyang Xue2, Yanwei Fu2 1DAMO Academy, Alibaba Group 2Fudan University 3Hupan Lab Project lead Camera and human motion controls have been extensively studied for video generation, but existing approaches typically address them separately, suffering from limited data with high-quality annotations for both aspects. To overcome this, we present Uni3C, unified 3D-enhanced framework for precise control of both camera and human motion in video generation. Uni3C includes two key contributions. First, we propose plug-and-play control module trained with frozen video generative backbone, PCDController, which utilizes unprojected point clouds from monocular depth to achieve accurate camera control. By leveraging the strong 3D priors of point clouds and the powerful capacities of video foundational models, PCDController shows impressive generalization, performing well regardless of whether the inference backbone is frozen or fine-tuned. This flexibility enables different modules of Uni3C to be trained in specific domains, i.e., either camera control or human motion control, reducing the dependency on jointly annotated data. Second, we propose jointly aligned 3D world guidance for the inference phase that seamlessly integrates both scenic point clouds and SMPL-X characters to unify the control signals for camera and human motion, respectively. Extensive experiments confirm that PCDController enjoys strong robustness in driving camera motion for fine-tuned backbones of video generation. Uni3C substantially outperforms competitors in both camera controllability and human motion quality. Additionally, we collect tailored validation sets featuring challenging camera movements and human actions to validate the effectiveness of our method. Date: April 22, 2025 Project page and code: https://ewrfcas.github.io/Uni3C/ Correspondence: caochenjie.ccj@alibaba-inc.com"
        },
        {
            "title": "1 Introduction",
            "content": "Recent advancements in foundational video diffusion models (VDMs) [5, 64, 7, 27, 46, 26, 54] have unlocked unprecedented capabilities in creating dynamic and realistic video content. significant challenge in this field is achieving controllable video generation, feature with broad applications in virtual reality, film production, and interactive media. In this paper, we focus on two aspects of controllable video generation: camera control [17, 63, 69, 2, 66, 44] and human motion control [21, 57, 62, 72, 51, 22]both of which are critical and interdependent in real-world scenarios. Recent pioneering works have extensively studied controlling camera trajectories for VDMs through explicit conditions like Plücker ray [17, 2, 69, 18, 30], point clouds [66, 44, 28, 12, 40]. Concurrently, controllable human animation also attracted lot of attention based on poses [21, 62, 51, 22] or SMPL characters [72, 75, 73]. Despite these advancements, several challenges remain: 1) Most approaches deeply hack the inherent capacities of VDMs, which have been trained with domain-specific data and conditions, inevitably undermining the generalization to handle out-of-distribution scenarios. 2) Very few works investigate the joint control of both camera trajectories and human motions. This requires diverse camera trajectories in human-centric videos with high-quality annotations [57], which are often expensive to obtain. 3) There is lack of explicit and synchronized guidance that incorporates strong 3D-informed priors to concurrently control both camera movements and human motions. Relying on separate conditions, like point clouds and SMPL, struggles to represent physically reasonable interactions and positional relations between characters and environments. Figure 1 Given single-view image across various domains (e.g., real-world, text-to-image, animation), we first extract the monocular depth and focal length of it via Depth-Pro [6] and then achieve relative point clouds. Then, the proposed Uni3C can generate impressive videos under arbitrary camera trajectories (a), human motion characters (SMPL-X [36]), or both of these conditions (b). (c) Uni3C further supports the camera-controlled motion transfer. 2 To address these challenges, we present Uni3C, novel framework that Unifies precise 3D-enhanced camera and human motion Controls for video generation as shown in Figure 1 via two key innovations. Firstly, we propose the insight that controlling camera trajectories for powerful foundational VDMs can be achieved by lightweight, trainable modules with rich informed guidance and reasonable training strategies. By avoiding hacking the underlying capacities of VDMs, our camera-controlling model can be directly generalized to versatile downstream tasks rather than costly joint training and extensive Structure from Motion (SfM) labeling [47, 68, 29]. Secondly, we claimed that camera and human motion controls are inherently interdependent. Therefore, we propose to align their conditions into global 3D world during the inference phase, enabling 3D consistent generation across both domains. Formally, Uni3C is built upon the foundational VDMWan2.1 [54]. For the control of camera trajectories, we propose PCDController, plug-and-play control module with only 0.95B parameters (compared to 12B of Wan2.1) that operates on unprojected 3D point clouds derived from monocular depth estimation [6]. Thanks to the rich geometric priors of point clouds, PCDController is capable of fine-grained camera control, even when trained on constrained multi-view images and videos with frozen backbone. Furthermore, PCDController can be compatible with fine-tuned VDM backbones for versatile downstream tasks. This surprising factor supports domain-specific training, i.e., camera and human motion modules can be trained independently without jointly annotated data. For the global 3D world guidance, we align scenic point clouds (for camera control) and SMPL-X characters [36] (for human animation) into the same world-coordinate space via the rigid transformation [52], while the 2D keypoints [61] bridge the relation of two presentations. Note that our alignment enables complicated motion transfer, covering disparate characters, positions, and viewpoints as verified in the last row of Figure 1. Extensive experiments validate the efficacy of Uni3C. To evaluate the remarkable generalization of PCDController, we collect an out-of-distribution test set across different domains, where each image has four different camera trajectories. For the joint controllability, we build comprehensive test set of in-the-wild human videos. GVHMR [48] is used to extract SMPL-X as the condition, while three complex and random camera trajectories are assigned for each video. VBench++ [23] is employed to verify the overall performance of our method. Uni3C significantly outperforms other competitors, both quantitatively and qualitatively. We highlight the key contributions of Uni3C as: PCDController. robust, lightweight camera control module is proposed, which enjoys strong 3D priors from point clouds, compatible with both frozen or adapted VDMs. Global 3D World Guidance. unified inference framework that aligns scene geometry (point clouds) and human characters (SMPL-X) for 3D-coherent video control. Comprehensive Validation. We propose new benchmarks and datasets to evaluate challenging camerahuman interaction scenarios, demonstrating Uni3Cs superiority over existing approaches."
        },
        {
            "title": "2 Related Work",
            "content": "Camera Control for VDMs. Controlling camera trajectories in video generation has garnered significant attention in recent times. Some works focused on injecting camera parameters into VDMs to achieve camera controllability [58, 17, 3, 2, 69, 18, 30], typically utilizing the Plücker ray presentation. For instance, VD3D [3] designed tailored framework for Diffusion Transformer (DiT) [38], while AC3D [2] further emphasized the generalization with less trainable parameters. Moreover, DimensionX [50] further decoupled the spatial and temporal control with different LoRAs [20]. Despite the progress made by these methods, they cannot control the camera movements precisely, particularly when the case is beyond the training domains with an unknown metric scale. Thus, other recent works have employed point cloud conditions through both trainingbased [66, 44, 28, 12, 40] and training-free [19, 65] manners. However, these methods fail to accommodate the generalized model design and training strategy to handle imperfect point clouds or out-of-distribution data, especially in scenarios involving humans or animals. Unified Control for VDMs. Recent works have unified multiple conditions to guide video generation [58, 12, 13, 57, 10, 14, 70]. MotionCtrl [58] integrated the camera and object motion controls through separate pose 3 and trajectory injections. Subsequently, researchers further explored the presentation of conditions, such as point trajectories [12], point tracking [13, 14], and 3D-aware signals [10]. Humanvid [57] first unified the control of human scenes, while VidCraft3 [70] further considered the lighting control. Although these pioneering approaches achieved promising performance, they often rely heavily on joint training with various conditions and well-labeled datasets. Furthermore, there has been limited discussion on unified control within foundational VDMs that exceed 10B parameters. Our work offers solution to address these issues: unifying existing models for different downstream tasks without the need for costly fine-tuning or fully annotated datasets. This strategy is particularly well-suited for large VDMs, enabling models to focus more on enhancing performance within their specific domains."
        },
        {
            "title": "3 Preliminary: Video Diffusion Models",
            "content": "We briefly review VDMs and Wan2.1 [54] in this section as preliminary knowledge. VDMs are mainly based on the latent diffusion model [45], modeling the conditional distribution p(z0ctxt, cimg), where z0 indicate clean video latent features encoded by 3D-VAE; ctxt, cimg denote the text condition for Text-to-Video (T2V) and the optional image condition for Image-to-Video (I2V), respectively. The training of VDM involves reversing the diffusion process by the noise estimator ϵθ as: Ez0,t,ϵ,ctxt,cimg [ϵθ(zt, t, ctxt, cimg) ϵ2], min θ (1) where ϵ (0, I) indicates Gaussian noise; timestep [0, Tmax]; zt is the intermediate noisy latent state of timestep t. Recently, most VDMs have employed Flow Matching (FM) [33] as the improved diffusion process with faster convergence and more stable training. Based on the ordinary differential equations (ODEs), FM formulates the linear interpolation between z0 and z1, i.e., zt = tz1 + (1 t)z0, where [0, 1] is sampled from the logit-normal distribution. The velocity prediction vθ can be written as: Ez0,t,ϵ,ctxt,cimg [vθ(zt, t, ctxt, cimg) vt2], min θ (2) where the ground truth velocity denotes vt = dzt 26, 54] are built with DiT [38] to achieve more capacities for video generation. dt = z1 z0. Additionally, recent foundational VDMs [7, 64, Wan2.1 [54] is an open-released, large-scale VDM with DiT architecture trained with flow matching [33]. umT5 [11] is utilized as the multi-language text encoder to inject textual features into Wan2.1 through cross-attention. For image-to-video, Wan-I2V further incorporates features from CLIPs image encoder [42] to improve the results. Uni3C is primarily designed for Wan-I2V with 14B parameters, but we empirically find that it is compatible with the Wan-T2V version as verified in Section 5.4, showing convinced generalization of PCDController."
        },
        {
            "title": "4 Method",
            "content": "Overview. Given reference view Iimg R3hw, camera trajectories {ci of target views, and cam}N i=1 RN 3hw under textual condition, Uni3C uses PCDController to produce the target video {V specified camera trajectories. This can be formulated as p(z0ctxt, cimg, ccam), where z0 indicates the clean latent video feature, ctxt, cimg are textual features from umT5 and image latent condition encoded from Iimg, respectively. We show the overview pipeline of PCDController in Figure 2, which is core component of Uni3C for generalized camera control (Section 4.1) across diverse downstream tasks. In this work, we focus on human animation (Section 4.2). Subsequently, we introduce the global 3D world guidance illustrated in Figure 5 to unify both camera and human characters into consistent 3D space for inference (Section 4.3). i=1 tar}N"
        },
        {
            "title": "4.1 PCDController with 3D Geometric Priors",
            "content": "Architecture. Following AC3D [2], PCDController is designed within simplified DiT module rather than copying modules and weights from the main backbone as ControlNet [67]. To preserve the generalization of 4 Figure 2 The overview pipeline of PCDController. PCDController is built as lightweight DiT trained from scratch. We first obtain the point clouds via the monocular depth extracted from the first view. Then, the point clouds are warped and rendered into the video Vpcd. The input conditions for PCDController comprise rendered Vpcd, Plücker ray P, and the noisy latent zt. Note that only the PCDController and camera encoder are trainable in our framework. Wan-I2V, we follow the insight of training as few parameters as possible once the effective camera control has been achieved. Formally, we reduce the hidden size of PCDController from 5120 to 1024, while zero-initialized linear layers are used to project hidden size back to 5120 before being added to Wan-I2V. Moreover, as investigated in [2, 30], VDMs mainly determine camera information through shallow layers. Thus, we only inject camera-controlling features into the first 20 layers of Wan-I2V to further simplify the model. Additionally, we discard the textual condition for PCDController to alleviate intractable hallucination and remove all cross-attention modules. In this way, the overall number of trainable parameters for PCDController is reduced to 0.95B, significant reduction compared to Wan-I2V (14B). 3D Geometric Priors. In contrast to merely utilizing Plücker ray as the camera embedding [2, 30], we incorporate much stronger 3D geometric priors to compensate the model simplification, i.e., videos {V i=1 RN 3hw rendered from point clouds under given camera trajectories. Specifically, we first use Depth-Pro [6] to extract the monocular depth map from the reference view. We then align this depth map into metric representation using SfM annotations [47] or multi-view stereo [8]. Following [9], we employ RANSAC to derive the rescale and shift coefficients, preventing the collapse of constant depth outcomes. Subsequently, the point clouds Xpcd Rhw3 are got by unprojecting all 2D pixels from Iimg into the world coordinate via its metric depth ˆDimg as follows: pcd}N Xpcd(x) Rcw ˆDimg(x)K 1 ˆx, where denotes the 2D coordinates of Iimg, while ˆx refers to the homogeneous coordinates; K, Rcw mean the intrinsic and extrinsic cameras of the reference view, respectively. After that, we render {V for the remaining (N 1) views by PyTorch3D according to their respective camera intrinsics and extrinsics. Note pcd = Iimg, to confirm the identity. We apply that the first rendering corresponds to the reference image, i.e., 1 Vpcd to the 3D-VAE of Wan2.1 to achieve cpcd as the point latent condition. To further handle the significant viewpoint changes, which may extend beyond the point clouds visibility of the first frame, PCDController also includes Plücker ray embedding [60], {Pi}N is encoded by small camera encoder, comprising casual convolutions and 4-8-8 downsampling factor to keep the same sequential length as 3D-VAE outputs. Therefore, the distribution modeling of PCDController can be written as p(z0ctxt, cimg, cpcd, P). i=1 RN 6hw, as the auxiliary condition. {Pi}N i=1 pcd}N i=2 (3) 5 Figure 3 Results of PCDController with imperfect point clouds. Benefiting from the well-preserved capacity from VDM, PCDController enjoys robust generation with inferior point clouds. Figure 4 The assignment of multi-modal conditions for Uni3C. Camera, point clouds, SMPL-X [36] and Hamer [37] are all presented as 3D conditions. Discussion. We empirically find that our method demonstrates robust performance even when working with imperfect point clouds obtained through monocular depth unprojection. In this context, the point clouds serve as the primary camera control signal, facilitating the convergence of training rather than dominating the multi-view geometric and textural generations, as illustrated in Figure 3. By achieving earlier model convergence, we can better maintain the inherent capabilities of VDMs."
        },
        {
            "title": "4.2 Human Animation",
            "content": "In this paper, we explore the unified control through two human animation approaches, both of which are built on the Wan2.1 framework, targeting I2V and T2V, respectively. While these methods are not the primary focus of our work, we provide brief introduction here. Formally, the concurrently pioneering work, RealisDance-DiT [73], is used to replace the Wan-I2V backbone for high-quality human animation. RealisDance-DiT incorporates SMPL-X [36] and Hamer [37] as additional input conditions to guide human motions. To ensure the flexibility for motion transfer, RealisDance-DiT randomly selects the reference frame in the video sequence, which is not perfectly aligned with the given SMPL-X. RealisDance-DiT only trains self-attention modules and patchify encoders to confirm the generalization. However, we clarify that integrating the control branch trained within different backbones is still challenging, which is addressed by our elaborate PCDController. Furthermore, we tried another version, RealisDance-DiT-T2V, based on Wan-T2V without reference image conditions to explore the generalization of PCDController. Remarkably, PCDController adapts successfully to Wan-T2V, empowering it with impressive I2V ability. 6 Figure 5 The overview of global 3D world guidance. (a) We first align the SMPL-X characters from the human world space Whum to the environment world space Wenv comprising dense point clouds. (b) GeoCalib [53] is used to calibrate the gravity direction of SMPL-X. (c) The rigid transformation coefficients s, R, are employed to align the whole SMPL-X sequence. We re-render all aligned conditions under specific camera trajectories as the global 3D world guidance."
        },
        {
            "title": "4.3 Global 3D World Guidance",
            "content": "Definition. As illustrated in Figure 4, Uni3C adopts multi-modal conditions, including camera, point clouds, reference image, SMPL-X [36], and Hamer [37]. The first three conditions are used for PCDController, while the latter three are used for human animations. We employ GVHMR [48] to recover SMPL-X characters. One can also retrieve desired motion sequences from motion datasets [39, 41, 16, 31] or generate new motions through text-to-motion models [24, 15]. Although most of the conditions above are formulated as 3D presentations, they stay in two different world coordinates. We define the point cloud world coordinate as the environmental world space Wenv, which is the main world space controlled by cameras, while the SMPL-X is placed in the human world space Whum. It is non-trivial to control the camera across two different world spaces consistently. For example, determining the initial camera position within Whum is particularly ambiguous, especially for tasks involving motion transfer. Therefore, we propose the global 3D world guidance that places the human condition into the environment, i.e., aligning SMPL-X from Whum to Wenv as shown in Figure 5(b). Multi-Modal Alignment. Fortunately, 2D human pose keypoints subtly bridge Whum and Wenv. Formally, we i=1 R172 from the reference view Iimg by ViTPose++ [61]. Then, first estimate 17 human keypoints {ki we unproject 2D keypoints into Wenv to obtain 3D keypoints {ki i=1 R173 through metric monocular depth ˆDimg and the intrinsic camera of the reference image. For SMPL-X in Whum, the COCO17 regressor [48] is utilized to project the first SMPL-X character into {ki corresponding to the same human keypoints. Consequently, least-squares estimation [52] based rigid transformation can be used to align khum to kenv as follows: hum}17 i=1 env}17 2D}17 17 (cid:88) i=1 min s, R,t wi(s R(ki hum)T + t)T ki env2, (4) 2D where s, R, indicate the optimized scaling factor, rotation matrix, and translation vector, respectively. wi denotes the confidence weight of 2D keypoint ki . We discard any keypoints with confidence below 0.7, as they typically degrade alignment quality. Once the transformation parameters s, R, are determined, we apply them to all other SMPL-X sequences under the assumption that they share the same rigid transformation. However, even minor orientation errors can accumulate, leading to physically unrealistic motion trajectories, such as ascending into the sky or descending into the ground. To address this, we adopt GeoCalib [53] to estimate the gravity direction in Wenv, which is then employed to calibrate the SMPL-X to ensure parallel gravity directions, as illustrated in Figure 5(b). For the alignment of Hamer [37], which shares common vertices with the hand parts of SMPL-X, Hamer can also be aligned to Wenv through the rigid transformation (Equation (4)). Additionally, we address the issue of hand occlusion for Hamer by masking occluded hand parts based on the rendered depth from SMPL-X. After the alignments of SMPL-X and Hamer sequences, we place all conditions into Wenv, establishing the global 3D world guidance that allows for rendering concurrently controlled conditions under arbitrary camera trajectories and human motions as shown in Figure 5(c). Finally, these re-rendered conditions are sent to PCDController and RealisDance-DiT for generated outcomes, as shown in Figure 4. 7 Table 1 Quantitative results of camera control. VBench++ scores (%) are normalized (higher is better). Injected camera features are divided as Plücker ray and point clouds (Pcd). denotes the results with challenging 360 camera rotations. Camera I2V Plücker Pcd Score Consist Consist Quality Quality Flicker Smooth Subject Bg Aesthetic Imaging Temporal Motion Overall Subject I2V Bg ATE RPE RRE ViewCrafter [66] SEVA [71] Ours (CogVideoX) Ours (CogVideoX) Ours (Wan-I2V) Ours (Wan-I2V) PCDController Ours (Wan-I2V) PCDController 85.39 87.39 86.48 87.22 89.16 87.95 88. 89.69 91.86 93.17 91.26 94.71 91.71 92.20 91.68 93.37 93.02 92.44 94.93 92.97 93.37 55.13 56.79 55.00 56.90 60.42 58.52 58.99 64.33 68.43 66.75 69.53 72.20 71.12 71.96 92.94 95.74 95.10 94.79 96.51 95.51 95.56 97.66 98.59 98.36 98.47 98.51 98.55 98.66 95.59 97.08 94.45 96.60 97.74 97.24 97.38 96.11 0.210 0.117 0.873 97.23 0.077 0.029 0.223 95.94 0.356 0.162 1.280 97.79 0.123 0.045 0.346 98.29 0.402 0.095 0.728 97.96 0.091 0.028 0.211 98.01 0.102 0.031 0.246 1.327 0.551 6.334 1.010 0.416 4."
        },
        {
            "title": "5.1 Implementation Details\nPCDController is trained with the frozen Wan-I2V [54] on multi-resolution images scaled from [480 × 768,\n512 × 720, 608 × 608, 720 × 512, 768 × 480] of 81 frames. The learning rate is warmed up to 1e-5 for 400 steps\nand then fixed. We train the model for 6,000 steps with a batch size of 32, while more training steps would\nslightly hurt Wan-I2V’s generalization. The training is accomplished with 64 H100 GPUs for 40 hours. We\nalso provided results based on CogVideoX-5B-I2V [64], training for 20k steps with a batch size of 16. During\nthe training, we randomly drop 10% texts, as well as 5% point cloud renderings and Plücker embeddings. For\ninference, we set the classifier-free guidance scale to 5.0 for textual conditions, keeping other guidance on the\ndefault scale 1.0.",
            "content": "Datasets. To ensure the generalization of PCDController, we collect large-scale training data for camera control, including DL3DV [32], RE10K [74], ACID [34], Co3Dv2 [43], Tartainair [56], Map-Free-Reloc [1], WildRGBD [59], COP3D [49], UCo3D [35]. This comprehensive dataset encompasses variety of scenarios, featuring both static and dynamic scenes, as well as object-level and scene-level environments. Furthermore, all datasets are annotated with metric-aligned monocular depth through the way proposed in [9] or are provided with ground-truth depth."
        },
        {
            "title": "5.2 Results of Camera Control",
            "content": "Benchmark. To evaluate camera control ability, we build an out-of-distribution benchmark with 32 images across various domains, including text-to-image generation1, real-world [25, 4], object-centric, and human scenes as displayed in Figure 8. Each image features four distinct camera trajectories, resulting in 128 test samples. We used VBench++ [23] to evaluate the video quality, while absolute translation error (ATE), relative translation error (RPE), and relative rotation error (RRE) are used to verify the camera precision. We use VGGT [55] to produce the extrinsic cameras of generated images, which are evaluated with the pre-defined cameras after the trajectory alignment. Analysis. We present the quantitative results in Table 1, comparing our model with ViewCrafter [66], SEVA [71], and the CogVideoX [64] version of our framework. The qualitative outcomes are shown in Figure 9. Our experiments demonstrate that point clouds significantly enhance the controllability of both the Wan2.1 and CogVideoX, as verified by the improvement of ATE, RPE, and RRE. Although SEVA achieves precise camera trajectories, it requires massive training with static multi-view data (0.8M iterations), struggling to handle dynamic out-of-distribution scenarios, such as humans and animals, as illustrated in Figure 9. We should clarify that our baseline, Wan-I2V with only Plücker ray, suffers from inferior camera movements. While this setting achieves strong VBench overall score, it compromises with poor camera metrics. Overall, the proposed PCDContoller achieves the optimal balance between video quality and camera precision. By integrating both Plücker rays and point clouds, it further enhances the performance in challenging scenes featuring substantial viewpoint changes, as validated in Table 1 and Figure 10. 1https://github.com/black-forest-labs/flux 8 Table 2 Quantitative results of unified camera and human motion controls. We re-render SMPL-X and Hamer under new camera trajectories for the comparison of RealisDance-DiT [73] without explicitly incorporating point clouds or cameras. means masking out the foreground point clouds. Control Overall Subject Bg Camera Human Score Consist Consist Quality Quality Aesthetic Imaging Temporal Motion Smooth Flicker ATE RPE RRE RealisDance-DiT [73] PCDController Uni3C (T2V) Uni3C (T2V) Uni3C (I2V) 85.21 83.19 83.34 83.16 83.43 93.03 89.08 88.45 88.67 89. 95.34 91.63 91.45 91.38 93.05 57.89 57.23 57.45 56.79 57.25 68.71 68.27 69.84 69.42 67.28 97.44 95.22 95.21 95.14 95.70 98.82 97.71 97.63 97.57 97.86 0.549 0.195 0.547 0.256 0.092 0.661 0.296 0.098 1.167 0.262 0.083 0.606 0.093 0.490 0. Figure 6 Ablation results of Plücker ray and point clouds during the training phase. Point clouds enjoy highly accurate camera control against Plücker ray."
        },
        {
            "title": "5.3 Results of Unified Camera and Human Motion Control",
            "content": "Benchmark. We have developed new benchmark of 50 videos, each featuring person performing challenging motions. For guidance, SMPL-X is extracted using GVHMR [48]. Each video is assigned three different types of camera trajectories, resulting in total of 150 test cases. To ensure that the person remains within the cameras viewpoint, we employ follow shooting technique for all test cases, adjusting the cameras position based on the movement of the SMPL-X center. These noisy and subtle movements further increase the difficulty of camera control. We follow the same camera metrics as mentioned in Section 5.2. To facilitate generalization for motion transfer, RealisDance-DiT is not specifically designed to perfectly recover the first frame aligned with the reference image. Consequently, we remove the metrics that heavily depend on the reference view (I2V subject and background) for fairness. Analysis. We show the quantitative results in Table 2, while qualitative results are displayed in Figure 11. To the best of our knowledge, there are currently no publicly available models that effectively address the challenge of unified camera and human motion controls. Therefore, we compare Uni3C against the baseline of RealisDance-DiT [73] and various ablation versions of our model. Notably, while RealisDance-DiT, which focuses solely on human control, achieves the best visual quality, it struggles to produce accurate camera trajectories, resulting in poor camera metrics. In contrast, Uni3C shows good VBench scores alongside impressive camera metrics. Note that the proposed PCDController can also be generalized to the T2V model with comparable quality, featuring the robust generalization of PCDController. Moreover, an interesting insight is revealed from Figure 11 that the aligned SMPL-X characters can further strengthen the camera controllability, while the PCDController trained with I2V formulation also enhances the visual quality and consistency of RealisDance-DiT. This illustrates the complementary features of these two components. Additionally, we show that the proposed Uni3C enables control of detailed hand motions under various camera trajectories as in Figure 12."
        },
        {
            "title": "5.4 Ablation Study and Exploratory Discussions",
            "content": "Plücker Ray vs Point Clouds. As shown in Table 1, point clouds enjoy significantly more precise camera trajectories. We further clarify that video camera control trained with point clouds achieves much faster convergence with lower training loss, as illustrated in Figure 6. Only 1,000 training iterations can hold the general camera trajectory. We empirically find that timing large-scale VDM like Wan-I2V through Plücker ray is difficult. Maybe enabling more trainable parameters would improve the performance, potentially hindering the generalization, which is not considered in this work. Moreover, as verified in the last two rows of Table 1, 9 Figure 7 Rendering results with and without gravity calibration by GeoCalib [53]. using both Plücker ray and point clouds improves the results of very challenging camera trajectories. Point Clouds of Humans. The point clouds of humans are always frozen in the world space without any motion, which creates conflict with the human motion conditions provided by SMPL-X. Eliminating this redundant information is straightforward idea to improve motion quality However, as verified in Table 2, while Uni3C-T2V with human-masked point clouds achieves slightly better visual quality, this masking adversely affects camera precision, particularly when humans occupy significant area of the image. Therefore, retaining the point clouds of humans is essential for effective camera control. Given that Wan2.1 [54] is trained on videos featuring substantial motion, it can generate natural and smooth movements even when the foreground point clouds remain fixed. Gravity Calibration. As mentioned in Section 4.3, gravity calibration is critical for aligning the global 3D world space. Results shown in Figure 7 verify that the calibration can correct the SMPL-X characters aligned with skewed human point clouds and eliminate the error accumulation for humans long-distance movements. Motion Transfer. We present results of motion transfer achieved by the Uni3C framework in Figure 13. Our model effectively controls both camera trajectories and human motions, even when reference motions are sourced from different videos or distinct domains, such as animation and real-world scenes. Meanwhile, Uni3C can be further extended to generate vivid videos based on other conditions, like text-to-motion guidance or retrieved motions from motion databases. To prove this point, we randomly integrate several motion clips from BABEL [41] and use Uni3C to control both motion and camera, as illustrated in Figure 14. More results are shown on our projected page. Limitation. Although Uni3C supports flexible and diverse unified control and motion transfer, it operates under the constraints of predefined camera trajectories and human characters (SMPL-X). Consequently, Uni3C may struggle to produce physically plausible outcomes when human motions conflict with environmental conditions, as illustrated in Figure 15. For instance, if humans movement trajectory is blocked by walls, barriers, or other objects, the generated results may exhibit artifacts such as distortion, clipping, or sudden disappearance. This limitation could be mitigated by employing more advanced human motion generation method that accounts for physical obstructions within the environment."
        },
        {
            "title": "6 Conclusion",
            "content": "This paper introduced Uni3C, framework that unifies 3D-enhanced camera and human motion controls for video generation. We first propose the PCDController, demonstrating that lightweight, trainable modules, and rich geometric priors from 3D point clouds can efficiently manage camera trajectories without compromising the inherent capacities of foundational VDMs. This not only enhances generalization but also facilitates versatile downstream applications without joint training. Furthermore, by aligning multi-modal conditions, including both environmental point clouds and human characters in global 3D world space, we established coherent framework for jointly controlling camera movements and human animations. Our comprehensive experiments validated the efficacy of Uni3C across diverse datasets, showcasing superior performance in 10 Figure 8 Our out-of-distribution benchmark for camera control. The validation set includes generative, human, scene-level, and object-level images with diverse aspect ratios. both quantitative and qualitative assessments compared to existing approaches. The significance of our contributions lies not only in improving the state-of-the-art in controllable video generation but also in proposing robust way to inject multi-modal conditions without the requirements of heavily annotated data. We believe that Uni3C paves the way for more advanced controllable video generation."
        },
        {
            "title": "References",
            "content": "[1] Eduardo Arnold, Jamie Wynn, Sara Vicente, Guillermo Garcia-Hernando, Aron Monszpart, Victor Prisacariu, Daniyar Turmukhambetov, and Eric Brachmann. Map-free visual relocalization: Metric pose relative to single image. In European Conference on Computer Vision, pages 690708. Springer, 2022. [2] Sherwin Bahmani, Ivan Skorokhodov, Guocheng Qian, Aliaksandr Siarohin, Willi Menapace, Andrea Tagliasacchi, David Lindell, and Sergey Tulyakov. Ac3d: Analyzing and improving 3d camera control in video diffusion transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. [3] Sherwin Bahmani, Ivan Skorokhodov, Aliaksandr Siarohin, Willi Menapace, Guocheng Qian, Michael Vasilkovsky, Hsin-Ying Lee, Chaoyang Wang, Jiaxu Zou, Andrea Tagliasacchi, et al. Vd3d: Taming large video diffusion transformers for 3d camera control. In International Conference on Learning Representations, 2025. [4] Jonathan Barron, Ben Mildenhall, Dor Verbin, Pratul Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded anti-aliased neural radiance fields. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 54705479, 2022. [5] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [6] Aleksei Bochkovskii, Amaël Delaunoy, Hugo Germain, Marcel Santos, Yichao Zhou, Stephan R. Richter, and Vladlen Koltun. Depth pro: Sharp monocular metric depth in less than second. arXiv, 2024. 11 Figure 9 Qualitative results of camera control on our benchmark. We compare the proposed PCDController to ViewCrafter [66], SEVA [71], and our model without point cloud guidance. The leftmost image is the reference condition. full indicates using both Plücker ray and point clouds as conditions. 12 Figure 10 Results of the challenging orbital 360 rotations from PCDController. The leftmost images are the reference views. 13 Figure 11 Results of unified camera and human motion controls. The leftmost images are the reference views, while the first row indicates the aligned 3D world guidance. 14 Figure 12 Results of unified camera, human motion, and Hamer controls. The leftmost images are the reference views, while the first and second rows indicate the aligned 3D world guidance and Hamer rendering, respectively. 15 Figure 13 Results of motion transfer. The first row indicates the reference video, while others show our generated videos transferring motions from the reference sequence. 16 Figure 14 Results transferred from random integrated motion clips of BABEL [41]. The motion sequences are listed on the left, which are executed from light to dark colors. Figure 15 Failed cases generated by Uni3C. These results are primarily limited by the conflict between human motions and environments. 17 [7] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. [8] Chenjie Cao, Xinlin Ren, and Yanwei Fu. Mvsformer++: Revealing the devil in transformers details for multi-view stereo. In International Conference on Learning Representations, 2024. [9] Chenjie Cao, Chaohui Yu, Shang Liu, Fan Wang, Xiangyang Xue, and Yanwei Fu. Mvgenmaster: Scaling multi-view generation from any image via 3d priors enhanced diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. [10] Yingjie Chen, Yifang Men, Yuan Yao, Miaomiao Cui, and Liefeng Bo. Perception-as-control: Fine-grained controllable image animation with 3d-aware motion representation. arXiv preprint arXiv:2501.05020, 2025. [11] Hyung Won Chung, Noah Constant, Xavier Garcia, Adam Roberts, Yi Tay, Sharan Narang, and Orhan Firat. Unimax: Fairer and more effective language sampling for large-scale multilingual pretraining. arXiv preprint arXiv:2304.09151, 2023. [12] Wanquan Feng, Jiawei Liu, Pengqi Tu, Tianhao Qi, Mingzhen Sun, Tianxiang Ma, Songtao Zhao, Siyu Zhou, and Qian He. I2vcontrol-camera: Precise video camera control with adjustable motion strength. In International Conference on Learning Representations, 2025. [13] Daniel Geng, Charles Herrmann, Junhwa Hur, Forrester Cole, Serena Zhang, Tobias Pfaff, Tatiana Lopez-Guevara, Carl Doersch, Yusuf Aytar, Michael Rubinstein, Chen Sun, Oliver Wang, Andrew Owens, and Deqing Sun. Motion prompting: Controlling video generation with motion trajectories. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2025. [14] Zekai Gu, Rui Yan, Jiahao Lu, Peng Li, Zhiyang Dou, Chenyang Si, Zhen Dong, Qifeng Liu, Cheng Lin, Ziwei Liu, et al. Diffusion as shader: 3d-aware video diffusion for versatile video generation control. arXiv preprint arXiv:2501.03847, 2025. [15] Chuan Guo, Yuxuan Mu, Muhammad Gohar Javed, Sen Wang, and Li Cheng. Momask: Generative masked modeling of 3d human motions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19001910, 2024. [16] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. Generating diverse and natural 3d human motions from text. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 51525161, 2022. [17] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. In International Conference on Cameractrl: Enabling camera control for text-to-video generation. Learning Representations, 2025. [18] Hao He, Ceyuan Yang, Shanchuan Lin, Yinghao Xu, Meng Wei, Liangke Gui, Qi Zhao, Gordon Wetzstein, Lu Jiang, and Hongsheng Li. Cameractrl ii: Dynamic scene exploration via camera-controlled video diffusion models. arXiv preprint arXiv:2503.10592, 2025. [19] Chen Hou, Guoqiang Wei, Yan Zeng, and Zhibo Chen. Training-free camera control for video generation. In International Conference on Learning Representations, 2025. [20] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. [21] Li Hu. Animate anyone: Consistent and controllable image-to-video synthesis for character animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81538163, 2024. [22] Li Hu, Guangyuan Wang, Zhen Shen, Xin Gao, Dechao Meng, Lian Zhuo, Peng Zhang, Bang Zhang, and Liefeng Bo. Animate anyone 2: High-fidelity character image animation with environment affordance. arXiv preprint arXiv:2502.06145, 2025. 18 [23] Ziqi Huang, Fan Zhang, Xiaojie Xu, Yinan He, Jiashuo Yu, Ziyue Dong, Qianli Ma, Nattapol Chanpaisit, Chenyang Si, Yuming Jiang, et al. Vbench++: Comprehensive and versatile benchmark suite for video generative models. arXiv preprint arXiv:2411.13503, 2024. [24] Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and Tao Chen. Motiongpt: Human motion as foreign language. Advances in Neural Information Processing Systems, 36:2006720079, 2023. [25] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and temples: Benchmarking large-scale scene reconstruction. ACM Transactions on Graphics (ToG), 36(4):113, 2017. [26] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [27] Kuaishou. Kling. https://klingai.kuaishou.com, 2024. [28] Teng Li, Guangcong Zheng, Rui Jiang, Tao Wu, Yehao Lu, Yining Lin, Xi Li, et al. Realcam-i2v: Realworld image-to-video generation with interactive complex camera control. arXiv preprint arXiv:2502.10059, 2025. [29] Zhengqi Li, Richard Tucker, Forrester Cole, Qianqian Wang, Linyi Jin, Vickie Ye, Angjoo Kanazawa, Aleksander Holynski, and Noah Snavely. Megasam: Accurate, fast, and robust structure and motion from casual dynamic videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. [30] Hanwen Liang, Junli Cao, Vidit Goel, Guocheng Qian, Sergei Korolev, Demetri Terzopoulos, Konstantinos Plataniotis, Sergey Tulyakov, and Jian Ren. Wonderland: Navigating 3d scenes from single image. arXiv preprint arXiv:2412.12091, 2024. [31] Jing Lin, Ailing Zeng, Shunlin Lu, Yuanhao Cai, Ruimao Zhang, Haoqian Wang, and Lei Zhang. Motion-x: large-scale 3d expressive whole-body human motion dataset. Advances in Neural Information Processing Systems, 36:2526825280, 2023. [32] Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, et al. Dl3dv-10k: large-scale scene dataset for deep learning-based 3d vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2216022169, 2024. [33] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [34] Andrew Liu, Richard Tucker, Varun Jampani, Ameesh Makadia, Noah Snavely, and Angjoo Kanazawa. Infinite nature: Perpetual view generation of natural scenes from single image. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1445814467, 2021. [35] Xingchen Liu, Piyush Tayal, Jianyuan Wang, Jesus Zarzar, Tom Monnier, Konstantinos Tertikas, Jiali Duan, Antoine Toisoul, Jason Zhang, Natalia Neverova, et al. Uncommon objects in 3d. arXiv preprint arXiv:2501.07574, 2025. [36] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and Michael Black. Expressive body capture: 3d hands, face, and body from single image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1097510985, 2019. [37] Georgios Pavlakos, Dandan Shan, Ilija Radosavovic, Angjoo Kanazawa, David Fouhey, and Jitendra Malik. Reconstructing hands in 3d with transformers. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, pages 98269836, 2024. [38] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [39] Matthias Plappert, Christian Mandery, and Tamim Asfour. The kit motion-language dataset. Big data, 4(4):236252, 2016. 19 [40] Stefan Popov, Amit Raj, Michael Krainin, Yuanzhen Li, William Freeman, and Michael Rubinstein. Camctrl3d: Single-image scene exploration with precise 3d camera control. arXiv preprint arXiv:2501.06006, 2025. [41] Abhinanda Punnakkal, Arjun Chandrasekaran, Nikos Athanasiou, Alejandra Quiros-Ramirez, and Michael Black. Babel: Bodies, action and behavior with english labels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 722731, 2021. [42] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [43] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny. Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021. [44] Xuanchi Ren, Tianchang Shen, Jiahui Huang, Huan Ling, Yifan Lu, Merlin Nimier-David, Thomas Müller, Alexander Keller, Sanja Fidler, and Jun Gao. Gen3c: 3d-informed world-consistent video generation with precise camera control. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. [45] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1068410695, 2022. [46] RunwayML. Gen-3 alpha. https://runwayml.com/research/ introducing-gen-3-alpha, 2024. [47] Johannes Lutz Schönberger, Enliang Zheng, Marc Pollefeys, and Jan-Michael Frahm. Pixelwise view selection for unstructured multi-view stereo. In European conference on computer vision, 2016. [48] Zehong Shen, Huaijin Pi, Yan Xia, Zhi Cen, Sida Peng, Zechen Hu, Hujun Bao, Ruizhen Hu, and Xiaowei Zhou. World-grounded human motion recovery via gravity-view coordinates. In SIGGRAPH Asia 2024 Conference Papers, pages 111, 2024. [49] Samarth Sinha, Roman Shapovalov, Jeremy Reizenstein, Ignacio Rocco, Natalia Neverova, Andrea Vedaldi, and David Novotny. Common pets in 3d: Dynamic new-view synthesis of real-life deformable categories. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 48814891, 2023. [50] Wenqiang Sun, Shuo Chen, Fangfu Liu, Zilong Chen, Yueqi Duan, Jun Zhang, and Yikai Wang. Dimensionx: Create any 3d and 4d scenes from single image with controllable video diffusion. arXiv preprint arXiv:2411.04928, 2024. [51] Shuai Tan, Biao Gong, Xiang Wang, Shiwei Zhang, Dandan Zheng, Ruobing Zheng, Kecheng Zheng, Jingdong Chen, and Ming Yang. Animate-x: Universal character image animation with enhanced motion representation. In International Conference on Learning Representations, 2025. [52] Shinji Umeyama. Least-squares estimation of transformation parameters between two point patterns. IEEE Transactions on Pattern Analysis & Machine Intelligence, 13(04):376380, 1991. [53] Alexander Veicht, Paul-Edouard Sarlin, Philipp Lindenberger, and Marc Pollefeys. Geocalib: Learning single-image calibration with geometric optimization. In European Conference on Computer Vision, pages 120. Springer, 2024. [54] Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [55] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2025. 20 [56] Wenshan Wang, Delong Zhu, Xiangwei Wang, Yaoyu Hu, Yuheng Qiu, Chen Wang, Yafei Hu, Ashish Kapoor, and Sebastian Scherer. Tartanair: dataset to push the limits of visual slam. 2020. [57] Zhenzhi Wang, Yixuan Li, Yanhong Zeng, Youqing Fang, Yuwei Guo, Wenran Liu, Jing Tan, Kai Chen, Tianfan Xue, Bo Dai, et al. Humanvid: Demystifying training data for camera-controllable human image animation. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. [58] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. [59] Hongchi Xia, Yang Fu, Sifei Liu, and Xiaolong Wang. Rgbd objects in the wild: scaling real-world 3d object learning from rgb-d videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2237822389, 2024. [60] Yinghao Xu, Hao Tan, Fujun Luan, Sai Bi, Peng Wang, Jiahao Li, Zifan Shi, Kalyan Sunkavalli, Gordon Wetzstein, Zexiang Xu, and Kai Zhang. Dmv3d: Denoising multi-view diffusion using 3d large reconstruction model. In International Conference on Learning Representations, 2024. [61] Yufei Xu, Jing Zhang, Qiming Zhang, and Dacheng Tao. Vitpose++: Vision transformer for generic body pose estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 46(2):12121230, 2023. [62] Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng Shou. Magicanimate: Temporally consistent human image animation using diffusion model. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14811490, 2024. [63] Shiyuan Yang, Liang Hou, Haibin Huang, Chongyang Ma, Pengfei Wan, Di Zhang, Xiaodong Chen, and Jing Liao. Direct-a-video: Customized video generation with user-directed camera movement and object motion. In ACM SIGGRAPH 2024 Conference Papers, pages 112, 2024. [64] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. In International Conference on Learning Representations, 2025. [65] Meng You, Zhiyu Zhu, Hui Liu, and Junhui Hou. Nvs-solver: Video diffusion model as zero-shot novel view synthesizer. In International Conference on Learning Representations, 2025. [66] Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, Tien-Tsin Wong, Ying Shan, and Yonghong Tian. Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis. arXiv preprint arXiv:2409.02048, 2024. [67] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 38363847, 2023. [68] Wang Zhao, Shaohui Liu, Hengkai Guo, Wenping Wang, and Yong-Jin Liu. Particlesfm: Exploiting dense point trajectories for localizing moving cameras in the wild. In European conference on computer vision (ECCV), 2022. [69] Guangcong Zheng, Teng Li, Rui Jiang, Yehao Lu, Tao Wu, and Xi Li. Cami2v: Camera-controlled image-to-video diffusion model. In International Conference on Learning Representations, 2025. [70] Sixiao Zheng, Zimian Peng, Yanpeng Zhou, Yi Zhu, Hang Xu, Xiangru Huang, and Yanwei Fu. Vidcraft3: Camera, object, and lighting control for image-to-video generation. arXiv preprint arXiv:2502.07531, 2025. [71] Jensen Jinghao Zhou, Hang Gao, Vikram Voleti, Aaryaman Vasishta, Chun-Han Yao, Mark Boss, Philip Torr, Christian Rupprecht, and Varun Jampani. Stable virtual camera: Generative view synthesis with diffusion models. arXiv e-prints, pages arXiv2503, 2025. 21 [72] Jingkai Zhou, Benzhi Wang, Weihua Chen, Jingqi Bai, Dongyang Li, Aixi Zhang, Hao Xu, Mingyang Yang, and Fan Wang. Realisdance: Equip controllable character animation with realistic hands. arXiv preprint arXiv:2409.06202, 2024. [73] Jingkai Zhou, Yifan Wu, Shikai Li, Min Wei, Chao Fan, Weihua Chen, Wei Jiang, and Fan Wang. Realisdance-dit: Simple yet strong baseline towards controllable character animation in the wild. arXiv preprint, 2025. [74] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: learning view synthesis using multiplane images. ACM Trans. Graph., 37(4), July 2018. [75] Shenhao Zhu, Junming Leo Chen, Zuozhuo Dai, Zilong Dong, Yinghui Xu, Xun Cao, Yao Yao, Hao Zhu, and Siyu Zhu. Champ: Controllable and consistent human image animation with 3d parametric guidance. In European Conference on Computer Vision, volume 15113, pages 145162, 2024."
        }
    ],
    "affiliations": [
        "DAMO Academy, Alibaba Group",
        "Fudan University",
        "Hupan Lab"
    ]
}